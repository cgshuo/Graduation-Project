 Although PageRank has been designed to estimate the popularity of Web pages, it is a general algorithm that can be ap plied to the analysis of other graphs other than one of hypertex t documents. In this paper, we explore its application to sentiment analysis and opinion mining: i.e. the ranking of items based on user textual reviews. We first propose various techniques using collocation and pivot words to extract a weighted graph of terms fr om user reviews and to account for positive and negative opinions. We refer to this graph as the sentiment graph. Using PageRank and a very small set of adjectives (such as  X  X ood X ,  X  X xcellent X , etc.) w e rank the different items. We illustrate and evaluate our approach usin g reviews of box office movies by users of a popular movie review si te. The results show that our approach is very effective and that t he ranking it computes is comparable to the ranking obtained from the box office figures. The results also show that our approach is able to compute context-dependent ratings. H.3.3 [ Information Search and Retrieval ]: Retrieval models, Selection process; I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  Text Analysis; H.2.8 [ Database Applications ]: Data mining Algorithms, Measurement, Performance, Experimentati on. Keywords : Opinion Mining, Ranking, PageRank. The success of Web 2.0 can be sized by the increasi ng popularity of forums and media in which users and organizations e xpress and share views on anything and everything. Reviews can be found on tripadvisor X , for instance. Focused reviews are ava ilable on the World Wide Web for items as varied as consu mer electronics hotels, public schools and election candidates. Nam e it; folks have reviewed it! 1 Unfortunately, the abundance of reviews also makes it challenging for users to compare and rank different items. Alth ough quantitative scale, in notation, in criteria, etc. from website to website and hence are hard to aggregate. Some reviews are not even ra ted. In addition, quantitative ratings have been found to be insuffic ient in reflecting the opinions of the corresponding textual reviews [ 1, 2]. The ability to automatically rank different items based on thei r textual reviews will definitely be beneficial. This is the purpose of our paper. In the field of information retrieval, given a grap h where edges are weighted by the probability of traversing from one vertex to another, to rank the vertices we can simulate a random walk on the graph. As a walker proceeds in this random walk from vertex t o vertex, he visits some vertices more often than the others. Th e vertices can therefore be ranked according to their scores: the probabilities that a walker will arrive at the vertices after the random walk. To simulate such random walk and compute the scores of the vertices, we can represent the graph by its adjacen cy matrix and compute the fix point of the product of the matrix with itself [3] or approximate the computation of the fix point with P ageRank [4] which introduces  X  X atigue X  to the random walker. In this paper, we use the idea of random walk using PageRank to rank movies according to the opinions expressed in their textual reviews. PageRank has been designed to rank Web pages. Unlik e hypertext documents where edges are explicitly available (hyp erlinks), there are no obvious edges that can be derived from movie reviews to build a graph where movies are vertices. Comparing movies based on general textual similarities of their reviews ma y not be entirely appropriate as movies tell different stories. The g eneral textual similarities also do not reflect differences in the opinions expressed about the movie. We could then consider looking for sentences like  X  X ovie A is better than movie B X , which make direct comparison between movies. Yet, such sentences are rarely foun d in individual reviews. Instead, in the reviews we commonly find s entences such as  X  X  had a great time X  and  X  X he movie was horrible  X  which are expressing an opinion about the movie by means of a djectives. This work was partially funded by the National Uni versity of Singapore ARG project R-252-000-285-112, "Mind Your Language"  X  X reat X  suggests a positive opinion.  X  X orrible X  sug gests a negative opinion. Whether an adjective expresses a positive or negative opinion is referred to as its semantic orientation. Other researchers have studied the semantic orientation of adjectives to infer opinion [5, 6, 7]. We also very commonly find sentences suc h as  X  X his movie is good and funny X  or  X  X his movie is boring but has a good ending X  in the reviews. The collocation of adjectives in su ch sentences forms, reinforces and amends the opinion expressed.
 However, although some adjectives may have some pos itive or negative universal semantic orientation (e.g.  X  X ood  X ,  X  X xcellent X ,  X  X ad X ,  X  X oor X ) other adjectives X  orientation may no t be known or depend on context [8, 9, 10, 11]. The design of eff ective context-dependent methods is generally considered a challen ge in natural language processing [9]. We propose to use PageRank with a graph where vertices are adjectives and edges represent c ollocation (i.e. context-dependent clues on the semantic relationshi ps between adjectives). Starting from a set of known adjective s (i.e. adjectives that have positive or negative universal semantic o rientations), PageRank propagates their semantic orientations to other vertices whose orientations are not yet known and computes t he semantic orientation scores. We can then rank movies by comp uting their individual scores from the semantic orientation sco res of the adjectives in the movies X  reviews. The higher the s core of its adjectives, the more positive the opinions expresse d about a movie: the higher the rank of the movie. Furthermore, the semantic orientation of an adjecti ve may depend on further facets of its context. For example, the adjective  X  X unny X  may have a positive semantic orientation when used in the review of a comedy movie:  X  X he movie is so funny I had a good laugh X , but may have a negative semantic orientation when used in the review of an action movie:  X  X he villain looks a bit funny it was weird X . We can therefore build the graph of adjectives for dif ferent context and granularity: we can build a single graph based on a ll reviews, we can build a graph by genre, or a graph by movie (we cou ld also build graph by authors, by date or any other facets of co ntext). In this paper we present the results for graph built from a ll reviews, graph built from reviews grouped by genre (e.g. comedy, a ction, horror etc.) and graph built from reviews grouped by movie . In summary, we propose a practical context-dependen t ranking procedure that can rank movies directly from their user reviews with no other resource required. The procedure is threef old. We first propose a simple yet effective technique for constr ucting a weighted graph of adjectives from the reviews. We use part-o f-speech tagging, collocation and pivot words such as conjunctions (e .g.  X  X nd X ) and adverbs (e.g.  X  X owever X ) to create the graph. We re fer to this graph as the sentiment graph. The graph is then used to c ompute semantic orientation scores of individual adjectives using P ageRank. The scores of the individual adjectives from all the mo vie X  X  reviews are combined to get the movie X  X  score. The movies are r anked according to their scores. We illustrate and evaluate our approach using revie ws of recent box office movies by users of a popular movie review si te. To measure the effectiveness of our ranking we use different m etrics such as average ranking error , percentage of overlap , and percentage of rank overlap . We also look into the granularity of ranking and measure its effectiveness with regard to the information loss that a coarser ranking incurs. The results of this extensi ve performance evaluation demonstrate that the method we propose i s very effective and can produce ranking comparable to the ranking i nduced from the box office figures and also show the limitation s of user ratings. The results also confirm the context-dependence of the method. Naturally, the approach can be straightforwardly ap plied to other items and reviews such as hotels, books and so on. Although we do not report these results here, we have conducted fu rther experiments in other such domains that confirm the general effe ctiveness of our proposed approach. Our contribution is fourfold. Firstly, from a ranki ng perspective, we contribute by making it possible to rank items usin g PageRank applied on a different graph other than the graph w here the vertices are the items to rank. We use a related graph const ructed from smaller components (adjectives) that express opinio ns about the items. This makes it possible to rank items based o n opinions. Secondly, from the opinion mining perspective, we c ontribute by using PageRank algorithm to rank items context-depe ndently. Thirdly, we contribute by introducing information loss as a novel metric for measuring ranking. Lastly, we contribute a practical, effective and perhaps even predictive method for ra nking items based on opinions expressed in their reviews. The rest of the paper is organized as follows. In s ection 2, we survey related works. In section 3 we present our proposed method. In section 4 we present results of our experiments and we conclude in section 5. Sentiment analysis and opinion mining are the gener ic natural language processing and text mining tasks involved in the processing of documents that express views and revi ews in order to identify attitudes. One specific instance of opinio n mining is the rating and ranking of items based on textual review s. Its subtasks consist of determining and quantifying semantic ori entation (positive or negative). The semantic orientation of an item, the feature of an item or the review for an item is usu ally aggregated from the semantic orientation of terms in the revie ws: words, word senses or words of certain classes. The semantic or ientation of terms is determined using starting set of terms whose sem antic orientation is known and the terms X  context in the review (coll ocation and pivot words such as conjunctions and adverbs, for instanc e), in some corpus (the Web), or in some known ontological reso urce like WordNet. The authors of [12] propose to determine the semant ic orientation of adjectives in texts. They use conjunctions (e.g.  X  X  nd X ,  X  X ut X ) to derive the semantic orientation of adjectives. For instance  X  X nd X  connects adjectives of the same orientation and  X  X u t X  connects adjectives of different orientation. A clustering a lgorithm partitions adjectives into positive and negative clusters base d on the conjunctions that links them. The authors of [13] propose to determine the semant ic orientation of word senses. They construct a lexicon called SentiW ordNet where each word sense is associated with three scores, an objective score, a positive score and a negative score, to represent i ts semantic orientation. They use WordNet synsets and lexical r elations together with a machine learning classifier to determine the scores. The same authors in [14] use PageRank on WordNet for the sam e task. The author of [5] proposes to determine the semanti c orientation of reviews by determining and aggregating the semantic orientation of phrases in the reviews that contain adjectives and adverbs. He quantifies the semantic orientation of a phrase usi ng its collocation with positive and negative adjectives and adverbs i n Web documents as retrieved by search engines. The revie w is then classified as  X  X ecommended X  if the average semantic orientation of otherwise. The authors of [8] propose to determine the semanti c orientation of user friendliness of its menus, etc.). For this the y determine and aggregate the semantic orientation of words in revi ews. They leverage the observation that opinions with the sam e semantic orientation are commonly expressed in consecutive s entences, unless words such as  X  X ut X ,  X  X owever X  articulate th e successive sentences. If such words appear, the orientation is changed. The orientations of other words. If the overall score o f words expressed on a feature f in the sentence s is positive (resp. negative), then the semantic orientation of the opinion on f in the sentence s is positive (resp. negative). The authors of [8] argue that semantic orientation should be context-dependent. It must capture usage in context. For in stance the adjective  X  X harp X  may be positive or negative depen ding on the item being reviewed. Our approach quantifies the semantic orientation of opinions about the items in order to rank the items. We use colloc ation and pivot words such as conjunctions and adverbs to construct the sentiment graph. We use PageRank algorithm and a starting set of adjectives to quantify semantic orientations of adjectives in the graph. We aggregate these semantic orientation scores of adje ctives to determine the final scores of the items. We rank th e items according to their scores. Our approach is context-dependent. In this paper, in order to measure the effectivenes s of the proposed approach and its variants, we study different granu larity of ranking. Based on the quantitative scores of items obtained, we may either rank the items individually or we may group the ite ms based on their scores: i.e. the highest m items, second highest m items, third highest m items, etc., and then rank the groups. m can take the integer value between 1 to N item , where N total number of items. m = 1 means we rank the items individually (finest granularity), m = N item means we group all items into one group and assign this group a rank (coarsest granul arity). We introduce coarser granularity ranking ( m &gt; 1) because we believe users may often be more interested in knowi ng which group of movies is good, which group of movies is medium, and which group of movies is bad instead of the individual ra nking of each movie. We use the metric introduced by [15] to measure the coarseness of grouping in terms of information loss. In this mode l, information scores of items in its group. Total information los s is the sum of information loss of all items in the data set. We o ppose ranking effectiveness to the information loss that a coarse r ranking incurs. As far as we know, this is a novel way of measuring ranking. The procedure we propose for ranking items based on the text of their reviews is threefold. We first construct a se ntiment graph from the collocation of adjectives, taking into account pivot words such as conjunctions and adverbs. Then we compute the se mantic orientation scores of individual adjectives using P ageRank algorithm and a starting set of known adjectives. F inally we aggregate the semantic orientation scores of adject ives in all the reviews of an item to compute the item X  X  semantic o rientation score and ranking. The sentiment graph is constructed as follows. We d efine three variants of the method, depending on whether we con struct a sentiment graph from reviews grouped by item, a sen timent graph from reviews grouped by genre, or a sentiment graph from all reviews. We refer to these variants as individual_ byGenre_, and all_ respectively. The text of the reviews to be processed is first ta gged using a part of speech tagger to identify adjectives. It is also se gmented into sentences. We use Brill X  X  part-of-speech tagger [16 ] and Ratnaparkhi X  X  sentence splitter [17]. We then extract adjectives from the text of the rev iews. The adjectives constitute the vertices of the graph. He re, we have assumed that the adjectives in the reviews are rela ted to the movie. There may be other adjectives in the reviews that m ay not be related to the movie. For example the adjective  X  X errible X  in the sentence  X  X  watch this movie in a terrible cinema X . However, we believe that such usage of adjectives (which is not related to t he movie in review) is infrequent; therefore its effect can be minimized when we take a large number of reviews. There exists an edge between two vertices if the co rresponding adjectives occur in the same sentence (i.e. if they collocate). The weight of the edge is commensurate to the number of sentences in which the two adjectives collocate. Collocation bet ween adjectives indicates either reinforcement or amendments of sem antic orientations between the adjectives. We obtain a graph G pn =&lt;N, E pn &gt; with N its set of vertices and E set of weighted edges. The weight W pn (i, j) of the edge between the vertices i and j is the number of collocations. W matrix called the adjacency matrix. For example, given the sentences  X  X he camera is sma ll but smart X ,  X  X lthough the camera is small, I think it is quite smart X ,  X  X he camera is small but affordable X ,  X  X  think it is good that the camera is affordable X , and  X  X t is small and has poor quality X  , G pn figure 1 (the number in the square brackets indicat e the weight of the edge). If two adjectives occur in a sentence where they ar e separated by words like  X  X ut X ,  X  X lthough X  or articulated in simp le constructions such as  X  X ven if ..., ...  X  we refer to this situation as negative collocation. Negative collocation between adjective s indicates amendments of semantic orientations between the adj ectives. We obtain a graph G n =&lt;N, E n &gt; with N its set of vertices and E set of weighted edges. The weight W n (i, j) of the edge between the vertices i and j is the number of negative collocat ions. W adjacency matrix. For example, given the sentences  X  X he camera is sma ll but smart X ,  X  X lthough the camera is small, I think it is quite smart X ,  X  X he camera is small but affordable X ,  X  X  think it is goo d that the camera is affordable X , and  X  X t is small and has poor quali ty X , G in figure 2 (the number in the square brackets indi cate the weight of the edge). If two adjectives are negatively collocated to the same adjective in different sentences, we treat them as being positiv ely collocated. For example, if we have two sentences:  X  X he camera is small but smart X  and  X  X he camera is small but affordable X  the adjectives  X  X mart X  and  X  X ffordable X  are considered positively collocated. We compute the co-citation matrix W c of W n [18]. Positive collocation between adjectives indicates reinforcem ent of semantic orientations between the adjectives. The final sentiment graph G is a structure &lt;N, E&gt; w ith N its set of vertices and E its set of weighted edges (self-loop s removed) where W is the adjacency matrix of our sentiment graph. For example, given the sentences  X  X he camera is sma ll but smart X ,  X  X lthough the camera is small, I think it is quite smart X ,  X  X he camera is small but affordable X ,  X  X  think it is goo d that the camera is affordable X , and  X  X t is small and has poor quali ty X , G is shown in figure 3 (the number in the square brackets indi cate the weight of the edge). PageRank algorithm is applied to the sentiment grap h obtained in 3.1 to compute the semantic orientation scores of a djectives. We define two sets containing known adjectives with positive and negative semantic orientation respectively. We assi gn non-zero initial semantic orientation scores to these adject ives. These semantic orientation scores will be propagated to o ther adjectives during the course of PageRank application on the gr aph. In this manner, the semantic orientation scores of unknown adjectives can be computed. The set Good_Adjectives is the set containing known adjectives with positive orientation. The set Bad_Adjectives is the set containing known adjectives with negative orientati on. The vertex in the graph is assigned a non-zero init ial semantic orientation score if the corresponding adjective is in the set Good_Adjectives or Bad_Adjectives , and is assigned zero initial semantic orientation score otherwise: i.e. we const ruct a vector a = &lt;a 1 ... a |N| &gt; in which a i is 1/|Good_Adjectives corresponding adjective is in Good_Adjectives and 0 otherwise, and we construct a vector a n 0 = &lt;a 1 ... a |N| |Bad_Adjectives  X  N| if the corresponding adjective is in Bad_Adjectives and 0 otherwise. PageRank [4] computes the fix point or stable state of the product of an adjacency matrix with itself and a damping fa ctor (the probability, at any step, that a walker will contin ue walking). This is similar to a random walk in the graph defined by the matrix, for a walker getting fatigued and switches to a random vertex according to the damping factor. The input to PageRank algorithm is the adjacency ma trix W normalized into W norm where W norm (i, j) = W(i, j) /  X  orientation scores assigned to the vertices (adject ives). In the formula below,  X  is the damping factor and e represents the probability that a random walker will choose a rand om vertex when it gets tired. As in [4] this probability is s et to be equal for all the vertices. PageRank algorithm iteratively co mputes the semantic orientation scores of the vertices (adject ives), i.e. the vector a: As in [4], we set  X  to be 0.85. e = &lt;e 1 ...e iterations. We set e i =1/|N| for any i as in [4]. When we use a p 0 , we propagate the semantic orientation scores of known positive adjectives to other adjectives in th e graph. Correspondingly, when we use a n 0 , we propagate the semantic orientation scores of known negative adjectives to other adjectives in the graph. Therefore, depending whether we use a obtain methods that compute positive or negative se mantic orientation scores of the adjectives in the graph, respectively. We refer to these methods as _Positive and _Negativ e, respectively. The vertices (adjectives) can also be ranked accord ing to their semantic orientation scores to produce context-depe ndent ranking of adjectives. The positive (resp. negative) score of each item is computed as the sum of positive (resp. negative) scores of adjectiv es from all its reviews. The sum considers duplicates, i.e. an adje ctive that appears twice in the reviews will contribute its sc ore twice towards the total sum. In this paper we use sum to combine the scores of the adjectives to compute the score of th e item. Other aggregate function is certainly possible and can be explored in the future work. Depending on how we construct a sentiment graph, we define three variants: (1) individual_: we construct a sen timent graph from reviews grouped by individual item, (2) byGenr e_: we construct a sentiment graph from reviews grouped by genre, and (3) all_: we construct a sentiment graph from all r eviews. Depending on our input to PageRank, we define two v ariants: (1) _Positive: we input the matrix W and the vector a p 0 to PageRank to compute positive semantic orientation scores of adjectives, (2) _Negative: we input the matrix W and the vector a n 0 to PageRank to compute negative semantic orientation scores of adjectives. Using the computed positive and negative semantic o rientation scores, we define another variant called _PositiveN egative which computes positive semantic orientation score minus negative semantic orientation score. Therefore in total we propose 9 methods: (1) indivi dualPositive, (2) individualNegative, (3) individualPositiveNegat ive, (4) byGenrePositive, (5) byGenreNegative, (6) byGenrePositiveNegative, (7) allPositive, (8) allNe gative, and (9) allPositiveNegative. In our experiment, we define the set Good_Adjectives to contain 19 adjectives:  X  X ood X  and its synonyms:  X  X xcellent X  ,  X  X rilliant X ,  X  X ell X ,  X  X etter X ,  X  X est X ,  X  X orthy X ,  X  X orth X ,  X  X ice X  ,  X  X reat X ,  X  X erfect X ,  X  X ositive X , and negative antonyms:  X  X ot bad X ,  X  X ot horrible X ,  X  X ot terrible X ,  X  X ot awful X ,  X  X ot worse X  ,  X  X ot worst X ,  X  X ot negative X . We define the set Bad_Adjectives to be the exact mirror image of Good_Adjectives (i.e. it contains 19 adjectives:  X  X ot good X ,  X  X ot excellent X ,  X  X ot brilliant X , etc.) We illustrate and evaluate our approach using revie ws of box office movies written by users of a popular movie r eview site. We pick 50 movies randomly from box office list of Nov ember 2007 to February 2008. For each movie, we download all i ts users X  reviews. For each movie we note its box office figu re, its overall quantitative user rating, and its genre. The movies are of genre action, animation, children, comedy, drama, foreign film, horror, musical, romance, science fiction, chick flick, cri me, political, or psycho. In our experimental data, we have quantitative user rating for each user and each movie (on a scale of 1 to 10 stars). These ratings are averaged to provide an overall user rating for the movie. However, there is evidence [1, 2] that such rating for measuring reviews is not reliable. The unreliability of user quantitative rating is attributed to its inconsistency [1] and its too coarse granularity [2]. Similar qualitative textual reviews can yield very different quantitative ratings from users. In the most extrem e case, the users do not understand the rating system and give a 1 in stead of a 10. Choosing a number between 1 and 10 to quantify one X  s opinion is subjective and difficult [1]. The coarse granularit y of the rating scale (1 to 10 stars) for measuring reviews has the underlying assumption that the opinion in textual review is pe rfectly classified (summarized) into the 10 classes of the star rating [2]. Yet the findings in [2] clearly indicate that the a ctual text contains significantly more information than the ratings. Th e loss of information due to the mapping from textual reviews to the coarser star rating is irretrievable. Inconsistency and coarse granularity are the paradox of user rating because to reduce one will mean to increase the other. For example, altho ugh it is easier to be consistent when choosing between  X  X ood X  or  X  X  ad instead of choosing a number from 1 to 10; the 2 classes of ra ting (coarser granularity) loses more information than the 10 cla sses of rating (finer granularity). We further investigate the eff ectiveness of user ratings in our experiments. In our experimental data, we have also objective fi gures that represent the opinions of the general audience: i.e . the box office figures. The box office figure is the gross income of the movie, which is the number of tickets sold (indicates the audience X  X  decision to watch the movie) times the price of the ticket (indicates the audience X  X  willingness to pay). If we assume that reviewers are representative of t he general audience and that reviews are representative of the general audience X  X  opinions, then the box office figures sh ould be a suitable source of reference ranking to measure per formance in our experiments. We recognize that box office figures may not be the only robust and objective source of reference ranking; it is ho wever an important and valuable one, especially from the mar keting point of view. We construct the sentiment graph and run PageRank o n the graph. PageRank computes semantic orientation scores of ea ch vertex (adjective) in the graph. We sum the semantic orien tation scores of adjectives in the reviews of an item to determin e the semantic orientation score of the item. We rank the items ac cording to their scores. In this paper we present three metrics for measuring ranking performance. The first metric is Percentage of Overlap [19] which is the size of the overlap between two top-k lists: i.e. how many movies in the top-k list of box office ranking are in the top-k list of our ranking. We normalize this measure by dividing it with k to get the Percentage of Overlap . The bigger the overlap, the better is our ranking in matching the box office ranking. k can take a value between 1 to N item , where N item is the total number of movies. The second metric is Average Rank Error . For each movie, we compute the difference between the rank we produce for the movie and the movie X  X  box office rank. Average Rank Error is the average of these rank differences. The smaller the average, the better is our ranking in matching the box office ra nking. The third metric is Percentage of Rank Overlap which is the percentage of movies out of the total number of mov ies that have the same numerical rank in our ranking as in the box office ranking. This is a stricter measure than the Percentage of Overlap metric [19] which does not care about the actual nu merical ranks. The bigger the rank overlap, the better is our rank ing in matching the box office ranking. In this paper we present two methods for evaluating ranking performance. We can evaluate the ranking of the entire data or w e can evaluate the ranking of just the subset (top-k ) of the data. We call this method of evaluating ranking Top-k , for k = 1 (we evaluate the ranking of the top 1 movie), k = 2 (we evaluate the ranking of the top 2 movies), to k = 50 (we evaluate the ranking o f the entire data). We can evaluate the ranking of individual movies or we can evaluate the ranking of the groups of movies. We ca ll this method of evaluating ranking Granularity-g , for g = 1 (we evaluate the ranking of individual movies), g = 2 (we group movi es by 2 based on their scores (i.e. highest 2 movies, second high est 2 movies, third highest 2 movies, etc.), assign each group a rank, and evaluate the ranking of the groups), to g = 50 (we group all movies in one group, assign this group a rank, and evaluate this coarsest ranking). We measure ranking effectiveness with regard to the information loss that a coarser ranking incu rs. A combination of the first and second method is pos sible. For example, we can consider grouping the movies then e valuating top-k lists of each group or we can consider grouping th e movies then evaluating the ranking of the top-k groups only. However we do not explore it in this paper due to space consid eration. We present results of evaluating our ranking agains t the box office ranking. We use Top-k and Granularity-g method for evaluating performance. For each of the evaluation, we present metrics for measuring ranking performance. We also present inte resting result for the ranking of adjectives of each genre. We compare the top-k list of our ranking with the top-k list of the box office ranking, for k = 1 to k = 50. For each k , we present percentage of overlap and average rank error. Percentage of overlap is measured as the size of ov erlap (the number of movies in the top-k list of our ranking which are in the top-k list of the box office ranking), divided by k . Average rank error is measured as the average of ra nk differences between the ranks of movies in the top-k list of the box office ranking and their ranks in our ranking. In figure 4, we present the percentage of overlap b etween the top-k list of our ranking and the top-k list of the box office ranking. We see that all our methods (except individualNegat ive, byGenreNegative, and allNegative) perform better (h igher percentage of overlap with box office ranking) than the ranking from user ratings. individualPositive and byGenrePo sitive perform the best, achieving more than 70% of overla p with box office ranking for almost all k . In figure 5 we compare the average rank error betwe en the ranks of movies in the top-k of box office ranking and their ranks in our ranking. From figure 5 we can see that all our methods (exce pt individualNegative, byGenreNegative, and allNegativ e) perform better (lower average rank error) than the ranking from user ratings. individualPositive and byGenrePositive per form the best. % o f o v e r l a p
A v e r a g e R a n k E r r o r From these results, we observe that methods which u se negative semantic orientation scores are not as effective as those with just the positive semantic orientation scores. This high lights a question on how best to perceive and use the negati ve semantic orientation scores: i.e. non-negative orientation m ay not always mean positive orientation, and positive and negativ e semantic orientation scores may not always combine in a line ar fashion. From these results, we also observe that methods wh ich use reviews grouped by genre (byGenre_) perform better than the methods which combine all the reviews (all_). This maybe because, when we combine all reviews, we lose infor mation on the genre context of the adjectives. Such context i nformation maybe important in determining the semantic orienta tions of adjectives which depend on genre: e.g. funny  X  whic h maybe positive in comedy genre but negative elsewhere, sc ary  X  which maybe positive in horror genre but negative elsewhe re, etc. From these results, we also observe that all our me thods (except individualNegative, byGenreNegative, and allNegativ e) perform better than user ratings in inferring the box offic e ranking, which we believe to be an objective measure for ranking o pinions about the movies. Our results confirm similar observation s in [1, 2]. We group movies at different granularity g , for g = 1 (one movie in a group) to g = 50 (all movies in one group). After grouping, movies in the same group are assigned the same rank . Hence, different grouping results in different ranking. For each g , we present percentage of rank overlap and average rank error with the information loss incurred from coarser ranking. Percentage of rank overlap is measured as the perce ntage of movies out of all movies in our dataset which has t he same numerical rank in our ranking as in the box office ranking. Average rank error is measured as the average of ra nk differences between the rank we produce for each movie and the movie X  X  box office rank. Information loss is measured as the sum of informat ion loss of each movie. The information loss of each movie is t he range of box office figures in its group divided by the maxi mum range of box office figures of all movies in our dataset. In figure 6 we present the percentage of rank overl ap vs. information loss at different granularity g when we compare our ranking to box office ranking. individualNegative, byGenreNegative, and allNegativ e) perform better (higher percentage of rank overlap for the s ame granularity g ) than the ranking from user ratings. individualPos itive and byGenrePositive perform the best. In figure 7 we present the average rank error vs. i nformation loss at different granularity g when we compare our ranking with the box office ranking. When g = 1 (each movie is a group of its own), the inform ation loss is zero and the average rank error is maximum. As we group movies ( g &gt; 1), the average rank error decreases more rapidl y than the increase in information loss. This shows that g rouping movies can improve the ranking result greatly without incu rring too much information loss. When we zoom in on figure 7, we see that all our me thods (except individualNegative, byGenreNegative, and allNegativ e) perform better (lower average rank error for the same granu larity g ) than the ranking from user ratings. individualPositive a nd byGenrePositive perform the best. Figure 6. Percentage of Rank Overlap vs. Informatio n Loss at Here we present the results of evaluating the sensi tivity of our method to the starting adjectives we define in Good_Adjectives and Bad_Adjectives sets. We present results of ranking by one of our best performing method (individualPositive) as compared to the box office ranking. In figure 8, we present the results of running indi vidualPositive method with different number of starting adjectives extracted from our Good_Adjectives set. From figure 8, we can see that different number of starting adjectives do not vary the ranking results much in terms of their percentage of overlap with the box office ranking. Even when we only use 1 starting adjective, our method still wor ks in producing high quality ranking. From k = 6, the percentage of overlap with the box office ranking is always higher than 60% fo r various number of starting adjectives. In figure 9, we present the results of running indi vidualPositive method with different subsets of starting adjectives extracted from our Good_Adjectives set. From figure 9, we can see that different subset of starting adjectives do not vary the ranking results much in terms of their percentage of overlap with the box office ranking. % o f o v e r la p ( in t o p -k ) These results show that our method is not overly se nsitive to the number or the choice of starting adjectives. The se ntiment graph itself may have contained enough information on the semantic orientations of its vertices (adjectives). The interesting thing about our proposed methods is that, not only they can produce the ranking of the movies; they ca n also produce the context-dependent ranking of the adjectives in the reviews. Here we present some interesting results from the r anking of the adjectives when we conduct our method: byGenrePosit ive using only one positive adjective:  X  X ood X  in our starting set. Using only the adjective  X  X ood X  as its starting adj ective, our method is able to find that the adjective  X  X reat X  h as also a universal positive semantic orientation in all the genres: i.e.  X  X reat X  is ranked in the top 1% of adjectives with highest positive orientations, in all the genres. Among other interesting top 1% adjectives with high est positive orientations are:  X  X unny X  (in comedy, chick flick, animation and children genres),  X  X tupid X  (in the comedy genre),  X  animated X  (in animation and children genres),  X  X usical X  (in the musical genre),  X  X olitical X  and  X  X lawed X  (in the political genre),  X  X riginal X  (in the psycho genre),  X  X nchanted X  and  X  X airy X  (in the chil dren genre),  X  X eal X  (in the drama genre),  X  X oung X  and  X  X ritish X  (in the romantic genre). Some of these adjectives have eith er ambiguous orientations or orientations that are genre-specifi c. For example, the adjectives  X  X lawed X  and  X  X tupid X  h ave ambiguous semantic orientations: i.e. they can have positive or negative semantic orientations depending on how the y are used in the sentence. Our method is able to identify that t hese adjectives have positive orientations in the political and com edy genre, respectively. Further investigation to the actual r eviews reveals interesting usage of the adjective  X  X lawed X  in the political genre:  X ... a rather affectionate look at a flawed man who fe lt compelled to right what was wrong X ,  X  X ilson Hanks, a flawed a nd fun loving Congressman from the piney woods of East Tex as... X , and the interesting usage of the adjective  X  X tupid X  in the comedy genre:  X  X  like a stupid movie where I do not have t o think in and just sit back X , which suggest the positive orientat ions of the adjective  X  X lawed X  and  X  X tupid X  in the sentences. Further,  X  X olitical X ,  X  X usical X , and  X  X nimated X  are adjectives whose usage and orientations maybe specific only to the political, musical and animation genre respectively. Our metho d is able to identify that these adjectives have indeed positive orientations in their respective genres even when these adjectives are not in our starting set of positive adjectives. Lastly, another interesting issue to explore is whe ther or not the adjectives actually reflect the audience demands fo r what will be considered good movies for a particular genre. For example, among the top 1% of adjectives (with highest positi ve semantic orientations) in the romantic genre is the adjectiv e  X  X ritish X . Indeed, British romantic movie has done continuousl y well in topping the box office list with movies such as  X  X r idget Jones X  Diary X  and  X  X our Weddings and a Funeral X . Another e xample is the adjective  X  X nimated X  that is ranked among the t op 1% of adjectives with highest positive semantic orientati ons in the children genre. Indeed, animated movies in children genre have done very well in the past with movies such as  X  X hr ek X ,  X  X inding Nemo X , and  X  X oys Story X . We are interested in expl oring this issue further in our future work. Our contributions include a novel and practical pro cedure, a comprehensive performance analysis and a methodolog y for performance evaluation that includes metrics novel in this application domain. We propose a novel and practical context-dependent ranking procedure that can rank items directly from the tex t of their reviews. The method uses simple contextual relation ships such as collocation, negative collocation and coordination by pivot words such as conjunctions and adverbs to construct a sen timent graph. From a small starting set of adjectives whose orien tation is universally known, the orientation of other adjecti ves in the reviews and the items reviewed can be computed and ranked using the PageRank algorithm. The method has severa l variants whether it uses positive or negative starting adjec tives and whether the sentiment graph is constructed for indi vidual items, by genre, or globally. From the ranking perspective , we have contributed by making it possible to rank items usi ng PageRank applied to a different graph other than the graph w here the vertices are the items to rank. We use instead a re lated graph constructed from the smaller components (terms, adj ectives) that express opinions about the items. From the opinion mining perspective, we contribute by using PageRank algori thm to design context-dependent ranking of items based on opinion s in their reviews. We instantiate and evaluate our procedure and its v ariants using reviews of box office movies. While the design of e ffective context dependent methods is generally considered a hard topic in natural language processing [9], we show that our m ethod is very effective and produces a ranking comparable to the one of the box office. Our best performing method uses positive st arting adjectives and a sentiment graph constructed for in dividual items (individualPositive). This interestingly happens to be the simplest method. We also show that our method is not overly sensitive to the number or choice of starting adjectives. We co mpare our method and the box office ranking with the ranking induced from user ratings and show, if it was necessary, the lim itation of the latter. This highlights the practical application o f our proposal. Our performance evaluation examines the usage of se veral metrics for measuring ranking used by different authors. We show how the notion of information loss can be used in evalu ating coarseness when measuring ranking effectiveness at different granularities. This is a novel metric in evaluating ranking. From the experiments, we find that ranking based on negative semantic orientation scores does not give as good a performance as ranking based on positive semantic orientation s cores. This highlights the question on how best we should perce ive the negative orientation scores. We find that positive and negative orientation may not combine well in a linear fashio n and that non-negative orientation may not always mean positive o rientation. In future, more combination of positive and negative o rientation can be explored. Lastly, the practical thing about our methods is it s applicability to many more domains, to rank items based on their rev iews. With such automated ranking of items based on reviews, c ompanies could track public response to their products or se rvices, politicians could track public opinions, and so on. In future, we will explore the application of our methods to many more domains. [1] Dave, K., Lawrence, S., and Pennock, D.M. 2003. Min ing the [2] Ghose, A., Ipeirotis, P. G., and Sundararajan, A. 2 007. Opinion [3] Gondran, M. and Minoux, M. 1984. Graphs and Algorit hms. [4] Brin, S. and Page, L. 1998. The anatomy of a large-scale [5] Turney, P.D. 2002. Thumbs Up or Thumbs Down? Semant ic [6] Hu, M. and Liu, B. 2004. Mining Opinion Features in Customer [7] Whitelaw, C., Garg, N., and Argamon, S. 2005. Using appraisal [8] Ding, X. and Liu, B. 2007. The Utility of Linguisti c Rules in [9] Liu, B., Hu, M., and Cheng, J. 2005. Opinion Observ er: [10] Wiebe, J., Wilson, T., and Bell, M. 2001. Identifyi ng [11] Wilson, T., Wiebe, J., and Hoffmann, P. 2005. Recog nizing [12] Hatzivassiloglou, V. and McKeown, K.R. 1997. Predic ting the [13] Esuli, A. and Sebastiani, F. 2006. SentiWordNet: A Publicly [14] Esuli, A. and Sebastiani, F. 2007. PageRanking Word Net [15] Byun, J., Kamra, A., Bertino, E., and Li, N. 2007. Efficient k-[16] Brill, E. 1995. Transformation-Based Error-Driven L earning and [17] Reynar, J. and Ratnaparkhi, A. 1997. A Maximum Entr opy [18] Zhou, X. and Pu, P. 2002. Visual and Multimedia Inf ormation [19] Bar-Ilan, J., Mat-Hassan, M., and Levene, M. 2006. Methods 
