 Electrical and Computer Engineering CAP T CHAs [1] are automated tests designed to tell computers and humans apart by presenting users with a problem that humans can solve but current computer programs cannot. Because CAP TCHAs can distinguish between humans and computers with high probability, they are used for many different security applications: they prevent bots from voting continuously in online polls, automatically registering for millions of spam email accounts, automatic ally purchasing tickets to buy out an event, etc. Once a CAP TCHA is broken (i.e., computer programs can successfully pass the test), bots can impersonate humans and gain access to services that they should not. Therefore, it is important for CAP T CHAs to be secure.
 To pass the typical visual CAP TCHA, a user must correctly type the characters displayed in an image of distorted text. Many visual CAP TCHAs have been broken with machine learning techniques [2] -[3], though some remain secure against such attacks. Because visually impaired users who surf the Web using screen -reading programs cannot see this type of CAP TCHA, audio CAP TCHAs were created. Typical audio CAP T CHAs consist of one or several speakers saying letters or digits at randomly spaced intervals. A user must correctly identify the digits or characters spoken in the audio file to pass the CAP TCHA. To make this test difficult for current computer systems, specifically automatic speech recognition (ASR) programs, background noise is injected into the au dio files. S ince no official evaluation of existing audio CAP T CHAs has been reported, we tested the security of audio CAP TCHAs used by many popular Web sites by running machine learning experiments designed to break them. In the next section, we provide a n overview of the literature related to our project. Section 3 describes our methods for creating training data, and section 4 describes how we create classifiers that can recognize letters, digits, and noise. In section 5, we discuss how we evaluated our methods on widely used audio CAP TCHAs and we give our results. In particular, we show that the audio CAP TCHAs used by sites such as Google and Digg are susceptible to machine learning attacks. Section 6 mentions the proposed design of a new more secure aud io CAP TCHA based on our findings. To break the audio CAP TCHAs, we derive features from the CAP TCHA audio and use several machine learning techniques to perform AS R on segments of the CAP T CHA. There are many popular techniques for extra cting features from speech. The three techniques we use are mel -frequency cepstral coefficients (MF CC) , perceptual linear prediction (PLP), and relative spectral transform -PLP (RAS TA -PLP). MF CC is one of the most popular speech feature representations used . Similar to a fast Fourier transform (FF T) , MF CC transforms an audio file into frequency bands, but (unlike FF T) MF CC uses mel -frequency bands, which are better for approximating the range of frequencies humans hear. PLP was designed to extract speaker -in dependent features from speech [4]. Th erefore, by using PLP and a variant such as RAS TA -PL P, we were able to train our classifiers to recognize letters and digits independently of who spoke them. S ince many different people recorded the digits used in one of the types of audio CAP T CHAs we tested, PLP and RAS TA -PLP were needed to extract the features that were most useful for solving them.
 In [4] -[5], the authors conducted experiments on recognizing isolated digits in the presence of noise using both PLP and RAS TA -PL P. However, the noise used consisted of telephone or microphone static caused by recording in different locations. The audio CAP T CHAs we use contain this type of noise, as well as added vocal noise and/or music, which is supposed to make the autom ated recognition process much harder.
 The authors of [3] emphasize how many visual CAP TC HAs can be broken by successfully splitting the task into two smaller tasks: segmentation and recognition . We follow a similar approach in that we first automatically s plit the audio into segments, and then we classify these segments as noise or words.
 In early March 2008, concurrent to our work, the blog of Wintercore Labs [6] claimed to have successfully broken the Google audio CAP TCHA. After reading their Web article and viewing the video of how they solve the CAP TCHAs, we are unconvinced that the process is entirely automatic, and it is unclear what their exact pass rate is. Because we are unable to find any formal technical analysis of this program, we can neither be sure of its accuracy nor the extent of its automation. S ince automated programs can attempt to pass a CAP T CHA repeatedly, a CAP T CHA is essentially broken when a program can pass it more than a non -trivial fraction of the time; e.g ., a 5% pass rate is enough.
 Our approach to breaking the audio CAP TCHAs began by first splitting the audio files into segments of noise or words: for our experiments, the words were spoken letters or digits. We used manual transcriptions of the audio CAP T CHAs to get information regarding the location of each spoken word within the audio file. We were able to label our segments accurately by using this information.
 We gathered 1,000 audio CAP T CHAs from each of the following Web sites: google.com, digg .c om, and an older version of the audio CAP TC HA in recaptcha.net. Each of the CAP T CHAs was annotated with the information regarding letter/digit locations provided by the manual transcriptions. For each type of CAP TCHA, we randomly selected 900 samples for t raining and used the remaining 100 for testing. Using the digit/letter location information provided in the manual CAP T CHA transcriptions, each training CAP TCHA is divided into segments of noise, the letters a -z, or the digits 0 -9, and labeled as such. We ignore the annotation information of the CAP T CHAs we use for testing, and therefore we cannot identify the size of those segments. Instead, each test CAP TCHA is divided into a number of fixed -size segments. The segments with the highest energy peaks are t hen classified using machine learning techniques (Figure 1). S ince the size of a feature vector extracted from a segment generally depends on the size of the segment, using fixed -size segments allows each segment to be described with a feature vector of th e same length. We chose the window size by listening to a few training segments and adjusted accordingly to ensure that the segment contained the entire digit/letter. There is undoubtedly a more optimal way of selecting the window size, however, we were still able to break the three CAP TCHAs we tested with our method .
 The information provided in the manual transcriptions of the audio CAP T CHAs contains a list of the time intervals within which words are spoken. However, these intervals are of variable size and the word might be spoken anywhere within this interval. To provide fixed -size segments for training , we developed the following heuristic . First, divide each file into variable -size segments using the time intervals provided and label each segment accordingly. Then, within each segment, detect the highest energy peak and return its fixed -size neighborhood labeled with the current segment X  X  label. T his heuristic achieved nearly perfect labeling accuracy for the training set. Rare mistakes occurred when the highest energy peak of a digit or letter segment corresponded to noise rather than to a digit or letter.
 To summarize this subsection, an audio file is transformed into a set of fixed -size segments labeled as noise, a digit between 0 and 9, or a letter between a and z. These segments are then used for training . Classifiers are trained for one type of CAP T CHA at a time. From the training data we extracted five sets of features using twelve MF CCs and twelfth -order spectral (SPEC) and cepstral (CEP S) coefficients from PLP and RAS TA -PL P. The Matlab functions for extracting these features were provided online at [7] and as part of the Vo icebox package. We use AdaBoost, SVM, and k -NN algorithms to implement automated digit and letter recognition . We detail our implementation of each algorithm in the following subsections. 4 . 1 A d a B o o s t Using decision stumps as weak classif iers for AdaBoost, anywhere from 11 to 37 ensemble classifiers are built. The number of classifiers built depends on which type of CAP T CHA we are solving . Each classifier trains on all the segments associated with that type of CAP T CHA, and for the purpose of building a single classifier, segments are labeled by either -1 (negative example) or +1 (positive example). Using cross -validation, we choose to use 50 iterations for our AdaBoost algorithm . A segment can then be classified as a particular letter, digi t, or noise according to the ensemble classifier that outputs the number closest to 1 . 4 . 2 S u p p o r t v e c t o r m a c h i n e To conduct digit recognition with SVM, we used the C++ implementations of libSVM [8] version 2.85 with C -SMV and RBF kernel. F irst, all featu re values are scaled to the range of -1 to 1 as suggested by [8]. The scale parameters are stored so that test samples can be scaled accordingly. Then, a single multiclass classifier is created for each set of features using all the segments for a particul ar type of CAP TCHA. We use cross -validation and grid search to discover the optimal slack penalty ( C=32 ) and kernel parameter (  X  =0.011 ). 4 . 3 k -n e a re s t n e i g h b o r ( k -NN) We use k -NN as our final method for classifying digits. For each type of CAP TCHA, five d ifferent classifiers are created by using all of the training data and the five sets of features associated with that particular type of CAP TCHA. Again we use cross -validation to discover the optimal parameter, in this case k=1 . We use Euclidian distance a s our distance metric. Our method for solving CAP TCHAs iteratively extracts an audio segment from a CAP T CHA, inputs the segment to one of our digit or letter recognizers, and outputs the label for that segment. We co ntinue this process until the maximum solution size is reached or there are no unlabeled segments left. Some of the CAP T CHAs we evaluated have solutions that vary in length. Our method ensures that we get solutions of varying length that are never longer t han the maximum solution length . A segment to be classified is identified by taking the neighborhood of the highest energy peak of an as yet unlabeled part of the CAP TCHA.
 Once a prediction of the solution to the CAP T CHA is computed, it is compared to the true solution. Given that at least one of the audio CAP TCHAs allows users to make a mistake in one of the digits (e.g., reCAP TCHA), we compute the pass rate for each of the different types of CAP T CHAs with all of the following conditions: However, since we are only sure that these conditions apply to reCAP T CHA audio CAP T CHAs, we also calculate the percentage of exact solution matches in our results for each type of audio CAP TCHA. These results are described in the follow ing subsections. 5 . 1 G o o g l e Google audio CAP TCHAs consist of one speaker saying random digits 0 -9, the phrase  X  o nce again, X  followed by the exact same recorded sequence of digits originally presented. The background noise consists of human voices speakin g backwards at varying volumes. A solution can range in length from five to eight words. We set our classifier to find the 12 loudest segments and classify these segments as digits or noise. Because the phrase  X  X nce again X  marks the halfway point of the C AP TCHA, we preprocessed the audio to only serve this half of the CAP TCHA to our classifiers. It is im portant to note, however, that the classifiers were always able to identify the segment containing  X  X nce again, X  and these segments were identified before all other segments. Therefore, if necessary, we could have had our system cut the file in half after first labeling this segment.
 For AdaBoost, we create 12 classifiers: one classifier for each digit, one for noise, and one for the phrase  X  X nce again. X  Ou r results ( Table 1) show that at best we achieved a 90% pass rate using the  X  X ne mistake X  passing conditions and a 66% exact solution match rate. Using SVM and the  X  X ne mistake X  passing conditions, at best we achieve a 92% pass rate and a 67% exact solutio n match. For k -NN, the  X  X ne mistake X  pass rate is 62% and the exact solution match rate is 26%.
 Table 1: Google audio CAP TCHA results: Maximum 67% accuracy was achieved by SVM.
 5 . 2 D i g g Digg CAP TCHAs al so consist of one speaker, in this case saying a random combination of letters and digits. The background noise consists of static or what sounds like trickling water and is not continuous throughout the entire file. We noticed in our training data that Digg audio CAP T CHA is also the verbal transcription of the visual CAP T CHA, we believe that these characters are excluded to avoid confusion between digits and let ters that are similar in appearance. The solution length varies between three and six words. Using AdaBoost, we create 28 classifiers: one classifier for each digit or letter that appears in our training data and one classifier for noise. Perhaps because w e had fewer segments to train with and there was a far higher proportion of noise segments, AdaBoost failed to produce any correct solutions. We believe that the overwhelming number of negative training examples versus the small number of positive training samples used to create each decision stump severely affected AdaBoost X  X  ability to classify audio segments correctly. A histogram of the training samples is provided in Figure 2 to illustrate the amount of training data available for each character. When using S VM, the best feature set passed with 96% using  X  X ne mistake X  passing conditions and passed with 71% when matching the solution exactly. For k -NN, the best feature set produced a 90%  X  X ne mistake X  pass rate and a 49% exact solution match. Full resul ts can be found in Table 2 .
 Table 2: Digg audio CAP T CHA results: Maximum 71% accuracy was achieved by SVM.
 5 . 3 re C A P T C H A The older version of reCAP TCHA X  X  audio CAP TCHAs we tested consist of several speakers who speak random digits. The background noise consists of human voices speaking backwards at varying volumes. The solution is always eight digits long. For AdaBoost, we create 11 classifier s: one classifier for each digit and one classifier for noise. Because we know that the reCAP TCHA passing conditions are the  X  o ne mistake X  passing conditions, SVM produces our best pass rate of 58%. Our best exact match rate is 45% ( Table 3).
 Table 3: reC AP T CHA audio CAP T CHA results: Maximum 45% accuracy was achieved by From our results, we note that the easiest CAP TCHAs to break were from Digg. Google had th e next strongest CAP TCHAs followed by the strongest from reCAP T CHA. Although the Digg CAP TCHAs have the largest vocabulary, giving us less training data per label, the same woman recorded them all. More importantly, the same type of noise is used through out the entire CAP T CHA. The noise sounds like running water and static which sounds very different from the human voice and does not produce the same energy spikes needed to locate segments, therefore making segmentation quite easy. The CAP TCHAs from Goog le and reCAP TCHA used other human voices for background noise, making segmentation much more difficult. Although Google used a smaller vocabulary than Digg and also only used one speaker, Google X  X  background noise made the CAP TCHA more difficult to solve. After listening to a few of Google X  X  CAP T CHAs, we noticed that although the background noise consisted of human voices, the same background noise was repeated. reCAP TCHA had similar noise to Google, but they had a larger selection of noise thus making i t harder to learn. reCAP TCHA also has the longest solution length making it more difficult to get perfectly correct. Finally, reCAPT CHA used many different speakers causing it to be the strongest CAP T CHA of the three we tested. In conclusion, an audio C AP T CHA that consists of a finite vocabulary and background noise should have multiple speakers and noise sim ilar to the speakers. Due to our success in solving audio CAP TCHAs, we have decided to start developing new audio CAP T CHAs that our methods, and machine learning methods in general, will be less likely to solve. From our experiments, we note that CAP TCHAs containing longer solutions and multiple speakers tend to be more difficult to solve. Also , because our methods depend on the amount of training data we have, having a large vocabulary would make it more difficult to collect enough training data. Already since obtaining these results, reCAP TCHA.net has updated their audio CAP T CHA to contain more distortions and a larger vocabulary: the digits 0 through 99. In designing a new audio CAP TCHA we are also concerned with the human pass rate. The current human pass rate for the reCAP TCHA audio CAP T CHAs is only 70%. To develop an audio CAP T CHA with an im proved human pass rate, we plan to take advantage of the human mind  X  X  ability to understand distorted audio through context clues. By listening to a phrase instead of to random isolated words, humans are better able to decipher distorted utterances because they are familiar with the phrase or can use contextual clues to decipher the distorted audio. Using this idea, the audio for our new audio CAP TCHA will be taken from old -time radio programs in which the poor quality of the audio makes transcription by AS R systems difficult. Users will be presented with an audio clip consisting of a 4 -6 word phrase. Half of the CAP T CHA consists of words, which validate a user to be human, while the other half of the words need to be transcribed. This is the same idea behin d the visual reCAP TCHA that is currently digitizing text on which OCR fails. We expect that this new audio CAP TCHA will be more secure than the current version and easier for humans to pass. Initial experiments using this idea show this to be true [9]. We have succeeded in  X  X reaking X  three different types of widely used audio CAP TCHAs, even though these were developed with the purpose of defeating attacks by machine learning techniques. We believe our results can be improved by selecting optim al segment sizes, but that is unnecessary given our already high success rate. For our experiments, segment sizes were not chosen in a special way; occasionally yielding results in which a segment only contained half of a word, causing our prediction to co ntain that particular word twice. We also believe that the AdaBoost results can be improved, particularly for the Digg audio CAP T CHAs, by ensuring that the number of negative training samples is closer to the number of positive training samples. We have sh own that our approach is successful and can be used with many different audio CAP TCHAs that contain small finite vocabularies.
 A c k n o w l e d g m e n t s Th is work was partially supported by generous gifts from the Heinz Endowment, by an equipment grant from Intel Co rporation, and by the Army Research Office through grant number DAAD19 -02 -1 -0389 to CyLab at Carnegie Mellon University. Luis von Ahn was partially supported by a Microsoft Research New Faculty Fellowship and a MacArthur Fellowship. Jennifer Tam was partia lly supported by a Google Anita Borg Scholarship .
 R e f e re n c e s
