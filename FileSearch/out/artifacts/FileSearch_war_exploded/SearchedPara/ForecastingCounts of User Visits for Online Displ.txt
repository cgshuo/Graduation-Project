 Display advertising is a multi-billion dollar industry where advertisers promote their products to users by having pub-lishers display their advertisements on popular Web pages. An important problem in online advertising is how to fore-cast the number of user visits for a Web page during a par-ticular period of time. Prior research addressed the prob-lem by using traditional time-series forecasting techniques on historical data of user visits; (e.g., via a single regression model built for forecasting based on historical data for all Web pages) and did not fully explore the fact that different types of Web pages have different patterns of user visits.
In this paper we propose a probabilistic latent class model to automatically learn the underlying user visit patterns among multiple Web pages. Experiments carried out on real-world data demonstrate the advantage of using latent classes in forecasting online user visits.
 Categories and Subject Descriptors: H.4 [ Information Systems Applications ]: Miscellaneous General Terms: Experimentation, Performance, Theory Keywords: Forecasting, User Visits, Display Advertising
Online display advertising is a multi-billion dollar industry where advertisers buy user visits from publishers in order to promote their products by displaying advertisements (ads) on popular Web pages. An important problem in display advertising is forecasting the count of user visits for a Web page during a particular period of time (e.g., day, hour, etc.). Over-forecasting or under-forecasting leads to undesired ad delivery outcomes, such as missing an advertiser X  X  goal or revenue loss due to unsold user visits [1].

Prior research has mainly adopted traditional time-series forecasting techniques [1, 3, 4]. Forecasting models are trained from historical user visits as a single regression model for all Web pages (assuming independence among different Web pages). In real online world, user visits among Web pages are not independent; and this results in groups of Web pages that have similar user visit patterns. For instance, one ob- X 
Th is research was partially supported by the NSF research grants IIS-0746830, CNS-1012208, IIS-1017837, and a re-search grant from Yahoo!.
 vious factor shaping user visits is the physical structure of Web pages. A large proportion of users follow the naviga-tion from a parent Web page, and visit the children Web pages step-by-step. Hence, some of the Web pages will have similar user visit patterns with each other, while having dif-ferent user visit patterns than some other Web pages. To our best knowledge, prior research has not differentiated the groups of Web pages with similar user visit patterns.
In this paper, we learn user visit patterns hidden behind the online traffic of a large number of Web pages by using a novel probabilistic latent class model. In particular, differ-ent types/classes of Web pages that share similar patterns of user visits are automatically identified from historical data, and a regression model is built for each type/class of Web pages for making accurate prediction. The detailed formula-tion of the model is presented in Section 2. We then evaluate the performance of the model against typical traditional so-lutions using real-world data from Yahoo! in Section 3.
For forecasting user visit volume v st for a Web page s (or bucket of web pages defined by a publisher) at time t , the proposed probabilistic latent class model can be described as follows: where P ( s ) is assumed to be uniform distribution, P ( z | s ) denotes the conditional probability of a latent class z given Web page s , and the N z is the number of latent Web page classes that is empirically set to 5. The visit pattern in a class P ( v st | z ) can be modeled with a Laplace distribution as follows: where f st i is the i th feature for a Web page s and time t pair (more information about the features can be found in Section 3),  X  zi is the weight of latent class z for the i feature, and K is the number of features.

The parameters of the model in Eqn.(1) ( P ( z | s ),  X  ) can be estimated by the EM algorithm [2]. The E-step can be derived by computing the posterior probability of z , i.e. P ( z | s, t, v st ). By optimizing the auxiliary Q-function, we can derive the following M-step update rules: Eqn.(4) is differentiable, and can be solved with gradient descent solvers. In particular, we use the Quasi-Newton method. An extreme case of the proposed latent class model, referred as Latent S M od , is using only one latent class N z = 1. In this case only the Laplace regression power is employed. We particularly report this case as Laplace R egr in the experiments as one of the baselines. It should be noted that the Latent S M od model and the Laplace R egr model are run on the log-scaled (base-e) count data, and the esti-mated count is rescaled back for comparison with the raw user visit counts during evaluation.
Experiments are conducted on 1 month user visit logs of tens of millions users from Yahoo!. Data from the second and third weeks is used for training, and the last week is used for testing. Features are extracted for each Web page from its past week history. User visits in the same hour are aggregated together. Starting from the first hour of the second week, we extract the first 4 features as the av-erage of visit volumes of the same hour-of-the-day in the past 1 , 3 , 5 , 7 days. For instance, the features for the hour, 9:00pm-9:59pm on Jan.10 th , are extracted from the visits during 9pm-9:59pm on days between Jan.9 th , 8 th ,..., Jan.3 Note that the first week data is only used while extracting this first set of features for the second week. We extract the second 4 features as the average number of visits in the past 1 , 3 , 6 , 9 hours. The number of user visits for the most visited i) 500, and ii) 1000 properties are computed (for each hour), and the corresponding datasets are referred as Top500Prop and Top1000Prop respectively. The Top500Prop and the Top1000Prop datasets have around 156 K and 306 K train-ing, and 83 K and 164 K test data instances respectively. The proposed model is compared to 3 types of baselines. The first baseline follows a simple forecasting approach that uses the average of past visit volume as the forecast of the coming hour. We use the 8 features as a set of 8 Web-page-independent baseline forecasts B LastN Day for N  X  (1 , 3 , 5 , 7) and B LastN Hour for N  X  (1 , 3 , 6 , 9). The fore-casting error is measured by the absolute percentage er-ror between forecast and truth: | v forecast st  X  v st | /v v st is the forecast and v st is the actual visit count. In Table 1, we reported the average the forecasting error for all 8 baselines for the Top500Prop and Top1000Prop datasets. In order to protect the confidential information from the company, the actual errors are normalized with (i.e., divided by) the error of B Last 1 H our baseline on the Top1000Prop dataset (note that the actual error is in the range 0.5-1), and only the normalized errors are reported for relative compari-son. It can be seen that B Last 1 H our is the best performing model out of all other 7 baselines, showing that the user vis-its in the last hour is the most relevant to the current hour, which is totally consistent with the common sense.
The second baseline of this work, namely BB P ropSpec , is similar to the first one, but allows each web page to have its own best model selected from the 8 features. The best feature is selected in the training data, and tested on the testing data. Interestingly, BB P ropSpec performs better than all other methods except B Last 1 H our . Selecting the best model for each Web page overfits the training data, and Table 1: (Normalized) Results of the proposed prob-abilistic latent class model (i.e., Latent S Mo d) in comparison to several baselines.
 gen erates more forecasting errors even in comparison to the simple baseline B Last 1 H our . Potential improvements can be achieved by following a direction between these two types of baseline approaches.

The third baseline is a traditional time-series regression model. We added a binary flag indicating weekends as an additional feature into the existing 8 features, and perform regression with Laplace distributions. This model is re-ferred as Laplace R egr . It can be seen in Table 1 that the Laplace R egr outperforms all previously introduced ap-proaches. This shows that combining the different informa-tion from past user visits intelligently is more effective than using only a specific type of information. Table 1 also shows that the proposed model (i.e., Latent S M od ) outperforms all the presented approaches by modeling the latent Web page classes that provide much higher modeling flexibility leading to its superior performance. This explicitly shows that differentiating the Web pages with different user visit patterns, and specializing the forecast model for different classes of Web pages that share similar patterns of user vis-its is important for achieving higher forecast accuracy.
Forecasting the number of user visits is an important task for display advertising. Different Web pages have different user visit trends, and it is important to learn specialized fore-casting models for properties with different user visit trends. This paper proposes a probabilistic latent class model that identifies the latent classes for Web pages with similar user visit trends, and learns a separate forecasting model for each type/class of Web pages. Experiments on real-world data from a major internet company show the effectiveness of the proposed probabilistic latent class model.
