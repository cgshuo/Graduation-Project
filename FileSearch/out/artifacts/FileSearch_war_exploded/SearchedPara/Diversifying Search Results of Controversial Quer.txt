 Diversifying search results of queries seeking for different view points about controversial topics is key to improving satisfaction of users. The challenge for finding different opin-ions is how to maximize the number of discussed arguments without being biased against specific sentiments. This pa-per addresses the issue by first introducing a new model that represents the patterns occurring in documents about controversial topics. Second, proposing an opinion diver-sification model that uses (1) relevance of documents, (2) semantic diversification to capture different arguments and (3) sentiment diversification to identify positive, negative and neutral sentiments about the query topic. We have con-ducted our experiments using queries on various controver-sial topics and applied our diversification model on the set of documents returned by Google search engine. The results show that our model outperforms the native ranking of Web pages about controversial topics by a significant margin. H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval] Algorithms, Experimentation
In the last decade, Web search has become the predom-inant method to access information. Typically, queries are expressed using few keywords which are often ambiguous and have different interpretations [1, 12]. For example, the query  X  X aguar X  may refer to the car ,the cat ,the operating system ,orthe classic Fender guitar . Since no additional information is always available to determine the real user intent, several result diversification techniques have been proposed to solve this problem [7, 1, 4, 13]. Their aim is to produce a set of results that cover different interpreta-tions. While there is a considerable effort on diversifying search results of ambiguous queries, there is a little effort on covering different view points in the result set of controver-sial queries, meaning queries about debatable topics, such as  X  X ssisted suicide X ,  X  X uman evolution X ,  X  X FOs existence X , and queries looking for different versions of the same break-ing news. Users seeking for such type of knowledge are not necessarily interested in a specific way of looking at a given issue, but possibly learning about that topic and interested in finding all different view points. Thus, it is important to focus on how best to produce a set of diversified results that cover different arguments and sentiments.

Diversifying results of controversial queries involves min-ing opinions (also called sentiment analysis), a problem that have been widely studied in the past few years [20, 6, 21]. Most of these efforts have focused on product reviews, while few attempts [2, 5] have addressed the problem of mining controversial documents (i.e., documents about controver-sial topics). Typically, mining controversial documents has followed the same line of mining product reviews, where a pro/con classification is used to identify documents that have either positive or negative sentiment regarding a given topic. However, discussing a controversial topic involves much more than just positive, negative or neutral senti-ments. It involves different ways of looking at the prob-lem using different arguments. For example, two documents can be both negative about  X  X uman evolution X  but for com-pletely different reasons. One is negative because of the high deleterious mutation rate in humans , while the other is negative because it promotes creationism . These two docu-ments are not diverse with respect to their sentiments, but they are diverse with respect to the arguments they provide. Thus, one need to use both sentiment analysis and novelty detection to diversify search results of controversial queries.
Addressing queries looking for both various arguments and sentiments face two main challenges. First, finding documents discussing similar arguments without being mis-lead by the difference they might exhibit in presenting them. Second, diversifying a set of documents by maximizing the number of discussed arguments without being biased against specific sentiments. This paper presents a solution towards solving the above problems by first introducing a new model that captures the main features of controversial documents, and second proposing an opinion diversification model that uses (1) relevance to select relevant documents for the query topic , (2) semantic diversification to avoid redundancy and cover a diverse set of documents presenting different argu-ments, and (3) sentiment diversification to cover different types of sentiments that can be positive, negative, or neu-tral. The diversification model is presented with a new tech-nique for computing similarity between documents based on the discussion arguments. We applied the proposed diversi-fication model on the set of documents returned by Google search engine. The results show that our opinion diversifi-cation model always outperforms the native ranking of con-troversial Web pages by a significant margin.
Search result diversification has been investigated exten-sively following two different approaches.The first one is taxonomy-independent where no knowledge base is used to diversify search results [19, 12, 13, 7, 15]. Some of the works falling into this category include the work by Gollapudi et al. [7] that uses a diversification model combining both nov-elty and relevance of search results. Zhai et al. [18] propose a risk minimization framework that allows users to define an arbitrary loss function for a given set of results. Radilinski et al. [12] use query expansion to enrich search results gen-erating more relevant documents for various interpretations. The second approach to result diversification is taxonomy-based . [1, 4, 3]. Representative works include the one by Agrawal et al. [1] which makes use of a taxonomy for clas-sifying queries and documents and create a diverse set of results according to this taxonomy. Clarke et al. [4] fo-cus on developing a framework of evaluation that takes into account both novelty and diversity. Carterette et al. [3] propose a probabilistic approach to maximize the coverage of the retrieved documents with respect to the aspects of a query.

In our work, we use a taxonomy-independent diversifica-tion model that exploits both relevance and novelty. The difference between our work and existing approaches is the integration of sentiments in the diversification model. The closest work to ours is by Demartini et al. [5]. They propose a diversification model for search results based on positive, negative, and neutral sentiments. The difference with our work is that we do not restrict the diversification to these three sentiments but we exploit also novelty to capture var-ious arguments about controversial topics.

Another research problem that is directly related to our work is opinion mining , also called sentiment analysis .This problem has been studied in the past few years [14, 6, 16] exploiting two main directions: (1) finding product features that have been commented on by reviewers and (2) decid-ing whether the comments are positive or negative. The second direction is the one we are exploiting in our work. Two main categories of classifying sentiments have been ex-plored: classification at the document level [11, 14] and a more fine-grained classification at the sentence level [16, 6]. As stated in [6], most sentence level and document level clas-sification methods follow two approaches: (1) corpus-based approaches, and (2) lexical-based approaches. Corpus-based approaches find co-occurrence patterns of words to deter-mine the sentiments of words or phrases, e.g., the work in [14]. Lexical-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed words. Such approaches are studied in [6].

Most of the techniques described above focus on product reviews. Few attempts [2, 5] have addressed the problem of mining documents about controversial topics. In our work, we take a more general approach to address the problem of classifying search results of controversial queries, depending on their sentiment orientations. To this end, we propose a model that can handle both sentence-level and document level classification. Depending on the granularity of classifi-cation, both corpus-based and lexical based approaches can be used.
We define a controversial document as a document that discusses a debatable topic. A controversial document is rep-resented as a set of topic elements ,E= { e 1 ,e 2 , ..., e reflect the different points of the discussion. For instance, a document about  X  X bortion X  discusses: the pregnancy ,the choice of abortion, the right to abort, the privacy of the ac-tion, the fetus ,the ethics of abortion, and the family role. Concretely, topic elements are assumed to be the nouns that occur in the text. Each topic element occurs within a set of sentences. These sentences give the context in which the topic element was discussed. We define the context C i of a topic element e i by the set of words composing the sen-tences to which it belongs. We call these words, context words and we write C i = { cw 1 ,cw 2 , ..., cw m } where cw a context word of topic element e i . We restrict context words only to nouns, adjectives, and adverbs. For exam-ple, we take the topic element  X  X etus X  that occurs in the following sentence:  X  If the fetus is a person, then abortion is murder and should be illegal  X . In this case, the set of context words would be { person,abortion,murder,illegal } Note that the topic element  X  X etus X  is linked to topic ele-ments person , abortion ,and murder , which are basically the nouns. For each context word, we define its sentiment orien-tation, which will be used to predict the sentiment assigned to the corresponding topic element. Words that encode a desirable state (e.g., perfect, right) have a positive orienta-tion, while words that represent undesirable states have a negative orientation (e.g., illegal, against). While sentiment orientations apply to many words, there are also those words that have no sentiment orientation (e.g., person, universe) that we call neutral. Combining topic elements, context words, and sentiment orientations, we define a document as a set of triples: where e i is a topic element, cw j is a context word associated with e i ,and o j is the sentiment orientation of cw j . Note here that we have used OpenNLP[10] to extract the differ-ent types of words from the text, while the extraction of sentiments is done through classification.

This model is used both to compute similarity between documents and classify their sentiments. It allows both sentence-level and document-level classification of senti-ments. In sentence-level classification, sentiments are as-signed to each topic element depending on its context. By contrast, document-level classification assigns one sentiment orientation to the whole document with respect to the query topic (e.g., Abortion). In this paper, we focus on document-level classification as a first step towards diversifying search results of controversial queries.

We consider a multi-class classification model where we have three classes { positive, negative, neutral } .Eachdocu-ment with features { f 1 , ..., f N } is assigned to one class. The features, we use in our classification model, are the set of context words that have either positive or negative senti-ment orientation. This can include nouns as well as adjec-tives and adverbs. To know the sentiment orientation of a context word, we use the opinion lexicon provided by [6]. In this paper, we have used both NaiveBayes classifier and Logistic Regression provided by [17]
This section introduces an opinion diversification model and algorithm. Before describing our approach, we define an opinion about a controversial topic to be composed of a set of arguments and sentiments . Formally, we extend the framework proposed in [7] by adding sentiment diversifica-tion to diversify search results of controversial queries. We are given a set of documents D = { d 1 ,d 2 , ...., d n } n  X  2, and a set of queries Q .Givenaquery q  X  Q and an integer k , our goal is to select a subset L k  X  D of doc-uments that is diverse. We assume three main components that define the diversity of a set of documents about contro-versial topics: relevance , semantic diversity ,and sentiment diversity . Naturally, before discussing whether a set is di-verse or not, it should first contain relevant documents to the query. The relevance of each document is given by a func-tion r : D  X  Q  X  R + , where a higher value implies that the document is more relevant to the query. To diversify a set of documents, we need to give more preference to dissimilar documents. In the context of controversial queries, two doc-uments are dissimilar if (1) they use different arguments to discuss the query topic, and/or (2) they exhibit different sen-timents about the query topic, including positive, negative, and neutral sentiments. To satisfy these two requirements, we define two distance functions. The first one is a seman-tic distance function d : D  X  D  X  R + between documents, where smaller the distance, the more similar the two docu-ments are. The second one is a sentiment distance function s : D  X  D  X  R + between documents, where smaller the distance, the closest in sentiments the two documents are.
We formalize a set selection function f :2 D  X  Q  X  r  X  d  X  o  X  R + , where we assign scores to all possible subsets of D , given a query q  X  Q , a relevance function r ( . ), a semantic distance function d ( ., . ), a sentiment distance function s ( ., . ), and a given integer k  X  Z + ( k  X  2). The goal is to select a set L k  X  D of documents such as the value of f is maximized. In other words, the objective is to find: where all arguments other than L k are fixed inputs to the function. Based on this objective function, we present an opinion diversification model that extends the Max-sum di-versification model presented in [7]. The goal of this model is to maximize the sum of the relevance, the semantic dissim-ilarity, and the sentiment dissimilarity of the selected set. The function we aim at maximizing can be formalized as follows: f ( L )=  X  ( k  X  1) where | L | = k ,and  X ,  X ,  X  &gt; 0 are parameters specifying the trade-off between relevance, semantic diversity, and sen-timent diversity. The model allows to put more emphasis on relevance, on semantic diversity, on sentiment diversity, or on any mixture of these measures. Note that we need to scale up the three terms of the function. The reason is that there are k ( k  X  1) 2 numbers in the semantic similarity sum, and k ( k  X  1) 2 in the sentiment sum as opposed to k numbers in the relevance sum.

In the context of controversial queries, we are particu-larly interested in detecting documents that use similar ar-guments to discuss a given topic. Thus, we do not use the whole vocabulary of a document to compute the semantic distance.We rather use only the topic elements to compute the distance between two documents. Formally, we define an opinion-based Jaccard similarity function O Jaccard as follows: where E ( a )and E ( b ) are the set of topic elements of docu-ment a and document b respectively. We note that Jaccard similarity is known to be a metric. As for sentiment distance, we define it as follows: where the sentiment orientation includes positive , negative , and neutral sentiments.
 Algorithm 1 Algorithm for MaxSumDispersion Input :Documents D , k Output :Set L ( | L | = k ) that maximizes f ( L )
Initialize the set L =  X  for i  X  1 to k 2 do end for If k is odd, add an arbitrary document to L
The problem of diversifying search results is NP-hard [7, 1]. Therefore, we need to exploit existing approximation al-gorithms to solve it. Gollapudi et al., [7] show that their Max-sum diversification objective can be casted to a facility dispersion problem, known as the MaxSumDispersion prob-lem [8, 9]. In our work, we follow the same principle and model our diversification problem as a MaxSumDispersion problem which aims at maximizing the sum of the pairwise distances between nodes in a metric space: where d X (.,.) is a distance metric. We show in the following that f is equivalent to our f function. To this end, we define the distance function d ( a, b ) as follows: d ( a, b )=
Considering the binary sentiment function, we claim that if d(.,.) is a metric then d X (.,.) is also a metric (proof skipped). We replace d X (.,.) by its definition in f X (L), disre-garding pairwise distances between identical pairs, thus we obtain: f ( L )=  X  ( k  X  1) we can easily see that each r ( a ) is counted exactly ( k times. Hence, the function f is equivalent to our function f . Given this mapping, we can use a 2-approximation algorithm proposed in [8, 9] and illustrated by algorithm 1 to maximize our MaxSum objective f .
We evaluated our opinion diversification model using two different types of controversial queries: (1) informa-tive queries and (2) debatable queries. Informative queries calls for different theories and information about a given topic rather than sentiments. For example, search results of queries like X  X rigin of the universe X  X r X  X verpopulation X  X yp-ically do not express positive or negative sentiments about the subject but rather present different theories and argu-ments. Note that search results of informative queries typ-ically contain one predominant sentiment rather than a va-riety of sentiments. Examples include  X  X hild abuse X  and  X  X acism X  where you can hardly find, in the search results, Web pages that are positive about these topics. By con-trast, a debatable query is a query for which search results can express different sentiments including positive, negative, or neutral. For example,  X  X eminism X ,  X  X rranged Marriage X  and  X  X ssisted Suicide X  are issues one can be for, against, or neutral about them.

To assess the effectiveness of our model, we used 50 queries: 25 informative queries and 25 debatable queries. We posed each query to Google search engine and we collected the top 50 from the results list. Before applying any diversifi-cation, we showed the entire pool of results to human judges for assessment. The judges proceeded with the assessment of positive, negative, and neutral Web pages. The assessment was done as follows. First, Web pages giving definitions about the query topic or stating some facts (historical facts, events, or statistics) were considered neutral because they do not express any sentiment. Examples of these Web pages in-clude Wikipedia pages, factual news, dictionary definitions, and historical articles. Additionally, pages describing both positive and negative sentiments about a topic were consid-ered neutral due to their objectivity. Second, Web pages with clear positive statements were considered positive .For example, pages stating explicitly that  X  X eath penalty is the right punishment for killers because it reduces crime rates X  are obviously positive. There are also some pages with a known and determined position, for instance, Church pages which are clearly positive about  X  X od existence X . Third, Web pages with clear negative statements were considered negative . For example, pages stating explicitly that  X  X eath penalty should be abolished X . In this case, we can also find pages with a determined position such as Amnesty Interna-tional pages that are clearly against  X  X eath penalty X .
Recall that our goal, in diversifying search results of con-troversial queries, is to cover many different view points. Thus, a natural way to assess the effectiveness of our model is to measure the number of different topic elements and sentiments covered, as a function of the size of the selected set. We call this measure Novelty and note that it is the same S-recal l measure proposed in [18] and Novelty q mea-sure presented in [7]. More precisely, consider a topic T with N topic elements e 1 ..., e N and a set p 1 , ..., p k of k Web pages. Let elm ( p i ) be the set of topic elements covered by p . We define the Novelty of a set of size k as follows: This Novelty measure helps quantifying the success in cov-ering different arguments. However, it does not capture the sentiment with which the argument was brought into discus-sion. To address this problem, we define an opinion-based novelty measure that we call O Novelty . The idea is to con-sider, for each topic element, three different contexts: Posi-tive , Negative ,and Neutral . To do so, we define three sets of sub topic elements: (1) elm pos ( p i ) the set of topic elements covered by p i in a positive context about the query topic, (2) elm neg ( p i ) the set of topic elements covered by p negative context about the query topic, and (3) elm neu ( p the set of topic elements covered by p i in a neutral context about the query topic. Now, we define the O Novelty of a set of size k as follows: Noteherethatwedivideby3  X  N since a topic element might be counted three times if it is covered in the three different contexts.

To find the set of topic elements related to a given query, we make use of Wikipedia portals. A portal introduces the reader to key articles and categories that further describe a given topic. We submit each query of our test set to Wikipedia and get all its related portals. Then, we analyze the page of each portal and take its title and categories as the topic elements related to the query. Examples of topic elements extracted from the portals related to the query  X  X bortion Ethics X  include Embryology , Surgery , Medical-treatments , Religion-and-politics , Liberalism , Human-rights , Social-inequality , Humanists , Personal-life , Family , Women-rights , Feminist-movement , Ethical-theories ,etc.
Relevance-based selection. The selection of the best set of Web pages for a given query is based solely on rele-vance. This is translated by setting the model parameters as follows:  X   X  and  X   X  .Inourexperiment  X  =1,  X  =1  X  8 ,and  X  =1  X  8 . To compute the relevance score, we inverse the Google rank of each Web page and normalized it in such a way that scores are ranging from 0 to 1, with 1 being the score of the top page. Note that using relevance selection, the best set of size k would always correspond to the topk results from Google. This method represents the baseline approach of our work.

Semantic-based selection. The selection of the best set of Web pages for a given query is based solely on semantic diversity. This is translated by setting the model parameters as follows:  X   X  and  X   X  .Inourexperiment  X  =1  X  8 ,  X  =1,and  X  =1  X  8 . We have used both Jaccard similarity distance and O Jaccard .

Sentiment-based selection. The selection of the best set of Web pages for a given query is based solely on sen-timent diversity. This is translated by setting the model parameters as follows:  X   X  and  X   X  .Inourexperi-ment  X  =1  X  8 ,  X  =1  X  8 ,and  X  =1.

Integrated selection. An integrated selection involves more than one measure to select the best set of Web pages. This requires setting the values of the parameters  X  ,  X  ,and  X  accordingly. For informative queries we mainly use the rel-evance and semantic diversity measures while for debatable queries all the three measures can be used.
Informative Queries. The results using informative queries are shown in Table 1 using a set size of 5. We can see that our model improves the novelty measure and out-performs the baseline approach. Compared to the original top 5 results returned by Google, our diversification model increases the novelty value from 0 . 334 to 0 . 381 using seman-tic diversification. This means that 4 . 7% of new discussion elements were added compared to the original top 5 results. The interesting observation that we can make here is that our O Jaccard distance outperforms the standard Jaccard distance. We can see that the standard Jaccard almost does not improve the novelty measure and it even loses some in-formation when combining it with the relevance. Having a closer look at the results of individual informative queries shown in Table 2, we can clearly see that in many cases, the standard Jaccard distance decreases the novelty mea-sure when using semantic diversification. The reasons for this behavior are discussed further in section 5.4.
Debatable Queries. The results of debatable queries are shown in Table 3. We can see that for the relevance baseline, our model always improves the novelty measure. Compared to the original top 5 results returned by Google, our diversifi-cation model increases the novelty value from 0 . 393 to 0 . 606. This means that we can include 21 . 3% of new topic elements and sentiments in the best 5 results compared to the original set. Looking at the different diversification methods, we ob-serve that the novelty value increases by combining different measures for set selection. While semantic diversification is improving the baseline with 8 . 6%, a combination with sen-timent diversification gives further improvement of 12 . 7%. It is also important to note that sentiment diversification is improving the novelty up to 0 . 549%. This is also due to the fact that the set selection process is ordered starting from the sets of highly ranked documents. In the integrated selec-tion, combining all three measures, we observe that adding relevance seems to decrease slightly the novelty, however, we still improve considerably compared to the baseline. An interesting observation, in the results of individual queries presented in Table 4, is that in some cases the standard Jaccard distance performs better than our distance for the semantic diversification (e.g., Abortion Ethics). However a combined semantic and sentiment diversification always performs best with our O Jaccard distance. The reasons for this behavior are discussed further in section 5.4.
The experimental results show our model always outper-forms the native ranking of controversial Web pages by a significant margin. We are performing particularly well in thecaseof debatable queries , for which the full set of search results contains various sentiments and discussion elements. This rich pool of results allows our model to effectively select Web pages having dissimilar contents and sentiments. Thus, overcoming the bias of the top results of the native ranking. By contrast, in the case of informative queries , search results are more monotonous in terms of sentiments, thus we only apply semantic diversification to learn about various argu-ments and view points about the topic. Although the gains for informative queries are smaller than debatable queries , we are always able to provide a richer set of results with novel information.

We observed through the experiments of individual queries that, in the context of controversial Web pages, a standard distance (e.g., Jaccard ) does not perform well for semantic diversification. The main reason is that two pages can discuss the same aspect of a controversial topic but us-ing different context words. Sometimes authors use different words and approaches to describe the same idea. Therefore, a standard distance is successful to measure how different the two pages are but not how different are the topic ele-ments being discussed. By contrast, our opinion-based dis-tance O Jaccard shows better performance by taking into account only topic elements. Semantic diversification fo-cuses on topic elements while sentiment diversification fo-cuses on the sentiments derived from their context. In this way, the results are effectively merged giving a better per-formance.
Diversifying search results of controversial queries is a promising direction to satisfy users looking for different ver-sions and information about the same topic. A challenging issue is the nature of opinions in controversial topics that goes beyond being positive , negative ,or neutral . An opinion also includes different arguments and angles from which the problem can be addressed. In this paper, we proposed a new model for representing controversial documents as a set of topic elements and contexts . These two main components represent the basis for computing semantic and sentiment similarity between documents. Additionally, we proposed an opinion diversification model that combines relevance , se-mantic ,and sentiment measures. The experiments showed that our model always outperforms the native ranking of opinionated Web pages by a significant margin. For prag-matic reasons, our choice of queries was heavily dependent on how well Wikipedia covers a given topic which calls for ex-ploring additional data sources about controversial issues. [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [2] R. Awadallah, M. Ramanath, and G. Weikum.
 [3] B. Carterette and P. Chandar. Probabilistic models of [4] C. L. A. Clarke, M. Kolla, G. V. Cormack, [5] G. Demartini. Ares: A retrieval engine based on [6] X. Ding, B. Liu, and P. S. Yu. A holistic lexicon-based [7] S. Gollapudi and A. Sharma. An axiomatic approach [8] R. Hassin, S. Rubinstein, and A. Tamir.
 [9] B. Korte and D. Hausmann. An analysis of the greedy [10] openNLP. http://opennlp.sourceforge.net/. [11] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [12] F. Radlinski and S. T. Dumais. Improving [13] R. L. T. Santos, C. Macdonald, and I. Ounis. [14] P. D. Turney. Thumbs up or thumbs down? semantic [15] J. Wang and J. Zhu. Portfolio theory of information [16] T. Wilson, J. Wiebe, and R. Hwa. Just how mad are [17] I. H. Witten and E. Frank. Data Mining: Practical [18] C. Zhai, W. W. Cohen, and J. D. Lafferty. Beyond [19] C. Zhai and J. D. Lafferty. A risk minimization [20] Z.Zhai,B.Liu,H.Xu,andP.Jia.Clusteringproduct [21] L. Zhuang, F. Jing, and X. Zhu. Movie review mining
