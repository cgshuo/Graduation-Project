 The ability of fast similarity search at large scale is of great importance to many Information Retrieval (IR) applications. A promising way to accelerate similarity search is semantic hashing which designs compact binary codes for a large num-ber of documents so that semantically similar documents are mapped to similar codes (within a short Hamming dis-tance). Although some recently proposed techniques are able to generate high-quality codes for documents known in advance, obtaining the codes for previously unseen doc-uments remains to be a very challenging problem. In this paper, we emphasise this issue and propose a novel Self-Taught Hashing (STH) approach to semantic hashing: we first find the optimal l -bit binary codes for all documents in the given corpus via unsupervised learning, and then train l classifiers via supervised learning to predict the l -bit code for any query document unseen before. Our experiments on three real-world text datasets show that the proposed ap-proach using binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine (SVM) outperforms state-of-the-art techniques significantly.
 H.2.8 [ Database Management ]: Database Applications X  data mining ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Stor-age and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning; I.5.2 [ Pattern Recognition ]: Design Methodology X  classifier design and evaluation Algorithms, Experimentation, Performance Similarity Search, Semantic Hashing, Laplacian Eigenmap, Support Vector Machine.
The problem of similarity search (aka nearest neighbour search ) is: given a query document 1 , find its most similar documents from a very large document collection (corpus). It is of great importance to many Information Retrieval (IR) [30] applications, such as near-duplicate detection [18], pla-giarism analysis [43], collaborative filtering [26], caching [32], and content-based multimedia retrieval [28].

Recently, with the rapid evolution of the Internet and the increased amounts of data to be processed, how to conduct fast similarity search at large scale has become an urgent re-search issue. A promising way to accelerate similarity search is semantic hashing [34] which designs compact binary codes for a large number of documents so that semantically simi-lar documents are mapped to similar codes (within a short Hamming distance). It is extremely fast to perform similar-ity search over such binary codes [42], because Furthermore, we usually just need to retrieve a small number of the most similar documents (i.e., nearest neighbours) for a given query document rather than computing its similarity to all documents in the collection. In such situations, we can simply return all the documents that are hashed into a tight
In similarity search, a document is used as the query for retrieval, which is fundamentally different with the standard keyword search paradigm, e.g., in TREC. Hamming ball centred around the binary code of the query document. For example, assuming that we use 4-bit binary codes, if the query document is represented as  X  0000  X , then we can just check this code as well as those 4 codes within one Hamming distance to it (i.e., having one bit difference with it)  X   X  1000  X ,  X  0100  X ,  X  0010  X , and  X  0001  X  X  X ndreturn the associated documents back. It will also be easy to filter or re-rank the very small set of  X  X ood X  documents (returned by semantic hashing) based on their full content, so as to further improve the retrieval effectiveness with just a little extra time [42].

In addition, similarity search serves as the basis of a clas-sic non-parametric machine learning method, the k-Nearest-Neighbours (kNN) algorithm [31], for automated text cate-gorisation [37] and so on. By enabling fast similarity search at large scale, semantic hashing makes it feasible to exploit  X  X he unreasonable effectiveness of data X  [14] to accomplish traditionally difficult tasks. For example, researchers re-cently achieved great success in scene completion and scene recognition using millions of images on the Web as training data [15, 44].

Although some recently proposed techniques are able to generate high-quality codes for the documents known in ad-vance, obtaining the codes for previously unseen documents remains to be a very challenging problem [42]. Existing methods either have prohibitively high computational com-plexity or impose exceedingly restrictive assumptions about data distribution (see Section 3.2). In this paper, we em-phasise this issue and propose a novel Self-Taught Hashing (STH) approach to semantic hashing. As illustrated in Fig-ure 1, we first find the optimal l -bit binary codes for all documents in the given corpus via unsupervised learning, and then train l classifiers via supervised learning to predict the l -bit code for any query document unseen before.
Our experiments on three real-world text datasets show that the proposed approach using binarised Laplacian Eigen-map (LapEig) [3] and linear Support Vector Machine (SVM) [23, 36] outperforms state-of-the-art techniques significantly, while maintaining a high running speed.

The rest of this paper is organised as follows. In Section 2, we review the related work. In Section 3, we present our approach in details. In Section 4, we show the experimental results. In Section 5, we make conclusions.
There has been extensive research on fast similarity search due to its central importance in many applications. For a low-dimensional feature space, similarity search can be carried out efficiently with pre-built space-partitioning in-dex structures (such as KD-tr ee) or data-partitioning index structures (such as R-tree) [7]. However, when the dimen-sionality of feature space is high (say &gt; 10), similarity search aiming to return exact results cannot be done better than the naive method  X  a linear scan of the entire collection [45]. In the IR domain, documents are typically represented as feature vectors in a space of more than thousands of di-mensions [30]. Nevertheless, if the complete exactness of results is not really necessary, similarity search in a high-dimensional space can be dramatically speeded up by using hash-based methods which are purposefully designed to ap-proximately answer queries in virtually constant time [42].
Such hash-based methods for fast similarity search can be considered as a means for embedding high-dimensional fea-ture vectors to a low-dimensional Hamming space (the set of all 2 l binary strings of length l ), while retaining as much as possible the semantic similarity structure of data. Unlike standard dimensionality reduction techniques such as Latent Semantic Indexing (LSI) [5, 8] and Locality-Preserving In-dexing (LPI) [17, 16], hashing techniques map feature vec-tors to binary codes, which is key to extremely fast simi-larity search (see Section 1). One possible way to get bi-nary codes for text documents is to binarise the real-valued low-dimensional vectors (obtained from dimensionality re-duction techniques like LSI) via thresholding [34]. An im-provement on binarised-LSI that directly optimises a Ham-ming distance based objective function, namely Laplacian Co-Hashing (LCH), has been proposed recently [50].
The most well-known hashing technique that preserves similarity information is probably Locality-Sensitive Hash-ing (LSH) [1]. LSH simply employs random linear projec-tions (followed by random thresholding) to map data points close in a Euclidean space to similar codes. It is theoretically guaranteed that as the code length increases, the Hamming distance between two codes will asymptotically approach the Euclidean distance between their corresponding data points. However, since the design of hash functions for LSH is data-oblivious , LSH may lead to quite inefficient (long) codes in practice [34, 48].

Several recently proposed hashing techniques attempt to overcome this problem by finding good data-aware hash functions through machine learning. In [34], the authors pro-posed to use stacked Restricted Boltzmann Machine (RBM) [19, 20], and showed that it was indeed able to generate compact binary codes to accelerate document retrieval. Re-searchers have also tried the boosting approach to Similarity Sensitive Coding (SSC) [38] and Forgiving Hashing (FgH) [2]  X  they first train AdaBoost [35] classifiers with simi-lar pairs of items as positive examples (and also non-similar pairs of items as negative examples in SCC), and then take the output of all (decision stump) weak learners on a given document as its binary code. In [44], both stacked-RBM and boosting-SSC were found to work significantly better and faster than LSH when applied to a database containing tens of millions of images. In [48], a new technique called Spectral Hashing (SpH) was proposed. It has demonstrated signifi-cant improvements over LSH, stacked-RBM and boosting-SSC in terms of the number of bits required to find good similar items. There is some resemblance between the first step of SpH and the unsupervised learning stage of our STH approach, because both are related to spectral graph parti-tioning [6, 13, 40]. Nevertheless, we use a different spectral method and take a different way to address the entropy max-imising criterion (see Section 3.1). More importantly, in or-der to process query documents, SpH has to assume that the data are uniformly distributed in a hyper-rectangle, which is apparently very restrictive. In contrast, our proposed STH approach can work with any data distribution and it is much more flexible (see Section 3.2). The superiority of STH to SpH has been confirmed by our experimental results (see Section 4).

A somewhat related, but different, line of research is to use hashing representations for machine learning [41, 47]. The objective of such techniques is to accelerate complex learning algorithms, but not similarity search. Our work is basically the other way around.
The proposed Self-Taught Hashing (STH) approach to se-mantic hashing is a general learning framework that consists of two distinct stages, as illustrated in Figure 1. We call the approach  X  X elf-taught X  because the hash function is learnt from the data that are auto-labelled by itself in the previous stage 2 .
Given a collection of n documents which are represented as m -dimensional vectors { x i } n i =1  X  R m .Let X denote the m  X  n term-document matrix: [ x 1 ,..., x n ]. Suppose that the desired length of code is l bits. We use y i  X  X  X  1 , +1 to represent the binary code for document vector x i ,where is on, or  X  1otherwise.Let Y denote the n  X  l matrix whose i -th row is the code for the i -th document, i.e., [ y 1 ,...,
A X  X ood X  X emantic hashing should be similarity preserv-ing to ensure effectiveness. That is to say, semantically sim-ilar documents should be mapped to similar codes within a short Hamming distance.

Unlike the existing approaches (such as SpH [48]) that aim to preserve the global similarity structure of all docu-ment pairs, we focus on the local similarity structure, i.e., k -nearest-neighbourhood, for each document. Since IR ap-plications usually put emphasis on a small number of most similar documents for a given query document [30], preserv-ing the global similarity structure is not only unnecessary but also likely to be sub-optimal for our problem. Therefore, using the cosine similarity 3 [30], we construct our n  X  n
It is, however, worth noticing that the term  X  X elf-taught learning X  X as been mentioned in [33] where the intention was to describe a strategy for transfer learning based on sparse coding, whereas in this paper the term has a rather different meaning.
Our approach can work with any legitimate similarity mea-sure, though we focus on cosine similarity in this paper. similarity matrix W as where N k ( x ) represents the set of k -nearest-neighbours of document x .Inotherwords, W is the adjacency matrix of the k -nearest-neighbours graph for the given corpus [3]. A by-product of focusing on such a local similarity structure instead of the global one is that W becomes a sparse ma-trix. This not only leads to much lower storage overhead, but also brings a significant reduction to the computational complexity of subsequent operations. Furthermore, we in-troduce a diagonal n  X  n matrix D whose entries are given by D ii = n j =1 W ij .Thematrix D provides a natural mea-sure of document importance: the bigger the value of D ii the more  X  X mportant X  is the document x i as its neighbours are strongly connected to it [3].

The Hamming distance between two binary codes y i and y j (corresponding to documents x i and x j )isgivenbythe number of bits that are different between them, which can be calculated as 1 4 y i  X  y j 2 .Tomeetthe similarity pre-serving criterion, we seek to minimise the weighted average Hamming distance (as in SpH [48]) because it incurs a heavy penalty if two similar documents are mapped far apart. After some simple mathematical transformation, the above objective function can be rewrit-ten in matrix form as 1 4 Tr( Y T LY ), where L = D  X  W is the graph Laplacian [6], and Tr(  X  ) means the matrix trace.
We found the above objective function (2) actually pro-portional to that of a well-known manifold learning algo-rithm, Laplacian Eigenmap (LapEig) [3], except that LapEig does not have the constraint y i  X  X  X  1 , +1 } l . So, if we relax this discreteness condition but just keep the similarity pre-serving requirement, we can get the optimal l -dimensional real-valued vector  X  y i to represent each document x i by solv-ing the following LapEig problem: where Tr(  X  Y T L  X  Y ) gives the real relaxation of the weighted average Hamming distance Tr( Y T LY ), and the two con-straints prevent the collapse into a subspace of dimension less than l . The solution of this optimisation problem is given by  X  Y =[ v 1 ,..., v l ]whosecolumnsarethe l eigenvec-tors corresponding to the smallest eigenvalues of the follow-ing generalised eigenvalue problem (except the trivial eigen-value 0):
The above LapEig formulation (3) may look similar to the first step of SpH [48]. This is because SpH is motivated by a spectral graph partitioning method ratio-cut [13], while LapEig is closely connected to another spectral graph par-titioning method normalised-cut [40]. Many independent studies have shown that normalised-cut has better theoreti-cal properties and empirical performances than ratio-cut [6, 40].

We now convert the above l -dimensional real-valued vec-tors  X  y 1 ,...,  X  y n into binary codes via thresholding: if the p -th element of  X  y i is larger than the specified threshold, y i = +1 (i.e., the p -th bit of the i -th code is on); other-wise, y ( p ) i =  X  1 (i.e., the p -th bit of the i -th code is off).
A  X  X ood X  semantic hashing should also be entropy max-imising to ensure efficiency, as pointed out by [2]. Accord-ingtothe information theory [39]: the maximal entropy of a source alphabet is attained by having a uniform probabil-ity distribution. If the entropy of codes over the corpus is small, it means that documents are mapped to only a small number of codes (hash bins), thereby rendering the hash ta-ble inefficient. To meet this entropy maximising criterion, we set the threshold for binarising  X  y ( p ) 1 ,...,  X  median value of v p . In this way, the p -th bit will be on for half of the corpus and off for the other half. Furthermore, as the eigenvectors v 1 ,..., v l given by LapEig are orthogonal to each other, different bits y (1) ,...,y ( l ) in the generated bi-nary codes will be uncorrelated. Therefore this thresholding method gives each distinct binary code roughly equal proba-bility of occurring in the document collection, thus achieves the best utilisation of the hash table.
Mapping all documents in the given corpus to binary codes does not completely solve the problem of semantic hash-ing, because we also need to know how to obtain the binary codes for query documents, i.e., new documents that are unseen before. This problem, called out-of-sample extension in manifold learning, is often addressed using the Nystrom method [4, 9]. However, calculating the Nystrom extension of a new document is as computationally expensive as an exhaustive similarity search over the corpus (that may con-tain millions of documents), which makes it impractical for semantic hashing. In LPI [17, 16], LapEig [3] is extended to deal with new samples by approximating a linear function to the embedding of LapEig. However, the computational complexity of LPI is very high because its learning algorithm involves eigen-decompositions of two large dense matrices. It is infeasible to apply LPI if the given training corpus is large. In SpH [48], new samples are handled by utilising the latest results on the convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds. It can achieve both fast learning and fast prediction, but it relies on a very restrictive assumption that the data are uniformly distributed in a hyper-rectangle.

Overcoming the limitations of the above techniques [4, 9, 17, 16, 48], this paper proposes a novel method to com-pute the binary codes for query documents by considering it as a supervised 4 learning problem: we think of each bit y i  X  X  +1 ,  X  1 } in the binary code for document x i as a binary class label (class- X  X n X  or class- X  X ff X ) for that docu-ment, and train a binary classifier y ( p ) = f ( p ) ( x )onthe given corpus that has already been  X  X abelled X  by the above binarised-LapEig method, then we can use the learned bi-nary classifiers f (1) ,...,f ( l ) to predict the l -bit binary code y (1) ,...,y ( l ) for any query document x . As mentioned in the previous section, different bits y (1) ,...,y ( l ) in the generated binary codes are uncorrelated. Hence there is no redundancy among the binary classifiers f (1) ,...,f ( l ) ,andtheycanalso be trained independently.

In this paper, we choose to use the Support Vector Ma-chine (SVM) [23, 36] algorithm to train these binary classi-fiers. SVM in its simplest form, linear SVM f ( x )=sgn( w consistently provides state-of-the-art performance for text classification tasks [10, 22, 49]. Given the documents x 1 together with their self-taught binary labels for the p -th bit y 1 ,...,y by solving the following quadratic optimisation problem A notable advantage of using SVM classifiers here is that we can easily achieve non-linear mappings if necessary by plugging in non-linear kernels [36], though we do not explore this potential in this paper. We name the above proposed two-stage approach Self-Taught Hashing (STH). In this paper, we choose binarised-LapEig [3] for the unsupervised learning stage and linear-SVM [23, 36] for the supervised learning stage, but obviously it is possible to use other machine learning algorithms.
The learning process of STH for a given corpus can be summarized as follows. 1. unsupervised learning of binary codes :
Since in the second stage, the supervised learning algorithm uses only the pseudo -labels input from the previous unsuper-vised learning stage, the entire STH approach remains to be unsupervised. 2. supervised learning of hash function : Let s denote the average number of non-zero features per document. In the first stage, constructing the k -nearest-neighbours graph takes O( n 2 s + n 2 k )timeusingtheselection algorithm [7], solving the LapEig problem (4) takes O( lnkt time using the Lanczos algorithm [12] of t iterations (the value of t is usually quite small), and the median-based bi-narisation takes O( ln ) time again using the selection algo-rithm [7]. In the second stage, thanks to the recent advances in large-scale optimisation, each of the l linear SVM classi-fiers can be trained in O( sn ) time or even less [24, 21], so all training can be done in O( lsn ) time. Both the value of l and the value of k can be regarded as small constants, as usually a short code length is desirable and just a few near-est neighbours are needed. For example, l  X  64 and k =25 in our experiments (see Section 4). Therefore the overall computational complexity of the learning process is roughly quadratic to the number of documents in the corpus while linear to the average size of the documents in the corpus.
The predicting process of STH for a given query docu-ment is simply to classify the query document using those l learned classifiers and then assemble the output l binary labels into an l -bit binary code. For linear SVM, classi-fying a document only requires one dot-product operation between two vectors, the aggregated support vector and the document vector (with s non-zero features), which can be done quickly in O( s ) time. Therefore the overall computa-tional complexity of the prediction process for each query document is linear to the size of the query document.
We now empirically evaluate our proposed STH approach (using binarised-LapEig and linear-SVM), and compare its performance with binarised-LSI [34], LCH [50], and SpH [48] that represents the state of the art (see Section 2).
In the following STH experiments, the parameter k = 25 when constructing the k -nearest-neighbours graph for LapEig 5 , and the SVM implementation is from LIBLINEAR [11] with the default parameter values 6 .
We have conducted experiments on three publicly avail-able real-world text datasets: Reuters21578 7 , 20Newsgroups and TDT2 9 .
In principle, the value of k for LapEig should be set to the desired number of original nearest neighbours to be retrieved (see Section 4.2).
It is not necessary to fine tune the SVM parameters (such as
C ) because it has already worked very well with its default parameter values. http://www.daviddlewis.com/resources/testcollections/ reuters21578/ http://people.csail.mit.edu/jrennie/20Newsgroups/ http://www.nist.gov/speech/tests/tdt/tdt98/index.htm
The Reuters21578 corpus is a collection of documents that appeared on Reuters newswire in 1987. It contains 21578 documents in 135 categories. In our experiments, those doc-uments appearing in more than one category were discarded, and only the largest 10 categories were kept, thus leaving us with 7285 documents in total. We use the ModeApte split here which gives 5228 (72%) documents for training and 2057 (28%) documents for testing.

The 20Newsgroups corpus was collected and originally used for document categorisation by Lang [27]. We use the popular  X  X ydate X  version which contains 18846 documents, evenly distributed across 20 categories. The time-based split leads to 11314 (60%) documents for training and 7532 (40%) documents for testing.

The TDT2 (NIST Topic Detection and Tracking) corpus consists of data collected during the first half of 1998 and taken from 6 sources, including 2 newswires (APW, NYT), 2 radio programs (VOA, PRI) and 2 television programs (CNN, ABC). It consists of 11201 on-topic documents which are classified into 96 semantic categories. In our experi-ments, those documents appearing in more than one cate-gory were discarded, and only the largest 30 categories were kept, thus leaving us with 9394 documents in total. We ran-domly selected 5597 (60%) documents for training and 3797 (40%) documents for testing. The averaged performance based on 10 such random selections is reported in this pa-per.

All the above datasets have been pre-processed by stop-word removal, Porter stemming, and TF-IDF weighting [30].
For the purpose of reproducibility, we shall make the datasets and code used in our experiments publicly available at the first author X  X  homepage upon paper publication.
Given a dataset, we use each document in the test set as a query to retrieve documents in the training set within a specified Hamming distance, and then compute standard retrieval performance measures: precision , recal l ,andtheir harmonic mean ( F 1 measure) [30]. precision = the number of retrieved relevant documents The reported performance scores in the following Section are averaged over all test queries in the dataset.

To determine whether a retrieved document is  X  X elevant X  to the given query document, we adopt the following two evaluation methodologies: 1. retrieving original nearest neighbours  X  X he k 2. retrieving same-topic documents  X  the documents The former methodology is used in [48] 10 , while the lat-ter methodology is used in [34]. In our opinion, these two
Actually only precision is used in [48], which is appropriate methodologies emphasise different aspects of semantic hash-ing, and thus are suitable for different target IR applications. Therefore we use both of them in this paper.

The absolute performance scores of STH are not as im-portant as how they compare with those of other semantic hashing techniques. As previously mentioned in Section 1, if necessary, we can always spend a little extra time to filter or re-rank the similarity search results based on their full content, thus achieve higher performance scores [42].
Figure 2 and Figure 3 show the F 1 measure of STH for retrieving original nearest neighbours and same-topic doc-uments respectively 11 . We vary the code length from 4-bit to 64-bit and also the Hamming ball radius (i.e., the max-imum Hamming distance between any retrieved document and the query document) from 0 to 3, in order to show their influences on the retrieval performance. It can be seen that when the code length increases, STH is able to achieve a higher F 1 measure (using a bigger Hamming ball radius). However, longer binary codes demand more memory and a bigger Hamming ball radius requires more computation. The optimal trade-off between effectiveness and efficiency can be found by using a validation set of query documents.
Figure 4 and Figure 5 compare STH with several other typical semantic hashing methods in terms of their precision-recall curves (created by varying the code length from 4-bit to 64-bit while fixing the Hamming ball radius at 1), for retrieving original nearest neighbours and same-topic documents respectively 12 . It is clear that on all datasets and under both evaluation methodologies, STH outperforms binarised-LSI, LCH, and the state-of-the-art technique SpH (that has already been shown to work much better than LSH [1], stacked-RBM 13 [34] and boosting-SSC [38]). Using 16-bit codes and Hamming ball radius 1, the performance im-provements are all statistically significant ( P value &lt; according to one-sided micro sign test ( s -test) [49].
We think the superior performance of STH is due to two reasons: for their application of pattern recognition but obviously insufficient from the IR perspective. Due to this difference in performance measurement, their results are not directly comparable with ours.
The F 1 measure scores reported here should not be directly compared with those in text categorisation papers, as we are addressing a very different problem even though the same datasets may have been used for experimentation.
Although we could achieve higher retrieval performance by utilising a bigger Hamming ball radius (e.g., 4), a large num-ber of binary codes (e.g., C 4 64 = 635376 for 64-bit codes) would need to be checked for each query and then the effi-ciency gain brought by semantic hashing would diminish.
For example, on the 20Newsgroups dataset, stacked-RBM achieves a maximum of F 1 =0 . 276 for retrieving same-topic documents with 128-bit codes, while the same level of per-formance can be obtained using our STH approach with just 8-bit codes.

We have also examined the approximation errors accumu-lated in each step of STH (see Section 3.3). Our anatomy re-veals that almost all approximation errors come from the di-mensionality reduction step using LapEig. However, LapEig does work better than alternative methods (such as LSI) for this step in our experiments, and it is a well-known hard problem to accurately detect the (intrinsic) dimen-sionality of data or effectively reduce the dimensionality of data. The median-based binarisation and SVM-based out-of-sample extension both work perfectly incurring little ap-proximation errors.

The proposed STH approach (using binarised-LapEig and linear-SVM) to semantic hashing is pretty fast: on an ordi-nary PC with Intel Pentium4 3.00GHz CPU and 2GB RAM, our Matlab implementation of 64-bit STH takes approxi-mately 0.0165 second per document for training (which is about 10 times faster than SpH), and 0.0007 second per document for prediction. The main contribution of this paper is a novel Self-Taught Hashing (STH) approach to semantic hashing for fast simi-larity search. By decomposing the problem of finding small codes for large data into two stages  X  unsupervised learn-ing and supervised learning  X  we achieve great flexibility in choosing learning algorithms. Using binarised-LapEig for the first stage and linear-SVM for the second stage, STH sig-nificantly outperforms binarised-LSI, LCH, and the state-of-the-art technique SpH [48]. Since STH is a general learning framework, it is promising to achieve even higher effective-ness and efficiency if more powerful unsupervised or super-vised learning algorithms can be employed.

We shall apply this technique to text mining tasks (such as automated text categorisation [37]) and content-based multimedia retrieval [28] in the near future. It would also be interesting to combine semantic hashing and distributed computing (e.g., [29]) to further improve the speed and scal-ability of similarity search.
 We are grateful to Dr Xi Chen (Alberta) for his valuable discussion and the London Mathematical Society (LMS) for their support of this work (SC7-09/10-6). We would also like to thank the anonymous reviewers for their helpful com-ments.
