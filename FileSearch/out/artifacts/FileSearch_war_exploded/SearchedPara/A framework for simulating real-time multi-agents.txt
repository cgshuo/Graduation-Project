 Chris Micacchi  X  Robin Cohen Abstract In this paper, we describe an implementation of use in demonstrating the effec-tiveness of architectures for real-time multi-agent systems. The implementation provides a simulation of a simplified RoboCup Search and Rescue environment, with unexpected events, and includes a simulator for both a real-time operating system and a CPU. We present experimental evidence to demonstrate the benefit of the implementation in the context of a particular hybrid architecture for multi-agent systems that allows certain agents to remain fully autonomous, while others are fully controlled by a coordinating agent. In addition, we discuss the value of the implementation for testing any models for the construction of real-time multi-agent systems and include a comparison to related work.
 Keywords Agent architectures and systems  X  Multi-agent systems  X  Distributed intelligent systems  X  Soft real-time environments  X  Adjustable autonomy systems 1 Introduction In this paper, we present a framework for simulating multi-agent systems operating in soft real-time environments with unexpected events. We discuss the benefits of this implementa-tion for the design of architectures for real-time multi-agent systems. Our aim is to enable designers of multi-agent systems to experiment with architectures that allow their agents to coordinate, to address unexpected events and to adjust their autonomy, in simulated envi-ronments X  X roviding insight into how best to model and manage the real-time scenarios that these agents must address. While we argue for the general value of the implementa-tion, emphasizing its ability to model real-time constructs, we include a concrete experiment conducted in the context of a specific multiagent system architecture, known as the hybrid approach. We begin by presenting this specific architecture, used as the motivation for the implementation and as the test case for the experimentation presented in this paper. We also briefly discuss the RoboCup Search and Rescue Simulation League challenge, used as the basis for the simulation that is running within the implementation. We outline the main components of the implementation before proceeding with a detailed discussion of the experimentation, demonstrating sample scenarios. We conclude by discussing the value of the research, its relationship to other work and the potential for future directions. 2 A hybrid model for designing multi-agent systems In our research, we developed a specific architecture for the design of multi-agent systems. Our architecture has two types of agents: the worker agents do tasks and make observations, while the coordinator agent oversees the workers and ensures that the system X  X  mission is completed successfully.

The coordinator is the system X  X  interface to the outside world, including any human over-seers and possibly the coordinators of other systems. All communication to the workers must come from the coordinator, and all communication from the workers must go to the coordi-nator. The coordinator is also responsible for assigning a mission (i.e., a top-level goal) to each individual worker, and the coordinator is the only agent in the system that can modify a worker X  X  mission. The coordinator receives the system X  X  mission from an overseer (human, or another agent), and cannot modify its own mission.

To reduce the need for communication, each worker is initially given a high level of auton-omy. When a worker is fully autonomous in our system it can generate and assign itself new goals, subject to the constraints of its mission. In addition, an autonomous worker generates its own plans that will allow it to accomplish its mission. However, when a worker encoun-ters an unexpected condition or situation it may choose to cede some of its autonomy to the coordinator to allow the condition to be resolved.
 Note that this architecture contrasts with more distributed approaches such as that of Balch and Arkin [ 2 ]. The aim is to take advantage of the coordinating agent to provide a global perspective for dispatching agents to resolve unexpected events in a timely fashion. We also contrast with approaches where agents elect a chief negotiator, as in Ortiz and Rauenbusch [ 9 ]. Instead, one agent is designated as the coordinator, once more reducing time in determining how best to allocate workers to address tasks when there are critical time constraints.

We develop a classification of the three types of unexpected events that workers can encounter, below. In distinguishing these kinds of events, we are outlining distinct chal-lenges to the agent X  X  execution of its plans that arise in the dynamic environment, each with its own method for resolving the challenge.
 Opportunities Opportunities are conditions that, while unexpected, are not detrimental to a worker. For example, if an agent is controlling a robot sent to explore the terrain on Mars, then finding an interesting new rock to study would present an Opportunity. A worker must gauge the value of an Opportunity and determine whether it is worth attempting to exploit it. If an Opportunity is deemed valuable, the worker will attempt to take advantage of it itself by searching for a suitable plan. If no such plan is found, the worker can instead choose to relay the unexploited Opportunity to the coordinator for possible assignment to another worker. However, if the worker succeeds in finding a plan to exploit the Opportunity, it sends a message to the coordinator informing it of the worker X  X  new goal and immediately begins executing the plan. Note that not all Opportunities need be exploited; when to generate new goals and plans is a domain-specific issue. See, e.g., [ 3 ]. Barriers Barriers are conditions that prevent a worker from progressing towards its goals. For example, a robot exploring the terrain on Mars that encounters the edge of a pit, as part of its current path, is facing a Barrier. Upon encountering a Barrier event, a worker will attempt to resolve the situation on its own by searching for an appropriate plan. If the worker cannot find a feasible plan, it must instead send a message to the coordinator indicating the problem. The coordinator then becomes responsible for resolving the Barrier. If the worker does find a plan that can resolve the Barrier, it instead only sends a message indicating its new goal to the coordinator, and begins to implement the solution immediately.
 Potential causes of failure Potential causes of failure (PCFs) are conditions that, if not cor-rected, will cause failure in the future. For example, an agent controlling a robot in a search and rescue operation that detects a fire spreading towards its locating has encountered a PCF. A worker can still make progress towards its goals up until the time of the failure, but when the worker believes that an event in the future will endanger one of its goals, it attempts to generate a plan that can avert the failure. If the attempt succeeds, the plan is executed and the worker informs the coordinator of its new state. If the attempt to resolve the PCF fails, the worker sends the problem to the coordinator to handle. The worker will continue to work on its current task while the coordinator resolves the PCF. 2.1 RoboCup search and rescue The RoboCup rescue simulation league [ 13 ] is part of an annual competition designed to foster research into intelligent robotics for search-and-rescue. Rescue agents are designated as one of police, fire, or ambulance agents. Each agent type has its own special ability: only fire agents can extinguish fires, only ambulance agents can rescue civilians, and only police agents can clear blocked roads. There are also Civilian agents that are controlled by the simu-lator and represent people to be rescued. Civilians can be trapped under rubble, which affects the time needed to rescue them. The simulation begins with a simulated disaster (generally an earthquake). The disaster simulators begin simulating collapsed buildings, fires, and blocked roads. The civilian simulator tracks the health of each civilian, which deteriorates over time if the civilian is trapped or on fire. The goal of the simulation is to minimize both civilian casualties and property damage (due to fires, which spread quickly). We use RoboCup Res-cue as the basis for the software simulation used in our overall implementation, but include some simplifying assumptions. These are described in more detail in the sections below. 3 Our implementation 3.1 Overview We have implemented a software simulation of a rescue scenario that we have used to test our model and evaluate its performance. Our simulation is based on the RoboCup Rescue simulator mentioned previously, though with some differences.

A number of properties of the simulation are represented by integer values; such properties are referred to as points . For example, civilians start the simulation with a certain number of hit points , which indicate the level of injury the civilian has sustained. 1 As well, each civilian has a number of rescue points , which count the amount of effort applied by the agents in the system towards rescuing that civilian. Likewise, fires in the simulation have an associated number of extinguish points , which count the amount of effort applied towards extinguishing the specific fire.

The simulated world is divided into a number of cells . The size of a single cell is 0.5 by 0.5m. Cells are the smallest unit of position in the simulation, and fractional positions are not permitted. A single cell contains information about the terrain at that location (such as elevation), as well as a list of all the objects (agents, civilians, and fires) present in the cell.
The specifics of the simulation are given in a scenario . A scenario describes the starting conditions of the simulation, including the number, type, and properties of both agents and civilians. The rules of the simulation (such as the cost of communication or the number of rescue points required to complete a rescue) are also specified in the scenario. See Table 3 of Appendix A for a complete list of configurable options.

The simulator is written in the Perl programming language. It is broken into a number of classes and modules, each of which performs a specific function within the simulation. The main components of the system are as follows. 1. The Real-Time Operating System (RTOS) and CPU simulators. 2. The World simulator. 3. The Agent simulators.

Each of these components is discussed in greater detail below. 3.2 The real-time core The most fundamental part of the real-time simulation is the real-time core, which simulates the actual hardware and the real-time operating system that would be running on top of it. The modules and classes described in this section comprise this part of the system.
The RT module is a Perl module that maintains the simulation clock. It is also responsible for keeping a list of all the schedulers in the system (see below), and for telling each sched-uler to execute any pending actions whenever the clock advances. Time in the simulation is measured in ticks , starting from 0. The number of ticks per simulation second is configurable.
As the simulation advances, the real-time core will advance the simulation clock auto-matically to the time of the next scheduled action, according to the schedulers the core is managing. That is, if the time is currently t = 0, and the next scheduled action on any sched-uler is at t = 435, the core will advance the clock to time 435 and then sequentially execute each scheduler X  X  scheduled actions for that time.
 The real-time core provides the following functionality.
 RT::Scheduler This class simulates a real-time scheduler, scheduling tasks for a single pro-cessor. The schedule consists of a list of actions to perform, accompanied by the time at which they are to be performed. An action in this context is an object reference and the name of the method to be called on that object.
 RT::Thread This class provides a mechanism for automatically rescheduling an action so that it runs repeatedly, and for accounting for the simulated time that action requires. Because the program is executing in actual time, each action that an agent or another simulator takes must estimate how much simulation time would be required to perform that action. This time is used to delay the execution of the simulated thread, to account for the time that would be needed to complete the action. Multiple threads can be created and attached to a single scheduler. To differentiate between them, each thread can be assigned a name when it is first created.
 RT::MASMessage This class simulates a wireless link between two simulated computers. It provides methods for sending messages to, and receiving messages from, other simulated processors. Messages are delayed automatically by the appropriate amount based on their length and the configured communication bandwidth. This class does not simulate a noisy channel; however, should such a channel be desired, the message latency can be increased by an appropriate amount to simulate the time required for error correction.
 RT::Notifier This class simulates asynchronous events (such as hardware interrupts) and allows threads to check if a specific event has occurred. It can track an arbitrary number of events; to differentiate between them, each event is assigned a name when it is first signalled. RT::IPC This class simulates a mechanism for inter-process communication (IPC). Mes-sages can only be sent between threads running on the same scheduler with this mechanism; the RT::MASMessage class handles messages sent to a thread on another scheduler. Unlike the MASMessage messaging system, IPC messages take no time to deliver. However, this class provides a mechanism to delay the receipt of a message by the receiver. This ability can be used, for example, when a plan has been generated. The generation of a plan takes a certain amount of simulated time: using this mechanism, the results of the planning can be delayed appropriately before they are received by the thread responsible for executing the plan. 3.3 The World class The World class maintains the world map , including data structures that allow look-up of objects by name or location. It is also responsible for drawing the map to the user interface. The world map is a data structure composed of a grid of cells that together represent the physical environment, including the present locations of all objects. The World class con-tains an RT::Scheduler object (but none of the other RT classes) to provide a mechanism for scheduling upcoming world events. This class also loads the scenario files specified in the configuration file, and creates the world and the objects (including agents) within it. Scenarios A scenario is composed of three parts. The first part is a configuration file that specifies all the configurable aspects of the simulation. The second part is a terrain map of the world, specified as a grey scale PNG file. The final part of a scenario is the object specification file, which describes all the objects in the scenario. Details on specifying a scenario can be found in [ 6 ]. 3.4 The fire simulator The fire simulator is implemented in the World::Fire module. It simulates the spreading and extinguishing of fires, and is also responsible for detecting when an agent has caught on fire. Fire in the simulation is modelled on a cell-by-cell basis. Periodically (based on the BURNINATE_INTERVAL option), the World object X  X  scheduler executes a function in the Fire module. The function examines each cell that is currently in the  X  X n fire X  state and, for each of its neighbours that aren X  X  in the  X  X n fire X  state, it adds the value of BURNINATE_POINTS to that cell X  X  combustion level. Once a cell X  X  combustion level exceeds the limit specified by COMBUST_TARGET , the Fire module marks it as  X  X n fire X . Any objects currently located within that cell are then notified of the state change, and can take any necessary actions. For example, a Civilian object will begin to lose hit points much more rapidly when it is on fire.
The fire simulation also handles Extinguish actions made by agents to put out fires. Cells subject to an extinguish action have their extinguish points increased by EXTINGUISH_POINTS for each Extinguish action. Once this value exceeds the EXTINGUISH_TARGET , the cell X  X   X  X n fire X  state is cleared. 3.5 The path planner The world simulator also contains a simple path planner. Given a map and starting and ending locations, the path planner will attempt to produce a path from the start to the end. If such a path exists it will return the sequence of cells to traverse along the path and the estimated cost of the movement at each step.

Each ModelWorker agent (described below) contains its own map of the world, which consists of all the information it has observed through its sensors. When a ModelWorker agent generates a path it does not have access to the true state of the world, only to its (possi-bly out-of-date) model. The path planner must therefore be able to generate paths using this outdated information.

Note that this path planner was chosen for its simplicity, since our focus was on the coor-dination issues between agents rather than on robotic navigation. However, because the type of path planner used does limit each agent X  X  flexibility, we would eventually like to replace it with a more flexible navigation system such as those discussed in [ 5 ]. 3.6 Unspecified components Our architecture does not specify the details of certain components. We assume the following components are available.
 A real-time planner and scheduler: A real-time planner and scheduler such as those described in Atkins et al. [ 1 ];Vincentetal.[ 14 ].
 A state evaluator: A mechanism for detecting Opportunities, and for detecting plan failures (Barriers) and potential failures (PCFs). For PCFs, the state evaluator needs to also determine the time of the potential failure.
 An opportunity evaluator: A mechanism for determining the utility of an Opportunity, such as is described in Estlin et al. [ 3 ].
 A task allocator: A mechanism to allow the coordinator to select workers to perform tasks it needs to perform. 3.7 Improvements over RoboCup search and rescue Our approach is very similar in intent to that of RoboCup rescue simulation system: our goal is to create a simulation of a rescue scenario in which different multi-agent coordination algorithms can be tested.
 However, our implementation provides a few necessary features not present in the RoboCup rescue simulator. For example, though RoboCup rescue allows only limited time for processing in each iteration of its simulation, its processing is dependent on the hardware the simulator is running on: a faster computer will be able to perform more processing in the same amount of time. Our approach, in which we simulate the computer hardware as well as the environment, allows us to remove this variable from our testing. We are also able to more accurately simulate the delays in communication present in a real environment. 4 The worker-coordinator model We have implemented our model on top of the basic real-time simulator described above. The main portion of the simulation resides in the agent simulators. Each agent simulator is a subclass of the Agent class.

There are two main types of agents in the simulator: ModelWorker agents and ModelCo-ordinator agents. These types of agents are comprised of a simulated computer, a planner, a state evaluator, and a control thread. In addition there is a Civilian agent that is very simple and only contains a scheduler and a few callback functions. There is no reasoning about directing the behaviour of Civilian agents X  X hese are agents whose presence drives the actions of the Workers and the Coordinators. In order to effectively model the environment, we simply log the status of civilians within their agents. 4.1 ModelWorker agents In our model, each worker is composed of its local state, a stack of its goals, and a number of threads of execution. The local state is the worker X  X  model of its local environment, as observed by its sensors as described below. The information in the state tables is used by the worker X  X  planner for evaluating possible plans, and by its state evaluator to determine if an Opportunity, Barrier, or PCF event has occurred. Appendix B.1 describes the goals that a worker can attempt and Appendix B.2 describes the events that a worker can encounter in our simulation.

The goal stack is used to track the worker X  X  current goal. At the bottom of the goal stack is the worker X  X  mission as given to it by the coordinator. As the worker generates new goals they are pushed onto the stack, and as the worker accomplishes them they are popped off it. The goal stack is only used when the worker is autonomous.

ModelWorker agents implement the design and the algorithms for our worker agents, and contain instances of the real-time classes described in the previous sections as well as classes and modules that implement message handling, a control thread, a planner and plan library, sensors, and motors.
 Worker messages One challenge in designing our processing algorithms for agents is deter-mining the conditions for communication within the community of agents. We propose that communication is always channelled through the coordinator, using the framework described below. Through this communication, the coordinator continuously tracks the state of the worker agents in the community and maintains a global state representation of the environ-ment. Workers can send three types of messages to the coordinator.
 Heartbeat messages are sent by workers periodically simply to keep the coordinator up-to-date on the worker X  X  state. The frequency of the Heartbeats is determined by the com-munications and processing limitations of the coordinator, as well as the requirements of the problem domain. Heartbeat messages do not require a response from the coordinator. Notification messages are sent whenever an agent exploits an Opportunity, when an agent generates a plan to solve a problem (a Barrier or PCF), or when an agent completes a task assigned to it by the coordinator. Notification messages are also used to report the results of a task or goal completion (such as the results of a scientific instrument) to the coordinator. Like Heartbeats, Notification messages do not require a response.
 Panic messagesaresentwhenaworker(orpossiblyeventhecoordinatorforanothersystem) requires the assistance or direction of the coordinator. Panic messages are sent by workers in response to Barrier and PCF events that the worker cannot solve, and will result in the coordinator taking an active role in the affected agent X  X  problem. 2
In the situation where all workers are proceeding towards their mission goals, the coordina-tor will have relatively little to do. In this scenario, the only types of messages the coordinator will receive are Notifications and Heartbeats. The coordinator X  X  processing will be limited to incorporating the information from these messages into its global state representation. However, if the coordinator receives a Panic message from a worker it will have to formulate and implement a solution, possibly by a deadline.
 Planning Planning by ModelWorker agents is divided into three parts. The first part is a simple reactive planner. Each agent can be attempting one of six possible goals, as listed in Appendix B.1. The possible events that can occur in the domain are described in Appendix B.2. Note that we simplify the simulation by making each agent capable of both finding civilians and extinguishing fires.

When an event (Opportunity, Barrier, or PCF) is received from the control loop, the plan-ner uses the event X  X  properties (in particular, its type and number) to decide the appropriate response. Some events have only one response; for example, if a ModelWorker agent encoun-ters Barrier B1 ( X  X ivilian has died X ), the only response it can make is to give up its  X  X escue Civilian X  goal, since no other action will allow it to complete the goal.

However, some events present the agent with a choice of responses. In this case, the agent evaluates each possible response and selects the plan that resolves the event the fastest. For example, if the agent encounters PCF P2 ( X  X ire is approaching the civilian X ), it can either attempt to postpone its rescue actions and instead extinguish the fire, or relay the problem to the coordinator to handle. If the ModelWorker expects to extinguish the fire and complete the rescue by its deadline, it will choose the  X  X xtinguish X  plan; otherwise, the worker must relay the problem to the coordinator to deal with.

The second component of the planning process is a plan library consisting of six plans (each in its own Perl module), corresponding to the six possible goals the agent can attempt. Each plan is further broken into three parts: initialization/generation, execution, and consis-tency checking.

When a plan is created, the initialization/generation portion of the plan is called, causing the plan X  X  internal state to be initialized, and all information necessary to execute the plan to be collected, potentially including a call to the path planner. The plan is checked for basic errors as it is generated; if an error (such as Barrier) is found, the error is returned to the planner and plan generation is aborted. Otherwise the generation was successful and the plan may be executed if desired.
The execution portion of a plan is a large if block, decided based on the current step and sub-step of the plan. For example, step 1 of a plan may be to travel to a specific location. The sub-steps of step 1 would then be the individual movements necessary to reach that location. Step 2 might be to perform Rescue actions until the target civilian has been rescued. The execution portion of a plan automatically recognizes when a step or sub-step has been completed; the next call to the execution portion will result in the next step of the plan being executed.

The final portion of a plan in the plan library is consistency checking. This portion of the plan is generally called by the state evaluator (described in the next section), though it can be called any time the agent needs to verify that its state is still consistent with that required by the plan.

Consistency checking is also divided into parts based on the current step (and possibly sub-step) of the plan, since different steps will have different requirements for their execu-tion. When the agent X  X  state evaluator calls this portion of the plan, the agent X  X  most recent local state information is compared with the state required for the plan. If part of the agent X  X  local state is inconsistent with the plan X  X  expected state, the appropriate Barrier or PCF is generated and returned to the caller. This is the essential mechanism by which Barriers and PCFs are recognized and dispatched by worker agents.

The last part of the planning process involves sending the newly generated and approved plan to the plan executor, a separate thread that is responsible for calling the plan X  X  execution portion repeatedly until the plan is complete. The plan is sent to the plan executor via the delayed IPC mechanism described in Sect. 3.2 .

Some parts of a plan X  X  execution portion may require the agent to wait for an asynchronous event to occur. For example, once the agent X  X  motors have been instructed to move the agent to the next location on a path, the plan must wait until the agent arrives at the new location before sending the next command. The plan executor allows a plan to  X  X lock X  itself until the asynchronous event occurs, without preventing the planner from running.
 Sensors Periodically, the agent will receive updates from its sensors. The frequency of sen-sor updates is controlled by the SENSOR_UPDATE_COST option. A sensor update consists of the agent reading the most recent information within its visibility range from the world map data structure described in Sect. 3.3 and storing it in a buffer local to the agent.
Once this operation is complete the sensor hardware signals an asynchronous event, trig-gering the execution of the state evaluator thread. The state evaluator reads the updated map information from the buffer and merges it with the agent X  X  local state. If a new Civilian is observed at this time, the state evaluator will generate an Opportunity event and relay it to the control loop (and thence to the planner). Likewise, if new terrain is observed, the state evaluator will generate the appropriate Opportunity events and relay them to the control loop as well.

Because the sensors update relatively infrequently, it is possible that a condition that a plan is waiting for will occur, but will not be noticed by the agent for a short period of time. Motors A ModelWorker agent can move about the world by issuing commands to its motor simulators. Motor commands are simply directives to move to the next cell in a certain direc-tion. Movement is simulated by first verifying that the agent can move into the desired map cell. If the cell contains fire, is outside the map, or requires a height change larger than that allowed by the MAX_SLOPE configuration parameter, the movement is aborted and a special flag is set indicating a collision has occurred. If the movement is allowed, the motor simulator computes the time required to travel to the new cell and registers an action for that time in the agent X  X  scheduler. When the action occurs the movement is considered complete and the agent X  X  position on the world map is updated to reflect the new state. In either case, the motors signal an asynchronous event when the agent stops moving. 4.2 ModelCoordinator ModelCoordinator agents implement the design and the algorithms for our model X  X  coor-dinator agent. Like ModelWorker agents, ModelCoordinator agents derive from the main Agent class and contain instances of the real-time simulation classes described in Sect. 3.2 , as well as classes and modules that implement a control loop, a planner and a plan library. However, unlike the ModelWorker class, the ModelCoordinator has no motors or sensors. Instead it has classes to model each of its workers, and to help it select workers to perform tasks. Appendix B.1 describes the goals that the coordinator can attempt and Appendix B.2 describes the events that the coordinator can encounter in our simulation.
 Coordinator messages Because each ModelCoordinator message is essentially a command, instructing a worker to start or stop performing a task, there is no need to prioritize between messages, as is done for the ModelWorker class in Sect. 4.1 ; all ModelCoordinator messages are important.
 New Mission messages are sent by the coordinator to assign or reassign a worker X  X  top-level goal. A worker must first receive this message from the coordinator before it can begin to perform any tasks. The coordinator can also use this type of message to reassign a worker to a new task without revoking its autonomy. For example, if a worker encounters a Barrier that it cannot itself handle, the coordinator may elect to give it a new mission rather than to revoke its autonomy. This message may also include constraints the coordinator wishes to place on the worker.
 Revoke Autonomy messages are sent to a worker when the coordinator wishes to take con-trol of the worker. The message body should include a new plan or goal for the controlled worker to begin working on.
 Restore Autonomy messages release a worker from the coordinator X  X  control. This message complements the Revoke Autonomy message, above.
 New Task messages reassign an already-controlled worker to a new task. The coordinator can send such a message to a worker if it must use the worker to perform a new task before the first task it assigned the worker has been completed.
 Resume Task messages instruct an already-controlled worker to stop working on its current task and resume working on a previously suspended task instead.
 Planning Planning by ModelCoordinator agents is structured in the same manner as plan-ning for ModelWorker agents. Like ModelWorker agents, the ModelCoordinator agent has a planner and a plan library. However, there are some key differences in how plans are generated, and how they are executed and monitored.

When the ModelCoordinator receives an event (such as a Barrier), it must construct a plan to resolve the problem posed by the event, including determining which workers are to be used to execute the new plan. Therefore, part of the ModelCoordinator X  X  planning phase includes evaluating the candidate workers and selecting one or more to perform the task. In the implementation used in the ModelCoordinator, this step is accomplished by evaluating, for each possible solution to the event, the potential of each worker to perform the plan asso-ciated with the solution. If the ModelCoordinator expects that a worker could successfully complete a plan, the worker and the plan are added to a list of candidates. Once all possible solutions for the problem are examined and the list of candidate workers is built, the Model-Coordinator selects the worker with the earliest plan-completion date and proceeds to assign it the new task.

Because the ModelCoordinator does not actually execute the plans it generates, it does not have an equivalent of the ModelWorker X  X   X  X lan executor X . Instead, when a plan has been generated and assigned to a worker, it is bundled into a message and sent (via the RT::MAS-Message mechanism) to the selected worker. The worker then unbundles the plan and, after a cleanup step to correct for small differences in global and local state models, begins execution of the plan.
 Global state representation The ModelCoordinator agent receives updated state informa-tion from the various ModelWorker agents it controls. This information is processed by the agent X  X  state evaluator, which splits the information into global and worker-specific infor-mation. Global information includes updated map information and new objects discovered. Worker-specific information includes the worker X  X  current position and its current goal. For example, the workers use a stack to track the state of all of their goals, and the coordinator, when modelling the worker X  X  state, maintains its own version of this goal stack. 4.3 Civilian The Civilian agent is a very simple Agent subclass that models the health of a civilian in the simulation using hit points, as mentioned in Sect. 3.1 . The civilian starts with a given number of these points, as specified in the scenario configuration files. A callback is used to periodically deduct hit points from the civilian X  X  total. If the number of hit points a civilian has drops below zero, the civilian is  X  X ead X . The Civilian agent is informed by the World X  X  fire simulator when the cell it occupies is ignited. While the cell remains in the  X  X n fire X  state, the Civilian will lose hit points at a rapid rate.

ModelWorker agents that are performing a rescue action on the Civilian agent do so by calling the Civilian X  X  Rescue function. Each rescue action increments the Civilian agent X  X  rescue counter. Once the counter exceeds the threshold set by the civilian X  X  difficulty (which is multiplied by the RESCUE_TARGET configuration parameter), the Civilian agent has been  X  X escued X . Any ModelWorker agents performing rescue actions can detect this condition to determine when a rescue is complete. 5 Experiments Table 1 provides a list of the abbreviations used within this section. 5.1 Agents and actions Our experiments include only one type of agent. 3 The agent can perform certain actions, described below.
Rescue actions allow an agent to spend its time rescuing a trapped civilian. The rescuing agent must be in a cell adjacent to the target civilian. Performing a Rescue action requires a certain amount of time and adds a certain number of rescue points to the civilian X  X  rescue counter (as specified by the RESCUE_COST and RESCUE_POINTS configuration options). The scenario specification includes a rescue target for each civilian, which is the number of rescue points required for the civilian to be successfully rescued. If multiple agents perform Rescue actions on a single civilian simultaneously, their efforts are combined. Thus, collaboration will reduce the time required to complete the rescue.

Extinguish actions allow an agent to spend its time putting out fires. Agents performing this action can extinguish a small area of fire a number of cells away. The exact size and range of the extinguished area are specified by the EXTINGUISH_RADIUS and EXTINGUISH_PROX-IMITY configuration options. Extinguish actions are similar to Rescue action: each action increments the target fire X  X  extinguish points until the number of extinguish points exceeds a specific threshold, at which point the fire has been put out. Also like Rescue actions, multiple agents extinguishing the same cells will combine their efforts, causing the fire to be put out faster.
 Move actions allow the agent to travel to an adjacent cell, provided the movement is legal. Illegal movement includes attempts to move outside of the world map X  X  boundaries, to move into a cell that is on fire, or to move up or down a slope that exceeds the maximum slope allowed by the MAX_SLOPE option (this includes attempts to move through walls). The time required to complete a Move action depends on the specific terrain being moved through: travelling uphill requires more time than travelling across flat ground.

Only one of the above actions can be performed by an agent at a time. An agent cannot simultaneously move around a fire and attempt to extinguish the fire; however, the agent could alternate between performing single Move and Extinguish actions to achieve a similar effect. 5.2 Assumptions To simplify the implementation of our model, we make certain assumptions about the capa-bilities of the agents. In particular, in contrast to RoboCupRescue, where each agent can perform only one type of action (Rescue, Extinguish, or Clear Debris), our worker agents all have the same capabilities. This assumption was made to simplify the planning step, allowing us to focus more fully on implementing and testing our architecture for various scenarios. Further details on assumptions made are available in [ 6 ]. 5.3 Scenario One In this scenario there are two Worker agents and two injured civilian agents, as shown in Fig. 1 . Each Worker agent will initially detect only one of the two civilians, and proceed to rescue the particular civilian that it sees. However, due to various obstructions and deadlines, a number of interesting events occur as the simulation progresses. In particular, to successfully complete the rescue of civilian Civ6, Worker3 will require the aid of Worker2; otherwise, the rescue will not complete before Civ6 dies due to its injuries.
 The next section examines the sequence of events that result from this scenario in detail. As will be seen, a wide variety of unexpected events can be handled efficiently and effectively, demonstrating the value of the Worker X  X oordinator architecture.
 Scenario details The very first action that each Worker agent performs (as part of its ini-tialization) is to send a registration message to the Coordinator. This message enables the Coordinator to begin modelling the actions of the Worker, and lets it know that the Worker is available to perform tasks should the Coordinator find it necessary.

In response to the registration message, the Coordinator sends a NEW_MISSION message to the Worker. Upon receipt of the message, each Worker becomes fully activated and begins executing its mission, which in this case is to explore the terrain and rescue civilians.
Worker2 must spend some time to observe and process its surroundings. Once it has done so, the agent will recognize that there is a civilian (Civ1) within its field of view, and will generate an Opportunity event ( O1 ) to rescue the civilian it sees. This excerpt of the log also shows the various threads in the agent running. The CONTROL_LOOP thread runs and receives an Opportunity event Inter-Process Communication (IPC) message, via the simulated IPC mechanism, from the STATE_EVALUATOR thread that ran previously. The CONTROL_LOOP then relays the event to the PLANNER thread.

The planner uses the information given in Appendix B.2 to determine the possible actions to take in response. In this case, the planner first tries using the goal G1 as the response. It generates a plan for the goal, tests it against the current and predicted state, and finds that the plan will succeed. The planner therefore selects the plan to achieve G1 as the proper action to take. Likewise, Worker3 will observe a different civilian (Civ6) and decide to rescue it, in a manner similar to that described above.

Once the plan has been finalized, Worker2 sends a Notification message to the Coordinator informing it of Worker2 X  X  new goal, and wakes up the PLAN_EXECUTOR thread, allowing it to start executing the current plan.

Each time the sensors receive new information, the Workers check the newly observed state of the world against the state required for their plans. Because Worker3 X  X  plan requires it to travel a longer path than Worker2, its planning step is correspondingly longer. Thus, as shown in the log excerpt below, Worker3 does not begin working on its plan until time 357, after Worker2 has already begun moving towards its target.

When the Coordinator receives Worker2 X  X  GOAL_CHANGED Notification message, it incor-porates the information contained in it (which includes updated map information) into its global state model.

Shortly after the Coordinator receives the updated state information, Worker2 completes its first movement action, triggering an asynchronous event that awakens its PLAN_EXECU-TOR thread. The PLAN_EXECUTOR thread then initiates the next movement action and begins waiting for it to complete. As each worker moves, new and unexplored cells of the map are discovered. Each cell generates an Opportunity event ( O2 ), but because O2 has a lower priority that the worker X  X  current goal ( O1:G1 ), these Opportunities are ignored. Barriers arising After a short time, Worker2 arrives at a cell adjacent to Civ1. When its PLAN_EXECUTOR next runs, it recognizes that the first phase of the plan is complete and begins executing the next phase, to rescue the civilian. Worker2 compares the expected time to rescue Civ1 with the expected time before Civ1 dies. In this case, Civ1 is expected to live a long time compared to the time required for the rescue to be complete, so Worker2 does not generate a PCF event. However, Worker3 X  X  plan now appears to be in danger, as the fire is blocking the entrance to the building in which Civ6 is located. When Worker3 tries to move into the building, it finds its path blocked by the fire. The agent X  X  hardware recognizes this condition and aborts the movement. When the PLAN_EXECUTOR is notified of the error state, it automatically suspends the affected goal ( G1 ).

At this point, the agent does not know the cause of the problem, so no further action can be taken until the next sensor update. The STATE_EVALUATOR finds the cause of the problem (fire is blocking the path), and generates the appropriate Barrier event ( B3 ). The new event is sent to the CONTROL_LOOP thread, which relays the IPC message to the appropriate thread to be handled (the PLANNER thread). The planner determines the best solution is to simply put out the fire, so a new goal is generated ( G2 ) to perform this task.
 Once the plan is generated, Worker3 sends a GOAL_CHANGED Notification message to the Coordinator to inform it of Worker3 X  X  new state and then begins executing the goal.
Worker3 is continues to check the observed state of the world against the state expected for each of its plans, including the suspended rescue plan. A short time later, the Coordinator receives the GOAL_CHANGED message sent by Worker3. The Coordinator adds the new goal to its model of what Worker3 is currently doing. In this scenario, Heartbeat messages are sent if 1,500 ticks (5s) have elapsed since the last interagent message (including Notifications, Panics, and previous Heartbeats) was sent.

After some time, Worker3 performs a final Extinguish action on the fire. In this scenario, a fire must accumulate 10,000 extinguish points to be considered put out. The fire is now extinguished, so at the next sensor update the agent detects this change and generates an appropriate Barrier event, B2 . The Barrier causes Worker3 to remove the current goal ( G2 ) from its goal stack and resume its previous goal instead. The worker also sends two Notifica-tions messages to the Coordinator. The first is a TASK_COMPLETE message, and indicates that the previous task ( G2 ) is now complete. The second is a NEW_TASK message that indicates the previous task that the worker has resumed working on. When the Coordinator receives the TASK_COMPLETE message, it removes the finished goal from its model of Worker3.
As Worker3 moves, new Opportunities arise to explore new parts of the map. However, the simple priority system we use ensures that the Opportunity to rescue a civilian takes precedence, so Worker3 ignores the new Opportunities.
 Sending a panic message Worker3 continues to make its way towards Civ6, putting out fires as it moves (Fig. 2 ). However, the time required to extinguish the fires cuts into the time available to complete the rescue of Civ6. At time 12100, Worker3 encounters another Barrier event ( B3 ). However, when Worker3 attempts to generate a plan to resolve the Barrier, it finds that doing so would cause it to miss the deadline imposed by its earlier goal G1 .The estimated completion time for G1 is now later than the deadline by which G1 must be com-pleted. Thus, Worker3 cannot perform goal G2 without violating the constraints imposed on it by G1 . In our architecture, this situation requires that Worker3 suspend its actions, send a Panic message to the Coordinator, and await its response.
 Losing autonomy A short time later the Coordinator receives the UNHANDLED_BARRIER Panic message from Worker3 and attempts to find a solution. The first solution it tries is to send Worker2 to help rescue Civ6; however, this plan is rejected because Worker2 will not arrive at Civ6 in time to complete the rescue either. 4 Because no agent can successfully be sent to help rescue Civ6, the Coordinator falls back on a  X  X efault X  plan to have Worker3 extinguish the fire regardless of the deadline faults. Once the plan has been generated, a REVOKE_AUTONOMY message is sent to Worker3 containing the new goal and plan objects for it to perform, and the new goal is added to the Coordinator X  X  model of Worker3 X  X  goal stack.

Worker3 receives the message and performs a quick check of the new plan against its current state. If there are any inconsistencies (such as Worker3 being in a different location than the plan expects), they are corrected. The new goal is marked as the active goal, and its plan is dispatched to the agent X  X  PLAN_EXECUTOR thread, which begins executing the plan.
Worker3 has now lost its autonomy and must spend its time executing the plan given to it by the Coordinator. Of course, as one would expect, Worker3 immediately determines that the current plan will cause its original plan (for G1 ) to miss its deadline. However, because Worker3 is not autonomous, it cannot handle the problem itself. Instead, Worker3 must send an UNHANDLED_PCF Panic message to the Coordinator.

The Coordinator receives the Panic message and determines that, at least for the moment, none of the plans it has available can remedy the situation. 5
When Worker3 extinguishes the fire, it detects the condition as a Barrier ( B2 ) (because the fire was extinguished before the agent thought it would be) and, because it has no autonomy, relays the Barrier to the Coordinator as a Notification message.
The Coordinator determines that the worker has finished extinguishing the fire and decides to tell the worker to stop working on goal G4 . It removes the entry for the goal from its model of Worker3 X  X  goal stack and, because G4 was the only goal that the Coordinator had given to Worker3, sends Worker3 a REVOKE_AUTONOMY message.
 Worker3 receives the message and removes the completed goal from its stack. It then resumes its last autonomous goal (which was G1 ). Finally, Worker3 notifies the Coordinator that it has changed goals with a GOAL_CHANGED Notification message.
 Handling potential causes of failure Worker3 encounters a similar fire-related Barrier. How-ever, just as it is completing the plan to extinguish the fire, its analysis of the state for its plan for goal G1 determines that the Civilian is going to die before the rescue completes, causing it to generate a new PCF event, P1 . Because Worker3 is still being controlled by the Coordinator, it relays the problem to that agent as a Panic message.

The Coordinator attempts to resolve the PCF by assigning a second worker (Worker2) to help with the rescue. By sending Worker2 to help Worker3, the Coordinator determines that the rescue can be completed by the PCF X  X  deadline, without also violating any constraints imposed by Worker2 X  X  current goals. Note that the Coordinator can be highly confident that its model of Worker2 X  X  current state (including its current goals) is accurate because Worker2 has been sending periodic Heartbeat messages to it, and has not yet sent a TASK_COMPLETION or GOAL_CHANGED message.

Meanwhile, having dealt with the PCF, Worker3 continues to execute the extinguish plan giventoitearlierbytheCoordinator.Thenextstepofthisplanistosenda TASK_COMPLETION Notification message to the Coordinator.

Even while the Coordinator X  X  planner is attempting to determine the best course of action, its CONTROL_LOOP thread receives the TASK_COMPLETION message sent to it by Worker3 and responds by removing the completed goal from its model of the Worker3 X  X  goal stack and then restoring that worker X  X  autonomy with a RESTORE_AUTONOMY message. Worker3 receives the message and once again resumes working on its original goal, G1 . Once planning is complete, the Coordinator adds the new goal to its model of Worker2 X  X  goal stack and sends a REVOKE_AUTONOMY message to Worker2, which at this time is still rescuing the original civilian it discovered, Civ1.
Meanwhile, Worker3 has arrived at its destination and can now begin performing rescue actions on Civ6. When Worker2 receives the Coordinator X  X  message, it stops rescuing Civ1 and begins to move towards Civ6 using the plan given to it by the Coordinator.

As the simulation progresses, the fire slowly spreads. One of the dangers the workers must watch for is the proximity of the fire to their rescue targets. As Worker3 begins performing rescue actions, it predicts that a nearby fire is likely to reach Civ6 X  X  location before the rescue is complete. This prediction causes Worker3 to generate a new PCF, P2 . Worker3 determines that it can handle the problem on its own if it spends some time extinguishing the fire at issue. After the planning completes, the worker sends a GOAL_CHANGED Notification message to the Coordinator and begins working on the new goal.

When the fire is extinguished, Worker3 generates a Barrier event ( B2 ), which causes it to discard its current goal. However, as the worker resumes rescuing Civ6, it detects another nearby fire that poses a threat to its rescue goal. Worker3 cannot extinguish the fire because it cannot get close enough to the fire X  X  location to perform an extinguish action on it, so it must relay the problem to the Coordinator to handle. While it awaits the Coordinator X  X  response, it will continue to perform rescue actions.

The Coordinator gets the message and attempts to find a solution to the PCF. First, it attempts to send another worker to help perform the rescue. However, it finds that Worker2, the only other worker in the scenario, is already performing a more important task: Worker2 is on its way to rescue Civ6, and that goal has a higher priority than the new goal would. 6 See Appendix B.2 for the priorities assigned to each event.

The Coordinator then attempts to find workers than could extinguish the fire. First it checks whether Worker3 can perform the task, and finds (as Worker3 itself did) that Worker3 cannot reach the location of the fire to perform the extinguish action. When the Coordinator checks whether Worker2 can perform the action, it again finds that Worker2 is already performing a more important action. The Coordinator has no plans available to deal with the situation, and so is forced to ignore the problem. Since Ignore Event is a possible response to PCF P2 , this response is allowed. The Coordinator does not send a message to Worker3 in this case; Worker3 will simply continue to perform rescue actions and operate autonomously, as it did before it found the PCF.
 Handling barriers with the coordinator On its way to aide Worker3, Worker2 frequently finds that fire has blocked its movement forward. Because it is not autonomous, the Barrier events that it generates ( B3 ) are relayed to the Coordinator. The Coordinator examines its global state each time it receives one of these events and determines that Worker2 should extinguish the fire in its path itself.

Eventually, Worker2 puts out the fires and sends a TASK_COMPLETION Notification mes-sage to the Coordinator. The Coordinator drops the completed goal and sends the worker a RESUME_TASK message, telling it to resume working on the previous goal the Coordinator hadsentit(thatis, G3 ).

Once Worker2 makes it past the various fires, it arrives at Civ6 X  X  location and begins helping Worker3 perform rescue actions (Fig. 2 ). The nearby fire that was previously inac-cessible eventually spreads to an area that the workers can reach. When it does, Worker2 detects the condition and, because it is not autonomous, informs the Coordinator. The Coor-dinator attempts to find a solution to the problem once more, and this time is able to assign Worker3 to perform the extinguish action on the fire.
When it receives the REVOKE_AUTONOMY message, Worker3 will temporarily lose its autonomy and stop performing rescue actions on Civ6 to put out the fire. Once the fire is out it will send either a Barrier event ( B3 ), or a TASK_COMPLETION Notification message, depending on whether the fire is extinguished at the time predicted or not. 7 Whichever mes-sage is sent, the Coordinator will in turn send Worker3 a RESTORE_AUTONOMY message, allowing it to resume rescuing Civ6 with Worker2.
 Monitoring tasks Worker2 X  X  STATE_EVALUATOR  X  X  analysis of the situation now shows that the rescue should be completed before any more dangers approach, as illustrated in the log excerpt below. Civ6 should be rescued by time 88100, before Civ6 dies and before the nearest fire reaches its position at time 89600.
 Note that, as the simulation progresses, both worker agents are continually adjusting their predictions for when their various tasks will be complete and when the various deadlines will occur. Both workers are also sending Heartbeat messages to the Coordinator, keeping it up-to-date on their progress.
 Completion of mission At time 88,001 Worker2 performs the final rescue action on Civ6, and, at time 88,100, Worker2 tests the observed state against that expected for step 3.0 of its plan. Step 3.0 is the final  X  X lean up X  phase of the plan for goals G1 and G3 , indicating that Worker2 has already detected that the rescue operation is finished. Note, however, that Worker3 is still working on step 2.0 of the plan, since it has not yet detected that the rescue operation is finished. Worker2 now sends a TASK_COMPLETION Notification message to the Coordinator and waits for the Coordinator X  X  next command.

The Coordinator receives the message and determines that Worker2 has completed all the goals assigned to it by the Coordinator, so the Coordinator sends Worker2 a RESTORE_AUTON-OMY message.

When Worker2 receives the message, it performs a state evaluation against the plan for its previous goal, G1 , and detects a new Barrier event, B6 . Barrier B6 indicates that the agent is not in the correct position to perform the action it wishes to (in this case, Worker2 is too far from Civ1 X  X  location to perform rescue actions on it). Worker2 responds to the Barrier event by regenerating its plan for goal G1 . The new plan includes travelling back to the location of Civ1 and then performing rescue actions on it until it is rescued.

Worker2 begins moving back towards Civ1. However, while it was rescuing Civ6 the fire has spread and covered much of the area around the exit to the building Civ6 was located in. Worker2 must therefore spend time extinguishing the fires that block its movement. Each time it encounters fire in its path, it goes through the same process shown above: it is able to determine on its own a solution to the problem, and proceeds each time to extinguish the fire without requesting help from the coordinator. Worker2 encounters fire blocking its path at times 89,261, 94,474, 100,084, 105,799, 111,513, 117,164, 123,056, 128,677, 134,223, 140,074, and 145,811.
 Eventually, Worker2 reaches the location of Civ1 and resumes making rescue actions. The rescue of Civ1 is uneventful, and, at time 165,300, Civ1 X  X  rescue is complete and the simulation ends. See Fig. 2 for screenshots of this scenario X  X  progression. 5.4 Summary Scenario one demonstrates that our implementation can be used to test different multi-agent architectures in a simulated real-time environment that can change unexpectedly. In addition, it shows that our model is capable of handling these unexpected events in a timely manner, and that our coordinator is able to facilitate teamwork amongst otherwise independent agents by adjusting their autonomy when necessary.

In particular, scenario one demonstrates the following ideas.  X  Workers can autonomously exploit interesting Opportunities that arise, for example, when  X  Workers can autonomously discard uninteresting Opportunities. For example, as each  X  Workers can autonomously detect and handle unexpected Barriers that occur. For exam- X  Workers can autonomously detect and handle unexpected PCFs that occur. For exam- X  When workers cannot handle a Barrier or PCF, they can get help from the coordinator.  X  The coordinator can maintain a global picture of the entire world using information given  X  The coordinator can facilitate teamwork between workers by adjusting their autonomy.  X  Controlled workers are able to depend on the coordinator to solve problems they may 5.5 Scenario two This scenario is very similar to the first scenario. There are two Worker agents and three injured civilian agents, as shown in Fig. 3 . Unlike in scenario one, in this experiment only Worker2 will detect a civilian (Civ3); Worker1 will instead exploit an Opportunity to explore the terrain nearby. However, because of the wall that separates it from the civilian, Worker2 will be unable to exploit its Opportunity to rescue Civ3. Thus, the coordinator must allocate another worker (Worker1) to perform this task. As the worker executes the plan, it encounters some additional civilians that need to be rescued. This requires the coordinator to reassign an already controlled worker to perform an additional task. 6 Discussion 6.1 Real-time issues We have designed our system from the ground up to support real-time decision-making and response. In particular, our architecture is based on a model commonly found in real-time control systems, in which there is a central Administrator process and a number of Worker processes. The Administrator is responsible for dispatching Worker processes, for handling interactions between Workers, and for facilitating communication between Workers and other processes in the system. We have adapted this model to allow the Workers and Administrator to be independent autonomous agents.

Our system is also designed to support real-time application domains and real-time fail-ures. By their nature, PCF events have a deadline by which they must be resolved (recall that a PCF is a problem that will cause failure in the future). In this manner, our system can incorporate application domain-imposed real-time constraints. We treat such deadlines as PCF events: a worker will generate a PCF event if it comes to believe that the task with the time constraint will not be completed by its deadline. To detect potential problems, the worker agent must be able to predict the time of failure. Thus, PCF events are real-time deadlines imposed on the coordinator: the coordinator must determine a solution and ensure its execution is completed before the failure occurs.

When operating under real-time constraints it is especially important to minimize response time. In our architecture, workers are able to solve problems locally if possible, eliminat-ing the communication delay of contacting the coordinator. The extent to which a worker may solve problems on its own is dependent on the problem domain, and is dictated by the worker X  X  mission. For example, the mission might explicitly specify that a worker travelling to a destination must keep its speed below a certain threshold. If the worker determines that it cannot arrive at the destination by the required time, it is allowed to increase its speed. However, if the increase in speed exceeds the threshold, the worker will instead be required to request help from the coordinator (in the form of a PCF). The coordinator can then instruct the worker to adjust its speed to a value higher than the threshold, or even to take a different path. 6.2 Value of the implementation The implementation described in this paper encodes a methodology for addressing soft real-time constraints within a multi-agent system. As discussed in Sect. 3.2 , the real-time core incorporated into the implementation serves to simulate the actual real-time operating system and hardware, and is therefore useful for testing real-time multi-agent systems. Our implementation provides a valuable testbed for our model, which we have used in Sect. 5 to demonstrate its abilities. However, our implementation also provides an archi-tecture-neutral method that can be used to compare different multi-agent system designs. Because the implementation is split into world simulators and agent simulators, it is a straight-forward process to replace the currently implemented agents, based on our model, with new agents that use an entirely different form of coordination or have an entirely different struc-ture. New agents built to use our simulator can still take advantage of the supporting functions it provides, such as real-time schedulers and simulated radio links.
 It is an easy matter to create new scenarios with which to test agents (ours or otherwise). Because scenarios are specified using only plain text files and standard PNG-format images, no special software is required to create new ones or adjust existing ones. For example, a scenario could be modified that has more or fewer agents, or more or fewer civilians, than the previous scenario. Individual civilians could be made harder or easier to rescue by adjusting their starting hit points or the civilian X  X  rescue difficulty. Even the basic rules of the simulation can be changed: a scenario could be created in which inter-agent communication was free, or was very expensive, or where fires are easier or harder to extinguish. Simulator scenarios are very flexible and are easy to create and adjust.

It is also easy to create new agents for use with our simulator. To create a new agent, simply create a subclass of the Agent class. The Agent class provides hardware simulators for mobile agents, such as motors and sensors. Also provided by this class are the Goal, Dead-line, and Event objects that encapsulate goals, deadlines, and the three types of unexpected events described in Sect. 2 . The existing Goal and Event objects detail those goals and events described in Appendix B, and may need to be augmented to include any new goals, Barriers, Opportunities, or PCFs the new agents can encounter. Of course, any new agents can also take advantage of the many real-time and world simulation classes that are part of the basic simulator.

The final step towards creating a new agent involves creating a new scenario for the agent, and ensuring that the scenario X  X  configuration file is set to refer to the new agent class. Once configured, the World class will automatically instantiate the new agent class based on the entries in the scenario X  X  object specification file.

Note that to use our implementation to test a fully distributed architecture where agents must communicate directly with one another to coordinate their actions and to assign tasks, one would need replace our ModelWorker agents with agents capable of such negotiation. Our ModelCoordinator agents would also not be needed and could simply be omitted from the new test scenarios. We would expect the negotiations in this architecture to slow down the allocation of tasks, so that fewer unexpected events would be effectively addressed within their time constraints.

To use our implementation to test a fully centralized architecture where worker agents are always controlled by a central coordination agent, one would need to modify our Model-Worker agents to remove the autonomous planning portions. Our ModelCoordinator would need to be extended to more fully support the various goals that currently are only handled by individual ModelWorker agents (such as goal G2 ). We would expect the loss of autonomy for some of the agents would result as well in a slower reaction time to unexpected events, causing some deadlines for successfully completing these tasks to be missed.

Note that the process of creating agents and their associated goals, deadlines and events is in effect an ontology driven approach to eliciting the requirements of the problem at hand. It would be worthwhile to explore for future work how best to establish these specifications, working with system designers. Research such as [ 4 , 11 , 10 ] may be useful, in this regard. Jurisica et al. [ 4 ] provides some insight into the role for social ontologies (relevant to mul-tiagent systems) and the ability to capture dynamically changing elements (such as goals) within dynamic ontologies. Richard [ 11 ] discusses the challenges in properly engineering ontologies with users, providing some insight into how to discover the basis for these ontolo-gies from examples in the domain. Finally, Pinto and Martins [ 10 ] points out the dimensions under which ontology building approaches may be categorized, distinguishing those built from scratch from those designed by re-using existing elements. The many references pro-vided in this paper would be a useful starting point for determining the best method to employ as we explore this topic, in the future. 6.3 Related work Real-time planning As our system does not provide a real-time planning capability, we refer to two possible real-time planning methods that could be used within our implementation. The first is TAEMS [ 14 ], a model for representing the interdependencies between agents that are planning in a real-time system. Using plan templates that provide multiple ways of achieving a goal, the system selects those templates from the plan library that accomplish that goal and passes them to the planner. The planner has a fixed time interval in which to complete its job; if the interval elapses before a plan is finalized, the current-best plan will be used instead.

The second possible real-time planning framework is CIRCA [ 1 , 8 ], a combination of two loosely coupled subsystems: a real-time control subsystem and an artificial intelligence subsystem. The real-time control subsystem provides monitoring and execution of individ-ual real-time tasks, while the artificial intelligence subsystem is responsible for determining which tasks should be executed. CIRCA relies on an extensive plan library and a reactive planning system, in which each plan is composed of a set of state requirements and a set of actions to be performed when those requirements are met.

The real-time scheduler can then perform these tests at the appropriate times, and if the state requirements of the plan match the observed state, the scheduler executes the action associated with the test. If the test fails, the schedule can execute a remedy action, which is designed to correct the inconsistencies between the expected and actual states. Task allocation Part of our framework requires a mechanism for assigning tasks to agents. Ortiz and Rauenbush [ 9 ] describe a system capable of allocating tasks amongst distributed agents by reasoning about temporal and spatial constraints. As well, their solution can han-dle situations where the set of tasks and the set of agents can change unexpectedly. Their system uses an anytime auction-based algorithm called the Mediation Algorithm. Because the algorithm is anytime, it is suitable for use in time-sensitive environments such as a RTMAS.
Whenever a new task needs to be assigned, a mediator agent is chosen. This mediator then announces a proposed assignment for the new task to a set of bidders (agents who could accomplish the task). The candidates then produce bids that contain information about their current task (if any), their bid on the new task, and a potential bid on the task if the agent were not otherwise occupied. These enriched bids allow the mediator to modify the proposal and adjust the list of candidates for a task. The mediator then issues the revised proposal to the current set of candidates. The process repeats until the algorithm X  X  time limit is exceeded or until every task has been satisfactorily assigned.
 Coordinator-based architectures Similar to the approach of Scerri et al. [ 12 ], we aim to reduce the overall communication in the multi-agent system by removing the requirement that agents must communicate with all others, in order to address new, unexpected events. Their approach is designed to dynamically create sub-teams of agents. In our model, it is the coordinator that helps to identify which agents should essentially be brought together into a sub-team, in order to collectively work on a problem. In this way, when new information becomes available locally to an agent, it has a clear directive to forward that information to the coordinator. This is in contrast to the approach of Scerri et al., where an agent within one sub-team needs to reason about which other sub-teams may need to make use of its information.

Our work also contrasts with that of Wagner et al. [ 15 ] which advocates the use of coor-dinating agents for disaster response scenarios. The framework outlined in our research provides a testbed for exploring and fine tuning architectures such as that of Wagner et al., by providing a simplified RoboCup Search and Rescue environment within the model. We have a particular focus on addressing the soft real-time constraints that may be inherent in these kinds of disaster scenarios.
 Robotics Our proposed hybrid architecture includes both agents controlled by a coordinator and fully autonomous agents. As mentioned earlier, it is possible to develop other kinds of architectures for designing multi-agent systems. We note that our architecture may in fact be especially helpful in applications such as RoboCup Search and Rescue. Our approach imposes more coordination among the individual agents, in contrast to that of Morimoto et al. [ 7 ]. Their initial entry into the RoboCup Rescue contest, YabAI, relied on an algorithm similar to the emergent behaviour strategy employed by Balch and Arkin above. While YabAI was the winner in the first contest, its performance was still far below the level that would be required in a real disaster rescue scenario. An explicit coordination mechanism, such as in our model, can provide this needed enhancement.

Our proposed architecture also fits well with certain efforts in robotic tele-operation with human users. Wegner and Anderson [ 16 ] are interested in using robotic agents to help humans find victims after a disaster. Robots are particularly suited to this domain due to their ability to enter places that too small or too dangerous for humans to go. To be able to increase the number of robotic searchers in such an environment, rescue coordinators must be able to allocate enough trained humans to control the additional robots, since each user can only directly control a single robot at a time. Wegner and Anderson propose a hybrid system in which those robots that are not being controlled remotely can operate autonomously instead. Both this architecture and our own allow many agents to operate autonomously, handing any minor unexpected events as they occur, and falling back on a more intelligent central operator in case of more difficult situations.
Other approaches to organizing robotic agents are still possible. Balch and Arkin [ 2 ] examine three different modes of communication in a system of autonomous agents. Agents observe each other for state markers (such as lights or sounds) that indicate the agent is doing something interesting. Their system produces cooperation between agents when, for exam-ple, one agent observes that another is performing a task that the agent wants to perform as well. Thus, their system does not use a coordinating agent; it relies instead on worker agents observing a problem and proactively working to solve it. In contrast, our system uses a central coordinator to help manage urgent problems that individual agents cannot solve, making use of its global perspective to organize the team of agents in an environment with many different, possibly conflicting, tasks.

Comparing the relative advantages of these two kinds of architectures would be possible by coding both approaches in our simulated RoboCup-Rescue-like implementation. For future work, we will explore how best to perform this comparative study. For example, we will need to devise many more scenarios, to provide a broader and more complete representation of the kinds of situations rescue agents might encounter in a real rescue situation. Particularly, scenarios where more than a few agents are operating would need to be extensively tested. For the comparison to be truly useful, we would also have to replace certain parts of our model, such as the planner, with more robust algorithms such as those mentioned in Sect. 6.3 .As well, the inclusion of more agents and more advanced planners could require rewriting of the simulator into a more scalable language, such as Java or C#. 7Conclusion In this paper, we have discussed a real-time simulator implementation based on the RoboCup Rescue simulation system. This implementation is an effective testbed for archi-tectures designed for multi-agent systems that must operate in soft real-time environments with unexpected events. The challenge in developing these systems is to include provision for agents to adjust their autonomy, when faced with new tasks. We provide a classification for possible unexpected events and a framework for simulating an environment where real-time requirements must be addressed.

We have described how we used this implementation to validate our own model for real-time multi-agent systems, a hybrid approach where a coordinating agent serves to adjust the autonomy of agents, under certain conditions. We have shown experiments to illustrate the use of our implementation for demonstrating the effectiveness of this architecture. We have also discussed how to adjust the implementation to test other kinds of architectures, clearly indicating the parameters that must be set with each experiment. As such, we provide an important framework for testing architectures that adjust the autonomy of agents in real-time multi-agent systems.
 Appendix A: Scenario configuration Table 2 summarizes some of the terms used when discussing scenarios.
Table 3 describes the available configuration parameters and gives an example value for each.
 Appendix B: Goals and events B.1. Goals Based on an analysis of the capabilities of the workers and the coordinator in the implemented system, we produced a list of six goals that can arise during the course of the simulation. These goals are displayed in Table 4 . Within the simulation, these goals are referred to by number, as G2 , to indicate goal number two. Likewise, events (Barriers, Opportunities, and PCFs) are specified by a letter indicating their type followed by a number indicating the specific event. For example, P3 indicates PCF number three, and O1 indicates Opportunity number one. The specific Opportunities, Barriers, and PCFs that can arise during the simulation are described in the next section.
 B.2. Events Tables 5 , 6 and 7 describe the possible events that can occur during the course of the simula-tion, and the goals or actions that will handle the event. For each event, a brief description of the event X  X  meaning is provided, as well as an indication of the conditions under which the event can occur. The possible responses, in the form of one or more goals or special actions that can deal with the event, are also given. Finally, each event is assigned a priority, which is used to break ties should multiple events occur at the same time. This priority system is used for simplicity and in lieu of a more complex mechanism to decide precedence between simultaneous events.
 There are three special actions: Ignore Event , Restart Goal ,and Give Up Goal . Ignore Event means the event is dropped (not handled) by the agent. Restart Goal means the plan for the goal to which the event pertains should be regenerated to correct for an unexpected state. Give Up Goal means that the agent will no longer spend time performing the goal to which the event pertains; for example, event B1 will cause the pertinent goal (likely a G1 or G3 ) to be discarded, since further progress on the goal is no longer possible. References Author Biographies
