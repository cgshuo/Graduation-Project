 Astronomy increasingly faces the issue of massive, unwieldly data sets. The Sloan Digital Sky Survey (SDSS) [11] has so far gener-ated tens of millions of images of distant galaxies, of which only a tiny fraction have been morphologically classified. Morphologi-cal classification in this context is achieved by fitting a parametric model of galaxy shape to a galaxy image. This is a nonlinear re-gression problem, whose challenges are threefold, 1) blurring of the image caused by atmosphere and mirror imperfections, 2) large numbers of local minima, and 3) massive data sets.

Our strategy is to use the eigenimages of the parametric model to form a new feature space, and then to map both target image and the model parameters into this feature space. In this low-dimensional space we search for the best image-to-parameter match. To search the space, we sample it by creating a database of many random parameter vectors (prototypes) and mapping them into the feature space. The search problem then becomes one of finding the best prototype match, so the fitting process a nearest-neighbor search.
In addition to the savings realized by decomposing the original space into an eigenspace, we can use the fact that the model is a linear sum of functions to reduce the prototypes further: the only prototypes stored are the components of the model function. A modified form of nearest neighbor is used to search among them.
Additional complications arise in the form of missing data and heteroscedasticity, both of which are addressed with weighted lin-ear regression. Compared to existing techniques, speed-ups ach-ieved are between 2 and 3 orders of magnitude. This should enable the analysis of the entire SDSS dataset.
 Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00.
 I.5 [ Computing Methodologies Pattern Recognition ]: Implemen-tation Algorithms Astronomy,Morphology,Nearest Neighbor,Regression,Pincipal Com-ponent Analysis
In order to understand the formation of large scale structures in the universe, it is necessary to understand the varied galaxy mor-phologies. This is still an open area of research in astronomy; it is not precisely known how galaxy shapes arise. The distribution of shapes and their correlation with other measured properties of galaxies is important to generating and testing hypotheses about the nature of the universe. This requires extracting various types of information from large numbers of faint and noisy images of galaxies, e.,g., whether the galaxy is spherical, elliptical, or disk-shaped, the size of the central bulge relative to the size of the disk, etc. Example images are given in Figure 1.

Figure 2 illustrates a common classification system still in use to-day: the Hubble  X  X uning fork X  [3], which roughly divides all galax-ies into ellipticals (bulge-dominated) and spirals (disk-dominated). The model parameter that corresponds to this is the bulge-to-disk ratio, which describes the relative amounts of light emanating from the two components. Hence a coarse galaxy classification of can be made directly from a single structural parameter, which can be obtained by fitting a mathematical model to a galaxy image. In this case, the model is an additive combination of a disk image, a bulge image, and a background (sky) image. The form of this model will be given in Section 2.1.
 There are significant obstacles, however, to fitting these models. The most challenging is the Point Spread Function (PSF). Images of galaxies from ground-based telescopes are smeared by a turbu-lent atmosphere, and distorted by lens imperfections (telescope, fil-ters, mirrors, lenses, etc.) Figure 4 illustrates the effect of the PSF on a disk image. To understand the mechanism of the PSF, imagine a single ray of light coming from the direction of the galaxy and aimed at the center pixel/detector of the telescope. Without dis-tortion, the resulting image would be a single point. However, as the ray travels through the atmosphere and the telescope X  X  optics, it spreads out and is distorted before it hits the center pixel/detector. The resulting image is a PSF. 1 The PSF can be viewed as the prob-ability mass function for the arrival of a single photon at a given pixel, given that the photon was initially aimed at the center pixel. The problem is that every incoming photon is subjected to the in-fluences summarized in the PSF, so the resulting image is smeared. Section 2.1.2 describes the action of the PSF.

Large numbers of local minima are another problem. The noise of the images and a sometimes too-flexible model make finding the correct fit difficult. For example, some disks can be well approxi-mated by a combination of bulge and sky. In the presence of noise, the two possibilities can be impossible to distinguish. In order to enumerate these local minima, some form of global search is gener-ally necessary, and this is quite time-consuming. The most trusted of the current 2-d morphology techniques is a simulated annealing algorithm [9], which is robust to local minima, but is slower due to its caution. The algorithm described in this paper is able to sam-ple the parameter space with on order of 50,000 samples, and is therefore probably more robust to local minima.

Currently standard nonlinear regression techniques are used to fit these images, such as simulated annealing [9] and Levenberg-Marquardt [7]. These approaches are all effective, but time con-suming, e.g., roughly 1-3 minutes per 64  X  64 image on a 1.4 GHz pentium desktop. Assuming this, 100 million galaxies would re-
The Point Spread Function is more generally a smooth two-dimensional function. We discretize it into a finite-resolution im-age.
 Figure 2: Hubble tuning fork diagram. The fundamental divi-sion of galaxies is into ellipticals (E) and spirals (S). The num-ber after the ellipticals is the ratio of their major axis to their minor axis, called the ellipticity. The total light from the cen-tral bulge relative to that from the disk (the bulge-to-disk ratio) diminishes from left to right. quire about 200 years of CPU time. For higher resolution images, performance rapidly degrades. The code introduced in this paper performs the same fits in less than a second, and is robust to changes in resolution of the target image.

Other machine learning approaches to similar problems have in-cluded the use of EM in classifying certain  X  X ent-double X  galax-ies [5], the application of the Information Bottleneck method to classification of galaxy spectra [10], and the use of artificial neu-ral networks in classifying galaxies along a single  X  X alaxy-type X  dimension [1].
The general task is to fit a parametric model to data. In this case the data is an image of a galaxy. In the galactic morphology (GM) task, the model is a function whose 12 free parameters are the morphological characteristics of the galaxy, e.g., shape, size, and location (see Appendix for complete list.) This task is described in detail in [9]. We assume all images are square with N pixels per side, giving a total of N 2 pixels. Occasionally images must be represented as vectors, so they are denoted with an overhead arrow (  X  ). Images can be turned into vectors by vertically concatenating the columns.
The most basic assumption that we make is that the target image can be modeled. Here we use a model consisting of a set of four main elements: {  X  ,  X  ,  X  ,  X  } , which describes the expected number of photons to arrive at a particular pixel ij in a matrix of pixels. The ultimate objective is to invert this model for each target image.
The elements of the model described in this section are illus-trated in Figure 3. Each pixel has an independent Gaussian distri-bution, so the target image y is assumed to have the distribution where the distribution for each pixel value is Gaussian with a mean of  X  ij (  X  ,  X  ) and a variance of  X  ij .
The model f is a function on a 2-dimensional plane which indi-cates the density of flux at a given point on the plane. The general shape of the function is a sharp peak at the center of the galaxy, ta-pering off with distance. The disk tapers off exponentially with re-spect to distance from the center of the galaxy, and the bulge tapers off exponentially w.r.t. the cube root of the distance. The appendix contains the details of the function and its motivations. Importantly, the model is not smooth and has no derivatives at ( 0 , 0 ) .Thereis a single sharp spike at this location, which creates difficulties later on when trying to deconvolve  X  and  X  .
In the GM task, the relationship between the fixed parameter  X  (the PSF) and the model  X  is one of convolution. Where  X  is an image with a single delta function, which indicates no blurring; convolution with  X  results in a perfect replication of the original image. The effect of convolution is illustrated in Figure 4.
Since  X  is an image of the PSF, convolving an image with a par-ticular  X  is equivalent to blurring with a particular atmospheric con-dition and/or mirror imperfection. Fortunately, convolution is a lin-ear operation, since each pixel becomes a linear combination of all other pixels. Hence, Equation 3 can be rewritten as Due to the physical interpretation of  X  , all the pixels of the PSF must be nonnegative. Also, conservation of energy requires that the pixels sum to one.
As an approximation to the true noise distribution, which is Pois-son. brightness
Although  X  is the expected number of photons to arrive during an exposure, the realized number will be a discrete counting pro-cess. So the value will follow a Poisson distribution with a mean and variance of  X  (  X  ,  X  ) . Because of this, error variance at a pixel is proportional to the amount of signal at the pixel. This heterosce-dasticity must be accounted for in the regression.

We consider the Poisson to be well approximated in this case with a Gaussian distribution with a mean and variance of  X  (  X  ,  X  ) , since the number of photons is usually greater than 30 in the more influential central pixels.

The noise model can also be used to  X  X ask X  bad pixels in the input image. If the galaxy of interest is close to another galaxy, or artifacts are present in the image, then a mask is used to specify which pixels to ignore. Bad pixels can be masked out entirely by setting their  X  values to infinity.
The objective is to invert  X  . Specifically, to take a given target image y , PSF  X  , and error model  X  , and to find a parameter vector  X   X   X  that, when fed to the generative model of Equation 1, pro-duces an image  X  y close to y . By  X  X lose X  we mean to minimize the distance between the two images. This distance function will be denoted by  X  2 . This distance function is the least squares criterion with hetero-scedastic noise. Assuming that the noise model  X  is correct, its minimum will occur at the most likely fit.
The algorithm must be able to quickly invert the galaxy image model and to deconvolve images that have been blurred by a PSF. The most successful algorithm type we have found has been a vari-ant of the nearest neighbor algorithm. This approach creates a map-ping from image space R N  X  N to parameter space  X  by remem-bering and generalizing from many previous  X  -to-R N  X  N (via prototypes).

We are thus using a nonparametric technique to perform a para-metric regression. Two reasons primarily motivate this choice: 1) the model is expensive to evaluate because the PSF convolution re-quires O ( N log N ) operations each time a model image is generated. Iterative techniques, e.g., Levenberg-Marquardt, need to evaluate the model at every step, so search becomes costly, and 2) the num-ber of local minima is large, so most descent-based methods are inappropriate due to their vulnerability to local minima.
For purposes of exposition, we will start with a naive, infea-sible approach. The prototypes that will be used to map from images to parameters are the members of the set of prototypes X = { ( x i ,  X  i ,  X  i ) } p i = 1 ,where x i =  X  (  X  i ,  X  of prototypes. We generate this set by sampling uniformly from  X  and  X  .

The regression task in this context is to find  X   X  by finding the smallest distance between the target image and each of the proto-types, where  X   X  is the parameter vector corresponding to the prototype x . This is the core strategy of this regression algorithm. There are, however, clear barriers to overcome. First, the dimensionality of the prototype space is very high; one dimension per pixel for a 32  X  32 image gives 1024 dimensions. Comparing prototypes to in-coming queries will thus be expensive. Second, the presence of the uncontrollable fixed parameters  X  means that a large number of pro-totypes will be required to adequately sample the parameter space deviation.
  X   X   X  . However, nearest neighbor search can exploit three strate-gies, which account for most of the speed gains made, 1) Principal Components Analysis, 2) PSF-local Principal Com ponent Analy-sis, and 3) prototype decomposition.
One problem with working in image space R N  X  N is the com-putational load of so many dimensions, i.e., one per pixel. With-out noise, however, the model {  X  ,  X  ,  X  } creates images that can at most occupy a manifold whose dimension is equal to the number of model parameters. Ideally, one should focus one X  X  efforts in the most relevant subspace and ignore the rest, especially since near-est neighbor algorithms are sensitive to excessive dimensionality. Principal Components Analysis (PCA) achieves this by determin-ing the linear subspace in which the most variance resides.
Each prototype is a point in R N  X  N . To determine the best sub-space of R N  X  N , we would like to know the shape of the subspace that these prototypes occupy, which is the manifold {  X  (  X  ,  X  ) |  X   X   X  ,  X   X   X  } . One measure of shape is the covariance between the values of each dimension/pixel of the model manifold.
 where  X  is the mean model image over all  X   X   X  and  X   X   X  . This is the pixel covariance matrix, UU T , whose ij -th element is the co-variance between pixel i and pixel j in the model space. Once the pixel covariance is known, the first K eigenvectors of UU an  X  X ptimal X  subspace of R N  X  N . This subspace, out of all possible linear K -dimensional subspaces of R N  X  N , explains the maximum variance possible. These eigenvectors are the first K principal com-ponents.

We must first estimate UU T . This can be done efficiently by sampling the model manifold, i.e., by randomly choosing  X  sand  X  s from the parameter space  X   X   X  and generating images from them using the model {  X  ,  X  ,  X  } . These sample images are then mean-normalized 3 , lined up as column vectors as the matrix U , and multiplied to produce UU T .
 In the PCA paradigm, the first K N 2 eigenvectors of UU form the basis for a new space, which we will denote  X  .Thisbasis  X  is an N  X  K orthonormal matrix. The span of the columns of  X  will be referred to as an eigenspace and the individual columns as eigenimages. See Figure 5 for eight sample eigengalaxies. Note that  X  T is a projection matrix such that  X  y =  X  T y . Vectors projected into eigenspace will be denoted by  X  and are also vectors.
The entire nearest neighbor search should now take place within the eigenspace  X  . This requires projecting all of the prototype im-ages and all target images into  X  . Since the eigenvectors are or-thogonal, projection into the eigenspace is straightforward:
However, if the noise is heteroscedastic, the projection requires more care. Different pixels/dimensions will be weighted differently by the distance function  X  2 , so distances between images will be-have as if the image space has been warped. The stretching will be axis-aligned in image space, because each pixel X  X  noise is indepen-dent of the others X .

If noise is heteroscedastic, the optimal projection is a weighted linear regression. The diagonal matrix  X  y is the covariance of y in R
N  X  N . The matrix  X  y is just a reorganization of exactly the same information contained in  X  .
The mean of all sample images is subtracted from each image, as in Equation 7. The projection of y into  X  is and the resulting error covariance of  X  y is The matrix  X   X  y is the covariance matrix of  X  y , reflecting any uncer-tainty in the eigenspace projection of y . In contrast to  X  variance  X   X  y usually has off-diagonal terms because the noise is not axis-aligned with respect to the basis  X  . Thus  X  y will most likely have correlated errors due to the projection.
Now we can attack the regression problem while inside the eigen-space, where we enjoy a much reduced dimensionality. The al-gorithm uses only the eigencoordinates of the prototypes, denoted X . The algorithm becomes slightly more complicated since the  X  distance function must now account for any correlated errors in  X  y introduced by projection of y onto  X  . The new algorithm: where  X  x and  X  y are the eigencoordinates of x and y respectively. The fitted parameter vector  X   X  is the  X  corresponding to  X  x Up to this point we have included all possible PSFs in our PCA. The model manifold {  X  (  X  ,  X  ) |  X   X   X  ,  X   X   X  } has relatively few di-mensions, 4 but it is highly nonlinear with respect to  X  . This means that the number of eigenspace dimensions required to represent it adequately will be larger. There are at least two approaches to this problem: attempting to remove the effect of  X  and PSF-local PCA.
The most direct approach is to modify y to remove the effect of the PSF  X  . This can be accomplished via deconvolution. Un-fortunately, deconvolution has difficulty with regions of the image with sudden changes in intensity, which is the part of the image with the most information relevent to our model. The central spike (which is the galaxy center) is a discontinuity that is very difficult
We assume that the class of possible PSFs is constrained to lie in some manifold with dimension much less than R N  X  N . E.g., images of trees, human faces, white noise, etc. are not in  X  . for deconvolution to reconstruct. Also, deconvolution suffers from instability in the presence of noise.

The next approach is PSF-local PCA. This maintains a differ-ent  X  for every PSF. Each eigenspace is obtained by fixing  X  and repeating the steps of PCA in Section 4. Each eigenspace is opti-mal for its  X  . The number of dimensions required is therefore quite small, 20 dimensions captures over 99 . 999% of the variance.
It is feasible to reuse a  X  generated from a single PSF  X  be-cause most PSFs in a given run of galaxy images are similar, and because small differences between PSFs generally pr oduce small differences in resulting images.

The algorithm stores a relatively small number n s of PSF-specific few Gaussians of varying standard deviation, to which PSFs are added during on-line operation. The decision to add a PSF to the database is made, somewhat arbitrarily, when an incoming PSF dif-fers by more than 0.02 in variance explained from its closest match in the database. Variance explained here is 1  X   X  N i (  X  Where  X  is a PSF from the existing database.
The fact that the model is of the form  X  (  X  ,  X  )=  X  can be used to good advantage; only prototypes for the compo-nent  X  k (  X  k ,  X  ) images need to be created. For example, in the GM task the component functions are the disk, bulge, and background. Instead of generating and storing large numbers of individual com-binations of disks and bulges to form the set X , we can store three much smaller sets of prototypes: a disk set X d ,abulgeset X askyset X sky . Far fewer prototypes will be needed to represent the same number of images. The size of the representable number of prototypes is now | X d | X | X b | X | X sky | , but only | X images need be created.

The feature vectors of the prototypes are decomposable as well because The same is true in the heteroscedastic case; since projection is still achieved with a matrix operation. The matrix in question is the product of the matrices which are multiplied by y in the right hand side of Equation 10.

Nearest neighbor algorithms require some notion of distance, and distances are typically defined between two points. However, since we are combining components of prototypes we must de-fine a distance metric between a particular set of components {  X  x  X  x be finding the distance between a galaxy image and, for instance, bulge #55 with disk #1244. The c components will define a (hy-per)plane of possible images which consists of all non-negative lin-ear combinations of the c components.

Given our definition of  X  2 , the error-minimizing metric is the distance to the nearest point on that plane. This distance  X  culated via weighted linear regression: Importantly, this linear regression also determines the optimal lin-ear combination of the particular components {  X  x 0 used in Equation 14 for the particular target  X  y . The optimal coeffi-cients are the elements of the vector  X  . Having thus determined c of the parameters of the model via the relatively cheap operation of a linear regression, the remaining dimension of the search space is reduced by c . In the case of the GM problem, the three coefficients of  X  are the total fluxes of disk, bulge, and background. Figure 6 illustrates the procedure for just a bulge and a disk component.
At this point, we in principle have only to calculate  X  2 combinations of disks and bulges, and select the combination with the smallest  X  2 . Unfortunately, speed would then be unacceptably compromised, so instead we search selectively.
After the eigenspace has been selected and the target image has been projected into the space, then the search for a nearest neighbor begins. The search could be accomplished by an exhaustive search of all bulge-disk combinations. However, we save time with the following two-part search algorithm which has global and a local search components: 1. Global: Random Pair Sampling starts by extensively ran-2. Local: Iterative search starts with the best candidate from The process is guaranteed to converge because the search space is finite, and the sequence of pairs must always have a decreasing  X  . Phase 2 is run on the top 10 or 20 candidates from phase 1. We have found the local search as described to generally converge to a better minimum than simple local (e.g., hillclimbing) search. We conjecture that this is due to the large number of local minima inherent in the problem.
Table 1 summarizes the strongest difference between this algo-rithm and its predecessors, which is speed. Implemented in MAT-LAB [4], GMORPH can analyze a 64  X  64 image in approximately 1 second. The nearest competitor can do the same image in about 30 seconds, but it is a descent method and vulnerable to local min-ima. The times were obtained by generating random galaxy pa-rameters from the range F d  X  [ 0 , 1 ] , F b  X  [ 0 , 1 ] ,  X  ( 0 , 16 ] ,  X  inc  X  [ 0  X  , 85  X  ] ,  X  d  X  [ 0  X  , 180  X   X   X  [ 0  X  , 180  X  ] , and were used to generate 64  X  64 images of galax-ies. The PSFs were Gaussian with a standard deviation of 2 pixels.
Figure 7 contains the results of a comparison between GMORPH and the traditional and currently most-trusted measure of galaxy shape: human classification. We tested the agreement between GMORPH and an already-classified dataset with 300 galaxies. Each image had been classified visually by a panel of four human experts onto a scale which varies from 0 (all bulge) to 5 (all disk), with 6 being  X  X rregular X . The results show a clear correlation between GMORPH and expert classification.

Figure 8 plots the agreement between GMORPH and GIM2d [9] on the disk radius for low-noise, predominantly disk galaxy im-ages from the Sloan Digital Sky Survey. Both algorithms were run on 100 images, each with a unique PSF. The catalog of proto-types used by GMORPH had | X disk | = 1000, | X bulge | = 1000, and | X sky | = 1. The size of the images varied, but were approximately 50  X  50. The agreement between the two methods is apparent here, however, in high-noise images the two methods produce different results. Although we are still investigating the source of these oc-casional discrepencies, there is preliminary evidence that these are cases in which either the galaxy morphology diverges from the as-sumed bulge/disk model, or noise is too severe to fit the data with confidence.
We report on an ongoing investigation of a nonlinear regression problem from astronomy: given a massive dataset of noisy, dis-torted images of unknown galaxies, rapidly fit a nonlinear model to each image in the dataset. A instance-based method for accom-plishing this task has been described. Fitted B/T parameter Figure 7: Comparison to human expert classification of 300 galaxies. The horizontal axis is the galaxy classification, which varies from 0 (all bulge) to 5 (all disk), with 6 being  X  X rregu-lar X . The vertical axis is the bulge-to-total flux ratio returned by GMORPH. Each box indicates the 25th, 50th, and 75th quar-tiles. Figure 8: Noise and PSF effect on recovered disk radius er-ror. The agreement between disk radius parameters fitted by GIM2d and those fitted by GMORPH on images from the Sloan Digital Sky Survey.

Instance-based methods allow for very fast identification of these galaxies through sampling of the parameter space, the use of an eigenspace, and through splitting the prototypes into components. GMORPH can avoid the expense of calculating the PSF during the search process, and can scan through the space of galaxy images rapidly because it restricts search to the much smaller subspace de-termined by PCA. We have measured the performance via simula-tion and it should in principle allow for unprecedented analysis of astronomical datasets of galaxy images. [1] N. Ball, M. Fukugita, O. Nakamura, S. Okamura, [2] K. C. Freeman. On the Disks of Spiral and s0 Galaxies. [3] E. Hubble. The realm of the nebulae . Yale University Press, [4] Mathworks Inc. Matlab, version 6, release 13, 2003. [5] S. Kirshner, I. Cadez, P. Smyth, and C. Kamath. Learning to [6] J. Kormendy. Brightness distributions in compact and normal [7] C. Y. Peng, L. C. Ho, C. D. Impey, and H. Rix. Detailed [8] K. Ratnatunga, R. Griffiths, and E. Ostrander. Disk And [9] Luc Simard, Christopher N. A. Willmer, Nicole P. Vogt, [10] N. Slonim, R. Somerville, N. Tishby, and O. Lahav. [11] D. G. York, J. Adelman, J.E. Anderson, and et al. The Sloan
Before being blurred by the PSF, the galaxy is created by the surface brightness function,  X  , which takes as an argument a vector from  X  . Here are the 12 model parameters of  X  , a brief description, and their units: F , F d total integrated flux of bulge and disk components ( erg  X   X  ,  X  y the x and y offset of the galactic center from the center of r , r d bulge and disk scale lengths ( pixels )  X  b apparent bulge ellipticity ( unitless )  X  inc disk inclination ( degrees ). Rotation toward viewer  X   X  d bulge and disk angle of rotation ( degrees ). Clockwise rota-sky sky background offset ( flux / cm 2 ) Sersic a bulge shape parameter that is fixed to the value 4 for all The classic model of galaxies has been additive: a linear combina-tion of a bulge image, a disk image, and a sky (background) image [9, 7, 8]. The sky image is a constant, and will omitted from the formulae for clarity.

Before discretization into pixels, g is a continuous brightness function defined over the 2-dimensional image plane uv . 5 pected number of photons to hit a pixel/detector is found by inte-grating g over the uv area of the pixel on the plane.

The function g is the sum of c component functions: In the galactic morphology task, g has three components: a disk, a bulge, and constant background. We will assume henceforth that the PSF is a delta function, so we omit  X  from the discussion. We refer to the entire unblurred disk function as g disk (  X  ) , and the un-blurred bulge image as g bulge (  X  ) .

The surface brightness, g disk , of a pure disk galaxy w.r.t. radius has been found to have an exponential form [2, 6]. A commonly used model consists of an infinitely thin disk with brightness in the plane of the disk tapering off exponentially away from the center. When projected onto the image plane, the brightness g disk form where F d is the integrated brightness of the disk,  X  inc of inclination of the disk towards the viewer, and r  X  X adius X , or scale parameter. Both Equation 18 and Equation 19 are simplified for presentation in that they omit clockwise rotation and fix the center of the galaxy at ( 0 , 0 ) .
 The bulge is modeled with a classical de Vaucouleurs profile. Also known as the r 1 / 4 law, de Vaucouleurs X  law is perhaps the most widely used empirical law to describe the surface brightness profile of a pure bulge galaxy. The bulge brightness is where F d is the integrated brightness of the bulge,  X  b describes the ellipticity of the bulge, and r b is the bulge  X  X adius X .
The actual image recorded by the telescope is digitized into pix-els. Pixels are elements of the matrix  X  .
The variables u and v are used only because x and y appear else-where in this paper.
