 One of the primary forms of adding background knowledge for clustering the data is to provide constraints during the clustering process [1]. Recently, data clustering using constraints has received a lot of attention. Several works in the literature have demon-strated improved results by incorporating external knowledge into clustering in differ-ent applications such as document clustering, text classification. The addition of some background knowledge can sometimes significantly improve the quality of the final results obtained. The final clusters that do not obey the initial constraints are often in-adequate for the end-user. Hence, adding cons traints and respecting these constraints during the clustering process plays a vital role in obtaining desired results in many practical domains. Several methods are pr oposed in the literature for adding instance-level and cluster-level constraints. Constrained versions of partitional [19,1,7], hierar-chical [5,13] and more recently , density-based [17,15] clu stering algorithms have been studied thoroughly. However, there has been little work in utilizing the constraints in the graph-based clustering methods [14]. 1.1 Our Contributions We propose an algorithm to systematically add instance-level constraints to the graph-based clustering algorithm. In this work, we primarily focused our attention to one such popular algorithm, CHAMELEON, an overview of which is provided in section 3.2. Our contributions can be outlined as follows:  X  Investigate the appropriate way of embedding constraints into the graph-based clus- X  Propose a novel distance limit criteria for must-links and cannot-links while em- X  Study the effects of adding different types of constraints to graph-based clustering. The remainder of the paper is organized as follows: we briefly review the current ap-proaches for using constraints in different methods in Section 2. In Section 3, we will describe the various notations used throughout our paper and also give an overview of a graph-based clustering method, namely, CHAMELEON. Next, we propose our algo-rithm and discuss our approach regarding how and where to embed constraints in Sec-tion 4. We present several empirical results on different UCI datasets and comparisons to the state-of-the-art methods in Section 5. F inally, Section 6 concludes our discussion. Constraint-based clustering has received a lot of attention in the data mining community in the recent years [3]. In particular, instan ce-based constraints have been successfully used to guide the mining process. Instance-based constraints enforce constraints on data points as opposed to and  X  constraints which work on the complete clusters. The -constraint says that for cluster X having more than two points, for each point x  X  X , there must be another point y  X  X such that the distance betweeen x and y is at most .The  X  -constraint requires distance between any two points in different clusters to be at least  X  . This methodology has also been termed as semi-supervised clustering [9] when the cluster memberships are available for some data. As pointed out in the literature [19,5], even adding a small number of constraints can help in improving the quality of results.

Embedding instance-level constraints into the clustering method can be done in sev-eral ways. A popular method of incorporating constraints is to compute a new distance metric and perform clustering. Other methods directly embed constraints into optimiza-tion criteria of the clustering algorithm [19,1,5,17]. Hybrid methods of combining these two basic approaches are also studied in the literature [2,10]. Adding instance-level con-straints to the density-based clustering methods had recently received some attention as well as [17,15]. Inspite of the popularity of graph-based clustering methods, not much attention is given to the problem of adding constraints to these methods. Let us consider a dataset D , whose cardinality is represented as D . The total number of classes in the dataset are K . Proximity graph is constructed from this dataset by computing the pair-wise Euclidean distance between the instances. A user-defined pa-rameter k is used to define the number of neigbors considered for each data point. The hyper-graph partitioning algorithm generates in termediate subgraphs (or sub-clusters) to be formed which are represented by  X  .
 3.1 Definitions Given a dataset D with each point denoted as ( x, y ) where x represents the point and y represent the corresponding label, we define constraints as follows: Definition 1: Must-Link Constraints(ML) : Two instances ( x 1 ,y 1 ) and ( x 2 ,y 2 ) are said to be must-link constraints, if and only if, y 1 = y 2 where y 1 ,y 2  X  K .
 Definition 2: Cannot-Link Constraints(CL) : Two instances ( x 1 ,y 1 ) and ( x 2 ,y 2 ) are said to be cannot-link constraints, if and only if, y 1 = y 2 ,where y 1 ,y 2  X  K . Definition 3: Transitivity of ML-constraints :Let X , Y be two components formed using ML-constraints. Then, a new ML-constraint ( x 1 must  X  link  X  x 2 ) where x 1  X  X and x 2  X  Y introduces the following new constraints: ( x i must  X  link  X  x j )  X  x i ,x j where x  X  X and x Definition 4: Entailment of CL-constraints :Let X , Y be two components formed using ML-constraints. Then, a new CL-constraint ( x 1 cannot  X  link  X  x 2 ) ,where x 1  X  X and x where x i  X  X and x j  X  Y , i = j , X = Y . 3.2 Graph-Based Hierarchical Clustering We chose to demonstrate the performance of adding constraints to the popularly stud-ied and practically successful CHAMELEON clu stering algorithm. Karypis et al. [11] proposed CHAMELEON algorithm which can find arbitrarily shaped, varying density clusters. It operates on sparse graphs containing similarity or dissimilarity between data points. Compared to various graph-based clustering methods [18] such as Minimum Spanning Tree clustering, OPOSSUM, ROCK, SLINK, CHAMELEON is superior be-cause it incorporates the best features of graph-based clustering (like similarity mea-sure on vertices as in ROCK) and the hierarchical clustering part which is similar or better than SLINK. These features make CHAMELEON attractive to add constraints to obtain better results. Moreover, CHAMELEON outperforms other algorithms like CURE [11] and also density-based methods like DBSCAN [18]. Thus, we believe adding constraints to CHAMELEON will not only give better results but also provide some in-sights on the performance of other similar algorithms in the presence of constraints.
Unlike other algorithms which use a given st atic modeling parameters to find clus-ters, CHAMELEON find clusters by dynamic modeling. CHAMELEON uses both closeness and interconnectivity while identifying the most similar pair of clusters to be merged. CHAMELEON works in two phases. In the first phase, it finds the k -nearest neighbors based on the similarity between the data points. Then, using an efficient multi-level graph partitioning algorithm (such as METIS [12]), sub-clusters are cre-ated in such a way that similar data points a re merged together. In the second phase, these sub-clusters are combined by using a novel agglomerative hierarchical algorithm. Clusters are merged using RI and RC metrics defined below. Let X, Y be two clusters. Mathematically, Relative Interc onnectivity (RI) is defined as follows: where EC ( X, Y ) is the sum of edges that connects clusters X and Y in the k -nearest neighbor graph. EC ( X ) is the minimum sum of the cut-edges if we bisect cluster X ; and EC ( Y ) is the minimum sum of the cut-edges if we bisect cluster Y .Let l x and l y represents size of the clusters X and Y respectively. Mathematically, Relative Closeness (RC) is defined as follows: where S EC ( X, Y ) is the average weight of edges connecting clusters X and Y in k -nearest neighbor graph. S EC ( X ) , S EC ( Y ) represents the average weight of edges if the clusters X and Y are bisected correspondingly.

There are many schemes to account for both of the measures. The function used to combine them is ( RI  X  RC  X  ). Here another parameter  X  is included so as to give preference to one of the two measures. Thus, we maximize the function: CHAMELEON, like other graph-based algorithms, is sensitive to the parameters as a slight change in similarity values can both dramatically increase or decrease the quality of the final outcome. For CHAMELEON, changes in similarity measures might result in different k-nearest neighbors. Overlapping clusters or clusters with very minimal inter-cluster distance sometimes leads to different class members in the same cluster. In this work, the primary emphasis is to demonstrate that adding constraints to graph-based clustering can potentially avoid this problem at least sometimes, if not always. Our basis for this assumption, is the transitivity of ML constraints and the entailing property of CL constraints (Section 3.1). 4.1 Embedding Constraints Using distance (or dissimilarity) metric to enforce constraints [13] was claimed to be effective in practice, despite having some drawbacks. The main problem is caused due to setting the distance to zero between all the must-linked pair of constraints. i.e., Let ( p i ,p j ) be two instances in a must-link constraint then, To compensate for distorted metric, we run all-pairs-shortest-path algorithm so that new distance measures is similar to the original space. If we bring any two points much closer to each other, i.e. At the first look, it may seem that this minute change will not affect the results signif-icantly. However, after running all-pairs-shortest-path algorithm, the updated distance matrix in this case, will respect the original distance measures better than setting the dis-tance to zero. Similarly for cannot-link constraints, let ( q i ,q j ) be a pair of cannot-link constraints, then the points q i and q j are taken apart as far as possible. i.e.,
Thus, by varying the values of  X  and  X  , we can push and pull away points reasonably. It seems that this might create a problem for finding optimal values of  X  and  X  .However, our preliminary experiments show that the basic limiting values for these parameters is enough in most of the cases. This addition of c onstraints (and thus the manipulation of the distance matrix) can be performed in the CHAMELEON algorithm primarily in any of the two phases. We can add these constraints before (or after) the graph partitioning step. After the graph partitioning, we can add constraints during agglomerative cluster-ing. However, we prefer to add constraints before graph partitioning primarily due to the following reasons:  X  When the data points are already in sub-clusters, enforcing constraints through dis- X  Intermediate clusters formed are on the ba sis of original distance metric. Hence, RI 4.2 The Proposed Algorithm Our approach for embedding constraints into the clustering algorithm is through learn-ing a new distance (or dissimilarity) function. This measure is also adjusted to ensure that the new distance (or dissimilarity) function respects the original distance values to a maximum extent for unlabeled data points. We used Euclidean distance for calculat-ing dissimilarity. For embedding constraints, an important and a intuitive question is: where to embed these constraints to achieve the best possible results? As outlined in the previous section, we choose to embed constraints in first phase of CHAMELEON. Now, we will present a step-by-step discussion of our algorithm.
 Using Constraints. Our algorithm begins by using constraints to modify the distance matrix. To utilize properties of the constraints (Section 3.1) and to restore metricity of the distances, we propogate constraints. The must-links are propogated is done by running the fast version of all-pairs-shortest-path algorithm. If u, v represents the source and destination respectively, then the shortest path between u, v involves only points u, v and x ,where x must belong to any pair of ML constraints. Using this modification, the algorithm runs in O ( n 2 m ) (here m is the number of unique points in ML). The complete-link clustering inherently propagates the cannot-link constraints. Thus, there is no need to propagate CL constraints during Step 1. Algorithm 1. Constrained CHAMELEON(CC) Importance of Constraint Positioning. Imposition of CL constraints just before Stage 4 rather than in Stage 1 might seem reasonable. We used CL constraints in Stage 1 due to our experimental observations stated below: 1. Hyper-graph partitioning with constraints is often better than constrained agglomer-2. Clusters induced by graph partitioning have stronger impact on the final clustering After Step 1, we create the k nearest neighbor graph ( Step 2) and partition the k -nn using a graph partitioning algorithm (METIS). The  X  number of clusters are then merged us-ing the agglomerative clustering where the aim is to maximize the product ( RI  X  RC  X  ) . Complete-link agglomerative clustering is used to propogate CL constraints embedded earlier. The cut-off point in dendrogram of the clustering is decided by parameter K (See Algorithm 1). The time complexity of unconstrained version of our algorithm is O ( n X  + nlogn +  X  2 log X  ) [11]. The time complexity of Stage 1 consists of adding constraints which is O(l) (l = ML + CL )and O ( n 2 m ) for propagation of ML con-straints. Thus, overall complexity of O ( n 2 m ) for Stage 1. Therefore, time complexity of our algorithm is finally O ( n X  + nlogn + n 2 m +  X  2 log X  ) . We will now present our experimental results obtained using the proposed method on benchmark datasets from UCI machine Learning Repository [8]. Our results on various versions of Constrained CHAMELEON(CC) were obtained with same parameter set-tings for each dataset. These parameters w ere not tuned particularly for CHAMELEON, however we did follow some specific guidelines for each dataset to obtain these param-eters. We used the same default settings for a ll the internal parameters of the METIS hyper-graph partitioning package except  X  , which is dataset dependent. We did not compare our results directly with constrained hierarchical clustering, since CC itself contains hierarchical clustering, which will be similar or better than stand-alone hi-erarchical clustering algorithms . Instead, we compared with those algorithms which embed constraints into the distance function in the same manner as our approach. Our CC with fixed values of (0,  X  )for(  X ,  X  ) is similar to [13] except that we have graph-partitioning step on nearest-neighbor graph b efore agglomerative clustering. So, we ruled out this algorithm and instead compared our results with MPCK-means [4] as this algorithm also embeds constraints in the distance function. MPCK-means incorporates learning of the distance function on labeled data and on the data affected by constraints in each iteration. Thus, it learns different distance function for each cluster.
For the performance measure, we used the R and Index Statistic [16], which measures the agreement between two sets of clusters X and Y for the same set of n objects as cluster in both X and Y ,and b is the number of pairs of objects assigned to different clusters in both X and Y . All the parameter selection is done systematically . For all the clustering results, K is set to be the true number of classes in the dataset. The value of  X  is chosen between 1-2 with the increments of 0.1. We ran some basic experiments on CHAMELEON for each dataset, to figure out the effect of  X  on the results. We choose the particular value of  X  for each dataset which can potentially produce better result. We used similar procedure for k and  X  . It is important to note that  X  is dataset dependent parameter among all the other parameters. We are assuming that at least 10 data points should be present in a single cluster. Thus K  X   X   X  D /10. We used the class labels of the data points to generate constraints. We randomly select a pair of data points and check their labels: if the labels a re same, they are denoted as must-link, else they are denoted as cannot-link. To assess the impact of the constraints on the quality of the results, we varied the number of constraints. We generated results for ML only, CL only and ML, CL constraints. The complete dataset is used to randomly select data points for constraints, thus removing any bias towards the generated constraints. We used five UCI datasets in our experiments as shown in Table 1. Average Rand Index values for 100 Must-link and Cannot-lin k constraints clearly outlines that on most occasions, MPCK-means [4] is outperform ed by both the variants of CC. CC(fixed) performed marginally better than CC(p=1). Also, we only show results for CC(p=5) and CC(p=15), since the results of CC(p=1) and CC(p=10) are similar to the other two.
For each dataset, we randomly select cons traints and run algorithm once per con-straint set. This activity is done 10 times and we report the average Rand-Index value for all the 10 runs. We used this experimentation for all the variants of CC and MPCK-means. The results are d epicted in Figs. 1-3. We state that the distance value for must-links and cannot-links can be varied instead of fixed values like 0 and  X  cor-respondingly. The CC(fixed) uses (0,  X  ) for distance measures. Ideally, the values of (  X ,  X  ) could be anything close to extreme values of 0 and  X  , yet they have to be quan-tifiable. In order to quantify them in our experiments, we defined as follows: where D max ,D min represents maximum and minimum distance values in the data ma-trix respectively. In order to study the effect of p, we varied it X  X  values: p = 1, 5, 10 and 15. Thus, we have CC(p = 1), CC(p = 5), CC(p = 10) and CC(p = 15) referring to differ-for constraints are different for each dataset due to different minimum and maximum distance values. In this manner, we respect the metricity of original distances and vary our constraint values accordingly .

We tried various parameter settings and found only few selected ones to be making some significant difference in the quality of the final results. It is also important to note that these settings were found by running the basic CHAMELEON algorithm rather than CC. This is because, finding optimal para meters for CC using various constraints will be constraints-specific and it will not truly represent the algorithmic aspect. We then run CC using a few selected settings for all the variants of CC using all constraints size and finally report the average values specific to one set of parameters only show-ing better performance on average across all CC variants. The individual settings of parameters ( k , X , X  ) for each dataset shown in results are as follows: Ion(19,10,1.2), Iris(9,3,1), Liver(10,5,2), S onar(6,3,1) and Wine(16,3,1). In summary, we selected the best results obtained by the basic version of the CHAMELEON algorithm, and have shown that these best results can be improved by adding constraints .

We observed across all the variants of C C and MPCK-means for all datasets con-sistently that the performan ce decreases as the number of constraints increase, except in some prominent cases (Figs. 1(d),2(a),2(b) and 3(d)). This observation is consistent with the results outlined in the recent literature [6]. We stated earlier that we did not attempt to satisfy constraints implicitly or explicitly. However, we observed that during Step 3 of Algorithm 1, for fewer constraints, most of the times the constraint violation is zero in the intermediate clusters, which is often reflected in the final partitions. As the number of constraints increase, the number of constraint violations also increase. How-ever, on an average, violations are roughly between 10 % -15 % for must-link constraints, 20 % -25 % for cannot-link constraints, and about 15 % -20 % for must-links and cannot-links combined. We also observed that few times, the constraint violations are reduced after Step 4, i.e., after the final agglomerative clustering. Thus, we can conclude that the effect of constraints is significant in Step 3 and we re-iterate that embedding constraints earlier is always better for CC.

Overall, different variants of our algorithm CC outperformed MPCK-means. Iris and liver datasets are examples where for all combinations of constraints, CC results are clearly much better than MPCK-means. In th e rest of the two datasets, CC performed nearly similar to MPCK-means. Only in so me cases, MPCK-means performed slightly better than CC variants as shown in Figs. 1(a), 2(a) and 2(d). Even in these particu-lar scenarios, at least one variant of CC out performed (or nearly matched) the result of MPCK-means. Surprisingly, CC(fixed) was slightly better or worse compared to the other variants of CC. A direct comparison of CC(fixed) with MPCK-means reveals that only in two cases (Figs. 2(a) and 2(d)), it outperformed CC(fixed). In the rest of the scenarios, CC(fixed) performed better. The p rimary reason for wavering performance in Ionosphere and Sonar datasets could be attributed to the large number of attributes in these datasets (Table 1). Due to curse of dimensionality, distance function looses its meaning by directly affecting nearest neigbours. Adding constraints do provide some contrast so as to group similar objects, but overall discernability is still less. It is impor-tant to note that, we did not search or tune for optimal values of (  X ,  X  ) for any particu-lar dataset. During our initial investigati on, we found that, for some change in values, the results were improved. We did some experiments on iris dataset and were able to achieve average Rand Index value of 0.99 and quite often achieved perfect clustering (Rand Index=1) during some of the runs for 190 constraints, with same settings as used in all other experiments shown. However, it will be too early to conclude that finding tuned values for (  X ,  X  ) will always increase performance, based on some initial results and will need further experimental evidence.

Based on our findings, we observed that changing values for (  X ,  X  ) did sometimes increase performance, but not consistently and can also sometimes lead to decrease in performance. We were also surprised by this phenomenon demonstrated by both the algorithms. In our case, carrying more experiments with additional constraints revealed that this decrease in performance is true upto a particular number of constraints. Af-ter that we again see rise in performance and with enough number of constraints (1% to 5% of constraints in our case with these datasets), we are able to decipher original clustering or close to it (Rand Index close to 1.0). CC(fixed) compared to other vari-ants of CC were only slightly different on an average. CC(fixed) performed reasonably well across all the datasets on nearly all se ttings with MPCK-means. Other variants of CC were also better on an average compared to MPCK-means. Thus, our algorithm performed better than MPCK-means in terms of handling the decrease in performance when the number of constraints increase . Most importantly, our algorithm performed well despite not trying to satisfy constraints implicitly or explicitly.
 In this work, we presented a novel constrained graph-based clustering method based on the CHAMELEON algorithm. We proposed a new framework for embedding con-straints into the graph-based clustering algorithm to obtain promising results. Specifi-cally, we thoroughly investigated the  X  X ow and when to add constraints X  aspect of the problem. We also proposed a novel method for the distance limit criteria while em-bedding constraints into the distance function. Our algorithm outperformed the popular MPCK method on several real-world data sets under various constraint settings.
