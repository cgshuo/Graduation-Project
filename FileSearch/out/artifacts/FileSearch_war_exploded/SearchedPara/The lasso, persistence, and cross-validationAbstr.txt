 Darren Homrighausen darrenho@stat.colostate.edu Daniel J. McDonald dajmcdon@indiana.edu Since its introduction in the statistical (Tibshirani, 1996) and signal processing (Chen et al., 1998) commu-nities, ` 1 -penalized linear regression has been a fixture as both a data analysis tool and as a subject for deep theoretical investigations. In particular, for a response vector Y  X  R n , design matrix X  X  R n  X  p , and tuning parameter t , we consider the lasso problem of finding where B t := {  X  : ||  X  || 1  X  t } and || X || 2 and || X || 1 the Euclidean and ` 1 -norms respectively. By convex-ity, for each t , there is always at least one solution in equation (1). Note that, while the solution is not nec-unimportant for our purposes and we abuse notation slightly by referring to b  X  t as  X  X he X  lasso solution. There is a large and growing literature investigating the asymptotic properties of the lasso solution. We highlight some results here, but it is not our intention of the earliest theoretical papers, Fu &amp; Knight (2000) examine the asymptotic distribution of the lasso solu-tion under the assumption that the sample covariance matrix has a nonnegative definite limit and p is fixed. Alternatively, Zou (2006); Wainwright (2009); Donoho et al. (2006); Meinshausen &amp; Yu (2009); Meinshausen &amp; B  X uhlmann (2006) and Zhao &amp; Yu (2006) have in-vestigated the model selection properties of the lasso. These results, which hold under various sparsity and  X  X rrepresentability X  conditions, show that if we assume the best predicting linear model to be sparse, the lasso will tend to asymptotically recover those predictors. sistency, alternatively known as persistence. That is, we require the prediction risk of the estimated model to converge to that of the best linear oracle predictor. Risk consistency has previously been investigated by Bunea et al. (2007), van de Geer (2008), and Green-shtein &amp; Ritov (2004). These results depend critically on the choice of tuning parameters and are typically rate a n , such as a n = ( n/ log( n )) 1 / 4 , then b  X  t this theoretical guidance says little about the proper-ties of the lasso when the tuning parameter is chosen in a data-dependent, and hence stochastic, way. There are several proposed techniques for choosing t , or equivalently, the parameter in the Lagrangian for-mulation, commonly denoted by  X  . Zou et al. (2007) and Tibshirani &amp; Taylor (2012) investigate using the  X  X egrees of freedom X  of a lasso solution which can be tween the lasso solution and the response Y . The logic of this procedure is that an unbiased estimator of the degrees of freedom provides an unbiased estimator of the risk. Hence, minimizing this estimator of the risk provides a method for choosing the tuning parameter. Another risk estimator is the adapted Bayesian infor-plug-in estimator of the second-order Taylor X  X  expan-sion of the risk.
 However, in many papers, for example (Tibshirani, 1996; Greenshtein &amp; Ritov, 2004; Hastie et al., 2009; Efron et al., 2004; Zou et al., 2007; Tibshirani, 2011; glmnet described by Friedman et al. (2010), the rec-ommended or default technique for selecting t in the lasso problem is to choose t = b t such that b t minimizes the cross-validation (which we abbreviate CV) estima-tor of the risk.
 The main contribution of this paper is to show that the use of cross-validation to choose the tuning pa-rameter in lasso remains persistent relative to the the-oretically optimal, but empirically unavailable, non-stochastic choice. We consider the high-dimensional regime where p n = n  X  , for a positive  X  that is to be discussed in Section 3.
 Some results supporting the use of CV for statistical algorithms other than lasso are known. For instance kernel regression (Gy  X orfi et al., 2002, Theorem 8.1), k -nearest neighbors (Gy  X orfi et al., 2002, Theorem 8.2), and neural networks (Plutowski et al., 1994) all behave well with tuning parameters selected via CV. However, the vast literature on the lasso is strangely silent on the theoretical behavior of the cross-validated estimator. The prevailing heuristic understanding of the perfor-mance of b  X  by CV, is encapsulated in the statement The supporting theory for non-lasso methods suggests that there should be corresponding theory for the lasso. However, other results are not so encouraging. In particular, Shao (1993) shows that cross-validation itly does model selection, and shares many connections with forward stepwise regression (Efron et al., 2004), this raises a concerning possibility that lasso might similiarly be inconsistent for prediction under cross-validation. Likewise, Leng et al. (2006) show that us-ing prediction accuracy (which is what cross-validation consistently in an orthogonal design setting. Further-more, Xu et al. (2008) show that sparsity inducing al-gorithms like lasso are not (uniformly) algorithmically lasso estimator are not uniformly close to each other. As shown in Bousquet &amp; Elisseeff (2002), algorithmic stability is a sufficient, but not necessary, condition for persistence.
 unsatisfactory position, with some theoretical results and generally accepted practices advocating the use of cross-validation while others indicate that cross-validation may not be a sound method for selecting the tuning parameter at all.
 In this paper, we show that the lasso under random design with cross-validated tuning parameter is indeed risk consistent under some conditions on the joint dis-tribution of the design that generates X and the re-sponse Y . In Section 2, we outline the mathemati-cal setup for the lasso prediction problem and discuss some empirical concerns. Section 3 contains the main result and associated conditions. Section 4 presents some useful lemmas and provides the proof of our re-sults, while Section 5 summarizes our contribution. 2.1. Preliminaries Suppose we observe pairs Z &gt; i,n = ( Y i,n ,X &gt; Y , where Z i,n i.i.d  X  F n for i = 1 , 2 ,...,n and the dis-tribution F n is in some class F to be specified later. Here, we use the notation p n to allow the number of predictor variables to change with n . For simplicity of notation, in what follows, we omit the subscript n when there is little risk of confusion.
 We consider the problem of estimating the best lin-from the same distribution and  X  is constrained to be in some set B . We use the L 2 -risk of a predictor  X  = (  X  1 ,..., X  p ) &gt; , defined as taken only over the new datum Z and not over any observables which may or may not be used to choose  X  . This will be our convention throughout: L (  X  ) denotes the expectation over a new data point conditional on the original sample.
 Using the n independent observations Z 1 ,...,Z n , we can form the response vector Y := ( Y i ) n i =1 and design we can write the squared-error objective function as ogously to equation (3), we write the K-fold cross-regularization set B t , which we abbreviate to CV-risk, as Here, V n = { v 1 ,...,v K } is a set of validation sets, is the estimator in equation (1) with the observations in the validation set v removed, and | v | indicates the cardinality of the set v . Notice in particular that the CV-risk is a function of B t , and hence t , rather than a single predictor b  X  . As the more complete notation makes clear, the CV-risk is actually a function of K minimizing choice of tuning parameter to be 2.2. Choosing the set T n In practice, an upper bound must be selected for any grid-search optimization over t . Note that more ad-vanced optimization techniques are generally not prac-often noisy. To define such an upper bound in a prac-tical way, it should be large enough to include all pos-sible estimators in a given class while still being finite. This implies we must choose T n to be a function of the data, in the same way that b t is. The specifics of T n depend on the regularizing set B t . This upper bound has a nontrivial impact on the quality of the recovery, as choosing a value too small may possibly eliminate the best solutions. Thus, treating the upper bound as a random function of the data is more realistic from a statistical practice point of view.
 ` -ball with radius t . This constraint is only binding (Osborne et al., 2000) if (  X  )  X  is a pseudoinverse, and K := { a : X a = 0 } is the null space of X . Observe that K = { 0 } if n  X  p and otherwise K has dimension p  X  n , and b  X  0 is not unique (both of these statements assume the columns of X are linearly independent). In either case, if t  X  t 0 , then b  X  t is equal to a least squares solution. Therefore, we define T n := [0 ,t max ], where by the Moore-Penrose inverse rather than any other pseudoinverse. It holds that t max  X  t 0 as 0  X  K and therefore t max is a bit conservative in the sense that eliminate the explicit dependence on the null space of X in the R implementation of LARS (Efron et al., 2004), used, with a default fraction value of 1. This coincides least squares solution. We define the oracle estimator generated by B t to be A natural criterion for studying the performance of the estimator b  X  which we define as This criterion allows for meaningful theory when the oracle linear model is not risk consistent; that is, when the term L (  X  t ) does not necessarily go to zero. This tional expectation of Y given X need not be even ap-proximately linear. It is important to clarify two as-pects of this definition of excess risk. First, E ( is random due to the term L b  X  Here, L b  X  pectation involved is only over a new test random variable Z and not with respect to the observed data used to choose either b t or b  X  B t = B t , and so the excess risk is necessarily nonneg-ative. However, as we are examining the case where the tuning parameter b t (and hence the optimization set B that P ( E ( b t,t )  X  0)  X  P ( B t  X  B  X  t is the risk minimizer over all of B t . We return to this issue in the proof of our main result in the next estimator with tuning parameter chosen by CV goes set of distributions Definition 3.1. Let tions where a universal constant exists that bounds the variance of each the ( p + 1) 2 interaction terms. Remark 1. Definition 3.1 is the same moment con-dition imposed in Greenshtein &amp; Ritov (2004) to show tings.
 We also state the following conditions.
 Condition 1. All ( F n )  X  X  are such that Condition 2. For any cross-validation procedure V n , c . Additionally, for any v 6 = v 0  X  V n , v  X  v 0 =  X  . For example, with K -fold cross-validation, we can take c n = b n/K c , which is the integer part of n/K . Before explaining these conditions in depth, we state our main result.
 Theorem 3.2. Suppose p n = n  X  for some  X  &gt; 0 . Let ( F n )  X  F be given and suppose Condition 1 and Condition 2 hold.
 Then, for any  X  &gt; 0 , This result shows that choosing the tuning parameter b t with CV and then estimating  X  by b  X  asymptotic risk as minimizing the true risk over the set B t n as long as c n n which is the case for K -fold CV.
 The inclusion of t n in Theorem 3.2 deserves comment. Here, t n is any sequence of non-random constants which determine the amount of regularization. As mentioned in Greenshtein &amp; Ritov (2004), if t n grows as fast or faster than n log n does not necessarily converge to 0 in probability and hence the lasso is not persistent. However, if t n fore, we choose the oracle risk over B t n as our compar-ison for persistence.
 We discuss Condition 1 next.
 Explaining Condition 1 Some assumptions about the design distribution can be used to derive sufficient conditions for the moment condition we impose. Sup-pose that p n = n  X  for  X  &gt; 0. Then value of a matrix A , ||| X ||| s is the operator norm corre-sponding to the ` s vector norm, and equation (7) uses the sub-multiplicative property of the operator norm. Suppose that our model is the usual nonparametric regression model Y = m ( X ) + e , where X and e are stochastically independent random variables and e has zero mean. If there exists a constant C independent of n such that ess sup x m ( x ) &lt; C with respect to the C 0 &lt;  X  , again independent of n , such that this with equation (8) and writing E X  X  + min ( X )  X  4 = O ( n  X  u ), with u  X  0, we see that Therefore, Condition 1 follows if n  X  u +2  X  +1 = of larger order than n  X / 2 , Condition 1 holds. Indeed, as shown by Rudelson &amp; Vershynin (2009), a random matrix composed of independent and identically dis-tributed sub-Gaussian random variables has, with high probability, In either case,  X  + min ( X ) is at least of order n  X / 2 To show the results of this paper, we decompose the ex-cess risk into several parts. Define t  X  := min { t max ,t Then, we write
E ( b t,t n ) = L b  X  as t  X   X  T n , ( II )  X  0. Also, by the discussion in Sec-tion 2.2, To see this, note that for any t  X  t max , b  X  t is a least squares solution. Therefore, by the definition of t b L b  X  t  X  = b L b  X  t n and hence ( IV ) = 0.
 To bound the remaining terms, we rewrite them as quadratic forms (Section 4.1) and present three lem-mas (Section 4.2). The actual proofs are contained in Section 4.3. 4.1. Squared-error loss and quadratic forms quadratic forms. Define the parameter to be  X  &gt; (  X  1 , X  &gt; ), with associated estimator We can rewrite equation (2) as has the following form tion (4) as With this notation, each part of the decomposition can be written as the difference of quadratic forms. Careful modifications will allow us to use the following lemmas to prove bounds for each part. 4.2. Supporting lemmas Several times in our proof of the main results we need to bound a quadratic form given by a symmetric ma-trix and an estimator indexed by a tuning parameter. To this end, we state and prove the following simple lemma.
 Lemma 4.1. Suppose a  X  R p and A  X  R p  X  p . Then norm.
 Proof of Lemma 4.1. where the first inequality follows by H  X older X  X  inequal-ity.
 Additionally, we include Nemirovski X  X  inequality for completeness. See Nemirovski (2000) or D  X umbgen et al. (2010) for details.
 Lemma 4.2 (Nemirovski X  X  inequality) . Let  X  1 ,..., X  n be independent random vectors in R d , for d  X  3 constant e C (independent of s , n , d , v , and the distri-bution of the  X  i  X  X ) such that where || X || s is the ` s norm.
 Finally, we will use Lemma 4.2 to find the rate of con-vergence for the sample covariance matrix to the pop-ulation covariance.
 Lemma 4.3. Let V n = { v 1 ,...,v K n } be a set of vali-dation sets satisfying Condition 2. Then, Proof of Lemma 4.3. First, note that by independence and disjoint elements of v . Let  X   X  R ( p +1) 2 be the vectorized version of the zero-mean matrix 1 c by Jensen X  X  inequality. Using Lemma 4.2 with s =  X  and d = ( p + 1) 2 , we find
E X  X  e C log ( p + 1) 2 X . log(4 n 2  X  )  X  log(4 n 2  X  ) where second to last inequality follows by Defini-tion 3.1 with associated constant C . Therefore, 4.3. Proof of theorems We break this section into parts based on the decom-position of equation (6).
 Final predictor and cross-validation risk (I) Note that by equation (9) and equation (10) Addressing each of the terms in order, The first inequality follows by Lemma 4.1 while the equality in the last line follows by the definition T n . Likewise, " b  X  = =  X  The last inequality follows as b  X  ( b  X  Continuing and using Lemma 4.1, 1
K  X   X  = (1 + t max ) 2  X  Combining these results together, we obtain the fol-lowing upper bound for (I)
L b  X   X  (1 + t max ) 2  X  By Lemma 4.3 with V n = {{ 1 ,...,n }} and c n = n , Additionally, by Lemma 4.3 with V n = { v 1 ,...,v K n } , Furthermore, E (1 + t max ) 4 E ( t max ) 4 implies E (1 + t max ) 4 ( t 4 n ). Combining these three bounds together, we get E | ( I ) |  X  = o Hence, for any V n .
 Cross-validation risk and empirical risk (III) Then, b L = =  X   X  (1 + t  X  ) 2  X  The second-to-last inequality follows by Lemma 4.1 and the fact that ( b  X  b  X  Using a straight-forward adaptation of Lemma 4.3
E As n &gt; c n , by assumption, we see Therefore, following the analogous steps established in the proof of (I), we get Empirical risk and expected risk (V, VI) The somewhat different proof for completeness. Observe the following bounds and Therefore, both ( V ) and ( V I ) follow since by Lemma 4.3.
 This completes the proof of Theorem 3.2. In particu-lar, we have shown that A common practice in data analysis is to estimate the coefficients of a linear model via lasso and choose the regularization parameter via cross-validation. Unfor-fect of choosing the tuning parameter in this data-dependent way.
 In this paper, we demonstrate that the lasso with tun-ing parameter chosen by cross-validation is persistent. derstanding of the interaction between the lasso and a data-dependent tuning parameter. In particular, by imposing an eigenvalue type condition on the design matrix, we can achieve the same risk-consistency re-sults with a data-dependent tuning parameter as with the optimal tuning parameter. We feel that this paper provides some theoretical justification for the received applied researcher in the context of the lasso. The authors would like the thank Cosma Shalizi and Ryan Tibshirani for reading preliminary versions of this manuscript and offering helpful suggestions. We also thank three anonymous referees for their careful and insightful comments and Larry Wasserman for the inspiration.

