 PRASENJIT MAJUMDER, MANDAR MITRA, DIPASREE PAL, AYAN BANDYOPADHYAY, SAMARESH MAITI, SUKOMAL PAL, DEBOSHREE MODAK, and SUCHARITA SANYAL Indian Statistical Institute 1. BACKGROUND The success of the Text REtrieval Conference 1 (TREC), the Cross-Language Evaluation Forum 2 (CLEF), and the NII Test Collection for IR Systems 3 (NTCIR) has established the importance of building reusable, large-scale standard test collections for information access research.

The Text REtrieval Conference (TREC) [Voorhees and Harman 2005], a conference series cosponsored by the National Institute of Standards and Technology (NIST) and the U.S. Department of Defense, was started in 1992 with the aim of encouraging research in text retrieval by using realistically large test collections. During its initial years, TREC focused exclusively on test collections in English, but later included monolingual and cross-lingual retrieval tasks in other European and Asian languages.

CLEF evolved out of a cross-lingual Information Retrieval (IR) track involv-ing European languages that was organized as a part of TREC during 1997 X  1999. Braschler and Peters [2004], in tracing the history of CLEF, write:
Similarly, while TREC included a Chinese language track in 1996 and 1997, as well as a cross-lingual English-Chinese task in 2000, extensive experimen-tation with East Asian languages, specifically Chinese, Japanese, and Korean, was made possible by the NTCIR project [Nakagawa et al. 2005; Kando et al. 2008].

Indian languages did not receive attention until 2003, when DARPA orga-nized a Surprise Language Exercise (SLE) under the TIDES (Translingual In-formation Detection, Extraction, and Summarization) programme [Oard 2003]. The main aim behind the SLE was to investigate the portability of existing information processing tools to new languages by starting with few or no lan-guage resources, and constructing workable IR systems within a short time-frame (10 to 29 days). The SLE consisted of information processing tasks in Cebuano (a language spoken in the Philippines) and Hindi.

For retrieval evaluation, 15 English topics, Hindi translations of these topics, and a collection of 41,697 Hindi news articles from several different sources were distributed. Text encoding issues and obtaining parallel news corpora turned out to be the biggest challenges. About half the allotted time was used up in solving these problems. Most of the system development work was done in the remaining two weeks.

Meanwhile, the proliferation of the Internet in South Asia had led to a rapidly growing mass of available information in Indian languages. Internet editions of several Indian language (IL) newspapers were appearing online, and several magazines and information portals serving content in various In-dian languages were being created. The problem of retrieving information from these repositories poses its own set of challenges (for a more detailed discussion about some of the problems involved in searching for IL content on the Web, see Pal et al. [2008]).
  X  Diversity of character encodings . The proper functioning of any IR system is crucially dependent on a low-level detail, viz. the character encoding scheme used to represent characters in an electronic document. For English, other
European languages, and some of the major Asian languages, these character encoding schemes have long been standardized. Thus, a given character in any one of these languages is always represented by the same numerical code in any digital text document. Unfortunately, this is not true for ILs. Content creation started well before the Unicode standard was widely adopted. In the absence of a universally (or even widely) accepted standard, content in these languages came to be represented very often in terms of font codes rather than character codes. In this scheme, a character is represented by the nu-merical code(s) for the glyph(s) that make(s) up the visual representation of that character. The set of glyphs and their numerical codes could be different for different fonts for the same language. The same character can, therefore, be represented by two different numerical codes in two different documents.
None of the standard text processing methods will work unless these discrep-ancies are first resolved by transcoding all documents in a collection to use a uniform encoding scheme.  X  Inflectionality . While some ILs like Hindi have a fairly simple morphology, with only a few inflected forms corresponding to each base word, some lan-guages like Tamil and Telugu are highly inflectional and agglutinative. A language like Bengali lies in between these extremes in terms of its mor-phological richness. Apart from morphological complexity, most ILs have a large number of compound words. The process of forming a compound also transforms the constituent words in various different ways.  X  Spelling variations . For some ILs, notably Bengali, a number of orthographic  X  X tandards X  exist. This is an inevitable source of confusion for content cre-ators, and various alternative spellings of the same word may occur in a col-lection of documents (sometimes even when the documents are drawn from the same source).  X  Query translation . Cross-lingual IR systems that accept queries in one lan-guage and search through documents in another language typically translate the query into the language of the target document collection. Query transla-tion for IL queries can be challenging because (among other reasons) abstract nouns or adjectives are also used as names of persons or organizations in ILs (e.g., mahan , meaning great, can also be the name of a person). Since there is no distinction between uppercase and lowercase letters in ILs, there is no easy way to tell such usages apart. If the word is used as an abstract noun in an IL query, it should be translated; if it is used as a proper noun, it should be transliterated. Transliteration of IL named entities (from an Indian script to Roman, or even to another Indian script) is itself a difficult problem. Various research groups have reported work that attempts to address some of the above challenges. For example, Pingali et al. [2006] describe a search engine called WebKhoj that attempts to tackle the problem of non-standard encodings used by a substantial fraction of IL Web content. Ramanathan and Rao [2003] describe a lightweight stemmer for Hindi that uses a hand-crafted suffix list. Surve et al. [2004] present a multilingual, domain-specific, informa-tion access system that serves agricultural information.

These efforts were, however, largely sporadic in nature, and the absence of a TREC-like platform hampered cohesive development. In 2007, a number of research groups participated in an IL  X  X rack X  at CLEF [Peters et al. 2008]. However, appropriate IL corpora were not available at the time, so experiments used an English document collection (articles from the Los Angeles Times pub-lished in 2002), with 50 search topics in Indian languages. The topics were originally created in English and later translated into other languages. Around this time, a large consortium-mode project X  X he CLIA (Cross-Lingual Information Access) project (Department of Information Technology http://www.clia.iitb.ac.in/locale.jsp?en) X  X as initiated by the government of In-dia, with the aim of building a cross-lingual information access portal on the Web where a user would be able to present a query in one of seven Indian lan-guages (Bengali, Hindi, Marathi, Punjabi, Tamil, Telugu, and English) and get back information from various sources in his/her own language. These lan-guages were selected because (1) they were felt to have a substantial presence on the Web, and (2) because of the presence of significant research groups which were interested in taking on the system-building task for these languages.
The consortium consists of 10 academic and industrial institutions. The Fo-rum for Information Retrieval Evaluation (FIRE) is a part of this project; it aims to create a TREC-like platform for Indian languages by providing test collections and a common forum for comparing models and techniques. The Indian Statistical Institute, Kolkata, is responsible for the overall coordination of FIRE, with each language being handled by a consortium member.

The first evaluation exercise conducted by FIRE was completed in 2008. This article describes the test collections used at FIRE 2008 (Section 2), summarizes the approaches adopted by various participants (Section 3), discusses the limi-tations of the datasets (Section 4), and outlines the tasks planned for the next iteration of FIRE (Section 5). 2. THE TEST COLLECTIONS While TREC, CLEF, and NTCIR have evolved over the years, and conduct tracks addressing various IR tasks such as image search, log file analysis, patent retrieval, video retrieval, and searching in the  X  X nterprise X  and blog domains, it was felt that FIRE should initially offer only well-understood tasks that have an established evaluation methodology, for two reasons. Participants would have to handle  X  X ew X  languages, and the issues involved in porting stan-dard IR techniques to these languages would be interesting in themselves. Sec-ondly, this would ensure that the inevitable organizational and logistic hurdles would not be compounded by methodological issues. Thus, the only task at FIRE 2008 was the standard, ad hoc retrieval task. The ad hoc task is intended to model a situation in which a user submits a query (representing a one-time or casual information need) to a system, which then tries to retrieve documents from within a text collection that are relevant to the user X  X  information need.
The original plan was to have, for each of the seven languages being han-dled by the CLIA project, (1) a monolingual task in which the target document collection would be in the same language as the queries, and (2) a cross-lingual task in which the queries in a particular language would be used to retrieve documents from a document collection in one of the six other languages. The FIRE 2008 evaluation exercise eventually covered only four languages X  Bengali, Hindi, Marathi, and English. For the remaining languages, suitable corpora (in terms of size, genre, and time period covered) were either unavail-able or could not be prepared in time. These (and possibly other) languages will hopefully be covered by future rounds of the FIRE exercise. This section describes the corpora and topics used for the FIRE 2008 tasks, as well as the relevance assessment procedure. 2.1 Corpora The corpora consist of news articles published in the four languages during the years 2004 to 2007 in prominent online news sources. Table I lists the sources used for the various corpora and the institute responsible for each language.
The original articles in Bengali, Hindi, and Marathi use non-standard, font-based encoding schemes as explained in Section 1. All such documents have been transcoded to use the UTF-8 encoding scheme so that the corpus is uni-formly in Unicode. In the process of transcoding, however, some errors crept in. These are most noticeable in the Hindi corpus. A number of files contain se-quences of invalid characters. More seriously, many files contain sequences of Hindi characters that are not actually permitted in the language (e.g., a string of vowel modifiers with no intervening consonants). One of the participating groups [Paik and Parui 2008] distributed a programme to the other partici-pants that fixed some of these errors.

Each document typically contains a single news article, and consists of a title, the author X  X /correspondent X  X  name and the body of the article. Figures 1 X  3 show sample documents in Bengali, Hindi, and Marathi, respectively.
Some statistics about the corpora are given in Table II. These corpora are comparable in size and type to the ad hoc corpora used at CLEF, which are also mostly drawn from the news genre, but lack the diversity of the early TREC collections, which include significant non-news sources such as U.S. Federal Register and computer-related articles from Ziff-Davis Publishing. Based on the publisher X  X  categorization of the news items, the files in the Bengali and English corpora are organized into subdirectories corresponding to news categories such as Business, Rajya (state news), Travel, Bidesh (in-ternational news), Desh (national news), Sports, Health, etc. 4 The articles in the Hindi and Marathi corpora, however, are not organized into semantic categories. 2.2 Topics Following the tradition established by TREC [Harman 1995], we distinguish between the topics  X  X tatements of a user X  X  information need, and the actual query string that is used by IR systems. The topics consist, as usual, of  X  X  title T (typically a phrase stating the main focus of the user X  X  query),  X  X  description D (a short, natural-language statement of the user X  X  informa-tion need that is no more than one sentence long X  X his was written to be read in the context of the title, though for most of the topics, it is self-contained and can be understood independently of the title), and  X  X  narrative section referred to as N (a supplement to the title and descrip-tion that contains a more detailed specification of what makes a document relevant for the corresponding topic).
 The actual query string submitted to an IR system usually consists of some combination of the T, D, and N fields of the topic.
 For the topic formulation process, our plan was to use the strategy adopted at CLEF [Braschler and Peters 2004]. For each language, native speakers would contribute a set of topics covering subjects of regional, national, and interna-tional interest. From these contributions, topics for which there are a reason-able number of relevant documents in each language would be selected to form the final training/testing set. However, owing to other responsibilities within the CLIA project, not all consortium members were able to participate equally in this exercise. As a result, not all languages were equally represented in the topic formulation process.

Initially, a set of 95 topics was created in both English and Bengali si-multaneously by browsing through the news collections. The topics naturally included names of locations, persons, and acronyms. For each topic, approxi-mately 25 documents were retrieved in each language using the Terrier [Ounis et al. 2006] IR system. Stopwords were removed, words were stemmed using Porter X  X  stemmer for English and YASS [Majumder et al. 2007] for Bengali, and the I( n e )C2 term-weighting scheme of the Divergence from Randomness (DFR) model [Amati and Rijsbergen 2002] was used for these runs (this strategy gave good results in earlier experiments). Topics for which the retrieved set in each language contained between 5 and 15 relevant documents were selected. The remainder were discarded, since queries with too many or too few relevant doc-uments are problematic from the point of view of evaluation: for queries with too few relevant documents, evaluation metrics are unstable in the sense that small changes in the ranks of relevant documents may cause large changes in the metric values; on the other hand, a query with many relevant documents in the top 25 is probably too easy, and will not be useful for discriminating between systems.

Of the selected topics, 25 were distributed as training data, and a set of 50 topics was used for the final evaluation. The English versions of the top-ics were translated into six languages X  X indi, Malayalam, Marathi, Punjabi, Tamil, and Telugu X  X y native speakers of the respective languages who had at least a Bachelor X  X  degree (and therefore a college-level education in Eng-lish). The translators were told to produce natural (as opposed to pedantic or formal) translations that reflect common usage by speakers of the target language. Figure 4 shows versions of Topic 53 ( X  X ndia-U.S. Nuclear Deal X ) in various languages.
 2.3 Relevance Judgments We used the pooling method [Sparck Jones and van Rijsbergen 1976] for gath-ering relevance assessments. The pooling method works as follows. Let S be a set of runs (i.e., retrieval results) from which the pool is to be constructed. For each query, we take the set of N top-ranked documents for that query from each run (where N is some number) and take the union of these sets. This forms the depth-N pool for this topic. The documents in the pool are judged by human assessors. Any document that is not included in the pool is assumed to be nonrelevant for the purposes of computing evaluation measures such as mean average precision, precision at various rank cutoffs, and recall.
For this method to work well, a wide variety of retrieval methods and models should contribute to the pool. Also, in addition to results produced by automatic systems, the results of manual, interactive searches (performed preferably by expert searchers) should also be included in the pool, since such searches some-times find relevant documents that all automatic methods miss.

Since there was a reasonable chance that the number of official submissions at FIRE 2008 would not be as large as at the other major evaluation forums, we did some preliminary pooling using Terrier for Bengali and English (lan-guages with which the assessors available at that time were familiar). For the set of official test topics, retrieval results were obtained using some of the mod-els/methods provided by Terrier (e.g., the Divergence from Randomness (DFR) model and the language modeling approach) [Mitra 2008]. A pool of documents was created from these results as described above (with N = 75), and this pool was judged before the official submissions were received. The pool was later supplemented by contributions from the official submissions to FIRE 2008. Table III shows the details of the pooling process. The Preliminary columns show the number of runs and the pool depth used to construct the preliminary pool. The last two columns show the same information for the pool constructed from official submissions. The pool depth for a language was decided based on an estimate of how many documents could be judged given the constraints of available time and manpower. Table IV shows the total number of documents assessed in each language, as well as the minimum and maximum number of documents judged for any particular query.

There was some debate about the order in which the pooled documents for a particular topic should be presented to the assessors for judgment. When doc-uments are presented to the assessor in some sort of rank order, most relevant documents are expected to occur early on. Conversely, most documents that oc-cur later in the ordering are likely to be nonrelevant. This method of ordering the documents is used at NTCIR [Sakai et al. 2008]. However, there is a po-tential risk in this method: if an assessor notices this pattern (as is likely), the assessment for a document may end up being biased by where in the ordering the document occurs. On the other hand, if documents are presented in order of their identifying numbers (or DOCNOs) (as at TREC, for example), the rele-vant documents are likely to be encountered relatively infrequently. This may also lead to a sense of frustration and a consequent lapse of concentration on the part of the assessor. Eventually, the second option was chosen: documents were presented to assessors in order of DOCNOs.

For a given language, the pool for a single topic was judged in its entirety by one assessor. The judges were either members of the project staff, or recruited specifically for the assessment work. They had at least a Bachelor X  X  degree, and most of them had a Master X  X  degree as well, or were enrolled in a Master X  X  program (typically in English, Hindi, Marathi, or Linguistics). However, they had no prior experience in doing relevance assessments. In order to cross-check the assessments, each Hindi topic was assigned to at least two assessors. Documents on which the assessors disagreed were identified, and the assessors were asked to discuss and resolve these differences. The resulting assessments were used for the final evaluation. For want of manpower, dual assessments could not be obtained for the other languages.

While multi-grade relevance assessments have been in use for some years now at TREC and NTCIR, with documents typically being marked on a 3-or 4-point scale (e.g., highly relevant, relevant, and non-relevant; or relevant, partially relevant, and nonrelevant), binary judgments continue to be used at CLEF (i.e., a document is judged as either relevant or nonrelevant). The rel-evance criterion used at CLEF, adopted from that used for the TREC ad hoc task, is very simple: a document is judged relevant if any part of the docu-ment is relevant to the query. This has two operational advantages [Peters 2010]. First, the use of binary judgments makes it easier to ensure that the assessments, which are done in a geographically distributed fashion, are fairly uniform across languages. Second, it is reasonable to expect that an assessor will take slightly longer to judge a document on a graded scale than on a binary scale, since graded relevance judgments demand a greater degree of cognitive effort from the assessors. This can be an important concern if the assessments need to be completed within a limited amount of time. These were important considerations for FIRE as well, particularly in view of the fact that the as-sessors were doing this work for the first time (indeed, a number of them had no prior exposure even to Web search). Therefore, binary relevance judgments were used for FIRE 2008.

Table V shows the total number of relevant documents found in each lan-guage, along with the minimum, maximum, mean, and median number of rel-evant documents found across topics. The table reflects the fact that the topic formulation process was somewhat biased towards Bengali and English (the languages in which the assessors at ISI Kolkata were competent). At least five relevant documents were found from the Bengali and English collections for each topic, but no relevant Hindi documents were found for five topics, while for one topic, no relevant Marathi documents were found. 3. RESULTS Nine teams submitted a total of 64 official runs. Two teams submitted runs after the deadline; these were designated as unofficial submissions and were not included in the pooling or evaluation process. Tables VI and VII show the distribution of official runs across institutes and tasks. For any task, the query submitted to a system is typically formed from some combination of the T, D, and N fields of the topic. Of all possible combinations, TD (title and description) and TDN (title, description, and narrative) are the most widely used. Table VII also gives a break-up of the runs for each task according to the source fields used to construct queries. In the rest of this article, participants are referred to using the code names given in Table VI.
 Most participants appear to have been interested in the IL to English tasks. This is not unexpected, as English is still a major interlingua, both within India and internationally, especially in academic circles. Also, some of these institutes had already taken part in the IL track at CLEF 2007, which offered a similar task and thus had IL to English IR systems ready. There was also a reasonable number of submissions in the English to Hindi category. In the fol-lowing subsections, we will discuss monolingual and cross-lingual submissions, grouped by target language. Since most runs were based on TDN queries, we use these runs when presenting results and comparing groups. 3.1 Hindi There were 13 submissions in the monolingual Hindi category, and seven English to Hindi cross-lingual runs (i.e., runs that retrieved Hindi documents in response to English queries). 3.1.1 Monolingual Runs. The interpolated recall-precision graphs for the best monolingual TDN runs are shown in Figure 5. The corresponding Mean uninterpolated Average Precision (MAP) figures are shown in Table VIII. The figures are quite close to each other, and the differences between the JHU run and the UNINE runs were not found to be statistically significant (two-sided paired t -test, p &gt; 0 . 4). The table also presents a quick comparison of the ap-proaches used by these groups. More details about the various systems are given below.
 JHU [McNamee 2008] used the HAIRCUT system and obtained the highest MAP score using character 5-grams as indexing units. No language specific re-sources such as stopword lists, morphological analyzers, or stemmers were used in these experiments. The language modeling approach proposed by Hiemstra [2001] (abbreviated to LM) was used to compute document similarity scores. All runs used Pseudo Relevance Feedback (PRF) for query expansion.
UNINE [Dolamic and Savoy 2008] submitted four monolingual runs. Three of them used the title and description fields (not shown in the table above), and one used the narrative in addition. A stopword list was created from the most frequently occurring words in the corpus and modified manually. Both character 4-grams and words were used as indexing units. A light stemmer that removes inflectional suffixes attached to nouns and adjectives was used on words. Queries were expanded using pseudo-relevance feedback. For term-weighting, variants of the DFR model were tried, along with the BM25 formula [Sparck Jones et al. 2000] and a language modeling approach. Finally, a Z -score based fusion method [Savoy 2004] was used to combine the results of multiple retrieval runs. This data fusion method yielded good results.
ISIK [Paik and Parui 2008] tested a simple and efficient statistical stemmer using FIRE data. The main idea behind their stemming algorithm is to form equivalence classes of words such that all members of a class share a com-mon prefix of length not less than a given threshold. The common prefix then becomes the root of the class. The threshold value for the prefix length is a tunable parameter for this approach. The main aim of these experiments was to find a suitable value of this threshold for each language. It was found that a prefix length of 4 did well for Hindi. A variant of the DFR model (IFB2) was used for similarity computations.

IIITH [Sethuramalingam and Varma 2008] also submitted three Hindi monolingual runs using TD, TN, and TDN queries, obtaining MAP scores of 0.2579, 0.2652, and 0.2472. Surprisingly, queries formed from the full topics performed the worst. Their retrieval system was based on Lucene and uses the BM25 term-weighting scheme. 3.1.2 Cross-Lingual Runs. Results for the English to Hindi cross-lingual task are shown in Figure 6 and Table IX. Various query translation techniques were tried for this task. UMD obtained the highest MAP scores, with their best run achieving about 80% of the performance of the best Hindi monolingual run (see Table IX). They used a  X  X eaning matching X  approach based on translation probabilities for query translation (the so-called Derived Aggregated Meaning Matching (DAMM) technique). Two different stemming strategies were tried: a stemmer developed at the University of California, Berkeley for the Surprise Language Exercise described in Section 1, and YASS. YASS was found to be more effective. Expanding the English query using PRF before translation was also found to be useful.

IIITH [Sethuramalingam and Varma 2008] used a bilingual lexicon for query translation. The lexicon was constructed from three sources: Shabdanjali, 7 the Hindi WordNet 8 and a relatively small, manually constructed Hindi-English dictionary. A Conditional Random Field-based Named Entity Recognizer was used to identify named entities, which were then transliterated; however, their results turned out to be significantly worse than the best results obtained by UMD (two-sided paired t -test, p &lt; 0 . 01).

IITB1 [Padariya et al. 2008] also participated in this task, using a dictionary-based query translation method in combination with a page-rank style iterative disambiguation algorithm for selecting from among translation candidates. A rule-based transliteration method was used for out of vocabulary (OOV) words. Queries were based only on the title field of topics. This run achieved a MAP score of only 0.1061. 3.2 Bengali A total of 10 Bengali monolingual runs were submitted by three teams. The results for the best TDN runs are shown in Figure 7 and Table X. No cross-lingual runs involving Bengali were received.

UNINE [Dolamic and Savoy 2008] found that their light stemmer performed better than the aggressive stemmer or using character 4-grams on training data. As for Hindi, they used a Z -score-based fusion method and obtained the best MAP among all Bengali monolingual submissions. ISIK [Paik and Parui 2008] used the same approach for Bengali as they did for Hindi. It was found that a prefix length threshold of 3 worked well for Bengali. JHU [McNamee 2008] compared various character n -gram tokenization schemes for Bengali and, as in the case of Hindi, obtained their best results using regular 5-grams. 3.3 Marathi Monolingual runs in Marathi were submitted by five teams. The results for the best TDN runs are shown in Figure 8 and Table XI. UNINE [Dolamic and Savoy 2008] obtained the highest MAP among these runs. They tried a light stemmer as well as character 4-gram indexing. A later analysis showed that 4-gram indexing yielded slightly better results than word-level indexing with light stemming. This observation was also reported by JHU [McNamee 2008], which used character n -gram tokenization for all runs. Among the JHU submissions, the run using 4-gram tokenization performed best, while the run using raw words performed worst. 3.4 English Fifteen cross-lingual runs using IL queries and English documents were sub-mitted: 10 used Hindi queries, three used Marathi queries, and two runs used Tamil queries. Results for the runs that used TDN queries are given in Figure 9 and Table XII.

The best results were obtained by AU-KBC [Rao and Sobha 2008]. They sub-mitted two runs using Tamil as the query language. Queries were translated using a bilingual dictionary. OOVs and named-entities were transliterated us-ing a statistical method. An ontology-based query expansion strategy was used on the translated (English) queries. This yielded significant improvements over using no expansion and achieved 86.5% of the MAP obtained by the best mono-lingual English run.

MSR [Udupa et al. 2008] used a probabilistic translation lexicon, created from a sentence-aligned English-Hindi parallel corpus. Out of their four offi-cial Hindi to English cross-language runs, the best performance was obtained by using a novel transliteration mining technique [Udupa et al. 2009] for OOV words. Similarity scores were calculated using the language modeling ap-proach [Ponte and Croft 1998]. 3.5 Summary Only nine groups participated at FIRE 2008. These groups had diverse inter-ests and language competencies. Further, the participation guidelines did not specify a compulsory task. As a result, there was too much variability among the submissions to permit much meaningful comparison. Table XIII presents a quick look at the space of approaches that were tried by the participants.
In general, no startling results were observed. Standard, well-known tech-niques for monolingual retrieval (such as effective term-weighting and pseudo-relevance feedback) that have been developed in the context of other languages were found to yield good results.

In particular, a language-independent, character n -gram-based indexing method works reasonably well. Indeed, for the noisy Hindi corpus this method gave the best results. For the other languages, however, better results were ob-tained using either rule-based or statistical stemmers, although in the absence of a stemmer, n -gram-based indexing remains an acceptable alternative, with n = 4 working well for Marathi and n = 5 for Bengali and Hindi. 4. DISCUSSION This was the first evaluation exercise conducted by FIRE, and the test col-lections that have been developed are likely to suffer from some  X  X eething problems. X  Given that relevance assessments were created using the pooling approach, one important concern is that the total number of submissions was relatively small. Further, none of the participants submitted  X  X anual X  runs. A manual run is one that involves human effort at some stage of the indexing and/or retrieval processes. Manual runs created by expert searchers who use techniques such as relevance feedback or interactive query modification are known to be effective at finding relevant documents that completely automatic methods miss [Voorhees and Harman 1997].

Given the relatively small pool depth, a low number of submissions, and the lack of manual runs in the pool, it is quite likely that a nontrivial number of relevant documents are missing from the set of documents that was manually judged. Since these documents are not explicitly judged as relevant, they are automatically regarded as nonrelevant when computing widely-used evalua-tion measures such as mean average precision for any run. This is of particular concern if a group that did not participate in FIRE wishes to use such measures along with these assessments to evaluate new ILIR approaches.

Figure 10 also suggests that the pool may be missing some relevant docu-ments. It shows the number of relevant documents that would be found from the Hindi collection if the pool depth was taken to be 10, 20, ... , 60 (this is the actual pool depth that we used for Hindi). The plot has a nontrivial upward trend even at depth = 60. This suggests that more relevant documents would be found if a larger pool depth were used. Unfortunately, this was not possible given the amount of time and manpower available for the official evaluation. To gauge the seriousness of this problem, we conducted two experiments after the official evaluations were completed. 4.1 Leave-One-Out Experiments The completeness of the pool may be estimated using an approach proposed by Zobel [1998]. In this approach, the contributions due to each individual partic-ipant are removed from the pool in turn, relevance assessments are restricted to documents contained in the residual pool, and these restricted assessments are used for evaluation. If the evaluation figures thus obtained are reasonably close to the original figures, we may assume that the pool is sufficiently com-plete and that a new participant is unlikely to add a significantly large number of relevant documents to the pool.

We used Zobel X  X  approach to study the Hindi assessments, since the Hindi pool was the most diverse, consisting of 20 contributions from six groups. On the whole, MAP figures for a run dropped appreciably when the corresponding group X  X  contributions to the pool were ignored. The mean absolute change in MAP (across the 20 runs) was 0.0196, with a standard deviation of 0.0115. The median relative change in MAP across the 20 runs was more than 5%. MAP dropped by as much as 0.0386 (13.8%) for the run that was most affected. For all but three of the runs, the drop in MAP was found to be statistically significant (one-sided paired t -test, p &lt; 0 . 05).

More important, however, than the absolute changes in MAP values is the following question. How would the ranks of a group X  X  runs be affected if the group had not participated in the FIRE 2008 exercise, but used the assessments for evaluating their runs afterwards? Table XIV gives a quantitative answer to this question.

Table XIV lists the six groups that contributed to the Hindi pool, the number of runs submitted by each group, the number of relevant documents added to the pool by that group alone, and the correlation between the official ranking, and the ranking that would be obtained if all groups were evaluated using the residual assessments that would remain after the corresponding group X  X  contri-butions are omitted from the pool. Table XIV shows that the relative positions of the runs would remain largely unaffected (  X &gt; 0 . 9), when the contributions of any one of four groups are omitted from the pool. For the remaining two groups, participation has a somewhat more noticeable impact on the relative positions of runs. On the face of it, it may appear interesting that even though IIITH contributes the largest number of unique relevant documents, rankings remain unchanged when these relevant documents are removed from the pool. This is because the unique relevant documents contributed by IIITH are re-trieved at low ranks. Thus, a number of the IIITH runs are ranked low to start with in this group of 20 submissions. Removing the contribution of these runs to the pool causes a further drop in their absolute performance, without signif-icantly affecting the performance of other runs. The relative positions of the runs therefore remain largely unchanged. In general, however, it is unsafe to draw too much reassurance from this small group of observations (as the next set of experiments demonstrates). 4.2 Manual Runs The incompleteness of the pool was further explored by doing some manual retrieval experiments. Two users were given the search topics, and an inter-face where they could enter a query, look at retrieved documents, filter the results using Boolean expressions involving query terms, perform relevance feedback, and interactively add/remove terms from the query. In this process, each user looked at approximately 100 documents per topic. The top N docu-ments found by each user (where N was the pool depth) were then assessed for relevance. This post hoc manual run yielded 303 new relevant documents for Hindi and 409 new relevant documents for Bengali. These documents were not included in the original assessments. Any new system that retrieves these rel-evant documents will receive no credit if evaluation is done using the current assessments. 5. CONCLUSIONS AND FUTURE PROSPECTS It is probably fair to characterize FIRE 2008 as a learning exercise about how to run an evaluation campaign. Over the last decade or so, a great deal of research has been devoted to the problem of IR system evaluation, and evalu-ation principles and methods are better understood now than they were in the early days of TREC, for example. In spite of this, actually running an evalua-tion campaign is still a nontrivial task, not least because of various constraints on the quantity and quality of manpower available.

As far as Indian language IR itself is concerned, it is difficult to draw firm conclusions about what was learned. Few new IR approaches were tried; in general, the applicability of standard, well-known techniques for monolingual retrieval that have been developed in the context of other languages was once again demonstrated. Participants focused more on monolingual retrieval, whereas the interesting challenges probably lie in the cross-lingual task. Before any conclusions can be drawn, more queries, more participants, and above all a more careful post-mortem of participating systems is needed in or-der to identify how frequently (if at all) the challenges outlined in Section 1 are faced in real life, and to determine how successful current systems are in meeting these challenges.

Moreover, an evaluation campaign that aims to address the needs of ILs should cover at least representative languages from the four major language groups (Indo-Aryan, Dravidian, Austro-Asiatic, and Sino-Tibetan) that exist in India. The languages covered by FIRE 2008 all belong to the Indo-Aryan family. Languages belonging to the Dravidian family, such as Tamil and Telugu, have a significant presence on the Web, but have remained essentially untouched by the FIRE 2008 exercise.

The second iteration of the FIRE evaluation exercise was held during 2009-2010. Learning from the first year X  X  experiences, certain changes were put in place for FIRE 2010.  X  X s discussed, manual runs do succeed in finding relevant documents not found at top ranks by any automatic method. Manual searching was used during the preliminary pooling stage for FIRE 2010, but once again, no man-ual runs were submitted by the participants.  X  X or the first year, no task was made compulsory. There was thus too much variability in the submissions to permit meaningful comparison. For all
FIRE 2010 tasks, a run using only the title and description fields of queries was made compulsory.  X  X leaner versions of the document collections were made available. Since the Hindi corpus was particularly noisy, it was supplemented by a new cor-pus that consists of nearly 55,000 documents and is approximately 370 MB in size.  X  X he narrative sections of some of the FIRE 2008 queries listed interesting aspects of the topic, without being explicit about whether a document should address any one or all of these aspects in order to be deemed relevant. The narratives of FIRE 2010 topics were framed more carefully to make it easier for assessors to determine the relevance of retrieved documents.
 For the ad hoc mono and cross-lingual tasks, manually created Roman translit-erations of the IL queries were also made available, even though none of the participants used these queries.
 In addition to the core ad hoc tasks, two pilot tasks were offered. IBM India Research Lab 9 organized an ad hoc task that involves looking for information in technical mailing lists and discussion forums. The dataset for this task is approximately 1GB in size and consists of approximately 220,000 documents.
Yahoo! Labs Bangalore organized another pilot task that involves retriev-ing Wikipedia pages corresponding to entities mentioned in news documents. Given a particular query document (collected from http://news.yahoo.com), relevant named entities contained in the document were to be identified. Wikipedia entity pages corresponding to these entities were then to be found from a Wikipedia dump (approximately 24 GB of text in uncompressed form).
While the datasets generated out of FIRE 2008 and FIRE 2010 have definite limitations, it is hoped that FIRE will evolve over the years, and that the test collections it makes available will also mature into datasets that can be used for reliable experimentation. A third round of the FIRE campaign has tentatively been planned for 2011. FIRE 2011 is expected to cover some of the important languages that are as yet unaddressed. It is also expected to include additional domains/genres (e.g., tourism or health) besides news. Possibly the greatest challenge that the FIRE 2011 effort will have to face is the task of building a larger community of researchers interested in IR, getting them familiarized with state-of-the-art baseline techniques and then encouraging them to try new approaches.
 We would like to thank the Department of Information Technology, Govern-ment of India, for supporting the CLIA project and FIRE; Google, HP Labs India, IBM India Research Laboratory, Microsoft Research, and Yahoo! India, for financial support; Anandabazar Patrika, Jagran, Amar Ujala, Maharashtra Times, and Sakal for the FIRE corpora; and the translators who translated the FIRE queries into various languages.
 We are grateful to Donna Harman, Noriko Kando, Doug Oard, Carol Peters, Prabhakar Raghavan, Stephen Robertson, Mark Sanderson, Amit Singhal, and Ellen Voorhees for their support and guidance. Thanks also to the members of the FIRE steering committee. Finally, we would like to thank the anonymous reviewers for their detailed comments on an earlier draft of this article.
