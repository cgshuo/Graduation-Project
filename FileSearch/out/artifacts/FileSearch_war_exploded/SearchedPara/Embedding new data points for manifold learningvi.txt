 Shiming Xiang  X  Feiping Nie  X  Yangqiu Song  X  Changshui Zhang  X  Chunxia Zhang Abstract In recent years, a series of manifold learning algorithms have been proposed for nonlinear dimensionality reduction. Most of them can run in a batch mode for a set of given data points, but lack a mechanism to deal with new data points. Here we propose an extension approach, i.e., mapping new data points into the previously learned manifold. The core idea of our approach is to propagate the known coordinates to each of the new data points. We first formulate this task as a quadratic programming, and then develop an iterative algorithm for coordinate propagation. Tangent space projection and smooth splines are used to yield an initial coordinate for each new data point, according to their local geometrical relations. Experimental results and applications to camera direction estimation and face pose estimation illustrate the validity of our approach.
 Keywords Manifold learning  X  Out-of-sample  X  Coordinate propagation  X  Tangent space projection  X  Smooth spline  X  Quadratic programming 1 Introduction In many areas of natural and social sciences, one is often confronted with intrinsically low-dimensional data points which lie in a very high-dimensional observation space. Examples range from visual object recognition in digital images and text clustering in Web pages. Developing algorithms in machine learning and data mining [ 25 , 48 ] should also consider the implementation of high dimensional data. In general, the intrinsic dimensionality of the data is less than the observation dimensionality. Reducing the dimensionality of such data is needed to discover the underlying lower-dimensional structure, which can facilitate the visualization of the data in scientific data processing, make clustering and classifica-tion more easy in pattern analysis, vision computing and data mining, and reduce the cost of communication and storage of high-dimensional data in information retrieval and data engineering.

This task can be roughly explained with an underlying generative model, x = F ( y ) , where x is located in a high-dimensional space and y is a lower-dimensional parameter. set. If the generative model F is linear, the well known principal component analysis [ 24 ] can form can be obtained to deal with unseen data or new data [ 21 ].

However, when the generative model is nonlinear, inverting such model is proven to be a very challenging task. Many nonlinear mapping methods are developed [ 31 , 39 ], such as self-organizing map [ 27 ], auto-encoder neural network [ 1 ], generative topographic map [ 4 ], most of these algorithms can only give a local solution, and thus may fail to discover the true nonlinear structure hidden in the data.

Recently, some manifold learning algorithms have been proposed for nonlinear dimensio-nality reduction (NLDR), assuming that the data points are sampled from a manifold embed-ded in a high-dimensional observation space. Typical algorithms include Isomap [ 43 ], Local Linear Embedding (LLE) [ 38 ], Laplacian Eigenmap [ 2 ], Local Tangent Space Alignment (LTSA) [ 54 ], charting [ 6 ], Hessian LLE (HLLE) [ 12 ], maximum variance unfolding [ 47 ], conformal eigenmap [ 41 ], Local Spline Embedding (LSE) [ 49 ], and so on. Real performances on many data sets show that they are effective methods to discover the underlying structure hidden in the high-dimensional data set. The utility of NLDR or manifold learning has been demonstrated in many applications, such as visualization [ 7 , 16 , 45 ], image analysis [ 11 ], colorization [ 36 ], texture analysis [ 35 ], and motion analysis [ 23 ],andsoon.
All the manifold learning algorithms are initially developed to obtain a low-dimensional embedding for a set of given data points. The problem in general is formulated as an optimi-zation problem, in which the low-dimensional coordinates of all the given data points need to be solved. Matrix eigenvalue decomposition or other mathematical optimization tools (for example, the semidefinite programming [ 47 ]) are used to obtain the final results, i.e., the intrinsic low-dimensional coordinates. Accordingly, the algorithms run in a batch mode, for generalization. When new data points arrive, one needs to rerun the algorithm with all the data points. In applications, however, rerunning the algorithm may become impractical as more and more data points are collected sequentially. On the other hand, rerunning means the previous results are discarded. This may be very wasteful in computation.

Our interest here is to map the new data points into the previously-learned results. This is also known as out-of-sample problem. In literature, out-of-sample extensions for LLE, Isomap, Laplacian Eigenmap are given by Bengio et al., using kernel tricks to reformulate these algorithms [ 3 ]. The idea is to generate an approximate function with kernel terms to deal with once a new data point. Recently, for out-of-sampling problem, Carreira X  X erpi X  X n and Lu provided a probabilistic formulation based on Latent Variable Model (LVM) under the Laplacian Eigenmap framework [ 9 ]. Such a formulation combines the advantages of LVMs, kernel density estimation and spectral method, and as a result it is simple, nonparametric and computationally not very costly. As a common point, these two algorithms [ 3 , 9 ]are all developed in terms of generative models. That is, like the manifold linearization method of the locality preserving projection [ 21 ], a mapping can be yielded for treating new data points. Later, this linear mapping approach is extended in terms of kernel tricks to explore Kernel Locality Preserving Projections (KLPP) and shows satisfactory performance in many real-world data sets. The out-of-sampling problem is also formulated as an incremental learning problem, and extensions for LLE and Isomap are given by several researchers bra methods is used to treat the new data point. These approaches are suitable for once dealing with one new data point. For more than one new data points, however, we need to deal with them one by one.

Let us use Fig. 1 to explain our motivation. We are given two sets of data points which are well sampled from a manifold embedded in a high-dimensional Euclidean space. The low-dimensional embedding coordinates of one data set are also given. For example in Fig. 1 a, we assume that the low-dimensional embedding coordinates of data points with purple (dark) color are known. Under these conditions, our task is to map the new data points with yellow (light) color. In this work setting, the coordinates of the neighbors of a new data point may be known, partly known or even all unknown (Fig. 1 a).

Our idea to solve this problem is to propagate the known coordinates to the new data points. To this end, we first consider this problem in view of coordinate reconstruction in the intrinsic space and formulate it as a quadratic programming. In this way, we can get a global embedding for the new data points. Then, we develop an iterative algorithm via a regularization framework. Through iterations, each new data point gradually obtains a coordinate (Fig. 1 b). We call this process coordinate propagation . In this process, tangent space projection and smooth splines are used to generate an initial coordinate for each new data point to help the propagation.
 In contrast to the generative models [ 3 , 9 ], our method is purely geometrically motivated. Although our method is unable to generate an explicit mapping for new data points, it is unnecessary to develop the out-of-sample extensions one-by-one for each of the existing manifold learning algorithms, such as LLE, Laplacian Eigenmap, LTSA, LSE, and so on. Generally, our model treats their out-of-sample extensions in a unified way. Furthermore, in our framework, different manifold learning algorithms can be combined together to solve the out-of-sample task.

The remainder of this paper is organized as follows. Section 2 introduces the model with quadratic programming and the iterative algorithm for coordinate propagation. Tangent space projection and smooth splines are constructed for coordinate prediction in Sect. 3 .Wereport the experimental results in Sect. 4 . Applications to camera direction estimation from images and face pose estimation from images are also illustrated in this section. Section 5 briefly reviews the related work. We draw conclusions in Sect. 6 . 2 Model and algorithm 2.1 Problem formulation The NLDR problem . Given a set of data points X ={ x 1 , x 2 ,..., x n } X  R m , which lie on a manifold M embedded in a m -dimensional Euclidean space. The goal is to invert an underlying generative model x = F ( y ) to find their low-dimensional parameters (embedding coordinates) Y ={ y 1 , y 2 ,..., y n } X  R d with d &lt; m . In this form, NLDR is also known as manifold learning.

The out-of-sample extension problem . Given a set of data points X ={ x 1 ,..., x l , x + 1 ,..., x n } X  R m and the low-dimensional embedding coordinates Y L ={ y 0 1 ,..., y 0 l }  X  R d learned from the first l data points. The goal is to obtain the low-dimensional coordi-first l data points. 2.2 Model 2.2.1 NLDR model and out-of-sample model A large family of NLDR algorithms can be viewed as the approaches based on minimizing the low-dimensional coordinate reconstruction error. The algorithms in this family include Isomap [ 43 ], LLE [ 38 ], Laplacian Eigenmap [ 2 ], LTSA [ 54 ], and LSE [ 49 ], and so on. The optimization problem can be uniformly formulated as follows: where tr is a trace operator, M is an n  X  n reconstruction error matrix which is calculated according to the corresponding geometrical preserving criterion, C is an n  X  n matrix used solved, in which y i is a d -dimensional embedding coordinate of x i ( i = 1 ,..., n ) .Thatis, each row of Y corresponds to a low-dimensional coordinate of a data point in X .
We use Problem ( 1 ) to model the out-of-sample problem. Taking the known low-dimen-sional embedding coordinates of the first l data points as constraints, naturally we obtain the following model: Now we give some explanations on Problem ( 1 )and( 2 ) as follows:
First, adding Y T CY = I to Problem ( 1 ) is to avoid degenerate solutions. Without it, Y = 0 is an optimum in R n  X  d matrix space which minimizes the objective function.

Second, it can be proven that the low-dimensional data points in Y ={ y 1 , y 2 , ..., y n } , which are obtained by solving Problem ( 1 ), are located near and around the coordinate origin of R d space. Here we take the commonly used case of C = I to explain this fact.
 On the one hand, Y T Y = I indicates that the n data points in Y have unit variance matrix. origin.
 On the other hand, in many manifold learning algorithms (for examples, LLE, Laplacian Eigenmap, LTSA and LSE), the sum of each row of M equals to zero. Thus, it can be easily justified that M has a zero eigenvalue whose corresponding eigenvector only consists of the same constants, namely, v 1 =[ 1 , 1 ,..., 1 ] T  X  R n . Further denote the eigenvectors corresponding to the first ( d + 1 ) minimum eigenvalues by v 1 , v 2 ,..., v d + 1 . According to construct Y . Note that naturally we have v T 1 v j = 0, j = 2 , 3 ,..., d + 1, due to the orthogonality between eigenvectors. This indicates that the mean of the n data points in Y is zero. In other words, they are located around the coordinate origin.

Third, in out-of-sample work settings as formulated in Problem ( 2 ), the known coordinates of the previously treated data points specify where the new data points should be located. That is, the new data points should be embedded (mapped) near the previously treated data points. Thus after the equality constraints are introduced to Problem ( 2 ), we simply discard the constraint Y T CY = I in Problem ( 1 ).

Finally, the natures of the optimization Problems ( 1 )and ( 2 ) are different from each other. Problem ( 1 ) is a typical quadratic constrained quadratic programming [ 5 ], which can be solved via performing eigenvalue decomposition of matrix M . Problem ( 2 ) is a quadratic programming (QP) with linear equality constraints, in which the variable to be optimized is amatrix Y  X  R n  X  d . We will show that it can be divided into d subproblems, each of which is a QP with only n variables to be solved. 2.2.2 Model simplification Problem ( 2 ) is a QP problem with linear equality constraints, which can not be solved by performing eigenvalue decomposition of matrix. To reduce the complexity, we introduce the following Proposition: lent to the following d subproblems, each of which is responsible for optimizing one of the d coordinate component vectors : where y =[ y 1 ,..., y n ] T is an n-dimensional vector to be solved.
 ..., y n ] X  R d  X  ( n  X  l ) . Accordingly, we divide M as four sub-blocks: where M LL is an l  X  l sub-block, M LU is an l  X  ( n  X  l ) sub-block, M UL is an ( n  X  l )  X  l sub-block, and M UU is an ( n  X  l )  X  ( n  X  l ) sub-block. Then is known, equivalently, the constrained optimization problem in ( 2 ) can be solved via the following unconstrained quadratic programming:
Differentiating the above objective function with respect to Y U and setting the derivative to be zero, we can obtain the following equations from which Y U can be solved:
Now consider each of the subproblems in ( 3 ) and denote y =[ y T L , y T U ] T  X  R n in which y we have  X 
From each subproblem in ( 8 ), we can derive a group of linear equations to solve a coordi-nate component vector, namely, a row vector of Y U  X  R d  X  ( n  X  l ) . Combining these equations
This proposition indicates that we can respectively solve d subproblems to get the lower-dimensional coordinates of n  X  l new data points. 2.3 Matrix computation and problem solution In this Subsection, we simply introduce how to calculate matrix M  X  R n  X  n in problem ( 2 ), 2.3.1 Computing M Actually, M is constructed according to the corresponding manifold learning algorithm employed to model the out-of-sample problem. The calculation of M is related to all of the n data points, including the l previously treated data points and the rest n  X  l new data points.
 In computation, for each data point x i , first we determine its k neighbors from n samples X ={ x i } n where i 1 , i 2 ,..., i k are k indices.

For Isomap, the form of M can be derived according to the work in [ 50 , 52 ], which is related to the manifold distances between any pairs of data points [ 43 ]. Thus, the computation for large scale data set is usually time consuming.

In LLE [ 38 ], the element M ij in M is calculated as M ij  X   X  ij  X  W ij  X  W ji + m W mi W mj , where the weights W ij summarize the contributions of the j -th data point to the linear reconstruction of the i -th data point, and  X  ij is 1 if i = j and 0 otherwise. The linear matrix, M can be formulated as M = ( I  X  W ) T ( I  X  W ) ,inwhich W = ( W ij )  X  R n  X  n .
In Laplacian Eigenmap [ 2 ], M can be calculated by three steps. First, for each x i , k weights recorded into the i -th row of matrix W . Then, we calculate a diagonal matrix D by summing each row of W . Finally, the Laplacian matrix M can be constructed as M = D  X  W . In LTSA [ 54 ], M is filled submatrix-by-submatrix, namely, M ( I i , I i )  X  M ( I i , I i ) + I  X  G i G T and S is a selection matrix [ 54 ].
 S , M = S T BS ,inwhich B assembles { B i } n i = 1 (see the Eq. ( 15 )inLSE[ 49 ]). As a summary, matrix M in LLE, Laplacian Eigenmap, LTSA and LSE is highly sparse. In LLE, totally there are 4 kn non-zero elements to be determined. In LTSA and LSE, there are at most n ( k + 1 ) 2 non-zero elements to be determined. In Laplacian Eigenmap, totally there are n ( k + 1 ) elements to be calculated. In Isomap, usually M is a dense matrix. 2.3.2 Computing M UU , M UL and M LU In out-of-sampling settings, from Problem ( 8 ) we see that we only need to calculate M UU , M
UL and M LU and need not calculate the whole matrix M . Thus in the case that the number of the previously treated data points is largely greater than that of new data points, namely, l ( n  X  l ) , the computation will be largely saved. Furthermore, generally M is a symmetrical matrix. 1 Thus, we only need to calculate M UU and M UL .

Note that like LTSA and LSE, in LLE when accumulating the errors derived from x i and The difference is that B i is calculated in different way. Thus, we can calculate M UU and M
UL in a unified way in LLE, LTSA and LSE. The steps can be summarized as follows: (2) construct a new enlarged subset including S and the n  X  l new data points, namely, (3) For each data points in new subset S , determine its k neighbors, fill the sub-matrix with
Now for Laplacian Eigenmap, we only consider the n  X  l new data points. For each new are subtracted from this sum, then we fill them into the i -th row of W and set the diagonal element M ii to be this sum.

Finally, for Isomap, we use the formulation in [ 50 ] to calculate M . After it is calculated, we simply divided it into four sub-matrices as those in Eq. ( 4 ) and get M UU and M UL . 2.3.3 Model solution We see that Problem ( 2 ) is equivalent to d QP problems, which can be solved via linear equations. Many existing iteration approaches to linear equations can be employed, such the well-known methods as symmetric LQ method, generalized minimum residual method, bi-conjugate gradients stabilized method, conjugate gradients squared method, preconditio-ned conjugate gradients method, and so on. Such methods are developed in view of linear algebra theories. Generally, in each iteration, the computation is determined globally on the coefficient matrix of linear equations.
In this paper, we will introduce an iterative method to solve the subproblems in ( 8 ) from the perspective of manifold learning. In each iteration, the calculation will be largely confi-ned into the local neighborhoods of data points sampled on the manifold. Differing from solving a general task of linear equations, here we know the local relationship between data points. This gives us a way to explain the data in view of geometrical analysis and develop out-of-sample algorithm on the data graph by connecting local neighbors together. We will introduce an idea of coordinate propagation, in which the new data points will gradually receive its embedding coordinates from its own neighbors, without considering its non-neighbors. In this way, the main computation is limited in each of the local neighborhoods. In addition, the computational complexity will be reduced linearly to the number of new data points to be treated. 2.4 Iterative algorithm for coordinate propagation In this subsection, we develop an iterative algorithm to solve the out-of-sample extension problem. The iterative algorithm can reduce the computational complexity and need much less computation resources. In an iterative framework, it would be possible for us to deal with a very large number of new data points.
 Here we consider one of the subproblems in problem ( 3 ) since they have the same form. For convenience, we omit the superscripts and the subscripts in the constraints, and rewrite the problem as follows: the following regularization representation: i = l + 1 ,..., n . Here, g i will be evaluated from the neighbors of x i . We use tangent space projection and spline interpolation to solve this problem (in Sect. 3 ).

In Eq. ( 10 ), the first term is the smoothness constraint, which means that the embedding coordinates should not change too much between neighboring data points. The second term is the fitting constraint, which means that the estimated function y should not change too much from the given values. The third term is the predicting constraint, which means that each y should not bias too much from the predicted value. The trade-off among these three constraints in ( 9 ) in the case of  X  1  X  X  X  .

Let us assume for the moment that each g i is known. Differentiating the objective function with respect to y =[ y T L , y T U ] T and setting the derivative to be zero, we have extension problem, we need not solve y L since y L = f L is known. Thus we only need to consider y U . Here we see that the second equation in ( 11 ) is equivalent to the subproblem in ( 8 )when  X  2 = 0. Now it can be transformed into where S = I  X  M UU and I is an ( n  X  l )  X  ( n  X  l ) identity matrix. Let us introduce two new variables:  X  = 1 1 +  X  In this case, we have the following proposition: (
I  X   X  S )  X  1 b ,where b =  X  g U  X   X  M UL f L .
  X  e
Furthermore, based on Eq. ( 12 ), in the ( t + 1)-th iteration, we have
In the case of  X ( X  S )&lt; 1, we have [ 14 ]
Then, the sequence { y ( t ) U } will converge to Thus we finish the proof.

Some remarks. In Eq. ( 12 ), the first term is the contribution from the new data points, while the second term is the contribution from the previously learned data points on the manifold. Note that these contributions are decreased, due to  X &lt; 1. If we only consider these two terms, then the sequence will converge to y  X  U = X   X ( I  X   X  S )  X  1 M UL f L .Thismay be far from the optimization optimum  X  M  X  1 UU M UL f L , especially when  X  is a small number. we call it a prediction to the new data points in the iterative framework, and its contribution is necessary for us to get a good prediction to g U . We will evaluate it along the manifold via tangent space projection and spline interpolation on the neighbors. Now the steps of the iterative algorithm can be summarized as follows:
Algortihm 1: iterative framework for embedding new data points (1) Provide an initial coordinate component vector g ( 0 ) U (in Sect. 3 ); Let i = 0. (2) Let g U = g ( i ) U and run the iteration Eq. ( 12 ) to obtain a y  X  U . (3) Justify the convergence condition. If it is satisfied, stop the iteration and exit. (4) Predict a g ( i + 1 ) U according to y  X  U and f L (in Sect. 3 ). (5) i  X  i + 1, go to step (2).

Through the iterations, each new data point gradually receives a value (here it is a coor-dinate component). To construct d coordinate component vectors, we need to perform the iterative algorithm d times. In this way, the known coordinates are finally propagated to the new data points. We call this process as coordinate propagation .

In computation, we set the maximum iteration times to be 100 when performing the step (4) in the above algorithm. Details will be introduced in Sect. 3 . 3 Coordinate prediction predict the new data point  X  X  X  since it has the maximum number of neighbors with known coordinates. After  X  X  X  is treated, then we select  X  X  X . After  X  X  X  and  X  X  X  have been treated,  X  X  X  is one of the candidates in next time, and so on.

In the above process, a basic task can be summarized as follows. Given a new data point x coordinates of the first r data points are known. The task is to generate a coordinate for the center point x (for example, the data point  X  X  X  in Fig. 2 b). Our method includes two steps: tangent space projection and spline interpolation. The goal of tangent space projection is to locally represent the data points in a local coordinate system with lower dimensionality. Based on the local coordinates, splines can be constructed. Finally the local coordinate of x is predicted via spline interpolation. Figure 2 b explains the process. 3.1 Tangent space projection This subsection constructs a local coordinate system to represent x and its k neighbors, and space projection.

This step is performed on the assumption that the n data points in X are densely sampled from a smooth manifold. In this case, the tangent space of the manifold at point x  X  X can be well estimated from the covariance matrix of the neighbors of x [ 32 ]. We use this space to define the local coordinates [ 12 , 54 ]. To be robustness, we select to simultaneously coordinatize the k + 1 neighboring data points.
 Performing a singular value decomposition of the centralized matrix of X ,wehave where I is an ( k + 1 )  X  ( k + 1 ) identity matrix, e is an ( k + 1 ) -dimensional vector with e =[ 1 , 1 ,..., 1 ] T  X  R k + 1 , is a diagonal matrix which contains the k + 1 singular values in descending order, U is an m  X  m matrix whose column vectors are the left singular vectors, and V is an ( k + 1 )  X  ( k + 1 ) matrix whose column vectors are the right singular vectors. Finally, we get the local coordinate t and t i via subspace projection: and an d -dimensional column vector which contains the first d components of the i -th row of matrix V . 3.2 Spline mapping After the local coordinates are evaluated, then we need to map them into the global coordinate system (see Fig. 2 b). We hope to construct a function g : R d  X  R through which we can get a value f = g ( t ) for x . To this end, the following r conditions should be satisfied: where f j is assigned to be a known lower-dimensional coordinate component of x j ( j = 1 ,..., r ) .

To satisfy the conditions in ( 14 ), spline regression method is used to construct the function g . Beyond the commonly used linear regression [ 30 , 37 ], this spline is developed from the Sobolev space, and has the following form [ 13 , 46 ]: where the first term is a linear combination of r Green X  X  functions  X  j ( t )( j = 1 ,..., r ) , space. Here we take the one-degree polynomial space as an example to explain p i ( t ) .Let t case, p is equal to 3.

In addition, the Green X  X  function  X  j ( t ) is a general radial basis function [ 13 , 46 ]. For  X  ( t ) =|| t  X  t j || .

To avoid degeneracy, we add the following conditionally positive definition constraints [ 53 ]: system for solving the coefficients  X   X  R r and  X   X  R p : where K is an r  X  r symmetrical matrix with elements K ij =  X ( || t i  X  t j || ) , P is an r  X  p matrix with elements P ij = p i ( t j ) ,and f =[ f 1 ,..., f r ] T  X  R r .
 We point out that g is a smooth function and f j = g ( t j ) holds for all j , j = 1 ,..., r . function g may be degraded. This may be not a good choice for predicting new points. Thus we should keep a balance between the training data points and new data points. We consider the regularization situation, where a trade-off parameter  X  is introduced [ 46 ]. In this case, the matrix K in Eq. ( 17 ) should be replaced by K +  X  I . We manually set it to be 0.0001 in this paper.
 error accumulation, the above performance is only used to yield a coordinate component for the center point x and not used to map the rest new data points x r + 1 ,..., x k .Togetthe d d times. Note that in each time the coefficient matrix keeps unchanged since it is only related Fig. 2 , each new data point can get an initial coordinate.
 the above algorithm again for each new data point. 4 Experiments and applications We evaluated the algorithm on several data sets including toy data points and real-world images. Experimental results obtained by the global optimization model (GOM), namely, the model given in ( 8 ), and the coordinate propagation (CP) are illustrated. Comparative experiments with the Kernel Locality Preserving Projections (KLPP) [ 8 ] are also conducted. Applications to camera direction estimation from images and face pose estimation from images are also reported in this subsection. In addition, the computation complexity is also analyzed in this section. 4.1 Experimental results Figure 3 a illustrates 1,200 data points sampled from a S-surface. Among these data points, 600 data points above the plane  X  p  X  are treated as new data points. The intrinsic manifold shape hidden in these data points is a rectangle (Fig. 3 b). The sub-rectangle located in the left of the center line in Fig. 3 b can be viewed as the 2-dimensional (2D) parameter domain of the original data pints. Our goal is to use the new data points to extend it to the right sub-rectangle.

We use LLE [ 38 ], LTSA [ 54 ]andLSE[ 49 ] to learn the original data points. The results with k = 12 nearest neighbors are shown in the left region of the dash line in Fig. 4 a, b, d, e, and g, h, respectively. Naturally, the learned structure is only a part of the whole manifold shape. The intrinsic manifold structure hidden in these 600 original data points is a small rectangle.
 The new data points are embedded into the right region by GOM and CP. In Fig. 4 a and b, the M matrix in Eq. ( 4 ) is calculated according to LLE with k = 12, i.e., M = ( I  X  W ) T ( I  X  W ) .InFig. 4 dande,the M matrix is calculated according to LTSA with k = 12, i.e., M = S T W T WS ;InFig. 4 g and h, the M matrix is calculated according to LSE with k = 12, i.e., M = S T BS . From the ranges of the new coordinates, we see that the learned manifold shape is extended along the correct direction by the new data points.
For comparison, Fig. 4 c, f and i show the 2D embedding results of all the 1,200 data points directly by LLE, LTSA and LSE. As can be seen, the results are confined into a square region, not extended to be a long rectangle. The reason we get a square is that in LLE, LTSA ans LSE an orthogonality constraint YY T = I is introduced to avoid degenerate solutions.
 Figure 5 shows the results by GOM and CP, which use a combination of LSE and LLE. That is, the original data points are learned by LSE to get their low-dimensional coordinates, but the M matrix in Eq. ( 4 ) is calculated via LLE. Compared with the results only based on LLE (see Fig. 4 a, b), here the shape of the manifold is better preserved. This experiments indicates that different manifold learning algorithms can be combined together to solve the out-of-sample problem.

Figure 6 shows two experiments on image data points. In Fig. 6 a and b, a face moves on a noised background image from the top-left corner to the bottom-right corner. The data set includes 441 images, each of which includes 116  X  106 grayscale pixels. The manifold is ( a ) (b) (c) ( g ) (h) (i) ( a ) (b) embedded in R 12296 . Among these images, 221 images are first learned by LSE with k = 8. The M matrix in Eq. ( 4 ) is calculated according to LSE. The learned results are shown with (red)  X + X . The rest data points are treated as new data points, and their final results are illustrated with (blue) filled squares. From Fig. 6 a and b, we see that they are faithfully mapped into the previously learned structure by GOM and CP.

In Fig. 7 a and b, 400 color images are treated, which are taken from a teapot via different viewpoints located on a circle. The size of the images is 101  X  76. The manifold is embedded in
R 23028 . Among the images, 200 images are first learned by LSE with k = 7, and the results their final results are illustrated with (blue) filled circles.

Learning manifold from noise data is an open yet challenging problem in manifold lear-ning. To investigate the ability of our algorithm to deal with noise data, here we conduct one experiment on the teapot images used in Fig. 7 . We add slat and pepper noise to each image  X  X  X  with 101  X  76 pixels by the following Matlab sentences:
Here the parameter  X  X Ratio X  controls the proportion of noise pixels to all of the pixels. In other words,  X  X Ratio = 0.12 X  means that totally there are 12% of the pixels which are added noise. Figure 8 shows some examples of such noise images.

Among the 400 noise images, 200 images are first learned by LSE with k = 7, and the results are illustrated with (red) filled circles. Compared with the results learned from images with no noise, the learned shape is slightly degraded. We treat the rest 200 images as new data points and use GOM and CP to map them into the previously learned shape. In experiment, LSE is employed to calculate the M matrix in Eq. ( 4 ). The results are reported in Fig. 9 aand b, with (blue) filled circles. We see that CP generates almost the same result as that generated by GOM. We also test different proportions of pixels with noise. It can be summarized that when  X  X Ratio X  is greater than 0.2, none of Isomap, LLE, Laplacian eigenmap, HLLE, LTSA and LSE can learn a satisfactory shape from 200 noise images. Mapping the new data points into a distorted shape will also generate distorted result. Thus learning from data with very large amount of noise is a challenging task.

Finally, for comparison, we employ KLPP [ 8 ] to deal with the above four data sets used kernel tricks. Intrinsically, in KLPP, the data is mapped first into a high-dimensional feature space and the mapped data is then linearly projected onto a low-dimensional space. In this process, the data points with nonlinear structure in low-dimensional space may be mapped on a linear structure in a high-dimensional feature space. As a result, they could be unfolded in a lower-dimensional space in way of subspace projection.

In experiments, the Matlab code of KLPP is used 3 and the Gaussian kernel function is employed to construct the weight matrix:
In Fig. 10 a, the first 600 data points in Fig. 4 which are previously learned by LSE are used to train the KLPP model. Based on the learned model, the rest 600 data points are then projected onto 2D space by using the learned KLPP model. In KLPP, the number of nearest neighbors k is set to be 12, which equals to that used in Fig. 4 . The kernel parameter  X  in by manually selecting the trial with the best shape-preserving. Compared with those results reportedinFig. 4 , we see that KLPP fails to unfold the data points in this 3D S-shape.
Figure 10 b illustrates the results obtained by KLPP from the data set used in Fig. 6 .In experiment, the 221 images learned first by LSE are used to train the KLPP model. The learned KLPP model is then used to map the rest 220 images onto 2D space. We also set k = 8. The results in Fig. 10 b are obtained by KLPP with  X  = 400. Here  X  is also manually selected from  X   X  X  0 . 1 , 4000 . 0 ] to obtain a better shape-preserving. As can be seen from Fig. 10 b, most of the data points near the center region are well embedded. But the data points near the border are not well embedded.
 In Fig. 10 c, the same 200 teapot images learned first by LSE in Fig. 7 are used to train the KLPP model. With this model, the rest 200 teapot images are mapped onto 2D space. The results in Fig. 10 c are obtained by KLPP with k = 8and  X  = 400. Here,  X  is also manually selected from the trials with  X   X  X  0 . 1 , 4000 . 0 ] . We see that KLPP keeps well the order relationship between images. But compared with the true shape (a circle), the learned shape is slightly compressed. Figure 10 d reports the results by KLPP with k = 5and  X  = 400 . 0 from the 400 noise teapot images used in Fig. 9 . We see that in this experiment the order relationship between image is also well maintained.
 KLPP performs well on the teapot sets with high dimensionality. However, in experiments KLPP has a parameter  X  in Eq. ( 18 ) to be tuned to the data. 4 This parameter  X  may differ from data set to data set, and should be well tuned to the data to achieve satisfactory performances. In contrast, our approach has no such parameter to be determined from the data, which may facilitate real-world applications. 4.2 Applications In this subsection, we show some novel applications of our method to image data processing for vision computing. The work setting can be described as follows. Suppose we are given a group of images of an object. We want to know from which direction each of the images is taken by the camera. The motivation is that with known camera directions it is more easy to align the objects or reconstruct the three-dimensional shape from images. In the absence of prior knowledge, however, directly estimating the camera directions from images is proven to be a difficult task. Another example is face pose estimation. Actually, it is well known that most existing face recognition systems can generate higher recognition accuracy from frontal face images. However, in most real-world applications, the person is free of the camera and the system may receive face images with different poses. Thus estimating the face pose is considered to improve the robustness of the face recognition system. It is also difficult to estimate the poses from face images. However, such data points are proven to be located on a high-dimensional manifold. We use this knowledge to treat these problems.
 Here we take one data set with known camera directions or face poses as a reference. Our basic idea is to transfer the known directions or poses (parameters) to another object or person, by exploring and exploiting the manifold structure hidden in the data. The problem X or pose of each data point in X 1 is known; (2) the first l data points in X 2 are previously stored in one database and the rest n 2  X  l data points are obtained later; and (3) without loss is summarized for the above work settings. For the third information, the correspondences can be manually labeled in advance since the first l data points in X 2 have been stored in the database.
 The computation steps are summarized as follows:
Algorithm 2: transfer information to new data points (1) Reduce the dimensionality of data points in X 1 to d , and obtain a lower-dimensional manifold embedding: Y 1 ={ y 1 i } n 1 i = 1  X  R d . (2) Reduce the dimensionality of the first l data points in X 2 to d , and obtain a lower-dimensional manifold embedding: { y 2 i } l i = 1  X  R d . (3) For data points in X 2 , treat the rest n 2  X  l data points in X 2 as new data points, obtain their lower-dimensional embedding coordinates: { y 2 i } n 2 i = l + 1 . (4) Construct d splines for mapping d coordinate components, according to the method introduced in Subsection 3.2 , based on the known r correspondences: { y 2 i  X  y 1 i } r i = 1 . namely, y 2 i  X   X  y 2 i , i = l + 1 ,..., n 2 . neighbors (1-NN) in Y 1 ={ y 1 i } n 1 i = 1 to it.

Note that when we use step 4 to estimate d splines, we only need to replace t j in Eq. ( 14 ) by y 2 j and replace f j by y 1 j .

Figure 11 shows an example of camera direction estimation from images. The images of two objects (duck and wedge) are taken from the COIL-20 database [ 34 ]. Each object contains 72 images taken from different directions. Each image is resized to be 64  X  64 pixels. We assume that the camera directions of the duck images are all known, which are used as references to estimate the camera directions of the wedge images.

The 72 duck images are first learned by LSE with k = 7and d = 2. Figure 11 a shows the For the wedge images, we take 60 images as original data points which are learned by LSE with k = 7, and take the rest 12 images as new data points to be mapped. The results in Fig. 11 b are obtained by GOM, while the results in Fig. 11 c are obtained by CP. The 12 new wedge images are illustrated near their positions, labeled as circles in Fig. 11 bandc.We see that GOM and CP generate similar results. Here the results learned by CP are selected to estimate the camera directions. In this way, we finish the Steps 2 and 3 of Algorithm 2, and points: { y 2 i } 72 i = 61  X  R d .

To estimate two splines by performing the Step 4 in Algorithm 2, r = 18 pairs of duck and wedge images are uniformly selected and manually labeled as the pairs having the same camera directions. After two splines are constructed, they are used in turn to map the seen, the shape in Fig. 11 c is approximately mapped to that in Fig. 11 a. Thus, we finish the Step 5 of Algorithm 2. ( a ) (b) (c) same camera directions are shown in Fig. 11 f. From the poses we observe that the camera directions of the new wedge images are evaluated.

Figure 12 shows an example for face pose estimation. The images of two subjects are used from the pose database [ 17 ]. For each subject, we use a subset, which contains 65 images with horizontal and vertical pose angles from  X  60  X  to 60  X  . Each face image is resized to be 50  X  35 pixels. The first subject is taken as the reference subject. The experimental steps and parameters ( k and d ) are exactly the same as those used in Fig. 11 . The results learned by LSE from 65 face images of the first person are shown in Fig. 12 a. For the second subject, ( a ) (f) we take 52 images as original data points, which are learned by LSE with k = 7, and take the rest 13 images as new data points to be mapped. The results with GOM and CP are illustrated in Fig. 12 b and c, respectively. We can see that almost the same results are obtained from these two methods. Furthermore, we use the results learned by CP to finish the task of pose estimation. We labeled 26 pairs of face images from two subjects to construct two splines. With these two splines, the data points in Fig. 12 c are mapped to be those in Fig. 12 d. Figure 12 e shows together the data points, including those in Fig. 12 a and those in Fig. 12 d. The estimated face images with similar poses are illustrated in Fig. 12 f. The poses of the face images of the second subject (see the second row in Fig. 12 f) are previously unknown. Now they are assigned as the same poses as those in the first row. 4.3 Computation complexity Both GOM and CP require to calculate M UU and M UL in Eq. ( 4 ). Differently, in GOM we need to solve d QP problems (actually via solving linear equations). The computation systems formulated as Eq. ( 17 ). The computation complexity of SVD is O (( k + 1 ) 3 ) , while Compared with O ( d ( n  X  l ) 3 ) in GOM method, the computation complexity in CP is only linear to the number of new data points. In GOM, The computational complexity is determi-ned by the number of new data points ( n  X  l ) and the lower-dimensionality d , while in CP it is depended on ( n  X  l ) , d and the number of neighbors k . In real applications, for given ( n  X  l ) , d and k , when the complexity in GOM is less than that of CP, we can consider to use GOM to solve the out-of-sample task, for example, when the number of new data points is less than 500, GOM may take less time than CP. In contrast, when there are thousands of new data points or more data points to be treated, the iterative algorithm can be employed to solve out-of-sample problem.

In most experiments, the real performance of CP is convergent in several iterations. Accor-during iterating Eq. ( 12 ). This is reasonable since the data points are assumed to be well (densely) sampled in manifold learning and the new data points are well assumed to be sam-pled near the previously treated data points. Thus we can get a good prediction to each new data point along the manifold via subspace projection and spine interpolation. 5 Related work on semi-supervised manifold learning A parallel work related to out-of-sample extension for manifold learning is the semi-supervised manifold learning [ 15 , 19 , 52 ]. In a semi-supervised framework, the coordinates of some landmark points are provided to constrain the shape to be learned. The landmark points are usually provided according to prior knowledge about the manifold shape or simply given by hand. The goal is to obtain good embedding results via a small number of landmark responding algorithm runs in a batch mode. In contrast, out-of-sample extension starts with a known manifold shape which is learned from the original data points, and focuses on how to map the new data points, saying, in a dynamic setting which are collected sequentially. In this process, the coordinates of previously learned data points can maintain unchanged. 6Conclusion We have introduced an approach to out-of-sample extension for NLDR. We developed the global optimization model and gave the coordinate propagation algorithm. Promising expe-rimental results have been presented for 3D surface data and high-dimensional image data, demonstrating that the framework has the potential to map effectively new data points to the previously learned manifold, and has the potential to use the new points to extend an incomplete manifold shape to a full manifold shape. Applications in camera direction estimation and face pose estimation from images are also reported. In the future, we would like to automatically introduce the edge information about the manifold shape from the data points so as to obtain a low-dimensional embedding with better shape-preserving. References Author Biographies
