 A query is unlively if it always returns an empty answer. Debugging a database schema requires not only determining unlively queries, but also fixing them. To the best of our knowledge, the existing methods do not provide the designer with an explanation of why a query is not lively. In this paper, we propose a method for computing explanations that is independent of the particular method used to determine liveliness. It provides three levels of search: one explanation, a maximal set of non-overlapping explanations, and all explanations. The first two levels require only a linear number of calls to the underlying method. We also propose a filter to reduce the number of these calls, and experimentally compare our method with the best known method for finding unsatisfiable subsets of constraints. H.2.1 [Database Manageme nt]: Logical Design  X  schema and subschema . Keywords: Database schema validation, query liveliness. Queries that are not lively [2, 4] are meaningless since they will always have an empty extension. Debugging a database schema requires not only determining unlively queries, but also fixing them. To make that task easier it is required to provide the designer with some explanation of why a query is not lively. However, as far as we know, ex isting methods for liveliness checking in databases do not provide such explanations. An explanation is understood as the minimal set of constraints that are responsible for the non-liveliness of the tested query. In general, there may be more than one explanation for a certain liveliness test. Explanations are intended to help the database designer to find the problem and to fix it. A query may be non-lively due either to (1) its own definition (it may contain a contradiction) or to (2) the integrity constraints that prevent the query to ever have any instance. Assuming that all the tables and views mentioned by the query have already been checked lively, the first cause of unliveliness is easy to fix since the designer has to 2. BACKGROUND phase_1 ( Q : predicate, S = ( DR , IC ): schema): explanation set of deductive rules and IC a finite set of constraints. A deductive rule has the form: and a constraint the denial form (states what may not happen): are either variables or constants. Each C i is a built-in literal in the positive and negative ordinary literals (those that are not built-in). Every rule and constraint must be safe , i.e., every variable appear in the head of a rule are derived predicates (views and queries). The rest are base predicates (tables). A database violates ground substitution  X  . We assume that we have a procedure isLively to perform liveliness tests of queries. Therefore, a liveliness test is a call to otherwise. D EFINITION 3.1. An explanation E for a liveliness test isLively ( Q , S = ( DR , IC )) that returns false is a minimal subset of constraints from S such that considering only these constraints the tested predicate Q is still not lively, i.e., isLively ( Q , S X  = ( DR , E )) returns false too. In summary, the backward approach obtains explanations by discarding successively those cons traints that are not included in any explanation. Liveliness tests are performed with a decreasing number of constraints, starting from the initial full set. The approach consists of three phases . Phase 1 gets one explanation. Phase 2 gets a maximal set of disj oint explanations, including that from the previous phase. Finally, Phase 3 gets all remaining explanations, which must overlap with the ones already found in the two previous phases. Phases 1 and 2 perform a linear number of calls to isLively , with respect to the number of constraints in the schema. Instead, Phase 3 requi res an exponential number of such calls. Figure 1 shows the form alization of the three phases. For the sake of example, let us assume that Q is as follows: Let us also assume that S is a database schema containing the following constraints, labeled as c1 , c2 , c3 and c4 , respectively:  X  T ( X , Y , Z )  X  Z = 2 ( c1 )  X  R ( X , Y , Z )  X  Y &gt; X ( c2 )  X  R ( X , Y , Z )  X  X &gt; 5 ( c3 )  X  V ( X , Y , Z )  X  Z &lt; 10 . ( c4 ) Let us call phase_1( Q , S ), with S = { c1 , c2 , c3 , c4 } to find one of these three explanations. If we assume that the constraints are considered in the order that they were listed above, c1 is considered first. Since isLively ( Q , { c2 , c3 , c4 }) returns false, c1 is discarded. Constraint c2 is considered next. Since isLively ( Q , { c3 , and remove them from the original schema. In this example, there are two possibilities: (1) remove { c1 , c3 } and (2) remove { c1 , c4 }. Let us consider the first option. In this case, isLively ( Q , { c2 , c4 }) returns true and, thus, no further explanation can be found. In contrast, if we consider th e second option, we get that isLively ( Q , { c3 , c2 }) returns false. Therefore, we can still find further explanations. Next, we call phase_1( Q , { c3 , c2 }), which returns a new explanation: { c3 , c2 }. Clearly, phase_2( Q , { c3 , c2 }, { c3 , c2 }) will return {{ c3 , c2 }} as a new set of explanations. As we have found new explanations, we mu st repeat the process taking now into account all the explanations discovered so far. This time, there are four possible ways of removing one constraint from each explanation: (1) remove { c1 , c2 , c3 }, (2) remove { c1 , c2 , c4 }, (3) remove { c1 , c3 } and (4) remove { c1 , c3 , c4 }. After trying all four, we conclude that there are no further explanations and, thus, Phase 3 ends. The final outcome is {{ c3 , c4 }, { c1 }, { c3 , c2 }}. The filter described in this s ection consists in detecting those candidates that contain some constraint that we can ensure it is not relevant for the liveliness test. We can say that a constraint is not relevant for the test when to get a fact about the tested predicate it is not required to have also a fact about all the positive ordinary predicates in the constr aint. The idea is that we do not need to perform the liveliness test for these candidates. In the backward approach, the filter can be applied along the phase 1 (which is called also from phases 2 and 3). First, before starting the phase, we can remove the constraints that are already non-relevant for the test over the original schema. Then, during Phase 1, when we remove a constraint c from the schema, we can recompute what predicates are relevant for the test when this is performed considering only the remaining constraints. If some of these remaining constraints are not relevant, we can also remove them before calling isLively . However, if the test says that now the predicate is lively, we will have to put back the constraint c together with the non-relevant ones into the schema. To characterize formally the constraints that are relevant for a certain liveliness test, we are going to assume that each constraint is reformulated as a rule defining a derived predicate IC i in such a way that the constraint is violated when IC i is true in the database. We will also assume that deductive rules have no recursion. Let Q be a generic derived predicate defined by k rules with the following pattern: Q ( X  X  )  X  P j 1 ( X  X  1 )  X  ...  X  P follows: neg_preds ( Q ) = {{ S {{ neg_preds ( P j R is a base predicate. There will be two types of relevancy: p-relevancy and q-relevancy . The p-relevant predicates will be those that in order to build a database where P is intended to be lively, it may be required to insert some fact about them in that database. The q-relevant predicates will be the derived predicates such that although it is not explic itly required for them to be lively in order to make P lively, they may become lively as a result of the facts inserted in the database. D EFINITION 3.2. Assuming that we are testing the liveliness of a certain predicate P , we can say the following:  X  Predicate P is p-relevant.  X  If Q is a derived predicate and it is p-relevant, then P i  X  s and 1  X  j  X  k , are also p-relevant predicates. j SVT (Schema Validation Tool) [7]. They are able to deal with database schemas having literals, and comparisons. In the first experiment, we compare the backward approach with its three phases against th e first two phases only, and also with the hitting set dualization [1 ] approach. We used a database schema formed by the following two chains (parameter N controls There is also a constraint in R 1 N stating A 1 N  X  C 1 N , and another one stating C 1 N  X  200 . We are going to check the liveliness of: P  X  R ( X , Y )  X  R 2 1 ( U , V ), for the unliveliness of which there will be three explanations. The first ch ain will provide two of them, which will overlap. The second chain will provide the third explanation. This way, as long as each phase provides one explanation, we will be able to compare them. Table 1 shows the running times for this experiment . As expected, introducing the phase 3 supposes a considerable increment of time, but anyway the backward approach is still much faster than dualization [1]. In the second experiment, we st udy the effect of the filter described in Section 3.2 in reducing the number of tests executions when applied in the backward approach. This time we used a database schema similar to the previous one, but without R
N . C 1 N (so there will be only two disjoint explanations) and with some additions. First, for each table R j i , we added the following chain: R j i,1 ( A 1 , B easily applied in our context. added an additional foreign key to each table R j i that references R i,s .A s . Finally, we added M tables from the relational schema of the Mondial [5] database (out of 28 tables), and connected them We considered the Mondial database schema with its primary key, unique and foreign key constraints. This gives us a scenario containing not only the constraints that form the explanations, but also additional constraints that do not affect the liveliness of the tested predicate, as usually happens with the major schemas. Figure 2 shows the number of calls performed by the backward approach with and without filter , when increasing the number of constraints in the database schema. We used schemas with 30, 58, 95 and 139 constraints, respective ly, which we got by changing the value of N and M . It can be seen how using the filter redu considerably the number of ca lls We have already compared our backward approach with the hitting set dualization approach [1] in Section 4, showing that our backward method is more efficient in the context of explaining query unliveliness in databases. Another significant difference is that our backward method provides three levels of search, requiring the first two only a lin underlying liveliness method. In Description Logics (DL), the axiom pinpointing process described in [6] is similar to our definition of computing explanations. However, since their approach is based on the construction of an ALC-tableau and a minimisation function by means of applying rules that wo rk on unfoldable ALC-TBoxes, it is too much dependant on th
