 In this section we will give proofs for theorems in the UFF section and introduce a robust version of UFF when the measurements have adversarial noise. Proof of Theorem 2 . Consider k + 1 distinct elements as The cardinality of ements of B 0 are chosen independently and uniformly at random from [ m ], we have: is n n  X  1 k . Using union bound, the probability that Algorithm 1 does not return a ( d, k )-UFF is This finishes the proof.
 Proof of Theorem 3 . Let S  X  = supp( x  X  ). We know that | S  X  |  X  k . Wlog, assume that non-zero ele-{ 1 , 2 , , | S  X  |} .
 S . Furthermore, b i  X  = 1 { P rithm 2 ), and it follows that b S  X  S  X  .
 B , we have: b i = 1 { P Therefore, min i  X  B rithm 2 :  X   X  b S . Hence, S  X   X  b S .
 surements no longer satisfy ( 3 ) but are given by is the number of adversarial errors. In the case of the measurement matrix as in ( 2 ) and the following algorithm to reconstruct x  X  .
 Algorithm 8 Support recovery algorithm when A is constructed from a ( d, k,  X  )-UFF input A : measurement matrix,  X  : robustness param-1: b S  X   X  2: for j = 1 , , n do 3: if | supp( b )  X  B j | &gt; | B j | 2 then 4: b S  X  b S  X  X  j } 5: end if 6: end for output b S Theorem 8 shows that Algorithm 8 recovers x  X  even Theorem 8. Suppose x  X   X  R n  X  0 is a vector of non-matrix constructed according to ( 2 ) and the measure-2  X   X  d adversarial errors in the measurement (i.e., b Proof. The proof of this theorem is along lines of the proof for Theorem 3 . Let S  X  = supp( x  X  ). We know that | S  X  |  X  k . Wlog, assume that non-zero elements are in the first k dimensions of x  X  , i.e., S  X  = { 1 , 2 , , | S  X  |} .
 We show b S = S  X  , by first proving b S  X  S  X  and then S  X   X  b S .
 tion 3 ): have b S  X  S  X  .
 i  X  B  X  \ supp (  X  ), we have: .
 In this section we will prove Theorem 5 for which we need the following lemma.
 Lemma 2. With the sensing matrix A constructed as S  X  = supp ( x  X  ) .
 Proof of Lemma 2 . Since | S  X  | &lt; k + 1, we have with at least two neighbors in S  X  .
 So the number of edges between S  X  and N ( S  X  ) is | N 2  X  ) d | S  X  | .
 Proof of Theorem 5 . We first prove S  X   X  b S . Let j  X  Using Lemma 2 with the above inequality we get: Algorithm 3 will add j to b S and hence, S  X   X  b S . k + 1. Using expansion property, (1  X   X  ) d ( | S  X  | + 1) &lt; | N ( S  X   X  X  j } ) | Step 4 of Algorithm 3 will not add j to b S . Hence, b S  X  S  X  .
 Proof of Theorem 6 . Let r = log k, z = P x  X  and z  X  = where the second inequality follows from Stirling X  X  ap-proximation. By union bound, we have rem 3 .
 This section is almost entirely from ( Garg &amp; Khan-ness. Before we present the GraDeS algorithm, we have the following definition: Definition 5. Let H k : R n  X  R n be a function that to zero. More precisely, for x  X  R n , let  X  be a per-mutation of [ n ] such that x  X  (1)  X  x  X  (2)  X   X  x b x Algorithm 9 GraDeS ( Garg &amp; Khandekar , 2009 ) input b z , A 1 ,  X  and  X  1: Initialize b x  X  0 2: while k b z  X  A 1 b x k 2 &gt;  X  do 3: b x  X  H k b x + 1  X  A T 1 ( b z  X  A 1 b x ) 4: end while output b x The following theorem which shows the correctness of Algorithm 9 is a restatement of Theorem 2.3 from ( Garg &amp; Khandekar , 2009 ).
 Theorem 9. Suppose x  X  is a k -sparse vector satisfy-There exists a constant D &gt; 0 that depends only on  X  in at most constant D to be 6 .
 Here we state a theorem from ( Jacques et al. , 2011 ) which guarantees that all unit vectors which agree with the 1-bit measurements obtained from a random Gaus-sian matrix must be very close to each other. Theorem 10 (Theorem 2 of ( Jacques et al. , 2011 )) . Let A  X  R m  X  n be a matrix generated as A  X  of measurements( m ) satisfy: and y : Here we prove Theorem 7 which is a proof of correct-ness of Two-stage algorithm (Algorithm 6 ). Proof of Theorem 7 . We prove the theorem by ana-lyzing both the stages of our algorithm.
 Stage 1 : Let z  X  = A 1 x  X  . As b = sign( A 2 z  X  ), ( a ear programming, we can find a vector b z consistent with the measurements b i.e. b = sign( A 2 b z ). Using Theorem 10 , Stage 2 : In stage 2 of Algorithm 6 , we run GradeS with inputs b z k b where k  X  k 2  X   X  . Also, since A 1 satisfies RIP with  X  That is, k  X  x k 2 2 + k  X  x k 2 k A 1 x  X  k 2 Using the fact that t + 1 /t  X  2 and using RIP, for  X  &lt; 1 4 .
 The following is a lower bound on the reconstruc-tion error of any approximate recovery algorithm from ( Jacques et al. , 2011 ).
 Theorem 11 (Theorem 1 of ( Jacques et al. , 2011 )) . sup
