 The goal of most machine learning problems is to generalize f rom a limited number of of the form w  X  x + b = 0, w  X  R m , x  X  R m , b  X  R is recovered as a decision boundary after observing a limited number of training examples. The p arameters of the hyperplane ( w , b ) are estimated by maximizing the margin (the distance betwe en w  X  x + b = 1 and w  X  x + b =  X  1) while minimizing a weighted upper bound on the misclassifi cation rate on the training data (the so called slack variables). In practi ce, the margin is maximized by While this works well in practice, we point out that merely ch anging the scale of the data can give a different solution. On one hand, an adversary can ex ploit this shortcoming to transform the data so as to give bad performance. More distre ssingly, this shortcoming can naturally lead to a bad performance especially in high di mensional settings. The key problem is that SVMs simply find a large margin solution givin g no attention to the spread of the data. An excellent discriminator lying in a dimension with relatively small data spread may be easily overlooked by the SVM solution. In this p aper, we propose novel formulations to overcome such a limitation. The crux here is to find the maximum margin solution with respect to the spread of the data in a relative sense rather than finding the absolute large margin solution.
 is large while within class scatter is small. However, it onl y makes use of the first and the second order statistics of the data. Feature selection w ith SVMs [12] remove that have low discriminative value. Ellipsoidal kernel machines [9] normalize data in feature space by estimating bounding ellipsoids. While these previous me thods showed performance im-provements, both relied on multiple-step locally optimal a lgorithms for interleaving spread information with margin estimation. Recently, additional examples were used to improve the generalization of the SVMs with so called  X  X niversum X  sa mples [11]. Instead of leverag-ing additional data or additional model assumptions such as axis-aligned feature selection, the proposed method overcomes what seems to be a fundamental limitation of the SVMs and subsequently yield improvements in the same supervised setting. In addition, the for-mulations derived in this paper are convex, can be efficiently solved and admit some useful generalization bounds.
 Notation Boldface letters indicate vectors/matrices. For two vecto rs u  X  R m and v  X  R m , all zeros and the identity matrix respectively. Their dimen sions are clear from the context. Figure 1: Top: As the data is scaled along the x-axis, the SVM s olution (red or dark shade) deviates from the maximum relative margin solution (green o r light shade). Bottom: The projections of the examples in the top row on the real line for the SVM solution (red or dark shade) and the proposed classifier (green or light shade ) in each case. Let us start with a simple two dimensional toy dataset to illu strate a problem with the SVM solution. Consider the binary classification example sh own in the top row of Figure 1 where squares denote examples from one class and triangles denote examples from the other class. Consider the leftmost plot in the top row of Figu re 1. One possible decision boundary separating the two classes is shown in green (or lig ht shade). The solution shown in red (or dark shade) is the SVM estimate; it achieves the lar gest margin possible while still separating both the classes. Is this necessarily  X  X he best X  solution? Let us now consider the same set of points after scaling the x-axis in the second and the third plots. With progressive scaling, the SVM increasingl y deviates from the green solution, clearly indicating that the SVM decision boundary is sensit ive to affine transformations of and affine transformations is worrisome. If there is a best and a worst solution in the family that the SVM solution we recover is poor. Meanwhile, an algor ithm producing the green decision boundary remains resilient to such adversarial sc alings.
 In the previous example, a direction with a small spread in th e data produced a good discriminator. Merely finding a large margin solution, on th e other hand, does not recover the best possible discriminator. This particular weakness in large margin estimation has only received limited attention in previous work. In the abo ve example, suppose each class is generated from a one dimensional distribution on a line with the two classes on two parallel lines. In this case, the green decision boundary should obta in zero test error even if it is estimated from a finite number of samples. However, for finite training data, the SVM solution will make errors and will do so increasingly as the d ata is scaled along the x-axis. Using kernels and nonlinear mappings may help in some cases b ut might also exacerbate such problems. Similarly, simple prepossessing of the data (affine  X  X hitening X  to make the dataset zero mean and unit covariance or scaling to place the data into a zero-one box) may fail to resolve such problems.
 For more insight, consider the uni-dimensional projection s of the data given by the green and are mapped to a single coordinate and all points in the other c lass are mapped to another (distinct) coordinate. Meanwhile, the red solution produc es more dispersed projections of the two classes. As the adversarial scaling is increased, th e spread of the projection in the SVM solution increases correspondingly. Large margins are not sufficient on their own and what is needed is a way to also control the spread of the data af ter projection. Therefore, rather than just maximizing the margin, a trade-off regulari zer should also be used to minimize the spread of the projected data. In other words, we will couple large margin rather relative to the spread of the data in that projection direction. tributed from a distribution Pr( x , y ), the Support Vector Machine primal formulation 2 is as follows: The above formulation minimizes an upper bound on the miscla ssification while maximizing the margin (the two quantities are traded off by C ). In practice, the following dual of the formulation (1) is solved: by Ax i where A  X  R m  X  m , A  X  A = I , then the solution remains the same. However, the solution is not guaranteed to be the same when A is not a rotation matrix. In addition, the solution is sensitive to translations as well.
 Typically, the dot product between the examples is replaced by a kernel function k : R m  X  R space to obtain non-linear decision boundaries in the input space. Thus, in (2), x  X  i x j is Next, we consider the formulation which corresponds to whit ening the data with the covari-sample covariance and mean respectively. Consider the foll owing formulation which we call  X  -SVM: where 0  X  D  X  1 is an additional parameter that trades off between the two re gularization terms.
 The dual of (3) can be shown to be: invariant solution when D tends to one. When 0 &lt; D &lt; 1, it can be shown, by using the Woodbury matrix inversion formula, that the above formulat ion can be  X  X ernelized X  simply by replacing the dot products x  X  i x j in (2) by: where K i is the i th column of K . For D = 0 and D = 1, it is much easier to obtain the kernelized formulations. Note that the above formula invol ves a matrix inversion of size n , making the kernel computation alone O ( n 3 ). 3.1 RMM and its geometrical interpretation From Section 2, it is clear that large margin in the absolute s ense might be deceptive and could merely be a by product of bad scaling of the data. To over come this limitation, as we pointed out earlier, we need to bound the projections of th e training examples as well. As in the two dimensional example, it is necessary to trade off between the margin and the spread of the data. We propose a slightly modified formulatio n in the next section that can be solved efficiently. For now, we write the following form ulation, mainly to show how it compares with the  X  -SVM. In addition, writing the dual of the following formula tion gives some geometric intuition. Since we trade off between th e projections and the margin, implicitly, we find large relative margin. Thus we call the following formulation the Relative Margin Machine (RMM): This is a quadratically constrained quadratic problem (QCQ P). This formulation has one extra parameter B in addition to the SVM parameter. Note that B  X  1 since having a B less than one would mean none of the examples would satisfy y i ( w  X  x i + b )  X  1. Let w
C and b C be the solutions obtained by solving the SVM (1) for a particu lar value of C , inactive for each i and the solution obtained is the same as the SVM estimate. still find a large margin solution such that all the projectio ns of the training examples are bounded by B . Thus by trying out different B values, we explore different large margin solutions with respect to the projection and spread of the da ta.
 In the following, we assume that the value of B is smaller than the threshold mentioned above. The Lagrangian of (5) is given by: 1 2 k w k 2 + C  X   X  1  X  where  X  ,  X  ,  X   X  0 are the Lagrange multipliers corresponding to the constra ints. Differ-entiating with respect to the primal variables and equating them to zero, it can be shown that: ( I + the dual of (5) can be shown to be:  X   X  corresponds to a  X  X hape matrix X  (potentially low rank) dete rmined by x i  X  X  that have non-zero  X  i . From the KKT conditions of (5),  X  i ( 1  X  Geometrically, in the above formulation (6), the data is whi tened with the matrix ( I +  X   X  ) while solving SVM. While this is similar to what is done by the  X  -SVM, the matrix ( I +  X   X  ) is determined jointly considering both the margin of the dat a and the spread. In contrast, in  X  -SVM, whitening is simply a prepossessing step which can be d one independently of the the expense of one additional parameter however this will no t be investigated in this paper. The proposed formulation is of limited use unless it can be so lved efficiently. Solving (6) amounts to solving a semi-definite program; it cannot scale b eyond a few hundred data points. Thus, for efficient solution, we consider a different b ut equivalent formulation. constraints : ( w  X  x i + b )  X  B and  X  ( w  X  x i + b )  X  B . With these constraints replacing the quadratic constraint, we have a quadratic program to sol ve. In the primal, we have 4 n constraints (including  X   X  0 ) instead of the 2 n constraints in the SVM. Thus, solving RMM as a standard QP has the same order of complexity as the SVM. In the next section, we briefly explain how the RMM can be solved efficiently from the du al. 3.2 Fast algorithm The main idea for the fast algorithm is to have linear constra ints bounding the projections rather than quadratic constraints. The fast algorithm that we developed is based on SVM light [5]. We first write the equivalent of (5) with linear constrai nts: The dual of (7) can be shown to be the following: where, the operator  X  denotes the element-wise product of two vectors.
 The above QP (8) is solved in an iterative way. In each step, on ly a subset of the dual for the other two indices) in a particular iteration. Then th e optimization over the free variables in that step can be expressed as: max which it is optimized), the second term is only linear.
 The algorithm, solves a small sub-problem like (9) in each st ep until the KKT conditions We omit the details due to lack of space. Since only a small sub set of the variables is optimized, book-keeping can be done efficiently in each step. Moreover, the algorithm can be warm-started with a previous solution just like SVM light . Experiments were carried out on three sets of digits -optica l digits from the UCI machine learning repository [1], USPS digits [6] and MNIST digits [7 ]. These datasets have different number of features (64 in optical digits, 256 in USPS and 784 i n MNIST) and training examples (3823 in optical digits, 7291 in USPS and 60000 in MN IST). In all these multi-class experiments one versus one classification strategy wa s used. We start by noting that, on the MNIST test set, an improvement of 0.1% is statisticall y significant [3, 4]. This corresponds to 10 or fewer errors by one method over another o n the MNIST test set. All the parameters were tuned by splitting the training data in each case in the ratio 80:20 and using the smaller split for validation and the larger spl it for training. The process was repeated five times over random splits to pick best parame ters ( C for SVM, C and D for  X  -SVM and C and B for RMM). A final classifier was trained for each of the 45 classification problems with the best parameters found from cross validation using all the training examples in those classes.
 In the case of MNIST digits, training  X  -SVM and KLDA are prohibitive since they involve inverting a matrix. So, to compare all the methods, we conduc ted an experiment with 1000 examples per training. For the larger experiments we simply excluded  X  -SVM and KLDA. The larger experiment on MNIST consisted of training with tw o thirds of the digits (note that this amounts to training with 8000 examples on an averag e for each pair of digits) for each binary classification task. In both the experiments, th e remaining training data was used as a validation set. The classifier that performed the be st on the validation set was used for testing.
 Once we had 45 classifiers for each pair of digits, testing was done on the separate test set available in each of these three datasets (1797 examples in t he case of optical digits, 2007 examples in USPS and 10000 examples in MNIST). The final predi ction given for each test example was based on the majority of predictions made by the 4 5 classifiers on the test example with ties broken uniformly at random.
 Table 1 shows the result on all the three datasets for polynom ial kernel with various degrees and the RBF kernel. For each dataset, we report the number of m isclassified examples using the majority voting scheme mentioned above. It can be seen th at while  X  -SVM usually performs much better compared to SVM, RMM performs even bett er than  X  -SVM in most cases. Interestingly, with higher degree kernels,  X  -SVM seems to match the performance of the RMM, but in most of the lower degree kernels, RMM outper forms both SVM and  X  -SVM convincingly. Since,  X  -SVM is prohibitive to run on large scale datasets, the RMM was clearly the most competitive method in these experiment s.
 Training with entire MNIST We used the best parameters found by crossvalidation in the previous experiments on MNIST and trained 45 classifie rs for both SVM and RMM with all the training examples for each class in MNIST for various ker nels. The test results are reported in Table 1; the advantage still carries over to t he full MNIST dataset. Figure 2: Log run time versus log number of examples from 1000 to 10000 in steps of 1000. Table 1: Number of digits misclassified with various kernels by SVM,  X  -SVM and RMM for three different datasets.
 Run time comparison We studied the empirical run times using the MNIST digits 3 vs 8 and polynomial kernel with degree two. The tolerance was se t to 0 . 001 in both the cases. The size of the sub-problem (9) solved was 500 in all the cases . The number of training examples were increased in steps of 1000 and the training tim e was noted. C value was set at 1000. SVM was first run on the training examples. The val ue of maximum absolute B a log-log plot comparing the number of examples to the run tim e in Figure 2. Both SVM and RMM have similar asymptotic behavior. However, in many c ases, warm starting RMM with previous solution significantly helped in reducing the run times. We identified a sensitivity of Support Vector Machines and ma ximum absolute margin cri-teria to affine scalings. These classifiers are biased towards producing decision boundaries that separate data along directions with large data spread. The Relative Margin Machine was proposed to overcome such a problem and optimizes the pro jection direction such that the margin is large only relative to the spread of the data. By deriving the dual with quadratic constraints, a geometrical interpretation was a lso formulated for RMMs. An im-plementation for RMMs requiring only additional linear con straints in the SVM quadratic program leads to a competitively fast implementation. Expe riments showed that while affine transformations can improve over the SVMs, RMM performs eve n better in practice. problems handled by the SVM framework such as ordinal regres sion, structured prediction etc. These are valuable future extensions for the RMM. Furth ermore, the constraints that bound the projection are unsupervised; thus RMMs can readil y work in semi-supervised and transduction problems. We will study these extensions i n detail in an extended version of this paper.
 In this section, we give the empirical Rademacher complexit y [2, 8] for function classes used by the SVM, and modified versions of RMM and  X  -SVM which can be plugged into a generalization bound.
 Maximizing the margin can be seen as choosing a function f ( x ) = w  X  x from a bounded class of functions F E := { x  X  w  X  x | 1 the projection on the training examples as in (5), we conside r bounding the projections Note that if we have an iid training set, it can be split into two parts and one part can be used exclusively to bound the projections and the other part can be used exclusively for classification constraints. Since the labels of the example s used to bound the projections now consider the following function class which is closely r elated to RMM: H E,D := { x  X  w and small bound on the projections. Similarly, consider: G E,D := { x  X  w  X  x | 1  X  -SVM. The empirical Rademacher complexities of the three cl asses of functions are as below: R ( F E )  X  U F R ( H E,D )  X  U H where  X  D = I + D n upper bound is not a closed form expression, but a semi-defini te optimization. Now, the upper bounds U F  X  R ( F ) to obtain Rademacher type generalization bounds.
 Theorem 1 Fix  X  &gt; 0 , let F be the class of functions from R m  X { X  1 } X  R given by Pr
D [ y 6 = sign ( g ( x ))]  X   X   X  1 /n + 2 are the so-called slack variables.
