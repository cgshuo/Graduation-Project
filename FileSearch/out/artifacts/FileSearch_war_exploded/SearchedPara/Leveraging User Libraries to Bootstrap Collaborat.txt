 We introduce a novel graphical model, the collaborative score topic model (CSTM), for personal recommendations of textual documents. CSTM X  X  chief novelty lies in its learned model of individual libraries, or sets of documents, associ-ated with each user. Overall, CSTM is a joint directed prob-abilistic model of user-item scores (ratings), and the textual side information in the user libraries and the items. Creating a generative description of scores and the text allows CSTM to perform well in a wide variety of data regimes, smoothly combining the side information with observed ratings as the number of ratings available for a given user ranges from none to many. Experiments on real-world datasets demonstrate CSTM X  X  performance. We further demonstrate its utility in an application for personal recommendations of posters which we deployed at the NIPS 2013 conference.
With the advent of online services and the wealth of infor-mation made accessible through them systems with the abil-ity to filter the relevant from the irrelevant, such as recom-mendation systems, are becoming ubiquitous. Collaborative filtering (CF) models have rapidly established themselves as the de facto standard for many recommendation tasks where user-item preferences, scores or ratings, are available. In ad-dition to such preferences, side information about users or items, for example other information that may be collected by an online service, is often available. Side-information has been particularly useful to address the cold-start prob-lem that plagues collaborative filtering systems. Cold-start refers to a regime where scores for a set of users or items are unavailable or scarce. In some cases side information has also been shown to improve on the performance of collabo-rative filtering models in non-cold start ( warm-start ) data regimes [7, 20].  X  W ork done primarily while author was at the University of Toronto.

In this paper we are interested in the task of document recommendation using both user-item preferences and side information. The primary novelty of our work lies in lever-aging a particular form of side information: the content of documents associated with users, which we call user li-braries . A typical scenario that can be modelled in this way is scientific-paper recommendation for researchers; for example, Google Scholar recommends papers based on an individual X  X  profile. A second scenario is paper-reviewer as-signment, where each reviewer X  X  previously published papers can be used to assess the match between their expertise and each submitted paper. Another relevant application domain is book recommendation, as online book merchants typically allow users to collect items in a virtual container akin to a personal library. 1 In each case a user X  X  library, or side in-formation, consists of documents which are not necessarily explicitly rated but nonetheless likely contain information about a user X  X  preferences.

To model user-item scores as well as user and item con-tent we introduce a novel directed graphical model. This model uses twin topic models, with shared topics, to model the side information. User and item topic proportions are then used as features to predict user-item scores with a col-laborative filtering model. The collaborative filtering com-ponent allows the model to effectively make use of the side information with varying amounts of observed scores. We demonstrate empirically that the model outperforms sev-eral others on three datasets in both cold and warm-start data regimes. We further show that the model automati-cally learns to gradually trade off the use of side information in favor of information learned from user-item scores as the amount of user preference-data increases.
Our approach to document recommendation relies on hav-ing: a) a set of observed user-item preferences ( { r ud } ); b) contents of the items ( { w s d } ); and c) the content of user-libraries ( { w a u } ). The model X  X  aim is to utilize the content in its user-item score predictions (which can then be used to recommend items to users). This contrasts with standard CF, which is not content-based.

Our content-based model is mediated by topics: we learn a shared topic model from the words of the documents and user libraries. We represent topic proportions with a normal
F or example., Amazon X  X  Kindle and Kobo X  X  tablets have an option for users to populate their libraries, while Barnes and Nobles X  Nook gives users an active shelf . distribution and realized topics z u a nd z d using the logistic normal [5]. User and item topic proportions offer a compact representation of user and item side information. We use these representations as covariates in a regression model to predict user-item preferences. The regression has two sets of parameters. The first are user-specific parameters on the item-topics covariates (  X  u ). The second are compatibility parameters , which are shared across users and items, and are based on the compatibility between the item topics and the topics of the user library (  X  ).

We now introduce the complete graphical model for this collaborative score topic model (CSTM) . A graphical repre-sentation of the model is given in Figure 1. The associated generative model is:  X  Draw compatibility parameters:  X   X  N ( 0 ,  X   X  I )  X  Draw shared-user parameters:  X  0  X  N ( 0 ,  X   X  0 I )  X  For each user u = 1 . . . U :  X  For each document d = 1 . . . D :  X  For all of user u  X  X  user-library words, n = 1 . . . N :  X  Repeat the above for all of document d  X  X  M words  X  For each user-document pair ( u, d ), draw scores: where N (  X ,  X  2 ) represents a normal distribution with mean  X  and variance  X  2 ,  X  stands for the elementwise product,
The specific parametrization of the preference regression is important. Our model is designed to perform well in both cold-start and warm-start data regimes. In cold-start set-tings the model needs the user X  X  side information to predict user-item preferences. When the amount of observed pref-erences increases the model can gradually leverage that in-formation, smoothly combining it with information gleaned from the side information to refine its model of missing pref-erences. To accomplish this, the regression model is sepa-rated in two parts: one part that exploits user side informa-tion (( a u  X  s d ) T  X  ) and another that does not ( s T
Item side information is incorporated by modulating the user information through an element-wise product. The weights  X  then serve several purposes: 1) they can act to amplify or reduce the effect of certain topics (for example diminish the influence of topics bearing little preference in-formation); 2) they allow the model to more easily calibrate its output to the range of observed preference values; 3) changing the magnitude of  X  allows the model to control how much it uses the side information for preference predic-tion.

When user-item preferences are more abundant, the model can use them to learn a user-specific model,  X  u , over item features. Note that these user-specific parameters are com-bined with a shared set of parameters,  X  0 , which allows for
F or simplicity, we X  X l assume in the notation that all user-libraries contain N words and all item documents contains M words. s ome transfer across users. The individual X  X   X  u can be used to increase that user X  X  reliance on user-item preferences at the possible expense of item side information, as the joint magnitude of the  X   X  X  defines the weights associated with this part of the model.

Our model learns a single set of topics to model user and item content. Sharing topics ensures that the user and item representations ( a u and s d  X  u,  X  d ) are aligned and render their element-wise product meaningful.
Simplifying the proposed CSTM model in various ways produces other models that have been used for similar tasks. First, setting  X  0 and  X  u  X  u to zero and  X   X  X  to a vector of ones we obtain a version of a language model (LM) first used by [12] for a similar task. 3 [12] and [7] used the LM in a preference prediction task and found its performance particularly strong in low-data regimes.

Further, setting  X  and  X  0 to zero we obtain an individual user regression model (LR), which was shown to outperform purely collaborative filtering models in a similar preference prediction task with textual side information [7].
By modelling preferences as a combination of user features and item features, our model can also be seen as an instance of collaborative filtering. Collaborative filtering models have proven to be extremely powerful for missing preference pre-diction problems [15, 14, 3].

Finally, we have opted to represent topic proportions using a normal distribution instead of the more standard simplex representation used in latent Dirichlet allocation (LDA) [6]. This parametrization was proposed for correlated topic mod-
On e difference with our model is that since we represent users and items in topic space we do not have to handle nor-malization nor smoothing issues which are typical of word-space models. els [5]; in our case we utilize the logistic normal due to its r epresentational form, and not as a means of learning topic correlations. 4 Compared to a multinomial the normal dis-tribution adds a level of flexibility that may be useful to better calibrate CSTM X  X  preference predictions; the draw-back is additional complexity in model inference.
For learning we use a version of the EM algorithm where we alternate between updates of the user-item specific vari-and updates of the parameters or shared variables ( X  = {  X  0 ,  X  ,  X  } ) in the M-step. The inference and learning pro-cedures are similar to those proposed for nonconjugate LDA models in [20]. The general EM algorithm is shown in Al-gorithm 1.
 Inference in this model being intractable, we must rely on approximations when manipulating the posterior over the user-item specific varaibles. The log-posterior over user-item variables, given the fixed model parameters and the data, is
L :=  X  1 where O stands for the set of observed preferences and Z ( X ) is an intractable normalizing constant of the posterior, in part because a u and s d cannot be analytically integrated out since they are not conjugate to the distribution over topic assignments [5].

We address this computational issue by employing varia-tional approximate inference [11]. For the topic-proportion and regression variables { a u } , { s d } , {  X  u } , we use a Dirac delta posterior parameterized by its mode { b a u } , { b s For the topic-assignment variables { z a } , { z s } , we instead utilize a mean-field posterior. The full approximate pos-terior is thus: where  X   X  ( x ) is the delta function with mode  X  and {  X  are the mean-field parameters (e.g.,  X  a u is a matrix whose en-tries  X  a unj are the probabilities that the n th word in user u  X  X  library belongs to topic j ).
S ince learning topic correlations has been found to improve on standard LDA, it is possible that learning the topic cor-relations could also improve our model.

Approximate inference entails finding the variational pa-rameters { b a u } , { b s d } , { b  X  u } , {  X  a u } , {  X  s divergence with the true posterior
KL :=  X  E q [ L ]  X  H( q ) = 1 + 1  X   X  + constant . (2) Our strategy is to perform one pass of coordinate descent, optimizing each set of variational parameters given the oth-ers. 5
For b  X  u , we obtain a closed-form update by differentiating the above equation and setting the result to 0: where O ( u ) is the set of indices for documents that user u has rated. The { b a u } , { b s d } parameters do not have closed-form solutions, hence we resort to optimization using conjugate gradient descent. We report the derivatives with respect to the posterior KL:  X  KL  X  KL where,  X  r ud = ( b a u  X  b s d ) T  X  + b s T d (  X  0 + b  X  For the mean-field parameters {  X  a u } , {  X  s d } , minimizing the KL while enforcing normalization leads to the following so-lutions: We update the variational parameters of all users and sub-sequently of all documents (see Algorithm 1).
Wh ile we could cycle through all variational parameters until convergence before beginning the M-step, we X  X e found a single pass of updates per E-step to work well in practice. Algorithm 1 E M for the CSTM
Input: { w a u } , { w s d } , { r u d }  X  O . while Convergence criteria not met do end while T he M-step aims to maximize the expectation of the com-plete likelihood under the variational posterior (taking into account the prior over the parameters  X  0 ,  X  ):
Setting derivatives to zero (and satisfying the  X  jw param-eters X  normalization constraint), we obtain the following up-dates:
At test time, prediction of missing preferences is made using  X  r ud , which is readily available. That is we use the expectation of the variational variables to form estimates of  X  r
Previous work includes a few models that have combined item-only topic and regression models for user-item prefer-ence prediction. We are not aware of any earlier work that develops a text-based model of a user, nor one that combines user and item side information as in CSTM.

In [2] the authors model several sources of side informa-tion including item textual side information using LDA. The topic assignment proportions of documents ( are used as item features and combined multiplicatively with user-demographic and behavioural features. The result is linearly combined with the other sources of side information to generate preferences.

In [20] the authors also combine LDA with a regression model for the task of recommending scientific articles. Here the item topic proportions are used as a prior mean on normally-distributed item (regression) latent variables. User latent variables are also normally distributed from a zero-mean prior. A specific user-item score is then generated as the inner product of item and user latent variables: r ud a ( s d +  X  d ), where  X  d is drawn from a zero-mean normal. The preference prediction model is the same as the one used in probabilistic matrix factorization [15]. [20] also report that on their data a modified version of their model which is analogous to the model of [2] performed worse. [16] pro-posed a similar model without  X  and used CTM [5]. For a similar application, [18] propose an approach based on link-prediction in a user-item graph based on user and item similarity as well as user (binary) preferences.

The fact that we model an additional type of informa-tion (user textual side information) makes it difficult to di-rectly compare our model to the ones above. In addition, the parametrization we use to predict preferences is very differ-ent from previous models. We initially experimented with a parametrization similar to [20], albeit modified to also model user side information, and found it did not perform as well as CSTM (see Section 5 for an experimental comparison).
Finally, [1] propose a collaborative filtering model with side information. Although the form of the side information is not amenable to using topic models, the authors utilize a combination of linear models to obtain good performance in both cold and warm-start data regimes. We first describe the three datasets used for experiments. We then introduce a set of methods for empirical compar-isons, ranging from pure CF methods to pure side informa-tion methods. We report three separate sets of experiments. In the first we focus on cold-start users and examine the effect of including user libraries. In the second we study how the methods perform on users with varying amounts of observed scores. Finally, we design a synthetic paper rec-ommendation experiment and simulate incoming users in order to test the value of both the user library and the user-provided item scores. We evaluate the models using these three datasets:
Conf-1: A dataset from the 2010 edition of the neural in-formation procession systems (NIPS) conference. Users are conference reviewers while documents are the set of papers submitted to the conference. The dataset consists of 48 users and 1251 documents. Each user X  X  library consists of his/her own previously published papers. Users have an average of 31 documents which are concatenated into one. After some basic preprocessing the length of the joint vocabulary was slightly over 18,000 words. In this dataset all users have ex-pressed scores (integers between 0 and 3) for an average of 143 papers (std. 14). Figure 2: Number of each rating values for the three d atasets, from left to right: Conf-1, Conf-2, Books-1.
 Table 1: A comparison of the modelling capabilities o f each model.  X  X hared Params X  stands for mod-els that share information in-between users and/or items (in other words those which use some form of CF).

Conf-2: A second dataset is from the international con-ference on machine learning (ICML 2012). This dataset consists of 433 users (reviewers) and 861 documents (sub-missions). Users have an average of 25 documents (std. 29) each and the length of the joint vocabulary is 16,201 words. In this dataset the average number of expressed scores per user is 48 (std. 25).

Books-1: The third dataset is from a large North Amer-ican online book retailer. 6 It contains 316 users and 2601 documents (books). Users average 81 documents (std. 100). We removed very-infrequent and very-frequent words (those appearing in less than 1% or more than 95% of all doc-uments). The resulting vocabulary contains 6,440 words. Users have a minimum of 15 expressed scores (mean 22, std. 6).

For each dataset the number of available preference values is shown in Figure 2. We note that the size of our datasets, and not the computational cost of learning in our model, limits our ability to scale up. In fact, learning CSTM on our largest dataset takes on the order of 2 hours on a modern machine using our Matlab implementation.
We introduce several models which will serve as compar-ison to CSTM. Each model has particular characteristics (Table 1) which will help in understanding CSTM X  X  perfor-mance.

Note that we use topic representations of documents for all competing models that use side information. Such rep-resentations were learned using a correlated topic model of-fline [5]. We re-use some of our previous notation to describe these models. Namely A u and S d are K -length vectors which designate a user X  X  and a document X  X  (topic) representation respectively. Ko bo: http://www.kobo.com
Constant: Model predicts the average observed prefer-ences for all missing preferences. Comparison to this base-line is useful to evaluate the value of learning.
LM-I: This model is meant to be a supervised version of the language model (LM[12]):  X  r ud := ( A T u  X  A )( S where, the parameters,  X  A ,  X  S are K  X  F matrices. F is a hyper-parameter determined using a validation set (ranges from 5 to 30).

LM-II: Uses isotonic regression (see for example [4]) to calibrate the LM. The idea is to learn a regression model that satisfies the implicit ranking established by the LM: where the constraints enforce a, user-specific, document or-dering specified by the output of the LM. Once learned {  X  r } are used as the model X  X  predictions. To obtain predictions for an unobserved document we have found that using the average score given to the two (observed) documents ranked directly above and below the new document works well. The performed regression is user-specific and thus cannot be used for users with no observed preferences. For such users we simply re-use the learned parameters of its closest user (based on users X  topic representations). A more princi-pled approach, for example a collaborative one, lies outside of the scope of this paper.

LR: This is a user-specific regression model (see Section 2.1 for details) where predictions are given by: r ud =  X  T u
PMF [15]: PMF is a state-of-the-art collaborative filter-ing approach. PMF X  X  generative model postulates that users and documents live in a low-dimensional latent space, rep-resented respectively by U u and V d . A user-item preference is generated by taking the dot product between the corre-sponding user and item representations: r ud = U T u V d . The size of the latent space is determined using a validation set (range from 1 to 30).

Collaborative topic regression (CTR-CTM): CTR, is ma-trix factorization with document-content model introduced in [20]. CTR was briefly reviewed in Section 4. We use a slightly different version than the one introduced by its au-thors. Namely, we have replaced LDA by CTM. Also, in our application since all user-item scores are given we use a single variance value over scores (  X  ).

For LM-I , LR and PMF learning is performed using MAP by assuming a Gaussian likelihood model and zero-mean Gaussian priors over the model X  X  parameters. The priors X  variance are determined using a validation set.

We investigated a few other models which we do not fully describe here. Of note: instead of modeling user libraries as side-information we used the documents of user libraries as explicitly (highly-)scored items. We experimented with various scoring schemes but none lead to consistent improve-ments over baselines. We also experimented with replacing directed topic models with an supervised extension of an undirected topic model [13]. Further we experimented with replacing both topic models by (unconstrained) probabilis-tic matrix factorization [17]. However, in both cases, initial experiments were not as promising.
To run CSTM on the above datasets we first concatenated user documents (for example a researcher X  X  previously pub-lished papers) into a single document. To get user and item t opic proportions we learned a CTM topic model [5] using the content of the items and then projected user documents into that (learned) topic space to obtain user topic propor-tions. We directly used these topics in LM-1,LM-II,LR. Fur-ther we used these topics as initialization in models which jointly learn topics and scores (CSTM,CTR). In all experi-ments we use 30 topics.
 For training we create 5 folds from the available scores. Each fold is split into 80 percent observed and 20 percent test. We used the first fold to determine the hyper-parameters of the model. We report the average results over the five folds as well as the variance of this estimator.

We want to evaluate the performance of CSTM in set-tings where some users have no observed scores. The cold-start setting is of particular practical importance and one that should allow a good model to leverage the user X  X  side-information. Accordingly, in our datasets we randomly se-lected one fourth of all users and removed all of their ob-served scores but kept their test scores. Further, for Conf-1 and Books-1, whose users have a more uniform number of ratings, we binned the remaining users (three quarters) uniformly into three categories . For Conf-1, users in each category had 15, 30 and 55 observed scores respectively. In each of the three categories 5 ratings per user were kept for validation. For Books-1 users in the first two categories had 8 and 10 scores while the scores of users in the last cat-egory were left untouched (5 scores per user were kept for validation). For Conf-2 since users are already naturally dis-tributed into categories, we split the observed data into 25 percent validation and 75 percent train.

For the next two experiments, for each dataset, we train each model on all of the data but we divide our discussion into two parts. First we discuss cold-start users and after we examine the (other) user categories.
We first report the results for the completely cold-start data regime. For the cold-start users, it is difficult to cal-ibrate the output of the model to the correct score range since only the users X  side-information is available. The hope is that the models can use the side-information to get a better understanding of users preferences and discriminate between items of interest and other items. Accordingly we report results using NDCG . Normalized DCG (NDCG) is a well-established ranking measure, where a value of 1 indi-cates a perfect ranking and 0 a reverse-ordered perfect rank-ing [10]. NDCG@T considers exclusively the top T items. Table 2 reports results for the three datasets using NDCG@5 (note that other values of NDCG gave similar results). We can only report results for the methods that have the ability to predict scores for cold-start users. PMF, LR and CTR do not use any user side-information and hence do not have that ability.

In this challenging setting CSTM significantly outperforms the other methods. Further we see that methods using side-information typically outperform the constant baseline. This demonstrates that the useful information about user preferences can be leveraged from the user libraries. Fur-ther, the good performance of CSTM in this setting shows that the model is able to leverage that information. Table 2: Comparisons between CSTM and competi-t ors for cold-start users using NDCG@5.
The goal of CSTM is to perform well across different data regimes. In the previous section we examined models X  per-formance on cold start users, we now focus on users with ob-served scores. For each dataset we report the performance of the various methods for each user category. For Conf-2 we separated users into roughly equal sized bins according to their number of observed scores. Results for our three datasets are provided in Figure 3. First we note that as the number of observed scores is increased the performance of the different methods also increases. CSTM outperforms all other methods on lower data-regimes. On users with more observed scores CSTM is competitive with both CTR-CTM and LR.

We notice that overall in this task, and even when many observed preferences are available, PMF is not competitive with most of the methods that have access to the side infor-mation. This highlights the value of content side-information on both user and item sides. This is further made clear by the relatively strong performance of both LM-I and LM-II. Overall user libraries do not seem to help as much on the Books-1 dataset. There are several explanations for this. First, in Books-1 the distribution over scores is very skewed toward high scores. Thus a constant baseline does quite well. Further, bag-of-words representations are particularly well suited for academic papers where the presence (absence) of specific words are very good indications of the documents field and hence it X  X  targeted audience. However, in (non-technical) books user preferences also rely on other aspects such as the document X  X  prose which is harder to capture with a bag-of-words assumption.
We explore a different scenario which is meant to be closer to what would happen when a model is deployed in a com-plete recommendation system, for example to guide users to posters of interest in an academic conference. Specifically, we evaluate the performance of CTR and CSTM as new users arrive into the system and gradually provide informa-tion about themselves. We postulate that users first provide the system with their library. Then users gradually express their preferences for certain (user-chosen) items.
We trained CTR-CTM and CSTM on all but 50 randomly-chosen Conf-2 users with enough observed preferences (min. 15). We then simulated these users entering the system one by one. Since this experiment is about recommendations we report the results using NDCG. 7 Figure 4 presents the performance of CSTM and CTR-CTM as a function of the amount of data available in the system. When a user first
I n the absence of a recommendation objective and con-straints it is reasonable to recommend the top-ranked items to each user Figure 3: Test RMSE of the different methods across C onf-1, Conf-2, Books-1. In each figure each group of bars reports results for different subsets of users. Each user is part of a single subset. The x-axis indi-cates the number of training observations per users of a given subset. The subsets correspond to users with the least observed preferences (left) to the most (right). Figures better seen in color (however the or-dering in the legends corresponds to the ordering of the bars in each group). enters the system no data is available about her (indicated by  X 0 X  in the figure). The methods revert to using a con-stant predictor which predicts the mean of the previously observed scores across all users. Once a user provides a library (Lib.) we see that CSTM X  X  performance increases very significantly. CTR cannot leverage that side informa-tion. Then once users provide scores, the performance of both methods increases and the performance of CTR even-tually reaches the performance of CSTM.

Figure 4 demonstrates the advantage of having access to user side-information, namely, the system can quickly give good recommendations to new users. Further, in absolute terms the system performs relatively well without having ac-cess to any scores. It is also interesting to note that, in this experiment, as far as NDCG goes, the performance of CSTM only modestly improves as the number observed scores in-creases. This may be a consequence of our fairly primitive online learning procedure. As far as modelling goes this experiment is also a demonstration that our model of user libraries is effective at extracting features ( a u for all users) indicative of preferences and that the regression model then successfully combines the user and item side information. As a practical experiment we deployed a system based on CSTM to a subset of the attendees of the most recent NIPS conference (NIPS-2013). NIPS is one of the most important machine learning conferences. We had previously gathered a dataset containing a few hundred of attendees X  libraries and ratings. We also obtained text representations of the confer-ence papers. Both sources of information were used to train CSTM. We used 20% of the observed ratings as a validation set used to determine the value of the hyper-parameters. Using the trained model we then generated predictions for each user using the same conditional inference procedure as above. We used each user X  X  highest (predicted) ratings as their personalized recommendations. Furthermore, since NIPS 2013 had four daily poster sessions we allowed users to obtain independent paper recommendations for each day. A screen capture of the online user interface is provided in Figure 5.

We did not have a formal method of evaluating the qual-ity or usefulness of the system beyond using the metrics we discussed in previous sections. Anecdotally, over 200 NIPS attendees accessed their recommendations and user feedback was almost unanimously positive. Furthermore we can ex-plore the learned representations of the model as a way to as-sess its quality. Figure 6, shows the two-dimensional embed-dings of user representations ( a u ) obtained using a popular (non-linear) dimensionality reduction technique for visualiz-ing high-dimensional data [19]. We notice that, even in this low-dimensional representation, users cluster into different groups according to their areas of research. The model has discovered these groups using the similarities in user libraries and in their rating profiles. Similar results were obtained for paper representations.
We have introduced a novel graphical model to leverage user libraries for preference prediction tasks. We showed experimentally that CSTM overall outperforms competing methods and can leverage the information of other users and of user libraries to perform particularly well in cold-start regimes. We also explored a paper recommendation Figure 4: Comparison of CSTM and CTR X  X  N DCG@10 performance on new users as a function of the amount of data provided by users. With-out any user data (0) methods revert to a constant predictor. Then CSTM takes advantage of user li-braries (Lib.). Finally, scores are added one by one. Error bars indicate the variance across users.
 Figure 5: User interface of the NIPS-13 poster rec-o mmendations. The menu on the top of the page al-lows users of the system to obtain recommendations for one of the four daily poster sessions. Below the menu are the top-10 recommendations for a given user. Each recommendation links to the paper and contains the paper title and its unique conference identifier.
 Figure 6: We used t-SNE to obtain a two-d imensional representation of users. Each user is denoted using his or her email address. We note that users cluster according to easily iden-tifiable subject areas some of which we have highlighted. A fully vectorized map is available at http://www.cs.toronto.edu/~lcharlin/tmp/n13_tsne.pdf . task and demonstrated the benefits of having access to user libraries.

Future work offers several possibilities. On one side we could refine the inference procedure used in training our model such as by using a fully variational approach or by leveraging the latest inference procedures of CTM [21]. Fur-thermore, it would also be straightforward to implement stochastic variational inference [9] for example by sampling users and updating relevant document and global parame-ters using a natural gradient. Stochastic inference is likely to be especially useful as we scale CSTM to very large datasets. On the other side we are examining extensions that enable the modelling of other types of side-information, such as book genres or academic-paper subject areas.

Another aspect of practical importance is that once we move to online recommendation, models must also be able to adapt to new data, including novel items and users, updates to user libraries, and new user-item scores. In the poster recommendations experiment we have seen that a simple conditional inference method works relatively well for novel users. However, one would also like to use the information from novel users to learn better representations of all users. In other words, we would need a mechanism which updates model parameters once a sufficient amount of new data is available. Furthermore, we could refine such a method inter alia to allow the system to adapt to the evolving preferences of users over time. For example, [1] propose a decaying mechanism to emphasize more recent scores over older ones. A similar mechanism could be use to weight the different documents in a user X  X  library (for example based on date of publication for research papers or purchase date for books).
There is also the question of other potential applications for which CSTM could be useful. In addition to modelling text, topic models have also been shown to model images [8]. CSTM could then be used as an image recommendation tool (for example to photographers). In that case, much like f or the books of the Books-1 dataset, it remains to be seen whether topic models can capture features of images which are indicative of preferences.
 Acknowledgments: We thank the NIPS X 10 and ICML X 12 program chairs for allowing us to use their data as well as Kobo for providing us with a useful dataset and support. We also acknowledge the support of CIFAR and NSERC. [1] D. Agarwal and B.-C. Chen. Regression-based latent [2] D. Agarwal and B.-C. Chen. flda: matrix factorization [3] R. M. Bell and Y. Koren. Lessons from the netflix [4] M. J. Best and N. Chakravarti. Active set algorithms [5] D. M. Blei and J. D. Lafferty. A correlated topic [6] D. M. Blei, A. Y. Ng, M. I. Jordan, and J. Lafferty. [7] L. Charlin, R. Zemel, and C. Boutilier. A framework [8] P. P. E. Bart, M. Welling. Unsupervised organization [9] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. [10] K. J  X  arvelin and J. Kek  X  al  X  ainen. Ir evaluation methods [11] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and [12] D. M. Mimno and A. McCallum. Expertise modeling [13] R. Salakhutdinov and G. Hinton. Replicated softmax: [14] R. Salakhutdinov and A. Mnih. Bayesian probabilistic [15] R. Salakhutdinov and A. Mnih. Probabilistic matrix [16] H. Shan and A. Banerjee. Generalized probabilistic [17] A. P. Singh and G. J. Gordon. Relational learning via [18] G. Tian and L. Jing. Recommending scientific articles [19] L. van der Maaten and G. Hinton. Visualizing Data [20] C. Wang and D. M. Blei. Collaborative topic modeling [21] C. Wang and D. M. Blei. Variational inference in Figure 7: Averaged norm of parameters under users w ith varying number of scores (left Conf-1, right Conf-2).
 Table 3: For the un-modified Conf-2 dataset, com-p arisons between CSTM and competitors for cold-start users using NDCG@5.

In Section 2 we motivated the specific parametrization of CSTM by its ability to trade off the influence of the user library side information versus that of the user-item scores. Here we show that learning in our model performs as expected. Figure 7 reports the relative norm of  X  versus (  X  0 +  X  ) as a function of the number of observed scores. As hypothesized as the number of scores increases the relative weight of the user library side information decreases.
We also experimented with variations of CSTM to better understand the roles played by the different aspects of the model and its training.

CSTM fixed topics (CSTM-FT): This model uses the ex-act preference regression model used by CSTM but it uses fixed topic user and document topic representations. That is it predicts preferences with: r ud = ( a u  X  s d )  X  T + s where A and S are previously learned offline.

CSTM no user side information (CSTM-NUSI): To eval-uate the gain of using user side information we experimented with a version of our model that does not model user side information (i.e., as if a user did not have any documents). Specifically, in this model a u  X  0  X  u .

We provide some results comparing CSTM with its vari-ations in table 4.

In Figure 8 and Table 3 we provide comparisons of the different methods on the original version of Conf-2. Figure 8: RMSE results on the un-modified Conf-2 dataset.
 Table 4: Comparisons between CSTM and two vari-a tions. Results report NDCG@5 over all users.
