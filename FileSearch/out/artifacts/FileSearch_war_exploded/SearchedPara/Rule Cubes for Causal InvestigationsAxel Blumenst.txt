
With the complexity of modern vehicles tremendously in-creasing, quality engineers play a key role within today X  X  automotive industry. Field data analysis supports correc-tive actions in development, production and after sales sup-port. We decompose the requirements and show that asso-ciation rules, being a popular approach to generating ex-planative models, still exhibit shortcomings. Recently pro-posed interactive rule cubes are a promising alternative. We extend this work by introducing a way of intuitively visual-izing and meaningfully ranking them. Moreover, we present methods to interactively factorize a problem and validate hypotheses by ranking patterns based on expectations, and by browsing a cube-based network of related influences. All this is currently in use as an interactive tool for warranty data analysis in the automotive industry. A real-world case study shows how engineers successfully use it in identifying root causes of quality issues.
Taking action requires knowledge about cause-effect re-lationships. Such knowledge predicts what results actions will probably have, and helps to decide among several ac-tion options.

This very general statement holds in our industrial do-main, too. Here, vehicle engineers keep on improving prod-uct quality and reliability through careful design and elabo-rate tests. However, this is not always enough. Sometimes, in real vehicle life, quality issues occur that could not be foreseen. Hence, analysis of field data X  X redominantly, this is warranty data X  X romises valuable knowledge about how to make vehicles even better.

In this application, causality plays the key role. Only if engineers know what really generated a quality issue, they can devise actions that precisely address it. This contrasts to many other data mining applications, which realize some kind of detection or prediction without referring to the no-tion of causality. Unfortunately, causal induction is never complete [13]. In this paper, we restrict our scope to what is feasible and reasonable within the realm of data analy-sis. The entire process of causal investigations comprises much more: delineating the case, finding hypotheses, and knowledgeable decisions about where to get the informa-tion that supports these hypotheses X  X ossibly even by con-ducting experiments to understand the causal chain behind the case. Hence, it is actually the experts who build causal models in their minds, supported by various tools. Some of them may be application specific (e.g., simulators), and one of them is our data analysis system, providing the experts with plausibility-rated hypotheses.

Hence, our ultimate goal may be formulated as to have a fast and reliable way of finding what (and how) to change within a set of reachable influence variables so that the probability of quality issues is reduced at the best . The quality issue is encoded as the class variable, and the in-fluence variables comprise, for instance, vehicle design and configuration.

This task decomposes to several subgoals: 1. Unrestricted search. If the answer is not available 2. Focusable search. If you point your flashlight to the 3. Problem-adequate pattern validation. In our key 4. Ease of use and understanding to transport the find-Many suggested to address these requirements by associa-tion rules along with some post processing to reduce redun-dancy and spot the most interesting ones [5, 6, 8, 9, 15]. This approach has been attractive not only due to the ex-istence of efficient mining algorithms. To some extent, rules also fulfill the goals above. But no kind of post pro-cessing will mitigate the shortcomings that originate in the model type itself (Section 3). Adding these up, users cannot work productively with rules in the application of interac-tive causal investigations. Liu et al. propose rule cubes as an alternative [16].

Our contributions in this paper are the following:  X  We review the shortcomings of association rules in  X  We extend the work of [16] by introducing a way to  X  Furthermore we re-state two methods to factorize a  X  On top of this, we devise a simple way of using cubes All this has been cast into a productive tool, which is suc-cessfully applied by our domain experts to solve real-world problems. As a whole, the process of generating actionable knowledge from raw data has evolved from a long tradition within the KDD community. In a way, we just adapt this pro-cess in a few aspects: model choice, pattern validation, and presentation. We claim that such tuning makes our users X  work of improving product quality more effective. While this claim is hard to evaluate, we wish to give an idea of it by describing an analysis session in Section 8. For us, the primary source of inspiration was the work of Liu et al. on rule cubes [16]. Their application task is to find causes for phone call failures at Motorola. They observed that experts never looked at rules with three influence fac-tors or more. With this restriction to at most two influence variables per rule, they mine the entire set of rules without support constraints, and present the underlying distributions using bar charts. Additionally, they generate more abstract descriptions ( X  X eneral impressions X ). Cubes can be ranked, but only those with a singular influence variable.
Liu and his group were not the first to display rules in context. Mosaic plots and double decker plots are other vi-sualizations of discrete distributions [4]. Here it is more difficult to spot which tiles are semantic neighbors, and they do not help selecting an interesting attribute set.
Referring to prior knowledge [12] for a more focused pattern ranking has been studied by several groups as well. Padmanabhan et al. propose putting expectations as rules and inducing further rules that confirm or contradict these expectations textually [9]. Jaroszewicz et al. [5, 6] make use of the maximum entropy principle to derive expected itemset supports from subrules or Bayesian networks. Sev-eral studies, originally [14], exploit taxonomic or parto-nomic hierarchies in attribute domains to automatically cre-ate expected support and confidence values and use them for pruning rules and spotting exceptional rules.

Sequential covering algorithms refer to some kind of prior knowledge as well: patterns discovered earlier. They incorporate this knowledge by removing or downweighting covered examples. However, this does not suit induction of exhaustive descriptions, because dissimilar patterns can be indistinguishable on the instance level. Furthermore, re-moval of instances can erode interesting structures beneath. Scholz devised a resampling approach to address this [11].
Mining association rules fulfills our application require-ments to a good extent. E.g., it searches a defined space jective interestingness [12, 5] even background knowledge shortcomings that deserve improvement:  X  In our application, patterns are interpreted by humans.  X  Rules are widely considered understandable. There is  X  Understandability is strongly determined by pattern vi-
A rule cube is a discrete frequency distribution (contin-gency table) over a small set of variables, among which is the class attribute. Rule cubes are not to be confused with OLAP cubes. 1 An OLAP cube is not a model but a mere data representation to allow for fast and easy exploration. Rule cubes are patterns that can take on the role of rules. Each cube states that its variables caused the problem at hand. Our ultimate goal is to assess these patterns according to how valid this statement is.

Liu et al. generate cubes by reorganizing the output of a class association rule miner [16]. Section 8.2 shows that there is no runtime benefit in this intermediate step. Direct counting on the data base is just as good. Counting time can be reduced by adaptively marginalizing cubes from higher dimensions.

Our primary result set is as follows. Let I be the set of all potential influence variables, C the class variable and P  X  I a fixed set of variables we want to condition on later (see Section 6.1). We generate the respective instance distributions over the attribute sets V = A X  X  C } X  X  for all A X  X \P with |A| X  X  1 , 2 } . These are |I\P| +1
Each cube cell can be considered a rule. Cubes, contain-ing much more information, thus require different visual-ization techniques. While double decker plots [4] draw in-fluence variables along one dimension only, Liu et al. make use of nesting in their bar charts [16]. We consider either vi-sualization hard to grasp. We exploit that our class variable is binary (one class of interest). This allows to visualize
Figure 1. Visualization of a rule cube (labels are not part of it), and the data behind. A and
B are two influence variables with dom ( A ) = { r,s } and dom ( B ) = { 0 , 1 , 2 , 3 , 4 } , and C is the class variable with dom ( C ) = { + ,  X  X  . The center column is colored red, showing high shares of the positive class. The adjacent columns are yellowish (for a lift near 1), and the outer columns are painted green, repre-senting low lift values. Apart from a clear description how B influences the class, this presentation points out that A is a false influ-ence, just  X  X ushed X  by its statistical depen-dence on B . its share by a color scale, freeing the second dimension for another influence variable.

Figure 1 shows an example. Each cell corresponds to a contingency table cell (when marginalized over the class variable). The area of the block therein is proportional to the coverage of the respective rule, while its lift determines the color, taken from a continuous scale from green (for lifts near zero) to red (for lifts far greater than one). From the dissimilar block sizes over the rows, one can easily see that the influence variables interact strongly. Furthermore it is immediately clear that the positive concentrate around the slot for B = 2 , while A is irrelevant.

Compare this to what an association rule miner would generate, when constrained to a minimum lift of 1 and with the consequent fixed to C . In this most simple case already, the roles of A and B are not obvious:
We first consider individual cube cells. A cell corre-sonable subgroup quality measure should grow if recall increases or coverage decreases, other parameters respec-tively fixed. These properties are known as Piatetsky-Shapiro X  X  claims on good rule measures [10].

Many rule quality measures can be cast as a compar-ison between an observed frequency distribution N V = ( n p 1 ,...,n p s ( V ) ) and some expected (or: theoretical) distri-bution E V = ( e p 1 ,...,e p and s ( V ) = Q A  X  X  | dom ( A ) | is the contingency table cell count. By default, E V just reflects statistical independence of the variables in V . Each p i is a predicate (a conjunction of variable-value pairs) that selects the examples falling into the respective cell. For example, the well-known rule mea-sure lift for an association rule  X   X   X  can be written as with p =  X   X   X  .
 The rule measure of our choice is When squared, I ( p ) becomes a summand in the  X  2 statistic. It thus complies with Piatetsky-Shapiro X  X  criteria. Example. Consider the case with a single influence vari-able A with dom ( A ) = { 0 , 1 } and a class variable with dom ( C ) = { + ,  X  X  . N AC (short for N { AC } ) thus is: where a predicate like ( A = 1)  X  ( C = +) is just denoted 1+ for readability. Our  X  X nterestingness X  measure of a rule ( A = 1)  X  ( C = +) can then be calculated as I (1+) where e 1+ = n 1 n + n . The lift of this rule is n p e
To assess and rank cubes , Liu et al. only seem to con-sider cubes with |A| = 1 , i.e., univariate influences [16]. These cubes are assessed according to either discriminative power (weighted sum of cell assessments) of an attribute, or strength of a trend within its domain. Since we include bivariate influences into our ranking as well, we take a dif-ferent approach (Section 5.3).

As the first idea to assess a cube over V , we simply take the maximum of the cell assessments: For later reference, we abbreviate this interestingness-targeted measure of divergence of frequency distributions as d I . Using the maximum therein ensures that the best subgroup determines the best cube.

The second idea addresses the scale problem: The as-sessment of a cube should not depend on scale, e.g., whether a continuous variable is discretized to 10 or 100 bins. We choose bin sizes rather small in the beginning, which is no problem with the minimum support limit dropped. On each cube, we adapt the attribute scales by merging subgroups to maximize overall cube quality. This remedies not only the problem of discretizing appropriately, but also applies to finding the best subset of values of categorical variables. Even if a domain is not ordered, it is not necessary to search the entire power set space. Instead, we just sort the sub-groups by their lifts. There is only a linear number of ways of assigning the k best and the | dom | X  k worst cells to two partitions. Then, cube interestingness is cell interestingness of the better partition. This approach is a significant step
In classic subgroup assessment, measures are calculated no matter how many influence variables constitute the an-tecedent predicate p . For a rule ( A = a )  X  ( B = b )  X  ( C = c ) , e abc is calculated as n ab n c n . This is fine in appli-cations focusing on instance subsets themselves (e.g., clas-sification). For causal analyses however, we must look into tern about two influence variables should reflect whether the class shift originates from one variable only, from the two independently, or from the interaction of both. A pattern with two variables actually states a joint influence. In the first two cases, however, this statement is wrong, because there is no such joint influence but only a superposition. Hence this pattern is uninteresting. The idea of rating a joint influence only according to its additional value over its components is quite common in other fields, such as anal-ysis of variance. What we do, is to transfer this practice to cubes.

Recall that interestingness measures share the idea of comparing some observed value to the respective expecta-tion as taken from a model-based distribution. The model behind e abc = n ab n c n is that not only the joint influence A  X  B is independent of C , but also its individual elements A and B .

This is not the only model we can choose. A more ap-propriate one would reflect that A and B have been assessed individually elsewhere and thus take the dependences in A  X  C and B  X  C into account, hence, the marginals N
AC and N BC in addition to N AB . Constructing the three-dimensional E ABC from three two-dimensionals produces a still under-determined system of equations. The maxi-mum entropy principle states to choose from the solution space the (unique) distribution with greatest entropy H , or semantically equivalent, the least commitment. In general, this solution can only be approximated numerically. We use generalized iterative scaling [2] for that. In other words, we replace the expected distribution E V in Equation 2 by where the D S are all proper marginal sums of the frequency distribution D , which are claimed to fit the respective ob-served distributions N S . Note that for two-dimensional contingency tables, the resulting distribution exactly reflects statistical independence.
Presenting a list of ranked influences to the user is only the beginning of an explorative process. Often the user finds influences he expected, and he will want to skip these. This does not mean to just discard them from the list, but to re-assess all influences based on the information that some in-
Incorporating knowledge about expected influences does not only serve as a means of getting rid of patterns that might be regarded a nuisance. Silberschatz et al. reason that actionable patterns are often recognizable by unexpected-ness [12]. Unexpectedness, in turn, is meaningful only if expectations have been modeled and can be referred to.
Moreover, eliminating a variable of choice and observing what happens provides insight into how influences relate to one another. Series of such actions help to factorize the problem.

These are three important reasons for assessing patterns based on prior knowledge. Incorporating prior knowledge is what literature refers to as  X  X ubjective interestingness X  [12]. In our context the term relative interestingness might suit better. Expecting an influence means knowing the ob-served distribution over the influence variable and the class. However any kind of  X  X elief X  expressible as a distribution can be incorporated.

Above, we compared the observed and a theoretical dis-tribution to get some absolute interestingness I ( AC ) with-out reference to some expected influence. Now, we present two ways of implementing relative interestingness account-ing for B influencing C .
This is the classic way as used in, e.g., Bayesian net-works: To get rid of an influence, condition on it.
We denote the relative interestingness as calculated by this method with I cond ( AC | B ) . It can be calculated in a similar way as in Equation 2 by comparing the observed and expected distributions, both extended by one dimen-sion for B . The theoretical distribution is constrained by the marginals N AB and N CB instead of just N A and N C . Put more generally: where the expected distribution is built from marginals in which the variables used as priors ( P ) are always present: Eliminating more than one influence variable means to add even more dimensions. However, this produces serious problems, including that the distribution sizes grow expo-nentially in the number of variables, and that scattering in-stances over a big table renders statistical treatment point-less. These difficulties made us ask for a better method.
We denote the relative interestingness as calculated by this method with I weight ( AC | B  X  C ) . We obtain it like I ( AC ) after changing weights of all instances such that B and C become independent X  without changing the marginals N B and N C . Scholz showed that there is a unique solution that matches these constraints [11]: where for all b  X  dom ( B ) and c  X  dom ( C ) : and E 0 ABC is calculated from N 0 ABC like in Equation 3.
The lift ( B = b  X  C = c ) = n bc n n of n bc to its expectation for the case B is independent of C . Therefore, with the weights modified, B and C are indepen-dent by construction. By analogy to sequential covering, we term this method of eliminating influences lift covering .
Scholz [11] however does not consider what happens when eliminating more than one dependence, say A  X  C
Figure 2. A  X  X  X  C | B rules out a causal link, while it does not state anything about the va-lidity of the link between B and C or the di-rection of the link between A and B . and B  X  C . If we eliminate them sequentially by Equa-tion 7, and A is not independent of B (given C ), some dependence between A and C will reappear after handling B  X  C . Hence, we iterate cyclically through all influences to be eliminated. This converges to a distribution in which all these have indeed gone. Sketch of proof: Lift covering is equivalent to fitting the distribution over all variables to that marginal over B  X  C which states the independence of these two. This reduces lift covering to iterative propor-tional fitting, proven to converge [2].
 Unfortunately, method 2 has a serious drawback as well: If at least one cell is zero, elimination is impossible. Even if some lifts are only near zero, the reweighted data set will contain instances with extraordinarily high weights, which jeopardizes induction of anything reasonable. Sometimes, empty cells are domain artifacts that can be remedied, e.g. by leaving out the respective instance set partitions, or by rescaling attributes. Still, we must note that method 2 can-not be applied universally either.
 Let X  X  compare these methods. Both I cond ( AC | B ) and I weight ( AC | B  X  C ) are zero if and only if A and C are independent given B (in symbols: A  X  X  X  C | B ). Beyond this simple test on conditional independence both measures capture some relative interestingness, yielding the greater a value the greater the distance to the conditional indepen-dence point. However, we do not evaluate which one is su-perior. Either one has its application, and we implemented and offer both.
Actionability has been associated with unexpectedness [12]. Yet even more essential for deciding about actions is causality. Inference of positive causal relations requires strong assumptions [13] that do not hold in our real-world application. In particular, we cannot assume causal suf-ficiency: There may be some hidden influence on two or more of the visible variables.

What works for sure however is ruling out a direct causal link between variables A and C in case A  X  X  X  C | B for any B (Figure 2). This decision about conditional independence is usually based on a statistical test with some arbitrary er-ror probability. To better reflect the vagueness of attribute interaction, it is desirable to incorporate at least this kind of causal validation into a continuous interestingness mea-sure. Besides, such a causality-aware interestingness would patterns. However, there are problems:  X  With our data failing the causal sufficiency condition,  X  If both A  X  X  X  C | B and B  X  X  X  C | A hold, A and B We therefore take a pragmatic approach. Recall that the rel-ative interestingness measures from above are soft replace-ments for the conditional independence test. So we define a directed suppression strength as This reflects how much A vanishes when knowing B .
For each variable B we define its neighborhood as all other variables A , ranked by s C ( B  X  A ) + s C ( A  X  B ) . Following the argument above, if the first of these sum-mands is much greater than the second, we deem B a possi-ble cause of A , and vice versa. If the summands are roughly equal (e.g., within a range of 2 : 3 ), we deem the influences congeneric. That way, for each variable, there are three cat-egories of neighbors. Though theoretically not fully sound, they turn out quite meaningful in practice, as can be seen in our case study below (Figure 5).
In our domain, influence variables interact strongly. Our users thus need multivariate analyses. When they create models based on decision trees or rule sets, they even dig to depths of four conditions and more. This raises ques-tions what we lose by constraining search to combinations of only two influences.
We observed that in many cases some of the conditions in trees or rules had been  X  X bused X : When users recognized that an issue divided into several subproblems, they added restrictions to better focus the problem. However these con-ditions were not part of the final causal explanation.
Our cubes implementation separates these two usages of conditions. It allows for an unlimited number of such fo-cusing restrictions, taking this burden from the cube-based search for causal influences. Recall that a cube over two in-fluences states that these two really interacted to cause the issue. In only three of eight reviewed cases, the final ex-planation referred to such interaction of two variables, and in all cases things were already apparent among the one-dimensional projections. Therefore we are confident that a search depth of two suffices in practice.
Association rule algorithms [3] heavily rely on support-based pruning. As we drop the minsupp constraint, ques-tions about computational costs arise, the more so as the system is interactive. Since with cubes, runtime does not depend on data density any more (Section 4), we report on runs with only a few data sets. These are taken from real in-vestigations. Association rule algorithms were set up with a support limit of zero and a search depth of up to two an-tecedent items. Experiments were carried out on a modest 1.7 GHz/1 GB machine, with all algorithms implemented in Java, except for Apriori, for which we used the C imple-mentation of [1]. For Apriori, I/O time is not included in the listed runtimes.

Table 1 shows no runtime penalty while generating cubes. Association rule mining even took more time, which can be attributed to several factors: First, rule miners that realize support pruning entail considerable organizational overhead. Second, in cube generation we exploited the fact that cubes can be marginalized from higher dimensions. Third, for cubes, a rule is a bare counter within an array, whereas in rule mining, a rule sooner or later must be mate-rialized as an object. Figure 3. The simple user interface to cubes.

The header presents information about pa-rameters, such as restrictions applied to the data, and eliminated influences (Section 6).

The left part of the screen is a list of singu-lar and paired variables along with their esti-mated interestingness value. The cube de-tails of the highlighted entry are shown on the right.  X  Here, Opt_Emission is recognized as one of the top influences. Most vehicles are equipped with Opt_Emission=A (left tile is big-ger) but most failure events occur on vehi-cles with Opt_Emission=N (right tile is dark red).
The primary claim of our approach is to be better un-derstandable and to increase effectiveness of the engineers X  work. User surveys could be a way to measure this, but apart from that, exact evaluation of added value is difficult. We therefore present a real-world case study to give an idea of the system and its overall value.
 Our users want to identify root causes of quality issues. Our approach provides them with a ranked list of cubes that have the strongest influence on an issue. Variables describe vehicle configuration, manufacturing, and usage. One im-portant feature of the system is its ability to recognize false cause might not even be among the variables, the interac-tive approach supports the engineers to better understand the problem and allows them to derive hidden influences. The following real-world example illustrates how our ap-
Figure 4. Rank and distribution of derived at-tribute CARBStates after restricting instances vice events occur in California (represented by the middle tile, which is large but green), whereas most oxygen sensor problems can be found in north-eastern CARB states (repre-sented by the right tile, which is red). proach applies in practice. Several vehicles are brought to dealerships because a lamp indicates an engine issue. Diag-nostic tools point to the exhaust system. Finding no trouble, dealers replace oxygen sensors on suspicion. Early warn-ing systems for warranty cost control show a significant in-crease in warranty costs for these sensors while engineers cannot find an explanation for the issue. The replaced sen-sors are ok, and no other part seems to have failed.
The data analyst knows that only one engine type can set the fault code. Therefore he restricts the data set to all instances with Opt_Engine=E (Figure 3). The system shows a ranked list of many possible influences (e.g. State , sumption that all service claims are related to the CARB states emissions system. 2 After further restricting the data does no longer show up. However, State remains a high-ranked influence, and the north-eastern CARB states, espe-cially New York, still have high positive rates. Therefore, he derives a new attribute CARBStates . Figure 4 shows its rank and distribution. He observes that most service events occur in the CARB states although some vehicles sold to CARB states run in other states. It is interesting that north-eastern CARB states still show a much higher failure rate than California. A possible explanation for this could be the stop-and-go traffic in New York.
 scribed in Section 7, the tool suggests that the high rank Figure 5. Neighborhood of the influence
CARBStates : In the three list boxes on the left, the tool suggests potential causes, similar in-fluences, and possibly caused influences, re-spectively. Min7Temp is listed among the sug-gested causes. On selection of this vari-able the 2D-cube on the right provides de-tails. It opposes the CARBStates variable (hor-izontal axis) to the Min7Temp variable (vertical axis). Indeed, there are stronger color (i.e., lift) differences over the Min7Temp axis than over the CARBStates axis in each row, and it is only the statistical dependence (visible at the tile sizes) that makes the marginal at the bot-tom (the CARBStates distribution) a seemingly strong influence. of CARBStates might have been caused by weather condi-tions, e.g., the minimum temperature over the last seven days (variable Min7Temp ). As temperatures in California are higher than in the north-eastern states (as well visible in Fig-ure 5) the failure just does not occur that often there. To verify this, the engineer eliminates the influence but Min7Temp is still ranked quite high. If however the user almost vanishes (Figure 7). Note that, despite this interac-tion the cube CARBStates  X  Min7Temp is ranked low (1.95), since the main influence is the temperature, and there is no interaction of these two as to cause the issue.

Based on these results the engineer actually finds out that there was a calibration issue that sometimes caused the en-gine control module to set a diagnostic code when the ve-hicle was driven at cold temperatures in wide open throttle mode.
In this paper, we decomposed the ultimate goal of finding actionable causes to quality issues in vehicles into several
Figure 6. Variable Min7Temp remains an influ-ence after the elimination of variable CARB-
States : There is a strong color gradient from red (left, cold temperatures) to green (right, warm temperatures).

Figure 7. Variable CARBStates is ranked quite low after the elimination of the influence
Min7Temp ; all three tiles are in yellowish colors, representing lift values near 1. (possibly still not exhaustive) subgoals. We noted that asso-ciation rule based approaches fulfill many of them, but are less adequate to some. These critical requirements include evaluation of rules as black box predicates, and limitations to understand what is behind a rule. We therefore adopted the work of Liu et al. [16], added an intuitive visualization introduced two different ways of incorporating background knowledge, supporting the user to interactively factorize a discussed how cubes can be used in particular to see how factors relate causally. That way, the user can walk through a web of related influences and at least recognize those that
We evaluated the design decision of a search depth limit of two as reasonable, and the runtime as acceptable for in-teractive use. Our expert users appreciate data analysis as an essential source of inspiration and as a support for their decisions. It is however hard to quantify the exact value our approach adds. We are going to conduct a systematic survey to get a better idea of this.

Several open issues remain. We are going to extend subgroup merging within singular variables to an optimiza-tion over two-influence cubes. Although the suppression strength (Equation 8) works fine in practice, we are work-ing on a better founded definition. Finally, we are interested in how the web of causally related influences can be visual-ized more intuitively.

