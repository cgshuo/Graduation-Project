 pattern classification and machine learning and many discriminant analysis algorithms have been proposed. In this paper, a Pairwise Covariance-preserving Projection Method (PCPM) is proposed for dimension reduction. PCPM maximizes the class discrimination and also preserves approximately the pairwise class covariances. The optimization involved in PCPM can be solved directly by eigenvalues decomposition. Our theoretical and empirical analysis reveals the relationship between PCPM and Linear Discriminant Analysis (LDA), Sliced Average Variance Estimator (SAVE), Heteroscedastic Discriminant Analysis (HDA) and Covariance preserving Projection Method (CPM). PCPM can utilize class mean and class covariance information at the same time. Furthermore, pairwise weight scheme can be incorporated naturally with the pairwise summarization form. The proposed methods are evaluated by both synthetic and real-world datasets. pattern classifier is a common technique to overcome estimation problems, and problems related to this. The most well-known technique for linear dimension reduction (LDR) in the K-class problem is linear discriminant analysis (LDA). LDA projects the data onto a lower dimensional vector space such that the ratio of the between-class distance and the within-class distance is maximized, thus achieving maximum determination of the LDA transform is equivalent to finding the maximum-likelihood (ML) parameter estimates of a Gaussian model, assuming that all class discrimination information resides in a d-dimensional subspace of the original n-dimensional feature space and that the within-class covariances are equal for all classes [1] . areas, such as computer vision, data mining etc.[2-4], it coincide; 2) it may not find the best projection when class covariance matrices vary; 3) it is suboptimal for large class distances are overemphasized; and 4) the reduced dimension of LDA is no larger than k-1 (k denotes the number of classes), which may not be sufficient for complex data. study of where and why LDA does not work. For the above limitation 2, Marco Loog [6] have proposed a method called the approximate pairing wise accuracy criterion (aPAC) to solve this problem, where the weighting is derived from an attempt to approximate the Bayes error for pairs of classes. Generalization of LDA by fitting Gaussian mixtures to each class has been studied by Hastie [7]. Cook et al. [8] proposed the Sliced Average Variance Estimator (SAVE), which is shown to be capable of dealing with the limitations in LDA. Kumar and Andreou[9] proposed HDA, based on a different model, in which the classes are still Gaussian, yet are allowed to have different covariance matrices, under the condition that both centroids and covariance matrices coincide in a subspace of the observation space. Marco Loog [10] proposed a heteroscedastic extension of LDA based on Chernoff criterion instead of Fisher criterion. Ye [11] proposed CPM, a covariance-preserving projection method maximize the class discrimination and at the same time preserve approximately the class covariance with a tuning parameter. The performance of CPM over LDA was also demonstrated, but like LDA, it has the similar shortcoming of over-affected by outlier classes. reduction method by preserving pairwise covariance distance, called PCPM. Furthermore, a weighted PCPM is also proposed with weights for contribution of individual class pairs to the overall criterion, which makes the algorithm more robust to outliers. K classes as points from the i-th class, and Wxmxm the centroid of the i-th class. It has been shown [6, 12] that the between-class scatter S b can be rewritten as follows: During CPM, the transform matrix G is sought to maximize 2 weighted variance of the K covariance matrices (after the projection G ), 
W = , and the total scatter matrix is defined as 
SS S =+ . To see what the criterion of CPM uses, we expand the formula 2 Lemma 2.1. Let W i , S w , p i be defined above, then Proof. = X  = X   X  X  follows from Lemma 2.1. covariance preserving in terms of class-covariance actually the between-class covariance matrix for the decomposition, we obtain for the CPM criterion which makes the CPM criterion not as obvious as the pairwise Fisher criteria in [6, 12]. for the above reason, the criteria is defined as assume the total scatter matrix has been normalized, i.e., following expression for the new criterion: where is the squared Frobenius distance between the covariance of class i and class j in the dimension covariance preserving criterion (PCPC). Here, we see that the new criterion used here is the linear transformation that maximizes the squared Frobenius distance between the class covariances in the lower-dimensional space. attempts to preserve pairwise class covariance distances. Specially, PCPM considers the information from both the class centroids and the class covariances simultaneously, via a tuning parameter  X  between 0 and 1. Mathematically, assuming where 01  X   X  X  X  , and solve. Instead, we maximize a upper bound, ( ) f G () hG  X  , where  X  X  X   X  X  X  Since G has orthonormal columns. Thus, Furthermore, we have the following result: Lemma 2.2 Let Proof. Denote symmetric. Let be the SVD (Singular value decomposition) of B, and ( ) q rank B = . since T that and the equality holds when GU = . Next, we can show that The proof follows from: and and can solved with eigenvalues decomposition as in traditional LDA. (1) Normalize the data with (2) Set the value of tuning parameter  X  , 01  X   X  X  X  ; (3) Compute the first d eigenvectors (4) covariance information simultaneously, it is different from minimizing the classification error. To illustrate that, consider an n-dimensional model that is to be reduced to lower dimension. Assume that class means coincide, and one class X  covariance is located remotely from the other classes and can be considered an outlier. In this case, the direction to project on found by optimizing the pairwise covariance preserving criterion used in PCPM is the one that separates the outlier as much from the remaining classes as possible. In maximizing the squared Frobenius distance of covariance distance, pairs of classes, between which there are large distances, completely dominate the eigenvalues decomposition. As a result, there is a large overlap among the remaining classes, leading to an overall low and suboptimal classification rate. Hence, in general, LDR by PCPM is not optimal with respect to minimizing the classification error rate in the lower-dimensional space. Because outlier classes dominate the eigenvalues decomposition, the LDR transform obtained tends to over-weight the influence of classes that are already well separated. closely related to the classification error. We would like to keep the general form of (6) because that the optimization can again be carried out by solving a generalized eigenvalues problem without having to resort to complex iterative optimization schemes. To do so, (6) is generalized by introducing a weighting function w : where suggested by Loog[6], the pairwise Chernoff distance measure between the [10, 13] relative a priori taking into account two classes that WWW  X  X  + are the average pairwise within-class covariance matrices of class i and j . maximization ( ) and can be obtained by eigenvalues decomposition or SVD. Proposition 2.2.1 WPCPM with const weight ()1 wd = is equivalent to LDA, if either of the following two conditions hold: (1) 0  X  = ; or (2) all classes have the same covariance matrix, i.e., for all i . Proof. Recall that in WPCPM, the optimal G is computed by maximizing ( ) f G  X  , defined as ()1 wd = , WPCPM becomes PCPM, and When 0  X  = , or  X  vanishes. Thus, This completes the proof of the proposition. PCPM, SAVE, and HDA. More specificically, SAVE is shown to be closely related to a special class of PCPM, and PCPM is shown to be approximation of HDA. The theoretical analysis provides the justification for the PCPM algorithm and gives us insights into the nature of these different algorithms. Variance Estimator (SAVE) to overcome the limitations in LDA. Like PCPM, each class in SAVE may have different covariances. In their approach, the data is normalized so that the total covariance matrix is identity (as in CPM) and then the discriminant directions are chosen by maximizing over n  X   X  \ of unit length, that is, 1  X  = solutions are given by the top eigenvectors of Since = X ++ X + X  algorithm presented in Section 3. With the empirical approximation [11, 14] The solution to PCPM can be approximated by maximizing approximation, that is, we use approximated by the first d eigenvectors of which contains the same set of eigenvectors as 
SpWS when p , and  X  replaced with 2  X  , we get the CPM method in [11], that is, the parameter value in PCPM  X  X  X  = . Our experimental results show that with the above relation, PCPM often has similar performance as CPM. Notice that the matrices in (9) SAVE, where related to a special case of modified PCPM (which is equal to CPM) when parameter  X  is set to be 2 between the separation of class centroids and the preservation of class covariances. PCPM is more flexible in dealing with different situations by varying the values of  X  . proposed by Kumar and Audreou assumes each class is Gaussian, but with possibly different covariance matrices, under the assumption that both centroids and covariance matrices coincide in a subspace of the observation space. That is, HDA assumes that there dimension nd  X  , and i [, ] nn GGF  X  = X  \ , such that and for some c and i W , which are common for all classes. between classes lie solely in a subspace of d complementary subspace of n -d dimensions (projection by F ) they are identical, i.e. useless for discrimination. model is given as where transformation i G and under the constrained Gaussian model above is maximized with respect to its parameters. Since there is no closed-form solution for maximizing the maximization has to be performed numerically. More details on this can be found in [9], where quadratic programming is performed for the required optimization. As pointed out in [9], even though the quadratic-optimization techniques are used, the likelihood surface is not strictly quadratic, and the optimization problem occasionally fails. In the rest of this section, we show that the PCPM algorithm proposed in this paper is an approximation of HDA. i.e., most of the information on computed so that is large. Similarly, this implies that is large. Thus, PCPM can be considered as an approximation of HDA. Our experimental results in the next section show that PCPM is comparable to HDA in classification, while PCPM is much more efficient and robust than HDA. both synthetic and real-world datasets. The KNN algorithm with k=1 is applied for classification. As can be seen from the formula (6) and (7), the parameter  X  balance between class mean and class covariance information. After some experiments, we found that  X  = 0.3 is usually a good choice for PCPM and WPCPM in all experiments. However, the best value of  X  may be estimated through cross-validation. similar dataset is also used in [9, 11], the dataset contains 20 dimensional 3 classes data points, where the class centroids of three classes do coincide, and have different covariance matrices. For the last 18 dimensions, data is simulated from standard Gaussian coordinates, class 1 with 400 data points is simulated from a standard multivariate Gaussian with mean (0,0) and covariance C=diag(2,2). Class 2 and 3 are mixtures of two shifted standard Gaussians, with mean (-6,0), (6,0) and (0,6), (0,-6) respectively. Each Gaussian component also has covariance C and have 200 data points. Note that in all the below experiments, the HLDA implementation in [15] is used for the original HDA does not converge on many dataset as noted in [9, 11]. From Fig.1, SAVE, HDA, CPM and PCPM separate the three classes better than LDA, which shows the effect of incorporating the class covariance information. Note that due to the mean is (0,0) for the three classes, between-class scatter matrix does not contain any information, and so the SAVE, CPM, and PCPM produce the same low dimensional coordinates. Class covariance information is omitted in LDA and so LDA fails to find the best projection when class covariances vary. from the UCI machine learning repository. It contains the (x, y) coordinates of hand-written digits. Each digit is represented as a vector in 16-dimensional space. The dataset is divided into a training set (7494 digits) and a test set (3498 digits). For the purpose of visualization as in [11], we select the 1 X  X , 3 X  X  and 7 X  X . We apply LDA, SAVE, HDA, CPM and PCPM to this 3-class subproblem and extract 2 leading discriminant directions. The dataset is then projected onto these 2 that LDA, HDA, CPM and PCPM separate the three classes well. SAVE, on the other hand, picks up directions where one class has a much larger variance than the other two classes. This further confirms our claim in Section 3 that PCPM is more flexible in dealing with different situations by choosing different  X  , than SAVE. on two datasets: Pendigits and Wisconsin breast cancer (Wisconsin for short), both from the UCI machine learning repository. For Pendigits, a subset consisting consisting of 699 instances of breast cancer data, with 9 dimensions (458 instances for training and 241 instances for test). The result is summarized in Fig.3, where the x-axis denotes the reduced dimension d for LDA, HDA, SAVE, CPM and PCPM, and the y-axis denotes the classification accuracy. We observe from Fig.3 that the best accuracy for HDA, SAVE, CPM and PCPM often occurs when 1 dk &gt; X  . Note that the accuracy curves of CPM and PCPM follow similar trend on the Pendigits, while they are quite different from SAVE. Overall, PCPM is very competitive with the other four algorithms. SAVE and CPM in terms of classification on four datasets from UCI. The results are summarized in Table 1. The reduced dimension used in PCPM and WPCPM are set to be k-1, which is the largest reduced dimension used in LDA. Note that in fact the performance of PCPM and WPCPM may be improved by using larger number of dimensions. From the table, we can observe that PCPM and WPCPM outperform SAVE for all cases, while PCPM is very competitive with LDA. Further, WPCPM can further improve the classification accuracy by utilizing pairwise mean and covariance distribution information compared with PCPM. for dimension reduction. It aims to maximize the pairwise class discrimination and pairwise covariance distance simultaneously. The optimization problem can be approximated directly as PCA. We have applied PCPM to various datasets and the experimental results show that PCPM is capable of recovering the features for classification, even when all class centroids coincide, and PCPM is competitive with LDA, SAVE, HDA, and CPM as respect to classification. Besides, by incorporate weighting of the contributions of individual class pairs to the overall criterion, a weighted PCPM called WPCPM is also proposed, which can further improve the classification accuracy. Reference: 
