 Latent variable generative models are widely used in inducing meaningful representations from un-labeled data. Maximum likelihood estimation is a standard method for fitting such models, but in most cases we are not so interested in the likelihood of the data as in the distribution of the latent variables, which we hope will capture regularities of interest without direct supervision. In this pa-per we explore the problem of biasing such unsupervised models to favor a novel kind of sparsity that expresses our expectations about the role of the latent variables. Many important language pro-cessing tasks (tagging, parsing, named-entity classification) involve classifying events into a large number of possible classes, where each event type can have just a few classes. We extend the poste-rior regularization framework [7] to achieve that kind of posterior sparsity on the unlabeled training data. In unsupervised part-of-speech (POS) tagging, a well studied yet challenging problem, the new method consistently and significantly improves performance over a non-sparse baseline and over a variational Bayes baseline with a Dirichlet prior used to encourage sparsity [9, 4].
 A common approach to unsupervised POS tagging is to train a hidden Markov model where the hidden states are the possible tags and the observations are word sequences. The model is typi-cally trained with the expectation-maximization (EM) algorithm to maximize the likelihood of the observed sentences. Unfortunately, while supervised training of HMMs achieves relatively high accuracy, the unsupervised models tend to perform poorly. One well-known reason for this is that EM tends to allow each word to be generated by most hidden states some of the time. In reality, we would like most words to have a small number of possible tags. To solve this problem, several studies [14, 17, 6] investigated weakly-supervised approaches where the model is given the list of possible tags for each word. The task is then to disambiguate among the possible tags for each word type. Recent work has made use of smaller dictionaries, trying to model the set of possible tags for each word [18, 5], or use a small number of  X  X rototypes X  for each tag [8]. All these approaches initialize the model in a way that encourages sparsity by zeroing out impossible tags. Although this has worked extremely well for the weakly-supervised case, we are interested in the setting where we have only high-level information about the model: we know that the distribution over the la-tent variables (such as POS tags) should be sparse. This has been explored in a Bayesian setting, where a prior is used to encourage sparsity in the model parameters [4, 9, 6]. This sparse prior, which prefers each tag to have few word types associated with it, indirectly achieves sparsity over the posteriors , meaning each word type should have few possible tags. Our method differs in that it encourages sparsity in the model posteriors, more directly encoding the desiderata. Additionally our method can be applied to log-linear models where sparsity in the parameters leads to dense posteriors. Sparsity at this level has already been suggested before under a very different model[18]. We use a first-order HMM as our model to compare the different training conditions: classical expectation-maximization (EM) training without modifications to encourage sparsity, the sparse prior used by [9] with variational Bayes EM (VEM), and our sparse posterior regularization (Sparse). We evaluate these methods on three languages, English, Bulgarian and Portuguese. We find that our method consistently improves performance with respect to both baselines in a completely unsuper-vised scenario, as well as in a weakly-supervised scenario where the tags of closed-class words are supplied. Interestingly, while VEM achieves a state size distribution (number of words assigned to hidden states) that is closer to the empirical tag distribution than EM and Sparse its state-token distribution is a worse match to the empirical tag-token distribution than the competing methods. tagger. In order to express the desired preference for posterior sparsity, we use the posterior regularization (PR) framework [7], which incorporates side information into parameter estimation in the form of linear constraints on posterior expectations. This allows tractable learning and inference even when the constraints would be intractable to encode directly in the model, for instance to enforce that each hidden state in an HMM is used only once in expectation. Moreover, PR can represent prior knowledge that cannot be easily expressed as priors over model parameters, like the constraint used in this paper. PR can be seen as a penalty on the standard marginal likelihood objective, which we define first: over the parameters  X  , where b E is the empirical expectation over the unlabeled sample x , and z are the hidden states. This standard objective may be regularized with a parameter prior  X  log p (  X  ) = C (  X  ) , for example a Dirichlet.
 Posterior information in PR is specified with sets Q x of distributions over the hidden variables z defined by linear constraints on feature expectations: The marginal log-likelihood of a model is then penalized with the KL-divergence between the de-revised learning objective minimizes: Since the objective above is not convex in  X  , PR estimation relies on an EM-like lower-bounding scheme for model fitting, where the E step computes a distribution q ( z | x ) over the latent variables and the M step minimizes negative marginal likelihood under q ( z | x ) plus parameter regularization: In a standard E step, q is the posterior over the model hidden variables given current  X  : q ( z | x ) = p ( z | x ) . However, in PR, q is a projection of the posteriors onto the constraint set Q x for each example x : Figure 1: An illustration of ` 1 /`  X  regularization. Left panel: initial tag distributions (columns) for 15 instances of a word. Middle panel: optimal regularization parameters  X  , each row sums to  X  = 20 . Right panel: q concentrates the posteriors for all instances on the NN tag, reducing the ` /`  X  norm from just under 4 to a little over 1.
 The new posteriors q ( z | x ) are used to compute sufficient statistics for this instance and hence to update the model X  X  parameters in the M step. The optimization problem in Equation 4 can be solved efficiently in dual form: constant. There is one dual variable per expectation constraint, which can be optimized by projected gradient descent where gradient for  X  is b  X  E q [ f ( x , z )] . Gradient computation involves an expec-the model p  X  ( z | x ) [7]. In this work, we modify PR so that instead of hard constraints on q ( z | x ) , it allows the constraints to be relaxed at a cost specified by a penalty. This relaxation can allow combining multiple con-straints without having to explicitly ensure that the constraint set remains non-empty. Additionally, it will be useful in dealing with the ` 1 /`  X  constraints we need. If those were incorporated as hard constraints, the dual objective would become non-differentiable, making the optimization (some-what) more complicated. Using soft constraints, the non-differentiable portion of the dual objective turns into simplex constraints on the dual variables, allowing us to use an efficient projected gradient method. For soft constraints, Equation 4 is replaced by where b is the constraint vector, and R ( b ) penalizes overly lax constraints. For POS tagging, we will design R ( b ) to encourage each word type to be observed with a small number of POS tags in the projected posteriors q . The overall objective minimized can be shown to be: Soft PR Objective: arg min 3.1 ` 1 /`  X  regularization We now choose the posterior constraint regularizer R ( b ) to encourage each word to be associated with only a few parts of speech. Let feature f wti have value 1 whenever the i th occurrence of word w has part of speech tag t . For every word w , we would like there to be only a few POS tags t such that there are occurrences i where t has nonzero probability. This can be achieved if it  X  X osts X  a lot to allow an occurrence of a word to take a tag, but once that happens, it should be  X  X ree X  for other occurrences of the word to receive that same tag. More precisely, we would like the sum ( ` 1 norm) over tags t and word types w of the maxima ( `  X  norm) of the expectation of taking tag t over all occurrences of w to be small. Table 1 shows the value of the ` 1 /`  X  sparsity measure for three different corpora, comparing fully supervised HMM and fully unsupervised HMM learned with standard EM, with standard EM having 3-4 times larger value of ` 1 /`  X  than the supervised. This discrepancy is what our PR objective is attempting to eliminate.
 Formally, the E-step of our approach is expressed by the objective: where  X  is the strength of the regularization. Note that setting  X  = 0 we are back to normal EM where q is the model posterior distribution. As  X   X   X  , the constraints force each occurrence of a word type to have the same posterior distribution, effectively reducing the mode to a 0th-order Markov chain in the E step.
 The dual of this objective has a very simple form (see supplementary material for derivation): where z ranges over assignments to the hidden tag variables for all of the occurrences in the training data, f ( z ) is the vector of f wti feature values for assignment z ,  X  is the vector of dual parame-projected gradient, as described by Bertsekas [3].
 Figure 1 illustrates how the ` 1 /`  X  norm operates on a toy example. For simplicity suppose we are only regularizing one word and our model p  X  is just a product distribution over 15 instances of the word. The left panel in Figure 1 shows the posteriors under p  X  . We would like to concentrate the posteriors on a small subset of rows. The center panel of the figure shows the  X  values determined by Equation 9, and the right panel shows the projected distribution q , which concentrates most of the posterior on the bottom row. Note that we are not requiring the posteriors to be sparse, which would be equivalent to preferring that the distribution is peaked; rather, we want a word to concentrate its tag posterior on a few tags across all instances of the word. Indeed, most of the instances (columns) become less peaked than in the original posterior to allow posterior mass to be redistributed away from the outlier tags. Since they are more numerous than the outliers, they moved less. This also justifies only regularizing relatively frequent events in our model. Recent advances in inference methods for sparsifying Bayesian estimation have been applied to unsupervised POS tagging [4, 9, 6]. In the Bayesian setting, preference for sparsity is expressed as a prior distribution over model structures and parameters, rather than as constraints on feature posteriors. To compare these two approaches, in Section 5 we compare our method to a Bayesian approach proposed by Johnson [9], which relies on a Dirichlet prior to encourage sparsity in a first-order HMM for POS tagging. The complete description of the model is:
Here,  X  i controls sparsity over the state transition matrix and  X  i controls the sparsity of state emis-sion probabilities. Johnson [9] notes that  X  i does not influence the model that much. In contrast, as  X  i approaches zero, it encourages the model to have highly skewed P ( w i | t i = tag ) distributions, that is, each tag is encouraged to generate a few words with high probability, and the rest with very low probability. This is not exactly the constraint we would like to enforce: there are some POS tags that generate many different words with relatively high probability (for example, nouns and verbs), while each word is associated with a small number of tags. This difference is one possible explanation for the relatively worse performance of this prior compared to our method.
 Johnson [9] describes two approaches to learn the model parameters: a component-wise Gibbs sampling scheme (GS) and a variational Bayes (VB) approximation using a mean field. Since John-son [9] found VB worked much better than GS, we use VB in our experiments. Additionally, VB is particularly simple to implement, consisting only a small modification to the M-Step of the EM al-gorithm. The Dirichlet prior hyper-parameters are added to the expected counts and passed through a squashing function (exponential of the Digamma function) before being normalized. We refer the reader to the original paper for more detail (see also http://www.cog.brown.edu/~mj/ Publications.htm for a bug fix in the Digamma function implementation). We now compare first-order HMMs trained using the three methods described earlier: the classi-cal EM algorithm (EM), our ` 1 /`  X  posterior regularization based method (Sparse), and the model presented in Section 4 (VEM). Models were trained and tested on all available data of three cor-pora: the Wall Street Journal portion of the Penn treebank [13] using the reduced tag set of 17 tags [17] (PTB17); the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank [1] used for the ConLL X shared task on dependency parsing (PT-CoNLL); and the Bulgarian BulTreeBank [16] (BulTree) with the 12 coarse tags. We also report results on the full Penn treebank tag set in the supplementary materials. All words that occurred only once were replaced by the token  X  X nk X . To measure model sparsity, we compute the average ` 1 /`  X  norm over words occurring more than 10 times (denoted  X  X 1LMax X  in our figures). Table 1 gives statistics for each corpus as well as the sparsity for a first-order HMM trained using the labeled data and using standard EM with unlabeled data.
 Table 1: Corpus statistics. All words with only one occurrence where replaced by the  X  X nk X  token. The third column shows the percentage of tokens replaced. Sup. ` 1 /`  X  is the value of the sparsity measure for a fully supervised HMM trained on all available data and EM ` 1 /`  X  is the value of the sparsity measure for a fully unsupervised HMM trained using standard EM on all available data. Following Gao and Johnson [4], the parameters were initialized with a  X  X seudo E step X  as follows: we filled the expected count matrices with numbers 1 + X  X  U (0 , 1) , where U (0 , 1) is a random number between 0 and 1 and X is a parameter. These matrices are then fed to the M step; the re-sulting  X  X andom X  transition and emission probabilities are used for the first real E step. For VEM, X was set to 0.0001 (almost uniform) since this showed a significant improvement in performance. On the other hand, EM showed less sensitivity to initialization, and we used X = 1 which resulted in the best results. The models were trained for 200 iterations as longer runs did not significantly change the results (models converge before 100 iterations). For VEM we tested 4 different prior combinations, (all combinations of 10  X  1 and 10  X  3 for emission prior and transition prior), based on Johnson X  X  results [9]. As previously noted, changing the transition priors does not affect the Table 2: Average accuracy (standard deviation in parentheses) over 10 different runs (random seeds identical across models) for 200 iterations. 1-Many and 1-1 are the two hidden-state to POS map-pings described in the text. All models are first order HMMs: EM trained using expectation maxi-mization, VEM trained using variational EM observation priors shown in parentheses, Sparse trained using PR with the constraint strength (  X  ) in parentheses. Bold indicates the best value for each col-umn. All results except those starred are significant (p=0.005) on a paired t-test against the EM model. Figure 2: Detailed visualizations of the results on the PT-Conll corpus. (a) 1-many accuracy vs ` /`  X  , (b) 1-1 accuracy vs ` 1 /`  X  , (c) tens of thousands of tokens assigned to hidden state vs rank, (d) mutual information in bits between gold tag distribution and hidden state distribution. results, so we only report results for different emission priors. Later work [4] considered a wider range of values but did not identify definitely better choices. Sparse was initialized with the pa-rameters obtained by running EM for 30 iterations, followed by 170 iterations of the new training procedure. Predictions were obtained using posterior decoding since this consistently showed small improvements over Viterbi decoding.
 We evaluate the accuracy of the models using two established mappings between hidden states and POS tags: 1-Many maps each hidden state to the tag with which it co-occurs the most; 1-1 [8] greedily picks a tag for each state under the constraint of never using the same tag twice. This results in an approximation of the optimal 1-1 mapping. If the numbers of hidden states and tags are not the same, some hidden states will be unassigned (and hence always wrong) or some tags not used. In all our experiments the number of hidden states is the same as the number of POS tags. Table 2 shows the accuracy of the different methods averaged over 15 different random parameter initializations. Comparing the methods for each of the initialization points individually, our ` 1 /`  X  regularization always outperforms EM baseline model on both metrics, and always outperforms VEM using 1-Many mapping, while for the 1-1 mapping our method outperforms VEM roughly half the time. The improvements are consistent for different constraint strength values. Figure 2 shows detailed visualizations of the behavior of the different methods on the PT-Conll cor-pus. The results for the other corpora are qualitatively similar, and are reported in the supplemental material. The left two plots show scatter graphs of accuracy with respect to ` 1 /`  X  value, where accuracy is measured with either the 1-many mapping (left) or 1-1 mapping (center). We see that Sparse is much better using the 1-many mapping and worse using the 1-1 mapping than VEM, even though they achieve similar ` 1 /`  X  . The third plot shows the number of tokens assigned to each hidden state at decoding time, in frequency rank order. While both EM and Sparse exhibit a fast de-crease in the size of the states, VEM more closely matches the power law-like distribution achieved by the gold labels. This difference explains the improvement on the 1-1 mapping, where VEM is assigning larger size states to the most frequent tags. However, VEM achieves this power law distri-bution at the expense of the mutual information with the gold labels as we see in the rightmost plot. From all methods, VEM has the lowest mutual information, while Sparse has the highest. 5.1 Closed-class words We now consider the case where some supervision has been given in the form of a list of the closed-class words for the language, along with POS tags. Example closed classes are punctuation, pro-nouns, possessive markers, while open classes would include nouns, verbs, and adjectives. (See the supplemental materials for details.) We assume that we are given the POS tags of closed classes along with the words in each closed class. In the models, we set the emission probability from a closed-class tag to any word not in its class to zero. Also, any word appearing in a closed class is as-sumed to have zero probability of being generated by an open-class tag. This improves performance significantly for all languages, but our sparse training procedure is still able to outperform EM train-ing significantly as shown in Table 3. Note, for these experiments we do not use an unknown word, since doing so for closed-class words would allow closed class tags to generate unknown words. Table 3: Results with given closed-class tags, using posterior decoding, and projection at test time. Figure 3: Accuracy of a supervised classifier when trained using the output of various unsupervised models as features. Vertical axis: accuracy, Horizontal axis: number of labeled sentences. 5.2 Supervised POS tagging As a further comparison of the models trained using the different methods, we use them to generate features for a supervised POS tagger. The basic supervised model has features for the identity of the current token as well as suffixes of length 2 and 3. We augment these features with the state identity for the current token, based on the automatically generated models. We train the supervised model using averaged perceptron for 20 iterations.
 For each unsupervised training procedure (EM, Sparse, VEM) we train 10 models using different random initializations and got 10 state identities per training method for each token. We then add these cluster identities as features to the supervised model. Figure 3 shows the average accuracy of the supervised model as we vary the type of unsupervised features. The average is taken over 10 random samples for the training set at each training set size. We can see from Figure 3 that using our method or EM always improves performance relative to the baseline features (labeled  X  X one X  in the figure). VEM always under performs EM and for larger amounts of training data, the VEM features appear not to be useful. This should not be surprising given that VEM has very low mutual information with the gold labeling. Our learning method is very closely related to the work of Mann and McCallum [11, 12], who concurrently developed the idea of using penalties based on posterior expectations of features to guide learning. They call their method generalized expectation (GE) constraints or alternatively expectation regularization. In the original GE framework, the posteriors of the model are regularized directly. For equality constraints, our objective would become: Notice that there is no intermediate distribution q . For some kinds of constraints this objective is difficult to optimize in  X  and in order to improve efficiency Bellare et al. [2] propose interpreting the PR framework as an approximation to the GE objective in Equation 10. They compare the two frameworks on several datasets and find that performance is similar, and we suspect that this would be true for the sparsity constraints also. Liang et al. [10] cast the problem of incorporating partial information about latent variables into a Bayesian framework using  X  X easurements, X  and they propose active learning for acquiring measurements to reduce uncertainty. Recently, Ravi et al. [15] show promising results in weakly-supervised POS tagging, where a tag dictionary is provided. This method first searches, using integer programming, for the smallest grammar (in terms of unique transitions between tags) that explains the data. This sparse grammar and the dictionary are provided as input for training an unsupervised HMM. Results show that using a sparse grammar, hence enforcing sparsity over possible sparsity transitions leads to better results. This method is different from ours in the sense that our method focuses on learning the sparsity pattern they their method uses as input. We presented a new regularization method for unsupervised training of probabilistic models that favors a kind of sparsity that is pervasive in natural language processing. In the case of part-of-speech induction, the preference can be summarized as  X  X ach word occurs as only a few different parts-of-speech, X  but the approach is more general and could be applied to other tasks. For example, in grammar induction, we could favor models where only a small number of production rules have non-zero probability for each child non-terminal.
 Our method uses the posterior regularization framework to specify preferences about model poste-riors directly, without having to say how these should be encoded in model parameters. This means that the sparse regularization penalty could be used for a log-linear model, where sparse parameters do not correspond to posterior sparsity.
 We evaluated the new regularization method on the task of unsupervised POS tagging, encoding the prior knowledge that each word should have a small set of tags as a mixed-norm penalty. We compared our method to a previously proposed Bayesian method (VEM) for encouraging sparsity of model parameters [9] and found that ours performs better in practice. We explain this advantage by noting that VEM encodes a preference that each POS tag should generate a few words, which goes in the wrong direction. In reality, in POS tagging (as in several other language processing task), a few event types (tags) (such the NN for POS tagging) generate the bulk of the word occurrences, but each word is only associated with a few tags. Even when some supervision was provided with through closed class lists, our regularizer still improved performance over the other methods. An analysis of sparsity shows that both VEM and Sparse achieve a similar posterior sparsity as measured by the ` 1 /`  X  metric. While VEM models better the empirical sizes of states (tags), the states it assigns have lower mutual information to the true tags, suggesting that parameter sparsity is not as good at generating good tag assignments. In contrast, Sparse X  X  sparsity seems to help build a model that contains more information about the correct tag assignments.
 Finally, we evaluated the worth of states assigned by unsupervised learning as features for supervised tagger training with small training sets. These features are shown to be useful in most conditions, especially those created by Sparse. The exceptions are some of the annotations provided by VEM which actually hinder the performance, confirming that its lower mutual information states are not so informative.
 In future work, we would like to evaluate the usefulness of these sparser annotations for down-stream tasks, for example determining whether Sparse POS tags are better for unsupervised parsing. Finally, we would like to apply the ` 1 /`  X  posterior regularizer to other applications such as unsu-pervised grammar induction where we would like sparsity in production rules. Similarly, it would be interesting to use this to regularize a log-linear model, where parameter sparsity does not achieve the same goal.
 Acknowledgments J. V. Gra X a was supported by a fellowship from Funda X  X o para a Ci X ncia e Tecnologia (SFRH/ BD/ 27528/ 2006). K. Ganchev was supported by ARO MURI SUBTLE W911NF-07-1-0216 The authors would like to thank Mark Johnson and Jianfeng Gao for their help in reproducing the VEM results. [1] S. Afonso, E. Bick, R. Haber, and D. Santos. Floresta Sinta(c)tica: a treebank for Portuguese. [2] K. Bellare, G. Druck, and A. McCallum. Alternating projections for learning with expectation [3] D.P. Bertsekas, M.L. Homer, D.A. Logan, and S.D. Patek. Nonlinear programming. Athena [4] Jianfeng Gao and Mark Johnson. A comparison of Bayesian estimators for unsupervised Hid-[5] Y. Goldberg, M. Adler, and M. Elhadad. Em can find pretty good hmm pos-taggers (when [6] S. Goldwater and T. Griffiths. A fully bayesian approach to unsupervised part-of-speech tag-[7] J. Gra X a, K. Ganchev, and B. Taskar. Expectation maximization and posterior constraints. In [8] A. Haghighi and D. Klein. Prototype-driven learning for sequence models. In In Proc. NAACL , [9] M Johnson. Why doesn X  X  EM find good HMM POS-taggers. In In Proc. EMNLP-CoNLL , [10] P. Liang, M. I. Jordan, and D. Klein. Learning from measurements in exponential families. In [11] G. Mann and A. McCallum. Simple, robust, scalable semi-supervised learning via expectation [12] G. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning of [13] M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. Building a large annotated corpus of [14] B. Merialdo. Tagging English text with a probabilistic model. Computational linguistics , [15] Sujith Ravi and Kevin Knight. Minimized models for unsupervised part-of-speech tagging. In [16] Kiril Simov, Petya Osenova, Milena Slavcheva, Sia Kolkovska, Elisaveta Balabanova, Dimitar [17] N.A. Smith and J. Eisner. Contrastive estimation: Training log-linear models on unlabeled [18] K. Toutanova and M. Johnson. A Bayesian LDA-based model for semi-supervised part-of-
