 Ranking function performance reached a plateau in 1994. The reason for this is investigated. First the performance of BM25 is measured as the proportion of queries satisfied on the first page of 10 results  X  it performs well. The performance is then compared to human performance. They perform comparably. The conclu-sion is there isn X  X  much room for ranking function improvement. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Search process .
 Measurement, Performance, Experimentation, Human Factors. Relevance Ranking, Document Retrieval. Armstrong et al. [2, 3] investigate the magnitude of the improve-ments in ad hoc searching seen on the TREC collections since 1994. They find  X  X o discernible upward trend in Ad-Hoc scores over time X  and find the results  X  X  eported in 2008 generally indis-tinguishable from those reported in 1999 X . In 2009 the best ad hoc relevance-in-context run submitted to INEX did not use structure in ranking [5]. A better ranking f unction and new features that improve ranking have remained elusive. As a rule of thumb, the best ranking algorithms all perform at about the same level. BM25 with two tuning parameters performs comparably to language models with one and Divergence From Randomness [1] with none. Ranking performance has plateaued. This plateau could be crossed if we knew the cause. Buckley [4] investigates the reasons why s earch engines fail and concludes that significant improvement may be seen if we could identify the cases where we incorrectly emphasized some aspect of the query. Query performance prediction research has not been fruitful. We ask: Is there room for impro vement in ad hoc ranking? We explore this question by comparing stemmed BM25 perform-ance to human performance (langua ge models might equally be used). To do this we take all th e TREC and INEX test collections that were multiply assessed and generate synthetic human runs. Comparing the performance of these runs to a search engine sug-gests that there is little room for improvement . We validate this by comparing manual and automatic runs submitted to TREC and again see the same result. not, followed by those they both agree are relevant. These lists are the upper and lower bound on human performance, but it is im-portant to compute the expected performance. For each topic, 10,000 random permutations of the alternate as-sessor X  X  relevant documents were generated and Average Preci-sion (AP) computed for each. The average of these APs is the expected AP. The mean over all topics is the expected MAP. The expected MAP scores were compared to those of the search engine. In Figure 2 the line repres ents the range of human scores (highest to lowest) with a tick at the expected score. The bar is the search engine. For TREC 4 the score of the 2 alternate assessors are designated alt1 and alt2. For TREC 6 the designation is alt; but also included is the original assessor X  X  performance measured against the alternate assessor X  X  ground truth (designated org). Manual and automatic runs were submitted to TREC 4 and 6. The best, worst, and mean MAP of thes e runs is presented in the last two columns of Figure 2. The line is the range of manual runs (the tick is the mean). The bar is the best automatic run. It can be seen that the search engine often performs inside the human range and sometimes better than expected . The notable exception is TREC 4. This, we believe, is because the alternate assessors were given a small and biased pool and were not asked to assess the original pool (they were unlikely to find new rele-vant documents). We also note that where there were more than 200 relevant documents for a topic those that were not reassessed were added to the alternate assesso r X  X  results (artificially inflating MAP). The runs submitted to TREC 4 are a better performance indicator  X  and the figure shows that by TREC 4 (1995) manual and automatic runs were performing comparably. Although of questionable utility for the INEX collection (with few topics), a paired 2-tailed t -test was conducted. The results are presented in Table 1 where it can be seen that with TREC 6 and INEX IEEE, there is no statisti cally significant difference be-tween the performance of the alternate assessor and the search engine. Upper 0.19 0.54 0.00 0.00 0.00 0.00 0.00 Expected 0.06 0.32 0.03 0.00 0.00 0.08 0.01 Lower 0.01 0.13 0.51 0.00 0.00 0.63 0.58 It is pertinent to ask whether this result is consequence of the methodology and search engine training. The high performance of the alternate assessors at TREC 4 is discussed in the previous section. In 1995 at TREC 4 and in 1997 at TREC 6 the manual and automatic runs performed at comparable levels. At INEX the number of multiple assessed topics is low which is likely reflected in the t -test results. None the less, it is reasonable to conclude that search engine performance is comparable to human performance. To examine training, EXPERI MENT 1 was re-conducted on the INEX 2006 double-assessed topics . The nonparametric Diver-gence from Randomness (I(ne)B2), TF.IDF (inner product), and BM25 functions were used. The re sults are presented in Figure 3 from where it can be seen that when BM25 was introduced (circa 1994) there was room for improvement on TF.IDF. It can also be 
