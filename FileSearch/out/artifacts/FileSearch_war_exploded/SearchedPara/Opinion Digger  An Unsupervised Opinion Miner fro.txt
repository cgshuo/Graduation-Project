 Mining customer reviews (opinion mining) has emerged as an in-teresting new research direction. Most of the reviewing websites such as Epinions.com provide some additional information on top of the review text and overall rating, including a set of predefined aspects and their ratings, and a rating guideline which shows the intended interpretation of the numerical ratings. However, the ex-isting methods have ignored this additional information. We claim that using this information, which is freely available, along with the review text can effectively improve the accuracy of opinion min-ing. We propose an unsupervised method, called Opinion Digger, which extracts important aspects of a product and determines the overall consumer X  X  satisfaction for each, by estimating a rating in the range from 1 to 5. We demonstrate the improved effectiveness of our methods on a real life dataset that we crawled from Epin-ions.com.
 H.3.3 [ Information Search and Retrieval ]: Information filtering; I.2.7 [ Natural Language Processing ]: Text Analysis Algorithms, Design, Experimentation Opinion Mining, Text Mining, Aspect Extraction, Rating Predic-tion, Sentiment Analysis
In this paper, we propose an unsupervised method, called  X  X pin-ion Digger X , for mining and summarizing opinions from unstruc-tured customer reviews. Opinion Digger can help users make better decisions by providing a summary view of the ratings for the major aspects of the product. An aspect (also called product-feature) is an attribute or component of the product that has been commented on in a review e.g.  X  X attery life X  and  X  X oom X  for a digital camera. The
However, the existing methods have ignored this additional in-formation which is freely available in the review websites. Since none of the existing benchmark datasets contains this important in-formation, we crawled the well-known review website Epinions.com and built a new dataset containing 2.5K reviews about 40 products in 5 categories.
 The main contributions of this paper are as follows:
In aspect extraction, Opinion Digger uses known aspects to mine opinion patterns from reviews and to determine the threshold for the number of matchings. In aspect rating, Opinion Digger uses the rating guideline to estimate the rating of sentiments (adjectives) and aspects. Note that the ratings of known aspects are withheld from the learning method and are used only for evaluation pur-poses. The experimental evaluation on our Epinions.com dataset supports our claim that the use of this additional information effec-tively improves the accuracy of opinion mining.
Let C = f C 1 ; C 2 ; :::; C k g be a set of product categories, like  X  X ellular phone X ,  X  X p3 Player X , and etc. For each product category C i we have a set of products P i = f P i; 1 ; P i; 2 ; :::; P i;n g , such as  X  X pple Smartphone X , Nokia 6210 X , and etc. for product category  X  X ellular phone X . For each product P i;j there is a set of reviews R of &lt;known aspects, rating&gt;, review text (sequence of words), and the overall rating. In our dataset, review text contains complete sentences (unstructured reviews). In the following we define the basic notations we use throughout this paper.

Aspect : An aspect is an attribute or component of the product that has been commented on in a review. For example,  X  X attery life X  in the opinion sentence  X  X he battery life is too short X .
Known Aspect : Known aspects are predefined aspects for each category in the review website for which users explicitly expressed ratings. Each category C i has a set of known aspects K i provided by the review website (e.g.  X  X attery life X  and  X  X urability X  for cam-corder category).

Sentiment : Sentiment is a linguistic term which refers to the direction in which a concept or opinion is interpreted [5]. We use sentiment in a more specific sense as an opinion about a product as-pect expressed by an adjective. For example,  X  X reat X  is a sentiment for the aspect  X  X icture quality X  in the sentence  X  X t has great picture quality X .

Orientation : A sentiment can be classified in n -level orienta-tion scale. In two-level orientation scale (polarity), a sentiment is either positive or negative. While most of opinion mining research considered two-level orientation scale, most of the review websites use five-level orientations, presented by stars (1 to 5 stars). In this paper we use five-level orientation and rating as synonyms.
Rating Guideline : In some of the review websites, when a user writes a review he is asked to assign an overall rating to that prod-uct. Theses websites provide some guidelines for users in assign-ing overall ratings. For example, Epinions.com provides a rating guideline stating that "rating 5 means  X  X xcellent X , rating 4 means  X  X ood X , rating 3 means  X  X verage X , rating 2 means  X  X oor X , and rating 1 means  X  X errible X ".

Problem Definition : Given a set of reviews R about multiple products that can be from different categories from C , a set of
In this paper we will employ a simple constraint on the number of matching patterns. We leave more complicated and potentially even more accurate constraints to future work. We define the factor P num which is the number of opinion patterns that are matched at least once by the potential aspect. Since the average value of P num for known aspects is 2, a potential aspect will be filtered out if P num &lt; 2 . After applying this constraint on potential aspects, for each product in each category, Opinion Digger outputs a list of filtered potential aspects to the next phase. Applying opinion patterns can eliminate most of non-aspects from the set of potential aspects. However, some of them still remain since they match the minimum number of patterns.
Current research just considers two orientations for an opinion, positive and negative, but they do not express the strength of posi-tiveness or negativeness of an opinion. In other words, they do not clarify how much an opinion is positive/negative and to what extent a reviewer recommends/not recommend that product to others. In this paper, we consider a 5-level orientation scale (1 to 5 stars) and estimate the rating of each aspect of a product on that scale.
Note that the aspect rating phase is performed for each product separately. In each product P i;j , for each aspect a i;t , Opinion Dig-ger first extracts the nearest sentiments to each occurrence of that aspect in the set of reviews R i;j . Sentiments are usually nearest ad-jectives in the same sentence segment which describe the quality of the aspect. Then a set of rated adjectives provided by Epinions.com (rating guideline) is used, and a k nearest neighbor (KNN) algo-rithm is applied to estimate the rating of each extracted sentiment. Wordnet [2] is used to compute the similarity between adjectives for the KNN algorithm. Finally, Opinion Digger aggregates the ratings of all sentiments expressed about each aspect to estimate its rating. For each product, Opinion Digger outputs a set of extracted aspects and their estimated ratings.
 The rating guideline provided by Epinions.com is illustrated in Figure 2. It is shown that in a 5-level orientation scale, most adjec-tives have two nearest neighbors, like  X  X efective X  which is placed between  X  X oor X  and  X  X errible X . Some of the adjectives, like those se-mantically placed above  X  X xcellent X  or below  X  X oor X  have just one nearest neighbor. Therefore, we set k equal to 2 and use a 2-NN algorithm for aspect rating.

For each sentiment snt Opinion Digger performs breadth first search in the Wordnet synonymy graph with the maximum depth 5 to find two rated synonyms from the rating guideline. Then it uses a distance-weighted nearest neighbor algorithm with a continuous-valued target function to return the weighted average of the rat-ings of 2-nearest neighbors as the estimated rating for the sen-aspects, and recall is equal to the percentage of true aspects which were extracted by the method.

To have a fair comparison, the comparison partners should be unsupervised and should take the same review structure as input. Since Opinion Digger takes unstructured reviews as input, and Feature-Based Summarization (FBS) [4] is the only method applicable on unstructured reviews, FBS is chosen as a comparison partner. We compare our proposed aspect extraction method with three meth-ods: Naive baseline (as also used in [4, 6, 3]), FBS, and Com-FBS. In Naive baseline, the top k frequent noun phrases for each prod-uct are selected as aspects. In this paper, since none of the gold standard lists has more than 15 aspects, we set k equal to 15. In FBS [4] two types of pruning are applied on frequent noun phrases to extract aspects from reviews. We propose Com-FBS as another comparison partner, a method which applies FBS to extract aspects for each product, and considers only candidate aspects which ap-pear in at least half of the products of the corresponding category. Table 3: Average Precision and Recall of Aspect Extraction
Table 3 shows that each improvement of the method achieves a substantial performance gain in terms of precision and recall. FBS outperforms Naive, showing the gain of the pruning meth-ods proposed by [4]. Com-FBS outperforms FBS, demonstrating the benefit of aggregation over product categories, as proposed in our approach. Finally, the full Opinion Digger method is the clear winner, demonstrating that using available additional information about known aspects effectively improves the accuracy of aspect extraction.

The comparison of the precision and recall of different categories and the consideration of the number of reviews available per cate-gory shows that all of the methods perform better for those cat-egories that have more reviews (e.g. digital camera and cellular phone).
Estimated ratings are evaluated using Ranking Loss which mea-sures the average distance between the true and predicted numerical ratings [1, 8].

Table 4 shows the ranking loss of our algorithm OPD along with various comparison partners, including the simple MAJOR-ITY baseline (as also used in [8]), the POLARITY baseline (similar to the method used in [4, 10, 11]), and the Prank method (as also used in [8]) proposed by [1]. All of these methods output a rating in the range from 1 to 5 similar to OPD. As mentioned in Section 7.1 a rating of 4 is the most common rating for all aspects and thus a pre-diction of all 4 X  X  for known aspects gives a MAJORITY baseline and a natural indication of task difficulty [8]. In the POLARITY baseline [1, 8], the rating of an aspect is determined by aggregat-ing the polarity of the corresponding adjectives. This method starts from a set of seed adjectives which are labeled as positive or nega-tive, and uses a bootstrapping mechanism to determine the polarity (positive or negative) of a new adjective. A new adjective will be labeled as positive/negative, if it appears in the synset (synonym
