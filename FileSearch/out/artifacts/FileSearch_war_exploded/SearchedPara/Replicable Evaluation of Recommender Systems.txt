 Recommender systems research is by and large based on compar-isons of recommendation algorithms X  predictive accuracies: the better the evaluation metrics (higher accuracy scores or lower pre-dictive errors), the better the recommendation algorithm. Com-paring the evaluation results of two recommendation approaches is however a difficult process as there are very many factors to be considered in the implementation of an algorithm, its evaluation, and how datasets are processed and prepared.

This tutorial shows how to present evaluation results in a clear and concise manner, while ensuring that the results are comparable, replicable and unbiased. These insights are not limited to recom-mender systems research alone, but are also valid for experiments with other types of personalized interactions and contextual infor-mation access.
 H.3.3 [ Information Search and Retrieval ]: information filtering, relevance feedback, retrieval models, search process, selection pro-cess.
 Algorithms; Design; Experimentation; Measurement; Performance Evaluation; Replicability; Reproducibility; Experimental Design; Experimental Methodology
The Recommender System community strives towards improv-ing the quality of recommendation algorithms, in order to do so, it is imperative that comparisons across recommendation approaches can be performed in an accurate and unbiased fashion. Assum-ing the assumption  X  X he higher the evaluation scores, the better the recommender algorithm X , which is usually taken for granted, it is important for both researchers and practitioners that their evalua-tions are fair. However, it is difficult to compare the results from a given evaluation of a recommender system, mainly because the very many alternatives that exist in designing and implementing an evaluation strategy. At the ACM RecSys conferences, every year there usually are several papers on evaluation, additionally there have been a number of workshops (UCERSTI [5, 8], RUE [2], Rep-Sys [4], REDD [1]) and tutorials on related topics. However, there has been little focus on the reproducibility and replicability of the evaluation and results themselves; hence, this tutorial aims to pro-vide a broader view that integrates also this aspect into a general evaluation methodology.

A related tutorial was given at ACM Hypertext 2014 [3].
This tutorial aims to give an introduction to clear and concise re-porting of evaluation methodologies and results, while at the same time ensuring that the results of the evaluation are comparable, replicable and unbiased to allow for fair comparisons with related work. The tutorial defines and presents evaluation metrics, method-ologies, and experimental configurations used in the recommender systems literature. Using these definitions, we present specific guide-lines towards reporting experimental results in the recommender systems area. As a particular focus of interest, in the tutorial we ad-dress the commons datasets and benchmarking frameworks avail-able, and how they can be applied in future publications in the rec-ommender systems field in order to overcome limitations related to the lack of reproduction and reproducibility of the experiments and results. Introduction. This part of the tutorial focuses on the basics of Evaluation. This section provides the necessary setting to under-Replication. This section focuses on replication itself, i.e. how Replication by example. This is an interactive session which presents Conclusions and wrap-up. This session concludes the tutorial and Q&amp;A. This session gives the participants the opportunity to ask
The tutorial is designed to be useful for researchers, students, and practitioners in the Recommender Systems and personalization communities, and in related areas such as Information Retrieval, Data Mining, Machine Learning and Human-Computer Interaction, working in different application domains, and concerned with im-plementation, reproduction, evaluation, research and practice.
Alan Said is a Machine Learning engineer at Recorded Future in Gothenburg, Sweden. Prior to this he was a postdoctoral re-searcher at TU Delft, and Marie Curie (ERCIM ABCDE) Fellow at Centrum Wiskunde &amp; Informatica in Amsterdam, The Netherlands. He obtained a doctorate at Technische Universit X t Berlin. His re-search interests include evaluation, benchmarking, user modeling and different aspects of recommender systems. He has served as PC member of international conferences and workshops (e.g. Rec-Sys, UMAP, HT, IIiX, ECIR, ECMLPKDD, IUI) and as reviewer for journals (e.g. UMUAI, TIST, TKDD, TWEB). In 2010, 2011, 2012 and 2014 he co-organized the CAMRa and RecSys Challenge benchmarking challenges at ACM RecSys. At the 2012 ACM Rec-Sys conference, Alan co-presented a tutorial on Best Practices in Recommender System Challenges . The tutorial outlined the neces-sary steps to be taken in order to achieve comparable results in the context of a competition or a challenge.

Alejandro Bellog X n is a Lecturer at the Aut X noma University of Madrid. Previously, he was an ERCIM Post-doctoral fellow at http://github.com/recommenders/evaltutorial http://rival.recommenders.net Centrum Wiskunde &amp; Informatica in The Netherlands. His re-search is focused on recommender systems, in particular, adap-tations from the information retrieval area, such as performance prediction techniques, evaluation methodologies, and probabilis-tic models. He has authored papers on national and international conferences, journals, and workshops in the aforementioned areas. He has co-organized a workshop on the topic of reproducibility in evaluation at the ACM RecSys conference. He has served as PC member of international conferences and workshops (such as CIKM, ECIR, and RecSys) and as reviewer for journals (e.g., IRJ, IPM, INS, TIST). At the 2014 ACM Hypertext conference, Alejan-dro Bellog X n gave a tutorial on Evaluating Recommender Systems -Ensuring Replicability of Evaluation . The tutorial was tailored to a broad audience from other fields than recommendation.
Supported in part by the Ministerio de Educaci X n y Ciencia (TIN2013-47090-C3-2). [1] A DAMOPOULOS , P., B ELLOG X N , A., C ASTELLS , P., [2] A MATRIAIN , X., C ASTELLS , P., DE V RIES , A. P., AND [3] B ELLOG X N , A. Evaluating recommender systems:ensuring [4] B ELLOG X N , A., C ASTELLS , P., S AID , A., AND T IKK [6] S AID , A., AND B ELLOG X N , A. Comparative recommender [7] S AID , A., AND B ELLOG X N , A. Rival: a toolkit to foster [8] W ILLEMSEN , M. C., B OLLEN , D. G. F. M., AND
