 More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been re-written entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and busi-ness, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003. The Waikato Environment for Knowledge Analysis (WEKA) came about through the perceived need for a unified work-bench that would allow researchers easy access to state-of-the-art techniques in machine learning. At the time of the project X  X  inception in 1992, learning algorithms were avail-able in various languages, for use on different platforms, and operated on a variety of data formats. The task of collecting together learning schemes for a comparative study on a col-lection of data sets was daunting at best. It was envisioned that WEKA would not only provide a toolbox of learning algorithms, but also a framework inside which researchers could implement new algorithms without having to be con-cerned with supporting infrastructure for data manipulation and scheme evaluation.
 Nowadays, WEKA is recognized as a landmark system in data mining and machine learning [22]. It has achieved widespread acceptance within academia and business cir-cles, and has become a widely used tool for data mining research. The book that accompanies it [35] is a popular textbook for data mining and is frequently cited in machine learning publications. Little, if any, of this success would have been possible if the system had not been released as open source software. Giving users free access to the source code has enabled a thriving community to develop and fa-cilitated the creation of many projects that incorporate or extend WEKA.
 In this paper we briefly review the WEKA workbench and the history of project, discuss new features in the recent 3.6 stable release, and highlight some of the many projects based on WEKA.
 The WEKA project aims to provide a comprehensive collec-tion of machine learning algorithms and data preprocessing tools to researchers and practitioners alike. It allows users to quickly try out and compare different machine learning methods on new data sets. Its modular, extensible architec-ture allows sophisticated data mining processes to be built up from the wide collection of base learning algorithms and tools provided. Extending the toolkit is easy thanks to a simple API, plugin mechanisms and facilities that automate the integration of new learning algorithms with WEKA X  X  graphical user interfaces.
 The workbench includes algorithms for regression, classifi-cation, clustering, association rule mining and attribute se-lection. Preliminary exploration of data is well catered for by data visualization facilities and many preprocessing tools. These, when combined with statistical evaluation of learning schemes and visualization of the results of learning, supports process models of data mining such as CRISP-DM [27]. WEKA has several graphical user interfaces that enable easy access to the underlying functionality. The main graphical user interface is the  X  X xplorer X . It has a panel-based in-terface, where different panels correspond to different data mining tasks. In the first panel, called  X  X reprocess X  panel, data can be loaded and transformed using WEKA X  X  data preprocessing tools, called  X  X ilters X . This panel is shown in Figure 2: The WEKA Knowledge Flow user interface. Figure 1. Data can be loaded from various sources, including files, URLs and databases. Supported file formats include WEKA X  X  own ARFF format, CSV, LibSVM X  X  format, and C4.5 X  X  format. It is also possible to generate data using an artificial data source and edit data manually using a dataset editor.
 The second panel in the Explorer gives access to WEKA X  X  classification and regression algorithms. The corresponding panel is called  X  X lassify X  because regression techniques are viewed as predictors of  X  X ontinuous classes X . By default, the panel runs a cross-validation for a selected learning al-gorithm on the dataset that has been been prepared in the Preprocess panel to estimate predictive performance. It also shows a textual representation of the model built from the full dataset. However, other modes of evaluation, e.g. based on a separate test set, are also supported. If applicable, the panel also provides access to graphical representations of models, e.g. decision trees. Moreover, it can visualize prediction errors in scatter plots, and also allows evaluation via ROC curves and other  X  X hreshold curves X . Models can also be saved and loaded in this panel.
 Along with supervised algorithms, WEKA also supports ap-plication of unsupervised algorithms, namely clustering al-gorithms and methods for association rule mining. These are accessible in the Explorer via the third and fourth panel respectively. The  X  X luster X  panel enables users to run a clustering algorithm on the data loaded in the Preprocess panel. It provides simple statistics for evaluation of cluster-ing performance: likelihood-based performance for statisti-cal clustering algorithms and comparison to  X  X rue X  cluster membership if this is specified in one of the attributes in the data. If applicable, visualization of the clustering struc-ture is also possible, and models can be stored persistently if necessary.
 WEKA X  X  support for clustering tasks is not as extensive as its support for classification and regression, but it has more techniques for clustering than for association rule mining, which has up to this point been somewhat neglected. Nev-ertheless, it does contain an implementation of the most well-known algorithm in this area, as well as a few other ones. These methods can be accessed via the  X  X ssociate X  panel in the Explorer.
 Perhaps one of the most important task in practical data mining is the task of identifying which attributes in the data are the most predictive ones. To this end, WEKA X  X  Explorer has a dedicated panel for attribute selection,  X  X e-lect attributes X , which gives access to a wide variety of algo-rithms and evaluation criteria for identifying the most im-portant attributes in a dataset. Due to the fact that it is possible to combine different search methods with different evaluation criteria, it is possible to configure a wide range of possible candidate techniques. Robustness of the selected attribute set can be validated via a cross-validation-based approach.
 Note that the attribute selection panel is primarily designed for exploratory data analysis. WEKA X  X   X  X ilteredClassifier X  (accessible via the Classify panel) should be used to apply attribute selection techniques in conjunction with an un-derlying classification or regression algorithm to avoid in-troducing optimistic bias in the performance estimates ob-tained. This caveat also applies to some of the preprocess-ing tools X  X ore specifically, the supervised ones X  X hat are available from the Preprocess panel.
 In many practical applications, data visualization provides important insights. These may even make it possible to avoid further analysis using machine learning and data min-ing algorithms. But even if this is not the case, they may inform the process of selecting an appropriate algorithm for the problem at hand. The last panel in the Explorer, called  X  X isualize X , provides a color-coded scatter plot ma-trix, along with the option of drilling down by selecting in-dividual plots in this matrix and selecting portions of the data to visualize. It is also possible to obtain information regarding individual datapoints, and to randomly perturb data by a chosen amount to uncover obscured data. The Explorer is designed for batch-based data processing: training data is loaded into memory in its entirety and then processed. This may not be suitable for problems involving large datasets. However, WEKA does have implementations of some algorithms that allow incremental model building, which can be applied in incremental mode from a command-line interface. The incremental nature of these algorithms is ignored in the Explorer, but can be exploited using a more recent addition to WEKA X  X  set of graphical user interfaces, namely the so-called  X  X nowledge Flow X , shown in Figure 2. Most tasks that can be tackled with the Explorer can also be handled by the Knowledge Flow. However, in addition to batch-based training, its data flow model enables incre-mental updates with processing nodes that can load and preprocess individual instances before feeding them into ap-propriate incremental learning algorithms. It also provides nodes for visualization and evaluation. Once a set-up of in-terconnected processing nodes has been configured, it can be saved for later re-use.
 The third main graphical user interface in WEKA is the  X  X xperimenter X  (see Figure 3). This interface is designed to facilitate experimental comparison of the predictive per-formance of algorithms based on the many different eval-uation criteria that are available in WEKA. Experiments can involve multiple algorithms that are run across multiple datasets; for example, using repeated cross-validation. Ex-periments can also be distributed across different compute nodes in a network to reduce the computational load for in-dividual nodes. Once an experiment has been set up, it can be saved in either XML or binary form, so that it can be re-visited if necessary. Configured and saved experiments can also be run from the command-line.
 Compared to WEKA X  X  other user interfaces, the Experi-menter is perhaps used less frequently by data mining prac-titioners. However, once preliminary experimentation has been performed in the Explorer, it is often much easier to identify a suitable algorithm for a particular dataset, or col-lection of datasets, using this alternative interface. We would like to conclude this brief exposition of WEKA X  X  main graphical user interfaces by pointing out that, regard-less of which user interface is desired, it is important to provide the Java virtual machine that is used to run WEKA with a sufficient amount of heap space. The need to pre-specify the amount of memory required, which should be set lower than the amount of physical memory of the ma-chine that is used, to avoid swapping, is perhaps the biggest stumbling block to the successful application of WEKA in practice. On the other hand, considering running time, there is no longer a significant disadvantage compared to programs written in C, a commonly-heard argument against Java for data-intensive processing tasks, due to the sophistication of just-in-time compilers in modern Java virtual machines. The WEKA project was funded by the New Zealand gov-ernment from 1993 up until recently. The original funding application was lodged in late 1992 and stated the project X  X  goals as:  X  X he programme aims to build a state-of-the-art facility for developing techniques of machine learning and investigating their application in key areas of the New Zealand economy. Specifically we will create a workbench for machine learning, determine the factors that contribute towards its successful application in the agricultural industries, and develop new methods of machine learning and ways of assessing their ef-fectiveness. X  The first few years of the project focused on the development of the interface and infrastructure of the workbench. Most of the implementation was done in C, with some evaluation routines written in Prolog, and the user interface produced Figure 4: Back then: the WEKA 2.1 workbench user inter-face. using TCL/TK. During this time the WEKA 1 acronym was coined and the Attribute Relation File Format (ARFF) used by the system was created.
 The first release of WEKA was internal and occurred in 1994. The software was very much at beta stage. The first public release (at version 2.1) was made in October 1996. Figure 4 shows the main user interface for WEKA 2.1. In July 1997, WEKA 2.2 was released. It included eight learn-ing algorithms (implementations of which were provided by their original authors) that were integrated into WEKA us-ing wrappers based on shell scripts and data pre-processing tools written in C. WEKA 2.2 also sported a facility, based on Unix Makefiles, for configuring and running large-scale experiments based on these algorithms.
 By now it was becoming increasingly difficult to maintain the software. Factors such as changes to supporting li-braries, management of dependencies and complexity of con-figuration made the job difficult for the developers and the installation experience frustrating for users. At about this time it was decided to rewrite the system entirely in Java, including implementations of the learning algorithms. This was a somewhat radical decision given that Java was less than two years old at the time. Furthermore, the runtime performance of Java made it a questionable choice for im-plementing computationally intensive machine learning al-gorithms. Nevertheless, it was decided that advantages such as  X  X rite Once, Run Anywhere X  and simple packaging and distribution outweighed these shortcomings and would facil-itate wider acceptance of the software.
 May 1998 saw the final release of the TCL/TK-based sys-tem (WEKA 2.3) and, at the middle of 1999, the 100% Java WEKA 3.0 was released. This non-graphical version of WEKA accompanied the first edition of the data mining book by Witten and Frank [34]. In November 2003, a sta-ble version of WEKA (3.4) was released in anticipation of the publication of the second edition of the book [35]. In the time between 3.0 and 3.4, the three main graphical user interfaces were developed.
 In 2005, the WEKA development team received the SIGKDD Data Mining and Discovery Service Award [22]. The award
The Weka is also an indigenous bird of New Zealand. Like the well-known Kiwi, it is flightless.
 Figure 5: Capabilities and technical information meta-data. recognized the longevity and widespread adoption of WEKA. In 2006, Pentaho Corporation became a major sponsor of the software and adopted it to form the data mining and predictive analytics component of their business intelligence suite. Pentaho is now an active contributer to the code base, and the first author is currently the maintainer-in-chief of the software. As of this writing, WEKA 3.6 (released in De-cember 2008) is the latest version of WEKA, which, given the even-odd version numbering scheme, is considered to be a feature-stable version. Many new features have been added to WEKA since ver-sion 3.4 X  X ot only in the form of new learning algorithms, but also pre-processing filters, usability improvements and support for standards. As of writing, the 3.4 code line com-prises 690 Java class files with a total of 271,447 lines of code 2 ; the 3.6 code line comprises 1,081 class files with a total of 509,903 lines of code. In this section, we discuss some of the most salient new features in WEKA 3.6. The largest change to WEKA X  X  core classes is the addition of relation-valued attributes in order to directly support multi-instance learning problems [6]. A relation-valued attribute allows each of its values to reference another set of instances (typically defining a  X  X ag X  in the multi-instance setting). Other additions to WEKA X  X  data format include an XML format for ARFF files and support for specifying instance weights in standard ARFF files.
 Another addition to the core of WEKA is the  X  X apabilities X  meta-data facility. This framework allows individual learn-ing algorithms and filters to declare what data characteris-tics they are able to handle. This, in turn, enables WEKA X  X  user interfaces to present this information and provide feed-back to the user about the applicability of a scheme for the data at hand. In a similar vein, the  X  X echnicalInformation X  classes allow schemes to supply citation details for the al-gorithm that they implement. Again, this information is formatted and exposed automatically by the user interface. Figure 5 shows technical information and capabilities for the LogitBoost classifier.
 Logging has also been improved in WEKA 3.6 with the ad-
As computed by the Unix command: wc -l. dition of a central log file. This file captures all information written to any graphical logging panel in WEKA, along with any output to standard out and standard error. Many new learning algorithms have been added since WEKA 3.4 and some existing ones have been improved. An exam-ple of the latter category is instance-based learning, where there is now support for pluggable distance functions and new data structures X  X uch as ball trees and KD trees X  X o speed up the search for nearest neighbors.
 Some of the new classification algorithms in WEKA 3.6 in-clude In addition to these algorithms, an entire package of multi-instance algorithms has been added to WEKA since ver-sion 3.4, most of which were first distributed in the separate MILK package for multi-instance learning [37].
 WEKA 3.6 also has new  X  X eta X  algorithms that can be wrapped around base learning algorithms to widen applica-bility or enhance performance: The set of clustering algorithms has also been expanded with the following members: Just as the list of learning schemes in WEKA has grown, so has the number of preprocessing tools. Some of the new filters in WEKA 3.6 include: Aside from the afore-mentioned exposure of capabilities and technical information meta data, there has been further re-finement and improvement to the GUIs in WEKA since version 3.4. The GUI Chooser X  X EKA X  X  graphical start point X  X as undergone a redesign and now provides access to various supporting user interfaces, system information and logging information, as well as the main applications in WEKA. Figure 6 shows the revamped GUI Chooser.
 Scatter plots, ROC curves, trees and graphs can all be ac-cessed from entries under the  X  X isualization X  menu. The  X  X ools X  menu provides two new supporting GUIs: Figures 7 and 8 show the SQL viewer and Bayes network editor respectively.
 Often it is useful to evaluate an algorithm on synthetic data. As mentioned earlier in this paper, the Explorer user in-terface now has a facility for generating artificial data sets Figure 9: The Explorer with an  X  X xperiment X  tab added from a plugin. using WEKA X  X  data generator tools. Artificial data suit-able for classification can be generated from decision lists, radial-basis function networks and Bayesian networks as well as the classic LED24 domain. Artificial regression data can be generated according to mathematical expressions. There are also several generators for producing artificial data for clustering purposes.
 The Knowledge Flow interface has also been improved: it now includes a new status area that can provide feedback on the operation of multiple components in a data mining pro-cess simultaneously. Other improvements to the Knowledge Flow include support for association rule mining, improved support for visualizing multiple ROC curves and a plugin mechanism. A number of plugin mechanisms have been added to WEKA since version 3.4. These allow WEKA to be extended in various ways without having to modify the classes that make up the WEKA distribution.
 New tabs in the Explorer are easily added by writing a class that extends javax.swing.JPanel and implements the in-terface weka.gui.explorer.Explorer.ExplorerPanel . Fig-Figure 10: A PMML radial basis function network loaded into the Explorer. ure 9 shows the Explorer with a new tab, provided by a plu-gin, for running simple experiments. Similar mechanisms al-low new visualizations for classifier errors, predictions, trees and graphs to be added to the pop-up menu available in the history list of the Explorer X  X   X  X lassify X  panel. The Knowl-edge Flow has a plugin mechanism that allows new compo-nents to be incorporated by simply adding their jar file (and any necessary supporting jar files) to the .knowledgeFlow/ plugins directory in the user X  X  home directory. These jar files are loaded automatically when the Knowledge Flow is started and the plugins are made available from a  X  X lugins X  tab. WEKA 3.6 includes support for importing PMML mod-els (Predictive Modeling Markup Language). PMML is a vendor-agnostic, XML-based standard for expressing statis-tical and data mining models that has gained wide-spread support from both proprietary and open-source data mining vendors. WEKA 3.6 supports import of PMML regression, general regression and neural network model types. Import of further model types, along with support for exporting PMML, will be added in future releases of WEKA. Figure 10 shows a PMML radial basis function network, created by the Clementine system, loaded into the Explorer.
 Another new feature in WEKA 3.6 is the ability to read and write data in the format used by the well known Lib-SVM and SVM-Light support vector machine implementa-tions [5]. This complements the new LibSVM and LibLIN-EAR wrapper classifiers. There are many projects that extend or wrap WEKA in some fashion. At the time of this writing, there are 46 such projects listed on the Related Projects web page of the WEKA site 3 . Some of these include: http://www.cs.waikato.ac.nz/ml/weka/index_ related.html Pentaho corporation is a provider of commercial open source business intelligence software. The Pentaho BI suite consists of reporting, interactive analysis, dashboards, ETL/data in-tegration and data mining. Each of these is a separate open source project, which are tied together by an enterprise-class open source BI platform. In late 2006, WEKA was adopted as the data mining component of the suite and since then has been integrated into the platform.
 The main point of integration between WEKA and the Pen-taho platform is with Pentaho Data Integration (PDI), also known as the Kettle project 4 . PDI is a streaming, engine-driven ETL tool. Its rich set of extract and transform opera-tions, combined with support for a large variety of databases, are a natural complement to WEKA X  X  data filters. PDI can easily export data sets in WEKA X  X  native ARFF format to be used immediately for model creation.
 Several WEKA-specific transformation steps have been cre-ated so that PDI can access WEKA algorithms and be used as both a scoring platform and a tool to automate model creation. The first of these, shown in Figure 11, is called  X  X eka Scoring. X  It enables the user to import a serialized WEKA model (classification, regression or clustering) or a supported PMML model and use it to score data as part of an ETL transformation. In an operational scenario the predictive performance of a model may decrease over time. http://kettle.pentaho.org/ Figure 11: Scoring open sales opportunities as part of an ETL transformation.
 Figure 12: Refreshing a predictive model using the Knowl-edge Flow PDI component.
 This can be caused by changes in the underlying distribu-tion of the data and is sometimes referred to as  X  X oncept drift. X  The second WEKA-specific step for PDI, shown in Figure 12, allows the user to execute an entire Knowledge Flow process as part of an transformation. This enables automated periodic recreation or refreshing of a model. Since PDI transformations can be executed and used as a source of data by the Pentaho BI server, the results of data mining can be incorporated into an overall BI process and used in reports, dashboards and analysis views. The WEKA project has come a long way in the 16 years that have elapsed since it inception in 1992. The success it has enjoyed is testament to the passion of its community and many contributors. Releasing WEKA as open source software and implementing it in Java has played no small part in its success. These two factors ensure that it remains maintainable and modifiable irrespective of the commitment or health of any particular institution or company. Many thanks to past and present members of the Waikato machine learning group and the external contributers for all the work they have put into WEKA.
 [1] I. Altintas, C. Berkley, E. Jaeger, M. Jones, B. Lud-[2] K. Bennett and M. Embrechts. An optimization per-[3] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. [4] S. Celis and D. R. Musicant. Weka-parallel: machine [5] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [6] T. G. Dietterich, R. H. Lathrop, and T. Lozano-P  X erez. [7] J. Dietzsch, N. Gehlenborg, and K. Nieselt. Mayday-[8] L. Dong, E. Frank, and S. Kramer. Ensembles of bal-[9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [10] E. Frank and S. Kramer. Ensembles of nested di-[11] R. Gaizauskas, H. Cunningham, Y. Wilks, P. Rodgers, [12] J. Gama. Functional trees. Machine Learning , [13] A. Genkin, D. D. Lewis, and D. Madigan. Large-[14] J. E. Gewehr, M. Szugat, and R. Zimmer. BioWeka X  [15] M. Hall and E. Frank. Combining naive Bayes and de-[16] K. Hornik, A. Zeileis, T. Hothorn, and C. Buchta. [17] L. Jiang and H. Zhang. Weightily averaged one-[18] R. Khoussainov, X. Zuo, and N. Kushmerick. Grid-[19] M.-A. Krogel and S. Wrobel. Facets of aggregation ap-[20] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and [21] D. Nadeau. Balie X  X aseline information extraction : [22] G. Piatetsky-Shapiro. KDnuggets news on SIGKDD [23] R Development Core Team. R: A Language and En-[24] J. J. Rodriguez, L. I. Kuncheva, and C. J. Alonso. Ro-[25] K. Sandberg. The haar wavelet transform. [26] M. Seeger. Gaussian processes for machine learning. In-[27] C. Shearer. The CRISP-DM model: The new blueprint [28] H. Shi. Best-first decision tree learning. Master X  X  thesis, [29] N. Slonim, N. Friedman, and N. Tishby. Unsupervised [30] J. Su, H. Zhang, C. X. Ling, and S. Matwin. Discrimina-[31] D. Talia, P. Trunfio, and O. Verta. Weka4ws: a wsrfen-[32] K. M. Ting and I. H. Witten. Stacking bagged and [33] J. S. Vitter. Random sampling with a reservoir. ACM [34] I. H. Witten and E. Frank. Data Mining: Practical Ma-[35] I. H. Witten and E. Frank. Data Mining: Practical [36] I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, [37] X. Xu. Statistical learning in multiple instance prob-[38] Y. Yang, X. Guan, and J. You. CLOPE: a fast and [39] F. Zheng and G. I. Webb. Efficient lazy elimination for
