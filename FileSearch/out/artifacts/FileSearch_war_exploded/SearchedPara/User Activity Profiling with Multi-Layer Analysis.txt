 In this paper, we are interested in discovering semantically mean-ingful communities from a single user X  X  perspective. We define a multi-layer analysis problem to derive a user X  X  activity profile. Such an activity profile would include what activity areas a user is involved with, how important each activity is to the user, and who else is involved with the user on each activity as well as each par-ticipant X  X  participation level. We believe a semantically meaningful community (corresponding to an activity area) must also consider the topics of the social messages rather than only the social links. While it is possible to use a hybrid approach based on traditional topic modeling, in this paper we propose a unified user modeling approach based on direct clustering over the social messages taking into considerations of both social connections and topics of social messages. Our clustering algorithm can be performed in a unified way in a unsupervised fashion as well as semi-supervised fashion when the user wants to give our algorithm some seeding inputs on his viewpoints. Moreover, when the new data comes, our algo-rithm can perform incremental updates on the new data without re-clustering the old data. Our experiments on social media datasets available from both within an enterprise and public social network demonstrate the effectiveness of our approach.
 H.3.3 [ Information Storage and Retrieval ]: clustering Social Network, User Profiling, Community Discovery
With the rapid development of online social network and col-laboration systems, people are increasingly socially connected or working together through social media. In our paper we consider social media a general sense. It consists of documents shared among a number of people. For example, an email, a tweet or a Facebook update are all considered as a piece of social media. A major task in social network analysis is to identify the underlying community structure. Current developments in machine learning and data mining provide useful techniques to discover communi-ties from text and social links.

Existing work on community discovering is a single-layer anal-ysis and takes a flattened view on the social linkages as illustrated in Figure 1 (a). In other words, the link between a pair of users is represented by a collapsed evaluation of their relationship by calcu-lating the existence or strength of linkage between any two entities. Even when trying to combine topics and links, the focus is on iden-tifying topics that are typical for such a pair.

However, for user activity profiling, we believe it is necessary to take a multi-layer view of one X  X  activities. More specifically, the closeness or topics should be specific to each area that they collab-orate on, and such information can only be revealed by consider-ing the social context (e.g., who else are involving). As a result, a multi-layer view allows one to know what activities one is in-volved, who else are involved and their activeness levels in that activity. The goal of our work in this paper is to analyze a user X  X  social messages and discover the user X  X  semantically meaningful communities shown as an example in Figure 1(b).

The main contribution of this paper is that we propose a commu-nity discovery approach based on both social links and topics of the social messages rather than only topics or only social links.
A piece of social data can be represented by a tuple &lt; W,U,T &gt; with W being the textual content, U the people involved, and T the timeline of the data. For a group G i = { e 1 ,...,e m } ( m  X  1) of data items, we concatenate or union together the corresponding parts in e 1 ,...,e m . A centroid in our approach is also represented by a tuple &lt; W,U,T &gt; .

Given two groups G 1 and G 2 of data items, the similarity mea-sures are along different dimensions, namely textual topic, people and timeline. It is possible to combine these measures to be an overall similarity. For example, a linear combination may be used. The weight of the combination may reflect user preferences. For example, if a user favors the topical factor, the weight for topical similarity may be higher.

We may employ the classic TF-IDF weighting to represent W and U with the vector space model. Then we can use the classic cosine similarity to measure sim ( W 1 ,W 2 ) and sim ( U measure the time distance between G 1 and G 2 , we have where d ( tc 1 ,tc 2 ) returns the number of days between the means of the time-stamps in T 1 and T 2 ; and  X   X  (0 , 1] is a decay factor.
Data clustering has been long studied. Some are top down parti-tioning (divisive), the others are bottom up or agglomerative [4]. It is known in literatures that partitioning clustering can have a more global view of the entire data set while agglomerative clustering tends to be more local in nature. While agglomerative clustering is easy to group together coherent clusters, it is not easy to cor-rect any mistake during the merge process because it does not have the iterative refinement process. Lastly partitioning clustering has relatively low computational overhead.

To make the best use of these two strategies, in our approach we combine both. We first use partitioning clustering similar to k-mean, then we merge clusters together bottom up with hierarchical agglomerative clustering.

For our algorithm users can input a rough number of clusters that they want to see in the results. But this is not necessary the result-ing number of clusters. This is different from k-mean clustering which always results in k clusters. In our algorithm, the hierarchi-cal agglomerative clustering step will automatically determine the best number of resulting clusters.

Suppose user X  X  input on the number of resulting clusters is k , our algorithm estimates a number s = k log n ( k ) initial centroids to start our first step. The estimated number s is based on the coupon collection theorem [2] or the classic bin-ball tossing problem in counting and probability. Consider the process of randomly toss-ing identical balls into k bins, numbered 1 , 2 ,...k . The tosses are independent and on each toss the ball is equally likely to end up in any bin. The probability that a tossed ball lands in any given bin is 1 /k . Thus the ball-tossing process is a sequence of Bernoulli tri-als with a probability 1 /k of success, where success means that the ball falls in the given bin. How many balls must one toss until every bin is likely to contain at least one ball? The answer is k log k .
Based on the coupon collection theorem, our choice of this num-ber s provides a probabilistic guarantee that for each of the k po-tential final clusters, at least one of the s initial centroids will lead to it. Our algorithm will start with initializing the s initial clusters.
In an unsupervised case, the user does not input any seeding ini-tial clusters. Our initialization will assign the largest s threads in the n data items to be the initial centroids ( representative data item) of the initial s cluster. The remaining n  X  s data items are assigned to its nearest centroid if it meets the similarity criteria function. In a semi-supervised case, user may give our algorithm t inputs. We assign a data item in n to the nearest centroid in the seeding t cen-troids if it meets the similarity criteria function.

Also note that in our initialization, our algorithm discards the data items that do not meet the criteria function. Keep in mind the goal of our clustering is only to derive the activity areas.
After the initialization of clustering, our first step performs a s  X  mean type of clustering. It iteratively refines the assignment of the data item to a cluster in order to maximize the objective func-tion. The objective function is basically the summation of the sim-ilarities between each data item and the centroid it is assigned to. For each data item i , as long as it is not the representative data item to a cluster, our algorithm will test to see if moving to an-other cluster can improve the objective function and will actually get moved to the best centroid/cluster right away. Note that this is different from traditional k  X  mean clustering. After the move the new centroid gets computed. See Steps 7-12 in Algorithm 1. The refinement process iterates until no data item needs to move to another centroid, meaning the objective function can no longer be improved.
 Also note that the representative data item does not get moved. This guarantees that the data items in an existing cluster stay in that cluster. Furthermore, when calculating the centroid, our algorithm weighs more on the representative data items. Our algorithm also weighs the user input heavily when calculating centroid so as to make sure that our algorithm can generate clusters around those user inputs. Lastly our algorithm also weighs the thread size so that a real cluster is more likely to be centered on a large thread. Our second step is the bottom up hierarchical clustering. Its similarity measurement is as discussed earlier.

As one can imagine, an incremental update function can be viewed as the semi-supervised clustering over the new data while taking the current existing clusters as seeding clusters to our al-gorithm. As a result, an incremental update can be seamlessly combined int our clustering algorithm. All in all our clustering algorithm performs unsupervised, semi-supervised clustering and incremental update on new data in a unified fashion.
We know a completely bottom up agglomerative clustering takes Algorithm 1 Unified Clustering ( n , k , t ) with t ( t may be 0) user-given seeds 1: s = max ( n,k log n ( k )) 2: Initialization: assign representative data item and initial cen-3: loop 4: for all d i that is not the representative of the current s clus-5: Let C i is the current centroid of the cluster that document 6: calculate objective function f = 7: for all C x where x ! = i do 8: suppose move d i to C x 9: re-calculate the supposedly new centroid based on the 10: if exist any f 0 x &lt; f then 11: actually move document d i to C x where its resulting 12: re-calculate the centroids after the move and actually 13: if there was no actual move of any documents then 14: return 15: iteratively merge s small clusters into big clusters based on O ( n 3 ) . However we employed a partitioning clustering to generate the s small clusters first before the agglomerative step. So in our algorithm the agglomerative clustering only takes O ( s 3 known that the partitioning clustering has the runtime advantage over the agglomerative clustering. In our algorithm it may option-ally take O ( nlogn ) steps to sort the data based on its size. In the refinement step, for each of the O ( n  X  s ) non-representative data items, our greedy algorithm tries to figure out which is the best s cluster to move the data item so as to improve the objective func-tion the best. So it takes O ( s  X  ( n  X  s )) steps. Usually the number of iterations m is a small number (eg, 10, or 20).
 Our algorithm X  X  complete runtime complexity is therefore O ( nlogn + s ( n  X  s ) m + s 3 ) in which the O ( nlogn ) sorting is optional when a random assignment of the ( s  X  t ) initial centroids is used. In our problem setting, s &lt;&lt; n and can be even con-sidered as a constant. So our runtime complexity is approximately O ( s  X  n ) . This is a significant improvement over the agglomerative clustering complexity. We profile an activity area  X  G,f w ,f u ,tl,s p  X  from each group G of data items returned by the clustering algorithm. In particular, we measure the weight of the topical words and the participants X  contribution level as well as automatically select a representative label for each community.
First, given a word w i , f w ( w i ) = 0 if w i is a stop word or a common English word; otherwise, f w ( w i ) is the number of data items in G that contain w i in their textual contents.

Second, we measure the contribution level f u ( u j ) of a partici-pant u j in the activity area based on the content he generated for that activity area. Let L = { e 1 ,...,e m } be the list of the data items in G sorted by recency, such that e i is more recent than e when i &lt; j . In reality, we may only consider a number of the most recent data items (say, m = 50 ) instead of all the items in G . Let E p is the set of data items that is contributed/generated by person p . r i is 1 if the i th item of L is in E p and r i is 0 otherwise.
We employ the Normalized Discounted Cumulative Gain (NDCG) to estimate the importance of a person X  X  contribution to the activity area derived from G . We have where Z x is chosen so that an all-positive list has NDCG value of 1. In the above measurement, the reactions to more recent items have greater effect on the contribution score. The above formula intends to capture the participant X  X  trend of contribution to the activity area and can detect his evolving active interests in different activity areas over time. For the user u who we are deriving the multi-layer social links, we also use the above calculated score as an estimate of the activity area X  X  importance level to the user over time.

Third we can also measure the contribution of a participant from topic point of view. From all the content contributed by the partic-ipant p , we can derive the top topic words to see his contribution more specifically. For example, for user u we can see the particular topic aspects that he is interested in this activity area. This is nice to know especially in a large activity area.
To select a representative keyword, the word should not only be important to the activity area but also be distinguishable from other activity areas. We employ a TFIDF-like approach to compute a representation score for each word in an activity area and select the one with the highest score. More specifically, given a word w representation score with regards to an activity area is where f w ( w i ) is the weight of w i in the activity area, | F | is the total number of the user X  X  activity areas, and | F w i | is the number of the user X  X  activity areas that contain w i as one of their top x (say x = 20 ) keywords.
Given that we are mainly interested in a single user X  X  perspective, we collected individual user X  X  communications from both within a big research institute as well as from Twitter to test our algorithm. We conducted 15 user studies in the research institute over a period of 2 months for emails and social data collected from enterprise social software such as wiki, blogs and communities. The average number of documents is around 600, and interestingly the average number of contacts for each user is about 800.

For each user, we include bi-direction communications. In other words, for email datasets, we include both inbox and outbox; and for twitter datasets, we include tweets written by this user as well as those that mention this user (such as re-tweets). Basic data pre-processing has been conducted on the raw social media documents.
Micro blogs has become a popular way for online social inter-actions. Here we show our result using the United States Presi-dent Barack Obama as an example. We have crawled all the tweets posted by him as well as the replied-to tweets by other users, started from November 1, 2009 to February 28, 2010. As a result, we have collected 6 , 134 tweets for Barack Obama. We also collected 5 , 077 tweets for Justin Bieber. Due to space limit we will only show re-sults from President Obama in this subsection.

We split the data into 2 equal data sets. We first run our algo-rithm on the first data set. Table 4.1 shows the top 6 community results for President Obama based on the first data set. Then we run incremental update on top of the first set result using the sec-ond data set. Table 2 shows the top 6 communities results. The rank of each community is only based on the size of the tweets that were clustered into same group. The participants in each community are ranked based on the number of occurrences of their name in the all tweets including their own, replied and mentions.

The results are fairly self-explanatory. Our model can capture well the coherent activity area for President Obama. From the sec-ond data set results, noticeably user 251666 was Martha Coakley, Massachusetts Attorney General, who is on top list of participants in the community which is about passing the health care reform bill. We also observe that there are two non-overlapping communities following President X  X  talking about Haiti earthquake. user 7008 is Nicki Minaj, a singer who originated from Haiti. Not surprisingly she is most concerned about the tragedy that has happened in her home country.
 Table 2: President X  X  communities incrementally on 2nd dataset.
In this section we will show the aggregated results from our user studies conducted among 15 colleagues in our institute. We divided their data into two sets. The first set contains 2 / 3 of the data, while the second data set contains the remaining 1 / 3 of the data. We run our algorithm on the first data set and record as the first result R . We then run our algorithm on the second data set on top of our first result and record as the second result R result intends to test our incremental update functionality. Before we show to the user with the result, we also ask users to come up with some activity areas that they would like to see in the results. Our algorithm is run again on the first data set but with those user inputs. We record this as the third result R 3 .

For LDA experiments, R 1 is the result when LDA is run on the first data set. Because LDA cannot incrementally update previous result with new data, we feed all data to LDA to obtain R more, because LDA is unsupervised and cannot take user inputs, we did not collect R 3 .

We presented both R 1 s from our algorithm and from LDA to-gether to users and asked the users to examine the results in same manner. We also presented both R 2 s together to users.

In our experiments, we used k = 5 and k = 10 . For each result, we asked the following questions and used the following criteria to measure the overall quality of the result:
Title precision : For each activity area, the user states whether its title label is indicative or not. For evaluating title label from LDA result, we choose the top 3 keywords for each activity area from LDA to present to the user.

Coherent precision : For each activity area, the user is asked to look at the data items inside it and mark it "coherent" or "inco-herent". A  X  X oherent X  activity area represents one of the user X  X  ongoing or past projects, while an  X  X ncoherent X  activity area does not. The user is also asked to write down any major projects that he worked on but is "missing" by any activity areas in the list.
For each activity area, the user is also asked how he would like to modify the given activity areas to better reflect the activity areas in his mind. (1)  X  X plit X : The user indicates that an activity area should be split into x ( x  X  2) smaller activity areas. (2)  X  X erge X : The user specifies that a certain set of y ( y  X  2) activity areas should be merged into one. (3)  X  X erfect X : The user indicates that the activity area correctly represents his work at the right granularity;
We re-measure the accuracy of our identified activity areas for each user based on his modification instructions. Let N k the number of the top k activity areas that are marked with both  X  X oherent X  and  X  X erfect X . The Granularity precision p 0 k k activity areas in the list is computed as p 0 k = N k ( c,p ) /k . The Granularity recall r 0 k is defined as r 0 k = N k ( c,p ) /N is the number of activity areas the user prefers to have. N puted in such a way: each of the activity areas marked with  X  X iss-ing X  or with both  X  X oherent X  and  X  X erfect X  counts 1; a activity area that needs to be split into x ( x  X  2) smaller work ares counts x ; if a group of y activity areas should be merged, the y activity ar-eas together counts 1. For example, assume that 10 activity areas { w 1 ,...,w 10 } are presented to Alice. Among the 10 activity ar-eas, the first five are  X  X oherent X  and  X  X erfect X . Alice prefers that w be split into 2 activity areas and { w 7 ,...,w 10 } be merged into one. Furthermore, Alice states that 3 activity areas are missing from the given list. In this case, we have N 0 = 5 + 2 + 1 + 3 = 11 .
Importance precision : The users are asked to mark if an activity area is important. We say that the system has estimated the relative importance of an activity area a i correctly when one of these condi-tions holds: (1) the user marks a i as  X  X mportant X , and a the top 5 activity areas in the user X  X  profile; (2) the user marks a as  X  X mportant X , and there does not exist any  X  X nimportant X  activity area that is ranked higher than a i ; (3) the user marks a portant X , and the ranking of a i is lower than 7; (4) the user marks a as  X  X nimportant X , and there does not exist any  X  X mportant X  activity area that is ranked lower than a i .

Activeness precision : The users are also asked to look at the ranks of the participants. Users are allowed to re-arrange the ranked list focusing on the top half of the ranks. We say a participant is ranked correctly if its ranks in the two lists differ at most 2.
Our evaluation results are shown in Table 3, 4, 5 and 6. We list Table 3: Coherent, Granularity Precision/Recall for our model
Table 4: Coherent, Granularity Precision/Recall for LDA results for the top 5 and 10 activity areas. The values in the tables are the average values over the 15 users we interviewed.

From Table 6, LDA results are not as good. This is partly due to the mixed topics in each activity area. It made users difficult to mark whether or not an activity area is important. Also due to the mixed topics, the participants in each activity area were not identified very correctly, leading to low precision results for LDA.
Overall our model captures well a user X  X  multi-layer activity ar-eas with different people who contribute to the activity areas at dif-ferent levels.

The "activeness" measurement result from Table 5 is between 75%  X  80% , indicating that people are indeed heavily using email or other social media nowadays. While 75%  X  80% accuracy is decent, it shows that using activeness as an estimate of importance is reasonable but not super good. When examining the discrepancy between our discovered important people list versus the participat-ing user X  X  re-ranked list, we found the reason is that some people still prefer to use phone calls or face-to-face meetings to communi-cate with each other. As a result, those people X  X  importance cannot be measured by the activeness of the content they generated. Probabilistic topic models, such as Probabilistic Latent Semantic Analysis (PLSA) [3] and Latent Dirichlet Allocation (LDA) [1], have been widely applied for analyzing text documents.

Linkages among documents, such as hyperlinks among blog posts, and citations among scientific publications, have also been extensively studied in the literature. The focus of such type of mod-els is to find the relationship of topics in documents.

Since documents are commonly related to people, recent devel-opment has taken into account the people that relate to the topics. For example, the Author-Topic (AT) model [7] considers the in-terests of each author across multiple documents, and the Author-Recipient-Topic (ART) model [6] considers topics that are specific Table 5: Importance, Activeness, Title precision for our model
Table 6: Importance, Activeness and Title precision for LDA to each author-recipient pair. The above mentioned models only consider pairwise links relation instead of community.

Some more recent work aim to discover communities by con-sidering social networks [8], and such models can discover users X  mixed membership in various communities. However these models are single-layered, without considering the general context of  X  X ho are collaborating on what X . In this problem setting, our aim is to discover a number of multi-layer semantically meaningful clusters of the user X  X  activities automatically, based on both the topics and the social connections contained in the social media.
In this paper, we propose an approach to analyze a user X  X  activity areas. Our approach can derive who a user is working/connecting with on what activity and over what time line. Our approach is based on clustering a user X  X  social media in a unified fashion for both unsupervised case and when the user gives the algorithm some seeding inputs. The algorithm also incorporates seamlessly the in-cremental update on new data without needing to re-cluster the old data, capturing the evolving process of the user X  X  social contacts and activities. Experiment results on social media datasets demon-strate the effectiveness of our model. To the best of our knowledge, this is the first model to discover a user X  X  multi-layer connections with its social contacts. It semantically enriches the communities discovered by traditional social network analysis approaches.
For future work, we are very interested in developing new appli-cations based on our modeling results. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [2] W. Feller. An introduction to probability theory and its [3] T. Hofmann. Probabilistic latent semantic analysis. In Proc. of [4] G. Karypis, E.-H. Han, and V. Kumar. Chameleon: [5] A. McCallum, X. Wang, and A. Corrada-Emmanuel. Topic [6] M. Rosen-Zvi, T. L. Griffiths, M. Steyvers, and P. Smyth. The [7] D. Zhou, E. Manavoglu, J. Li, C. L. Giles, and H. Zha.
