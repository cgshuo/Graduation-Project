 Distributed search engines comprise multiple sites deployed across geographically distant regions, each site being spe-cialized to serve the queries of local users. When a search site cannot accurately compute the results of a query, it must forward the query to other sites. This paper consid-ers the problem of selecting the documents indexed by each site focusing on replication to increase the fraction of queries processed locally. We propose RIP, an algorithm for repli-cating documents and posting lists that is practical and has two important features. RIP evaluates user interests in an online fashion and uses only local data of a site. Being an online approach simplifies the operational complexity, while locality enables higher performance when processing queries and documents. The decision procedure, on top of being on-line and local, incorporates document popularity and user queries, which is critical when assuming a replication budget for each site. Having a replication budget reflects the hard-ware constraints of any given site. We evaluate RIP against the approach of replicating popular documents statically, and show that we achieve significant gains, while having the additional benefit of supporting incremental indexes. H.3.3 [ Information Storage Systems ]: Information Re-trieval Systems Design, Experimentation, Performance Multi-site web search engine, distributed index, replication
Distributed search engines aim at horizontal scalability for Web search [2, 6]. The search engine is distributed over several search sites deployed in (smaller) data centers spread across the world. Each search site only indexes a fraction of the documents, and receives the queries of the users in its region. When a query cannot be answered locally, it is forwarded to the other sites in order to compute accurate responses [2, 8]. Distributed search engines share the work-load among several sites. Thus, deploying an extra site in a new data center increases the capacity of the search engine.
Designing an efficient distributed search engine is a chal-lenging task. Traditional search engine algorithms rely on all the resources being accessed within a single data center. However, in the case of distributed search, the index is split among several distant locations. Accessing data in another search site translates into a higher response time due to net-work latency. Ideally, a site processes most queries locally to offer low response time and avoid the cost of processing the query in several sites. To enable this property, a care-ful selection of the documents to index on each site becomes necessary to guarantee that most frequently requested docu-ments can be retrieved locally. A trivial solution is of course to replicate all documents across all sites. This solution, however desirable from a latency perspective, induces a high utilization of computer resources. Consequently, an impor-tant goal is to reduce resource utilization while providing low latency to end users through local query processing.
Brefeld et al. use machine-learning techniques to assign each document to one or multiple sites [4]. Their approach, however, does not consider the popularity of documents or a resource budget for each site. The popularity of docu-ments becomes important when having to selectively drop documents due to resource constraints. Following a differ-ent approach, Blanco et al. propose to assign each document to a single site, and later rely on user activity to proceed to documents replication [3]. Their method relies on terms dis-tribution and cache invalidations of a document to predict at which search site it is most likely to be requested. However, they do not propose any replication strategy.

Upon receiving a query, a search site processes it against its local index and then estimates whether other sites may have better documents, in which case the query is forwarded. Several heuristics have been developed for estimating the quality of documents in different sites. They rely on some partial knowledge of the content of the other search sites, either per-term score thresholds [2], or the scores of previ-ously processed queries [8]. These algorithms are conserva-tive: they can generate false positives, but no false negatives. Hence, the results computed by the distributed search en-gines are always the same as the ones generated by a single-Figure 1: Architecture of a distributed search engine site search engine. While these two approaches achieve a reasonable precision, they only target static indexes and re-quire offline computation.

Contributions. We make the following contributions in this paper:
Roadmap. The remainder of this paper is organized as follows. We describe the architecture of the distributed search engine in Section 2. In Section 3 we present RIP, and evaluate its performance in Section 4. Finally, we review related work in Section 5 and conclude with Section 6.
We consider a distributed search engine comprising a set of search sites S deployed across geographically different re-gions. The collective of the sites forming the search engine indexes a document collection D . We present a global view of the architecture of the distributed search engine in Fig-ure 1, and describe the elements composing it throughout this section.
The architecture of a search engine is typically divided into three components. The crawler fetches documents from the Web and discovers new content by following hypertext links. The indexer processes D , the collection of documents, to generate an inverted index. For each term t present in the collection, the inverted index contains a posting list, i.e. , a list of the documents that contain t . A popular technique for implementing an index is incremental indexing. Incremental indexing enables the addition, deletion, or update of indexed documents without fully regenerating the index. This fea-ture is particularly important in the case of large scale Web search engines where the cost of regenerating the full in-dex to update frequently modified Web pages ( e.g. news) is prohibitive. Incremental indexes offer a good trade-off between the freshness of the documents and the processing performance. Hence, in this paper, we consider the case of an incremental indexer. The query processor receives user queries and processes them against the index. Historically, commercial Web search engines have preferred conjunctive query processing [14]: a document has to contain all the terms in the query to be in the result set. To obtain the result set, the query processor computes the intersection of the posting lists of the query terms, evaluating the scores of the documents using a ranking function. The k (typically 10) results with the highest scores are returned to the user.
In a distributed search engine, each search site has its own index and processes the queries of the users in its region. It is therefore important to carefully select documents to index in each site to tailor the data structures of the sites for their users. For a short response time, a search site must be able to process most of the queries it receives using its local index alone. Indexing documents that are popular in a region enables locality. However, it is also important to limit the number of documents indexed at each site. The query processing latency in fact increases with the size of the index [5]. Furthermore, replicating a large fraction of the documents at each site limits the scalability of the search engine. In this work, we assume that each search site has a fixed index capacity, either due to limited resources, or arbitrarily chosen to reduce the processing time. We express this limit using number of postings, i.e. , the sum of the length of all the posting lists in the index.

To the extent of our knowledge, two approaches have been developed to compute an assignment of documents to search sites. The first one relies on machine learning algorithms to analyze new documents upon their discovery and assigns them to one or more search sites [4]. The second one assigns each document to a single master site [3]. This assignment results in a minimal index, with each document indexed in a single location. The master site of a document is respon-sible for maintaining it in its index, which guarantees that the distributed search engine has the same recall as a cen-tralized implementation. Our work builds up on the master selection approach [3]. We assume the existence of an al-gorithm that assigns each document to a single search site. Hence, the search site S i has a master index MI i which in-dexes all the documents S i is the master of. Typically, this index represents the majority of S i  X  X  index capacity.
The popularity of Web pages typically follows a power law: while most of the Web pages are unpopular, a few of them are requested very frequently. A Web page might present high locality, being popular in a single region, or be popular across many regions. Distributed search engines work better in a context where documents have a strong locality. Indeed, this means that each document only needs to be indexed by the search site located in the region where it is popular. Fortunately, a large fraction of the popular Web pages exhibit a high divergence in their popularity across regions [3]. Nevertheless, there are still documents which are popular across region boundaries and are requested by users of different search sites. The master selection algorithm, ensures that each document is indexed by its master site.
In this paper, we propose RIP, a Reactive Indexing Pro-tocol. RIP uses part of the remaining index capacity of each search site to replicate documents that are frequently re-quested locally, but were assigned to a different search site by the master selection algorithm. Contrary to the mas-ter selection algorithm, which only relies on the content of the document, RIP is reactive : it analyzes the behavior of the users at each site to dynamically adjust replication de-cisions. These fully replicated documents form the shadow index, denoted SI i for the site S i .
In a distributed search engine, a search site indexes locally only a fraction of the documents. To preserve the quality of results, a distributed search engine must generate the same results as a centralized implementation. A query submitted to a site S i must be evaluated on the full set of documents D , whether they are indexed locally (MI i and SI i ) or not. So far, the main approach to executing queries in a distributed search engine relies upon query forwarding [2, 8]. When processing a query, a search site first computes results using its local index, and then relies on a forwarding heuristic to determine whether another site may be able to provide higher quality results. If there are such sites according to the evaluation of the heuristic, then the query is forwarded to the relevant search sites for further processing. Finally, the results are then merged and returned to the user.
The forwarding heuristic is conservative with respect to query forwarding, and it can generate false positives, but no false negatives. As already mentioned in Section 2.2, it is preferable to answer a query locally, since it reduces the response time. It is therefore important to devise an accurate forwarding heuristic to reduce the forwarding rate.
Existing forwarding heuristics [2, 8] leverage properties of the search engine ranking function to compute an upper bound on the score of documents which are not indexed lo-cally. This ranking function s ( d | q ), presented on Figure 2, was introduced by Baeza-Yates et al. [2]. The score of a document d is computed by averaging partial scores over the terms of the query q . A document d has a quality score, expressed by f ( d ), and a relevance score for a term t , com-puted by g ( d | t ). The parameters w f and w g weight quality and relevance respectively. The partial score of a document d for a term t , expressed as r ( d | t ), is typically maintained in the posting lists of the index to improve query evalua-tion performance. Note that distributed search engines are compatible with more complex ranking functions, including positional features, machine learning and diversification. In these cases, the s ( d | q ) ranking function is used in the first phase of a two-phase ranking [9], which ensures that relevant documents will be known locally before the execution of the second phase of the ranking.

The heuristic we propose here supports the same rank-ing function. A site S i may partially replicate the posting lists of the master indexes of other sites in order to estimate the need to forward queries. This forwarding index is des-ignated as FI i . The posting lists of forwarding indexes are ordered by partial score. For a given term t , S i replicates the list of documents that have the highest partial score r ( d | t ). A document whose master is S j and that contains several terms may only be indexed in one of the posting lists of FI provided its other partial scores are low.
In this section, we summarize the different elements of the index of a search site. The index of a site S i is logically divided into three components: Master index MI i : It contains the documents that were assigned to S i by the master selection algorithm.
 Shadow index SI i : It contains documents that are fully replicated by S i . They were assigned to one of the other search sites by the master selection algorithm, but are repli-cated to improve the query processing locality.
 Forwarding index FI i : It contains partial information about documents assigned to the other search sites. For a given term t , the posting list associated to t in FI i contains the list of documents that have the highest partial scores for t . Documents having high partial scores are also likely to have a high popularity. Hence, SI i and FI i may overlap.
These logical indexes represent the data available to S i processing queries. The posting lists of search engines are, in most cases, ordered by document ID. This allows high compression rates, and is particularly efficient for conjunc-tive query processing [14]. In this work, we do not make any assumption on the layout of MI i and SI i . However, we present FI i as an index sorted by impact. This means that the postings are ordered by partial scores. This assumption simplifies the description of the algorithm, but there is, in practice, no reason not to implement these three indexes as a single incremental index relying on document ID ordered posting lists.
We consider the following problem. In a distributed search engine, each search site is assigned an index budget ex-pressed as the maximum number of posting lists entries a site can accomodate. The collection of documents D is split across the search sites through an initial master index se-lection algorithm. The remaining index capacity of each site is freely used to replicate documents and posting lists of the other sites. A sequence of queries is submitted to each search site and the goal is to maximize the amount of queries that are answered locally, without query forwarding. Consequently, a search site updates its index as it processes queries. A search site modifies its index only between query executions, and it does so for a given query only after its results are returned to the user.

Let res ( q ) be the set of the k documents obtaining the highest scores for the query q according the the search en-gine X  X  ranking function s ( d | q ). A site S i has to fulfill two conditions to answer q locally. First, the documents of res ( q ) must all be indexed and copied locally. The search site needs a copy of the document data to generate the snippet pre-sented on the results page, and also in the case that it uses using a two-phase ranking [9]. The search site also needs to be able to compute accurate scores for all the top-k results to display them properly ranked. This requirement can be expressed as follows: Second, S i should be able to determine, using local data structures, that no other document could potentially score higher than the lowest score of the results: where scBound ( d | q ) is the function that computes an upper bound on the score of the document d for the query q using only local information, i.e. the information from MI and FI i .

We estimate the future queries Q f i of S i using Q i , a recent query stream received by S i . Thus, at any given point, we are trying to maximize the locality of the queries in Q i increase the probability that future queries will be answered locally. Let us focus on the first locality condition. The cost of replicating a document is equivalent to the number of posting list entries it requires in the index. To simplify the problem, suppose that all documents contain the same num-ber of terms and therefore have the same cost. The problem we are trying to solve is a particular form of the knapsack problem. The objects we are selecting are queries. The util-ity of selecting a query is proportional to its frequency, while its cost is equal to the indexing of the results, as well as the partial information ensuring the quality of results.
Given that the search engine aims at serving the k best results for each query, we could make the simplifying as-sumption that all queries have the same cost: indexing k documents. However, even in this case, the complexity of the problem arises from the fact that many queries share re-sults. Hence, the cost of a selection is not equal to the cost of each query, as some documents would be counted several times. The knapsack problem is NP -hard, but has greedy heuristics that perform reasonably well. In particular, the most common approach consists of selecting the objects in a decreasing order of value cost . In our case, the cost of selecting a query depends on the previously selected ones; hence the costs should be re-evaluated at each step of the algorithm.
As presented in Section 3.1, a search site has to satisfy two conditions to answer a query locally. The first one is that the results should be indexed locally, and the second one is that the search site should have enough information to guarantee that no other document in D can score higher. The computation of an exact optimal solution is NP-hard, which leads us towards heuristic solutions. Moreover, we need to consider practical implementation constraints in the design of our algorithm.

This work targets search engines performing incremental indexing. The index of the search engine is regularly up-dated, and the algorithm must account for the presence of new documents. This eliminates the possibility of relying solely on an offline algorithm executed during the initial in-dex generation. A practical solution should also use a min-imal amount of computation and memory, so as to ensure that as many resources as possible are dedicated to query processing. Part of the computational cost of the problem presented in Section 3.1 arises from the fact that the bene-fit of replicating a document cannot be simply evaluated, it depends on the full selection of replicated documents. From this consideration, it would be tempting to devise a solution based on hypergraphs of documents and queries to model replication dependencies. However, given the scale of the document collections we consider, such data structures do not scale, both with respect to their memory consumption and the processing cost required to exploit them.

Inspired by previous work in Web caches [13], we propose a Reactive Indexing Protocol (RIP). Each search site en-gine monitors the queries of its users to gather local statis-tics about the frequency of terms and documents. Based on these observations, our algorithm evaluates the utility of replicating information. A search site S i may either repli-cate documents, to ensure that the results are copied locally, or fragments of the posting lists of other sites, to increase its knowledge of their document collection and make more accurate query forwarding predictions by computing tighter score bounds. As introduced in Section 2.4, S i indexes fully replicated documents in SI i , while the replicated fragments of posting lists form FI i .
Distributed search engines rely on query forwarding heuris-tics to determine whether a given query q should be eval-uated on other search sites to improve the quality of the results. The role of the forwarding heuristic is to compute, for all documents d  X  X  , a score upper bound scBound ( d | q ). If the top-k documents are either not fully replicated locally, or cannot be clearly identified, the search engine decides to forward the query to the other sites to guarantee the quality of results. In this work, we introduce a new query forwarding heuristic and RIP, its associated index replication algorithm.
The forwarding heuristic we propose stems from the NRA top-k processing algorithms [11], with a few adaptations to deal with incomplete posting lists. In NRA, posting lists are sorted by impact and processed from top to bottom. NRA maintains a sorted heap of potential top-k results with up-per and lower bounds on their scores. These bounds are updated as the processing progresses down the posting lists. As soon as the upper bound of the ( k + 1) th document is lower than the lower bound of the k th document, the top-k results are identified and the algorithm terminates. In the worst case, NRA has to process the full posting lists, but, in most situations, it achieves significant performance gains and only processes a small fraction of the index. The forwarding heuristic performs a similar computation. It pro-cesses the query over the forwarding index FI i and computes upper bounds on scores. The posting lists of the forwarding index are only partial, but they are continuous. A posting list replicated by S i for a term t down to the score value v contains all the documents of D whose master is differ-ent from S i and whose partial score r ( d | t ) is higher than v . Therefore, for a given term, FI i provides either an exact par-tial score, or an upper bound equal to the score of the last posting list entry. While processing, the forwarding heuristic ignores the documents present in the shadow index SI i , as they are already evaluated by traditional query evaluation and are assigned a precise score.

We illustrate the forwarding algorithm with the exam-ple of Figure 3. The query of the user is  X  t 1 the figure displays the posting lists of FI i corresponding to those terms which the forwarding heuristic evaluates to de-cide whether the query should be forwarded. The top doc-uments for t 1 are replicated in SI i (in bold), so they do not need to be considered. The following document is d 555 , so we know its exact partial score for this term. This document is also present in the posting list of t 2 , so we will also find its exact partial score for t 2 as the top-k execution progresses. However, d 555 is not represented in the posting list of t The last known document of this posting list is d a consequence, we use its partial score as an upper bound of d 555  X  X  partial score for t 3 . Hence, the upper bound score computed for d 555 is (23.1+17.3+14.9)/3=18.4. We can also compute a bound on the score of any document absent from these posting lists using the scores of the last entries (22.8, 16.7 and 14.9 in this example). Using FI i , the forwarding heuristic computes the highest possible score for a document that is not indexed locally and compares it with the score of local documents (MI i and SI i ).

As an optimization, when a posting list is fully replicated ( i.e. replicated down to the 0 score), the forwarding heuristic leverages the conjunctive properties of the ranking function. Any document that is absent from this posting list can be ignored, as it cannot be part of the results.
The replication algorithm works as follows. For each term t , a site maintains two replication thresholds, expressed in par-tial score values: the document replication threshold td t the postings replication threshold tp t . RIP reactively ad-justs these thresholds using the activity of the local users to determine which documents and postings are replicated.
For the example described on Figure 3, td t 1 is 24.2, while tp 1 is 22.8. By lowering td t , RIP decreases the highest scores associated to t for a non local document. Lowering tp decreases the lowest score associated to t in FI i . Both these actions increase the information related to the term t and decrease the amount of query forwarding. However, their impact and cost can vary significantly. Fully replicating a document is costly, as it generates one posting entry per unique term in the document. On average, a Web page con-tains 250 unique terms [15], therefore replicating a document is 250 times more costly than replicating a posting entry. Given that the differences in partial scores between entries are, in most cases, higher among high quality documents, fully replicating a document often has a higher positive im-pact on query forwarding. RIP X  X  objective is to achieve a good balance between documents and postings replication to use the replication budget as efficiently as possible.
After each query execution, RIP analyses the query results to determine which data should be replicated to ensure that, in the future, this query could be processed locally. Let w be the lowest score of the last document returned as a one term, then the replication operation is trivial, and the algorithm determines that td t 1 should be w . However, if the query contains several terms, then the algorithm has to decide whether it should replicate documents or posting lists. The algorithm we propose relies on a parameter  X  to balance the replication between documents and postings. Using the scoring function s ( d | q ), it is possible to verify that for all the documents present in a single posting list of FI the forwarding heuristic has enough data to compute a score upper bound at most equal to w : By definition, replicating document provides the correspond-ing postings, so td t  X  tp t . As a consequence, given that | q |  X  2,  X   X  0 . 5. When  X  is low, RIP favors replicating documents, which increases the probability of having query results in the local index. However, the forwarding heuris-tic has less information to ensure that these local results are optimal. On the contrary, a high value of  X  provides a very accurate forwarding heuristic, but fewer replicated documents.

In practice, some of the documents are present in several posting lists. Hence, they have precise values for several terms, and their score estimations may exceed w . The re-sults of the query, for instance, will be present in all the posting lists matching the query, and will generate scores higher than w . Similarly, other documents present in at least 2 posting lists, such as d 555 in Figure 3, may have high upper bounds on their score and could trigger the query for-warding mechanism. When these documents are not part of the query results, query forwarding is unnecessary. In order to avoid these cases of false positives, RIP identifies these documents and fully indexes them in SI i .
RIP needs to estimate the amount of documents or post-ings a replication decision represents before deciding whether it should be applied or not. Furthermore, taking replication decisions at the level of a single posting may lead to unstable results and generate a high overhead.

Each search site estimates loosely the score distribution for each term by regularly probing the other search sites. This data structure is comprised of blocks, and is illustrated in Figure 4. A block constitutes a unit of replication identi-fied by its index as well as its score bounds. This information is not required to be perfect, and can be obtained through sampling. The first block has size k , and the size of the following blocks increases exponentially, using a power of 2.
We adapt RIP to apply the replication thresholds td t and tp t on blocks of documents and postings instead of single el-ements. Figure 4 presents the computation of the thresholds for the query  X  t 4 , t 5  X . The lowest score w = 8 . 5, and  X  = 0 . 6. RIP determines that td t 4 should be 0 . 6  X  2  X  8 . 5 = 10 . 2. Therefore, the first 2 blocks of documents should be repli-cated (in bold), and dt t 4 becomes 9.8, once adjusted to the block limit. tp t 5 is evaluated to be (1  X  0 . 6)  X  2  X  8 . 5 / (2  X  1) = 6 . 8, which means that 3 blocks of postings should be repli-cated (in italics) and tp t 5 is adjusted to 6.4. This operation is then repeated for td t 5 and tp t 4 .

The usage of blocks brings several benefits to RIP. It ma-terializes a small number of well defined replication bounds, which favors clear replication decisions while lowering the amount of memory required to maintain them. In addition, it ensures the continuity of replicated data. A replication decision caused by a given query may, as a side effect, trig-ger the replication of blocks that contain documents useful for other future queries. RIP does not directly proceed to the replication of data. Instead, it maintains counters about the number of times a particular piece of information was determined to be useful. We refer to these counters as the temperature of the data. The higher the temperature, the more useful it is. The tem-perature values are used to determine which data should be replicated within the budget constraint. As explained previ-ously, depending on the data type, the cost of the replication varies. Given that a document has on average 250 distinct terms, the cost of replicating a document is on average 250, the cost of replicating the documents of a block of size n is on average 250 n , and the cost of replicating only the postings of this block is exactly n .

We rely on the cubic selection scheme [13] to decide which elements are selected for replication. This approach was ini-tially designed to cache Web objects. Hence, in this con-text, the size of an object is always precisely known, and there are no dependencies between objects. In our case, the size of an object is first estimated, and then corrected when a replication decision is actually taken. For instance, RIP first assigns a cost of 250 to a document, and then corrects it upon receiving the content of the document. Similarly, evicting a block whose documents are replicated does not necessarily free 250 n , as some of the documents may remain replicated due to other decisions ( e.g. replicated blocks of other terms). We also ensure that the continuity of block replication is maintained. It is impossible to evict a block without evicting all the following blocks.
Web search engines are often designed to support incre-mental indexing. This feature is particularly used for pop-ular Web pages that are frequently modified, such as news Websites. Replicating information in several locations poses the problem of data consistency. If a search site does not take into account a new version of a Web page, it will serve stale results to its users.

To ensure that index updates are propagated, a search site keeps track of which other sites replicate the documents it is the master of. In addition, for each term, it maintains the documents and postings replication thresholds of each one of the other sites. When the crawler sends a new version of a document to its master site S i , MI i is updated and all the sites replicating any stale data are notified.
To evaluate RIP X  X  efficiency, we simulate a distributed search engine configuration consisting of five search sites S = { S 1 ...S 5 } . We first compute optimal query results through a centralized search engine indexing all documents, and then evaluate each site X  X  ability to generate these re-sults using its master index and the data it replicates. Each search site is associated with a query log originating from neighboring countries and collected from the front-end of a commercial search engine. In total, we sample 7 , 023 , 102 consecutive queries, and split them chronologically into a training set and a testing set of equal size. The collection of documents consists of 31 , 599 , 910 Web pages randomly sampled from the index of the same search engine.

We rely on the distribution of terms in queries and doc-uments to assign each document to a master site ( feature), as in the work of Blanco et al. [3]. This process creates, for each site, the master index MI, and represents the initial assignment of documents to sites in our evalu-ation. In this work, the master index creation process is fixed, and these indexes remain constant throughout the ex-periments. We evaluate RIP X  X  ability to generate SI and FI, and to reduce the query forwarding rate.

Commercial search engines often cache query results to avoid re-evaluating repeated queries. To make our experi-ment more realistic, we assume that the search engine im-plements a results cache with a Time-To-Live (TTL): cached results are only served if their TTL has not expired. We use a TTL value of 2 hours. For incremental indexes, it is possi-ble for a results cache to return stale results, which happens when the result set does not reflect accurately the current state of the index. In our setup, the TTL of the cache is 2 hours. Such a TTL value is moderately aggressive: the staleness of the results will never exceed 2 hours.
Blanco et al. [3] observe that most documents exhibit a high divergence of popularity among the different search sites. They are very popular in a given region, but are rarely requested at other search sites. We confirm this observation by evaluating how the popularity ranking of documents dif-fers across search sites. We execute the training queries on the search engine and count how often each document is returned as part of the top-10 results.

In Figure 5, we present the similarity between the list of documents that are most popular at each individual search site, and the documents that are globally popular, among Figure 5: Similarity between documents locally pop-ular and documents globally popular Figure 6: Similarity between locally popular docu-ments at 2 sites all the search sites taken together. On average, the set of the x documents most popular at a given site and the set of the x documents most popular considering the activity of all sites globally only overlap by 40%. This observation clearly illustrates how users from different regions have diverse in-terests. This argues in favor of an index replication policy that relies on local user activity and optimizes the replicated data for each site individually.

The comparison of popular documents between pairs of sites, presented in Figure 6, shows even more differences. The pair of sites exhibiting the highest similarity is un-der 40%. This similarity is due to the language used in queries, as those two regions share the same regional lan-guage. Note that the similarity of the very few most pop-ular documents is typically higher. This is due to very few documents being popular across different regions.
The results cache generates a hit when identical queries are submitted to the search engine within its TTL interval. Longer queries typically present lower frequency compared to short ones [1], and therefore the results cache affects them differently. In our experiments, the average hit rate of the results cache is 48.4%. Figure 7 illustrates the distribution of queries depending on their length. Queries of length 2 repre-sent the largest fraction of the workload, followed by single term queries. As expected, the hit rate of the cache drops
Figure 7: Queries distribution and cache efficiency as we increase the length of queries; long queries are more likely to be unique. Hence, the workload of a search engine is often dominated by the processing of longer queries. This observation significantly hardens the task of the forwarding heuristic. Indeed, long queries involve more posting lists, and generally have results whose partial scores are lower: they require more replicated data to be indexed locally.
We evaluate RIP and its query forwarding heuristic by running queries in our logs against the collection of docu-ments. First, we warm up the replication algorithm and the cache using the training queries. Next, we process the testing queries to evaluate the amount of query forwarding. For these experiments, we use the forwarding heuristic of Section 3.3.1 and one of the following replication schemes: Static global documents replication (SDR): Using the training queries, we determine which documents are globally popular and replicate this static set across all sites [2, 8]. In this case, FI contains an upper bound per term for non replicated documents, which corresponds to the thresholds of Baeza-Yates et al. [2].
 Reactive documents replication (RDR): Each search site reactively determines which documents are most fre-quently part of the results of their users and replicate the most popular ones. This setup computes a different set of replicated documents for each site to match the activities of their users. As with SDR, FI contains one bound per term, dynamically adjusted to reflect document replication. Reactive Indexing Protocol (RIP): Each site reactively replicates blocks of documents and posting lists, as well as individual documents when they generate false positives. This is the approach we describe in Section 3.3.3. Each site replicates data matching the needs of its users, while preserving the continuity of information in posting lists.
We evaluate the three different replication schemes us-ing different replication budgets and present the results in Figure 8. The budget is represented as the maximum num-ber of documents replicated, and we assume, as explained in Section 3.3.2, that 250 individual posting list entries use the same space as a fully indexed document. The results clearly indicate that the two algorithms based on local in-formation outperform the static documents replication us-
Figure 8: Query locality wrt replication budget ing global statistics. Indeed, as observed in Section 4.1, the users of each site are interested in different documents.
For a low replication budget, below 100,000, we observe that simply replicating the results of the queries is more efficient than replicating blocks of documents and posting lists. However, as the budget increases, the blocks replica-tion scheme clearly outperforms the replication of individ-ual documents. This difference grows with the amount of space dedicated to replication. With a replication budget of 1,000,000 documents, each search site has an average index-ing capacity of 7,119,982 documents (22.5% of the total col-lection), which represents an overhead of 14% over a setup without any replication. In this configuration, RIP raises the amount of queries processed locally by 23%, while RDR raises it by 13%. Note that the  X  parameter of RIP, used to compute replication thresholds, is set to 0.6. In practice, any value between 0.55 and 0.65 obtains good performance, above 59.7% with a budget of 1,000,000.
 We detail the performance of RDR and RIP in Figure 9. With a small replication budget, it is most efficient to focus replication on single term queries. They only require lit-tle replicated data to be answered locally, as the results are simply the documents with the highest scores for the query term. RDR performs well for these easy queries. Given that one-term queries are less likely to be unique, the tem-perature of their results increases over time and they are replicated. However, when the replication budget increases, it becomes more interesting to also replicate data for longer queries. The results show that RDR is unable to answer these queries, even with a large budget. The documents that are part of the results may be replicated. However, given that the corresponding posting lists are not replicated, the search engine is unable to ensure that the query results are optimal, and the forwarding heuristic returns a false positive. RIP can compute low thresholds, even for longer queries, and is able to answer locally over 10% of long queries by replicating continuous blocks of documents and postings.
Figure 10 presents the position of the query results in the posting list blocks of RIP X  X  forwarding index, depending on the query length. For a one term query, the result set comprises the documents with the highest partial scores for the term. Hence, they are all located in the first posting list block, which is a small amount of information for RIP to replicate to answer these queries correctly. However, as the length of the query increases, the matching documents are less frequent, due to the conjunctive nature of the query processing. As a consequence, they are located in deeper blocks, and require more replicated information to enable local processing. For example, 67% of the results of 5-term queries are located in the 10 th block. Given that the size of blocks is a power of two, this data is costly to replicate, making forwarding more frequent for long queries.

The replication algorithms rely on previous queries to compute a replication scheme and increase the probability of answering future queries locally. When a query is repeated, it can be answered by the results cache, if it falls within the TTL, or by the data replicated upon the first occurrence of the query. New queries however are more challenging. We examine the query processing locality for new queries on Figure 11, with a replication budget of 1,000,000.
SDR is particularly efficient at processing new one-term queries locally, since it benefits from document popularity information from all search sites. A query that is processed for the first time in a site might have been present at another site during the training period. Consequently, the static replication has included it in the computation of the list of replicated documents. Figure 11: Query forwarding rate for new queries
RDR has higher overall performance than SDR, since it targets the set of replicated documents for each site. How-ever, since it only relies on the queries processed at a given site to build the list of replicated documents, the perfor-mance for new queries is low. New queries can only be processed locally if their data has been replicated due to previous distinct queries executed at this site. As RDR only replicates the documents in result sets, it is unlikely that the site has replicated all the results of the new query and the data necessary to ensure that local results are sufficient.
RIP performs well for new queries. Although it only relies on local knowledge, as it is the case for RDR, the block replication pattern favors new queries. Instead of precisely replicating the documents answering a particular query, RIP transfers blocks of data, which contain additional documents and postings related to the query terms. When a new query arrives, the algorithm is more likely to have replicated a sufficient amount of information for each of the query terms, which increases the probability of processing it locally.
Our forwarding heuristic always forwards a query if there another search site may improve its results. As a conse-quence, the forwarding heuristic does not generate false neg-atives. For some queries, the search site has the query results in its index but forwards the query nevertheless, because it cannot prove that those are the best results. These cases constitute false positives, as forwarding the query does not modify its results. Considering a setup with a replication budget of 1,000,000, the proportion of false positives among the forwarding decisions is 51%, 53% and 46% for SDR, RDR, and RIP respectively.

Allowing the search engine to return non-optimal results by allowing false negatives reduces the query forwarding rate. The forwarding heuristic computes a score that corre-sponds to the limits of its knowledge. We use this score, as well as the fact that long queries are more difficult to pre-dict, to rank forwarding decisions. We display, on Figure 12, the distribution of true and false positives (ROC curve [12]). As one-term queries only involve one posting list, making it impossible to generate a false positive, we ignore them in this experiment. The remaining query lengths generate dis-tinctive curve fragments. These fragments remain relatively close to a diagonal, which means that separating true and false positives using solely the knowledge score and query length is difficult. Indeed, scores can vary significantly de-pending on the query term, and cannot be easily compared. Note that the curve representing RIP is bellow the one of the other algorithms. This is because RIP presents an overall higher performance, and therefore forwards fewer queries. The remaining false positives are therefore harder to detect.
The problem of assigning documents to sites in distributed search engine has been studied in previous work, and two main approaches have been developed to assign documents to sites. Baeza-Yates et al. propose to replicate a set of global, high quality documents to all search sites [2]; Cam-bazoglu et al. follow the same approach [8]. As shown in Section 4.2, the documents users are interested in signifi-cantly differ across regions. Thus, it is more efficient to perform fine grain replication, and select different replicated documents for each search site. Our experiments presented in Section 4.3 show that per-site replication algorithms sys-tematically outperform global replication decisions.
Brefeld et al. propose to use machine-learning techniques to statically assign documents to search sites [4]. This ap-proach optimizes the replication of documents for each site, but it relies on attributes, such as the language of the docu-ment, which are weakly correlated with its popularity. Con-trary to this approach, the algorithm we propose is reactive, and only replicates documents when it observes that users actually request them. The algorithm we propose also ex-plicitly prioritizes pieces of replicated data depending on their size and temperature. The approach of Brefeld et al. relies on an algorithm that simply assigns documents to search sites, and it does not enable an operator to vary the indexing capacity.

Existing query forwarding algorithms rely on the compu-tation of upper bounds to estimate the score of documents that are not indexed locally. Baeza-Yates et al. [2] compute a bound for each term and each search site. Cambazoglu et al. [8] refine this approach and show that maintaining bounds on pairs of terms more precise estimates of scores and can reduce the amount of query forwarding. In the case of a static index, these bounds can easily be computed dur-ing the generation of the index, and do not vary at runtime. However, when the index and the set of replicated docu-ments are dynamically selected, these bounds need to be updated. This process can become particularly costly when each site keeps track of many bounds, which is the case in the algorithm of Cambazoglu et al. [8]. In that case, one possibility is to maintain these bounds lazily using a sliding window scheme. Yet, it reduces the effectiveness of the re-sults cache, since results computed using these bounds will be time-stamped with the oldest bound used during their computation. The approach we propose explicitly maintains two bounds per term, which is a reasonable trade-off between their accuracy and the maintenance cost.

The main difference between the algorithms we propose and previous work [7, 8] is the interaction between the repli-cation algorithm and the forwarding heuristic. While other algorithms focus on replicating popular documents, our ap-proach also maintains single posting lists elements to ensure the continuity of the information on the scores for a given term. As there is no  X  X ole X  in the forwarding index, the upper bounds for each term are lower. In addition, by repli-cating postings further, we efficiently lower the bounds com-puted for longer queries. Overall, this significantly reduces the amount of query forwarding.

Ding and Suel [10] developed an algorithm for fast top-k processing on document ID-ordered posting lists. By main-taining upper bounds on the partial scores of compressed posting list segments, they significantly reduce both the amount of computation and the processing time. Our ap-proach for block replication could be adapted to replicate document ID-ordered blocks using these thresholds. How-ever, this would lead to a significant overhead for storing the index, since documents obtaining low scores would also be replicated if they belong to a segment with a high thresh-old. Consequently, we opted for strictly following the order of partial scores to replicate information.
We propose RIP, a Reactive Indexing Protocol for dis-tributed search engines. With RIP, the search engine ini-tially indexes each document on a single master site, and monitors the local user activity of each site to generate an index replication scheme. Our scheme replicates docu-ments and fragments of posting lists. RIP enables a signif-icant reduction of the amount of query forwarding between search sites, and consequently, of query processing latency. We show experimentally that, by making local decisions, RIP significantly outperforms previous replication strate-gies based on global information: in a 5-site setup, when each site has an index capacity of 22.5% of the documents collection, 60% of the queries are processed locally. We also show that replicating fragments of posting lists allows the query forwarding heuristic to compute lower score thresh-olds on unknown documents, which increases performance. Finally, RIP has by design the additional benefit of being an online approach, supporting both incremental indexing and a precise index capacity configuration, which simplifies operations in a production environment.
 This work has been partially supported by the COAST project (ICT-248036), funded by the European Community. The authors have been also supported by the INNCORPORA -Torres Quevedo Program from the Spanish Ministry of Sci-ence and Innovation, co-funded by the European Social Fund.
