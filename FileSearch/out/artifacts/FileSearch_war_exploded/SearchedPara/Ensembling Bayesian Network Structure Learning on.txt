 In recent years, Bagging method has been applied to learn Bayesian networks (BNs), especially on limited datasets. However, the BNs learned using Bagging method from lim-ited datasets can be biased towards complex models. We present an efficient approach to produce more accurate BNs from limited datasets. Based on the Markov condition of BN learning, we proposed a novel sampling method, called Root Nodes based Sampling (RNS), and a BNs fusion method. The experimental results reveal that our ensemble method can achieve more accurate results in terms of accuracy on limited datasets.
 H.2.8 [ Database Management ]: Database applications X  Data Mining ; I.2.6 [ Artificial Intelligence ]: Learning X  Knowledge acquisition Algorithms, Experimentation, Performance, Reliability Bayesian network, Ensemble method, Sampling method, Fu-sion method
Bagging method [3] is one of the most popular methods of ensemble learning. Bagging method uses Efron X  X  boot-strap sampling [2]. Bootstrap sampling is a powerful tool for estimating various properties of a given statistic in statis-tics. Therefore, Bagging method quickly gained popularity in model selection. When learning the structure of a graphi-cal model from limited datasets, such as the gene-expression datasets, Bagging has been applied to explore model struc-tures.[4],[5],[8],[1] However, Bagging method has some disadvantages over BN learning. This is because bootstrap sampling method involves a few problems. For example, as described by Steck and Jaakkola in [8], the bootstrap method produces the spu-rious dependences problem which can cause the graphical models learned from the datasets be biased towards com-plex models, especially from limited datasets. As a result, many extra edges may be present in the learned model struc-tures, and the confidence in the presence of the extra edges may be overestimated.
 In this paper, we propose a novel sampling method called Root Nodes based Sampling (RNS) method and a compo-nents fusion method. Since the distributions over the sam-pling datasets obtained using RNS method and the true BN satisfy the Markov condition [7], RNS method reduces the structural inconsistencies between the Bayesian networks learned from the sampling datasets and the true BN.
The rest of this paper is organized as follows. In Section 2, we propose the process of our ensemble based BN learning approach. We present the details of our sampling technique and components fusion technique in sections 3 and 4. In section 5, experimental results are compared and analyzed. Finally, we conclude our work in section 6.
Ensemble methods are the learning algorithms that a com-ponent learner is trained multiple times for the same task, and these trained outcomes (components) are combined for dealing with future instances.
 Given the original training dataset D , our ensemble based BN learning approach applies sampling technique to gen-erate several sampling datasets D i . Then, the component learner learns a component (BN) BN i from each sampling dataset D i . Finally, using components fusion technique, these learned components are combined into a more accurate BN.
Constructing a good ensemble for BN learning involves holding the accuracy and increasing the diversity among the component learners. The idea behind RNS method is based on the following 2 facts: 1. The distributions over the sampling datasets obtained 2. The joint marginal probability distributions of the com-
The detail of RNS method is shown in Fig.1.
The idea behind our root nodes search method is based on the following rules.

Rule 1. If Ind ( X, Z | NULL ) and Dep ( X, Z | Y ) ,then we can add a head-to-head pattern among X , Y and Z , that is X  X  X  X  Y and Z  X  X  X  Y .
 Rule 2. Given Ind ( X, Z | NULL ) , Dep ( X, Y | NULL ) , Dep ( Y, Z | NULL ) , X  X  X  X  Y  X  Z ,if Ind ( X, Z | Y ) ,thenwe can direct the edge Y  X  Z to be Y  X  Z , that is X  X  X  X  Y  X  Z .
The search method is shown in Fig.2.  X  denotes the upper bound on the number of nodes in a group of nodes.
We consider the following two possibilities for the node X which is not root node in the true BN. 1. There exist two or more root nodes that have directed 2. There exists only one root node R 0 that has directed
However, the faithfulness assumption often can not be sat-isfied on limited datasets. Some results after step 5 in Fig.2 may be not root nodes in the true BN, which are called  X  X seudo root node X . These exceptions often occur when there are many nodes in a group.
 We take 2 steps to solve the pseudo root nodes problem. The first step is to limit the number of nodes in a group (see step 4 in Fig.2). We discard the groups in which the number of nodes are larger than a given threshold  X  .The step makes exhaustive search practically feasible and greatly enhance the accuracy of our search method. The other step is to detect pseudo root nodes and delete them (step 6 in Fig.2). The detection for pseudo root nodes is based on the fact that the ancestors of a node in a group must be in the same group. So, we applies the following rule to detect the pseudo root nodes.

Assume that GrN i is a nodes group. X, Y, R  X  GrN i . R is a root node found after step 5 in Fig.2.
 Rule 3. If  X  S , S  X  GrN i , R/  X  S , Ind ( X, Y | S ) and Dep ( X, Y | S, R ) ,thennode R is pseudo root node.
Even if our method finally learned a tiny number of pseudo root nodes from small training datasets, we can prove that if RNS method is used by the pseudo root node, it does not change the Markov independences of other nodes implied by the true BN, except for its ancestor nodes. This is one cause that we limit the number of nodes in a group. Moreover, Our intergroup fusion method discussed in Section 4.2 further eliminated the effect of the pseudo root nodes problem.
Our fusion method includes 2 parts: to integrate the com-ponents (BNs) learned from the sampling datasets in the same group; to integrate the intergroup undirected networks generated in the first part.

Assume that we obtained m groups Gr i ( i =1 ,...,m )of sampling datasets using RNS method on the original train-ing dataset D and each group Gr i ( i =1 ,...,m )contains L sampling datasets.

Assume that we learned m groups GBN i ( i =1 ,...,m )of components (BNs) using selected BN learner. Each group GBN i consists of the L i BNs { BN i 1 ,...,BN iL i } learned from the L i sampling datasets in the group Gr i using se-lected BN learner.
We see that each Bayesian network learned from the sam-pling datasets obtained using RNS method contains all the Markov independences implied by the true BN, which avoids the advent of extra edges in the learned BNs. Moreover, al-most each edge in the true BN can be included in most of the BNs of a group GBN i ( i =1 ,...,m ). So, after the intra-group BNs fusion, we can get an undirected network which nearly includes all the edges in the true BN. The process is shown in Fig.4.
For any edge e which directed arc e is in the BNs BN i 1 , ... , BN iL i , consider the quantity
In the undirected network UG i ,everyedge e has a prob-ability table g i ( e ) which represents the probabilities g directed arcs e  X  X  e  X  ,e  X  } : g i ( e )= 1 L i { e
Note: the values of function 1( ... ) are 1(TRUE)=1 and 1(FALSE)=0.
After the intragroup BNs fusion, we obtained m undi-rected networks (UNs) UG i ( i =1 ,...,m ) which approx-imate the true BN in the form of undirected network to maximal extent.

As discussed in the above subsection, most of the edges in the true BN can be included in most of the undirected net-works. So, our intergroup fusion approach firstly uses the majority voting to obtain the most probable edges which exist in the true BN and generate the final undirected net-work UG . Then, our approach directs the edges in UG and obtain an initial result Bayesian network. Finally, we insert the rest edges in the learned UNs according to the maximal score principle and the acyclic property of BN and get the result BN. The process is shown in Fig.5. /* Create the undirected network UG */ 2) Create a undirected network UG consisting of the undirected /* Direct the UG to get the initial BN*/ 4) Direct the edges in the UG using causality inference rules or /* Insert the rest arcs */ 6) Return the result Bayesian network G The occurrence rate us ( e ) of each undirected edge e in the UG i ( i =1 ,...,m )isthequantity, us ( e )= 1 m
Directing edges is conducted according to causality rules[7] by identifying V-structures, i.e., non-adjacent parents hav-ing a common child, directing the relevant edges, and ap-plying additional rules to further direct edges until no more edges can be directed. For the rest undirected edges in UG , we consider the quantity pw ( e )= 1 m e  X  X  e  X  ,e  X  } .If pw ( e  X  ) &gt;pw ( e  X  ), then it is more prob-able that e  X  exists in the true BN than e  X  does, and then we direct e to be e  X  .Viceversa.

On limited datasets, it is possible that a small number of edges in the true BN are only included in a small number of UNs learned in the first part of our fusion method. So, we take greedy search &amp; Bayesian score method to insert the rest edges in the UNs into the initial Bayesian network and obtain the result Bayesian network.
BN learner is an individual BN learning algorithm and needs to be efficient computationally. Therefore, we selected OR algorithm [6] as BN learner. We implemented OR al-gorithm [6], OR-BSV algorithm, OR-RNS algorithm. OR-BSV algorithm uses Bagging method and the simple voting integration method, where m bootstrap sampling datasets D ,...,D m are generated from the original training dataset D and a component (BN) BN i is obtained from each D i , an integrated result BN BN r is built from BN 1 ,...,BN m The structure of BN r is determined through majority vot-ing, where an edge exists if and only if such an edge exists in majority BNs.
 Tests were run on a PC with Pentium4 1.5GHz and 1GB RAM. The operating system was Windows 2000. 3 Bayesian networks (Alarm, Hailf(inder)) were used. Table 1 shows the characteristics of these networks.
 BN Nodes Arcs Roots Max In/ Domain Alarm 37 46 12 4/5 2-4 Hailf 56 66 17 4/16 2-11
We performed experiments on the training datasets with 200, 500, 1000 instances. For each network and sample size, we sampled 10 original training datasets. We applied boot-strap sampling with 200 times in OR-BSV algorithm. Let  X  = 5 in Fig.2.

For each network and sample size, we recorded the average results of the number of the true root nodes and the number of the pseudo root nodes found by our root nodes search method from the original training datasets. From the results in Tables 2-3, we can see that our root nodes search method is very efficient on all the original training datasets.  X  X osi-RNs X  denotes the number of true root nodes learned by our search method.  X  X seudo-RNs X  denotes the number of pseudo root nodes learned by our search method.

We compared the accuracy of Bayesian networks learned by these algorithms according to the average BDeu score. The BDeu score corresponds to the posteriori probability of the structure learned. The BDeu scores in our experiments were calculated on a seperate testing dataset sampled from the true BN containing 20000 instances. For each network and sample size, we recorded the average result and standard deviation of the BDeu scores of the BNs learned from the original training datasets by these algorithms. Tables 4-5 report the results.
 There are several noticeable trends in the average results. Firstly, as expected, as the number of instances grow, the quality of learned Bayesian network improves. Secondly, RNS based BN learning algorithms OR-RNS are almost bet-ter than or at least equal to the individual BN learning algo-rithms in terms of accuracy on limited datasets. Thirdly, in most cases, RNS based BN learning algorithms have better performance than Bagging based BN learning algorithms.
From the results of standard deviation, we can see the fol-lowing trends. Bagging based BN learning algorithms pro-duced much larger standard deviation on small datasets be-cause the spurious dependences problem of bootstrap meth-ods discussed in section 1. However, RNS based BN learning algorithms produced less standard deviation, especially on small datasets. So, this confirms that RNS method alleviates the spurious dependences problem of bootstrap methods on small datasets to some extent. In the context of model se-lection, this also makes us limit the range of approximately true BNs more definitely on small datasets.

In this paper, we proposed a novel sampling technique and components fusion technique to incorporate ensemble based learning into BN learning on limited datasets. Our re-sults are encouraging in that they indicate that our ensemble based BN learning algorithms achieved more accurate BNs on limited datasets.
This work was partially supported by NSFC(60503017). [1] H.H.Dai,G.Li,andZ.H.Zhou.Ensemblingmml [2] A. C. Davison and D. V. Hinkley. Bootstrap methods [3] T. G. Dietterich. Machine learning research: Four [4] N. Friedman, M. Goldszmidt, and A. Wyner. Data [5] N. Friedman, M. Linial, I. Nachman, and D. Pe X  X r. [6] A. W. Moore and W.-K. Wong. Optimal reinsertion: A [7] P. Spirtes, C. Glymour, and R. Scheines. Causation, [8] H. Steck and T. S. Jaakkola. Bias-corrected bootstrap
