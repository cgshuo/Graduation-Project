 People are using social media to generate, share, and com-municate information with each other. Finding actionable insights from such big data has attracted a lot of research at-tentions on, for example, finding targeted user groups based on their historical on-line activities. However, existing ma-chine learning algorithms fail to keep up with the increas-ing large data volume. In this paper, we develop a scal-able regression-based algorithm called distributed iterative shrinkage-thresholding algorithm (DISTA) that can identify potential users. Our experiments conducted on Facebook data containing billions of users and associated activities show that DISTA with feature selection not only enables on-line audience-targeted approach for precise marketing but also performs efficiently on parallel computers.
 H.2.8 [ Information Systems ]: Database Applications X  Data mining ; G.1.6 [ NUMERICAL ANALYSIS ]: Opti-mization X  Unconstrained optimization Social brand; feature selection; DISTA; advertising
Most social platforms, such as Facebook, Twitter, Youtube, and Amazon.com, have mechanisms allowing users to gen-erate, share, and communicate with each other for their in-terested topics. For example, users can give rating scores and leave their reviews on products they purchased. Peo-ple can also  X  X ike X  or make comments on social brands (e.g. celebrities, institutes, organizations, companies, and prod-ucts). Analyzing these user-generated contents to find ac-tionable insights can help users make informed decisions, which has attracted a lot of attention in research. Research in social media data analysis falls into two categories. The first one is from the text-mining perspective: text senti-ment analysis for decision making [4]; the second one is from the social network perspective: study of static and dynamic properties of networks [5].

Recently, the trend to social content-driven advertising is becoming increasingly evident in business management. Finding targeted audience for precise on-line advertising based on user historical behaviors is one of the most important marketing tasks. BIA/Kelsey X  X  study estimates that the so-cial advertising revenues in the U.S. will grow over 3 billion dollars by 2017 [1]. Machine-learning methods have been widely used, for example, for building a predictive model based on users X  profile, historical activities, and social net-working information. Many psychological and sociological models were also proposed to build user sociality from user access log data so that they can be used to guide marketing managers to find their targeted audience. In this work, we focus on user preference prediction on social brands.
However, there are some challenges given the big size of the training samples and the large number of training fea-tures. First, existing feature selection algorithms is infea-sible and inefficient, which motivates us to find a scalable solution. Secondly, implementing distributed algorithms to efficiently and accurately learn predictive models is also not straightforward. To address the first challenge, we imple-ment a MapReduce-based Apriori algorithm to find a given brand the group of correlative brands that share the most user activities. The identified brands will be used as the selected features in the model learning. To solve the sec-ond problem, we implement a distributed regression-based algorithm called iterative shrinkage-thresholding algorithm (DISTA), a stochastic optimization algorithm that can han-dle a large amount of training instances. The experiments show that our DISTA can get up to 16% increase of accuracy by incorporating our feature selection strategy comparing to other baselines.
Our problem is a typical classification in machine learning domain. The training features are social brands ( b 1 , b b ) and the value of each feature is the number of historical activities a user had on the corresponding brands (e.g. the number of likes, the number of comments, or both). The target brand ( b t ) is labeled in a binary form: 1 if a user is interested in this brand, 0 otherwise. Before mathemati-cally formulating this problem, we define the terms of social brands and activity matrix used in this paper.

A social brand is an entity in the social network that allows other users to leave comments on its page. Exam-ples are companies, organizations, individuals, or consumer products. The activity matrix is represented as the follow-ing.
 where u i is the i th user; b j is the j th brand; The entry x ij is the number of activities made by i th user on brand j . x ij = like ij + comment ij , where like ij is the number of likes user i gave to all posts initiated by brand j and comment is the number of comments made by user i on brand j .
To obtain the k th user X  X  preference on a specified target brand b t , we calculate P kt .
 P where A k is the k th row of activity matrix A. P kt  X  [0 , 1] is the output value for the target brand b t , representing the All these x ki are given for a testing user and all  X  i obtained through the training process, which involves solving the fol-lowing convex optimization problem. min  X  f (  X  ) +  X  k  X  k 1 = min  X  k A X   X  b t k 2 2 +  X  k  X  k where  X  is a vector of n dimensions; b t is a vector of the targeted brand t of n dimensions;  X  is a constant and k  X  k is the l 1 -norm of the parameter vector. k  X  k 1 = P |  X  |  X  2 | + . . . + |  X  n | .

In this work, we used Facebook Graph API to download social brand data from Facebook. The data covers many different categories, including sports, movies, politics, fast-food, and many others. The first issue we need to handle is feature selection, because not all brands in the feature set are related to the targeted brand. We start with selecting top related brands to reduce the size of the activity matrix A . The method we use here is association rule mining to find patterns like  X  b i  X  b t  X  with high confidence scores. In the next section, we will discuss a MapReduce-based tech-nique to find top k other brands ( b 1 , b 2 , . . . , b k confidence score with the pattern of  X  b i  X  b t  X . Then the size of the new activity matrix A  X  is significantly reduced from m  X  n to u  X  k ( k &lt;&lt; n and u is the number of users The next step is the binary classification problem to identify potential users.
In this section, we describe our distributed iterative shrinkage-thresholding algorithm. In addition, to address the large data volume challenge in feature selection, we use MapReduce-based Apriori to select top associated brands.
As most of the features/brands are not closely related to the targeted brand, removing irrelevant features during the learning can not only reduce the size of training data, but also help mitigate bias. In this work, we use a distributed Apriori algorithm to select top brands (features) based on diving into the details, we first describe the data we collect from the Facebook.

Data Preparation For the public social brands, users can like or make comments on campaigns posted by brand administrators. In this work, we assume that a user is in-terested in a brand if he/she makes positive comments on it or likes campaigns on that brand. OpinionFinder [6] is used to identify sentiments. We consider likes and com-ments as user activities, which can be represented as a 3-tuple: [ user id , brand id , # of activities ]. We then combine al l activities across all brands for each user. After this pro-cess, each user is described with the format of &lt;user id b | w 1 , b 2 | w 2 ,  X   X   X  , b i | w i ,  X   X   X  &gt; , where b w i is the corresponding number of activities, DEL could be any delimiter.

Confidence score: The goal here is to find the frequent pattern  X  b i  X  b t  X  based on a large amount of user historical activities across brands. Two-itemset ( I x , I y ) Apriori ( X  I I  X ) indicates their correlation. Here, I x could be any brand b is the target brand b t . We choose top k brands based on the confidence score of the pattern  X  b i  X  b t  X . The confidence is calculated using the following equation.
 Whe re Support ( X ) is the occurrence frequency of X . In our case, it is the number of users who have activities on both brands b i and b t for Support ( b i , b t ), on brand b for Support ( b i ). The key sketchlon of the MapReduce-based algorithm of calculating confidence score (CSC) is shown in Algorithm 1.
Given large amounts of user historical activities, a very intuitive way to solve the problem mentioned in (  X  ) is build-ing a regression model. We intend to develop our model to have the following two properties: (1) less sensitive to outliers, and (2) can promote sparse solutions because most of the features are irrelevant to the class/label, even using top k features after feature selection. Consider the uncon-strained minimization problem of a continuously differen-tiable function f (  X  ): R n  X  R : min { f (  X  ) ,  X   X  R n One of the simplest methods for solving (  X  ) is the gradi-ent descent algorithm which generates a sequence of  X  k via Algorithm 1 CSC . al : an activity list for a user 1: map function : 2: fo r all b i  X  al do 3: if b t  X  al then 4: output &lt; ( b i , b t ), 1 &gt; ; 5: end if 6: output &lt;b i , 1 &gt; ; 7: end for 8: 9: reduce function : 10: for all keys: ( b i , b t ) and b i do 11: sum all values  X  S it or S i ; 12: end for 13: 14: for all b i  X  b t sequentially do 15: Conf ( b i  X  b t ) = S it /S i ; 16: end for  X  suitable step size. It is very well known [3] that the gradient iteration in (  X  ) can be viewed as a proximal regularization  X  k  X  1 k 2 2 } . Ad opting this same basic gradient idea to the non-smooth l 1 regularized problem: min { f (  X  )+  X  k  X  k R n } . It leads to the iterative scheme:  X  k = argmin  X  { f (  X  R n  X  R n is the shrinkage soft threshold; T x ( y ) = ( | y |  X  x ) + sign ( y ), where ( Y ) + = max { 0 , Y } and sign is the sign function. Therefore,  X  k = ( |  X  k  X  1  X  t k  X  f (  X  k  X  1  X  t
Theorem 1.  X  k is separable to calculate. Since the l norm is separable, the computation of  X  k reduces to solving a one-dimensional minimization problem for each of its components.
 Proof:  X  k is equivalent to argmin  X  { 1 2 t k k  X   X   X  k  X  1 +  X  k  X  k 1 } aft er ignoring constant terms, because:  X  k = argmin  X  { 1 ) w here a =  X   X   X  k  X  1 , b = t k  X  f (  X  k  X  1 ), and c =  X  t  X  f (  X  k  X  1 ). t k is the step length. From this derivation, we could see that we can minimize each component of  X  sep-arately. This also provides our opportunities of distributed computing. Therefore,
There are still some key points that need to be addressed, including: (I) step length. Usually, we use t k = 1 L as t he step length where L is the lipschitz continuity. In this work, we set L to k A T A k 2 . (II) Stopping condition. We use the following criteria to stop the iterative learning process. whe re k X k F is called the Frobenius norm and k X k F = q (  X   X  is the optimal value of  X  ), namely, shares a sublinear global rate of convergence. In [2], authors proved the con-verge in function values as O (1 /k 2 ), where k is the iteration counter. (IV) Backtracking. There are a number of differ-ent accelerated backtracking schemes and these are made under different criteria for the same reason. We use one of the simpler schemes -line search backtracking. Algorithm 2 describes the learning process of DISTA.
 Algorithm 2 DIS TA: Distributed Iterative Shrikage-Thresholding Algorithm with Line Search Backtracking 1: choose  X  , su ch that 0 &lt;  X  &lt; 1; 2: t 0 = 1; 3: repeat 5: for all i such that 1  X  i  X  n do 6: { distributed computing of  X  i as indicated in } 8: end for 9: while ( f (  X  + ) &gt; f (  X  k  X  1 ) +  X  f (  X  k  X  1 ) 10 : { line search backtracking step } 11: t k =  X t k ; 12: for all i such that 1  X  i  X  n do 14: end for 15: end while 16: until the stopping criteria meets 17: return  X  + ;
In t his section, we first describe the structure of data used in our experiments. As the social media data is generated by the public, there are many noisy factors. It is necessary to filter out spams to obtain a high-quality data for producing unbiased results. Then, we discuss the experimental results of feature selection and DISTA under different parameter settings and compare them with some baselines.
On Facebook, the largest and most popular social net-work platform, many companies, organizations, and individ-uals build their own pages to communicate with social users (fans), which generates an extensive amount of networked and textual information. In this paper, we mainly consider social brands as our target objects. We use Facebook Graph API to download the available activities made on brand side such as posts and user side, such as comments on posts, likes on posts, and public profiles. We have designed some rules to filter out spam users and their activities in our previous work, such as users having an abnormal amount of brand accesses (e.g. &gt; 100). Table 1 describes the cleaned data used in our experiments. For labels in the training dataset, we consider users who make all positive comments on the target brand as positive samples and negative comments as negative samples.
The input data used in our experiments is big. Using sin-gle machine to do feature selection, and regression model accuracy on 10 target brands.
 # of unique users 97 , 699 , 832 # of social brands 7 , 580 # of the triple (user, page, comments) 102 , 517 , 478 # of the triple (user, page, likes) 192 , 442 , 757 The number of total post likes 5 , 275 , 921 , 8 75 Table 3: Top 5 associated brands sorted by the con-fide nce score of the rule:  X  b i  X  Nordstrom  X  building is infeasible. In fact, we could not finish the job wit hin 10 hours using only single machine. Hence, we con-duct our experiments on a Hadoop-based environment which has 10 machines. Each machine has 8 compute processors. We randomly select 10 different target brands in our exper-iments. Table 3 shows top 5 correlated brands to the target brand X  X ordstrom X  in terms of the confidence score. Table 2 compares the performance of using DISTA between with and without incorporating this feature selection strategy under different size of training sets with three other baselines. It shows that with our feature selection strategy can obtain up to 16% increase of accuracy and also always beat without incorporating feature selection.

To build the model, we used the training dataset of size 10 , 000 positive instances and 10 , 000 negative instances. We use 10-fold cross validation. For such training sets, it takes a long time to finish learning. But our DISTA learning al-gorithm significantly speeds it up, as shown in Figure 1.
In this work, we build a user predictive model based on their historical behaviors on social media for on-line adver-tising. We implemented a distributed Apriori feature selec-tion for reducing the training dataset. In addition, we imple-mented a distributed iterative shrinkage thresholding model to predict user X  X  preference. The experiments conducted on Facebook data has shown that all proposed techniques Figure 1: The time (seconds) consumed for DISTA on d ifferent number of processors. in this work are scalable and efficient for social audience-targeted advertising. Future work includes deeply under-standing and incorporating semantics of user-generated con-tents; finding more accurate and fast predictive learning al-gorithms. [1] http://www.biakelsey.com/index.asp . [2] Amir Beck and Marc Teboulle. A fast iterative [3] Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen. [4] Bing Liu. Sentiment Analysis and Opinion Mining . [5] Jie Tang, Tiancheng Lou, and Jon Kleinberg. Inferring [6] Theresa Wilson, Paul Hoffmann, Swapna
