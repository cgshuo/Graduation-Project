 As an alternative to requiring substantial super-vised relation training data (e.g. the ~300k words of detailed, exhaustive annotation in Automatic explored bootstrapping relation extraction from a few (~20) seed instances of a relation. Key to such approaches is a large body of unannotated text that can be iteratively processed as follows: 1. Find sentences containing the seed instances. 2. Induce patterns of context from the sentences. 3. From those patterns, find more instances. 4. Go to 2 until some condition is reached. es , like hasBirthDate(Wolfgang Amadeus Mozart, with easily spotted anchors like Wolfgang Amadeus Mozart was born in 1756 . ence can improve the learning process. That is, if the algorithm considered texts like his birth in 1756 for the above relation, would performance of the learned patterns be better? There has been much work in relation extraction both in traditional supervised settings and, more recently, in bootstrapped, semi-supervised settings . To set the stage for discussing related work, we highlight some aspects of our system. Our work initializes learning with about 20 seed relation in -stances and uses about 9 million documents of un-corpus. We use both normalized syntactic structure and surface strings as features. extractors using lots of supervised training, as in ACE, which evaluates system performance in de-tecting a fixed set of concepts and relations in te xt. Researchers have typically used this data to incor-porate a great deal of structural syntactic infor-mation in their models (e.g. Ramshaw, 2001), but the obvious weakness of these approaches is the resulting reliance on manually annotated examples, which are expensive and time-consuming to create. tion from seed examples. Agichtein &amp; Gravano (2000) and Ravichandran &amp; Hovy (2002) reported results for generating surface patterns for relatio n identification; others have explored similar ap-proaches (e.g. Pantel &amp; Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macro-reading, precision and recall can be improved by learning a large set of interconnected relations an d concepts simultaneously. In all cases, the ap-proaches used surface (word) patterns without co-reference. In contrast, we use the structural features of predicate-argument structure and em-ploy coreference. Section 3 describes our particula r approach to pattern and relation instance scoring and selection. Zhou et al., 2008) explores semi-supervised rela-tion learning using the ACE corpus and assuming manual mention markup. They measure the accu-racy of relation extraction alone, without includin g the added challenge of resolving non-specific rela-tion arguments to name references. They limit their studies to the small ACE corpora where mention markup is manually encoded. tion have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retriev-al Conference (TREC) Question Answering track, where extracting one text of a relation instance ca n be sufficient, rather than detecting all texts. Mit ch-ell et al. (2009), while demonstrating high preci-sion, do not measure recall. primary focus on precision allows one to ignore many relation texts that require coreference or long-distance dependencies; one primary goal of our work is to measure system performance in ex-actly those areas. There are at least two reasons t o not lose sight of recall. For the majority of entit ies there will be only a few mentions of that entity in even a large corpus. Furthermore, for many infor-mation-extraction problems the number documents at runtime will be far less than web-scale. Figure 1 depicts our approach for learning patterns to detect relations. At each iteration, the steps a re: (1) Given the current relation instances, find poss i-ble texts that entail the relation by finding sente nc-es in the corpus containing all arguments of an in-stance. (2) As in Freedman et al. (2010) and Boschee et al. (2008), induce possible patterns using the con-text in which the arguments appear. Patterns in-clude both surface strings and normalized syntactic corpus to find a set of hypothesized texts. For each pattern, a confidence score is assigned using esti-(3) The patterns are applied to the corpus to find additional possible relation instances. For each proposed instance, we estimate a score using a Na-ive Bayes model with the patterns as the features. When using coreference, this score is penalized if an instance X  X  supporting evidence involves low-confidence coreference links. The highest scoring instances are added to the instance set. (4) After the desired number of iterations (in the se experiments, 20) is complete, a human reviews the resulting pattern set and removes those patterns which are clearly incorrect (e.g.  X  X visited Y X  for no access to coreference information, while +Coref (the original system) does. The systems are other-wise identical. Coreference information is provided by BBN X  X  state-of-the-art information extraction system (Ramshaw, et al., 2011; NIST, 2007) in a mode which sacrifices some accuracy for speed (most notably by reducing the parser X  X  search space). The IE system processes over 50MB/hour with an average EDR Value score when evaluated on an 8-fold cross-validation of the ACE 2007. in which the arguments are expressed as either name or non-name mentions. When the text of an argument of a proposed instance is a non-name, the system uses coreference to resolve the non-name to a name. -Coref can only propose instances based tails a relation instance expresses one of the argu -ments as a non-name mention (e.g .  X  X ue X  X  husband is here. X  ), -Coref will be unable to learn an in-stance from that text. Even when all arguments are expressed as names, -Coref may need to use more specific, complex patterns to learn the instance (e.g.  X  X ue asked her son, Bob, to set the table X  ). We expect the ability to run using a  X  X enser, X  more local space of patterns to be a significant advanta ge of +Coref. Certain types of patterns (e.g. patterns involving possessives) may also be less likely to b e learned by -Coref. Finally, +Coref has access to much more training data at the outset because it leading to better and more stable training. Estimating recall for bootstrapped relation learnin g is a challenge except for corpora small enough for complete annotation to be feasible, e.g., the ACE corpora. ACE typically had a test set of ~30,000 words and ~300k for training. Yet, with a small corpus, rare relations will be inadequately repre-2009) have not estimated recall, but have measured precision by sampling system output and determin-ing whether the extracted fact is true in the world . Here we extend this idea to both precision and re-call in a micro-reading context. over the background corpus and randomly sam-pleing 100 texts that the system believes entail each relation. From the mentions matching the ar-gument slots of the patterns, we build a relation instance. If these mentions are not names (only possible for +Coref), they are resolved to names using system coreference. For example, given the passage in Figure 2 and the pattern  X (Y, poss:X) X , the system would match the mentions X=her and Y=son, and build the relation instance hasChild(Ethel Kennedy, Robert F. Kennedy Jr.) . whether, in the context of the whole document, a given sentence entails the relation instance. We thus treat both incorrect relation extraction and incorrect reference resolution as mistakes. stances and search the corpus for sentences con-taining all arguments of a test instance (explicitl y or via coreference). We randomly sampled from this set, choosing at most 10 sentences for each te st instance, to form a collection of at most 200 sen-tences likely to be texts expressing the desired re la-tion. These sentences were then manually annotated in the same manner as the precision an-notation. Sentences that did not correctly convey the relation instance were removed, and the re-maining set of sentences formed a recall set. We consider a recall set instance to be found by a sys -tem if the system finds a relation of the correct type in the sentence. We intentionally chose to sample 10 sentences from each test example, rather than sampling from the set of all sentences found. This prevents one or two very commonly ex-pressed instances from dominating the recall set. As a result, the recall test set is biased away fro m  X  X rue X  recall, because it places a higher weight on the  X  X ong tail X  of instances. However, this gives a more accurate indication of the system X  X  ability to find novel instances of a relation. for +Coref (+) and  X  X oref (-). In all cases remov-ing coreference causes a drop in recall, ranging from only 33%( hasBirthPlace ) to over 90% ( GPEEmploys) . The median drop is 68%. 5.1 Recall lower recall. For some relation instances, the text will contain only non-named instances, and as a result -Coref will be unable to find the instance. -Coref is also at a disadvantage while learning, since it has access to fewer texts during bootstrap -in the recall test set for which both argument names appear in the sentence. Even with perfect patterns, -Coref has no opportunity to find roughly 25% of the relation texts because at least one ar-gument is not expressed as a name. mance, we created a third system, *Coref, which used coreference at runtime but not during train-ing. 12 In a few cases, such as hasBirthPlace , *Coref is able to almost match the recall of the system that used coreference during learning (+Coref), but on average the lack of coreference at runtime accounts for only about 25% of the differ-ence, with the rest accounted for by differences in the pattern sets learned. mention types for +Coref on the recall set. Com-paring this to Figure 3, we see that +Coref uses name-name pairs far less often than it could (less than 50% of the time overall). Instead, even when two names are present in a sentence that entails th e relation, +Coref chooses to find the relation in name-descriptor and name-pronoun contexts which are often more locally related in the sentences. call, ORGEmploys and GPEEmploys , +Coref and  X  Coref have very different trajectories during train -ing. For example, in the first iteration,  X  X oref learns patterns involving director , president , and head for ORGEmploys , while +Coref learns pat-terns involving joined and hired . We speculate that  X  X oref may become stuck because the most frequent name-name constructions, e.g. ORG/GPE title PERSON (e.g. Brazilian President Lula da Silva), are typically used to introduce top officials. For such cases, even without co-reference, system specific effort and tuning could potentially have improved  X  X oref X  X  ability to learn the relations. 5.2 Precision the relations +Coref is higher, for the 6 others th e addition of coreference reduces precision. The av-erage precisions for +Coref and  X  X oref are 82.2 and 87.8, and the F-score of +Coref exceeded that of  X  X oref for all relations. Thus while +Coref pays a price in precision for its improved recall, in ma ny applications it may be a worthwhile tradeoff. erence would reduce precision of +Coref, such er-rors may be balanced by the need to use longer patterns in  X  X oref. These patterns often include error-prone wildcards which lead to a drop in pre-cision. Patterns with multiple wildcards were also more likely to be removed as unreliable in manual pattern pruning, which may have harmed the recall of  X  X oref, while improving its precision. 5.3 Further Analysis reading which requires a system find all mentions of an instance relation  X  i,e, in our evaluation Or-gLeader(Apple, Steve Jobs) might occur in as many as 20 different contexts. While  X  X oref per-forms poorly at micro-reading, it could still be ef -fective for macro-reading, i.e. finding at least on e instance of the relation OrgLeader(Apple, Steve Jobs) . As a rough measure of this, we also evaluat-ed recall by counting the number of test instances for which at least one answer was found by the two systems. With this method, +Coref X  X  recall is still higher for all but one relation type, although the gap between the systems narrows somewhat. ured the number of sentences containing relation instances found by each of the systems when ap-plied to 5,000 documents (see Table 3). For al-most all relations, +Coref matches many more sentences, including finding more sentences for those relations for which it has higher precision. Our experiments suggest that in contexts where recall is important incorporating coreference into a relation extraction system may provide significant gains. Despite being noisy, coreference infor-mation improved F-scores for all relations in our test, more than doubling the F-score for 5 of the 10. very harmful to +Coref? We speculate that there are two reasons. First, during training, not all co -reference is treated equally. If the only evidence we have for a proposed instance depends on low confidence coreference links, it is very unlikely t o be added to our instance set for use in future iter a-tions. Second, for both training and runtime, many of the coreference links relevant for extracting th e relation set examined here are fairly reliable, suc h as wh -words in relative clauses. question, however. It is also unclear if the same result would hold for a very different set of rela-tions, especially those which are more event-like than relation-like. This work was supported, in part, by DARPA un-der AFRL Contract FA8750-09-C-179. The views expressed are those of the authors and do not re-flect the official policy or position of the Depart -ment of Defense or the U.S. Government. We would like to thank our reviewers for their helpful comments and Martha Friedman, Michael Heller, Elizabeth Roman, and Lorna Sigourney for doing our evaluation annotation. 
