 The data-mining literature is rich in problems asking to assess the importance of entities in a given dataset. At a high level, existing work identifies important entities either by ranking or by Ranking methods assign a score to every entity in the population, and then use the assigned scores to create a ranked list. The major shortcoming of such approaches is that they ignore the redundancy between high-ranked entities, which may in fact be very similar or even identical. Therefore, in scenarios where diversity is desirable, such methods perform poorly. Selection methods overcome this drawback by evaluating the importance of a group of entities tively . To achieve this, they typically adopt a set-cover which identifies the entities in the minimum set cover as the impor-tant ones. However, this dichotomy of entities conceals the fact that, even though an entity may not be in the reported cover, it may still participate in many other optimal or near-optimal solutions. In this paper, we propose a framework that overcomes the above drawbacks by integrating the ranking and selection paradigms. Our approach assigns importance scores to entities based on both the number and the quality of set-cover solutions that they participate in. Our methodology applies to a wide range of applications. In a user study and an experimental evaluation on real data, we demon-strate that our framework is efficient and provides useful and intu-itive results.
 H.4 [ Information Systems Applications ]: Miscellaneous; G.2.1 [
Discrete Mathematics ]: Combinatorics X  Combinatorial algorithm-s, Counting problems counting, importance sampling, set cover
How can we identify a subset of important entities from a given population? This question arises in numerous application domain-s. For example, how can we select a small subset of reviews to show to a user from over 35,000 reviews hosted on Amazon.com for the Kindle Keyboard 3G? Or, as another example, how can we select a set of experts from a large population of skilled individ-uals registered on sites like linkedin.com , odesk.com and guru.com ?
Existing data-mining methods address this type of questions via entity-ranking or entity-selection methods.

Entity-ranking methods assign a score to each entity and then report to the user the top-k entities with the highest scores. For ex-ample, review portals like Yelp or Amazon rank reviews based on user-submitted helpfulness votes. Such score-assigning schemes ignore the redundancy between the highly-scored entities. For ex-ample, the top-k reviews about a laptop may all comment on the battery life and the weight of the laptop, but may provide no infor-mation about the quality of its screen or other important features.
Entity-selection methods overcome this drawback by collective-ly evaluating sets of entities, and reporting the members of the highest-scoring sets as important. For example, review-selection methods select a small subset of reviews that collectively comment upon all the attributes of a particular product [16, 18, 23]. Similar-ly, expert-selection methods identify a set of skilled experts that can collectively perform a given task [2, 17]. That is, entity-selection methods identify the entities in the selected subset as important, and na X vely dismiss the rest as unimportant. Such a dichotomy conceal-s the fact that there may be entities not selected in the discovered solution, but which may participate in equally-good (or almost as good) solutions.

In this paper, we propose a framework that combines the entity-ranking and entity-selection paradigms and overcomes their respec-tive drawbacks. Given a set of entities C , our methods assign an importance score to entities based on the number of high-quality solutions that they participate. In particular, we focus on entity-selection problems, which are formalized as minimum set-cover problems [1, 14, 16, 17, 21, 23]. The common characteristic of all these formulations is that the input consists of two parts: ( verse U = { u 1 , . . . , u n } of items that need to be covered, and ( collection of entities, where each entity is a subset of this universe, i.e., C = { C 1 , . . . , C m } with C i  X  U . Any subset whose union contains the entire universe U , i.e.,  X  C  X  X   X  called a set cover or simply a cover . The data-mining task is typi-cally mapped to the problem of finding the minimum set cover assumption in such formulations is that the entities that participate in the minimum cover compose an important subset. Some of the applications this formulation are the following.
 Review selection: What are the best reviews to display for a given product? In this application, the universe U consists of all the at-tributes of the reviewed product. Each review C  X  X  comments on-ly on a subset C  X  U of the attributes. Given the limited attention-span of the users, the review-selection problem asks for a sma ll set of reviews that comment on all the attributes of a product [16, 23]. Expert selection: Who are the best experts to select for executing a given task? In this application, the universe U consists of the skills required for a particular task. Each expert C  X  X  is represented by the set of his skills C  X  U . The team-formation problem asks for a small-cardinality team whose members have the required set of skills [2, 17].

The underlying thesis in this paper is that by simply selecting the entities in the minimum set cover and dismissing other set-cover solutions one ignores useful information about the landscape of the solution space. For example, our analysis of the Guru dataset 1 re-vealed that although there are more than 340 000 registered expert-s, there are only 200 unique combinations of skills. That is, every expert is (on average) identical with approximately 1700 other ex-perts. This means that every set S of cardinality |S| is as good as tion na X vely selects to report as important the members of only one of these subsets
In our work, we take into consideration the landscape of solu-tions by adopting a counting framework ; instead of focusing on a single optimal solution, we count the number of high-quality set covers that each entity participates. We use the cardinality of a set cover as a measure of its quality; compact covers are preferable, since they lead to lower-cost solutions.
 On a high level, we address the following problem.

P ROBLEM 1. We are given a universe of elements U and a set of entities C where each C  X  C is a subset of U . The goal is to evaluate the importance of each entity by counting the number of compact set covers that it participates.

In the context of reviews, solving Problem 1 will allow us to to evaluate the utility of each review in the covering of all the at-tributes of a given product. Intuitively, our methodology will assign high importance to reviews that cover a large number of a produc-t X  X  attributes, or comment on a small number of attributes that are rarely commented by other reviewers. In expert-management cations, the scores assigned by our approach will be driven by the number of skills that each individual can contribute, as well as the rarity of these skills. Intuitively, these are the individuals who play a key role in forming effective teams. While we use the domains of reviews and experts to motivate our work, our methodology is applicable to any setting that can be expressed in the terms of the set-covering formulation.
 Contribution: Given a set system X = h U, Ci , where U is a uni-verse of elements and C is a set of entities defined as subsets of our paper proposes a general methodology that assigns importance scores to entities in C . The score of each entity depends both on the number and the quality of the set covers that the entity partic-ipates. Enumerating all the set covers that an entity participates is a computationally intractable task. Here, we take advantage of the fact that we are only interested in the number of set covers an enti-ty participates. Therefore, we develop practical counting schemes that allow us to accurately estimate this number in polynomial time. More specifically, we propose a counting algorithm that is based on Monte Carlo (MC) sampling. The algorithm builds on exist-ing techniques for counting the number of satisfying assignments of DNF formulas [22]. We significantly extend this technique in the following ways: ( i ) we modify it so that it can efficiently es-timate the number of set covers of all entities simultaneously; ( w ww.guru.com we show how we can incorporate weights that capture the size of set cover solutions; ( iii ) we show how to handle efficiently entities with multiple copies. Finally, our experimental evaluation shows the scalability of our algorithm, and the utility of our methodology in different application domains. Also, our user study demonstrates that our concept of importance is in agreement with human percep-tion regarding important entities.
 Roadmap: The rest of the paper is organized as follows. In Sec-tion 2 we present an overview of our framework, and in Section 3 we present our algorithms. We present our experimental results in Section 4 and we discuss the related work in Section 5. We con-clude the paper with discussion and conclusions in Section 6.
In this paper, we consider problems for which the input con-sists of two parts: ( i ) a universe set U = { u 1 , . . . , u s that need to be covered, and ( ii ) a collection of entities, where each entity is a subset of this universe, i.e., C = { C 1 with C i  X  U . We use the notation X = h U, Ci to represen-t such an input. We visualize this type of input using a bipartite graph; the nodes on the one side of the graph correspond to the elements of U , and the nodes of the other side correspond to the entities in C . An edge connects a node u  X  U to node C  X  C if u  X  C . A bipartite-graph representation of input X = h U, Ci with U = { u 1 , u 2 , u 3 , u 4 } and C = { C 1 , . . . , C Figure 1(a).

D EFINITION 1. For input X = h U, Ci we say that a subset S of the collection C ( S  X  X  ) is a set cover or simply a cover U  X  For the rest of the paper, we will use L to represent all the subsets of C and L sc to represent all possible set covers.
 The minimum set cover. A set cover S  X  that has minimum car-dinality is called a minimum set cover . The problem of finding the minimum set cover, henceforth called S ET -C OVER , is a well-studied NP-hard problem, which can be approximated within a fac-tor of O (log n ) using a simple greedy algorithm [24].
In many application domains, a solution to S ET -C OVER is a nat-ural way to select a small subset of C : an entity C of the collection C is  X  X elected X  if and only if C is part of the solution S  X  . Then, not in the solution. However, such a strict dichotomy can be un-fair. For example, an entity C may be part of another minimum or near-minimum set cover. Such a case is shown in the following example.

E XAMPLE 1. Consider the instance of the S ET -C OVER prob-lem represented by the bipartite graph shown in Figure 1(a) with 4 items in U , 5 entities in C , and C 1 = { u 1 } , C 2 = { u C 3 = { u 2 , u 3 } , C 4 = { u 3 , u 4 } , and C 5 = { u 5 the minimum set-cover solution is C = { C 2 , C 4 } . The minimum-cardinality solution, in which set C 3 participates, has cardinality 3 . Nevertheless, there is no reason to believe that set C 3 is so much less important than C 2 or C 4 . After all, C 3 has as many elements as C 2 and C 4 .
 Minimal set covers. Instead of focusing on a single minimum-cardinality solution, we could count the number of solutions each entity C  X  X  participates. One way of applying this idea is to consider the concept of minimal set covers . A set cover S mal if S \ C is not a set cover for all C  X  S . Minimal set covers have the property that every other cover is a superset of a minimal Figure 1: Examples used for illustrating the inadequacy of mini-mum and minimal set covers in value computations. cover. In order to estimate the importance of a particular entity one could count in how many minimal covers it participates. Al-though this initially looks like a natural approach, the following example indicates that it can lead to unintuitive results.
E XAMPLE 2. Consider the instance of the S ET -C OVER prob-lem shown in Figure 1(b). In this instance, U = { u 1 , u C = { C  X  , C, C 1 , . . . , C m } , and C 1 = C 2 = . . . = C C = { u 1 , u 2 , u 3 } and C  X  = { u 2 , u 3 } . Observe that C can cover all the elements in U . Nevertheless, it participates in a single mini-mal solution { C } . On the other hand, C  X  participates in m minimal C participates in significantly less minimal solutions than C clearly as useful as C  X  (since it is actually a superset of C Counting set covers. We overcome the deficiencies of the above approaches by introducing a score R ( C ) for every entity The cover score (or simply score ) of entity C is defined as follows: In the above equation, L sc represents all the set covers the function  X  ( C, S ) is an indicator variable that takes value 1 if is a set cover and takes value 0 otherwise. The term w ( S ) to every solution S a weight such that better solutions  X  smaller cardinality covers  X  are assigned higher weights. In other words, for two set covers S and S  X  if |S| X  |S  X  | , then w ( S )  X  w ( S other words, important entities participate in many good set covers and therefore are assigned higher scores.

In this paper, we consider three weighting schemes:
From the algorithmic point of view, computing R ( C ) is compu-tationally intractable; note that there are exponentially many solu-tions in L sc . More formally, for the uniform weighting scheme, the task of estimating R ( C ) is identical to the task of counting all set covers for h U, C\{ C }i , which is a #P-complete problem [22]. Discussion: The interpretation of the cover score is the following: if we sample solutions from L such that every solution S is sam-pled with probability proportional to w ( S ) (i.e., good solutions are sampled with higher probability), then R ( C ) is proportional to the probability that the sampled solution contains C .
 The Equation (1) indicates that the framework we propose bridges Bayesian model-averaging [4] with optimization-based data min-ing. To draw the analogy, the objective function (expressed via the weight w ( ) ) corresponds to the Bayesian prior. The validity of the solution (expressed via the indicator function  X  ( , ) ) corresponds to the probability of the data given the model. At a very high lev-el, one can think that R ( C ) is the average  X  X oodness X  of solutions with entity C .
As already discussed, estimating R ( C ) is computationally in-tractable and we cannot hope for computing it exactly in polyno-mial time. Therefore, we are content with algorithms that approxi-mate the cover score in polynomial time. In this section, we discuss such algorithms for the different weighting schemes.

Apart from R ( C ) we will also compute the sum of the weights of all the subsets of C . Formally: R = P cases, for clarity of presentation, we will use the uniform weighting scheme . Under this scheme, R represents the cardinality of set namely, the number of set covers for X = h U, Ci .
We begin by presenting a na X ve Monte-Carlo counting algorith-m. We discuss why this algorithm fails to compute R ( ) under the uniform weighting scheme, but the discussion applies to the other weighting schemes too.

For input X = h U, Ci one could compute estimates R ( C ) the following procedure: first, set the counters r = 0 and for every C  X  C . Then retrieve N samples from L . For each sample S , check if S is a set cover. If it is, then: ( i value of r by 1 and ( ii ) increase the value of r ( C ) by S . In the end of the sampling procedure, report the estimate for R where m is the number of entities in C .

The above algorithm, which we call NaiveCounter , is a stan-dard Monte-Carlo sampling method. Using Chernoff bounds, we can state the following fact [22, Theorem 11.1].

F ACT 1. The NaiveCounter algorithm yields an  X  -approxi-mation to R ( C ) with probability at least 1  X   X  , provided that T he drawback of NaiveCounter is that, as  X  c becomes smaller, the number of samples N required to obtain a good approximation increases. Since we do not know the value of  X  c (in fact, this is what we are trying to approximate) we have to rely on a lower bound. In the absence of any evidence, the only reasonable lower bound is  X  c  X  1 of samples, leading to an inefficient and impractical algorithm.
We overcome the problem of NaiveCounter with a novel and efficient algorithm, based on the principles of importance sam-Algorithm 1 T he CountSimple algorithm.
 1: r  X  0 2: for iterations N , polynomial in d do 4: S = RandomSample ( M  X  ) 5:  X   X  = CanonicalRepresenative ( S ) 6: if  X   X  =  X  then r  X  r + w ( S  X  ) 8 : return r pling . 2 W e call our algorithm Compressed-IC . Our algorithm works for all our weighting schemes and therefore we present it here in its full generality.

The algorithm requires as input a seed of d minimal set cov-ers M = {M 1 , . . . , M d } . In principle, all minimal set covers are required. However, computing all minimal set covers can be a computationally expensive task. Thus, we propose to use as seed a subset of all the minimal covers. As we show in our experiments this is sufficient to accurately estimate the desired counts. Later in the section, we give details on how to obtain such a seed.
In the sections that follow, we give a bottom-up presentation of the Compressed-IC algorithm. That is, we describe a sequence of steps that allow us to modify the classic importance sampling algorithm in order to efficiently approximate the cover scores.
We start with an overview of importance sampling for the task of estimating R . For a minimal set cover M  X  we define M the collection of all supersets of M  X  . Given two (small) positive numbers  X  and  X  , the method provides an  X  -accurate estimate for with probability at least (1  X   X  ) , provided that the following three conditions are satisfied: 1. We can compute w ( M  X  ) = P 2. We can obtain a random sample from each S  X  ; where the sam-3. We can verify in polynomial time if S  X  M  X  .
 For the weighting schemes we consider in this paper, we have the following lemma.

L EMMA 1. For the uniform, step, and cardinality-based weight-ing schemes, all three conditions above are satisfied.

Due to space constraints, we omit the proof of this lemma.
First, we present an algorithm for estimating R = w ( L ) sum of the weights of all the subsets in L , i.e., the collections of all possible subsets of C . Recall that all three considered weighting schemes assign a weight of 0 to all subsets that are not set-covers. Therefore, the problem of computing R is reduced to computing the sum of the weights of all the set covers
The basic idea is to consider the multiset U = M 1  X  . . .  X  M where the elements of U are pairs of the form ( S ,  X  ) corresponding discussion on the foundations of this technique.
 Algorithm 2 T he ImportanceCounter algorithm.
 1: R  X  0 2: for C  X  X  do 3: R ( C )  X  0 4: for iterations N , polynomial in d , do 6: S = RandomSample ( M  X  ) 7:  X   X  = CanonicalRepresenative ( S ) 8: if  X   X  =  X  then 9: R  X  r + w ( M  X  ) 10: for C  X  X   X  do 11: R ( C )  X  R ( C ) + w ( M  X  ) 12: for C  X  X  do 13: R ( C )  X  R ( C ) w ( U ) 1 5: return R , R ( C ) for every C  X  X  to
S  X  M  X  . I.e., for every set cover S , U contains as many copies of
S as there are M  X   X  X  for which S  X  M  X  . Notice that the total weight of U can be trivially computed as w ( U ) = P d
The multiset U is then divided into equivalence classes, where each class contains all pairs ( S ,  X  ) that correspond to the same set cover S . For each equivalence class, a single pair ( S ,  X  ) to be the canonical representation for the class. Now, R can be ap-proximated by generating random elements from U and estimating the fraction of those that correspond to a canonical representation of an equivalent class. The generation of random samples from is done by first picking a minimal set M  X  from M and with proba-bility proportional to the weight of M  X  . Then, we randomly sample S from M  X  . The different weighting schemes imply different sam-pling methods within M  X  . However, as Lemma 1 indicates, this sampling can be done in polynomial time.

The pseudocode of this algorithm, named CountSimple , is shown in Algorithm 1. The key idea of the algorithm is that instead of sampling from the whole space L , it only draws samples from Observe that each cover S can contribute weight at most d w ( S ) w ( U ) . Therefore, the ratio  X  = w ( L sc ) /w ( U ) is bounded from below by 1 /d . Following a similar argument as in Fact 1, we can estimate the value of w ( L sc ) using N  X  4 d number of samples required is polynomial to the size of M
To estimate the values of R ( C ) , we modify Algorithm 1 as fol-lows: every set M  X  M needs to be extended so that it contains set C . Using as M = {M 1  X  X  C } , . . . , M d  X  X  C }} , we can then run CountSimple for each entity separately and thus compute the number of covers that contain C .

Such an approach, however, is computationally expensive. Each execution of the CountSimple algorithm requires N calls to the CanonicalRepresentative routine and therefore O ( Nmd ) time. Since CountSimple needs to be executed m times, the total running time is O ( Nm 2 d ) . Even if N = O ( d ) any reasonable seed size is at least d = O ( m ) , implying that the approach needs time O ( m 4 ) .

We overcome this computational bottleneck by computing all the score values R ( C ) for every C  X  X  in a single (yet modified) call of the CountSimple routine. Algorithm 2 gives the pseudocode of this modified algorithm, which we call ImportanceCounter .
The main result of this section is that I mportanceCounter yields estimates for the m values of R ( C ) that are as good as ex-ecuting m times the CountSimple algorithm, while it requires almost the same number of samples.

T HEOREM 1. If the weighting scheme satisfies a certain  X  X -moothness X  condition, then Algorithm 2 gives an  X  -approximation of R ( C ) and R with probability (1  X   X  ) if w here  X  is a finite integer greater or equal to 1 . Furthermore, for the uniform weighting scheme  X  = 1 , for the cardinality-based weighting scheme  X  = 2 , while for the the threshold weighting there does not exist a finite  X  for which the above inequality holds.
Although we do not give the proof of the above theorem, we state the smoothness condition that we require for the weighting scheme w ( ) , so that the above theorem holds for finite  X  .

C ONDITION 1. If S and T are set covers with |T| = |S| + 1 , then the weighting function satisfies w ( S )  X   X  w ( T ) for some  X  &gt; 0 . In other words, if T is larger than S by one element, the value of the weighting function cannot drop too much.
 Running time: Note that the ImportanceCounter algorithm requires only N calls to the GenerateSuperset routine, ob-taining a factor of O ( m ) improvement over the na X ve approach of executing CountSimple m times.
Even though ImportanceCounter is much more efficien-t than NaiveCounter , its running time can still be prohibitive for very large datasets. Here, we present the Compressed-IC algo-rithm, which outputs the same scores as ImportanceCounter but it is much more efficient in practice.

In many of the datasets we considered, we observed that many of the entities in C are identical. For example, in the case of reviews, there are many identical reviews that comment on the same product attributes. Similarly, in the case of experts, there are many experts who have exactly the same set of skills. Compressed-IC takes advantage of this phenomenon while optimizing for running time.
One can observe that, for all our weighting schemes, identical entities are assigned the same cover scores. Compressed-IC takes advantage of this observation and compresses all identical entities into into super-entities . Then, the algorithm assigns scores to super-entities and in the end it assigns the same score to al-l the original entities that are part of the same super-entity. The key technical challenge we face here is to design an algorithm that operates on this compressed input and still assigns to all entities the correct scores . We say that a score assignment on the com-pressed input is correct , if the scores assigned to entities are iden-tical to the scores that would have been assigned to them by the ImportanceCounter algorithm before compression.

A na X ve way of dealing with identical entities in C is to simply ignore their multiplicities. In this case, all identical entities have a single representative and in the end they are all assigned the score of this representative. Although this procedure will reduce the number of entities and the running time of the ImportanceCounter algorithm, the output scores will be incorrect. In order to see that these scores are incorrect consider the following example.
E XAMPLE 3. Assume X = h U, Ci , where U = { u 1 , u 2 , u and C = { C 0 , C 1 , . . . , C K } with C 0 = { u 3 } and C C K = { u 1 , u 2 } . If we represent all entities C 1 , . . . , C gle representative b C = { u 1 , u 2 } we have a new problem instance X  X  = h U, b Ci , with b C = { C 1 , b C } . If we now compute the cover scores of C 0 and b C for input X  X  and for the uniform weighting scheme, we get that both weights are equal to 1 . However, in the original problem instance X = h U, Ci , the score of C 0 is K times larger than the score of any one of the sets C 1 , . . . , C
Instead of assigning the score of the representative to all the i-dentical entities, we could divide this score by the cardinality of each set of identical entities. This solution also fails to produce the correct cover scores. This is shown in the following example. E XAMPLE 4. Consider the problem instance X = h U, Ci , where U = { u 1 , u 2 , u 3 , u 4 } and C = { C 1 , C 2 , C 3 , C C 2 = { u 1 , u 2 , u 3 } , C 3 = { u 1 , u 2 } and C 4 = { u assume that we form new set b C = { b C 12 , C 3 , C 4 } , where { u 1 , u 2 , u 3 } . For the problem instance X = h X, b Ci and unifor-m weighting scheme, the scores assigned to b C 12 , C 3 and C respectively 3, 3, and 4. Now, if we attempt to find the scores in the original problem by dividing the score of b C 12 equally into C and C 2 we get that the final scores are: R ( C 1 ) = R ( C R ( C 3 = 3) and R ( C 4 ) = 4 . One can see that these scores are incorrect; they do not correspond to the number of solutions that the different entities participate. Even further, ranking the entities using these scores gives a totally unintuitive ranking: C appear to be worse than C 3 although they both cover a superset of the elements that C 3 covers!
The above two examples illustrate that if we want to form one super-entity for each set of identical entities in C , we need to make a very meticulous adaptation of the ImportanceCounter al-gorithm. We describe this adaptation here. First, we transform the input entities C into b C = { b C 1 , . . . , b C K } where every b entity that represents a set of identical input entities. Each such en-tity b C k  X  b C is characterized by its cardinality T k . The mechanics of the Compressed-IC algorithm are similar to the mechanics of the ImportanceCounter algorithm, shown in Algorithm 2. The main differences are explained below.

First, note that every minimal set M  X  now consists of super-entities and therefore the probability of M  X  being sampled is the weighted sum of the probabilities of all the minimal sets it rep-resents. Once a minimal set M  X  is selected, a particular element S is sampled from M  X  . The sampling is identical to the corre-sponding step of the ImportanceCounter algorithm. The re-sult of sampling is a set cover S that consists of j super-entities b C , . . . , b C j ; for each super-entity b C i that participates in know its cardinality T i as well as the number t i of actual entities that were sampled from b C i . By treating super-entities in the same way as entities, we check whether the canonical representative of the sampled set cover S is indeed the previously-selected minimal set M  X  . If this is not the case, no counter is increased. Other-wise, we need to do the following: first, we accept the match with probability Q b the score of C is increased by w ( M  X  ) t i T  X  1 of the Compressed-IC algorithm remain identical with those of the ImportanceCounter algorithm.
 We have the following result.

T HEOREM 2. The cover scores computed by Compressed-IC are correct. That is, they are identical to the scores computed by the ImportanceCounter algorithm. Although we omit the proof of Theorem 2 we consider this theore m as the second main technical contribution of our work.

The running time of the Compressed-IC algorithm depends only on the number of super-entities and not on the actual number of entities. Therefore, for datasets that contain small number of distinct entities the Compressed-IC algorithm is extremely effi-cient, while at the same time it outputs the correct scores for all the individual entities.

From now on, whenever we use the term entities, we will refer to the super-entities that are formed after the compression of the identical input entities.
As discussed early in the section, importance sampling requires the complete set M of all minimal set covers. However, enumerat-ing all the minimal set covers is a computationally expensive task; this problem is known as the transversal hypergraph problem. Al-though we experimented with various available methods for the problem [3, 9, 12], none of them was able to generate all minimal set covers in reasonable time, even for small inputs.

Therefore, we propose to execute Compressed-IC with a seed of minimal sets, which is a subset of M . Our intuition is that even a small-cardinality seed is adequate to accurately estimate the counts of the entities in the input. We form such seeds by starting with set C and randomly removing elements until we reach a minimal solu-tion. We repeat this process until we generate a seed of the desired size. Our experiments indicate that a seed of size moderately larger than m , where m is the total number of entities, is sufficient.
Assume that the input can be partitioned into disjoint connected components , where a connected component is defined over the bi-partite graph representation that we used in Figure 1. In this case, to compute R and R ( C ) for every C  X  X  , it is sufficient to compute the R ( C )  X  X  for all the C  X  X  that are in the same connected compo-nent separately. Such separate computations are much faster, since they operate on smaller inputs. We can then obtain the final counts by simply computing the product of the values reported for each component. For the datasets we have experimented with, this de-composition alone has yielded significant speedups. For example, in one of our datasets there were 2227 entities, while the largest component included only 705, i.e., about 30%, of them. Oftentimes, counting problems are solved using the Markov chain Monte Carlo method [20], and Metropolis-Hastings [11]. In our setting, such a method would sample elements of L sc by perform-ing a random walk on the space L sc . Appropriate definitions of the random walk can guarantee unbiased estimates of the required s-cores. Using the appropriate random walk, we experimented with such a method and found it is very inefficient when compared to Compressed-IC . Thus, we do not present this method in detail, neither do we show any experiments for it.
In this section, we describe the experiments we conducted to e-valuate the Compressed-IC algorithm, in terms of both utility and performance. We also report the results of a user study that demonstrates the validity of our scores and their closeness to the human perception of important entities. All our results are obtained using the step weighting scheme, since it is the obvious choice in applications where the size of the set cover solutions is important. For all our experiments, we use a machine with 8GB of RAM and 2.66GHz processor speed.
We evaluate our algorithms using three families of datasets, which we describe below.
 GPS datasets: We consider the reviews of the five most-frequently reviewed GPS systems sold on Amazon.com, as of 2010. Thus we compile five different datasets in total. For this dataset family, the universe U is the set of product attributes. Each review C ments on a subset of the product attributes. In other words, each review is represented by the set of the attributes discussed in the re-view. We construct the subsets C i by parsing the reviews using the method by Hu and Liu [13], which can be shown to extract the re-view attributes with high accuracy. An attribute is covered if there exists at least one review that discusses this attribute. The cover score R ( C i ) expresses the usefulness of the review C i to the complete text for each review, the datasets also include the number of the accumulated helpfulness votes of each review; we anonymize the five datasets and simply refer to them as GPS-1 GPS-2 , GPS-3 , GPS-4 and GPS-5 . The number of reviews in these datasets are 375, 285, 254, 216 and 222, respectively. Guru datasets: We construct the Guru datasets by crawling the www.guru.com website, which hosts profiles of active freelancer-s from various professional disciplines, such as software engineer-ing, marketing, etc. We extracted the skills of each each freelancer and we compiled five different datasets as follows: each dataset cor-Each dataset consists of all the freelancers who have at least one of the skills required by the respective task. We refer to the five datasets as Guru-1 , Guru-2 , Guru-3 , Guru-4 and Guru-5 . The number of freelancers in each one of these datasets is 109 158, 184 520, 239 856, 300 256 and 317 059 respectively.

With the Guru datasets, we evaluate the ability of our algorithm to identify the freelancers who are most valuable for a given task. Consequently, the universe U is the set of skills required for the task. Each freelancer C i is represented by the set of his skills, as they appear in his profile. The cover score R ( C i ) given by our al-gorithm quantifies the usefulness of the freelancer C i , with respect to the given task and can be used to rank experts.
In this section we provide a comprehensive experimental evalua-tion of the behavior of the scores R ( C ) output by Compressed-IC as well as the running time of the algorithm.
 Inefficiency of the na X ve Monte-Carlo sampling: We start the presentation of our experimental results by pointing out that the the na X ve Monte-Carlo sampling technique presented in Section 3.1 is impractical. The reason for that is that the na X ve sampling rarely succeeds in forming a set from L sc . In practice, for each one of our 10 datasets, we sampled 10 000 sets from L : in 5 out of 10 datasets 0% of the samples were set covers; in 4 out of 10 1% of the samples were set covers and in the remaining one 12% of the samples were set covers. This indicates, that we cannot rely on this technique in order to compute the R ( C ) scores.
 Compressed-IC vs. ImportanceCounter : As we discussed in Section 3.2.4, the Compressed-IC algorithm gives the same results as ImportanceCounter , but it is designed to handle datasets with multiple copies of entities in C . Thus, the only differ-ence between the Compressed-IC algorithm and Importance-Counter is that the former keeps a single copy for every set of i-dentical entities, rather than considering each one of them separate-Table 1: C ompressed-IC ; GPS and Guru datasets: ratio of the num-ly. In other words, the C ompressed-IC algorithm compresses entities into super-entities. Table 1 shows the corresponding pression ratio ; that is, the ratio of the number of super-entities (after compression) to the number of total entities (before compression) in all 10 datasets. The smaller the value of this ratio the larger the compression achieved by the super-entities.
 The results show that the compression ratio achieved for the datasets ranges from 67% to 81%. For the Guru datasets, this ratio is much smaller, ranging from 0.01% to 2.5%. Such large com-pressibility can be explained by the nature of this particular data type; recall that, in the Guru datasets, each freelancer is associated with a set of skills. It is much more likely to have multiple free-lancers with identical sets of skills; for example, it is expected that all programmers know more or less the same set of programming languages. On the other hand, when it comes to product reviews, it is less likely (although possible) to find reviews that comment on exactly the same set of product attributes.
 Effect of the size of the seed of minimal sets: As we discussed in Section 3.2, ImportanceCounter requires as input a set of minimal solutions. While, in principle, the complete set of all such solutions is required, we show here that using a subset is sufficient for the algorithm to converge. First, we run the algorithm with d-ifferent seed sizes and retrieve the produced cover scores for each individual. Specifically, we try all the seed sizes in { m 30 m , 40 m } (in that order), where m is the total number of entities. We then compare the scores obtained for the different entities after consecutive runs (i.e., m vs. 10 m , 10 m vs. 20 m , etc.). Since we use these scores to rank entities, we assess the stability of the dif-ferent runs by comparing the ranked lists induced by the computed scores. For comparing ranked lists, we use the popular Kendall-rank correlation coefficient [15]. The measure takes values in the lower the value the more different the two lists are.
The process is repeated for all 10 datasets. The results are shown in Table 2. Each value is an average of 50 experiments, each us-ing a different seed set. The results show that even though we use very small seed sets, the scores obtained with seeds of different sizes produce very similar ranked lists. In particular, as the size of the seed set grows very moderately most values of  X  become very close to 1.00. This behavior is verified across all three families of datasets. Therefore, we conclude that the Compressed-IC algo-rithm is robust, and the estimated scores produce stable rankings even when considering small-size seed sets.
 Effect of the number of samples: Theorem 1 provides a theoreti-cal bound on how many samples are necessary for the Importance-Counter and Compressed-IC algorithms compute the true val-ues of R ( C ) under different weighting schemes. In this experimen-t, we demonstrate that, in practice, even smaller number of sample is sufficient.
 Table 2: Kendall- X  rank correlation between rankings induced from cover Table 3: N umber of samples and time required until the Kendall-Our experimental setting is the following: first we apply the C ompressed-IC algorithm to each of the 10 datasets, using a seed of size 20 m . As a convergence criterion, we use the Kendall- X  rank correlation coefficient. We let the algorithm run until the Kendall- X  coefficient reaches a value of 0 . 90 (while checking the  X  coefficient every 1000 samples).

The results of our experiments are summarized in Table 3. A-gain, the results reported in Table 3 are averages over 50 repeti-tions, where each repetition considers a different seed set. For each dataset, in the second column of the table, we report the number of entities in the dataset since it has direct impact on the complex-ity of algorithm.Next, we report the total number of total samples required until reaching Kendall- X  value of 0.90 (third column), as well as the actual computation time in seconds (fourth column).
As shown in Table 3, in most cases a small number of samples is needed: in all cases but one the number of samples required is less than 10 000. This is true even for the Guru datasets, which have three orders of magnitude more sets than the other datasets. Therefore, the number of required samples grows very mildly with the number of sets in the dataset. On the other hand, the number of entities impacts the overall running time of the algorithm. This is expected since the time required for drawing one sample grows linearly with the number of entities in the dataset.
Here, we compare the scores produced by Compressed-IC with other baselines. Due to space constraints, we only show results for the largest dataset from each family.

For the GPS-1 dataset, we use as baseline score the helpfulness score of a review, i.e., the number of users that have declared this review to be helpful. The helpfulness score is a popular criterion for ranking reviews in review-hosting sites (e.g., a mazon.com Figure 2(a) shows the scatterplot between the cover and the help-fulness scores; note that the cover scores are normalized, i.e, scores R ( C ) are divided by R so that the measures are in [0 , 1]
We observe that there is no correlation between the cover and helpfulness. For example, helpfulness does not directly consider the contribution of each review in the coverage of an item X  X  at-tributes. Instead, it relies on the willingness of the users to reward a  X  X ood X  review by giving a helpfulness vote. In such a scheme, older reviews are more likely to collect higher helpfulness votes, regardless of whether they are more informative [19]. Our scoring scheme, on the other hand, is unaffected by such biases, since it objectively evaluates reviews based on their contribution.
For the Guru-5 dataset, we select a frequency-based baseline score, i.e., the number of skills of each freelancer. The scatter plot between our measure and the baseline is shown in Figure 2(b). This baseline measure can take only one of 5 distinct values {1,2,3,4,5} and considers all freelancers with the same number of skills as e-qually important. This is an over-simplifying assumption that does not take into consideration factors like the rarity of each skill. For example, one of the freelancers has the following five skills: { dren X  X  Writing, Writing-Editing &amp; Translation, Reports, Technical Writing, Blogs }. While this freelancer has the maximum number of skills, all of these skills are very common. This fact is captured by our own measure, since the particular freelancer participates in far less covers than most of his peers. As a result, this freelancer is given one of the lowest observed scores (0.007). On the oth-er hand, a freelancer with only three skill { Commercial Interior Design , Landscape Design } is assigned one of the top-30 scores (0.49). This is because his second skill ( Landscape Design rather rare skill, possessed by only 260 other individuals. There-fore, our scoring scheme can differentiate between individuals with the same number of skills, and it can also identify important experts even when they have small number of skills.
In this section, we present a user study that we conducted in or-der to validate the assumption that cover scores give rankings that agree with human intuition. Using the original dataset from we sampled 10 small groups of freelancers; the cardinality of each group was randomly selected between 5 and 10. For the i -th group of freelancers we created a task T i , which was defined as the u-nion of the skills of the freelancers in the i -th group. Using these 10 tasks we set up a survey on kwiksurveys.com , in which 30 human subjects participated. The subjects were asked to inspec-t the skills required for a task and the skills of the freelancers in the corresponding group. The subjects were then asked to rank the freelancers in the group by assigning to every freelancer a numeric  X  X sefulness X  score with the following interpretation: 4 if the free-lancer is absolutely necessary, 3 if she is very useful, 2 if she is somewhat useful, and 1 if she is not useful. In the instructions it was made clear that the team will have to complete the task repet-itively and that the task can only be completed if all the required skills are represented.

Figure 3 shows the average Kendall- X  rank correlation coeffi-cient between the rankings obtained by the humans and the rank-ings obtained by three automated measures: ( i ) using the cover scores, ( ii ) the number of skills per freelancer, and ( mum set-cover solution; in the latter ranking all freelancers in the minimum set cover were ranked higher than the rest. Recall that the Kendall- X  coefficient takes values in [0 , 1] , with 1 (resp. 0) de-noting the largest (resp. smallest) similarity between the compared rankings. The results indicate that the ranking obtained by cover s-Figure 2: Scatterplots of (normalized) scores of entities against their baseline scores. Figure 3: Average Kendall- X  r ank-correlation coefficient between human rankings and rankings obtained by cover scores, number of skills and minimum set covers. cores has consistently high Kendall- X  value; this value never drops below 0.8 and it is close to 1 for most of the tasks. On the oth-er hand, with the exception of task T 4 , the Kendall- X  coefficients between human rankings and rankings by the number of skills per freelancer are notably low. For task T 4 , we observe the highest sim-ilarity between these two rankings. This is because the freelancers in the 4 -th group are more or less equivalent both in terms of their count scores and in terms of their number of skills (all of them have 1 or 2 common skills). Therefore, the frequency and the cover-score rankings are very similar to each other and also similar to the human rankings. This also explains why for task T 4 the minimum set-cover ranking does not correlate well with the human ranking. Apparently, the single set-cover solution does not reflect the equiv-alence between the freelancers and the fact that they all participate in many different set-cover solutions. Other than T 4 , the minimum set-cover rankings correlate well with the human rankings. This is mostly because for many of the tasks we considered the number of freelancers is small and therefore the majority of the freelancers participate in the minimum cover. Nevertheless, this correl ation is not as strong as the one between human and cover-score rankings.
Overall, the user study demonstrates that humans perceive cover scores as a natural ranking mechanism.
To the best of our knowledge, we are the to formalize frame-work that allows counting techniques for the evaluation of entity importance. Nonetheless, our work has ties to existing research. We summarize some of this work here.
 Counting set covers. There are many theoretical studies on the problem of hitting-set counting, [6, 7, 8, 10] including complexi-ty studies and algorithms for special inputs. Since the hitting set and the set cover problems are isomorphic, these methods can al-so count set covers. Among these works, the closest to ours is the work of Damaschke and Molokov [8], that proposes a parameter-ized algorithm for counting all k -hitting sets (i.e, hitting sets of size at most k ) in set systems, where the size of the maximum set is at most r . Their algorithm runs in time that is exponential in in r , and despite its theoretical elegance, it is impractical for rea-sonably large datasets.
 Review selection. Lappas and Gunopulos [16] considered the prob-lem of finding a small set of reviews that cover all product at-tributes. Tsaparas et. al. [23] studied the problem of selecting a set of reviews that includes both a positive and negative opinion on each attribute. More recently, Lappas and Terzi [18] proposed a scheme for showing to users different covers of the same corpus of reviews. All these works focus on picking a single set of re-views that optimizes a particular function under a set of constraints. On the other hand, our framework evaluates reviews by assigning a score based on the weight of all the solutions that each review participates in. Although the notion of coverage is central to our framework, our intuition and methodology are distinct from exist-ing work in the area.
 Team formation. The problem of identifying a set of individu-als from a pool of experts who are capable of performing a given task has been an active area of research in Operations Research [5, 25, 26]. Our own recent work has introduced the problem in the computer science domain [2, 17]. This work focuses on identifying a single team of experts that collectively meet a certain set of re-quirements. Our framework is complementary to this: it identifies the marginal value of each expert with respect to a given task. Also, instead of reporting a single solution of the optimization problem at hand, our framework counts all solutions. Thus, the technical challenges we face are orthogonal to the those faced when looking for a single good solution.
Our work is motivated by data-mining problems that have been formalized in terms of the set-cover problem. For such formula-tions, we have developed a novel framework for evaluating the im-portance of entities. Instead of looking at a single set-cover solu-tion, our framework computes the importance of entities by count-ing of the number of good set covers an entity participates. Our al-gorithmic contribution is the design of Compressed-IC , which is an efficient algorithm for solving this problem. Our algorithm is proven to provide the correct counts and scales extremely well. Our framework has applications in numerous domains, including those of human-resource management and review-management systems. In a thorough experimental evaluation, we have demonstrated the efficiency and the effectiveness of our methods, using real datasets from such domains.
 Acknowledgements. Aristides Gionis was partially supported by the Torres Quevedo Program of the Spanish Ministry of Science and Innovation, co-funded by the European Social Fund, and by the Spanish Centre for the Development of Industrial Technology under the CENIT program, project CEN-20101037,  X  X ocial Medi-a X  ( http://www.cenitsocialmedia.es/ ).
 Theodoros Lappas and Evimaria Terzi were supported by the NSF award #1017529, and from gifts from Microsoft, Yahoo! and Google.
