 Plant traits are morphological, anatomical, biochem-ical, physiological or phenological features of individ-uals or their component organs or tissues, e.g., the height of a mature plant, the mass of a seed or the nitrogen content of leaves ( Kattge et al. , 2011 ). They result from adaptive strategies and determine how the primary producers respond to environmental factors, affect other trophic levels, and influence ecosystem functioning ( McGill et al. , 2006 ). Plant traits there-fore are a key to understanding and predicting the adaptation of ecosystems to ongoing and expected en-vironmental changes ( McMahon et al. , 2011 ). To im-prove the empirical data basis for such projections, in 2007 the TRY project (http://www.try-db.org) was initiated, aimed at bringing together different plant trait databases worldwide. Since then the TRY database has accomplished an unprecedented cover-age. It contains 2.88 million trait entries for 750 traits of 1 million plants, representing 70,000 plant species. The consolidated database is likely to become a stan-dard resource for the ecological community and to sub-stantially improve research in quantitative and predic-tive ecology and global change science.
 Despite its large coverage, TRY data are highly sparse, which constrains the usefulness of the joint trait database. Since traits are correlated and they do not vary independently, quite a few quantitative or predictive tasks in ecology require each  X  X eferenced X  object (It could be an individual plant or a species at a site, but we only use the plant as an exam-ple in the following.) to have multiple traits fully available. However, in TRY database, the number of plants with more than same three traits available is extremely small, making it tricky to perform such tasks on TRY data directly. There are two possible solutions: The first is X  X hopping X , i.e., removing all plants with target traits missing. Such a simple strat-egy results in reduced statistical power and may sig-nificantly alter parameter estimates and model selec-tion ( Nakagawa &amp; Freckleton , 2008 ), and for TRY this would actually reduce the data available to a nearly uselessly small number of plants. The second strategy is  X  X illing X , i.e., based on non-missing trait entries, fill-ing in the missing entries with predicted values, which yields a complete data set for further processing. In this paper, we focus on the  X  X illing X  strategy. If we consider the trait data as a plant  X  trait matrix X , with each entry being a trait value, X is a highly sparse matrix and the  X  X illing X  problem becomes a matrix completion problem. Meanwhile, the plant kingdom is hierarchically structured. Based on genetic and phe-notypic similarity, individual plants can be grouped to species, species to genera, genera to families, and families to phylogenetic groups. Besides trait measure-ments, each individual plant thus has a set of hierarchi-cally structured phylogenetic information. Therefore, our matrix completion task aims at effectively using such hierarchical structure for better prediction. Matrix factorizations have achieved great success in matrix completion ( Salakhutdinov &amp; Mnih , 2007 ; Salakhutdinov &amp; Srebro , 2011 ; Candes et al. , 2009 ). However, one limitation of most methods when applied to TRY data is the inability to use hierarchical phylo-genetic information. In this paper, we propose hierar-chical probabilistic matrix factorization (HPMF) to in-corporate phylogenetic information into matrix factor-ization for trait prediction. We demonstrate HPMF X  X  high prediction accuracy, effectiveness of incorporating hierarchical structure and ability to capture trait cor-relation through experiments. Meanwhile, since the phylogenetic tree of plants is constructed under the parsimony principle, plants within a species usually have highly similar trait values, only with small vari-ations due to individual or environmental differences. Therefore, species mean is widely used for gap filling in the ecological community. In experiments, we also show that HPMF generates significantly higher accu-racy than prediction using species mean. Although HPMF is proposed specifically for trait data in this paper, in principle it could be generalized and applied to other data with a similar form. In particular, it could be applied to the data matrix with a hierarchy on one side. The hierarchy is balanced such that the distances from the root to all leaf nodes are the same in terms of the levels. In addition, the nodes at the same level have a uniform interpretation, e.g., all nodes at the top level denote phylogenetic groups in our case. The rest of the paper is organized as follows: Section 2 proposes the HPMF. Section 3 shows experimental re-sults. We give a brief overview of the related work in Section 4 and conclude in Section 5 . We propose HPMF to incorporate hierarchical infor-mation into matrix factorization for missing value pre-diction. The hierarchy for trait data includes the phy-logenetic group , family , genus , species , and plant from top to bottom. The hierarchy is considered as side in-formation for the plant  X  trait matrix with missing en-tries. In addition, we also have data matrices at upper levels, such as species  X  trait matrix, genus  X  trait ma-trix, etc.. These matrices could be constructed from the plant  X  trait matrix and the hierarchy. The details are discussed in Section 3 . We assume the data matri-ces at all levels to be available before running HPMF. Denote the data matrix at each level  X  with X (  X  )  X  R tom level L . X (  X  ) at each level has S (  X  ) non-missing entries, and each row n (  X  ) and column m (  X  ) has a latent trated in Figure 1 , the generative process for HPMF at level  X  is as follows: 2. For each column m , generate v (  X  ) m  X  In general, the upper-level latent factor is used as the prior parameter to generate the lower-level latent fac-tor. On the row side, the upper-level latent factor is picked based on the phylogenetic information; on the column side, the one on the same trait is always used. Denoting latent factor matrices at level  X  with U (  X  )  X  R  X  where {} L  X  =1 denotes the data at all L levels ( L = 5 for TRY data), and  X  (  X  ) nm = 1 when the entry ( n, m ) of X is non-missing and 0 otherwise. MAP inference on the logarithm of the posterior in ( 1 ), which boils down to minimizing the regularized squared loss as E = +  X  u X where  X  u =  X  2 / X  2 u and  X  v =  X  2 / X  2 v . Next, we explore approaches for doing the MAP inference.  X  X iagonally X  stacking X (1) to X ( L ) together yields a and column M (  X   X  1)+1 to M  X  in  X  X . Stacking U (  X  ) and V (  X  ) s together for  X  = 0 . . . L yields  X  U  X  R k  X  construct an undirected graph W u on the  X  U side, where W u ( n, n  X  ) = 1 if n is the parent or child of n  X  based on the phylogenetic information. Similarly, we have W v on the  X  V side, where W v ( m, m  X  ) = 1 if m and m  X  are the same trait at two consecutive levels. Given W u , we can define the graph Laplacian ( Luxburg , 2007 ) L u and L v on rewritten using graph Laplacian for regularization as: The problem is not jointly convex on (  X  U ,  X  V ), so one can consider alternately updating  X  U and  X  V to reach a stationary point. Keeping  X  V fixed, the objective is a quadratic form in vec(  X  U ), whose solution is given by a linear system of the form A vec(  X  U ) = b . In spite of the sparsity structure in A , solving such a linear sys-tem in every iteration can lead to a prohibitively slow algorithm. Hence, we focus on an efficient alternative: stochastic block co-ordinate descent ( Bertsekas , 1999 ). At each step, we update U (  X  ) or V (  X  ) at level  X  through minimizing the objective function in ( 2 ) while keeping U at each level  X  (  X  = 1 . . . L ), the objective function containing U (  X  ) and V (  X  ) is given by E +  X  u X +  X  v X where c ( n ) is the set of child nodes of n , e.g, if n is a species, c ( n ) denotes plants of that species, and 1 (  X &lt;L ) is an indicator function taking value 1 when  X  &lt; L and 0 otherwise. The regularization terms k u (  X  ) n  X  u (  X   X  1) responding latent factors at level  X   X  1, and the regular-tors at level  X  + 1 (if applicable). Stochastic gradient descent (SGD) is used to optimize ( 4 ) by taking one spondingly.
 An alternative objective function to ( 2 ) is E = X +
X In this case, we do not have X (  X  ) for  X  &lt; L , so u (  X  ) and v (  X  ) m at levels  X  &lt; L are only used for regular-ization, we hence refer to it as hierarchy-regularized PMF (HRPMF). However, since there is no data cor-the minimizer of ( 5 ) will have k u (  X  ) n  X  u (  X   X  1) words, the latent factors u (  X  ) n ( v (  X  ) m ) at intermediate levels  X  = 2 . . . L  X  1 effectively reduce to one variable u n ( v archy. In Section 3 , we show that HRPMF does not generate satisfactory results.
 Once we have U ( L ) and V ( L ) inferred, any missing en-try ( n, m ) in the original matrix X ( L ) can be predicted In this section, we present experimental results for trait prediction on TRY data. We show results in pre-diction accuracy and correlation between traits. 3.1. Dataset Currently, data collection and cleaning is still going on in the TRY project. In our experiment, we use a cleaned subset, which is a matrix containing 273,777 plants and 17 traits (Table 1 ), and 95.3% of entries are missing. The percentage of missing entries in each trait is highly unbalanced, ranging from 66.8% to 99.6%. Starting from the top of the phylogenetic hierarchy, there are 8 phylogenetic groups, 450 families, 7160 gen-era, 45,824 species, and 273,777 plants.
 Given the original plant  X  trait matrix and the hierar-chy, we construct a data matrix on each level of the hierarchy by taking the means. For example, given the plant  X  trait matrix, together with species for each plant, we can construct a species  X  trait matrix by tak-ing the mean of the plants in the same species. Simi-larly, we can also construct a genus  X  trait matrix, fam-ily  X  trait matrix, etc..
 The traits in question have log-normal distribu-tion ( Kattge et al. , 2011 ), so we first take the loga-rithm for entries in the plant  X  trait matrix, and then calculate the z -score for each trait, i.e., for x nm cor-responding to plant n and trait m , we convert it to nm = (log( x nm )  X  lm m ) /ls m , where lm m and ls m are the mean and standard deviation of the logarithm of trait m . After this step, most of the traits are dis-tributed normally ranging from -4 to 4. The results we show are in the transformed space. 3.2. Accuracy in Trait Prediction We first show the accuracy of trait prediction for HPMF by comparing it with other methods. 3.2.1. Algorithms We run five algorithms on the TRY data: MEAN, PMF, PMF using hierarchical information for initial-ization level by level (LPMF), HPMF and HRPMF. The details of each approach are as follows: MEAN : A  X  X ierarchical mean X  strategy is used. For example, to predict trait m of plant n , among all plants with trait m available for training, if there are plants in the same species as plant n , we use species mean for prediction; otherwise, if there are plants in the same genus with plant n , we use the genus mean, and so on. In general, among species mean, genus mean, fam-ily mean, and phylogenetic group mean, we use the first available one at the lowest level. In the ecological com-munity, taking species mean is a common way to deal with missing data and is highly accurate, since most of the variation is between species and only little within species ( Kattge et al. , 2011 ).
 LPMF : We run PMF ( Salakhutdinov &amp; Mnih , 2007 ) on data matrices level by level following a top-down and bottom-up mode iteratively: first from X (1) to X ( L ) (top-down), and then from X ( L ) to X (1) (bottom-up), repeated for several times. At each level  X  , we have where initU (  X  ) and initV (  X  ) are initializations of U and V (  X  ) . To incorporate the phylogenetic informa-tion, we use the following strategy: in top-down mode, at each level  X  , we set initU (  X  ) and initV (  X  ) based on immediate upper level factorization result U and V , larly, in bottom-up mode we set initU (  X  ) and initV (  X  ) based on immediate lower level factorization result initV (  X  ) m = V (  X  +1) m , where | | denotes the number of elements in the set. The intuition is that we use the most updated U and V for the current level initializa-tion immediately.
 HPMF : We run stochastic block coordinate descent as explained in Section 2 . In principle, block co-ordinate descent allows us to update { U (  X  ) } L  X  =1 and { V (  X  ) } L  X  =1 in an arbitrary order. Empirically, we do it level by level iteratively following a top-down and bottom-up order. In each iteration, we first do a top-down pass to update U (1) , V (1) to U ( L ) , V ( L ) , fol-lowed by a bottom-up pass to update U ( L ) , V ( L ) to
U (1) , V (1) , and repeat the process for several itera-tions. The intuition is that after updating U (  X  ) , V (  X  ) we want to immediately use it for regularization in the next level update. Empirically, we observed that such a strategy converges faster than only doing top-down updates repeatedly.
 HRPMF : We run HRPMF in a similar way as HPMF, but use ( 5 ) as the objective function.
 PMF : We run PMF ( Salakhutdinov &amp; Mnih , 2007 ) directly on the plant  X  trait matrix without any phylo-genetic information, so the prediction is purely based on non-missing traits in the matrix. PMF can be con-sidered as a special case for LPMF and HPMF when no hierarchical information is used. 3.2.2. Methodology Some plants only have one trait available in our data, and we need at least one trait for each plant to run matrix factorization methods. Therefore, we split the training, test and validation sets as follows: For each plant, if it has at least three traits available, we ran-domly hold out one trait for test, one trait for valida-tion, and rest for training; if it has two traits available, we randomly hold out one trait for training and one for test; if it only has one trait available, we use it for training. Following such a strategy, each plant has at least one trait in the training set. The test set is used for test and the validation set is used during the training process for early stopping, i.e., after 5 itera-tions, if the performance on validation set decreases, we stop training. We repeat the holding-out process 5 times to get 5 randomly split datasets, and construct the upper-level matrices for training and validation, but the test set is only at the plant  X  trait level. To investigate how the algorithms react to increasing hierarchical information, we first only use the phylo-genetic group information, and gradually add family, genus, and species information one level at a time, un-til we incorporate all levels into the algorithm. RMSE is used for evaluation. Assuming there are to-tally T entries for test, a t is the true value and  X  a t the predicted value, RMSE is defined as RM SE = p P 3.2.3. Results The result for different algorithms are in Table 2 . The first row shows the result without using phylogenetic information. In this case, MEAN uses the overall mean of all plants for each trait for prediction. The rest rows show the result with increasing phylogenetic informa-tion being used. The result for LPMF, HPMF and HRPMF are obtained from 5 top-down and bottom-up passes. The main message is as follows: (1) For MEAN, LPMF, HPMF, and HRPMF, RMSE keeps decreasing when more phylogenetic information gets incorporated. (2) HPMF outperforms MEAN at all levels by a large margin. We run a paired t -test and find HPMF is sig-nificantly better than MEAN with a p -value smaller than 10  X  5 at all levels. (3) HPMF outperforms LPMF, and its advantage is more distinct when only coarse-level phylogenetic in-formation is available. One possible reason is as fol-lows: Finer-level phylogenetic data privide more pre-cise information than the coarse-level data. For ex-ample, trait values on plant level are usually closer to trait values on the species level than those on the genus level. Therefore, using species information even just for initialization, LPMF yields a fairly good result. However, for coarse-level phylogenetic data, incorpo-rating it through initialization alone may not work well. A model which captures the hierarchical infor-mation explicitly, such as HPMF, is more powerful. (4) HRPMF does not perform well. As explained in only used in the regularization and there is no data directly associated with them, the hierarchy is not ef-fectively used. (5) Figure 2 shows how RMSE changes with increas-ing number of iterations (top-down and bottom-up passes). In the first three iterations, for both LPMF and HPMF, RMSE drops quickly, and then it does not change much along the time.
 We do a closer comparison between HPMF and MEAN based on one run of these two algorithms. As we have mentioned, using species mean is highly accurate due to the small variance within the species. Therefore, it is interesting to differentiate the result from the species mean and upper level means for the MEAN strategy, and compare them to HPMF respectively. In particu-lar, the test data is divided to 17 parts, one for each trait, and each part is further divided to two: in part A, for each trait m of plant n , there exist plants in the same species with n and their trait m is non-missing. Therefore, the species mean is used for MEAN on this part of data. The rest of the test data are in part B, on which genus or upper-level means are used as available. Such a split of test data does not affect the training process of HPMF, but for test, we compute HPMF X  X  results on part A and B of each trait respectively. The comparison of HPMF and MEAN is presented in Fig-ure 3 , where we show RM SE MEAN  X  RM SE HP MF on each trait, so if the bar is above zero, HPMF per-forms better, and otherwise MEAN performs better. From Figure 3 , we can see that even on part A, where species mean are used for MEAN, HPMF is perform-ing slightly better than MEAN on most traits. The only two traits where MEAN is doing a better job is trait 3 (Seed Mass) and 9 (Stem Conduit Density). These traits have large variation among species, hence species mean is doing well and the collaborative filter-ing strategy of HPMF is not very helpful by borrow-ing information from other species. For the result on part B, where upper level means are used for MEAN, the advantage of HPMF is more distinct. This result is important, because it demonstrates HPMF X  X  good prediction performance when trait data in the same species is not available. 3.3. True Trait vs Predicted Trait In the following sections, we will show a variety of results for HPMF other than RMSE.
 While RMSE gives an overall accuracy, it masks de-tails about the distribution of values and about fits of predicted to true trait values. Therefore, we do a scat-ter plot for each trait, using true value and predicted value on test entries. Such scatter plots are useful to the ecological community since they show the dis-tribution of values; the shape, variance and scatter of predictions in relation to the full spectrum of trait val-ues; and whether the predictions are close to the 1:1 line. The results for HPMF are presented in Figure 4 , where we use two traits as examples, viz., Wood Ves-sel Element Length and Leaf Fresh Mass. Each dot denotes one entry in the test set matrix.
 The result in the first column is from HPMF without phylogenetic information (i.e., PMF). We show two typical results. For Wood Vessel Element Length, the dots roughly form a horizontal line. For Leaf Fresh Mass, the dots form an  X  X  X -shape plot containing two parts: one part is close to a 1:1 line and the other part is close to a horizontal line. A horizontal line indicates poor prediction, since there is no evident correlation between the true and predicted value, while a 1:1 line indicates good prediction. The reason for the  X  X  X -shape plot is as follows: Leaf Fresh Mass is strongly correlated with Leaf Area (as indicated in Figure 5 in the next subsection), so when we predict the Leaf Fresh Mass, for plants with Leaf Area trait available, PMF is able to use the information on Leaf Area and makes a fair prediction (dots close to the 1:1 line), but for plants whose Leaf Area is not available, PMF cannot do much and gives a poor prediction (dots on the horizontal line).
 Starting from the second column, when more phylo-genetic information is used, the horizontal line grad-ually rotates counter clockwise and the shape of the plot becomes more concentrated to the 1:1 line. But note that especially in the second column the slope of the line is still less than 1, suggesting overpredicting at low values and underpredicting at high. Finally, in the last column when all phylogenetic information is used, the plots are roughly 1:1 lines, indicating that the predictions are close to their true values. Thus, for HPMF with all phylogenetic information, one can get accurate prediction in Leaf Fresh Mass even for plants whose Leaf Area is not available, and accurate predic-tion for Wood Vessel Element Length even if there is no trait closely correlated with it. 3.4. Trait Correlation One of the most interesting aspects for the ecolog-ical community is to explore the correlation among the traits and how they vary jointly ( Wright et al. , 2004 ; Baraloto et al. , 2010 ). Traits do not vary in-dependently since there are always constraints, e.g., tiny plants cannot have large leaves; and there are also tradeoffs, e.g., plants with thin leaves (low SLA) tend to have high leafN and leafP and thus high photo-synthetic rate, and vice-versa. These constraints and trade-offs have implications for ecosystem functioning and the potential of plants to adjust to changing en-vironmental conditions. Therefore, it is interesting to explore the correlation among traits and to further un-derstand the underlying causes and mechanisms. How-ever, due to the high sparsity of the TRY data, it is usually not possible to derive correlation pattern be-yond two or three traits, since there are not enough plants having a specific set of more than two or three traits available. Matrix completion technique makes such analysis possible. However, it requires the pre-dicted correlation to be close to the true correlation. In this subsection, we show the correlation result for pairs of traits. Given the plant  X  trait matrix, we ran-domly hold out 80% of entries for training, 10% for validation, and 10% for test. A few test entries do not have training data in the same row (plant) so we can-not predict their values. We ignore these entries and only focus on the ones we can predict.
 Given any pair of traits, for plants with both traits available in the test data, we can do a scatter plot for each pair as in Figure 5 , where each dot is one plant, and its two coordinates are the two trait values. Figure 5 presents some examples of trait pairs with strong positive or negative correlation. We can see that the correlation from the prediction (green) is close to the correlation from the true test set (red). Interestingly, HPMF is picking up accurate correlations, which is a second order information and was not part of the objective function. We explain the observed correlation for each pair as follows: LeafFreshMass-LeafArea : Leaves with large area tend to have higher mass, as leaf thickness is con-strained. (physical constraint) LDMC-SLA : Thin leaves with little density (high SLA) tend to have only few structural components to make the leaves robust, they often get their tension for water pressure within the cells (turgor). Thus they have little dry matter content relative to water content: these plants invest little carbon to make the leaves robust. These leaves are often short lived. (leaf economic trade-off ( Wright et al. , 2004 )) LeafN-LeafP : For the processes of living, both nitrogen (N) and phosphorus (P) are needed in a specific relation. N is for proteins, and P is for genes and energy distribution within cells. (physiological constraint ( Wright et al. , 2004 )) Apart from using species mean for filling gaps in trait matrices, missing value prediction in ecol-ogy is commonly solved by two widely used ap-proaches ( Nakagawa &amp; Freckleton , 2008 ). First is multiple imputation ( Rubin , 1987 ) methods, which typically replace the missing by 3-10 simulated ver-sions. Second is augmentation methods, which im-plicitly fill in missing entries in the context of model parameter estimation. In these cases the gap filling becomes part of model parameter estimation and the missing entries are not explicitly available. In the machine learning community, low-rank factor-ization based algorithms have been developed fast for matrix completion. Salakhutdinov &amp; Mnih ( 2007 ) propose probabilistic matrix factorization (PMF). Lawrence &amp; Urtasun ( 2009 ) propose a non-linear ma-trix factorization with Gaussian processes. In addi-tion, there has been work on trace norm regularized matrix factorization ( Salakhutdinov &amp; Srebro , 2011 ), and algorithms which discover low-rank and sparse components ( Candes et al. , 2009 ). Recent years have seen emergence of work on incorporating hierarchi-cal structure into matrix factorization. Menon et al. ( 2011 ) use the hierarchical structure to help factorize the click through rate matrix on advertisements. In this paper, we focus on predicting missing traits for plants in TRY database. We propose HPMF which can incorporate hierarchical phylogenetic information into matrix factorization. We show that HPMF im-proves the prediction accuracy considerably by effec-tively using the phylogenetic information in the con-text of probabilistic matrix factorization. It gener-ates higher prediction accuracy than the species mean strategy, which is considered accurate in the ecolog-ical community. We also show that HPMF captures the correlation among the traits accurately.
 Baraloto, C. et al. Functional trait variation and sam-pling strategies in species-rich plant communities. Functional Ecology , 24(1):208 X 216, 2010.
 Bertsekas, D. P. Nonlinear Programming . Athena Sci-entific, 1999. Candes, E.J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of ACM , 58 (1):1 X 37, 2009.
 Kattge, J. et al. Try  X  a global database of plant traits. Global Change Biology , 17:2905 X 2935, 2011. Lawrence, N. and Urtasun, R. Non-linear matrix fac-torization with Gaussian processes. In ICML , 2009. Luxburg, U. A tutorial on spectral clustering. Statis-tics and Computing , 17(4), 2007.
 McGill, B. J., Enquist, B. J., Weiher, E., and Westoby,
M. Rebuilding community ecology from functional traits. Trends in Ecology &amp; Evolution , 21(4):178 X  185, 2006.
 McMahon, S. M. et al. Improving assessment and mod-elling of climate change impacts on global terrestrial biodiversity. Trends in Ecology &amp; Evolution , 26(5): 249 X 259, 2011.
 Menon, A., Chitrapura, K., Garg, S., Agarwal, D., and
Kota, N. Response prediction using collaborative filtering with hierarchies and side-information. In KDD , 2011.
 Nakagawa, S. and Freckleton, R. P. Missing inaction: the dangers of ignoring missing data. Trends in Ecol-ogy and Evolution , 23(11):592 X 596, 2008.
 Rubin, D. B. Multiple Imputation for Nonresponse in Surveys . J. Wiley &amp; Sons, 1987.
 Salakhutdinov, R. and Mnih, A. Probabilistic matrix factorization. In NIPS , 2007.
 Salakhutdinov, R. and Srebro, N. Collaborative fil-tering in a non-uniform world: Learning with the weighted trace norm. In NIPS , 2011.
 Wright, I. J. et al. The worldwide leaf economics spec-
