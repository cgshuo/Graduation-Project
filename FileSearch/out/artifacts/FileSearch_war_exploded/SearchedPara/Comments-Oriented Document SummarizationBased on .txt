 Automatic Document summarization (ADS) has been widely studied in past decades. It aims to generate a compressed summary by extracting the major in-formation from a document. Existing methods mostly focus on the document X  X  content information while ignoring readers X  opinions. With the development of web 2.0, readers are inspired to express and share their views by commenting. This kind of information coming from readers is much valuable for some special IR (Information Retrieval) tasks. On the one hand, these comments are gener-ated based on readers X  understanding of the document, which to a certain extent reflect some aspects of the document. On the other hand, comments, which have consistency with document text on the content, can be viewed as an additional part of the document. In this paper, we focus on comments-oriented document summarization (CODS), aiming to gener ate summaries for a single web doc-ument based on not only its content but also the comments generated by its readers.

The research of comments-oriented do cument summarization (CODS) is pop-ular in recent years. Its task is to summa rize a document by extracting rep-resentative sentences from the document based on the information hidden in its comments [3]. There are two common challenges for comments-oriented doc-ument summarization: the first is that a comment set might contain diverse information, which is either related or unrelated to the document content, so it is necessary to design effective methods to extract useful information from comments to help summarize document co ntent; the second is that based on the information extracted fr om comments, how to rank the document sentences to get effective summaries. After all, sentence ranking is always the issue of most concern in summarization tasks.

Some existing CODS methods firstly derived the main information from com-ments with different strategies, such as, by identifying different relations be-tween the comments to choose the most popular ones [3,4]. After this, based on the selected comments, chose top-N document sentences with different ranking strategies and generated the summaries. However, these methods ignored a key point that comments are supplement of document content but cannot always cover all aspects. Indeed, a large pro portion of the comments are not directed to the content of the document or just act on a few aspects of the document. The selected comments may be related or unrelated to the main aspect of the document. So it is not always effective to generate the summaries based on these comments. Moreover, these works didn X  X  consider that different comments might have different effects on the final ranking results.
 Inspired by these, we propose a novel m ethod called MultiAspectCoRank for CODS in this study. Firstly, in order to obtain useful information from comments and weaken the negative effect of the unrelated or one-sided ones, we try to fuse the document sentences and its comments together to get multiple aspects. Each aspect can be regarded as a part of the document. With these aspects, we can get multiple ranking lists, indicating different kinds of candidate summaries based on different angles. To overall consider the influence of different aspects on the ranking results, we use the co-feedback ranking framework, selecting top-N sentences from each ranking list and providing them as feedback to update other aspect based ranking lists in order to boost the ranking performance. This process continues iteratively until the t op-N of each ranking list are unchanged. We get the final result by integrating thes e different ranking lists according to the weights of different aspects. Compared with previous work, our proposed MultiAspectCoRank method performs better on a real-world blog dataset with manually labeled sentences.

The rest of this paper is organized as follows. We briefly review the related work in Section 2. In Section 3 we present t he details of multi-aspect co-feedback ranking method for CODS. Experiments andresultsarediscussedinSection4. Finally we draw a conclusion of this study in Section 5. Comments-oriented document summarization is developed from automatic docu-ment summarization, so the related work will be introduced in two parts. Firstly we describe some representative summa rization methods and then the special work about comments-oriented docume nt summarization is presented briefly. 2.1 Automatic Document Summarization (ADS) Traditional document summarizati on is a process to generate a summary by reducing documents in size while retaining the main characteristics of the original documents. In order to achieve this goal, different features and ranking strategies have been studied.

Feature-based sentence-ranking approaches are widely used in automatic doc-ument summarization. Radev et al. [10 ] proposed a centroid-based method, which implemented MEAD as a centroid-based summarizer by combining sev-eral predefined features including TF*IDF, cluster centroid and position to score the sentences. Lin and Hovy [5] built the NeATS multi-document summarization system using term frequency, sentence p osition and stigma words. Nenkova et al. [8] proved that high-frequency word s were significant in reflecting the focus of documents. Ouyang et al. studied the influence of different word positions in summarization [9].

Graph-based ranking algorithms nowadays are successfully applied in sum-marization. LexPageRank [1] is the repr esentative work which is based on the PageRank algorithm. Graph-based ranking algorithms take global information into consideration rather than rely only o n vertex-specific info rmation, therefore have been proved successful in document summarization. Some methods have been proposed to extend the conventiona l graph-based models recently including multi-layer graph incorporated with different relationship [12], multi-modality graph based on the manifold-ranking method [13] and DivRank [7] introducing the time-variant matrix into a reinforced random walk to balance prestige and diversity. 2.2 Comments-Oriented Document Summarization (CODS) Comments-oriented document summariza tion (CODS) is a special task devel-oped from traditional document summarization. The task is to summarize a blog post using the information hidden in its comments. Yang et al. [15] explored an approach by modeling web documents and social contexts into a unified frame-work. They proposed a dual wing factor graph (DWFG) model, utilizing the mutual reinforcement between web docum ents and their associated social con-texts to generate summaries. Hu et al. [4 ] firstly scored the importance of each comment based on three relations (namely, topic, quotation, and mention) with a graph based method and a tensor based method, and then extracted sentences from the given web document with two approaches: feature-biased approach and uniform-document approach. For the former one, words appearing in comments but not in the document do not contribute to scoring sentences, which is more tolerant to noise in comments. For the latter one, when there is no or very few comments, the problem naturally degrades to single document summarization. In this section, we propose a multi-asp ect co-feedback ranking model (Multi-AspectCoRank), which utilizes the mut ual reinforcement on the correlations based on each aspect. This MultiAspectCoRank model incorporates both the document content and its corresponding comments together to generate a high quality summary. 3.1 Multiple Aspects Extraction Comments represent the readers X  feedback about the web document. However, not all comments are directed to the conte nt of the document. In order to derive useful information from comments, we try to fuse the document sentences and its affiliated comments together and extract multiple aspects with clustering technique. In this way, most related comments will be clustered with document sentences together.

Affinity propagation is a graph based cluster algorithm [2]. Compared with traditional clustering algorithms such as K-means, singular value decomposition (SVD), graph-based approach using affinity propagation performs best in clus-tering short text data with minimal cluster error [11]. Affinity propagation takes as input a collection of real-valued similarities between data points. In order to identify the exemplars (centers of clusters) of a document, two kinds of message exchanged between data points: one is called  X  X esponsibility X   X  ( i, j ), sent from data point i to candidate exemplar point j , the other is  X  X vailability X   X  ( i, j ), sent from candidate exemplar point j to point i . To begin with, the availabilities are initialized to zero:  X  ( i, j ) = 0. Then the responsibilities are computed using the rule: where sim ( i, j ) represents the similarity between data point i and j .Whereasthe above responsibility update lets all candidate exemplars compete for ownership of a data point, the following availability update gathers evidence from data points as to whether each candidate exemplar would make a good exemplar: Given a web document D , we denote the graph of the whole sentences set as G =( S, E ss ) to represent the whole document including comments. S = { s i | 0  X  i  X  n } is the set of vertices in the graph and stands for the set of both the document sentences and its affiliated comment sentences, and E ss = { e ij | s i ,s j  X  S, i = j } corresponds to the relationship bet ween each pair of sentences. Each e ij is associated with a weight  X  ij which indicates the similarity of the pair of sentences. The weight is computed by using the standard cosine measure between two sentences as follows: where respectively. The graph we propose to build is undirected so we have  X  ij =  X  ji here and define  X  ii = 0 to avoid self transition. TF -ISF (term frequency -inverted sentence frequency) value of each term is applied to describe the elements in the sentence vector. Then the weight of each pair of sentences in the web document can be den oted as a symmetric matrix W , which is used as the input of the affinity propagation. For the affinity propagation with W andaspecifiednumberofexemplars K , the message passing procedure will be terminated after a fixed number of iterations. And then we get the result of K clusters. Each cluster is a sentence list standing for an aspect of the document. 3.2 Sentence Ranking Based on Each Aspect As we get K aspects, each aspect can be viewed as a part of the document. We need to estimate the probability score p ( s | A ) of a sentence s being selected as a summary sentence given an aspect A . Instead of simply comparing the relevance between the sentence s and the aspect A , we take this as a task based on the whole background, where both the similarities between the sentence s and other sentences in the document and the similarity between s and the aspect A are considered. Formally, p ( s | A ) is computed by the following formula: where A is a set of sentences standing for an aspect, and S D is the set of all sentences in the document, d is a trade-off parameter in the interval [0 , 1], which is used to specify the relative contribution of the two parts in Eq.(4). For bigger value of d , more importance is given to the similarities between the sentence s and other sentences in the document D than that between s and the aspect A . The denominators in both parts are used for normalization. The formula in Eq.(4) can be written as: where M , C and R are all square matrices. Elements in C represent the similar-ities between sentences in the document. Elements in R represent the relevance between each sentence in the document and each aspect. k represents the k th it-are looking for, which corresponds to the stationary distribution of the matrix M . The iteration is guaranteed to converge to a unique stationary distribution given that M is a stochastic matrix.

To calculate the similarities between the sentences in the document, we use the Cosine similarity. To calculate the relevance between s and an aspect A ,we use the average Cosine similarity as follows: where a represents a sentence belongs to the aspect A , count ( A ) stands for the number of sentences in aspect A .

In order to initialize each p ( s | A ), we can regard each aspect as a short docu-ment and construct a language model to estimate it using Dirichlet prior smooth-ing as follows: where | A | is the length of the sentence aspect A , c ( w,A ) is the count of word w in A , u s is the smoothing parameter, p ( w | B ) is the background model used as smoothing factor. Generally p ( w | B ) is estimated by the whole document D , using following formula: After estimating the score of each sentence in the document, we can get K lists in the document, no comments included, K is the number of aspects. 3.3 Co-feedback Ranking Based on Multiple Aspects Each ranking list indicates a kind of candidate summaries from one angle, but cannot determine the final ranking results. In this subsection, we are aiming to rank all the document sentences based on the K groups of ranking lists and selecting the top-N ranked ones to generate summaries. Each aspect can be regarded as a part of the document. The summary we are aiming to generate should base on the whole document. So how to combine the ranking scores based on each aspect to get an overall score as the ranking criterion is the key problem to solve. Traditional strategy is to combine different ranking lists into a unified one with a linear function. However it fails to make full use of the information provided by the different ranking lists an d neglects the interactions among them before combination. Also we believe that each aspect based ranking list is able to provide valuable information to help promote other aspects based ranking performance with the mutual ranking refinement, which, in turn, may lead to an overall improvement in ranking.

Co-feedback ranking framework [14] is a good mutual reinforcement method to combine multiple ranking results into a unified one. In this paper, we select the top-N ranked sentences from each ranking list as feedback to update other aspect based ranking results according to the relevance of sentences. So we update the ranking result of s j in ranking list list i as follows: where  X  is a balance parameter which can be viewed as the proportion of the dependence of the new ranking results on its initial ranking results.  X  ij is the feedback value from other ranking lists to s j of ranking list list i . t represents the t th iteration.  X  t ij is computed as follows: where  X  t ikj represents the feedback value on sentence s j of ranking list list i from ranking list list k .  X  k represents the number of sentences in aspect k .  X  N k represents the top N sentences selected from ranking list list k . sim ( s j , X  N k )is defined as the similarity between s j and the top N sentences from ranking list list k .Inthispaper N is set to 10 as 10 sentences are basically sufficient for the summarization task we work on. This p rocess continues iteratively when the top N results of each ranking list don X  X  change. We get the final ranking results based on these ranking lists as follows: This co-ranking method is designed based on the principle that the new ranking result of the sentence s from one aspect consists of two parts: one is the initial ranking result based on the aspect, the other part is the similarities between s and the top N feedback sentences provided by other ranking lists. The top N ranked sentences based on one aspect are presumed to be highly supported by that aspect, which are more likely to be chosen as summary sentences. So these top N sentences are selected as feedback to pr omote the weight of these sentences in other ranking lists. Through this co-feedback and mutual reinforcement, these ranking lists are considered to be more reliable ranking results.
After we get the final ranking lists, we apply the variant version of MMR algorithm proposed in [12]. This method is to penalize the sentences that highly overlap with the sentences that have been chosen as the summary. In this way, we can choose more informative but less redundant sentences as the final summary. 4.1 Dataset To evaluate our method for comments-or iented document summarization, we conduct our experiments on the dataset used in [4]. This dataset was collected from two famous blogs, i.e., Cosmic Variance 1 and IEBlog 2 , both receiving large number of comments. To guarantee the validity of the data, we randomly picked up 50 posts from each blog, and constructed our experiment dataset with these 100 articles. Each article consists of two parts, document content and its affiliated comments. Table 1 gives the brief description of the 100 posts. To generate reference summaries, each post has 7 l abeled sentences by 4 human summarizers. 4.2 Performance Metric We use the ROUGE 3 [6] (Recall Oriented Understudy for Gisting Evaluation) toolkit to evaluate our proposed method, which has been widely applied for summarization evaluation. It evaluates the quality of a summary by counting the overlapping units between the candidate summary and reference summaries. There are many kinds of ROUGE metrics to measure the system-generated summarization such as ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-U, of which the most important one is ROUGE-N with 3 sub-metrics: precision, recall, and F-score.
 where RS represents the reference summaries. N-gram  X  RS in the metrics denotes the N-grams in reference summary. Count match ( N  X  gram )isthemaximum number of N-grams co-occurring in the candidate summary and in the set of reference summaries. Count ( N  X  gram ) is the number of N-grams in the reference summaries.

The ROUGE toolkit can report separate scores for 1, 2, 3, and 4-gram. In the experimental results we report fo ur ROUGE F-measure scores: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), ROUGE-4 (extension of ROUGE-S, which is the skip-bigram co-occurrences statistics), and ROUGE-W (based on the weighted longest common subsequence) metrics. The higher the ROUGE scores, the similar the two summaries are.
 4.3 Experimental Results Given a web document and its corresponding comments, we first decompose them into sentences and then remove the stop-words and words stemming are performed. After these steps, we implement the following algorithms which are widely used in comments-oriented document summarization. We conduct the same preprocessing for all algorithms for fairness.  X  Random (system1): The method selects s entences randomly for the sentence set including comments.  X  LexPageRank [1] (system2): This system applies Cosine similarity in stan-dard LexPageRank method based on the document content sentence set (no comments is given) to summarize the document.  X  LexPageRank  X  X  X  (system3): This system applies Cosine similarity in standard LexPageRank method based on the whole document sentence set (including comments) to generate the summary.  X  MultiAspectCoRank (system4): As we have showed, this method extracts multiple aspects with affinity propagation firstly, and then utilizes mutual re-inforcement co-feedback ranking frame work based on these aspects to promote the ranking performance.
 Table 2 and Fig 1 show the performance of these systems on a same dataset. The parameters of MultiAspectCoRank approach are set as follows: the number of aspects K = 5, the balance parameter u s =0 . 1, d =0 . 6and  X  =0 . 4. From Table 2 and Fig 1 we have following observations:  X  Generally, the Random gets the worst performance;  X  From the results of system 2 and system 3, both of these two systems use LexPageRank, but the results of system 2 in which no comments is given is better than those of system 3 which comb ines document contents and comments together. This is mainly because most o f the comments are not directed to the content of the post and comments are not equally useful for identifying important document sentences.  X  From Fig 1, we can see that on the same dataset, our system (system 4) outperforms other systems on different e valuation measures, which indicates the effectiveness of the proposed MultiAspectCoRank model.
 All the comparison results suggest that it X  X  effective to derive useful informa-tion from comments by fusing the document sentences and its corresponding comments together and detecting multiple aspects from them. And it X  X  better to promote the performance when utilizing mutual reinforcement co-feedback ranking method on the same data based on multiple aspects. 4.4 Parameter Tuning There are mainly four parameters in our proposed method: number of clusters K , balance parameter u s , d and  X  . In this section, we are trying to evaluate the influence of these four parameters on the experimental results respectively. First we compare the four kinds of ROUGE m easure results with aspect number K ranging from 3 to 11 in Fig 2. It is observed that when the aspect number K is 7 the performance is best. In the same way, the four kinds of ROUGE measure results with balance parameter u s , d and  X  are shown in Fig 2. It is obvious that the value of parameter u s has less effect on the performance of our proposed method, and when the balance parameter d is set to 0.7 and  X  is set to 0.3 this method performs best. In this work, we propose a novel model n amed MultiAspectCoRank for comments-oriented document summarization task. M ore specifically, by fusing the docu-ment content and its comments together , we extract multiple aspects from the given web document, and then utilize mutual reinforcement co-feedback ranking method to promote the performance. Experimental results on a set of real-world blog data prove the effectiveness of our proposed method in improving the sum-mary quality. In future work, we will try to find more and deeper relationships between document contents and comments.
 Acknowledgments. We thank the anonymous reviewers for their valuable and constructive comments. This work is financially supported by NSFC Grant 61272340.
