 right-hand noun. However, a significant portion  X  about 25% (Liberman and Sproat, 1992)  X  of them are assigned rightmost prominence (such as cherry pie, Madison Avenue, silk tie, computer program-mer, and self reliance from the list above). What factors influence speakers X  decision to assign left or right prominence is still an open question.
There are several different theories about rela-tive prominence assignment in noun-noun (hence-forth, NN) compounds, such as the structural the-ory (Bloomfield, 1933; Marchand, 1969; Heinz, 2004), the analogical theory (Schmerling, 1971; Olsen, 2000), the semantic theory (Fudge, 1984; Liberman and Sproat, 1992) and the informativeness theory (Bolinger, 1972; Ladd, 1984). 1 However, in most studies, the different theories are examined and applied in isolation, thus making it difficult to com-pare them directly. It would be informative and il-luminating to apply these theories to the same task and the same dataset.

For this paper, we focus on two particular the-ories, the informativeness theory and the seman-tic composition theory. The informativeness theory posits that the relatively more informative and un-expected noun is given greater prominence in the NN compound than the less informative and more predictable noun. The semantic composition theory posits that relative prominence assignment in NN compounds is decided according to the semantic re-lationship between the two nouns.

We apply these two theories to the task of pre-dicting relative prominence in NN compounds via statistical corpus-driven methods, within the larger context of building a system that can predict appro-priate prominence patterns for text-to-speech syn-thesis. Here we are only focusing on predicting rela-tive prominence of NN compounds in a neutral con-text, where there are no pragmatic reasons (such as contrastiveness or given/new distinction) for shifting prominence. All except the first the aforementioned five infor-mativeness measures are relative measures. Of these, PMI and Dice Coefficient are symmetric mea-sures while Bigram Predictability and PKL are non-symmetric (unidirectional) measures. We modeled the semantic relationship between the two nouns in the NN compound as follows. For each of the two nouns in each NN compound, we maintain a semantic category vector of 26 elements. The 26 elements are associated with 26 semantic categories (such as food, event, act, location, arti-fact, etc.) assigned to nouns in WordNet (Fellbaum, 1998). For each noun, each element of the semantic category vector is assigned a value of 1, if the lem-matized noun (i.e., the associated uninflected dic-tionary entry) is assigned the associated semantic category by WordNet, otherwise, the element is as-signed a value of 0. (If a semantic category vector is entirely populated by zeros, then that noun has not been assigned any semantic category information by WordNet.) We expected the cross-product of the se-mantic category vectors of the two nouns in the NN compound to roughly encode the possible semantic relationships between the two nouns, which  X  fol-lowing the semantic composition theory  X  corre-lates with prominence assignment to some extent. For each noun in each NN compound, we also maintain three semantic informativeness features: (1) Number of possible synsets associated with the noun. A synset is a set of words that have the same sense or meaning. (2) Left positional family size and (3) Right positional family size. Positional family size is the number of unique NN compounds that in-clude the particular noun, either on the left or on the right (Bell and Plag, 2010). These features are ex-tracted from WordNet as well.

The intuition behind extracting synset counts and positional family size was, once again, to measure the relative informativeness of the nouns in NN com-pounds. Smaller synset counts indicate more spe-cific meaning of the noun, and thus perhaps more information content. Larger right (or left) posi-tional family size indicates that the noun is present the majority class to all test cases. We avoided over-fitting by using 5-fold cross validation. 5.1 Results The results of the evaluation of the different models are presented in Table 1. In this table, INF denotes informativeness features (Sec. 2), SRF denotes se-mantic relationship modeling features (Sec. 3) and SIF denotes semantic informativeness features (Sec. 4). We also present the results of building prediction models by combining different features sets.
These results show that each of the prediction models reduces the baseline error, thus indicating that the different types of feature sets are each cor-related with prominence assignment in NN com-pounds to some extent. However, it appears that some feature sets are more predictive. Of the indi-vidual feature sets, SRF and INF features appear to be more predictive than the SIF features. Combined together, the three feature sets are most predictive, reducing model error over the baseline error by al-most 33% (compared to 16-22% for individual fea-ture sets), though combining INF with SRF features almost achieves the same reduction in baseline error.
Note that none of the three types of feature sets that we have defined contain any direct lexical infor-mation such as the nouns themselves or their lem-mata. However, considering that the lexical con-tent of the words is a rich source of information that could have substantial predictive power, we included the lemmata associated with the nouns in the NN compounds as additional features to each feature set and rebuilt the prediction models. An evaluation of these lexically-enhanced models is shown in Table 2. Indeed, addition of the lemmatized form of the NN compounds substantially increases the predic-tive power of all the models. The baseline error is reduced by almost 50% in each of the models  X  the error reduction being the greatest (53%) for the model built by combining all three feature sets. Several other studies have examined the main idea of relative prominence assignment using one or more of the theories that we have focused on in this paper (though the particular tasks and terminology used were different) and found similar results. For exam-ple, Pan and Hirschberg (2000) have used some of the same informativeness measures (denoted by INF above) to predict pitch accent placement in word bi-In his work, Sproat reported a baseline error of 30% and a model error of 16%. The reported relative im-provement over the baseline error in Sproat X  X  study was 46.6%, while our relative improvement using the lexically enhanced SRF based model was 49.5%, and the relative improvement using the combined model is 52.95%.

Type-based semantic informativeness features of the kind that we grouped as SIF were analyzed in Bell and Plag (2010) as potential predictors of prominence assignment in compound nouns. Like us, they too found such features to be predictive of prominence assignment and that combining them with features that model the semantic relationship in the NN compound makes them more predictive. The goal of the presented work was predicting rel-ative prominence in NN compounds via statistical corpus-driven methods. We constructed automatic prediction models using feature sets based on two different theories about relative prominence assign-ment in NN compounds: the informativeness theory and the semantic composition theory. In doing so, we were able to compare the two theories.

Our evaluation indicates that each of these theo-ries is relevant, though perhaps to different degrees. This is supported by the observation that the com-bined model (in Table 1) is substantially more pre-dictive than any of the individual models. This indi-cates that the different feature sets capture different correlations, and that perhaps each of the theories (on which the feature sets are based) account for dif-ferent types of variability in prominence assignment.
Our results also highlight the difference between being able to use lexical information in prominence prediction of NN compounds, or not. Using lexical features, we can improve prediction over the default case (i.e., assigning prominence to the left noun in all cases) by over 50%. But if the given input is an out-of-vocabulary NN compound, our non-lexically enhanced best model can still improve prediction over the default by about 33%.
 Acknowledgment We would like to thank Richard Sproat for freely providing the dataset on which the developed models were trained and tested. We would also like to thank him for his advice on this topic.
