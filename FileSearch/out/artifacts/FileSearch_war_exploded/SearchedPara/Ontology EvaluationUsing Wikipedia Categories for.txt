 Ontology evaluation is a maturing discipline with methodologies and measures being developed and proposed. However, evaluation methods that have been proposed have not been applied to spe-cific examples. In this paper, we present the state-of-the-art in on-tology evaluation -current methodologies, criteria and measures, analyse appropriate evaluations that are important to our applica-tion -browsing in Wikipedia, and apply these evaluations in the context of ontologies with varied properties. Specifically, we seek to evaluate ontologies based on categories found in Wikipedia. H.3.4 [ Systems and Software ]: Performance evaluation (efficiency and effectiveness) Experimentation, Measurement, Performance Ontology evaluation, Browsing, User Studies, Wikipedia
Ontology evaluation techniques are improving as more measures and methodologies are proposed. However, few specific examples of these evaluations have been found in literature. That is, spe-cific examples of ontologies, applications and their requirements, measures and methodologies to link these together in one cohesive evaluation. This could partly be due to the lack of good ontologies made available publicly.

An ontology is an explicit model of a domain of knowl-edge consisting of a set of concepts, their definitions and inter-relationships [17]. Parties commit to an ontology and agree upon its definitions and assertions. An agreement with an ontology is called an ontological commitment [8]. McGuiness describes the spec-trum of ontology specifications from simple ontologies to struc-tured [14]. Simple ontologies possess at the least a finite con-Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. trolled vocabulary, unambiguous interpretation of classes and term relationships, and strict hierarchical subclass relationships between classes, for example, the Yahoo categories. Structured ontologies take the simple ontology further and include more specific forms of expressivity and constraints on concepts and relations as well as axioms and equivalence mappings. The difficulty with some simple ontologies is that they tend to be loosely defined, small and often not agreed upon.

Recently, Wikipedia has become a medium for allowing users to contribute to articles on numerous subject areas over the WWW. Each article in Wikipedia can be accessed using its category struc-ture. The Wikipedia category structure is equivalent to a simple ontology according to our definition above. However, in contrast to other simple ontologies it is a real application used by many users and one that is constantly refined by Wikipedia editors.

In this work, we consider domains in Wikipedia category struc-ture as ontologies and seek to perform ontology evaluation mea-sures proposed in the literature on them. Specifically, we take a task-based approach in our evaluation in the context of browsing articles using the category structure. Section 2 introduces Wikipe-dia and its category structure, and highlights its needs and require-ments in the context of browsing. Section 3 briefly discusses what browsing is. In Section 4, we look at existing ontology evaluation approaches, criteria and measur es proposed in literature. After dis-cussing the requirements of the Wikipedia categories in the context of browsing and existing evaluation techniques in literature, we dis-cuss and analyse which evaluation measures apply. Section 6 then presents our task-based evaluation involving users and we report on our findings from that user study. Lastly, Section 7 concludes this work and discusses some future work to pursue.
Wikipedia is a multi-lingual onlin e encyclopedia written from volunteer contributions around the world. Since 2001, it has grown into a large pool of information with topics ranging from art, tech-nology to pop. In the English langua ge, it has over 1.7 million ar-ticles. Whilst anyone can add their contributions, they are subject to guidelines set by editors to ensure neutrality and that informa-tion is verifiable. The nature of Wikipedia lends itself for users to find information on a wide range of topics. It is also an ongoing and evolving application where information is continuously being updated, edited and discussed.

Apart from the article text, Wikipedia articles have various meta-data attached to them. Within an article, it may contain hyperlinks to other related articles, external web pages, as well as one or more related categories. These categories are organised and structured to allow users to browse their way around to find related information.
Wikipedia X  X  category structure may be seen as an information hi-erarchy. It is by no means a strict and logically grounded ontology as it has many inconsistencies and is loose in its definition of rela-tionships. However, it can be seen as a simple ontology rather than a formal one, as it has an explicit, shared and agreed upon concep-tualisation. In this manner, it can be seen as one of the largest public ontologies available on the web having a large coverage of informa-tion, utilised by many users and is constantly evolving. However, it is not without any guidelines or requirements for its specification, which we discuss next.
The editors of Wikipedia have established guidelines for which categories are to be created (refer to online guidelines 1 kipedia category structure is not complete nor is it a perfect one. However, these categories allow users to navigate around to find related information. Hence we adopt the Wikipedia category struc-ture as our dataset and derive task domain for our ontology evalu-ations from it. We elaborate on the requirements of the Wikipedia categories drawn from the editors guidelines below.

Specifically, the requirements are: 1. Allowing intersecting category structure.
 2. Group similar articles.
 3. Categories should have the  X  X ight X  number of subcate-4. Avoiding cycles in the category structure.

The type of information gathering activity, in using and navigat-ing the Wikipedia category structure, fits with the characteristics of browsing more than it does with search. We will consider the distinction between these in the next section.
Browsing is affected by the user X  X  knowledge of the domain and the specificity of the browse task. It is characterised by move-ment . Thompson and Croft [16] describe browsing as an  X  X nformal or heuristic search through a well connected collection of records in order to find information relevant to one X  X  need X . In a browsing activity, users evaluate the information that is currently displayed, its value to the information need, and what further action to take. Thus, it is an informal search through a connected collection of documents.

A browsing activity is distinguished from a search activity. Both have goals in mind, however, Baeza-Yates and Ribeiro-Neto [1] http://en.wikipedia.org/wiki/Wikipedia:Category differentiate search from browse by the clarity of user goals. In search , users enter keywords into a system that is related to their information need. They are then presented with a list of results the system returns as related and users can decide to select one of the results or refine their search query. In comparison to this, browsing displays a different type of behaviour. There may not be a specific query as such associated. However, answers to user goals and in-formation needs can be readily recognised in a browsing activity. Thus, it is the clarity and mode of accessing this information that differs in browse .

There are a different kinds of browsing. Marchionini [13] dis-cusses these types of browsing from a directed browse to an undi-rected browse. A directed or intentional browsing behaviour is usu-ally associated with tasks that are closed or specific . These refer to a task where there is usually not more than a few answers to the information need. On the other hand, an undirected browse is ex-ploratory in nature and its browsing behaviour is associated with tasks that are more open or broad . These refer to a task where there may be many answers to the information need.
Having introduced the Wikipedia categories as a simple ontol-ogy and its requirements, we now look at some methods of on-tology evaluation. In this section, we will introduce the 3 main approaches to ontology evaluation, criteria for ontology evaluation and proposed measures from litera ture. We will then seek to match these methods of ontology evaluation to the requirements of the Wikipedia category structure in the next section.
 With the increase of ontologies being made available via the WWW, evaluating which ontology is suitable becomes a problem. It is difficult to discern whether one ontology is better than another. If one is picked, it will lack definitions, axioms or relations required in a domain or application. If none is found to be suitable, an on-tology may need to be built from scratch. However, the process in which ontologies are specified can be ad hoc at times. Whether an ontology is to be selected from a set of candidate ontologies or an ontology is to be constructed, methods for evaluating its suitabil-ity and applicability are needed. However, what are the means for evaluating an ontology? In this section, we will discuss the main approaches in ontology evaluation, some criteria for ontologies and some measures that have been proposed in literature. There are 3 main approaches to ontology evaluation: Gold standard evaluation This approach compares an ontology Criteria based evaluation This approach takes the ontology and Task-based evaluation This approach evaluates an ontology based Also, ontologies can be measured in various ways. What is mea-sured may not necessarily tell us much nor evaluate the ontology in a very meaningful manner either.
Various criteria have been proposed for the evaluation of ontolo-gies as listed in Table 1. These criteria can be used to evaluate the design of an ontology and in aiding requirements analysis.
Some of these criteria can be successfully determined using on-tology tools. Reasoners, such as FaCT and RACER, provide the means to check for errors in ontologies, such as redundant terms, inconsistencies between definitions and missing definitions. Ad-ditionally, Dong et al. [3] have used existing software engineering tools and techniques to check for errors in ontologies in the military domain.

Some criteria, such as clarity and expandab ility , can be difficult to evaluate as there are no means in place to determine them. More-over, while the completeness of an ontology can be demonstrated, it cannot be proven.

Other criteria can be more challenging to evaluate as they may not be easily quantifiable. They require manual inspection of the ontology. For example, correctness requires a domain expert or ontology engineer to manually verify that the definitions are correct with reference to the real world. This may not always be feasible for a large ontology or even a repository of many ontologies.
Upon analysis, some of the criteria proposed by the different re-searchers address similar aspects when evaluating ontologies and do overlap. We have previously described existing criteria pro-posed in literature and summarised these as 8 distinct criteria [18]. These criteria are:
Before we can apply criteria to the requirements for the Wikipe-dia categories, we need to discuss evaluation measures. Evaluation measures may help us to determine which criteria is applicable to the requirements.
Ontology evaluation measures are a quantitative means in as-sessing various aspects of an ontology. G  X  omez-P  X  erez [6] outlines a list of measures looking at possible errors that could manifest with regards to ontology consistency, completeness and concise-ness. Brewster et al. [2] propose measures for analysing whether an ontology has the right  X  X it X  over a given domain by applying cov-erage measures like precision and r ecall over a corpus representing the domain. Gangemi et al. [4] present a suite of measures focusing on structure, function and usability of an ontology. Tartir et al. [15] propose measures to evaluate an ontology X  X  capacity or  X  X otential for knowledge representation X . The latter two focus on the struc-tural aspects of an ontology. In our evaluations, we will be con-sidering measures presented by Tartir et al. [15] and the structural measures from Gangemi et al. [4] as a means for analysing the Wi-kipedia requirements. Below we summarise and collate these into Table 2. Also, some of these measures are equivalent. These are presented in Table 3.
 Tartir et. al [15] Schema Relationship richness Attribute richness Inheritance richness Knowledgebase-Instance Class richness Avg. Population Cohesion Knowledgebase-Class Importance Fullness Inheritance Richness (c) Relationship Richness (c) Connectivity Readability
Despite advances in this area, there may not be a complete set of measures for all aspects of an ontology. As in the case of evaluation criteria, there may be parts of an ontology which are simply not measurable. For example, we cannot prove whether an ontology is complete [6]. There may also be other aspects that are difficult to measure in an ontology. For example, how do we determine adequate expandab ility ? Having ontology measures does not mean that it is significant or important to us.

Measures that are feasible but done in isolation are not as mean-ingful compared with measures put into the context of indicators and benchmarks from application requirements or needs. An ex-ample is comparing various ontologies for adequate coverage in a domain or performance measures in an application deployment. The coverage or performance measures taken in the context of the application give meaning to the measures taken.

We will elaborate on the less intuitive proposed measures. Tan-gledness and fanout measures are related to how each category ex-pand up with its parents and downward with its children. Measures looking at relationship richness , attribute richness , class richness , average population look at the quality of the overall ontology or knowledge base (if we include instances of classes). Connectivity and cohesion look at relations in the ontology. The definitions are drawn existing work [4, 15], which we will consider in our analysis are shown in Figure 1.
 Fanout factor (leaf to nodes) = No . Lea f Classes No . Classes Tangledness Connectivity = No. Instances of other classes connected to in-Cohesion = No. Separate Connected Components
Our initial intention was to map requirements to criteria and cri-teria to the relevant measures. However, mapping criteria to the measures proved difficult. Some measures do not resolve to any criteria, for example, connectivity and importance . Some measures cover a range of criteria but not completely, for example, depth and breadth . These two measures related to expandab ility but the rela-tion is limited. Some criteria are difficult to quantify, for example, correctness , completeness , logical adequacy and minimal ontolog-ical commitment . Furthermore, some measures are relevant to the requirements but do not resolve to any criteria, for example, tan-gledness .

In light of this, we perform an analysis to obtain measures which directly addresses requirements from our application -Wikipedia. We would still find some measures which will not directly nor fully address each requirement but at the very least, these measures are quantifiable.
For brevity, we will be presenting measures which would par-tially or directly address the requirements.
These address requirement 3, having the  X  X ight X  number of sub-categories. There may be cases where extremes in these measures will indicate that it may not be  X  X ight X , for example, high breadth or fanout. This may indicate that there are too many subcategories for a given category. In another example, high depth and low breadth may indicate too much categorisation happening or an incomplete set of subcategories at each level.

Tangledness measures something of the distribution of multiple parent categories. This measure may help us understand how inter-sected the category structure is.

Degree distribution and density are related measures and ascer-tain the probability a vertex has a certain  X  X egree X . That is, the sum of parent categories and child categories. It may help indicate an ineffective subcategory structure if it is too dense or not dense enough. This may address the requirement of  X  X ight X  number of subcategories.

This measure gives the number of  X  X slands X  or disjoint sets of categories. This could indicate that a more cohesive organisation of the categories is required. However it is unlikely to encounter disjoint category sets in a subtree of Wikipedia. All categories lead to the root category.
 Importance measures the distribution of instances in a subtree. The assumption is that if a class subtree has more instances, it in-dicates that it is more important than another subtree that does not have as many instances.
 Connectivity indicates which nodes are highly connected.

An emerging pattern could be that the more highly connected categories will be general ones  X  like history, arts, information. Perhaps in combination, importance and connectivity could help suggest relevant categories that may allow a more intersecting struc-ture.

Class richness measures the number of classes that are utilised by looking at the instances that have been attributed to them. This may highlight those classes that do not have articles. Although in Wikipedia it is unlikely for a category to be without an associated article (if we took articles to be instances of a category).
Measuring circularity errors would address requirement 4 in avoid-ing cycles within the category structure. However, we can avoid cycles by simply removing them. Hence, this measure is of little impact in our evaluations.
 Table 4 shows a summary of measures that satisfy the requirements outlined in our application domain.
Relationship richness and attribute richness measures are not considered because, with regards to Wikipedia, there are no other relationships between categories besides: a) parent and child cate-gories; b) attributed articles to categories; and c) links from within article text to categories.

The readability measure is also not considered because the cat-egory structure does not have annotations regarding the design of the structure itself.
Of the measures indicated, tangledness addresses requirement 1 directly. Regarding tangledness , a highly tangled ontology would not be desirable for structured ontologies. However, in the context of Wikipedia, it is deemed beneficial and a requirement as it allows for greater intersectedness of the domain.

Depth , breadth ,and fanout measures partially addresses require-ments 2 and 3. These are easily measured. Depth and breadth mea-sures are related. For ontologies with a similar number of classes, we would expect one that is very broad would be less deep. Con-versely, a deep ontology would not be as broad. Breadth and fanout measures are also related. Changing the fanout factor would in-crease the breadth of an ontology. Thus, we propose to measure these as well.
For measures of connectivity , importance and density we find them to partially address requirements 1 and 3 respectively. How-ever, it is not considered in our evaluations here as we do not expect these to have high impact on the requirements. Furthermore, these measures would also be difficult to vary in a systematic way. For example, how do we allow additional relations between classes in a meaningful way? Also, in the context of browsing in Wikipedia, allowing more relations to vary measures like density may hinder the browsability of the category structure.

In summary, we have identified tangledness , depth , breadth and fanout as measures to consider in our analysis looking at address-ing requirements in browsing Wikipedia articles using its category structure.
In carrying out a task-based approach to ontology evaluation, we propose to model the task on the browsing of an information space using a given category structure  X  much in the same way users would do when browsing categories from Wikipedia. In this sec-tion, we describe the dataset used and ontologies taken from Wiki-pedia X  X  category hierarchy, the experimental design for the evalua-tions and present outcomes from a user study we undertook.
For this user study, we considered categories from the English-language version of Wikipedia and its associated articles. The arti-cles were taken directly from a database dump of the articles from Wikipedia 2 . Regarding the Wikipedia category structure, we ob-http://download.wikimedia.org/enwiki tained this from System One X  X  RDF representation of it 3 .Thiswas from a Wikipedia database dump dated March 2006. In it, each article and category is represented as an RDF triple with category and inter-article relations. The relations represented in the Wiki-pedia categories are: category-subcategory , category-article and article-article relations .

Upon analysis, we found that for a given category, no restrictions are put on the number of parent and sub categories. That is there may be multiple parent and child categories. Also, there are no restrictions as to the number of categories to associate an article with (as long as it is related).

However, there are some limitations with regards to the Wiki-pedia categories. Some categories are administrative in nature, for example,  X  X porting stubs X  . These are categories which contain ar-ticles that have yet to have information written for it but have been linked from another article previously. Also, not all articles have categories associated with it. This means that some articles are not viewable from navigating the category structure. Despite this, the Wikipedia categories are overall a content-rich organisation, which we will use as the basis for our task-based evaluations.
 We applied measures to the Wikipedia categories, discussed in Section 3, which were feasible to apply. In processing the cate-gories, we traversed the subtree in breadth first search fashion start-ing from the category  X  X ategories X  , which we take to be the root of the content section, and present these measures in Table 5.
From Table 5 we observe that the Wikipedia categories have a ratio of about 1:10 with regards to the number of categories and its associated articles. Also, we find that the category structure is not deep considering the number of articles and categories with the number of levels as 14. Instead, we find it to be quite broad with an average breadth of 8559.5 in a given level. The overall Wikipedia category structure is also quite tangled with 69% of the all Wikipe-dia categories having multiple parents  X  that is, categories which have more than one parent.
The goal of our task was to examine properties of the category structure through browsability . That is, being able to locate infor-mation using the category structure for articles for a given infor-mation need by browsing. Users should also be able to explore the http://labs.systemone.at/wikipedia3 domain space in an intuitive manner with reasonable information organisations.
For this user study, we needed to vary the original subtree in a manner that was: semantically reasonable, utilised all the cate-gories in the subtree and comparable to the original subtree. There were two options presented to us  X  either vary the original Wiki-pedia subtree or generate a subtree category structure according to an automated technique  X  which was a variation on a document clustering technique.

In exploring both options, we found that varying the original Wi-kipedia structure to remove tangledness was reasonable. Removing tangledness meant removing occurrences of multiple parents in a given category. The specific algorithm we used was Dijkstra X  X  al-gorithm for finding a single-source shortest path tree. This is the most appropriate shortest path algorithm since we know the root of the subtree. Where there were more than one parent candidate cate-gories we chose the one that was most similar to the category being considered. We performed a TF-IDF cosine similarity measure on article titles within categories of a given subtree. We found this worked well and kept the subtree mostly semantically equivalent.
We looked at varying the original Wikipedia subtree in a reason-able manner for reducing or increasing the number of subcategories to consider breadth and fanout factors. However, we could not find a feasible way of systematically varying the number of subcate-gories. Specifically, the difficulty faced was in increasing the num-ber of subcategories in a sensible manner. Thus, we considered the second option from above  X  using a form of document clustering.
For a given subtree of the Wikipedia category hierarchy, we re-moved all category relations from it and applied a document clus-tering technique over the categories contained in the base subtree. We used partition-based criterion-driven document clustering on features gathered from a combination of the category title and as-sociated article information [19] provided in the Cluto clustering toolkit 4 . Algorithm 1 describes the pseudocode for varying a given category subtree.
 Algorithm 1 Varying a subtree Let N : = maximum number of elements in cluster
Add root of subtree to queue q repeat until queue has no more clusters to process
We used the category title and clustered on a few varying data parameters: category title only, category title and the associated article titles, and category title and the associated article text. http://glaros.dtc.umn. edu/gkhome/vi ews/cluto
We also varied the clustering technique based on the number of features considered and also the resulting number of clusters on each clustering event. We used the cosine similarity function for this.

Using the two methods discussed above, we can obtain 4 varied subtrees for a given domain. Table 6 presents the original and var-ied subtrees we considered. We use Subtrees a and b to look at the effects on tangledness and in future work, Subtrees c , d and e to consider depth, breadth and fanout.

We now outline the tasks and domains we used in our user stud-ies. In each experiment, participants were given a set of tasks to complete within a 10 minute duration (inclusive of pre and post task questions). The given tasks were domain specific, and hence would not be reasonable in another domain. We chose to use domains that were as separate from each other as possible so as to reduce the learning effect from completing tasks on a given domain. Also, we chose 3 levels of specificity regarding the nature of the tasks (see Table 7). We proposed Tasks 1 to 3 and Tasks 4 to 6 to have increas-ing levels of specificity, from broad to specific, in their respective domains X and Y. For example, International racing competitions (Task 1) covered a broad range of possible answers within the Rac-ing Sport domain (X). Whereas Makers of F1 racing cars (Task 3) was a very specific task type in the same domain.
 Domain Task Description Racing Sport (X) T1: International racing competitions Foods (Y) T4: Non-alcoholic beverages Table 7: Experiment design for comparing Wikipedia with gen-erated structure
The tables below outline the experimental design we used to compare various aspects of the generated subtrees. We propose two experiments to obtain results for different comparisons. In each ex-periment, we used the Latin squares method of determining the order participants use the subtrees to be compared. We did this to remove the learning factor of users progressing from one subtree to another in a given domain and to increase statistical significance in our studies. Using this configuration we guarantee each user to have a unique sequence and for users to use each subtree in a dif-ferent position. We also applied blocking on the domain. Lastly, we rotated the domain after 9 users.

First, in Table 8, the original Wikipedia subtree a is compared with two other variations: 1) the same subtree altered to remove multiple parents b , hence being untangled ; and 2) a generated sub-tree c with similar properties using the document clustering tech-nique.

We propose a second experiment, with a similar setup as the pre-vious experiment but instead using the generated untangled Sub-tree c as our base subtree and compare it with generated subtrees that had varying number of subcategories  X  with Subtrees d and e having more subcategories and fewer subcategories respectively. With the generated subtrees, it is possible to adjust its breadth and fanout by altering the number of subclusters. This was to inves-tigate the effect of depth, breadth and fanout. However, this de-pends on Subtrees b and c being approximately equivalent in per-formance. We needed to establish this in the first experiment de-scribed above.
After varying each subtree for the two domains, we took mea-surements on these to analyse the changes and present them in Ta-bles 9 and 10. For the Racing sports domain (X), we had 1185 categories. For the Foods domain (Y), we had around 652 cate-gories in total. These were ideal sizes for the time given to each user to browse through in that they were sufficiently large such that users would probably not look at all categories. They were also more general topic areas. These were typically between 15 and 20 articles in a category.

We observed that for each domain, Subtrees b and c do not have any multiple parents nor are they tangled. Untangled subtrees also reduce the number of parents in total in comparison with the Wiki-pedia original subtree ( a ). In effect, untangling a subtree removes links from the subtree.
The generated subtree ( c ) had fewer levels as they were gener-ally broader than the others. This is also reflected in the average breadth. The effect of this is presenting the user with about twice as many narrower category links compared with the other subtrees.
We had several versions of generated subtrees with varying pa-rameters. The best subtree was chosen on the basis of the number of category-subcategory relations in the generated subtree ( c )that appeared in the original Wikipedia subtree ( a ). This tended to yield broad subtrees using our method for generating subtrees.

We also observed that broader subtrees had an effect in reducing subtree depth.
To evaluate the performance of users with regards to browsing and marking relevant articles for a given task, we propose browsing efficiency and effectiveness measures.

For efficiency, we looked at the number of backtracking a user does. Included are the number of clicks a user makes to: 1. go back to the previous category 2. go back to the top category 3. clickona past category or article from history links
For effectiveness, we considered the number of relevant articles users marked for each task. For each article marked, we evaluated the marked article as: Not relevant: does not relate to the task Somewhat relevant: has bits of relevant information Mostly relevant: most of article is relevant
Definitely relevant: all of article is relevant We summarise our user study findings below:
User behaviour:
Original Wikipedia subtree ( a ):
Wikipedia untangled subtree ( b ):
Generated subtree ( c ): Figure 2 shows a comparison between the average number of user clicks for a given task on a given system and the breakdown of those clicks into average number of: backtracking clicks ; category clicks ; article clicks ;and other clicks 5
Figure 3 compares systems on a given task for relevant articles retrieved by a user. The breakdown on each bar includes articles ranging from definitely-relevant to not-relevant.

We summarise our findings in Table 11 below, give further de-tails on the main measures for each subtree and include significance tests. The major rows in Table 11 are grouped by Individual Tasks , followed by Ta s k Ty p e s , and finally Overall results .
Other clicks refers to clicks like marking relevant articles and re-viewing those articles
For Overall Results, we omitted Task 6 as this task appeared to favour Subtree c over the others, as we elaborate later. The minor rows are grouped into backtracking clicks and measures of rele-vant articles found. We sum up measures of Definitely , Mostly and Somewhat relevant articles found and present this in the table as relevant articles found. The first column shows the task compar-isons. We list the corresponding averages for each measure on each subtree beside it. The last set of columns in Table 11 presents p-values for the significance tests carried out on each measure. For the significance tests, we used a two-tailed unpaired unequal vari-ance t-test. The p-value shows the probability that the distributions of the users X  performance values for the specific comparison are the same. We may consider the performance of a given subtree to be different from another with statistical significance if the p-value is lower than 0.05. That is, there is less than 5% chance that the two distributions are from the same population.
Looking at the individual tasks, the main differences found be-tween the Subtree a and the Subtree b were highlighted in Task 3 and 6. Of the set of tasks given, these two were the most specific.
In Task 3, users were asked to find articles about  X  X ormula One X  car makers. We found that users performed three times better using Subtree a in finding more relevant articles. Using Subtree a ,users found an average of 7.5 definitely relevant articles compared with Subtree b where they found 2.5 and the difference was statistically significant. Upon closer inspection of the category structure, the Formula One section of the subtree had many categories with mul-tiple parents that were related which explains how the user was able to browse more effectively.

In Task 6, users had to find wine regions in Australia. Subtree a did significantly better than Subtree b . Using Subtree a , 4 out of the 6 users found relevant articles for this task, of which 3 users found definitely relevant articles, while all users using Subtree b failed to find any relevant articles.

In observing users performing Tasks 3 and 6, the key was in find-ing the specific gateway category. This gateway category opened up the relevant categories and were often clustered together around the gateway category. In task 6, this gateway category was more difficult to find in Subtree b . This was because there were relations missing from categories which users were looking in. The key cat-egory for task 6 in Subtree b was located in a related but obscure category called  X  X erbs and Medicinal herbs X  . In contrast, users performing the task on Subtree a tended to find the key category Wine as a multiple parent of  X  X rape varieties X  whichhelpedthem perform this task well.

Overall, Subtree b was comparable to Subtree a . If we exclude results from Task 6, we observe that backtracking on Subtree b was not significantly different to Subtree a . There is also little differ-ence in performance between Subtree a and b in the number of relevant articles users found. However, there seems to be a trend in task types that were most general. For Tasks 1 and 4, Subtree b seems to perform better in terms of both backtracking and relevant articles found. This could point to users being more confident us-ing Subtree b than with Subtree a . However, this was not observed to be statistically significant.
For the two domains, there were 2 generated subtrees made with different parameters. We found that users who performed tasks on Subtree c tended to have more  X  X n-the-middle X  relevant articles. That is, articles that were mostly but not definitely relevant. This is highlighted in Task 2 where users found a statistically signifi-cant higher number of mostly relevant articles compared with the Subtree b having on average 1.7 articles that were mostly relevant as compared with 0.2 respectively. We can attribute this to how the subtree was generated using the clustering technique, which is based on term frequencies, IDF scores and the cosine similarity measure.

Inspecting the subtree showed that the clustering technique was effective in segmenting the domain but made poor decisions in choosing the representative or parent category for a given subclus-ter. Having looked at the higher level categories, we confirm that this did not produce a subtree that was semantically similar to the untangled subtree.

Generally, users who performed tasks on the generated subtree were often not confident. We observed a larger proportion of back-tracking clicks with Subtree c than with Subtree b . From Figure 2, we can observe that this was noticeable in Tasks 1 and 4. In Task 1 and 4, Subtree c had on average over 5 times more clicks being backtracking clicks compared to Subtree b . This was found to be statistically significant in Task 1 but not in Task 4.

Users generally found more relevant articles in Subtree b than with Subtree c . This was particular highlighted in the general task types, that is, Tasks 1 and 4. In Task 6, users performed better us-ing Subtree c than the other two subtrees. However, on inspection, we found that the key category was among the list of top level cat-egories. We felt this biased the outcome but was not intentional. There was, however, no significant difference in performance for Tasks 2, 3 and 5. From Figure 3, we observed, that although Sub-tree b generally had more relevant articles, the result was close. Moreover, using the t-test did not yield any statistical difference between these tasks, with regards to finding relevant articles. Also, in Figure 2, there was no significant difference in the number of backtracking users made with regards to these tasks. Thus, it seems reasonable to carry out a further user study into the effects of depth, breadth and fanout with Tasks 2, 3 and 5 using Subtree c .
There are very few specific examples of ontology evaluations in existing literature. In this paper, we performed a task-based ontol-ogy evaluation using existing measures proposed in literature. This was based on requirements that came out of an analysis of a real world application  X  Wikipedia and its categories.

In the user studies which were carried out, we found that tan-gledness may be desirable in ontologies and category structures for browsing in general knowledge application areas like Wikipedia. This was especially significant in tasks that required specific infor-mation.

Also, we studied a method for generating a category structure but generally found that it was not comparable to a Wikipedia version. However, we found no significant differences in performance for some tasks. Thus for future work, we propose further studies into the effects of depth, breadth and f anout in an additional user study with these tasks.
 Acknowledgements. Mingfang Wu at RMIT for her advice re-garding the user experiments. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Info. Retrieval . [2] C. Brewster, H. Alani, S. Dasmahapatra, and Y. Wilks. Data [3] J. S. Dong, C. H. Lee, H. B. Lee, Y. F. Li, and H. Wang. A [4] A. Gangemi, C. Catenacci, M. Ciaramita, and J. Lehmann. [5] A. G  X  omez-P  X  erez. Towards a framework to verify knowledge [6] A. G  X  omez-P  X  erez. Evaluation of ontologies. Intl. Journal of [7] T. R. Gruber. A translation approach to portable ontology [8] T. R. Gruber. Toward principles for the design of ontologies [9] M. Gr  X  uninger and M. Fox. Methodology for the design and [10] N. Guarino. Some ontological principles for designing upper [11] N. Guarino and C. Welty. Evaluating ontological decisions [12] A. Maedche and S. Staab. Measuring similarity between [13] G. Marchionini. Information Seeking in Electronic [14] D. L. McGuinness. Spinning the Semantic Web: Bringing the [15] S. Tartir, I. Arpinar, M. Moore, A. Sheth, and [16] R. Thompson and W. Croft. Support for browsing in an [17] M. Uschold and M. Gr  X  uninger. Ontologies: Principles, [18] J. Yu, J. A. Thom, and A. Tam. Evaluating ontology criteria [19] Y. Zhao and G. Karypis. Hierarchical clustering algorithms
