 Skill assessment has long been an active area of research. Perhaps the most well-known application is to the game of chess, where the need to gauge the skill of one player versus another led to the development of the Elo rating system [1]. Although mathematically simple, Elo performed well in practice, treating skill assessment for individuals as a paired-comparison estimation problem, and was subsequently adopted by the US Chess Federation (USCF) in 1960 and the World Chess Federation (FIDE) in 1970. Other ranking systems have since been developed, notably Glicko [2], [3], a generalization of Elo which sought to address Elo X  X  ratings reliability issue, and TrueSkill [4], the well-known Bayesian model used for player/team ranking on Microsoft X  X  Xbox Live gaming service.
With hundreds of thousands to millions of players competing on networks such as Xbox Live, accurate estimations of skill are crucial because unbalanced games -those giving a distinct advantage to one player or team over their opponent(s) -ultimately lead to player frustration, reducing the likelihood they will continue to play. For multiplayer-focused games, this is a particularly relevant issue as their success or failure is tied to player interest sustained over a long period of time.
While previous work in this area [4] has been evaluated using data from a general population of players, less attention has been paid to certain boundary conditions, such as the case where the entire player population is highly-skilled individually. As in team sports [5], [6], less tangible notions, such as  X  X eam chem-istry X , are often cited as key differentiating factors, particularly at the highest levels of play. However, in existing skill assessment approaches, player perfor-mances are assumed to be independent from one another, summing individual player ratings in order to arrive at an overall team rating.
 In this work, we describe four approaches (TeamSkill-K, TeamSkill-AllK, TeamSkill-AllK-EV, and TeamSkill-AllK-LS) which make use of the observed performances of subsets of players on teams as a means of capturing  X  X eam chem-istry X  in the ratings process. These techniques use ensembles of ratings of these subsets to improve prediction accuracy , leveraging Elo, Glicko, and TrueSkill as  X  X ase learners X  by extending them to handle entire groups of players rather than strictly individuals. To the best of our knowledge, no similar approaches exist in the domain of skill assessment.

For evaluation, we introduce a rich dataset compiled over the course of 2009 based on the Xbox 360 game Halo 3, developed by Bungie, LLC in Kirkland, WA. Halo 3 is a first-person shooter (FPS) played competitively in Major League Gaming (MLG), the largest professional video game league in the world, and is the flagship game for the MLG Pro Circuit, a series of tournaments taking place throughout the year in various US cities. Our evaluation shows that, in general, predictive performance can be improved through the incorporation of subgroup ratings into a team X  X  overall rating, especially in high-level gaming contexts, such as tournaments, where teamwork is likely more prevalent. Additionally, the modeling of variance in each rating system is found to play a large role in determining the what gain (or loss) in performance one can expect from using subgroup rating information. Elo, which uses a fixed variance, is found to perform worse when used in concert with any TeamSkill approach. However, when the Glicko and TrueSkill rating sy stems are used as base learners (both of which model variance as player-level variables), several TeamSkill variants achieve the highest observed prediction accuracy, particularly TeamSkill-AllK-EV. Upon further investigation, we find this performance increase is especially apparent for  X  X lose X  games, consistent with the competitive gaming environment in which the matches occur.

The paper is structured as follows. Sect ion 2 reviews some of the relevant re-lated work in the fields of player and team ratings/ranking systems and competitive gaming. In Section 3, we introduce our proposed approaches, TeamSkill-K, TeamSkill-AllK, TeamSkill-AllK-EV, and TeamSkill-AllK-LS. For Section 4, we describe the Halo 3 dataset, how it was compiled, its charac-teristics, and where it can be found should other researchers be interested in studying it. In Section 5, we evaluate the TeamSkill approaches and compare them to  X  X anilla X  versions of Elo, Glicko, and TrueSkill, in game outcome pre-diction accuracy. Finally, in Section 6 we provide a number of conclusions and discuss our future work. In games, the question of how to rank (or provide ratings of) players is old, trac-ing its roots to the work of Louis Leon Thurstone in the mid-1920 X  X  and Bradley-Terry-Luce models in the 1950 X  X . In 1927 [7], Thurstone proposed the  X  X aw of comparitive judgement X , a means of measuring the mean distance between two physical stimuli, S a and S b . Thurstone, working with stimuli such as the sense-distance between levels of loudness, asserted that the distribution underlying each stimulus process is normal and that as such, the mean difference between the stimuli S a and S b can therefore be quantified in terms of their standard devi-ation. This work laid the foundation for the formulation of Bradley-Terry-Luce (BTL) models in 1952 [8], a logistic variant of Thurstone X  X  model which pro-vided a rigorous mathematical examination of the paired comparison estimation problem, using taste pref erence measurements as it s experimental example.
The BTL model framework provided the basis for the Elo rating system, introduced by Arpad Elo in 1959 [1]. Elo, himself a master chess player, developed the Elo rating system to replace the US Chess Federation X  s Harkness rating system with one more grounded in statistical theory. Like Thurstone, the Elo rating system assumes each player X  X  skill is normally distributed, where player i  X  X  expected performance is p i  X  N (  X  i , X  2 ). Notably, though, Elo also assumes players X  skill distributions share a constant variance  X  2 , greatly simplifying the mathematical calculation at the expense of capturing the relative certainty of each player X  X  skill.

In 1993 [3], Mark Glickman sought to improve upon the Elo rating system by addressing the ratings reliability issue in the Glicko rating system. By introduc-ing a dynamic variance for each player, the confidence in a player X  X  skill rating could be adjusted to produce more conservative skill estimates. However, the inclusion of this information at the player level also incurred significant com-putational cost in terms of updates, and so an approximate Bayesian updating scheme was devised which estimates the marginal posterior distribution Pr (  X  | s ), where  X  and s correspond to the player strengths and the set of game outcomes observed thus far, respectively.

With the advent of large-scale console-based multiplayer gaming on the Mi-crosoft Xbox in 2002 via Xbox Live, there was a growing need for a more gener-alized ratings system not solely designed for individual players, but teams -and any number of them -as well. TrueSkill [4], published in 2006 by Ralf Herbrich and Thore Graepel of Microsoft Research, used a factor graph-based approach to accomplish this. Like Glicko, TrueSkill also maintains a notion of variance for each player, but unlike it, TrueSkill samples an expected performance p i given a player X  X  expected skill, which is then summed for all players on i X  X  team to represent the collective skill of th at team. This expected performance p i is also assumed to be distributed normally, but similar to Elo, a constant variance is assumed across all players. Of note, TrueSkill X  X  summation of expected player performances in quantifying a team X  X  exp ected performance assumes player per-formances are independent of one another. In the case of team games, especially those occurring at high levels of competit ion where team chemistry and cooper-ative strategies play much larger roles, this assumption may prove problematic in ascertaining which team has the true advantage a priori. We explore this topic in more depth later on.
 Other variants of the aforementioned approaches have also been proposed. Coulom X  X  Whole History Rating (WHR) method [9] is, like other rating systems such as Elo, based on the dynamic BTL model. Instead of incrementally updating the skill distributions of each player after a match, it approximates the maximum a posteri over all previous games and opponents, resulting in a more accurate skill estimation. This comes at the cost of some computational ease and efficiency, which the authors argue is still minimal if deployed on large-scale game servers. Others [10] have extended the BTL mod el to use group comparisons instead of paired comparisons, but also assume player performance independence by defining a team X  X  skill as the sum of its players X .

Birlutiu and Heskes [11] develop and evaluate variants of expectation propaga-tion techniques for analysis of paired comparison data by rating tennis players, stating that the methods are generalizable to more complex models such as TrueSkill. Menke, et al. [12] develop a BTL-based model based on the logistic distribution, asserting that weaker teams are more likely to win than what a normally-distributed framework would predict. They also conclude that models based on normal distributions, such as TrueSkill, lead to an exponential increase in team ratings when one team has more players than another.
 The field of game theory includes a number of related concepts, such as the Shapley value [13], which considers the problem of how to fairly allocate gains among a coalition of players in a game. In the traditional formulation of skill assessment approaches, however, gains or losses are implicitly assumed to be equal for all players given the limitation to win/loss/team formation history during model construction and evaluation. That is, no additional information is available to measure the contribution of each player to a team X  X  win or loss. As discussed, the characteristic common to existing skill assessment approaches is that the estimated skill of a team is quantified by summing the individual skill ratings of each player on the team. Though understandable from the perspective of minimizing computational costs and/or model complexity, the assumption is not well-aligned with either intuition or research in sports psychology [5], [6]. Only in cases where the configuration of players remains constant throughout a team X  X  game history can the summation of individual skill ratings be expected to closely approximate a team X  X  true skill. Where that assumption cannot be made, as is the case in the dataset under study in this paper, it is difficult to know how much of a player X  X  skill rating can be attributed to the individual and how much is an artifact of the players he/she has teamed with in the past.
Closely related to this issue is the notion of team chemistry.  X  X eam chemistry X  or  X  X ynergy X  is a well-known concept [5], [6] believed to be a critical component of highly-successful teams. It can be thought of as the overall dynamics of a team resulting from a number of difficult-to-quantify qualities, such as leadership, confidence, the strength of player/player relationships, and mutual trust. These qualities are also crucial to successful Halo teams, which is sometimes described by its players as  X  X eal-time chess X , where teamwork is believed to be the key factor separating good teams from great ones.

The integration of any aspect of team chemistry into the modeling process doesn X  X  suggest an obvious solution, though. However, a key insight is that one need not maintain skill ratings only for individual players -they can be main-tained for groups of players as well. The skill ratings of these groups can then be combined to estimate the overall skill of a team. Here, we describe four methods which make use of this approach -TeamSkill-K, TeamSkill-AllK, TeamSkill-AllK-EV, and TeamSkill-AllK-LS. 3.1 TeamSkill-K At a high level, this approach is simple: for a team of K players, choose a sub-group size k  X  K , calculate the average skill rating for all k -sized player groups for that team using some  X  X ase learner X  (such as Elo, Glicko, or TrueSkill), and finally scale this average skill rating up by K/k to arrive at the team X  X  skill rating. For k = 1, this approach is equivalent to simply summing the individual player skill ratings together. As such, TeamSkill-K can be thought of as a gen-eralized approach for combining skill ratings for any K -sized team given player subgroup histories of size k .

Formally, let s  X  i be the estimated skill of team i and f i ( k ) be a function returning the set of skill ratings for player subgroups of size k in team i .Let each member of the set of skill ratings returned by f i ( k ) be denoted as s ikl , corresponding to the l -ith configuration of size k for team i . Here, s ikl is assumed to be a random variable drawn from some underlying distribution. Then, given some k , the collective strength of a team of size K can be estimated as follows: Though simple to implement and useful as a generalized approach for estimat-ing a team X  X  skill given ratings for player subgroups of size k , this choice of k introduces a potentially problematic trade-off between two desirable skill esti-mation properties -game history availability and player subgroup specificity. As k becomes larger, less history is available and as k becomes smaller, subgroups capture lower-level inte raction information. 3.2 TeamSkill-AllK To address this issue, a second approach was developed. Here, all available player subgroup information, 1  X  k  X  K , is used to estimate the skill rating of a team. The general idea is to model a team X  X  skill rating as a recursive summa-tion over all player subgroup histories, building in the ( k  X  1)-level interactions present in a player subgroup of size k in order to arrive at the final rating estimate.

This approach can be expressed as follows. Let s  X  ikl be the estimated skill rat-ing of the l -ith configuration of size k for team i and g i ( k ) be a function returning size k in team i .When k =0, g i ( k )= {  X  } and s  X  ikl = 0. As before, let s ikl be the skill rating of the l -ith configuration of size k for team i .Additionally,let  X  k be a user-specified parameter in the range [0 , 1] signifying the weight of the k -ith level of estimated skill ratings. Then, To compute s  X  i ,let s  X  i = s  X  ikl where k = K and l = 1 (since there is only one player subset rating when k = K ). This recursive approach ensures that all player subset history is used. 3.3 TeamSkill-AllK-EV In TeamSkill-AllK, if no history is available for a particular subgroup, default values (scaled to size k ) are used instead in order to continue the recursion. Problematically, cases where limited player subset history is available will pro-duce team skill ratings largely dominated by default rating values, potentially resulting in inaccurate skill estimates. As such, another approach was developed, called TeamSkill-AllK-EV. The core idea behind TeamSkill-AllK -the usage of all available player subgroup histories -was retained, but the new implementa-tion eschewed default values for all player subsets save those of individual players (consistent with existing skill assessment approaches), instead focusing on the evidence drawn solely from game history. Re-using notation, TeamSkill-AllK-EV is as follows: Here, h i ( k )= f i ( k ) where there exists at least one player subset history of size k ,else  X  is returned. 3.4 TeamSkill-AllK-LS In this context, it is natural to hypothesize that the most accurate team skill ratings could be computed using the largest possible player subsets covering all members of a team. That is, given some player subset X and its associated rating, ratings for subsets of X should be disregarded since they represent lower-level interation information X would have already captured in its rating. Formally, such an approach can be represented as follows: One obvious advantage to this approach is its speed, since this method prunes away from consideration ratings of subsets of previously-used supersets. The data under study in this paper was collected throughout 2009 as part of a larger project to produce a high-quality, competitive gaming dataset. Halo 3, re-leased in September 2007 on the Xbox 360 game console, is played professionally as the flagship game in Major Le ague Gaming (as were its pre decessors Halo:Combat Evolved and Halo 2). Major League Gaming (MLG) is the largest video gaming league in the world and has grown rapidly since its inception in 2002, with Internet viewership for 2009 events topping 750,000. After its release, Halo 3 replaced Halo 2 beginning with the 2008 season (known as the Pro Circuit).

The dataset contains Halo 3 multiplayer games between two teams of four players each. Each game was played in one of two environments -over the Internet on Microsoft X  X  Xbox Live service in custom games (known as scrimmages) or on a local area network at an MLG tournament. Information on each game includes the players and teams involved, the date of the game, the map and game type, the result (win/loss) and score, and per-player statistics such as kills, deaths, assists (where one player helps another player from the same team kill an opponent), and score.

The dataset has several interesting chara cteristics, such as the high frequency of team changes from one tournament to the next. With four players per team, it is not uncommon for a team with a poor showing in one tournament to replace one or two players before the next. As such, the resulting dataset lends itself to analyses of skill at the group level since the diversity of player assignments can aid in isolating interesting characte ristics of teams who do well versus those who do not. Additionally, since the players making up the top professional and semi-professional teams are all highly-skilled individually,  X  X asic X  game famil-iarity (such as control mechanics) are not considered as important a factor in winning/losing as overall team strategy, execution, and adaptation to the opposi-tion. This focus also helps mitigate issues pertaining to the personal motivations of players since all must be dedicated to winning in order to have earned a spot in the top 32 teams in the league, winnowing out those who might intentially lose games for their teams (as is commonplace in standard Halo 3 multiplayer gaming). Taken together, these elements make for a very high quality research dataset for those interested in studying competitive gaming, skill ratings sys-tems, and teamwork.
 The dataset has been made available on the HaloFit web site in two formats. The first, http://stats.halofit.org, contains several views into the dataset similar to statistics pages of professional sports leagues such as Major League Baseball. Users can drill down into the dataset using a series of filters to find data rele-vant to favorite teams or players. The second, http://halofit.org, contains partial and full comma-separated exports of the dataset. The dataset currently houses information on over 9,100 games, 566 players, and 186 teams. The four proposed TeamSkill approaches were evaluated by predicting the out-comes of games occuring prior to 10 Pro Circuit tournaments and comparing their accuracy to una ltered versions ( k = 1) of their base learner rating systems -Elo, Glicko, and TrueSkill. For TeamSkill-K, all possible choices of k for teams of 4, 1  X  k  X  4, were used. Given two teams, t 1 and t 2 , The prior probability of t 1 winning is a straightforward derivation from the negative CDF at 0 of the distri-bution describing the difference between two independent, normally-distributed random variables: For each tournament, we evaluated each rating approach using:  X  3 types of training data sets -games consisting only of previous tournament  X  3 periods of game history -all data except for the data between the test  X  2 types of games -the full dataset and th ose games considered  X  X lose X  (i.e., In the case where only tournament data is used as training set data, the most recent tournament preceding the test tour nament replaced the inter-tournament scrimmage data for the  X  X ong X  and  X  X ecen t X  game history config urations. Simi-larly,  X  X ecent X  game history when consid ering both tournam ent and scrimmage data included the most recent tournament.  X  X lose X  games were defined using a slightly modified version of the  X  X hallenge X  method [4] in which the top 20% closest games were selected for one ra ting system and presented to the other (and vice versa). In this evaluation, the closest games from the  X  X anilla X  ver-sions of each rating system (i.e., k = 1) were presented to each of the TeamSkill approaches while the closest games from TeamSkill-AllK-EV were presented to the  X  X anilla X  versions. The reasons these two were chosen is because all the TeamSkill approaches are intended to improve upon their respective  X  X anilla X  versions and that repeated testing had shown TeamSkill-AllK-EV to be the best performing approach on full datasets in many cases. The default values used dur-ing the evaluation of Elo (  X  = 0.07,  X  = 193.4364,  X  0 = 1500,  X  2 0 =  X  2 ), Glicko ( q = log (10) / 400,  X  0 = 1500,  X  2 0 = 100 2 ), and TrueSkill ( =0 . 5,  X  0 = 25,  X  0 =(  X  0 / 3) Additionally, for Glicko, a rating period of one game was assigned due to the continuity of game history over the course of 2008 and 2009, as well as to ap-proximate an  X  X pples to apples X  comparison with respect to Elo and TrueSkill. In the interest of space, a subset of the 3 ,780 total evaluations are presented corresponding to the  X  X omplete X  cases. The  X  X ong X  results essentially mirrored the  X  X omplete X  results while the  X  X ecent X  re sults were virtually identical across all TeamSkill variations for all non-cl ose games and produced no clear patterns for close games (with differences only emerging after one or two tournaments, as can be seen in the  X  X o mplete X  results). 5.1 Findings and Analysis The results in figures 1, 2, and 3 show that in general, Glicko and TrueSkill ben-efit from the incorporation of team che mistry components and tend to improve the prediction accuracy overall in comparison to the  X  X anilla X  versions ( k =1). The TeamSkill-AllK and TeamSkill-AllK-EV approaches -TeamSkill-AllK-EV in particular -outperform k = 1 in nearly all cases. TeamSkill-AllK-LS, on the other hand, shows no similar performance gain, nor do any of the TeamSkill versions in the range 1 &lt;k  X  4. These results suggest that group-level ratings alone are insufficient for accurately assessing the strength of a team -player-level ratings must be incorporated as well.

No similarly positive effect is observed for Elo, although TeamSkill-AllK-EV X  X  accuracy does approach that of k = 1. In fact, the accuracy for all non-k =1 approaches is, at best, equal to k = 1. Interestingly, Elo still performs well for k = 1, in some cases outperforming Glicko and TrueSkill. Considering Elo was developed in the mid-1950 X  X , that it still competes with state-of-the-art approaches is an impressive result unto itself.

As to the source of Glicko and TrueSkill X  X  improved overall performance, fur-ther inspection (figures 4, 5, and 6) revea ls significant performance increases (with respect to k = 1) in close games. At times, the margin of difference is as much as 8%. It can also be seen that over time, this margin tends to widen. Taken together, these results indicate that the group-level ratings have the effect of better distinguishing which team has the true advantage in close match-ups, a key finding well-aligned with prior research [5], [6]. 5.2 Discussion As mentioned, Elo doesn X  X  benefit from the inclusion of group-level ratings in-formation. The reason stems from Elo X  X  use of a constant variance and as such, Elo is not sensitive to the dynamics of a player X  X  skill over time. For groups of players, this issue is compounded since the higher the k under consideration, the less prior game history can be drawn on to infer their collective skill. With the TeamSkill approaches, the net effect is that incorporating ( k&gt; 1)-level group ratings  X  X ilute X  the overall team rating, resulting in a higher number of closer games since there is no provision for Elo X  X  constant variance to differ depending on the size of the group under consideration.

Similarly, variance also accounts for much of the differences between Glicko and TrueSkill X  X  performances. Both make use of player-level variances (and, thus, group-level variances using the TeamSkill approaches). However, TrueSkill also maintains a constant  X  X  erformance X  variance,  X  2 , across all players, which is applied just prior to computing the predicted ordering of teams during updates.  X  2 is a user-provided parameter which, when increased, similarly increases the probability of TrueSkill believing teams will draw, discounting the potentially small differences between them in collective skill. As such, this  X  X erformance X  variance has a similar  X  X ilution X  effect as in Elo, but are less pronounced because TrueSkill also maintains player/group-level variances.

These results highlight the critical role played by skill variance in estimating the skill of a group of players. The ways in which it is maintained can result in certain biases which arise when models X  p rior beliefs are different relative to the observations. Methods for tackling such an issue could consist of maintaining a prior distribution over the skill variance itself or using a mixture model for the skill variance. Such extensions to Glicko or TrueSkill could result in techniques that can better assimilate new observations with prior beliefs in order to generate superior predictions.

Additionally, given the ensemble methodology employed by the TeamSkill ap-proaches, a logical next step is to consider boosted (or online) versions of the TeamSkill framework to see if any further gains can be made. The additional computation cost of boosting in this context, though, could render it unfeasible in a real-world deployment, but would b e of academic interest with respect to studying how accurate skill assessment c an be using only win/loss/team for-mation information. Given these real-world constraints, a fully online ensemble framework for TeamSkill would be ideal and as such, our future work is concerned with developing this idea more fully. Our experiments demonstrate that in many cases, the proposed TeamSkill ap-proaches can outperform the  X  X anilla X  versions of their respective base learner, particularly in close games. We find that the ways in which skill variance is ad-dressed by each base learner has a large e ffect on the prediction accuracy of the TeamSkill approaches, the results suggesting that those employing a dynamic variance (i.e., Glicko, TrueSkill) can benefit from group-level ratings. In our fu-ture work, we intend to investigate ways of better representing skill uncertainty, possibly by modeling the uncertainty itself as a distribution, and constructing an online version of TeamSkill in order to improve skill estimates.
 We would like to thank the Data Analysis and Management Research group, as well as the reviewers, for their feedback and suggestions. We would also like to thank Major League Gaming for making their 2008-2009 tournament data available.
