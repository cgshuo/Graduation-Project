 In recent years many hard problems have been transformed into easier problems that can be solved efciently via linear methods [1] or convex optimization [2]. One area where these trends have not yet had a signicant impact is Reinforcement Learning. Indeed the discrete and unstructured nature of traditional MDPs seems incompatible with simplifying features such as linearity and convexity. This motivates the search for more tractable problem formulations. Here we construct the rst MDP family where the minimization over the control space is convex and analytically tractable, and where the Bellman equation can be exactly transformed into a linear equation. The new formalism enables efcient numerical methods which could not previously be applied in Reinforcement Learning. It also yields accurate approximations to traditional MDPs.
 Before introducing our new family of MDPs, we recall the standard formalism. Throughout the p where a non-empty subset A S of states are absorbing and incur zero cost: p ij ( u ) = j ` ( i; u ) = 0 whenever i 2 A . Results for other formulations will be summarized later. If A can be reached with non-zero probability in a nite number of steps from any state, then the undiscounted innite-horizon optimal value function is nite and is the unique solution [3] to the Bellman equation For generic MDPs this equation is about as far as one can get analytically. In our new class of MDPs the control u 2 R jSj is a real-valued vector with dimensionality equal to the number of discrete states. The elements u j of u have the effect of directly modifying the tran-probability matrix P with elements p Note that P (0) = P . In some sense this is the most general notion of "control" one can imagine  X  However there are two constraints implicit in (2). First, p Thus the admissible controls are Real-valued controls make it possible to dene a natural control cost. Since the control vector acts directly on the transition probabilities, it makes sense to measure its magnitude in terms of the difference between the controlled and uncontrolled transition probabilities. Differences between probability distributions are most naturally measured using KL divergence, suggesting the following probabilities from state i to all other states under control u . The control cost is dened as From the properties of KL divergence it follows that r ( i; u ) 0 , and r ( i; u ) = 0 iff u = 0 . Substituting (2) in (4) and simplifying, the control cost becomes This has an interesting interpretation. The Markov chain likes to behave according to P but can controller expects to pay before observing the transition.
 Coming back to the MDP construction, we allow an arbitrary state cost q ( i ) 0 in addition to the above control cost: We require q ( i ) = 0 for absorbing states i 2A so that the process can continue indenitely without incurring extra costs. Substituting (5, 6) in (1), the Bellman equation for our MDP is We can now exploit the benets of this unusual construction. The minimization in (7) subject to the constraint (3) can be performed in closed form using Lagrange multipliers, as follows. For each i dene the Lagrangian The necessary condition for an extremum with respect to u j is When p Taking another derivative yields and therefore (10) is a minimum. The Lagrange multiplier i can be found by applying the constraint (3) to the optimal control (10). The result is and therefore the optimal control law is Thus we have expressed the optimal control law in closed form given the optimal value function. Note that the only inuence of the current state i is through the second term, which serves to nor-specifying how to get there. The details of the trajectory emerge from the interaction of this con-troller and the uncontrolled stochastic dynamics. In particular, the optimally-controlled transition probabilities are These probabilities are proportional to the product of two terms: the uncontrolled transition prob-abilities p abilities (14) correspond to a Gibbs distribution where the optimal value function plays the role of an energy function.
 Substituting the optimal control (13) in the Bellman equation (7) and dropping the min operator, Rearranging terms and exponentiating both sides of (15) yields We now introduce the exponential transformation which makes the minimized Bellman equation linear: Dening the vector z with elements z ( i ) , and the diagonal matrix G with elements exp ( q ( i )) along its main diagonal, (18) becomes Thus our class of optimal control problems has been reduced to a linear eigenvalue problem. 2.1 Iterative solution and convergence analysis answer to both questions is afrmative, because the Bellman equation has a unique solution and v is a solution to the Bellman equation iff z = exp ( v ) is an admissible solution to (19). The only remaining question then is how to nd the unique solution z . The obvious iterative method is P has spectral radius 1 . Multiplication by G scales down some of the rows of P , therefore G P has spectral radius at most 1 . But we are guaranteed than an eigenvector z with eigenvalue 1 exists, therefore G P has spectral radius 1 and z is a largest eigenvector. Iteration (20) is equivalent to the power method (without the rescaling which is unnecessary here) so it converges to a largest eigenvector. The additional constraints on z are clearly satised at all stages of the iteration. In particular, for i 2A the i -th row of G P has elements j to 1 for all k .
 We now analyze the rate of convergence. Let m = jAj and n = jSj . The states can be permuted so that G P is in canonical form: for i 2A . From (21) we have A stochastic matrix P with m absorbing states has m eigenvalues 1 , and all other eigenvalues are values of T 1 are smaller than 1 and so lim k !1 T k (and large transition probabilities from non-absorbing to absorbing states). Convergence is inde-pendent of problem size because has no reason to increase as the dimensionality of T 1 increases. Indeed numerical simulations on randomly generated MDPs have shown that problem size does not systematically affect the number of iterations needed to reach a given convergence criterion. Thus the average running time scales linearly with the number of non-zero elements in P . 2.2 Alternative problem formulations While the focus of this paper is on innite-horizon total-cost problems with absorbing states, we have obtained similar results for all other problem formulations commonly used in Reinforcement Learning. Here we summarize these results. In nite-horizon problems equation (19) becomes stage problems equation (19) becomes stage turns out to be log ( ) . In innite-horizon discounted-cost problems equation (19) becomes where &lt; 1 is the discount factor and z is dened element-wise. Even though the latter equation is nonlinear, we have observed that the analog of iteration (20) still converges rapidly. Suppose the state space S of our MDP corresponds to the vertex set of a directed graph, and let D be of a directed edge from vertex i to vertex j . Let A S be a non-empty set of destination vertices. Our goal is to nd the length s ( i ) of the shortest path from every i 2 S to some vertex in A . For i 2A we have s ( i ) = 0 and d ij = j i .
 We now show how the shortest path lengths s ( i ) can be obtained from our MDP. Dene the elements of the stochastic matrix P as corresponding to a random walk on the graph. Next choose &gt; 0 and dene the state costs the optimal value function for the MDP dened by (26, 27). If the control costs were 0 then the bounded. This can be shown using which implies that for p the elements of u , the following bound holds: The control costs are bounded and we are free to choose arbitrarily large, so we can make the state costs dominate the optimal value function. This yields the following result: Thus we have reduced the shortest path problem to an eigenvalue problem. In spectral graph theory many problems have previously been related to eigenvalues of the graph Laplacian [4], but the shortest path problem was not among them until now. Currently the most widely used algorithm is nding largest eigenpairs have running time O ( n ) for sparse matrices.
 Of course (30) involves a limit and so we cannot obtain the exact shortest paths by solving a single eigenvalue problem. However we can obtain a good approximation by setting large enough  X  but not too large because exp ( ) may become numerically indistinguishable from 0 . Fig 1 illustrates the solution obtained from (30) and rounded down to the nearest integer, for = 1 in 1A and = 50 in 1B . Transitions are allowed to all neighbors. The result in 1B matches the exact shortest paths. Although the solution for = 1 is numerically larger, it is basically a scaled-up version of the correct solution. Indeed the R 2 between the two solutions before rounding was 0 : 997 . In the previous section we replaced the shortest path problem with a continuous MDP and obtained an excellent approximation. Here we obtain approximations of similar quality in more general set-tings, using an approach reminiscent of LP-relaxation in integer programming. As in LP-relaxation, theoretical results are hard to derive but empirically the method works well.
 We construct an embedding which associates the controls in the discrete MDP with specic control vectors of a continuous MDP, making sure that for these control vectors the continuous MDP has the and reasonable assumptions, as follows. Consider a discrete MDP with transition probabilities and This matrix has elements We need two assumptions to guarantee the existence of an exact embedding: for all i 2S the matrix for any/all a 2U ( i ) . Remove the zero-columns of B ( i ) and restrict j 2N ( i ) . The rst step in the construction is to compute the real-valued control vectors u a corresponding to and continuous MDPs: These constraints are satised iff the elements of the vector u a are The second step is to compute the uncontrolled transition probabilities p continuous MDP so as to match the costs in the discrete MDP. This yields the set of constraints For the control vector given by (33) the KL-divergence cost is where h ( i; a ) is the entropy of the transition probability distribution in the discrete MDP: The constraints (34) are then equivalent to inequality follows from the assumption that B ( i ) has full row-rank. Suppressing the dependence on the current state i , the constraints (34) can be written in matrix notation as Since the probabilities p We are given B; y and need to compute q; x satisfying (38, 39). Let b x be any vector such that
B b x = y , for example b x = B y y where y denotes the Moore-Penrose pseudoinverse. Since B is a stochastic matrix we have B 1 = 1 , and so Therefore x = b x + q 1 satises (38) for all q , and we can adjust q to also satisfy (39), namely This completes the embedding. If the above q turns out to be negative, we can either choose another Such scaling does not affect the optimal control law for the discrete MDP, but it makes the elements of B y y more negative and thus q becomes more positive.
 We now illustrate this construction with the example in Fig 2 . The grid world has a number of obstacles (black squares) and two absorbing states (white stars). The possible next states are the immediate neighbors including the current state. Thus jN ( i ) j is at most 9 . The discrete MDP has among the other states. The costs e ` ( i; a ) are random numbers between 1 and 10  X  which is why the optimal value function shown in grayscale appears irregular. Fig 2A shows the optimal value function for the discrete MDP. Fig 2B shows the optimal value function for the corresponding con-tinuous MDP. The scatterplot in Fig 2C shows the optimal values in the discrete and continuous MDP (each dot is a state). The values in the continuous MDP are numerically smaller  X  which is to be expected since the control space is larger. Nevertheless, the correlation between the optimal values in the discrete and continuous MDPs is excellent. We have observed similar performance in a number of randomly-generated problems. So far we assumed that a model of the continuous MDP is available. We now turn to stochastic approximations of the optimal value function which can be used when a model is not available. All state cost incurred at i k , and k is the sample number. Equation (18) can be rewritten as This suggests an obvious stochastic approximation b z to the function z , namely where the sequence of learning rates k is appropriately decreased as k increases. The approxima-tion to v ( i ) is simply log ( b z ( i )) . We will call this algorithm Z-learning. Let us now compare (43) to the Q-learning algorithm applicable to discrete MDPs. Here we have have a control u k generated by some control policy. The update equation for Q-learning is To compare the two algorithms, we rst constructed continuous MDPs with q ( i ) = 1 and transitions to the immediate neighbors in the grid worlds shown in Fig 3 . For each state we found the optimal transition probabilities (14). We then constructed a discrete MDP which had one action (per state) that caused the same transition probabilities, and the corresponding cost was the same as in the continuous MDP. We then added jN ( i ) j 1 other actions by permuting the transition probabilities. Thus the discrete and continuous MDPs were guaranteed to have identical optimal value functions. Note that the goal here is no longer to approximate discrete with continuous MDPs, but to construct pairs of problems with identical solutions allowing fair comparison of Z-learning and Q-learning. We run both algorithms with the same random policy. The learning rates decayed as k = run to which sample k belongs. When the MDP reaches an absorbing state a new run is started from a random initial state. The approximation error plotted in Fig 3 is dened as convergence, however for larger problems ( Fig 3B ) the new Z-learning algorithm was clearly faster. This is not surprising: even though Z-learning is as model-free as Q-learning, it benets from the analytical developments in this paper and in particular it does not need a maximization operator or state-action values. The performance of Q-learning can be improved by using a non-random (say -greedy) policy. If we combine Z-learning with importance sampling, the performance of Z-learning can also be improved by using such a policy. We introduced a new class of MDPs which have a number of remarkable properties, can be solved efciently, and yield accurate approximations to traditional MDPs. In general, no single approach is likely to be a magic wand which simplies all optimal control problems. Nevertheless the results so far are very encouraging. While the limitations remain to be claried, our approach appears to have great potential and should be thoroughly investigated.
 [1] B. Scholkopf and A. Smola, Learning with kernels. MIT Press (2002) [2] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge University Press (2004) [3] D. Bertsekas, Dynamic programming and optimal control (2nd ed). Athena Scientic (2000)
