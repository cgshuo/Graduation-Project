 1. Introduction 1.1. Wrappers for semi-structured Web sources
In today X  X  Web, there are many sites which provide access to structured data contained in an underlying database. Typically, these sources provide an HTML form allowing queries to be issued against the database, and they return the query results embedded in HTML pages conforming to a certain template.
Several approaches have been reported in the literature for building  X  X  X rappers X  X  for this kind of sites (see, for instance [10,11,13,20] ; [15] provides a brief survey).

Wrappers accept a query against the Web source from a calling application and return a set of structured results, thus enabling it to access Web data in a similar manner to information from databases. Wrappers can be used for a wide variety of data extraction and automation tasks in the Web environment. They are also a key component in mediation architectures for Web data integration [24] . Fig. 1 outlines the process followed by a wrapper to execute a certain query on an Internet bookshop. Wrappers need to perform two kinds of tasks:  X  Executing automated navigation sequences through Web sites to access the pages containing the required the result listing is paginated in step 3), and  X  Generating data extraction programs for obtaining the structured records from the retrieved HTML pages (e.g. extracting book information in step 4 of Fig. 1 ).

The vast majority of works dealing with automatic and semi-automatic wrapper generation have focused on the second task. The first task has received much less attention, although the problem of easily building automated navigation sequences (not necessarily in the context of Web wrappers) has been addressed in works such as [2,20] .

It has been frequently argued that technologies such as XML, Web Services turn wrapper technologies obsolete. While this may become a reality in the future, the vast majority of today X  X  web content is still accessible exclusively by  X  X  X raditional X  X  means. Furthermore, we do not have to deal only with  X  X  X egacy X  X  websites. Many of the new websites that appear every day on the Internet keep supporting only conventional HTTP/HTML access to their data. 1.2. Wrapper maintenance
The main problem with wrappers is that they can become invalid when the Web sources change. This is called the  X  X rapper maintenance X  problem, and it can be divided into three main tasks:  X  Detecting the changes on the source that invalidate the current wrapper.  X  Regenerating the automated navigation sequences required to access the pages containing the required data.  X  Regenerating the data extraction programs needed to extract the structured results from the HTML pages.
The first task (called wrapper verification ) has been addressed in [12,16] and the third in [16,18,19,23] . Nev-ertheless, to the best of our knowledge, the second task remains mainly ignored in research literature. In this paper, we present heuristics and algorithms to deal with this important issue.

The techniques presented here are part of a global system called ITPilot (see [21] for an overview). In [23] the techniques used in ITPilot for maintaining the data extraction programs used by the wrappers are described. The work presented here complements those techniques to provide a complete solution for auto-matic wrapper maintenance.

The rest of the paper is organized as follows. Section 2 provides the needed context for Web navigation sequences and wrapper maintenance in ITPilot. Section 3 is the core of the paper, and it describes our approach to maintaining Web wrapper navigation sequences. Section 4 describes our experiments with real
Web sites. Section 5 discusses related work. Finally, Section 6 presents our conclusions and outlines our future work. 2. Web navigation and automatic maintenance in ITPilot
This section provides the necessary context for the rest of the paper, and it is organized as follows. Section 2.1 explains how navigation sequences are modeled and executed in ITPilot. Section 2.2 describes the wrapper model used, which is comprised of a series of automatic navigation and data extraction tasks. Section 2.3 describes the sequence of steps followed by ITPilot to maintain wrappers. Knowing the precise order followed by ITPilot is needed to understand what information we have available at each step of the processes that will be described later in the paper. 2.1. Navigation sequence model and execution
A navigation sequence is a list of navigation steps specifying how to reach a certain Web page. Navigation sequences are needed because, in today X  X  Web, there are many pages that cannot be directly accessed through a
URL. This is due to issues such as pages that are dynamically generated according to user-provided input, and session maintenance mechanisms.

ITPilot models a navigation sequence as the list of events a human user would need to produce on a Web browser interface to reach the desired page. With this aim, we have defined NSEQL [20] , a language to declar-atively define such sequences. The execution environment for NSEQL sequences is made up of  X  X  X ightweight X  X  automated Web browsers built by using the APIs of the most popular browser technologies (we have created NSEQL implementations for both Microsoft Internet Explorer and Firefox).

Representing navigation sequences as actions on a Web browser interface makes wrapper creators forget about problems such as Javascript or non-standard session mechanisms in the same way a human user of a
Web browser is not bothered about those issues when navigating the Web. Let us consider the following exam-ple to get a first flavor of NSEQL.

Example 1. Fig. 2 shows an NSEQL program to execute a query on a sample Internet bookshop (based on the actual bookshop Amazon). Fig. 3 shows excerpts from its home ( Fig. 3a ) and query form ( Fig. 3b ) pages, along with some relevant fragments of their HTML code.

The Navigate(url,pages ) command makes the browser navigate to the given URL, and waits for the spec-ified number of pages to be loaded in the browser. Its effects are equivalent to those of a human user typing the
URL on their browser address bar. The ClickOnAnchorByText (text, position) command looks for the position-th anchor on the page which contains the given text and generates a click event on it, causing the browser to navigate to the target page (other ClickOnAnchor commands exist to locate anchors according to other parameters).

The FindFormByName (name, position) command looks for the position-th HTML form in the DOM tree of the page with the given name attribute value (other FindForm commands exist to locate forms according to other parameters). Then, the SetInputValue (fieldName, position, value) commands are used to assign values to the form fields  X  X itle X  and  X  X uthor X . The value parameter might be a string (closed by  X  X  X  X ) or a variable whose value will be obtained at runtime. Variable names are prefixed by the  X  X  X  character.

The SelectIndexByText (element _ name, element _ position, value _ name, value _ position) command selects a value in a  X  X elect option X  HTML field. In the example, we select the first value containing the word  X  X nglish X  in the field named  X  X anguage X .

The clickOnElement (name, type, position) command clicks on the position-th element of the given type and name from the currently selected form. In this case, it is used to select the  X  X aperback X  format (second option of the element of the type radio called  X  X ormat X ). Finally, the submitForm(pages) command submits the current form and waits for the specified number of pages to be loaded in the browser.
 NSEQL also includes commands to deal with issues such as frames or pop-up windows. In addition,
NSEQL programs can be easily graphically generated  X  X  X y example X  X : ITPilot includes a toolbar that can be installed in the user X  X  browser (the toolbar is currently available only for MSIE); the toolbar is able to record the events generated by a human user when navigating and translating them into an NSEQL program. See [21] for more details. 2.2. Wrapper model
We define a wrapper as an executable component which is able to execute a query on the contents of a Web form and return the obtained results as a set of structured data records to the calling application.
We model the query results obtained by a wrapper as belonging to a type . Every type has an associated name and is composed of a list of attributes, each one having a name and a value. The value for atomic attri-butes is a character string. Compound attributes built by using register and array type constructors are also supported. At wrapper generation time, the user may also optionally specify a set of aliases for each attribute name. The aliases represent alternative labels to name the attribute.
 string (it can be empty). We will term the attributes of a type that may appear in queries as searchable attributes.

In our model, wrappers need three kinds of navigation sequences to access the pages containing the data records that constitute the answer to a given query: 1.  X  X  X uery sequence X  X  : This kind of navigation sequence is in charge of obtaining the HTML page containing the first set of results for a given query. Typically, this sequence is in charge of navigating to the page con-taining the HTML query form and of filling it in according to the query input values, thus obtaining the first page of results. In some cases, issuing a query may involve filling in several consecutive forms. 2.  X  X  X ext interval sequences X  X  : Many Web forms paginate their query results. This kind of sequence is needed to navigate through the successive result intervals. 3.  X  X  X ore detail sequences X  X  : Often the pages containing the query result listings do not show all the informa-tion about each individual result. In these cases, one or more  X  X  X etail X  X  pages must be accessed to obtain the complete data of each result. 2.3. Sequence of steps for maintaining wrappers in ITPilot
ITPilot covers all the necessary steps for automatic wrapper maintenance. This section briefly overviews the global maintenance process and shows how the tasks related to maintaining navigation sequences fit into it. During normal wrapper operation, the wrapper answers the queries issued by some calling application.
During this process, the system selects some of these queries and stores their results in a local database. These stored results will be used at later stages of the maintenance process and constitute a crucial point in our approach. The wrapper is also examined to detect if the source has changed ( wrapper verification stage). When a change in the source is detected, the maintenance system starts. The steps followed are: 1. If the query navigation sequence is not working properly, then the system tries to regenerate it. This process takes as input the previous navigation sequence and the formerly mentioned stored queries and their results. The output is a new navigation sequence that is able to execute a query on the source and obtain the first result listing page. 2. If the  X  X  X ext interval X  X  sequence is not working properly, we need to regenerate it. After regenerating this sequence, we can combine the  X  X  X uery X  X  sequence and the  X  X  X ext interval X  X  sequence to execute each stored query on the source and obtain all its  X  X esult listing X  pages.
 3. The following step regenerates the data extraction programs needed to extract the data contained in the result listing pages. As inputs to this process we have for each stored query (1) the results of the last exe-cution of the query stored during wrapper operation (before the change in the source), and (2) the set of current  X  X esult listing X  pages of the query (obtained after the change in the source). How we can regenerate the data extraction programs provided with the aforementioned input is not described in this paper but in [23] . 4. Once we are able to extract the data from the result listing pages, the next step is to regenerate the  X  X ore detail X  sequences (in case they are needed in that particular source). 5. Finally, the system can regenerate the data extraction programs used for detail pages using the same method as for step 3, thus completing the wrapper regeneration. 3. Maintaining navigation sequences
Now that we have provided the necessary context, we are ready to describe our techniques for automatic maintenance of navigation sequences. We have the following input for this process:  X  The type of extracted results (including the name and aliases defined for each attribute).  X  The previous navigation sequence (before the change in the source occurred). operation the wrapper automatically stores the results obtained for some of the queries it answers in a local database. For each stored query q i we have the input values provided in the query for each  X  X  X earchable X  X  attri-bute and the set of structured results obtained by the wrapper in the last execution of q
The heuristics and methods we use to maintain each kind of sequence identified in Section 2.2 are detailed in the next subsections, respectively. 3.1. Maintaining the query sequence
Our approach to maintaining the  X  X  X uery sequence X  X  relies on a crawling process that begins on the source home page and searches for candidate query forms in the site (a maximum crawling depth may be configured).
To reach new pages of the site, the crawling process automatically generates new crawling steps from the anchors of the previously reached pages and from what we will term bounded forms, which are those forms that are exclusively composed of bounded fields .

A bounded field is a field offering a finite list of possible query values (e.g. checkbox fields, radio buttons, select-option fields, etc.). With bounded forms, it is possible to compute every possible way of filling in the form and to generate a crawling step for each one.

The pending crawling steps are ordered so that the most  X  X  X romising X  X  ones are tried first. A crawling step is ranked by obtaining the texts visually associated with it in the page, and comparing them to the texts involved in the previous navigation sequence (before the change in the source). For instance, if the previous navigation sequence contained a command for generating a  X  X lick X  event on an anchor having  X  X ook search X  as associated text, anchors having associated texts such as  X  X dvanced book search X  or  X  X earch for books X  will rank higher.
We will discuss later, for other purposes, the techniques used to compute text similarity measures, hence we will not provide further detail here.

In each page reached by the crawling process every form (bounded or not) is examined to determine if it is a valid  X  X uery form X  for the wrapper by performing the following steps: 1. The system tries to match the  X  X  X earchable X  X  attributes of the type of the wrapper with the fields of the form using several heuristics (see Section 3.1.1 ). 2. To further check the matches obtained at step 1, the system uses them to perform a validity test . This test consists in executing some of the queries { q 1 , ... , q the change in the source. Then it examines the obtained response pages to determine if they are correct (we will consider them correct if they contain a subset of the expected answers to the queries; see Section 3.1.2 ). is an  X  X xact match X . In that case, we select it as new  X  X uery form X  and stop the crawling process.
An intermediate situation occurs when a match is found for some (but not all) the searchable attributes, and the validity test in step 2 is successful. In that case, we will consider the form a  X  X artial match X  and continue with the crawling process.

If the crawling process is completed without finding any  X  X xact match X , we consider the best  X  X artial match X  (the  X  X artial match X  having matches for more searchable attributes) and try to add new matches to it. More precisely, for each searchable attribute a not yet matched: 1. For each field f of the form not yet assigned to any other attribute, we generate a  X  X  X andidate match X  X  between a and f . 2. For each candidate match, we issue some of the stored queries involving a and examine the obtained response pages to determine if they are correct (according to the same test used previously). If the test is successful for a candidate match, we add it to the list of valid matches.

The next subsections detail some aspects of the process of maintaining the query sequence: Section 3.1.1 describes how to find matches for searchable attributes in a form, while Section 3.1.2 shows how to determine whether the response to a query is correct or not. Section 3.1.3 describes two special situations: executing a query involving several forms in consecutive pages and sources requiring authentication. 3.1.1. Finding matches for searchable attributes in a form
Given a form f located in a certain HTML page and a type t describing the schema of the expected query t . The method we use to determine this consists of the following steps: 1.  X  X  X arsing X  X  the query form. This step consists in logically grouping the labels and elements of the query form, so that each group represents a logical query field . 2. Matching the logical query fields of f obtained in the previous step, with the searchable attributes of t .
In step 1, for each logical query field in the form, the parsing process outputs its associated form fields, text been proposed in the literature for addressing the problem of parsing query forms, such as [5,26] and our pre-vious work [1] . Our techniques for automatic maintenance of navigation sequences leverage on them.
Fig. 4 shows a sample form corresponding to a Web bookshop and the results of the parsing process com-the possible values  X  X  X ardcover X  X ,  X  X  X aperback X  X  and  X  X  X -Books &amp; Docs X  X .

In step 2, the basic idea to rank the  X  X  X imilarity X  X  between a logical query field f and a searchable attribute a is to measure the textual similarity between the labels associated with f in the page (obtained as output of the  X  X  X arsing X  X  step) and the texts associated with a in the type (attribute name and aliases specified at wrapper creation time).

When the field is bounded (that is, when it has a limited set of possible values such as in select option fields, radio buttons, checkboxes, etc.), the system takes also into account the text similarities between the query input values used for a in the stored queries and the possible values of f in the page. Text similarity measures are obtained using a method proposed in [7] that combines TFIDF weighting and the Jaro-Winkler edit dis-tance algorithm. The experiments in [7] found this method the most effective for name-matching tasks among a variety of standard text similarity methods.

More precisely, we use the following method to rank the  X  X  X imilarity X  X  between a logical query field f and a searchable attribute a : 1. We notate the texts describing a (that is, the attribute name and its aliases specified at wrapper creation of the form parsing step; we notate them as { f _ text 1 , ... , f _ text 2. If f is a bounded field, then we also obtain its possible values from the page { f _ value put of the parsing step. We also examine the stored queries to obtain the set of input values used in them for attribute a{a _ value 1 , ... , a _ value q }. 3. Now, we compute the text similarity between each pair ( a _ alias 4. If f is  X  X  X ounded X  X , then we also compute the text similarity between each pair (i.e. the mean of the maximum similarities obtained for each a _ value 5. If f is bounded, then the similarity between a and f is set to max{sim similarity is set to sim 1 .

As a result of applying the previous steps, we can obtain a table with the estimated similarity between each attribute and each logical field. Then we proceed as follows: 1. The pairs from the table that do not reach a minimum similarity threshold are discarded. 2. If the table does not contain two entries for the same attribute, the process finishes and the table contains the valid associations between form logical fields and attributes. 3. If the table contains more than one entry for the same attribute, we choose for each attribute the entry with a higher similarity but trying to guarantee that no logical field with an entry above the threshold is left unas-f 1 as an additional option for a 1, we would make the assignments a 1 X  f 1 and a 2 X  f 2 instead of the sole assignment a 1 X  f 2.

If we obtain matches for all the searchable attributes of the type, then the form is an  X  X xact match X  (as long as the validity test is also passed; recall Section 3.1 ). If we obtain matches for some (but not all) searchable is a  X  X artial match X  (as long as the validity test is also passed). 3.1.2. Determining if the response pages obtained through a form are valid
Once a form has been found relevant and we have learned how to execute queries on it (or when we are analyzing the obtained response pages to determine if they are  X  X esult listing X  pages to the issued queries.
Our method to perform this test is based on the observation that, in most sources, the results to a given query are similar when that same query is repeated some time later. For instance, if we search in an Internet bookshop for the books containing the word  X  X ava X  in their title, the obtained results will be similar when we repeat the query a few hours later. Of course, the list of returned books may be slightly different, and the val-in the new response pages. Based on this observation, at this stage we perform the following steps: 1. We issue a subset of the queries { q 1 , ... , q n } using the candidate form and the previously found matches between its logical fields and the searchable attributes. As a response, we obtain { p the response page obtained with q i . the string values forming each individual result. We consider the process successful if we find at least 10% of the expected values 3 in the new pages. We have heuristically deduced this value from our experiments.
While this value may seem too low, it is enough to achieve a very high accuracy (the probability of finding result values only by chance is quite small), while still allowing the process to be successful when the sources have a very high data variability with time (e.g. news sites). 3.1.3. Multi-step forms and login/password forms
Multi-step forms . In some sources, issuing a query may involve filling in several forms through several con-secutive pages. For instance, an electronic music shop could present the user with a first query form to specify the albums of the chosen artist so that the user may specify one of them.

To deal with these situations, when processing a form, if the system finds suitable form logical fields for only some of the searchable attributes, then it fills those fields in and submits the form. In the page obtained, the system looks for a form with logical fields matching the remaining searchable attributes.
Authentication forms. Many sources include authentication forms that must be filled in before any further navigation on the site can take place. Our approach consists in including a built-in type to deal with authen-tication forms. This type includes  X  X  X earchable X  X  attributes such as LOGIN and PASSWORD along with common aliases for them. Given this type , the system can detect and fill in these forms using the same tech-niques we have described for query forms. In this case, the values stored from previous queries contain access credentials. Credentials are stored in encrypted form by the system. 3.2.  X  X  X ext interval X  X  sequences
Since many Web forms paginate their query results, wrappers typically need sequences to browse through the successive result intervals. Our method for regenerating such sequences is based on the following observations:  X  In many sources there exists some navigation step (typically an anchor) pointing to the  X  X ext X  interval of results. This navigation step is usually associated with a text such as  X  X ext X ,  X  X ore results X  or similar.  X  In other sources,  X  X ext interval X  navigation steps are presented as an ordered list pointing to all or some of the available intervals (e.g.  X  X 11-20 ] [21-30 ] ...  X ,  X 1 2 3  X ,  X  X A-B ] [C-E ] ...  X ).

According to these observations, we perform the following process to regenerate these sequences: 1. We search for navigation steps that are located visually close to strings with high textual similarity with a set of customizable texts such as  X  X  X ext X  X ,  X  X  X ext results X  X ,  X  X  X ore results X  X ,  X  X  X ext interval X  X , etc. 2. We also search for sets of navigation steps verifying all the following requirements:
Then, we evaluate the candidate navigation steps discovered in steps 1 and 2 (we give preference to those obtained in step 2). To do this, we first use the regenerated  X  X uery sequence X  to obtain the first  X  X esult list-ing X  page from some of the stored queries (we select queries with a higher number of results, since we are interested in queries requiring pagination). Secondly, we execute the candidate navigation steps in the pages and analyze the new obtained pages to check if they contain more stored query results that are different from the ones contained in the first  X  X esult listing X  page (this can be checked using the same method described in Section 3.1.2 ). If this process is successful, we consider we have found the  X  X  X ext interval X  sequence. 3.3. Maintaining the  X  X ore detail X  sequence
It often occurs that the pages containing the query result listings do not show all the desired data fields about each individual result, and the wrapper must access one or more  X  X  X etail X  X  pages that show the complete data. The method we use to regenerate  X  X  X etail X  X  sequences is based on the following observations:  X  The navigation steps leading to detail pages are visually associated with the item they refer to (e.g. an  X  For every result, the HTML navigation elements used to access the detail pages are located in the same relative position within each result. We say two navigation elements from two different results are in the same position if:  X  Their path in the DOM tree of the response page is the same. The tag (for instance, h a i or h select i ) must  X  Both elements appear in their results in the same relative position with respect to the attributes of the
Based on these observations the method we use is: which HTML fragment corresponds to each result and where each attribute is located. 2. Searching the HTML fragments corresponding to each result for sets of navigation elements (each set must contain at most one navigation step from each result) verifying the previous observations. 3. For each set obtained in the previous step, we execute its navigation steps, obtaining the pages they point to, and we check if they contain the values of the attributes of the result associated with the navigation step. 4. The set verifying the above requirements and with a higher score in step 3 is chosen. 4. Experiments
To measure the effectiveness of our approach, we performed two sets of experiments:  X  We tested our system with real changes by monitoring 30 Web sites during four months. We selected sources from different domains such as e-commerce, patent information, news sites or databases of public sector R&amp;D projects.  X  Furthermore, we tested our system by simulating more changes in the following way: we used data collected by the wrapper of one source belonging to a specific domain (e.g. source Amazon in the domain  X  X nternet bookshops X ) and used it to generate the navigation sequences needed in another source from the same domain (e.g. AllBooks4Less ).

The next subsections detail each of these sets of experiments. 4.1. Experiments monitoring real changes
In this set of experiments, for every Web site, we first generated a wrapper and used it on a daily basis to execute different queries. The system automatically stored some of the queries and their results, as mentioned in previous sections. When a change in the navigation sequences of the Web site was detected, the system tried to repair it. Then, we tested the wrapper with a new set of queries and checked if they were executed successfully.

Table 1 shows the Web sources (14 of 30) that experienced changes during our study period. For each source, we show the searchable attributes we defined in its wrapper, the number of changes produced (#CH), and the results of the regeneration process for each kind of navigation sequence: query sequence (QS), next interval (NS), and  X  X ore detail X  (DS). NA means the wrapper did not need that sequence.  X  X  X  means the system correctly regenerated the sequence.  X  X  X  means the system generated an incorrect new sequence.  X  X  X  means the system was unable to regenerate the sequence, providing no output.

The process was successful in every case except in the source FreePatentsOnline , where the system incor-rectly identified the query form field corresponding to the description attribute. Although the form-parsing for both the attribute name and its aliases. Therefore, the system tried to find a suitable field by generating  X  X  X andidate matches X  X , as described in Section 3.1 . The incorrect candidate match assigning the description attribute with the form field corresponding to the claims attribute (not used by the wrapper) passed the validity test (described in Section 3.1.2 ), because the keywords appearing on the description of some patents also appeared on its claims, and, therefore, there was an overlap between the results of the queries using each attribute.

To deal with these cases in the future, we can modify the way we evaluate candidate matches: instead of choosing the first match passing the test, we can evaluate all possible candidate matches for the attribute and, if several pass the test, choose the one with a better score in it.
 A limitation of this set of experiments is that, in our period of study, the wrappers experienced few changes. Only three of the monitored sources changed twice while the rest changed only once or did not change at all.
In the sources that changed twice, the system was as successful the second time as it was the first one. Nev-ertheless, to ensure that the effectiveness of the proposed techniques does not degrade as successive changes occur, it would be interesting to extend the period of study until the sources suffer a higher number of changes.
This is part of our future work. 4.2. Experiments simulating changes
In the second set of experiments, we simulated changes on the sources as follows: we used data collected by the wrapper from one source belonging to a specific domain (e.g. source Amazon in the domain  X  X nternet bookshops X ), and used it to generate the navigation sequences needed in another source from the same domain (e.g. AllBooks4Less ).

We proceeded as follows. Firstly, we grouped the sources in pairs. For these tests to be possible, the sources in each pair must verify certain pre-requisites:  X  The results provided for the same query by the two different sources must show substantial overlapping.
For instance, in the case of two Internet bookshops, when searching for books with  X  X ava X  in their title, some books will be returned in both bookshops. This pre-requisite is needed so that the validity test still holds.  X  Both sources must have a common subset of searchable attributes.

Once the pairs were chosen, we generated a wrapper for one of the sources (we say this source plays the origin role in the process). The searchable attributes of the wrapper were the common searchable attributes in both sources.

Then we used the wrapper to execute some queries on the origin source. Once we had executed a few que-ries, we used the information collected by the wrapper to try to generate a new wrapper for the other source (we say this source plays the destination role in the process). The process for generating the new wrapper is the same we have explained in the previous sections: the only difference is that the  X  X hanged X  source provided as input is actually the other source in the pair. Finally, for each pair, we reversed the roles of the origin and destination sources and repeated the process.

Table 2 shows the source pairs we used, along with the considered searchable attributes and the obtained results. The third column in the table shows the results obtained for each kind of sequence when considering the first source as origin and the second as destination , while the fourth column shows the results obtained when considering the first source as destination and the second as origin .

As can be seen, the process was successful in every case except in the pair Yahoo People/Big Book . The reason for the failure was that the sources used different formats to represent attributes such as person names and addresses: for instance, while Yahoo People uses the  X  X ame Surname X  format for the person name attribute (e.g.  X  X ohn Doe X ), BigBook uses the  X  X urname, Name X  format (e.g.  X  X oe, John X ). These for-mat discrepancies made the validity test fail, since the overlapping of results did not reach the threshold.
Notice that, in this case, the system does not produce an incorrect regenerated sequence. Instead, the system produces no output.
 Similar situations occurred in other sources, although, in those cases, the threshold was reached anyway. For instance, the exact same difference in formatting names occurred with the author attribute in the pairs
Amazon/Used.addall and Barnes &amp; Noble/Used.addall . In the pair FreePatentsOnLine/Espacenet , the patent numbers were also represented differently.

This suggests that our current system may fail if the source changes the way it formats the values of some of changes in the visual layout of the source will not usually imply changes in the format of the database infor-mation), we are considering using text similarity measures when searching for stored values during the validity test. 5. Related work
Wrapper generation has been an active research field for years (see, for instance, [10,11,13,20] ; [15] provides a brief but comprehensive survey).

Some aspects of the maintenance problem have also been addressed in the literature such as wrapper ver-ification [12,16] and the maintenance of data extraction programs [16,18,19,23] .

The automatic generation of data extraction programs (without user-provided input examples) has been of data extraction programs, since examples are not required, and the only task of the maintenance system is to annotate the extracted results.

Nevertheless, none of these previous systems addresses the maintenance of the navigation sequences required to reach the pages containing the desired data. Therefore, these efforts are complementary to our work.

Our techniques for regenerating the  X  X uery sequence X  leverage on previous techniques for  X  X  X arsing X  X  a query lem has been addressed in works such as [5,14,26] and we have developed our own techniques, which are described in [1] .

The works developed to access the hidden web, such as [1,4 X 6,9,17,22,26] , also have some relation with our work. Refs. [5,6,9] follow a metasearch approach. They identify several hidden web sources and automatically build a unified interface to query them. The main common point between these works and ours are the afore-mentioned techniques for parsing a query form.

Refs. [4,22] adopt a crawling-based approach. In addition of parsing query forms, they also share with our system the need to crawl websites to find forms suitable to perform certain domain-specific queries. That pro-cess has similarities with our crawling-based approach to regenerate the query sequence.

Ref. [22] takes as input a user-defined domain description which includes metadata about the data fields expected in the target query forms. They combine visual distance and text similarity measures to match
HTML forms with these descriptions, automatically determining if a form is relevant for the target task. Their focus is different to ours: while they search for every form relevant to a general domain in order to index the hidden Web, we try to find the particular form used by the wrapper.

Ref. [4] proposes several techniques to perform a focused crawling process to find pages containing web forms in websites relevant to a certain application domain. Again, our focus is different: we look for the par-ticular form used by a wrapper, while they try to locate all the forms in a web site that are related with the target domain.

In addition, none of those systems is concerned about the remaining steps of wrapper maintenance (such as detail pages, navigation through result intervals or data extraction programs). 6. Conclusions and future work
In this paper, we have presented new techniques for the automatic maintenance of the navigation sequences used by wrappers for semi-structured Web sources. Our approach is based on collecting some information during wrapper creation and operation, including some of the executed queries and their results. When the source changes, this information is used as input to a crawling process that searches for a query form that is able to execute the previously stored queries. Once the query form has been found, the sequences for accessing the  X  X ext interval X  result pages and the  X  X etail X  pages are also regenerated, if needed. We have experimentally tested our techniques for a variety of real-world Web sources, obtaining a high degree of effectiveness. Nevertheless, our techniques have some limitations we will address in our future work:  X  We have assumed the wrappers follow a fixed sequence of steps. The wrappers begin executing a  X  X uery sequence X  and then, optionally, use a  X  X ext interval X  sequence to access more result pages and a  X  X etail sequence X  to access more information from each result. While this represents the typical flow followed by a wrapper used for query purposes, wrappers performing other kinds of web automation tasks could follow a different sequence of steps. We plan to extend our techniques to allow modeling a wrapper as a workflow, so that each wrapper may use any arbitrary steps.  X  Since the regeneration process executes sample queries to recognize the new navigation sequences, the tech-niques can only be used with idempotent wrappers (that is wrappers, whose actions do not change the state of the source). Wrappers that perform queries are idempotent. Nevertheless, if we extend our models to deal with other kind of wrappers, this will be an important issue to consider.  X  Finally, the techniques to regenerate the query sequence look for forms allowing queries for the  X  X  X earchable attributes X  X  of the previous wrapper. An interesting issue is discovering new attributes that may have appeared in the source after the change, thus extending wrapper functionality.

As mentioned in Section 4.1 , it is also part of our future work to perform experiments with our techniques during a larger period. The objective is to ensure that the effectiveness of the techniques does not degrade as successive changes occur in a source and the wrapper is regenerated after each change. Acknowledgements This research was partially supported by the Spanish Ministry of Education and Science under Project
TSI2005-07730 and the Spanish Ministry of Industry, Tourism and Commerce under Project FIT-350200-2006-78. Alberto Pan X  X  work was partially supported by the  X  X  X amo  X  n y Cajal X  X  programme of the Spanish Ministry of Education and Science.

References
