 Knowledge sharing communities , such as Wikipedia or Yahoo! Answers, add greatly to the wealth of information available on the Web. They repre sent co mplex social ecosystems that rely on user participation and the quality of users X  contri butions to prosper. However, quality is har der to achieve when knowledge sharing is facilitated through a high degree of personal interac tion s. The individuals X  obje ctives may change from knowledge sharing to socializing, with a profound impact on the community and the value it delivers to the broader population of Web users . In this paper we provide new insights into the types of content that is shared through Commu nity Ques tion Ans wering (CQA) services. We demons trate an approach that combines in-depth content analysis with social network analysis techni ques . We adapted the Undirected Inductive Coding method to analyze samples of user questions and arrive at a c omprehensive typo logy of the user intent. In our analysis we focused on two types of intent, social vs. non -social , and define measures of social enga gement to characterize the users X  par ticipation and content contribu tions. Our approach is applica ble to a broad class of online communities and can be used to monitor the dynamics of communit y eco -system s.
 H.1.2 [Information Systems] : User/Machine Systems  X  human factors ; H.3.5 [Information Systems] : On -line Information Ser vices .
 Experimentation, Human Factors , Measurement Q&amp;A Communit y, Question Typology, User Intent, Social Scores . Community Question Answering (CQA) services have gained wide adoption over recent years, pr oviding a community approach to question answering. Unlike automatic question answering sys -tems (e.g., TREC Q&amp;A Track [ 17]) or expert networks (e.g., allexperts.com or justanswer.com), answers are provided by a large community o f users who can actively engage in answering any question, irres pective of their level of expertise. Still, the answer quality can sometimes reach , or even surpass , the quality of answers given by library reference services and experts [11]. Furthermore, the promptness of the answers is very attractive to users, even more so when users seek advice or opinion s, which are unlikely to be obtained through standard Web search. This type of social media generates a rich and evolving know -ledge base , valuable to the broader population of Web users. Search engines have already started to surfac e CQA content, when deemed relevant to user s X  queries. However, given the diver sity in content quality [14], it is imp ortant to develop methods for identify ing relevant questions and answers [ 1, 2, 8] and rank ing them accordingly [ 3, 15]. At the same time, it is important to under stand how affordances of the CQA services reflect upon the inte raction among individuals [ 13] and what can be done to enc ourage the exchange of desirable content . Inevitably, the CQA eco -system depends on the exper tise of community members it attracts , the community respon siveness to ques tions, and the nature of users X  interactions. Thus, it is important to gain a good under standing of the community behavior and indi viduals X  contri -bu tions , e.g., by analyzing properties of the network represen -tations that capture question response patterns [ 1, 2]. In our research we begin with the hypothesis that each question reflects a particular intent and therefore instigates a specific type of engagement by the community. Thus, characte rizing the user intent is an important step towards understanding th e community as a whole. For that reason we devised a method to analyze ques -tions and arrive at a comprehensive typology of user intent. In this paper, we primarily focus on the impact that social behavior has on CQA communities and thus consider two broad classes of user intent, social vs. non -social . More precisely, all the questions that are intended for purely social engagement are considered social , and those that seek information, advice or opinion are considered non -social but instigating a knowledge sharing engagement . In order to characterize the communication patterns around social and non -social intent, we combine content analysis with social network analysis and develop measures of social engagement that quantify the users X  participation and cont ent contributions. In the following section we discuss related work and , in section 3 , we provide the relevant background about CQA communities . In section 4 we describe the methodology for deriving a typology of user intent and , in section 5, we use the typology to analyze the types of questions asked and answered by the top content contri -butors in two communities , Yahoo! Answers and MSN QnA . In section 6 , we present the results of automatic classifica tion of question s and we classify about half million questions from MSN QnA to characteriz e the level of social vs. non -social engagement in that community. We conclude with the sum mary of our work and directions for future work. CQA services include a social networking component , where by use rs may create ties to other users through asking and answering questions and establish a reputation in the community for their content contributions. A t the same time, CQA services offer a vast and evolving knowledge base of questions and answers . They are a valuable resource for users with similar information needs. Re -cently , the research com munity has been actively looking into various problems associated with th is type of communities. That includes research on identifying and predicting the quality of answers [ 1, 2, 8] and user satisfaction [ 10], learning to rank answers [ 3, 15], modeling network evolution [ 9], and modeling the user authority and level of expertise [ 4, 12]. Agichtein et al. [ 2] proposed a classification model for estima ting the quality of answers in Yahoo! Answers based on features that are derived from the content and the answer -to social network , e.g., the authority measures. They classified q uestion -answer pa irs along several aspects, including how well -formed, readable, useful, and interesting they are. Further work on characteriz ing CQA content was done by Adamic et al. [ 1] by focusing on three specific topic categories from Yahoo! Answe rs. The three categories were selected among the 189 most active Yahoo! Answers categories , as the best representatives of the 3 clusters obtained by k -means clustering . The clustering considers three primary dimen sions: thread length , given by the avera ge number of answers, text length , given by the average number of characters in the answers, and asker/replier overlap , obtained by the cosine similarity between the asking and replying frequency of users. From the patterns in user interactions, they ident ified users with behavior similar to the  X answer -person X  role s found in newsgroup communities [ 18]. Bian et al. [ 3] presented a ranking framework for retrieving factual information from social media that utilizes data about user interaction to retrieve high quality content . Harper et al. [ 11] probed various Q&amp;A servi ces, including Yahoo! Answers and MSN QnA , to assess the value of answers provide d by each of these communities. They defined a set of probing questions according to three generic categories , factual , opinion and advice , posted them to each service , and subsequently analyzed the quality of received answers . Our work complements the ex isting research by focusing on the user intent, as reflected in the user X  X  questions, and the social engagement, rather than the quality of the content exchanged. CQA services provide a platform for users to engage through asking and respo nding to each others X  questions. They include features th at facilitate pos ing questions and provid ing answer s, and incen tives to encourage user participation and self -regulati on of the content quality , e.g., through content rati ngs , abuse report s, user reputation scores , and user rankings. Most of the existing research has focused primarily on Yahoo! Answers . W e study and contrast two CQA communities: Yahoo! Answers and MSN QnA . We begin by providing a brief description of the Yahoo! Answers and MSN QnA services and discuss their social network structure. CQA services offer a fairly common set of features . Figure 1 rep resents a typical QA thread with the main entities involved in the question answering process. A question can receive several answers from multiple users during an answering phase . The community is then given an opportunity to provide comments an d to vote on the quality of the received answers. In Yahoo! Answers users can cast both positive and negative votes (thumbs -up and thumbs -down) during the voting phase . They can also provide comments once the best answer has been selected. At the time of posting the question, the user is asked to select one of the topics from the set of pre defined Yahoo! categories. The cate gories are used by other members of the community to find content on topics of their interest . MSN QnA users can perform the same f unctions: ask, answer questions, comment, vote, and categorize questions . However, these are supported in slightly different ways. The user X  X  vote is interpreted as approval and candidacy for the best answer. The users can comment on any answer at any time during the question lifecycle. This flexibility opens up opportunities for richer user interactions during the answering process, which results in a different structure for the QA threads. Finally, at the time of posting the question, the user is asked to assign a set of tags that best describe the ques tion. The service suggests a set of candidat e tags from a pool of community generated tags but the user can choose to create new tags as appropriate. CQA services also provide search and browsing facilitie s so that users can explore recently posted questions and answers . In Yahoo! Answers browsing is supported through a topic taxonomy and each question is a ssociated with a single topic. I n MSN QnA , content is described through tag ging which leads to ever -growing non -hierarchical, community generated categoriza tion scheme. The tagging approach enables browsing through questions by topic but is also conducive to creating tags that can cover questions of social nature [13]. Such are questions with social intent, e.g., announcements , greetings , celebrations and news, person al questions, and similar. CQA includes three main types of user interactions that lead to implicit social networks: (1) answer to other users X  questions, (2) comment on other user s  X  answers, and (3) vote on other users X  answers. The answer -to network , in particu lar, represents the pri -mary form of user interaction on CQA services. We analyze this social network to identify structural features that are useful for predicting question types and user intent. We represent the net work as a directed graph,  X  G A = ( V , E ), where a node u  X  V ( G denotes a community member and a directed edge ( u , v )  X  E ( G indicates that user u answered a qu estion from user v . We use N ( u ) t o denote t he set of in-neighbors of u and N out -neighbors of u (see Figure 2). The in-neighbors of u are users who have responded to questions from u , and the out -neighbors are users whom u has responded to. More precisely, N { w  X  V ( G A ): ( w , u )  X  E ( G A )} and N + ( u )={ v  X  V ( G Furthermore, we use d  X  (u)=| N  X  (u)| and d + (u)=| N the in -degree and out -degree of u , respectively . La ter, in section 6.2 , we use these notions to characte rize the social engagement of individual users . We calculate separately the edge density for the in-neighbor and out -neighbor sub -graphs. More precisely, we compute the clustering coefficient of the in-neighborhood C and out -neighborhood C + (u) : We collected and analyzed data from both Yahoo! Answers and MSN QnA services.
 YA . The Yahoo! Answers dataset was obtained by seeding a crawler with pages linked to the top leve l categories. Each category page list s recent questions assigned to th at category and its sub -categories. Over 95% of the content included in our dataset was created during a three month period, from March to May 2008. It comprises 309,599 questions, posted by 217,615 distinct users and 1,151,453 answers, provided by 195,869 distinct users. On average 72.5% (  X  17.1%) of users active in a spec ified day answered questions, 55.2% of users only ans wered questions and 14.9% of users both asked and ans wered questions. Our crawl does not include all the questions and all the answers posted within th ose three months . Thus, we are working with a partial representation of the answer -to social network (see network statistics in Table 1). QN A. The MSN QnA dataset spans the first year of the service , starting with its release in Sep tember 2006. The complete dataset consists of 488,760 questions, 1,330,819 answers , and 901,752 comments. The questions were posted by 241,616 distinc t users, while the answers and comments were contributed by 42,941 and 34,068 distinct users, respectively. On average, 45.5% (  X  16.2%) of users active in a day answered questions, 8.2% of users only answered questions and just 9.6% of users both asked and answered questions. In this case, the dataset is a full snapshot of the service and thus, the answer -to social network is complete (see Table 1). Figure 3 shows in -and out -degree distributions for t he answer -to social network s from both datasets. We see that most users provide answers to very few other users and , similarly , receive answers from very few users . This indicates a low level of invol -vement in ans wering questions . A question posed by a CQA user reflect s a specific intent. The user may ask for advice or wish to instigate a debate, or learn about other members of the community. We manually inspected over five thousand question s from Yahoo! Answers and MSN QnA . From the preliminary analysis it was apparent that some questions requested factual infor mation , some sought advice , and others requested opinions (section 5 provi des further details about the questions analyzed) . These three broad types were also acknowledged in [ 1, 2] and used to develop questions for the study in [ 11]. However, it was also apparent that a fair number of ques tions , especially on MSN QnA, were posted to engage with other community users through informal conver sations, as typi -cally occurs in online forums an d chat rooms. In this section, we describe the method we used to characterize question types and we summarize the main outcomes.
 Figure 2 . Ego -centric network of user u and the sub -graphs of Table 1 . Answer -to network statistics: T D no. of days present in the dataset, V no. of nodes (i.e. users), V A an d V Q without incident and outgoi ng edges, respectively, E no. of Figure 3 . Distributions of in -degree and out -deg ree for the YA (top) and Q N A (bottom) answer -to social network datasets. In order to achieve a systematic characterization of question types, we used an Undirected Inductive Coding (UIC) metho d [16] to capture the intent behin d each question. We performed a qualita -tive content analysis of questions sampled from both com muni -ties , defined the codes that described the intent, and develo ped a detailed typo logy 1 of intents that emerged organically from the data. The UIC method involves identifying and assigning to each ques -tion a set of codes that characterize the question. New codes are generated every time a data item cannot be covered by the exis -ting ones . Eventually the coding scheme stabilizes. The second stage involves the reduction of codes by identifying com mo nality and thus, grouping the codes and corresponding data examples. The re are two advantage s of this approach. First, we can evolve the nu mber of dimensions by simply adding new code types and refining the taxonomy as needed . The overhead is in making sure that the previously pro cessed questions are tagged with new codes. However, it is assu med that codes are generated exhaustive ly when p rocessing the questions. Thus, the need for retrospective work is expected to be minimal. Second, since the coding is done at the atomic level , i.e., starting with basic concepts, we can flexibly define higher level categories by combining the appropriate codes. This is in contrast with the common approach where higher -level categories are defined first and the labeling of data needs to be repeated whenever a different perspective is taken. There are many different per spectives that one can take when coding the content. Similarly, there are different ways in which one can synthesize the codes into higher level categories and arrive at the typology with reduced dimensions. We focus on the users X  objectives and the type o f infor mation request conveyed by the question . For example, for the question  X  Name all the presi -dents of the United States ?  X  we cap ture ( 1) the intent of satisfying the questioner X  X  information need and (2) the particular type of content, i.e., obtaini ng an objective response , such as facts that are verifiable or commonly accepted as knowledge by the society . More specifically, we group codes along the following dimen -sions:  X  Personal vs. General perspective  X  the answer is expected  X  Community vs. Individual issue  X  the question is directed to  X  Social vs. Non -social intent  X  the objective of the question These groupings lead us to eight main question types on which we decided to focus our analysis of user intent (see also Table 2):  X  Factual Information (FI)  X  the question is a r equest for http://research.microsoft.com/pubs/79403/QCodes.pdf .  X  General Advice (GA)  X  request for advice or recommen - X  Personal Advice (PA)  X  request for advice that involves  X  General Opinion (GO)  X  requ est of opinion about a general  X  Personal Opinion (PO)  X  request for opinion about a stance  X  Chatting (C)  X  the question is a vehicle for the user to chat  X  Entertainment (E)  X  the user intent is to entertain the com - X  Other (O)  X  the question is not formulated as a question, Table 2 . Definition of question t ypes along 3 dimensions: 
General vs. Personal perspective, Community vs. Individual issue, and Social vs. Non -social intent.
 The question typology that we developed emerged from manual annotation of random samples of questions from both datasets (YA and QNA). The questions were coded and labeled by 2 annotators  X  the inter -annotator agreement was m easured using the Kappa statistic [ 5], which indicat ed substan tial agreement :  X  =0.761. The labels that were assigned consist of the 8 main question types described in the previous section.
 Figure 4 shows the overall distribution of question s per type after labeling 2000 questions from each dataset. We found that in both samples , factual information and general advice were the predominant question types. However, we also observed a considerable n umber of chatting questions in the QNA sample (~15%), indicating that users of this community often contribute content with social intent . Such questions are often personal or useless outside of their original context (e.g. , time sensitive:  X  How X  X  the weat her in your home town today?  X  ), wh ich makes them poor contributions to the question answering knowledge base . Nonetheless, they strengthen the ties among core users who regularly visit the service and are likely to be central to esta -bli shing a sense of co mmunity. The challenge lies on strik ing the right balance between social and non -social interactions so that all the users can benefit from the service, including th e majority of one -time users who come with a question and expect quick and quality response s. Based on our analysis of the sample questions w e raised the hypothesi s that users X  activity level s (i.e. , number of content con -tributions) and social network ties might be reflected in the types of questions they ask and answer . To investigate this hypo thesis, we ranked users based on the overall number of question s and answer s contribut ed to the service, and selected the top 5 contri -butors from each dataset for further analysis , as we present next . The level of users X  activity in CQA can be measured by the number of individual content contributions (i.e. questions, ans -wers and comments posted). We focus this analysis on the top 5 contributors of questions and top 5 contribut ors of answers in each dataset. From the whole dataset we randomly sampled 50 ques -tions and 50 answers ( including the respective questions) , from each of those users . For users who posted less than 50 questions we took all that were present in the dataset . This resulted in a total of 1 ,446 questions , 537 from YA and 909 from QNA . The ques -tions were then manually labeled with respect to their type by the same annotators as before . In QNA , two of the top answerers were also among the top questioners, while on YA some of the top answer ers did not post any question s with in the time span of our sample. Table 3 and Table 4 present the distribu tion of manually assigned labels to: questions answered (A) and quest ions asked (Q) by the top answerers and the top questioners.
 Type of Content . We note that YA top contributors engage mostly in questions that seek factual informa tion and general advice . For example, most of the contributions from YA user u adv ice on digital cameras, with many of the answers being referrals to camera -related websites, not offering direct solutions to the pro blem as such. In contrast, the predominant question type in which the top QNA contributors engage in is chatting , indicati ng strong partici pation with a social intent . However, they 
Table 3 . Top 5 users from YA with the most answers ( u
Table 4 . Top 5 users from QNA with the most answers ( u
Figure 4 . Distribution of question types across the samples of also make contri butions on other question types. Interestingly , some QNA users ( e.g. user s u 1 , u 4 and u 5 ), primarily ask chatting questions but answer to various question types, including factua l information ones. Thus, the QNA dataset presents rich data for gaining new insights into social vs. non -social interactions in CQA. Social Engagement . Figure 5 shows the evolution over time of ego -centric network structure for QNA users u 1 , u terms of : (1) in-degree, out -degree, neighborhoods overlap, and (2) in-and out -clustering coefficients, for the full time span of our dataset ( T D =397 days) . In all cases w e observe a high overlap of in-and out -neighbors, whic h indicates high reciprocity in question answering . For example, about half of the users who responded to user u 1 also received answers from u 1 . Such high reciprocity indi -cates that users are acquainted with many of the users who respond to their question s. Furthermore, their acquaintances are likely to be connected among themselves, consider ing the relati -vely high values and st abilization of the in -clustering coefficients . In real networks the clustering coeffi cient of a node tends to decrease with its degree [ 9]  X  that is also the case here . However, we see periods when there was increase of C  X  ( u ) or relatively steady densification of the in -neighborhoods, despite the increase of d  X  ( u ). For example, about one month after u 1 community there was an increase in their C  X  ( u ), followed by steadiness or a slow decrease . C  X  ( u 4 ) was also relatively stable two months after user u 4 joined the community. Note that the initial sharp oscillations of the clustering coefficients are due to the low degree values  X  e.g. for a degree of 2, a single edge between the 2 neighbors results in a clustering coefficient C = 0.5; the arrival of a new neighbor can decrease it to C = 0.16. In summary, the social network properties of users wh o predo -minantly ask Chatting questions indicate reciprocal ties and connectedness among acquaintances. These findings prompted us to analyze the whole community with respect to the types of questions posted to the CQA services. For that we first train ed auto matic classifiers for selected classes of questions using manually labeled data. We then use d them to classify the full set of questions and analyze the social network structure for all the users in the QNA community. These experiments are described in the next section. CQA services can only thrive if individuals who join the com -munity have their information needs satisfied and are willing to contribute by helping others to meet their needs. They cultivate and rely upon the users X  se nse of community. As suggested by the findings in the previous section, the users build social ties and it is important to understand how different question types affect the social network structure. To that effect we investigate the influence of question s with social vs. non -social intent (section 6.1 ) on the community social structure (section 6.2 ). We took the manually labeled samples of questions from YA and QNA and g rouped them based on the intent: questions labeled as FI, GA and PA types are part of the non -social training data set, and PO, C, and E are part of the social training set. Since the general opinion type (GO ) included both social and non -social questions and accounts for a small percentage of questions in the training dataset, we did not attempt to train a clas sifier to pre dict such type of questions. As part of the training process, we represented each question by a vector of representative features. We defined the features by consider ing various e ntities associated with CQA threads (see Figure 1): the question itself, information about user tags assigned to the ques tion, characteristics of the answers and c omments, properties of the users involved in the thread , and similar. The complete list of features we used in our classification experiments is presented in Table 5. The first set of features refers to Question properties. We mod el the question text as a bag -of-words feature vector, and include information about its length and the presence of URLs in the text. We take a simple classifier with the Q TEXT feature s as the baseline for other experiments. The second set of features desc ribes the properties of the CQA thread. The Thread Features include the number of answers and comments given , the number of users involved in the thread, and similar. We also include aggregate statistics about the usage of tags in the QNA dataset and the usage of topic labels in YA (see Tag &amp; Topic Features ). Building on previous work [13], that classified questions from QNA onto the Yahoo! Answers topic hierarchy, we consider both the QNA tags and the automatically assigned Yahoo ! top ics.
 Finally, we take into account properties of the answer -to net work that characterize interactions of a given user with the rest of the CQA community. Given that the network is directed and that a user may exhibit distinct questioning and answering beh avior, we consider the se two type s of interaction s separately . We use similar network metrics as in the previous section: in-and out -degree , overlap between in -and out -neighbors , and clustering coefficient s of in-and out -neighbor hoods . To capture the dy namic properties of the social network , we calculate these metrics at the time the interaction occurred . Consequently, the same user will have diffe -rent scores depending on the timestamp of the question.
 We conducted a comprehens ive set of experiments with linear SVM classifiers [ 6, 7] to investigate feature sets that are effective in pre dicting the two classes : social and non -social . We applied one vs. all approach for multi -class classification using SVM as the binary classifier. In order to account for imbalance between the positive and negative class in our data sample s, we modified the SVM cost function to increase the penalty for misclassifying soc ial questions. For each class, w e ranked the classified ques -tions based on the SVM score and calculated the break -even -point (BEP) for the ranked list (i.e. , the rank at which the p recision and recall are equal). To assess the performance of the classifi ers, we performed 5-fold cross -validation on the training dataset.
 In Table 6 and Table 7 we detail the classification results with individual feature sets and several combinations of feature sets for the QNA dataset . We also performed classification experiments on the YA dataset but, as ex pected, the small number of represen -tative questions of the social type was not sufficient to train a classifier for this class. The tables show P, R, F1 calculated a s set precision, recall, and F1 measures, and BEP scores determined from the ranked lists of classified questions. We note that the baseline performance of the classifier for the non -social class is quite high. This is partially due to the relatively small data sample and the imbalance in the number of questions in the social and non -social classes. Nevertheless, we can observe the relative contribution of different feature types and use that as a guide for further work in question classification.
 Classific ation of non -social questions  X  The use of content features in addition to thread properties  X  The use of simple social network features considering the  X  Combining network features with the content and thread 
Classification of social questions  X  The baseline classifier (F1) perfor ms well in comparison  X  It is interesting to observe that the classifier F4TOPIC that It is worth noting that the Yahoo! topics associated with the social questions in QNA span 93 categories, among which the most frequent are Entertainment and Music , Games and Recrea tion , Education and References , and Polls and Surveys. This insight may be helpful in understanding where social engagements may happen within Yahoo! Answers. We applied the binary SVM classifiers with the best performing feature set (F1, F3, F6 Q +) to the full set of que stions from QNA comprising almost 0.5 million questions . This resulted in 84.5% questions assigned to the non -social class and 6.5% assigned to the social class . The remaining 9.0% of questions were not assig -ned to either class. Table 8 lists the total number of questions and respective answers and comments for assigned to each class. A semi -supervised learning approach could have been used to leverage the large proportion of unlabelled data in our dataset and possibly yield incr eas ed classification accuracy. However, we do not expect that would significantly impact our analysis . We were able to perform a small scale valida tion of our results by using the manually labeled questions as test data . The classifiers perfor -mance is gi ven in Table 9, showing high precision for questions of the social type. Next, we present analysis of the social network structure considering the breakdown of the full set of QNA questions into the two classes . Analysis of the classified questions reveals that 16.4% of the total answers and 30.5% of the total comments in the full dataset belong to social question threads. This is quite significant considering that such questions only represent 6.5% of the total in the dataset. Furthermore, questions assigned to the social class receive on average more answers and more comments than the Table 8 . Questions, answers and comments per question class. 
Figure 6 . Ratio of social questions, answers and comments, ones assigned to the non -social class. Over time, the fraction of social questions posted to the service has increased not iceably, as did the number of respective answers and com ments (Figure 6). This indicate s that the MSN QnA com munity ecosystem is evolv ing in a way that encourages interac tions of a social nature. In order t o quantify the use rs level of social engagement w e define a social score , S ( u ), as the ratio of social vs. non -social content contributions to the CQA service. S ( u )  X  1 indicates th at user u predominantly contributes content with a social intent. We analyze the social score of the all the users in the community from ques tio ning ( S Q ), answe ring ( S A ) and commenting ( S ves , i.e. considering each type of contribution (no. questions, no. answers and no. comments) separately. Figure 7 shows scatter plot s of these social scores for the whole QNA community ( i.e., the 241,616 users) . Points above the horizontal dotted lines correspond to users who predominantly ask social questions, and points to the right of the vertical dotted line correspond t o users whose answers (top plot) or comments (bottom plot) were predominantly given to social questions. We observe that there is a minority of users , shown in red , who contribute social content (less than 100). However, among these users are some of the t op contributors (see social scores of users shown in Figure 5). Table 10 contains the number of users, n , on each quadrant, and the average in -degree, out -degree, neighborhood overlap, and the in-and out -clustering coefficients. We can see that users who generate more social questions than non -social ones ( S on average a fairly high in -and out -degree. We also note a relati -vely large overlap of in-and out -neighbors, which indicates that these users establish reciprocal ties with other users , who answer ed their questions at least once . Furthermore, we observe a large C  X  ( u ) for users who answered and commen ted prima rily on social questions, and posted primarily non -social ques tions. In Figure 8, w e take a closer look at the two clustering coefficients metrics, C  X  ( u ) and C + ( u ). In plot (a) we show the average clustering coefficients for user s of a given degree , considering the whole comm unity . We observe a significant difference in these two metrics for users of low degree (see distance between C in red, and C + ( u ), in blue) . This is due to the fact that more than 82.2% of users post very few answers or even none, resulting in very l ow out -degree and clustering coefficient score close to zero , which bring s the average down quite significantly.
 In p lots (b) -(d) we show the same two metrics for subsets of users, which we clustered based on the social scores . Plot (b) refers to users who primar ily contribute non -social questions, answers , and comments. It is interesting to observe that the exclusion of users with high social score from this plot leads to a sharp decrease of the average C  X  ( u ) for users of degree equal to 2 (points on the l eft of the chart indicated with a circle) . This implies that these users primarily receive responses from users who were removed from this view, i.e. , users who engage in social interactions.
 Plot (c) refers to users who predominantly ask questions with a social intent and plot (d) to users who primarily answer social questions. We observe that in (c) users with very high degree have on average a relatively high C  X  ( u ) and a much lower C + ( u ) (see 
Figure 7 . Scatter plot of the social score s of the whole QNA community ( 241,616 users). Top: S A vs. S Q ; Bottom: S
Table 10 . Average in -and out -degree s , neighbors overlap, and clustering coefficient s for users with a given social score , for question ( S Q ) , answer ( S A ) and comment ( S C ) contributions.
Figure 8 . Average clustering coefficients of in -and out -neighborhoods , C  X  1 and S C  X  1 ; ( c) and users with S Q  X  1 , (d) users with S points with degree  X  X  11K ). Since users with such degrees do not appear in plot (d) this implies that the top contributors who ask social questions are also those who respond to many users who ask non -social questions. These findings suggest that the core par -ticipants of t he MSN QnA community have social interactions with one another, while at the same time provide answers to non -social questions generated by the rest of the community. Our findings suggest that the approach of combining the analysis of social network struc ture with the understanding of the user intent offers a promising framework for studying online know -ledge sharing communities. Modeling the entire social ecosystem of a CQA service or any other online communit y, is challenged by the scale, t he comple xi -ty of the user interactions, and the dynamic nature of these servi -ces. Thus, it is essential to diversify the analysis and develop methods that enable us to probe into different aspects of such com muni ties . In this paper we present a meth od for analyzing the nature of the interactions among individuals in CQA communities, focus ing on the user intent that is reflected in their questions . Our approach consists first of developing a typology of ques tions with respect to the user intent, through an iterative manual method . The typology can be used both for higher level analy sis (e.g. , for distinguishing between social and non -social questions) and for focus ing on a very specific (lower -level ) user intent . This represents a new con -tributio n to the research commu nity that can support future studies of similar knowledge -sharing commu nities.
 We demonstrate the use this typology to investigate features that would be useful to identify socially intended questions in CQA . Using a fine level characterization of question types into factual information, general advice, personal advice, general opinion, personal opinion, chatting, and entertain ment , we analyzed samples of question from Yahoo! Answer s and MSN QnA, to gain insights about the com munity as a whole and its most active community members. We developed effective SVM classifiers to distinguish between social and non -social questions and use them to analyze the entire social network of the MSN QnA community. The same classifiers could b e used in search applica tions to ensure that the service primarily surfac es the non -social content. Second, we define new metrics, the social scores of individual users, to complement the social network metrics and analyze the community behavior. The social scores and the socia l network metrics consider questio ning and answer ing activities separa tely , which enable s us to inves tigate the pre sence of social ties among the most active users. The answer to our research question:  X  X ocializing or know ledge sharing? X  in CQA communities is thus  X  both  X  . Indeed, the CQA users do engage in information sharing and they do socializ e. Our methodology can be extended to include the level of users X  social engage ment and can offer new insights that can be exploited by the online services in various ways. Examples include monitoring the community, devising incentive mechanisms to promote quality contributions, defining user reputation, or recommending content to individuals , so that users interested in socializing could easily find social questions and those who are topic experts could be guided to respond to informa tion or advice seeking questions. [1] Adamic, L. A., Zhang, J., Bakshy, E., and Acker man, M. S., [2] Agichtein, E., Castillo, C., Donato, D., Gionis, A., and [3] Bian, J., Liu, Y., Agichtein, E. and Zha, H. Finding the right [4] Bouguessa, M., Dumoulin, B., and Wang, S., Identifying [5] Cohen, J. A Coefficient of Agreement for Nominal Scales. [6] Cortes, C. and Vapnik, V. Support vector networks. Machine [7] Text Gar den , Grobelnik, M. and Mladenic, D., available at: [8] Gyongyi, Z., Koutrika, G., Pedersen, J., Garcia -Molina, H., [9] Leskovec, J., Backstrom, L., Kumar, R., and Tomkins, A. [10 ] Liu, Y., Bian, J., and Agichtein, E. 2008. P redicting informa -[11 ] Harper, F. M., Raban, D., Rafaeli, S., and Konstan, J. A., Pre -[12 ] Jurczyk, P. and Agichtein, E., Discovering authorities in [13 ] Mende s Rodrigues, E., Milic -Frayling, N., and Fortuna, B. [14 ] Su, Q., Pavlov, D., Chow, J., and Baker, W. Internet scale [15 ] Surdeanu, M., Ciaramita, M. and Zaragoza, H. Learning to [16 ] Thoma s, R. A General Inductive Approach for Analyzing [17 ] Voorhees, E. M. 2001. The TREC question answering track. [18 ] Welser, H.T., Gleave, E., Fisher, D., Smith, M. Visualizing 
