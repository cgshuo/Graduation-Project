 cumscription of life online and offline faded, and uploads nearly all the information to [1]. To solve this problem, numerous methods have been proposed: from information activity records, tap the users X  preferences, then automatically find the matching con-tent from the mass of information and make right recommendation [2,3,4]. 
Nevertheless, the mainstream of recommender systems are troubled by cold start little chance to be selected [5]. So, it is difficult to give users precise recommendation. This defines the so-called cold start issue. If this issue could be resolved satisfactorily, much more personalized Web services. 
Lots of remarkable related works, which inspired us, try to solve this issue in dif-ferent field, but the works have their limitations:  X  It has been proved that new users are more likely to choose the popular items [6], which indicates that the hot-selling list can do something for cold start issue. This method could meet the basic need, but it X  X  helpless for further requirements.  X  Some researchers believe the extensive application of labelling systems could solve the cold start issue [7]. Because labels indicate the content of items and preference solve the issue properly. But it is also helpless for sheer cold start, these users have labeled nothing.  X  There are many other constructive solutions, including mining the multidimension-al data on overlapped social networks [8,9] and transfer learning algorithm [10]. R. 
Sinha et al. point out that users prefer recommendation from friends rather than the recommender systems [11]. Social impact is considered more important than the similarity of historical behavior [12]. Social recommendation from a friend can in-crease the popularity of items and their reviews [13], which could compensate the lack of ratings. However, these solutions X  performance on cold start issue is usually unstable [14 ,15 ]. person X  X  basic background information. Then they use their rich life experience to do the personality-recognition work. At last, th ey start a conversation you are interested in. They feel like old friends. This phenomenon inspired us. The recommender sys-recommendation job precisely. dataset. the proposed background-based semi-supervised tri-training method (BSTM); Section work. 2.1 BBPM On Internet, collaborative filtering methods have been applied to many recommender between the predicted ratings and real ratings in the labeled training set. movie information and user attrib utes into consideration [3]. attributes of users X  background, respectively. Generally speaking, when the users rate and vice versa. Therefore, we can quantify the impact of the movie genre by  X  respectively. The parameters can be obtained by stochastic gradient descent algo-rithm. 
In order to achieve continuous improvement on resolving cold start issue, we need to consider more factors. The more factors been taken into consideration, the better of the recommendation. 
Apart from calculating the impact (as shown in equation (1) &amp; (2)) of movie genre and users X  background information, we fo cus on accurately characterizing the mutual Adding these factors to model construction makes the model much more comprehen-sive and integrated. We built such a model called BBPM (short for background-based websites and make more precise recommendation. 
Life always inspires us: the elderly with abundant experience tend to have a good knowledge of youths X  thoughts; old friends are able to interest each other for knowing each other X  X  hobbies; and so on. They can make it, because they know each other very much more precise. 
Therefore, we add more users X  background factors into BBPM. For example, users X  age which can be classified into seven intervals. Users who are in different age inter-val demonstrated certain age-related disposition and aesthetic taste. And users X  occu-age, occupation and sex. These factors really matter for discovering users X  tastes. 
Firstly, the mutual impact of users X  age and sex, is a significant factor for predict-males usually like Westerns. Thus, the model can be written as follow: 
Secondly, users X  occupation and sex also can produce a chemical reaction. Such as: the Living Body"; while female doctors may prefer the interesting drama "Grey's Anatomy". Therefore, the model evolves: whose sex is s . 
Finally, the mutual impact between age and occupation may manifest like this: a prentice young civil servant could be attracted by Machiavellian stories in movie "The like "Forrest Gump". Thus, we finish the comprehensive model: occupation o . 
We can adopt stochastic gradient descent (SGD) to train these parameters. Since [  X  we can obtain the model BBPM by minimizing the recommendation errors (equation (6)) on the entire training set. In the above equation,  X   X  ui represents the movie ratings predicted by BBPM, || X || repre-sents the Euclidean distance of corresponding parameters, and  X  is the regularization mately acquire various parameters of BBPM via training samples one by one, and gradually update the parameters. 2.2 BSTM We proposed a background-based semi-supervised tri-training method (referred to learning process. Try to exploit both the labeled data and unlabeled data to optimize work, according to Bagging method and users' background information, we formulate diversified models. Then we integrate the multiple models to improve the accuracy of recommendation. More specifically, BSTM involves three main steps: 
Firstly, formulate three regressors. Recommendation is a regression issue, so we formulate three regressors ( these regressor-labeled samples are used to optimized other regressors. 
Secondly, tri-training. During this process, three regressors learn from each other: the samples predicted ratings), and then use those samples to train other regressors. grated according to certain pr inciple, thus receive the ultimate recommendation. To be more intuitive, we give the corresponding algorithm flow chart as follow: 
This is the specific technical route of our work. We construct BBPM (background-integrate the recommendation results of all regressors. set, this scheme is represented by  X   X  ; the other is to take different factor into consid-eration by operating background attributes, this scheme is represented by  X   X  . 
A common method to build multiple training subset is Bagging, which randomly select n samples from the original training set to form new training subsets in each run learning. The favorable diversity among regressors is conducive for the performance of joint training [17]. The diversity of regressors depends on the deviation of training samples. In addition, there should be enough samples in the subsets to ensure consid-erable accuracy of the regressors. But too many samples can damage the diversity of take equation (7) as fundamental learner for formulating the regressors. Then we train the learner on three diverse training subsets and get different regressors. They cooper-ate during tri-training process. This is the  X   X  scheme.  X   X   X  X   X  X   X  X  X  X   X  X   X  X  X   X  X   X  X  X  X  (7) into multiple points. Intuitively, users with different pairwise attributes do show differentiated interest. The above regressors A/B/C characterize the mutual impact of different pairwise back-ground attributes (age/sex/occupation), respectively. This is source of regressors X  diversity. These regressors would be trained on the overall training set to ensure con-siderable accuracy, which is differ from  X   X  . This is the  X   X  scheme. regressors, which means getting regressors by training models (A/B/C) on training subsets. We call this hybrid scheme as  X   X  X  X  . We've got multiple regressors, a following work is tri-training different regressors. would be iteratively optimized by their peer regressors. Here comes the problem, how to select samples from the un-rating dataset  X   X  to found the teaching dataset? We do this work under the criterion of data confidence. Confidence indicate samples contri-bution to the accuracy of the prediction. Ho w to describe the confidence? The accurcy of recommendation increases with the rising number of rating data. So, the confidence of regressors X  predicted rating is defined as follows: Equation (11) expresses the confidence on a recommendation made by regressor r for  X  movies are watched more frequently, the confidence of the data is higher. The accura-cy of recommendation derived from these data could be better. 
Furthermore, confidence (11) can be improved as the following form: particular type of movie and activeness of specific users. For example,  X   X  X  expresses the popularity of "Documentary" movies,  X   X  X  X  expresses the popularity of "Western" and O={  X   X  ...  X   X  X  } 21 occupations, respectively. (associated with the confidence of regressor  X   X   X /  X   X /  X  ) expresses the rating number of Therefore, the confidence of the three regressors can be very different. But the confi-semi-supervised learning process. So it is not comparable.  X  is full of popular movies and active users. We abandon to directly take confidence pick samples for each teaching dataset. The probability of selecting a sample is: The final step of the solution is to integrate the prediction of three regressors. We take confidence as the integration weight. And the final regressor is: used. As mentioned above, we use a public significant dataset (Movie Lens) to validate our solution. In order to verify the robustness and efficiency, the solution is accomplished 50,000 ratings of 1,466 movies awarded by 489 users; there are 100,000 ratings from 943 users on 1682 movies in the second one; The third one contains 200,000 ratings of 3,266 movies awarded by 1429 users; and 1,000,209 ratings of approximately ferred to  X   X   X ,  X   X ,  X   X ,  X  . The data states the ratings with users' background information and movie genre. implement BSTM via operating the background attributes. Specifically, Movie Lens datasets depict users X  background informat ion, involving age, occupation and sex. In the experiments, the significant ratings were partitioned into three subsets: (1) valida-and meta-parameters adjustment; (2) training set possess 80% of ratings, which coop-model; (3) test set encompass the rest, and we give the solution X  X  results (RMSE) on it, as shown in table 1. 
In our experiments, the accuracy of the recommendation results is indicated by the root mean square error:  X  result. We compared different solutions: firstly, we validate the state-of-the-art algo-rithms; than we validate the solutions proposed recently; at last, we validate our work on Movie Lens data. 
Specifically, the solutions are:  X  the classic and common used user-based k-nearest neighbor collaborative filtering algorithm, short for UB k-NN;  X  item-based k-nearest neighbor collaborative filtering algorithm (IB k-NN);  X  the factorization-based collaborative filtering (CF) method in [16];  X  a functional matrix factorization (fMF), which extract new users' preference with a decision tree by progressively querying user responses through an initial interview process [18];  X  in the work [19], discovering latent factors from movies genres (LFfMG) based on a factorized matrix;  X  CSEL, the main compare method in the work [3].  X  BSTM, our background-based semi-supervised tri-training method. Experiment results show that the overall recommendation performance of our solution is better than others. Besides the solution X  X  overall recommendation accuracy, we also investigated its performance on cold start issue. BSTM take advantage of the mutual user by the times of watching. And then divide users into 10 groups by users' activity. At last, we measure RMSE separately on each user group. And the RMSE show us the performance of the solutions on cold start issue as the following Table 2: 
In Table 2, Bin(u)1 is the group of most active users; Bin(u)10 is the group of most inactive users. The property of these algori thms (k-NN, CF  X  X  X ), additionally, make it because the performance of these solutions relays on the similarity of users or items. While, the result reveals the good efficiency of our solution on cold-start issue. 
The regressors X  collaboration is one of the key in our solution. In the experiment, expressed in section 2.2. We give each regressor a subset of the training set. The sub-set contains unlabeled samples, and the corresponding regressor will give all the sam-subsets are used to train one regressor, except the subset that labeled by this regressor. This is the process of multiple regressors X  collaboration. 
We managed to study the effect of different numbers of regressors X  collaboration, and have showed the result in Fig. 2. X axis represents the numbers of regressors and different collaboration. The experiments reveal that three is the appropriate number of be implemented, but the accuracy is dissatisfactory. While more than three regressors X  more than three, the accuracy is lower compared to the accuracy of three regressors X  collaboration. issue in Recommender Systems. Firstly, we proposed the BBPM to address the lack of ratings problem by bringing the mutual impact among the attributes of users X  back-ground information into account. Then, we explore a semi-supervised tri-training method to involve the unlabeled samples. The experimental results show that our solution improves the overall system performance. This solution makes a remarkable progress in solving the cold start issue. However, the instantaneity requirement is still great challenge of our solution, which can be considered as the future work. The pos-sible solution includes, selecting the most influential users to reduce the sample size, using parallel training method to accelerate the training procedure, and so on. 
