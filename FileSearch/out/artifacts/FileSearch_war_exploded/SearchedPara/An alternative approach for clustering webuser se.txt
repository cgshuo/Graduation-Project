
Indian Institute of Management, Indore, India Indian Institute of Management, Lucknow, India 1. Introduction
With the advent of digital technology, the generation and capture of web related data has become possible. Web data can be analyzed on three dimensions namely structure, content and usage [1,2]. Analysis of web usage data can provide useful insights related to user browsing behaviour. Data mining techniques commonly used for web usage mining include, association rule generation, sequential pattern generation, clustering, and classification.

Association rule mining techniques [3] are used to discover correlations between items found in a large database of transactions. In the context of Web Usage Mining a transaction is a group of web page accesses by web users, with an item being a one page access. An example of association rule found from an IBM analysis of the server log of the official 1996 Olympics web site is  X  X f a visitor accesses a page about Indoor Volleyball then the visitor also accesses a page on Handball in 45% of the cases. This pattern is present in 0.23% of the transactions X  [4].

Discovering sequential pattern is to find patterns such that the presence of a set of items is followed by another set of items in the ordered transaction set. The transaction set may be ordered with respect to time, space etc. [5]. Again, taking the example from IBM X  X  report on Olympic database an example of sequence pattern is  X 5.77% of the site visitors accessed the Atlanta home page followed by the Sights and Sounds main Page X  [4].

The generated association or sequential pattern rules from the web usage data of a user has potential applications in areas such as web personalization, recommendation system and intrusion detection sys-tem. Web personalization has been used to design web recommendation system [6,7]. The core step in designing an effective recommender system is to identify the group of products and interested user set. Clustering is used to identify these groups. The web pages which come from one domain set leads to the formation of a dense region. The main challenge in clustering web pages which exhibit the dense nature is to figure out a dense region and separate the noise data from the dense region.

Various clustering algorithms for dense data have been proposed in literature like Density based spatial clustering of applications with noise (DBSCAN) [8] Ordering point to identify the clustering structure (OPTICS)algorithm [9], DENsity-based CLUstering algorithm (DENCLUE) [10], Improved DBSCAN (IDBSCAN) algorithm [11] and Density Differentiated Spatial Clustering (DDSC) algorithm [12]. The philosophy of these algorithms in forming clusters is to find density variance present in the data set. All the above mentioned algorithms separate regions of high density from regions of low density. The high density regions are termed as clusters while separated low density regions are recognized as noise points. The density based clustering generates clusters in an evolving manner.

All these techniques assume that the web user can belong to one and only one group. But in real life applications, it is not so. Consider a scenario when a user is searching for a data mining book for his research work, as well as a travel book since he has to attend a conference in USA. The system analyzing his browsing behaviour should be able to put him into two categories, namely data mining interest group and travel interest group. The above mentioned algorithms will put the user in one and only one group considering the frequency of highest visited pages or binary representation of dataset i.e. visited or not visited web pages. Incorporation of soft computing based clustering techniques with sequential pattern discovery measures may be helpful in forming such types of web user groups.
 Rough set is a soft clustering technique [13] which deals with ambiguity and vagueness present in data. Rough set can be approximated by two crisp (definite or non-vague) sets termed as upper approximation and lower approximation. These sets are derived using the similarity among the objects. Similarity upper approximation can be used to derive incremental similarity among the objects. Incremental clustering can result in deriving arbitrary shaped clusters hence it can be utilized in dense data clustering where dense regions within the sparse regions can be found.

In this paper we have incorporated the rough set based similarity upper approximation clustering tech-nique [14] with sequence based similarity measure [15] to form dense clusters. In the proposed clustering framework, initially soft clusters are formed. In subsequent steps based on the strength of data points hard clusters are generated.We compared our technique with well established dense clustering algo-rithm DBSCAN [8]. The cluster quality obtained using our approach is better with respect to DBSCAN clustering algorithm.

The rest of the paper is organized as follows. In Section 2, we present a review of literature related to our work. Section 3 discusses modified sequence data clustering using similarity upper approximation. Section 4 presents experimental results and discussion. Conclusion and future work has been discussed in Section 5. Reference section has been presented in Section 6. 2. Related work
In this section, we review the recent literature related to sequence data clustering and rough set based clustering.
 2.1. Sequence data clustering
Sequence mining is an area which deals with finding the useful patterns from the sequence data sets and understanding their behaviour. Analysis of sequential data has become increasingly important since it has been found in many areas as in biological sequences, text documents and web access logs.
Classical clustering algorithms (as k-means) fails to give good results for sequential data sets as it is difficult to compute the pair wise similarity between the sequences [16]. However in most of the cases pair wise similarity computation happens to computationally complex having at least quadratic expression on the number of sequences. Hence, it can be applied only on small data sets resulting in to scalability problem for sequential data.

Sequences can be represented using various schemes as usage based (UB) representation, frequency-based (FB) representation, viewing-time based (VTB) representation and visiting-order based (VOB) representation [17].

Different set of techniques have been used for sequence mining which includes association rules, frequent sequences and frequent generalized sequences [18]. Association rules consider the problem of finding association among visited web pages similar to finding association among item sets in transaction databases.

Yang and Wang [16] have developed a clustering technique for sequences, based on their sequential features. Clustering technique faces problem in case of sequential data (in categorical domain) due to the lack of any efficient similarity measure. They have proposed a new model CLUSEQ for sequential clustering using significant statistical properties possessed by the sequences.

Guralnik and Karypis [19] have proposed a new technique for sequence clustering that does not require an all-against-all analysis and uses a near linear complexity k-means based clustering algorithm. The proposed approach finds a set of features that capture the sequential nature of the various data-sequences. Further it projects each data sequence into a new space where dimensions are identified features for the data sequence. Traditional k-means algorithm has been used for clustering of the data-sequences after the transformation.
 Kum et al. [20] have proposed a sequence mining algorithm termed as ApproxMAP (APPROXimate Multiple Alignment Pattern mining) using approximate sequential pattern mining which deals with iden-tifying patterns approximately shared by many sequences.

Sayed et al. [18] have suggested a novel algorithm  X  X S: Miner X  for discovering frequent patterns in sequence databases. FS miner requires only two scans of the database.Kumar et al. [15] proposed a new similarity measure for sequential data termed as S 3 M and utilized it for clustering of sequential data using partition around medoids (PAM) algorithm. The clusters formed were crisp in nature. S 3 M happens to be a non-vector-based similarity measure for clustering web user sessions which consider content and sequential information both while clustering web pages. A new clustering algorithm SeqPAM has been developed by Kumar et al. [21] for sequential data. 2.2. Rough set based clustering
Many researchers have utilized the concept of rough set in their work. Lingras [22 X 24] has focused on rough set clustering for web mining. Paper has described unsupervised classification using rough sets along with genetic algorithms to represent clusters as interval sets. Indiscernibility based clustering has been proposed by Hirano and Tsumoto [25,26] which is able to deal with relative proximity of data points. Rough approximation has been applied for clustering of web transactions through web logs [14,27]. Various attempts have been performed to model the uncertainty and vagueness present in data during clustering with the help of integration of fuzzy and rough sets [28 X 30].

Rough set has been used to capture the inherent uncertainty involved in clustering. Asharaf et al. [31] have developed an incremental clustering approach for interval data using rough sets. The technique assumes that it is possible to consider data points one at a time and assign them to existing clusters. Any new data item is assigned to a cluster without looking at the previously seen patterns. The proposed incremental algorithm scales well with the size of the dataset.

Mohebi and Sap [32] have applied rough sets to self organizing maps (SOM) and proposed a two-level clustering based on SOM with rough sets. Rough set theory has been utilized to capture the inherent uncertainty involved in cluster analysis. In the first stage SOM is used to develop the prototypes by training the data with SOM neural network. Clustering has been performed in the next step using rough set based incremental clustering technique.

Kumar et al. [33] have proposed a novel new indiscernibility-based rough agglomerative hierarchi-cal clustering algorithm. The indiscernibility relation has been extended to a tolerance relation with the transitivity property being relaxed. In their approach the initial clusters are formed using similarity up-per approximation. Subsequent clusters are formed using the concept of constrained-similarity upper approximation where a relative similarity is used as a merging criterion.

Kandwal et al. [34] have proposed rough set based clustering using active learning approach. They have extended the concept of Hamming distance and propose a dissimilarity measure which helps in finding the approximations of clusters in the given data set. The paper has utilized rough set theory for clustering. The proposed approach has been tested on bench mark data set from UCI machine learning repository and produced favourable results.

Trabelsi et al. [35] have presented two classification approaches using rough sets (RS) which learn decision rules from uncertain data. It has been assumed that the uncertainty exists only in the decision attribute values of the decision table (DT) which has been represented by the belief functions. Belief Rough Set Classifier (BRSC) is the first technique which used basic concepts of rough sets. The second technique happens to be more complex and based on Generalization Distribution Table (BRSC-GDT), which is a hybridization of the Generalization Distribution Table and the Rough Sets (GDT-RS). Both classification techniques try to simplify the uncertain decision table (UDT) to create significant decision rules for classification process. A heuristic method based on rough sets has been used for attribute selection and reduction of the time complexity. The performance of the proposed classifier has been evaluated with the help of experiments using bench mark real world datasets where uncertainty in the decision attributes has been introduced artificially.

Yanto et al. [36] have used variable precision rough set model for clustering. They have applied rough set clustering for clustering of students suffering from anxiety. They utilized the mean of accuracy of approximation using variable precision of attributes. Data had been collected through survey to find the anxiety sources among the students. The paper had shown the way to use variable precision rough set model for grouping.

All the above mentioned clustering techniques do not consider the order information present in the sequence data. These techniques first convert them into frequency domain or absence/presence of data within the sequence. Treating sequences in such a way may lead to loss of ordering information.
In this work we have utilized rough set based similarity upper approximation algorithm with S 3 M similarity measure which considers sequence information besides content information.
 3. Clustering sequence data using similarity upper approximation algorithm
With the advances in technology, there has been a rapid growth in the volume and complexity of electronic data being generated and stored. As a result of this increase, the task of extracting meaningful knowledge in the form of patterns, relationships and groupings to be used in applications such as decision support, prediction and anomaly detection has become machine intensive and essential. Furthermore, the need to discover underlying data structures in mixed attribute data calls for efficient data analysis with minimal human intervention.

Cluster analysis is one such task which helps us to figure out the hidden relationships/group. In clus-tering task the main objective is to find the natural groups of similar data points. Clustering techniques group data points which are similar to each other. The similarity has been decided on the basis of the sim-ilarity measures. The value of similarity among the data points varies with respect to type of similarity measures.

When dealing with sequence data researchers use to transform the sequences into structured data by using frequency encoding or binary encoding. Sequence mining suffers due to the lack of suitable similarity measures for finding similarity between sequences.

The main challenge in dealing with sequence data and using above mentioned technique is that we lose the information related to order of occurrence. In this paper we have combined the sequence clustering algorithm with similarity upper approximation to keep into account the above challenges associated with web data.

The objective of current paper is to figure two things. First, whether the sequence information available within the data set can help in forming better clusters. Second, how to modify the similarity upper approximation based clustering technique for dense data.

Web users X  traversal may be within a web site or may correspond to different types of interrelated web sites. Our study is limited to study the traversal behaviour of web users within the web site. For large number of web users and limited number of web pages a dense region is generated. For such type of dense web domain, compact clusters are the desired output. The generated clusters will be validated on the basis of their compactness.

Compactness of any cluster can be defined as the degree of similarity of grouped users. Intra cluster distance indicates the density of the objects within the cluster that means how dense it is packed. For a good clustering algorithm, the intra-cluster distance should be minimum and inter cluster distance should be maximum. The inter cluster similarity refers to the separation among the various clusters. In our work the separation of the clusters is not an important aspect as we are looking for dense clusters, which can be ensured by intra cluster similarity. The compactness will be calculated on the basis of the intra-cluster similarity of the clustering scheme. More the intra cluster similarity more the compactness of the clusters. We will validate the clusters on the basis of the compactness of the obtained clusters.
The rough set theory is mathematically simple and quite useful in data mining areas. Rough set theory is widely used tool in information retrieval, decision support, machine learning, and knowledge based systems. A wide range of applications utilize the c oncepts of rough set theory. Medical data analysis, aircraft pilot performance evaluation, image processing, and voice recognition are a few examples.
Let U be the collection of web user sessions and is non-empty set containing n user sessions denoted as { x 1 ,x 2 ,x 3 ,...x n } . Each user session comprises of web page visits. Let D be a similarity matrix [ D ] web user sessions is computed using S 3 M measure. S 3 M measure considers both the content as well as order information while computing similarity between web user sessions. Once the similarity matrix has been computed the initial set of clusters ar e formed using simila rity upper approximation. Definition 1 [33] Given a threshold value  X   X  (0 , 1] , for any element x i  X  U ,where U is the non empty set consisting of web user sessions, the similarity upper approximation with respect to x i , { x of the corresponding cluster.

Thus, from above definition it is clear that elements in the non-empty set U can be partitioned into a family of overlapping sets {
From the generated set of cluster family formed due to definition 1, only one set will be taken if two sets A and B are equal (consider set A and set B were two sets out of several generated sets). Also, if set A is the proper subset of set B then consider only set B. Thus, considering only unique and proper supersets from the cluster family set formed from definition 1 a new set family will be generated with reduced size denoted as { x
However, the family of sets { such a partitioning an element should be in only one partition.

After forming the cluster due to first similarity upper approximation a web user session will be a member of more than one group. Such objects are referred as ambiguous objects. These ambiguous objects may be made fully definable that is, to find with which set the element exactly belongs. While forming the second similarity upper approximation we calculate the strength of the ambiguous web user session with all the sets to which it belongs. The strength of the web user session is calculated using Definitions 2 and 3 as given below.
 Definition 2. For an object x  X  C 1  X  U and then the lower approximation of set C 1 due to object x is given by where, x  X  C 2 ,C 3 ,...C n .
 Definition 3. For any element x  X  C i ,let where, i the total number of sets intersecting due to the object x , j is the element in the lower approxi-
The greater the value of relationship of an object with the cluster X  X  lower approximation the object will reside in that cluster. To explain approach consider two sets A and B with a common object x d .  X  R c denotes the intersecting region between set A and B. following conditions. 1) For x d  X  2) For x d  X  3) For x d  X  4) For x d  X  Applying the above condition the user web sessions which belongs to more than one cluster are assigned to only and only one cluster. The resultant cluster set is now crisp in nature. The strength of our approach is that initially we assign a web user session in mul tiple groups. Later as the a lgorithm pr ogresses the web user sessions are moved to the cluster where its strength is higher. To compute the strength of membership we have used both the concept of lower and upper approximation.

In our work, we compute the similarity using various similarity measures. S 3 M similarity measure is weighted linear combination of sequence and set similarity measures. To compute sequence similarity we have utilized the length of longe st common subsequence (LLCS) and Longest common subsequence (LCS) [37]. The set similarity is computed using Jaccard similarity measure [38].

We have outlined the algorithm for forming clusters from sequential data using similarity upper ap-proximation as below Begin Step1: For two web user sessions x1 and x2  X  T call function Sim (x 1 ,x 2 ) Step2: For a given  X   X  (0 , 1] form the first similarity upper approximation.
 Step3: Remove proper subsets.
 Step4: Identify the cluster centres, if exists, and remove them from other clusters .

Step5: Apply definitions 2 and 3 to get the strength of relationship of all the elements with the clusters to which they belong.
 End Function Sim (x1, x2) Begin End
The upper approximation gives the group of sequences on the basis of specified value of similarity measure. Modified algorithm seems to perform similar to density based clustering algorithm DBSCAN. The nature and type of clusters formed due to modified algorithm is very much similar to that of DB-SCAN clustering algorithm. The various terminology used in DBSCAN clustering algorithm can be mapped to the terminologies used in the similarity upper approximation based clustering technique for example, similarity threshold value (  X  ) instead of concept of neighbourhood parameter (  X  ). Similarity threshold value (  X  ) has been used for finding the upper approximation of groups which results in to clusters.
 Cluster centres are termed as core, elements other than cluster centres are termed as Border Points. The object that is not the part of any cluster is termed as Noise. Mins define the minimum number of points or objects that constitute the cluster.

We compared the similarity threshold (  X  ) of rough set based clustering using similarity upper approx-imation and neighbourhood radius of (  X  ) of DBSCAN clustering algorithm. The increase in similarity threshold value (  X  ) increases the expectation of similarity that exist between the objects present in the same cluster. More the value of (  X  ) more similar objects will be there in the cluster.
In DBSCAN algorithm neighbourhood radius has been used as a distance measure for finding the dis-similarity between the objects. Increased similarity between objects will result in less distance between the objects that are grouped in the cluster, in other terms it will shrink the neighbourhood region (  X  )of a core point. It means with increase in the Similarity threshold value (  X  ), Neighbourhood parameter (  X  ) will decrease.

It is clear from the discussion that there exists a reciprocal relationship between similarity threshold value (  X  ) and Neighbourhood parameter (  X  ). The exact one to one relationship between these two param-and reciprocal of Neighbourhood parameter (  X  ), can be expressed by the Eq. (2)
For two constants A and B a linear relationship between  X  and 1/  X  can be expressed as Assuming the two instances of clustering where  X  =  X  1 , X  =  X  1 and  X  =  X  2 , X  =  X  2 Solving the above equation for two instances, values of A and B can be computed as Putting the values of A and B in the Eq. (3) the linear relationship between  X  and  X  can be expressed as. The instances have been taken for the purpose of illustration of possible relationship that may exist between  X  and  X  .
 4. Experimental results and discussion
This section has been divided in three subsections. In first subsection we provide the description of dataset used for the experiments. In second subsection we report the performance of modified rough set based clustering algorithm using similarity upper approximation with several similarity measures. Finally in third subsection we compare the modified rough set based clustering with the DBSCAN clustering. Clustering algorithms can be classified into various categories as density-based, partitioning, hierarchical, grid-based and model-based [40]. Density based clustering techniques deals with finding dense regions in the data space. DBSCAN happens to be the most prominent algorithm in density based clustering hence we have chosen DBSCAN as a representative of density based clustering.

All the experiments reported in this paper were performed on a PC having an Intel Core 2 Duo Proces-sor (1.83 GHz) with 2 GB RAM using JAVA as programming language on the Windows XP platform. Experiments were conducted on two datasets namely msnbc web navigation [39] and simulated sequen-tial dataset. 4.1. Description of the datasets
The msnbc web navigational dataset has been collected from UCI dataset repository. The dataset consists of Internet Information Server (IIS) logs for msnbc.com web site and news-related portions of msn.com for the entire day of September 28, 1999 (Pacific Standard Time). Each web log is a sequence representation of page views of a web user during that twenty-four hour period. The length of web user session varies from from 1 to 500. The average length of web user session is reported to 5.6, hence for our experiments we have taken only those user sessions whose length is 6. Also comparing a web user session of length one with user session of size 500 may not lead to any useful information. The data set has seventeen categories that are  X  X rontpage X ,  X  X ews X ,  X  X ech X ,  X  X ocal X ,  X  X pinion X ,  X  X n-air X ,  X  X isc X ,  X  X eather X ,  X  X ealth X ,  X  X iving X ,  X  X usiness X ,  X  X ports X ,  X  X ummary X ,  X  X bs X  (bulletin board service),  X  X ravel X ,  X  X sn-news X , and  X  X sn-sports X . We have converted the categories into numbers for the purpose of experiments starting from 1 to 17.

The second dataset used for the experimentation purpose has been simulated over twenty five different categories which are numbered from  X 1 X  to  X 25 X . Each sequence of the dataset is of length six. 4.2. Performance of modified algorithm
The current sub-section reports the performance of rough set based clustering algorithm using similar-ity upper approximation. We have arbitrarily selected 200, 500, 1000, 2000 and 3000 web user sessions from datasets (msnbc &amp; simulated) and formed five different datasets of varying sizes. We conducted our experiments on all these five datasets over different distance/similarity measure namely, Jaccard, Lev-ensthein, S 3 M with different p (weightage to sequence similarity) values. In Tables 1 and 2 we report the number of clusters generated clusters using rough set clustering algorithm for different mins and  X  value over different dataset sizes. Tables 1 and 2 also report the result for different distance/similarity measures.

Jaccard similarity measure is an example of content based non vector similarity measure. It is used to find the relative commonality present among two set s. It is measured as a ratio of number of common Levenshtein similarity measure is a representative of sequence based non vector similarity measure [41]. The Levenshtein distance is also called as edit distance. It is an approximate sequence matching algorithm, that is used to solve the problem of finding sequences p = p 1 p 2 in another sequence s = s 1 s 2 s 3 ...s
If p =  X  X attle X  and s =  X  X ettle X  then the Levenshtein distance between these two sequence will be  X 2 X , and the Levenshtein similarity will be 0.66 (1-(2/6)).

S 3 M measure is a hybrid non vector similarity measure. It is a combination of the content and sequence based non vector similarity measures. S 3 M measure is given as follows: where p + q = 1and p , q 0. Here, p and q determine the relative weights given for order of occurrence (sequence similarity) and to content (set similarity), respectively. In practical applications, user could specify these parameters.

The cluster quality can be measured in terms of number of clusters formed as well as the intra cluster distance/similarity. If any clustering algorithm results in too many clusters it means that the clusters are not densely packed or the data items are sparse in nature. Whereas if the number of clusters is less it means all the data points are put in few clusters thus losing the significance and importance of clustering technique. Both the cases are undesirable hence reaching an optimal number of clusters required is not an easy task rather a domain expert can identify the best optimal number of clusters.

More compact clusters infer grouping of more similar users in a cluster. The compactness of clusters can be measured using intra cluster similarity/distance of clusters. Total Intra-cluster similarity of clus-sents total number of objects (in this case web users) in i th cluster.  X  X  X  represents the total number of clusters generated by the particular clustering scheme. Equation (6) represents the average intra-cluster similarity of all the clusters. Clustering scheme with high intra-cluster similarity provide more compact deals with separation of the existing clusters from each other. In the web applications we are interested in finding the dense domains hence only intra cluster similarity has been considered.
 The inter cluster similarity can be expressed by Eq. (7) total number of clusters.

The clusters have been validated using total intra-cluster similarity. Tables 3 and 4 represent the total intra cluster similarity for datasets of size 200, 500, 1000, 2000 and 3000 of  X  X snbc X  dataset, while Tables 9 and 10 report the same for the simulated dataset. Clusters with highest intra cluster similarity represent the most intact clusters, hence considered as best case clustering. Tables 5 and 6 represents the inter cluster similarity of generated clusters for  X  X snbc X  dataset, which has been calculated using Eq. (7). Tables 7 X 12 represent the experimental results for simulated dataset.

It has been clear from the experimental results that much better (compact in this case) clusters have values of p and q ) which considers both content and sequence similarity present among the uses. For both the Jaccard as well as the Levensthein measure it can be seen that the intra cluster similarity is low and hence does not form good clusters. 4.3. Comparison of DBSCAN with the modified Rough set based clustering using similarity upper
DBSCAN algorithm is used to find dense clusters and Noise. Clusters grow gradually to find dense areas in dataset. The points that are left ungrouped i.e. which are not part of any cluster are termed as Noise points.

The dense regions are separated with the regions of low density. The algorithms group the points on the basis of density gradient. DBSCAN [8] happens to be the most prominent density based algorithm. It connects the objects (data points) in gradual manner. It can find clusters of non spherical shape and defines any cluster as a maximal set of density connected points. It defines two parameters that are Neighborhood defined by radius of Neighborhood (  X  ), minimum number of specified points in a cluster termed as Minpts. Besides that clustering scheme defines three types of points that are Core point, Border point and Noise Point. The c lusters are formed using the conc ept of density reach ability. Core point happens to be the center of the cluster. All the points other than the core point are termed as border points. The points that are not part of any cluster are termed as Noise points.
Rough set is a soft clustering technique. It approximates an imprecise set using two crisp sets that are upper approximation set and lower approximation set. In this paper we have modified the rough set based similarity upper approximation clustering algorithm for sequential data. Initially soft clusters are generated which are further converted to hard clusters using the maximum membership assignment approach. In maximum membership assignment approach if any object is present in more than one clusters then it has been assigned to a cluster for which it has maximum membership value. In our case similarity value with respect to any cluster has been considered as membership value for that cluster.
The paper compares the clusters generated by DBSCAN and rough set based clustering algorithm using similarity upper approximation. Both type of clustering generates hard clusters as an end product. The comparison has been made considering the compactness of the generated clusters. Compact cluster represents the dense region. The more the compactness of the clusters the more dense the region.
The generated clusters are compared in various scenarios by varying different parameters which in-cludes type of similarity measure, threshold of similarity upper approximation, minimum number of points need to be there in a cluster ( Mins ). The experiments are performed on various sizes of the datasets. In Figs 1 X 9 X axis represent the similarity threshold in case of similarity upper approximation clustering algorithm or value of neighborhood in case of DBSCAN clustering algorithm. The Y axis in Figs 1 X 9 represents the density of the best cluster. Figures 1 X 5 represents the results for msnbc dataset while Figs 6 X 9 represents the results for simulated dataset.

The best cluster of both the clustering algorithm has been compared for the compactness. In this case we are looking for compact clusters as it gives more similar data points which may be most desired in case of various e-Commerce applications for finding similar users. Figures 1 X 5 represent the output of the experiments performed for the study. They represent the density of the best clusters and their variation with respect to the similarity threshold (  X  ) /Neighbourhood radius (  X  ), similarity measures and other parameters. It is clear from the results that for the given msnbc web dataset rough set based clustering using similarity upper approximation has produced more compact clusters than DBSCAN clustering algorithm. 5. Conclusion and future work
The paper investigates the effect of incorporation of sequential information during the clustering in dense web domain. It also suggests a new way for clustering in dense web domain. The paper tries to capture and use both sequence and content information in deriving the clusters. We have utilized content based, sequence based and hybrid similarity measures (combination of content and sequence based similarity measures) to see the effect of sequential information during the clustering. Jaccard, Levenshtein and S 3 M have been utilized as a representative of content based, sequence based and hybrid similarity measure, respectively.

The paper uses the rough set based clustering using similarity upper approximation algorithm with a hybrid similarity measure and in particular tested it with S 3 M for dense web domain. The clusters have been validated using total intra-cluster similarity. The more the intra-cluster similarity, more the compactness of the clusters. More compact clusters group the more similar users that are important in e-commerce applications. Rough set based clustering using similarity upper approximation has been used with S 3 M similarity measure (which happens to be a hybrid similarity measure) for clustering and the results have been compared with the DBSCAN clustering algorithm. Results have shown that rough set based clustering algorithm using similarity upper approximation has produced more compact clusters than DBSCAN algorithm for  X  X snbc X  web navigational dataset and simulated dataset. Paper has also presented mapping between the various parameters of the rough set based clustering using similarity upper approximation with the DBSCAN clustering parameters.

The paper explores a novel and different method of clustering in dense web domain and suggests a logical mapping of the parameters of rough set based clustering using similarity upper approximation and DBSCAN clustering algorithm. The suggested mapping can be further explored and validated with the experiments and the values of parameters can be calculated using the experimental results. References
