  X  This paper develops and evaluates an enhanced corpus based approach for semantic processing. Corpus based models that build representations of words directly from text do not require pre-existing linguistic knowledge, and have demon-strated psychologically relevant performance on a number of cognitive tasks. However, they have been criticised in the past for not incorporating sufficient structural information. Using ideas underpinning recent attempts to overcome this weakness, we develop an enhanced tensor encoding model to build representations of word meaning for semantic pro-cessing. Our enhanced model demonstrates superior per-formance when compared to a robust baseline model on a number of semantic processing tasks.
 I.2.4 [ Knowledge Representation Formalisms and Meth-ods ]: Semantic Networks General Terms: Experimentation, Performance, Theory Keywords: Semantics, Tensor Encoding
The field of knowledge management is increasingly recog-nising the need to process the semantics of information. Some of this need is being addressed by semantic technolo-gies, which have originated out of the  X  X emantic web X  [11] into a broad research programme to deliver effective infor-mation processing technologies. These technologies process the  X  X eaning X  of information to enhance the effectiveness of search, data integration, intelligent information systems, business intelligence, and the like [8]. Semantic technologies are poised to have a major impact on information process-ing applications, for example, the increased maturity and  X 
Department of Computer Science, Qld University of Tech-nology, Brisbane, Qld, Australia, 4001  X  Department of Mathematics, Qld University of Technology, Brisbane, Qld, Australia, 4001 adoption of semantic technologies will enable two-thirds of (manual) information management related tasks to be au-tomated[2]. Muchoftheinformationtobeprocessedisin textual form and in this context two core issues in semantic processing will be addressed in this paper: (1) semantic as-sociation: how to compute the semantic relatedness between words and (2) semantic categorisation: how to compute the relevant semantic category a word, or concept belongs. To this end, a corpus-based approach will be adopted.
Corpus based semantic models build representations of words from exposure to streams of natural language. There is a growing body of research that demonstrates that psy-chologically relevant and plausible representations of word meaning can be created within these models [4, 3].
Many of these approaches build semantic representations of words in a geometric space. These semantic space mod-els are often built by collecting word co-occurrence frequen-cies from streams of natural language. Building a model in this way removes the dependency on pre-existing linguis-tic knowledge, allowing these models to be applied across languages. This approach also removes the overhead asso-ciated with linguistically aware approaches, that use manu-ally maintained ontologies or pre-processing of the text using part of speech (POS) tagging [6].

As corpus based approaches learn word representations from a training corpus, the representations are often time and context sensitive. This means corpus based approaches can automatically support changes in word meaning over time (e.g., artificial ), or between contexts (e.g., mouse ). Some critics have highlighted the lack of structural infor-mation used by semantic spac emodelsasalimitingfactor in their ability to emulate human performance [7, 6]. Re-searchers argue that a model that pays attention to both context and structure while learning, stands a greater chance of matching the trends found in human data [3].

A number of linguistically independent, semantic space models have since attempted t o better encode structural information [3, 12]. Of these, the tensor encoding (TE) model provides the ability to model linguistically relevant word associations and has outperformed humans and other like-models on a well accepted synonym judgement task [12].
In this paper we develop and evaluate a new model for semantic processing, based on the TE model X  X  structurally enhanced representations. We found that our enhanced TE model provides superior performance on a number of seman-tic processing tasks when compared to an existing state-of-the-art semantic space model that does not encode structure.
Researchers have argued that the meaning of words can be modelled by comparing the distributions of words within text [10]. A popular approach to representing these word distributions is to collect word occurrence frequencies and place them in high-dimensional context vectors to form a semantic space. This approach allows techniques from lin-ear algebra to be used to model relationships between ob-jects, including semantic associations, within the geometric space. An excellent summary on the developmental of se-mantic space models is provided by [13].

Two of the most well-known semantic space models in lit-erature are HAL (Hyperspace Analogue to Language; [5]) and LSA (Latent Semantic Analysis; [4]). These two mod-els differ in the way they build their context vectors. HAL builds context vectors by storing pre-and post-order word co-occurrence frequencies in a word-by-word matrix. To il-lustrate, consider the HAL matrix (Table 1) produced from the sentence  X  a dog bit the mailman  X , using a sliding context window with radius 2. The co-occurrence information pre-ceding and post-ceding each word are recorded separately by the row and column vectors.
However, as the number of terms in the vocabulary grows the HAL matrix becomes very large. Therefore, it is com-mon to only use the co-occurrence frequencies of the top x most frequent terms in the corpus. There is often a small drop in task performance by doing this. Within HAL based models, semantic similarity can be calculated using both ge-ometric and information theoretic measures, since explicit word co-occurrence frequencies exist. A robust comparison of various semantic distance measures found that a combina-tion of a probabilistic measure, known as positive pointwise mutual information (PPMI), and the cosine metric achieves the strongest overall performance on a variety of semantic tasks [1]. Therefore, we chose to use the cosine of the PPMI vectors within a HAL based approach to form a strong base-line model for this research. This model is referred to as the PPMI model.

In contrast to HAL based models, LSA uses a mathemat-ical technique, known as single value decomposition (SVD), to reduce the dimensions of the context matrix to the k most significant latent concepts [4]. The drawback of LSA is the computational costs associated with the SVD process. Even though models based on LSA and HAL have been shown to simulate human performance on a number of cognitive tasks, it has been argued that these models do not capture concepts such as syntax or achieve other basic cognitive lan-guage abilities [7].
While some corpus based approaches [6] have used lin-guistic knowledge available via POS tagging, to incorporate more structural information w ithin representations, a num-ber of models have tried to encode structural information using geometric operations within the space. These include the Bound Encoding of the Aggregate Language Environ-ment (BEAGLE) model [3] and the tensor encoding (TE) model [12].

BEAGLE builds reduced dimension representations of words using randomly assigned, environment vectors of a fixed length that create an approximately orthogonal basis. The process to encode structural information involves circular convolution ( ), which is a mathematical operator that com-presses the Kronecker (outer) product of two environment vectors. The main drawbacks of BEAGLE X  X  encoding method comes from the cost of the binding process, and the inabil-ity to use information theoretic measures like PPMI. The TE model on the other hand provides an efficient, formal framework for encoding structural information that allows both geometric and information theoretic measures to be utilised. The TE model has also outperformed BEAGLE on a benchmark synonym judgement task [12]. For these reasons, we chose to use the TE model to underpin our en-hanced approach to performing semantic processing tasks.
The similarity measures underpinning the TE model were grounded in structural linguistic theory. These theories state that there are two fundamental relationships between words that create meaning, they are: (i) syntagmatic and (ii) paradig-matic associations.

A syntagmatic association exists between two words if they co-occur more frequently than expected from chance. Some common examples may include X  X offee-drink X  X nd X  X un-hot X . A paradigmatic association exists between two words if they can substitute for one another in a sentence with-out affecting the grammaticality or acceptability of the sen-tence. Some common examples may include  X  X rink-eat X  and  X  X rticle-book X  [9].

The TE model provides a measure of strength of each type of association. The strength of syntagmatic associations be-tween a word q and word w , is measured by finding the cosine of the angle between the geometric representations of q and w . In the second order TE model this simplifies to: where  X  = and f qw is the co-occurrence frequency of word q appear-ing before word w in the vocabulary; f wq is the co-occurrence frequency of word q appearing after word w ,and N is the size of the vocab. This measure produces a higher score if word w occurs more frequently with word q in text, and was shown to effectively predict words that would likely succeed or precede another in text [12].

Within the second order TE model the strength of paradig-matic associations between word q and word w is defined as: where f q is the vocabulary frequency of word q .Intuitively, this paradigmatic measure enhances the score for word w , if q and w co-occur with the same words often, indepen-dent of whether q and w co-occur with each other. This paradigmatic measure outperformed human judgement and BEAGLE on a benchmark synonym judgement task, with the best performance achieved when a context window ra-dius of 1 was used to build the semantic space.

In the original TE model research, these measures were evaluated independently on a number of semantic processing tasks. Based on structural linguistic theory, word meanings are formed by both syntagmatic and paradigmatic associa-tions. Considering many tasks, including semantic process-ing, rely heavily on the meaning of words, there is a strong motivation to create a combined measure using S syn ( q,w and S par ( q,w ). Therefore, our enhanced TE model uses lin-ear interpolation to create a scoring function that combines both, and is defined as: where  X   X  [0 , 1] mixes the syntagmatic and paradigmatic measures. We believe that by providing a single framework to access both forms of linguistic associations, our enhanced TE (ETE) model opens up the range of tasks this model can be applied to. In this research we focus on applying ETE to a number of semantic processing tasks, as the performance of such tasks are intuitively dependent on the meaning of words, and such tasks are likely to play an increasingly im-portant role in future knowledge management research. This enhanced TE model will be referred to as ETE in the re-mainder of this paper.
To evaluate the benefit of having access to both forms of linguistic associations when performing semantic processing tasks, we compare the effectiveness and efficiency of the ETE model with the PPMI model on the following tasks: 1. Semantic distance: The semantic distance task, pre-sented in [1], is a multiple choice task used to test the abil-ity of a word meaning model to detect semantic difference among more common words. This task involves 200 pairs of semantically related words, such as  X  X ing X  and  X  X ueen X ,  X  X oncept X  and  X  X hought X . It is implemented by comparing the semantic relatedness, as defined by each model, of the target word to its pair and 10 other randomly chosen words in the list. The resulting performance is the percentage of control words that are further from the target than its re-lated word. 2. Semantic categorization: This task aims to evalu-ate the ability of the models to classify words within their correct semantic category. Ten words are taken from each of 70 semantic categories (e.g., fruits, sports, colours) based on human category norms [14]. The resulting performance is the percentage of the 700 test words that fell closer to their own category centre (correctly categorized) rather than an-other. The category centres are constructed by finding the mean of the geometric representations corresponding to the words in each category (excluding the target word under consideration).
For all models a stoplist 1 was used to remove high fre-quency, low information terms, often referred to as closed terms, from the vocabulary. The use of stoplists have been
The Lemur stoplist used is made up of 418 words and is provided with the Lemur toolkit for information retrieval research, http://www.lemurproject.org Figure 1: Sensitivity of the PPMI and ETE models with respect to the context window radius, using an ETE  X  equal to 0.1. shown to assist with performance of semantic space models on a wide range of tasks [9].
 The semantic space models were trained using the British National Corpus (BNC). The BNC is a 100 million word cor-pus that is made up of 90% written text with the remaining 10% being transcribed spoken text. This corpus has been used extensively in past research [4, 1]. The PPMI approach was run using the vectors constructed from the 100,000 most frequent terms in the corpus, as recommended by [1].
Both the PPMI and ETE models construct their seman-tic space by moving a triangular context window across the underlying text. A triangular context window gives terms closer to the focus term a higher weighting, with the weight-ing reducing linearly with the distance from the focus term. Figure 1 shows how the performance of the ETE and PPMI models depend on the context window radius. The results suggest that the ETE model achieves optimal performance for a context window radius larger than those used for opti-mal PPMI performance. We believe the slight variations in PPMI performance in our results when compared to [1], may be related to the seed used to generate the random choice of test terms in the semantic distance task and the use of a more recent and extended set of category norms in the semantic categorization task.

Figure 1 also suggests that a context window radius of 4 allows the ETE model to more effectively access infor-mation about syntagmatic and paradigmatic associations to assist with task performance. The influence of each feature in achieving optimum performance can be seen in Figure 2. It appears that the optimal performance of ETE relies more heavily on the syntagmatic feature. This may be the reason why the ETE model performs better with a larger context window. Past research [15] has found that syntagmatic asso-ciations often exist between words farther apart in text than those exhibiting paradigmatic associations. However, given the performance of ETE does not continue to increase past a context window radius of 5, the ability for the syntagmatic feature to excel on its own appears limited.

To investigate whether there are any computational costs associated with ETE X  X  superior task effectiveness, a com-plexity analysis of PPMI and ETE is undertaken. Figure 2: Sensitivity of the ETE model with respect to gamma (  X  ), using a context window radius of 4.
The number of dimensions used to store the geometric representations of terms within a semantic space impacts the models computational complexity. Like [1] we chose to use vectors of 100,000 dimensions in our PPMI based HAL model. The storage complexity of the PPMI and ETE models are proportional to the number of storage vectors | D max || V | ,where | D max | is the dimensionality of the under-lying storage vectors and | V | is the size of the vocabulary. Therefore, PPMI has a storage complexity of 100 , 000 | V | The time complexity of PPMI is based on operation of cal-culating the cosine of the PPMI vectors, and is O ( n )= | D max | = 100 , 000.

The TE model stores the geometric representations in storage vectors using an efficient compression technique out-lined in [12]. The impact of the dimensionality of the storage vectors on the semantic distance and categorization tasks is shown in Figure 3. The results demonstrate that the ETE model X  X  compression technique provides robust performance on both tasks for | D max | X  500. The storage complexity of the ETE model using a storage vector of 500 dimension is 500 | V | . This is 200 times smaller than that of PPMI. Based on [12], the time complexity of the ETE model is: O (
This analysis suggests that the ETE model is able to achieve superior effectiveness for less computational com-plexity when compared to PPMI on both tasks.
This paper has presented a novel corpus based approach to semantic processing that builds on recent attempts to encode structural information within distributed semantic models. Both semantic association and semantic categoriza-tion were studied as these tasks are core to many semantic technologies that are emerging to support knowledge man-agement. Our enhanced TE model demonstrates superior effectiveness on a benchmark semantic distance and seman-tic categorization task when compared to a HAL-like model using the successful PPMI measure.

Given syntagmatic associations often exist between words far apart in text, while paradigmatic associations are best modelled by words found in close proximity to each other, basing the ETE model X  X  syntagmatic and paradigmatic fea-tures off two different semantic spaces may be an area for future research. Figure 3: Sensitivity of the ETE model with respect to the storage vector dimensionality.
