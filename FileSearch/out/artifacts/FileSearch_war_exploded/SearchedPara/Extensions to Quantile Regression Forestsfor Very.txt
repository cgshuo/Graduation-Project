 Nguyen Thanh Tung 1 , Joshua Zhexue Huang 2 , Imran Khan 1 , Mark Junjie Li 2 , Regression is a task of learning a function f ( X )= E ( Y | X ) from a training data L = { ( X ,Y )=( X 1 ,Y 1 ) , ..., ( X N ,Y N ) } ,where N is the number of objects in L , X  X  R M are predictor variables or features and Y  X  R 1 is a response variable or feature. The regression model has the form where error  X   X  N (0 , X  2 ).

A parametric method assumes that a formula for conditional mean E ( Y | X ) is known, for instance, linear equation Y =  X  0 +  X  1 X 1 ,..., X  M X M . The linear regression model is solved by estimating parameters  X  0 , X  1 ,..., X  M from L with least squares method to minimize the sum of square residuals. A nonparametric method does not require that a model form be known. Instead, a model structure is specified, such as a neural network and L is used to learn the model. Linear regression models do not perform on nonlinear domains and suffer the problem of curse of dimensionality. Neural networks are not scalable to big data.
Decision tree is a nonparametric regression model that works on nonlinear situations. A decision tree model partitions the training data L into subsets of leaf nodes and the prediction value in each leaf node is taken as the mean of Y values of the objects in that leaf node. D ecision tree model is unstable in high-dimensional data because of the large prediction variance. This problem can be remedied by using an ensemble of decision trees or random forests [3] built from the bagged samples of L [2]. Regression random forests takes the average of multiple decision tree predictions to re duce the prediction variance and increase the accuracy of prediction.

Quantile regression forests (QRF) represents the state-of-the-art technique for nonparametric regression [7]. Instead of modeling Y = E ( Y | X ), QRF models F ( y | X = x )= P ( Y&lt;y | X = x ), i.e., the conditional distribution function. Given a continuous distribution function and a probability  X  ,the  X  -quantile Q  X  ( x ) can be computed as where 0 &lt; X &lt; 1. Given two quantile probabilities  X  l and  X  h , QRF enables to predict the range [ Q  X  l ( x ) ,Q  X  h ( x )] of Y with a given probability  X  that gression forests can perform well in situations where the conditional distribution function is not in normal distribution.

Both regression random forests and quantile regression forests suffer perfor-mance problems in high-dimensional data with thousands of features. The main cause is that in the process of growing a tree from the bagged sample data, the subspace of features randomly sampled from the thousands of features in L to split a node of the tree is often dominated by less important features, and the tree grown from such randomly sampled subspace features will have a low accuracy in prediction which affects the final prediction of the random forests.
In this paper, we propose a new subspace feature sampling method to grow trees for regression random fores ts. Given a training data set L ,wefirstuse feature permutation method to measure the importance of features and produce raw feature importance scores. Then, we apply p -value assessment to separate important features from the less important ones and partition the set of fea-tures in L into two subsets, one containing important features and one con-taining less important features. We independently sample features from the two subsets and put them together as the subspace features for splitting the data at a node. Since the subspace always contains important features which can guarantee a better split at the node, this subspace feature sampling method en-ables to generate trees from bagged sample data with smaller regression errors. For point regression, we choose the prediction value of Y from the range between two quantiles Q 0 . 05 and Q 0 . 95 instead of the conditional mean used in regression random forests. Our experiment results have shown that random forests with these extensions outperformed regression random forests and quantile regression forests in reduction of root mean square residuals (RMSR). 2.1 Regression Random Forests Given a training data L = { ( X 1 ,Y 1 ) , ..., ( X N ,Y N ) } ,where N is the number of objects in L , a regression random forests model is built as follows.  X  Step 1: Draw a bagged sample L k from L .  X  Step 2: Grow a regression tree T k from L k .Ateachnode t , the split is deter- X  Step 3: Let  X  Y k be the prediction of tree T k given input X . The prediction
Since each tree is grown from a bagged sample, it is grown with only two-third of objects in L . About one-third of objects are left out and these objects are called out-of-bag (OOB) samples which are used to estimate the prediction errors. 2.2 Quantile Regression Forests Quantile Regression Forests (QRF) uses the same method as described above to grow trees [7]. However, at each leaf node, it retains all Y values instead of only the mean of Y values. Therefore, QRF keeps the raw distribution of Y values at leaf node.

To describe QRF with notation by Breiman [3], we compute a positive weight w ( x,  X  k ) by each tree for each case X i  X  L ,where  X  k indicates the k th tree for X = x , the prediction value is
The weight w i ( x ) assigned by random forests is the average of weights by all trees, that is
The prediction of regression random forests is
We note that  X  Y is the average of conditional mean values of all trees in the regression random forests.

Given an input X ,wecanfindtheleafnode l k ( x,  X  k ) from all trees and the set of Y i in these leaf nodes. Given all Y i and the corresponding weights w ( i ), we can estimate the conditional distribution function of Y given X as Given a probability  X  , we can estimate the quantile Q  X  ( X )as For range prediction, we have where  X  l &lt; X  h and (  X  h  X   X  l )=  X  . Here,  X  is the probability that prediction Y will fall in the range of [ Q  X  l ( X ) ,Q  X  h ( X )].

For point regression, the prediction can choose a value in a range such as the mean or median of Y i values. The median surpasses the mean in robustness towards extreme values/outliers. We use the median of Y values in the range of two quantiles as the prediction of Y given input X = x . 3.1 Importance Measure of Features by Permutation Given a training data set L and a regression random forests model RF ,Breiman [3] described a permutation method to measure the importance of features in the prediction. The procedure for computing the importance scores of features consists of the following steps. 1. Let L oob k be the out-of-bag samples of the k th tree. Given X i  X  L oob k ,usethe 2. Choose a predictor feature j and randomly permute the value of feature j 3. For M i trees grown without X i , compute the out-of-bag prediction by RF 4. Compute the two mean square residuals (MSR) with and without permuta-5. Let  X MSR j i = max (0 ,MSR j i  X  MSR i ). The importance of feature j is With the raw importance scores by (9) we can rank the features on the impor-tance. 3.2 p -Value Feature Assessment Permutation method only gives the importance ranking of features. We need to identify important features from less i mportant ones. To do so, we use Welch X  X  two-sample t-test that compares the importance score of a feature with the max-imum importance scores of generated noisy features called shadows. The shadow features do not have prediction power to the response feature. Therefore, any feature whose importance score is smaller than the maximum importance score of noisy features, it is less important. Otherwise, it is considered as important. This idea was introduced by Stoppiglia et al. [10], and were further developed in [5], [11].

We build a random forests model RF from this extended data set. Following the importance measure by permutation procedure, we use RF to compute 2 M importance scores for 2 M features. We repeat the same process R times to compute R replicates. Table 1 shows the importance measure of M features in input data and M shadow features generated by permutating the values of the corresponding feature in data.

From the replicates of shadow features, we extract the maximum value from each row and put it into the comparison sample V  X  = max { A ri } , ( r =1 , ..R ; i = M +1 ,.. 2 M ). For each data feature X i , we compute t-statistic as: where s 2 1 and s 2 2 are the unbiased estimators of the variances of the two samples. For significance test, the distribution of t i in (10) is approximated as an ordinary Student X  X  distribution with the degrees of freedom df calculated as where n 1 = n 2 = R .

Having computed the t statistic and df , we can compute the p -value for the feature and perform hypothesis test on X i &gt; V level, we can identify important features. This test confirms that if a feature is important, it consistently scores higher than the shadow over multiple permuta-tions. 3.3 Feature Partition and Subspace Selection The p -value of a feature indicates the impo rtance of the feature in prediction. The smaller the p -value of a feature, the more correlated the predictor feature to the response feature, and the more powerful the feature in prediction.
Given all p values for all features, we set a significance level as the threshold  X  for instance  X  =0 . 05. Any feature whose p -value is smaller than  X  is added to the important feature subset X high , and it is added to the less important feature subset X low otherwise. The two subsets partitions the set of features in data. Given X high and X low , at each node, we randomly select some features from X high and some from X low to form the feature subspace for splitting the node. Given a subspace size, we can form the subspace with 80% of features sampled from X high and 20% sampled from X low . Now we can extend the quantile regression forests with the new feature subspace sampling method to generate splits a t the nodes of decision trees and select prediction value of Y from the range of low and high quantiles with a high probability. The new quantile regression forests algorithm eQRF is summarized as follows. 1. Given L , generate the extended data set L e in 2 M dimensions by permutat-2. Build a regression random forests model RF e from L e and compute R repli-3. For each predictor feature, take R importance scores and compute t statistic 4. Compute the degree of freedom df as (11). 5. Given t statistic and df , compute all p -values for all predictor features. 6. Given a significance level threshold  X  , separate important features from less 7. Sample the training set L with replacement to generate bagged samples 8. For each sample L k , grow a regression tree T k as follows: 9. Given a probability  X  ,  X  l and  X  h for  X  h  X   X  l =  X  , compute the corresponding 10. Given a X , estimate the prediction value from a value in the quantile range 5.1 Simulation Data We used three models as listed in Table 2 to generate synthetic data for sim-ulation analysis. Each model has 5 predictor variables or features. With each model, we first created 200 objects in 5 dimensions plus a response feature. Af-ter this, we expanded the data set with different numbers of noisy features and obtained 5 data sets named as { L M5, L M50, L M500, L M2000, L M5000 } where the number in the data name indicates dimensions of the data set. Similarly, we generated extra 5 data sets with 1000 objects from each model as test data sets named { H M5, H M50, H M500, H M2000, H M5000 } . 5.2 Evaluation Measure The performance of a model was evaluated on test data with the root mean of square residuals ( RMSR ) computed as where  X  f H ( X i ) is the prediction given X i , H is a test data set and H is the number of objects in test data H . 5.3 Evaluation Results We used regression random forests RF, quantile regression forests QRF and our algorithm eQRF to build regression models from the training data sets and used evaluation measure (12) to evaluate the models with the test data sets. We used the latest RF and QRF packages randomForest, quantregForest in R in these experiments [6], [8]. For each training data set, we built 100 regression models, each with 500 trees and tested the 100 models with the corresponding test data. Then, the result was evaluated with (12) and the average of 100 models was computed.
Figure 1 shows the evaluation results of three random forests regression meth-ods in RMSR measures. Each random fores ts method produced 5 test results on 5 simulation data sets from the left to right as { H M5, H M50, H M500, H M2000, H
M5000 } . We can see that the more noisy features in the data, the lower accu-racy in the prediction model. Clearly, in a ll simulated data generated by the three models in Table 2, eQRF performed the best and its RMSR was significantly lower than those of QRF and RF. 6.1 Real-World Data Five real-world data sets were used to evaluate the performance of our new regression random forests algorithm. The general characteristics of these data sets are presented in Table 3.

The computed tomography (CT) data was taken from the UCI 1 which was used to build a regression model to calculate the relative locations of CT slices on the axial axis. The data set was generated from 53,500 images taken from 74 patients (43 males and 31 females). Each CT slice was described by two histograms in a polar space. The first histogram describes the location of bone structures in the image and t he second represents the location of air inclusions inside of the body. Both histograms are concatenated to form the feature vector.
TFIDF-2006 2 is a text data set containing financial reports. Each document is associated with an empiri cal measure of financial risk. These measures are log transformed volatilities of stock returns.

The Microarray data  X  X iffuse Large B-cell Lymphoma X  (DLBCL) was col-lected from Rosenwald et al. [9]. The DLB CL data consisted of measurements of 7399 genes from 240 patients with diffuse large B-cell lymphoma. The outcome was survival time, which was either ob served or censored. We used observed survival time as the response feature b ecause censored data only indicates two states, dead or alive. A detailed d escription can be found in [9].  X  X eukemia X  and  X  X ung cancer X  are two gene data sets taken from NCBI 3 . Each of those data sets contains two classes. We changed one class label to 1 and another label to 0. We treat 0 and 1 as continuous values and consider this problem as a regression problem. We built a regression random forests model to estimate the outcome and used a defined threshold to divide the outcomes into two classes. 6.2 Experiments and Results For each real-world data set, we used two-third of data for training and one-third for testing. We generated 10 models from each training data and each model contained 200 trees. We computed the average of RMSRs of the 10 models with (12). The average RMSRs of three regression random forests models on five real-world data sets are shown in Table 3 on the right. We can see that eQRF had the lowest average RMSR. RF performed better than QRF.
Figure 2 plots the predicted values by RF and eQRF against the true values of the response feature in CT test data. We can see from Figure 2 (a) that there are some regions that RF predicted higher than the true value, for instance [25 cm, 35 cm], and some regions that RF predicted lower than the true value, for instance &gt; 70 cm. These are the prediction error regions including shoulder [20-30cm] and abdomen [60-75cm]. On the contrast, the predicted values of eQRF were more close to the true values as shown in Figure 2 (b) and the prediction results are consistent and more stable in all regions of human body.
Figure 3 shows the average RMSR box plots of three regression models from the real-world data sets. Figure 3 (a) is the result of CT data and Figure 3 (b) is the result of DLBCL data. We can see that eQRF produced less RMSR than QRF and RF and the variance is also small.
Figure 4 shows the computational time of three regression models on the two data sets. We can see that the computational time of the three models linearly increases as the number of objects increases if the size is small, such as DLBCL data. However, for data set with a la rge number of objects as CT data, the computational times of RF and QRF increase exponentially as shown in Figure 4 (a) but eQRF still maintains a linear increase as shown in Figure 4 (b). We have presented a new regression random forests algorithm for high-dimensional data with thousands of features. In this algorithm, we have made two extensions to the quantile regressi on forests. One is the subspace sampling method to select the set of features for splitting a node in growing trees. The other is to use the median of Y values in the range of two quantile as the predic-tion of Y given an input X . The first extension increas es the prediction accuracy of decision trees. The second extension r educes the effect of outliers and reduces the variance of random forests regression. Experiment results have demonstrated the improvement in reduction of RMSR in comparison with regression random forests and quantile regression forests.
 Acknowledgment. This research is supported in part by NSFC under Grant No.61203294, She nzhen New Industry Development Fund under Grant No.JC201 005270342A, No.JCYJ20120617120716224, the National High-tech Research and Development Program(No. 2012AA040912), and Guangdong-CAS project(No. 2011B090300025).

