 During the past 10 X 15 years offline learning to rank has had a tremen-dous influence on information retrieval, both scientifically and in practice. Recently, as the limitations of offline learning to rank for information retrieval have become apparent, there is increased atten-tion for online learning to rank methods for information retrieval in the community. Such methods learn from user interactions rather than from a set of labeled data that is fully available for training up front.

Below we describe why we believe that the time is right for an intermediate-level tutorial on online learning to rank, the objectives of the proposed tutorial, its relevance, as well as more practical details, such as format, schedule and support materials. Online learning to rank; Bandit algorithms; Exploration vs. exploita-tion
Today X  X  search engines have developed into complex systems that combines hundreds of ranking criteria with the aim of producing the optimal result list in response to users X  queries. For automatically tuning optimal combinations of large numbers of ranking criteria, learning to rank [ 22 , LTR] has proved invaluable. For a given query, each document is represented by a feature vector. The features may be query dependent, document dependent or capture the relationship between the query and documents. The task of the learner is to find a model that combines these features such that, when this model is used to produce a ranking for an unseen query, user satisfaction is maximized.

Traditionally, learning to rank algorithms are trained in batch mode, on a complete dataset of query and document pairs with their associated manually created relevance labels. This setting has a number of disadvantages and is impractical in many cases. First, creating such datasets is expensive and therefore infeasible for smaller search engines, such as small web-store search engines [ 24 ]. Second, it may be impossible for experts to annotate documents, as in the case of personalized search [ 18 ]. Third, the relevance of documents to queries can change over time, like in a news search engine [7].

Online learning to rank addresses all of these issues by incre-mentally learning from user feedback in real time [ 34 ]. Online learning is closely related to active learning, incremental learning, and counterfactual learning. However, online learning is more diffi-cult because the agent has to balance exploration and exploitation: actions with unknown performance have to be explored to learn better solutions [11].

There is a growing body of established methods for online learn-ing to rank for information retrieval (see the schedule below for a broad range of examples). The time is right to organize and present this material to a broad audience of interested information retrieval researchers, whether junior or senior, whether academic or indus-trial. The online learning to rank methods available today have been proposed by different communities, in machine learning and information retrieval. A key aim of the tutorial is to bring these together and offer a unified perspective. To achieve this we illustrate the core and state of the art methods in online learning to rank, their theoretical foundations and real-world applications, as well as existing online learning algorithms that have not been used by information retrieval community so far.

We expect the tutorial to be useful for both academic and indus-trial researchers who either want to develop new online learning to rank methods, use them in their own research, or apply the methods described in the tutorial to improve search and recommendation systems.
Online learning to rank from user interactions is fundamentally different from currently dominant supervised learning to rank ap-proaches for information retrieval, where training data is assumed to be randomly sampled from some underlying distribution, and where absolute and reliable labels are provided by professional annota-tors [ 15 ]. When learning from user interactions, a system has no control over which queries it receives, it only receives feedback on the result lists it presents to users, and it has to present high quality result lists while learning, to satisfy user expectations.
Following Hofmann et al. [11] , in this tutorial we formulate on-line learning to rank as a reinforcement learning problem, in which an agent , the search engine, learns from interactions with an envi-ronment , the user and their interactions, by trying out actions (e.g., returning a ranked list of items) and observing rewards (e.g., inter-preting user feedback as absolute or relative feedback) in multiple rounds or discrete time steps; see Figure 1.

Particularly relevant to this tutorial are methods for tackling so-called contextual bandit problems (also known as bandits with side information) [ 1 ]. A contextual bandit problem is a special case of Figure 1: The information retrieval problem modeled as a con-textual bandit problem, with information retrieval terminology in black and the corresponding reinforcement terminology in green and italics. (Figure taken from [11].) a reinforcement learning problem in which states are independent of the agent X  X  actions. In information retrieval terms, the context could consist of the user and the query and the actions are the search engine result pages. A difference between typical contextual bandit formulations and online learning to rank for information retrieval is that in information retrieval (absolute) rewards cannot be observed directly. Instead, feedback for learning is inferred from observed user interactions as noisy preference indications.

As we will demonstrate in the tutorial, an important benefit of reducing IR problems to bandit approaches is that the rich body of work on bandit approaches can be used. At the same time, information retrieval poses unique challenges that inspire additional research on bandit algorithms.

The objectives of the tutorial are as follows: These objectives are meant to give participants a thorough under-standing of existing learning to rank for information retrieval meth-ods and to present online learning methods that have so far not been applied to learning to rank, let alone to learning to rank for information retrieval.
The tutorial will be organized in two halves of 90 minutes each, each mixing theory and experiment, with formal analyses of online learning to rank methods interleaved with discussions of code and of experimental outcomes. Part I is aimed at familiarizing participants with the key concepts and algorithms. In Part II we select a small number of topics to provide a more in-depth technical treatment. [10 minutes] Introduction, aims and historical notes
Here we discuss the context in which online LTR is applied and the most important historical milestones in its develop-ment. [10 minutes] LTR in IR. [15 minutes] Online LTR: balancing exploration and exploitation. [5 minutes] Introduction to bandits and reinforcement learning. [10 minutes] Online signals to learn from. [20 minutes] Dueling bandit gradient descent. [5 minutes] Real world applications. [15 minutes] Discussion.
 In this part we dig deeper into the foundations of some of the concepts introduced in Part I. [5 minutes] Introduction. [30 minutes] Online LTR in K-armed bandits setting. [20 minutes] Current problems: non-linear models, better explo-[10 minutes] Existing online algorithms not used in information [10 minutes] Datasets and resources. [15 minutes] Discussion and conclusion. This research was supported by Ahold, Amsterdam Data Science, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community X  X  Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the ESF Research Network Program ELIAS, the Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project, the Microsoft Research Ph.D. program, the Nether-lands eScience Center under project number 027.012.105, the Nether-lands Institute for Sound and Vision, the Netherlands Organisa-tion for Scientific Research (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, the Yahoo Faculty Research and Engagement Program, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
 [1] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learnng Research , 3:397 X 422, 2003. [2] Alexey Borisov, Pavel Serdyukov, and Maarten de Rijke. Using metafeatures to increase the effectiveness of latent semantic models in web search. In WWW 2016: 25th International World Wide Web Conference . ACM, April 2016. [3] Christopher J.C. Burges. From ranknet to lambdarank to lambdamart: An overview. Technical Report MSR-TR-2010-82, June 2010. [4] Giuseppe Burtini, Jason Loeppky, and Ramon Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. CoRR , abs/1510.00757, 2015. URL http://arxiv.org/abs/1510.00757. [5] R X bert Busa-Fekete and Eyke H X llermeier. A survey of preference-based online learning with bandit algorithms. In Algorithmic Learning Theory: 25th International Conference, ALT 2014, Bled, Slovenia, October 8-10, 2014. Proceedings , pages 18 X 39, Cham, 2014. Springer International Publishing. [6] R X bert Busa-Fekete and Eyke H X llermeier. A survey of preference-based online learning with bandit algorithms. In ALT  X 14 , number 8776 in LNCS, pages 18 X 39. Springer, 2014. [7] Susan T. Dumais. The web changes everything: Understanding and supporting people in dynamic information environments. In Research and Advanced Technology for Digital Libraries, 14th European Conference, ECDL 2010 , 2010. [8] Artem Grotov and Maarten de Rijke. Online learning to rank for information retrieval: A survey. Draft , 2016. [9] Artem Grotov, Shimon Whiteson, and Maarten de Rijke. Bayesian ranker comparison based on historical user interactions. In SIGIR 2015: 38th international ACM SIGIR conference on Research and development in information retrieval . ACM, August 2015. [10] Artem Grotov, Maarten de Rijke, and Shimon Whiteson. Online LambdaRank. In Submitted , 2016. [11] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Balancing exploration and exploitation in learning to rank online. In ECIR 2011: 33rd European Conference on Information Retrieval . Springer, April 2011. [12] Katja Hofmann, Anne Schuth, Shimon Whiteson, and Maarten de Rijke. Reusing historical interaction data for [13] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. [14] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. [15] Katja Hofmann, Shimon Whiteson, Anne Schuth, and [16] Thorsten Joachims. Optimizing search engines using [17] Youngho Kim, Ahmed Hassan, Ryen W. White, and Imed [18] Ron Kohavi, Roger Longbotham, Dan Sommerfield, and [19] Branislav Kveton, Csaba Szepesv X ri, Zheng Wen, and Azin [20] John Langford and Tong Zhang. The epoch-greedy algorithm [21] Damien Lefortier, Pavel Serdyukov, and Maarten de Rijke. [22] Tie-Yan Liu. Learning to rank for information retrieval. [23] Harrie Oosterhuis, Anne Schuth, and Maarten de Rijke. [24] Mark Sanderson. Test collection based evaluation of [25] Anne Schuth, Katja Hofmann, Shimon Whiteson, and [26] Anne Schuth, Krisztian Balog, and Liadh Kelly. Overview of [27] Anne Schuth, Harrie Oosterhuis, Shimon Whiteson, and Maarten de Rijke. Multileave gradient descent for fast online learning to rank. In WSDM 2016: The 9th International Conference on Web Search and Data Mining . ACM, February 2016. [28] Aleksandrs Slivkins, Filip Radlinski, and Sreenivas Gollapudi. Ranked bandits in metric spaces: learning optimally diverse rankings over large document collections. Technical report, arXiv preprint arXiv:1005.5197, 2010. [29] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . MIT Press., 1998. [30] Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. CoRR , abs/1502.02362, 2015. URL http://arxiv.org/abs/1502.02362. [31] Aibo Tian and Matthew Lease. Active learning to maximize accuracy vs. effort in interactive information retrieval. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR  X 11, pages 145 X 154, New York, NY, USA, 2011. ACM. [32] TREC. OpenSearch track. http://trec-open-search.org, 2016. [33] Aleksandr Vorobev, Damien Lefortier, Gleb Gusev, and Pavel Serdyukov. Gathering additional feedback on search results by multi-armed bandits with respect to production ranking. In Proceedings of the 24th International Conference on World Wide Web , pages 1177 X 1187. ACM, 2015. [34] Yisong Yue and Thorsten Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In ICML  X 09 , 2009. [35] Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and Remi Munos. Relative confidence sampling for efficient on-line ranker evaluation. In 7th ACM WSDM Conference (WSDM2014) . ACM, February 2014. [36] Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten de Rijke. Relative upper confidence bound for the k-armed dueling bandit problem. In ICML 2014: International Conference on Machine Learning , June 2014. [37] Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. Mergerucb: A method for large-scale online ranker evaluation. In WSDM 2015: The Eighth International Conference on Web Search and Data Mining . ACM, February 2015. [38] Masrour Zoghi, Shimon Whiteson, Zohar Karnin, and Maarten de Rijke. Copeland dueling bandits. In NIPS 2015 , December 2015. [39] Masrour Zoghi, Tom X  X  Tunys, Lihong Li, Damien Jose, Junyan Chen, Chun Ming Chin, and Maarten de Rijke.
 Click-based hot fixes for underperforming torso queries. In SIGIR 2016: 39th international ACM SIGIR conference on Research and development in information retrieval . ACM, July 2016.
