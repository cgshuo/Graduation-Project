 Predicting student performance is an important task for many core problems in intelligent tutoring systems. This paper proposes a set of novel probabilistic latent class mod-els for the task. The most effective probabilistic model uti-lizes all available information about the educational content and users/students to jointly identify hidden classes of stu-dents and educational content that share similar character-istics, and to learn a specialized and fine-grained regression model for each latent educational content and student class. Experiments carried out on large-scale real-world datasets demonstrate the advantages of the proposed probabilistic latent class models.
 Categories and Subject Descriptors: H.4 [ Information Systems Applications ]: Miscellaneous Keywords: Latent Class; Model; Performance; Predict; Student; User
Increasing trend in computers X  utilization for teaching has led to the development of many intelligent tutoring systems (ITS). ITS improve students X  learning by providing individ-ualized guidance to each student via means of adjusting the difficulty levels of educational tasks, providing hints when necessary, etc. Predicting student performance (PSP) is one of the core tasks, successful execution of which is highly useful for many of such important means. Therefore, there has been vast amount of prior research on the topic. How-ever, most of those studies followed traditional classification and regression approaches as summarized in [5, 10]. There has recently been more interest on student performance pre-diction following the KDD Cup 2010 challenge [6, 9, 13, 15]. Some of the techniques used the traditional regres-sion/classification approaches as well [9, 15]. On the other hand, others researchers have proposed to use recommen- X  Work done when the author was with Purdue University. dation system techniques such as collaborative filtering [6, 13] and matrix factorization [11, 12, 13], showing that rec-ommendation system techniques could improve prediction results compared with regression methods [12]. Although most of those methods are shown to be effective, they are significantly dependent on the skills (knowledge component) information that are manually extracted by the domain ex-perts as necessary steps for the solution of a problem [9, 13, 15]. Therefore, these methods require extensive human ef-fort or expert knowledge of the domain [11, 12]. More recent work proposed to use matrix factorization, and showed that it is effective to generate reasonable prediction performance by identifying latent factors, and is efficient as it does not need to use the skills information that requires extensive human pre-processing effort [11, 12]. Yet, recommendation system techniques such as matrix factorization or collabora-tive filtering use student, item (educational task), and rating (success or not) triplets; and ignore other types of available data about the students and educational materials.

In this paper, we propose to intelligently utilize the infor-mation about the student and educational content for pre-dicting student performance in ITS by using a series of novel probabilistic latent class models. In particular, we present three probabilistic latent class models that automatically identify latent groups (classes) of students and educational materials that share similar characteristics (e.g., intelligent level, knowledge level, difficulty level), and learn a special-ized regression model for each group of students and educa-tional materials for more accurate prediction. The first la-tent class model identifies latent groups of students, the sec-ond latent class model identifies latent groups of educational tasks, and finally the third model discovers latent groups of students and tasks that jointly share similar character-istics. The proposed models effectively utilize all available information about a student and educational task/content, are efficient as they are not dependent on the (manually-extracted) skills-information, and are compared to a large number of baselines including traditional regression-based models as well as recommendation system techniques such as collaborative filtering and matrix factorization. An exten-sive set of experiments on real-world large-scale data shows the effectiveness of the proposed probabilistic latent class models for intelligently combining student and educational content information.
Probabilistic latent class models have been shown to be effective in several applications such as text analysis [7, 2], query analysis [14], social network analysis [4], and online advertising [3]. However, no prior work utilized it for pre-dicting student performance. We propose novel probabilistic latent class models to intelligently utilize all available infor-mation about the student and educational content.

Formally, let f cu be the set of all features consisting of educational content (i.e., problem) features f c for a specific educational content c , and user (i.e., student) features f for a specific user u , i.e. f cu and P ( s cu | f cu conditional probability of success where s cu is a real number between 0 and 1 indicating how likely user (student) u is to correctly complete an educational task/problem c , then the probabilistic latent class model can be described as follows: P ( s cu | f cu ) = cients, which is the probability of choosing latent content class z and student class x given content features f c user features f u while  X  and  X  are the corresponding param-eters and N z and N x are the number of latent content, and user classes respectively. The mixing proportions P ( z | f (as well as P ( x | f u ;  X  )) can be modeled by a soft-max func-that scales the exponential function to be a proper proba-bility distribution (i.e., Z c = P z exp ( P L z j =1  X  zj f c is a bag of content features ( f c 1 ,...,f c L number of content features. The success level in a class P ( s cu | f cu ,z,x ;  X  ) can be modeled with the Gaussian distri-bution as follows P ( s cu | f cu ,z,x ;  X  ) = (1 / P and educational task/content c (more information about the features can be found in Section 3.1),  X  zxi is the weight of latent content class z and latent user class x for the i th ture, and K is the number of features. It is important to note that it is possible to choose a different distribution to model the success level in a class, such as the Laplace dis-tribution. Yet, this work aims to minimize RMSE (that will be introduced in Section 3.3); therefore, Gaussian distribu-tion is a better choice as it is more strict about (i.e., more effected by) major deviations. l  X , X , X  = X The parameters (  X , X , X  ) can be determined by maximiz-ing the data log-likelihood function shown in Equation 1. A typical approach to maximizing the data likelihood func-tion above is to use the Expectation-Maximization (EM) algorithm [4, 3], which can obtain a local optimum of log-likelihood by iterating the Expectation (E) step and Maxi-mization (M) step until convergence.

P ( z,x | f cu ) = (2) The E-step can be derived as follows by computing the pos-terior probability of z and x as shown in Equation 2. By optimizing the auxiliary Q-function, we can derive the M-step update rules (  X   X  x  X  can be achieved similar to  X  follows:  X  which are differentiable and can be solved with gradient descent solvers. In particular, we use the Quasi-Newton method. This joint latent class model will be referred as Latent CU Mod for the rest of the paper.

In order to better understand the performance of the pro-posed probabilistic latent educational content and student class model, i.e., Latent CU Mod, several groups of sub-models are also constructed, which use only one latent class group or no latent class group at all. When Latent CU Mod uses only one latent class group, i.e., N z = 1 and N 1, or N z &gt; 1 and N x = 1, the latent educational con-tent and user class model degenerates to a latent user class model, and a latent educational content class model that will be referred as Latent U Mod and Latent C Mod respec-tively. Note that the number of latent classes used by La-tent U Mod, Latent C Mod, and Latent CU Mod are em-pirically set to be 5, 5, and 9 respectively. An extreme case is when Latent CU Mod uses only one content and one user class (i.e., N z = 1 and N x = 1). In this case only the Gaus-sian regression power is employed. We particularly report this case as Gaussian Regr in the experiments as one of the baselines.
We use two large-scale real-world datasets, namely  X  X l-gebra I 2008-2009 X  and  X  X ridge to Algebra 2008-2009 X  (re-ferred as  X  X lgebra X  and  X  X ridge X  respectively) that became available with the Knowledge Discovery and Data Mining Challenge 2010 [1, 8]. The data consist of the records of interactions between (3.3K vs. 6K) students and (9.4M vs. 20.7M) educational tasks (steps) that are required for solv-ing problems that belong to sections of units. For each step, (both training and test) data have fields such as student ID, problem hierarchy that includes step, problem, unit, section names, knowledge components used in the problem and the number of times a problem has been viewed. Additional fields that are only available in training data are correct first attempt (CFA) indicating whether student was correct on the first attempt on a step (1 for correct attempt, 0 for incorrect attempt), number of hints requested, and step du-ration. More information about the dataset, and available fields can be found in [1]. In this work, we divided the avail-able training data into an internal training and internal vali-dation set similar to the approach followed by [15], and only used one percent of the internal training data to train the proposed models. Note that the performance of the mod-els with respect to different training data sizes such as one, two, five, twenty percentage was tested and no significant difference in model performance was observed (results omit-ted due to space restrictions). As labels of test data are not publicly shared, we use the submission website to evaluate the models on the whole test data (of 508K vs. 756K steps for Algebra and Bridge datasets respectively) as is done in the literature, and report these results in Section 4.
Feature selection is an important part of modeling re-gardless of the selected learning method. Yu et al. did an extensive study on feature selection on the datasets, and reported that extracting only a small set of condensed fea-tures has comparable performance with extracting millions of sparse features. Specifically, they proposed using  X  X orrect first attempt rate X  (CFAR) as the numerical feature value (indicating success level), which can be calculated for a stu-dent, unit, section, problem, problem view, step, and their combinations. In this work, we extract a total of eighteen condensed features that includes ten content features and eight user/student features. Specifically, we extract CFAR for unit, section, problem, step, (problem, step), (unit, sec-tion, problem, step, problem view), and four binary features indicating whether there is training data available to cal-culate the CFAR for the last four features (i.e., problem, step, (problem, step), (unit, section, problem, step, prob-lem view)). Similarly, CFAR is calculated for (student,unit), (student, problem view), (student, problem), (student, step), (student, problem, step), and three binary features indicat-ing whether there is available training data to calculate the CFAR for the last three features (i.e., (student, problem), (student, step), (student, problem, step)). Other features and feature combination are also tested, yet these eighteen features have achieved the best results, and are used in this work. Similar features are used in the related prior work as well [12, 15]. Note that these features can be easily calcu-lated from the ITS logs, and do not use data such as skills (knowledge components) that require tremendous manual work from the experts of the domain. Therefore, the pro-posed models can be built efficiently unlike the models pro-posed by majority of the prior work [9, 13, 15].
The proposed probabilistic latent class models are com-pared to four types of baselines. The first baseline type is simply the global success average for all steps of i) all prob-lems and ii) each user/student, which will be referred as Global Avg and User Avg respectively.

The second baseline type is the traditional regression-based approach that fits a regression model on all avail-able data instances. Particularly, we use the Gaussian Regr model that has been proposed in Section 2 as a regression-based baseline, and compare this model with Logistic Re-gression (referred as Logistic Regr) that has been proposed as a regression-based baseline in the literature [12].
The third and fourth baselines are the popular recom-mendation system techniques: collaborative filtering (CF) and matrix factorization [12]. Both techniques use student (user), educational task/step (item), and success level (rat-ing) triplets as their data. Prior work used CF as a baseline for performance prediction (referred as Collab Filtering), and chose problem hierarchy, problem, step, and problem view as the item dimension [12]. Recent prior work that proposed the matrix factorization approach for performance prediction (referred as Matrix Fact) used problem hierarchy, problem, step, problem view as the item dimension [12]. Table 1: Results (in RMSE) of all models in comparison to each other. Results of base-lines Global Avg, User Avg, Logistic Regr, Col-lab Filtering, Matrix Fact can be found in [12].

The error between the estimate success level from the models and the actual success level is measured by the Root Mean Squared Error (RMSE). RMSE is the accepted evalua-tion metric for the KDD Cup 2010 Educational Data Mining Challenge, and the challenge submission website still evalu-ates all submissions with RMSE. It is used by all prior work that used the datasets of the challenge [1, 9, 12, 13, 15].
We first compare the proposed Gaussian Regr approach with several baselines introduced in the literature. It can be seen in Table 1 that Gaussian Regr significantly outperforms the two basic baselines, namely Global Avg and User Avg, which have by far the worst performance out of all baselines. This is due to the fact that these baselines use only the global success average out of all step data or out of all data of a student to make their predictions respectively, both of which are too coarse approaches to be effective.

Gaussian Regr outperforms the Logistic Regr approach as well. Although both are regression-based approaches, the Logistic Regr model in the literature (which is the best per-forming Logistic regression model reported in the literature on these datasets) uses a less comprehensive set of features than Gaussian Regr for modeling; and is, therefore, outper-formed by the proposed Gaussian Regr model.

Out of the two recommendation system techniques, Col-lab Filtering outperforms the Gaussian Regr model on the Bridge dataset, and is outperformed by the Gaussian Regr model on the Algebra dataset. These results can be ex-plained by the fact that the Bridge dataset has many more students than the Algebra dataset, and it is easier for Col-lab Filtering approach to identify more similar students for a target test student, which eventually helps the model achieve better performance. Since Gaussian Regr is regression-based on a small set of condensed features, it is much less likely to be affected by this difference. Their average performances are comparable, with no significant difference.

The final comparison is done between the proposed Gaus-sian Regr model and Matrix Fact. Similar to the compar-ison with Collab Filtering, Gaussian Regr performs better than Matrix Fact on Algebra dataset, and is outperformed by Matrix Fact on the Bridge dataset. Since matrix factor-ization uses similar data with the collaborative filtering ap-proach, similar analysis applies to this set of results as well. On average, Matrix Fact outperforms the Gaussian Regr. Overall, this set of results shows that the proposed Gaus-sian Regr baseline approach is a strong baseline, and beats comparable methods in the literature (i.e., Logistic Regr), performs comparably with collaborative filtering (i.e., Col-lab Filtering), and is only outperformed by the matrix fac-torization approach (Matrix Fact).
We compare the proposed probabilistic latent class models with each other as well as with the best performing baselines. In Table 1, we show that the proposed probabilistic latent educational content class model, i.e., Latent C Mod, signifi-cantly outperforms all baseline models, i.e., Gaussian Regr, Collab Filtering, and Matrix Fact. This is due to the high modeling capability that Latent C Mod has by being able to utilize all available information about the student and the educational content as well as being able to identify la-tent educational content classes with similar characteristics and fit a specialized model for each latent class. Specifically, unlike Collab Filtering and Matrix Fact that only use stu-dent, task, rating (success level) triplets, Latent C Mod is able to utilize all available data. Unlike Gaussian Regr that learns a coarse single global model over all data instances, Latent C Mod is able to differentiate different groups of con-tent, and fit a specialized, fine grained model for each latent class to improve the prediction accuracy.

Next, we compare the proposed probabilistic latent user (student) model, Latent U Mod, with other models. It can be seen that Latent U Mod significantly outperforms all base-line models, i.e., Gaussian Regr, Collab Filtering, and Ma-trix Fact. Similar to Latent C Mod, Latent U Mod is able to identify latent student classes with similar characteris-tics, and specialize the prediction model for each latent stu-dent class for utilizing all available student and content data. Therefore, Latent U Mod is able to generate better results with its more flexible modeling capabilities. It should be noted that Latent C Mod performs slightly better than La-tent U Mod on both datasets. This shows that identifying latent content classes are more important than identifying latent student classes for specializing the learning.
Finally, we compare the proposed probabilistic latent edu-cational content and latent user class model, Latent CU Mod, with Latent C Mod and Latent U Mod as well as the base-lines. It can be observed that Latent CU Mod is the best performing model out of all models, and significantly out-performs all baseline approaches as well as the probabilistic latent content class model, Latent C Mod, and the proba-bilistic latent user class model, Latent U Mod. This is due to the fact that Latent CU Mod is able to jointly capture educational content and users (students) with similar char-acteristics and learn a specialized regression model for each joint content and student class. This further shows that identification of latent content and student classes are both very important. Therefore, Latent CU Mod has the highest modeling capability out of all models and achieves the best performance for both datasets.
Predicting student performance is an important and chal-lenging task in ITS. This paper proposes a set of novel prob-abilistic latent class models to intelligently learn how to com-bine the information about users/students and educational content for predicting student performance. The most ef-fective probabilistic model utilizes all available information about the educational content and users/students to jointly identify hidden groups/classes of students and educational content with similar characteristics, and to learn a special-ized and fine-grained regression-based prediction model for each latent educational content and user/student classes. An efficient learning algorithm based on Expectation and Maximization algorithm has been proposed to simultane-ously learn the model parameters. Empirical studies have been conducted with large-scale real-world datasets. Exper-imental results demonstrate the advantages of the proposed probabilistic latent class models. This work is partially supported by NSF research grants IIS-0746830, CNS-1012208 and IIS-1017837; by the Center for Science of Information (CSoI); and by the NSF Science and Technology Center grant CCF-0939370.
