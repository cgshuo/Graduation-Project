 Universitat Polit ` ecnica de Catalunya Universitat Polit ` ecnica de Catalunya Universitat Polit ` ecnica de Catalunya a discourse that refer to the same entity.
 by relaxation labeling; and (ii) research towards improving coreference resolution performance using world knowledge extracted from Wikipedia.
 expressiveness than the pair-based ones, and overcome the weaknesses of previous approaches information by adding constraints, and research has been done in order to use world knowledge to improve performances.
 and participated in international competitions: SemEval-2010 and CoNLL-2011. RelaxCor achieved second place in CoNLL-2011. 1. Introduction
Coreference resolution is a natural language processing (NLP) task that consists of determining which mentions in a discourse refer to the same entity or event. A men-expression we mean noun phrases (NP), named entities (NEs), embedded nouns, and pronouns (all but pleonastic and interrogative ones) whose meaning as a whole is a this article, we do not deal with coreference involving events, and focus only on entity correference.
 same referent. Thus, a coreference chain is formed by all mentions in a discourse that refer to the same real entity. Given an arbitrary text as input, the goal of a coreference mentions considered coreferential during resolution.
 necessary knowledge sources. For instance, morphological and syntactic analysis is needed to detect mentions, and semantic/world knowledge to know that Messi is a star striker and a young Argentine .

In this sense, dealing with such a problem becomes important for tasks in which the higher their comprehension of the discourse, the better such systems will perform X  tasks such as machine translation (Peral, Palomar, and Ferr  X  andez 1999), question an-swering (Morton 2000), summarization (Azzam, Humphreys, and Gaizauskas 1999), and information extraction.
 the incorporation of new information such as world knowledge and discourse co-herence. In some cases, this information cannot be expressed in terms of pairs of partial entities. Furthermore, an experimental approach in this field should over-come the weaknesses of previous state-of-the-art approaches, such as linking contra-dictions, classifications without context, and a lack of information when evaluating pairs.
 labeling. One of the main goals of developing such an approach is the incorporation of world knowledge and discourse coherence in order to improve performance while addressing the problems mentioned previously.
 machine learning approaches to coreference resolution, highlighting their most rele-vant parts with their corresponding issues. Section 3 defines our proposed approach and Section 4 provides details about the implementation and the training methods.
The experiments and error analysis are described in Section 5. Section 6 presents our approach to incorporate world knowledge in order to improve coreference resolution performance. Experiments and a detailed error analysis are also included. Finally, we discuss the conclusions of this article in Section 7. 848 2. Coreference Resolution: State of the Art
In this section we summarize the main machine-learning X  X ased approaches to corefer-ence resolution. For a wider study, we refer the reader to Mitkov (2002).
 text with coreference annotations as output. Most existing coreference resolution sys-tems can be considered instances of this general process, which consists of three main steps: mention detection, characterization of mentions, and resolution (see Figure 2). to find the boundaries of the mentions in the input text. Next, in the second step, the identified mentions are characterized by gathering all the available knowledge about them and their possible compatibility. Typically, machine learning systems introduce all the knowledge by means of feature functions. Finally, the resolution itself is performed difficult given the diversity of approaches and algorithms used for resolution. Even so, the diverse approaches in current systems have at least two main processes in the resolution: classification and linking .
 2.1 Classification Models
The models found in the state of the art for the classification process are: mention pairs, rankers, and entity-mention.

Mention pairs. Classifiers based on the mention-pair model determine whether two mentions corefer or not. To do so, a feature vector is generated for a pair of mentions confidence value about the decision taken. The class and the confidence value of each evaluated pair of mentions will be taken into account by the linking process to obtain the final result.
 mation and contradictions in classifications. Figure 3 shows an example of lack of (also marked with an X) returns the NC class. In this case, the lack of information is due to the impossibility of determining the gender of A. Smith . Next, Figure 4 shows a possible scenario with contradictions. In this scenario, the classifier has determined when generating the final coreference chains given that the pairs ( Alice Smith , he )and ( he , she ) do not corefer.

Rankers. The rankers model overcomes the lack of contextual information found using mention-pairs. Instead of directly considering whether m i corefer with an active mention. Rankers can still fall in contradictions, however, and need to rely on the linking process to solve that.

Entity-mention. The entity-mention model classifies a partial entity and a mention, or two partial entities, as coreferent or not. In some models, a partial entity even has its 850 own properties or features defined in the model in order to be compared with the cases this model overcomes the lack of information and contradiction problems of the mention-based models. For example, a partial entity may include the mentions Alice
Smith and A. Smith , whose genders are  X  X emale X  and  X  X nknown X  respectively. In this case, the partial entity is more likely to be linked with the subsequent mention she than with he (Figures 3 and 4). The features used for entity-mention models are almost the same as those used for mention-based models. The only difference is that the value of belonging to it. 2.2 Resolution paradigms depending on their resolution process (i.e., combinations of classification and linking processes): up to 2011. Recently, the CoNLL-2012 shared task (Pradhan et al. 2012) offered an evaluation framework similar to that of CoNLL-2011. The second column specifies which resolution step is used. The third column shows the classification model used by the system, and the fourth column identifies the algorithm followed in the linking process.
 3. A Constraint-Based Hypergraph Partitioning Approach to Coreference Resolution
One of the possible directions to follow in coreference resolution research is the incorpo-ration of new information such as world knowledge and discourse coherence. In some cases, this information cannot be expressed in terms of pairs of mentions, that is, it is information that involves either several mentions at once or partial entities. Therefore, an experimental approach in this field needs the expressiveness of the entity-mention model as well as the mention-pair model in order to use the most typical mention-pair features. Furthermore, such an approach should overcome the weaknesses of previous state-of-the-art approaches, such as linking contradictions, classifications without con-text, and a lack of information when evaluating pairs. Also, the approach would be more flexible if it could incorporate knowledge both automatically and manually. that represents the problem in a hypergraph and solves it by relaxation labeling, re-ducing coreference resolution to a hypergraph partitioning problem with a given set of constraints. The main strengths of this system are: the problem representation in a (hyper)graph. Next, Section 3.2 explains how the 852 knowledge is represented as a set of constraints, and Section 3.3 explains how attach-ing influence rules to the constraints means that the approach incorporates the entity-mention model. Finally, Section 3.4 describes the relaxation labeling algorithm used for resolution. 3.1 Graph and Hypergraph Representations
The coreference resolution problem consists of a set of mentions that have to be mapped to a minimal collection of individual entities. By representing the problem in a hypergraph, we are reducing coreference resolution to a hypergraph partitioning problem. Each partition obtained in the resolution process is finally considered an entity.
 these vertices is connected by hyperedges to other vertices. Hyperedges are assigned hyperedge weight in absolute terms, the more reliable the hyperedge. In the case of the mention-pair model, the problem is represented as a graph where edges connect pairs of vertices.
 a set of hyperedges. Let m = ( m 1 , ... , m n ) be the set of mentions of a document with n mentions to resolve. Each mention m i in the document is represented as a vertex v
A hyperedge e g  X  E is added to the hypergraph for each group ( g ) of vertices ( v affected by a constraint, as shown in Figure 6. The subset of hyperedges that incide on v is E ( v i ).
 is used to compute the weight value of the hyperedge e g .Let w ( e of the hyperedge e g : where  X  k is the weight associated with constraint k . The graph representing the mention-pair model is a subcase of the hypergraph where | g | = 2. Figure 7 illustrates a graph. For simplicity, in the case of the mention-pair model, an edge between m e . In addition, sometimes w ij is used instead of w ( e ij 3.2 Constraints as Knowledge Representation
In this approach, knowledge is a set of weighted constraints where each constraint between mentions. A constraint is a conjunction of feature-value pairs that are evaluated over all the pairs or groups of mentions in a document. When a constraint applies to a set of mentions, a corresponding hyperedge is added to the hypergraph, generating the representation of the problem explained in Section 3.1 (Figure 6).
 constraint ( | g | ). A pair constraint has order N = 2, and a group constraint has N &gt; 2.
The mentions evaluated by a constraint are numbered from 0 to N are found in the document.
 one sentence, their genders match, m 0 is not the first mention of its sentence, m also is a maximal NP, both mentions are ARG0 in semantic role labeling, and both mentions are pronouns. 1 The constraint in Figure 9 applies to three mentions and requires that: The distance between consecutive mentions is one sentence, all three mentions agree in both gender and number, m 0 and m 2 are aliases, all three mentions are ARG0 in their respective sentences, and m 0 and m 2 are named entities and m 854 a common NP. 2 There are many examples of negative constraints, that is, constraints that restrict mentions from being in the same entity. For instance GENDER NO(0,1) &amp;TYPE P(0) &amp; TYPE P(1) expresses that m 1 and m 0 in gender.
 graph (see Equation (1)). A constraint weight is a value that, in absolute terms, reflects the confidence of the constraint. Moreover, this weight is signed, and the sign indicates whether the adjacent mentions corefer (positive) or not (negative). The use of negative information is not very extensive in state-of-the-art systems, but given the hypergraph representation of the problem, where most of the mentions are interconnected, the negative weights contribute information that cannot be obtained using only positive weights. Moreover, in our experiments, the use of negative weights accelerates the convergence of the resolution algorithm. The training process that determines the weight of each constraint is explained in Section 4.3. 3.3 Entity-Mention Model Using Influence Rules
We have explained how groups of mentions satisfying a constraint are connected by hyperedges in the hypergraph. This section explains how the entity-mention model is definitively incorporated to our constraint-based hypergraph approach. The entity-mention model takes advantage of the concept of an entity during the resolution pro-information can be used to make new decisions.
 the influence rule, which is attached to a constraint. An influence rule expresses the conditions that the mentions must meet during resolution before the influence of the constraint takes effect.

The constraint specifies the feature functions that the involved mentions must meet, such as semantic role arguments, sentence distances, and agreements. The influence rule then determines that when mentions 0 and 2 belong to the same entity, and mention 1 belongs to a different entity, mention 3 is influenced in order to belong to the same entity as mention 1. This figure also contains some text to help understand why this kind of constraint may be useful. A mention-pair approach could easily make the mistake of classifying mentions 2 and 3 as coreferent. This is an example of introducing information about discourse coherence using an entity-mention model.
 in this approach are assigned a default influence rule that depends on the sign of the opposite. Figure 11 shows the default influence rules for mention-pair constraints with both positive and negative weights.
 constraints that applies to the same group of mentions and has the same influence rule.
In the case that some constraints apply to the same group of mentions but have different influence rules, a hyperedge is added to the graph for each influence rule. Therefore, in
Equation (1), C g  X  C refers to the constraints that apply to the group and share the same influence rule. 3.4 Relaxation Labeling
Relaxation is a generic name for a family of iterative algorithms that perform function gradient steps. Relaxation labeling has been successfully used in engineering fields to
Hummel, and Zucker 1976), and in many other AI problems. The algorithm has also been widely used to solve NLP problems such as part-of-speech tagging (Padr  X  o 1998), chunking, knowledge integration, semantic parsing (Atserias 2006), and opinion mining (Popescu and Etzioni 2005).
 856 titioning problem by dealing with (hyper)edge weights as compatibility coefficients . possible. In each step, the algorithm updates the probability of each vertex belonging vertices proportional to the edge weights.
 hypergraph is a variable in the algorithm. Let L i be the number of different labels that are possible for v i . The possible labels of each variable are the partitions that the vertex can be assigned. Note that the number of partitions (entities) in a document is a priori unknown, but it is at most the number of vertices (mentions) because, in an extreme case, each mention in a document could refer to a different entity. Therefore, a vertex with index i can be in the first i partitions (i.e., L i = i ).
 is maximized. A weighted labeling is a weight assignment for each possible label of each variable: H = ( h 1 , h 2 , ... , h n ), where each h each possible label of v i ;thatis, h i = ( h i 1 , h i 2 these weights (of between 0 and 1) vary in time. We denote the probability for label l of variable v i at time step t as h i l ( t ), or simply h i that the label assigned to a variable at the end of the process is the one with the highest weight ( max ( h i )). Figure 12 shows an example.
 each variable, which is defined as the weighted sum of the support received by each of its possible labels X  X hat is, L i l = 1 h i l  X  S il , where S from the context.
 signment of label l to variable v i compared with the labels of neighboring variables, according to the edge weights. Although several support functions may be used (Torras 1989), we chose the following (Equation (2)), which defines the support as the sum of the influences of the incident edges.
 weight and the influence rules attached to the constraints involved with this edge (see
Section 3.3). An influence rule determines how the current probabilities for the same label of adjacent vertices ( h j l ) are combined.
 of the following steps: 1. Start with a random labeling, or with a better-informed initial state. 2. For each variable, compute the support that each label receives from the 3. Normalize support values between  X  1 and 1. Supports are divided by a 4. Update the weight of each variable label according to the support obtained 858 5. Iterate the process until the convergence criterion is met. The usual process searches the partitioning  X   X  which optimizes the goodness function F (  X  , W ), which depends on the edge weights W. In this manner,  X   X  is optimal if: each variable the label with maximum probability. The supports and the weighted labeling depend on the edge weights (Equation (2)). To satisfy Equation (6) is equivalent to satisfying Equation (5). Many studies have been done towards the demonstration of the consistency, convergence, and cost reduction advantages of the relaxation algorithm (Rosenfeld, Hummel, and Zucker 1976; Hummel and Zucker 1983; Pelillo 1997). For instance, Hummel and Zucker (1983) prove that maximizing average consistency (left-hand-side term of Equation (6) produces labelings satisfying Equation (5) when only binary constraints are used. Although there is no formal proof for higher order constraints, the presented algorithm (that forces a stop after a number of iterations) has proven useful for practical purposes in our case.
 the algorithm can be straightforward parallelized. In the following, there are some examples of the Relax implementation of the edge influences ( Inf ( e )) given the influence rules attached to the constraints.
 m . The influence rule attached to the constraint is (0) A  X  Equation (7) and is the kind of influence used in the mention-pair model.
 the resolution in order to influence mention m 2 . The influence rule is (0, 1)
Equation (8). Mentions m 0 and m 1 are tending to corefer (belong to the same entity: l ) when their values for label l are tending to 1 (and the other labels are tending to 0). In this case, multiplying h 0 l and h 1 l achieves a value close to 1, and the influence is almost the weight of the edge. In other cases when the coreference between m clear (or they are clearly not coreferent), at least one of the values of h close to 1 and the value of their product rapidly decreases, so the influence of the edge also decreases.
 it is required that m 1 does not belong to the same entity as m using its complementary value (1  X  h 0 l ), as is shown in Equation (9). The corresponding influence rule is (0) A ,(1) B  X  (2) A .
 any number of mentions and entities can be involved. This last example (Equation (10)) shows how to represent (0, 2) A ,(1) B  X  (3) B , an influence rule requiring m belong to the same entity, while m 1 belongs to a different one in order to influence m 4. RelaxCor R
ELAX C OR is the coreference resolution system implemented in this work to perform experiments and test the approach explained in Section 3. This section explains the implementation and training methods, before the experiments and error analysis are presented in the following sections. R ELAX C OR is programmed in Perl and C++, is open source, and is available for download from our research group X  X  Web site. detection system determines the mentions of the input document and their boundaries.
The mention detection system is explained in Section 4.1. Alternatively, true mentions can be used when available, allowing this step to be skipped. Next, for each pair or group of mentions (depending on the model), the set of feature functions calculate their values, and the set of model constraints is applied. The set of feature functions used 860 by R ELAX C OR and its knowledge sources are explained in Section 4.2. A (hyper)graph labeling is executed to find the partitioning that maximizes constraint satisfaction. tions 4.3 and 4.4. The former explains the method for training the mention-pair model, and the latter concerns the entity-mention model. 4.1 Mention Detection R
ELAX C OR includes a mention detection system that uses part-of-speech and syntactic information. Syntactic information may be obtained from dependency parsing or con-stituent parsing. The system extracts one candidate mention for every: tags and a set of rules from Collins (1999) when constituent parsing is used, or using dependency information otherwise. In case some NPs share the same head, the larger
NP is selected and the rest are discarded. Also, mention repetitions with exactly the same boundaries are discarded. Note that a mention detection system in pipeline configuration with the resolution process acts as a filter and the main objective at this point is to achieve as much recall as possible. 4.2 Knowledge Sources and Features
The system gathers knowledge using a set of feature functions that interpret and evalu-ate the input information according to some criteria. Given a set of mentions numbered from 0 to N  X  1 following the order found in the document, each feature function evaluates their compatibility in a specific aspect. R all linguistic layers: lexical, syntactic, morphological, and semantic. Moreover, some structural features of the discourse have also been used, such as distances, quotes, and sentential positions. A feature function with only one argument indicates that it offers information about only one mention. For example, REFLEXIVE(0) indicates that mention 0 is a reflexive pronoun. Figure 15 shows an exhaustive list of the features used and a brief description of each one.
 of binary features favors a better performance in this type of learning (Rounds 1980;
Safavian and Landgrebe 1991), all of the used feature functions are binary. The original sources that had a list of possible values have been binarized by a set of feature functions that each represent a different value. Even in numerical cases, there is a set of binary ranges. 4.3 Training and Development for the Mention-Pair Model This section describes the training and development process for the implementation of R
ELAX C OR using the mention-pair model and the graph representation. The training precision of the constraint finding coreferent mentions.
 can also be added writing them by hand. Adding manual constraints is expensive, 862 however, given that it takes a group of linguistic experts many hours devoted to this task. An alternative option is to use constraints from other coreference resolution automatically learned constraints.
 the training data set and then a machine learning process obtains the constraints. is evaluated. The precision of each constraint determines its weight. The develop-ment process optimizes two parameters X  balance and N prune imum performance given a measure for the task. Figure 17 shows the development process. 4.3.1 Data Selection. Generating an example for each possible pair of mentions in the training data produces an unbalanced data set in which more than 99% of the examples are negative (not coreferent). This bias towards negative examples makes the task of the machine learning algorithms difficult. Many classifiers simply learn to classify every the case of decision trees and rule induction, this imbalance is also counterproductive.
In addition, some corpora have more examples than the maximum affordable by the learning algorithm, given our computational resources. In this case, it is necessary to reduce the number of examples.
 to clustering is run using the positive examples as the centroids. We define the distance between two examples as the number of features with different values. A negative example is then discarded if the distance to all the positive examples is always greater than a threshold, D . The value of D is empirically chosen depending on the corpora and the computational resources available.
 4.3.2 Learning Constraints. Constraints are automatically generated by learning a deci-sion tree and then extracting rules from its leaves using C4.5 software (Quinlan 1993).
The algorithm generates a set of rules for each path from the learned tree, then checks whether the rules can be generalized by dropping conditions. These rules become our rules from a decision tree that are useful in constraint satisfaction algorithms (M ` arquez, Padr  X  o, and Rodr  X   X guez 2000).
 but shifted by a balance value: of positive examples and the number of examples where the constraint applies. Note that the data selection process (Section 4.3.1) discards some negative examples to learn the constraints, but the weight of the constraints is calculated with the precision of the constraint over the whole training data.
 tween precision and recall. On the one hand, a high balance value causes most of the weight. In this case, the system is precise but the recall is low, given that many rela-tions are not detected. On the other hand, a low value for balance causes many low-precision constraints to have a positive weight, which increases recall but also decreases precision (see Figure 18). The correct value for balance is thus a compromise solution found in the development process, optimizing performance for a specific evaluation measure.
 864 4.3.3 Pruning. As explained in Section 3.2, when a constraint applies to a set of mentions, a corresponding hyperedge is added to the hypergraph. In the case of the mention-pair model with automatically learned constraints, the most typical case is that each pair of mentions satisfy at least one constraint, which produces an edge for each pair of mentions. There are three main issues to take into account when the problem is represented by an all-connected graph: incidence list E ( v i ), only a maximum of N prune edges remain and the others are pruned.
In particular, the process keeps the N prune / 2 edges with the largest positive weight and the N prune / 2 with the largest negative weight. The value of N by maximizing performance over the development data. After pruning, (i) the contribu-tion of the edge weights does not depend on the size of the document; (ii) most edges of the less informative pairs are discarded, avoiding further confusion without limitation on distance or other restrictions that cause a loss of recall; and (iii) computational costs are reduced from O ( n 3 )to O ( n 2 ), given that the innermost loop has a constant number of iterations ( N prune ). 4.3.4 Reordering. Usually, the vertices of the graph would be placed in the same order corresponds to m i . As suggested by Luo (2007), however, there is no need to generate the model following that order. In our approach, the first variables have a lower number of possible labels. Moreover, an error in the first variables has more influence on the performance than an error in later ones. It is reasonable to expect that placing named usually the most informative mentions.
 logical order of the document is taken into account by the constraints, regardless of the graph representation. Our experiments (Sapena, Padr  X  o, and Turmo 2010a) confirm that placing named entity mentions first, then nominal mentions, and finally the pronouns, increases the precision considerably. Inside each of these groups, the order is the same as in the document. 4.4 Training and Development for the Entity-Mention Model
The training process for the entity-mention model is, in theory, exactly the same as for the mention-pair model, but with predefined influence rules and groups of N mentions instead of pairs. For each combination of influence rule and N , the training process has the same steps as explained in previous sections: Learn constraints, apply them to the training data, calculate the weights, and perform the development process to find the optimal balance value. The positive examples are those that satisfy the final condition of the influence rule, and the rest are negative examples. A machine-learning process to discover group constraints has a considerable cost, however, if all the training data need to be evaluated. The number of combinations increases exponentially as the number of implied mentions increases. Moreover, the ratio of positive to negative examples (Section 4.3.1) has a high computational cost.
 only the examples that the mention-pair model could not solve. Thus, after training and running R ELAX C OR over an annotated data set using just pair constraints, its errors are now used as examples for training the entity-mention model. The type of errors are those in which three mentions ( N = 3) corefer (0, 1, 2) pair model has determined that just two of them corefer and discarded the third one correspond to a positive example (corefer) and all other combinations of three men-tions between mentions 0 and 2 are considered negative examples. The influence rules for the constraints learned this way are (0, 1) A  X  (0)
A , depending on which mention was wrongly classified by the mention-pair model.
 system is executed using both the mention-pair and entity-mention models at the same time.
 by writing them. Figure 19 shows an example of a manually written entity-mention constraint (i.e., a group constraint with an influence rule). This kind of constraint has great potential to take advantage of the structure of discourses. The example shows how the algorithm can benefit from knowing that nested mentions have some kind of relation. In the case that two coreferring mentions are related with two other mentions with the potential to corefer, the entity-mention model can use this information to find more coreference relations.
 as for the mention-pair model. The weights of group constraints are obtained by 866 evaluating their precision over the training data, and the balance value is determined by a development process. In our experiments, however, the number of group constraints is typically lower than the number of pairwise ones, so there is no need for pruning. 4.5 Related Work
In Section 2, we introduced an overview of many approaches, with their classification models and resolution processes (see Figure 5). Our approach can be classified simi-larly as a one-step resolution that uses the entity-mention model for classification and conducts hypergraph partitioning for the linking process. This classification matches that of the C OPA system described in Cai and Strube (2010). Both approaches represent the problem in a hypergraph, where each mention is a vertex, and use hypergraph partitioning in order to find the entities. The differences between these two approaches are substantial, however. The most significant differences are as follows: 5. Experiments and Results
Several experiments have been performed on coreference resolution in order to test our approach. This section includes a short explanation and result analysis of the most significant experiments. First, there is an explanation of a set of experiments to evaluate the performance of coreference resolution and mention detection. The scores
Next, our participation in Semeval-2010 and CoNLL-2011 shared tasks is explained in detail with performance, comparisons, and error analysis. Finally, a set of experiments using the entity-mention model are described.
 sures to facilitate replication and comparison. Corpora used are ACE 2002 (NIST 2003), the same portion of OntoNotes v2.0 used in Semeval-2010 (Recasens et al. 2010), and the same portion of OntoNotes v4.0 used in CoNLL Shared Task 2011 (Pradhan et al. 2011).
Regarding the measures, we used MUC (Vilain et al. 1995), B 1998), and two variants of CEAF (Luo 2005): mention-based (CEAFm) and entity-based (CEAFe). 5.1 Mention Detection
The performance of the mention detection system achieves a good recall, higher than 90%, but a low precision, as published in Sapena, Padr  X  o, and Turmo (2011) and repro-duced in Table 1. The OntoNotes corpora have been used for this experiment, as they were used in CoNLL-2011. Given that the mention detection in a pipeline combination acts as a filter, recall should be kept high, as a loss of recall at the beginning would result in a loss of performance in the rest of the process. At this point, however, the precision is not a priority as long as it remains reasonable, given that the coreference resolution pro-cess is able to determine that many mentions are singletons. Moreover, the evaluation of precision on the OntoNotes corpora only take into account mentions included in a coreference chain, not singletons. The R ELAX C OR output, however, includes singletons.
This means that the precision value is not really evaluating the precision of the mention detection system. A fair evaluation of mention detection should be performed in a corpus with annotations of every referring expression, but such a corpus is not available as far as we know.
 not referential (e.g., predicative and appositive phrases) and mentions with incorrect boundaries. The incorrect boundaries are mainly due to errors in the predicted syntactic 868 column and some mention annotation discrepancies. Furthermore, the coreference an-notation of OntoNotes used in CoNLL-2011 included verbs as anaphors of some verbal nominalizations. But verbs are not detected by our mention detection system, so most of the missing mentions are verbs. The methodology of the mention detection system is explained in Section 4.1. 5.2 State-of-the-Art Comparison R
ELAX C OR performance has been compared several times with other published results from state-of-the-art systems. We claimed Sapena, Padr  X  o, and Turmo (2010a) to have the best performance for the ACE-phase02 corpus, using true mentions in the input and evaluating with the CEAF and B 3 measures. The table comparing scores with the best results found at that moment is reproduced as Table 2.
 SemEval-2010 (Sapena, Padr  X  o, and Turmo 2010b) and CoNLL-2011 (Sapena, Padr  X  o, and
Turmo 2011). R ELAX C OR achieved one of the best performances in SemEval-2010, but contradictory results across measures prevented the organization from determining a winner. In addition, R ELAX C OR achieved second position in the CoNLL-2011 Shared
Task; Figure 20 reproduces the official table of results. Following sections describe the shared tasks in detail.
 the-art systems in M ` arquez, Recasens, and Sapena (2012). 5.2.1 SemEval-2010. The goal of SemEval-2010 task 1 (Recasens et al. 2010) was to eval-uate and compare automatic coreference resolution systems for six different languages in four evaluation settings and using four different evaluation measures. This complex scenario aimed at providing insight into several aspects of coreference resolution, in-cluding portability across languages, relevance of linguistic information at different levels, and behavior of alternative scoring measures. The task attracted considerable attention from a number of researchers, but only six teams submitted results. Moreover, participating systems did not run their systems for all the languages and evaluation settings, thus making direct comparisons among all the involved dimensions very difficult.
 mentions. Thus, participation was restricted to the gold-standard evaluation, which included the manual annotated information and also provided the mention boundaries. respectively. The version of R ELAX C OR used in SemEval had a balance value fixed to 0.5, which proved to be an inadequate value. Thus, the results have high precision but a very low recall. This situation produced high scores with the CEAF and B due in part to the annotated singletons. The system was penalized by measures based on pair-linkage, however, particularly MUC. Although R 870 precision scores (even with MUC), the recall was low enough to finally obtain low scores for F 1 .
 2010), R ELAX C OR obtained the best performance for Catalan (CEAF and B (closed: CEAF and B 3 ; open: B 3 ), and Spanish ( B 3 ). Moreover, R precise system under all metrics in all languages, except for CEAF in English-open and Spanish. This confirms the robustness of the results of R the necessity of searching for a balance value other than 0.5 to increase the recall of the system without losing much by way of precision. Indeed, the idea of using development (Section 4.3) to adapt the balance value occurred after these results were obtained. difference between our implementation in the English-open and English-closed tasks.
The scores were slightly higher when using WordNet, but not significantly so (75.8% vs. 75.6% for CEAF and 34.2% vs. 33.7% for MUC). Analyzing the MUC scores, note (from 74.4% to 70.5%), which corresponds to the information and noise that WordNet typically provides.

Recasens, and Sapena (2012). 5.2.2 CoNLL-2011. The CoNLL-2011 Shared Task was based on the English portion of the OntoNotes 4.0 data 5 (Pradhan et al. 2011). As is customary for CoNLL tasks, there was a closed and an open track. For the closed track, systems were limited to using the distributed resources, in order to allow a fair comparison of algorithm performance, whereas the open track allowed for almost unrestricted use of external resources in addition to the provided data. About 65 different groups demonstrated interest in the shared task by registering on the task Web page. Of these, 23 groups submitted system outputs on the test set during the evaluation week. Eighteen groups submitted only closed track results, three groups only open track results, and two groups submitted both closed and open track results.
 2011). All the knowledge required by the feature functions was obtained from the annotations of the corpus, and no external resources were used with the exception of
WordNet, gender and number information, and sense inventories. All of these were allowed by the task organization and are available on their Web site.
 lack of annotated singletons, mention-based metrics B 3 and CEAF produce lower scores (near 60% and 50%, respectively) than typically achieved with different annotations and mapping policies (usually near 80% and 70%). Moreover, the requirement that systems use automatic preprocessing and do their own mention detection increases the difficulty of the task, which obviously decreases the scores in general. The official ranking score was the arithmetic mean of the F-scores of MUC, B 3 , and CEAFe.

Thus, it is the only one comparable with the state of the art at this point. The results obtained with the MUC scorer show an improvement in R ELAX that needed improvement given the remarkably low SemEval-2010 results with MUC.
Note that these improvements in MUC scores, specially in recall, are mainly due to the introduction of the balance value in the development process but also to many other refinements done in the whole process such as new feature functions and bug fixing.
 system X  X tanford (Lee et al. 2011) X  X oes not use machine learning but combines ones. It is thought that the difference between R matically learned constraints. Note that Lee et al. (2011) solve coreferences by applying straints given that these ones have the highest weights and are the most influencing ones. 5.3 Languages
Sapena, Padr  X  o, and Turmo (2010b), and M ` arquez, Recasens, and Sapena (2012) show the performance of our approach for English, Catalan, and Spanish. The scores for Spanish and Catalan do not seem as good as for English, because the system was originally designed with the English language in mind. As a result, it does not include language-among the state of the art. 872 = = 5.4 Experiments with the Entity-Mention Model
Constraints for the entity-mention model are automatically obtained using the training data examples that the mention-pair model could not solve, with predefined influence rules and limited to N = 3. The training process is explained in Section 4.4. Experiments with the entity-mention model are conducted using both models at the same time. The goal of the experiments is to improve the performance of the mention-pair model itself. table compares the entity-mention results (R ELAX C OR influence rules, including the whole set of N = 2 constraints) with those using mention-pairs (R ELAX C OR using just N = 2 constraints). The entity-mention model outperforms the mention-pair model. The number of really useful examples (i.e., mentions wrongly classified by the mention-pair model but correctly classified by the entity-mention model), however, is low. Consequently, the difference in their scores is not significant.
The N = 3 constraints have a good precision and also an acceptable recall, although most of the mentions affected by these constraints were already affected and correctly solved by the mention-pair model. Further research is needed in order to find more useful constraints, either by writing more elaborate group constraints or finding a better system that automatically finds them.
 the same feature functions and, consequently, the same information as the mention-pair model. In fact, only the new information is that information already included in the conditions of the influence rules, which take into account the entities assigned to each mention during resolution. In addition, group constraints can also include, in an implicit way, information about the structure of the discourse. It seems clear, however, that this new information is either minimal or not relevant enough. Figure 21 shows an example of a learned entity-mention constraint.
 mention-pair model, we can draw some positive conclusions from these experiments.
First of all, the approach is ready to use either model (mention-pair or entity-mention) in a constructive way. As soon as new feature functions specific to entity-mention models appear, the results will reflect this. One research line to follow in this field is the incorporation of feature functions following discourse theories, such as focusing and centering. Another research line is the introduction of world knowledge using these models, as explained in the next section. 6. Adding World Knowledge to Coreference Resolution
Often, common sense and world knowledge is essential to resolve coreferences. For example, we can find coreferential mentions in any newspaper, such as
President } , { Messi , Barcelona striker } ,or { Beirut , the Lebanese capital a lack of world knowledge, the partial and total scores of R set of OntoNotes 2.0 (the same data set used for the English task in SemEval-2010) are shown in Table 9 for each mention class described in Table 8. Analyzing the table, we observe that PN N ,CN P ,andCN N are the classes with the lowest recall, especially
PN N and CN N . In addition, PN N and CN N have the lowest precision. The final column shows the number of mentions corresponding to the class of that row and the percentage representing the total number of coreferent mentions. Note that these three classes together represent 27% of coreferent mentions.
 using similar information, and even other languages. Therefore, these classes require attention in order to improve global performance, and the fact that lexical, morpholog-ical, syntactic, and semantic levels are not very useful to deal with them encourages the research on adding world knowledge to coreference resolution systems. In state-of-the-art systems, we can find some attempts to add world knowledge to coreference resolution, using Wikipedia (Ponzetto and Strube 2006; Uryupina et al. 2011) or YAGO and Freenet (Rahman and Ng 2011a).
 874 ence resolution, represented in Figure 22. The nature of our model allows the integration of world knowledge encoded not only as features, but also as constraints, which is a more expressive and natural way.
 of R ELAX C OR to absorb knowledge from heterogeneous sources. Results show that although the algorithm is able to successfully handle the added knowledge, the per-formance is hardly increased due to the noisy nature of the knowledge automatically extracted from Wikipedia.
 ledge potentially useful for the resolution of coreferences is acquired from Wikipedia, and second, this knowledge is incorporated to R ELAX models: feature functions and constraints. These phases are described in Sections 6.1 and 6.2, respectively. Finally, Section 6.3 describes our experiments and analyzes their results. 6.1 Acquiring World Knowledge
Our methodology to acquire world knowledge useful for coreference resolution consists of finding the real-world entities occurring in the document (i.e., Entity Linking) and extracting information related to them from Wikipedia.
 6.1.1 Entity Linking. One approach to finding real-world entities mentioned in a docu-ment is to select the NE mentions of that document and to disambiguate them in order to determine which entities in the real world X  X n our case, which Wikipedia entries X  are referred to by the mentions. Using every NE mention in a document, however, may add noise to the process of coreference resolution. For instance, consider a document with Bill Clinton and, some sentences later, Clinton . If we try to get information about
Clinton from Wikipedia, we obtain a page about the English family name Clinton with
Clinton appears in the same document, it seems more convenient to select the most informative NE mention and discard the less informative ones, like Clinton , which are probably pointing to the same real-world entity. This is why we just take into account the most informative NE mentions, called MI mentions from now on.
 where the ALIAS function is true for all the pairs, and all mentions in the group belong to the same class (Person, Organization, or Location). Finally, the longest NE mention from each group is selected as an MI mention.
 an information retrieval system to find the most relevant pages in Wikipedia. The query is generated from the MI mention as the mention head plus all nouns, proper names, and adjectives that appear immediately before it. From the results provided by Google, we select as the real world entity for the MI mention the first URL that corresponds to a Wikipedia entry and includes the head of the MI mention (or a string that matches as an alias) in the title or in the first sentence of the first paragraph. we assume that the MI mention does not exist in Wikipedia. 6.1.2 Information Extraction. For each Wikipedia entry obtained in the entity disambigua-tion step, we extract information from the description, the infobox, and the categories the  X  X hat Links Here X  section. Concretely, we extract all names (i.e., official names, nicknames, and aliases), as well as properties indicating the most descriptive aspects or qualities of the entity.
 NP-&lt;property&gt;  X ). 7 name , office , title , profession , company name , playername , occupation , nickname , official name , native name , settlement type , type . In addition, all categories associ-ated with the entry are also extracted as properties.

Here X  section, those sentences including the link are selected. From each one of them, the anchor text used to link the entry is extracted as a name. In addition, the following 876 expressions are extracted as properties using patterns: the set of nouns and adjectives to the left of the anchor text, the set of NPs in apposition to the link, and the set of NPs denoting a property of a list of entries in which one of them is the current one (e.g.,  X  &lt; NP &gt; -&lt; property &gt; such as entry 1 , entry with each expression X  X he most repeated expressions are the most reliable. In order to avoid incorrect information as much as possible, we define a threshold below which all the extracted names and properties are discarded. 6.2 Incorporating World Knowledge to the Models
Two approaches for the incorporation of the knowledge extracted from Wikipedia have been studied. The first is to add some feature functions for the mention-pair model that evaluate whether a pair of mentions may corefer according to Wikipedia X  X  information, similar to other state-of-the-art studies (Ponzetto and Strube 2006; Rahman and Ng 2011a). The second approach adds a set of constraints to the hypergraph connecting groups of mentions, using the entity-mention model. 6.2.1 Feature Functions. In this approach, new feature functions are added to evaluate pairs of mentions, and some learned constraints may use them as any other feature function. These feature functions are only applied to pairs &lt; MI , X &gt; , where MI is a MI mention and X is any other mention but a pronoun, and use the information extracted from Wikipedia to determine their value. Concretely, the feature functions used in our experiments are the following ones: 6.2.2 Constraints. In this approach, world knowledge is incorporated by adding con-straints relating the mentions that may corefer given the extracted information about the entities. In this case, the features of the previous model are now replaced by constraints.
In addition, other constraints can be added to take advantage of the entity-mention model. The following is a list of constraints used in our experiments: behind cWiki3 is to link the nominal mention (2, The organization ) with a closer men-tion in the document than the MI mention (0, the Organization of Petroleum Exporting
Countries ). Linking nearest mentions may take advantage of information given by other constraints, such as syntactic patterns. When the Organization of Petroleum Exporting
Countries is tending to corefer with OPEC , mention The organization is influenced by both mentions. The second case, cStructWiki3 , takes advantage of a typical discourse structure where the same entity is the subject of some consecutive sentences. First mention 0, Google Inc. , is the MI mention, whereas 2 ( Google ) is just an alias. Between them we find a nominal mention ( The company ), which we expect to solve using world knowledge. Both N = 3 constraints are expected to have high precision but low recall. model. The difference is that, in the case of constraints, they are always applied when
WIKI ALIAS and WIKI DESC are true, and so their weight is added to the edge weight of that pair in the hypergraph. In the model using feature functions, however, the constraints learned by the model may or may not include those features. 6.3 Experiments and Results
The experiments consist of the execution of R ELAX C OR using each one of the models to incorporate information. R ELAX C OR + features incorporates the new features to the original model and repeats the training process from the beginning. Constraints are learned using these new feature functions mixed with all the others (a detailed list of features is in Section 4.2). R ELAX C OR + constraints incorporates the new constraints. In this case, the learning process uses the constraints already learned for R adds the new constraints to the model. The training process is then applied normally to compute the weight of the constraints using their precision in the training files. the results of R ELAX C OR without world knowledge. The first three columns list the 878 results of R ELAX C OR using the mention-pair model, as explained in Section 4, the next three columns are the results of R ELAX C OR adding the features of Section 6.2.1, and the final three columns are the scores for R ELAX C OR with the constraints of Section 6.2.2. Note that the main improvements are focused around PN expected. Moreover, the global scores also improve, but the global improvements are not statistically significant.
 there are some collateral effects that decrease the performance for other classes such as PN P and P 3 U (ungendered pronouns: it ). The latter is a strong decrease and, given that the class P 3 U represents 18% of the total coreferent mentions, this affects the global results. This decrease in pronoun classification performance is related to the balance value learned in the development process. One possible solution would be to have a different balance value depending on the class.
 that the improvement in global scores is in precision but not in recall. This is because the development process is optimizing scores for the CEAF measure, which encourages precision more than recall compared with the MUC scorer.
 knowledge. In general terms, we have found that, although performance is slightly improved on average, few new coreference relations are found, taking into account the expected potential for improvement. Moreover, some of these new relations do not change the final output and, even worse, many of them are incorrect. In addition, some coreferences that were correctly solved before this process are now incorrectly classified. In particular, the recall of ungendered pronouns has decreased considerably. reduced number of mentions in test documents that end up having an actual Wikipedia-influenced constraint (e.g., fewer than 1% of the mentions in features model). Thus, better extraction procedures or a knowledge source more suitable for entities appearing in the target documents should yield larger improvements.
Sapena (2012). The error analysis shows how the extracted knowledge was often redun-dant (i.e., used only in cases where the algorithm already produced the right answer) or noisy (due to errors in the entity disambiguation or information extraction steps).
Thus, we think that the experiments show that R ELAX C OR knowledge into the resolution model in an easy and natural way, and that further work is required on acquiring more accurate and useful knowledge to feed the coreference resolution process. 7. Conclusions
In this work, we defined an approach based on constraint satisfaction that represents the problem in a hypergraph and solves it by relaxation labeling, reducing coreference resolution to a hypergraph partitioning problem under a set of constraints. Our ap-proach manages mention-pair and entity-mention models at the same time, and is able to introduce new information by adding as many constraints as necessary. Furthermore, our approach overcomes the weaknesses of previous approaches in state-of-the-art systems, such as linking contradictions, classifications without context, and a lack of information in evaluating pairs.
 mention-pair model without new knowledge. Moreover, experiments with the entity-mention model showed how it is able to introduce knowledge in a constructive way. in the field of coreference resolution, the process required to introduce such information in a constructive way has not yet been found. In this work, we tested a methodology that identified the real-world entities referred to in a document, extracted information about them from Wikipedia, and then incorporated this information in two different ways in the model. It seems that neither of the two forms work very well, however, and that the results and errors are in the same direction: The slight improvement of the few new relationships is offset by the added noise. Other state-of-the-art systems have better improvements than ours (Ponzetto and Strube 2006; Uryupina et al. 2011; Rahman and
Ng 2011a), but these also seem too modest given the large amount of information used and the room for improvement outlined in the Introduction.
 to incorporate it. The extracted information is biased in favor of the more famous and popular entities (those in Wikipedia, and having larger entries). This causes the system to find more information about these entities, including false positives, and causes an imbalance against entities with little or no information in Wikipedia. Moreover, it is not possible to use negative information in the absence of complete information. more reliable and concise information, so that the information added, no matter how minimal, should always be constructive and avoid false positives. On the other hand, we would need to find some process of reasoning to expand the scope of the information obtained using logic and common sense. Only then could the full potential of the knowledge base be exploited.
 Acknowledgments 880 References 882
