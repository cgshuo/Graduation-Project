 Vanderbilt University University of Texas at Dallas ument collection. Extracting a rich set of features for each event mention allows us to cast time, etc.).
 unsupervised way stem from (a) the choice of representing ev ent mentions through a rich set each event mention. Furthermore, to overcome some of the lim itations of this extension, we infer the number of features associated with each event ment ion from data and, at the same this task. 1. Introduction
Event coreference resolution consists of grouping togethe r the text expressions that refer to real-world events (also called event mentions ) into a set of clusters such that all the mentions from the same cluster correspond to a unique event. The problem of event coreference is not new. It was originally studied in philosop hy, where researchers tried to determine when two events are identical and when they are d ifferent. One relevant theory in this direction was proposed by Davidson (1969), wh o argued that two events are identical if they have the same causes and effects. Later on, a different theory was proposed by Quine (1985), who considered that each event is a ssociated with a physical object (which is well defined in space and time), and therefore , two events are identical if their corresponding objects have the same spatiotemporal location. According to Malpas (2009), in the same year, Davidson abandoned his sugg estion to embrace the Quinean theory on event identity (Davidson 1985).
 processing (NLP) applications. For instance, in topic dete ction and tracking, event coreference resolution is required in order to identify new seminal events in broadcast news that have not been mentioned before (Allan et al. 1998). In information extraction, event coreference information was used for filling predefine d template structures from text documents (Humphreys, Gaizauskas, and Azzam 1997). In qu estion answering, a novel method of mapping event structures was used in order to provide answer justi-fication (Narayanan and Harabagiu 2004). The same idea of mapp ing event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi,
Ng, and Manning 2005). Event coreference information was al so used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008).
 ing methods that explore various linguistic properties in o rder to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizausk as, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite of being successful for a particular labeled corpus, in gene ral, these pairwise models are dependent on the domain or language that they are trained on. For instance, in order to adapt a supervised system to run over a collection of documents written in a different language or belonging to a different domain of in terest, at least a minimal annotation effort needs to be performed (Daum  X  e III 2007). Furthermore, because these models are dependent on local pairwise decisions, they are u nable to capture a global event distribution at the topic-or document-collection le vel.
 proaches for event coreference resolution and explored how a new class of unsuper-vised, nonparametric Bayesian models can be used to probabi listically infer coreference clusters of event mentions from a collection of unlabeled do cuments. In addition, because an event can be mentioned multiple times in a documen t collection and its mentions may occur both in the same document or across multip le documents, we designed our unsupervised models to solve the two subproble ms of within-document and cross-document event coreference resolution. In order to evaluate the unsup ervised models for these two subproblems, we annotated a new data set encoding both within-and cross-document event coreference information.
 cross-document event coreference, in this article we prese nt novel Bayesian models that provide a more flexible framework for representing data t han current models. By starting from the generic problem of clustering observable linguistic objects (i.e., event 312 mentions) encoded into a large collection of text documents where the clusters (i.e., events) can be shared across documents, we devised our unsup ervised models such that they provide solutions to the following four desiderat a:
We believe that these four desiderata constitute a more natu ral approach for clustering complex linguistic objects from a large collection of docume nts and relax many of the constraints imposed in the current clustering tasks.
 ated by tasks not only from the area of computational linguis tics, but also from other research areas as well. For instance, in biomedical informa tics, clinical researchers can use the new Bayesian models to perform studies over various c ohorts of patients. In this configuration, the observations to be clustered corres pond to patients, and the features associated with the patients can be extracted from clinical reports or can be represented by structured clinical information (e.g., whi te blood cells, temperature, heart rate, respiratory rate, sputum culture). Another ins tance of the generic problem described here is from data mining. In this domain, clusterin g tasks can be performed over structured information stored in large tables (e.g., p roducts, restaurants, hotels).
For this type of problem, each object is associated with a row i n a table and the features correspond to table columns. 2. Related Work
Unlike entity coreference resolution, event coreference r esolution is a relatively less-studied task. One rationale is that events are expressed in m any more varied linguistic constructs. For example, event mentions are typically pred ications that require more complex lexico-semantic processing, and furthermore, the capability of extracting fea-tures that characterize them has been available only since s emantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and
Lowe 1998) corpora have been developed. In contrast, entity c oreference resolution has been intensively studied and many successful technique s for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghi ghi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011).
 tational linguistic researchers, there is only limited wor k that incorporates event-related information to solve entity coreference, typically by cons idering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions a s features in their model.
Rahman and Ng (2011) used event-related information by look ing at which semantic role the entity mentions can have and the verb pairs of their p redicates. More recently,
Lee et al. (2012) proposed an approach to jointly model event a nd entity coreference by allowing information from event coreference to help entity coreference, and the other way around. Their supervised method uses a high-precision e ntity resolution method based on a collection of deterministic models (called sieve s) to produce both entity and event clusters that are optimally merged using linear regre ssion. A similar technique that treated entity and event coreference resolution jointl y was reported in He (2007) using narrative clinical data.
 template merging task required in MUC evaluations and was pr imarily focused on scenario-specific events (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999). More recently, various supervised approaches using a mention-pair probabilistic framework (Ahn 2006), spectral graph clustering (Chen and J i 2009), and tree kernel X  based methods (Chen, Su, and Tan 2010b) have been used to solv e event coreference.
Tree kernel X  X ased methods have also been used to solve a spec ial case of event coref-erence resolution called event pronoun resolution (Chen, S u, and Tan 2010a; Kong and
Zhou 2011). To the best of our knowledge, the framework for so lving event coreference presented in this article, extending the approach reported in Bejan and colleagues (Bejan et al. 2009; Bejan and Harabagiu 2010), is the only line of resea rch on event coreference resolution that uses fully unsupervised methods and is base d on Bayesian models. of solving similar problems or subproblems of the generic pr oblem presented in the previous section. In 2003, Blei, Ng, and Jordan proposed a par ametric approach, called latent Dirichlet allocation (LDA), for automatically learning probability distributi ons of words corresponding to a specific number of latent classes (or topics ) from a large 314 collection of text documents. In this latent class model, doc uments are expressed as probabilistic mixtures of topics, while each topic has assi gned a multinomial distribu-tion over the words from the entire document collection. Thi s approach also uses an exchangeability assumption by modeling the documents as ba gs of words. The LDA model and variations of it have been used in many application s such as topic modeling (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), wor d sense disambiguation (Boyd-Graber, Blei, and Zhu 2007), object categorization fr om a collection of images (Sivic et al. 2005, 2008), image classification into scene ca tegories (Li and Perona 2005), discovery of event scenarios from text documents (Bejan 2008 ; Bejan and Harabagiu 2008b), and attachment of attributes to a concept ontology ( Reisinger and Pas  X ca 2009).
The LDA model, although attractive, has the disadvantage of requiring a priori knowl-edge regarding the number of latent classes.
 process (HDP) model described in Teh et al. (2006). Like LDA, this mode l considers problems that involve groups of data, where each observable object is sampled from a mixture model and each mixture component is shared across g roups. However, the
HDP mixture model is a nonparametric generalization of LDA th at is also able to automatically infer the number of clustering components K (the first desideratum for our problem). It consists of a set of Dirichlet processes (DPs) (Ferguson 1973), in which each DP is associated with a group of data. In addition, these D Ps are coupled through a common random base measure which is itself distributed acc ording to a DP. Due to the fact that a DP provides a nonparametric prior for the numb er of classes K , the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existi ng HDP inference algo-rithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 20 12). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 20 08), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2 007).
 inferring the number of categorical outcomes K , they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each observable object can be generated from a c ombination of multiple features. For example, in HDP, each data point is represented only by its corresponding word. For this reason, we built new Bayesian models on top of a lready-existing models with the main goal of providing a more flexible framework for re presenting data.
The first model extends the HDP model such that it takes into acc ount additional linguistic features associated with event mentions. This e xtension is performed by using a conditional independence assumption between the ob served random variables corresponding to object features. Thus, instead of consider ing as features only the words that express the event mentions (which is the way an observab le object is represented in the original HDP model), we devised an HDP extension that is a lso able to represent features such as location, time, and agent for each event men tion. This extension was inspired from the fully generative Bayesian model proposed by Haghighi and Klein (2007). However, Haghighi and Klein X  X  model was strictly cust omized for the task of entity coreference resolution. As also noted in Ng (2008) an d Poon and Domingos (2008), whenever new features need to be considered in Haghig hi and Klein X  X  model, the extension becomes a challenging task. Also, Daum  X  e III and Marcu (2005) performed related work in this direction by proposing a generative mod el for solving supervised clustering problems.
 that are able to represent feature-rich objects is the Indian buffet process (IBP) model presented in Griffiths and Ghahramani (2005). The IBP model de fines a distribution over infinite binary sparse matrices that can be used as a nonp arametric prior on the features associated with observable objects. Moreover, ext ensions of this model were considered in order to provide a more flexible approach for mod eling the data. For example, the Markov Indian buffet process (mIBP) (Van Gael, Teh, and Ghahramani 2008) was defined as a distribution over an unbounded set of bi nary Markov chains, where each chain can be associated with a binary latent featu re that evolves over time according to Markov dynamics. Also, the phylogenetic Indian buffet process (pIBP) (Miller, Griffiths, and Jordan 2008) was created as a non-exc hangeable, nonparametric prior for latent feature models, where the dependencies bet ween objects were expressed as tree structures. Examples of applications that utilized these models are: identification of protein complexes (Chu et al. 2006), modeling of dyadic da ta (Meeds et al. 2006), resolution (Bejan et al. 2009; Bejan and Harabagiu 2010).
 generic problem introduced in Section 1. It still requires a m echanism to automati-cally select a finite set of salient features that will be used in the clustering process (third desideratum) as well as a mechanism for capturing the structural dependencies between objects (fourth desideratum). To overcome these lim itations, we created two additional models. First, we incorporated the mIBP framewor k into our HDP extension to create the mIBP X  X DP model. And second, we coupled an infinite latent feature model with an infinite latent class model into a new discrete t ime series model. For the infinite latent feature model, we chose the infinite factorial hidden Markov model (iFHMM) (Van Gael, Teh, and Ghahramani 2008) coupled with the mIBP mechanism in order to represent the latent features as an infinite set of parallel Markov chains; for the infinite latent class model, we chose the infinite hidden Markov model (iHMM) (Beal, Ghahramani, and Rasmussen 2002). We call this new hyb rid the iFHMM X  X HMM model. 2.1 Contribution
This article represents an extension of our previous work on unsupervised event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010). In this work, we present more details on the problem of solving both within-a nd cross-document event coreference as well as describe a generic framework for solv ing this type of problem in an unsupervised way. As data sets, we consider three diffe rent resources, including our own corpus (which is the only corpus available that encod es event coreference annotations across and within documents). In the next sectio n, we provide additional information on how we performed the annotation of this corpu s. Another major contri-bution of this article is an extended description of the unsu pervised models for solving event coreference. In particular, we focused on providing fu rther explanations about the implementation of the mIBP framework as well as its integr ation into the HDP and iHMM models. Finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments perf ormed over the OntoNotes
English corpus (LDC-ON 2007). 316 3. Event Coreference Data Sets
Because our nonparametric Bayesian models are also unsuper vised, they do not require the data set(s) on which they are trained to be annotated with event coreference infor-mation. The only requirement for them to infer coreference c lusters of event mentions is to have the observable objects (i.e., the event mentions) i dentified in the order they occur in the documents as well as to have all the linguistic fe atures associated with these objects extracted. However, in order to see how well thes e models perform, we need to compare their results with manually annotated clust ers of event mentions. For this purpose, we evaluated our models on three different dat a sets annotated with event coreference information.
 automatic content extraction (ACE) task (LDC-ACE 2005). Th is resource contains only a restricted set of event types such as LIFE , BUSINESS , CONFLICT data set, we used the OntoNotes English corpus (release 2.0) , a more diverse resource that provides a larger coverage of event (and entity) annota tions. The utilization of the ACE and OntoNotes corpora for evaluating our event coref erence models is, how-ever, limited because these resources provide only within-document event coreference annotations. For this reason, as a third data set, we created the EventCorefBank (ECB) corpus 1 to increase the diversity of event types and to be able to eval uate our models for both within-and cross-document event coreference reso lution. Recently, Lee et al. (2012) extended the EventCorefBank corpus with entity core ference information and additional annotations of event coreference.
 sets of related documents that describe the same seminal event notation of coreferential event mentions across documents is possible. In this regard, we searched the Google News archive 3 for various topics whose description contains describing the same seminal event for each of these topics. In a subsequent step, for every Web document, we automatically tokenized and split th e textual content into sentences, and saved the preprocessed data in a uniquely ide ntified text file. Next, we manually annotated a limited set of events in each text file in accordance with the
TimeML specification (Pustejovsky et al. 2003a). To mark the e vent mentions and the coreferential relations between them we utilized the Calli sto tools, respectively. Additional details regarding the ann otation process for creating the ECB resource are described in Bejan and Harabagiu (2008a).
 event mentions are annotated at the sentence level, sentenc es are grouped into docu-ments, and the documents describing the same seminal event a re organized into topics.
The topics shown in Example (1) describe the seminal event of arresting sea pirates by a Navy warship (topic 12), the event of buying ATI by AMD (topic 43), the event of buying EDS by HP (topic 44), and the event of arresting a reputed footb all player (topic 55).
When taken out of context, the event mentions annotated in thi s example refer only to two generic events : arrest and buy . On the other hand, when these mentions are contex-tually associated with the event properties expressed in Ex ample (1), five individuated events can be distinguished: e 1 = { em 2 , em 3 } , e 2 = { em e event (of buying EDS by HP), whereas em 2 ( buy ) and em 4 318 individuated events because they have a different A GENT (i.e., B ferent from B UYER ( em 4 )= HP ). Similarly, the mentions em do not corefer because they correspond to different spatial and temporal locations (e.g., L
OCATION ( em 1 )= Gulf of Aden is different from L OCATION archy as the one illustrated in Figure 1. Specifically, this fi gure depicts the hierarchy of the events described in Example (1). In this hierarchy, the nodes on the first level correspond to event mentions (e.g., em 11 corresponds to arrested ), the nodes on the second level correspond to individuated events (e.g., e 5 nodes that refer to the arrest of Vincent Jackson), and, final ly, the nodes on the third level correspond to generic events (e.g., the node arrest contains all possible arrest events). In this article, our focus is to discover the nodes on the second level of this hierarchy.
 many interesting challenges. For instance, in order to solv e the coreference chain of event mentions that refer to the event e 2 , we need to take into account the following issues: (i) a coreference chain can encode both within-and c ross-document coreference information; (ii) two mentions from the same chain can have d ifferent word classes (e.g., em 4 ( buy ) X  X erb, em 5 ( purchase ) X  X oun); (iii) not all the mentions from the same chain are between them (e.g., in WordNet [Fellbaum 1998], the genus of buy is acquire ); (iv) not all the properties associated with an event mention are expr essed in text (e.g., all the of the event coreference problem that are not observed in Exa mple (1). 4. Linguistic Features for Event Coreference Resolution
The main idea for solving event coreference is to identify th e event mentions (from the same or different documents) that share the same characteri stics (e.g., all the mentions in a cluster convey the same meaning in text, have the same par ticipants, and happen in the same space and temporal location). Moreover, finding c lusters of event mentions that share the same characteristics is identical to finding c lusters of mention features that correspond to the same real event. For instance, Figure 2 depicts five clusters of linguistic features that characterize the five individuate d events from Example (1). As can be observed, each individuated event corresponds to a su bset of features that are usually common to all the mentions referring to it. For this p urpose, we extracted var-ious linguistic features associated with each event mentio n from the ACE, OntoNotes, and ECB corpora. solving event coreference, we would like to emphasize that w e make a clear distinction between the notions of feature type and feature value throughout this article. A feature type is represented by a characteristic that can be extracte d with a specific methodology and is associated with at least two feature values. For insta nce, the feature values corresponding to the feature type WORD consist of all the distinct words extracted from a given data set. In order to differentiate between the same va lues of different feature types, we inserted to the notation of each feature value the n ame of its corresponding feature type (e.g., WORD : play ). 4.1 Lexical Features (LF)
We capture the lexical context of an event mention by extract ing the following features: the head word ( HW ), the lemmatized head word ( HL ), the lemmatized left and right words surrounding the mention ( LHL , RHL ), and the HL features corresponding to the left and right mentions ( LHE , RHE ). For instance, the lexical features extracted for the event mention em 8 ( bought ) from our example are
RHL : Compaq , LHE : acquisition , and RHE : acquire . 4.2 Class Features (CF)
This category of features aims to group mentions into severa l types of classes: the part-of-speech of the HW feature ( POS ), the word class of the class of the mention ( EC ). The HWC feature type is associated with the following four fea-ture values: VERB , NOUN , ADJECTIVE , and OTHER . As feature values for the type, we consider the seven event classes defined in the TimeM L specification language
I ACTION , and I STATE . To extract all these event classes for all the event mention s, we used an event identifier trained on the TimeBank corpus (Pust ejovsky et al. 2003b), a linguistic resource encoding temporal elements such as eve nts, time expressions, and temporal relations. More details about this event identifie r are described in Bejan (2007). 4.3 WordNet Features (WF)
In our efforts to create clusters of attributes correspondin g to event mentions as close as possible to the true attribute clusters of the individuat ed events, we built two sets of word clusters using the entire lexical information from t he WordNet database. After Buyer : HP Seller : Compaq Money : $ 19 billion
Time : 2002 e 3 Suspect 320 creating these sets of clusters, we associated each event me ntion with only one cluster from each set. For the first set, we used the transitive closur e of the WordNet MOUS relation to form clusters with all the words from WordNet ( the verbs buy and purchase correspond to the same cluster ID because there exist a chain of
SYNONYMOUS relations between them in WordNet. For the second set, we con sidered as grouping criteria the categorization of words from the Wo rdNet lexicographer X  X  files (
WNL ). In addition, for each word that is not represented in WordNe t, we created a new cluster ID in each set of clusters. 4.4 Semantic Features (SF)
To extract features that characterize participants and pro perties of event mentions, we used the semantic parser described in Bejan and Hathaway (20 07). One category of semantic features that we identified for event mentions is the predicate argument structures encoded in the PropBank annotations (Palmer, Gildea, and Ki ngsbury 2005).
The predicate argument structures in PropBank are represen ted by events (or verbs) and by the semantic roles (or predicate arguments ) associated with these events. For example, ARG0 annotates a specific type of semantic role which represents t he
DOER , or ACTOR of a specific event. Another argument is ARG1 of the PATIENT , THEME , or EXPERIENCER of an event. In Example (1), for instance, the predicate arguments associated with the event mention em ARG1 :[ Compaq Computer Corp. ], ARG3 :[ for $19 billion ], and nouns and adjectives. Therefore, for a better coverage of sem antic features, we also used the semantic annotations encoded in the FrameNet corpus (Ba ker, Fillmore, and Lowe 1998). FrameNet annotates word expressions capable of evok ing conceptual structures, or semantic frames , which describe specific situations, objects, or events. The semantic roles associated with a word in FrameNet, or frame elements , are locally defined for the semantic frame evoked by the word. In general, the words an notated in FrameNet are expressed as verbs, nouns, and adjectives.
 elements to the predicate arguments by running the PropBank semantic parser on the manual annotations from FrameNet as well as running the Fram eNet parser on the
PropBank annotations. Moreover, to obtain a better alignme nt for each semantic role, we ran both parsers on a large amount of unlabeled text. The resu lt of this process is a map with all frame elements statistically aligned to all predic ate arguments. For instance, in 99.7% of the cases the frame element BUYER of the semantic frame C is mapped to ARG0 , and in the remaining 0.3% of the cases to used this map to create a more general semantic feature that a ssigns a frame element label to each predicate argument. Examples of semantic feat ures for the em (
FR ) evoked by every mention in the data set, since in general, fr ames are able to capture properties of generic events (Lowe, Baker, and Fillmore 199 7); and (2) the applied to the head word of every semantic role (e.g., WSARG0 4.5 Feature Combinations (FC)
We also explored various combinations of the given features . For instance, the feature resulting from the combination of the HW and HWC feature types for em
Example (1) is HW + HWC : bought + VERB . Examples of additional feature combinations we experimented with are HL + FR , HW + POS , FR + POS + EC , 5. Finite Feature Models
In this section, we first present HDP, a nonparametric Bayesian model that is capable of clustering objects based on one feature type (i.e., WORD extension of this model that describes an algorithm for clus tering objects characterized by multiple feature types.
 i has J i event mentions. Each event mention is characterized by L feature types ( and each feature type is represented by a finite vocabulary of feature values ( fv ). For example, the feature values extracted from an event corefer ence data set and associated with the feature type HW constitute all possible head words of the event mentions annotated in the data set. Therefore, we can represent the ob servable properties of an event mention as a vector of pairs h ( FT 1 : fv 1 i ), . . . , ( index i ranges in the feature value space of its corresponding featu re type. In the description of these models, we also consider Z : the set of indicator random variables document collection where Z i , j represents the event index of the event mention j from model parameters; and X : a notation for all random variables that represent observa ble features. As already introduced in Section 1, we denote by K the total number of latent events.
 the best assignment of event indices Z  X  , which maximize the posterior probability
P ( Z | X ). In a Bayesian approach, this probability is computed by int egrating out all model parameters: 5.1 The HDP 1 f Model
The one feature model, denoted here as HDP 1 f , constitutes the simplest representation of an HDP model. In this model, depicted graphically in Figure 3 (a), the observable com-ponents are characterized by only one feature type (e.g., th e head lemma corresponding to each event mention). The distribution over events associ ated with each document,  X  , is generated by a Dirichlet process with a concentration pa rameter  X &gt; 0. Because this setting enables a clustering of event mentions at the do cument level, it is desirable that events be shared across documents and the number of even ts, K , be inferred from data. To ensure this flexibility, a global nonparametric DP pr ior with a hyperparameter  X  and a global base measure H can be considered for  X  (Teh et al. 2006). The global distribution drawn from this DP prior, denoted as  X  0 in Figure 3(a), encodes the event mixing weights. Thus, the same global events are used for eac h document, but each event has a document specific distribution  X  i that is drawn from a DP prior centered on  X  0 . 322  X   X   X   X  and used a Gibbs sampling algorithm (Geman and Geman 1984) based on the direct assignment sampling scheme. In this sampling scheme, the  X  and  X  parameters are integrated out analytically. The formula for sampling an ev ent index for mention j from document i , Z i , j , is given by: 6 where HL i , j is the head lemma of event mention j from document i . pled by using a mechanism that facilitates sampling from a pr ior for infinite mixture models called the Chinese restaurant franchise (CRF) representation, as reported in (Teh et al. 2006):
In this formula, n z is the number of event mentions with event index z , z with the K events, and  X  u 0 is the weight for the unknown mixture component. associated with a multinomial emission distribution over t he the parameters  X  = h  X  hl Z i . We assume that this emission distribution is drawn from a symmetric Dirichlet distribution with concentration  X  HL : where HL i , j is the head lemma of mention j from document i , and n times the feature value hl has been associated with the event index z in ( Z , HL  X  5.2 The HDP flat Model
A model in which observable components are represented only by one feature type has the tendency to cluster these components based on their corr esponding feature values.
This model may produce good results for tasks such as topic di scovery where the linguistic objects rely only on lexical information. Becaus e event coreference involves clustering complex objects characterized by a large number o f features, it is desirable to extend the HDP 1 f model with a generalized model where additional feature typ es can be easily incorporated. Moreover, this extension should al low multiple feature types to be added simultaneously.
 dependent given Z . This assumption considerably reduces the complexity of co mputing
P ( Z | X ). For example, if we want to incorporate into the previous mo del the feature type associated with the semantic frame evoked by every even t mention (i.e., FR ), the formula becomes:
In this formula, we omit the conditioning components of Z , HL , and FR for the sake of clarity. The graphical representation corresponding to this model is illustrated in
Figure 3(b). In general, if X consists of L feature variables, the inference formula for the Gibbs sampler is defined as: 324
The graphical model for this general setting is depicted in F igure 3(c). Drawing an analogy, the graphical representation involving Z and feature variables resembles the graphical representation of a naive Bayes classifier. 5.3 The HDP struct Model
When dependencies between feature type variables exist (e.g ., in our case, frame elements are dependent on the semantic frames that define the m, and frames are dependent on the words that evoke them), various global dist ributions are involved probability is given by: a feature type variable from the set X = h HL , POS , FR i . However, one limitation of this particular model is that it requires domain knowledge i n order to establish the dependencies between the feature type variables.
 as described in the HDP 1 f model. In the inference mechanism, we assigned soft counts to those likelihood factors whose corresponding feature va lues cannot be extracted for a given event mention (e.g., unspecified predicate argument s). It is worth noting that there exist event mentions for which not all the features can be extracted. For instance, the feature types corresponding to the left and right lemmat ized head words (denoted in
Section 4 as LHE and RHE , respectively) are missing for the first and last event menti ons in a document. Also, many semantic roles can be absent for an e vent mention in a given context. 6. Infinite Feature Models
One of the main limitations of the HDP extensions presented in the previous section is that these models have limited capabilities in represent ing the observable objects characterized by a large number of feature types. This is bec ause, in order to sample the event indices into the set of indicator random variables Z , the HDP models need to store in memory large matrices that encode the significant st atistics for the observable components associated with each cluster. More specifically , in order to compute the likelihood factors in Equation (5), for each feature type counting matrix having the number of rows equal with the numb er of distinct feature values corresponding to FT i and K + 1 columns, where K represents the number of inferred events. For instance, the counting matrix corresp onding to the head lemma feature type (HL) stores the number of times each feature valu e of the HL feature type has been associated with each event index during the HDP gener ative process. The number n hl , z in Equation (5), for example, is stored in a cell of this matri x. events from OntoNotes, we made the following calculation. In OntoNotes, we automat-ically identified a total number of 81,938 event mentions for which we extracted 454,170 distinct feature values. For all data sets, we considered L = 132 feature types, which means that, on average, each feature type is associated with approximately 3,440 feature values. Because K is bounded by the total number of event mentions considered ( i.e., the case when each event mention is associated with a differe nt event), the maximum value that it can reach when inferring the event indices from OntoNotes is 81,938. If we consider that each cell from the counting matrices associat ed with each feature type is represented into the memory by one byte, the total space requ ired to store only one such matrix is, on average, 81,938  X  3,440 bytes. By a simple computation, the total amount of memory to store all 132 matrices is  X  34.6 gigabytes (GB). Furthermore, by adding more data, the amount of memory needed by the HDP models increases c onsiderably. For instance, if we consider all three data sets (with a total num ber of 148,402 event mentions and 832,611 distinct feature values), the memory space requ ired increases to 115 GB.
Because in our implementation we used the int type (4 bytes) t o represent the counting matrices, the total amount of memory required by the HDP exten sions to infer the event indices from OntoNotes and all three data sets when consider ing all 132 feature types is in fact 4  X  34 . 6 = 138 . 4 GB and 4  X  115 = 460 . 3 GB, respectively.
 manually selected set of feature types. 7 Therefore, the existence of a novel methodology that is able to consider a much smaller subset of representat ive feature values from the entire feature space is necessary. For this purpose, we devi sed two novel approaches that provide a more flexible representation of the data by mode ling event mentions with an infinite number of features and by using a mechanism to automatically select a finite set of the most salient features for each mention in th e inference process. The first approach uses the Markov Indian buffet process (mIBP) to represent each object as a sparse subset of a potentially unbounded set of latent fe atures (Griffiths and
Ghahramani 2006; Ghahramani, Griffiths, and Sollich 2007; V an Gael et al. 2008), and combines it with the HDP extension presented in the previous s ection. We call this hybrid the mIBP X  X DP model . The second approach uses the infinite factorial hidden
Markov model (iFHMM), which is an extension of mIBP, and combines it with the infinite hidden Markov model (iHMM) to form the iFHMM X  X HMM model . 6.1 The mIBP X  X DP Model
In this section, we describe a model that is able to represent e vent mentions charac-terized by an unbounded set of feature values into the HDP fram ework. Although the feature space describing event mentions is unbounded, this approach is able to model the uncertainty in the number of feature values M that will be used for clustering event mentions and, at the same time, is able to guarantee that this number is finite at any point in time during the generative process. First, we use mIB P to describe a mechanism for assigning to each event mention a sparse subset of featur e values from the set of M observed feature values used in the clustering process. We w ill use the set of notations introduced in this description when presenting both mIBP X  X DP and iFHMM X  X HMM models. Then, we will show how this mechanism is coupled into the HDP framework. 6.1.1 The Markov Indian Buffet Process. The Markov Indian buffet process (Van Gael, Teh, and Ghahramani 2008) defines a distribution over an unbounde d set of independent hidden Markov chains, where each chain is associated with a b inary latent feature value 326 that evolves over time according to Markov dynamics. Specifi cally, if we denote by M the total number of Markov chains associated with the latent feature values and by T the number of observations, mIBP defines a probability distribut ion over a binary matrix F with an unbounded number of rows M ( M  X  X  X  ) and T columns.
 values that will be used for clustering event mentions (deno ted as { f well as to determine which of these feature values will be sel ected to explain each event mention. The sequence of observations is associated with th e sequence of event men-with one observed feature value from the unbounded set of fea tures that characterize our event mentions. It is worth mentioning that, at any given t ime point during the mIBP generative process, from the unbounded set of observed f eatures, we index only these M observed feature values that correspond to the set of hidden feature values. tion in the clustering process is determined by the indicato r random variables of the binary matrix F . For instance, the selection of the observed feature value f mention y t is indicated by an assignment of the binary random variable F mIBP generative process. More specifically, the set of observ ed feature values that will represent the event mention y t is indicated in the matrix by the column vector of binary represents them as feature value factors, which can then be a ssociated with hidden variables in an iFHMM model as described in Van Gael, Teh, and G hahramani (2008). where W ( m ) ij = P ( F m t + 1 = j | F m t = i ), the parameters a and the initial state F m 0 = 0. In the mIBP process, the hidden variable associated with an observed feature value f m and an event mention y t is generated from the following Bernoulli distribution: which the parameters a and b are integrated out analytically) by recording the number of 0  X  0, 0  X  1, 1  X  0, and 1  X  1 transitions for each binary chain m into the counting variables c 00 m , c 01 m , c 10 m , and c 11 m , respectively. For example, the c feature value representing the VERB class ( f m = HWC : VERB this feature value was assigned to the event mention y t when it was also assigned to the previous event mention y t  X  1 during the generative process.
 variables is defined as follows. In the first step, the process a ssigns a value of 1 to a number of Poisson (  X   X  ) latent features for the first component. In our implementati on, this statement is equivalent with the process of randomly se lecting for the first event mention a number of Poisson (  X   X  ) observed feature values. In the general case, the sam-pling of the binary variable from the m th Markov chain and associated with the t mention depends on the value assigned to the hidden variable in the previous t  X  1 step:
As a result, in our implementation, the observed feature val ue f event mention according to the probabilities presented in E quation (11). For example, in order to select the feature value which indicates that the t
OCCURRENCE event class (i.e., f m = EC : OCCURRENCE ), we need to determine whether or not the event mention t  X  1 from the document collection selected this feature value.
In the cases when EC : OCCURRENCE was previously selected for the event mention t  X  1 ( F t  X  1 = 1), we select this feature value according to P ( F selection is determined according to P ( F m t = 1 | F m t  X  1 the generative process, the same sampling mechanism is repe ated until all M latent feature values are generated. After sampling all these feat ure values for the t mention, an additional number of Poisson (  X   X  / t ) new feature values are assigned to this mention, and M gets incremented accordingly.

M grows logarithmically with the number of observed componen ts (in our case, event mentions) (Ghahramani, Griffiths, and Sollich 2007; Doshi-Velez 2009). This type of growth is desirable because it provides a scalable solution for our models to work in an efficient way on fairly large data sets. 6.1.2 Integration of mIBP into HDP. One direct application of the mIBP model is to integrate it into the framework of the HDP extension model des cribed in the previous section. In this way, the new nonparametric extension will ha ve the benefits of capturing the uncertainty regarding the number of mixture components that are characterized by a potentially infinite number of feature values. However, to m ake this hybrid work, we have to devise a mechanism in which only a finite set of relevan t feature values will be selected to explain each observation (i.e., event mention) in the HDP inference process. mention is based on a heuristic approach that is used after ru nning the mIBP generative process. Specifically, by considering the event mention y that characterizes y t , q m the number of times f m was selected for all mentions during mIBP, and v t a threshold variable for y t such that v t  X  Uniform (1, max { q define the finite set of feature values B t corresponding to the observation y feature values f m with the corresponding counts q m above the threshold indicated by v are selected in B t . The finiteness of this feature set is based on the observatio n that, at any time point during the generative process of the mIBP mod el, only a finite set of latent features have assigned a value of 1 for an event mentio n. Furthermore, based on 328 v q the assumption that the more a feature value is selected duri ng the mIBP generative process the more relevant it is for the event coreference tas k, each set B most informative feature values that are able to explain its corresponding event mention y . This last property is ensured by the second constraint impo sed when building each uniform distribution, we denote this model as mIBP X  X DP uniform tions in the clustering process of the HDP. The main differenc e from the original imple-mentation of the HDP extensions is that, in this new model, ins tead of representing the event mentions by the entire set of feature values from the in itial feature space (which
Furthermore, due to the random process of selecting the feat ure values, the number of feature values associated with each event mention can vary s ignificantly. We adapted the implementation of the HDP framework to this modification b y truncating all count-ing matrices such that they will represent only the feature v alues selected in mIBP. More specifically, we removed from each counting matrix the rows c orresponding to all the feature values that were not selected during the mIBP generat ive process. Because M grows as O (log T ), it now becomes feasible for the HDP extension models to repr esent event mentions using the entire set of feature types. It is imp ortant to mention that this modification does not affect the implementation of the Gibbs sampler in the HDP frame-work because we always normalize the probabilities corresp onding to the likelihood factors in Equation (5) when computing the posterior distri bution over event indices. with the number of times it was selected during the mIBP genera tive process, we explored additional heuristics for building the sets of fea ture values B mention. In general, we chose these new heuristics to be biase d towards selecting more relevant feature values f m for each event mention y t (i.e., their counts q event mention y t all feature values f m with the counts q each set B t contains all the observed feature values selected for each e vent mention y during the mIBP process, and therefore it represents a subset of the set of observed fea-ture values { f 1 , f 2 , . . . , f M } . It is worth mentioning that all the subsets of { f are finite due to the fact that M is finite at any given point in time during mIBP. In consequence, all the B t sets derived using this heuristic are finite. Because no feat ure value is filtered out after it was assigned to an event mention during mIBP, we denote the model implementing this heuristic as mIBP X  X DP unfiltered bution of the counting variables q m corresponding to those feature values f during the mIBP generative process for an event mention y for building each set B t only the feature values with the counts above the median of this distribution (mIBP X  X DP median ). Finally, the last heuristic we experimented with is based on the idea of sampling the threshold variables v t of the counting variables associated with each event mentio n y implementation of these three heuristics is possible due to the observation that in the mIBP X  X DP framework the size of each set B t is not required to be known in advance. 6.2 The iFHMM X  X HMM Model
Over the years, the hidden Markov model (HMM) (Rabiner 1989) has proven to be one of the most commonly used statistical tools for modeling tim e series data. Due to the efficiency in estimating its parameters, various HMM general izations were proposed for a better representation of the latent structure encoded in this type of data. Figure 5 illustrates a hierarchy of HMM extensions whose main criteri a of expansion is based on relaxing the constraints on the parameters M (the number of state chains) and K (the number of clustering components). In the factorial hidden Markov model (FHMM),
Ghahramani and Jordan (1997) introduced the idea of factori ng the hidden state space into a finite number of state variables, in which each of these variables has its own Markovian dynamics. Later on, Van Gael, Teh, and Ghahramani (2008) introduced the 330 infinite factorial hidden Markov model (iFHMM) with the purpose of allowing the number of parallel Markov chains M to be learned from data. Although the iFHMM provides a more flexible representation of the latent structu re, it cannot be used as a framework where the number of clustering components K is infinite. In this direction,
Beal, Ghahramani, and Rasmussen (2002) proposed the infinite hidden Markov model (iHMM) in order to perform inferences with an infinite number o f states K . To further increase the representational power for modeling discrete time series data, we introduce a novel nonparametric extension that combines the best of th e iFHMM and iHMM models (denoted as iFHMM X  X HMM) and lets both parameters M and K to be learned from data.
 of a sequence of hidden state variables, ( s 1 , . . . , s of event mentions ( y 1 , . . . , y T ). Each hidden state s latent events, s t  X  X  1, . . . , K } , and each mention y defined as  X  ij = P ( s t = j | s t  X  1 = i ), and a mention y model F that is parameterized by a state-dependent parameter  X  observation parameters  X  are independent and identically distributed drawn from a prior base distribution H . 6.2.1 Inference. The main idea of the inference mechanism corresponding to th is new model is illustrated in Figure 6. As depicted in this figure, e ach step in the generative process of the new hybrid model is performed in two consecuti ve phases. In the first phase, the binary random variables associated with each fea ture value from the iFHMM framework are sampled using the mIBP mechanism, and conseque ntly, the most salient feature values are selected for each event mention (Figure 6 : Phase I). Of note, the B sets of feature values associated with each event mention y same set of heuristics as described in Section 6.1. In the seco nd phase, the feature values sampled so far, which become observable during this phase, a re used in an adapted beam sampling algorithm (Van Gael et al. 2008) to infer the clustering components or latent events (Figure 6: Phase II).
 ture values for each event mention (as described in Section 6 .1), in this section we focus on describing our implementation of the beam sampling algor ithm. The beam sampling algorithm (Van Gael et al. 2008) combines the ideas of slice s ampling (Neal 2003) and dynamic programming for an efficient sampling of state trajec tories. Because in time series models the transition probabilities have independe nt priors (Beal, Ghahramani, and Rasmussen 2002), Van Gael et al. (2008) also used the HDP me chanism to allow couplings across transitions. For sampling the whole hidde n state trajectory s , this algorithm uses a forward filtering-backward sampling technique .
 is sampled for each mention y t , u t  X  Uniform (0,  X  s to filter only those trajectories s for which  X  s the probabilities P ( s t | y 1: t , u 1: t ) are computed as follows:
In this formula, the dependencies involving parameters  X  and  X  are omitted for clarity. P ( s
T | y 1: T , u 1: T ) and then, for all t : T  X  1, 1, each state s following formula: characterized by a finite set of representative features, in our implementation of the beam sampling algorithm, we set the base distribution H to be conjugate with the data distribution F in a Dirichlet-multinomial model with the multinomial para meters ( o , . . . , o K ) defined as: where n mk counts how many times the feature value f m was assigned in the generative process to event k , and B t stores a finite set of feature values for y 332
Section 6.1. As can be noticed, the multinomial parameters d efined here are finite due to the fact that each set of feature values B t is finite and the number of event mentions T is fixed. This allows us to define a proper emission distribution for the new hybrid model.
In a similar manner to the notations of the mIBP X  X DP model, we mak e notations of the iFHMM X  X HMM model according to the heuristic used for selectin g the feature values. 7. Evaluation
In this section, we present the evaluation framework of the Ba yesian models for both within-document ( WD ) and cross-document ( CD ) coreference resolution. We start by briefly describing the experimental set-up and coreferenc e evaluation measures, and then continue by showing the experimental results on the ACE, OntoNotes, and
EventCorefBank data sets. Finally, we conclude with an anal ysis of the most common errors made by the Bayesian models. 7.1 The Experimental Set-up
In the data processing phase, we extracted the linguistic fea tures described in Section 4 for each event mention annotated in the three data sets. As a r esult of this phase, in the ACE corpus, we identified 6,553 event mentions grouped in to 4,946 events, and in the OntoNotes corpus, we identified 11,433 event mentions gr ouped into 3,393 events.
Likewise, in the new ECB corpus, we distinguished 1,744 even t mentions, 1,302 within-document events, 339 cross-document events, and 43 seminal events (or topics). Table 1 efforts. This is because, in spite of the fact that OntoNotes provides coreference annotations for both entity and event mentions, the annotat ions from this data set do not specify which of the mentions refer to entities and which of t hem refer to events. There-fore, in order to identify only the event mentions from OntoN otes, we first ran our event identifier (Bejan 2007) and then marked as event mentions only those mentions anno-tated in this data set that overlap with the mentions extract ed by the event identifier.
Using this procedure, we marked a number of 4,940 mentions as event mentions from the total number of 67,500 mentions annotated in OntoNo tes. In a second step of processing OntoNotes, we extended the number of event menti ons to 11,433 by marking all the mentions that share the same cluster with at least one event mention from the set of 4,940 previously identified event mentions. From the 6,49 3 event mentions marked in this step, the majority of them correspond to nouns (4,707) an d to the it pronoun (767). coreference information in the three data sets (also called the set of true or gold event mentions ), during the generative process, we considered all possibl e event mentions that are expressed in the data sets for every specific event. W e believe this is a more realistic approach, in spite of the fact that we evaluated on ly the manually annotated events. For this purpose, we ran the event identifier describ ed in Bejan (2007) on the
ACE, OntoNotes, and ECB corpora, and extracted 45,289, 81,9 38, and 21,175 event men-tions, respectively. It is also worth mentioning that the set of event mentions obtained from running the event identifier (also called the set of system event mentions ) on
ACE and ECB includes more than 98% from the set of true event me ntions. In terms of feature space dimensionality over the two data sets, we perf ormed experiments with a set of 132 feature types, where each feature type consists, o n average, of 6,300 distinct feature values.
 data set and from the test sets of a five-fold cross validation scheme on the OntoNotes and ECB data sets. For evaluating the cross-document corefe rence annotations from
EventCorefBank, we adopted the same approach as described i n Bagga and Baldwin (1999) by merging all the documents from the same topic into a meta-document and then scoring this document as performed for within-documen t evaluation. To compute the final results of our experiments, we averaged the results over five runs of the generative models. 7.2 Coreference Resolution Metrics
Because there is no agreement on the best coreference resolu tion metric, we used four metrics for our evaluation: the link -based MUC metric (Vilain et al. 1995), the mention -based B 3 metric (Bagga and Baldwin 1998), the entity -based the pairwise ( PW ) metric. These metrics report results in terms of recall (R) , precision (P), and F-score (F) by comparing the true set of coreference chains T (i.e., the manually annotated coreference chains) against the set of chains pre dicted by a coreference res-olution system S . Here, a coreference link represents a pair of coreferential mentions whereas a coreference chain represents all the event mentions from the same cluster with coreference links between consecutive mentions.
 divided by the number of links in T , and the MUC precision computes the number of common links in T and S divided by the number of links in S . As was previously noted (Luo et al. 2004; Denis and Baldridge 2008; Finkel and Mannin g 2008), this metric favors the systems that group mentions into smaller number of clust ers (or, in other words, systems that predict large coreference chains) and does not take into account single mention clusters. For instance, a system that groups all ent ity mentions into the same cluster achieves a MUC score that surpasses any published results of known systems developed for the task of entity coreference resolution.

This metric computes the recall and precision for each menti on and then estimates the 334 overall score by averaging over all mention scores. For a giv en mention m , the scorer compares the true coreference chain that contains the menti on m ( T chain that contains the same mention m ( S m ). Thus, the recall for m is the ratio of the number of common elements in S m and T m over the number of elements in T the precision corresponding to the mention m is the ratio of the number of common elements in S m and T m over the number of elements in S m the precision and recall for each mention, it will penalize i n precision the systems that predict a small number of clusters. Because of the same reaso n, this metric includes single mention clusters in the evaluation.
 ment between the set of true coreference chains T and the set of predicted coreference chains S . This is equivalent to finding the best mapping in a weighted b ipartite graph.
We computed the weight of a pair of coreference chains ( T by using the  X  4 similarity measure described in Luo et al. (2004). Therefor e, the recall and precision measures are computed as the overall si milarity score of the best alignment divided by the self-similarity score of the coref erence links in T and S , respectively.
 dences between all mentions pairs ( m i , m j ) from the true and system chains with the coreference chains linking the mentions m i and m j in the system and true chains, respectively. As can be noticed, this metric overpenalizes those systems that predict too many or too few clusters when compared with the number of true clusters. 7.3 Experimental Results
Tables 2, 3, 4, and 5 list the results performed by our propose d baselines (rows 1 X 2), by the HDP models (rows 3 X 8), by the mIBP X  X DP model (row 9), and by the iFHMM X  iHMM model (rows 10 X 13). We discuss the performance achieved by these models in the remaining part of this section.
 7.3.1 Baseline Results. A simple baseline for event coreference, which was proposed by
Ahn (2006), consists of grouping event mentions by their eve nt classes (BL pute this baseline, we grouped mentions into clusters accor ding to their corresponding feature value. In consequence, this baseline categorizes ev ents into a small number of clusters, since the event identifier for extracting the the seven event classes annotated in TimeBank. A second base line that we implemented groups two event mentions if there is a (transitive) SYNONYMOUS corresponding head lemmas (BL syn ). To implement this baseline, we used the clusters built over the WordNet SYNONYMOUS relations as described in Section 4. Similarly to the
MUC results reported for entity coreference resolution, the ba selines that group event mentions into very few clusters are overestimated by the MUC
F-scores of BL eclass in Table 5). 7.3.2 HDP Results. Due to memory limitations, we evaluated the HDP models on a restricted set of manually selected feature types. For the HD P 336 the role of baseline for the HDP flat and HDP struct models, we considered most representative feature type for performing the cluste ring of event mentions. In this configuration, the HDP 1 f model outperforms the BL eclass the HDP flat models (rows 4 X 7 in Tables 2 X 5), we classified the experiment s according to the set of manually selected feature types. We found that t he best configuration of features for this model consists of a combination of feature types from all the categories of features described in Section 4 (row 7 in Tables 2 X 5). For t he experiments of the
HDP struct model, we considered the set of features of the best HDP well as the conditional dependencies between the HL , FR , and test data set (the results in Table 2), whereas the HDP struct dependencies between feature types, proved to be more effec tive on the ECB data set for both within-and cross-document event coreference eval uation (as shown in Tables 4 and 5). On the OntoNotes data set, as listed in Table 3, HDP than HDP struct when considering the B 3 and PW metrics, whereas HDP HDP flat when considering the MUC and CEAF metrics. Moreover, the results of the
HDP flat and HDP struct models show an F-score increase by 4 X 10 percentage points ov er the HDP 1 f model, and therefore prove that the HDP extensions provide a m ore flexible representation for clustering objects characterized by ric h properties than the original HDP model.

For instance, Figure 7 shows that the HDP flat model corresponding to the experiment from row 7 in Table 2 converges in 350 iteration steps to a post erior distribution over event mentions from the ACE corpus with around 2,000 latent e vents. 7.3.3 mIBP X  X DP Results. In spite of its advantage of working with a potentially infinit e number of features in an HDP framework, the mIBP X  X DP model (row 9 in Tables 2, 4, and 5) did not achieve a satisfactory performance in compa rison with the other pro-posed models. However, the results were obtained by automati cally selecting only 2% of distinct feature values from the entire set of values extrac ted from both corpora. When compared with the restricted set of features considered by t he HDP models, the percentage of values selected by the mIBP X  X DP mode l is only 6%. 7.3.4 iFHMM X  X HMM Results. The results achieved by the iFHMM X  X HMM model using automatic selection of feature values remain competitive a gainst the results of the
HDP models, where the feature types were manually tuned. When c omparing the strategies for filtering feature values in the iFHMM X  X HMM fram ework, we could not find a distinct separation between the results obtained by th e iFHMM X  X HMM served from Tables 2, 4, and 5, most of the iFHMM X  X HMM results fa ll in between the iFHMM X  X HMM model is a better framework than the HDP framework fo r capturing the event mention dependencies simulated by the mIBP feature sampling scheme. the number of feature values selected in the iFHMM X  X HMM framew ork is presented in Figure 8. The results plotted in this figure show a small var iation in performance for different values of  X   X  indicating that the iFHMM X  X HMM model is able to successfully handle the feature values that introduce additional noise i n the data. Figure 8 also shows that the iFHMM X  X HMM model achieves the best results on th e ACE data set for a relative small value of  X   X  (  X   X  = 10), which corresponds to 0.05% feature values sampled from the total number of feature values considered. However, because the number of event mentions in the ECB corpus is smaller than the number of mentions in the ACE corpus, the iFHMM X  X HMM model utilizes a larger number of featu res values (0.91% of feature values selected for  X   X  = 150) extracted from the new corpus in order to obtain most of its best results.
 heuristic for selecting feature values in the iFHMM X  X HMM mode l. Similar results were 338 obtained when considering the rest of the heuristics integr ated in the iFHMM X  X HMM framework. Also, the iFHMM X  X HMM experiments using the unfiltered , discrete , median , and uniform heuristics from Tables 2 X 5 were performed by setting  X   X  to 10, 100, 100, and 50, respectively. For the parameters  X   X  and  X   X  , we considered a default value of 0.5. in Figure 9 the performance results obtained by this model fo r different sets of feature types. For this purpose, we ran the iFHMM X  X HMM uniform model with a fixed value of the  X   X  parameter (  X   X  = 50) on increasing fractions of feature types. the fact that the sampling scheme of the feature values used i n the iFHMM X  X HMM framework does not guarantee the selection of the most salie nt features. However, the constant trend in the performance values shown in Figure 9 pr oves that iFHMM X  X HMM is a robust generative model for handling noisy and redundan t features. For instance, noisy features for our problem can be generated from errors i n semantic parsing, event class extraction, POS tagging, and disambiguation of polys emous semantic frames.
To strengthen this statement, we also compare in Table 6 the r esults obtained by an iFHMM X  X HMM model that considers all the feature values associ ated with an observ-able object (iFHMM X  X HMM all ) against the iFHMM X  X HMM models that use the mIBP sampling scheme and the unfiltered , discrete , median , and uniform heuristics. Because of the memory limitation constraints, we performed the expe riments listed in Table 6 by selecting only a subset of feature types from the ones that proved to be salient in the HDP experiments. As listed in Table 6, all the iFHMM X  X HMM mod els that used a heuristic approach for selecting feature values significan tly outperform the iFHMM X  iHMM all model; therefore, this proves that all the feature selectio n approaches consid-ered in the iFHMM X  X HMM framework are able to successfully filte r out a significant number of noisy and redundant feature values. 7.4 Error Analysis
We performed an error analysis by manually inspecting both s ystem and gold-annotated data in order to track the most common errors made b y our models. One frequent error occurs when a more complex form of semantic in ference is needed to find a correspondence between two event mentions of the same i ndividuated event.
For instance, because all properties and participants of em
Example (1), and no common features exist between em 2 ( buy ) and em indicate a similarity between these mentions, they will mos t probably be assigned to different clusters. This example also suggests the need for a better modeling of the discourse salience for event mentions.
 to coreferential event mentions. Although we simulated ent ity coreference by using various semantic features, the task of matching participan ts and properties associ-ated with coreferential event mentions is not completely so lved. This is because, in many coreferential cases, partonomic relations between se mantic roles need to be in-ferred. 10 Examples of such relations extracted from ECB are Israeli forces an Indian warship PART OF  X  X  X  X  X  X  X  the Indian navy , his cell properties, many coreferential examples do not specify a cl ear location and time interval 340 (e.g., Jabaliya refugee camp PART OF  X  X  X  X  X  X  X  Gaza , Tuesday to build relevant clusters using partonomies and taxonomie s such as the WordNet tions, respectively. 11 8. Conclusion
We have described a new class of unsupervised, nonparametri c Bayesian models designed for the purpose of solving the problem of event core ference resolution. Spe-cifically, we have shown how already existing models can be ex tended in order to relax some of their limitations and how to better represent t he event mentions from a particular document collection. In this regard, we have foc used on devising models for which the number of clusters and the number of feature val ues corresponding to event mentions can be automatically inferred from data.
 these models are able to successfully handle such types of re quirements on a real data application. Based on these results, we also demonstrated t hat the new HDP extension, which is able to model observable objects characterized by mu ltiple properties, is a better fit for this type of problem than the original HDP model. Moreover, we believe that the HDP extension can be used for solving clustering prob lems that involve a small number of feature types and a priori known facts about the sal ience of these feature 342 types. On the other hand, when no such prior information is kn own with respect to the number of feature types, or the total number of features is re latively large, we believe that the iFHMM X  X HMM model is a more suitable choice. The main re ason is because the new hybrid model is able to perform an automatic selection of feature values. As shown in our experiments, this model was capable of achieving comp etitive results even when only 2% of feature values were selected from the entire set of features encoded in the ACE, OntoNotes, and ECB data sets.
 Acknowledgments References 344 346
