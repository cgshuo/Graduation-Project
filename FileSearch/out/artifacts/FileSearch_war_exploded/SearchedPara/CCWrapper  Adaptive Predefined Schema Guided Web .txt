 With the web becoming the abundant information resource, web data extraction, which can map loosely-structured data from web pages into a formal structure, has used to provide users with better service. The module of web extraction is also called the wrapper. 
How to extract data from web pages of different web sites while at the same time to reduce the human involvement is a big challenge. Since the initial purpose of HTML is for human browsing rather than computer processing, the structure, style, or the layout of web pages are different from each other. In addition, some web sites take the anti-extraction strategy to prevent their data from being extracted. The structures for HTML document trees may be changed dynamically with time. Although the users may not be aware of those changes, some HTML page structure based wrapper cannot work. 
There are several types of data extractions from web pages: (1) region extraction (extracting the interesting region in the form of HTML from the original HTML (extracting individual data items scattered and annotating individual data according to some ideas from the methods in the first two categories. 
Although the current methods have made great achievement in the web extraction, they can be improved in the following aspects: improving the quality of the extracted data; improving the adaptability of the wrapper; reducing the human burden in the web extraction. 
In order to overcome the limitation of the existing methods and handle the prob-lems in extraction, we attempt to propose a method called CCWrapper. Specifically, the contributions of this paper can be summarized as follows: 
The rest of the paper is organized as follows. Section 2 introduces some prelimi-nary knowledge. Section 3 proposes the extraction rules generation and application in CCWrapper. Section 4 shows experimental results, and Section 5 reviews the related work. Section 6 concludes the whole work and discusses the further work. This section reviews the preliminary knowledge including HTML and target schema for the extracted information, and discusses the criteria to evaluate the web extraction methods. 2.1 HTML HTML can be modeled as a node labeled tree. Each node is annotated with one node The text nodes are always the leaf nodes in the HTML document tree, which contains normal text node , or else, it is a hyperlink text node . The style node controls the ap-pearance of the HTML text nodes. The code nodes include the scripts codes which can run in the HTML page. 2.2 The Target Predefined Schema Due to the heterogeneous nature of web pages, we use the flexible DTD to represent the schema of extracted data. DTD can support the element definition with the regu-ment e , the  X * X  annotated on sub element e 1 of e indicates zero-many occurrence of e one time occurrence of sub element e 1 in the expression is represented by Card ( e )=1. If the element definition is not recursive, DTD can be represented by a schema tree. 2.3 The Evaluation of Web Extraction Methods web pages set W , the data set D 2 extracted from the web pages W with method M , the | D
A web wrapper can be represented as a set of extraction rules. Suppose the rules of automatically rather than hand crafting, we call E the semi-automatically generated web wrapper. The problem handled in the paper can be described as follows: Given a training set W of web pages from a web sites set S , the target schema T with the regular expression element definition including  X * X  or  X ? X , how to build a web wrapper W to extract data human burden in the process. 3.1 The Framework of CCWrapper The framework of CCWrapper can be illustrated in the following figure. First, a user establishes mappings from the nodes in training HTML document to the target ele-ments with a user friendly interface. Second, CCWrapper extracts the features includ-HTML nodes which have been mapped to the target elements. Third, CCWrapper generates a Bayes network classifier C to represent the extraction rules over the fea-tures set. Fourth, CCWrapper uses classifier C to get the probability of target element for each node in a new HTML page. In the final step, CCWrapper exploits the intra-document relationship in HTML page to cluster the nodes and annotate the HTML nodes with the target elements in the schema. 3.2 Extraction Rules Generation Based on Classification With the mappings from the HTML tree to the target schema tree, the next step is to extract the features of the corresponding HTML nodes for each target schema node, which can be used as the basis for the extraction rules. CCWrapper combines differ-ent features of HTML nodes into one unified model. The attributes of HTML node attributes, structure attributes, thesaurus attributes and the data type attributes. Style Attributes: The style attributes control the appearance of the web page. Differ-ent HTML nodes may have different style features. In addition, it is observed that the web pages in the same domain are often displayed in a similar way. For example, the font size of the news title is always largest in the whole web page. The product name in the E-commerce domain is always emphasized, etc. As a result, the extraction rules based on the site independent features can be used in the web sites not in the training set. Some selected style attributes on the HTML node used in CCWarpper are de-scribed in the following table: Structure Attributes: The structure attribute in the HTML page represents the path from the root node to the leaf node in the HTML page. Since many data intensive web sites are generated from the backend database, the paths for the HTML node corre-sponding to the same element in the target schema are similar for HTML pages in the one web site. In order to exploit structure attribute as one of the important features of the HTML node, we give the following definition: Definition 1. (the similarity between the paths). Given two HTML paths p 1 and p 2 , we M ( n 1 ) is annotated with the same node name as that of element node n 1 . In addition, if p . 
In the extraction rules training, we can construct the HTML paths set P ( e ) for tar-get element e from different training HTML pages. In the extraction rules application, given a path p from the root node to the leaf node n in a new HTML tree, the similar-similarity between p and every path in P ( e ). n describes the content of node n 1 . 
Different web pages may have different thesaurus attributes for the same element node in the target schema. For example, in the E-commerce domain, the preceding node of the Price node for product may be labeled by  X  X he price X  or  X  X he discounted price X  or  X  X ur price X . In order to handle this problem, we extract the thesaurus attrib-utes from the HTML nodes in different pages which are mapped to the target element in the training phase. In the rule application phase, given a node n 1 in a HTML web page, we can locate its thesaurus attribute ta . If ta is one of the concepts in the ontol-mapped to element e . The Data Type Attribute: The data types of the contents on different nodes can also reveal some differences among nodes. For example, the data type of  X  X ublication web sites are numerical data. The currently supported data types in CCWrapper in-clude money, date time, and user defined date types, for example, address, telephone, e-mail, postal code, etc.

The given predefined schema is only validated for one specific domain. As for the rules. For example, we need not combine the thesaurus features into the training set in new domain since there are little useful thesaurus features of the HTML nodes in news domain. However, the thesaurus attributes play very important roles in the ex-traction rules in E-Commerce domain. 3.3 Basic Extraction Rules Application With the generated Bayes network classifier based on the HTML nodes features set, we can determine the probability of the target element e for each HTML node n , de-noted as Prob ( n , e ). In the following, the probability can be used as the basis to build a valid mapping from the HTML tree to target schema tree. Definition 2. ( Mapping M from HTML DOM tree T 1 to Target Schema Tree T 2 ). A T case of Card ( n 2 )=1. There are zero or more mapping nodes in T 1 in the case of Card ( n )=*. There is at most one mapping node in T 1 in the case of Card ( n 2 )=?. 
Since there are many valid mappings from a HTML DOM tree to a target schema tree, we need a benefit model to evaluate which mapping is better than others. Schema Tree T 2 ). The weight of the mapping M denoted as Weight(M) can be defined map to element e . 
In the case of Card ( e )=1, we select a HTML node n with the maximum Prob ( n , e ) maximum Prob ( n , e ) among all HTML nodes if Prob ( n , e ) is greater 50%, or else, we HTML nodes { n 0 ,.. n k }with Prob ( n i , e ) (0  X  i  X  k ) greater than 50%. 
The above rules can be used to establish the mapping from the HTML tree nodes to with Card ( e )=1 if the necessary features are used. However, the simple strategy may lead to the problem in the mapping to the element with Card ( e )=*. Taking the news content extraction as example, the hyper link node n may have low Prob ( n , e ) due to the limited length of the text even though n belong to the news content; The advise-content nodes. In order to handle this problem, we not only consider the HTML node n itself, but also consider the context of node n . We make the following assumption: HTML node mapping to element e with Card ( e )=1 can not be in the middle of adja-cent nodes set { n 1 ,.. n k }. 
Assumption 1 can be accepted in many domains. For example, the nodes for the content of the news web page are always adjacent. In addition, it is less likely that the nodes for the news title are located in the middle of the news content nodes. Definition 4. (Stub Node) Given one target element e with Card ( e )=1, if there exists e ), where n 1 is any HTML leaf node and n 1  X  n , we call HTML node n the Stub node. Since it is likely to map the stub node n to the correct target element node e with Card ( e )=1, we know that node n is less likely in the middle of the adjacent nodes sub divide the whole HTML leaf nodes sequence into a set of the node units: Definition 5. (HTML Nodes Unit) Given a HTML leaf nodes sequence S= { n 0 ,..., n k } S or S 2 is called nodes unit. 
With the HTML node units, we avoid the case that we map the nodes from differ-HTML node units can be described in the following algorithm. Algorithm 1. The basic extraction rule application Input: the HTML web page P , the Classifier C , the target schema S Output: the mapping M for the extraction on the web page P For each leaf node n in the web page P Calculate Prob ( n , e ) with Classifier C for each target element e For each element e with Card(e )=1 Select node n with the maximum Prob ( n , e )// n maps to e ; If node n meets the requirement of stub node For each element e with Card(e )=* or Card(e )=? 
Return the mapping from HTML nodes to all target elements. 3.4 Extraction Rules Application Based on Cluster over the HTML Tree Although the basic method can implement the web extraction, the precision of the basic method can be further improved. For example, even if we can select the map-ping with the maximum weight from the nodes in HTML nodes unit U to the element element e . basic method cannot remove the unrelated nodes from the HTML node units correctly lies in that the features used to generate the extraction rules only include the features totally. Next, we make another assumption on the HTML tree structure in the News or E-commerce domain. from another web page which is pointed by p 2 , we assume that users are not interested in the extraction of hyperlink p 2 in the process of the extraction of p 1 . 
In order to remove these adjacent hyperlink nodes more reasonable, we also need to consider the internal structure in one HTML nodes unit. We introduce the notation of the weight of the internal node to decide which nodes in one HTML nodes unit can be removed. Definition 6. (the weight of the Internal Node for HTML nodes Unit). Given a HTML LinkNodes )/ TotalNodes * P , where TotalLen is the total length of the node content for nodes in S , TotalNodes is the number of the all nodes in S , LinkNodes is the number of the hyperlink text nodes in nodes S . P is the sum of Prob ( s , e ), where s  X  S . 
The above definition conforms to the assumption 2. We notice that the more the nodes) is much higher than that of the internal node for the leaf hyperlink nodes. Such a characteristic can be used to locate one internal node  X  X over X  all and only extraction target nodes for one HTML node unit. Definition 7. (the lowest common ancestor LA for a nodes unit U with the threshold LA is called the lowest common ancestor of a nodes unit U with threshold T . 
The location of lowest common ancestor LA from HTML nodes unit U with thresh-from the root HTML node. Since the weight of one direct child node Body of HTML node is much higher (more than 30 times) than that of other child nodes, we know that it is more likely that the nodes interesting to the users are under node Body . We han-dle the internal nodes similarly until we find the last TD node. Notice that the lowest common ancestor can be used to improve the recall of web extraction, especially node in Fig.2, it will be extracted since it is under the lowest common ancestor node TD . 
Compared with the basic method, we improve the precision of CCWrapper by ex-ploiting the HTML internal structure to remove the unrelated nodes, mainly the adja-cent hyperlink text nodes, from the HTML nodes unit when mapping HTML nodes to the target element e with Card(e )=*. Such a process can be described as Algorithm 2. Algorithm 2. The HTML structure based node cluster for the nodes mapping 
Input: the HTML nodes units set in web page P , the element e with Card(e )=*, the threshold T Output: the nodes mapping M from the nodes in web page P to e For each HTML nodes unit U Select a nodes unit Umax with the maximum Weigtht ( M ); 
Output the mapping M from the nodes under the lowest common ancestor node in nodes unit Umax to element e ; We implemented CCWrapper with the JDK 1.4 on Windows 2000 r unning on a Dell Optiplex GX260 with P4 2GHz CPU and 512MB RAM. The Bayes Classifier pack-age used in CCWrapper is JBNC[13]. We evaluate CCWrapper against the news web pages from the influential web sites in China. The target schemas in the experiments contain the element e with Card(e )=*. 4.1 The Analyze of CCWrapper The precision of CCWrapper is related to the features used in the extraction rules. In distinguish the node from others, especially for the target element e with Card(e )=1. The recall of CCWraper can be improved by the notation of the lowest common an-cestor. All leaf descendant nodes under the lowest common ancestor node will be extracted, regardless what the mapping probabilities of these nodes are. 
The threshold used in locating the lowest common ancestor affects the recall and recall of the web extraction. The increase of threshold denotes that it is less likely to find a child node whose weight meets the requirement. Therefore, we cannot lower the common ancestor for the HTML nodes unit, which leads to the increase of recall decrease of the recall and increase of the precision. 
The adaptability of CCWrapper is addressed by the extracted rules based on the include the style attributes or the thesaurus attributes or date type attributes, etc. The the web pages from different web sites, which also improve the adaptability of CCWrapper. 
As for the human involvement in CCWrapper, human needs to select features used in extraction for target elements in a given predefined schema. In addition, user needs to establish the mapping from HTML nodes to target element nodes. The remaining steps in extraction rules generation from the training web pages and the application of these rules over the testing web pages can be performed automatically. 4.2 The Real Data Set Test We evaluate CCWrapper over the web pages in the news domain. The training set includes three HTML pages from www.sina.com.cn, www.sohu.com.cn, www.tom. news content , news link with Card ( news content )=*. We build the mapping from the HTML pages to the target schema. The evaluation data set also includes the web pages from other web sites. The precision and recall can be obtained from the nodes mapping generated from the CCWrapper. mapping is the irregularity of the real web page. For example, there may be only one picture as the main content of the HTML news, or the features of the news content are similar to those of other parts of the HTML pages. In addition, we also notice that the average precision and recall of the extracted result for the web sites not in the training set is not as high as that in the training se t. This is because the features on the HTML hand, the results in table 2 also demonstrate the adaptability of CCWrapper. www.chinadaily.com.cn 49 1029 88.9 93.2 www.cnradio.com.cn 67 1474 91.1 90.2
The CCWrapper can be implemented efficiently. Given n HTML leaf nodes and m demonstrates the efficiency of CCWrapper. The extraction time cost in CCWrapper is even less than that of parser time cost for some web pages. As for the target data extraction, RoadRunner[2], EXALG[3],Omini[5],MDR[6], template. Our method also shares some ideas from their work. For example, the gen-eralized node [6] plays the similar role as low common ancestor node in our method. However, the location of low common ancestor in our method is guided not only by the HTML tree structure, but also by the probability generated on the features of the HTML nodes. The method [4] also utilizes the visual information to extract the data. Our method supports an open framework which can incorporate different kinds of the heuristic features, including the visual features. Most important, the difference be-tween CCWrapper and this kind of methods lies in that CCWrapper not only extracts the data items, but also annotates the extracted item with the target element. 
The schema guide data extraction can be divided into the automatic wrapper induc-tion and handed crafted wrapper. Traditional Information extraction approach [7] treats web documents as tokens streams and use delimiter-based patterns to extract the web pages. If the query interface on the web is available, the fully automatic extrac-CCWrapper can exploit more semantic features rather than frequent tokens sequence. In addition, CCWrapper does not need the support from the query interface. 
The hand crafted wrapper receives high attention [9,10,11]. Although this kind of method can extract the web pages precisely, the construction and maintenance of the signed for one specific site is still valid on other web sites. 
The problem of schema mapping is studied recently in the data integration system ping. Different rules can be generated based on the structure, thesaurus, and date type features selection and extraction rules generation with Bayes classification. However, the HTML tree structure and the semantic meaning of the HTML node can also pro-schema mapping in the relational database context. In this paper, we propose a web extraction method called CCWrapper. Our method can combine different heuristics into one model and generate the extraction rules with classification method. In addition, our method exploits the HTML structure to im-periments on more web sites and incorporation of the inter-document relationship into our CCWrapper framework to improve the recall and precision of the web extraction. 
