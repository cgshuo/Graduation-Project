 1. Introduction
The Web is evolving toward the Semantic Web, in which the semantics of Web content is de understandable, and machine-processable [2] . Ontologies are considered a crucial component of the Semantic Web because they are the backbone of knowledge representation [57] . An ontology is an explicit formal conceptualization of a speci interest [29] . Ontologies have been studied by many researchers in various applications such as data annotation, data integration, and intelligent system development [47] .

Although many efforts have been made to develop ontologies, several issues still need to be resolved in order to construct an ontology effectively and ef fi ciently. First, since most ontology development processes are carried out manually, building an ontology is a time-consuming activity. This causes a bottleneck problem which results from the lack of a fast and ef developed for the automation of the ontology development process. These tools can be broadly classi and visualize domain knowledge before and during ontology construction. Ontology merging tools are used to create a single coherent ontology by unifying two or more existing ontologies. Ontology extraction tools support the automatic extraction of concepts and/or their relations by applying some techniques such as natural language processing or machine learning.
The second issue is related to tool evaluation. Several evaluation frameworks have been proposed to measure the performance most promising solution is the automation of the ontology development process using ontology extraction tools. Thus, when bottleneck problem. However, to the best of our knowledge, no evaluation framework exists on ontology extraction tools, which may be partly due to the inherent dif fi culty of the evaluation process.

The last issue deals with the quality of a constructed ontology. Evaluating ontology quality refers to assessing how well an ontology re fl ects the real world using concepts, relations, and axioms, among others. Since ontology quality may have a direct regardless of whether it is built manually or automatically [8,27] .

In this paper, we propose a novel evaluation framework that will guide us in measuring the various aspects of ontology measured based on three dimensions as adopted from the work of Burton-Jones et al. [8] : syntactic, semantic, and pragmatic qualities. Using our proposed evaluation framework, we carried out an experiment and reviewed four popular extraction tools:
OntoLT, Text2Onto, OntoBuilder, and DODDLE-OWL. 2. Related work 2.1. Automatic ontology extraction tools
Although some methodologies for building ontologies have been proposed to improve the ontology development process [11,18,25,55] , building ontologies manually is a time-consuming and laborious activity that requires the work of highly trained ontologyengineers.Moreover,anontologythatisbuiltmanuallytendstobebiasedtowarditsdeveloper'sview.Therefore,researchers have been interested in the automation of the ontology extraction process and have developed several ontology extraction tools.
Ontology extraction techniques rely on methods borrowed from various naturallanguageprocessing,statistics,andinformationretrieval.Beforetheintroductionofstatisticalandmachinelearningtechniques into the natural language processing domain, it was dif fi and adequateapproachesforlarge-scalecomputation.Nowadays,manylanguagescanbeprocessedbytools thatallowtherecognition
Typically, ontologies can be generated from various data types such as textual data, dictionaries, knowledge bases, semi-structured schemata, and relational schemata. Most works related to automatic ontology construction have been directed toward extracting an ontology from texts [56] . A typical approach in ontology extraction from text domain-speci fi c corpus at hand. Then, the extracted terms are clustered into groups with the purpose of identifying the taxonomies of potential classes. Relations can also be extracted by computing a statistical measure of connectedness between
OntoBuilder [26] , SOAT [60] , SVETLAN [9] , and Mo'K Workbench [3] . 2.2. Evaluation framework for ontology tools
The evaluation of the functionality of ontology tools is very important because it will help the ontology engineer choose an such as OntoClean [30] have been proposed for validating the ontological adequacy of taxonomic relationships based on philosophy, to the best of our knowledge, no comprehensive evaluation framework exists for ontology extraction tools. However, we believe that frameworks for ontology editing and merging tools would still be useful references when we develop an evaluation framework for ontology extraction tools. Thus, in this section, we review those designated for ontology editing and ontology merging tools. We also discuss the work of Navigli et al. [41] . Although Navigli et al. did not provide any evaluation criteria, they did try to evaluate ontology extraction tools.
 Lambrix and Edberg [34] evaluated two of the most popular ontology merging tools, Prot X g X -2000 with PROMPT [44] and
Chimaera [45] . The objective of their study was to measure how well these tools were suited for merging currently existing bio-interface. They tested these tools using literature and empirical studies. To de language, and functionality of Prot X g X  with PROMPT and Chimaera, they studied some relevant literature. They then tested these tools based on the criteria mentioned above. On the other hand, the user interfaces of Prot X g X  with PROMPT and Chimaera were with PROMPT and Chimaera were well-suited to model most of the current bio-ontologies.

Murshed and Singh [39] proposed an evaluation framework to help ontology engineers choose a proper editing tool for their reliability, robustness, learnability, availability, ef fi OntoEdit is suited for developing medium to large-sized ontology; and Metis is good for enterprise architecture modeling.
OntoLearn's main algorithms. Their goal was to compute the accuracy of OntoLearn under different learning circumstances. The three basic algorithms (terminology extraction, ontology learning, and semantic annotation) have all been individually evaluated in the economy and tourism domains. Most of the algorithms had good performances, while some algorithms, such as semantic annotation algorithm needed to be improved by updating the set of relations and enriching the training corpus with manually tagged examples from the new domain. Second, they conducted a per-concept qualitative analysis by domain specialists. They automatically generated natural language descriptions of formal concept speci hard to evaluate the formal aspects of a computational ontology. Two specialists participated in the per-concept qualitative analysis. The evaluation result showed that some of the descriptions would not be appropriate to take them over in an ontology just as they are. However, most of them turned out to be quite helpful, serving as a basis for building the ontology.
In summary, the previous research on the evaluation of ontology editing and merging tools proposed a set of criteria and merging tools. It is important to judge whether the ontology created by the tools satis and cons of these tools without any numerical and experimental comparison. In order to help users identify which tool is most suitable, however, the evaluation of ontology tools should provide a hands-on comparison of their functional performance using proper comparison methods.

Similarly, the foregoing issues should also be considered carefully when performing evaluations on the ontology extraction tools. As a fi rst attempt, we addressed these issues by, consider both usability and quality of a constructed ontology. In particular, to assess the ontology quality constructed by an extraction tool, we partly adopted Burton-Jones et al.'s well-de performed an empirical test using the Analytic Hierarchy Process (AHP) method, a widely used and well-tested method among available techniques in solving multi-attribute comparison problems [49] . 2.3. Ontology quality
Some efforts have been made to evaluate the quality of constructed ontologies [8,27,33,46] . Brank et al. [4,5] grouped various a domain expert. The concepts of a constructed ontology are evaluated by comparing them with those of a gold standard ontology, which are considered good representations of the concepts for the problem domain under consideration [17] . Typically, the gold standard approach is used to evaluate an ontology generated by a learning process. The second one is an application-based application or its performance on the given task might be better or worse depending on the ontology used in it. Ontologies may ontology is only a small component of the application, it is dif this approach evaluates an ontology by measuring the amount of overlap between the domain-speci approach relies on human judgment. In this approach, the evaluation is done by domain experts who try to assess how well the belongs to the last category.
 evaluation metrics from these four aspects and implemented them in a prototype tool called the validation results indicate that the metrics are feasible. In addition, the results highlight a wide variation in quality among of the Gene Ontology (GO), which would allow ontology engineers to pinpoint systemic way. They demonstrated the potential of these methods by applying them to isolate a subset of problematic GO terms.
By automatically aligning GO with other ontologies and taxonomies, they could propose alternative synonyms and de some of these problematic terms. They conclude that their methods provide triable indications of the quality of terms and de fi nitions in ontologies. Gangemi et al. [27] de fi ned three dimensions for ontology quality evaluation, namely, structural, functional, and usability-related. The structural dimension focuses on syntax and formal semantics. The functional dimension focuses on measuring how well an ontology is serving its purposes. The usability-related dimension checks whether the ontology the quality, completeness, and stability of ontology data as ontologies evolve. They compared automatic measurements and human evaluations of tourist ontologies and found that the software complexity metrics could be successful in automatically evaluating the complexity and cohesiveness of an ontology. They proposed a metrics suite based on standard software quality concepts to measure the complexity and cohesion of ontology data. They concluded that several of their metrics successfully determined ontology complexity or cohesion.

The features of ontology extraction tools are different from those of ontology merging tools and ontology editing tools. The main feature of the ontology merging tool is to create a single coherent ontology by identifying and matching the structures and semantics of two different ontologies. Meanwhile, the ontology editing tool helps ontology engineers organize domain knowledge from the corpora, and then, to automatically produce concepts and their relations from the identi new evaluation framework should consider both the functionality of a tool and the quality of a constructed ontology. As such, we attempt to propose a comprehensive framework addressing both in this paper. 3. Automatic ontology extraction tools To evaluate ontology extraction tools using our proposed framework, we selected OntoLT, Text2Onto, OntoBuilder, and
DODDLE-OWL for two reasons. First, these tools are well known as successful ontology extraction tools in the literature [7,14,19,28,52] . Second, most of other extraction tools are not available because these tools were developed as laboratory prototypes and are therefore not available to general users. 3.1. OntoLT
OntoLT [69] was developed at the DFKI GmbH, a German research center for arti plug-in. Prot X g X  [70] is a widely used ontology development tool. OntoLT enables the de rule-based system for German and English analysis. SCHUG provides annotation of part-of-speech and morphological in and decomposition. SCHUG is not an integrated part of OntoLT, but it can be accessed through a Web service.
The ontology extraction process with OntoLT is as follows. OntoLT provides a precondition language for de classes and slots, HeadNounToClass_ModToSubclass (mapping a head-noun to a class and in combination with its modi one or more sub-class(es)) and SubjToclass_PredToSlot_DObjToRange (mapping a linguistic subject to a class, its predicate to a allpossibleXML-elementsinthelinguisticannotationeithermanuallyorbyintegratingamachinelearningprocess.Thepreconditions
These preconditions are implemented as XPATH expressions over the linguistic annotation. If the precondition is satis classes and slots are automatically generated into a new ontology or integrated into an existing one [7] . 3.2. Text2Onto
Text2Onto [71] , a successor of Text-To-Onto, was developed at the AIFB Institute of the University of Karlsruhe in Germany for creating and maintaining ontologies, which was also developed at the AIFB Institute.

Text2Onto combines machine learning approaches with basic linguistic processing techniques such as tokenization, lemmatizing, and shallow parsing. Text2Onto is based on the GATE processing in Text2Onto begins with tokenization and sentence splitting. The resulting annotation set serves as an input for a
POS tagger that assigns appropriate syntactic categories to all tokens. Finally, lemmatizing or stemming is conducted through a morphological analyzer and stemmer, respectively. The learning process then begins based on machine learning and linguistic with respect to the corpus in question. In particular, Text2Onto contains several algorithms calculating the following measures:
Relative Term Frequency (RTF), Term Frequency Inverse Document Frequency (TFIDF), and entropy. Furthermore, there are various algorithms exploiting the hypernym structure of WordNet [24] , matching Hearst patterns [31] , and applying linguistic heuristics [35] in order to learn relations.
 of the extraction process (also called a  X  learning process independent concepts. Domain-independent concepts are removed to better adjust the vocabulary of the domain ontology. Therefore, the result of the process is a domain ontology that contains only the domain concepts learned from the input sources.
The whole process, supervised by ontology engineers, is a cycle which is iterated to re 3.3. OntoBuilder
OntoBuilder [68] was developed by the Israel Institute of Technology. It was originally developed for evaluating and improving automatic schema matching algorithms. OntoBuilder supports the extraction of ontologies from Web interfaces, ranging from simple search engine forms to multiple pages of reservation systems. It extracts concepts and their relations methods learned from a training set of HTML documents. This system can create ontologies automatically and combine them into a global ontology [26] .
 OntoBuilder was designed to operate like a Web browser (see Fig. 4 ). To navigate through a page, the user simply enters the
URL. Once the Web page from which the user wants to extract concepts is loaded into the OntoBuilder, it begins to build an ontology. Once a Web site is accessed by the OntoBuilder browser, each page is parsed into a data structure called a document object model. After identifying the elements of a page, OntoBuilder generates a dictionary of terms by extracting labels and names from the Web page. It also recognizes unique relations among terms and these relations are used in the matching algorithms.
 3.4. DODDLE-OWL Domain Ontology rapiD DeveLopment Environment-OWL extension (DODDLE-OWL) [63] was developed by Morita et al. [38] .
It is extended from DODDLE and DODDLE II in order for it to be used for the Semantic Web. DODDLE-OWL reuses existing ontologies and supports the semi-automatic construction of taxonomic and other relations using domain-speci ontologies.
 terms that are extracted by DODDLE-OWL and identi fi es the meaning of each term to map it to the corresponding concept in referring to the reference ontologies and documents. This is done through the construction module. In the re initial ontology generated by the construction module is re Lastly, in the translation module, the ontology constructed by DODDLE-OWL is exported into OWL language. Furthermore,
DODDLE-OWL can connect with MR 3 [65] which provides a graphical editor [38] . 4. The evaluation framework for ontology extraction tools
According to Hevner et al. [32] , newly developed systems need to be justi gathering and analysis of appropriate data. Hevner et al. de in their proposed framework for information system research. Moreover, they emphasized that each developed system must yield utility for the speci fi ed problem. Hence, thorough evaluation of the system is a crucial activity.

We identi fi ed important tool evaluation criteria and developed an evaluation framework that will guide us in assessing the extraction tools and the quality of the constructed ontology. In this section, we discuss our proposed evaluation framework.
As shown in Fig. 6 , the criteria are grouped into three features. The programs' general and exterior features such as user interface before they get down to using the main functions of each program algorithm among various ones. Hence, this part consists of seven criteria, namely, preprocessing required, ontology reusability, extraction level, degree of automation, algorithm selection, ef 4.1. General features
This part deals with a tool's exterior features and the convenience of its use. The [15,43] .Wede fi ne three properties of the user interface: supporting languages, graphical representation, and usability. The user to choose a familiar interface language among various languages, we consider it more one language. Graphical representation checks whether a tool can display an extracted ontology visually. If a tool represents an complex, it may require more time to master the tool's functions [42,53] .
 there exist several ontology extraction tools, most of these tools are not easily obtainable. Hence, we should availability of each tool before testing which extraction tool is better.

The last criterion is the time-to-fi rst-use . This concerns the following questions: specify additional platform requirements for installation?, its fi rst-time use. 4.2. Extraction features
Since our framework is developed to evaluate ontology extraction tools, examining the various aspects of an extraction process performed by a tool is considered the most important part of this study. When evaluating the extraction features, we two criteria pertinent to input data. First, we need to consider whether a tool requires speci format if the source data contains plain texts. Moreover, some ontology extraction tools require the source data to be annotated
Therefore, we should consider whether a tool requires additional preprocessing effort ( preprocessing requirement ). In many situations, it is more convenient to use concepts from existing ontologies when constructing a new ontology. Hence, we also need to investigate whether a tool allows the user to refer to existing ontologies, and whether it allows him/her to reuse them when constructing a new one ( ontology reusability ).

In addition, we should also consider the extraction level of a tool. When building an ontology, ontology engineers have to de and organize concepts and their relations. While some tools automatically or semi-automatically generate concepts only, other tools can extract various relations between concepts as well. Therefore, the extraction level examines whether a tool can ontology extraction tool can automatically extract both concepts and their relations.
 In addition to the extraction level, we also consider the degree of automation during the concept and relation extraction.
Generally, automatic extraction tools are considered to be one which requires no human intervention during the actual extraction proper extraction algorithms before the extraction process starts. On the other hand, if the extraction rules must be de user before extracting concepts/relations or any human intervention is required during the extraction process, these tools are classi fi ed as semi-automatic tools. When using an automatic extraction tool, whether the user de for his/her speci fi c purpose is optional. When using a semi-automatic tool, however, the user is forced to de rules before extracting concepts and/or relations, or he/she should intervene to set parameters. Note that in many cases, a tool concepts (and relations) automatically or semi-automatically.
 to extract a more suitable ontology for what he/she wants to construct. For example, some algorithms are good for distinguishing various forms of a verb, whereas the extraction result may include redundant and unnecessary forms. Other algorithms are more suitable for extracting fundamental terms, but the result of extraction may not be fully exploited. Hence, we need to look into whether a tool provides various algorithms from which the user can choose. (i.e., new algorithms) by adding plug-ins.

Finally,like other software programs, the ef fi ciency of a tool should be considered.Ef the source data and to present the target output to the user. Some tools may take a longer time to extract concepts due to their inef fi cient extraction algorithm. If tools present similar extraction results, it is better to choose more ef 4.3. Quality features measure the utility of an ontology. To evaluate an ontology, some studies employed a formal ontology as a benchmark [10,58] .
However, there is no way to validate whether such a formal ontology is ideal for benchmarking [59] . Moreover, formal ontologies measures to evaluate ontology quality. Their research was based on a semiotic framework (the more generaltheoretical framework derived from linguistics) which explicitly deals with pragmatic issues. Therefore, in this research, we adopt their well-de criteriatomeasurethequalityaspectsofan ontology.Thesemioticframeworkproposedby Burton-Jonesetal. consistsofcriteriafor depends on its preceding one; for example, semantic quality is a higher level than syntactic quality and it depends on syntactic quality.Amongthesequalities,socialqualitydealswithontologymaintenanceandevolutionissues.Itmeasurestheextentto which experimental timetable. Thus, in our framework, we only adopt their syntactic, semantic, and pragmatic qualities.
First, the syntactic quality dimension measures the syntax and richness. Correctness checks the degree to which the syntax of an ontology conforms to the speci language. In other words, it measures the correctness of the syntax. Not all ontology editors have syntactic error-checking constructs used in a constructed ontology, which are provided by the target ontology language. In other words, it measures how many syntactic vocabularies are used to describe an ontology. For example, let us assume that an ontology uses only two types of richer than the former in terms of syntactic quality. The richness measures the breadth of syntax being used.
Second, the semantic quality dimension examines the meaning of terms expressed in a constructed ontology. It has three the real world. Consistency measures whether the meaning of individual terms is consistently used throughout the ontology. For is one having morethan one meaningand can be used in different contexts to expresstwo or moredifferent meanings.For example, the word  X  bear  X  is polysemous. If an ontology claims that the class  X  Jones et al. measure clarity by counting the number of word senses in WordNet for the class or property name as a whole.
Finally, the pragmatic quality dimension evaluates an ontology's usefulness for users or their software agents regardless of its syntax and semantics. It has three attributes: accuracy, coverage, and completeness. Accuracy measures whether the claims the ontology makes are true. For example, let us assume that there is a UNIVERSITY ontology. If this ontology models Student as a the real world. In general, we can think that a student belongs to a department; thus, a student is a subset of a department.
However, because a student is not a subclass of a department, more knowledge to the user. Lastly, completeness 8 measures whether an ontology provides the syntactic vocabularies needed by a speci fi c application. For example, if an ontology provides only class/subclass and property information, it is not a complete ontology for an application that needs cardinality information. 5. An experiment of automatic ontology extraction tools compared the performance of the tools based on our evaluation framework in Section 5.1 . To compare the tools objectively, we an input source for OntoBuilder. Then, for Text2Onto and DODDLE-OWL, we extracted plain text Finally, the text fi les for OntoLT were annotated because the corpus for OntoLT has to be linguistically annotated in advance.
Second, we used an empirical approach to assess and rank the ontology extraction tools. Four participants evaluated them using the Analytic Hierarchy Process (AHP) method. Since the evaluation process of ontology extraction tools may be considered the selection process of choosing the right tool, we applied the AHP method, which is one of the more popular multi-criteria decision-making methods. The AHP method [49] is especially suitable for a comparison of decision elements that are dif quantify. This is a widely used and well tested method among available techniques to solve the multi-attribute problems. Two experts and two PhD candidates participated in our experiment. The two experts have extensive experience in ontology construction. One expert has been involved in several Semantic Web development projects as well as in teaching ontology and the Semantic Web to graduate students. The other is an ontology engineer who has several years of working experience in developing
Semantic Web platforms. The two PhD candidates are currently conducting ontology-related research, and have developed them using the AHP method. The empirical test results using the AHP method are presented in Section 5.2 . 5.1. Evaluation of ontology extraction tools
The primary goal of our experiment is to evaluate the functional performance of current ontology extraction tools. In this section, we discuss the characteristics of ontology extraction tools according to our proposed evaluation criteria. 5.1.1. General features
The general features include three criteria: user interface, availability, and time-to-the interface language. An evaluation of the second attribute, graphical representation, revealed that Text2Onto and OntoBuilder provide the ability to display a constructed ontology visually, which makes it easier for the user to understand the ontology structure. In contrast, DODDLE-OWL and OntoLT do not include a visualization module. DODDLE-OWL can represent the constructedontologyvisuallyby connecting withMR 3 . OntoLTcan use thevisualization modulein Prot X g X . Lastly, the userinterface of OntoLT is somewhat less user-friendly than other tools. In OntoLT, uploading corpus and extracting candidate concepts or for extracting candidate concepts. It is more convenient and ef
OWL can be obtained from SourceForge.net. SourceForge.net is the world's largest open source software development Web site and
The last criterion of the general features is the time-to-of Prot X g X ; thus, Prot X g X  must be pre-installed on a PC and the user must know how to install plug-ins. DODDLE-OWL uses text fi les as source data by default. However, if the user wants to extract words from documents not in text format, he/she can use xdoc2txt [72] . xdoc2txt can convert Microsoft Word, Microsoft Excel, Microsoft Power Point, and Adobe PDF documents into text fi les automatically. Note that converting formats such as Microsoft Word, or Adobe PDF document into text especially where complex tables and fi gures are included. 5.1.2. Extraction features
As discussed in Section 4.2 , the extraction features include seven criteria: preprocessing requirement, ontology reusability, extraction level, degree of automation, algorithm selection, ef not require the input data to be preprocessed. However, OntoLT requires preprocessing. In OntoLT, the corpus must be annotated using its proprietary XML format which includes part-of-speech tags and morphological analysis. As the most common form of corpus annotation, part-of-speech tags mark up the individual words in a text which correspond to a particular part of speech examines the internal structure of words.
 Some of the tools we tested reuse existing ontologies. Text2Onto can employ existing ontologies as its input source. DODDLE-
OWL uses WordNet and EDR, which are linguistic ontologies in English and Japanese, respectively. Furthermore, DODDLE-OWL supports the semi-automatic construction of taxonomic and other relations referring to other ontologies.
As shown in Table 1 , the evaluation results of extraction-level show that Text2Onto, OntoLT, and OntoBuilder are capable of extracting both concepts and relations from a corpus. DODDLE-OWL extracts both concepts and their taxonomies.
The tools we examined have different degrees of automation. With OntoLT, relations are extracted only by a semi-automatic method. When the user constructs an ontology using OntoLT, he/she must check to see whether the extracted relations are meaningful. In contrast, Text2Onto is capable of extracting both concepts and their relations semi-automatically and automatically. In other words, with Text2Onto, the user has the option of choosing automatic extraction or semi-automatic before sending it to the ontology editor. DODDLE-OWL extracts concepts automatically but taxonomies semi-automatically, while OntoBuilder can extract only concepts automatically.

Table 2 shows the numerical comparison of the extracted outputs using the same corpus related to tourism. Each text automated extraction without any user intervention. All experiments were carried out on a Windows 2002 with Intel Pentium processors (3.40 GHz) and 1 Gb of memory.

As shown in Table 2 , DODDLE-OWL takes a longer time to extract concepts. If extraction time takes less than 30 s, the ef is regarded as  X  high,  X  30  X  60 s is  X  normal,  X  and more than 60 s is are highly ef fi cient while the ef fi ciency of DODDLE-OWL is lower than other tools.

Our evaluation of the algorithm selection shows that OntoLT and Text2Onto are more and Text2Onto include, by default, various extraction algorithms and allow the user to de hand, when using OntoBuilder and DODDLE-OWL, users have no option to select a desirable extraction algorithm because they do not incorporate various ones.
 5.1.3. Quality features
As stated earlier, we adopted three criteria from Burton-Jones et al.'s framework [8] to evaluate ontology quality: syntactic, semantic, and pragmatic quality. The syntactic quality dimension has two attributes: correctness and richness. The correctness rules. All the tools we tested produced ontologies that were syntactically correct. In terms of richness, all the tools except
OntoBuilder used various types of syntactic vocabularies to describe ontologies. Ontologies constructed using OntoBuilder used
The next criterion is semantic quality. It consists of three attributes: interpretability, consistency, and clarity. As the evaluation attribute of semantic quality, interpretability measures whether the concepts in an ontology represent a correct meaning as used in the real world. The concepts in ontologies constructed by OntoLT, Text2Onto, and OntoBuilder have proper
DODDLE-OWL. Since DODDLE-OWL does not automatically assign the meaning of each term, unlike other tools doing so automatically, it is not feasible to evaluate the interpretability of pure concepts produced by DODDLE-OWL. In other words, when using DODDLE-OWL, the ontology engineer should manually identify and check the meaning of each term in order to map it to the corresponding concept in WordNet. In terms of consistency, the terms generated by OntoLT, Text2Onto, and OntoBuilder have measure the clarity, we used the Burton-Jones et al.'s method that measures clarity by counting the number of word senses in
WordNet for concept or relation names. We examined terms of each ontology's top 20 concepts that were most frequently occurred and they did not have multiple meanings. This result may be due to the simple tourism data set we used as an input source.
Lastly, pragmatic quality is measured by three attributes: accuracy, coverage, and completeness. Accuracy is determined by checking the information given by the ontology against existing knowledge known to be true. Thus, this attribute must be subjectively evaluated by a human expert. All the tools we tested were accurate. In terms of coverage, most of the tools were capable of building a large ontology. On the other hand, the coverage of the ontology generated by OntoBuilder is limited.
OntoBuilder was originally developed in an attempt to map between the structure of a speci understand the Web site's overall structure by extracting concepts and their relations existing in the HTML page. Although we
Our evaluation of the completeness showed that the ontology created by OntoBuilder is less complete than the rest of tools. As syntactic vocabularies that include only class, domain, attribute, and axiom. Therefore, if a speci information, the ontology constructed by OntoBuilder is not complete. Thus, since the ontologies constructed using OntoBuilder use a few types of syntactic vocabularies, it is less complete than the other tools.
 5.2. Application of the AHP for selecting the ontology extraction tool The AHP developed by Saaty [49] is a practical approach in solving relatively complex multi-criteria decision-making problems.
Based on mathematics and psychology, it provides a comprehensive and rational framework for structuring a decision problem, in a wide variety of decision situations in fi elds such as government, business, industry, and education [20,48] .
The AHP is a selection process that consists of the following four steps: 1. Decide upon the criteria for selection. 2. Rate the relative importance of these criteria using pair-wise comparisons. pair-wise comparisons of the choices). 4. Combine the ratings derived in steps 2 and 3 to obtain an overall relative rating for each potential choice. of one criterion in relation to the other and 9 meaning one criterion is extremely more important than the other with increasing single weight for each criterion and single priority for each ontology extraction tool by calculating the arithmetic mean.
The four participants then systematically evaluated various criteria and ontology extraction tools through pairwise comparison. Each participant's evaluation result is summarized in the pairwise comparison matrix. The pairwise comparison matrix is constructed as follows:
Let C 1, C 2 ... Cn be the set of elements, while aij represents a quanti comparisons are made in terms of which element dominates another (i.e., based on the relative importance of elements). If entered in row Cj , column Ci [49] .

Having done all pairwise comparisons, weight for each criterion is determined. The column of numbers is normalized by others. Those criteria are extraction level, reliability, and semantic quality.

Finally, numerical priorities were calculated for each of the ontology extraction tools. The priority score of each ontology extraction tool is calculated as follows: where best suited ontology extraction tool is concluded to be Text2Onto with a global priority of 0.3623.

Using Fleiss' Kappa index, we checked the consistency between heterogeneous inclinations toward expertise. The Kappa index may be used as a proxy instrument to conduct a power test, which is widely adopted in order to acknowledge a proper sample size with statistical assumptions. It ranges from 0 (chance agreement) to 1 (perfect agreement), and generally a Kappa considered satisfactory. In our experiment, the reliability for the participants was found to be Kappa=0.72, implying that the participants' four rating scores are signi fi cantly consistent.

To sum up, in our experiments, Text2Onto ranked the best among four ontology extraction tools. This may be due to the following reasons: First, Text2Onto not only extracts both concepts and relations automatically, but also provides various input data types, including text fi les, XML, HTML, and Adobe PDF. Second, Text2Onto is very algorithms from which the users can choose from. In addition, Text2Onto has good scores in terms of quality. Ontologies constructed by Text2Onto use various types of syntactic vocabularies and are semantically clear since Text2Onto identi meanings of polysemous words using WordNet-based extraction algorithms. Although Text2Onto received the best score among four tools in our experiments, our evaluation result is not carved in stone. In other words, it is dif many concepts as possible, or if the user wants to extract concepts from Japanese texts, DODDLE-OWL is the best among the four tools. If the user cares about generating more relations, OntoLT may be the top option. 5.3. Current issues of ontology extraction tools
This section discusses current issues of ontology extraction tools based on the results of our experiments. Current ontology extraction tools have good reliability and graphical representation, and do not require much preprocessing effort. However, the following should be improved.

First, although ontology extraction has been studied extensively, further efforts must be made to develop and improve approaches are easier to compute and implement, while symbolic ones are more precise and provide more reasonable results because generally, they perform semantic analysis and reasoning on the basis of background knowledge [52] . Although statistical approaches are usually general and can be used for different domains or languages, their main disadvantage is that it is dif extract general concepts contained in technical texts or speci symbolic approaches, such as linguistic-based or pattern-driven methods, are mostly language-dependent and need domain experts or linguists. In addition, the cost of adapting extracted patterns or semantic knowledge to a new domain may be high.
Thus, although symbolic approaches have good performance in particular domains for extracting speci they are limited and in fl exible. Consequently, some tools based on the symbolic approach are less ef precise. There might be different ways to enhance the performance of ontology extraction tools, such as a new hybrid approach combining both statistical and symbolic approaches.

Second, various aspects of ontology extraction tools must be improved to make them user-friendly. Ontology extraction tools were fi rst developed around a decade ago. Only a few were developed for commercial purpose, while most were developed for provide an installation and user guide. Currently, only a few ontology extraction tools provide such a manual. ontology extraction tools need to be improved further, and should be able to extract all the ontology elements such as concepts, taxonomies, and relations. Furthermore, they should support a fully automated extraction process. Although a few fully automatic compared to semi-automatic or cooperative extraction tools [52] . In addition, in many cases, a tool labeled as an ontologieswithouthumanintervention.Althoughseveralontologyextractiontoolshavebeendeveloped,mostofthetoolsarestillnot tools should be improved to show more acceptable results, thus minimizing human intervention.

Fourth, it is obvious that the reuse of an existing ontology is more ef
Although a few tools such as SPRAT [37] can reuse and modify an existing ontology to create a new one, many ontology extraction tools still fail to support ontology reuse. Therefore, to generate ontologies more ef designed to aid ontology reuse.

Finally, ontology extraction tools should be syntactically rich in displaying concepts or relations. Some ontology extraction tools used various syntactic vocabularies including cardinality, axiom, and subproperty, in addition to those simple syntactic terms, the constructed ontologies would be syntactically richer. Furthermore, it is desirable for ontology extraction tools to support a function that allows the user to export an ontology in various ontology languages. Currently, most ontology extraction tools can export extracted outcomes to OWL or RDF. However, if more diverse languages such as XTM, XOL, and F-Logic are supported by tools, the exported ontologies can be more usable for more various applications. 6. Conclusion Along with ontology editing and ontology merging tools, ontology extraction tools play a key role in the development of the
Semantic Web. However, there is no widely accepted evaluation framework that allows users to objectively compare the functional performances of various ontology extraction tools. In this paper, we proposed a comprehensive evaluation framework to evaluate ontologyextractiontoolsconsideringboththetool'sperformanceduringtheconstructiontimeandthequalityofthecreatedontology. Using the proposed evaluation framework, we carried out an experiment and evaluated four popular ontology extraction tools. experiences during the construction process. Several tools still need functional performance improvement. For example, OntoLT annotation functions are not included in OntoLT, the corpus should be annotated in advance with the help of other annotation tools. Moreover, most of the tools, except for Text2Onto, do not provide various extraction algorithms. Each algorithm might be extraction algorithm for a speci fi c source data, the extracted ontology may be neither satisfactory nor acceptable.
Ontology technologies have recently become popular and have attracted more attention than ever before because they are the cornerstones for the realization of the Semantic Web. Establishing a way to develop ontologies effectively and ef most needed to improve these ontology technologies. It is because of this that some researchers have paid great attention to standardizing ontology development and evaluation methodologies. In addition, others have tried to minimize the ontology-building time. To accomplish this, various ontology tools have been developed. In particular, ontology extraction tools are considered a major solution since they can speed up the ontology construction time through the automation of the ontology development process. When developing a new ontology extraction tool, if a standard that the tool has to meet is well set up, developers can clearly comprehend the requirement and hence easily undertake tool development. Furthermore, developers may also receive essential feedback about the ontology extraction tool under development. Our framework can be applied as a useful benchmark or guidance when developing an ontology extraction tool.

References
