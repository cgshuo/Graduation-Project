 Spike driven synaptic plasticity mechanisms have been thoroughly investigated in recent years to solve two important problems of learning: 1) how to modify the synapses in order to generate new memories 2) how to protect old memories against the passage of time, and the ov erwriting of new memories by ongoing activity. Temporal patterns of spikes can be encoded with spike-timing dependent plasticity (STDP) mechanisms ( e.g. see [1, 2]). However, STDP in its simples t form is not suitable for learning patterns of mean firing rates [3], and most of the proposed STDP learning algorithms solved the problems of memory encoding and memory preserv ation only for relati vely simple patterns of mean firing rates.
 Recently a new model of stochastic spike-driven synap tic plasticity has been proposed [4] that is very effective in protecting old learned memories, and captures the rich phenomenology observ ed in neuroph ysiolog ical experiments on synaptic plasticity , including STDP protoc ols. It has been shown that networks of spiking neurons that use this synaptic plasticity model can learn to classify comple x patterns of spike trains ranging from stimuli generated by auditory/vision sensors to im-ages of handw ritten digits from the MNIST database [4]. Here we describe a neuromorphic VLSI implementation of this spike-driven synaptic plasticit y model and present classification experiments using the VLSI device that validate the model X  s implementation. The silicon neurons and synapses inside the chip are implemented using full custom hybrid analog/dig ital circuits, and the network X  X  spikes are received in input and transmitted in output using asynchronous digital circuits . Each spike is represented as an Address-Event , where the address encodes either the source neuron or the destination synapse. This device is part of an increasing collection of spike-bas ed computing chips that have been recently developed within the frame work of Address-Ev ent Representati on (AER) systems [5, 6]. There are even multiple implem entations of the same spike-driven plasticity model being investigated in parallel [7, 8]. The focus of this paper is to show that the VLSI device pro-posed here can successfu lly classify comple x patterns of spike trains, producing results that are in accordance with the theoreti cal predictions. Figure 1: Layout of a test chip comprising a network of I&amp;F neurons and plastic synapses. The placement of a single neuron along with its synaps es is highlighted in the top part of the figure. Other highlighted circuits are described in the test.
 In Section 2 we describe the main features of the spike-based plasticity model and show how they are well suited for future scaled CMOS VLSI technologies; in Section 3 we characterize the function-ality of the spike-based learning circuits; in Section 4 we show control experiments on the learning properties of the VLSI network; and in Section 5 we present experimental results on comple x pat-terns of mean firing rates. In Section 6 we present the concluding remarks and point out future outlooks and potential applic ations of this system. Physical implementat ions of long lasting memories, either biological or electronic, are confronted with two hard limits: the synaptic weights are bounded (they cannot grow indefinitely or become negative), and the resolution of the synapse is limited ( i.e. the synaptic weight cannot have an infinite number of states). These constraints, usually ignored by the vast majority of softw are models, have strong impact on the classi fication performance of the network, and on its memory storage capacity . It has been demonstrated that the number of rando m uncorrelated patterns p which can be classified or stored in a network of neurons connected by bounded synapses grows only logarithmically with the number of synapses [9]. In addition, if each synapse has a n stable states ( i.e. its weight has to traverse n states to go from the lower bound to the upped bound), then the number of patterns p can grow quadratically n . However, this can happen only in unrealis tic scenarios, where fine tuning of the network X  X  parameters is allowed. In more realistic scenarios where there are inhomogeneities and variability (as is the case for biology and silicon) p is largely independent of n [9]. Therefore, an efficient strate gy for implementing long lasting memories in VLSI networks of spiking neurons is to use a large numb er of synapses with only two stable states ( i.e. n = 2), and to modify their weights in a stochastic mann er, with a small probability . This slows down the learning process, but has the positi ve effect of protecting previously stored memories from being ov erwritten. Using this strate gy we can build large networks of spiking neurons with very compact learning circuits ( e.g. that do not require local Analog-to-Digit al Converters or floating gate cells for storing weight values). By construction, these types of devices operate in a massi vely parallel fashion and are fault-tolerant: even if a considerable fractio n of the synaptic circuits is faulty due to fabrication problems, the ov erall functionality of the chip is not compromised. This can be a very favorable property in view of the potential problem s of future scaled VLSI processes.
 The VLSI test chip used to carry out classification experiments implementing such strate gy is shown in Fig. 1. The chip comprises 16 low-po wer integrate-and-fire (I&amp;F) neurons [5] and 2048 dynamic synapses. It was fabricated using a standard 0 . 35  X  m CMOS technology , and occupies an area of 6 . 1 mm 2 . We use an AER communicati on infrastructure that allo ws the chip to receive and transmit asynchronous events (spikes) off-chip to a workstation (for data logging and prototyp ing) and/or to other neuromorphic event-ba sed devices [10]. An on-chip multiple xer can be used to reconfigure the neuron X  s internal dendritic tree connec tivity. A single neuron can be connected to 128, 256, 512 or 1024 synapses. Depending on the multiple xer state the number of active neurons decrease from 16 to 2. In this work we configured the chip to use all 16 neurons with 128 synapses per neuron. The synapses are divided into different functional blocks: 4 are excitatory with fixed (externally adjustable) weights, 4 inhibi tory and 120 excitatory with local learning circuits.
 Every silicon neuron in the chip can be used as a classifier that separates the input patterns into two categories. During training, the patterns to be classified are presented to the pre-synaptic synapses, Figure 2: (a) Plastic synapse circuits belonging to the neuron X  s dendritic tree. The synaptic weight node w is modifie d when there is a pre-syna ptic input ( i.e. when S 1 and S 2 are on) depending on the values of V UP and V DN . In parallel , the bistable circuit slowly drives the node w toward either of its two stable states depending on its amplitude. The DPI is a pulse integrator circuit that produces an Excitatory Post-Synaptic Current ( I EPSC ), with an amplitude that depends on the synaptic weight w . (b) Neuro n X  X   X  X oma  X  block diagram with stop-learning module. It comprises a low-po wer I&amp;F neuron block, a DPI integrator , a voltage comparator and a three current comparators( CC ). Winner -take-all (WTA) circuits are used as current comparators that set the output to be either the bias current I B , or zero. The voltage comparato r enables either the I UP or the I DN block, depending on the value of V mem with respect to V mth . The voltages V UP and V DN are used to broadcast the values of I UP and I DN to the neuron X  s dendritic tree. in parallel with a teacher signal that represents the desired response. The post-synaptic neuron responds with an activity that is proportion al to its net input current, generated by the input pattern weighted by the learned synaptic efficacies, and by the teacher signal. If the neuron X  s mean activity is in accordance with the teacher signal (typically either very high or very low), then the output neuron produces the correct response. In this case the the synapses should not be updated. Otherwise, the synapses are updated at the time of arrival of the (Poisson distrib uted) input spikes, and eventually make a transition to one of the two stable states. Such stochastici ty, in addition to the  X  X top-learning X  mechanism which prevents the synapses from being modified when the output is correct, allows each neuron to classify a wide class of highly correlated, linearly separable patterns. Furthermore, by using more than one neuron per class, it is possible to classify also comple x non-linearly separable patterns [4]. The learning circuits are responsible for locally updating the synaptic weights with the spike-based learning rule proposed in [4].
 Upon the arrival of a pre-synaptic spike (an address-e vent), the plastic synapse circuit updates its weight w according to the spike-driven learning rule. The synapse then produces an Excitatory Post-Synaptic Current (EPSC) with an amplitude proportional to its weight, and with an exponential time course that can be set to last from microseconds to several hundreds of milliseconds [11]. The EPSC currents of all synapses afferent to the target neuron are summed into the neuron X  s membrane capacitance, and eventually the I&amp;F neuron X  s membrane potential exceeds a thresh old and the circuit generates an output spike. As prescribed by the model of [4], the post-synaptic neuron X  s membrane potential, together with its mean firing rate are used to determine the weight change values  X  w . These weight change values are expressed in the chip as subthre shold currents. Specifically , the signal that triggers positi ve weight updates is represented by an I UP current, and the signal that triggers weight decreases if represented by the I DN current.
 The weight updates are performed locally at each synapse, in a pre-synaptic weight update module , while the  X  w values are computed globally (for each neuron), in a post-synaptic weight contr ol module . Figure 3: Post-synaptic circuit data. (a) State of the V UP and V DN voltages as a function of the cal-cium concentration voltage V Ca . (b) State of the V UP and V DN voltages as function of the membrane potential V mem . This data corresponds to a zoomed-v ersion of the data shown in (a) for V Ca  X  2 . 8 V . 3.1 Pre-synaptic weight-up date module This module , shown in Fig. 2(a), comprises four main blocks: an input AER interf acing circuit [12], a bistable weight refresh circuit, a weight update circuit and a log-domain current-mode integrator , dubbed the  X  X iff-pair integrator X  (DPI) circuit, and fully characterize d in [11]. Upon the arrival of an input event (pre-synaptic spike), the asynchronous AER interf acing circuits produce output pulses that activate switches S 1 and S 2. Depending on the values of I UP and I DN , mirrored from the post-synaptic weight control module, the node w charges up, dischar ge toward ground, or does not get updated. The same input event activates the DPI circuit that produces an EPSC current ( I EPSC ) with an amplitude that depends on the synaptic weight value w . In parallel, the bistable weight refresh circuit slowly drives w toward one of two stable states depending on whether it is higher or lower than a set thresh old value. The two stable states are global analog parameters, set by external bias voltages. 3.2 Post-synaptic weight control module This module is responsible for generating the two global signals V UP and V DN , mirrored to all synapses belonging to the same dendritic tree. Post-synaptic spikes ( V spk ), generated in the soma are integrated by an other instance of the DPI circuit to produce a current I Ca proportional to the neuron X  s average spiking activity. This current is compared to three threshold values, I k 1 , I k 2 , and I k 3 of Fig. 2(b), using three current-mode winner -take-all circuits [13]. In parallel, the instantaneous value of the neuron X  s membrane potential V mem is compared to the threshold V mth (see Fig. 2(b)). The values of I UP and I DN depend on the state of the neuron X  s membrane potential and its average V mem &lt; V mth , then I DN = I B . Otherwise both I UP , and I DN are null.
 To characterize these circuits we injected a step current in the neuron, produced a regular output mean firing rate, and measured the voltages V Ca , V UP , and V DN (see Fig. 3(a)). V Ca is the gate volt-age of the P-FET transistor produc ing I Ca , while V DN , V UP are the gate voltages of the P-and N-FET transistors mirroring I DN and I UP respecti vely (Fig. 2(a)). The neuron X  s spikes are integrated and the output current I Ca increases with an exponential profile ov er time ( V Ca decreases accordingly ov er time, as shown in Fig. 3(a)). The steady-state asymptotic value depend s on the average input frequenc y, as well as the circuit X  s bias parameters [11]. As I Ca becomes larger than the first thresh-old I k 1 ( V Ca decreases below the corresp onding threshold voltage) both V UP and V DN are activated. When I Ca becomes larger than the second threshold I k 2 the V DN signal is deacti vated, and finally as I Ca becomes larger than the third threshold I k 3 , also the V UP signal is switc hed off. The small  X  300mV chang es in V UP and V DN produce subthres hold currents ( I UP and I DN ) that are mirrored to the synapses (Fig. 2(a)). In Fig. 3(b) the V DN and V UP signals are zoomed in along with the membrane potential of the post-synaptic neuron ( V mem ), for values of V Ca  X  2 . 8 V . Depending on the state of Figure 4: Stochas tic synaptic LTP transition: in both sub-figu res the non-plastic synapse is stim-ulated with Poisson distrib uted spikes at a rate of 250Hz, making the post-synaptic neuron fire at approximately 80Hz; and the plastic synapse is stimulated with Poisson distrib uted spike trains of 100Hz. (a) The updates in the synapt ic weight did not produce any LTP transition during the 250ms stimulus presentation. (b) The updates in the synaptic weight produced an LTP transition that re-mains consolidated.
 V mem , the signals V UP and V DN are activated or inacti vated. When not null, currents I UP and I DN are complementary in nature: only one of the two is equal to I B . To characterize the stochastic nature of the weight update process we stimulated the neuron X  s plastic synapses with Poisson distrib uted spike trains. When any irregular spike train is used as a pre-synaptic input, the synaptic weight voltage crosses the synap se bistability threshold in a stochastic manner , and the probability of crossing the threshold depends on the input X  s mean frequenc y. There-fore Long Term Potentiation (LTP) or Long Term Depression (LTD) occur stochastically even when the mean firing rates of the input and the output are always the same. In Fig. 4 we show two in-stances of a learning experiment in which the mean input firing rate (bottom row) was 100H z, and the mean output firing rate (top row) was 80Hz. Although these frequencies were the same for both experiments, LTP occurred only in one of the two cases (compare synaptic weight changes in middle row of both panels). In this experi ment we set the efficacy of the  X  X igh X  state of all plastic synapses to a relatively low value. In this way the neuron X  s mean output firing rate depends primarily on the teacher signal, irrespecti ve of the states of plastic synapses.
 One essenti al feature of this learning rule is the non-monotonicity of both the LTP/LTD probabilities as a function of the post-synaptic firing freque ncy  X  post [4]. Such a non-monotoni city is essential to slow down and eventually stop-learning when  X  post is very high or very low (indicating that the learned synaptic weights are already correctly classifying the input pattern). In Fig. 5 we show experimental results where we measured the LTP and LTD transitions of 60 synapses ov er 20 training sessions: for the LTD case (top row) we initialized the synapses to a high state (white pixel) and plotted a black pixel if its final state was low, at the end of the training session. The transitions (white to black) are random in nature and occur with a probability that first increases and then decreases with  X  post . An analogous experiment was done for the LTP transitions (bottom row), but with complemen tary settings (the initial state was set to a low value). In Fig. 5(b) we plot the LTD (top row) and LTP (bottom row) probab ilities measured for a single synapse. The shape of these curves can be modified by acting on the post-synaptic weight control module bias parameters such Figure 5: (a) LTD and LTP transitions of 60 synapses measured across 20 trials, for different values of post-synaptic frequenc y  X  post (top label on each panel). Each black pixel represents a low synaptic state, and white pixel a high one. On x-axis of each panel we plot the trial number (1 to 20) and y-axis shows the state of the synapses at the end of each trial. In the top row we show the LTD transitions that occur after initializing all the synapses to high state. In the bottom row we show the LTP transition that occur after initializing the synapses to lo w state. The transitions are stochastic and the LTP/LTD probabilities peak at different frequ encies before falling do wn at higher  X  post validating the stop-learning algorithm. No data was taken for the gray panels. (b) Transition probabilities measured for a single synaps e as a function  X  post . The transition probabili ties can be reduced by decreasing the value of I B . The probability peaks can also be modified by changing the biases that set I k 1  X  k 3 . (Fig. 2(b)) Figure 6: A typical training scenario with 2 random binary spatial patterns. High and low inputs are encoded with generate Poisson spike trains with mean frequencies of 30Hz and 2Hz respecti vely. Binary patterns are assigned to the C + or C  X  class arbitrarily . During trainin g patterns belonging to the C + class are combined with a T + (teacher) input spike train of with 250Hz mean firing rate. Similarly , patterns belonging to the C  X  class are combined with a T  X  spike train of 20Hz mean firing rate. New Poisson distributed spike trains are generated for each training iterations. In order to evaluate the chip X  s classification ability, we used spatial binary patterns of activity, ran-domly generated (see Fig. 6). The neuron X  s plastic synapses were stimulated with Poisson spike trains of either high (30Hz) or low (2Hz) mean firing rates. The high/lo w binary state of the input was chosen randomly , and the number of synapses used was 60. Each 60-input binary pattern was then randomly assigned to either a C + or a C  X  class.
 During training, spatial patterns belonging to the C + class are presented to the neuron in conjunction with a T + teacher signal ( i.e. a 250Hz Poisson spike train). Conversely patterns belonging to the C  X  class are combined with a T  X  teacher signal of 20Hz. The T + and T  X  spike trains are presented to the neuron X  s non-plastic synapses. Training sessio ns with C + and C  X  patterns are interlea ved in a random order , for 50 iterations. Each stimulus presentation lasted 500ms, with new Poisson distrib utions generated at each training session. After training, the neuron is tested to see if it can correctly distinguish between patterns belonging to the two classes C + and C  X  . The binary patterns used during training are presented to the neuron without the teacher signal, and the neuron X  s mean firing rate is measured. In Fig. 7(a) we plot the response s of two neurons labeled neuro n-A and neuron-B. Neuron-A was trained to produce a high output firing rate in response to patterns belonging to class C + , while neuro n-B was trained to respond to patterns belonging to class C  X  . As shown, a single threshold ( e.g. at 20Hz) is enough to classify the output in C + (high frequenc y) and C  X  (low frequenc y) class. Figure 7: Classification results, after training on 4 patterns. (a) Mean output frequencies of neurons trained to recognize class C + patterns (Neuron-A), and class C  X  patterns (Neuron-B). Patterns 1 , 2 belong to class C + , while patterns 3 , 4 belong to class C  X  . (b) Output frequenc y probability distrib u-tion, for all C + patterns (top) and C  X  patterns (bottom) computed ov er 20 independe nt experiments. Fig. 7(b) shows the probabil ity distrib ution of post-syna ptic frequencies (of neuron-A) ov er different classification experiments, each done with new sets of random spatial patterns.
 To quantify the chip X  s classification behavior statistically , we emplo yed a Recei ver Operating Char-acteristics (ROC) analysis [14]. Figure 8(a) shows the area under the ROC curve (AUC) plotted on y-axis for increasing number of patterns. An AU C magnitude of 1 represe nts 100% correct classi-fication while 0.5 represent s chance level. In Fig. 8(b) the storage capacity (p)  X  X xpressed as the number of patterns with AUC larger than 0.75 X  is plotted against the number of synapses N . The top and bottom traces show the theoretical predi ctions from [3], with (p  X  2 the stop learning condition, respecti vely. The performance of the VLSI system with 20, 40 and 60 synapses and the stop-learni ng condition lie within the two theoretical curves. We implemented in a neuromorphic VLSI device a recently proposed spike-driven synaptic plastic-ity model that can classify comple x patterns of spike trains [4]. We presented results from the VLSI chip that demonstrate the correct functionality of the spike-bas ed learning circuits, and performed classification experiments of rando m uncorrelated binary patterns, that confirm the theoretical pre-dictions. Additional experiments have demonstrated that the chip can be applied to the classification of correlated spatial patterns of mean firing rates and as well [15]. To our knowledge, the classifi-cation performance achie ved with this chip has not yet been reported for any other silicon system. These results show that the device tested can perform real-time classification of sequences of spikes, and is therefore an ideal computational block for adapti ve neuromorp hic sensory-motor systems and brain-machine interf aces.
 This work was supported by the Swiss National Science Foundation grant no. PP00A106556, the ETH grant no. TH02017404, and by the EU grants ALAVLSI (IST-2001-38099) and DAISY (FP6-2005-015803). Figure 8: (a). Area under ROC curve (AUC) measured by performing 50 classification experiments. (b) Storage capacity (number of patterns with AUC value  X  0.75) as a function of the number of plastic synapses used. The solid line represents the data obtained from chip, while top and bottom traces represent the theoret ical predictions with and without the stop learning condition.
