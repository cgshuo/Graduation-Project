 REGULAR PAPER Ankur M. Teredesai  X  Muhammad A. Ahmad  X  Juveria Kanodia  X  Roger S. Gaborski Abstract Generating captions or annotations automatically for still images is a challenging task. Traditionally, techniques involving higher-level (semantic) ob-ject detection and complex feature extraction have been employed for scene under-standing. On the basis of this understanding, corresponding text descriptions are generated for a given image. In this paper, we pose the auto-annotation problem as that of multi-relational association rule mining where the relations exist be-tween image-based features, and textual annotations. The central idea is to com-bine low-level image features such as color, orientation, intensity, etc. and cor-responding text annotations to generate association rules across multiple tables using multi-relational association mining. Subsequently, we use these association rules to auto-annotate test images.
 rithm to accomplish the association rule mining task effectively. The motivation for using multi-relational association rule mining for multimedia data mining is to exhibit the potential accorded by multiple descriptions for the same image (such as multiple people labeling the same image differently). Moreover, multi-relational association rule mining can also benefit the auto-annotation process by pruning the number of trivial associations that are generated if text and image features were combined in a single table through a join. In this paper, we discuss these issues and the results of our auto-annotation experiments on different test sets. Another con-tribution of this paper is highlighting a need to develop robust evaluation metrics for the image annotation task. We propose several applicable scoring techniques and then evaluate the performance of the different algorithms to study the utility of these techniques. A detailed analysis of the datasets used and the performance results are presented to conclude the paper.
 Keywords Image captioning  X  Multimedia data mining  X  Auto-annotation  X  Multi-relational association rule mining  X  FP-Growth  X  Multi-relational FP-Growth  X  Text-based image retrieval 1 Introduction It is said that a picture is worth a thousand words; but determining the likely words that constitute the correct description of the picture is considered to be a challeng-ing problem by the computer vision, text mining, and the multimedia data mining communities. Knowledge derived from these two domains, i.e., image and text data together is more descriptive compared to when each domain is considered in isolation from one another. On the basis of this fact, it is our conjecture that multi-relational associations [ 8 ] should capture more information from the com-bined metadata. Conventional approaches use metadata from individual image features or text domain annotations using a relational join and develop feature-based clusters. This paper describes a method of formulating this conjecture as a multi-relational hypothesis and tests the validity of integrated mining of combined multimedia data using multi-relational association rules.
 retrieval systems such as Viper [ 21 ] and MultiMediaMiner [ 27 ]tonameafew. The World Wide Web has emerged as the largest repository of image data in the world. Image retrieval based on keyword search from such large databases poses a significant challenge. The search results can be greatly improved if the images are already annotated. However, owing to the large number of images in these databases, the only viable means to annotate images is to automate this process, since manual annotation can be a tedious and expensive job.
 ing. There are two main approaches for association rule mining in images. The first one involves just mining images while the second one involves mining im-ages along with some textual data associated with the images. We apply the latter approach in this paper. We extract basic features such as color, orientation, and intensity from the images. Features that are more complex were specifically not preferred so as to study and analyze the performance of the multi-relational ap-proach with a minimum consideration for semantics of the image. The low-level features considered are color (number of pixels that are red, green, blue and yel-low), orientation (0  X  ,45  X  ,90  X  , and 135  X  edge orientations), and intensity. The image features are extracted based on the focus of attention theory initially pro-posed by Itti and Koch [ 16 ]. The selective attention model allows the system to concentrate on processing salient objects in the scene without the need to pro-cess the unimportant aspects. The attention model processes the input image in three parallel feature channels: intensity contrast, color, and orientation channels. The feature saliency maps topographically represent the saliency of objects in the scene based on respective features. For a detailed description of still-feature ex-traction from images, refer to our previous work [ 12 ].
 ers X  and  X  X akes and mountains X  or their combinations. This allows us to exploit the correlation between low-level features in an image and high-level semantic content without object identification in the image. It is our hypothesis that within categories of images, enough similarities exist to allow the discovery of multi-relational associations, which can be used for auto-annotation of images. Another motivation for using low-level features is the need to maximize system through-put while minimizing the overhead cost of storing high-level features. However, the application can be scaled to include high-level features such as shapes and objects by designing appropriate tables to hold them. Note that the application of this framework to specific domains implies that it should not be used for an open-ended domain such as  X  X ature X  (which may comprise of landscapes, flora and fauna, fruits and vegetables, underwater images, etc.) with these images are annotations or captions that are derived from multiple sources. Although annotations can be stored within the same database in differ-ent tables or combined together into a single table, the upshot of the latter ap-annotations for the same image depending upon the users (or the top-k choices for keywords of an auto-annotating system doing the annotation). Hence, there is a one-to-many relationship from the image domain to the text domain. Doing a join can be prohibitively expensive if M is large in the 1: M relation between the tables. Moreover, such a join would be unnecessary if the tuples in the table do not qualify as frequent patterns. The relation between annotations in multiple tables and image features can however, be captured by multi-relational association rules thereby getting formulated as a multi-relational mining problem. Because this is the motivation behind our project, we termed the framework CoMMA: Combined Multi-relational Multimedia mining using Associations. Additionally we propose several evaluation metrics for accessing the quality of annotations returned for an image. This has been a largely ignored arena in image mining and we hope that our proposal will be helpful to the image mining community.
 other related work is described in the next section. Section 3 describes the types of features extracted and the motivation behind extracting these kinds of features in CoMMA. We propose an extension to the FP-Growth algorithm called MRFP-Growth for multi-relational mining in Sect. 4 . In Sect. 5 we discuss the inadequacy of currently used evaluation metrics and propose several evaluation metrics that can be used for evaluating auto-annotations. Section 6 discusses experiments and their corresponding results. Section 7 outlines some of the issues that we would like to encourage the multimedia data mining community to discuss. 2 Related work The problem of auto-annotation is usually treated as a supervised learning prob-lem where higher-level features are extracted from images and complex object detection algorithms are employed to generate keywords. These usually involve image segmentation and object labeling, which are nontrivial tasks. Several clus-tering and classification techniques have been employed for auto-annotation of images [ 3 , 5 , 6 , 18 , 25 ]. The  X  X lob X  approach requires the images to be in a state where object recognition is possible [ 5 , 18 ]. Hsu et al. employed the idea of view-points, which refer to the notion of invariant relationships between objects in an image [ 7 ]. Object identification in images is usually expensive and thus increases the cost of auto-annotation [ 5 , 15 , 23 ].
 schemes have been proposed for faster and better image retrieval. The subfield of Content-Based Image Retrieval (CBIR) employs global features of images such as color histograms. CBIR was employed in IBM X  X  Query by Image Content (QBIC) [ 11 ] and also in region-based approaches involving  X  X lobs X  [ 5 , 18 ]. Fast and Semantics-Tailored Image Retrieval Methodology (FAST) [ 28 ] uses fuzzy logic to create a new indexing method Hierarchical Elimination-based A* Re-trieval (HEAR) to handle the region-based image information consisting of colors, texture and shape. Relevance Feedback (RF) analysis is another effective solution for CBIR. Semi-Automatic Image Annotation [ 26 ] depends on user feedback and adds successful image search keywords as annotations to images. Zhong et al. introduce PCA [ 24 ] to reduce the noise in original images and the dimensional-ity of the feature spaces. Authors of MultiMediaMiner [ 27 ] mention the usage of association rule mining to get rules based on colors of CT scan as an interesting application. Monay et al. compare [ 20 ] simple Latent Space Models for the task of annotations. Recently a KNN-based subspace clustering method was proposed for image auto-annotation by [ 25 ]. Image annotation has been mostly studied from a statistical as well as supervised learning perspective [ 19 ]. The multi-relational association rule approach is somewhat similar to the statistical approach in that the support and confidence of rules across the image and text domain are related to the statistical distribution of features, while relations capture more information than just simple statistical links. Two related subtasks are involved in recognition of images, i.e., auto-annotation of images that involves recognizing whole images and object recognition, which involves recognizing objects in the images. In this paper, we address the former task using minimal features from the image domain. We would also like to note that the problem that we are addressing is not only that of finding a particular suitable annotation for an image but also that of finding a set of keywords that can be used as query-set by a human for retrieval using a search engine. 3 CoMMA feature extraction CoMMA was developed as a general framework for employing multi-relational association rules for auto-annotation of images in specialized domains. In Sect. 4, we describe a multi-relational version of the FP-Growth algorithm, which forms the core of the current application. In CoMMA, a user may optionally upload an annotated image. Such an image is used for generating possibly new rules indicat-ing that the system has learnt something new (training mode). Otherwise, rules are not generated but are used to annotate the given image (test mode). Figure 1 gives an overview of how CoMMA works. Starting from the User Interface, images are uploaded into the image database along with the corresponding annotations. Low-level features are extracted from the image while the text features mainly consist of terms from the annotation or caption. Multi-relational association rules are gen-erated by applying the MRFP-Growth algorithm. Finally, these rules are used to annotate test images and the performance is compared with the original ground truth. When an annotation is not provided for an image, the image features are extracted and annotations are obtained from the previously generated association rules as shown by the dotted line in Fig. 1 .
 on Itti and Koch X  X  focus of attention theory [ 16 ]. Given a scene, humans selectively attend to important salient regions of the scene. For example while driving on a road, the red and yellow street signs stand out in the scene. The focus of attention algorithm used in this system, processes an image to extract the color, orientation, and intensity features. The 2D spatial filters [ 13 ] used in the system are modeled after the findings of the biological vision principles simulating the functioning of the retina, lateral geniculate nucleus and the early visual cortical areas. The spatial filters are convolved with the input image to obtain the topographical feature maps. Intensity contrast is extracted using difference of Gaussian filters. The intensity contrast filtering simulates the function of the retinal ganglion cells which posses the center-surround mechanism. The color information is extracted using the color opponent filters. The orientation processing employs Gabor orientation filters to extracts edges of (0  X  ,45  X  ,90  X  , and 135  X  ) orientations. The sine and cosine Gabor filters oriented in the spatial axis is modeled based on the receptive field properties of the orientation tuned simple cells of the early visual cortex. Due to the center surround antagonistic nature of the feature extraction filters, the topographical maps obtained are called the saliency maps [ 22 ].
 (0  X  ,45  X  ,90  X  , and 135  X  ) and intensity contrast maps are feature saliency maps. Gaborski et al. experimented [ 12 ] with feature saliency to infer the importance of each feature based on different scene type. Their approach collected human eye-tracks for images of natural landscape scenes, indoor scene, building/city scenes, and fractal images. The eye-tracks collected for the four scene types were used to find the feature that dominated the subject X  X  attention. On the basis of the corre-lation studies on multiple images, intensity contrast gave the highest correlation for natural scenes and building/city scenes. Color gave the highest correlation for indoor and fractal scenes. On the basis of these observations a test image can be classified as being one of the four scene types. These studies demonstrate the effi-ciency of low-level features in classifying scenes and motivated us to use low-level features for generating rules for image annotations.
 nine features described in Sect. 1. The vector space consists of all image features. The image features were further discretized, so that the final image feature vocab-ulary consisted of more than 2,700 feature terms. In the image mining domain, the modeling of image annotations has usually been done by the concatenation of image feature vectors and a feature vector of words [ 7 ]. If there are one-to-many relationships between the image feature vector and the term vectors, the same technique (concatenation) can still be used but the cases where terms in a descrip-tion are further related to other terms, as is the case in the current problem, cannot be handled without loss of information. This was the main motivation for keeping the images and the annotations/captions/descriptions in separate tables. human expert for annotating images, then clustering could also have worked well. However, the problem with employing clustering for the present task is that indi-vidual clusters usually have a much larger data spread as compared to association rules which are comparatively straight forward. This was the reason that clustering was not employed in this application. We do explore it as a baseline comparison with our approach. 4 Multi-relational FP-Growth 4.1 Association rule mining Consider a database D that contains a set of transactions T .If X and Y are items in T then an association rule is an implication of the form X  X  Y and has a support s in the database D if s % of the transactions in D contains both X and Y [ 2 ]. Similarly the rule X  X  Y has a confidence c if c % of the transactions in D that support X also support Y . The task of association rule mining is to thus generate a set of rules that have a minimum support and minimum confidence above certain user specified thresholds. Association rules over multiple tables can be described similarly. Association rules over multiple tables X and Y are atom sets of the form p ( The confidence is defined as follows: if c %oftransactionin D are covered by X then they are also covered by X  X  Y . Similarly the support is defined as follows: S is the support of rule if s % of all transactions in D are covered by X  X  Y . the one which is most widely used is the Apriori algorithm [ 1 ]. The algorithm exploits the anti-monotone property and states that for a k -itemset to be frequent all ( k  X  1) subsets of this itemset also have to be frequent. Though the algorithm reduces the computational cost of generating the itemsets, the computational cost is still high when the number of 1-frequent itemsets is sufficiently high, which in turn translates into a high cost for generating 2-frequent itemsets. The FP-Growth algorithm [ 14 ] was proposed to overcome this problem. The algorithm creates a compact tree-structure called the FP-tree that represents frequent patterns and mines the FP-tree to get the frequent patterns. It solves the multi-scan problem and improves itemset generation. 4.2 Multi-relational FP-tree As described in the previous sections we are posing the auto-annotation prob-lem as a multi-relational association rule mining problem. Again there are a few choices available for choosing a multi-relational association rule mining algo-rithm. WARMR [ 4 ] is perhaps the most well-known algorithm for multi-relational association rule mining and is based on the Apriori algorithm. There is also a distributed version of the Apriori algorithm called decentralized Apriori [ 17 ]. based on it. Building an FP-tree [ 14 ] requires scanning the database twice. The first scan generates all the 1-frequent itemsets. FP-tree generation can be broken down into two phases. In the first phase, the items appearing in the dataset are enumerated. All the items that have a support less than the threshold are weeded out. The remaining itemsets are organized in a table called the header table and are sorted by frequency. Pointers to the first occurrence of the itemset in the dataset are also maintained in order to maintain the reference for all other occurrences of the item. The second phase begins with another I/O scan of the database. Each transaction is read again and only those itemsets that occur in the header table are maintained. These are then sorted in the descending order based on their frequencies.
 check if it exists as one of the children of the root. If it does exist then increment its support. If not then make the current item a child of the root node and set the support to 1. Repeat the same process by considering the current node as the root. Whenever an item is added to the FP-tree a link is maintained between the node and its occurrence in the header table. The process continues until the entire FP-tree is generated. The complete FP-tree for the sample relation in Table 1 is given in Fig. 2 .
 rithm. It can be divided into two phases. Consider a database D that consists of k tables with one primary table and k  X  1 secondary tables. The problem can then be stated as that of finding relations from the primary table to each of the secondary tables and then between the secondary tables. The following are explications for some commonly used terms for elucidation of the MRFP-Growth algorithm.
 4.2.1 Explanatory terms 1. MRFP-tree: A frequent pattern tree which holds the support count and the Id 2. Primary table: A table that has a primary key. Example: Table 1 with TID as 3. Secondary table: A table that has field(s) which refer to the primary key of the 4. Id: The set of primary key values whose records contain an item. Example: Id 5. Id set: The set of Id (see Definition 4) which contains a frequent pattern. Let A junction with secondary Tables 2 and 3. Itemsets in Table 1 are in a one-to-many relationship with itemsets in Tables 2 and 3. Running the FP-tree algorithm on these tables yields additional FP trees as given in Figs. 3 and 4 .
 for that item and the numbers inside the curly braces represent rules generated from each of these trees. It then goes into the table Frequent Patterns (Table 3 ), employed to construct the final MRFP-tree (Fig. 5 ) across different tables and get rules (Table 4 ) from it.
 brackets, the associated Id. Please note that in Fig. 5 the number in boldface in the tree node is the frequent item and the number next to it (not boldface) is its support count. Rules obtained from the above tree (Fig. 5 ) are listed in Table 4 . taken to get the patterns to generate the final rules, as given in Table 5 . A possible performance drawback would be the time required to make combinations for each of the frequent patterns in the last phase. In this phase, the frequent patterns tend to get long depending upon the number of relations. Since rule generation depends upon the aforementioned Id set, generating combinations of items with in frequent patterns can be skipped by using the id set associated with the frequent pattern. Thus in the current example one can have 1, 2, 3 as a frequent pattern with 1, 5, 6, 7 as the Id set and still get the rules in Table 5 instead of making combinations of this pattern as given in rules 1, 2, 3 in Table 4 . Please note that the Id sets in this particular example are the same for the rules 1, 2, and 3 in Table 4 ,however,they can be different. 4.2.2 Algorithm description The multi-relational FP-Growth algorithm can be described as follows:  X  Input: One primary table and k  X  1 secondary tables.  X  Output: Multi-relational frequent patterns accross tables.
 Phase 1: For each secondary table do: 1. Generate an FP-tree for the items in the table. Keep track of the Ids for each 2. Mine the FP-tree for frequent patterns. Also, note the Id set of the frequent Phase 2: 1. Make an FP-tree for the Ids in the Id set of all the frequent patterns. 2. Mine this FP-tree for frequent patterns of Ids. This will generate frequent pat-3. Map the frequent patterns for Id sets to the frequent patterns from the sec-rately on all the tables but with one major difference. Each node in the tree not only keeps track of its support but also keeps track of the indices (Ids) in the dataset where it occurs. This results in k  X  1 FP-trees for the secondary tables. Mine these for frequent patterns.
 where the items occur in their respective tables, on the Id set. Each id in an Id set is considered to be an item. Once the tree is made, it is mined for rules as specified earlier. However the generated rules are not saved this time. Instead, the Id set corresponding to the rules are used to get frequent patterns that were placed in the table made in phase 1. The rules across different tables are generated thence. 5 Performance evaluation Although there is a whole corpus of literature available on the image auto-annotation problem, the research community has largely ignored a very important issue related to it, namely developing more effective metrics for quantitatively evaluating the quality of image annotations. More often than not, ad hoc evalua-tions are proposed. The main reason why a set of universally accepted evaluation metrics do not exist for evaluating the quality of image annotations is not known, although a set of similar metrics are used which are usually derived from ba-sic annotation accuracy formulae. In this section we will outline how the various evaluation metrics perform and question if using one over the other for image an-notation is even appropriate. In tradition Information Retrieval settings precision and recall are two popularly used metrics to measure the accuracy of any retrieval algorithm. Since we are also concerned with the distribution of keywords and the relative frequency of keywords in original annotations, one needs to effectively modify precision and recall to be suitable metrics for the auto-annotation setting. We begin this section by describing the raw score which is recall. We subsequently modify recall based on the shortcomings of precision and recall, and demonstrate the results of the metrics that we propose to compare the performance of various algorithms with MRFP. 5.1 Quantifying annotation quality The simplest metric for computing annotation accuracy calculates the annotation performance as the percentage of the terms that were correctly predicted. The metric raw annotation accuracy, can be defined as follows: 5.1.1 Raw annotation accuracy Let n be the number of keywords in the original annotation associated with an image. Let t be the number of keywords returned by the algorithm and let r be the correctly predicted keywords, where r  X  n and n  X  1. The raw annotation score is defined as the ratio of r over n and is also known as recall.
 take into account the number of annotations associated with the image and the annotations returned by an annotation system. Hence a system that returns a large portion of the annotation vocabulary can in fact get a high score for this metric, in fact a system that returns all the annotations in the annotation vocabulary as  X  X or-rect X  annotations for all the images will get a perfect score for the whole dataset over t to take into account the number of keywords returned by the algorithm. The metrices desribed below modify the raw score (recall) to accomodate precision. 5.1.2 Normalized score Bernard et al. describe a metric, the normalized score, for evaluating image an-notations which considers a solution to the above problem [ 3 ]. Normalized score takes into account both the correct and the incorrect annotations returned for an image. The range of possible scores is from  X  1 to 1. Hence for a given image if a system returns all the correct annotations the score is positive one. On the other hand, if it returns the whole vocabulary except the correct annotations as the anno-tation set then the score is negative one. The score is zero if the image is annotated with all the keywords in the annotation set or if nothing is returned. Normalized score is given as follows: where N is the vocabulary size, n the number of correct annotations for the image, r the number of correctly predicted words, and w the number of words predicted incorrectly. 5.1.3 Modified normalized score Normalized score can be modified to take into account the presence of multiple annotation tables as in our approach.
 where all the terms designate the same quantities as in the previous formula, while k denotes the total number of annotation tables. The problem with this metric is that the evaluation score goes down if there are a number of annotation tables with somewhat disparate annotations for the same images even though the quality of the system generated annotations remains the same, as evaluated by a human expert. A more suitable setting for the use of this metric would be when annotations for an image are available in several languages in different tables. 5.1.4 Normalized threshold score Both of these formulae are susceptible to be reduced to 1 given that N is suf-ficiently large. Hence for a database most of the annotations in the vocabulary occur only a few times, these metrics 2 and 3 will give scores similar to 1 . Hence, we propose that the metric 2 should be modified such that instead of N , i.e., the size of the vocabulary, only a subset of the vocabulary say N be considered. The modification can be described as follows: and where t min is the threshold specified by the user. If the number of annotations that are returned are equal to or greater than N then N should be taken to be N = n + w . 5.1.5 Normalized relative distribution score However, this still leaves the problem of the system  X  X uessing X  a keyword as an annotation just because of its dense statistical distribution within the data. In order to accommodate this difficulty we propose that the amount to which the system is penalized for guessing a wrong annotation should either depend upon the relative distribution of the word in the vocabulary or on a user specified threshold. Hence, for the first scenario we can use the following metric.
 where all the terms designate the same quantities as in the previous equation and k (dist) is the relative distribution of the annotation. Again if the number of anno-tations that are returned are equal to or greater than N then N should be taken to be N = n + w . 5.1.6 Normalized threshold distribution score And for the second scenario we can use the following metric: where If the number of annotations that are returned are equal to or greater than N then N should be taken to be N = n + w . 5.2 Manual scoring In order to corroborate the results given by the metrics given above and also to make sure that the score given by the metrics make sense, the annotations can be scored by randomly selected some images and performing a sanity check by the domain expert. The following metric can be used to measure the manual annota-tion score.
 where all the quantities are the same as in Eq. ( 4 )and r is the number of correct annotations. Note that in this case r  X  n which implies that E model MST can be greater than one.
 ing annotations that are although relevant to the image but were not present in the original annotation. Another advantage is that it does penalize the system for giv-ing wrong annotations for an image. The main disadvantage being that the process requires a lot of time and effort given that image databases are huge. Sampling the database and scoring a few images is a good way to corroborate the results by comparing them to other metrics as we describe in the next section. 6 Experiments and results 6.1 Data organization Although research in auto-annotation of images has been conducted for several decades, standardized datasets have not come into existence. The dataset that was finally employed consisted of images from different sources like Corel Profes-sional Photo CDs; University of California Berkeley floral images; 1 University of Washington ground truth dataset, 2 snapshot, 3 Freefoto, 4 United States Fish and Wildlife Services National Image Library. 5 Multiple sources were used instead of concentrating on a singular source to ensure that the results are not biased due to a singular source. After further sorting, the resultant dataset used for training consist of 1,482 images. The test set consists of 50 images. We generated five dif-ferent test and training sets from a total of 1,532 images in order to do 5-fold cross validation of the results against comparative techniques. Different human experts were asked to annotate all the 1,532 images.
 test dataset consisting of 159 images which is different from the above five test. We first study the effect of varying support and confidence for MRFP-Growth. Later, we compare this performance with competing algorithms including KNN, decentralized Apriori and always K best. The details for algorithms are described in Sect. 6.4 .
 that approximately 20 keywords account for about a half of the probability mass function.
 age table (not shown) is the primary table (see Definition 4.2.1) which is just the image-ids (primary key) and absolute paths to the location of each image on the disk. Table 7 , Annotation English, is a foreign table (see Definition 4.2.1) and has attributes image-id (foreign key) and annotation. The feature table (Table 8 )is another foreign table that holds image features and image-ids (foreign key). image feature was uniquely tagged to facilitate intra-feature distinction in gener-ating rules. Table 9 gives a list of the tags that were used and their meaning. below: in the next subsection. 6.2 MRFP evaluation The average evaluation scores training set for evaluation metrics described in Sect. 5 are given in Figs. 7  X  11 . As expected the graphs obtained for metrics ( 1 )and ( 2 ) are very similar to one another, owing largely due to the relatively low num-ber of annotations returned for the images. Also evident from these five graphs (Figs. 7  X  11 ) is that the overall trend in scoring is such that the scores go down as the support count and the confidence increases as was expected.
 and each might have its own particular advantages or disadvantages in varying contexts consider Fig. 12 . Here the scores for the aforementioned five evaluation metrics for individual images in a single test run are given. The images are first sorted in descending order with respect to the scores for raw annotation accuracy metric ( 1 ). The corresponding scores for the same images for other evaluation met-rics are given without sorting them to give a fair comparison of the metrics. Again the evaluation scores for metrics ( 1 ), ( 2 )and( 5 ) are close to one another, which illuminates the problem of deducting a relatively low score from the total score for generating wrong annotating in case of ( 2 ). In case of metric ( 5 )itshowsthat the relatively low spread of at least the words within the test set, if not the whole dataset. The similarity between metrics ( 4 )and( 6 ) is again evident, however the point to note here is that the scores for these two metrics do not seem to follow a well-defined variation with respect to the other three metrics. This is the case because these two metrics are more sensitive to wrong keywords as annotations, especially when the keywords occur more frequently than others.
 the confidence increases. It is evident from the graph that the execution time goes down almost exponentially as the confidence increases. This is due to the fact that as the confidence increases the number of candidates that have to be considered becomes less and less. The same applies for support, as the support increases there is a drastic decrease in execution time for similar reasons.
 6.3 Memory management in MRFP Memory issues can arise during the construction of MRFP-trees if the Id for each item, i.e., the primary key consists of a very long string. A possible solution to multi-relational rules, relevant information such as intermediate rules, Id sets (see Definition), and support counts are put in the MRFP-tree rule table such as the one given by Table 6 . Apart from this, the length of Id sets generated also needs adequate attention and the sets may need splitting if they get too large to accom-modate within the limits imposed by commercial database management systems on a single column size. 6.4 Comparative algorithms 6.4.1 KNN-based algorithms K nearest neighbor or KNN is a simple classification algorithm. Given a dataset of n data points, the distance between all data points is first determined and then K nearest neighbors are assigned to the same class. Euclidean distance is mostly used as the distance metric of choice in KNN. The numerical data in our case was the feature vector of each image. The distance between any two images was determined on the basis of the distance between the features in n dimensions, where n is the number of features. Hence for this particular dataset n = 9. 6.4.2 Always K best strategy Each image has an average of 3.18 annotations in the current dataset. If we also take into account the fact that some annotations within a dataset can have relatively high frequencies then it is difficult to establish how much of the performance gain is due to the algorithm and how much of it is due to the bias within the dataset. Hence, to get a rough estimate of skewness within the data we calculated the evaluation scores based on the metrics described in the previous section using the always K best strategy, i.e., annotate each image with the k most frequent keywords and evaluate the score.
 fact that other than a top few keywords, the distribution of the most keywords is fairly uniform. Always K best strategy demonstrates that a system should be  X  X ewarded X  differently for different annotations returned. In the previous section we suggested metrics 6 and 5 with such a situation in mind.
 6.4.3 Decentralized Apriori The decentralized Apriori algorithm is an extension of the Apriori algorithm [ 2 ] for mining multiple relations. Decentralized Apriori was chosen for comparison in order to evaluate the performance of another association rule mining algorithm for image annotation against the MRFP-tree algorithm. Decentralized Apriori in-volves finding frequent itemsets on individual tables and merging results using for-eign key relationships. The algorithm can be broken down into two steps: The first step involves running the Apriori algorithm on the tables separately. The counts for the occurrence of each itemset is maintained in a vector. In phase 2, itemsets across the tables are counted using the relationship table. The support of an item-set is incremented if it occurs in both the tables. Hence the joins of the two tables are computed without materialization. The result is an n -dimensional array that contains the support for all the candidate itemsets. Just like MRFP, decentralized Apriori can also find rules across multiple tables. 6.4.4 Evaluating performance The comparative performance of the above algorithms with respect to execution time and dataset size variation is described in Fig. 14 . As can be observed, KNN-based traditional annotation strategies take significantly longer time compared to MRFP-Growth and decentralized Apriori algorithms. Figure 15 describes the ex-ecution time comparison between MRFP-Growth and decentralized Apriori algo-rithms. MRFP-Growth consistently outperformed decentralized Apriori approach for multi-relational rule generation. 6.4.5 Evaluating annotation quality In Sect. 5 we described several evaluation metrics to measure the quality of an-notations generated by different algorithms. The generally accepted norm is the raw evaluation metric. As shown in Fig. 16 all algorithms obtained a positive raw score which is one of the drawbacks of raw score. Decentralized Apriori generated a large number of rules and consequently a large number of keywords were assigned to each image thereby signifying a high raw score. This figure also validates why raw score is not an effective measure to quantify the quality of annotations.
 Fig. 17 since it takes into account the number of keywords generated per image. For the five test datasets MRFP-Growth scored positively as compared to other algorithms. This again is not a very good metric because when the vocabulary size is very large, the normalized score is reducible to raw score.
 The reason that decentralized Apriori consistently gets a negative score because the number of wrong keywords that are generated are greater than the subset of the vocabulary size under consideration. The metric assigns negative one whenever such a situation arises since having a huge number of keywords for annotations opens up the possibility that the algorithm might get some of the annotations right just based on the distribution of these keywords. This is was the main motiva-tion behind this metric since it penalizes an algorithm more for generating wrong algorithms.
 the results for this metric are surprisingly similar to normalized score. This is so because of the although there is a limit to the vocabulary size under consideration as is the case in the previous metric but at the same time the algorithm is penalized differently based on the distribution of the keywords. Hence the results are slightly or slightly worse in some cases as compared to normalized score.
 count the distribution of the keywords within the dataset, this key advantage can actually go against it as seen in the previous graph. Hence in order to remedy this problem we modified the metric to device the normalized threshold distribution score the results of which are given in Fig. 20 . The main difference is that the amount to which the algorithm is awarded a score varies based upon two discrete values, 0.5 and 1 in this case, instead of being based on the distribution of these keywords. Again MRFP-Growth performs better in general as compared to the other algorithms.
 the original annotation. One might question whether this is appropriate? It is dif-ficult to arrive at a close form solution to the auto-annotation problem. As the raw annotation metric indicates most algorithms tend to make up for their loss of preci-sion by increasing the recall and vice versa. Though we do not have a solution for this problem, we have tried to highlight this issue by proposing a variety of other metrics to understand the effect of generating more keywords than present in the original annotation. Moreover, if viewed from a clustering viewpoint the original annotation is often insufficent to adequately describe the various concepts (clus-ters) that an image might represent due to the subjective natures of both clustering techniques and human annotations. 7 Conclusion and future work In this paper, we provide a framework for a multi-relational approach for inte-grated mining of multimedia databases. The problem domain for which we de-scribe a solution is auto-annotation of images. The performance of auto-annotation via multi-relational mining was effective as witnessed by the test results. In a con-strained setting the system was able to annotate the test images successfully in most cases. A dataset that consists of multiple descriptions of the same image would be an ideal candidate for further exploration of multi-relational mining for image annotation. We are currently extending the framework using the techniques that we described above for not only image-annotation but also for retrieval of images from an image database based on keyword-based queries. Future research will concentrate on query expansion given these varied parameters and finding the optimal value for annotation within a limited image category. The annotations returned from the images can be treated and used for query expansion and disam-biguation of terms related to an image.
 description of the given scene. Hence, given a set of images, another dataset that contains multiple annotations for an image from varied sources and a dataset that consists of facts related to individual terms, the challenge will be to generate a description for a particular image. The performance of this method on a larger dataset and a larger vocabulary is an open question that also needs to be addressed in the future. We believe that future advances in computer vision and data man-agement domains would benefit from adopting some of the suggestions provided to solve problems such as query expansion for image data, content-based image retrieval, auto-categorization of images, etc that are perched at their intersection. will not only be helpful to the text and multimedia mining community but will also simulate development of newer solutions in this area. The results given in the previous section demonstrate the inadequacy of traditional evaluation metrics since a system is bound to get some of the annotations right just on the basis of distribution of the keywords within the dataset would be even more appropriate for the evaluation task.
 References Author Biographies
