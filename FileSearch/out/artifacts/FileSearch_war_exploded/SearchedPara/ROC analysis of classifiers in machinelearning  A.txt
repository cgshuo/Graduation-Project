 Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia 1. Introduction
Receiver operating characteristics (ROC) analysis is a methodology for evaluating, comparing and selecting classifiers on the basis of their predicting performance. First known application of ROC anal-ysis took place during Second World War when it was employed for the processing of radar signals. Later its use began in the signal detection theory for illustrating the compromise between hit rates and false alarm rates of classifiers [18,34]. Other fields to which ROC analysis has been introduced include psychophysics [34], medicine (various medical imaging techniques for diagnostic purposes, including computed tomography, mammography, chest x-rays [53] and magnetic resonance imaging [45], and also diverse methods in epidemiology [38]) and social sciences. An extensive list of ROC analysis sources to support decision making in medicine has been published, consisting of over 350 entries divided into several Sections [64].

For over a decade, ROC analysis is gaining popularity more intensely also in the field of machine learning. First applications date back to late 1980 X  X  when ROC curves were demonstrated to be appli-cable to the rating of algorithms [50]. In the present, these curves already represent one of the standard metrics for assessing machine learning algorithms. A detailed introduction to the use of ROC analysis in research (with stress on machine learning) may be found in [21].

ROC analysis is already used in various applications and research in the field is lively developing in many directions: the two-class ROC methodology has been generalized to handle multi-class problems, extensions of basic ROC curves and the AUC metric have been proposed, advanced alternatives to basic ROC graphs appeared etc. Since the topic has been studied by many researchers from distinct points of view, questions came up which led to several polemics. The purpose of this paper is to present a survey on these issues in the context of machine learning which could guide a reader to the literature with more exhaustive explanations.

In this paper, the terms  X  X OC curve X ,  X  X OC graph X  and  X  X OC analysis X  are sometimes used inter-changeably, though the  X  X OC analysis X  is the most general, depicting the whole field of study, while the  X  X OC curve X  denotes a curve on an  X  X OC graph X  (chart). The paper is structured as follows. In Section 2, the basic features of ROC analysis are discussed. Section 3 shortly describes procedures of ROC curve construction for some well-known classifiers and Section 4 summarizes various applications of the ROC analysis for distinct purposes. Further, Section 5 presents some problems and beneficial im-provements of the basic ROC methodology, Section 6 sheds light on alternative visualizations to ROC graphs, and Section 7 concludes the paper. 2. Basic definitions
ROC analysis in its original form is used to deal with two-class classification problems. In the follow-ing subsections, main tools of this methodology are presented. At the end of the section an explanatory example is provided. 2.1. Contingency table
Suppose a set of instances with known classes is given, each of them belonging either to the positive or the negative class. The terms positive and negative originally stem out from early medical applications, where the instances describing the patients with some present observed medical phenomenon (e.g. an illness) were denoted as positive , and the rest of the patients as negative . After the learning phase, a classifier should be able to predict a class value of some new, unseen instances.

Since predicted classes of given instances are not necessarily same as true classes, a matrix is used to keep a record of the number of prediction errors. This matrix is called a contingency table or a confusion matrix (since it represents the confusion between classes) and is shown in Fig. 1. There are four possible outputs for a classification of each instance, as follows. If the instance is positive and is classified as such then we denote it as true positive (TP). If a classifier made a mistake and classified the instance as negative, we call it false negative (FN). Similarly, if the instance is negative and was also classified as negative we denote it as true negative (TN) and in the case of a misclassification we call it false positive (FP). The number of correct classifier decisions thus lies on the main diagonal of a contingency table, while other table elements represent a number of misclassifications.

A contingency table is a source for calculating further knowledge evaluation measures, including the true positive rate (TPR) , false positive rate (FPR) , true negative rate (TNR) and false negative rate(FNR) . They are defined as TPR = TP P , FPR = FP N , TNR = TN N and FNR = FN P , respectively, where P = TP + FN and N = FP + TN . As we may observe in Fig. 1, P is the number of all instances which are actually positive and N the number of all instances which are actually negative. In some fields of study, TPR is also called sensitivity or recall as well as the term specificity denotes TNR. 2.2. ROC graphs and ROC curves An ROC graph for original two-class problems is defined as a two-dimensional plot which represents TPR (sensitivity) on y-axis in dependence of FPR ( = 1-specificity) on x-axis. Performance of a particular classifier, represented by its sensitivity and specificity, is denoted as a single point on an ROC graph. There are some basic characteristic points on a graph of this type. The point with coordinates (0,0) (TPR = 0, FPR = 0) represents a classifier which never predicts a positive class. While such a classifier would never misclassify a negative instance as positive, it is usually not a good choice, since it would never make a single correct classification of a positive instance neither. Its relative in the point (1,1) represents the opposite situation (TPR = 1, FPR = 1) as it classifies all instances as positive, thus also producing a possibly high number of false positives. The classifiers in (0,0) and (1,1) are called default classifiers . In (0,1) the perfect classifier is located (TPR = 1, FPR = 0). While it is not realistic to expect such performance from any classifier on a real-world problem it represents a goal at which the induction of classifiers should aim. Classifiers which are located on the ascending diagonal of an ROC graph have the same performance as random guessing. For such classifiers we say, that they have no information about the problem. Useful classifiers are located above the ascending diagonal. Those under it are performing worse than random guessing. Nevertheless, they can be made useful very easily by inverting their predictions. Such classifiers are said to have useful information but are employing it in a wrong way [29].

An ROC curve is a curve on an ROC graph with start point in (0,0) and end point in (1,1). Drawing procedure for this curve depends on the type of classifiers we want to evaluate. In view of the amount of returned information, classifiers may roughly be divided into three groups: discrete (predicting a class membership), scoring (predicting a class score) and probability estimating (predicting a class probabil-ity). The score is defined as posterior probability (not necessarily calibrated) of the positive class. We should note, that a class score offers more information than a class membership. Similarly, the amount of information contained in a class probability is higher than in a class score. Main reason for the use of scores is that good probability estimates are not always available, for example, in a case of small amount of learning data. The meaning of scores may be interpreted as follows: if a classifier returns scores for two instances where the score of the first instance is greater than the score of the second, this indicates that the first instance has a higher probability as well. A disadvantage, however, is that scores from dif-ferent classifiers cannot be compared to each other in contrast to predicted probabilities which have a common interpretation. Procedures of drawing ROC curves for classifiers of above-mentioned types are discussed in Sections 2.2.1 and 2.2.2. The construction of ROC curves for some concrete classifiers is further described in Section 3.

We should have in mind the importa nt property of ROC curves  X  they measure the capability of clas-sifiers to output good scores [21]. Analyzed classifiers thus do not have to produce exact probabilities, all they have to do is discriminate positive instances from negative ones. Another useful feature of ROC curves is that they remain unchanged when altering class distribution. Class distribution is the proportion of positive instances (left column in Fig. 1) to negative instances (right column in Fig. 1). An ROC curve is based on TPR and FPR values and since TPR and FPR are each calculated from values of one col-umn, ROC curves are consequently independent of class distribution. The fact that ROC curves take into consideration sensitivity (i.e. TPR) and specificity (i.e. TNR = 1  X  FPR) also represents an advantage of these curves over simpler evaluation measures, such as classification accuracy.

An example of an ROC graph with four different ROC curves each representing one classifier is given in Fig. 2. Classifier A is by far better than the other three classifiers. ROC curves of classifiers B and C cross  X  each of these two is superior to the other for some deployment contexts (i.e. combinations of class distribution and misclassification costs). Classifier D is of no use as its performance is no better than chance. 2.2.1. ROC curve construction for scoring and probability estimating classifiers
To construct an ROC curve of a scoring classifier we first have to sort instances according to their scores. We then draw the first point at (0,0) and select the instance having the highest score. We check whether the instance X  X  true class is positive or negative  X  if it is positive, we move one unit up on the graph, if negative, one unit to the right. Horizontal and vertical unit sizes are inversely proportional to the number of negative and positive instances in the data set, respectively. We repeat this step by taking successive instances in a decreasing order and moving correspondingly over an ROC graph. The process terminates as the upper right corner in (1,1) is reached. All points, obtained by the process, are finally connected to form an ROC curve. The process may also be interpreted as applying different values of a threshold on scores. By varying this threshold one may obtain different (FPR,TPR) points, what may be seen as drawing an ROC curve. The procedure for probability estimating classifiers is exactly the same. 2.2.2. ROC curve construction for discrete classifiers
Since a discrete classifier is represented by only one point on an ROC graph, one may construct a very approximate ROC curve by connecting this point to the points of both default classifiers. However, a much better option is to analyze the classifier X  X  decision process and adapt it to issue scores in addition to class predictions. When the scores are obtained the same procedure of constructing an ROC curve is employed as in the case of scoring classifiers. 2.3. AUC measure
We may want to compare more than two ROC curves. If the number gets high, visual comparison of these curves may become a non-trivial task. This is especially true in the case that many of them intersect (meaning that the underlying classifiers do not dominate each other). To this end, another measure of classification model performance has been introduced in ROC analysis: Area Under the ROC Curve (AUC) . The purpose of this measure is to summarize individual ROC curves in the form of numerical information. Comparison of the quality of classifiers thus reduces to comparison of numerical values.
Statistical meaning of the AUC measure is the following: AUC of a classifier is equivalent to the probability that the classifier will evaluate a randomly chosen positive instance as better than a randomly chosen negative instance [21]. This statistical property is often referred to as the probabilistic form of the AUC measure. It originates from signal detection theory and was introduced to machine learning community mainly through the use of ROC analysis in radiology. In [34] an experiment employing two-alternative forced choice (2AFC) technique (commonly used in psychophysics) was performed. As a result, the meaning of the AUC measure was determined to be the probability of correctly distinguishing a random pair of one normal and one abnormal sample in a 2AFC task.
 AUC is related to other well-known measures. It is equivalent to the Wilcoxon statistic and to the Mann-Whitney statistic [3]. Further, the AUC is related to the Gini index [9]. In [36] the relation between statistical properties of the AUC and those of the Wilcoxon statistic are discussed in detail. Value of the AUC measure may be calculated using the formula below: where the sum passes over all pairs of one positive and one negative instance. Value of the variable difference is equal to the difference between the score of a positive and the score of a negative instance (in exactly this order) in an individual pair. Conditional statement is in the form where a is the value returned when a condition is met and b the value returned when a condition is not met.

To be convinced that the value of AUC of some ROC curve may be calculated using Eq. (1), we may consider y and x axes of an ROC graph to be divided to P and N sections, respectively, where P is the number of all positive instances and N the number of all negative instances. An ROC graph may thus be seen as composed of P  X  N rectangles (i.e. P rows and N columns). If we then have a set of instances sorted according to their scores in decreasing manner, the value of AUC is calculated as follows. For every positive instance, we count the number of negative instances which have lower score than the chosen positive instance. We accumulate the sum. At the end, we divide the final sum with the number of all pairs of one positive and one negative instance ( = P  X  N ), and finally we obtain the value of AUC. The gain of one positive instance may thus be regarded as one row on an ROC graph. The scalar value of the AUC metric thus exactly corresponds to what may graphically be seen as the portion of the area of an ROC graph lying under an ROC curve.

Value of the AUC lies on the interval from 0 to 1. Since any useful classification model should lie above the ascending diagonal of an ROC graph, AUC of such models exceeds the value of 0.5.
 In [8] the use of AUC as a performance measure for machine learning algorithms is investigated. AUC and overall accuracy measures are compared. It is shown that AUC has some convenient features: standard error decreasing when AUC and the number of test samples increase; it is independent of a decision threshold; it is invariant to prior class probabilities; and it indicates to what degree the negative and positive classes are separated.

While there are several possible measures that allow users to measure association between sensitivity and specificity (various information measures, such as mutual information gain etc.), AUC additionally provides a geometrical interpretation of the ROC graph. As there is no general rule specifying which measure has advantages or disadvantages in particular problem domains and using particular models, it is up to a user to select such a measure which has a required interpretation. 2.4. An example In this subsection we present an illustrative example of the ROC drawing process and AUC calculation. Suppose the following set of instances is given: It contains 8 instances represented by their scores (predicted by a classifier) and true classes. The letter  X  X  X  stands for the positive class while the letter  X  X  X  stands for the negative class. The data set is balanced, containing 4 positive and 4 negative instances what results in the fact that the horizontal and vertical unit sizes are both equal 0.25. The starting point is in (0,0). In our example, applying the threshold between the scores of instances 1 and 2 yields a situation with 1 true positive and 0 false positive classifications, which we denote as a point (0,0.25) on an ROC graph, i.e. we move one unit up. In next step we fix the threshold between the instances 2 and 3. The number of true positives increases to 2 while there are still no false positives, what results in moving up for another unit to a point (0,0.50). After fixing the threshold between the scores of instances 3 and 4, we obtain the point (0.25,0.50) on the graph, i.e. we move one unit to the right, since the instance 3 whose true class is negative has higher score than some positive instances. In a similar way we move up and right over the graph until we reach the point (1,1). As a result, the ROC curve shown in Fig. 3 is drawn in a step-by-step manner. For simplicity, we presumed equal misclassification costs with their value being 1. In the case of unequal misclassification costs or unbalanced class distribution ROC space features change, e.g. expected costs of classifiers on the ascending diagonal are different.

We may calculate the value of the AUC measure for such a set as follows. As already mentioned, the set contains 4 positive and 4 negative instances. Consequently, there exist 4  X  4 = 16 possible pairs of scores of one positive and one negative instance and the corresponding ROC graph in Fig. 3 is divided to 16 squares of equal size. The final value of the AUC measure for the set above is obtained in the following manner. 1. The difference between the score of instance 1 (p) and the score of any negative instance is always 2. The difference between the score of instance 4 (p) and scores of individual negative instances 3. The difference between the score of instance 7 (p) and scores of individual negative instances is 4. We divide the total sum (i.e. the sum in the numerator of the fraction in formula 1) with the number 3. Construction of ROC curves for commonly used classifiers In this section, the procedure of generating ROC curves for some typical classifiers is described briefly. When planning to draw an ROC curve, the main issue with the classifiers is how to obtain scores for their test instances. As already mentioned, some classifiers yield such score values in their original setting, while the others output discrete predictions which should be manipulated to output scores. It is presumed that each classifier is already trained on instances of a training set. The statistics about class distribution of training instances may then be used in calculations of a score output for individual test instances.
After the scores are obtained, we may generate an ROC curve by applying one of the following two equivalent processes: (1) we may traverse the scores as described in the previous section, or (2) alter-natively, alter the threshold on some parameter (not necessarily a score) of a classifier explicitly, in a systematic manner. For every threshold value, the contingency matrix is recalculated, TPR and FPR val-ues are recomputed and the corresponding point is added to an ROC graph. The set of points is finally connected to form an ROC curve. When the threshold is applied to the predicted score for the positive class, it should be noted, that by increasing the threshold, values of TPR and FPR decrease (each at its own rate), as less instances are classified as positive. Consequently, we move over an ROC curve in the south-west direction.

In the following, we briefly summarize how classifier scores are obtained from the most commonly used classification models.  X  Naive Bayes (NB) by default outputs a score in the interval [0.00, 1.00] for every test instance. A  X  Decision Tree (DT) in its basic form only returns a class label (prediction of a class membership) for  X  Artificial Neural Network (ANN) yields a score fo r every test instance. Common technique is to  X  In the case of k -Nearest Neighbors (KNN), a score for a test instance may be associated to the  X  Support Vector Machine (SVM) outputs scores by default. The threshold may simply be set on the
While instance statistics are a handy basis for score acquisition, they are not the only option. Discrete classifiers may also be transformed to scoring ones by classifier aggregation or a combination of scoring and voting [21]. A review of methods for generating ROC curves for various classifiers may also be found in [60]. 4. Applications
The use of ROC analysis in machine learning is heterogeneous and turns out to be very convenient when class distributions and misclassification costs are unknown (at training time). It is applied to model evaluation, presentation, comparison, selection, construction and combination. Thus, it is used as a post-processing technique and as a method that is actively taking part in the model construction to improve a given model. In the following, we describe the most common application areas of ROC analysis in machine learning. Activities connected with the fulfillment of ROC-related tasks are not strictly inde-pendent and some level of overlapping is present. For instance, model improvement (see Section 5) may be recognized as a task related to model construction as well as model combination since the goal in both cases is to gain a model with better performance. 4.1. Model evaluation, comparison, selection and presentation
ROC curves facilitate the task of model evaluation . Classifiers may be evaluated by merely observing their location on an ROC graph. If operating characteristics (i.e. class distribution and misclassification costs connected with each class) are unknown at the time of evaluation, global measure of performance in the form of the AUC measure may be employed. The use of AUC as a performance measure for machine learning algorithms is advocated in [8] and has already been discussed in Section 2.
In view of model evaluation, ROC space has even be used for the redefinition of other machine learning metrics. The theory which would define the use of different metrics for various goals is discussed in [31]. The authors believe that such a theory is missing and the choice of which metric to use in a certain context is historically determined. As the main tool, they use isometrics (i.e. sets of points for which a metric has the same value) and the 2D ROC space. Isometrics have originally been presented in [47,48] as iso-performance lines . These auxiliary lines have a characteristic that the expected cost of all classifiers located on an individual line is equal. They are used to separate the performance of a classification model from the specific class and cost distributions. They are also robust to imprecise class distributions and misclassification costs. The 2D ROC space, on the other hand, is derived from the 3D ROC space, represented by FPR on x -axis, TPR on y -axis, and relative frequency of positives (i.e. P/ ( P + N ) ) on z -axis, by discarding z -axis and introducing the skew ratio , a parameter summarizing the operating characteristics. The effective skew landscape of a metric (i.e. the slope of its isometric at any point in 2D ROC space) is found to be its defining characteristic. As a result, a simplification of the F-measure and a version of the Gini splitting criterion invariant to the class and cost distributions have been derived.
Another task to which ROC analysis may be applied is model comparison . Given two discrete classi-fiers, the former may be evaluated as better if it is situated more northern or western (or both) than the latter. When comparing two scoring (or probability estimating) classifiers, the former classifier may be determined as better only if its ROC curve is strictly above the ROC curve of the latter. Instead of visu-ally comparing ROC curves we may rather compare corresponding values of the AUC measure. In the case that ROC curves cross we should be aware that the classifier with the highest AUC value will not necessarily be the optimal one for some specific combination of class distribution and misclassification costs.

Employing ROC analysis for model selection then enables the selection of an optimal model (after the information about the operating characteristics of the model deployment is obtained). The final choice of which model is the most appropriate is thus postponed until the deployment stage. A common procedure for selecting the potentially optimal classifier is described in [28]. Class distribution and error costs are combined to determine the slope of an auxiliary line positioned on an arbitrary location on an ROC graph. Afterwards, the line is shifted in the direction of the upper-left corner of an ROC graph, until it touches the ROC convex hull (ROCCH, defined in Subsection 5.1) in one single point (i.e. the line becomes a tangent to the ROCCH). This point represents the optimal classifier for given operating characteristics. Suitability of the ROCCH for tasks of model selection has been tested in [5] where the performance of various classifiers has been compared through the perspective of misclassification costs. The ROCCH has been recognized as a robust alternative to the AUC measure.
 Since ROC analysis is based on graphical principles it is an appropriate tool for model presentation . As such, it visualizes performance and facilitates a general notion of classification models. It may be combined with other visualization techniques, several of which are listed in Section 6, e.g., AUC may be put in the role of the y-axis of a learning curve plot [7]. 4.2. Model construction and model combination
Learning algorithms may be adjusted to aim at constructing classifiers with good ROC curves instead of optimizing other criteria. ROC analysis may thus be employed for model construction .Theidea of model combination , on the other hand, is to combine a set of classifiers to obtain a hybrid model which demonstrates improved performance with respect to its component classifiers. One may combine different models, or alternatively, a single model with various parameter settings. As we may combine individual models to construct some other models, both activities may be highly overlapping.
A common principle which may be employed for model combination is ROCCH [48]. With this technique it is possible to obtain a combined model which will classify at least as well as the best of its constituent models for all possible operating characteristics. A theorem in [48] states that a hybrid model can achieve a tradeoff between true and false positive rates given by any point on the ROCCH, not only the vertices (the latter represent given classifiers), which results in a fact that sometimes a hybrid can actually be superior to the best classifier known. Such a hybrid classifier may be optimal for any target conditions, including imprecise class distributions and misclassification costs. ROCCH only includes classifiers that are optimal for some (FP,TP) pair and discards all other sub-optimal models what contributes to its spacial efficiency.

Another approach to model combination is given in [30]. Two methods (model assembly algorithms) are proposed to discover concavities in ROC curves and repair them. The idea is in adapting the predic-tions of questionable quality. The goal of the SwapOne algorithm is to enlarge the AUC of a probability estimating classifier by taking three models from different thresholds into account. A hybrid model is constructed by combining two better models and an inversion of the inferior one. The second algorithm, named SwapCurve , is designed to enlarge the AUC of a probability estimating classifier by detecting a section of an ROC curve that is under its convex hull. Afterwards, the ranks of the instances in that section are inverted. The algorithm is applicable to any model issuing scores.

In [24], the procedure implementing the above-mentioned ideas for a construction of decision trees is shown. The goal is to generate a decision tree for which its set of derived trees will result in an ROCCH with the maximum area under it. A novel splitting criterion for decision trees based on the AUC measure is defined. The criterion chooses the split which has the highest local AUC and is presented as the first splitting criterion based on the estimated probabilities that is not a weighted average of impurities of the children.

A method with similar aims of obtaining multiple classification models from a single one is presented in [6]. These models focus on various target conditions and as such cover different areas of an ROC graph. The method is based on the finding that classifiers may carry additional information apart from their coordinates on an ROC graph. Classifiers may often be divided into sub-components, each of them being a classifier itself. As before, the approach is discussed more deeply for decision trees, but may be employed to other classification schemes as well. Firstly, the leaves are ordered with regard to their probability of predicting the positive class. Predictions (positive and negative) in the leaves are then systematically varied, yielding new (biased) decision trees. The method may only improve the ROCCH (as in the worst case the original classifier will dominate all the derived ones) while at the same time being computationally cheap.

Another technique optimizing ROCCH has been presented in [52], using inductive logic programming (ILP) as the main tool. Background knowledge used to construct models is usually obtained without hav-ing in mind some particular target conditions in which such models will operate. Employing irrelevant information may result in suboptimal or even incorrect models, thus only an appropriate subset of all background knowledge should be considered for given conditions. Therefore, the main idea is to con-struct a convex hull by repeatedly running an ILP system on various subsets of background information.
Further, ROC curves may also be used to experimentally find more suitable decision thresholds for the naive Bayes classifier [39]. The authors treat the threshold as an additional parameter of a model which should be learned from the given data. Posterior estimates in the naive Bayes classifier are scores, not real probabilities, and in the case that independence assumptions do not hold, these probability estimates will be inaccurate. In such a case, there is no well-grounded reason to predict the class whose posterior probability is higher than 0.5 (in a two-class problem). The algorithm for a two-class case is an adaptation of the algorithm for drawing ROC curves (discussed in Section 2.2.1). The one for multi-class problems is implemented using weights (one per class) which are determined by greedy hill-climbing search. Its weaknesses are that finding a local optimum is not guaranteed and that the result may change while altering the order of classes. Searching for globally optimal solution has not been taken into account since it would result in a computationally intractable algorithm. However, both algorithms are capable of considering non-uniform misclassification costs and are applicable to the recalibration of all learning methods which return class scores as a result. 4.3. Polemics in the research community
ROC analysis has been an interesting research topic during last decades and has been studied by many researchers from various perspectives. As a result, some disagreements like the following emerged. In [58], cautiousness is recommended when employing ROC analysis for classifier evaluation at varying class distributions. It is argued that ROC analysis cannot guarantee accurate evaluation of classifiers at unstable class distributions if a) the classes contain causally dependent subclasses whose frequencies may vary at different rates between the base and target data, or b) if there are attributes upon which the classes are causally dependent. A reply to these issues may be found in [23] and states that the assertions in [58] are mainly related to only one of the two general domain types. Some real-world domains of the second type are given where ROC analysis is expected to be valid in spite of varying class distributions. 5. Improvements of basic ROC techniques
Since the basic ROC analysis is not able to answer satisfactorily all questions pertaining to classifiers X  performance, some extensions, approximations and other improvements have emerged over time. Some have been found to be useful while the practicability of others has not been generally acknowledged. In this section, the following advantageous upgrades of the basic ROC approach are discussed: ROC convex hull, confidence intervals and confidence bands, extensions and approximations of the ROC analysis to the multi-class case, variants of original ROC curves and the AUC metric taking scores and instance varying costs into account, and improvements that help to increase efficiency. 5.1. ROC convex hull
The ROC convex hull (ROCCH) is a line connecting classifiers which may be optimal for some oper-ating characteristics. This line is convex and no classifier may exist in the part of an ROC graph above it. Given a set of classifiers, their ROCCH may be generated by the following procedure. Classifiers have to be sorted according to their TPR values and plotted on an ROC graph. Afterwards, the line is con-structed starting in (0,0) and connecting consequent points on a graph in a way that the slope is always maximal possible. By repeating this step, one finally reaches the default classifier in (1,1), and all classi-fiers with minimum expected cost are located on the ROCCH. As an ROC curve, ROCCH is also useful when target conditions are completely unknown, representing a global performance indicator over all possible conditions. Figure 4 shows ROC curves of classifiers B and C together with their ROCCH. The key feature of ROCCH is that for some deployment contexts it may perform better than (and always at least equal to) the best of its constituent classifiers. In the figure such a case may be observed in the FP interval [0.15, 0.40].

A high level of similarity and the principal differences between an ROC convex hull and an ROC curve should be noted. Both are always monotonically non-decreasing, while a convex hull, as the name suggests, has to be convex as well. Nevertheless, some authors use the term  X  X urve X  when actually having in mind  X  X onvex hull X , thus attention should be given when this distinction may be of importance. Transforming an ROC curve into an ROC convex hull is feasible since given any two classifiers on an ROC graph, arbitrary classifier on the line connecting these two may be obtained by assigning a weight to each of them and then choosing one randomly (according to the weights) every time a new instance is processed. 5.2. Robustness and reliability
ROCCH is a more robust tool for measuring classifier performance than the AUC [49]. The AUC is a single-number measure and, as already mentioned, is not appropriate for an evaluation and comparison of classifiers when none of the considered classifiers dominates the others on a full range of operating conditions. In such cases, it may happen that the classifier with the highest value of such a single-number measure will not be the one with minimum cost for specific target conditions. Without providing these conditions no single-number metric may be absolutely trustworthy. In the case of ROCCH, ranges of operating conditions where some particular classifier is optimal may be specified by expressing them in the form of slopes of tangents to the ROCCH. As a result, a table of such regionally optimal classifiers is obtained. The approach has been supported by the experimental study of classifiers, such as decision tree, naive Bayes and k-nearest neighbor, applied to several UCI repository data sets.

Another, more statistically oriented approach, is the introduction of one-dimensional confidence inter-vals for ROC curves which may be constructed by Vertical Averaging (VA) [49]. The method is designed on the principle of sweeping over the FPR axis, fixing FPR at regular interval, and averaging TPR values of ROC curves being compared at each of these fixed FPRs (by computing the mean). From newly ob-tained (FPR,TPR) points, the averaged ROC curve is generated by employing linear interpolation (this is possible, since any classifier on the line connecting two other classifiers may be simulated). Finally, at these points of the curve, vertical confidence intervals for the mean of TPR values are calculated. While the procedure is simple and such confidence intervals are suitable for maximizing the TPR given some FPR value, the authors state that it may not be fully appropriate for the task of minimum expected cost evaluation. A noticeable disadvantage is the fact that the FPR may generally not be controlled by a re-searcher [20]. An example of the VA method application is given in Fig. 5 where confidence intervals are presented by vertical lines. Two dotted lines connecting all upper respectively lower ends of confidence intervals represent a confidence band described in the last paragraph of this subsection.
 To overcome the potential disadvantage above, ThresholdAveraging(TA) has been introduced [20]. Instead of fixing the FPR, the technique is based on fixing the threshold of a scoring function. A set of thresholds is first generated. Afterwards, for each of these thresholds a relevant point on every ROC curve under the comparison is located. The points (on different ROC curves) belonging to the same threshold value are then averaged using mean. In the resulting points confidence intervals may be calculated by applying standard deviation as in the VA method. In addition, horizontal intervals may now be produced as well. On the other hand, TA makes additional requirement that classifiers have to be able to issue scores. Since scores should not be directly compared across different classifiers, care should be taken when using this technique on their ROC curves, as the resulting averaged curve may be misleading. An example of the TA may be seen in Fig. 6. As before, dotted lines denote a produced confidence band.
With the aim of obtaining a more robust statistical technique, methods for generating confidence bands for ROC curves are discussed in [42]. As ROC analysis has long been used by medical researchers, the authors assume that it may be beneficial to introduce techniques from the medical field and evaluate their fitness on various machine learning tasks. The authors then present their methodology for constructing confidence bands by employing two existing machine learning techniques for generating confidence intervals, namely VA and TA, and introducing three techniques used in the medical field. The main idea of the methodology is that selected points produced by any of these methods are afterwards connected to form the upper and lower confidence bands of an ROC curve. An empirical evaluation has shown that bands generated by applying VA and TA methods are too tight. In the case of TA, this may also be a consequence of the procedure that has been used to convert confidence intervals into bands  X  horizontal intervals (FPR) have simply not been considered. The method which performs best in the evaluation is one of the three introduced from medicine  X  Fixed-Width Bands (FWB) method, of which an example is visualized in Fig. 7. One disadvantage of confidence bands may be an inappropriate effect of these bands on the AUC in the case that probabilities a re non-uniformly distri buted from 1 to 0 [25]. 5.3. Generalizations to multi-class problems
Original ROC analysis can only handle two-class decision problems. This is often satisfactory since numerous problems exist where a decision has to be made between two alternatives. Nevertheless, there are domains where three or more categorical alternatives should be considered. As the goal could be reached through the two-class ROC analysis by decomposing a multi-class problem to a number of binary problems, this often imposes a difficulty of dealing with such a system and understanding it. As a result, generalizations to multi-class ROC analysis have been developed. If a multi-class problem consists of k classes, there are now k  X  k possible classifications, considering k options for the true class and k options for the predicted class. This calls for different methods of visualization as simple two-dimensional graphs cannot represent such complex information. In case the number of classes gets high, the computational complexity may also become important. These issues are discussed in more depth in [41].

ROC analysis has been extended to three-class decision problems in [44]. In this way, an ROC surface may be plotted in three dimensions. The volume under the ROC surface (VUS) for three-class problems is analogous to the AUC metric in two-class decision making and equals the probability that the classifier will correctly sort three items containing a randomly-selected instance from each of three classes. ROC surfaces can be compared via a comparison of maximum information gain on each of them. A potential problem of three-class ROC analysis might be in estimating probabilities which is intellectually more complicated than in a two-class model. Domain experts in many fields of practice (e.g. physicians) make decisions between two classes without stating probabilities. A three-class case thus only adds difficulty of making good probability estimates. While with two classes estimates are needed for one pair of outcomes, in a three-class case the number of outcome pairs increases to three. The reliability (consistency from case to case) and validity (accordance with expert X  X  opinion) of those estimates may thus become questionable.

An extension of the AUC measure in the form of the VUS has also been presented in [27]. The default classifiers, minimum and maximum VUS, and the equations are derived. The procedure of computing polytopes (multi-dimensional geometric objects with flat sides  X  a polygon is a polytope in two dimen-sions) for a set of classifiers is presented. It consists of forming constraints (linear inequations) which are then solved by the Hyperpolyhedron Search Algorithm (HSA). Volume of the hyperpolyhedron is computed using QHull algorithm [4]. In this way, VUS of any classification model for any number of classes may be gained. A disadvantage of this technique is its inefficiency for a higher number of classes. It should be mentioned that the aut hors use different representation of an ROC graph by plotting FNR on the y-axis against FPR on the x-axis. In this case, the goal becomes the minimization of the area un-der the ROC curve what is essentially equivalent to the maximization of the area above the ROC curve (AAC). However, the authors are consistent with the terminology and refer to the AAC as AUC. It is demonstrated in [14] how a multi-class classifier can be directly optimized by maximizing the VUS. This is accomplished in two steps. Firstly, the discrete U-statistic (which is equivalent to the VUS) is approximated in a continuous way. Secondly, the resulting approximation is maximized by the gradient ascent algorithm. The drawback of this approach lies in its exponential time complexity. Another approach to an extension of the AUC measure to the multi-class case is presented in [46]. All classes are, one by one, put in the role of a reference class (i.e. class 0), while at the same time, all other classes represent the alternative class (i.e. class 1). Values of the AUC for all the resulting arrangements are calculated  X  put differently, the one-versus-all strategy is applied. Afterwards, the final AUC is obtained by computing the weighted average of above partial AUCs, where the weight of each AUC is proportional to the prevalence of its corresponding class in the data. The weakness of this approach is that the final multi-class AUC is sensitive to class distributions and misclassification costs.
Further multi-class generalization of the AUC is available in [35]. A generalization is done by ag-gregation over all pairs of classes. Firstly, AUCs (more exactly, their approximations) for all possible combinations of (different) classes are computed. Then, the sum of these intermediate AUCs is divided by the number of all possible misclassifications. This averaging of pairwise comparisons is insensitive to class distributions and misclassification costs. One interesting feature of the new measure is, that its value may become substantially higher for small improvements in pairwise separability. The extension is invariant to monotonic transformations of estimated probabilities. It should be noted that the authors, in view of the representation adopted in our paper, use swapped axes on an ROC graph.

It has been demonstrated that principles of the ROCCH extend to multi-class problems and multi-dimensional convex hulls [51]. Given a finite set of points, the minimum value of any real-valued linear function is reached at the vertices of the convex hull of the points. This determines the position of classifiers with the minimum cost and follows from the features of convex sets that represent the base of linear programming algorithms. As a result, if classifiers for n classes are considered to be points with coordinates assigned by their n  X  ( n  X  1) misclassification rates, then optimal classifiers are shown to lie on the convex hull of these points. This holds for an arbitrary number of classes.

Considering costs, the ROC analysis has also been extended through the perspective of an optimization problem [19]. The latter has been used to define an ROC surface for the n -class decision problem with the aim of minimizing n  X  ( n  X  1) misclassification rates. The final solution to the problem is finding the optimal trade-off surface between different types of misclassifications, known as a Pareto front . The Pareto front is represented by a set of all Pareto optimal solutions, each of them, in turn, being a solution not dominated by any other possible solution. An evolutionary algorithm for locating the Pareto front (based on greedy search) has been presented. In addition, a multi-class generalization of the Gini coefficient has been proposed.

Cost-sensitive optimization is further discussed in [40] in view of a lack of methods for handling multi-class ROC analysis for large numbers of classes. Algorithm in [19], for instance, has only been tested on domains with few classes and may become intractable since it uses sampling of operating points. A pairwise approximation is introduced to this end, which examines interactions between operating weight pairs (two-class ROC curves). The algorithm considers the most interacting pairings and the most expensive errors. Since some interactions are removed, the method becomes extensible to a larger number of classes.
 5.4. Considering scores
AUC ignores scores (i.e. posterior probabilities of the positive class) and considers only ranks of scores (i.e. an order). Since a part of information is ignored, this may lead to suboptimal results, for instance, overfitting to a test set when selecting classifiers with high values of the AUC. A different option of how to evaluate a success of learning is to replace the AUC by using some other measure, e.g. information gain, Brier score or LogLoss. However, these alternatives ignore the order and will not be further discussed in this work. Another possible approach is to use one of the four evaluation methods developed by various authors, described in the following. These measures consider both  X  scores and ranking  X  and are derived from the basic AUC metric. However, we should be aware of the advantage of the original AUC, which is its independence of any distribution assumptions.

Like with the basic AUC, values of these variants can be calculated directly without the explicit con-struction of corresponding ROC curve variants. AUC may be generalized to a form in which basic metric and its variants can be expressed more uniformly. Such a generalization has been carried out in [56]. To calculate a value of any AUC variant for a given set of instances, we traverse all possible pairs of one positive and one negative instance and accumulate the value of the difference function (called also the modifier function in [56]) over all pairs. The difference function handles a difference of scores of two instances in an individual pair. Afterwards, we divide the intermediate result with the number of all pos-sible pairs. The generalized form of the AUC may thus be interpreted as a mean value of the difference function for a set of instances. It is obvious that the AUC and its variants only diverge in their difference functions, i.e. the way how a score difference is dealt with. Difference function for the basic AUC is the step function.

The proposed four variants of the AUC metric are briefly summarized in the following. Table 1 shows an example of computed values of these variants for 15 different score sets in the classification of 6 instances. The second table column provides the predicted scores and the instances X  true classes. The following table columns illustrate how various AUC variants differently evaluate the quality of classi-fiers. The AUC variants are: 1. In [25] the first variant, called probabilistic area under the ROC curve (probAUC) ,ispresented. 2. The second variant is scored area under the ROC curve (scorAUC) [62,63]. Its value is equal to 3. Third variant, named softened area under the ROC curve (sondAUC) , has been proposed in [37]. 4. The last variant is called the soft AUC (softAUC) [10]. Besides considering scores, the main moti-Methods of drawing probROC and scorROC curves have been presented together with their correspond-ing AUC variants, while the notion of sondROC and softROC curves has not been explicitly mentioned. All four AUC variants are softer than the basic AUC as they aim at smoothing the difference function of the AUC. As such, they are intuitively expected to be more robust for small data sets. Since all four variants employ both, scores and ranks, any of them may possibly also be used as a statistic for testing the diversity of two samples (similar as the Wilcoxon-Mann-Whitney statistic). Difference functions of probAUC , scorAUC and softAUC variants are visualized in [56]. 5.4.1. An example In Table 1 we provide an artificial example, intended to illustrate behaviors of different AUC variants. The table contains 15 example data sets comprised of six instances. Each instance is represented by its (not necessarily calibrated) probability of belonging to the positive class (denoted with a real number) as predicted by a classifier (i.e. a score), and a sign denoting the true class ( + denotes the positive class and  X  denotes the negative class). For each set of instances, values of the basic AUC and its four variants are calculated. A couple of sets (no. 1 and 9) are used several times (thus annotated with suffixes a-e) for different values of parameters q and  X  which are required for a calculation of the sondAUC and softAUC , respectively. The sets of instances are ordered from intuitively the best to intuitively the worst (in a decreasing order). The table serves as an illustration of how basic AUC and its variants evaluate some possible sets of instances. The most obvious characteristics are exposed as follows.  X  Sets of instances 1a-1e represent an ideal case in which a classifier issues a score 1 for every positive  X  In sets of instances 2 X 7, all instances have still been perfectly ranked, although the margin between  X  In the set of instances 6, a narrow margin between positive and negative instances (0.10) is perceived  X  Another similar example follows from comparing the sets 7 and 8. A ranker with an ideal perfor- X  The sets 9a-9e demonstrate an influence of the parameter q on behavior of the sondAUC metric.  X  The sets 9 and 12 expose the main disadvantage of the AUC measure which the derived variants  X  The set 13 represents a case when the observed classifier is of no use  X  it issues a score 1 for every A much more extensive analysis of the three AUC variants, namely probAUC , scorAUC and softAUC , has been provided in [56], where their performance is claimed to be questionable. The authors believe that none of the variants should surpass the basic AUC, at least when applying them to evaluation and selection of classifiers. The variants may be biased with the variance possible in either direction. Nev-ertheless, the probAUC and the softAUC with appropriately chosen parameter values are recognized as exact approximations of the basic AUC metric. 5.5. Considering instance-varying costs
One of limitations of ROC graphs is their inability of handling problems where misclassification costs vary from one instance to another of a same class. Such costs are called instance-varying costs (also, example-specific costs ) and appear quite often in real-world problems. ROC graphs use true and false positive rates to construct a curve and assume that errors of one type are all equal.

An ROCIV graph , a transformation of the original ROC graph, is an approach to deal with instance-varying costs [22]. An intuitive interpretation of an ROCIV curve is that the axes are scaled by example costs within each class. In such a manner, the y -axis represents a true positive benefit, while the x -axis represents a false positive cost. An ROCIV curve is constructed similarly as an ROC curve: for each positive (negative) instance, its benefits (costs) are incremented accordingly. The interpretation of the area under the ROCIV curve (AUCIV) is related to the one of the original AUC and equals a probability that a classifier will rank a randomly chos en positive instance higher than a randomly chosen negative instance given that each is chosen in a proportion to their costs. If instance costs in a given class are all equal, the ROCIV graph is identical to the ROC graph. ROCIV graphs may offer more accurate picture of classifier quality in domains where error costs are not uniform within a single class and may prefer different classifiers (in different regions of the problem space) than traditional ROC graphs. On the other hand, the original ROC graphs presume that true and false positive rates of the test set will be similar to those in the training set. In the case of ROCIV graphs a new presumption arises, that the instance costs will also be similar. This new presumption is in contradiction with a feature of ROC curves that they are cost-invariant, and represents a potential drawback. An ROCIV curve thus becomes sensitive to variations in inter-class misclassification costs, but remains insensitive to intra-class variations. Inter-class error cost distributions in training and test sets should therefore be additionally checked for consistency. 5.6. Efficient computation of the AUC
Repetitive computations of the AUC may be relatively time-consuming. Since computations of this kind are important in many techniques, such as, for example, methods for direct optimization of the AUC, aspirations for more effective algorithms emerged.

In [10], a polynomial approximation of the AUC has been presented. Similarly as in the case of AUC variants, the only distinction between the exact (basic) AUC and its approximation is in the difference function: the step function is replaced by a general form of a polynomial. A degree of a polynomial should be chosen deliberately, optimizing the relation between accuracy and performance. The approxi-mation is believed to be more accurate than sampling and at the same time being computable in one pass over a database (thus having linear time-complexity). 6. Alternatives to tools of the ROC analysis
In this section, some other techniques which may be used instead of ROC graphs are listed and dis-cussed. Some of them are strongly related to ROC graphs while still providing an alternative form of presentation which may be favorable in particular domains. 1. Detection error tradeoff curve (DET curve) is an approach to performance comparison, presented 2. Loss comparison plot (LC plot) and Loss comparison index (LC index) have been presented as an-3. Lift chart is a technique similar to ROC graphs and is commonly used in some branches of data 4. Calibration chart (also calibration plot, calibration graph) is an approach that demonstrates how 5. Learning curve plot is another graphical technique for visualization of classifier X  X  performance. 6. Cost curve is a method developed with the aim of redressing the tools of ROC analysis [17] and 7. PN graph is a graphical technique in close relation to ROC graphs (the acronym PN is derived 8. Precision-recall curve (PR curve) is an alternative which is often used in information retrieval and 6.1. An example
We provide an illustration in which we compare three classifiers trained on an example set of 100 positive and 100 negative instances. We presume their misclassification costs to be all equal, i.e. all having value of 1. With such a set-up, a high level of similarity among the alternatives is revealed. It should be noted, however, that quite a different picture may be seen when class imbalance gets high or the cost of a false positive substantially differs from the cost of a false negative. The described ROC alternatives present classifiers X  performance from different angles and may thus be helpful in better understanding of a given problem. They are applicable to all application areas of the ROC analysis (see Section 4), e.g. model evaluation and presentation.

Some of the alternative techniques are computed on the base of same data as ROC curves, i.e. a con-tingency table, and may be transformed easily from one representation to another. Such techniques are shown in Fig. 8. On the other hand, a second group of techniques measures substantially different fea-tures and requires other information, as well (e.g. classifier scores, performance measured while varying the size of a training set etc.). Some of such performance curves are shown in Fig. 9. A transformation to some of them may only be accomplished if all the needed information is available. In all the represen-tations, the performance of the classifiers A, B and C (same classifiers as in Figs 2 and 8a) is visualized in different ways.

To present a meaningful LC plot (Fig. 9a), we change our initial presumption that misclassification costs for both types of misclassification are equal. In our example we presume that the misclassification of negative instances (FPs) is between two and five times as serious as misclassification of positive instances (FNs), with the most probable ratio being 3. These three values determine the location of three key points on an LC plot. As FPs are more costly than FNs, the goal should be to stay on the left side of an ROC graph, near the ordinal axis. In this area, the classifier C is superior to the classifier B (A is not included in the LC plot since it is superior to both, B and C, for any context) and this is exactly what the LC plot reveals.

Learning curves in Fig. 9b reveal information which is not contained in a single ROC graph, since the latter assumes a fixed number of training instances. An ROC graph with a fixed number of instances thus represents only a single point on each learning curve in a learning curve plot. In Fig. 9b, only three points are therefore actually related to our ROC graph example in Fig. 8a, namely those three which represent the AUC value for 200 instances. In a similar way, the calibration chart in Fig. 9c is based on data not contained in ROC graphs, but rather on additional information gained from a data set. 7. Conclusion
In our paper, we presented basic concepts of the ROC analysis in the area of machine learning. We explained basic notions of this approach as well as the most popular ways of how ROC curves and the AUC measure may be employed to tackle different classifier optimization problems.

Important improvements of basic two-class ROC curves and the AUC metric, including their gen-eralizations to the multi-class case, have been mentioned. Further, we shed some light on alternative approaches. The resu lting survey provides relev ant information ga thered on one place, and may serve as a signpost to other articles where the topic is discussed in greater detail.

During future development of ROC analysis it would be, in the face of recent findings, beneficial to periodically compare tools of ROC analysis with other widely-used measures of classification perfor-mance and verify their advantages and disadvantages from the theoretical as well as experimental point of view, similarly as the authors in [26].
Next task might be trying to solve remaining issues in multi-class ROC analysis, especially finding ways of more intuitive visualization and more efficient calculations.

It would be useful to check whether an introduction of further extensions and improvements from other fields where ROC analysis is extensively used (especially medical decision making) may be ad-vantageous in the area of machine learning. One option would be, for instance, to examine potential benefits of an introduction of time-dependent ROC analysis.

Furthermore, it would be advantageous to consider how in a balanced and theoretically well-founded manner take scores of instances into account. A new derivative of the AUC measure might be developed as a result which would eliminate most of deficiencies of the original AUC and its existing variants. This is also a part of our future work.

Since the ROC analysis is still becoming increasingly popular in the field of machine learning, this review shall still be complemented with other approaches and possible improvements. This, as well as analyzing applications of the ROC analysis in other machine learning areas (e.g. in regression), is the intended focus of our further work.
 References
