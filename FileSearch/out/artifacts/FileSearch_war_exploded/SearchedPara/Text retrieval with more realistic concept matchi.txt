 1. Introduction The best information retrieval (IR) machine one can think of is the human brain in terms of effectiveness. Unfortunately, the study of human brain function is still in its infancy and how exactly the brain performs its
IR functions is not known. However, the superiority of human brain in IR tasks seems to come from three major properties of the brain: 1. its ability to understand concepts, ideas or meanings central to a document, 2. its ability to reason about the usefulness of documents to information needs (queries). This is based on the understanding of the concepts that it gained from the content of documents and queries, and 3. its learning capability or adaptability to the environment which allows it to gain knowledge through expe-rience or interaction with the environment.

The ability of an information retrieval system (IRS) to understand concepts, ideas or meaning is limited by the underlying representation scheme used by the system. Typically, the basic element fundamental to the rep-resentation of textual material inside an IRS has been the  X  X  X eyword X  X . As far as the human brain is concerned it is unrealistic to treat a  X  X  X eyword X  X  as the sole representative of a concept. Although details of how exactly the brain formulates ideas or concepts and how they are structured for efficient storage and reasoning are not clear, its remarkable accuracy and robustness to deal with the imprecision and vagueness of the IR problem suggests that it will most probably be using more complex mechanisms rather than simple keyword matching.
Therefore in this work we assumed that the formalism and the structure of an idea or a concept in the brain are more complex and that it requires a more elaborate entity to represent concepts within an IRS. Also we assumed that concepts in the brain are organized in a more complex structure that retains the underlying inter-connections between concepts. This interconnected structure is assumed to be the key factor that allows the brain to disambiguate the meaning of an individual concept with respect to the collective meaning of the con-text in which the concept is being used and thereby helps its reasoning mechanism.

The framework of Formal Concept Analysis (FCA) ( Burmeister, 1998; Ganter &amp; Wille, 1999 ) provides a suitable formalism to this line of thinking and provides adequate means to help achieve our objectives.
FCA formulates concepts in terms of objects and their properties or attributes and provides a way of combin-ing and organising individual concepts of a given context into a hierarchically ordered conceptual structure (a concept lattice). A concept lattice represents and conveys a broad picture of the knowledge that the combina-tion of the individual concepts of the context possesses.

The adaptability or the learning aspect of the human brain, with respect to IR, seems to work in the fol-lowing way. A document that is found to be not useful for a query in the past is unlikely to be tried again by the same individual for the same or similar information need(s), at least in the near future. This is due to the ability of the brain to remember that the document does not help the particular information need. What this essentially means is that the experience gained in the early search sessions help in making intelligent decisions at later sessions. However, the brain does not retain all its memories forever. The memory fades away (forgets) over time. This forgetting feature, although it looks undesirable, is an extremely useful property for adaptabil-ity and also in preventing information explosion in the brain. We used a reinforcement learning strategy to mimic these properties in our IR model. Based on relevance feedback information given by the user for the retrieved documents, the significance of concepts and keywords that contributed for the retrieval of documents that the user finds useful are made stronger (rewarded) and the significance of the concepts and keywords of the rest of the documents, i.e. the ones that the user finds not useful or is not interested in are made weaker (penalized) as they have led to false hits.

In the following we first describe how concept matching between a query and a document lattice is per-formed. The reader is referred to ( Rajapakse &amp; Denham, 2002a ; Rajapakse &amp; Denham, 2002b ) for details of document representation in concept lattices, the implementation of concept lattices and for further details of concept matching. Our reinforcement learning strategy is presented next followed by the experimental results of the system obtained on the Cranfield collection, and finally by the conclusions we have drawn from this research. 2. The use of concept lattices in IR-related work
The use of concept lattices in IR in the past has been mainly for developing browsing mechanisms for domain specific IR. Typically, a single large concept lattice is created based on the keywords present in doc-uments. The objects in the concepts are the document identities (Identification numbers) and the attributes are the keywords. This formalism is not much different to keyword based document categorization approaches, except for the organisation of groups of documents hierarchically in concept lattices. The user is provided with a starting node, and allowed to navigate the lattice by expanding the nodes and traversing between nodes. The starting node can either be the root node or any other selected node based on an initial keyword based search. We have identified a number of disadvantages in the past approaches as found in Becker, Hereth, and Stumme (2002) , Carpineto and Romano (1996, 1993), Cole and Eklund (1996, 1999), Cole, Eklund, and
Stumme (2000, 2003), Godin, Missaoui, and April (1993), Kim and Compton (2001), Lindig (1995) and Priss (1997, 2000) : (1) the formulation of concepts using document identities as objects and keywords present in the documents as attributes is rather unrealistic in terms of the way the human brain formulates, perceives and communicates concepts; (2) use of a single large concept lattice to represent the entire document collection is computationally very expensive, and as a result such systems are limited to smaller document collections; (3) most of the past models are limited to browsing only; (4) creation of the lattice needs complex lattice build-ing algorithms. Also traversing in the lattice is expensive and increases proportional to the size of the lattice; (5) once created, the lattice is fixed and no learning facility is provided in any of the past FCA based approaches. Our approach is different to those approaches at least in the following four ways: 1. Concepts are formulated closer to the way that the human brain might do it, i.e. by extracting subjects, topics or objects mentioned in the text as objects of the formal concepts and properties or attributes related to the objects as attributes. A set of ad-hoc rules (see in Rajapakse &amp; Denham, 2002a, 2002b ) was devel-oped to extract such features from natural language text based on processing syntactic structures of the sentences. 2. Each document (query) is represented by an individual concept lattice. The advantage of this is two fold; firstly it allows us to operate on smaller lattices rather than on a single large lattice, and secondly, it allows different weights to be maintained for the same concept in different documents. 3. Concept lattices are encoded in bidirectional associative memory (BAM) structures ( Be  X  lohla  X  vek, 2000;
Kosko, 1988 ). Learning a BAM with a concept lattice ( Rajapakse &amp; Denham, 2005 ) is much more efficient than using complex lattice building algorithms. Also updating lattice representations of documents with additional concepts is much easier with BAMs ( Rajapakse &amp; Denham, 2002b ) and no node traversing over-head is involved. 4. Finally, we have employed a reinforcement learning strategy based on relevance feedback information to interactively learn document representations by (1) learning significances of concepts (formal concepts and keywords) and (2) updating the lattices with additional (query) concepts that would help retrieving documents. 3. Document representation
The main unit of the document representation is an object X  X ttribute pair called a unit X  X oncept. Unit X  X on-cepts extracted from the content of documents are organised in a hierarchical concept lattice structure using the subsumption relation defined in FCA. A unit X  X oncept has its own weight and weights of unit-concepts are held in concept weight tables. In addition, each part of a unit X  X oncept (object part and attribute part) is con-sidered as a keyword and is assigned a keyword weight. This is to supplement the unit X  X oncept matching when only parts of unit X  X oncept matches occur.

Consider the context of planets given in Table 1 as a set of objects, attributes and their relationships extracted from a document about the solar system. Each  X  X  X  X  X  in this table represents a unit X  X oncept, or in other words a related object X  X ttribute pair, and has a weight. Fig. 2 illustrates weight assignments to unit X  X on-cepts and Fig. 1 (right) shows the concept lattice that results from this set of concepts. In addition, each object (planet names on the leftmost column) and properties (column titles except the leftmost column) are keywords and have weights. Queries are represented similarly. 4. Concept matching between a query and a document
A node in a concept lattice represents a formal concept of the kind described above. The process of reason-ing the usefulness of a given document to an information need (query) is achieved based on the concepts com-mon to the query and the document X  X imilar to the way common features/terms are used in conventional IR.
That is based on how similar the nodes in a query concept lattice are to the nodes in the document concept lattice (node matching). The following example illustrates node matching between a query and a document.
Consider the context of planets given in Table 1 . This context can be regarded as a representation of a doc-ument that talks about the solar system. The concept lattice of this context is given in Fig. 1 (right). Consider an information need containing the terms  X  X  X ars X  X  as an object and  X  X  X oon X  X  as a property of it. Assume that the concept lattice of this query contains a node representing the formal concept { Ma } ! { my } (see Fig. 1 . (left)). Note that we disregard the rest of the nodes in the query lattice in this illustration. Document nodes that match with this query node are shown in Fig. 1 .

As can be seen in the above figure ( Fig. 1 ), comparing query nodes with document nodes is not as simple and straightforward as comparing just simple terms or keywords. Here, we need to maintain the consistency of our treatment of certain terms as objects and certain others as attributes. In addition, we wish to take into account the superiority/generality of concepts within the concept hierarchy in order to match more specific concepts whenever possible while avoiding duplications. However, problems of natural language including synonymy, polesemy, and others that are related to the variability of vocabulary cause mismatches between concepts. This and the size variability between query and document concepts (number of objects and attri-butes in the concepts) results in a perfect match between a query and a document node being impossible.
Therefore a mechanism to perform partial matching is required. 4.1. Partial matching
We define a partial match between two concepts as a concept m consisting of objects common to the query and the document extents in its extent and attributes common to the query and the document intents in its intent. This can be formally defined as given below:
Let q =  X  A , B  X  and d =  X  C , D  X  be two formal concepts, then the partial match between the two concepts is given by the concept m =  X  A \ C , B \ D  X  , where A , B , C , D are sets of terms of which terms in A and
C are interpreted as objects and terms in B and D are interpreted as attributes according to the FCA formalism. 4.2. Concept weighting
Partial matching of concepts needs a mechanism for determining the significance of a partial match. Assign-ing significant weights to full concepts does not help. Therefore weights are assigned to single object X  X ttribute pairs (we call them unit X  X oncepts) as shown in Fig. 2 .

The object X  X ttribute pairs (unit concepts) in m determine how similar the two concepts q and d are. If the query concept is identical to the document concept (ideal case) a complete node match occurs, i.e. m = q = d =  X  A , B  X  =  X  C , D  X  . 5. Keyword matching
It is possible that two concepts to share a common object or attribute but not both. In this case no unit X  concept match occurs. This could happen due to various reasons: due to the problems caused by term/element mismatches or due to the fact that the two nodes actually represent two distinct concepts, even though the same term/element has happened to be used by chance. Whatever the reason we do not want to ignore the possible contribution(s) that such a common feature (keyword) between two nodes could make in retrieving a relevant document. Therefore, such single object or attribute matches are considered as  X  X  X eyword X  X  matches and significance weights are maintained for individual keywords as well. Fig. 3 illustrates a concept match and a keyword match between concepts (nodes).

Since our preference is for unit X  X oncept matching, keyword matches are ignored for those keywords which are present as constituents of matching unit X  X oncepts. For instance, the keyword match of  X  X  X a X  X  will eventually be pruned out as the unit X  X oncept match of Ma ! my already contains the term  X  X  X a X  X  in this example. 6. Learning strategy
Our learning strategy works by accepting the user feedback in the form of yes (relevant) and no (not rele-vant) and accordingly reinforcing the document representations.

Traditionally, relevance feedback has been used to reformulate the query with additional keywords to enhance the query. Even though it has shown as much as 20% improvement on recall and precision, it does not retain what it learns from its experience. Important user decisions are used only within one query session for searching one information need. The results gained by relevance feedback at one query session are usually not available for the subsequent query sessions, because the IR system does not retain them or their implica-tions. Therefore a separate learning mechanism is required to make such systems adaptive.

In contrast, in our model user feedback is used to update the document representations and as such the experiences gained through past interactions are made available for later query sessions. We expect the doc-ument representations to converge to a well representative set of concepts (for each document) over a period of time. Such a set of concepts, indeed, will become more personalized to the vocabulary and the writing style of the end user, as (1) it is the concepts of the user formulated queries that are amended to relevant document representations, and (2) it is the user X  X  relevance assessments that are used for reinforcing significance weights of unit X  X oncepts and keywords in the document representations.

Our reinforcement learning process works as follows; if the user says a particular (retrieved) document is relevant to a given query, all the unit X  X oncepts of the query that are not present in the document are added to the document representation with an initial weight value. In the case that a particular unit concept of the query is already present in the document, we consider it an important unit concept because it has led to the docu-ment being retrieved in the first place and therefore its weight is increased by a small amount ( D W )as described in Section 6.1 below. Conversely, if a user says a particular (retrieved) document is not relevant to the query then the weights of matching unit X  X oncepts and keywords that are common to the query X  X ocu-ment pair, i.e. those that contributed to the document X  X  retrieval, are decreased to say that those units, though present in both query and the document, are not important in deciding the relevance of the document to the query. Concept addition may result in unnecessary unit concepts getting into the document representations, but we expect such unnecessary concepts to be penalized by our learning strategy as a result of not finding them important and therefore to end up with low weights in the long run. 6.1. Significance weights and step size of weight changes
Both keyword and unit X  X oncept weights are initialized at the beginning with the initial value of 2.5 and are allowed to learn over the user interactions. The range of a weight was selected arbitrarily and constrained to positive values between (0.1 X 5.0). The minimum value of the weight range was set to 0.1 instead of zero (0) to avoid complete ignorance of the existence of a feature.

The step size is made proportional to the current value of the weight. The idea is to make the changes based on how far it is from the boundary towards the direction of change (i.e. to the top boundary if rewarding or to the bottom boundary if penalizing. See Fig. 4 below). This makes learning faster if the difference between the current weight and the boundary towards which the modification is made is larger and slower otherwise. The weight modification formula is where D W = W max W old if a positive reinforcement or D W = W is the learning rate. 6.2. Learning rates for rewarding and penalizing
The learning rate is a constant that determines the proportion of the weight difference to take into account for the actual step size of the modification. The nature of IR is that only a few of the many documents retrieved are judged as useful by the user. Therefore only those few documents that the user decides useful for his information need are rewarded. All the other documents in the retrieved set are regarded as false hits by our model and therefore are penalized. As a result, on average, the weights of concepts/keywords tend to be negatively reinforced more times than they are positively reinforced. This imbalance of negative and positive reinforcements may lead all weights to end up with the minimum weight value (0.1), if not dealt with appro-priately. A way to get round this problem is to use different learning rates for positive and negative reinforce-ments. Deciding precise values for positive ( g ) and negative ( b ) learning rates is difficult as it depends on a number of factors, including the number of queries, composition of the queries and user judgments, etc. Based on the results of a few preliminary experimentations on the Cranfield collection, they were set to g = 0.04 and b = g /3 = 0.0133.

Fig. 5 illustrates the learning strategy described above. Here the query consists of only two unit X  X oncepts and the documents retrieved for this query contains two relevant documents (Doc#35 and Doc#50) and two non-relevant documents (Doc#20 and Doc#100). Matching units, weight updating and concept addition are shown in the diagram for the documents Doc#35 and Doc#20. 6.3. Informative factors of comparison units
The weight reinforcement strategy described in Section 6.1 above treats all forms of unit X  X oncepts and key-words as equal. Therefore the weight of each unit X  X oncept/keyword is reinforced by the amount decided based on the learning rate and the current value of the weight as described above regardless of how informative it is.
Since not every concept/keyword is equally informative we took into account four levels of  X  X  X nformativeness X  X  of the comparison units based on the number of terms that they possess. The weights of concepts/keywords are re-weighted using four pre-determined weighting factors which we call  X  X  X nformative Factors X  X  at the time of computing the similarity measure (RSV). Note that they are not used in weight modifications. The follow-ing are the four different levels of informativeness, listed in the increasing order of their informativeness. The experimentally chosen weightings (not optimized) are indicated within brackets. a. Single-term keywords (1.0). b. Key Phrases (Keywords with more than one term) (1.6). c. unit X  X oncepts with single-term components (both object and attribute) (2.0). d. unit X  X oncepts with multi-word components (at least one component constitutes more than one term) 7. The retrieval process and similarity (RSV) computation
The retrieval process begins when a user issues a query X  X  natural language expression. The query is pre-processed with POS tagging, concepts extracted and a concept lattice is setup. The nodes in the query concept represent formal concepts, and they are compared against the concepts (nodes) in the lattice representations of documents. The concept lattices of documents are setup, from a set of pre-constructed tables, only when they are required. Therefore, at any point in time during the concept matching process, only one query concept lattice and one document concept lattice are held in the memory. 7.1. Candidate node/concept pairs for comparison
Not all query concepts match with all document concepts between a given pair of a query and a document lattices and therefore attempting to perform all-to-all node matching is not worthwhile. Instead, we extract a set of  X  X  X andidate X  X  concept pairs to match between the query and the document based on the presence of com-mon features between them. The candidate concept extraction process works mainly by looking for the most specific concept in the document lattice for each query object (i.e. using object concepts). Attribute concepts (i.e. the most generic concept containing a given attribute) are also used in the cases where a related object concept is not available in the document. During this process, we make sure to extract the most specific con-cepts wherever possible and also not to extract the same concept pair more than once. Also we avoid extract-ing document (query) concepts that are general (in the general-specific hierarchy in the concept lattice) to any of the already extracted document (query) concepts to match with the same query (document) concept. In addition, in the case that an object or attribute in the query appears as both an object and an attribute in a document representation, we check whether there is any order relation (in the concept hierarchy) between them in order to avoid matching two related document concepts with the same query concept. Only the most specific concept is considered for matching in such cases.

However, there are some cases where we find the same query object appearing both as an object and attri-bute in document representations, but they represent two different ideas/concepts (i.e. they are not related in the concept hierarchy). In this case, the attribute concept given by document lattice for the query object is also taken into account as a candidate concept to match with the object concept obtained from the query lattice for the query object.
 7.2. Similarity measure (RSV)
Candidate concept/node pairs detected as matching between a query and a document, as described above, are extracted from the corresponding lattices and compared for partial concept matching (unit X  X oncept matching) and for keyword matching (in the absence of a unit X  X oncept matches). Matching unit X  X oncept pairs and keywords are then subject to pruning for removing duplicates. The sum of the weights of remaining unit X  concepts and keywords is taken as the similarity measure (RSV) of the document. This is represented in the following equation.
 where W u s are weights of the unit X  X oncepts and W k s are weights of keywords. 8. The evaluation/test strategy
Given the lack of availability of an appropriate methodology for evaluating the dynamic properties of inter-active IR systems, we were compelled to use our own test strategy which we call the incremental learning X  X est-ing strategy, for testing the performance dynamics of the system as it learns from experience. This was achieved by splitting the set of queries into two sets (training and testing sets) and then training the system on a cumu-equally represent query space in terms of desired properties (such as degree of overlap in relevance assessments and degree of similarity) is a difficult task. This was done based on the degree of overlap in relevant assessments, as it is the most important factor that helps interactive learning in our model. Degree of overlap was measured in terms of the number of documents assessed as relevant to each query. Out of the 225 queries available in the
Cranfield collection, 65 queries were used for testing and 160 queries for training. More queries were allocated to the training set simply because we needed more queries to create more training X  X esting (sub) sessions. This gives a fairly representative set of queries for testing in terms of cross-relations, but does not guarantee that queries are equally distributed in terms of their similarity and expressiveness in natural language.
A training set for each training phase was created by adding 40 randomly selected queries from the full training set (of 160 queries) into the training set used at the previous training session. No query is picked more than once. So the numbers of queries trained at the four training sessions were 40, 80, 120 and 160. Each query was iterated 20 times at each training session. The order of presentation of queries to the system was made random. At the end of each training session, the system was tested with the 65 test queries and the similarity measures were recorded for each query X  X ocument pair. 9. Results
Fig. 6 shows that the performance of the system (non-interpolated average precisions) increases consider-ably over training queries. Note that the shape of the curve varies depending on the amount of learning which has taken place at each training session and how much this learning has helped in retrieving relevant docu-ments for test queries. This is because the selection of training queries for sub-training sessions from the full training set (160) is done randomly. But the starting point (average precision before training) and the ending point (average precisions computed after training all training queries) remain the same.

Fig. 7 shows the P  X  R curves of test results obtained after each training session. It provides more evidence of the performance gains shown above by the system over the training process.

The performance gain shown by the system is a combined result of unit X  X oncept matching, keyword match-ing and all the components of reinforcement learning. The contribution of individual components of matching and learning towards this result is further analysed and compared below in Figs. 8 X 10 .

Fig. 9 shows that concept matching alone without any learning (the flat curve) is not of much use. Problems of concept extraction and mismatches between concepts caused by the vocabulary differences and word ambi-guity in natural language are the main reasons for the poor performance of the case with concept matching only. These problems are severe in our case compared to simple keyword matching because (1) concept extrac-tion from source documents is more complex and difficult as it needs identification of two terms, one as an object and the other as a property possessed by the object, and (2) the mismatch problem is doubled in our case because a concept match needs both the object and attribute constituents of a query concept (unit X  concept) to match with an equivalent in a document.

Allowing the system to learn concept weights, and also adding query concepts to relevant documents have each alone shown smaller improvements ( Fig. 8 ). Concept learning does not (and it is not expected to) solve the two main causes of the poor performance stated above. Although concept addition helps documents to learn different ways users might refer them and thereby helps alleviate both the word mismatch problem and poor concept extraction, it has not shown a significant improvement. This is mainly due to the fact that these results were based on testing unseen queries, and the lack of sufficient overlaps in the collection (i.e. use of the same or similar unit X  X oncepts to represent similar documents) has adversely affected evaluation of the main property of our learning strategy X  X etrieval of a document by a query as a result of the document being rein-forced (updated) by another query. However the interesting point here is that they both show increasing trends in performances. As a result of these positive improvements of each component, their combination has shown a significantly better improvement.

Interestingly the performance of the system has increased when only the keyword-learning component is used ( Fig. 10 ) despite the fact that keyword matches at each testing session were the same in this case. It is the same set of queries that were tested on the same collection and no concepts (and hence no keywords) are added to the documents during training. This is solely a result of keyword weight learning only. Our learn-ing strategy seems to have assigned higher weights to the keywords that were significant at least in terms of the document distinguishing power.

Finally, the performance curve of the system in its full capacity ( Fig. 8 ) shows that taking keyword match-ing into account helps improve performance. The reasons for this improvement are that (1) keywords help initial picking up documents for reinforcing, and (2) keyword matches that take place in the absence of unit X  X oncept matches help in increasing the similarity scores (RSVs) of documents. This helps in ranking a document with more features common with the query above those with fewer common features. The second point is valid only if more keyword matches occur with relevant documents than with non-relevant documents. This is a well-known observation first made by W.B. Croft. Although no experiments were tar-geted at examining the validity of this feature, the performance improvements shown by the system with key-word matching confirm its validity. For instance, if more keyword matches occurred with non-relevant documents, those documents are pushed up in the rank list and as a result the performance of the system should have been degraded.

Fig. 9 and 10 compare the performance of the system with its different learning components when only con-cept matching and keyword matching (respectively) are considered.

They both show that combining all three learning components give better performance results for both con-cept matching (only) and keyword matching (only). This result further confirms that the better performance shown by the system in its full capacity (in Fig. 6 ) is a combined result of all learning components on both concept matching and keyword matching but not a result of a subset of them.

However, note that not all the keyword matches that take place in the case of testing for  X  X  X eyword match-ing only X  X  become keyword matches when testing for the case with  X  X  X oth keyword and concept matches X  X . Because the keywords that participate in concept matches are considered duplicates and are pruned out, the test results of the case with  X  X  X oth keyword and concept matching X  X  are not the same as the sum of the two cases  X  X  X oncept matching only X  X  and  X  X  X eyword matching only X  X . 10. Performance dynamics over training iterations
The following figure ( Fig. 11 ) shows the average precisions at the different stages of training. There is a sharp increase in performance at the first iteration followed by smaller increases in the next few iterations. This is because the maximum learning takes place at the very first time a query is shown. This is when the concepts of queries are added to the relevant documents. The addition of query concepts at early iterations (retrieval sessions) trigger other similar queries (queries sharing the same unit X  X oncepts) to retrieve those reinforced doc-uments at subsequent iterations.

Since we repeated the same set of queries over the iterations in our simulations, no concept additions take place at the later iterations. This is because the set of queries has already been seen by the system. Only the weights of the concepts are tuned at the later iterations. As can be seen in the chart, weight tuning has caused a slight performance drop at the later iterations in the case of unseen query testing. This could be a result of  X  X  X verfitting X  X  X  X he effect that makes systems less robust for novel situations when tuned more than necessary to the training queries.

In contrast to the unseen query testing, the performance of the system on seen query testing does not show a drop in performance over training iterations ( Fig. 12 ). It shows a similar big jump in the first iterations fol-lowed by smaller increases thereafter. This is because all the relevant documents have already been retrieved and updated with the corresponding query concepts in the early iterations. Therefore, the number of concept matches with relevant documents is higher, and as a result those reinforced documents have much higher sim-ilarity scores than the non-relevant ones. As the training proceeds, the similarity scores of relevant documents get higher and higher, making them (the relevant documents) clearly separate from the non-relevant docu-ments. Although this does not help in increasing performance figures (the set of relevant documents always appears at the top in this case), it helps in rearranging the rank order within the relevant documents (and also within the non-relevant documents) by ranking the best-match documents at the top. For example, see the results of testing Query#100 of the Cranfield collection given below in Table 2 . It shows clearly how the sim-ilarity scores of the relevant documents keep increasing, making the block of relevant documents separate from the block of non-relevant documents; and how the order of documents within each block changes as weights are learnt over iterations.

Table 3 shows the results of testing the unseen query  X  X  X uery#1 X  X . This query has not been shown during training. The rank position of certain relevant documents, e.g. Doc#14 and Doc#858, which have started at a low position have moved up to higher positions as a result of concept weight learning despite the fact that no additional concept or keyword matches have taken place between iterations. On the other hand, the rank positions of certain other non-relevant documents, e.g. Doc#878 and Doc#874, have been lowered over the iterations. The documents whose rank positions were lowered over iterations tend to be the ones that have been retrieved by keyword matching only, while the documents whose rank positions were heightened tend to be the ones having at least one good (highly-weighted) unit X  X oncept match (see the keyword and unit X  X on-cept counts). In addition, there are certain other documents, e.g. Doc#843 in Table 2 and Doc#864 in Table 3 , that show mixed behaviour. The rank positions of these documents are increased at certain iterations and decreased at other iterations. These are the ones with concepts that conflict with training queries. Certain que-ries increase their weights while certain other queries decrease them. 11. Conclusions
We have shown firstly a way of using more elaborate and true concepts for creating more meaningful rep-resentations of textual material and their usage for explicit concept matching; secondly, a radically different approach of using concept lattices in IR and its feasibility; thirdly, the importance of an interactive learning strategy and the effectiveness of retaining the learnt knowledge for future use; and finally, the advantage of using a hybrid approach that takes into account both concept matching and keyword matching together with concept addition and weight learning for document retrieval.
 One of the main characteristics of our system is that it becomes more and more tuned to its environment.
Such a system demands consistency in its training examples in terms of the use of vocabulary in query formu-lation and relevance assessments. Consistency is maximized in a single user environment. Essentially, this makes the system customized to its only user, making it highly personalized. On the other hand, learning in a multi-user environment helps learning more exhaustive and better-generalized representations. In partic-ular, it helps the system to learn the different ways in which an information need is formulated by different users with different vocabularies targeting at the same set of documents. However, the consistency between the users in making relevance assessments is essential for convergence. In an environment with more inconsis-tent users, the system dynamics as well as retrieval performances may vary rapidly in time. As a result, a given user may not be guaranteed the same relevant document for the same query issued at a later attempt. These observations suggest that the system is likely to perform better in both more personalized (single user) envi-ronments as well as in multi-user environments with consistent users. Indeed, it has the potential to outper-form conventional keyword based systems in such environments.

In conclusion, this work is a first step towards making use of more elegant concepts, as may be the case with the human brain, and allowing users to decide the significances of concepts in the documents. The difficulty of automatic concept extraction from text and the lack of sufficient background information in documents for building more complete concept hierarchies are the major drawbacks that limited our investigation of the full potential of concept matching. However, the performance results are impressive and encouraging. Overall, this work shows a way of performing true concept matching using concept lattices in a different and more advan-tageous way to that of past concept lattice-based approaches. We are optimistic of the potential of the FCA framework to deliver better performance given that more complete and meaningful document/query represen-tations are created through the incorporation of background knowledge and extraction of more meaningful concepts from text using future advancements of natural language processing technology.
 References
