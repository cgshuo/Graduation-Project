 It is well known that tail queries contribute to a substantial fraction of distinct queries submitted to search engines and thus become a major battle field for search engines. Unfor-tunately, compared with popular queries, it is much more difficult to obtain good search results for tail queries due to the lack of important relevance signals, such as user clicks, phrase matches and so on. In this paper, we propose to utilize the similarities between different queries to overcome the data sparsity problem for tail queries. Specifically, we propose to jointly learn query similarities and the ranking function from data so that the relevance signals of different but related queries can be collaboratively pooled to enhance the ranking of tail queries. We emphasize that the joint optimization is critical so that the learned query similarity function can adapt to the problem of learning ranking func-tions. Our proposed method is evaluated on two data sets and the results show that our method improves the relevance of tail queries over several baseline alternatives. H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Retrieval functions ; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation Collaborative Ranking, Learning to Rank, Relevance, Tail Query, Gradient Boosting
It is well known that the query frequency follows a power law distribution  X  most distinct queries are only submitted for a very few times. These queries are usually called tail queries since they correspond to the tail of the power law distribution. At the other end of the distribution are those head queries which are very popular and submitted to search engines by many users.

Although tail queries only contribute to a relatively small fraction of the total traffic to search engines, the relevance of these queries, to a large extent, determines the user ex-periences. This is because almost all of today X  X  commer-cial search engines can generally respond to head queries with excellent results. On the other hand, even though tail queries tend to represent the detailed and specific informa-tion needs for a particular user, the performance of many search engines is still lacking in serving good results for tail queries, and it has been argued by some that this is one of the reasons that users switch search engines [13]. Con-sequently, it is the capability of providing good results for tail queries that distinguishes different search engines and in this specific sense, the relevance of tail queries is in fact the battle field of search engines today, making it extremely important to improve the performance for tail queries.
Unfortunately, improving the relevance of tail queries are much more difficult than head queries. We argue that the difficulty of tail queries is largely caused by the inherent data sparsity problem. In this paper, we analyze the issues that limit the relevance of tail queries and address the problem of improving the relevance of tail queries. In particular, we propose a method, Collaborative Ranking , to overcome the data sparsity problem of tail queries. Specifically, we utilize the data from multiple queries in a collaborative manner. This idea allows the problem to be formulated in a princi-pled way as a joint supervised learning problem, where the similarity measure between queries and the ranking function to determine the relevance between queries and documents are constructed simultaneously. The evaluations conducted on public domain data sets as well as a tail query data set from a commercial search engine show that the proposed method significantly improves the relevance for tail queries.
Tail Queries. Several studies have been focused on ap-plications related to tail queries [3,8,10,11]. Moreover, eval-uation results of search engines suggest that tail queries con-tribute to the major differences among search engines, while the results of head queries tend to be the same [13]. These studies show that it is vital and useful to improve the rele-vance of tail queries. Figure 1: Query frequency with respect to the num ber of distinct queries Figure 2: Query length with respect to differ-en t levels of query frequency
Query Similarity. It is an important research direction to obtain similarity estimations for queries that accurately reflect the degree of similarities in information needs of users. There are generally two different approaches to obtain query similarity: term based similarities [5,7,12] and user behavior based similarities [1,2,11]. The recent work [9] addresses the problem of learning the linear similarity function of queries in the context of query reformulation. In this work, our goal is not to modify the original queries. On the other hand, we propose to learn the similarity function to overcome the sparsity problem for tail queries based on the gradient boost-ing framework.

Learning to Rank. The ranking problem is frequently formulated as a supervised machine learning problem [4, 6, 14]. However, existing methods on learning to rank largely ignore the specific issues of tail queries and thus can not provide accurate results for tail queries when the data are sparse.
We investigate a large scale sample of query log from a commercial Web search engine and calculate various statis-tics of the queries. In Figure 1, we plot the number of queries with different levels of frequency. It can be observed that a large fraction of queries occurs only a few times. For exam-ple, nearly 70% of the query occurs only once to the search engine. Another characteristic of tail queries is that they usually carry more specific and detailed information needs from users. In Figure 2, we plot the average number of terms in queries with respect to different frequency levels. It is clear that the lengths of tail queries are generally longer than those of head queries and carry more specific informa-tion needs from users.

As a result, the quality of tail queries largely impact the users experiences and choices of search engines. Tail queries represent a large amount of diverse and detailed information needs that are very specific to users. Thus,the results of these queries distinguish search engines from each other.
Unfortunately, the relevance of tail queries is relatively worse than head queries [3]. Specifically, tail queries, by definition, are the queries submitted to search engines very infrequently. Therefore, several problems directly associated with the data sparsity may impact the relevance of these tail queries:
Lacking User Behavior Data. Almost all successful web search engines nowadays rely heavily on user behavior data such as user click-through data. However, they become less useful for tail queries since it is difficult to accurately in-fer the relevance of a document without collecting sufficient many user behavior data. Thus, these strong signals for head queries are weakened or even missing for tail queries, which makes tail queries very difficult for ranking.
Term Mismatch. Clearly, if a document matches the query exactly, it is likely that the document is relevant to the query. However, it is common that the same information need can be expressed in multiple ways. Therefore, if the information needs are expressed in less frequently used ways, it can become very difficult to find exact matches for the query, which making it difficult the predict the relevances of documents to the query.

In summary, the relevance for tail queries is relatively worse compared with that for head queries. This is a quite unsatisfactory situation considering the importance of tail queries to user experiences of web search engines.
In this section, we describe CollRank , the proposed col-laborative ranking framework, in detail. The basic idea is to make use of similar queries to improve the ranking of tail queries. When computing the relevance score f ( q, d ) between a tail query q and a document d , we take a few other queries q  X  into account rather than consider q and d alone. Specifically, we express the ranking function f ( q, d ) as follows: where h ( q, d ) is a scoring function that depends on a single query-document pair ( q, d ) and s ( q, q  X  ) represents the simi-larity between query q and q  X  . Assume that we can extract a feature vector x qd , x qq  X  for each query-document pair ( q, d ) and each query pair ( q, q  X  ), the ranking function f ( q, d ) de-scribed above can expressed as follows: where S q is the set of candidate queries for the original tail query q . In the above expression of the ranking function f ( q, d ), the first term h ( q, d ) represents the degree of match between the original tail query q and the document d . On the other hand, the second term captures the contribution from a set S q of other queries, weighted by their similar-ity s ( q, q  X  ) to the original query q  X  . Intuitively, consider a tail query q , it can be quite difficult for the feature vec-tor x qd to capture the relevance between q and d since the click-through data associated with the query q is too sparse. Hence, it is difficult to obtain a good ranking function based only on the feature vector x qd . However, if we can obtain h ( q  X  , d ) for some similar queries q  X  which has a lot of click-through data so that the relevance of the document d with respect to query q  X  can be estimated accurately, we can over-come the sparsity problem by utilizing the relevance scores h ( q  X  , d ) as well as the similarity between q and q  X  .
It turns out that we can estimate h ( x qd ) and s ( x qq  X  similar manner using gradient boosting [14]. To this end, we apply the squared error loss L ( y qd , f ( q, d )) = ( y for example, to measure the degree of divergence between the training data and the model estimation. Thus, the ob-jective function for the learning process can be expressed by minimizing the empirical risk on training data: which is equivalent to R reg ( h, s ) = X
The learning process for the pair-wise loss are quite simi-lar, which implies the following loss function
L ( f ( x qd ) , f ( x qd  X  )) = max(0 , f ( x qd  X  )  X  f ( x for all pairs of documents associated to the same query such that y qd &gt; y qd  X  [14].
To optimize the objective functions described above, we can minimize the empirical risk R ( h, s ) to obtain the two functions h ( x qd ) and s ( x qq  X  ) required to calculate the rele-vant scores. For the sake of concreteness, we describe the al-gorithm to optimize the objective function defined Equation (1) in detail. The pairwise loss function can be optimized in a similar approach. To this end, we apply the idea of alter-nating minimization. Specifically, we perform the following two steps alternatively: The empirical risk is ensured to decrease as we iterates the above two steps alternatively. Thus, we can obtain an esti-mation of score function h and similarity function s that is a minimal point of R . We apply gradient boosting to estimate both functions in this study.
The proposed framework models two functions h ( x qd ) and s ( x qq  X  ) depending on two different sets of features: query-document features and query-similarity features . The query-document features x qd capture the relevance between a pair of query q and document d . These features are widely ap-plied in today X  X  search engines to determine the relevance of documents.

In order to construct the similarity function s ( x qq  X  ) in our framework, we also need to extract query-similarity features for each pair of queries. These features capture the degree of similarity between two queries. To this end, we construct a high-dimensional feature vector for each pair of queries and other features can be included naturally to improve the performance. In general, we avoid features based on user behavior which are very rare for tail queries. The extracted features can be categorized into three types: string match-ing, term matching and semantic matching.
We make use of the normalized discount cumulative gain (NDCG)to evaluate the performance of the proposed method, which is defined as follows: w here r i is the grade assigned to the i -th document of the ranking list. The term log( i +1) takes the position of ranking list into account by assigning larger weight to the top of the ranking list. The constant Z n is chosen so that the perfect ranking gives an NDCG value of 1. We first describe experiments conducted over the Microsoft ate and verify that the proposed framework can utilize sim-ilar queries to enhance the quality of the learned ranking functions. There are two data sets Web10K and Web30K containing 10,000 and 30,000 queries respectively.
In order to evaluate the proposed framework, we need to extract features for a given pair of queries. However, the terms in queries are not available for this data set. Thus, we preprocess the data set as follows to simulate features for measuring query similarities: 1) We average the query-document features for all documents associated with each query and use the averaged features as the query features x for the corresponding query. 2) For each of the original query q , we generate a set of similar queries q  X  . In order to simu-late similar queries q  X  , we generate a random vector p q  X  sampling each component uniformly from [0 , 1]. The query-document features x q  X  d is simulated by multiply each com-ponent of the random vector p q  X  and the query-document features x qd for the original query. 3) The query-similarity features x qq  X  are generated by multiply each component of x
We sample 1000 queries from Web10K and Web30K as our training data. The performance is measured by the av-eraged NDCG over five independent samples. We compared CollRank to two baselines described as follows: GBTree: The ranking function is trained with gradient boosting using only the original queries and documents. AvgFeature: In this baseline, we extend the feature space by utilizing similar queries. We first average the feature vectors x q  X  d of similar queries and then append the averaged feature vector to the original feature vector x qd . For each of the above method, gradient boosting is applied to train models using the ex-tended feature vectors. We apply two different loss functions to learn the ranking function. The methods with squared and pair-wise loss functions are labeled with suffix Reg and Pair , respectively. For example, the method CollRank-Reg http://research.microsoft.com / en_us/projects/ mslr/ represents the Co llRank method with the squared loss de-fined in Equation (1). The CollRank-Pair represents the CollRank method with the pair-wise loss function defined in Equation (2).
 From Figure 3, we can see that CollRank outperforms GB-Tree on both data sets, which indicates that CollRank can make use of similarity queries to enhance the ranking func-tion. Moreover, AvgFeature outperforms GBTree which sug-gests that extending feature space can be quite useful to improve the quality of ranking. We can also observe that the pairwise loss function performs better than squared loss, which is expected because the pairwise loss function empha-sises on the relative orders of the documents rather then their absolute scores.
The data set is collected by a commercial search engine by sampling a number of queries from the query log. About 2000 features are used for each query-document pair. Five-level relevance judgements are associated with the query-document pairs. These judgments are utilized to train and evaluate the quality of the learned ranking functions. In order to evaluate the proposed method, we randomly split the data into training and test sets according to queries. For better analysis of the results, we further partition the test set into two subsets Test1 and Test2 according to the frequency of queries in query log. In particular, the set Test2 contains queries that are observed for the first time by the search engine.

Similar Query Set Generation. In order to construct a similar query set S q of queries for each query q , we first look up its documents in the click-through data and aggregate queries associated with these documents. We generate the set S q for a given query q by randomly sampling k different queries from this set of queries. In general, the size of the candidate set k is set to be 5 in our experiments.
Baselines. We compare the proposed method with the following baselines obtained by gradient boosting trees which is the state-of-the-art method for learning to rank: NoClick. In this method, we ignore the click-through data for all queries in this method and construct ranking functions with gradient boosting trees. ClickStream. In this method, all the clicked queries associated with a documents are aggre-gated and viewed as meta text contents of the documents called click stream. All content based features can be applied to this click stream. Therefore, in this method, the features extracted from the click stream is used together with other features in NoClick to construct ranking functions with gra-dient boosting trees. SimLabel. A straight forward method to utilize similar queries is to make use of them to expand training set. Specifically, assume that we have the labeled data ( q, d, y qd ) in the training set. We can add a new train-ing data ( q  X  , d, y qd ) if the query q and q  X  are quite similar to each other. Then, we include new training data in the training set and construct ranking function with gradient boosting trees. In this method, we measure the similarity between queries by the cosine similarity of the query feature vector and add a new training example when the similarity is greater than 0 . 7. SimFeature. In this baseline, we ag-gregate features from similar queries and append it to the original features of the query-document pairs. Specifically, the aggregated features are obtained by the weighted av-erage of feature vectors corresponds to similar queries and number of similar queries is set to be 5 in our evaluations.
For each of the above method, we apply two different loss functions to learn the ranking function. The methods with squared and pair-wise loss functions are labeled with suffix Reg and Pair , respectively.

We construct ranking functions with all the five methods on the training set and evaluate the obtained ranking func-tions on the test sets.

First, we can observe that ClickStream outperforms NoClick consistently, which indicates that click-through data can be quite important to improve the relevance of rankings. In fact, ClickStream extracts query-document features from the aggregated click logs, which is a common practice in Web search engines. In SimFeature , SimLabel and CollRank , query-document features extracted from click streams are also used. Comparing the performance over Test1 and Test2 , we can observe that the performance measured by NDCG is lower on Test2 than on Test1 . This is expected since Test2 contains queries that occur only once to the search engine. Thus, the problems with tail queries are much more signifi-cant over this test set than Test1 .

We can observe that CollRank with squared and pair-wise loss functions outperform the corresponding baselines. Moreover, the improvements are significant according to t -test over queries with significance level p = 0 . 05, which indi-cates that CollRank can better utilize the information from similar queries to improve the relevance of ranking.
In addition, we can see SimFeature actually performs quite well: it outperforms the other three baselines. We think this is because that the quality of features is the most important factor that impacts the performance of tail queries as we an-alyzed in Section 3. By making use of features from similar queries, SimFeature can overcome the sparsity problem asso-ciated with tail queries and thus improve the performance. From this viewpoint, the proposed CollRank also utilizes the features from similar queries to calculate the ranking scores of a given query-document pair, which partially explains the reason why CollRank can improve the relevance of tail queries. On the other hand, SimLabel does not perform as well as other methods. One reason is that SimLabel makes clear-cut decision of adding extra data rather than a soft-weighting, and the labeled data introduced by SimLabel can be quite noisy and thus reduce the performance of the ob-tained ranking functions.
In this work, we address the problem of improving the relevance of tail queries and propose CollRank , a framework that utilizes the similarities between queries to overcome the data sparsity problem associated with tail queries. In partic-ular, the similarity function between queries and the ranking function are estimated jointly from the training data so that they can adapt to each other well based on the given train-ing data. An alternating minimization process is applied to optimize the objective functions. Extensive evaluations con-ducted over two data sets show that the proposed method can improve the relevance of ranking by making use of sim-ilar queries.

For future work, we plan to investigate other information (b) Performance on Te st1 with pair-wise loss (d) Performance on Te st2 with pair-wise loss sources to further improve the ranking of tail queries. For example, we can incorporate open knowledge base such as Wikipedia into our framework. Part of the work is supported by NSF IIS-1116886, NSF IIS-1049694, NSFC 61129001/F010403 and the 111 project.
