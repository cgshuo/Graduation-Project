 regression model affords many practical advantages, such a s the ability to set rejection thresholds [3], to accommodate unequal relative class frequencies in t he training set and in operation [4], or to apply an appropriate loss matrix in making predictions th at minimise the expected risk [5]. As a result, these models have been adopted in a diverse range of applications, including cancer clas-recently, the focus of research has been on methods for induc ing sparsity in (multinomial) logistic methods are used to select a small number of basis functions t o form a compact non-parametric clas-data.
 A variety of methods have been explored that aim to introduce sparsity in non-parametric regression the context of least-squares regression using Radial Basis Function (RBF) networks, Orr [10], pro-regularisation parameters for each weight. The optimisati on of the Generalised Cross-Validation (GCV) score typically leads to the regularisation paramete rs for redundant basis functions achiev-ing very high values, allowing them to be identified and prune d from the network (c.f. [11, 12]). The computational efficiency of this approach can be further improved via the use of Recursive Or-thogonal Least Squares (ROLS). The relevance vector machin e (RVM) [13] implements a form of Bayesian automatic relevance determination (ARD), using a separable Gaussian prior. In this case, the regularisation parameter for each weight is adjusted so as to maximise the marginal likelihood, also known as the Bayesian evidence for the model. An efficient component-wise training algorit hm is given in [14]. An alternative approach, known as the LASSO [15], seeks to minimise the negative log-likelihood of the sample, subject to an upper bound on th e sum of the absolute value of the Laplace prior over the model parameters [17], which has been demonstrated to control over-fitting and induce sparsity in the weights of multi-layer perceptro n networks [18]. The equivalence of the Laplace prior and a separable Gaussian prior (with appropri ate choice of regularisation parameters) has been established by Grandvalet [11, 12], unifying these strands of research.
 In this paper, we demonstrate that, in the case of the Laplace prior, the regularisation parameters can be integrated out analytically, obviating the need for a lengthy cross-validation based model selection stage. The resulting sparse multinomial logisti c regression algorithm with Bayesian regu-mainder of this paper is set out as follows: The sparse multin omial logistic regression procedure with Bayesian regularisation is presented in Section 2. The proposed algorithm is then evaluated against competing approaches over a range of benchmark lear ning problems in Section 3. Finally, the work is summarised in Section 5 and conclusion drawn. Let D = { ( x n , t n ) }  X  features for the i th example, and t n  X  X  = { t | t  X  X  0 , 1 } c , k t k a generalised linear model [1] with a softmax inverse link function [19], allowing the outputs to be interpreted as a-posteriori estimates of the probabilities of class membership, Assuming that D represents an i.i.d. sample from a conditional multinomial distribution, then the negative log-likelihood, used as a measure of the data-misfi t, can be written as, The parameters, w of the multinomial logistic regression model are given by th e minimiser of a penalised maximum-likelihood training criterion, L , the partial derivatives of L with respect to the model parameters will be uniformly zero, giving w input feature can be pruned from the model. 2.1 Eliminating the Regularisation Parameters the parameters of the model given by (1), can be written as L model parameters, w , is then given by a separable Laplace distribution where W is the number of active (non-zero) model parameters. A good v alue for the regularisation parameter  X  can be estimated, within a Bayesian framework, by maximisin g the evidence [22] or the prior distribution over model parameters is given by mar ginalising over  X  , As  X  is a scale parameter, an appropriate ignorance prior is give n by the improper Jeffrey X  X  prior,  X  is strictly positive, Using the Gamma integral, R  X  fication, see [17]. Note that we integrate out the regularisa tion parameter and optimise the model parameters, which is unusual in that most Bayesian approach es, such as the relevance vector ma-chine [13] optimise the regularisation parameters and inte grate over the weights. 2.1.1 Practical Implementation The training criterion incorporating a fully Bayesian regu larisation term can be minimised via a simple modification of existing cyclic co-ordinate descent algorithms for sparse regression using a respectively, we have that where From a gradient descent perspective, minimising M effectively becomes equivalent to minimising L , assuming that the regularisation parameter,  X  , is continuously updated according to (5) following every change in the vector of model parameters, w [17]. This requires only a very minor modifica-need for a model selection procedure in fitting the model. 2.1.2 Equivalence of Marginalisation and Optimisation und er the Evidence Framework rameter analytically is equivalent to its optimisation und er the evidence framework of MacKay [22]. The argument provided by Williams can be summarised as follo ws: The evidence framework sets the value of the regularisation parameter so as to optimise t he marginal likelihood, also known as the evidence for the model. The Bayesian interpretation of the regularis ed objective function gives, where Z Z is redundant. Unfortunately this integral is analytically intractable, and so we adopt the Laplace approximation, corresponding to a Gaussian posterior dist ribution for the model parameters, centred on their most probable value, w MP , where A =  X  X  X  L is the Hessian of the regularised objective function. The re gulariser corresponding A =  X  X  X  E D . The negative logarithm of the evidence can then be written a s, the regularisation parameter, which is equivalent to the update rule obtained using the int egrate-out approach. Maximising the evidence for the model also provides a convenient means for m odel selection. Using the Laplace approximation, evidence for a multinomial logistic regres sion model under the proposed Bayesian regularisation scheme is given by where A =  X  X  X  L . 2.2 A Simple but Efficient Training Algorithm In this study, we adopt a simplified version of the efficient co mponent-wise training algorithm of Shevade and Keerthi [25], adapted for multinomial, rather t han binomial, logistic regression. The principal advantage of a component-wise optimisation algo rithm is that the Hessian matrix is not first partial derivatives of the data mis-fit term are given by , and  X  Similarly, the second partial derivatives are given by, The Laplace regulariser is locally a hyperplane, with the ma gnitude of the gradient given by the regularisation parameter,  X  , effective gradient of the regularised loss function as follows: Note that the value of a weight may be stable at zero if the deri vative of the regularisation term dominates the derivative of the data misfit. The parameters o f the model may then be optimised, using Newton X  X  method, i.e. Any step that causes a change of sign in a model parameter is tr uncated and that parameter set to step. In this study, we adopt the heuristic chosen by Shevade and Keerthi, in which the parameter An optimisation strategy based on scaled conjugate gradien t descent [27] has also be found to be effective. The proposed sparse multinomial logistic regression metho d incorporating Bayesian regularisation the regularisation parameter using a simple line search (SM LR). Table 1 shows the test error rate and cross-entropy statistics for SMLR and SBMLR methods ove r these datasets. Clearly, there is that the Bayesian regularisation scheme results in models w ith a slightly higher degree of sparsity (i.e. the proportion of weights pruned from the model). Howe ver, the most striking aspect of the comparison is that the Bayesian regularisation scheme is ty pically around two orders of magnitude faster than the cross-validation based approach, with SBML R being approximately five times faster in the worst case ( COVTYPE ). 3.1 The Value of Probabilistic Classification membership, can be used in minimum risk classification, usin g an appropriate loss matrix to account benchmark datasets. The best results for each statistic are shown in bold. The final column shows the logarithm of the ratio of the training times for the SMLR a nd SBMLR, such that a value of 2 would indicate that SBMLR is 100 times faster than SMLR for a g iven benchmark dataset. Benchmark Covtype 0.4051 0.4041 0.9590 0.9733 0.4312 0.3069 0.6965 Crabs 0.0350 0.0500 0.1075 0.0891 0.2708 0.0635 2.7949 Glass 0.3318 0.3224 0.9398 0.9912 0.4400 0.4700 1.9445 Iris 0.0267 0.0267 0.0792 0.0867 0.4067 0.4067 1.9802 Isolet 0.0475 0.0513 0.1858 0.2641 0.9311 0.8598 1.3110 Satimage 0.1610 0.1600 0.3717 0.3708 0.3694 0.2747 1.3083 Viruses 0.0328 0.0328 0.1670 0.1168 0.8118 0.7632 2.1118 Waveform 0.1290 0.1302 0.3124 0.3131 0.3712 0.3939 1.8133
Wine 0.0225 0.0281 0.0827 0.0825 0.6071 0.5524 2.5541 a probabilistic classifier can be adjusted after training to compensate for a difference between the a simple expectation-maximisation (EM) based procedure fo r estimating unknown operational a-priori probabilities from the output of a probabilistic classifier (c.f. [28]). Let p a-priori probability of class C a-priori probabilities, p beginning with p (0) this procedure. The adjusted estimates of a-posteriori probability are then given by the first part of equation (6). The training and validation sets of the COVTYPE benchmark have been artificially balanced, by random sampling, so that each class is represen ted by the same number of examples. marise the results obtained using the raw and corrected outp uts of a linear SBMLR model on this methods, for example the support vector machine (note the sa me procedure could be applied to the SMLR model with similar results).
 Table 2: Error rate and average cross-entropy score for linear SBMLR models of the COVTYPE benchmark, using the raw and cor-rected outputs.

Cross-Entropy 0.9590 0.6567 emerges as the marginal of a scale-mixture-of-Gaussians wh ere the corresponding prior is an Expo-nential such that where E hierarchical representation of the Laplace prior is utiliz ed to develop an EM style sparse binomial distribution in the above integral. This yields an improper parameter free prior distribution over w which removes the explicit requirement to perform any cross -validation. However, the method its use on moderately high-dimensional problems.
 Likewise in [13] the RVM employs a similar scale-mixture for the prior where now the Exponential No attempt is made to estimate the associated hyper-paramet ers and these are typically set to zero A similar multinomial logistic regression model to the one p roposed in this paper is employed in In this paper we have demonstrated that the regularisation p arameter used in sparse multinomial lo-at a greatly reduced computational expense. It is interesti ng to note that the SBMLR implements a over the hyper-parameter and optimises the weights, rather than marginalising the model parameters and optimising the hyper-parameters. It seems reasonable t o suggest that this approach is feasible weights are strongly determined by the data misfit term. A sim ilar strategy has already proved ef-and we plan to extend this work to multi-class cancer classifi cation in the near future. The authors thank the anonymous reviewers for their helpful and constructive comments. MG is supported by EPSRC grant EP/C010620/1.
 References
