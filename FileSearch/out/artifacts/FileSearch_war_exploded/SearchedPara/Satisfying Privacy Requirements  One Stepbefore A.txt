 The problem of privacy-pr eserving data publishing ha s received a lot of attention in recent years. Privacy preservation on tabular data has been studied exten-sively. A major category of privacy attacks on relational data is to re-identify individuals by joining a published table containing sensitive information with some external tables. Most of existing work can be formulated in the follow-ing context: several organizations publish detailed data about individuals (e.g. medical records) for research or sta tistical purposes [14,10,9,13].

Privacy risks of publishing microdata are well-known. Famous attacks include de-anonymisation of the Massachusetts hospital discharge database by joining it with a public voter database [14] and privacy breaches caused by AOL search data [5]. Even if identifiers such as names and social security numbers have been removed, the adversary can use linking [12], homogeneity and background attacks [10] to re-identify individual dat a records or sensitive information of in-dividuals. To overcome the re-identification attacks, k -anonymity was proposed [12,14]. Specifically, a data set is said to be k -anonymous ( k  X  1) if, on the quasi-identifier attributes, each reco rd is identical with at least k  X  1 other records. The larger the value of k , the better the privacy is protected. Several algo-rithms are proposed to enforce this principle [2,6,7,8]. Machanavajjhala et al. [10] showed that a k -anonymous table may lack of diversity in the sensitive at-tributes. To overcome this weakness, they propose the l -diversity [10]. However, even l -diversity is insufficient to prevent attribute disclosure due to the skewness and the similarity attack. To amend this problem, t -closeness [9] was proposed to solve the attribute disclosure vulnerabilities inherent to previous models.
Recently, a new privacy concern has emerge d in privacy preservation research: how to protect individuals X  privacy in large survey rating data. Though several models and many algorithms have been proposed to preserve privacy in relational data (e.g., k -anonymity [12,14], l -diversity [10], t -closeness [9], etc.), most of the existing studies are incapable of handling rating data, since the survey rating data normally does not have a fixed set of personal identifiable attributes as relational data, and it is characterized by high dimensionality and sparseness. The survey rating data shares the similar format with transactional data. The privacy preserving research of transac tional data has recently been acknowledged as an important problem in the data mining literature [3,15,16]. To our best knowledge, there is no current research addressing the issue of how to efficiently determine whether the survey rating data satisfies the privacy requirement. On October 2, 2006, Netflix, the world X  X  largest online DVD rental service, an-nounced the $1-million Netflix Prize to improve their movie recommendation service [4]. To aid contestants, Netflix publicly released a data set containing 100,480,507 movie ratings, created by 480,189 Netflix subscribers between De-cember 1999 and December 2005. Narayanan and Shmatikov shown in their recent work [11] that an attacker only need s a little information to identify the anonymized movie rating transaction of the individual. They re-identified Netflix movie ratings using the Internet Movie Database (IMDb) as a source of auxil-iary information and successfully identified the Netflix records of known users, uncovering their political preferences and other potentially sensitive information.
We consider the privacy risk in publishing survey rating data. For example, in a life style survey, ratings to some issues are non-sensitive, such as the likeness of a book. Ratings to some issues are sensitive, such as the income level. Assume that each survey participant is cautious about his/her privacy and does not reveal his/her ratings. However, it is easy to find his/her preferences on non-sensitive issues from publicly available information sources, such as personal weblog. An attacker can use these preferences to re-identify an individual and consequently find sensitive ratings of a victim. An example is given in the Table 1. In a social network, people make comments on various issues, which are not considered sensitive. Some comments can be summarized as in Table 1(b). We assume that people are aware of their privacy and do not reveal their ratings, either non-sensitive or sensitive ones. However, individuals in the supposedly anonymized survey rating data are potentially identifiable based on their public comments from other sources. For exampl e, Alice is at risk of being identified, since the attacker knows Alice X  X  preference on issue 1 is  X  X xcellent X , by cross-checking Table 1(a) and (b), s/he will deduce that t 1 in Table 1(a) is linked to Alice, the sensitive rating on issue 4 of Alice will be disclosed. This example motivates us the following research question: Given a survey rating data set T with the privacy requirements, how to efficiently determine whether T satisfies the given privacy requirements? We assume that a survey rating data set publishes people X  X  ratings on a range of issues. Each survey participant is cautious about his/her privacy and does not reveal his/her ratings. However, an attacker may find a victim X  X  preference (not exact rating scores) by personal familiarity or by reading the victim X  X  comments on some issues from personal weblog. We consider that attackers know prefer-ences of non-sensitive issues of a victim but do not know exact ratings and want to find out the victim X  X  ratings on some sensitive issues.

The auxiliary information of an attacker includes: (i) the knowledge of a victim being in the survey rating data; (ii) preferences of the victims on some non-sensitive issues. The attacker wants to find ratings on sensitive issues of the victim. In practice, knowledge of Typ es (i) and (ii) can be gleaned from an external database [11]. For example, in the context of Table 1(b), an external database may be the IMDb. By examining the anonymized data in Table 1(a), the adversary can identify a small number of candidate groups that contain the record of the victim. It will be the unfortunate scenario where there is only one record in the candidate group. For example, since t 1 is unique in Table 1(a), Alice is at risk of being identified. If the candidate group contains not only the victim but other records, an adversary may use this group to infer the sensitive value of the victim. For example, although it is difficult to identify whether t 2 or t 3 in Table 1(a) belongs to Bob, since both records have the same sensitive value, Bob X  X  private information is identified.
In order to avoid such attack, we propose a ( k, , l )-anonymity model. The first step is to require that in the released data, every transaction should be similar with at least k  X  1 other records based on the non -sensitive ratings so that no survey participants are identifiable. For example, t 1 in Table 1(a) is unique, and based on the preference of Alice in Table 1(b), her sensitive issues can be re-identified. Jack X  X  sensitive issues, on the other hand, is much safer. Since t 4 and t in Table 1(a) form a similar group based on their non-sensitive rating. Second is to prevent the sensitive rating from being inferred by requiring the sensitive ratings in a similar group be diverse. For instance, although t 2 and t 3 in Table 1(a) form a similar group, their sensitive ratings are identical. Therefore, an attacker can immediately infer Bob X  X  preference on the sensitive issue without identifying which transaction belongs to Bob. In contrast, Jack X  X  preference on the sensitive issue is much safer than both Alice and Bob.

Given a rating data set T , each transaction contains a set of numbers indicate the ratings on some issues. Let ( o 1 ,  X  X  X  ,o p ,s 1 ,  X  X  X  ,s q )beatransaction, o i  X  { 1: r, null } , i =1 , 2 ,  X  X  X  ,p and s j  X  X  1: r, null } , j =1 , 2 ,  X  X  X  ,q ,where r is the maximum rating and null indicates that a survey participant did not rate. o ,  X  X  X  ,o p stand for non-sensitive ratings and s 1 ,  X  X  X  ,s q denote sensitive ratings. Let T A = { o A the ratings for participants A and B , then dissimilarity of non-sensitive ratings ( Dis ( o A as follows: Definition 1 ( -proximate). Given a survey rating data set T with a small positive number ,twotransactions T A = { o A T i  X  p , Dis ( o transactions in T are -proximate.
 If two transactions are -proximate, the dissimilarity between their non-sensitive ratings is bound by .
 Definition 2 ( ( k, ) -anonymity). A survey rating data set T is said to be ( k, ) -anonymous if and only if every transaction is -proximate with at least k  X  1 other transactions. The transaction t with all the other transactions that are -proximate with t in T form a ( k, ) -anonymous group.
 Although ( k, )-anonymity can protect identity, it fails to protect sensitive infor-mation. For example, in Table 1(a), t 2 and t 3 are in a (2 , 1)-anonymous group, but they have the same rating on the sensitive issue, thus Bob X  X  private informa-tion is breaching. This example reflects the shortcoming of the ( k, )-anonymity. To mitigate this limitation, sufficient diversity of the sensitive values in each ( k, )-anonymous group should be allowed.

Forasensitiveissue s , let the vector of ratings of the group be ( s 1 ,  X  X  X  ,s g ), where s i  X  X  1: r, null } . The mean of the ratings is  X  s = 1 Q g i =1 s i ,where Q is the number of non-null values, and s i  X  null = s i . The standard deviation of the rating is then defined as SD ( s )= 1 g g i =1 ( s i  X   X  s ) 2 . anonymous if and only if the standard deviation of sensitive ratings is at least l in each ( k, ) -anonymous group. In this section, we formulate the satisfaction problem and develop a slicing tech-nique to determine the following Satisfaction Problem .
 Satisfaction problem: Given a survey rating data T and k, , l , the satisfaction problem of ( k, , l )-anonymity is to decide whether T satisfies k, , l requirements. The satisfaction problem is to determine whether the user X  X  given privacy re-quirement is satisfied by the given data set. If the data set has already met the requirements, it is not necessary to make any modifications before publishing. As follows, we propose a novel slice technique to solve the satisfaction problem.
We illustrate the slicing technique using an example in 3-D space. Given -proximate. Our approach is first to find the -proximate of t ,whichisthesetof transactions that lie inside a cube C t of side 2 centered at t .Since is typically small, the number of points inside the cube is also small. The -proximate of C t can be found by an exhaustive comparison within the -proximate of t .If there are no transactions inside the cube C t , we know the -proximate of t is empty, so as the -proximate of the set C t . The transactions within the cube can be found as follows. First we find the transactions that are sandwiched between a pair of parallel planes X 1 , X 2 and add them to a candidate set . The planes are perpendicular to the first axis of coordinate frame and are located on either side of the transaction t at a distance of . Next, we trim the candidate set by disregarding transactions that are not also sandwiched between the parallel pair of Y 1 and Y 2 , that are perpendicular to X 1 and X 2 . This procedure is repeated for Z 1 and Z 2 at the end of which, the candidate set contains only transactions withinthecubeofsize2 centered at t . Slicing ( , T, t 0 ) (Algorithm 1) describes how to find the -proximate of the set C t Our experimentation deploys the MovieLens data downloadable at http://www.grouplens.org/taxonomy/term/14 , which was made available by the GroupLens Research Project at the University of Minnesota. In the data set, a user is considered as an object while a movie is regarded as an attribute and many entries are empty since a user only rated a small number of movies.
Data used for Fig. 1(a) is generated by re-sampling the Movielens data set while varying the percentage of data from 10% to 100%. We evaluate the running time for the ( k, , l )-anonymity model with default setting k =20 , =1 ,l =2. The execution time for ( k, , l )-anonymity is increasing with the increased data percentage. This is because as the percen tage of data increases, the computation cost increases too, since the overhead i s increased with the more dimensions. Next, we evaluate how the parameters affect the cost of computing. In these experiments, we use the whole MovieLens data and evaluate by varying .With k =20 ,l = 2, Fig. 1(b) shows the computational cost as a function of ,in determining ( k, , l )-anonymity. At the initial stage, when is small, more com-putation efforts are put into finding -proximate of the transactions, but less used in exhaustive search for -proximate of the set, and this explains the initial decent of overall cost. As grows, the searching time for -proximate is reduced, but the number of transactions in the -proximate is increased, which results in huge exhaustive search effort and this causes the eventual cost increase.
In addition to the scalability, we experimented the comparison between the slicing algorithm (Slicing) and the heuris tic pairwise algorithm (Pairwise), which works by computing all the pairwise distances to construct the dissimilarity ma-trix and identify the violation of privacy requirements. We implemented both algorithms and studied the impact of the execution time on the data percentage and . Fig. 2(a) describe the trend of the algorithms by varying the percentage of the data set. From the graph, the slicing algorithm is far more efficient than the heuristic pairwise algor ithm especially when the v olume of the data becomes larger. This is because, when the dimension of the data increases, the disadvan-tage of the heuristic pairwise algorithm, which is to compute all the dissimilarity distance, dominates the execution time. On the other hand, the smarter group-ing technique used in the slicing process makes less computation cost for the slicing algorithm. The similar trend is shown in Fig. 2(b) by varying . We have studied the problem of protecting individuals X  sensitive ratings in the large survey rating data. We proposed a novel ( k, , l )-anonymity model and studied the satisfaction problem. A novel slicing technique was proposed to solve the satisfaction problem by searching closest neighbors in large, sparse and high dimensional survey rating data. The experimental results confirm the slicing technique is fast and scalable in practical.
 Thanks for the reviewers X  valuable comme nts. The research is supported by Aus-tralian Research Council (ARC) discovery grants DP0774450 and DP0663414.
