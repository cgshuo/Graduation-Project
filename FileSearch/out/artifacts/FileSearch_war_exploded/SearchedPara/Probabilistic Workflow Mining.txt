 In several organizations, it has become increasingly popu-lar to document and log the steps that makeup a typical business process. In some situations, a normative workflow model of such processes is developed, and it becomes im-portant to know if such a model is actually being followed by analyzing the available activity logs. In other scenario s, no model is available and, with the purpose of evaluating cases or creating new production policies, one is intereste d in learning a workflow representation of such activities. In either case, machine learning tools that can mine workflow models are of great interest and still relatively unexplore d. We present here a probabilistic workflow model and a corre-sponding learning algorithm that runs in polynomial time. We illustrate the algorithm on example data derived from a real world workflow.
 G.3 [ Mathematics of Computing ]: PROBABILITY AND STATISTICS Algorithms Workflow mining, graphical models, causal models
Most large social organizations are complex systems. Every day they perform various types of processes, such as assem-bling a car, designing and implementing software, organizi ng a conference, and so on. A process is a set of tasks to be  X 
This work was carried out while on internship at Clairvoy-ance Corporation accomplished, where every task might have pre-requisites within the process that have to be fulfilled before execution .
For instance, implementing a database query system should not be performed before the necessary data structures are designed. One should not add the doors to a car before the seats are in place. That is, some tasks are essentially se-quential . But it is fair to say that building the speakers of a car bears no implication on the manufacturing of the tires, and vice-versa, i.e., some tasks can be executed in parallel . Moreover, there are tasks that are mutually exclusive : for instance, one has to decide if a given share of coffee harvest is to be exported, or sent to the internal market. Some tasks might also be executed in cycles.

To analyze productivity, identify outliers, cut unneces-sary expenses, and design other production policies, models of work are important, i.e., abstract representations of typ-ical process instances modeling the causal and probabilist ic dependencies among tasks. Such models are based on the concepts of sequential, parallel, iterative (cyclic) and m utu-ally exclusive tasks and are used to evaluate costs, monitor processes, and predict the effect of new policies [7]. For the se reasons, empirically building process models from data is o f great interest. Such a problem has been called process min-ing , or simply workflow mining [8, 3, 4], because the usual representation of work processes is workflow graphs.
In this paper, we describe a probabilistic model for work-flow graphs, and algorithms for learning such graphs from data. The setup is similar to other graphical models. In Sec-tion 2, we introduce a formal description of workflow graphs and the associated generative models. Section 3 describes a data mining algorithm for learning the structure of workflow graphs from data. An empirical study is given in Section 4. Related work is discussed in Section 5.
In this section, we first give a description of the family of graphs that are allowed in our framework. This is followed by a probabilistic parameterization of such graphs. We then describe the role of temporal information in our approach, followed by our treatment of hidden variables and noise. We conclude this section with a concept (called faithfulness ) that links empirically observable constraints to graphs.
For simplicity, in this paper we will work with acyclic graphs only. A future extension of this work will cover the cyclic case.
In a typical process, each task T has pre-requisites , a set of other tasks whose execution will determine the probability of T being executed. A workflow graph G is a directed acyclic graph (DAG) where each task is a node, and the parents of a node are its direct pre-requisites . That is, the decision to execute T does not depend on any (other) task in G given its parents.

Motivated by other workflow representations (see [8] for a review) which are used to model a large variety of real-world processes, we adopt a constrained DAG representation. Let a AND/OR workflow graph (AO graph) be a constrained type of DAG, with any node being in one of the following classes:
We require that an AO graph must have exactly one node that has no parents (a start node ) and exactly one node that has no children (an end node ). Informally, split nodes are meant to represent the points where choices are made (i.e., where one among mutually exclusive tasks will be chosen) or where multiple parallel threads of tasks will be spawned. As a counterpart, join nodes are meant to represent points of synchronization . That is, a join node is a task J that, before allowing the execution of any of its children, waits for the completion of all active threads that have J as an endpoint. This particular property is very specific to workflow graphs, which we call synchronization property .

However, not any split-join pattern is permitted. Every split node T has also to obey the following constraints in an AO graph:
This property is desirable in order to give join nodes the semantics of real synchronization tasks, i.e., join nodes a s tasks that finalize threads started by the most recent split node. It essentially enforces nesting of threads. A case where this assumption is not respected is illustrated by Figure 1.
These constraints are the most characteristic constraints of workflow graphs adopted in the literature, and provide distinctive features to be explored by workflow mining algo-rithms.
Each task T is an event. It either happens or it does not happen. By an abuse of notation, we will use the same sym-bols to represent binary random variables and task events, Figure 1: This construction is not allowed because T 1 creates another thread that is not nested between the split point that generated { T 1 , T 2 } and its syn-chronization point T 3 . where T = 1 represents the event  X  T happened  X . We define a parametric model for a DAG by the conditional probability of each node given its parents, i.e. by assuming the Markov condition (Spirtes et al., 2000). There is, however, a speci al logical constraint in workflow graphs.

Let an OR-split be a split node that forces a unique choice of task to be executed among its children, i.e., all of its children are mutually exclusive. Any other type of split nod e is called an AND-split 1 . Children of OR-splits will have a special parameterization.
 Let P a T represent the parents of task T in an AO graph G . By another abuse of notation, let P a T also be a random variable representing the joint state of the parents of a tas k T , i.e., P a T = j is a particular combination of binary as-signments to the elements of P a T . In particular, P a T represents the event where all parents of T are assigned the value 0. The basic parametetrization is as follows:
The requirement that P ( T = 1 | P a T = 0) = 0 encodes the modeling assumption that a necessary condition for a task to be executed is that at least one of its parents is executed. We call this property backward determinism , typ-backward determinism will allow us to design an algorithm to learn workflow graphs in polynomial time .
We assume that the data available for our learning algo-rithm is a workflow log [1]. A workflow log consists of records actually behave as XOR operators, while an AND-split is technically an OR choice. We adopt this denomination since it is already widespread in this field. 2 The assumption  X  tj &lt; 1 is not an essential assumption and was introduced here for the purposes of simplifying the presentation. It does capture the common phenomenon that any process can be aborted non-deterministically. of which tasks were performed for which process instances at which starting time. For example, the following log
W orkflowLog = { ( Car 1 , BuildChassis, 09:10 am ), ( Car 2 , BuildDoors, 10:17 am ), ( Car 2 , AddSeats, 10:20 am ), ( Car 1 , Build Doors, 10:47 am ) } contains information concerning two instances ( Car 1 and Car 2 ) going through a series of tasks ( BuildChassis , Build Doors , AddSeats ) starting at differente times .

Workflow logs are by-products of workflow management systems [7]. We assume that our data are workflow logs.
We allow the possibility that non-simple nodes can be hid-den variables (i.e., split or join nodes might not be recorde d at all in the log). However, for identification purposes, we make the following assumptions: 1. no hidden AND-split is a child of a hidden AND-split, 2. no hidden task is both a split and join node; 3. no hidden join is followed by a simple task and no
These assumptions do not restrict the ability of the AO graphs to represent any combination of sequential, paralle l or exclusive patterns that appear in practice. Mathemati-cally, however, they assure that any AO graph can be dis-tinguished from any other AO graph given enough data, as it will be explained in Section 3. Furthermore, we allow the possibility of measurement error . For each task T that is measurable, we account for the possibility that T is not recorded in a particular instance even though T happened. That is, let T M be a binary variable such that T M = 1 if task T is recorded to happen. Then we have the following measurement model:
Note that we assume measurement error happens only in one direction. Although that might not be the case in every application, this greatly simplifies our problem, and will a l-low us to learn the structure of workflow graphs without fitting latent variable models.

In this sense, every task is hidden. However, in this paper, the name  X  X idden task X  will be applied only to tasks that cannot be measured at all. The description of a workflow model as a specialized hidden Markov model will be treated in Section 5. Notice also that for every OR-split T in G , Choice ( T ) is a hidden variable, and will not be explicitly represented in AO graphs, unlike hidden splits and joins.
To identify hidden AND-splits, we need to assume that the immediate observable descendants of a hidden AND-split T (i.e., those that do not have an observable proper ancestor that is a descendant of T ) should not be tied by any temporal constraint, i.e., given observable descendan ts T and T 2 , the probability that T 1 is executed (starts) before T 2 is positive.

We assume that there is also a fixed measurement noise for the temporal ordering information. For each pair of task s T , T 2 , there is some probability  X  that T 1 is recorded before T 2 even though in the true workflow graph T 2 is an ancestor of T 1 . We will assume that the noise level is the same for each pair.
The Markov condition gives us a way of parameterizing a probabilistic model as a AO graph. If one is interested in calculating the effect of a new policy that changes the probability distribution of some specific set of tasks, then the Causal Markov condition needs to be assumed [6].
If one is interested in a learning algorithm that will recove r the right structure, at least asymptotically, we have to hav e some extra assumptions linking the probabilistic distribu -tion of the tasks to the corresponding graphical structure. For the general case of learning the structure of DAGs, a sufficient condition for consistent learning is the faithful ness condition. This condition states that a conditional indepe n-dence statement holds in the probability distribution if an d only if it is entailed in the respective DAG by d-separation [6].

We want a similar assumption, because observed condi-tional independencies can provide information about the workflow graph underlying the data, but only if conditional independencies are a result of the workflow structure (i.e., if they are entailed by the workflow graph). We cannot just assume faithfulness to d-separation: due to backward de-terminism, a chain such as T 1  X  T 2  X  T 3 encodes that T is independent of T 1 given T 3 = 1 (because if T 3 happened, then by assumption T 2 happened, which means that T 1 does not add any information concerning the distribution of T 2 but T 2 is not d-separated from T 1 given T 3 .

Instead, we assume a variation of faithfulness. First, two definitions: an augmented AO graph is a modification of a AO graph G such that, for each OR-split T we introduce a new node, Choice ( T ), as a child of T , and make every original child of T a child of Choice ( T ) only. We denote the augmented version of G by Augmented ( G ). Also, given Augmented ( G ), we say that task A is a sure-ancestor of task B if for every ancestor C of B , C is an ancestor of A and A d-separates C and B , or A is an ancestor of C . We then assume that T i is independent of T j given a set of tasks T if and only if either of the following situations hold in the workflow graph G associating such tasks:
The idea embedded in faithfulness is that conditional in-dependences should be given by the graphical structure, not by the particular choice of parameters defining the proba-bility of a task being accomplished. Sure-ancestry entails independencies because in an AO graph G , if A is a sure-ancestor of B , then P ( A = 1 | B = 1) = 1 in any probability model parameterized by G .
Assume for now we have an ordering oracle O for a work-flow graph G such that O ( T 1 , T 2 ) returns true , false or exclusive as follows: Notice that according to this oracle it is possible to have O ( T 1 , T 2 ) = true even though T 1 is not an ancestor of T as long as T 2 is not an ancestor of T 1 .

Analogously, assume for now we have an independence oracle I for a workflow graph G such that I ( T i , T j , T if and only if T i and T j are independent given T k = 1. The motivation for defining such oracles is given by the followin g theorem:
Theorem 1. Let G 1 and G 2 be two AO graphs with re-spective ordering and independence oracles { O 1 , I 1 } and { O I } over a same set of observable tasks T . If O 1 and O 2 , and I 1 and I 2 agree on all queries concerning members of T , then G 1 = G 2 up to a renaming of the hidden tasks.
The proof of this theorem is given in Appendix A. In sim-ple terms, given certain partial information of ordering and conditional independences among the observable tasks, one is able to uniquely recover the proper AO graph. With these oracles, we claim that the algorithm Learn-OrderedWorkflow , given in Figure 2, will return the correct workflow structure.

This algorithm makes references to other sub-algorithms given in Section 3.2. We will first provide a higher-level description of its steps. The algorithm works by iterativel y adding child nodes to a partially built graph in a specific order. Initially, the ordering oracle will tell us which nod es are  X  X oot causes X  of all other measurable tasks, i.e., which nodes do not have any measurable ancestor. Such nodes are identified in Step 3 of Figure 2. If we have more than one measurable node as a  X  X oot cause X , and because an AO graph requires a single starting point and explicit control nodes (i.e., AND-splits and OR-splits), it is the case that unobserved splits have to be added to the graph. This is done by HiddenSplits .

At each main iteration (Steps 7 -12), we have a set of nodes called CurrentBlanket , which contains all and only the  X  X eaves X  of the current workflow graph H , i.e., all the task nodes that do not have any children in H . The initial choice of nodes for CurrentBlanket are exactly the root causes. The next step is to find which measurable tasks should be added to H . We are interested in building the graph by selecting only a set of tasks NextBlanket such that: Algorithm LearnOrderedWorkflow Input O , an ordering oracle for a set T of tasks;
Output H , an AO graph
Figure 2: An algorithm for learning AO graphs.
We claim that GetNextBlanket , as described later, re-turns a set corresponding to these properties. We still need to identify which elements in NextBlanket should be de-scendants of which elements in CurrentBlanket , and this is accomplished by Dependencies .

It is quite possible that between nodes in CurrentBlanket and nodes in NextBlanket there are several hidden join/split tasks. Such tasks are detected and added to H by InsertLatents .
This procedure is iterated till all observable tasks are placed in H . To complete the graph, we just have to make sure that all tasks are synchronized in a finalization task, as required by all AO graphs. If the end task is not visi-ble, several threads will remain open if we do not add latent joins. This is accomplished by the final HiddenJoins call. A sample execution of this algorithm is given in Appendix B.
While mutually exclusive tasks are directly identifiable from the ordering oracle, this is not true concerning parall el tasks. If two tasks are potentially parallel, they still mig ht be executed always in the same order. The only way we can identify parallelism is by identifying a previous task that make these two tasks independent. This is the purpose of algorithm GetNextBlanket , as described in Figure 3.
This algorithm select tasks, but does not indicate which elements are descendants of which previous tasks. This is the role of Dependencies (Figure 4). The fact that the inde-pendence oracle condition only positive values of T 2 M (Step 3 of Dependencies ) is a necessary and sufficient condition. Algorithm GetNextBlanket Input CurrentBlanket , a set of tasks;
Output NextBlanket , a subset of the tasks in G O Figure 3: Identifying the next set of elements to be added.
 It is necessary because by our assumptions there might be measurement error when we observe value 0. It is sufficient because by backward determinism (if a task happens, all ele-ments in a chain before it also happened), we do not need to condition on multiple tasks. Figure 5 illustrates an exampl e of this case.
 Algorithm Dependencies Input CurrentBlanket , a set of tasks;
Output AncestralGraph , a DAG Figure 4: Determining ancestors for a set of new tasks.

This algorithm runs in O ( N 3 ), N being the number of measurable tasks. It also requires simpler statistical tes ts of conditional independence than general DAG search algo-rithms, since we condition only on singletons.

Finally, there are several points in LearnOrderedW orkflow where we need to introduce hidden tasks. The algorithm HiddenJoins is shown in Figure 6. Notice that here we tag nodes according to their role ( X  X ND-join X  and  X  X R-join X ). We do not show an explicit description of HiddenSplits : this algorithm is analogous, with the exception that edges are added in the opposite direction. It is very similar in princi ple to an algorithm given by [4]. The algorithm InsertLatents builds upon HiddenJoins and HiddenSplits . It is given in Figure 7. The final steps of this algorithm just verify if a measurable task that has measurable children actually d-separates them. If not, a hidden task is introduced.
The independence oracle can be implemented by statis-tical tests of independence, such as the  X  2 test. Given the Figure 5: An example of why conditioning on a sin-gle element is enough. Here, T 3 M and T 5 M are in-dependent measures given T 2 M = 1. If T 2 M is 1, by assumption we know that T 2 = 1 , because measure-ment error is one-sided. T 0 is 1 by backward deter-minism, which means that we are effectively asking if T 3 M and T 5 M are independent given T 0 = 1 , which is entailed by the graphical structure. parameter  X  for the noise level, binomial tests can be used to create an ordering oracle by testing if the probability of task T i antecedes task T j given the instances where both are recorded is larger than  X  .

To learn a good level of ordering noise, one can do a grid search for  X  over the interval [0 , 0 . 5] and heuristically choose the one that maximizes some measure of fitness, such as a posterior probability for the output model (using a Dirichl et prior for the parameters, for instance), or some other mea-sure that relies on independence constraints only, which is the basis of our model. For instance, by adjusting  X  one could try to bring the set of independence constraints that are entailed by the output graph as close as possible to the ones judged to hold in the data. This does not require fit-ting a latent variable model and is not subject to constraint s other than independence constraints. Learning  X  will be treated in detail in a future work.

An important practical issue is how to avoid outputing invalid AO graphs, which can be due to deviations from the assumptions or statistical mistakes. Due to lack of space, we omit a discussion of the necessary conditions that the ordering and independence oracles should satisfy to genera te a valid AO graph.
Workflow data is not as easy to obtain as other data sources. In this paper, we perform a simulated study based on a theoretical workflow that models the annual process of writing final reports at Clairvoyance Corporation. The process basically consists of parallel threads of preparin g documents, preparing summaries, booking flights and hotel rooms for an annual workshop hosted by the parent company of Clairvoyance in Japan. The graph was constructed by manually analysing e-mail logs exchanged among the com-pany X  X  employees over the course of four projects. The de-tails are given [5].

There are 15 observable and 2 hidden tasks, with no mu-tually exclusive tasks and no measurement noise (the al-gorithm still assumes the possibility of noise). One task ( Printing materials ) naturally happens much later than the actions of booking flights and hotels, even though there is no temporal constraint that dictates that printing should b e performed only after travel is arranged. Many other work-Algorithm HiddenJoins Input H , a DAG;
Output H , a DAG
Algorithm JoinStep Figure 6: An algorithm for inserting required join nodes. flow approaches [8] would be deceived by this temporal in-formation, i.e., they would regard the two tasks as strictly sequential when in fact they are not.

The graph is parameterized by a single parameter  X  that gives the probability of a task being executed given its pre-requisites. In our model, a necessary condition for any task is that all of its parents have to be performed. We simulated samples of size of 100, 200 and 500 and with  X  = { 0 . 9 , 0 . 95 } 3 . We do not introduce noise in the time order of the samples, since this will only be explored in full detail in the future.

The independence oracle is implemented by a  X  2 test using a significance level of 0.05. We ran 10 trials for each config-uration, and evaluated the true model against the output of our algorithm, assuming the ordering information is correc t, by the following criteria: number of edges between measur-able tasks in the true graph that are not in the estimated graph (edge omission, out of 12 possible edges); number of edges between measurable tasks in the estimated graph that are not in the true graph (edge omission); number of mea-surable pairs that share a common parent in the true graph but not in the estimated graph (sibling omission). Sibling comissions did not happen in our experiments. The results are: for sample size 100 and  X  = 0 . 95, the average edge omis-sion was 5 . 1 (2 . 1 of standard deviation); the average edge comission was 1 . 7(0 . 7) and the average sibling omission was 2 . 6(1 . 4). For sample size 100,  X  = 0 . 9, we had 4 . 9(2 . 6), 3 The value of  X  cannot be too small, or otherwise we will need large sample sizes in order to have a relatively large number of instances that are completed. Workflows with large chains will usually have some deterministic steps. Algorithm InsertLatents Input H , a DAG H ;
Output a DAG H Figure 7: An algorithm to introduce required hidden tasks between two layers of measurable tasks. 0 . 7(1 . 1) and 2(1 . 4). For sample size 200, and  X  = 0 . 95, we got 0 . 4(0 . 5), 0 . 1(0 . 3), 0 . 1(0 . 3). For sample size 200 and  X  = 0 . 9, we got edge omission error of 0 . 2(0 . 4) and no other error. For sample size 500, we got the exact graph in all 10 trials for both values of  X  . In the experiments, missing edges usually implied sequential tasks being treated as parallel .
The results are convincing, but it is still of interest to obtain more robust outcomes with smaller sample sizes. We plan to pursue Bayesian approaches in an extended version of this framework.
Agrawal et al. [1] introduced the first algorithm for mining workflow logs. Greco et al. [3] approach the problem using clustering techniques. A broad survey on the current work in workflow mining, or process mining, is given by van der Aalst and Wejters [8]. None of the approaches in that survey are based on a coherent probabilistic model. Instead, they use a variety of heuristics to deal with noise, while focusin g on deterministic models such as Petri nets.

Herbst and Karagiannis [4] use a representation very sim-ilar to AO graphs with cycles. While some probability dis-tribution is informally applied to define the likelihood of a workflow graph, this likelihood is not used anywhere in learning the structure of workflow graphs as defined in our paper.

It is clear that workflow models could be represented by off-the-shelf methods such as dynamic Bayesian networks and stochastic Petri nets. In particular, the factorial hid den Markov model [2] seems to naturally apply to the problem of modeling parallel threads of tasks. However, workflow mod-eling has its own particular issues that are not efficiently explored by generic dynamic Bayesian networks: instances have a well defined beginning and end; the synchronization property; backward determinism, which naturally applies t o Figure 8: A simplified workflow model of the process of document preparation at Clairvoyance Corpora-tion. ( X  X cc. X  stands for  X  X ccomplishments X , and JSC refers to Clairvoyance X  X  parent company.) many real-world problems; the fact that the  X  X idden states X  of a workflow model are in general associated with one  X  X is-ible symbol X  only. Even if a same task might be generated under different contexts, as explored by [4], this is the exce p-tion, not the rule, and it seems wasteful to arbitrarily allo w hidden states of a workflow-like dynamic system to be able to generate any symbol. A generic dynamic model would not be as statistically efficient as a constrained model.
Moreover, one is often interested in understanding the causal chains of a business process. For instance, a generic factorial hidden Markov model with a fixed number of chains would be a very opaque model to provide such understand-ing, even if the fit is good.
We have presented an algorithm for learning workflow graphs that makes use of a coherent probability model. To the best of our knowledge, this is the first approach with such a property. Results from a real world workflow are very encouraging.

Several extensions are planned for a near future: more extensive experiments, learning with cycles, showing con-sistency of the learning algorithm and Bayesian variations . A very interesting problem is to determine identifiability conditions for learning semantic roles for tasks, i.e., how tasks can appear in multiple parts of a workflow model de-pending on context. Ultimately, we also want to extract a task ontology from text data obtained from groupware and e-mail software, therefore creating workflow logs from free text data.
Theorem 1. Let G 1 and G 2 be two AO graphs with respec-tive ordering and independence oracles { O 1 , I 1 } and { O over a same set of observable tasks T . If O 1 and O 2 , and I and I 2 agree on all queries concerning members of T , then G 1 = G 2 up to a renaming of the hidden tasks.

We will do induction on the number of observable tasks to prove the proposition. For that purpose, we need a few lemmas. The first two lemmas show that the start tasks in the two graphs are identical. Let s 1 be the start task in G and s 2 the start task in G 2 .

Lemma 1. Either s 1 = s 2  X  T , or they are both hidden tasks.
 Proof One of the following two cases must obtain.
 Case 1 : s 1 and s 2 are both observable. Then s 1 is the unique common predecessor of all observable tasks according to O and s 2 is the unique common predecessor of all observable tasks according to O 2 . Because O 1 and O 2 agree, s 1 = s Case 2 : One of them, say s 1 without loss of generality, is hidden, then by our assumption it must be a split and there is NO observable task that is a common predecessor of all observable tasks according to O 1 (this follows from our as-sumption about the immediate observable descendants of an AND-split). Because O 1 and O 2 agree, there is no common observable predecessor according to O 2 . It follows that s not observable.
 Therefore, either s 1 = s 2  X  T , or they are both hidden. 2 Lemma 2. s 1 is an AND-split iff. s 2 is an AND-split. Similarly, s 1 is an OR-split iff. s 2 is an OR-split. Proof By Lemma 1, we only need to consider two cases: Case 1 : s 1 and s 2 are both hidden. It suffices to show that it cannot be the case that one of them is an OR-split while the other is an AND-split. For the sake of contradiction, suppose, without loss of generality, s 1 is an OR-split and s is an AND-split. Then there exist two immediate observable descendants of s 1 , T 1 , T 2  X  T , that are mutually exclusive according to O 1 . Because O 2 agrees with O 1 , T 1 and T also immediate observable descendants of s 2 in G 2 , which means they are in the split-join session initiated by s 2 G . Furthermore, they are also mutually exclusive accord-ing to O 2 , so they cannot belong to different threads in that split-join session, since s 2 is an AND-split. So there must be another immediate observable descendant of s 2 , T 3  X  T , such that it is in parallel with both T 1 and T 2 according to O . It follows that T 3 is in the split-join session initiated by s 1 in G 1 , and is in parallel with both T 1 and T 2 accord-ing to O 1 . But this is impossible, because T 1 and T 2 are in different threads of that OR-split-join session initiated b y s . Hence either they are both AND-splits, or they are both OR-splits.
 Case 2 : s 1 = s 2 = T  X  T . By symmetry, we only need to rule out three scenarios: (i) T is an OR-split in G 1 but an AND-split in G 2 ; (ii) T is an OR-split in G 1 but a simple task in G 2 ; (iii) T is an AND-split in G 1 but a simple task in G 2 . (i) can be ruled out by rehearsing the arguments in case 1. In the case of (ii) and (iii), notice that T may not be followed by an observable task in G 2 , for otherwise that ob-servable task will be the unique common predecessor of all observable tasks but T according to O 2 but will not be such according to O 1 . Furthermore, by our assumption, T , as a simple task in G 2 , may not be followed by a hidden OR-split, so it can only be followed by a hidden AND-split in G 2 . (ii) can thus be ruled out by rehearsing the arguments in case 1, since T is an OR-split in G 1 . For (iii), notice that some immediate observable descendants of T will be independent conditional on T = 1 according to I 1 , but dependent condi-tional on T = 1 according to I 2 . Hence (iii) contradicts the assumptions, too. 2 Suppose, for the moment, that s 1 and s 2 are both splits. Let j i be the (full) join that synchronizes the split initiated by s i in G i , i = 1 , 2. We define a thread of the split-join session between s i and j i to be the subgraph between s i and any parent of j i (over the ancestors of that parent of j ). A thread, under this definition, can contain any number of (observable) partial joins of the split initiated by s is easy to see that each thread is either an AO graph or of the simple form s i  X  T , where s i is hidden. Furthermore, by our enforcement of nesting of splits and joins, it is easy to see that different threads will only intersect at the start -ing point s i . The next two lemmas concern the observable tasks that appear in the split-join session, and in particul ar, in each thread of the session.
 Lemma 3. Suppose s 1 and s 2 are both splits. For any T  X  T , T is in the split-join session initiated by s 1 in G iff. T is in the split-join session initiated by s 2 in G Proof Let IOD i be the set of immediate observable de-scendants of s i in G i , i = 1 , 2. Because O 1 and O 2 agree, IOD 1 = IOD 2 . Hereafter we will drop the subscripts and write IOD . By our assumption, any member in IOD must be in the split-join session initiated by s i , otherwise there exists some observable task that lies in between. So for any T  X  T , if T  X  IOD , then it is in the split-join session in G iff. it is in the split-join session in G 2 . If T /  X  IOD , there are two cases to consider: (i) s 1 and s 2 are both AND-splits. In this case, if T is in the session initiated by s 1 in G not in the session initiated by s 2 in G 2 , then there exist T , T 2  X  IOD such that T is independent of T 2 conditional on T 1 according to I 1 , but T is dependent of T 2 conditional on T 1 according to I 2 . (Specifically, let T 1 be an immediate observable descendant of s 1 in the same thread as T is in G , and T 2 be an immediate observable descendant of s 1 in any other thread.) Hence a contradiction. By symmetry, it may not be the case either that T is in the session initiated by s 2 in G 2 but not in the session initiated by s 1 in G s 1 and s 2 are both OR-splits. In this case, if T is in the session initiated by s 1 in G 1 but not in the session initiated by s 2 in G 2 , then T will be mutually exclusive with some member in IOD according to O 1 , but will not be mutually exclusive with any member in IOD according to O 2 . Hence a contradiction. By symmetry, it may not be the case either that T is in the session initiated by s 2 in G 2 but not in the session initiated by s 1 in G 1 . 2 Lemma 4. Suppose s 1 and s 2 are both splits. For any T , T 2  X  T that are in the split-join session initiated by the start task in both graphs, they are in a same thread of that session in G 1 iff. they are in a same thread of that session in G 2 .
 Proof Let IOD be the set of immediate observable descen-dants of s 1 (and s 2 ) according to O 1 (and O 2 ). By Lemma 2, we only need to consider two cases: Case 1 : s 1 and s 2 are both AND-splits. We first show that if T 1 , T 2  X  IOD , then it is not the case that they are in the same thread in one of the graphs but not in the other graph. Suppose otherwise and, without loss of generality, that T 1 and T 2 are in the same thread in G 1 but not in the same thread in G 2 . It follows that O 1 ( T 1 , T 2 ) and O are both true. Because O 1 agrees with O 2 , we also have O ( T 1 , T 2 ) and O 2 ( T 2 , T 1 ). Since T 1 and T 2 belong to the same thread initiated by s 1 in G 1 and are both in IOD , there must be an OR-split that lies between s 1 and T 1 , T 2 , as an AND-split cannot immediately follow another AND-split. This implies that there exists T 3  X  IOD that is mutually exclusive with both T 1 and T 2 according to O 1 . However, because T 1 and T 2 belong to different threads initiated by s , an AND-split, in G 2 , it is impossible that a task can be mutually exclusive with both of them according to O 2 . Hence a contradiction. Thus, if T 1 , T 2  X  IOD , then they are in a same thread in G 1 iff. they are in a same thread in G .
 Now suppose at least one of them, say T 1 without loss of generality, is not in IOD . If T 1 and T 2 belong to different threads in G 1 , then there exists T 3  X  IOD such that T 1 T 3 are in parallel and T 1 is independent of T 2 conditional on T 3 according to I 1 . On the other hand, if T 1 and T 2 belong to the same thread in G 2 , the only way that T 3 could render them independent is that T 3 and T 2 are two children of an OR-split, but in that case they will be mutually exclusive. So T 1 and T 2 must belong to the same thread in G 2 , too. By symmetry, the converse also holds.
 Case 2 : s 1 and s 2 are both OR-splits. If T 1 and T 2 belong to different threads in G 1 , then they are mutually exclusive ac-cording to O 1 , which means they are also mutually exclusive according to O 2 . So, if on the other hand T 1 and T 2 belong to the same thread in G 2 , then there must be an AND-split in between s 2 and the (yet another) OR-split that splits T and T 2 because an OR-split cannot immediately follow an-other OR-split. This implies that there exists T 3 such that it is not mutually exclusive with either T 1 or T 2 according to O . However, because T 1 and T 2 belong to different threads in G 1 , it is impossible that T 3 is not mutually exclusive with either T 1 or T 2 according to O 1 . Hence a contradiction. 2 Finally, we need a lemma about j i  X  X  that complete the split-join sessions initiated by s i  X  X .

Lemma 5. Suppose s 1 and s 2 are both splits. Let j 1 be the (full) join that synchronize the splits initiated by s 1 in G and j 2 be the (full) join that synchronize the splits initiated by s 2 in G 2 . Then either j 1 and j 2 are the same observable task or they are both hidden. Proof Two cases to consider: Case 1 : Suppose j 1 and j 2 are both observable. So j i is the descendant of all observable tasks within the split-join se s-sion initiated by s i and the ancestor of all other observable tasks, i = 1 , 2. By Lemma 3, the set of observable tasks within the split-join session initiated by s 1 is the same as the set of observable tasks within the split-join session in i-tiated by s 2 . It follows that j 1 = j 2 , otherwise O 1 totally agree with O 2 .
 Case 2 : Suppose one of them, say j 1 without loss of general-ity, is hidden. In this case, if j 2 is observable, then j immediately follow j 1 in G 1 , otherwise O 1 and O 2 do not agree. By our assumption, j 2 may not be a simple task. If it is an OR-split, then in G 2 a hidden-OR must immediately follow j 2 (by arguments very similar to those in previous lemmas), which, however, is ruled out by our assumption. If j 2 is an AND-split in G 1 , then some tasks after j 2 be independent conditional on j 2 according to I 1 , but de-pendent conditional on j 2 according to I 2 . A contradiction. Therefore, j 2 must be hidden, too. 2 We now prove the main proposition by induction on the number of observable tasks n . It is easy to see that n  X  2 by our assumptions.
 Base case : n = 2. Let T 1 and T 2 be the two observable tasks. Only four AO graphs are compatible with our as-sumptions (up to a renaming of latent tasks): (1) T 1  X  T (2) T 2  X  T 1 ; (3) T 1 and T 2 are two threads of an AND split-join session with a hidden split (start task) and a hid -den join (end task); (4) T 1 and T 2 are two threads of an OR split-join session with a hidden split (start task) and a hid -den join (end task). Obviously each graph entails a different ordering relationship between T 1 and T 2 . So, if O 1 and O agree, then G 1 = G 2 up to a renaming of the hidden tasks. Inductive Step : Suppose the proposition holds for n  X  m . Let n = m + 1  X  3. There are three cases: Case 1 : s 1 is a simple task in G 1 . By Lemmas 1 and 2, s = s 2 = T and T is also a simple task in G 2 . It is easy to see that the subgraph of G 1 over T \{ T } and the subgraph of G 2 over T \{ T } are also AO graphs (since n  X  3). By the inductive hypothesis, they are identical up to a renaming of hidden tasks. It follows that G 1 = G 2 up to a renaming of hidden tasks.
 Case 2 : s 1 is a split, and the split is joined before reach-ing the end task in G 1 . By Lemmas 1, 2 and 5, s 2 is also a split, and the split is joined before reaching the end task in G . Let T i be the set of observable tasks that belong to the split-join session initiated by s i in G i (including the initial split and the final join), i = 1 , 2. It follows from Lemmas 1, 2, 3 and 5 that T 1 = T 2 . By the inductive hypothesis, the subgraph of G 1 over T 1 is the same as the subgraph of G 2 over T 2 up to a renaming, and the subgraph of G 1 over T \ T 1 is the same as the subgraph of G 2 over T \ T up to a renaming. (Note that there is a special case where the subgraphs over T \ T i only contain one observable task, and hence the inductive hypothesis is not applicable. But in that case, the two subgraphs are trivially identical.) It follows that G 1 = G 2 up to a renaming of hidden tasks. Case 3 : s 1 is a split, and the split is joined at the end task in G 1 . By Lemmas 1, 2 and 5, s 2 is also a split, and the split is joined at the end task in G 2 . By Lemma 3, for each thread of that split-join session in G 1 , there is a thread of the split-join session in G 2 such that the two threads involve the exactly same observable tasks, and vice versa. By the in-ductive hypothsis, the two threads (subgraphs) are the same up to a renaming of hidden tasks. (Again, there is a special case where the inductive hypothesis is not applicable. That is, the threads are of the form s i  X  T , and s i  X  X  are hidden. In this case the two subgraphs are trivially identical.) So i n total G 1 = G 2 up to a renaming of hidden tasks. Q.E.D We will now go through an example of how LearnOrdered W orkflow works. Assume for now that the graph G in Figure 9 corresponds to the true generative model, from which we know the ordering oracle O and the independence oracle I for tasks { 1 , . . . , 12 } . We will demonstrate how LearnOrderedW orkflow is able to reconstruct G out of O and I . Figure 9: Unlabeled nodes represent hidden tasks.
 Each OR-split/join is represented as a rhombus.

Suppose that the directionality graph G O is given in Fig-ure 10. Notice that even though elements in { 8 , 10 } are con-current to elements in { 9 , 11 } , there is a total order among these elements: 8  X  9  X  10  X  11, according to O . 6 and 7 are not connected because by assumption they should hap-pen in either order a frequent number of times. We consider this assumption to be reasonable (at the moment of the split, tasks should be independent, and therefore no fixed time order implied). However, contrary to a naive workflow min-ing algorithm, we do not require, for instance, that 6 and 11 are recorded in random orders. This type of assump-tion seems considerably more artificial, because tasks in on e chain might take much longer than tasks in another chain, and a specific order may arise naturally. Figure 10: An ordering relationship for the graph in Figure 9. We do not represent explicitly the edges between elements in { 1 , 2 , 3 , 4 , 5 } and { 8 , 9 , 10 , 11 } in order to avoid cluttering the graph (symbolized by the unconnected edges out of { 1 , 2 , 3 , 4 , 5 } ).
In the initial step, the set CurrentBlanket will contain tasks { 1 , 2 , 3 , 4 , 5 } . The HiddenSplits algorithm will work as follows: a graph M will be created based on O and tasks { 1 , 2 , 3 , 4 , 5 } . M and its complemented are shown in Figure 11. Since M is disconnected, it will be the basis for the recursive call. We are going to insert an hidden OR-split separating { 1 , 2 , 3 } and { 4 , 5 } at the return of the recursion, as depicted in Figure 12.
 Figure 11: Graphs M and its complement M C in HiddenSplits for the first CurrentBlanket set. Figure 12: The first call of HiddenSplitsStep will sep-arate set { 1 , 2 , 3 , 4 , 5 } as { 1 , 2 , 3 } and { 4 , 5 } .
Consider the new call for HiddenSplitsStep with argu-ment S = { 1 , 2 , 3 } . The corresponding graphs M and M are now shown in Figure 13. M is not disconnected, but M C is. This will lead to an insertion of an AND-split separating sets { 1 } and { 2 , 3 } and another recursive call for { 2 , 3 } . Figure 13: Graphs M and M C corresponding to S = { 1 , 2 , 3 } in HiddenSplitsStep .

At the end of the first HiddenSplits , H will be given by the graph show in Figure 14. We now proceed to insert the remaining nodes into H .

From the ordering graph of Figure 10, we will choose as the next blanket the set { 6 , 7 , 12 } . Since they are not con-nected by any edge in Figure 11, we did not need to do any independence test to remove edges between them. When computing the direct dependencies between { 1 , . . . , 5 } and { 6 , 7 , 12 } , since no conditional independence holds between elements in { 6 , 7 , 12 } conditioned on positive measurements will be the direct dependencies of each element in { 6 , 7 , 12 } .
We now have to perform the insertion of possible latents between { 1 , 2 , 3 , 4 , 5 } and { 6 , 7 , 12 } . There is only one set Siblings in InsertLatents , { 6 , 7 , 12 } , and one AncestralSet ,
Figure 14: The partially constructed graph H . { 1 , 2 , 3 , 4 , 5 } . When inserting hidden joins for elements in AncestralSet , we will perform an operation analogous to our previous example of HiddenSplits , but with arrows di-rected in the opposite way. The modification in shown in Figure 15(a), while Figure 15(b) depicts the modification of the relation between { 6 , 7 , 12 } . The last step of our InsertLatents iteration simply connects the childless node of Figure 15(a) to the parentless node of Figure 15(b). Figure 15: Inserting latents between two layers of observable tasks.

Again, we proceed to add more observable tasks in the next cycle of LearnOrderedW orkflow . The candidates are { 8 , 9 , 10 , 11 } .
 By Figure 10, all elements in { 8 , 9 , 10 , 11 } are adjacent. However, by conditioning on singletons from { 6 , 7 , 12 } we can eliminate edges { 8  X  9 , 9  X  10 , 8  X  11 , 10  X  11 } . The parentless nodes in this set are now 8 and 9, instead of 8 only. CurrentBlanket is now { 6 , 7 , 12 } and NextBlanket is { 8 , 9 } .

When determining direct dependencies, we first select { 6 , 7 } as the possible ancestors of { 8 , 9 } . Since 8 and 7 are in-dependent conditioned on 6, and 9 and 6 are independent conditioned on 7, only edges 6 8 and 7 9 are allowed. Analo-gously, the same will happen to 8  X  10 and 9  X  11. Graph H , after introducing all observable tasks, is shown in Fig-ure 16. After introducing the last hidden joins in the final steps of LearnOrderedW orkflow , we reconstruct exactly the original graph in Figure 9. Figure 16: The graph H after introducing all ob-servable tasks and just before introducing the last hidden joins.
