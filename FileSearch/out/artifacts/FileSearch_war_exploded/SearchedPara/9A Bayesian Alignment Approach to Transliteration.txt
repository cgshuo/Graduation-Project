 ANDREW FINCH ,NICT EIICHIRO SUMITA ,NICT Asian languages frequently borrow words from other languages (very commonly from English) for various purposes that can include expressing named entities, such as personal names, place names, and company names; expressing new technical terms without generating a new term in the native language; or even as a means of relating to another culture. These borrowed words are usually imported into the language by modifying their phonentics to match the phonetic system of the language into which they are being imported. This process is called transliteration . In this article, we will draw our examples from English-Japanese transliteration, since we are most familiar with these languages, but transliteration is so widespread that similar examples could just as readily have been drawn from any other Asian language.
 We start by giving a simple example of a transliteration from English into Japanese. Consider the name of the singer  X  X ngelbert Humperdinck X . In many European lan-guages (e.g., German and Italian), it is imported into the language directly with-out changing its spelling. However, in languages that do not share the same writing system, such expressions are transcribed into the respective native writing system, usually in such a manner as to preserve the phonetics as much as possible. For ex-ample, in Japanese, the name would be transcribed into the katakana alphabet as upper-case form in parentheses is a romanized form of the preceding Japanese charac-ter sequence in Japanese script (katakana), where each Roman character or character pair corresponds to a single character in the Japanese writing system and furthermore corresponds very closely to the English phonetics of the character sequence. We will come back to this correspondence in the next section.

Transliteration mining is the process of obtaining lists of bilingual word pairs (we will refer to these as transliteration pairs ) automatically, that is, pairs of words that are transliterations of each other in parallel or comparable corpora. The mined word pairs have many applications, for example, as data for training a transliteration generation system, for the enhancement of the bilingual dictionary of a machine translation sys-tem for improving lexical coverage, and in query term translation for cross-language information retrieval.

The translation of named entities poses a major problem for machine translation sys-tems, as many of the terms that occur in them have not occurred in the training data for these systems, and as a consequence they are unable to translate them. In a recent study [Finch et al. 2011b], it was shown that adding transliterations of unknown words into the output of a machine translation system (as opposed to the common practice of deleting them) can improve the quality of machine translations. In their experiments, human evaluation was used rather than automatic metrics based on n -gram precision, such as BLEU [Papineni et al. 2001], that tend to have a bias that is dependent on output length and can favor shorter translations. This bias gives an unfair advantage to the strategy of simply deleting out-of-vocabulary words. Furthermore it is possible for an erroneous transliteration (e.g., one that is an incorrect spelling, but still pho-netically close or even phonetically equivalent to the correct form) to provide useful information to a user of a machine translation system. The field of transliteration mining is currently being actively researched, and there is a wealth of previous research [Bilac and Tanaka 2005; Brill et al. 2001; Darwish 2010; Jiampojamarn et al. 2010; Khapra et al. 2010; Lee and Chang 2003a; Nabende 2010; Noeman and Madkour 2010; Oh and Isahara 2006; Tsuji and Kageura 2006] as well as a recently shared task in the 2010 ACL Named Entities Workshop (NEWS2010) [Kumaran and Li 2010].

One common strategy for determining cross-lingual phonetic similarity between words is to transcribe them into the Roman alphabet and then use character-level similarity measures to compare them, for example, normalized edit distance [Jiampojamarn et al. 2010]. In practice, this seems to be an effective technique, and their systems were some of the highest ranked systems in the NEWS2010 shared mining evaluation (for full details of the results of this evaluation refer to [Kumaran et al. 2010a]). In the previous example, it is easy to see that the romanized string  X  X ngeruberuto funpaadeinku X  will be reasonably close in terms of edit distance to the English  X  X ngelbert humperdinck X , but very likely to be distanced from other English strings of which it is not a transliteration.

A large advantage of these approaches is that they can often be developed without the need to collect a training corpus. On the other hand, a potential drawback is that they are language dependent in nature, simply because they rely on a language-specific romanization scheme. Furthermore, performance will depend on the particular romanization scheme chosen, and often there are several to choose from, in addition to bespoke romanization schemes that might be devised for this task (e.g., deleting diacritics and performing character substitutions in European languages [Jiampojamarn et al. 2010]). In Japanese, for example, there are three main competing systems for romanizing Japanese kana characters: the Hepburn, Kunrei-shiki R  X  omaji, and Nihon-shiki R  X  omaji romanization systems.
One way to eliminate this language dependency is to build a transliteration genera-tion system to transduce a transliterated string into the other language, and then use a heuristic operating at the character level to measure the string-similarity between the two character sequences. This approach is taken by Noeman and Madkour [2010] who use an FST to generate a set of candidate transliterations and an FSA to accept those that can be used to form transliteration pairs. The approach is also used in the generation-based models of Jiampojamarn et al. [2010], where forward-and backward-generated transliterations are compared by edit distance against the corresponding strings in the other languages; a score consisting of weighted edit distances of these comparisons in both directions was used to classify the candidate transliteration pair. Other examples of the use of this approach exist [Lee and Chang 2003b; Tsuji and Kageura 2006].

A second advantage of approaches that do not require a system for phonetically transcribing a language is that these approaches can handle non-phonetic transcrip-tions in addition to transliterations, if necessary. In Japanese, it is very common for transliterations, transcriptions, and translations to be mixed. To give one example, the named entity  X  X ork University X , would usually be represented as  X  X OU-KU DA-I-GA-KU X  in Japanese X  X he word  X  X ork X  being transliterated, and the word  X  X niversity X  being translated.

The approach we take in this article is a direct approach that does not rely on an intermediate representation, but rather a direct grapheme-to-grapheme mapping be-tween the languages. We use a generative model directly to assess whether two strings constitute a transliteration pair and avoid the necessity to explicitly generate strings in either language. This type of approach was taken by Lee and Chang [2003b], who use a noisy channel model to assess transliteration pair candidates. Our approach dif-fers from theirs by the Bayesian model that we employ. Bayesian models, such as the one we use, have been successfully applied to transliteration generation [Finch and Sumita 2010; Huang et al. 2011] and are capable of producing good models that give rise to state-of-the-art performance on transliteration generation tasks [Finch et al. 2011a, 2012].

The most related work to ours was reported in Sajjad et al. [2012]. Here, the proba-bility from a generative model was used in the classification of word pairs for transliter-ation mining. The generative model they employ is based around unigram alignments and is trained to maximize the likelihood of the training corpus with the EM algorithm. Our approach differs from theirs in that we perform many-to-many alignment without null productions in the spirt of the generic string-to-string substitution distance model [Brill and Moore 2000], rather than using a model related to the one-to-one mapping framework of stochastic Levenstein distance [Ristad and Yianilos 1996]. The advan-tage of our approach is that since it is able to handle many-to-many mappings, it can be applied to transliteration mining tasks in non-Roman scripts. Our approach also differs fundamentally in that we use a nonparametric Bayesian model, rather than maximum likelihood training, and we highlight the benefits of taking this approach in the motivation section next.

This article is an extension of the work presented in Fukunishi et al. [2011], and contains additional experimental results and analysis together with a more thorough explanation of the Bayesian alignment model and its application to the task of translit-eration mining. In the next section, we will describe the Bayesian model that drives the alignment process that underpins our technique. We then present the methodology we use to exploit features from samples taken from this training process to determine whether two words constitute a transliteration pair. In Section 5, we describe the set of experiments we performed to investigate the effectiveness of our system on data from all the NEWS2010 shared tasks on transliteration mining, and also on a similar English-Japanese corpus that we constructed, and present our results in the following section. Finally, we conclude and offer some directions for future research.
Throughout the article, we use the following acronyms as shorthand for the various languages: Ar = Arabic, En = English, Ch = Chinese, Hi = Hindi, Ja = Japanese, Ru = Russian, Ta = Tamil. Bayesian models are becoming increasingly popular in the field of natural lan-guage processing and, in particular, have found application in word segmentation [Mochihashi et al. 2009] and bilingual alignment [Blunsom et al. 2009; DeNero et al. 2008; Neubig et al. 2011]. Bayesian models, due to the manner in which they are constructed, give rise to models with few parameters, and as a consequence, they align consistently and their models tend not to overfit the data. These assertions are sup-ported by experiments [Finch et al. 2011a] comparing the models learned by the same nonparametric Bayesian technique we use to models produced by the m2m aligner which can be seen as a generalized version of the stochastic Levenstein distance model (trained using maximum likelihood) that is able to handle many-to-many mappings. We refer the reader to the original paper for full details of these experiments but provide a brief summary of their results to motivate our approach.

Table I shows the number of parameters learned by each type of modeling strat-egy trained on several transliteration datasets. The model learned by the Bayesian approach was considerably more compact in every case, and this effect was most pro-nounced for language pairs that include languages with large grapheme set sizes: for Japanese kana to kanji conversion, for example, the Bayesian model contains fewer than half the number of parameters. This is consistent with the hypothesis that the model built using maximum likelihood training is overfitting the data. Furthermore, the results of Finch et al. [2011a] show that joint n-gram models trained on the aligned training data have lower perplexity on held-out data. This indicates that the Bayesian nonparametric model has learned a model that is able to better predict held-out data, assigning more of its probability mass to those sequence pairs likely to occur in data and less to  X  X oisy X  sequence pairs that are only likely to occur in the training data. Finally, Table I shows the performance of a transliteration generation system (shown in the  X  X -score X  columns of the table) trained identically on aligned data from both techniques. Models built from the Bayesian alignment outperform those built from the m2m alignment for all languages pairs that were tested.

We describe the nonparametric Bayesian model in more detail in the following sections. The Bayesian alignment model we use in our approach is a Dirichlet process model. A Dirichlet process is a stochastic process defined over a set S (in our case, the set of all possible bilingual sequence pairs) whose sample path is a probability distribution on S .
The Dirichlet process model we use in our approach is a simple model that re-sembles the cache models used in language modeling [Goldwater et al. 2006]. Our approach is related to other approaches [DeNero et al. 2008; Neubig et al. 2011] used to estimate phrase pair frequencies, but our work focuses on the simpler problem of phrasal alignment without reordering. In general, the process of transliteration may require re-ordering if multiword phrases are used, for example, in Japanese personal names, it is customary for the surname to come first, whereas in English, typically the surname is the last word in the sequence. Nonetheless, the process of transliteration can be defined as a word-by-word process, and this process is always monotone. Our approach employs the extension of the forward-filtering backward-sampling algorithm [Mochihashi et al. 2009] (explained later in this article) to the task of bilingual sequence alignment proposed by Finch and Sumita [2010].

Intuitively, the model has two basic components: a model for generating an outcome that has already been generated at least once before, and a second model that assigns a probability to an outcome that has not yet been produced. Ideally, to encourage the reuse of model parameters, the probability of generating a novel bilingual sequence pair should be considerably lower then the probability of generating a previously ob-served sequence pair. This is a characteristic of the Dirichlet process model we use, and furthermore, the model has a preference for generating new sequence pairs early on in the process but is much less likely to do so later on. In this way, as the cache becomes more and more reliable and complete, the model prefers to use it rather than generate novel sequence pairs. The probability distribution over these bilingual sequence pairs (including an infinite number of unseen pairs) can be learned directly from unlabeled data by Bayesian inference of the hidden alignment of the corpus.

The underlying stochastic process for the generation of a corpus composed of bilin-gual phrase pairs  X  is usually written in the following form.

G is a discrete probability distribution over all the bilingual sequence pairs according to a Dirichlet process prior with base measure G 0 and concentration parameter  X  . The concentration parameter  X &gt; 0 controls the variance of G ; intuitively, the larger  X  is, the more similar G 0 will be to G . 3.2.1. The Chinese Restaurant Process. Unfortunately it is not possible to estimate G directly, since there are an infinite number of possible bilingual sequence pairs, so instead we integrate over its possible values. To do this, we cast the bilingual sequence-pair generation process as an instance of the Chinese Restaurant Process (CRP) [Aldous 1985]. According to this representation, every bilingual sequence pair corresponds to the dish served at its table in a potentially infinite set of tables in a Chinese restaurant. The number of customers seated at each table represents the cumulative count of the bilingual sequence pair. A new customer to the restaurant can take a seat at an occupied table with a probability proportional to the number of customers at that table and must eat that table X  X  dish, or he can take a seat at an unoccupied table with a probability proportional to a constant, in which case he must eat a dish (i.e., a bilingual sequence pair) chosen by the chef (in this analogy, the chef X  X  choice is in accordance with the base distribution G 0 ). 3.2.2. The Base Measure. For the base measure that controls the generation of novel sequence pairs, we use a joint spelling model that assigns probability to new sequence pairs according to the following joint distribution.
 where | s | and | t | are the length in characters of the source and target sides of the bilingual sequence pair, respectively; v s and v t are the vocabulary (alphabet) sizes of the source and target languages, respectively; and  X  s and  X  of the source and target,respectively.

According to this model, source and target sequences are generated independently: in each case, the sequence length is chosen from a Poisson distribution, and then the sequence itself is generated given the length. Note that this model is able to assign a probability to arbitrary bilingual sequence pairs of any length in source and target sequence, but favors shorter sequences in both. 3.2.3. The Generative Model. The generative model is given in Equation (3). The equa-tion assignes a probability to the k th bilingual sequence pair ( s the corpus, given all of the other sequence pairs in the history so far ( s  X  k is read as:  X  X p to but not including k  X .
In this equation, N is the total number of bilingual sequence pairs generated so far, and N (( s k , t k )) is the number of times the sequence pair ( s history. G 0 and  X  are the base measure and concentration parameter, as previously. 3.3.1. Gibbs Sampling. We used a blocked version of a Gibbs sampler for training. Goldwater et al. [2006] report issues with mixing in the sampler that were overcome using annealing. Mochihashi et al. [2009] overcame this issue by using a blocked sam-pler together with a dynamic programming approach. Our algorithm is an extension of application of the forward-filtering backward-sampling (FFBS) algorithm [Scott 2002] to the problem of monolingual word segmentation presented in [Mochihashi et al. 2009]. We extend their approach to handle the joint segmentation and alignment of character sequences. We refer the reader to Mochihashi et al. [2009] for a complete de-scription of the monolingual FFBS process which will aid the reader in understanding the bilingual case.

Mochihashi et al. [2009], used a forward variable  X  [ t ][ k ] to represent the probabil-ity of a string c 1  X  X  X  c t with the final k characters being a word. This variable may then be calculated efficiently using dynamic programming techniques and used in the sampling process. Analogously, in the bilingual case, we use a forward variable  X  [ u ][ v ][ k ][ l ] that represents the probability of a pair of sequences s sequence) and t 1  X  X  X  t v (the target sequence) with the final k elements of the source sequence being aligned to the final l elements of the target sequence. The recursive formula used to calculate the  X  variables is given in Equation (4).
We implemented this algorithm graphically as explained next. We use a segmenta-tion graph (shown in Figure 1) to guide the process. This directed graph is a compact representation of all possible ways in which to segment and align a bilingual pair. Each node represents a set of partial alignment hypotheses of the whole sequence that share the same sequences of source and target tokens, and each arc represents the bilingual phrase pair used to transition from the tail of the arc to the head. In the figure, the arcs are labelled with the log probability of this sequence pair (given by the model in Equation (3)), therefore, the log probability of a full segmentation hypothesis is given by the sum of the arc labels on the respective path from the source node &lt; s &gt; to the sink node abba . The most probable alignment is indicated with bold arcs in the figure and corresponds to the segmentation a / b / ba / ; this is reasonable since both  X  X  X  and  X  X a X  are associated with their phonetic equivalents in Japanese, and the Japanese  X   X  indicates that the consonant immediately to the right is to be repeated. The log probabilities in the graph are real values taken from the third iteration of the training, and here the most probable alignment is already by far the most likely.
Nodes in the graph can have multiple in-and out-degrees. Two nodes are combined when the unsegmented part of the bilingual sequence pair is the same for both, giving rise to a compact, efficient representation.
 The FFBS algorithm operates directly on the segmentation graph and has two steps. The forward filtering step calculates for each node in the graph the probability of the subgraph (including the node itself) to the left of the node, back to the source node. This probability,  X  , is stored in the node itself (these  X   X  X  are shown in Figure 1). This process proceeds recursively in a depth-first post-order traversal of the graph, starting at the sink node. Nodes for which the probability has been calculated are marked as done, ensuring that  X  gets calculated only once for the node.

The backward sampling step samples a derivation of the bilingual word pair accord-ing to the probability distribution over all possible segmentations. This is done easily using the  X  values stored in the graph by the forward-filtering process. The backward sampling also proceeds recursively from the sink node. For each incoming arc, the prob-ability of including that arc in the sample is given by the product of the arc probability and the  X  value at the tail of the arc. This value is calculated for each incoming arc, and one arc from the set is sampled according to the probability distribution over the arcs. The sampling procedure is called recursively on the tail of the sampled arc until the source node of the graph is reached. The sequence of arcs traversed defines the sam-pled derivation of the bilingual pair for the current iteration of the training process, and this sample is in accordance with the probability distribution over all derivations with respect to the model.

We found our sampler converged rapidly without annealing. The algorithm has com-plexity O ( UVST ) , where U and V are the lengths of the source and target sequences, respectively, and S and T are the maximum allowed lengths for the source and target sequences in the bilingual sequence pairs. We found this was practicable for transliter-ation data, where these lengths are typically short. The number of iterations was set by hand after observing the convergence behavior of the algorithm in pilot experiments. We used a value of 30 iterations through the corpus in all our experiments. By repeatedly scoring bilingual sequence pairs with the probability from Equation (3), the algorithm is able to co-segment and align source and target grapheme sequences through the iterative process of Bayesian inference using Gibbs sampling described in Section 3.3.1.

An example of an aligned grapheme sequence pair, that is, the output of running this Dirichlet process model on the bilingual data is illustrated in Figure 2. Given such an alignment of source and target grapheme sequences, it is possible to perform generation by monotonic concatenation of grapheme sequence pairs to form words, as in the joint-source channel models of Li et al. [2004]. The probability of generat-ing a bilingual word pair is given by the product of the probabilities of the bilingual grapheme sequence pairs that generate it. Our idea is built on the assumption that the better able our model is to generate a bilingual word pair, the more likely it is that the word pair is a transliteration pair that we would like to mine. We use the Dirich-let process model to align the data, extract features from this alignment (explained in the next section), and use them to train a support vector machine (SVM) [Cortes and Vapnik 1995; Joachims 2002] to classify them as correct or incorrect transliteration pairs. In pilot experiments, the model did not appear to be particularly sensitive to the values of  X  parameters in the base measure. We set this value at 2 for all our experiments. The concentration parameter  X  was learned by sampling its value. Following Blunsom et al. [2009], we used a vague gamma prior Gamma ( 10  X  4 ,10 4 ) and sample new values from a log-normal distribution whose mean was the value of the parameter, and variance was 0.3. We used the Metropolis-Hastings algorithm to determine whether this new sample would be accepted. The values learned for  X  for each language pair are given later. Figure 2 shows the bilingual segmentation and alignment together with the scores for each segment for the incorrect candidate pair ANDORIYUU (in Japanese) and  X  X n-droid X  in English. The scores in the figure for each of the bilingual character sequence pairs arise directly from applying Equation (3). In this example, the candidate pair is not a transliteration pair; nonetheless, the pair comes quite close because they share a common substring as a prefix.

The Bayesian model is able to align the corresponding parts of these two words (and/ANDO) using bilingual sequence pairs that have been observed a number of times in the training corpus. The noncorresponding subsequences (roid/RIYUU) of these two words will not have been observed in the data, and the Bayesian model therefore must introduce a costly new sequence pair into its model to generate them. In our model, the cost of introducing a new feature increases exponentially with the lengths of the source and target components (it can be seen from Equation (3) that the probability of using a previously unobserved sequence pair in the derivation is given solely by the base measure defined in Equation (2)).

It would be possible to use any of a number of features derived from the alignment and the corresponding score. For example, using the log probability itself would be possible, but it is strongly determined by sequence length and therefore not directly comparable across lengths without modification. The features (described in detail next) we will use in our experiments are based on two basic hypotheses. The first is that the alignment scores for bad candidate pairs are likely to be lower than scores for good candidate pairs of the same length. Our second hypothesis is based on the process of forced alignment which co-segments the candidate pair piece by piece. Unobserved pieces typically have extremely low probability and are therefore very costly to introduce into the segmentation hypotheses. As a consequence, the model will be driven to generate as much as possible of the sequence pair by reusing the higher probability pieces that have already been observed. Our assumption is that the proportion of the sequence pair that cannot be generated using bilingual grapheme sequence pairs observed in the data will be a good indicator as to whether or not the pair is a correct transliteration pair.

We used a total of four features in our SVM classifier, shown in Table II. Feature f 1 is based on the first of the two preceding assumptions. Feature f 2 is a simple length-based heuristic which was expected to help the prediction. Feature f 3 is designed to capture the idea underpinning the second of the two preceding hypotheses, that is, the proportion of the candidate pair that cannot be modeled directly by the features learned by the Dirichlet process model. f 4 focuses on the score of the weakest part of the derivation.

In Table II, logprob is the log probability of the sampled derivation of the two grapheme sequences, according to our generative model. numsegs is the number of bilingual segments used in this derivation. minprob is the log probability of the seg-ment with the lowest probability in the derivation. | s | graphemes) of the source and target words, respectively. | ber of graphemes in both source and target, that is, in bad segments . Here by bad segment we mean a bilingual segment that has not been observed in the training cor-pus and thus is only receiving a contribution from the base measure component of our Dirichlet process model (a bad segment is illustrated in Figure 2 as the rightmost seg-ment in the sequence). To be explicit, in the example given in Figure 2, the values for each of the features we use in our classifier are f 1 = ln ( 0.034 f 3 = 4 + 3 6 + 7 ,and f 4 = 8.7  X  10  X  9 . For our experiments, we used data from all tracks of the NEWS 2010 Named Entity Workshop [Kumaran and Li 2010; Kumaran et al. 2010a, 2010b]. A complete descrip-tion of this shared task is given in Kumaran et al. [2010b] and the results for all 15 systems evaluated are presented in Kumaran et al. [2010a].

Our experiments were not part of the official NEWS2010 shared task but used ex-actly the same datasets. The training data for this track consisted of title pairs of interlanguage links between Wikipedia articles. These titles are noisy in the sense that they can be sequences of words, only some or even none of which may be translit-erations of each other. The proportion of correct transliteration pairs to incorrect pairs in the training data was unknown. In addition, 1,000 seed pairs of clean data were provided. The seed pairs contained only one word for each language, and all were posi-tive examples of transliteration pairs; no negative examples were included in the seed data.
 For evaluation, the participants were expected to mine transliteration pairs from the full training set. A set of approximately 1,000 interlanguage links (each giving rise to 0, 1, or more transliteration pairs) was randomly sampled from the training data and not disclosed to the participants. In our experiments, we used the same precision/recall/ f-score evaluation metrics that were used in the official runs for the NEWS2010 work-shop [Kumaran et al. 2010a]. Specifically, precision P = TP f-score F = 2  X  P  X  R P + R , where TP is the number of correct pairs that were labeled as cor-rect (true positives), FP is the number of incorrect pairs that were labeled as correct (false positives), and FN is the number of correct pairs that were labeled as incorrect (false negatives). A flowchart illustrating the end-to-end process that was used in our experiments to mine transliteration pairs is shown in Figure 3. As can be seen from the figure, the process starts with the Bayesian alignment of the large corpus of noisy title pairs. A set of synthetic negative examples are used together with the provided set of correct seed transliteration pairs to train an SVM classifier that discriminates using features derived from the bilingual alignment. 1 This classifier is applied to all the data includ-ing the test data, and finally the system is evaluated using the classes predicted for the test data with respect to a gold standard human annotated reference. No negative examples were provided for this task. Jiampojamarn et al. [2010] over-came this issue by generating their own set of negative examples. We propose a novel approach that creates a set of negative examples by exploiting the natural cluster-ing that is induced by the features derived from our Bayesian model (see Figure 4). This is described in the following section. We later compare this approach to two other strategies based on those employed in Jiampojamarn et al. [2010] in the experimental section. 5.4.1. Model-Based Selection. Figure 4 shows a scatter plot of two plausible features over the En-Ru training dataset. The first feature (vertical axis) is the arithmetic mean of the log probabilities of each of the segments. This averaging allows sequences of differing lengths to be compared. The second feature (horizontal axis) is the log prob-ability of the least probable segment in the sequence. As can be seen from the plot, the second feature in particular partitions the dataset quite cleanly into two clusters: 99.9% of the seed data (plotted on the graph in a lighter shade (red)) lie in the upper right-hand cluster.

We select negative examples by means of thresholds on these features. The thresh-olds used to gather negative examples were set using the seed data by choosing the lowest values of any seed data points. Negative examples are samples that have feature values lower than either of the thresholds. This process is illustrated visually in Figure 4; the negative samples being extracted from the lower-left clusters (in the shaded area of the graphs). We illustrate the clustering of the data for English-Russian and English-Japanese data, but we observed similar characteristics in all of the language pairs we studied. The English-Russian data is plotted with together with the seed data, and the English-Japanese plot is of a labeled development set. The thresholds used for all language pairs are given in Table III, together with the number of negative examples that were collected. We used these negative samples together with the provided seed sentences (known to be positive examples) to train the SVM classifier. For the English-Japanese data plotted in Figure 4, approximately 1.6% of the extracted negative examples were erroneous positive examples. 5.4.2. Model-Based Selection with Annotated Data. Another possible approach is to use the model-based approach described in the previous section together with a manually annotated development dataset to control the selection of negative examples for train-ing. We sampled 1,000 word pairs from the corpus, and a human annotator labeled them as transliterations or negative examples. We then used simple grid search to tune the threshold parameters used to select the negative examples by optimizing mining performance on the development data in terms of the f-score. Since this tech-nique is outside the scope of the NEWS2010 workshop, we applied it to the additional Japanese-English dataset we collected. We compare these results from using this approach to those model-based selection approach described in Section 5.4.1.
The results on the test data are shown in Figure 5 and only show a small im-provement in performance through the use of a development set to tune the selection parameters for negative examples. This suggests that it may be possible to use our model-based selection approach without the need to annotate a held-out set of development data. 5.4.3. Synthesizing Negative Examples from Positive Examples. Following Jiampojamarn et al. [2010], we investigated two other methods of generating negative examples. These methods create a large set of incorrect candidates by pairing each source se-quence in the seed data, with every target sequence except the correct target. In the first method of selecting negative examples, this large set of candidates is reduced to a smaller set by filtering out those candidates in which the source and target sequences are not phonetically similar X  X honetic similarity being measured as using the longest common subsequence ratio (LCSR) of the romanized forms. In our experiments, we ad-justed this threshold so that the same number of negative samples were generated in each case (10,000 samples). This approach generates negative examples that are sim-ilar to the positive examples, and it can be argued this is advantageous for training a discriminator.

The second approach simply takes a random sample from the large set of candi-dates. This approach generates samples that more closely approximate the similarity of examples in the real data. Plots for each of these methods and also our model-based approach are shown in the graphs of Figure 6. Figure 6 and Table IV present the results of our main experiment. Since the mixture of positive and negative examples in the test data is not known a priori, we provide results from our system for a range of values of the classification threshold on the output of the SVM. This gives precision/recall curves for each of the strategies for generating negative examples: our proposed approach, the approach based on LCSR, and the approach based on random sampling. The precision/recall/f-score trade-off for En-Ru is shown in Figure 7. As a reference, we plot a point for the precision and recall of the top-ranked system in the NEWS2010 transliteration shared task to represent the current state of the art. The graph on the bottom right shows similar results on a new English-Japanese task that we constructed in a similar manner to the NEWS workshop tasks.

It is clear that the results of English to Chinese are anomalous. The results on this task were very dependent on the strategy for choosing negative examples, and only the random sampling technique was effective. The English-Chinese task differs from the other language pairs in two important respects. First, in the data, as supplied for the task, there is no segmentation information on the Chinese side; whereas other lan-guages contained word boundaries. We would not expect this to pose problems for our technique which performs unsupervised segmentation of both source and target during the alignment process. The second respect in which this language pair differs is that the grapheme vocabulary size is much larger for Chinese than for the other languages. We believe this is the cause of the anomalous result and that the larger vocabulary size requires a larger amount of training data to build models that can function effectively. Choosing similar examples by using the prosed technique or the technique based on LCSR will reduce the variety of kanji seen in the negative examples, and this could handicap the models where the data size is too small.

On all the other language pairs, our proposed strategy for selecting negative examples performs as well as or better than the other strategies. Of the other two strategies, the method based on LCSR is generally the better approach. Moreover, our results show that our system is able to offer performance comparable to the state-of-the-art systems on these language pairs. For the English-Arabic and English-Tamil tasks in particular, our strategy for selecting negative examples offers higher scores in terms of both precision and recall than the other strategies. As will be discussed in Section 5.6, our approach typically makes errors on sequence pairs that are genuine but contain novel subsequences of graphemes for which our model has no correspond-ing sequence pair. Feature f 3 in our model was designed to address this issue by bal-ancing evidence from the lengths of the  X  X ad X  segments in the pairs against evidence from the lengths of the  X  X ood X , the idea being that an unobserved sequence pair within a much larger context of observed sequence pairs is likely to be a correct but novel alignment, rather than an incorrect alignment. Nonetheless some errors of this type remain, but the frequency of type of error can be expected to decrease with training set size.

We created a new task for our experiments based on English-Japanese data. Text from the titles of Wikipedia interlanguage links was used as the data to be mined, and we used a set of English-Katakana pairs from the publicly available EDict dictio-nary 2 to create the seed data. Four thousand pairs of interlanguage links were used, 1,000 of which were hand-annotated as correct or incorrect transliteration pairs and used as test data. One thousand seed pairs were selected randomly from the bilin-gual dictionary. The oracle performance (i.e., the performance of the configuration with the highest F-score representing an upper bound for system performance) on this dataset and the NEWS2010 tasks, together with the performance of the best perform-ing NEWS2010 systems, is shown in Table IV.

The precision and recall curves for the En-Ja (shown in Figure 6), together with the oracle score (shown in Table IV), indicate that mining Japanese ought to be performed reasonably effectively, relative to the language pairs used in the NEWS2010 tasks. All techniques for choosing negative examples were effective here; our proposed approach and the LCSR approach slightly outperforming random sampling. The English-Japanese precision/recall indicate that the automatic mining of English-Japanese transliteration pairs should be fruitful. We believe it would be possible to mine English-Japanese pairs at high-levels of precision and recall. In our experiments, for example, close to 100% precision can be achieved whilst still maintaining 70% recall.

In order to analyze the contributions of each of the features to the model of the first experiment, we later conducted a set of experiments on the English-Russian data in which features were removed from the model. The results are shown in Table V. The results indicate that the greatest contribution was made by f 1, with feature f 4 making the second most important contribution. Surprisingly, removing feature f 3 made no difference to the performance of the system, indicating that it provided very little additional information over and above that provided by the other features in the model. Similarly, removing feature f 2 had no effect on the system X  X  performance. Table VI shows a sample of typical errors made by our proposed method. Many of the errors producing false positives are due to the entire source being able to be aligned to part of the target (e.g.,  X  X ouda X  in the table), or vice versa (e.g.,  X  X uskmelon X ). In the  X  X ullshit X  example, the target is not a transliteration and has arisen from the Wikipedia interlanguage link for the card game  X  X ullshit X  which is known as DA-U-TO (doubt) in Japanese. In this case, a partial alignment was possible, since the final  X  X  X  can align to  X  X O X , and although the probability of aligning the remainder was low, it wasn X  X  suffi-cient to cause this to be labeled as negative. We believe that both of these issues could be addressed by allowing null alignments in our alignment model and introducing a new feature into the classifier to guage the degree of null alignment in the word pair. The  X  X elevision X  example is an error that has been caused by similar reasons. In this example, the Japanese word is not a direct transliteration but a contracted transliter-ation that is far easier to say. The co-segmentation/alignment of this word pair along with the scores for each sequence pair is shown in Figure 8. It can be seen from the figure that parts of the Japanese word  X  X E X  and  X  X E X  align with high probability (the log probabilities are  X  5.2 and  X  5.3, respectively) to  X  X e X  and  X  X e X , but  X  X I X  (the contracted form of the full transliteration BI-SHI-YO-N) and  X  X ision X  cannot be aligned without introducing a new, low-probability sequence pair (the log probability of this pair is  X  37.91) into the derivation. However, enough of the characters have been aligned with high probability to allow the word pair to be labeled as positive in spite of the lower probability component.

The true negatives seem to be coming form irregular phonetics, for example,  X  X nome X  where the initial  X  X  X  isn X  X  pronounced in English,  X  X udzu X  where the  X  X  X  has not been represented in the Japanese form, and  X  X ettuce X  which has an unusual phonetic tran-scription. We show the bilingual alignment for both  X  X nome X  and  X  X ettuce X  in Figure 8. In the case of  X  X nome X , it is clear that the first sequence pair has very low probability, as expected. In the case of  X  X ettuce X  the derivation consists of three sequence pairs, two of which have high probability because they have been observed several times in the corpus. The third sequence pair (ttu/ (TA)), which is rare and has not occurred in the corpus, has been assigned a low probability. The other true negatives have arisen from the loan words  X  X idet X  and  X  X ousseau X  that have been borrowed from French. Again we provide the alignment for  X  X idet X  in Figure 8. It is clear that  X  X i X  has been easy to model, as it is pronounced similarly to the English, but  X  X et X  is pronounced differently in French and as a consequence is modeled with low probability. We feel that this issue is common enough to warrant modifying our approach, and in the future we would like to address this by modeling the source language directly in the Bayesian alignment process. One potential problem with the approach used in the earlier experiments is that we ap-ply the Bayesian alignment process to noisy data, and this noise might add support to data similar in character to itself leading to a decrease in mining performance. Sajjad et al. [2012] present an elegant solution to this problem by building an explicit model of the noise and mining in two stages. In the first stage, they employ all of the data to align the labeled seed data. In the second stage, they estimate a smoothed probability distribution from the counts for the bilingual sequence pairs from both labeled and un-labeled data. This smoothing process gives a penalty to bilingual sequence pairs that occur in the unlabeled data but do not occur in the labeled data.

Another approach is to use a bootstrapping procedure to keep the data used to build the model as clean as possible. We performed a bootstrapping experiment on the English-Japanese data. Initially, only the seed data were aligned; subsequently, using the same mining technique described earlier in the article, small amounts of mostly clean data were selected from the data to be mined and added to the seed data to form a new set of clean data. This set of clean data was then aligned, and the process proceeds iteratively in this manner gradually adding cleaned data to the seed set. We added 10% of the data to the seed set in each bootstrapping iteration. The results are shown in Figure 9. From the figure, in can be seen that the model using only the seed data is outperformed by models that used more data and that the system improved rapidly during the bootstrapping process. The plots are somewhat noisy, but it appears that our approach is robust to noise in the data being mined. However, we believe that this is due to the nature of the Wikipedia data and that in general, the effectiveness of using bootstrapping will depend on the characteristics of the data being aligned. Some types of noise in the data will have little impact on the performance. For example, the translation elephant (English) and (ZOU) (Japanese) introduced the improbable sequence pairs (elep/ (ZO)) and (hant/ (U)) into our model. These are unlikely to occur in unseen data and have no effect on the segmentation or scoring of word pairs that do not contain these sequence pairs. However, in some datasets (e.g., data from bilingual dictionaries) large sets of related words having long subsequences of char-acters in common can support each other in the mining process if the corresponding words in the other language are also similar (which is often the case). Therefore, even though our bootstrapping did not yield any noticeable improvement on Wikipedia data, we believe that in general, the mining process will require strategies to reduce the effect of the noise in the data. Furthermore, although the approach we have taken is language independent, we believe that for mining from languages with low resources, techniques that are able to bootstrap from models incorporating human knowledge of phonetics may be necessary. In this article, we have presented a novel approach to identifying transliteration word pairs for transliteration mining based on features derived from a nonparamet-ric Bayesian process that simultaneously cosegments and force-aligns grapheme se-quences within the words. Our approach is simple and relies on no language-specific information about the two languages involved and will operate directly on grapheme sequences in the languages X  native scripts; it is not dependent on the existence of a method for romanizing either language. Furthermore, our method performs auto-matic alignment of both source and target sequences, eliminating any requirement for language-specific segmentation schemes.

The Bayesian alignment model we use strongly rewards the reuse of features already present in its model, leading to compact models that tend to segment in a consistent manner. A key advantage of our approach is that these characteristics allow us to train useful models based on many-to-many alignments, which are necessary to mine transliterations from many language pairs with non-Roman scripts where one-to-one alignments are not appropriate.

According to our model, the probability of unobserved bilingual subsequence pairs of the word pairs decreases exponentially with the length of their component source and target grapheme sequences, making the larger unobserved sections of the translit-eration pair candidates extremely unlikely. Our idea relies on the assumption that genuine transliteration pairs can be explained by concatenating bilingual grapheme sequence pairs that have occurred (preferably frequently) in the training data. The ability of the generative model to assign a probability to any unseen sequence-pair gives the technique the ability to score candidate transliteration data.
 We evaluated our approach on all of the transliteration mining tracks of the NEWS2010 Named Entity Workshop shared task. Our system, in spite of its simplicity, achieved performance comparable to the state-of-the-art systems on this task, indicat-ing that the features derived from the Bayesian forced alignment are strongly predic-tive in classifying transliteration pairs. We also proposed a model-based approach to selecting negative examples to train the classifier used for predicting the validity of a candidate transliteration pair. This method exploits the natural clustering of the data induced by features from the Bayesian alignement. Our experimental results show that this technique is in most cases able to match or better existing approaches to this problem.

This article also contributes a new set of results on an English-Japanese dataset we constructed in a similar manner to the NEWS workshop datasets. Our results indicate that mining English-Japanese transliteration pairs should be possible at high levels of precision and recall using the techniques proposed.

In future research, we would like to extend the scope of our work to integrate it into a broader framework to be used for mining named entity pairs (including but not limited to transliteration pairs) that will be used to improve a named entity translation system and to integrate this into an industrial machine translation system. We would also like explore methods for overcoming the effects of noise in the data by integrating human knowledge for resource-poor languages and study the effectiveness of introducing an explicit noise model into the Bayesian alignment framework. In addition, we intend to enhance the Bayesian model used to align the grapheme sequences by investigating the use of higher-order models, allowing null alignments, phonetic information, and alternative models for the base measure.

