 With the fast growth of text data in management systems and on the web, it is essential to develop efficient methods to organize the content of text documents. Text clustering is a fundamental method for organizing large number of documents, and can also be used to enhance the performance of information retrieval systems. However, most of the previous text clustering algorithms are based on the  X  X ag of words X  (BOW) model in which the semantic relations between the words are not considered. In BOW approach, a document is represented by its individual terms and their frequencies in the document X  X  content. However, two documents describing the same topic may falsely be categorized into different clusters if they use different sets One way to overcome this drawback is to use background knowledge to enrich the documents. 
WordNet, ODP, and Wikipedia have already been used to improve document clustering. WordNet has limited coverage. It mostly contains common words but not scarce ones. In that sense, using WordNet may cause information loss [10]. In contrast, Wikipedia is a large online repository which contains millions of articles and is regularly updated. It has a widespread coverage over different concepts and is more complete than other resources. In this paper we propose to utilize Wikipedia to enhance the results of text clustering. There have been few attempts to use Wikipedia to improve text clustering [9]. These methods mostly extract features from Wikipedia for documents to enrich their contents [10]. 
In this research, we adopt a language modeling approach for text clustering and propose to smooth the language models of text documents using Wikipedia articles in order to enhance the content of the documents. Language modeling has recently been successfully applied to many different information retrieval and text mining problems [13] and smoothing has been shown to be a critical part for many of these solutions. A to that topic. As a result, it becomes unlikely to have all related words in the language Smoothing refers to the adjustment of the maximum likelihood estimator of a language model so that it will be more accurate [14]. When estimating a language model based on a limited amount of text, smoothing is extremely important. 
In this paper, we propose two smoothing methods and for each method, we define a neighborhood of the Wikipedia articles, the target neighborhood, for each text document to smooth the content of documents. In the first method we extract the top features of each document, which are the terms, and define the target neighborhood in representatives of the document, document expansion based on these features can enhance the document content. In the second approach the content similarity of the documents and Wikipedia articles are used to define the neighborhoods. We also propose to combine the two methods and use both neighborhood sets for smoothing to benefit from the advantages of both methods. 
After smoothing, we model the smoothed documents by a graph constructed upon the language models, and perform link-based clustering on the resultant graph. For link-based clustering, we use a link-based algorithm proposed in [12]. For baseline we use the K-Means algorithm on the BOW model. Our experiments on Reuters-21578 [15] and 20-newsgroup (20NG) [16] show that both proposed smoothing methods improve the clustering results, and the combination of them provides the best performance, confirming the power of proposed methods in enriching document contents using Wikipedia articles. Link-based clustering of the documents has also shown to be superior to the traditional content based clustering method, K-Means. 
The rest of this paper is organized as follows. In section 2 we review some related works. Section 3 introduces the proposed smoothing methods based on Wikipedia articles and presents construction of the document similarity graph and link-based clustering algorithm. Experimental results are described in section 4. Finally we discuss our main results and conclude the paper in section 5. A number of different methods have been applied to improve basic text clustering algorithms. Recently the research on utilizing background knowledge and ontology to enhance text mining tasks such as text classification and clustering is increasing. 
In [1], WordNet is used to improve text cl ustering. They enrich the term vector of each document using three different strategies based on WordNet concepts. Another algorithm that represents the vector of te xt documents using WordNet is introduced in [2]. Both these researches somehow resolve the problem of BOW model. But they have some limitations. WordNet covers a limited number of words and is focused on common words not rare ones. Another problem is that most of the descriptions presented by WordNet for each term are too short. Another resource studied in recent researches is Wikipedia. Though using Wikipedia is more difficult because it is not as structured as WordNet, but researches on utilizing Wikipedia to enhance text mining are growing recently because it is wide-spreading and rectifies the limitations of WordNet. In [3] and [4], a method was proposed to incorporate encyclopedia knowledge with text classification systems. Wikipedia was also used in [5] to improve text categorization. Other text mining tasks have been enhanced by Wikipedia, such as cluster labeling [6]. Wikipedia can also improve information retrieval, for example in [7] Wikipedia was used for query expansion and relevance feedback. 
In addition, Wikipedia has been used for text clustering. The representation of short text documents are improved using Wikipedia features in [8]. In [9] an algorithm is presented which maps the terms in the text documents into Wikipedia concepts and the term vector of the document is reduced to a vector of Wikipedia concepts. They also compare Wikipedia with WordNet and show that most of the terms in a document that are appropriate features for the document are not covered in WordNet but they are included in Wikipedia concepts. 
Beside the concepts of Wikipedia, the categories of the articles have also been utilized to improve text clustering [10]. Although these methods show improvement in text clustering using Wikipedia, they have some limitations. They use Wikipedia to add new features to the document mostly using concepts and categories of the articles. But Wikipedia has a rich content for each article that can also be used to improve text clustering. In this paper we use the whole content and the categories of Wikipedia articles to smooth the language models of text document to enrich the documents and enhance text clustering. In this paper we propose to utilize Wikipedia to improve text clustering. Our approach is to smooth the language models of the text documents using Wikipedia articles in order to enrich the documents. We then construct a document similarity graph using the method proposed in [11] and based on the contents of the enriched documents. The document similarity graph is a graph of documents citing each other, where the weighted links are induced from the new, improved contents of documents. We finally cluster the documents using the result graph. 
For the first step we propose two smoothing methods and for each method, we define a neighborhood of the Wikipedia articles for any of the text documents. Next, we smooth each document using the given set. Wikipedia articles have different parts; in both proposed methods we choose to use the content and the categories of the articles for smoothing and discuss the effects of each part. The contents of Wikipedia articles are very rich as they are encyclopedia knowledge on the subject of the articles; and thus can be used for expansion and enriching text documents. Wikipedia contains a hierarchical categorization system , and each article belongs to at least one category. The category information of each article is an appropriate summarized narration of its subject and therefore is suitable for enriching documents. 
In the first proposed method,  X  X op-Feature Smoothing X , we extract the top features of each document and define the target neighbor hood in Wikipedia using these features. In the second approach,  X  X imilarity Smoothing X , the content similarity of the documents and Wikipedia articles are used to define the neighborhoods. We also propose to combine the two methods and use both neighborhood sets for smoothing to benefit from advantages of both methods. The details of these algorithms are described below. 3.1 Top-Feature Smoothing Algorithm For this algorithm we first extract the top m features of each document. We rank the document features using: where, w is a word in the document d and Score(w, d) is the score of w as a feature of document d indicating the importance of w in d . TF(w, d) is the raw frequency of w in select the top m words as top features of the document: f 1 , f 2 ... f m . document d . For this purpose, we match each feature f i with Wikipedia concepts, considering the titles of Wikipedia articles as Wikipedia concepts. We bring in all Wikipedia articles whose titles match f i and construct a set of Wikipedia articles for each feature. The neighborhood set of d , named S , is defined as the union of all these sets and raw content of the document is smoothed using Dirichlet prior smoothing method [14]: contents of the whole collection: maximum likelihood estimation, and |d| is the length of the document. Here,  X  and  X  are smoothing parameters; and p(w | S) is computed in two ways: 1. TopF-Content: In this method, we only use the content of the Wikipedia articles for smoothing, and p(w | S) is: features of the document have different importance compared to each other, we combine their language models in a weighted manner, and define  X  i as: 2. TopF-Content&amp;Cat: As described before, beside the content of the Wikipedia articles, we also use the categories of each article for smoothing. We combine the language model of the articles with the language model of categories. For this purpose, we define p ML (w | Cat fi ) as the probability of w occurring in the categories of method, p(w | S) is calculated as follows: combining the probabilities. 3.2 Similarity Smoothing Algorithm In our second smoothing method, the language model similarities are used. We define the distance between two text documents based on the Kullback-Leibler divergence (KL-divergence) measure between their language models: measure defined above between each document and all Wikipedia articles. Then we smooth the documents using the two stage smoothing method described in section 3.1, equations (2) and (3); only the parameter p(w | S) is different in this method; it is computed in two ways: 1. Sim-Content: In this method we use the contents of Wikipedia articles weighted based on the similarity between the articles and each document; so p(w | S) is: where a i is a Wikipedia article and the summation is performed on all Wikipedia articles for the document d . This summation is weighted by the KL-divergence distance between the document and Wikipedia articles, so the articles which are more similar to d will have more effect on its language model: 2. Sim-Content&amp;Cat: In this approach, both content and categories of the Wikipedia articles are used for smoothing. Therefore p( w | S) is: the language models. 3.3 Combination of the Two Methods  X  X op-Feature Smoothing X  and  X  X imilarity Smoothing X  algorithms can be used together for smoothing a document taking advantage of both methods. Similarity smoothing benefit from all Wikipedia articles, while Top-Feature smoothing increases the effect of the words resembling top features of the document. We again use the two stage smoothing formula in section 3.1; except in this method p(w | S) is: Here,  X  is a parameter to control the effect of each part. 3.4 Construction of the Document Similarity Graph and Link-Based Clustering After smoothing, we model the smoothed documents with a directed graph by using the method proposed in [11]. This method considers each document as a query q ; and then defines D init as a set of top documents returned by some initial retrieval algorithm in response to the query q . As defined in [11], the top n generators of a document o  X  D init , denoted TopGen(o) , is the set of n documents g p (o) , where, p g (.) is the unigram language model induced from g . Then the graph is defined as: So the links created between document o and other documents are weighted based on the probability which their induced language model assigns to o . Because the links are weighted, we set n = |D init | and consider all generators instead of selecting top n . Then we cluster the resultant graph to achieve a clustering for the documents. In all parts of this paper when we talk about modeling documents with a graph, this algorithm is used. To cluster the graph of the documents, we applied a link-based clustering method [12]. This algorithm needs an initial clustering of all nodes of the graph as its x n based on the score defined as: Score(n i , c m ) =  X  j [(m i,j + m j,i )*x j, m ]. reassigned iteratively until no reassignment is possible or a specific number of iterations have been performed. After smoothing the documents and modeling them with a graph as described before, we use this algorithm to cluster the resultant graph. the best results were achieved when the results of K-Means were given as input to the algorithm. Thus, in all experiments on graph clustering we used the result of K-Means as input of the graph clustering algorithm. The Wikipedia articles are available in form of database dumps and are updated periodically. They can be downloaded from http://download.wikipedia.org. The 3,000,000 articles. The experiments are performed on two datasets: Reuters-21578 [15] which contains 21,578 documents and 135 categories and 20-newagroup (20-NG) [16] with 19,997 documents and 20 classes. 4.1 Evaluation Metrics score [18] and normalized mutual information (NMI) [19]. Given a dataset with N documents and M classes in which documents are labeled as C = (c 1 , c 2 , ... c M ) , if our defined as: Purity is used extensively in clustering tasks but it increases as the number of clusters also use other metrics for evaluating the clustering. The f-score measure combines precision and recall: F-score = [2 (precision * recall)] / (precision + recall); and NMI labels (C) : where X is a random variable for cluster assignments and Y is a random variable for class labels. Unlike purity, when the number of the clusters increases, NMI does not necessarily grow. The values of the three measures described in this section range between 0 and 1, and the higher values indicate better clustering results. 4.2 Baselines large. Thus we need a clustering algorithm which can address these two issues. K-or a specific number of iterations has been performed. The time complexity of K-Means is K-Means clustering method with cosine similarity measure. We also perform another results achieved on baseline experiments are shown in Table 1. 
We conducted several experiments to evaluate our proposed algorithms. The details of the experiments are described in the remainder of this section. 4.3 K-Means Clustering of the Smoothed Documents After smoothing documents using the three methods described in section 3, and before construction of the similarity graph, we clustered documents using K-Means algorithm. In the first set of experiments, K-Means-TopF-Content , we used K-Means to cluster the documents smoothed using the contents of Wikipedia articles and specifically the proposed TopF-Content smoothing algorithm . In the second set of experiments, K-Means-TopF-Content&amp;Cat, we applied K-Means to documents smoothed with the contents as well as categories of Wikipedia articles, the proposed TopF-Content&amp;Cat algorithm . The results on the Reuters-21578 dataset are shown in Table 2 (Numbers in parentheses show the percentage of improvement compared with the baseline experiment presented in each table). 
In Top-Feature smoothing we tested different values for parameter m and the best result was achieved for m = 8 . Thus in all experiments with this smoothing algorithm, [14]. In the second smoothing step, we set  X  = 0.5 . Parameter  X  for combining content and categories is set to 0.1 ; and  X  = 0.5 in the combination of two smoothing methods so that both algorithms have same effect in the combination. enhanced after applying Top-Feature smoothing. The same experiments are performed for Similarity smoothing algorithm and the combination of the two smoothing methods: K-Means-Sim-Content: applying Sim-Content smoothing and cluster documents by K-Means algorithm. K-Means-Sim-Content&amp;Cat: applying Sim-Content&amp;Cat smoothing and cluster documents by K-Means algorithm. K-Means-Combination: applying combination of TopF-Content&amp;Cat smoothing method with Sim-Content&amp;Cat smoothing algorithm. The results of the experiments are shown in Table 3. 
As indicated in Table 3, Similarity smoothing improves text clustering results. The combination of the two smoothing methods improves clustering purity by the factor of 11% compared to the baseline which is the best performance among these experiments. From the results in Tables 2 and 3, in both smoothing algorithms, adding categories to the content for smoothing works better than using content only; we can also see that Top-Feature smoothing achieves more improvement than Similarity smoothing algorithm, on the Reuters-21578 dataset. 4.4 Link-Based Clustering of the Smoothed Documents Further, we construct the document similarity graph and apply link-based clustering as described in section 3.4. At this point, we performed the following experiments: Link-based-TopF-Content: applying TopF-Content algorithm for smoothing. Link-based-TopF-Content&amp;Cat: applying TopF-Content&amp;Cat algorithm for smoothing. 
The results in Table 4 indicate that not only Top-Feature smoothing enhances clustering performance, but also link-based clustering of documents has shown improvement, compared to the results in Table 2. We performed similar experiments for Similarity smoothing algorithm and the combination of the two smoothing methods: Link-based-Sim-Content: applying Sim-Content algorithm for smoothing. Link-based-Sim-Content&amp;Cat: applying Sim-Content&amp;Cat algorithm for smoothing. Link-based-Combination: applying combination of TopF-Content&amp;Cat method with Sim-Content&amp;Cat algorithm for smoothing. The results are shown in Table 5. 
From the performance results in Table 5, we observe that the combination of the two smoothing algorithms improves clustering results more than individual smoothing algorithms. Though Similarity smoothing algorithm has improved clustering results but compared to Top-Feature smoothing it achieved less improvement. Among all proposed methods in this research, Link-based-Combination has the best results with the three metrics and improved purity of the K-Means clustering results by the factor of 16.9%. The comparison of Link-Com results with K-Means is shown in Table 6. 4.5 Experiments on 20-NG Dataset To compare our results with previous clustering methods which utilized Wikipedia, we performed the experiments on 20-NG dataset and compared the results f our proposed methods with the algorithms presented in [10]. The results are summarized in Table 7. The Word-Category and Word-Concept-Category methods are proposed in [10] and had the best results in that research. Improvements are computed compared to the K-Means baseline. As indicated in the Table 7, Similarity smoothing algorithm outperforms Top-Feature smoothing algorithm on 20-NG dataset, unlike Reuters-21578. Since the performance of Top-Feature smoothing depends on the top features extracted from the documents, this can be because the feature extraction method did not get acceptable results on 20-NG dataset. This issue is to be examined in the future works. Using categories in addition to the content of the Wikipedia articles improves clustering more than using content only. The combination of both smoothing algorithms ( Link-based-Combination ) gives the best improvement (8.5% in purity) among all other experiments. Compared to Word-Category and Word-Concept-Category , that map text documents to Wikipedia concepts and categories, our best algorithm ( Link-based-Combination ) achieved more improvement based on three measures. Besides, in [10] the authors did not obtain any improvement without using categories, but our smoothing algorithms which used content only, had improvements based on all three metrics; because we utilize the whole content of Wikipedia articles articles). In this paper, we adopt a language modeling approach for text clustering and propose to enhance the content of the documents. We propose two smoothing methods and for each smooth the content of documents. In the first method we extract the top features of each second approach the content similarity of the documents and Wikipedia articles are used to define the neighborhoods. We also propose to combine the two methods and use both smoothing, we construct a document similarity graph using the enriched contents of documents, and perform link-based clustering on the resultant graph. 
The proposed algorithms are tested on two datasets: Reuters-21578 and 20NG. In order to evaluate the effectiveness of proposed smoothing methods on clustering we perform different experiments and compare them with K-Means clustering algorithm as the baseline. Based on the empirical results, both proposed smoothing algorithms algorithm achieves more improvement than Similarity smoothing algorithm, on Reuters-21578 dataset; and, Similarity smoothing algorithm outperforms Top-Feature smoothing algorithm on 20NG dataset. Since the performance of Top-Feature smoothing depends on the top features extracted from the documents, this can be because the feature examined in the future works. Another conclusion obtained from experimental results is improves text clustering performance. The combination of both smoothing algorithms achieves the best improvement, taking advantage of both methods. Similarity smoothing benefit from all Wikipedia articles, while Top Feature smoothing increases the effect of the words resembling top features of the document. 
Link-based clustering of the documents has also shown to be superior to the traditional content based clustering method, K-Means. Based on the experimental results, creating content similarity graph and performing link-based clustering generates better text clustering results. Our best algorithm (Link-based-Combination) achieves 16.9% and 8.5% improvement in purity compared with K-Means on Reuters-21578 and 20NG, respectively. Comparison with existing text clustering methods using Wikipedia has shown that our proposed smoothing approach is more effective in text clustering results; compared with concepts of Wikipedia (only titles of the articles). The results achieved in this research can be extended to other applications such as text classification and the proposed algorithms more accurate. Moreover, we will study how to utilize other parts of Wikipedia such as hyperlinks, to improve text clustering. Acknowledgments. This research is partially supported by Iran Telecommunication Research Center (ITRC). 
