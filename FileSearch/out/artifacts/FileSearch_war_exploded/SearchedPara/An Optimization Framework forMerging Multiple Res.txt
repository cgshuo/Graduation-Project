 Developing effective methods for fusing multiple ranked lists of documents is crucial to many applications. Federated web search, for instance, has become a common practice where a query is issued to different verticals and a single ranked list of blended results is created. While federated search is regarded as collection fusion, data fusion techniques aim at improving search coverage and precision by combining mul-tiple search runs on a single document collection. In this paper, we study in depth and extend a neural network-based approach, LambdaMerge [32], for merging results of ranked lists drawn from one (i.e., data fusion) or more (i.e., col-lection fusion) verticals. The proposed model considers the impact of the quality of documents, ranked lists and verti-cals for producing the final merged result in an optimization framework. We further investigate the potential of incor-porating deep structures into the model with an aim of de-termining better combinations of different evidence. In the experiments on collection fusion and data fusion, the pro-posed approach significantly outperforms several standard baselines and state-of-the-art learning-based approaches. H.3.3 [ Information Systems ]: Information Search and Re-trieval Data fusion; collection fusion; learning to merge; deep neural network
Developing effective methods for fusing multiple result lists into one is beneficial to many applications such as web-scale federated search. A large body of fusing techniques has been widely studied, and can be broadly categorized into two tasks, namely data fusion and collection fusion. While both tasks seek an optimal blending of results from different c  X  2015 ACM. ISBN 978-1-4503-3794-6/15/10...$15.00 lists, collection fusion has a focus on merging results re-trieved simultaneously from multiple collections. Collection fusion, often referred to interchangeably as federated search, is preferred over centralized search alternatives for reasons such as efficiency [35]. In addition, federated search systems can search the content of the hidden web without crawling, producing results comprised of answers returned from mul-tiple verticals [35]. Collection fusion techniques are mostly concentrated on approximating comparable document scores from different repositories [9, 38, 36]. In recent years, the TREC federated web search track established reusable test sets from real web search engines of different verticals [13, 14]. Better performing approaches for the result merging task in this track take into account combined signals such as the number of engines containing a document [31] and text similarity [18].

Data fusion, on the other hand, often refers to tasks where the aim is to merge results retrieved from a single document collection. Compared with individual ranking systems, data fusion provides the advantage that the merged list usually has a higher precision and recall [12]. Known as the chorus effect [39], many effective fusion methods are based on the premise that documents that are highly ranked in many of the lists are likely to be relevant [1, 2, 17, 24]. Recent de-velopments in data fusion algorithms rely on using training queries to better estimate the probability of relevance using signals such as document ranks and scores [25, 34, 26].
Recent work [32] on results fusion presents the first at-tempt that uses relevance judgments to directly optimize re-trieval performance metrics such as MAP or NDCG. In par-ticular, Sheldon et al [32] merged results from multiple query reformulations to improve search while mitigating risks such as query topic drift. This approach, LambdaMerge (or  X  -Merge), can be classified as a data fusion technique since search runs from different reformulations are conducted on the equivalent of a single collection 1 .  X  -Merge learns a scor-ing function to rank documents by combining features indi-cating document relevance (e.g., retrieval score or rank of a document) with features indicating the quality of the refor-mulation and its results (e.g., query clarity and drift).
While  X  -Merge has demonstrated strong performance in data fusion, it is unclear how an effective adaption to col-lection fusion can be carried out. In web-scale federated search, it is beneficial to understand the right medium and identify appropriate verticals for a query. System efficiency,
The industrial level of document indices should be dis-tributed, but experiments conducted in  X  -Merge assumes the presence of every document in every list. for instance, can be improved when a query is only issued to a number of top verticals. Ranking can further be guided by preferring certain verticals among those selected, result-ing in better search performance 2 . In this paper, we address the task of result merging and extend  X  -Merge in a way that the new model accommodates and considers the im-pact of drawing results from different verticals. Specifically, the proposed approach learns a scoring function character-ized by not only the quality of documents and resources (i.e., ranked lists produced by different search engines), but verticals that contain the most relevant content. The final ranking of blended results is determined by directly optimiz-ing the overall retrieval effectiveness.

Inspired by the recent success of deep structures [5, 16, 20], our model further investigates the potential of com-bining different ranking evidence using this approach. In particular, we incorporate a deep neural network (DNN) for learning the scoring function. The structure of a DNN is highly simple and generative, and can be applied to most classification and optimization problems given proper de-sign of the loss function in the output layer. We note that the focus of previous studies is mostly around learning rep-resentation [15, 20, 29]; we propose to apply deep structured models to the fusion task.

We conduct extensive experiments in a variety of fusion scenarios. Specifically, we use two recent collections from the TREC federated web search track, FedWeb13 and Fed-Web14, for testing collection fusion. For data fusion, it is typical to use existing runs submitted to TREC based on which a merged list is created. We experiment with two years of TREC Web Track data, WebTrack09 and Web-Track10, where measures capturing both precision and di-versity (e.g.,  X  -NDCG@k) of a ranked list can be evalu-ated using the sub-topic relevance judgments. The exper-imental results demonstrate that integrating vertical sig-nals significantly improves retrieval effectiveness for collec-tion fusion. We further show that, for both fusion scenar-ios, the performance of models with deep structures consis-tently outperforms standard baselines [17], state-of-the-art approaches [11, 7, 41], and models with shallow structures.
Since  X  -Merge focuses on combining results from different query reformulations, features that quantify reformulation quality and topic drift are unavailable in a more general fu-sion scenario (i.e., only a singe query is present). One contri-bution of our work is to introduce new effective features that are able to characterize the quality of ranked lists. The fea-tures we use are also more semantically informative (e.g., ag-gregated retrieval scores), compared with the coarse features (e.g., bi-grams or tri-grams) used in other deep structured work [15, 20]. Although the amount of training instances in our experiments is relatively small, the higher-level def-inition of our features can make learning an effective deep model possible.

After a discussion of related work, we describe in detail the merging framework in Section 3. We then introduce the tested environments along with the features devised for these collections in Section 4. Section 5 shows the results of fusion experiments using a variety of baselines and the proposed models. We discuss and conclude the paper in Section 6.
For instance, vertical Game may be preferred to Enter-tainment for query Mine Craft although both are deemed relevant.
This paper presents a merging framework that uses deep structures. This section first discusses general related work on merging multiple result lists, followed by a brief introduc-tion to the applications of deep structures in the information retrieval field. Methodologies for fusing multiple search results into a single one can be broadly categorized into two families: data fu-sion and collection fusion. Data fusion refers to tasks where a query is sent to multiple retrieval models that have access to the same document collection, and the rankings from dif-ferent models are merged into a final single list [12, 39, 42]. One main advantage of data fusion is that often the merged list will have greater relevance than the output of any indi-vidual system [3]. Fox and Shaw [17] suggested several com-bination methods; among these, CombSUM and CombMNZ showed better effectiveness by fusing results based on nor-malized document scores and the number of systems contain-ing a document. A follow-up work [24] modified CombMNZ by replacing document scores with ranks. Aslam and Mon-tague [1] proposed to merge documents using the probability of relevance estimated for documents at given ranks. Borda-fuse [2] also merged documents based on the ranks and no training data is required.

More recent developments in data fusion have laid an em-phasis on techniques that use training queries to estimate scores assigned to documents. ProbFuse [25] divides each re-sult list into equal length segments, and uses training queries to estimate the probability that a document returned in a particular segment is relevant. SegFuse [34] takes a simi-lar approach and modifies ProbFuse by allowing variant size of segments. Building on these techniques, SlideFuse [26] introduces a sliding window to estimate relevance probabil-ity. A recent cluster-based fusion approach [22] considers inter-document similarities created across the lists to im-prove merging effectiveness.
 Collection fusion focuses on a related yet somewhat different task. In particular, queries are issued to document collec-tions that are disjoint or partially overlapped, and the re-turned results are integrated and merged into a single list [10, 40]. Result merging is a challenging task as different retrieval algorithms may be applied to different collections that may have different lexicon statistics. Earlier approaches to collection fusion [9] used simple heuristics to normal-ize collection-specific document scores for ranking. More recent methods approximate comparable document scores with higher accuracy. A semi-supervised learning method proposed by Si and Callan [38] trained a regression model for each collection, which is used to map document scores into their global merging scores. Shokouhi and Zobel [36] assumed that the ranking of sampled documents is a sub-ranking of the original collection, and estimated the merging scores by applying curve fitting to the sub-ranking.
Collection fusion and federated search have been often used interchangeably [35]. Web-scale federated search has recently drawn much attention and become a common prac-tice. To promote related research, the TREC federated web search track established reusable test sets from real world search engines and their results [13, 14]. In a web setting, a query is issued to a selected subset of engines (or verticals); this is regarded as the task of resource selection . The re-sult merging step then merges the returned results from the selected resources into a single list. We only focus on the result merging task in this paper. In TREC FedWeb 2013, the best performing approach [31] fuses results by adapting the reciprocal rank fusion approach [11]. For FedWeb 2014, the best performing approach was submitted by the ICT-NET group [18] where they fused documents by combining signals such as ranks and text similarity.
Deep structured models have been successfully applied to many language related applications such as speech recog-nition (SR), natural language processing (NLP) and infor-mation retrieval (IR) [4, 5, 20, 33]. Deep learning tech-niques can learn high level abstractions from training data which has been shown useful for both classification tasks and semantic analysis. General structures of supervised deep networks include the recurrent neural network (RNN), deep stacking network (DSN), deep neural network (DNN) and convolutional neural network (CNN) [16]. RNN de-scribes a network that recurrently combines former acti-vation states of hidden neurons with current input layer features to predict the next output. It has been widely used in SR and NLP tasks [27, 29], but not as much in IR. Most deep learning applications in IR focus on the genera-tion of semantic representations for queries and documents. Deng et al. [15] first reported positive results with DSN models. The semantic representations extracted with DSN have higher retrieval performance than traditional learn-ing to rank algorithms. Huang et al. [20] developed a DNN based architecture to learn semantic models with click-through data. Huang X  X  model, namely the Deep Structured Semantic Model (DSSM), achieved significant improvements compared to many state-of-the-art latent semantic models. Shen et al. [29] further extended Huang X  X  model with a layer of convolution and max-pooling (which is the main idea of CNN) and obtained even better performance than DSSM.
In our work, we choose the general structure of DNN as the network for the fusion task. Compared with other deep learning models, DNN has a structure that is simple and generative. With proper design of the loss function in the output layer, DNN can be applied to most classification and optimization problems, and it is easier to train than DSN. CNN showed better performance than DNN in tasks of la-tent semantic analysis; however, it does not fit our problem since our training instances consist of features that have a fixed number of dimensions as opposed to a sequential struc-ture. Similar to CNN, RNN is also not appropriate for our task, which leaves DNN as our best choice.
Developing effective methods for fusing multiple ranked lists of documents is crucial to many applications. Con-ventionally, unsupervised approaches use retrieval scores or ranks to determine the final ranking of documents in a merged list. These methods have merits that they require no train-ing examples. It is, however, often the case that learning-based approaches achieve better effectiveness, partly because additional features are easy to incorporate and customized objectives can be derived and optimized. This section first introduces an existing learning-to-merge framework,  X  -Merge. We then describe our extension by introducing vertical esti-mation and deep structures.
The  X  -Merge approach is the first proposed merging method that is trained using relevance judgments to directly opti-mize retrieval metrics such as NDCG or MAP [32].  X  -Merge considers, for a query q , a set of reformulations { q 1 ,q (including the original q ), and the goal is to fuse the corre-sponding output result lists { D 1 ,D 2 ,  X  X  X  ,D N } into a single one. For each document d in the i th result list,  X  -Merge utilizes multiple query-document features x d i to capture how relevant d is to query q i . In addition, for the i list returned of the i th reformulation, a vector of query-list features z i is created to estimate the quality of that ranked list. These query-list features mostly focused on characteriz-ing each query reformulation X  X  difficulty and drift, including features such as query clarity score or the distance between original and rewrite queries (e.g., random walk probabilities between queries).

The core components of  X  -Merge consist of a scoring func-tion f ( x d i ;  X  ) and a gating function g ( z i ;  X  ), based on which the parameters  X  for features x d i as well as  X  for features z can be learned. The model defines the final score of a docu-ment d by re-weighting f ( x d i ;  X  ) with the gating component as in Equation 1, where L is the total number of lists.
In the implementation of  X  -Merge, the scoring function f ( x d i ;  X  ) is carried out using a fully connected neural network with a single hidden layer of four neurons, each having a tanh activation function. The hidden units are connected to an output unit, which is then re-weighted by the linear gating function g i . The objective of  X  -Merge is to produce a single fused list with NDCG optimized. The training is done by taking partial derivatives of s d with respect to the network parameters  X  and  X  .
Now we introduce our extensions of  X  -Merge. To apply  X  -Merge to collection fusion, we incorporate an extra com-ponent that captures the quality of a vertical into the frame-work. The idea is that the quality of a vertical in which a document exists can affect the overall relevance of this document to a query. Figure 1 shows the structure of our framework. Suppose that, for a query q , we have L search engines drawn from a set of V verticals, each search engine producing one ranked list in response to q . Typically we have V &lt; L , meaning that multiple engines are adopted for a vertical. The framework in Figure 1 first considers the fea-ture function f ( x d i,j ;  X  ) derived for a document d that is in the i th ranked list belonging to the j th vertical. The model then re-weights f by g i , which is defined as the softmax out-put of the feature function g ( z i ;  X  ) that measures the quality of a ranked list. Finally, we incorporate the estimation for vertical quality by combining g i  X  f with h j . The weight h stands for the softmax output of feature function h ( y that captures the goodness of a vertical given the current search context. Overall, the model produces the final docu-ment score s d in the merged list using Equation 2.
Inspired by the widely reported effectiveness of deep mod-els, our second extension investigates the potential of incor-porating multiple hidden layers in an artificial neural net-work for the fusion task. We take the definition of deep structures to include properties that (1.) multiple layers of nonlinear processing units are incorporated and (2.) the su-pervised or unsupervised learning of feature representations takes place in each layer, with the layers forming a hierarchy from low-level to high-level features [16]. As shown in Fig-ure 1, the implementation of the feature function f ( x d is based on a DNN structure.
 We implement the scoring function f ( x d i,j ;  X  ) using a stan-dard feed-forward artificial neural network that incorporates two or more layers of hidden neurons. Formally, denoting l as the input of intermediate hidden layer k and o as the output of neural network, the projection process can be de-scribed as in Equation 3. Here W k and b k are the projec-tion matrix and bias term for hidden layer k , and a denotes the activation function. The parameter space is therefore  X  = { W k ,b k :  X  k } .
We test the activation function a with both tanh and sig-moid . In practice, no substantial difference was observed and we opt for tanh in each neuron:
Similar to the implementation of f , we design a neural network structure separately for the list feature function g ( z i ;  X  ) =  X  T z i and the vertical feature function h ( y  X  y j . The two networks both follow a standard feed-forward propagation, and respectively taking query-list features z and query-vertical features y j as input layer signals. To de-termine the re-weighting component g i , we propagate g ( z through a softmax smoothing function as shown in Equa-tion 5, where L denotes the number of input rank lists. The same is done for h j using Equation 6. In theory, these two weighting networks can incorporate deep structures as in the document scoring network f ( x d i,j ;  X  ). This will require con-siderably more training data since list and vertical features are sparser than document ones; we thus choose to leave it as a shallow network. The result merging task seeks a single ranked list by fusing multiple ones with an aim of achieving retrieval effectiveness as high as possible. Accordingly, we train the model param-eters  X  ,  X  and  X  to directly optimize NDCG@20 of the fused list using a gradient-based approach. We note that other measures such as MAP or Precision@k can be used.

A problem that arises with the direct optimization of search effectiveness is that the retrieval metrics are discon-tinuous with respect to document scores 3 . To overcome the discontinuity of the objective, Burges et al [6] suggests it is sufficient to define the  X  functions for which some objective C exists (as opposed to defining C directly). This technique is also used by the  X  -Merge model. In our framework, it is sufficient to learn the model by implementing the gradients with respect to the model parameters  X  ,  X  and  X  ; that is, our goals are to compute  X  X   X  X 
The computation for these gradients can be decomposed using chain rule. Take  X  k as an example; we compute  X  X   X  X  [8] as in Equation 7. Here, d j denotes that d has a larger relevance label than j , meaning that d should be ranked
Metrics are determined by the ranks sorted by scores rather than the actual scores. search context. Overall, the model produces the final docu-search context. Overall, the model produces the final docu-above j . |  X  NDCG @20 | denotes the variation in NDCG@20 when the two documents d and j are swapped. document scoring network involves  X  , the gradients can be done as in Equation 8. The update process of  X  X  ( x follows the general algorithm of back-propagation (BP).  X  X 
Likewise, for  X  X   X  X  as we already know  X  X   X  X  in Equations 9 and 10.
Since the output of re-weighting network g ( z i ;  X  ) goes through a softmax smoothing before being incorporated into the model, the partial differential equations are transformed according process of BP. Similar computation can be done for  X  X  j  X  X  replacing Equation 11 with the corresponding parameters.
Our experiments are conducted in both collection and data fusion environments. This section describes the se-lected test sets for each environment, and discusses the fea-tures in detail.
Collection fusion refers to search tasks where a query is issued to disjoint or low overlap document collections, and a single ranked list of blended results is created. To this end, we use test sets derived from the TREC federated web search track in years 2013 and 2014. The main reason for using FedWeb collections is that the track provides test sets in a more realistic web setting. Specifically, the documents were collected from real web search engines as opposed to ar-tificial ones such as those created by dividing existing TREC collections by topic or source [19]. The test sets contain the actual results of approximately 150 real web search engines drawn from a fixed set of 24 verticals, each providing their own retrieval method and heterogeneous content types such as news, travel, social etc. In each year, 50 official queries for the result merging task were created, and up to 10 ranked results were crawled and stored from each of the search en-gines 4 . We note that only the ranks are stored and no score information is available. Graded relevance judgments are gathered for these queries, following the conventions used in TREC web tracks, in 5 levels { Nav, Key, HRel, Rel, NRel } . The main evaluation metric used in the FedWeb track is NDCG@20.

For both years of data, one characteristic of the document collections is that search engines can return duplicate doc-uments, although stored as different document identifiers. This can happen even within a search engine, meaning that this engine unexpectedly returned the same documents. To prevent rewarding merged search results containing dupli-cate (relevant) content, we pre-process the document collec-tions using two steps. First, for each ranked list, we keep only one copy of a document and discard any subsequent du-plicates. The remaining documents are sequentially moved to lower ranks following the original order. After the first pass of processing, we check if duplicates exist across differ-ent ranked lists for a given query. We assign the inter-list du-plicates the same document identifier so as to let the merging framework know that they are actually the same. The offi-cial lists of duplicate documents can be obtained from TREC FedWeb13 and FedWeb14 sites, which were produced based on a number of heuristics such as identical smoothed URLs.
Data fusion techniques combine the ranked lists of multi-ple document retrieval systems to improve search coverage and precision. Different from collection fusion, the retrieval systems operate on the same set of documents in general, and thus the overlap rate across lists is much higher. Con-ventionally, existing runs submitted to TREC are used to evaluate this task. We experiment with two years of TREC Web Track data, WebTrack09 and WebTrack10. Using these newer data sets for experiments provides the advantage that we are able to evaluate not only precision but also the diver-sity of a ranked list based on the sub-topic relevance judg-ments. The test set of each year consists of 50 queries; 71 and 56 runs were submitted to 2009 and 2010 respectively. We randomly sampled 30 ranked lists from each year for the merging experiments.
Our framework inherits the property of  X  -Merge where features approximating document relevance and list quality are employed together for predicting document scores. We further incorporate vertical features in addition to those two categories. Table 1 shows an overview of the feature set used for the experiments, which we describe in detail below. Denoted as x d i,j , document features are used to describe the relation between a query and a document. In Table 1, the rank features correspond to positions where a document is assigned in a list. Reciprocal rank score 1 / (60 + rank ) is incorporated for its reported effectiveness [11].

Typically a document should be promoted in the final ranked list if more search systems deem it relevant. As such,
FedWeb13 stores both snippets and documents of results while FedWeb14 only stores snippets Figure 2: Relation between aggregated document co-exist scores and list performance using Web-Track09. the co-exist feature counts the number of result lists (i.e., search systems) in which a document appears. In the special case of FedWeb test sets, we also consider the property that a document can appear more than once in a list. The number of duplicates of document in a list divided by the original list length is included as a feature, which is denoted as the exist feature.

The score features reflect how relevant a ranker believes a document is to a query. We use two retrieval systems to estimate the score, including a query likelihood model and a sequential dependency model [28]. We also consider the zero-one normalized scores, and the weighted scores com-puted by taking products of co -exist and normalized scores. Finally, the same computation applies to scores from runs submitted to TREC when available.
 Since the focus of  X  -Merge is to combine results from dif-ferent query reformulations, the emphasis for list features is mostly around the reformulation quality and its drift from original query, which is seldom available in a more general Figure 3: Relation between list ratio scores and list performance using WebTrack09. fusion setting. To propose more features that may be useful for quantifying list quality, we conduct a preliminary anal-ysis on the retrieval effectiveness of different ranked lists. We hypothesize that better effectiveness of a list is related to whether the documents in that list are also returned by other search systems frequently. In particular, we compute the average co-exist scores of documents in a list, and plots its relation to the retrieval performance of the correspond-ing lists, as shown in Figure 2. The x-axis is constructed by bucketing the normalized P@30 scores for all ranked lists, and the y-axis shows the corresponding average co -exist scores for lists in that bin. The same is done for the top k results, where k  X  { 10 , 20 , 30 } . Figure 2 shows consistently strong evidence that better performing ranked lists usually have more documents that appear across multiple lists. We accordingly incorporate mCo-exist and mCo-exist-k into the query-list features set. Note that we compute mCo-exist-k only for WebTrack since a ranked list from FedWeb only has up to 10 results.

Based on a similar idea, we correlate the percentage of documents that appear in more than one search engine (i.e., documents with co-exist &gt; 1) with the performance of ranked lists, as shown in Figure 3. We observe that retrieval per-formance is higher when a larger set of documents appear in more than one ranked list (see Ratio-1 ). We further in-spect how the trend changes w.r.t. the coverage over search systems. That is, in addition to examining documents with co-exist &gt; 1, we conduct the same analysis for co-exist larger than k percent of total number of search systems (e.g., we count documents with co-exist &gt; 6 given 30 ranked lists and k = 20%). Compared with Ratio-1 , Ratio-k % in gen-eral shows a similar trend but the gaps between performance buckets appear larger, indicating a higher potential of dis-tinguishing ranked list quality. When k grows to 40% or 60%, we see it is not always true that best performing lists have the largest Ratio-k %. This may imply that the list performance, while being highly positively correlated, is not solely determined by co-existence.

We adopt the mean score features (i.e., mScore ) as in the  X  -Merge work 5 . Lastly, for the FedWeb test sets, we include two additional features for quantifying the quality of ranked lists. Although FedWeb was created by requesting 10 results per search engine, there are cases where less than required were returned, which we hope to capture by the RatioRet feature. Recall that duplicates can be returned by the same engine for a query. RatioDup computes the ratio of number of remained documents after de-duplication to the number returned. These two features are designed to reflect the stability of the black box search engines.
 Intuitively, the quality of a vertical depends on the quality of the ranked lists it contains for a query. We accordingly de-vise features that take the average of list features across the ranked lists a vertical includes. Deriving vertical features is highly related to the task of vertical selection introduced in FewdWeb 2014 [14], where the goal is to classify each query into a fixed set of 24 verticals. The top performing sys-tems of this task assembled together the results of different techniques; for example, the similarity of vertical and query terms [18] and matching WordNet synonyms for queries and verticals [21]. As our focus is to investigate the utility of incorporating vertical estimates rather than exhausting all possible features, we leave more exploration of feature design as future work.
This section first introduces the setup for the experiments, followed by a detailed presentation of evaluation results tested under a variety of scenarios.
Our experiments are conducted using four TREC collec-tions as mentioned in Section 4. In each test case, a total number of 50 queries with associated graded relevance judg-ments are available. When learning is needed, we split train-ing and test data into 5 folds and perform cross-validation. We evaluate using NDCG@K, as well as  X  -NDCG@k when sub-topic relevance judgments are available. We train all neural-based models for 25 epochs with learning rate 5  X  10 Pairwise t-test is conducted when appropriate.
Other moments such as variance, skewness or kurtosis could be considered; we excluded those due to lower effectiveness in our experiments.

We compare the results of our framework with three cat-egories of baselines: Table 2 shows the retrieval performance tested on the Fed-Web 2013 and 2014 collections. We denote our approach as LTM-Deep for that it is essentially a framework for learn-ing to merge using deep structures. In each year X  X  data, we compare the proposed framework with a number of baseline approaches, including two runs of CombMNZ that are gen-erated based on normalized query likelihood or sequential dependency model [28] scores 6 . Among the baselines, we see that RRF is very competitive, even though its core idea is rather simple and it requires no training examples. The LTR group tends to perform better in NDCG@20, which is both the main evaluation metric in the track and the cho-sen optimization metric. In general, LambdaMART appears more effective than RankNet.
Recall that no score information is available in FedWeb; we use two retrieval systems as alternatives. (  X  is denoted for p-value &lt; 0.05).
  X  0.476  X  7.38 % 20.49 %  X  0.475  X  6.89 % 20.84 %  X  0.483  X  11.28 % 18.77 %  X  0.491  X  13.59 % 17.00 %  X  0.567 5.88 % 1.31 %  X  0.572 5.22 % 0.35 %  X  0.485  X  8.63 % 23.17 %  X  0.481  X  8.05 % 24.25 %  X  0.504  X  5.48 % 18.50 %  X  0.503  X  13.26 % 18.74 %  X  0.515  X  13.92 % 15.85 %
For LTM-Deep in FedWeb 2014, the results suggest that incorporating vertical signals (i.e., D+L+V) improves sig-nificantly the performance compared to that without (i.e., D+L). The trend is similar in FedWeb 2013. For other learning-based approaches, including the LTR group and LTM-Shallow, it is clear that the model with vertical es-timate consistently achieves better performance, confirming our hypothesis that vertical quality helps improve the fusion task.

LTM-Deep further outperforms LTM-Shallow in most cases, and the improvement in NDCG@20 is significant in Fed-Web 2014. Despite the scarcity of training data, the results show that merging ranked lists with deep structures can be a promising direction, and we expect the improvement will be more evident with more training data. Overall, LTM-Deep shows the best performance across different baselines, measures and collections.
The experimental results for WebTrack 2009 and 2010 are shown in Table 3. In addition to NDCG@k, we evaluate the results using the diversity measure  X  -NDCG@k based on sub-topic relevance judgments. The vertical component is omitted for data fusion experiments since all ranked lists are from the same vertical. Here, CombMNZ is generated using the normalized scores from runs submitted to TREC. In WebTrack09, RRF and LambdaMART again demonstrate better effectiveness among the baselines, while the RankNet method also appears highly effective in terms of precision.
The LTM group demonstrates overall the most effective results compared with the standard baselines and the LTR group. The LTM-Deep approach further achieves better per-formance than LTM-Shallow in terms of precision and diver-sity for both years. The improvement is significant in Web Track 2010. Similar to the results in collection fusion, LTM-Deep shows the best performance across different baselines, measures and collections.
Model Design . The framework in this paper models simul-taneously the impact of the quality of documents, ranked lists and verticals 7 for result merging. The experimental results suggest that modeling the diverse aspects of feder-ated search improves ranking in general. The design may further include the features derived from specific types of documents. Tweets, for instance, can be extracted with fea-tures that represent recency or popularity, while for a CQA posting, it can be helpful to include authority signals such as the overall rating of an answer. In such cases, the document scoring network f ( x k i,j ;  X , X  1 , X  2 ,..., X  N ) will be paramterized on  X  for shared features (e.g., ranks and scores) and  X  i non-shared features, where N indicates the number of types of documents. Since the FedWeb test collections were com-piled to conform to a uniform style, it will be necessary to get additional data that describes the characteristics of dif-ferent types of documents for evaluating this design. As
We note that vertical features are only available in a fed-erated search scenario. &lt; 0.05).
 Figure 4: Pearson correlation coefficient  X  between list goodness and query-list features on WebTrack10. aforementioned, the deep structures are also possible in the re-weighting networks for list and vertical features. This will require obtaining additional training data.

Feature Design . One reason behind the success of our framework, we believe, is the introduction of additional query-list features. As mentioned in [32], query-list features such as the mean and standard deviation of retrieval scores showed only small improvements, while the more effective features (e.g., reformulation scores) are not available in a general fu-sion task. Indeed, computing the Pearson correlation coeffi-cient between feature values and list goodness 8 , Figure 4 shows that the newly introduced features usually have a higher correlation. In addition, applying the same correla-tion analysis between document labels and query-document features, we re-confirm that the co-exist feature is the most effective in the data fusion environment. For the collec-tion fusion environments, co-exist is far less correlated with ground-truth since the overlap rate between collections is approximately only 3% (as reported in [31]). Interestingly, although co-exist independently is not effective enough, the aggregated query-list feature mCo-exist is highly correlated with list goodness with  X  = 0 . 7 in both FedWeb 2013 and 2014.
We use the average of document labels in a list as a surro-gate to approximate list goodness.

Effective features are critical to system performance. We may explore the feasibility of incorporating the findings from the large body of literature on query performance prediction. In particular, post-retrieval predictors attempt to predict the performance of a ranked list using strategies such as measuring the divergence between the list and the entire collection [23, 37]. Again, since the FedWeb collections use search engines as a black box service, additional access to the entire collection statistics may be required.
In this paper, we explored the task of result merging in an optimization framework. The proposed model incorporated vertical signals on the top of the feature functions for docu-ments and ranked lists, and learned to fuse results by opti-mizing the retrieval performance of the final list. The model further investigated the potential of applying deep neural networks to estimating the document feature function. We also developed a set of general query-list features that are effective and available for a general fusion task, compensat-ing for the absence of specific reformulation features (e.g., query clarity and drift) used in  X  -Merge.

We evaluated our model using two TREC FedWeb test sets for collection fusion, and two TREC Web Track test sets for data fusion. The results demonstrated that incor-porating the vertical signals consistently improves the per-formance using only document and list feature functions. The results further showed that the LTM-Deep approach can significantly outperform standard baselines and state-of-art techniques.

Developing an effective optimization framework for result merging is a complex problem. While our experiments have shown promising results, it will be important for us to con-sider different possibilities in infrastructure and feature de-sign as discussed in Section 5.4. Since the effectiveness of deep structures depends on the quality and quantity of train-ing data, using larger test sets such as query search logs will be important, and we expect the performance can be further improved. This work was supported in part by the Center for Intelli-gent Information Retrieval and in part by NSF IIS-1160894. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
