 accuracy.
 tor [7]: literature [4, 5, 7, 17, 20, 22]. 1.1 Contributions In this paper, we propose a multi-stage procedure based on the Dantzig selector, which estimates allowing to remove incorrect features from the previous supporting feature set. cally, we show: 1) the proposed method can improve the estimation bound of the standard Dantzig numerical experiments validate these theoretical results. 1.2 Related Work accuracy with the same order as LASSO was presented. An approximate equivalence between the LASSO estimator and the Dantzig selector was shown in [1]. In [11], the l  X  convergence rate was studied simultaneously for LASSO and Dantzig estimators in a high-dimensional linear regression model under a mutual coherence assumption. In [9], conditions on the design matrix X under which the LASSO and Dantzig selector coefficient estimates are identical for certain tuning parameters were provided.
 Many heuristic methods have been proposed in the past, including greedy least squares regression LASSO [24]. They have been shown to outperform the standard convex methods in many prac-greedy least squares regression algorithm (also named OMP or forward greedy algorithm) guaran-the mutual incoherence conditions. A multiple thresholding procedure was proposed to refine the solution of LASSO or Dantzig selector [23]. An adaptive forward-backward greedy algorithm was consistency is achieved if the minimal nonzero entry in the true solution is larger than O (  X  The adaptive LASSO was proposed to adaptively tune the weight value for the L 1 penalty, and it was shown to enjoy the oracle properties [24]. 1.3 Definitions, Notations, and Basic Assumptions We use X  X  R n  X  m to denote the design matrix and focus on the case m  X  n , i.e., the signal dimension is much larger than the observation dimension. The correlation matrix A is defined as A = X T X with respect to the design matrix. The noise vector  X  follows the multivariate normal  X  -supporting set (  X   X  0 ) for a vector  X  is defined as The l p norm of a vector v is computed by k v k p = ( of M with rows from the index set I and columns from the index set J . Additionally, we use the following notation to denote two probabilities: paper, the following assumption is always admitted.
 the feature dimension (i.e. m  X  n ), each column vector is normalized as X T i X i = 1 where X i N (0 ,  X  2 I ) .
 tion. However, this may lead to a slight difference of a factor automatically transformed conclusions from related work according to our assumption when citing them in our paper. 1.4 Organization Section 5. All proofs can be found in the supplementary file. the new signal  X   X  by solving the following linear program: the overestimation y . This is the rationale behind the constraint: k X T F other advantage is when all correct features are chosen, the proposed algorithm can be shown to selector.
 Algorithm 1 Multi-Stage Dantzig Selector Require: F (0) 0 ,  X  , N , X , y , 1: while i=0; i  X  N; i++ do 3: Form F ( i +1) 0 as the index set of the i + 1 largest elements of  X   X  ( i ) . 4: end while 3.1 Motivation edge about the supporting features is known in advance. In standard Dantzig selector, we assume F F 0  X  F , we have the following result: Theorem 1. Assume that assumption 1 holds. Take F 0  X  F and  X  =  X  optimization problem (6) . If there exists some l such that  X   X  , the solution of the problem (6) , and the oracle solution  X   X  is bounded as  X   X  , the solution of the problem (6) and the true solution  X   X  is bounded as p 2 log(( m  X  s ) / X  1 )  X  since larger than the second term. 3.2 Comparison with Dantzig Selector we rewrite the theorem in [7] equivalently as: Setting  X  p =  X  solution of the standard Dantzig selector  X   X  D obeys Specifically, we set F 0 =  X  , N = 0 , and  X  =  X  that with probability larger than 1  X   X  (  X  log m )  X  1 / 2 , the following bound holds: It is easy to verify that mance bound of the proposed multi-stage method ( N &gt; 0 ) with the one in (10). 3.3 Feature Selection The estimation bounds in Theorem 1 assume that a set F 0 is given. In this section, we show how for Dantzig selector. j  X  J and there exists a nonempty set where then taking F 0 =  X  , N = 0 ,  X  =  X  .
 The theorem above indicates that under the given condition, if min j  X  J |  X   X  j | &gt; O (  X  sion [16, 8, 19], two stage LASSO [20], and adaptive forward-backward greedy algorithm [18]. In all these algorithms, the condition min j  X  F |  X   X  j |  X  C X  O (  X  procedure with N &gt; 0 , as summarized in the following theorem: Theorem 4. Under the assumption 1, if there exists a nonempty set and there exists a set J such that | supp  X   X  i = 4 min then taking F (0) 0 =  X  ,  X  =  X  {  X  j } is a strictly decreasing sequence satisfying for some l  X   X  , than the standard Dantzig selector. 3.4 Signal Recovery In this section, we derive the estimation bound of the proposed multi-stage method by combing results from Theorems 1, 3, and 4. Theorem 5. Under the assumption 1, if there exists l such that and there exists a set J such that | supp  X   X   X  X  are defined in Theorem 4, then (1) taking F 0 =  X  , N = 0 and  X  =  X  than 1  X   X  0 1  X   X  0 2 , the solution of the Dantzig selector  X   X  D (i.e,  X   X  (0) ) obeys: (2) taking F 0 =  X  , N = | J | and  X  =  X  k  X  dominates in the estimated bounds. Thus, the performance of the multi-stage method approximately the adaptive forward-backward greedy algorithm [18]. 3.5 The Oracle Solution observation. We show in the following theorem that the proposed method can obtain the oracle solution with high probability under certain conditions: the supporting set F of  X   X  satisfies | supp  X  are defined in Theorem 4, then taking F 0 =  X  , N = s and  X  =  X  enough, the oracle solution can be achieved with high probability. selected: FSA = |  X  F  X  F | / | F | , where  X  F is the estimated feature candidate set. We generate an n  X  m random matrix X . Each element of X follows an independent standard Gaus-Figure 1: Numerical simulation. We compare the solutions of the standard Dantzig selector method ( N = 0 ), the proposed method for different values of N , and the oracle solution. The SRA and FSA comparisons are reported on the top row and the bottom row, respectively. The starting point the proposed method for different values of N . bution N (0 ,  X  2 I ) . For a fair comparison, we choose the same  X  =  X  The following experiments are repeated 20 times and we report their average performance.  X   X  bottom row of Figure 1 shows the FSA curve with respect to N . We can observe from Figure 1 that feature selection accuracy curve is decreasing with an increasing value of N . selection. The final numerical simulation validates our theoretical analysis. the analysis can be extended to other related techniques such as LASSO. The two-stage LASSO has been shown to outperform the standard LASSO. We plan to extend our analysis for multi-stage LASSO in the future. In addition, we plan to improve the proposed algorithm by adopting stopping rules similar to the ones recently proposed in [3, 19, 21].
 Acknowledgments This work was supported by NSF IIS-0612069, IIS-0812551, CCF-0811790, IIS-0953662, and NGA HM1582-08-1-0016. [6] E. J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information [8] D. L. Donoho, M. Elad, and V. N. Temlyakov. Stable recovery of sparse overcomplete rep-[9] G. M. James, P. Radchenko, and J. Lv. DASSO: connections between the Dantzig selector and [10] V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines on-line [11] K. Lounici. Sup-norm convergence rate and sign concentration property of Lasso and Dantzig [12] N. Meinshausen, P. Bhlmann, and E. Zrich. High dimensional graphs and variable selection [14] J. Romberg. The Dantzig selector and generalized thresholding. CISS , pages 22 X 25, 2008. [16] J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions [21] T. Zhang. Sparse recovery with orthogonal matching pursuit under RIP. arXiv:1005.2249 , [22] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning [24] H. Zou. The adaptive Lasso and its oracle properties. Journal of the American Statistical
