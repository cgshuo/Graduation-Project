 In this paper, we propose a new application of Bayesian lan-guage model based on Pitman-Yor process for information retrieval. This model is a generalization of the Dirichlet distribution. The Pitman-Yor process creates a power-law distribution which is one of the statistical properties of word frequency in natural language. Our experiments on Ro-bust04 indicate that this model improves the document re-trieval performance compared to the commonly used Dirich-let prior and absolute discounting smoothing techniques. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]:Information Search and Retrieval General Terms: Theory, Algorithm, Experimentation Keywords: information retrieval, language modeling, Pitman-Yor process, smoothing methods
Statistical language modeling has successfully been used in speech recognition and many natural language process-ing tasks. Language models for information retrieval have been the topics of intense research interest in recent years. The efficiency of this approach, its simplicity, the state-of-the-art performance it provides , and straightforward proba-bilistic interpretation are the most important factors which contribute to its popularity [3].

Smoothing plays an essential role when estimating a lan-guage model for retrieving relevant documents. A large number of smoothing methods have been proposed for lan-guage modeling; among them, three different techniques X  namely Jelinek-Mercer, Bayesian smoothing with Dirichlet priors, and absolute discounting X  X ave shown significant im-provements in information retrieval performance [6]. A hierarchical Bayesian language model based on Pitman-Yor processes has been recently proposed by Teh [5]. This model which is a nonparametric generalization of the Dirich-let distribution [5] has been shown to produce results supe-rior to the state-of-the-art smoothing methods. Hierarchical Pitman-Yor language model has also been applied in speech recognition task and improved the system performance sig-nificantly [1]. However, to the best knowledge of the authors this method has not been used for language model-based in-formation retrieval.
 In this work, we propose using the hierarchical Pitman-Yor language model for the document retrieval task, and compare this approach with the state-of-the-art smoothing methods widely studied for language model-based informa-tion retrieval.
In language model-based document retrieval, P ( Q | d )is estimated by the probability of generating each query term: where M is the number of terms in the query, q i denotes the i th term of query Q = { q 1 ,q 2 , ..., q M } ,and d is the document model. Therefore, the goal is to estimate P ( w | d )whichcan be simply calculated by the maximum likelihood estimation:
However, having the problem of unseen words, we need to use a smoothing technique to give a non-zero probabil-ity to the unseen words. We hypothesized that Bayesian smoothing based on Pitman-Yor process can be used as a new approach to solve the zero probability problem in doc-ument retrieval.

Pitman-Yor process is a nonparametric Bayesian model which recursively placed as prior for predicting probabilities in language model. Considering P ( w | d ) as the probability of word w given the observation of document d to be estimated, the Pitman-Yor process can be defined as: where  X  is a discount parameter,  X  is a strength parame-ter, and P BG is the prior/background probability of word w before observing any document.
 The procedure of drawing word probabilities from the Pitman-Yor process can be described using the  X  X hines restau-rant X  analogy. Imagine a Chinese restaurant with an infinite number of tables, each with an infinite number of seats. Cus-tomers, which correspond to word tokens, enter the restau-rant and seat themselves at a table. Each customer can sit at an occupied table k with probability c k number of customers already sitting there and c. = k c k ; the customer can also sit at a new unoccupied table with probability  X  +  X t. pied tables. It is necessary to mention that all customers that correspond to the same word type w can sit at different tables, in which t w denotes the number of tables occupied by customers w .

One of the advantages of Pitman-Yor process is improv-ing the Dirichlet prior by using a discounting parameter  X  (0 &lt; X &lt; 1) deriving from absolute discounting method. Another key advantage of Pitman-Yor process is generating a power-law distribution in the language model, which is one of the statistical properties of word frequencies in natu-ral language. This property, which is based on the scenario of rich-get-richer, implies that in the statistical property of word counts, words with low frequency have a high proba-bility and words with high frequency occur with low proba-bility. Benefiting from this idea in the document smoothing can help us to have different discounting value for each word based on the frequency of that word in the document.
Given the seating arrangement of customers as described above, the estimated probability of word w having the ob-servation of document d is given by:
If we set the discounting parameter  X  = 0, then the model reduces to the Dirichlet process. If we set the strength pa-rameter  X  = 0 and limit t w = 1, then the model reverts to the absolute discounting method.

Although this formula is based on unigram model, the hierarchical behavior of the Pitman-Yor process allows us to use this model for higher level n -grams as well.
The most important and computationally expensive part of the above formula is calculating t w for each word which should have a relation to the word count c ( w, d ). Towards this end, we use the power-law discounting model proposed by Huang and Renals [2]:
They showed that the above formula is a near optimum estimate for t w , which can be obtained without a computa-tionally expensive training procedure.
To evaluate our methods, we used TREC ad hoc testing collections from disk 4 and 5 minus CR which includes Fi-nancial Times (1991-1994) and Federal Register (1994) from disk 4 and Foreign Broadcast Information Service (1996) and Los Angeles Times (1989-1990) from disk 5. The total num-ber of documents are 528,155.

We used Robust04 topics for our experiment such that topics 301-450 have been used as development set and top-ics 601-700 for test set. For each of the topics, the set of top 1000 documents retrieved by Indri [4] was selected and then the documents are ranked with LSVLM, the language modeling toolkit developed by our chair, in the second step. Table 1 shows the results of our experiments in which Mean Average Precision (MAP) and Precision at 10 (P@10) serve as the primary metrics, and results are marked as sig-nificant* ( p&lt; 0 . 05), highly significant** ( p&lt; 0 . 01), or nei-ther according to 2-tailed paired t -test. This table presents our main results evaluating the accuracy of Bayesian smooth-ing with Dirichlet prior, absolute discounting and our pro-posed Bayesian smoothing based on Pitman-Yor process.
As shown by the tabulated results, the Pitman-Yor lan-guage model significantly outperforms both Dirichlet prior Table 1: Retrieval results with different smoothings. and absolute discounting. As mentioned, the major features of the Pitman-Yor process are generalizing Dirichlet prior and generating power-law dist ribution by having different discounting parameters for each word based on its frequency. We believe that the power-law distribution is the main con-tribution of the Pitman-Yor language model which causes such an improvement in retrieval performance. We also ap-plied Pitman-Yor language model while setting  X  = 0; i.e. the model became more similar to absolute discounting, but it still creates power-law distribution by benefiting from t parameter. The results are presented in the last raw of the table. From the results we can see that although setting  X  = 0 decreases the performance, the reduction is not sig-nificant; and the simplified version of Pitman-Yor smoothing which only has one parameter still beat the other smoothing methods.
We proposed a new smoothing method for language model-based document retrieval, named Bayesian smoothing based on Pitman-Yor process, and verified that this language model provides better performance than other state-of-the-art smooth-ing techniques. The key advantage of Pitman-Yor language model is generating a power-law word distribution, which is the primary reason for its superior performance.
 Saeedeh Momtazi is funded by the German research foun-dation DFG through the International Research Training Group (IRTG 715). [1] S. Huang and S. Renals. Hierarchical Pitman-Yor [2] S. Huang and S. Renals. Power law discounting for [3] J. Ponte and W. Croft. A language modeling approach [4] T. Strohman, D. Metzler, H. Turtle, and W. Croft. [5] Y. Teh. A hierarchical Bayesian language model based [6] C. Zhai and J. Lafferty. A study of smoothing
