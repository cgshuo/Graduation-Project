 Pinghua Gong gph08@mails.tsinghua.edu.cn Changshui Zhang zcs@mail.tsinghua.edu.cn State Key Laboratory on Intelligent Technology and Systems Zhaosong Lu zhaosong@sfu.ca Jianhua Z. Huang jianhua@stat.tamu.edu Jieping Ye jieping.ye@asu.edu Learning sparse representations has important appli-cations in many areas of science and engineering. The use of an  X  0 -norm regularizer leads to a sparse solution, however the  X  0 -norm regularized optimiza-tion problem is challenging to solve, due to the dis-continuity and non-convexity of the  X  0 -norm regu-larizer. The  X  1 -norm regularizer, a continuous and convex surrogate, has been studied extensively in the literature ( Tibshirani , 1996 ; Efron et al. , 2004 ) and has been applied successfully to many applica-tions including signal/image processing, biomedical in-formatics and computer vision ( Shevade &amp; Keerthi , 2003 ; Wright et al. , 2008 ; Beck &amp; Teboulle , 2009 ; Wright et al. , 2009 ; Ye &amp; Liu , 2012 ). Although the  X  norm based sparse learning formulations have achieved great success, they have been shown to be subopti-mal in many cases ( Candes et al. , 2008 ; Zhang , 2010b ; 2012 ), since the  X  1 -norm is a loose approximation of the  X  0 -norm and often leads to an over-penalized problem. To address this issue, many non-convex regularizers, interpolated between the  X  0 -norm and the  X  1 -norm, have been proposed to better approxi-mate the  X  0 -norm. They include  X  q -norm (0 &lt; q &lt; 1) ( Foucart &amp; Lai , 2009 ), Smoothly Clipped Abso-lute Deviation (SCAD) ( Fan &amp; Li , 2001 ), Log-Sum Penalty (LSP) ( Candes et al. , 2008 ), Minimax Con-cave Penalty (MCP) ( Zhang , 2010a ), Geman Penalty (GP) ( Geman &amp; Yang , 1995 ; Trzasko &amp; Manduca , 2009 ) and Capped- X  1 penalty ( Zhang , 2010b ; 2012 ; Gong et al. , 2012a ).
 Although the non-convex regularizers (penalties) are appealing in sparse learning, it is challenging to solve the corresponding non-convex optimization problems. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm for a large class of non-convex penalties. The key step of the proposed algorithm is to compute a proximal operator, which has a closed-form solution for many commonly used non-convex penalties. In our algorithm, we adopt the Barzilai-Borwein (BB) rule ( Barzilai &amp; Borwein , 1988 ) to initialize the line search step size at each iteration, which greatly accelerates the convergence speed. We also use a non-monotone line search criterion to further speed up the convergence of the algorithm. In addi-tion, we present a detailed convergence analysis for the proposed algorithm. Extensive experiments on large-scale real-world data sets demonstrate the efficiency of the proposed algorithm. 2.1. General Problems We consider solving the following general problem: We make the following assumptions on the above for-mulation throughout the paper: A1 l ( w ) is continuously differentiable with Lipschitz A2 r ( w ) is a continuous function which is possibly A3 f ( w ) is bounded from below.
 Remark 1 We say that w  X  is a critical point of problem ( 1 ), if the following holds ( Toland , 1979 ; Wright et al. , 2009 ): where  X  X  1 ( w  X  ) is the sub-differential of the function r ( w ) at w = w  X  , that is,  X  X  1 ( w  X  ) = We should mention that the sub-differential is non-empty on any convex function; this is why we make the assumption that r ( w ) can be rewritten as the dif-ference of two convex functions. 2.2. Some Examples Many formulations in machine learning satisfy the as-sumptions above. The following least square and logis-tic loss functions are two commonly used ones which satisfy assumption A1 : l ( w ) = izers (penalties) which satisfy the assumption A2 are presented in Table 1 . They are non-convex (except the  X  -norm) and extensively used in sparse learning. The functions l ( w ) and r ( w ) mentioned above are nonneg-ative. Hence, f is bounded from below and satisfies assumption A3 . 2.3. Algorithm Our proposed General Iterative Shrinkage and Thresh-olding (GIST) algorithm solves problem ( 1 ) by gener-ating a sequence { w ( k ) } via: In fact, problem ( 2 ) is equivalent to the following prox-imal operator problem: we first perform a gradient descent along the direction imal operator problem. For all the regularizers listed in Table 1 , problem ( 2 ) has a closed-form solution (de-tails are provided in the Appendix), although it may Algorithm 1 GIST: General Iterative Shrinkage and Thresholding Algorithm be a non-convex problem. For example, for the  X  1 and Capped  X  1 regularizers, we have closed-form solutions as follows: procedure of the GIST algorithm is presented in Algorithm 1 . There are two issues that remain to be addressed: how to initialize t ( k ) (in Line 4) and how to select a line search criterion (in Line 8) at each outer iteration. 2.3.1. The Step Size Initialization: 1 /t ( k ) Intuitively, a good step size initialization strategy at each outer iteration can greatly reduce the line search cost (Lines 5-8) and hence is critical for the fast con-vergence of the algorithm. In this paper, we propose to initialize the step size by adopting the Barzilai-Borwein (BB) rule ( Barzilai &amp; Borwein , 1988 ), which uses a diagonal matrix t ( k ) I to approximate the Hes-sian matrix  X  2 l ( w ) at w = w ( k ) . Denote x Then t ( k ) is initialized at the outer iteration k as 2.3.2. Line Search Criterion One natural and commonly used line search criterion is to require that the objective function value is mono-tonically decreasing. More specifically, we propose to following monotone line search criterion is satisfied: where  X  is a constant in the interval (0 , 1). A variant of the monotone criterion in Eq. ( 3 ) is a non-monotone line search criterion ( Grippo et al. , 1986 ; Grippo &amp; Sciandrone , 2002 ; Wright et al. , 2009 ). It possibly accepts the step size 1 /t ( k ) even if w ( k +1) yields a larger objective function value than w ( k ) . Specifically, we propose to accept the step size 1 /t ( k ) if w ( k +1) makes the objective function value smaller than the maximum over previous m ( m &gt; 1) itera-tions, that is, where  X   X  (0 , 1).
 2.3.3. Convergence Analysis Inspired by Wright et al. ( 2009 ); Lu ( 2012a ), we present detailed convergence analysis under both monotone and non-monotone line search criteria. We first present a lemma which guarantees that the mono-tone line search criterion in Eq. ( 3 ) is satisfied. This is a basic support for the convergence of Algorithm 1 . Lemma 1 Let the assumptions A1 -A3 hold and the constant  X   X  (0 , 1) be given. Then for any integer k  X  0 , the monotone line search criterion in Eq. ( 3 ) is satisfied whenever t ( k )  X   X  ( l ) / (1  X   X  ) . Proof Since w ( k +1) is a minimizer of problem ( 2 ), we have It follows from assumption A1 that Combining Eq. ( 5 ) and Eq. ( 6 ), we have It follows that Therefore, the line search criterion in Eq. ( 3 ) is sat- X  ( l ) / (1  X   X  ). This completes the proof the lemma. Next, we summarize the boundedness of t ( k ) in the following lemma.
 Lemma 2 For any k  X  0 , t ( k ) is bounded under the monotone line search criterion in Eq. ( 3 ).
 Proof It is trivial to show that t ( k ) is bounded from below, since t ( k )  X  t min ( t min is defined in Algo-rithm 1 ). Next we prove that t ( k ) is bounded from above by contradiction. Assume that there exists a k  X  0, such that t ( k ) is unbounded from above. Without loss of generality, we assume that t ( k ) in-creases monotonically to +  X  and t ( k )  X   X  X  ( l ) / (1  X  been tried at iteration k and does not satisfy the line search criterion in Eq. ( 3 ). But Lemma 1 states that line search criterion in Eq. ( 3 ). This leads to a contra-diction. Thus, t ( k ) is bounded from above. Remark 2 We note that if Eq. ( 3 ) holds, Eq. ( 4 ) is guaranteed to be satisfied. Thus, the same conclusions in Lemma 1 and Lemma 2 also hold under the the non-monotone line search criterion in Eq. ( 4 ). Based on Lemma 1 and Lemma 2 , we present our con-vergence result in the following theorem.
 Theorem 1 Let the assumptions A1 -A3 hold and the monotone line search criterion in Eq. ( 3 ) be satisfied. Then all limit points of the sequence by Algorithm 1 are critical points of problem ( 1 ). Proof Based on Lemma 1 , the monotone line search criterion in Eq. ( 3 ) is satisfied and hence which implies that the sequence monotonically decreasing. Let w  X  be a limit point of the sequence quence K such that Since f is bounded from below, together with the fact that lim k !1 f ( w ( k ) ) exists. Observing that f is contin-uous, we have Taking limits on both sides of Eq. ( 3 ) with k  X  X  , we have Considering that the minimizer w ( k +1) is also a critical have Taking limits on both sides of the above equation with k  X  X  , by considering the semi-continuity of  X  X   X  X  and Eq. ( 7 ), we obtain Therefore, w  X  is a critical point of problem ( 1 ). This completes the proof of Theorem 1 .
 Based on Eq. ( 7 ), we know that lim k 2K!1  X  w ( k +1)  X  w ( k )  X  2 = 0 is a necessary optimality condition of Al-gorithm 1 . Thus,  X  w ( k +1)  X  w ( k )  X  2 is a quantity to measure the convergence of the sequence { w ( k ) } to a critical point. We present the convergence rate in terms of  X  w ( k +1)  X  w ( k )  X  2 in the following theorem. Theorem 2 Let { w ( k ) } be the sequence generated by Algorithm 1 with the monotone line search criterion in Eq. ( 3 ) satisfied. Then for every n  X  1 , we have where w  X  is a limit point of the sequence { w ( k ) } . Proof Based on Eq. ( 3 ) with t ( k )  X  t min , we have Summing the above inequality over k = 0 ,  X  X  X  , n , we obtain which implies that min This completes the proof of the theorem.
 Under the non-monotone line search criterion in Eq. ( 4 ), we have a similar convergence result in the following theorem (the proof uses an extension of ar-gument for Theorem 1 and is omitted).
 Theorem 3 Let the assumptions A1 -A3 hold and the non-monotone line search criterion in Eq. ( 4 ) be sat-isfied. Then all limit points of the sequence generated by Algorithm 1 are critical points of problem ( 1 ).
 Note that Theorem 1 /Theorem 3 makes sense only if { assumption: A4 f ( w )  X  +  X  when  X  w  X  X  X  +  X  , we summarize the existence of limit points in the fol-lowing theorem (the proof is omitted): Theorem 4 Let the assumptions A1 -A4 hold and the monotone/non-monotone line search criterion in Eq. ( 3 )/Eq. ( 4 ) be satisfied. Then the sequence generated by Algorithm 1 has at least one limit point. 2.3.4. Discussions Observe that l ( w ( k ) )+  X  X  X  l ( w ( k ) ) , w  X  w ( k ) w ( k )  X  2 can be viewed as an approximation of l ( w ) at w = w ( k ) . The GIST algorithm minimizes an approx-imate surrogate instead of the objective function in problem ( 1 ) at each outer iteration. We further ob-condition of Eq. ( 3 )], we obtain It follows that where M ( w , w ( k ) ) denotes the objective function of problem ( 2 ). We can easily show that Thus, the GIST algorithm is equivalent to solving a sequence of minimization problems: and can be interpreted as the well-known Majorization and Minimization (MM) technique ( Hunter &amp; Lange , 2000 ).
 Note that we focus on the vector case in this paper and the proposed GIST algorithm can be easily extended to the matrix case. In this section, we discuss some related algo-rithms. One commonly used approach to solve problem ( 1 ) is the Multi-Stage (MS) convex relax-ation (or CCCP, or DC programming) ( Zhang , 2010b ; Yuille &amp; Rangarajan , 2003 ; Gasso et al. , 2009 ). It equivalently rewrites problem ( 1 ) as where f 1 ( w ) and f 2 ( w ) are both convex functions. The MS algorithm solves problem ( 1 ) by generating a sequence { w ( k ) } as where s 2 ( w ( k ) ) denotes a sub-gradient of f 2 ( w ) at w = w ( k ) . Obviously, the objective function in prob-lem ( 8 ) is convex. The MS algorithm involves solving a sequence of convex optimization problems as in prob-lem ( 8 ). In general, there is no closed-form solution to problem ( 8 ) and the computational cost of the MS algorithm is k times that of solving problem ( 8 ), where k is the number of outer iterations. This is computa-tionally expensive especially for large scale problems. A class of related algorithms called iterative shrink-age and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting ( Daubechies et al. , 2004 ; Combettes &amp; Wajs , 2005 ; Hale et al. , 2007 ; 2009 ), have been extensively applied to solve problem ( 1 ). The key step is by generating a sequence { w ( k ) via solving problem ( 2 ). However, they require that the regularizer r ( w ) is convex and some of them even require that both l ( w ) and r ( w ) are convex. Our pro-posed GIST algorithm is a more general framework, which can deal with a wider range of problems includ-ing both convex and non-convex cases.
 Another related algorithm called a Variant of Iterative Reweighted L  X  (VIRL) is recently proposed to solve the following optimization problem ( Lu , 2012a ): problem by generating a sequence { w ( k ) } as + In VIRL, t ( k 1) is chosen as the initialization of t ( k ) The line search step in VIRL finds the smallest integer  X  with t ( k ) = t ( k 1)  X   X  (  X  &gt; 1) such that The most related algorithm to our propose GIST is the Sequential Convex Programming (SCP) proposed by Lu ( 2012b ). SCP solves problem ( 1 ) by generating a sequence { w ( k ) } as w + , algorithm differs from SCP in that the original regu-operator in problem ( 2 ), while r 1 ( w ) minus a locally linear approximation for r 2 ( w ) is adopted in SCP. We will show in the experiments that our proposed GIST algorithm is more efficient than SCP. 4.1. Experimental Setup We evaluate our GIST algorithm by considering the Capped  X  1 regularized logistic regression problem, that is l ( w ) = 1 n  X   X  with the Multi-Stage (MS) algorithm and the SCP algorithm in different settings using twelve data sets summarized in Table 2 . These data sets are high di-mensional and sparse. Two of them (news20, real-sim) 1 have been preprocessed as two-class data sets ( Lin et al. , 2008 ). The other ten 2 are multi-class data sets. We transform the multi-class data sets into two-class by labeling the first half of all classes as positive class, and the remaining classes as the negative class. All algorithms are implemented in Matlab and exe-cuted on an Intel(R) Core(TM)2 Quad CPU (Q6600 @2.4GHz) with 8GB memory. We set  X  = 10 5 , m = 5 ,  X  = 2 , 1 /t min = t max = 10 30 and choose the start-ing points w (0) of all algorithms as zero vectors. We terminate all algorithms if the relative change of the two consecutive objective function values is less than 10 5 or the number of iterations exceeds 1000. The Matlab codes of the GIST algorithm are available on-line ( Gong et al. , 2013 ). 4.2. Experimental Evaluation and Analysis We report the objective function value vs. CPU time plots with different parameter settings in Fig-ure 1 . From these figures, we have the following ob-servations: (1) Both GISTbb-Monotone and GISTbb-Nonmonotone decrease the objective function value rapidly and they always have the fastest conver-gence speed, which shows that adopting the BB rule to initialize t ( k ) indeed greatly accelerates the con-vergence speed. Moreover, both GISTbb-Monotone and GISTbb-Nonmonotone algorithms achieve the smallest objective function values. (2) GISTbb-Nonmonotone may give rise to an increasing objective function value but finally converges and has a faster overall convergence speed than GISTbb-Monotone in most cases, which indicates that the non-monotone line search criterion can further accelerate the con-vergence speed. (3) SCPbb-Nonmonotone is compa-rable to GISTbb-Nonmonotone in several cases, how-ever, it converges much slower and achieves much larger objective function values than those of GISTbb-Nonmonotone in the remaining cases. This demon-strates the superiority of using the original regular-in problem ( 2 ). (4) GIST-1 has a faster convergence speed than GIST-t ( k 1) in most cases, which demon-in this way, making the step size 1 /t ( k ) monotonically decreasing when the algorithm proceeds. We propose an efficient iterative shrinkage and thresh-olding algorithm to solve a general class of non-convex optimization problems encountered in sparse learning. A critical step of the proposed algorithm is the com-putation of a proximal operator, which has a closed-form solution for many commonly used formulations. We propose to initialize the step size at each itera-tion using the BB rule and employ both monotone and non-monotone criteria as line search conditions, which greatly accelerate the convergence speed. Moreover, we provide a detailed convergence analysis of the pro-posed algorithm, showing that the algorithm converges under both monotone and non-monotone line search criteria. Experiments results on large-scale data sets demonstrate the fast convergence of the proposed al-gorithm.
 In our future work, we will focus on analyzing the the-oretical performance (e.g., prediction error bound, pa-rameter estimation error bound etc.) of the solution obtained by the GIST algorithm. In addition, we plan to apply the proposed algorithm to solve the multi-task feature learning problem ( Gong et al. , 2012a ; b ). This work is supported partly by 973 Program (2013CB329503), NSFC (Grant No. 91120301, 61075004, 61021063), NIH (R01 LM010730) and NSF (IIS-0953662, CCF-1025177, DMS1208952).
 Observe that r ( w ) = be equivalently decomposed into d independent uni-variate optimization problems: w i = arg min w where i = 1 ,  X  X  X  , d and u ( k ) i is the i -th entry of u tions, we unclutter the above equation by removing the subscripts and supscripts as follows:  X   X   X  LSP : We can obtain an optimal solution of prob- X  SCAD : We can recast problem ( 9 ) into the fol- X  MCP : Similar to SCAD, we can recast problem  X  Capped  X 
