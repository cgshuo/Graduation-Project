 Nicola Rebagliati nicola.rebagliati@gmail.com VTT Technical Research Centre of Finland P.O. Box 1000, VTT 02044, Finland Sum of Squares Error and Normalized Cut are two different clustering functionals widely used in Machine Learning applications (von Luxburg, 2007; Jain, 2010). Minimizing the Sum of Squares Error on a set of points allows the representation of each cluster with a sin-gle point called prototype, or centroid. On the other side, minimizing the Normalized Cut aims at divid-ing a graph into components with balanced volumes. These two minimization problems are NP-complete, but they both enjoy approximation algorithms which are quite successful in applications.
 A principal area of research for these functionals is model validation, i.e. choosing a correct number of clusters. It is easy to empirically observe, on a given set of points, that the minimum Sum of Squares Error value decreases with the number of clusters and it is zero if we cluster each point within itself. Similarly in (Nagai, 2007) it was proved that the minimum Nor-malized Cut value is monotonically decreasing with respect to the number of clusters. This monotonicity property characterizes these functionals and does not allow for automatic selection of the number of clusters, which is a trivial solution separating each data, or col-lecting all of them together. This is in contrast with other model-based approaches, like Gaussian Mixture Models, where the number of clusters can be learned from the data (Figueiredo &amp; Jain, 2002), or from Cor-relation Clustering (Bansal et al., 2004), where the functional is not monotone in the number of clusters. Our main purpose here is to prove strict monotonic-ity, for both functionals, not just on the minimizers but on the entire clustering lattice, which is the algebraic structure of a set partitions (Meila, 2005). E.g., we prove that the split operation, which divides a cluster into smaller different ones, strictly decreases the Sum of Squares Error, or strictly increases the Normalized Cut. In general a chain of clusterings, the typical out-put of hierarchical clustering algorithms, leads to a strictly monotone function. As a further relevant re-sult, we obtain data-dependent bounds on the change of the functional value for any pair of clusterings. These results furnish further evidences that for these two functionals it is not appropriate to choose the number of clusters by using solely the functional value. This fact is the main result of (Nagai, 2007) for the Normalized Cut and it is largely recognized, see (Jain, 2010), for the minimum Sum of Squares Error. How-ever, from a more general point of view, these results can be used as a base of reference for developing clus-tering techniques which allow the user to explore the clustering lattice.
 The reason for dealing, at the same time, with these two different functionals is the unified treatment with Linear Algebra tools which allow for clean, direct proofs and provide data-dependent rates. By proving strict anti-monotonicity of Normalized Cut we improve a result of (Nagai, 2007) with a short proof. 1.1. Related Literature In general, the problems of minimizing the Sum of Squares Error and the Normalized Cut are NP-complete (Aloise et al., 2009; Drineas et al., 2004; Shi &amp; Malik, 2000). Minimization of Sum of Squares Error functional is usually approximated, in practice, by the K -means algorithm (Ding &amp; He, 2004; Meila, 2006; Jain, 2010). On the other side, the Normalized Cut is approximated using spectral techniques (von Luxburg, 2007).
 A main tool in our proofs is eigenvalue interlacing. This was used in (Bolla, 1993; Bollob  X as &amp; Nikiforov, 2004; 2008) to prove lower and upper bounds on the Ratio Cut, a functional which has the same principle as Normalized Cut, and in (Zha et al., 2001; Steinley, 2011) for Sum of Squares Error. From this point of view our results can be considered as generalizations of theirs.
 The rest of the paper is structured as follows. In sec-tion 2 we present the necessary preliminaries of Linear Algebra, in section 3 we prove Sum of Squares Error is, under certain conditions, strictly monotone in the clustering lattice and in in section 4 we prove Nor-malized Cut is strictly anti-monotone in the clustering lattice. In a typical clustering problem we have n input objects that we want to divide into K &lt; n clusters, where K is a user-chosen parameter. To do so we usually minimize a functional which represent our notion of cluster. The resulting clustering C : { 1 ,...,n }  X  { 1 ,...,K } is a surjective function which assigns a cluster to each of these data. Alternatively, we write C = { C 1 ,...,C K } , with C i = C  X  1 ( i ). Let C be a clustering into K clusters and C 0 a clustering into K 0  X  K clusters. We say the C is a refinement of C if for each pair i,j  X  X  1 ,...,n } write C 0  X  X  . Equivalently, we say C is a coarsening of C . If K 0 &gt; K then the relation is strict . Transitivity, C 00  X  X  0 and C 0  X  X  implies C 00  X  X  , clearly holds. The join C X  X  0 is the clustering with the maximum number of clusters such that C  X  C X  X  0 and C 0  X  C X  X  0 . The meet C X C 0 is the clustering with the minimum number of clusters such that C  X C 0  X  C and C  X C 0  X  C 0 . For every pair C and C 0 both join and meet exist. Given these properties the pair ( C,  X  ) is called lattice . This lattice has at least two elements,  X  and &gt; , and it holds  X  X  X C  X &gt; . Thus  X  is the clustering with K = n and &gt; is the clustering with K = 1. A chain of clusterings is an ordered refinement of clusterings, for example on the set { a,b,c } we may have the following chain: We refer to (Meila, 2005) for comparing elements of a clustering lattice.
 We say that a clustering functional f : K n  X  R + is strictly monotone with respect to the lattice of clus-terings if C 0  X  C  X  f ( C ) &lt; f ( C 0 ), or strictly anti-monotone if C 0  X  X   X  f ( C ) &gt; f ( C 0 ).
 Given a matrix M  X  X  n,m , with m  X  n , we denote its singular values with  X  ( M ) =  X  1 ( M )  X   X  X  X   X   X  m ( M ). If A is square and symmetric its eigenvalues are simi-larly represented as  X  ( A ) =  X  1 ( A )  X   X  X  X   X   X  n ( A ). It holds  X  ( MM t ) =  X  2 i ( M ) for i = 1 ,...,m . The trace of a matrix product Tr ( AB ) = X Our main tool in proofs is the interlacing theorem. We say that a vector of numbers  X  1  X   X  X  X   X   X  m interlace  X  1  X  X  X  X  X  X   X  n , with n &gt; m , if Theorem 1. (Haemers, 1995) Let S  X  R n,m be a real matrix such that S t S = I and let A  X  R n,n be a symmetric matrix with eigenvalues  X  1  X  ...,  X   X  n . Define B = S t AS and let B have eigenvalues  X  1  X   X  X  X  X   X  m and respective eigenvectors v 1 ,...,v m . i) The eigenvalues of B interlace those of A . ii) If  X  i =  X  i or  X  i =  X  n  X  m + i for some i  X  [1 ,m ] , iii) If for some integer l ,  X  i =  X  i for i = 1 ,...,l iv) If the interlacing is tight then SB = AS . We suppose we are given as input a set of n distinct d -dimensional points and a target number of clusters K  X  1 ,...,n . The input points are stacked as rows of a matrix X  X  R n,d . Given a subset C r of points, X
C r is the sub-matrix of X with the points belonging to C r .
 We define the centroid of a cluster C r as the mean of its points, that is The Sum of Squares Error evaluates the sum of squared euclidean distances of input points from their own centroids: When matrix X is clear from the context we simply write SSE ( C ). By minimizing the Sum of Squares Er-ror we seek for the best set of K prototypes represent-ing the data. It is possible that choosing a correct K is part of the problem but it is well known that we cannot rely only on the minimum value of SSE ( .,. ) itself because it is strictly monotone and an optimal clustering is the one putting each point by itself. Observation 2 (Strict Monotonicity of Minimum Sum of Squares Error Clustering) . Consider an input dataset X  X  R n,d made of n distinct d -dimensional points. Then if K &lt; n This observation is simple and provable in many dif-ferent ways. Here we refer to our result of theorem 9.
 Proof of Observation 2. Let C  X  be the K -clusters min-imizer. We can create a new cluster C 0 with K + 1 clusters by choosing a cluster C i from C with more than one point and putting in the new cluster just a point of C i with higher distance from its centroid. By theorem 9 we obtain SSE ( C 0 ) &lt; SSE ( C  X  ). Our purpose is extending observation 2 with the addi-tional hypothesis that the solutions are related by the refinement relation of the lattice. In order to appreci-ate the impact of this hypothesis we should consider that the minimizers of (1), with different values of K , can be substantially different within each other, i.e. the minimizer with K + 1 clusters may not be a refine-ment of the minimizer with K clusters. A simple ex-ample of this fact is that of figure 1, a one-dimensional set of equally spaced points. The solutions here are equally-distributed partitions and clearly one solution with K 0 clusters refines one with K clusters only if K divides K 0 .
 It is central to our treatment to rewrite the Sum of Squares Error as the trace of a matrix product. To do so we use the following square indicator matrix of clusterings (Ding &amp; He, 2004; Meila, 2006): Matrices H  X  and H &gt; represent, respectively, the clus-tering where every point is put by itself and the clus-tering where all points are put together. Their dimen-sions depend on the context. The following lemma summarizes the relation between the Sum of Squares Error and indicator matrices, Lemma 3. Let X  X  R n,d be an input set of n distinct d -dimensional points and let C = { C 1 ,...,C K } be a clustering. i) The following equalities hold.
 ii) Matrix H C XX t H C has K , or less, strictly positive Proof of Lemma 3. i) The second equality is given ii) Define the indicator matrix Intuitively speaking, if a clustering C is coarser than C 0 then it contains less information than C 0 , but the contained information is consistent with the one of C 0 . Next lemma shows how this fact reflects into indicator matrices.
 Lemma 4. Let C be a clustering with K &lt; n clus-ters and C 0 a refinement of C . Then H C H C 0 = H C H Proof of Lemma 4. Suppose i  X  C s and j  X  C 0 r , then So if C 0 r  X  C s the last term is equal to 1 / | C otherwise, since C 0 r  X  C s =  X  , it is zero. Since H C symmetric we get H C = H C 0 H C .
 The monotonicity result we obtain on the Sum of Squares Error does not hold in general for every dataset and every clustering, because there exist par-ticular configurations of data which allow for not valid clusterings. The subset of clusterings we work on are characterized by the following property.
 Definition 1 (Proper clustering) . We say that a clus-tering C , of a given dataset X , with K  X  2 clusters is proper if it has K different centroids  X  1 ,..., X  K . Strictly speaking, a clustering which is not proper has less than K clusters because at least two of them have the same centroid and can be considered the same clus-ter. Clustering which are not proper may arise in sit-uations where data present symmetries, like in figure 2(a), where a clustering which pairs each point with its opposite w.r.t. the center, is non-proper. In general, also data without symmetries may admit non-proper clusterings, as in figure 2(b). On the other side, we have different arguments for considering a non-proper clustering unstable and with a marginal impact on practical applications. Firstly, a non-proper clustering can be made proper by slightly perturbing the data, whereas the opposite is unlikely. Secondly, since points are distinct, it just suffices to change the cluster of one of them to break the equality of two means and even-tually obtaining, with few changes, a proper clustering out of a non-proper one.
 In studying strict monotonicity for Sum of Squares Error we have to consider different cases for the di-mensionality of the input set.
 Definition 2 (Dimensionality) . We define the dimen-sionality dim ( X ) of a dataset, with X  X  R n,d , as the number of singular values, counted with multiplici-ties, which are different from zero. Clearly, dim ( X )  X  min ( { n,d } ) .
 If dim ( X ) = n we can state strict monotonicity with-out any further conditions. If dim ( X ) &lt; n we give a necessary and sufficient condition to have strict mono-tonicity. Since a non-proper dataset contains linear dependencies among points, it is easy to observe the following.
 Observation 5. Every clustering of a full dimen-sional dataset is proper.
 3.1. Full dimensional dataset A necessary condition for having a full dimensional dataset X is d  X  n . Perhaps, the most frequent practical cases where this condition holds are high-dimensional datasets, characterised by very large d . In these cases it is likely that  X  n ( X ) &gt; 0. Theorem 6 (Strict Monotonicity of Sum of Squares Error on ( C ,  X  ), dim ( X ) = n ) . Let X  X  R n,d be a d -dimensional dataset of n points with dim ( X ) = n . Let C be a clustering with K &lt; n clusters and C 0 a strict refinement with K 0 &gt; K clusters. Then in particular SSE ( C 0 ) &lt; SSE ( C ) .
 Proof of Theorem 6.
 In the last step we used the fact, entailed by theorem 3, that the eigenvalues of H C H C 0 XX t H C 0 H C interlace those of H C 0 XX t H C 0 , but since lemma 4 implies the first sum is maximized by zero. The second sum, again because of interlacing, cannot be greater than the negative of the sum of the K 0  X  K smallest eigen-values of  X  ( XX t ). Similarly we get the other inequal-ity.
 By using theorem 6 we obtain two different ways for lower bounding, and upper bounding, the quantity SSE ( D )  X  SSE ( C ) between any two clusterings C , D . Theorem 7. Let X  X  X  n,d be a d -dimensional dataset of n points with dim ( X ) = n . Let C be a clustering with K 1 clusters and D a clustering with K 2 clusters. Let n 1 = K 1  X  X C X D| and n 2 = K 2  X  X C X D| . Then
SSE ( D )  X  SSE ( C )  X 
X Proof. Using theorem 6 we get and by adding (5) and (6) we get (4).
 Theorem 8. Let X  X  X  n,d be a d -dimensional dataset of n points with dim ( X ) = n . Let C be a clustering with K 1 clusters and D a clustering with K 2 clusters. Let m 1 = |C X  X | X  K 1 and m 2 = |C X  X | X  K 2 . Then
SSE ( D )  X  SSE ( C )  X 
X Proof. From theorem 6 we have and the same with D instead of C . Thus we get and from which we obtain the two sides of inequality (7). 3.2. Non full dimensional dataset We treat separately the low dimensional case because if  X  dim ( X )+1 =  X  X  X  =  X  n = 0 then theorem 6 does not necessarily imply strict monotonicity.
 Theorem 9 (Strict Monotonicity of Sum of Squares Error, d &lt; n ) . Let C be a clustering into K &lt; n clusters and C 0 one of its refinements with K + 1 clusters. If C 0 is proper, as in definition 1, then SSE ( C 0 ) &lt; SSE ( C ) .
 Proof of Theorem 9. Since |C 0 | = K + 1 and C 0  X  C there exists a cluster C r such that | C r | X  2 and a pair C Let X r = X C r . By using lemma 3 we have Tr ( H r X r X r t H r ),  X  ( H r X r X r t H r ) =  X  1  X   X  Since The eigenvalues interlace and we have  X  1  X   X  1  X   X   X  X  X   X  0. If  X  1 = 0 then the set X r has mean zero, but since C 0 is proper at least one mean of X j and X k is different from zero, giving  X  1 &gt; 0, and the conclusion follows.
 Suppose  X  1 &gt; 0. Then, unless  X  1 =  X  1 and  X  2 = 0,  X  1 +  X  2 &gt;  X  1 . Suppose, by contradiction, that  X  1 =  X  and  X  2 = 0, we have by lemma 3 and theorem 1 so that  X  C 0 is proper. By lemma 3 we have SSE ( C 0 ) &lt; SSE ( C ). Finally we have the following corollary.
 Corollary 10. Any optimal K clusters solution to the minimization of Sum of Squares Error is proper. Let G = ( V,E ) be an undirected, weighted and connected graph. Given a clustering C K = { C 1 ,C 2 ,...,C K } let The degree d r of a vertex r is cut( r,V ) and the volume vol( C i ) = cut( C i ,V ). The Normalized Cut (Chung, 1997; Shi &amp; Malik, 2000) is defined as As for the Sum of Squares Error choosing the right K is a difficult problem. Indeed, in (Nagai, 2007) the following lemma was proved.
 Lemma (Nagai, 2007) . Let G = ( V,E ) be an undi-rected, weighted and connected graph. Let C be a clus-tering with K &lt; n clusters and C 0 a strict refinement with K 0 &gt; K clusters. Then Here we first prove strict anti-monotonicity on the clustering lattice, so if C 0  X  X  we obtain NCUT ( C 0 ) &gt; NCUT ( C ). This result turns out to imply strict anti-monotonicity also for the lemma of (Nagai, 2007), im-proving it. Then we obtain data-dependent bounds for the difference of the Normalized Cut of any two clusterings.
 Let d i = d i , W be the adjacency matrix of the graph and D = diag( d ). The normalized laplacian is the ma-trix L = I  X  D  X  1 / 2 WD  X  1 / 2 . If the graph is connected then  X  n ( L ) = 0 and  X  n  X  1 ( L ) &gt; 0 (Chung, 1997). The vector d / vol( G ) is the eigenvector of  X  n ( L ). The following matrix is an indicator matrix of parti-tions: The following lemma summarizes the relation between the Normalized Cut and indicator matrices.
 Lemma 11. Let G = ( V,E ) be an undirected, weighted and connected graph and L its normalized laplacian. Let C = { C 1 ,...,C K } be a clustering. Then i) NCUT ( C ) = Tr ( G C LG C ) ii) Matrix G C LG C has K  X  1 , or less, strictly positive Proof of Lemma 11. i) See (von Luxburg, 2007). ii) As in lemma 3 ii) , but with the following indicator and by observing that for every C , the null eigenvector d / vol( G ) is in the span of the columns of P C . Lemma 12 (Lemma) . Let C be a clustering with K  X  n  X  1 clusters and C 0 a refinement of C . Then G C G C 0 G Proof of Lemma 12. Suppose object i  X  C s and j  X  C .
 So if i  X  C r then C s  X  C 0 r and the last term is equal to p d i d j / vol( C s ) and it is zero otherwise. Since G is symmetric we get G C = G C 0 G C .
 In section 3 we proved strict monotonicity for the Sum of Squares Error in two different conditions, whether the input dataset X was full dimensional or not. Here we obtain strict anti-monotonicity using basically the same techniques. However the graph is connected and we know  X  n  X  1 ( L ) &gt; 0. So we work with dimensionality n  X  1. We have the following.
 Theorem 13 (Strict Anti-Monotonicity of Normal-ized Cut on ( C ,  X  )) . Let G = ( V,E ) be an undirected, weighted and connected graph and L its normalized laplacian. Let C be a clustering with 2  X  K &lt; n clus-ters and C 0 a strict refinement with K 0 &gt; K clusters. Then NCUT ( C 0 )  X  NCUT ( C ) + NCUT ( C ) + in particular NCUT ( C 0 ) &gt; NCUT ( C ) .
 Proof of Theorem 13. The proof is similar to that of theorem 6. The only difference being that matrix G
C LG C has at maximum K  X  1 eigenvalues greater than zero, instead of K . Similarly as before we have, by lemma 11, that the eigenvalues of G C G C 0 LG C 0 G C interlace those of G C 0 LG C 0 , and lemma 12 implies G Now In the last step we used the fact that the first sum is greater or equal than zero. The second sum, again be-cause of interlacing, cannot be greater than the sum of the K 0  X  K + 1 smallest eigenvalues of  X  ( L ), excluding the last one which is zero. Similarly we get the other inequality.
 Using theorem 13 we get the following corollary which improves the result of (Nagai, 2007) with strict mono-tonicity.
 Corollary 14 (Strict Anti-Monotonicity of Normal-ized Cut) . Let G = ( V,E ) be an undirected, weighted and connected graph and L its normalized laplacian. We have Proof of theorem 14. Let C  X  be the minimizer with K clusters. By theorem 13 a coarsening C 0 will have NCUT ( C 0 ) &lt; NCUT ( C  X  ) and the conclusion fol-lows.
 Similarly as in the Sum of Squares Error, theorems 7 and 8, we can bound the difference NCUT ( D )  X  NCUT ( C ) between two clusterings C and D in two different ways. The proofs are omitted here, but can be easily derived by the proofs of theorems 7 and 8 using the indicator matrix G C instead of H C , L instead of XX t and invoking theorem 13 instead of theorem 6.
 Theorem 15. Let G = ( V,E ) be an undirected, weighted and connected graph and L its normalized laplacian. Without loss of generality, let C be a clus-tering with K 1 &lt; n clusters and D a clustering with K 2  X  K 1 clusters. Let n 1 = K 1  X  |C  X  D| and n 2 = K 2  X  X C X D| . Then
NCUT ( D ) -NCUT ( C )  X 
X Theorem 16. Let G = ( V,E ) be an undirected, weighted and connected graph and L its normalized laplacian.Without loss of generality, let C be a clus-tering with K 1 &lt; n clusters and D a clustering with K 2  X  K 1 clusters. Let m 1 = |C  X  D|  X  K 1 and m 2 = |C X  X | X  K 2 . Then
NCUT ( D ) -NCUT ( C )  X   X 
X As a final observation, the results provided for the Nor-malized Cut holds also for the Ratio Cut but using the unnormalized laplacian L u = D  X  W and using the representation matrix defined for the Sum of Squares Error in section 3. In this paper we proved strict monotonicity for two clustering functionals, Sum of Squares Error and Nor-malized Cut, with respect to the refinement relation of the lattice of clusterings. As a consequence of these results we could get data-dependent bounds on the dif-ference between any two clusterings and, for the min-imizers of the Normalized Cut, we could improve the result of (Nagai, 2007) with strict monotonicity. These results are interesting for model validation, i.e. choosing the right number of classes. From one side they confirm that we cannot rely only on the functional value even if we constraint our solutions to form a chain of clusterings. From the other side they give quantitative ways to estimate how much a dataset is clusterable. For example, for the Normalized Cut we see that if a graph G has a small gap between  X  1 ( L )  X   X  n  X  1 ( L ), like in expander graphs, all clusterings of G , with the same number of classes K , will have similar values implying that G consists of just one cluster. This direction of work deserves attention for future developments.
 This work was carried out during the tenure of an ERCIM  X  X lain Bensoussan X  Fellowship Programme. The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement 246016.
 Aloise, Daniel and Hansen, Pierre. A branch-and-cut sdp-based algorithm for minimum sum-of-squares clustering. Pesquisa Operacional , 29:503 X 516, 2009. Aloise, Daniel, Deshpande, Amit, Hansen, Pierre, and
Popat, Preyas. NP-hardness of euclidean sum-of-squares clustering. Machine Learning , 75(2):245 X  248, 2009.
 Bansal, Nikhil, Blum, Avrim, and Chawla, Shuchi.
Correlation clustering. Machine Learning , 56(1-3): 89 X 113, 2004.
 Bolla, Marianna. Spectra, euclidean representations and clusterings of hypergraphs. Discrete Mathemat-ics , 117:19 X 39, 1993.
 Bollob  X as, B  X ela and Nikiforov, Vladimir. Graphs and hermitian matrices: eigenvalue interlacing. Discrete Mathematics , 289(1-3):119 X 127, 2004.
 Bollob  X as, B  X ela and Nikiforov, Vladimir. Graphs and hermitian matrices: Exact interlacing. Discrete Mathematics , 308(20):4816 X 4821, 2008.
 Chung, F. R. K. Spectral Graph Theory . American Mathematical Society, 1997.
 Ding, Chris H. Q. and He, Xiaofeng. K -means clus-tering via principal component analysis. In Brodley,
Carla E. (ed.), ICML , volume 69 of ACM Interna-tional Conference Proceeding Series . ACM, 2004. Drineas, Petros, Frieze, Alan M., Kannan, Ravi, Vem-pala, Santosh, and Vinay, V. Clustering large graphs via the singular value decomposition. Ma-chine Learning , 56(1-3):9 X 33, 2004.
 Figueiredo, M  X ario A. T. and Jain, Anil K. Unsuper-vised learning of finite mixture models. IEEE Trans. Pattern Anal. Mach. Intell. , 24(3):381 X 396, 2002. Haemers, W.H. Interlacing eigenvalues and graphs.
Linear Algebra Applications , 226/228:593 X 616, 1995.
 Jain, Anil K. Data clustering: 50 years beyond k-means. Pattern Recognition Letters , 31(8):651 X 666, 2010.
 Meila, Marina. Comparing clusterings: an axiomatic view. In Raedt, Luc De and Wrobel, Stefan (eds.),
ICML , volume 119 of ACM International Confer-ence Proceeding Series , pp. 577 X 584. ACM, 2005. ISBN 1-59593-180-5.
 Meila, Marina. The uniqueness of a good optimum for k-means. In Cohen, William W. and Moore,
Andrew (eds.), ICML , volume 148 of ACM Inter-national Conference Proceeding Series , pp. 625 X 632. ACM, 2006. ISBN 1-59593-383-2.
 Nagai, Ayumu. Inappropriateness of the criterion of k-way normalized cuts for deciding the number of clusters. Pattern Recognition Letters , 28(15):1981 X  1986, 2007.
 Shi, Jianbo and Malik, Jitendra. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. , 22(8):888 X 905, 2000.
 Steinley, D. &amp; Brusco, M. J. Testing for validity and choosing the number of clusters in k-means cluster-ing. Psychological Methods , 16:285 X 297, 2011. von Luxburg, Ulrike. A tutorial on spectral clustering. Statistics and Computing , 17(4):395 X 416, 2007. Zha, Hongyuan, He, Xiaofeng, Ding, Chris H. Q., Gu,
Ming, and Simon, Horst D. Spectral relaxation for k-means clustering. In Dietterich, Thomas G., Becker,
Suzanna, and Ghahramani, Zoubin (eds.), NIPS ,
