 Given an undirected, weighted graph, the commute distance between two vertices u and v is defined as the expected time it takes a random walk starting in vertex u to travel to vertex v and back to u . As opposed to the shortest path distance, it takes into account all paths between u and v , not just the shortest one. As a rule of thumb, the more paths connect u with v , the smaller the commute distance becomes. As a consequence, it supposedly satisfies the following, highly desirable property: It is because of this property that the commute distance has become a popular choice and is widely used, for example in clustering (Yen et al., 2005), semi-supervised learning (Zhou and Sch  X  olkopf, 2004), in social network analysis (Liben-Nowell and Kleinberg, 2003), for proximity search (Sarkar et al., 2008), in image processing (Qiu and Hancock, 2005), for dimensionality reduction (Ham et al., 2004), for graph embedding (Guattery, 1998, Saerens et al., 2004, Qiu and Hancock, 2006, Wittmann et al., 2009) and even for deriving learning theoretic bounds for graph labeling (Herbster and Pontil, 2006, Cesa-Bianchi et al., 2009). One of the main contributions of this paper is to establish that property ( F ) does not hold in many relevant situations.
 In this paper we study how the commute distance (up to a constant factor equivalent to the resistance distance, see below for exact definitions) behaves when the size of the graph increases. We focus on the case of random geometric graphs as this is most relevant to machine learning, but similar results hold for very general classes of graphs under mild assumptions. Denoting by H ij the expected hitting time, by C ij the commute distance between two vertices v i and v j and by d i the degree of vertex v i we prove that the hitting times and commute distances can be approximated (up to the constant vol( G ) that denotes the volume of the graph) by The intuitive reason for this behavior is that if the graph is large, the random walk  X  X ets lost X  in the sheer size of the graph. It takes so long to travel through a substantial part of the graph that by the time the random walk comes close to its goal it has already  X  X orgotten X  where it started from. For this reason, the hitting time H ij does not depend on the starting vertex v i any more. It only depends on the inverse degree of the target vertex v j , which intuitively represents the likelihood that the random walk exactly hits v j once it is in its neighborhood. In this respect it shows the same behavior as the mean return time at j (the mean time it takes a random walk that starts at j to return to its staring point) which is well-known to be vol( G )  X  1 /d j as well.
 Our findings have very strong implications: The raw commute distance is not a useful distance function on large graphs. On the negative side, our approximation result shows that contrary to popular belief, the commute distance does not take into account any global properties of the data, at least if the graph is  X  X arge enough X . It just considers the local density (the degree of the vertex) at the two vertices, nothing else. The resulting large sample commute distance dist ( v i , v j ) = 1 /d i + 1 /d j is completely meaningless as a distance on a graph. For example, all data points have the same nearest neighbor (namely, the vertex with the largest degree), the same second-nearest neighbor (the vertex with the second-largest degree), and so on. In particular, the main motivation to use the commute distance, Property ( F ) , no longer holds when the graph becomes  X  X arge enough X . Even more disappointingly, computer simulations show that n does not even need to be very large before ( F ) breaks down. Often, n in the order of 1000 is already enough to make the commute distance very close to its approximation expression (see Section 5 for details). This effect is even stronger if the dimensionality of the underlying data space is large. Consequently, even on moderate-sized graphs, the use of the raw commute distance as a basis for machine learning algorithms should be discouraged.
 Correcting the commute distance. It has been reported in the literature that hitting times and com-mute times can be observed to be quite small if the vertices under consideration have a high degree, and that the spread of the commute distance values can be quite large (Liben-Nowell and Kleinberg, 2003, Brand, 2005, Yen et al., 2009). Subsequently, the authors suggested several different methods to correct for this unpleasant behavior. In the light of our theoretical results we can see immediately why the undesired behavior of the commute distance occurs. Moreover, we are able to analyze the suggested corrections and prove which ones are meaningful and which ones not (see Section 4). Based on our theory we suggest a new correction, the amplified commute distance. This is a new distance function that is derived from the commute distance, but avoids its artifacts. This distance function is Euclidean, making it well-suited for machine learning purposes and kernel methods. Efficient computation of approximate commute distances. In some applications the commute distance is not used as a distance function, but for other reasons, for example in graph sparsification (Spielman and Srivastava, 2008) or when computing bounds on mixing or cover times (Aleliunas et al., 1979, Chandra et al., 1989, Avin and Ercal, 2007, Cooper and Frieze, 2009) or graph labeling (Herbster and Pontil, 2006, Cesa-Bianchi et al., 2009). To obtain the commute distance between all points in a graph one has to compute the pseudo-inverse of the graph Laplacian matrix, an operation of time complexity O ( n 3 ) . This is prohibitive in large graphs. To circumvent the matrix inversion, several approximations of the commute distance have been suggested in the literature (Spielman and Srivastava, 2008, Sarkar and Moore, 2007, Brand, 2005). Our results lead to a much simpler and well-justified way of approximating the commute distance on large random geometric graphs. We consider undirected, weighted graphs G = ( V, E ) with n vertices. We always assume that G is connected and not bipartite. The non-negative weight matrix (adjacency matrix) is denoted by W := ( w the volume of the graph. D denotes the diagonal matrix with diagonal entries d 1 , . . . , d n and is called the degree matrix . Our main focus in this paper is the class of random geometric graphs as it is most relevant to machine learning. Here we are given a sequence of points X 1 , . . . , X n that has been drawn i.i.d. from some underlying density p on R d . These points form the vertices v 1 , . . . , v n of the graph. The edges in the graph are defined such that  X  X eighboring points X  are connected: In the  X  -graph we connect two points whenever their Euclidean distance is less than or equal to  X  . In the undirected, symmetric k -nearest neighbor graph we connect v i to v j if X i is among the k nearest neighbors of X j or vice versa. In the mutual k -nearest neighbor graph we connect v i to v j if X i is among the k nearest neighbors of X j and vice versa. For space constraints we only discuss the case of unweighted graphs in this paper. Our results can be carried over to weighted graphs, in particular to weighted kNN-graphs and Gaussian similarity graphs.
 Consider the natural random walk on G , that is the random walk with transition matrix P = D  X  1 W . The hitting time H ij is defined as the expected time it takes a random walk starting in vertex v i to travel to vertex v j (with H ii := 0 by definition). The commute distance (also called commute time) between v i and v j is defined as C ij := H ij + H ji . Some readers might also know the commute distance under the name resistance distance. Here one interprets the graph as an electrical network where the edges represent resistors. The conductance of a resistor is given by the corresponding edge weight. The resistance distance R ij between i and j is defined as the effective resistance between the vertices i and j in the network. It is well known that the resistance distance coincides with the commute distance up to a constant: C ij = vol( G )  X  R ij . For background reading see Doyle and Snell (1984), Klein and Randic (1993), Xiao and Gutman (2003), Fouss et al. (2006), Bollob  X  as (1998), Lyons and Peres (2010).
 For the rest of the paper we consider a probability distribution with density p on R d . We want to study the behavior of the commute distance between two fixed points s and t . We will see that we only need to study the density in a reasonably small region X  X  R d that contains s and t . For convenience, let us make the following definition.
 Definition 1 (Valid region) Let p be any density on R d , and s, t  X  R d be two points with p ( s ) , p ( t ) &gt; 0 . We call a connected subset X  X  R d a valid region with respect to s, t and p if the following properties are satisfied: For readability reasons, we are going to state some of our main results using constants c i &gt; 0 . These constants are independent of n and the graph connectivity parameter (  X  or k , respectively) but depend on the dimension, the geometry of X , and p . The values of all constants are determined explicitly in the proofs. They do not coincide across different propositions. For notational convenience, we will formulate all the following results in terms of the resistance distance. To obtain the results for the commute distance one just has to multiply by factor vol( G ) . In this section we present our theoretical main results for random geometric graphs. We show that on this type of graph, the resistance distance R ij converges to the trivial limit 1 /d i + 1 /d j . For space constraints we only formulate these results for unweighted kNN and  X  -graphs. Similar results also hold for weighted variants of these graphs and for Gaussian similarity graphs.
 Theorem 2 (Resistance distance on kNN-graphs) Fix two points X i and X j . Consider a valid region X with respect to X i and X j with bottleneck h and density bounds p min and p max . Assume that X i and X j have distance at least h from the boundary of X and that ( k/n ) 1 /d / 2 p max  X  h . resistance distance on both the symmetric and the mutual kNN-graph satisfies The probability converges to 1 if n  X   X  and k/ log( n )  X   X  . The rhs of the deviation bound converges to 0 as n  X   X  , if k  X   X  and k/ log( n/k )  X   X  in case d = 3 , and if k  X   X  in case d &gt; 3 . Under these conditions, if the density p is continuous and if additionally k/n  X  0 , then kR ij  X  2 in probability.
 Theorem 3 (Resistance distance on  X  -graphs) Fix two points X i and X j . Consider a valid region X with respect to X i and X j with bottleneck h and density bounds p min and p max . Assume that X i and X j have distance at least h from the boundary of X and that  X   X  h . Then there exist constants c resistance distance on the  X  -graph satisfies The probability converges to 1 if n  X   X  and n X  d / log( n )  X   X  . The rhs of the deviation bound converges to 0 as n  X   X  , if n X  3 / log(1 / X  )  X   X  in case d = 3 , and if n X  d  X   X  in case d &gt; 3 . Under these conditions, if the density p is continuous and if additionally  X   X  0 , then Let us discuss the theorems en bloc. We start with a couple of technical remarks. Note that to achieve the convergence of the resistance distance we have to rescale it appropriately (for example, in the  X  -graph we scale by a factor of n X  d ). Our rescaling is exactly chosen such that the limit expressions are finite, positive values. Scaling by any other factor in terms of n ,  X  or k either leads to divergence or to convergence to zero.
 The convergence conditions on n and  X  (or k , respectively) are the ones to be expected for random geometric graphs. They are satisfied as soon as the degrees are of the order log( n ) (for smaller degrees, the graphs are not connected anyway, see e.g. Penrose, 1999). Hence, our results hold for sparse as well as for dense connected random geometric graphs.
 The valid region X has been introduced for technical reasons. We need to operate in such a region in order to be able to control the behavior of the graph, e.g. the average degrees. The assumptions on X are the standard assumptions used in the random geometric graph literature. In our setting, we have the freedom of choosing X  X  R d as we want. In order to obtain the tightest bounds one should aim for a valid X that has a wide bottleneck and a high minimal density.
 More generally, results about the convergence of the commute distance to 1 /d i + 1 /d j can also be proved for other kinds of graphs such as graphs with given expected degrees and even for power law graphs, under the assumption that the minimal degree in the graph slowly increases with n . Details are beyond the scope of this paper.
 Proof outline of Theorems 2 and 3 (full proofs are presented in the supplementary material). Con-sider two fixed vertices s and t in a connected graph and consider the graph as an electrical network where each edge has resistance 1. By the electrical laws, resistances in series add up, that is for two resistances R 1 and R 2 in series we get the overall resistance R = R 1 + R 2 . Resistances in parallel lines satisfy 1 /R = 1 /R 1 + 1 /R 2 . Now consult the situation in Figure 1. Consider the vertex s and all edges from s to its d s neighbors. The resistance  X  X panned X  by these d s parallel edges satisfies are very many paths. It turns out that the contribution of these paths to the resistance is negligible (essentially, we have so many wires between the two neighborhoods that electricity can flow nearly freely). So the overall effective resistance between s and t is dominated by the edges adjacent to i and j with contributions 1 /d s + 1 /d t .
 Providing a clean mathematical proof for this argument is quite technical. Our proof is based on Corollary 6 in Section IX.2 of Bollob  X  as (1998) that states that the resistance distance beween two fixed vertices s and t can be expressed as To apply this theorem one has to construct a flow that spreads  X  X s widely as possible X  over the whole graph. Counting edges and adding up resistances then leads to the desired results. Details are fiddly and can be found in the supplementary material. , Obviously, the large sample resistance distance R ij  X  1 /d i + 1 /d j is completely meaningless as a distance on a graph. The question we want to discuss in this section is whether there is a way to correct the commute distance such that this unpleasant large sample effect does not occur. Let us start with some references to the literature. It has been observed in several empirical studies that the commute distances are quite small if the vertices under consideration have a high degree, and that the spread of the commute distance values can be quite large. Our theoretical results immediately explain this behavior: if the degrees are large, then 1 /d i + 1 /d j is very small. And compared to the  X  X pread X  of d i , the spread of 1 /d i can be enormous.
 Several heuristics have been suggested to solve this problem. Liben-Nowell and Kleinberg (2003) suggest to correct the hitting times by simply multiplying by the degrees. For the commute distance, this leads to the suggested correction of C LNK ( i, j ) := d j H ij + d i H ji . Even though we did not prove it explicitly in our paper, the convergence results for the commute time also hold for the individual hitting times. Namely, hitting time H ij can be approximated by vol ( G ) /d j . These theoretical results immediately show that the correction C LNK is not useful, at least if we consider the absolute values. For large graphs, it simply has the effect of normalizing all hitting times to  X  1 , leading to C LNK  X  2 . However, we believe that the ranking introduced by this distance function still contains useful information about the data. The reason is that while the first order terms dominate the absolute value and converge to two, the second order terms introduce some  X  X ariation around two X , and this variation might encode the cluster structure.
 Yen et al. (2009) exploit the well-known fact that the commute distance is Euclidean and its kernel matrix coincides with the Moore-Penrose inverse L + of the graph Laplacian matrix. The authors now apply a sigmoid transformation to L + and consider K Yen ( i, j ) = 1 / (1 + exp(  X  l + ij / X  )) for some contant  X  . The idea is that the sigmoid transformation reduces the spread of the distance (or similarity) values. However, this is an ad-hoc approach that has the disadvantage that the resulting  X  X ernel X  K Yen is not positive definite.
 A third correction has been suggested in Brand (2005). As Yen et al. (2009) he considers the kernel matrix that corresponds to the commute distance. But instead of applying a sigmoid transformation he centers and normalizes the kernel matrix in the feature space. This leads to the corrected kernel K One the first glance it is surprising that using the centered and normalized kernel instead of the commute distance should make any difference. However, whenever one takes a Euclidean distance function of the form dist ( i, j ) = s ij + u i + u j  X  2  X  ij u i and computes the corresponding centered kernel matrix, one obtains where K s is the kernel matrix induced by s . Thus the off-diagonal terms are still influenced by u i but with a decaying factor 1 n compared to the diagonal. Even though this is no longer the case after normalization (because for the normalization the diagonal terms are important, and these terms still depend on the d i ), we believe that this is the key to why Brand X  X  kernel is useful.
 What would be a suitable correction based on our theoretical results? The proof of our main the-orems shows that the edges adjacent to i and j completely dominate the behavior of the resistance distance: they are the  X  X ottleneck X  of the flow, and their contribution 1 /d i + 1 /d j dominates all the other terms. The interesting information about the global topology of the graph is contained in the remainder terms S ij = R ij  X  1 /d i  X  1 /d j , which summarize the flow contributions of all other edges in the graph. We believe that the key to obtaining a good distance function is to remove the influence of the 1 /d i terms and  X  X mplify X  the influence of the general graph term S ij . This can be achieved by either using the off-diagonal terms of the pseudo-inverse graph Laplacian L  X  while ignoring its diagonal, or by building a distance function based on the remainder terms S ij directly. We choose the second option and propose the following new distance function. We define the amplified commute distance as C amp ( i, j ) = S ij + u ij with S ij = R ij  X  1 /d i  X  1 /d j and u ij = 2 w ij /d i d j  X  w ii /d 2 i  X  w jj /d 2 j . Of course we set C amp ( i, i ) = 0 for all i . Proposition 4 (Amplified commute distance is Euclidean) The matrix D with entries d ij = C amp ( i, j ) 1 / 2 is a Euclidean distance matrix.
 Proof outline. In preliminary work we show that the remainder terms can be written as S ij = (see the proof of Proposition 2 in von Luxburg et al., 2010). This implies the desired statement. , Additionally to being a Euclidean distance, the amplified commute distance has a nice limit behavior. When n  X  X  X  the terms u ij are dominated by the terms S ij , hence all that is left are the  X  X nteresting terms X  S ij . For all practical purposes, one should use the kernel induced by the amplified commute distance and center and normalize it. In formulas, the amplified commute kernel is (where I is the identity matrix, 1 the vector of all ones, and C amp the amplified commute distance matrix). The next section shows that the kernel K amp works very nicely in practice.
 Note that the correction by Brand and our amplified commute kernel are very similar, but not identi-cal with each other. The off-diagonal terms of both kernels are very close to each other, see Equation (1), that is if one is only interested in a ranking based on similarity values, both kernels behave simi-larly. However, an important difference is that the diagonal terms in the Brand kernel are way bigger than the ones in the amplified kernel (using our convergence techniques one can show that the Brand kernel converges to an identity matrix, that is the diagonal completely dominates the off-diagonal terms). This might lead to the effect that the Brand kernel behaves worse than our kernel with algorithms like the SVM that do not ignore the diagonal of the kernel. Our first set of experiments considers the question how fast the convergence of the commute distance takes place in practice. We will see that already for relatively small data sets, a very good approx-imation takes place. This means that the problems of the raw commute distance already occur for small sample size. Consider the plots in Figure 2. They report the maximal relative error defined as max ij | R ij  X  1 /d i  X  1 /d j | /R ij and the corresponding mean relative error on a log 10 -scale. We show the results for  X  -graphs, unweighted kNN graphs and Gaussian similarity graphs (fully connected weighted graphs with edge weights exp( k x i  X  x j k 2 / X  2 ) ). In order to be able to plot all results in the same figure, we need to match the parameters of the different graphs. Given some value k for the kNN-graph we thus set the values of  X  for the  X  -graph and  X  for the Gaussian graph to be equal to the maximal k -nearest neighbor distance in the data set.
 Sample size. Consider a set of points drawn from the uniform distribution on the unit cube in R 10 . As can be seen in Figure 2 (first plot), the maximal relative error decreases very fast with increasing sample size. Note that already for small sample sizes the maximal deviations get very small. Dimension. A result that seems surprising at first glance is that the maximal deviation decreases Figure 2: Relative deviations between true and approximate commute distances. Solid lines show the maximal relative deviations, dashed lines the mean relative deviations. See text for details. as we increase the dimension, see Figure 2 (second plot). The intuitive explanation is that in higher dimensions, geometric graphs mix faster as there exist more  X  X hortcuts X  between the two sides of the point cloud. Thus, the random walk  X  X orgets faster X  where it started from.
 Clusteredness. The deviation gets worse if the data has a more pronounced cluster structure. Con-sider a mixture of two Gaussians in R 10 with unit variances and the same weight on both compo-nents. We call the distance between the centers of the two components the separation. In Figure 2 (third plot) we show both the maximum relative errors (solid lines) and mean relative errors (dashed lines). We can clearly see that with increasing separation, the deviation increases.
 Sparsity. The last plot of Figure 2 shows the relative errors for increasingly dense graphs, namely for increasing parameter k . Here we used the well-known USPS data set of handwritten digits (9298 points in 256 dimensions). We plot both the maximum relative errors (solid lines) and mean relative errors (dashed lines). We can see that the errors decrease the denser the graph gets. Again this is due to the fact that the random walk mixes faster on denser graphs. Note that the deviations are extremely small on this real-world data set.
 In a second set of experiments we compare the different corrections of the raw commute distance. To this end, we built a kNN graph of the whole USPS data set (all 9298 points, k = 10 ), computed the commute distance matrix and the various corrections. The resulting matrices are shown in Figure 3 (left part) as heat plots. In all cases, we only plot the off-diagonal terms. We can see that as predicted by theory, the raw commute distance does not identify the cluster structure. However, the cluster structure is still visible in the kernel corresponding to the commute distance, the pseudo-inverse graph Laplacian L  X  . The reason is that the diagonal of this matrix can be approximated by (1 /d 1 , ...., 1 /d n ) , whereas the off-diagonal terms encode the graph structure, but on a much smaller scale than the diagonal. In our heat plots, all four corrections of the graph Laplacian show the cluster structure to a certain extent (the correction by LNK to a small extent, the corrections by Brand, Yen and us to a bigger extent).
 A last experiment evaluates the performance of the different distances in a semi-supervised learning task. On the whole USPS data set, we first chose some random points to be labeled. Then we classified the unlabeled points by the k -nearest neighbor classifier based on the distances to the labeled data points. For each classifier, k was chosen by 10-fold cross-validation among k  X  { 1 , ..., 10 } . The experiment was repeated 10 times. The mean results can be seen in Figure 3 (right figure). As baseline we also report results based on the standard Euclidean distance between the data points. As predicted by theory, we can see that the raw commute distance performs extremely poor. The Euclidean distance behaves reasonably, but is outperformed by all corrections of the commute distance. This shows first of all that using the graph structure does help over the basic Euclidean distance. While the naive correction by LNK stays close to the Euclidean distance, the three corrections by Brand, Yen and us virtually lie on top of each other and outperform the Figure 3: Figures on the left: Distances and kernels based on a kNN graph between all 9298 USPS points (heat plots, off-diagonal terms only): exact resistance distance, pseudo-inverse graph Lapla-cian L  X  ; kernels corresponding to the corrections by LNK, Yen, Brand, and our amplified K amp . Figure on the right: Semi-supervised learning results based on the different distances and kernels. The last three lines corresponding to the amplified, Brand and Yen kernel lie on top of each other. other methods by a large margin.
 We conclude with the following tentative statements. We believe that the correction by LNK is  X  X  bit too naive X , whereas the corrections by Brand, Yen and us  X  X end to work X  in a ranking based setting. Based on our simple experiments it is impossible to judge which out of these candidates is  X  X he best one X . We are not too fond of Yen X  X  correction because it does not lead to a proper kernel. Both Brand X  X  and our kernel converge to (different) limit functions. So far we do not know the theoretical properties of these limit functions and thus cannot present any theoretical reason to prefer one over the other. However, we think that the diagonal dominance of the Brand kernel can be problematic. In this paper we have proved that the commute distance on random geometric graphs can be approximated by a very simple limit expression. Contrary to intuition, this limit expression no longer takes into account the cluster structure of the graph, nor any other global property (such as distances in the underlying Euclidean space). Both our theoretical bounds and our simulations tell the same story: the approximation gets better if the data is high-dimensional and not extremely clustered, both of which are standard situations in machine learning. This shows that the use of the raw commute distance for machine learning purposes can be problematic. However, the structure of the graph can be recovered by certain corrections of the commute distance. We suggest to use either the correction by Brand (2005) or our own amplified commute kernel from Section 4. Both corrections have a well-defined, non-trivial limit and perform well in experiments.
 The intuitive explanation for our result is that as the sample size increases, the random walk on the sample graph  X  X ets lost X  in the sheer size of the graph. It takes so long to travel through a substantial part of the graph that by the time the random walk comes close to its goal it has already  X  X orgotten X  where it started from. Stated differently: the random walk on the graph has mixed before it hits the desired target vertex. On a higher level, we expect that the problem of  X  X etting lost X  not only affects the commute distance, but many other methods where random walks are used in a naive way to explore global properties of a graph. For example, the results in Nadler et al. (2009), where artifacts of semi-supervised learning in the context of many unlabeled points are studied, seem strongly related to our results. In general, we believe that one has to be particularly careful when using random walk based methods for extracting global properties of graphs in order to avoid getting lost and converging to meaningless results. R. Aleliunas, R. Karp, R. Lipton, L. Lov  X  asz, and C. Rackoff. Random walks, universal traversal sequences, and the complexity of maze problems. In FOCS , 1979.
 C. Avin and G. Ercal. On the cover time and mixing time of random geometric graphs. Theor. Comput. Sci , 380(1-2):2 X 22, 2007.
 B. Bollob  X  as. Modern Graph Theory . Springer, 1998.
 M. Brand. A random walks perspective on maximizing satisfaction and profit. In SDM , 2005. N. Cesa-Bianchi, C. Gentile, and F. Vitale. Fast and optimal prediction on a labeled tree. In COLT , 2009.
 A. Chandra, P. Raghavan, W. Ruzzo, R. Smolensky, and P. Tiwari. The electrical resistance of a graph captures its commute and cover times. In STOC , 1989.
 C. Cooper and A. Frieze. The cover time of random geometric graphs. In SODA , 2009.
 P. G. Doyle and J. L. Snell. Random walks and electric networks . Mathematical Association of America, Washington, DC, 1984.
 F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens. A novel way of computing dissimilarities between nodes of a graph, with application to collaborative filtering and subspace projection of the graph nodes. Technical Report IAG WP 06/08, Universit  X  e catholique de Louvain, 2006. S. Guattery. Graph embeddings, symmetric real matrices, and generalized inverses. Technical report, Institute for Computer Applications in Science and Engineering, NASA Research Center, 1998. J. Ham, D. D. Lee, S. Mika, and B. Sch  X  olkopf. A kernel view of the dimensionality reduction of manifolds. In ICML , 2004.
 M. Herbster and M. Pontil. Prediction on a graph with a perceptron. In NIPS , 2006.
 D. Klein and M. Randic. Resistance distance. Journal of Mathematical Chemistry , 12:81 X 95, 1993. D. Liben-Nowell and J. Kleinberg. The link prediction problem for social networks. In CIKM , 2003. R. Lyons and Y. Peres. Probability on trees and networks. Book in preparation, available online on the webpage of Yuval Peres , 2010.
 B. Nadler, N. Srebro, and X. Zhou. Statistical analysis of semi-supervised learning: The limit of infinite unlabelled data. In NIPS , 2009.
 M. Penrose. A strong law for the longest edge of the minimal spanning tree. Ann. of Prob. , 27(1): 246  X  260, 1999.
 H. Qiu and E. R. Hancock. Image segmentation using commute times. In BMVC , 2005.
 H. Qiu and E. R. Hancock. Graph embedding using commute time. S+SSPR 2006 , pages 441 X 449, 2006.
 M. Saerens, F. Fouss, L. Yen, and P. Dupont. The principal components analysis of a graph, and its relationships to spectral clustering. In ECML , 2004.
 P. Sarkar and A. Moore. A tractable approach to finding closest truncated-commute-time neighbors in large graphs. In UAI , 2007.
 P. Sarkar, A. Moore, and A. Prakash. Fast incremental proximity search in large graphs. In ICML , 2008.
 D. Spielman and N. Srivastava. Graph sparsification by effective resistances. In STOC , 2008. U. von Luxburg, A. Radl, and M. Hein. Hitting times, commute distances and the spectral gap in large random geometric graphs. Preprint available at Arxiv , March 2010.
 D. M. Wittmann, D. Schmidl, F. Bl  X  ochl, and F. J. Theis. Reconstruction of graphs based on random walks. Theoretical Computer Science , 2009.
 W. Xiao and I. Gutman. Resistance distance and Laplacian spectrum. Theoretical Chemistry Ac-counts , 110:284 X 298, 2003.
 L. Yen, D. Vanvyve, F. Wouters, F. Fouss, M. Verleysen, and M. Saerens. Clustering using a random walk based distance measure. In ESANN , 2005.
 L. Yen, F. Fouss, C. Decaestecker, P. Francq, and M. Saerens. Graph nodes clustering based on the commute-time kernel. Advances in Knowledge Discovery and Data Mining , pages 1037 X 1045, 2009.
 D. Zhou and B. Sch  X  olkopf. Learning from Labeled and Unlabeled Data Using Random Walks. In
DAGM , 2004.
