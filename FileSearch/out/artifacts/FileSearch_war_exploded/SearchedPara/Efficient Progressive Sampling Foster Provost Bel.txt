 specifies the size of a sample to be provided to an contains N instances in total, ni 2 N for all i. There are three fundamental questions regarding progressive sampling. 1. What is an efficient sampling schedule? 2. How can convergence (i.e., that model quality 3. As sampling progresses, can the schedule be adapted 
We discuss several ways to assess efficiency, and how various progressive sampling procedures fare with re-spect to each. Notably, we show that schedules in which the ni increase geometrically are optimal in an asymp-totic sense. We explore the question of optimal effi-ciency in an absolute sense: what is the most efficient schedule given one X  X  prior expectations of convergence? 
Next, we address the crucial practical issue of conver-gence detection. We describe an interaction between the sampling schedule and the method of convergence detection, and we describe a practical alternative that avoids the worst aspects of the tradeoffs this interac-tion requires. We also discuss algorithms that schedule adaptively, based on knowledge of convergence and ac-tual run-time complexity, obtained on the fly. 
We then investigate empirically how a variety of schedules perform on large benchmark data sets. Fi-nally, we discuss why progressive sampling is especially beneficial in cases where sampling from a large database is inefficient. We conclude that, in a wide variety of re-alistic circumstances, progressive sampling is preferable to analyzing all instances from a database. Surprisingly, it can be competitive even when the optimal sample size is known in advance. 23 n t no 
A4 t model induced from n instances while not converged end while return M of granularity of the sampling schedule. Given the relatively course granularity of many schedules, sudden increases in accuracy can occur without impairing progressive sampling. Our empirical results in section section 6 bear out these assumptions. 
When a learning curve reaches its final plateau, we say it has converged. We denote the training set size at which convergence occurs as n,in. 
Definition 1 Given a data set, a sampling procedure, and an induction algorithm, n,i, is the size of the smallest sufficient training set. Models built with smaller training sets have lower accuracy than models built with from training sets of size nmin, and models built with larger training sets have no higher accuracy. 
Figure 1 shows an example sampling schedule and its relation to a learning curve. Empirical estimates are necessary to determine nmin because the precise shape of a learning curve represents a complex interaction between the statistical regularities present in a given data set and the abilities of an induction algorithm to identify and represent those regularities. In general, these characteristics are not known in advance, nor is their interaction well understood. Thus, in many cases, n,in is nearly impossible to determine from theory. 
However, n,in can be approximated empirically by a progressive sampling procedure. We denote by &amp;ila a procedure X  X  approximation to n,in. 
Figure 2 is a generic algorithm that defines the family of progressive sampling methods, An instance of this family has particular methods for selecting a schedule, for determining convergence, and for altering the schedule adaptively. The next three sections consider each of these methods in turn. 
We now discuss several alternative methods for selecting an efficient schedule. For now, we assume that progressive sampling is able to detect convergence and we assume that this detection can be performed efficiently (its worst-case run-time complexity is not worse than that of the underlying induction algorithm). conditions sufficiently general that simple progressive sampling should be used routinely? 
Consider an underlying induction algorithm with a simple polynomial run-time complexity f(n), and a simple analytical model with three parameters: b, the number of schedule points executed prior to the point of convergence (nb is the first schedule point greater than &amp;in); T, the ratio of the total number of instances to the size of the final sample (r = g); and c, the exponent of the run-time complexity (f(n) = n X ). 
Based on these parameters, we can define the condi-tions under which the computational cost of progressive sampling and the cost of using all N instances are equal. That is, the conditions under which: where T = g and where the relationship among the elements of the partial schedule {no,nl, n2,. . . ,nr,} is determined by the given progressive sampling method (e.g., arithmetic or geometric). 
Sets of parameter values that satisfy the equation above are shown in Figure 3. The curves show the boundaries between regions where it is more eflicent to use progressive sampling and regions where it is more efficient to use all N instances. Above the curves, progressive sampling is more efficient; below the curves, complete sampling is more efficient. The left-and right-hand figures show curves for arithmetic and geometric sampling, respectively. 
The curves show that progressive sampling will be more efficient than learning with all the examples un-der a wide variety of circumstances. For example, arith-metic sampling that requires 30 samples to be tried be-fore convergence is detected is better if the computa-tional complexity is greater than n2 and the final sam-ple size is less than one-quarter of N. If, however, the final sample size is one-half of N, arithmetic sampling will not be better unless only 10 or fewer samples are needed prior to detecting convergence. 
Geometric sampling is far more forgiving. If com-putational complexity is n2 or worse, then geometric sampling is better as long as convergence can be de-tected with samples smaller than N/2. If computational complexity is linear, then geometric sampling is better as long as convergence can be detected with samples smaller than N/6. The number of samples prior to con-vergence is nearly irrelevant. This is the essential fea-ture of geometric sampling that makes it efficient. If a schedule contains a large number of samples prior to the sample when convergence is detected, those samples will almost all be quite small. In contrast, increasing the number of samples in an arithmetic schedule adds sam-ples along the schedule X  X  entire range, and this results in a much larger computational cost. samples processed before detecting convergence. 
Now, we assume convergence is well detected, so which means that run time of S, ( on all subsets, including running the convergence-detection procedure) is This is 
The final, finite sum is less than the corresponding infinite sequence. Because a &gt; 1 for Ss, this converges to a constant that is independent of n,in. Since O(f (e)) is polynomial, the overall run time of Ss is O(f (n,in)). 
Therefore, progressive sampling asymptotically is no worse than the optimal progressive sampling procedure, 
SO, which runs the induction algorithm only on the smallest suficient training set. 0 
Because it is simple and asymptotically optimal, we propose that geometric sampling, for example with a = 2, is a good default schedule for mining large data sets. We provide empirical results testing this proposition and discuss further reasons in section 6. 
First we explore further the topic of optimal schedules. 3.4 Optimality with respect to expectations 
Comparisons with SN and SO represent two ends of a spectrum of expectations about n,in. Can optimal 26 Table 1: Expected costs of various schedules given 
N = 10, f(n) = n2 and a uniform prior. {3,7, lo}. The value of C is as follows: 
C = 5 Wni-l)f(ni) = +(O)f(3) + @(3)f(7) + +(7)f(lO) 
With probability 1 (a(O) = l), an initial model will be built with 3 instances at a cost of f(3). If more than 3 instances are required for convergence, an event that occurs with probability @(3), a second model will be built with 7 instances at a cost of f(7). Finally, with probability @( 7) more than 7 instances are required for convergence and a third model will be built with all 10 instances at a cost of f(l0). The expected cost of a schedule is the sum of the cost of building each model times the probability that the model will actually need to be constructed. 
Consider another example in which N = 10; the uniform prior is used (a(n) = (N -n)/N), and f(n) = n2. The costs for three different schedules are shown in Table 1. The first schedule, in which a model is constructed for each possible data set size, is the most expensive of the three shown. The second schedule, in which a single model is built with all of the instances, also is not optimal in the sense of expected cost given a uniform prior. The third schedule shown has the lowest cost of all 21 X  = 1024 possible schedules for this problem. 
Given N, f and a, we want to determine the schedule S that minimizes C. That is, we want to find the optimal schedule. As noted previously, a brute force approach to this problem would explore exponentially many schedules. We take advantage of the fact that optimal schedules are composed of optimal sub-schedules to apply dynamic programming to this problem, resulting in a polynomial time algorithm. 
Let m[i,j] be the cost of the minimum expected-cost schedule of all samples in the size range [i, j], with the requirement that samples of i and of j instances be included in m[i, j]. The cost of the optimal schedule given a data set containing N instances is then m[O, N]. 
The following recurrence can be used to compute m[O, N]: . 
Both the bottom-up table-based and top-down mem-oized implementations of this equation require O(N2) 27 
We call progressive sampling based on the optimal schedule determined by this dynamic programming A key assumption behind all the progressive sampling procedures discussed above is that convergence can be detected accurately and efficiently. We present some preliminary results below, and we believe that convergence detection remains an open problem on which significant research. effort should be focused. 
Convergence detection is fundamentally a statistical judgment, irrespective of the specific convergence cri-terion or the method to estimate whether that crite-rion has been met. In their paper on arithmetic sam-pling, John and Langley [8] model the learning curve as sampling progresses. They determine convergence using a stopping criterion modeled after the work of Valiant [18]. Specifically, convergence is reached when 
Pr((acc(N) -acc(ni)) &gt; E) &lt; 6, where act(z) is the accuracy of the model that an algorithm produces after seeing x instances, E refers to the maximum acceptable decrease in accuracy, and 6 is a probability that the maximum accuracy difference will be exceeded on any individual run. A model of the learning curve is used to estimate act(N). 
Statistical estimates of convergence face several chal-lenges. First, statistical estimation of complete learn-ing curves is fraught with difficulties. Actual learning curves often require a complex functional form to esti-mate accurately. The curve shown in figure 1 has three regions of behavior-a primary rise, a secondary rise, and a plateau. Most simple functional forms (e.g., the power laws used by Frey and Fisher [4] and by John and Langley [S]) g enerally cannot capture all three re-gions of behavior, often causing the estimated curves to converge too quickly or never to converge. Estimating convergence is genera1ly more challenging than fitting earlier parts of the curve, and even fairly small errors can mislead progressive sampling methods. For exam-ple, a power law may fit the early part of the curve well, but will represent a long, final plateau as a long (perhaps slight) incline. 
More important, the need for accurate statistical es-timates must be balanced against the goal of computa-tional efficiency. Statistical estimates of convergence are aided by increasing the number of points in a schedule, but this directly impairs efficiency. Even worse, deter-mining convergence is aided most by samples for which ni &gt; Gin, because these points will most assist the sta-tistical determination that a. plateau has been reached. 
Of course, these are the very sample sizes that, for ef-ficiency reasons, progressive sampling schedules should avoid. 
The most promising approach we have yet identified 28 5.2 The cost of running the induction 
The second assumption of DP sampling is that we have an accurate model of the run-time complexity (in n) of the underlying induction algorithm. Run-time complexity models are not always easy to obtain. For example, our empirical results below use the decision-tree algorithm C4.5 [17], for which reported time complexity varies widely. 
Moreover, DP sampling requires the actual run-time complexity for the problem in question, rather than a worst-case complexity. We obtained empirical estimates of the complexity of C4.5 on the data sets used below, and found O(n1.22) for LED, O(n1.37) for WAVEFORM, and O(n1.38) for CENSUS.~ 
As with learning curve estimation, progressive sam-pling can determine the actual run-time complexity dynamically as the sampling progresses. As before, early in the schedule, with small samples, suboptimal scheduling due to an incorrect time-complexity model will have little overall effect. As the samples grow and bad estimates would be costly, the time-complexity model becomes more accurate. 
We have shown that, in principle, progressive sampling can be efficient. We now evaluate whether progres-sive sampling can be used for practical scaling. We hypothesize that geometric sampling and DP sampling are considerably less expensive than using all the data when convergence is early, and not too much more ex-pensive when convergence is late. We further hypothe-size that, for large data sets and non-incremental algo-rithms, these versions of progressive sampling are sig-nificantly better than arithmetic progressive sampling. 
We also investigate how well simple geometric sampling compares with DP sampling. 
We compare progressive sampling with several differ-ent schedules: S, = {N}, a single sample with all the instances; So = {n min}, the optimal schedule deter-mined by an omniscient oracle; S, = 100 + (100 . i), arithmetic sampling with no = ng = 10 X 0; S, = 100. 2i, geometric sampling with no = 100 and a = 2; and 
S+, DP sampling with schedule recomputation after dynamic estimation of priors and run-time complexity. 
Of the three progressive sampling methods, only DP sampling revises its schedules. In order to determine the probability of convergence, with Sdp progressive 4We followed a similar procedure to Frey and Fisher [4] and and used the resulting slope as an estimate of c. 29 Data set Full Arith Geo DP Oracle LED 16.40 1.77 0.55 0.56 0.18 CENSUS 16.33 59.68 5.57 5.41 2.08 WAVEFORM 425.31 1230.00 41.84 50.57 22.12 
Table 4: Computation time for several progressive sampling methods faster than geometric sampling. Constructing the DP schedule takes less than &amp;th of a second, so the DP overhead is not responsible. DP would be faster with more precise knowledge of n,in. Indeed, non-adaptive 
DP with uniform priors consistently produces poor schedules on these data sets. This is because the time complexity of C4.5 is rather low (O(n1.22) for 
LED O(n X .37) for WAVEFORM, and O(n1.38) for CENSUS) (cf. the schedule for linear complexity with uniform priors in Table 2), but these data sets have relatively large values for T* = N/n,in; specifically, r&amp;sus = 4, r* -8.33, and I$,~ = 
WavefoTm -50. Adaptive modeling of the probability of convergence is critical, but apparently we must devise a more accurate technique if we want to beat geometric sampling.5 Now we move on to the detection of convergence. We used LRLS to estimate convergence for adaptive 
DP sampling. For these experiments, at each schedule point, ten samples were taken for convergence estima-tion, 1 = 10, and convergence was indicated the first time that the 95% confidence interval on the slope of the regression line included zero. Table 5 compares the accuracy of the models built using LRLS convergence detection, with the accuracy using optimal convergence detection from above (i.e., it can query the oracle); each value represents the average of 10 runs where instances were randomized prior to each run. 
LRLS identifies convergence accurately. In most cases, LRLS correctly identifies nb as the first schedule point after n,in, and in nearly all other cases conver-gence is identified in a sample for which nb &gt; n,in. In no data set was the mean accuracy at estimated con-vergence statistically distinguishable from the accuracy on n,in instances, the point of true convergence. 
However, LRLS has a large effect on absolute compu-tation time. The additional sampling and executions of the induction algorithm create a large (constant) factor increase in the total computational cost. Table 6 com-pares the run time with LRLS to running with  X  X ree X  convergence detection, to running on the full data set, and to knowing n,in in advance. 5Experiments with increasingly precise knowledge of n,in show DP sampling to be less efficient than geometric sampling are wide. Of course, if the distributions are very narrow, DP sampling comes very close to SO, the optimal schedule. 30 and its statistical properties, they compute an estimate of the sample size needed to have less than a specified loss in information. However, because this estimate can overshoot A min greatly, they then calculate nd for an efficient arithmetic schedule, and revise the estimate after executing each schedule point. Other sequential multi-sample learning methods [14] are degenerate instances of progressive sampling, typically using fixed arithmetic schedules and treating convergence detection simplistically, if at all. 
For this paper, we have considered only drawing random samples from the larger data set. We believe that the results will generalize to other methods of sampling, but have not yet studied the general case. 
Methods for active sampling, choosing subsequent samples based upon the models learned previously, are of particular interest. A classic example of active sampling is windowing [16], wherein subsequent sampling chooses instances for which the current model makes errors. Active sampling changes the learning curve. For example, on noisy data, windowing learning curves are notoriously ill behaved: subsequent samples contain increasing amounts of noise, and performance often decreases as sampling progresses [5]. It would be interesting to examine more closely the use of the techniques outlined above in the context of active sampling, and the potential synergies. 
With this work we have made substantial progress to-ward efficient progressive sampling. We have shown that if convergence detection can be done very effi-ciently, then progressive sampling is far better than learning from all the data, and almost as efficient as being given the minimum sufficient training set by an oracle. We have shown that convergence detection can be done effectively and moderately efficiently. We have also shown that geometric sampling is remarkably robust: its efficiency is insensitive to the number of points in the schedule (unlike arithmetic sampling); it is asymptotically no worse than knowing the point of convergence in advance, and in practice it performs as well as much more complicated adaptive scheduling. What is left are two well-defined challenges for future 
KDD research: increase the efficiency of convergence detection, and devise an accurate method for estimating the point of convergence from a partial learning curve. 
One of the defining problems of KDD is classifier induction from massive data sets [14]. The existence of an efficient progressive sampling procedure would take a giant step toward solving it. 31 [I31 
