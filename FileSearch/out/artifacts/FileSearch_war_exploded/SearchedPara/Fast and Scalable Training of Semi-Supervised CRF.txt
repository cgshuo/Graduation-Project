 Conditional random fields (CRFs) are undirected graphical models that have been successfully ap-plied to the classification of relational and temporal data [1]. Training complex CRF models with large numbers of input features is slow, and exact inference is often intractable. The ability to select the most informative features as needed can reduce the training time and the risk of over-fitting of parameters. Furthermore, in complex modeling tasks, obtaining the large amount of labeled data necessary for training can be impractical. On the other hand, large unlabeled datasets are often easy to obtain, making semi-supervised learning methods appealing in various real-world applications. The goal of our work is to build an activity recognition system that is not only accurate but also scal-able, efficient, and easy to train and deploy. An important application domain for activity recognition technologies is in health-care, especially in supporting elder care, managing cognitive disabilities, and monitoring long-term health. Activity recognition systems will also be useful in smart environ-ments, surveillance, emergency and military missions. Some of the key challenges faced by current activity inference systems are the amount of human effort spent in labeling and feature engineering and the computational complexity and cost associated with training. Data labeling also has privacy implications because it often requires human observers or recording of video. In this paper, we intro-duce a fast and scalable semi-supervised training algorithm for CRFs and evaluate its classification performance on extensive real world activity traces gathered using wearable sensors. In addition to being computationally efficient, our proposed method reduces the amount of labeling required during training, which makes it appealing for use in real world applications. Several supervised techniques have been proposed for feature selection in CRFs. For discrete fea-tures, McCallum [2] suggested an efficient method for feature induction by iteratively increasing conditional log-likelihood. Dietterich [3] applied gradient tree boosting to select features in CRFs by combining boosting with parameter estimation for 1D linear-chain models. Boosted random fields (BRFs) [4] combine boosting and belief propagation for feature selection and parameter esti-mation for densely connected graphs that have weak pairwise connections. Recently, Liao et.al. [5] developed a more general version of BRFs, called virtual evidence boosting (VEB) that does not make any assumptions about graph connectivity or the strength of pairwise connections. The ob-jective function in VEB is a soft version of maximum pseudo-likelihood (MPL), where the goal is to maximize the sum of local log-likelihoods given soft evidence from its neighbors. This objective function is similar to that used in boosting, which makes it suitable for unified feature selection and parameter estimation. This approximation applies to any CRF structures and leads to a significant reduction in training complexity and time. Semi-supervised training techniques have been exten-sively explored in the case of generative models and naturally fit under the expectation maximization framework [6]. However, it is not straight forward to incorporate unlabeled data in discriminative models using the traditional conditional likelihood criteria. A few semi-supervised training meth-ods for CRFs have been proposed that introduce dependencies between nearby data points [7, 8]. More recently, Grandvalet and Bengio [9] proposed a minimum entropy regularization framework for incorporating unlabeled data. Jiao et.al. [10] used this framework and proposed an objective function that combines the conditional likelihood of the labeled data with the conditional entropy of the unlabeled data to train 1D CRFs, which was extended to 2D lattice structures by Lee et.al. [11]. In our work, we combine the minimum entropy regularization framework for incorporating unla-beled data with VEB for training CRFs. The contributions of our work are: (i) semi-supervised virtual evidence boosting (sVEB) -an efficient technique for simultaneous feature selection and semi-supervised training of CRFs, which to the best of our knowledge is the first method of its kind, (ii) experimental results that demonstrate the strength of sVEB, which consistently outper-forms other training techniques on synthetic data and real-world activity classification tasks, and (iii) analysis of the time and complexity requirements of our algorithm, and comparison with other existing techniques that highlight the significant computational advantages of our approach. The sVEB algorithm is fast and easy to implement and has the potential of being broadly applicable. Maximum likelihood parameter estimation in CRFs involves maximizing the overall conditional log-likelihood, where x is the observation sequence and y is the hidden state sequence: The conditional distribution is defined by a log-linear combination of k features functions f k asso-ciated with weight  X  k . A regularizer on  X  is used to keep the weights from getting too large and such as mean field approximation or loopy belief propagation [12, 13] are used.
 MPL [14] and VEB [5] are such techniques. For MPL the CRF is cut into a set of independent patches; each patch consists of a hidden node or class label y i , the true value of its direct neighbors and the observations, i.e., the Markov Blanket( MB y becomes maximizing the pseudo log-likelihood: MPL has been known to over-estimate the dependency parameters in some cases and there is no general guideline on when it can be safely used [15]. 2.1 Virtual evidence boosting By extending the standard LogitBoost algorithm [16], VEB integrates boosting based feature se-lection into CRF training. The objective function used in VEB is very similar to MPL, except that VEB uses the messages from the neighboring nodes as virtual evidence instead of using the true labels of neighbors. The use of virtual evidence helps to reduce over-estimation of neighborhood dependencies. We briefly explain the approach here but please refer to [5] for more detail. VEB incorporates two types of observations nodes: (i) hard evidence corresponding to the observa-tions ve ( x i ) , which are indicator functions at the observation values and (ii) soft evidence, corre-sponding to the messages from neighboring nodes ve ( n ( y i )) , which are discrete distributions over VEB learns a set weak learners f t s iteratively and estimates the combined feature F t = F t  X  1 + f t by solving the following weighted least square error(WLSE) problem: f t ( ve i ) = arg min data point, exactly as in LogitBoost. However, the least square problem for VEB (eq.3) involves NX points because of virtual evidence as opposed to N points in LogitBoost. Although eq. 4 is done that in our experiments. At each iteration, ve i is updated as messages from n ( y i ) changes with the addition of new features. We run belief propagation (BP) to obtain the virtual evidence before each iteration. The CRF feature weights,  X   X  X  are computed by solving the WLSE problem, where is the virtual evidence from the neighbors.:  X  k = 2.2 Semi-supervised training For semi-supervised training of CRFs, Jiao et.al. [10] have proposed an algorithm that utilizes unla-beled data via entropy regularization  X  an extension of the approach proposed by [9] to structured CRF models. The objective function that is maximized during semi-supervised training of CRFs is given below, where ( x l , y l ) and ( x u , y u ) represent the labeled and unlabeled data respectively: By minimizing the conditional entropy of the unlabeled data, the algorithm will generally find la-beling of the unlabeled data that mutually reinforces the supervised labels. One drawback of this objective function is that it is no longer concave and in general there will be local maxima. The authors [10] showed that this method is still effective in improving an initial supervised model. In this work, we develop semi-supervised virtual evidence boosting (sVEB) that combines feature selection with semi-supervised training of CRFs. sVEB extends the VEB framework to take advan-tage of unlabeled data via minimum entropy regularization similar to [9, 10, 11]. The new objective function L sV EB we propose is as follows, where ( i = 1  X  X  X  N ) are labeled and ( i = N + 1  X  X  X  M ) are unlabled examples: The sVEB aglorithm, similar to VEB, maximizes the conditional soft pseudo-likelihood of the la-beled data but in addition minimizes the conditional entropy over unlabeled data. The  X  is a tuning parameter for controlling how much influence the unlabeled data will have.
 By considering the soft pseudo-likelihood in L sV EB and using BP to estimate p ( y i | ve i ) , sVEB can use boosting to learn the parameters of CRFs. The virtual evidence from the neighboring nodes captures the label dependencies. There are three different types of feature functions f s that X  X  used: for continuous observations f 1 ( x i ) is a linear combination of decision stumps, for discrete obser-vations the learner f 2 ( x i ) is expressed as indicator functions, and for virtual evidences the weak learner f 3 ( x i ) is the weighted sum of two indicator functions (for binary case). These functions are computed as follows, where  X  is an indicator function, h is a threshold for the decision stump, and D is the number of dimensions of the observations: f 1 ( x i ) =  X  1  X  ( x i  X  h ) +  X  2  X  ( x i &lt; h ) , f 2 ( x i ) = Similar to LogitBoost and VEB, the sVEB algorithm estimates a combined feature function F that features). In other words, sVEB solves the following weighted least-square error (WLSE) problem to learn f t s: f t = arg min f [ For labeled data (first term in eq.7), boosting weights, w i  X  X , and working responses, z i  X  X , are com-puted as described in equation 4. But for the case of unlabeled data the expression for w i and z i becomes more complicated because of the entropy term. We present the equations for w i and z i below, please refer to the Appendix for the derivations: The soft evidence corresponding to messages from the neighboring nodes is obtained by running BP on the entire training dataset (labeled and unlabeled). The CRF feature weights  X  k s are computed by solving the WLSE problem (e.q.(7)),  X  k = Algorithm 1 gives the pseudo-code for sVEB. The main difference between VEB and sVEB are steps 7  X  10 , where we compute w i  X  X  and z i  X  X  for all possible values of y i based on the virtual evidence and observations of unlabeled training cases. The boosting weights and working responses are computed using equation (8). The weighted least-square error (WLSE) equation (eq. 7) in step 10 of sVEB is different from that of VEB and the solution results in slightly different CRF feature weights,  X   X  X . One of the major advantages of VEB and sVEB over ML and sML is that the parameter estimation is done by mainly performing feature counting. Unlike ML and sML, we do not need to use an optimizer to learn the model parameters which results in a huge reduction in the time required to train the CRF models. Please refer to the complexity analysis section for details. We conduct two sets of experiments to evaluate the performance of the sVEB method for training the first set of experiments, we analyze how much the complexity of the underlying CRF and the tuning parameter  X  effect the performance using synthetic data. In the second set of experiments, we evaluate the benefit of feature selection and using unlabeled data on two real-world activity datasets. We compare the performance of the semi-supervised virtual evidence boosting(sVEB) presented in this paper to the semi-supervised maximum likelihood (sML) method [10]. In addition, for the ac-tivity datasets, we also evaluate an alternative approach (sML+Boost), where a subset of features is selected in advance using boosting. To benchmark the performance of the semi-supervised tech-niques, we also evaluate three different supervised training approaches, namely maximum likelihood Algorithm 1 : Training CRFs using semi-supervised VEB inputs : structure of CRF and training data ( x i , y i ) , with y i  X  X  0 , 1 } , 1  X  i  X  M , and F 0 = 0 output : Learned F T and their corresponding weights,  X  for t = 1 , 2 ,  X  X  X  , T do end method using all observed features(ML), (ML+Boost) using a subset of features selected in advance, and virtual evidence boosting (VEB). All the learned models are tested using standard maximum a posteriori(MAP) estimate and belief propagation. We used a l 2 -norm shrinkage prior as a regularizer for the ML and sML methods. 4.1 Synthetic data The synthetic data is generated using a first-order Markov Chain with self-transition probabilities set to 0.9. For each model, we generate five sequences of length 4,000 and divide each trace into sequences of length 200. We randomly choose 50 % of them as the labeled and the other 50 % as un-labeled training data. We perform leave-one-out cross-validation and report the average accuracies. To measure how the complexity of the CRFs affects the performance of the different semi-supervised methods, we vary the number of local features and the number of states. First, we compare the per-formance of sVEB and sML on CRFs with increasing the number of features. The number of states is set to 10 and the number of observation features is varied from 20 to 400 observations. Figure (1a) shows the average accuracy for the two semi-supervised training methods and their confidence intervals. The experimental results demonstrate that sVEB outperforms sML as we increase the di-mension of observations (i.e. the number of local features). In the second experiment, we increase the number of classes and keep the dimension of observations fixed to 100. Figure (1b) demonstrates that sVEB again outperforms sML as we increase the number of states. Given the same amount of training data, sVEB is less likely to overfit because of the feature selection step. In both these ex-periments we set the value of tuning parameter,  X  , to 1.5. To explore the effect of tuning parameter  X  , we vary the value of  X  from 0.1 to 10 , while setting the number of states to 10 and the number of dimensions to 100. Figure (1c) shows that the performance of both sML and sVEB depends on the value of  X  but the accuracy decreases for large  X   X  X  similar to the sML results presented in [10].
Table 1: Accuracy  X  95% confidence interval of the supervised algorithms on activity datasets 1 and 2 4.2 Activity dataset We collected two activity datasets using wearable sensors, which include audio, acceleration, light, temperature, pressure, and humidity. The first dataset contains instances of 8 basic physical activities (e.g. walking, running, going up/down stairs, going up/down elevator, sitting, standing, and brushing teeth) from 7 different users. There is on average 30 minutes of data per user and a total of 3 . 5 hours of data that is manually labeled for training and testing purposes. The data is segmented into 0 . 25 s chunks resulting in a total of 49613 data points. For each chunk, we compute 651 features, which include signal energy in log and linear frequency bands, autocorrelation, different entropy measures, mean, variances etc . The features are chosen based on what is used in existing activity recognition literature and a few additional ones that we felt could be useful. During training, the data from each person is divided into sequences of length 200 and fed into linear chain CRFs as observations. The second dataset contains instances of 5 different indoor activities (e.g. computer usage, meal, meeting, watching TV and sleeping) from a single user. We recorded 15 hours of sensor traces over 12 days. As this set contains longer time-scale activities, the data is segmented into 1 minute chunks and 321 different features are computed, similar to the first dataset. There are a total of 907 data points. These features are fed into CRFs as observations, one linear chain CRF is created per day. We evaluate the performance of supervised and semi-supervised training algorithms on these two datasets. For the semi-supervised case, we randomly select 40 % of the sequences for a given person or a given day as labeled and a different subset as the unlabeled training data. We compare the performance of sML and sVEB as we incorporate more unlabeled data (20 % , 40 % and 60 % ) into the training process. We also compare the supervised techniques, ML, ML+Boost, and VEB, with increasing amount of labeled data. For all the experiments, the tuning parameter  X  is set to 1 . 5 . We perform leave-one-person-out cross-validation on dataset 1 and leave-one-day-out cross-validation on dataset 2 and report the average the accuracies. The number of features chosen (i. e. through the boosting iterations) is set to 50 for both datasets  X  including more features did not significantly improve the classification performance.
 For both datasets, incorporating more unlabeled data improves accuracy. The sML estimate of the CRF parameters performs the worst. Even with the shrinkage prior, the high dimensionality can still cause over-fitting and lower the accuracy. Whereas parameter estimation and feature selection via sVEB consistently results in the highest accuracy. The (sML+Boost) method performs better than sML but does not perform as well as when feature selection and parameter estimation is done within a unified framework as in sVEB. Table 2 summarize our results. The results of supervised learn-
Table 2: Accuracy  X  95% confidence interval of semi-supervised algorithms on activity datasets 1 and 2
Table 3: Accuracy  X  95% confidence interval of semi-supervised algorithms on activity datasets 1 and 2 ing algorithms are presented in Table 1. Similar to the semi-supervised results, the VEB method performs the best, the ML is the worst performer, and the accuracy numbers for the (ML+Boost) method is in between. The accuracy increases if we incorporate more labeled data during training. To evaluate sVEB when a small amount of labeled data is available, we performed another set of experiments on datasets 1 and 2, where only 5% and 20% of the training data is labeled respec-tively. We used all the available unlabeled data during training. The results are shown in table 3. These experiments clearly demonstrate that although adding more unlabeled data is not as helpful as incorporating more labeled data, the use of cheap unlabeled data along with feature selection can significantly boost the performance of the models. 4.3 Complexity Analysis The sVEB and VEB algorithm are significantly faster than ML and sML because they do not need to use optimizers such as quasi-newton methods to learn the weight parameters. For each training which reduces the cost of sML to O (( c l + c u ) ns 2 ) but still requires an optimizer to maximize the log-likelihood. Moreover, the number of training iterations needed is usually much higher than the number of boosting iterations because optimizers such as L-BFGS require many more iterations to reach convergence in high dimensional spaces. For example, for dataset 1, we needed about 1000 iterations for sML to converge but we ran sVEB for only 50 iterations. Table 4 shows the time for other hand the space complexity of sVEB is linearly smaller than sML and ML. Similar to ML, sML has the space complexity of O ( ns 2 D ) in the best case [10]. VEB and sVEB have a lower space cost of O ( ns 2 D b ) , because of the feature selection step D b  X  D usually. Therefore, the difference becomes significant when we are dealing with high dimensional data, particularly if they include a large number of redundant features.
 Dataset 1 34 18 2.5 96 48 4
Dataset 2 7.5 4.25 0.4 10.5 8 0.6 We presented sVEB, a new semi-supervised training method for CRFs, that can simultaneously select discriminative features via modified LogitBoost and utilize unlabeled data via minimum-entropy regularization. Our experimental results demonstrate the sVEB significantly outperforms other training techniques in real-world activity recognition problems. The unified framework for feature selection and semi-supervised training presented in this paper reduces the computational and human labeling costs, which are often the major bottlenecks in building large classification systems. Acknowledgments
