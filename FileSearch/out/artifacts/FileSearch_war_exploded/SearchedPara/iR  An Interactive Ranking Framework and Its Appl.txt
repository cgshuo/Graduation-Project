 We address the problem of unsuperv ised ensemble ranking in this paper. Traditional approaches either combine multiple ranking criteria into a unified representation to obtain an overall ranking score or to utilize certain rank fusion or aggregation techniques to combine the ranking results. Beyond the aforementioned  X  combine-then-rank  X  and  X  rank-then-combine  X  approaches, we propose a novel  X  rank-learn-combine  X  ranking framework, called Interactive Ranking (iRANK), which allows two base rankers to  X  X each X  each other before combination during the ranking process by providing their own ranking results as feedback to the others so as to boost the ranking performance. This mutual ranking refinement process continues un til the two base rankers cannot learn from each other any more. The overall performance is improved by the enhancement of the base rankers through the mutual learning mechanism. We apply this framework to the sentence ranking problem in que ry-focused summarization and evaluate its effectiveness on the DUC 2005 data set. The results are encouraging with consistent and promising improvements. H.3.3 [ Information Search and Retrieval ]: Retrieval Models; I.7.5 [Document and Text Processing] : Document Capture  X  document analysis. Algorithms, Experimentation. Unsupervised ensemble ranking, rank-learn-combine, interactive ranking, sentence ranking, que ry-focused summarization. Ranking plays an important role in information retrieval and natural language processing appli cations. Many factors (a.k.a. features) have been taken into account when designing the ranking functions (or to say the rankers), which results in the demand for a mechanism to integrate the feat ures. There are two alternative integration approaches in the litera ture. One is to first combine the text segments. The other is to utilize the rank fusion or rank aggregation techniques to combin e the ranking results (scores, ranks or orders) produced by the multiple ranking functions into a unified rank. The second approaches are also known as ensemble ranking, the most popular implemen tation of which is to linearly combine the ranking features to obtain an overall score which is then used as the ranking criterion. The weights of the features are either experimentally tuned or automatically derived by applying certain learning-based mechanisms. However, both of the above-approaches have a common drawb ack. They do not make full use of the information provided by th e different ranking functions and neglect the interactions among th em before combination. We believe that each individual ranking function (we call it a base ranker) is able to provide valuable information to the other base rankers such that they can learn from each other by means of mutual ranking refinement, which in turn may result in overall improvement in ranking. To the best of our knowledge, this is a research area that has not been well addressed in the past. The inspiration for the work presented in this paper comes from the idea of co-training [1], which is a very successful paradigm in the semi-supervised learning frame work for classification. In essence, co-training employs two weak classifiers that help augment each other to boost the performance of learning algorithms. We can shed the light of the spirit of co-training in the context of ranking. Although each base ranker cannot decide the overall ranking well by itself, its ra nking results indeed reflect its opinion towards the ranking from its point of view. The two base rankers can then share their own opinions by providing the feedback from other rankers cont ains additional information to guide the refinement of its own ranking results if feedback is defined and used appropriately. Th is process continues iteratively until the two base rankers can not learn from each other any more. We call this kind of ranking paradigm interactive ranking (iRANK). iRANK is applicable to many appli cations. In this paper, we are particularly interested in the ta sk of query-focused summarization, in which sentence ranking is the issue of most concern upon the extractive summarization framework. Up to now, the feature-based ranking approaches, whic h rank sentences based on the features elaborately designed to characterize sentences, have been among the most effective and popular approaches. As different features may reflect different aspects of the sentences, we therefore expect a framework to co mbine them together in order to produce significant overall ranking results. For this purpose, we design a new sentence ranking algorithm in which a query-dependent ranker and a query-inde pendent ranker mutually learn from each other upon the iRANK framework. Let {} n x x x X , , , 2 1 L = be the instances to be ranked. f base ranker, () X x x f i  X   X   X   X  , . Our goal is then to jointly improve multiple base rankers through the interactions among them, which can consequently re sult in the overall improvement in ranking reliability and accuracy. We develop an interactive ra nking (iRANK) framework. For the sake of explanation, let X  X  consid er two base rankers here. Given a set of instances X , one can define the two base ranker f some intended purposes. The ranking results produced by f individually are by no means perfect. However, either f 1 provide relatively reasonable ranking information to  X  X each X  each other so as to jointly improve them. In such a collaborative teach-and-learn mode, the ranking becomes an interactive process. One way to do the interactive ranking is to take the most confident ranking results (e.g. highly ranke d instances based on scores, ranks or orders) from one base ra nker as the feedback to update the other X  X  ranking results, and vice versa. This process continues iteratively until the termination condition is reached. The framework of interactive ranking is depicted in Procedure 1 below. 
Procedure 1 . iRANK ( f 1 , f 2 , X ,  X  ) 3: Normalize  X  1 r , () 6: Repeat 9: Until I( X ). 11: Return r . Notice that the interactive ranking process can be clearly divided into three main steps. 1. Rank : Run the two base rankers f 1 and f 2 and obtain the initial 2. Learn : The two base rankers refi ne their ranking results by 3. Combine : When the two base rankers can not learn from each What distinguishes iRANK from the traditional rank-then-combine approaches is that it involves a learning process in an unsupervised manner before the co mbination. While the feedback can be defined in different wa ys depending on the nature of the application, such as the ranking sc ores, the ranks, th e instances, or the combination of the aforementioned information. In addition, the ranking refinement  X  can be defined variously in different context and the termination condition I( X ) can be defined according to the different application scenarios. Recall that our ultimate goal is to generate the unified (thus consistent) ranking results from th e two base rankers. We build the following consistency driven ranking refinement strategy motivated by the following observation: 
Similar instances have similar ranks, and the refined ranking results should subject, as consiste ntly as possible, to its own initial ranking results and the rank of the similar instances in the feedback it receives.
 We model the above mentioned intuitions as follows. At the round k +1, for the two base rankers, we formally formulate two cost similar to the one propos ed in [6] as follows,  X  and 2  X  at the round k , respectively. The optimized ranking refinement strategy is the one that minimizes the regularization functions given in Equation (1) and (2). Let us take the objective function in Equations (1) as example. The first term on its right-hand si de is the  X  X moothness constraint X  on the ranks in the feedback 2  X  , which means that the refined ranking should be consistent to the corresponding ranks of the similar instances in 2  X  . The second term is the  X  X itting constraint X  to its initial ranking, which mean s that the refined ranking should not change too much from the initial ranking.  X  is a positive number used as the trade-off between the two competing constraints. Differentiating ( ) r sentences and  X  is a diagonal matrix with  X  =  X  j ij ii Let refined ranking results consist of two parts. The second term ranking results of f 1 , while the first term () from the ranking results of the highly ranked instances in f . The first tem can be interpreted in this way. In the ranking process of f 1 , an instance which is relevant to a high rank instance evaluated by f 2 earns a bonus score which steps up its initial rank. This can be viewed as the process of f 1 learning consistent ranking from f 2 . iRANK terminates when the top K instances in r interested in the top ranked instances. It is also very likely that r In this case, the two base rankers can not learn from each other any more and iRANK should be terminated either. As for iRANK, we can easily prove its termination. Take f does not change any more after se veral rounds of iterations, which indicate the two base ranker can not learn from each other any more, and thus should be terminated. It is the same for r up, iRANK is guaranteed to term inate through the aforementioned termination conditions. The task of query-focused summarization is to produce a short summary for a set of related documents D with respect to a query q which reflects the users X  information need. The query usually consists of one or more interr ogative and/or narrative sentences. summarization framework, where the most critical processes involved are sentence ranking and sentence selection. To design the sentence ranking algorithm based on the proposed iRANK framework, it is most important to design the two base rankers and to define the details of ranking refinement. In the context of query-focused summarizati on, two kinds of features, i.e. query-dependent and query-indepe ndent features, are necessary and they are supposed to comple ment each other. We then use these two kinds of features to develop the two base rankers. The query-dependent feature (i.e. the relevance of the sentence s to the query q ) is defined as the cosine similarity between s and q , Here the words in the sentences and query are weighted by tf where tf is the word frequency in the sentence or query, and w sf N isf S log = is the inverse sentence frequency (ISF) of w , where w sf denotes the sentence frequency of w , and D the total number of sentences in the document set D . As for the query-independent featur e, we consider the LexRank [3] sentence s i , the ranker can be iteratively computed as follow. where d is the damping factor and similarity between the two sentences s i and s j . As for iRANK, we need to compute the similarity matrix. It should be emphasized that the computation can be carried out offline in order to make iRANK efficient. We summarize the corresponding sentence ranking algorithms in Algorithm 1. Algorithm 1 . iRANK ( f 1 , f 2 , D, q ) 1: Extract sentences S ={ s 1 , ... s m } from D ; 2: Calculate the similarity matrix, 4: Define the ranking refinement strategy, 5: Return iRANK ( f 1 , f 2 , S ,  X  ). Because we focus on the sentence ranking in this paper, we develop a simple yet effective sentence selection strategy as follows. We incrementally add into the summary the highest ranked sentence if it doesn X  X  significantly repeat 1 already included in the summary until the word limitation of the summary is reached. We take the DUC 2005 data set as the evaluation corpus. The system generated summaries are limited to 250 words in length. The stop-words in both documents and queries are removed, and the remaining words are stemmed by Porter Stemmer (http://tartarus.org/~martin/PorterS temmer/). ROUGE [4] is used as the evaluation metrics in the following experiments. For the purpose of comparison, we implement the following two base rankers and the linear combination of them for reference. Let QRR denotes the query relevance based ranker (i.e. f denotes the LexRank based ranker (i.e. f 2 ), and LCR denotes the Linear combined ranker, which linearly combines QRR and LRR with  X  as the combination parameter. Note that, in LCR, QRR and LRR are normalized as the same in Step 3 in Procedure 1 before combining them. As for the termination condition, we set K A sentence is discarded if the cosine similarity of it to any sentence already selected into the summary is greater than 0.9. in Section 2.3 to 10 because ten sentences are usually sufficient enough in the DUC query-focused summarization task. The aim of the first set of experi ments is to compare the proposed  X  X ank-learn-combine X  approach es (i.e. iRANK) with the traditional  X  X ank-then-combine X  approach (i.e. LCR) on the DUC 2005 data set. The damping factor d in LRR is set to 0.75. To avoid the  X  X ink-by-chance X  problem (i.e. the two sentences are linked together only because they share a word or two by chance), we set the values in the affinity matrix W to 0 if they are below a threshold 0.03. These parameters are tuned in our experiments. The settings of the other parameters are: the combination factor  X  =0.4 (according to the experimental results on LCR) in LCR and iRANK; the balance factor  X  =0.7 in iRANK. We report the results on different  X  in Section 4.2. Table 1 shows the results of average recalls of ROUGE-1, ROUGE-2 and ROUGE-SU4 along with their 95% confidence intervals incl uded within the square brackets. Among them, ROUGE-2 is the primary DUC evaluation criterion. iRANK 0.3880 It clearly indicates that iRANK is superior to LCR. This is because both QRR and LRR are enhanced during ranking refinement, which in turn results in the increased overall performance. We then further examine the balance parameter settings in ranking refinement. Table 2 shows the results of iRANK with  X  ranging from 0.5 to 0.9. Notice that here  X  is not the combination factor as in LCR. We believe that a base ranker should have at least half balance factor  X  should be greater than 0.5. The combination factor  X  is also set to 0.4. As shown in Table 2, iRANK produces relatively stable and promising results regardless of the change of  X  . We then compare our results to the DUC participating systems. We present the following represen tative ROUGE results of (1) the top three DUC participating systems according to ROUGE-2 scores (S15, S17 and S10); and (2) the NIST baseline which simply selects the first sentences from the documents. The advantage of iRANK is clearly shown in Table 3. Table 3 Compare with participating systems in DUC 2005 
NIST Baseline 0.0403 0.0872 In this paper, we propose a novel unsupervised ensemble ranking framework called Interactive Ranking (iRANK). We also design and investigate the ranking refinement strategy to use the feedback to support mutual learning between two base rankers so as to jointly improve the final overall ranking results. As a case study, we examine the proposed iRANK framework in the context of query-focused summarization. Encouraging results are achieved. The work described in this paper was in part supported by a grant from HK RGC (PolyU5217/07E), the Hong Kong Polytechnic University internal the grants (G-YG80 and G-YH53) and a China NSF grant (60703008). [1] A. Blum and T. Mitchell. 1998. Combining Labeled and [2] C. Dwork, R. Kumar, M. Na or and D. Sivakumar. 2001. [3] G. Erkan and D. R. Radev. 2004. LexRank: Graph-based [4] C. Y. Lin and E. Hovy. 2003. Automatic Evaluation of [5] J. Pickens and G. Colovc hinsky. 2008. Ranked Feature [6] D. Y. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. 
