 YONG GE, HUI XIONG, and WENJUN ZHOU , Rutgers University Customer service support is becoming an integral part of most companies. Many com-panies have a customer service department that provides inspection, installation, and maintenance support for their world-wide customers. Problem ticket generation usu-ally is the first step in today X  X  process services management. This step is responsible for describing problem symptoms reported by customers and problem tickets are the link between customers and the services infrastructure. Once a problem ticket is generated, it will be enqueued in the ticketing system and routed to an appropriate service center or service people for problem determination. Most existing large call centers collect service data that are then used to assess and improve the performances of their rep-resentatives. Typically, these problem records are stored in relational databases with both structured (e.g., current status, problem type, support person/group handling the problem, type of system, and component related to the problem) as well as unstruc-tured (e.g., free-format text descriptions of problems and solutions as entered by the support personnel) attributes.

Indeed, an increasing availability of problem logs creates unprecedented opportu-nities to change the paradigm for risk management for avoiding/alleviating organi-zational crisis, product design, and product quality control. Currently, these problem logs are mostly used for tracking, auditing, and reporting the problem management processes [Gans et al. 2003; Brown et al. 2005; Chu-Carroll and Carpenter 1999]. Most steps in these processes (e.g., problem diagnosis, ticket routing, etc.) are still manually taken. However, a lot of subject knowledge and experiences that these manual steps rely on are embedded in the existing problem records (i.e., historical data) [Riccardi et al. 1997; IBM 2011]. It is expected that the ability will develop to automatically extract expert experiences as the knowledge to improve process services management and identify early symptoms of defect products.

To this end, in this study, we aim to exploit large-scale problem logs for predict-ing problem category/determination automatically. This is a main aspect of customer service profiling [Johansson and Olhager 2003]. If problems can be automatically determined, the problems can be routed to the right support group/personnel more efficiently. In turn, this can offer an organization tremendous benefit by, for instance, identifying sources of problem resolution error, delay, and optimization of the problem management processes. Also, this can help to reduce the expense caused by manual processing. Furthermore, it is possible to facilitate customer service profiling if we have the ability to automatically categorize problems.

While it is appealing to automate the process of customer problem categoriza-tion/determination, it is very challenging to make use of the information and expert knowledge encoded in problem logs. For problems reported by customers, the best in-formation we can use is the problem description provided by customers. If we treat the problem descriptions as training samples and the problem result categories (de-termined by experts) as labels, it appears that existing off-the-shelf classification algo-rithms [Tan et al. 2005] can be used for problem categorization. However, after carefully examining real-world problem logs, we have observed some unique characteristics in-herent in problem logs. A key issue is that all the problem descriptions for the same problem are provided by customers with diverse backgrounds and these problem de-scriptions can be quite different. Experienced customers usually give more precise and focused descriptions about the problem. In contrast, inexperienced customers usually provide diverse descriptions for the same problem. As an example, for a simple pass-word problem, the description from experienced customers can be as simple as  X  X eed to reset password. X  However, inexperienced customers provide much more diverse de-scriptions, such as  X  X annot connect to the network X  and  X  X y computer does not work. X  In other words, the training samples for the same class can be naturally in different focal groups. If we treat these problem descriptions as the same and fit into existing learning models, we may not be able to have desirable classification performance. In our preliminary work [Ge et al. 2009], we formalized a multifocal learning problem, where training data are partitioned into several different focal groups and the predic-tion model will be learned within each focal group. A key development challenge along this line is how to identify those focal groups in the training data. As mentioned before, for the same problem, the problem descriptions from experienced customers are very precise and focused. Accordingly, the descriptions of problem solutions are also very pre-cise and focused and highly correlated with the problem descriptions by experienced customers (if descriptions have been turned into vectors of words). In contrast, the problem descriptions for the same problem from inexperienced customers are weakly correlated with the descriptions of their corresponding problem solutions. Viewed in this light, we proposed a correlation method (Correlation) to partition problem descrip-tions within each class into two different groups: one for experienced customers and another one for inexperienced customer. In addition, to better capture the information encoded in problem logs, we also developed an ontology-enhanced correlation method (Ontology) for identifying different focal groups. After the learning models have been constructed for different focal groups, the new samples will be assigned to a learning model based on a nearest-neighbor method; that is, a new sample will be assigned to a learning model if this sample is closer to the centroid of the train samples for this learning model than that of any other model.

To evaluate the performance of multifocal learning, we also present a theoretical risk analysis of multifocal learning with the na  X   X ve Bayes classifier and reveal that the risk of multifocal learning with the na  X   X ve Bayes classifier is smaller than the risk of the single na  X   X ve Bayes learning model. Moreover, we exploit the multifocal learning model for categorizing customer problems using real-world problem logs collected in customer service centers. The experimental results show that multifocal learning can significantly boost the learning accuracies of existing learning algorithms, such as SVMs and RIPPER. Furthermore, we show that both Correlation and Ontology can lead to a better learning performance than other focal group formation methods, such as the methods based on clustering and random partition. However, Ontology results in slightly better learning performance than Correlation.

It is clear that multifocal learning can lead to better learning performance on cus-tomer problems by formalizing different focal groups and building learning models within groups. In this article, we further investigate the impact of the quality of focal groups on the performance of multifocal learning. Specifically, the focal groups can first be obtained by different focal group formation methods, such as Correlation, Ontology, and clustering-based approaches. Then, the quality of the formalized focal groups is evaluated by external clustering validation measures [Wu et al. 2009]. Finally, we ex-amine the correlation between the quality of focal groups as measured by clustering external validation measures and the performance of multifocal learning. Indeed, em-pirical results show that there is strong positive correlation between the quality of the focal groups and the performance of multifocal learning.

In addition, based on the observed correlation between the quality of focal groups and learning performance, we study more robust clustering methods to form better focal groups. To that end, we employ the shared nearest-neighbor similarity and noise removing techniques to improve the quality of focal groups (clusters). In this experi-ment, we observe that there is correlation between the level of noise and the learning performance of multifocal learning. In general, the related work can be grouped into two major categories: data mining for customer service support and problem log analysis.

In the first category, data mining techniques have been applied for customer service support in various ways. For example, Hui and Jha [2001] applied the DBMiner data mining tool for mining structured data in the customer service database to find pat-terns which can help to enhance the decision making process for better marketing and better management of resources. Also, they exploited neural network techniques for supporting machine fault diagnosis. In addition, data mining techniques have also been applied to analyzing customer behaviors with the goal of improving customer satisfac-tion [Godbole and Roy 2008]. In particular, those authors combined text classification techniques and a business intelligence solution along with an interactive document labeling interface for automating customer satisfaction analysis. Finally, Paprzychi et al. [2004] applied neural networks, regression trees, and Support Vector Machines (SVMs) for predicting the quality of services in call centers. The goal is to understand the relationship between the performance evaluation process and actual performance, and thus improve the performance of call centers.

In the second category, people mainly conducted the analysis of problem logs which are collected in customer service centers. In the field of customer problem analy-sis, categorization (classification) is often used [Busemann et al. 2000]. For instance, [Busemann et al. 2000] used classification methods to categorize the customers X  re-quest. Specifically, several classification methods, such as lazy learning, SVMs, and symbolic eager learning, have been used to classify email requests based on shallow text processing. Also, in Gamon et al. [2005], a bootstrapping approach was used to classify customer opinions. In addition, Gamon [2004] used SVMs to classify customer feedback data as positive or negative. These authors mainly developed algorithms to address the challenges caused by noise and large-scale features.

However, all the methods just mentioned are not aware of the multifocal nature in customer problem logs. Instead, we develop a multifocal learning method for customer problem analysis in this article. The rest of this article is organized as follows. Section 2 provides the details of cus-tomer problem logs. In Section 3, we introduce the problem of multifocal learning and the methods for finding focal groups. Section 4 presents the case study of multifocal learning for customer problem analysis. Finally, in Section 5, we draw conclusions and suggest future work. We aim to develop a problem categorization model for facilitating customer service analysis. To achieve this, we build a prediction model based on the problem logs col-lected from customer service centers. While the problem logs used here are only related to IT enterprise products and service functions, the developed prediction methodology and the prediction framework can be easily extended to deal with problem logs from broader business sectors.
 Problem logs used in this article were collected from IBM customer service centers. These problem logs are mainly IBM customer service scripts describing the customer problems and these scripts were recorded during problem incidences. While these problem logs record issues dominated by IBM-related products and services, there are some problem logs which report problems from other vendors as well. All problem logs are stored in relational databases with both structured (e.g., problem ID, cause code, person/group handling the problem, type of system, and component related to the problem) as well as unstructured (e.g., free-text descriptions of problem and problem solution descriptions entered by the support personnel) attributes. Table 1 shows a sample problem log. In the table, we can observe some structured attributes as well as two unstructured attributes of this sample problem log. As can be seen, problem descriptions are about the descriptions of problems and were supplied by customers with diverse backgrounds. In contrast, the descriptions about the problem solution were provided by IBM service people. Both problem descriptions and problem solutions are in the free-text format.

In this study, we focus on the unstructured attributes, since the free-text descrip-tions of problems supplied by customers are original and essential for the solutions of problems. Compared with structured data, unstructured free-text descriptions offer more original information that will reveal  X  X hat X  and  X  X hy X  aspects of the problems that a customer might have encountered. However, the analysis of unstructured text is extremely difficult because of the following reasons. First of all, any unstructured text is usually very noisy with many irrelevant text contents [Godbole and Roy 2008]. Second, the problem descriptions usually reflect the problem perceptions of customers and are given in customers X  own words. In other words, the same problem can be de-scribed in many different ways since different customers have very different levels of domain knowledge.

Finally, before we can apply learning models on problem logs for predicting problem categorization, we need to first do data preprocessing, which includes data cleaning and data transformation. For data transformation, we transform unstructured problem descriptions and problem solutions into structured vectors. This involves extracting keywords from text paragraphs and then representing each problem log as a vector with 0/1 values. This data preprocessing is similar to the data preprocessing in text categorization. Note that we have used the Porter word stemming algorithm [Porter 1980] to reduce words to their base stem. In this section, we first introduce the concept of multifocal learning. Then, we explain how the multifocal learning model works by providing an illustrating example. In addition, we propose three methods for forming focal groups in problem logs. Finally, we provide a theoretic analysis to show the effectiveness of multifocal learning. The idea of multifocal learning is motivated by the observation that there are inherent large variations in many real-world training data. For instance, problem descriptions in problem logs for the same problem can be quite different, since these descriptions may be from customers with different backgrounds. Some customers may be experienced people and they can provide more precise descriptions about the problems. In contrast, some inexperienced customers may provide diverse descriptions on the same problem. As a result, the learning performance can be significantly impacted by the diversity inherent in the training data. This is referred as the multifocal property in this article.
To deal with this multifocal property, we provide a multifocal learning framework as shown in Figure 1. As can be seen, there are three phases for multifocal learning. In the first and most challenging phase, there is a need to identify different focal groups in a way such that training samples in the same focal group are more similar to each other. This focal group formation process is challenging because there is no simple and effective way to identify focal groups in different types of real-world data. The best focal group formation method usually relies on special characteristics inherent in the data. Here, we provide two methods for finding focal groups in problem logs in the following subsections. The second phase is focused on building learning models in each focal group. Any traditional learning models, such as Support Vector Machines (SVMs), Bayesian learning, and decision trees, can be applied here. Finally, in the third phase, each testing sample will be first assigned to a focal group using a nearest-neighbor method. Then, the learning model from the corresponding focal group will be used for the prediction of this testing sample. Here, we exploit SVMs on a synthetic dataset to illustrate multifocal learning. Specif-ically, we generate two-dimensional synthetic data with two classes as shown in Figure 2(a). In the figure, the objects of two classes are represented by triangle and square symbols, respectively, and these objects are naturally located in two different focal groups: one dense group and one sparse group. Note that the SVMs tool we used here is LIBSVM [Chang and Lin 2011] with the linear kernel.

First of all, we generate the learning model by directly applying SVMs for the whole original data and the results are shown in Figure 2(b). In this figure, the solid line represents the Maximal Margin Hyperplane (MMH) learned by SVMs. As can be seen, there are many classification errors. This is due to the multifocal property of the original data. Indeed, even if we use nonlinear kernels (i.e., the radial basis function), the classification accuracy is still about 60%.

Instead, we exploit multifocal learning methods. Along this line, we first partition the data into two groups. One is the dense group and another one is the sparse group as shown in Figure 2(c). In this figure, we can see that samples from two classes coexist in these two focal groups. Next, we apply SVMs to build learning models in each group and the results are shown in Figure 2(d). As can be seen, there is one MMH in each group and there are few classification errors compared to Figure 2(b). Finally, to predict a test sample in the multifocal learning framework, we first assign the test sample to a focal group based on nearest-neighbor methods. Then, the learning model from the corresponding focal group will be used for the prediction of this test sample. The preceding example illustrates the multifocal learning process and the reasons why it can lead to better performance for data with the multifocal property. However, in practice, a key development challenge is how to effectively identify the focal groups from the data. As a practice, in this subsection, we propose the Correlation method to generate focal groups in problem logs. This method is motivated by our observation that problem logs have been provided by customers with diverse backgrounds. Experienced customers usually give more precise and focused descriptions about problems. These problem de-scriptions are highly correlated with the problem solutions provided by service people. In contrast, inexperienced customers usually give diverse descriptions for the same problem and their descriptions usually have low correlation with the final problem solutions.

To measure the correlation between problem descriptions and problem solutions, we need to first transfer problem descriptions and problem solutions into vectors with 0/1 values. Specially, we first extract keywords from the text descriptions of problems and problem solutions and build a word vector to include all the keywords. Then, a problem description or a problem solution can be transferred into a vector with 0/1 values based on whether the corresponding word in the word vector is in the problem description/solution or not. In this way, we transfer all the problem descriptions and problem solutions into binary vectors. To measure the strength of the relationships among these vectors, we use the Jaccard coefficient [Tan et al. 2005] as follows. Here f ij is the number of attributes where X is i and Y is j (i = 0or1;j = 0or1). After we compute the semantic correlation between problem descriptions and problem solutions, we partition problem logs into two focal groups: one with high correlation values and another one with low correlation values. Note that we empirically choose the correlation thresholds for partitioning. After carefully examining the problem logs, we have noticed that the keywords from problem descriptions/solutions can be naturally organized into a hierarchy of concepts. In the preceding Correlation method, all the keywords are treated equally in the cor-relation computation. However, the significance of keywords from different concept levels should be different. Thus, in this subsection, we propose an ontology-enhanced correlation method (Ontology) for focal group formation in problem logs.

Indeed, ontology is an effective tool to represent hierarchical concepts within a do-main [Cristani and Cuel 2005] and it has been used for text classification [Bloehdorn 2004]. In this study, we exploit ontology to improve focal group formation. Specifically, we construct an ontology with extracted concepts from problem logs. Figure 3 shows a sample ontology. In this ontology, the keywords at top levels are very general and the keywords at bottom levels are very specific. Both experienced and inexperienced customers are likely to use the keywords from the top levels. However, the key words from the bottom levels (e.g., 3945ABG and Ticket#32639225) are usually the words hard-coded into the software system as error messages. Inexperienced customers are more likely to copy these keywords when they report problems. In addition, there are some keywords which are synonyms, such as monitor and LCD. Different customers may use different words for the same meaning. To capture these synonyms, we produce synonym bags which include all synonyms for the same concept. These synonym bags are integrated in the ontology.

With the help of the ontology, we know there is still room to better capture expert knowledge and improve focal group formation. Specifically, in Ontology, we first gener-ate the ontology over all the keywords. This process includes the generation of synonym bags. Then, we exploit a weighting scheme to assign small weights on the words at the top and bottom levels and large weights on words at the middle levels. Since words at the middle levels are more likely used by experienced customers and the words at the top and bottom levels are more likely used by inexperienced customers, this weighting scheme can help to turn binary vectors into weighted vectors. In this case, we em-ploy the cosine similarity to compute the correlations instead of the Jaccard measure, since the Jaccard measure can only handle binary vectors. Let X and Y be binary vectors of problem descriptions and solutions. Also, let W represent the weight vector obtained from Ontology. Then the Ontology-enhanced correlation can be computed as defined as X  X  W = [ x 1 w 1 ,..., x d w d ], where X , Y and W are both d -dimension vectors.
Note that the Ontology-enhanced correlation computing can increase the correla-tions between problem descriptions and problem solutions for experienced customers and decrease the correlations between problem descriptions and problem solutions for inexperienced customers, thus better capturing two focal groups. In this subsection, we investigate how to exploit clustering techniques for forming focal groups in customer problem logs. A key focus is on the data characteristics which might have an impact on the quality of focal groups (clusters).

Indeed, we have observed that many clustering algorithms become unreliable for high-dimensional data [Ertoz et al. 2003]. In the high-dimensional space, data tends to be sparse and the traditional distance notion between data points becomes not very meaningful. An alternative way to measure similarity is to define the similar-ity/distance between a pair of points in terms of their Shared Nearest Neighbors (SNN) [Jarvis and Patrick 1973]. This alternative similarity measurement has been il-lustrated to be more effective than direct similarity measurement for high-dimensional data [Ertoz et al. 2003]. Since customer problem logs used in this article are high-dimensional data in terms of a huge number of keywords, we choose to use SNN as the similarity measure for clustering customer problems.

Moreover, in the real-world data, some problem descriptions provided by some cus-tomers are noisy and fuzzy. Sometimes, it is not clear to decide weather a problem belongs to which focal group. This kind of noisy data points can jeopardize the learning model if we force to assign it into any one focal group. Instead, if we remove this kind of noisy data points, we can get more consistent and representative samples to build learning models within focal groups. According to this understanding, we exploit a clustering method based on the Shared Nearest-Neighbor (SNN) similarity, which has the ability in removing noisy data points during the clustering process. Figure 4 shows the pseudocode of the designed clustering algorithm, denoted as CFocal for simplicity. Please note that the noise-removing threshold ( Thre ) and the number of neighbors ( K ) can be specified by users. Here, we present a theoretical risk analysis of multifocal learning. For illustration purposes, we use the na  X   X ve Bayes classifier as the base learning model.
First, given the input variable x and output variable y , the Expected Prediction Error (EPE) [Scholkopf and Smola 2002] is where L ( y , f ( x )) is the loss function, such as squared loss or 0-1 loss. The goal of classi-fication is to find f ( x ) so that EPE is minimized. In Bayesian classification terminology [Goel and Byrne 2000], EPE is also called risk which is actually more widely used than EPE. Assume that we have prior probability for class H i as p i = p ( H i ) , i = 1 , 2 ,..., M . We can assign a loss function ( L ij ) to each possible decision outcome; we also know the conditional/likelihood probability as p ( x | H i ). Then, the total risk in the Bayesian terminology can be rewritten as where R j is the classification boundary. Since R j partitions the entire space, any x belongs to exactly one such R j . In fact, the Bayesian classifier decides class j by maximizing the posterior probability; that is, j : = ar gmax j p ( H j | x ). Then, the total risk can be simplified as If we choose 0-1 criterion for loss function, Eq. (4) can be rewritten as Since p ( x ) is the same value for all x , the total risk is actually the product of one constant term c and the probability of error. Next, we introduce the following theorem. smaller than the risk of the single na  X   X ve Bayes learning model.
 and Ri sk be the risk of the single na  X   X ve Bayes learning. For a new object x , we assume that Class j is predicted by the multifocal learning model and Class j is predicted by constant number. We need to prove Ri sk ( Ri sk = Ri sk  X  Ri sk ) &lt; 0.
 Apparently, both p ( H j | x )and p ( H j | x ) are maximums as the posterior probability with na  X   X ve Bayes classifier, each attribute x k , k = 1 , 2 ,..., D , of x is assumed independent. each class. So we can write Ri sk T as where c is also a constant number. Similar to the text classification problem, we use frequency ratio to estimate p ( x k | H j )or p ( x k | H j ). We have where # ofx k means the times of x k attribute.

Then, if x k = 0, then x k only appears in one focal group (this can be enforced by focal group formation methods), thus (# of x k | gi v enH j ) in Eq. (8) and (# of x k | gi v enH j ) in Eq. (9) should be equal. However, (# of training samples | gi v enH j ) in Eq. (9) is smaller than (# of trainingsamples | gi v eH j ) in Eq. (8) for the whole train data. So p ( x k | H j )is bigger than p ( x k | H j )for x k = 0. Also, if x k = 0, the corresponding attribute tends to almost uniformly distributed , thus it almost has no impact on the overall computation of posterior probability [Tan et al. 2005]; therefore, it has no impact on the value of thus Ri sk T &lt; 0. So we can conclude Ri sk &lt; 0. Thus, this theorem is held. In this section, we provide an empirical study of the performance of multifocal learning for customer problem analysis. Real-World Problem Logs. In the experiments, we have used real-world problem logs collected from IBM customer service centers. A detailed description of problem logs has been given in Section 2. In the experiments, we have formalized two-class classi-fication as well as multiclass classification problems. For the two-class classification problem, we are focused on two categories of customer problems. One category includes the problems caused by the users, such as  X  X eset/forget password. X  Another category includes the problems related to product quality, such as  X  X attery dead X  and  X  X ystem crash. X  In contrast, to study the multiclass learning problem, we prepare three cate-gories of problems including user-causing problems, hardware problems, and software problems. All the problems were labeled by domain experts. Finally, to evaluate the fo-cal group formation methods, problem descriptions were also labeled as  X  X xperienced X  or  X  X nexperienced X  by domain experts. These labels are used as benchmark in the experiments.

Synthetic Data. Figure 5 shows four synthetic datasets including DS1, DS2, DS3, and DS4, which have the multifocal property. Some data characteristics are shown in Table II.

Experimental Tools. We employ four base classifiers, Support Vector Machines (SVMs), Bayesian classifier, decision tree, and rule-based classifiers (RIPPER [Cohen 1995]). Also, we use MFL to indicate multifocal learning.

To evaluate focal group formation methods, we compare Correlation and Ontology with clustering and random partitioning methods. For clustering methods, we employ Chameleon [Karypis et al. 1999] in [Karypis 2011] and the proposed clustering method CFocal. Problem descriptions labeled by domain experts are used as the benchmark.
Evaluation Metrics. The classification accuracy and F-measure [Tan et al. 2005] have been used for the performance evaluation. For all experiments, we did five cross-validation. In this subsection, we show the performance of multifocal learning on real-world prob-lem logs.

Two-Class Categorization. The goal is to group problems into two categories: user-causing problems and product problems. Here, we use Correlation for generating focal groups. Specifically, we measure the correlations between problem descriptions and problem solutions. If the correlation is lower than 0.07, the corresponding problem log is placed in a sparse (inexperienced) group. If the correlation is larger than 0.1, we put the problem log in a dense (experienced) group. The thresholds are empirically specified. Once we have two focal groups, we build learning models with some base classifiers on both focal groups.

Table III shows the comparison results of multifocal learning and traditional classi-fication methods including SVMs, Ripper, C4.5, and Bayes. In the table, we can observe that multifocal learning can improve the performance of these traditional classification methods with a significant margin.

Three-Class Categorization. Here, we target on grouping problem logs into three categories: user-causing problems, hardware problems, and software problems. In this experiment, we apply both Correlation and Ontology methods for generating focal groups. Table IV shows the comparison results of multifocal learning and traditional classification methods including SVMs and Bayes. As can be seen, multifocal learning can improve the classification performance of both SVMs and Bayes. Also, we can observe that Ontology leads to slightly better classification performance compared to Correlation. In this subsection, we evaluate the performance of several focal group formation meth-ods including Correlation, Ontology, clustering methods, and random partitioning on real-world problem logs. Here, we use SVMs as the base classifier and target the two-class categorization problem, which has a similar experimental setting as the two-class categorization problem in Section 4.2.

For Ontology, we stratify the domain concepts into four levels. The weights on con-cepts on the top and bottom levels are set as one and the weights on concepts on the middle two levels are set as three. Then, we measure weighted correlations between problem descriptions and problem solutions. If the correlation is lower than 0.13, the corresponding problem log is placed in a sparse group. If the correlation is greater than 0.23, the problem log is placed in a dense group. These two thresholds are empirically specified. The default parameters for running Chameleon in CLUTO were used ex-cept the number of neighbors (-nnbrs=80). Other parameters for Chameleon include the use of graph clustering method (-clmethod=graph) with correlation (-sim=corr) as the similarity and the use of agglomeration (agglofrom=30). For CFocal with SNN, the noise-removing threshold is specified as 200 and K is 70. Finally, we use the labels from domain experts (Expert) as the benchmark.

Table V shows the results. As can be seen, the performance of Ontology is slightly better than that of Correlation and is even close to the performance by domain experts as indicated by Expert in the table. Also, both Ontology and Correlation can lead to a better classification performance compared to the clustering methods and random partition, while CFocal with SNN can lead to a better learning performance compared to Chameleon. The preceding indicates that both Correlation and Ontology are effective methods for identifying focal groups in customer problem logs. In addition, we also compare the SNN method with a traditional similarity measure (cosine) to illustrate the effectiveness of SNN for high-dimension data. Specifically, we use the cosine similarity measure instead of SNN in the algorithm CFocal to generate the focal groups. Here, the threshold Thre is specified as the average cosine similarity between one object and all other objects. In the experiments, we set Thre as 0.2199. As can be seen in Table V, CFocal with SNN results in better results than CFocal with cosine. Intuitively, a better quality of focal groups leads to better learning performance because the samples in focal groups with better quality are more consistent and precise. In this subsection, we investigate the correlation between the performance of multifocal learning and the quality of focal groups.

First, we introduce the External Clustering Validation (ECV) measures [Wu et al. 2009] to validate the quality of focal groups obtained by different methods, such as Cor-relation, Ontology, and clustering-based approaches. Specifically we utilize normalized van Dongen criterion ( VD n ), normalized variation of information ( VI n ), and R n to ex-amine the quality of focal groups. Essentially, the process of finding focal groups is to partition the data into several groups (clusters). This process is the same as the clustering process. Therefore, it is naturally to adopt the external clustering validation measures for evaluating the quality of focal groups.

For each focal group formation method, we examine the learning performance with various measures, such as Accuracy and F-measure. Thus, we can obtain a group of pairs of ECV measures and learning performance measures. Finally, we compute the Pearson X  X  correlation between ECV measures and learning performance measures. Table VI shows the pairs of measures for different focal group formation meth-ods. Please note that we use SVMs as the base classifier and target the two-class categorization problem. Also, we use the same parameters as used in Table V for Cor-relation, Ontology, Chameleon and CFocal.

Figure 6 shows the Pearson X  X  correlation coefficients between the corresponding ECV measures and learning performance measures. As can be seen, there is consistent strong correlation between ECV measures and Learning Performance (LP) measures. For example, the correlation between nVD and F-measure is even less than  X  0 . 97. Since the lower value of nVD/nVI indicates the better quality of focal groups, we can observe that the correlation between nVD/nVI and accuracy/F-measure is negative. As for nR, the higher value indicates the better quality of focal groups, thus we observe the positive correlation between nR and accurac/F-measure.

In addition, we perform a similar correlation study for CFocal to observe the impact of noise on the performance of CFocal. Specifically, by tuning the noise-removing threshold as 100, 125, 150, 175, 200, and 225, we get a group of pairs of ECV measures and LP measures. A similar correlation effect is observed in Figure 7, from which we can find asimilartrendastheoneinFigure6.

Finally, we show the confidence interval for the population mean of observed corre-lation coefficients between ECV measures and the measures of learning performance. Specifically, in Figures 6 and 7, we have totally 12 correlation values between ECV measures and LP measures. With these 12 observations, we first compute the absolute value for each observation because the polarity of correlation does not matter here. Then, we estimate the confidence interval for the population mean of these 12 absolute values. Next, we exploit bootstrapping techniques to compute the confidence interval. As a result, we totally have 1000 bootstrap resamples and choose the confidence level as 0 . 95. In the end, we get the bootstrapping confidence interval as [0 . 8793 , 0 . 9588] for the mean of absolute correlation values.
 In this subsection, we exploit some synthetic data to better illustrate the multifocal property; that is, the impact of the diversity inherent in the data on the learning performance. For four synthetic datasets shown in Figure 5, we can see that there are simple linear-separable concepts as well as complex nonlinear-separable concepts [Wu et al. 2007] in these datasets. Indeed, multifocal learning is a natural solution to the data with complex nonlinear-separable concepts, because multifocal learning allows to decompose the complex concepts into simple linearly separable concepts by grouping data objects into different focal groups.

Since two-dimensional datasets in Figure 5 can be easily decomposed by the cluster-ing algorithms, we apply the clustering methods for generating focal groups. Specif-ically, we use Chameleon [Karypis et al. 1999] in CLUTO [Karypis 2011] with pa-rameters: -clmethod=graph (graph clustering), -sim=corr (correlation similarity), and -nnbrs=60 (60 nearest neighbors).

Table VII shows the results. Here, we use SVMs with both linear and nonlinear kernels as the base classifiers. As can be seen, in terms of F-measure, the performance of multifocal learning is much better than that of single SVMs, no matter whether linear or nonlinear kernels are used. Note that we only show the results of SVMs with linear kernels on DS1 and DS2 datasets and the results of SVMs with non-linear kernels on DS3 and DS4 datasets. A similar trend has actually been observed on all four synthetic datasets. Due to the space limitation, we omit these results. In addition, Figure 8 shows the learning performance in terms of the classification accuracy. In the figure, we can see that the performance of both dense group and sparse group improved significantly with multifocal learning.

Another interesting observation in Figure 8 is that, while the clustering algorithms generate the same focal groups for DS3 and DS4, the distributions of their class objects are actually different. Indeed, for SVMs with linear kernel, the single model has a much worse performance on DS4 than on DS3. This indicates that the complex concepts in DS4 have a negative impact on the performance of linear kernels. However, with multifocal learning, the performance of SVMs with linear kernels on DS4 has been improved a lot, as shown in Figure 8.

Finally, we show the reasons why we choose Chameleon for data with different densities instead of widely used K-means. Here, we use both K-means and Chameleon for generating focal groups on DS2 and DS4 datasets. Figure 9 shows the increased ratio of classification accuracy by multifocal learning. As can be seen, for both linear and nonlinear cases, Chameleon leads to much better performance than K-means, while K-means can also lead to an improved performance of multifocal learning. Here, we provide a simple case study to show that multifocal learning with a Bayesian classifier can lead to smaller classification errors. First, let us consider a binary clas-sification problem, where y 1 and y 2 represent two classes and X represents objects (samples). By the Bayesian classification theory, we have
To make a decision for classification, it is equal to compare the posterior probabil-if we assume p ( y 1 ) is equal to p ( y 2 ), then p ( y / X ) in Eq. (10) can be rewritten as probabilities p ( X / y 1 )and p ( X / y 2 ). There are several ways to estimate these conditional probabilities by exploiting training data [Tan et al. 2005]. Here, we take the Gaussian function for estimating conditional probability. To demonstrate the classification error, we present the class-conditional probabilities and classification boundary in one dimen-sion as shown in Figure 10, where x = h ( X = x i n one di me nsi on ) is the classification boundary for these two classes. The classification error can be calculated as According to Figure 10 or Eq. (11), we know that the classification error is proportional to the overlap area of these two Gaussian functions. As we know, the overlap area is determined by the means and variances of these two Gaussian functions. Similar to the use in Section 3.2, we use two-dimensional Gaussian functions to estimate conditional probabilities of the synthetic data introduced in Section 3.2. Specifically, for single model we use two Gaussian functions, Gau 1 and Gau 2 , to estimate the conditional probabilities of two classes. For multifocal learning, we need four Gaussian functions, Gau D 1 and Gau D 2 for the dense group and Gau S 1 and Gau S 2 for the sparse group. We show these Gaussian functions in Figure 11. As can be seen, there is more overlap between Gau 1 and Gau 2 than between Gau D 1 and Gau D 2 or Gau S 1 and Gau S 2 . Since this kind of overlap is an overlap across three dimensions, to make it more clear, we also show the projection of the overlap on the two-dimensional space as shown in Figure 11. In this case, the classification error can be computed by integration over the two-dimensional area as where, R 1 and R 2 are determined as Note that, for the dense group and the sparse group, both Eq. (12) and Eq. (13) have the same form. To this point, we can clearly see that the classification error of multifocal learning is much smaller than the error produced by the individual learning model from either the Gaussian function figure or the error computation in Eq. (12).
In addition, as shown in Figure 11, the size of training samples for each individual classifier becomes much smaller after partitioning the training samples into different focal groups. Indeed, the process of multifocal learning will partition the training data and may increase the chance of overfitting due to the reduced number of training samples. However, for many real-world data mining problems, such as the customer problem analysis studied in this article, there is a huge amount of training data. In other words, the chance for overfitting can be limited if multifocal learning is applied for these data mining applications. Finally, as we discussed in the Introduction, how to generate the focal groups is a real challenge to multifocal learning. Multifocal learning may lead to worse results if the training samples in each focal group become harder to classify than the original whole training samples.
 In this article, we formalized a multifocal learning problem, which was motivated by the observations of diversities of samples in training data. The key idea of multifocal learning is to divide training data into different focal groups and the learning models should be learned within each focal group instead of building a single learning model using all the training data as a whole. Multifocal learning allows the learning algo-rithms to mitigate the influence of the diversities inherent in training data, and thus leads to better learning performance.

As a practice, we have exploited the multifocal learning techniques for automatically problem categorization in real-world problem logs collected from customer service cen-ters. A critical challenge in the multifocal learning is how to identify focal groups in training data. To address this challenge in problem logs, we proposed a correla-tion method (Correlation) to partition problem descriptions within each class into two different groups: one for experienced customers and another one for inexperienced customers. Also, to better capture the information encoded in problem logs, we devel-oped an ontology-enhanced correlation method (Ontology) for identifying different focal groups. Moreover, we also developed a clustering algorithm (CFocal) based on the SNN similarity. CFocal can remove noise and produce more consistent focal groups (clusters) to build learning models. With these focal group formation methods, we also studied the impact of the quality of focal groups on the performance of multifocal learning.
Finally, as shown in the experimental results, both Correlation and Ontology led to better learning performance than other focal group formation methods, such as the methods based on clustering and random-partition, while the learning performance by ONTOLOGY is slightly better than that by Correlation. Also, we have observed that the CFocal clustering method can lead to better learning performance than Chameleon. In addition, the empirical study also shows that there is strong correlation between the quality of focal groups and the performance of multifocal learning.

Discussions. In this study, we have illustrated the concept of multifocal learning by exploiting problem logs collected in real-world customer service centers. While the solution for forming multiple focal groups has made use of the data characteristics unique to customer problem logs, the basic framework of multifocal learning can be applicable in a much more broad scope. For instance, let us consider a video surveillance system. There are different types of moving objects, such as cars, bikes, and human beings. Those moving objects have different sizes, speed, and moving capabilities. To better capture abnormal moving patterns, it is expected to apply the multifocal learning techniques to first group moving objects into different focal groups. The detection of abnormal moving patterns can then be performed in different focal groups. This will most likely lead to better performance for finding abnormal moving events.
