 Zhouxuan Teng  X  Wenliang Du Abstract In this paper, we propose a hybrid multi-group approach for privacy preserving data mining. We make two contributions in this paper. First, we propose a hybrid approach. Previous work has used either the randomization approach or the secure multi-party compu-tation (SMC) approach. However, these two approaches have complementary features: the randomization approach is much more efficient but less accurate, while the SMC approach is less efficient but more accurate. We propose a novel hybrid approach, which takes advan-tage of the strength of both approaches to balance the accuracy and efficiency constraints. Compared to the two existing approaches, our proposed approach can achieve much bet-ter accuracy than randomization approach and much reduced computation cost than SMC approach. We also propose a multi-group scheme that makes it flexible for the data miner to control the balance between data mining accuracy and privacy. This scheme is motivated by the fact that existing randomization schemes that randomize data at individual attribute level can produce insufficient accuracy when the number of dimensions is high. We parti-tion attributes into groups, and develop a scheme to conduct group-based randomization to achieve better data mining accuracy. To demonstrate the effectiveness of the proposed general schemes, we have implemented them for the ID3 decision tree algorithm and association rule mining problem and we also present experimental results.
 Keywords Privacy  X  SMC  X  Randomization  X  Hybrid 1 Introduction In today X  X  information age, both the volume and complexity of data available for decision-making, trend analysis and other uses continue to increase. To  X  X ine X  these vast datasets for useful purposes has spurred the development of a variety of data mining tech-niques. These approaches include various statistical, machine learning, and modeling methods. Of considerable interest is abstracting information from a dataset composed of information which may be located at different sites, or owned by different people or agen-cies, i.e., distributed databases. Data warehouses have been used traditionally to collect and mine information from distributed sources; however, data owners must be willing to share all their data. Issues of privacy and confidentiality can arise which prohibit data owners from contributing to a data warehouse. For example, a university and law enforcement agency may both have data about violent acts against certain people, and be interested in sharing information to identify trends or perpetrators. However, despite these shared interests, the university may be unwilling to compromise victim X  X  privacy or violate any relevant penal codes. To address these critical privacy and confidentiality issues, privacy-preserving data mining (PPDM) techniques have emerged.

This paper is the extended version of the paper [ 17 ]. In the paper, we propose a hybrid scheme and apply it to two PPDM problems. The first problem is to build decision trees on vertically partitioned data sets. In this PPDM problem, the original data set D is vertically divided into two parts, with one part D a known by Alice, and the other part D b known by Bob. This scenario is caused when some attributes of each record are observed by Alice only, and other attributes are observed by Bob only. The PPDM problem is to find out how Alice and Bob conduct data mining on the vertically joint data set D = D a  X  D b , without compro-mising their private information. The second problem is privacy preserving association rule mining over above vertically partitioned data sets.

A number of solutions have been proposed in the literature to solve various privacy preserv-ing data mining problems. They can be classified into two general categories: one is the secure multi-party computation (SMC) approach, and the other is the randomization approach. In the SMC approach, Alice and Bob run a cryptographic protocol to conduct the joint com-putation. SMC can conduct the required computation while ensuring that the private inputs from either party are protected from each other. Previous results using the SMC approach adds some noise to her data to disguise the original data D a , and then she sends the disguised data set D a to Bob; Several schemes have been proposed for conducting data mining based
The contribution of this paper is two-fold: First, we have developed a hybrid scheme that can harness the strength of both SMC and randomization schemes to achieve better accuracy and efficiency. Second, we have developed a general multi-group scheme, which provides a flexible mechanism for data miner to adjust the balance between privacy and accuracy.
Our hybrid scheme is based on the following observation: Both SMC and randomization approaches have their own advantages and disadvantages. In general, the SMC approach is quite expensive because of the cryptographic protocols, which require a great amount of communication and computation [ 16 ]. However, the SMC approach does not reduce the accuracy, i.e., it always produces the same results as those obtained directly from the original data set D a  X  D b (without privacy constraints). On the other hand, the randomization is quite efficient compared to the SMC approach, it only needs one round of communication (for sending the disguised data); after that, the data mining computations are conducted locally. Since no cryptographic computations are needed, the computations are also much less expen-sive than those in SMC. However, because of the random noise, the randomization approach is not able to produce the same results as the original algorithm. The accuracy is sacrificed to certain degree.

The complementary features of the randomization and SMC approaches motivate us to think whether there exist ways to combine these two seemingly-incompatible methods together, such that the new methods can take advantage of both the randomization X  X  perfor-mance strength and the SMC X  X  accuracy strength. No existing work has proposed such a solu-tion. In this paper, we propose the first solution to combine the strength of both approaches. We call this approach the Hybrid approach. To avoid confusion, we refer to the approach that only uses randomization as the randomization-only approach, and the approach that only uses SMC as the SMC-only approach.

The second contribution of this paper is a multi-group framework for randomization schemes. We have observed that randomizing each individual attribute of a data set inde-pendently (an approach used by most of the existing works) can produce undesirable results when the dimension of the data set is high. Our multi-group framework divides the attributes into g groups, and we conduct the randomization only on the group level, rather than on the individual attribute level. The higher the value of g , the better the privacy, but the worse the data mining accuracy. By selecting an appropriate value for g , one can achieve a desirable trade off between privacy and accuracy, which suits a specific application.

Our proposed hybrid approach and multi-group approach are general and can be applied to various data mining computations, including decision tree building and association rule mining. To demonstrate the effectiveness of these proposed approaches, we have applied them to the ID3 decision tree algorithm and association rule mining algorithm. 2 We compare the accuracy and the efficiency of the above two problems based on the proposed approaches with those based on the traditional approaches.

An expanding topic in the broader area of privacy-preserving data mining is the quanti-fication of privacy and associated algorithms to protect privacy. k -Anonymity is a popular metrics along this line and some other alternative metrics are also proposed, such as the one proposed by Wang et al that limits the confidence of inferring sensitive properties to protect against the threats caused by data mining abilities [ 21 ].

To the best of our knowledge, [ 17 ] is the first paper to propose the hybrid scheme that considers the trade off between data mining accuracy, privacy, and performance issues in privacy preserving data mining community. 2 Related work 2.1 SMC approach One approach to achieve privacy-preserving data mining is to use Secure Multi-party Com-putation (SMC) techniques. Briefly, an SMC problem deals with computing certain function on multiple inputs, in a distributed network where each participant holds one of the inputs; SMC ensures that no more information is revealed to a participant in the computation than what can be inferred from the participant X  X  input and the final output [ 7 ].
In data mining field, several SMC-based privacy-preserving data mining schemes have over the horizontally partitioned data [ 11 ]. Vaidya and Clifton proposed the solutions to the clustering problem [ 19 ] and the association rule mining problem [ 18 ] for vertically parti-tioned data. Du and Zhan proposed a SMC-based solution to build decision trees over the vertically partitioned data [ 4 ]. Sanil et al. described a privacy-preserving algorithm of com-puting regression coefficients [ 15 ]. Wright and Yang presented a privacy-preserving protocol for learning Bayesian network structure on distributed heterogeneous data [ 24 ]. Meng and his colleagues considered a random projection-based method, which was used to securely compute the inner product, to learn the parameters of a Bayesian Network [ 12 ]. Several SMC tools and fundamental techniques are also proposed in the literature [ 3 , 13 ].
Vaidya et al. proposed a privacy-preserving support vector machine classification scheme for vertically and horizontally partitioned data [ 20 ]. 2.2 Randomization The goal of the randomization approach is to add noise to the original data set, such that the original data are disguised, and at the same time, the aggregate information of the data set can still be estimated from the disguised data. Warner introduced randomized response technique in statistics field [ 23 ] which became the basic model for all other randomization schemes proposed later. These successive work on this technique were performed in both statistics community and data mining community.
 The randomization approach for privacy preserving data mining was first pioneered by Agrawal and Srikant proposed in [ 2 ], and hence extended by Agrawal and Aggarwal [ 1 ].
To randomize categorical data, Rizvi and Haritsa presented a scheme to mine associations with secrecy constraints in [ 14 ]; Evfimievski et al. proposed an approach to conduct pri-vacy preserving association rule mining [ 6 ]; Du and Zhan proposed an approach to conduct privacy preserving decision tree building [ 5 ]. Zhu and Liu proposed a general framework for randomization based on mixture models [ 26 ]. Wang et al. used an iterative bottom-up generalization to generate data, which remains useful to classification but difficult to dis-close private sources [ 22 ]. Recently, security properties of the randomization schemes and privacy preserving data mining in general are studied by Kargupta et al. [ 10 ] and Kantarcioglu et al. [ 9 ].

Xu et al. also propose an approach based on sparsified Singular Value Decomposition (SVD) method for data distortion, along with a few metrics to measure the difference between the distorted and original datasets and the degree of the privacy protection [ 25 ]. 3 Problem definition and background In this section we describe the first problem: decision tree building problem for vertically partitioned data.

Before the definition of the problem considered in this paper, we first introduce some terms. A database contains some records, and each record contains values of some attributes. So we can also say that a database consists of values of some attributes. If we split the database into two parts  X  vertically  X , i.e., one part consists of values of some attributes and the other
The problem is defined in the following and illustrated in Fig. 1 a. Definition 1 (Two-party decision tree building over vertically partitioned data) Two parties, Bob and Alice, each have values of different attributes of a data set. Both parties have the class values of all records. They want to efficiently build a decision tree based on the joint database containing attributes from both parties. Due to the privacy of the values of these attributes, neither of the two parties wants to share original values of the attributes he/she is holding with the other party. However, each is willing to share his/her data set with the other party if the values of the attributes he/she is holding are disguised to some extent to preserve privacy.

In the rest of this section, we give a brief overview of the ID3 algorithm, and then review two existing approaches that can be used to solve this problem. 3.1 ID3 algorithm In a decision tree, each non-leaf node contains a splitting point, and the main task for build-ing a decision tree is to identify the test attribute for each splitting point. The ID3 algorithm uses the information gain to select the test attribute. Information gain can be computed using Entropy ( S ) is defined as the following: where Q j ( S ) is the relative frequency of class j in S . Based on the entropy, we can compute the information gain for any candidate attribute A if it is used to partition S : where v represents any possible values of attribute A; S v is the subset of S for which attribute we compute information gain for each attribute. We then use the attribute with the largest information gain as the test attribute to split the node.
 In the original ID3 algorithm (i.e., the full data set is available), we get | S | , | S v | ,and Q j ( S ) simply by counting. However, when data are vertically partitioned, nobody can see a complete record; therefore it is impossible to know which record is in S or S v ,and randomization and SMC approaches must resolve. Unlike the horizontally-partitioned data, it is still unknown whether it is possible to build a local model on each partition first, then to merge the models to form the global decision tree. This is because the local models cannot capture the relationships among attributes from different partitions.

In decision tree building, assume that the set S is associated with a node V in the tree. All the records in S have the same values for certain attributes (each of these attributes corre-sponds to a node on the path from the root to V ). We use a logical AND expression E ( S ) to encode those attributes, namely all the records in S satisfy the expression E ( S ) . For example, E (
S ) = ( A 1 = 1  X  A 3 = 0  X  A 5 = 1 ) means that all the records in S have the values 1, 0, and 1 for attribute A 1 , A 3 ,and A 5 , respectively. Let D represent the entire data set. We use N ( E ) to represent the number of records in the data set D that satisfy the expression E .We then have the following formula: From the above equations, we know that as long as we can compute N ( E ) for any logical AND expression E , we can get all the elements that allow us to compute entropies and infor-mation gains. We show how to compute N ( E ) using the SMC approach or the randomization approach for vertically-partitioned data.

Recall that in our vertically-partitioned data settings, the data set D is vertically partitioned into two sets D a and D b , with D a being known only by Alice and D b being known only by Bob. The total number of records in D is n . 3.2 The SMC approach If all the attributes in E belong to the same party, then N ( E ) can be directly computed by that party. If some attributes in E belong to Alice, and some attributes belong to Bob, they can use secure multi-party computation protocols to compute N ( E ) . The approach is depicted in Fig. 1 b. Let us divide E into two parts, E = E a  X  E b ,where E a contains only the attributes from Alice, while E b contains only the attributes from Bob. Let V a be a vector of size n : V Alice can compute V a from her own share of attributes. Similarly, let V b be a vector of size n : V b ( i ) = 1ifthe i th data item satisfies E b ; V b ( i ) = 0 otherwise. Bob can compute V b from his own share of attributes.
 means the corresponding record satisfies both E a and E b , thus satisfying E . Therefore, to compute N ( E ) , we just need to find out how many entries in V are non-zero. This is equivalent to computing the dot product of V a and V b :
Because of the privacy constraints, Alice should not disclose her private data V a to Bob, neither should Bob. Therefore, we need a secure two-party protocol to enable Alice and Bob to compute the dot product of their private inputs. A number of dot-product protocols have already been proposed in the literature [ 4 , 18 ]. With these SMC protocols, Alice and Bob can get (and only get) the result of N ( E ) , neither of them knows anything about the other party X  X  private inputs, except the information that can be derived from N ( E ) .

Once we can compute N ( E ) for any expression E , especially when attributes in E are vertically partitioned, we can compute all the values needed by Eq. ( 2 ), and thus can build the ID3 decision trees. 3.3 The randomization approach To use the randomization approach to build decision trees, Alice generates a disguised data set D a from her private data D a . Alice then sends D a to Bob. Bob now has the full data set D a  X  D b , though part of which is disguised. Bob can conduct data mining based on this partially disguised data set [ 5 , 6 , 14 ]. This approach is depicted in Fig. 1 c.
There are a number of ways to perform randomization. Our scheme in this paper is based on the randomized response technique [ 23 ]. They were proposed in several existing works [ 5 , 6 , 14 ] to deal with categorical data in privacy preserving data mining. We briefly describe the technique in the following, and readers can get details from the literature. For simplicity, we assume that the data type in D is binary. The idea can be extended to other categorical data types.

In the randomized response technique, to disguise a binary value, the data owner flips a biased coin (the probability of the head is  X  = 0 . 5). If the result is the head, the data owner tells the truth, i.e., he/she sends the original value; if the result is the tail, the data owner tells a lie, i.e., he/she sends the inverse of the original value. Warner has shown that based on the disguised data, we can estimate the number of 1 X  X  (or 0 X  X ) in the original data set [ 23 ]. Warner X  X  randomized response technique on univariate data sets was recently extended to multivariate data sets. The data disguise is still the same, namely each data value is disguised based on the result of an independent coin flipping. It has been shown that we can estimate N ( E ) for any arbitrary logical AND expression E (e.g., E = ( A 1 = 1 )  X  ( A 2 = 0 ) ). Such estimation method serves as the basis for privacy preserving decision tree building [ 5 ]and association rule mining [ 6 , 14 ]. 4 A hybrid approach for privacy preserving data mining To obtain more accurate data mining results, we prefer to use the SMC approach because it gives us exactly the same results as those obtained directly from the original data set (without privacy constraints). However, because of the high costs, we cannot afford to conduct SMC over the entire data set for all the required computations. The challenging problem is whether we can reduce the amount of work that needs to be performed by SMC.

Many of the data mining computations involve searching among a set of candidates. For example, in building decision trees, at each tree node, we search for the best test attribute from a candidate set based on certain criteria (e.g. information gains); in association rule mining, we search through a set of candidates to find those whose supports are above certain threshold. Using SMC to conduct these searches is expensive because the search space can be quite large. If we can reduce the search space using some light-weight computations (in terms of both communication costs and computation costs), we can significantly reduce the total costs.

Randomization scheme is a very good choice for such a light-weight computation because of two reasons: first, it is much less expensive than SMC; second, although it does not produce accurate results, it does produce a good approximation. Due to the introduced random noise, randomization schemes tend to produce results that are less accurate than the original results (i.e., those from the original data set without disguising). However, although the randomi-zation results are not accurate enough for the final results, the approximations produced are good enough if we only use them for the filtering purposes. Assume that the computations and data involved in a task is represented by a set Z . Based on the (approximate) randomization results, we can identify a set Z u , the removal of which is unlikely to affect the data mining results. Then we use SMC schemes to repeat the same data mining task, but this time, the computations and data used are drawn from the reduced set Z  X  Z u , and SMC guarantees to produce the accurate searching results from this reduced set.

If Z u is a significant portion of Z , the costs for performing SMC are substantially reduced compared to the computations that use SMC alone. The entire hybrid approach is depicted in Fig. 2 . 4.1 Building decision trees using the hybrid approach Let D represent the entire data set. Let D a represent the part of the data owned by Alice, and let D b represent the part of the data owned by Bob. Alice disguises D a using the randomi-zation approach, and generate the disguised data set D a ; she sends D a to Bob. Bob does the same and sends his disguised part D b to Alice. Alice forms the entire data set D 1 = D a  X  D b , while Bob forms D 2 = D a  X  D b .

In the decision tree building algorithm, most of the computations are spent on finding the best splitting attribute for each tree node. In the ID3 algorithm, the information gain is calculated for each candidate, then the attribute with the largest information gain is selected. Using SMC to compute the information gains between two parties is doable, but very expen-sive [ 4 ]. On the other hand, using randomization is much faster, but due to the approximation, the selected splitting attribute might not be the one with the largest information gain.
We can combine the randomization and SMC to obtain better results. Figure 3 shows an example of how the hybrid approach works for the decision tree building. In this figure, a , information gain among these candidates.

First, we use the randomization approach to compute the information gain for each of the candidates. However, realizing the inaccuracy, we selected WS attributes with the largest information gains, rather than selecting only one. We call WS the window size . With WS &gt; 1, even if the (actual) one with the largest information gain is not the best test attribute based on the randomization results, it is still possible for this attribute to be selected.
Once we have obtained WS candidates, we can redo the computation just on these WS attributes. However, this time we use the SMC approach, which can tell us the accurate infor-mation gain. Because WS can be significantly smaller than the total amount of attributes, the computation and communication cost is much less than the SMC-only solutions. 4.2 The Hybrid-ID3 algorithm We describe the Hybrid-ID3 algorithm, which uses the randomization and SMC schemes as building blocks. In this algorithm, we use N ( E ) to represent the actual number of records in D that satisfy the expression E , i.e., N ( E ) is computed using the SMC approach. We use AL to represent a set of candidate attributes. Before conducting this algorithm, Alice and Bob have already exchanged the disguised data. Namely Alice has D 1 = D a  X  D b ,andBob has D 2 = D a  X  D b .

Hybrid-ID3( E , AL ) 1. Create a node V. 2. If N ( E  X  ( class = C )) == N ( E ) for any class C , then return V as a leaf node labeled 3. If AL is empty, then return V as a leaf-node with the class C = argmax C N ( E  X  ( class = 4. Find the splitting attribute using the following procedure: 5. Label node V with TA . 6. For each known value a i of TA Note that the values of N ( E  X  ( class = C )) at Step 2 and Step 3 can be obtained from Step 4.c of the previous round. Similarly, computations at Step 6.b can also be obtained from Step 4.c of the same round. Therefore, there are no extra SMC computations in Step 2, 3, and 6.b. 4.3 Privacy and cost analysis Because SMC computations do not reveal any more information about the private inputs than what can be derived from the results, the primary source of information disclosure is from the disguised data due to the randomization approach. Several privacy analysis methods for the randomization approach have been proposed in the literature [ 1 , 6 , 14 ]. We will not repeat them in this paper.

Regarding the computation and communication costs, we are only interested in the relative costs compared to the SMC-only approach. Since the computation and the communication costs of the randomization part is negligible compared to the SMC part, we use the amount of SMC computations conducted in the hybrid approach as the measure of the cost, and we compare this cost with the amount of SMC computations conducted in the SMC-only approach. This cost ratio between these two approaches is primarily decided by the window size. We will give the simulation results in Sect. 6 . 5 The multi-group randomization scheme For many data mining computations, calculating the accurate relationship among attributes is important. Randomization tends to make this calculation less accurate, especially when each attribute is randomized independently, because of the bias introduced by the random-ization schemes. The randomization schemes proposed in the literature mostly randomize attributes independently. We have found out that such randomization schemes lead to undesir-able results for privacy preserving decision tree building. For the purpose of achieving better accuracy, we propose a general multi-group framework, which can be used for randomization schemes.

In this scheme, attributes are divided into g (1  X  g  X  t ) groups (where t is the total number of attributes in the data set); randomization is applied on the unit of groups, rather than on the unit of single attribute. For example, if randomization is to add random noise, then we will add the same noise to the attributes within each group. 3 However, these num-bers are independent from group to group. The advantage of this multi-group scheme is that by adding the same random noise to hide several attributes together, the relationship of these attributes are better preserved than if independent random numbers are added. However, the disadvantage of this approach is that if adversaries know the information group. Thus, there is a balance between privacy and data mining accuracy. By choosing the appropriate value of g , we can achieve a balance that is suitable for a specific applica-tion.

To demonstrate the effectiveness of this multi-group framework, we apply it to a specific randomization scheme, the randomized response scheme, which has been used by various researchers to achieve privacy preserving data mining. We call our scheme the multi-group randomized response (MRR) scheme. The existing randomized response schemes are spe-cial cases of the MRR scheme: the scheme proposed in [ 5 ] is a 1-group scheme, while the schemes proposed in [ 6 , 14 ] are essentially t -group scheme because each attribute forms its own group. 5.1 Data disguise In the general randomized response technique, before sending a record to another party (or to the server), a user flips a biased coin for each attribute independently, and decides whether to tell a truth or a lie about the attribute based on the coin-flipping result. In MRR scheme, the process is still the same, the only difference is that now the coin-flipping is conducted lie about all of them.

We show how to apply the randomized response technique for each group. Let E represent any logical AND expression based on those attributes (e.g., E = ( A 1 = 1 )  X  ( A 2 = 0 ) ); let E denote the logical AND expression that reverses the 1 X  X  in E to 0 X  X  and 0 X  X  to 1 X  X ; we call E the bitwise-opposite of E . For example, for the E in the previous example, E = (
A 1 = 0 )  X  ( A 2 = 1 ) . If the user needs to tell the truth about E according to the  X  X oin-flipping X  result, the user tells the truth for all the attributes in this group, namely he/she sends E . If the user needs to tell lies according to the  X  X oin-flipping X , he/she tells lies to all the attributes within this group, namely he/she sends E . For instance, if the group con-0 ) .

With this scheme, the truth-or-lie decision is independent for each individual group, but the decision is the same for all the attributes within the same group. Therefore, if the data receivers know the information about one group, they will not be able to derive the infor-mation for the other groups because they are disguised independently. However, when the value of any attribute of one group is compromised, the values of all the attributes in the same group will be compromised. At one extreme, when g = t , each attribute is independently disguised; the privacy becomes the best. However, when g becomes large, the accuracy gets worse. This multi-group disguise scheme allows an application to choose the value for g based on its accuracy and privacy requirements. 5.2 Estimating N ( E ) As we have discussed in Sect. 3.1 , estimating N ( E ) , the number of records that satisfy the E in the joint data set, for any logical expression E is vital for ID3 decision tree building (actually for many data mining computations). Let P ( E ) represent the portion of the data set that satisfies E . Estimating N ( E ) is equivalent to estimating P ( E ) , so we focus on the estimation of P ( E ) . Obviously, direct counting on the disguised data set does not give us P (
E ) . We will show two different ways to estimate P ( E ) . 5.3 Computing P ( E ) Assume that the expression E contains attributes from m groups. We rewrite E using the following expression, with e k being an expression consisting of only attributes from the group k (we call e k a sub-pattern of E ):
We define a variation of E as E = f 1  X  X  X  X  X  f m ,where f i is equal to either e i or the bitwise-opposite of e i (i.e. e i ). For each expression E ,therearetotally2 m different varia-tions, including E itself. We denote these variations of E as E 0 to E  X  ,where E 0 = E and  X  = 2 m  X  1.
 Theorem 1 Let P ( E i  X  E j ) represent the probability that an expression E i in the original data becomes an expression E j in the disguised data after the randomized response process. We have the following formula: where u represents the number of the common bits between the binary forms of number i and number j .
 Proof See Appendix A.

Let P  X  ( E ) represent the expected number of records, in the disguised data set, that satisfy the expression E , given the original data set and the randomization parameter  X  . Although P  X  ( E ) cannot be observed directly either, as long as the number of samples in the data set is statistically significant, we can estimate the value of P  X  ( E ) by counting the number of records that satisfy E in the disguised data set.

A record that satisfies the expression E j in the disguised data can come from records that satisfy E 0 ,..., E  X  in the original data set. Therefore, we have
If we define a matrix A , such that A ( i , j ) = P ( E i  X  E j ) for i = 0 ,..., X  and j = 0 ,..., X  , we get the following linear system of equations. Theorem 2 Let A be defined as above. Then A is invertible if and only if  X  = 0 . 5 . Proof See Appendix B.

Therefore, as long as  X  = 0 . 5, the value of P ( E ) can be obtained by solving the above linear system of equations (recall that E is E 0 ). In addition, we can also obtain values of P ( is 2 m by 2 m . Note that the matrix A only depends on the randomization parameter  X  ,so A and A  X  1 can be calculated off-line for a pre-determined  X  . When the number of groups m is small, 4 the above approach is still efficient enough.

However, in situations where P ( E ) is the only thing we need, just like in the ID3 decision tree building algorithm, there is a much more efficient solution with cost O ( m ) . We will describe the technique in the following subsection. We use a technique similar to the one used in [ 6 , 14 ]. 5.4 Computing P ( E ) using linear forms from e 1 ,..., e m and m  X  i of the sub-patterns from e 1 ,..., e m .

We d e fi n e P ( i ) as the probability that the property F ( i ) is satisfied in data set D .We Theorem 3 Let P ( i  X  j ) represent the probability that a record that satisfies F ( i ) in D is transformed to a record that satisfies F ( j ) in D .P ( i  X  j ) can be calculated using the following formula: Proof See Appendix C.

A record that satisfies F ( j ) in the disguised data set D can come from and only from records that satisfy F ( 0 ),..., F ( m ) in the original data set, i.e.,
Similarly, if we define a matrix A ,suchthat A ( i , j ) = P ( i  X  j ) for i = 0 ,..., m and j = 0 ,..., m , we get the following linear system of equations.
Assume A is invertible, 5 then we can calculate P ( m ) by solving the above linear system of equations. Since satisfying m sub-patterns means satisfying E (recall that E has only m sub-patterns), we get P ( E ) = P ( m ) . 6 Evaluation To evaluate the proposed hybrid scheme, we have selected three databases from the UCI Machine Learning Repository: 6 (1) The Adult database was extracted from the 1994 Cen-sus database. It contains 48,842 records with 14 attributes. The prediction task is to deter-mine whether the salary is greater than 50,000 or not. (2) The Mushroom database includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. It contains 8,124 records with 22 attributes. The prediction task is to determine whether the mushroom is edible or poisonous. (3) The Tic-tac-toe database encodes the complete set of possible board configurations at the end of tic-tac-toe games. The prediction task is to determine the winning party of the game. It contains 958 records with 9 attributes each corresponding to one tic-tac-toe square. We convert the records in these three databases into binary format for our experiments.

Since data sets are vertically partitioned in our settings, we randomly divide all attributes of each data set into two parts with the same cardinality; one part belongs to Alice and the other part belongs to Bob.

In our experiments, we always used 80% of the records as the training data and the other 20% as the testing data. We use the training data to build the decision trees, and then use the testing data to measure how accurate these trees can predict the class labels. The percentage of the correct predictions is the accuracy value in our figures. We repeat each experiment for multiple times, and each time the disguised data set is randomly generated from the same original data set. We plot the means and the standard deviation for the accuracy values. 6.1 Accuracy versus number of groups Figure 4 shows the change of accuracy along the number of groups in the randomization-only approach for Adult dataset. In the figure,  X 1G X ,  X 2G X ,  X 3G X , and  X 4G X  indicate that the data are disguised using the 1-group, 2-group, 3-group, and 4-group randomization schemes respectively. From the figure, we can see that the accuracy decreases when the number of groups increases. When  X  is close to 0.5 (e.g.,  X  = 0 . 4), the rate of deterioration is rapid as the number of group increases. It is interesting to see that the results of the 4-group scheme are very close to those of the 3-group scheme. This is because in this specific Adult dataset, most of the expressions that are evaluated in building the tree contain attributes from less than 3 groups. 6.2 Accuracy: hybrid versus randomization-only Figure 5 a, c, and e show the accuracy comparisons between the hybrid approach and the randomization-only approach. The vertical bars in the figures depict the standard deviations. The comparisons are shown for different randomization parameter  X  and for different win-dow size WS . In these three figures,  X 4G X  and  X 1G X  indicate that the data are disguised using the 4-group randomization scheme and the 1-group randomized scheme, respectively.
The figures clearly show that the hybrid approach achieves a significant improvement on accuracy compared to the randomization-only approach. When  X  is near 0 . 5, the accuracy of the trees built via the randomization-only approach is just slightly better than the random guess (a random guess can yield 50% of accuracy on average). In contrast, the trees built via the hybrid approach can achieve a much better accuracy.
When the window size is increased to 3, the accuracy difference between the 4-group randomization scheme and the 1-group randomization scheme becomes much smaller. This means, choosing the 4-group randomization scheme does not degrade the accuracy much when WS = 3, while at the same time, it achieves better privacy than the 1-group randomi-zation scheme.

A surprising result in all these three figures is that when the window size is set to 1, the accuracy can be improved significantly compared to the randomization-only approach. Initially we thought that the hybrid approach with WS = 1 is equivalent to the randomiza-tion-only approach. From this result, we realized that they are different, and the difference is at Step 2 and 6.b of the Hybrid-ID3 algorithm. Step 2 detects whether all the records this node. With the hybrid approach, such a detection is conducted using SMC, which always generates the accurate results. However, using the randomization-only approach, because the result is inaccurate, it is very likely that we will continue splitting the node even when such a splitting is unnecessary. These extra splittings may result in a dramatic different tree struc-ture compared to the tree built upon the original undisguised data, thus cause the significant difference in their accuracy results. Step 6.b has the similar effect. 6.3 Accuracy versus window size WS Figure 5 b, d, and f show the relationship between the accuracy and the selected window size in the hybrid approach. In these figures, the number of groups g is fixed to 4.
The figures show that increasing SMC window size increases the accuracy of the decision tree. The increase is quite rapid when the window size is small; after certain point, the change of the window size does not affect the accuracy much. This means that the actual best test attribute is very likely among the top few candidates. This result indicates that choosing a small window size can be the very cost-effective: it achieves a decent degree of accuracy without having to conduct many expensive SMC computations. 6.4 Efficiency improvement The motivation of the hybrid approach is to achieve better accuracy than the randomiza-tion-only approach, as well as achieve better efficiency than the SMC-only approach. Our previous experiments have shown the accuracy improvement. We now show how well the hybrid approach achieves the efficiency goal. We have summarized the efficiency improve-ment in Table 1 , along with the degree of accuracy achieved (we fix the number of groups to 4, and the randomization parameter  X  to 0 . 45).

In Table 1 , A is the accuracy of the hybrid approach minus the accuracy of the randomiza-tion-only approach, i.e., A represents the accuracy improvement over the randomization-only approach. C is the ratio of the total number of SMC computations in the hybrid approach to that in the SMC-only approach, i.e., ( 1  X  C ) represents the efficiency improvement over the SMC-only approach. The smaller the C is, the better.

The table shows that the efficiency improvement for the Mushroom data set is the most significant. This is because the number of attributes in the Mushroom data set is the largest among the three data sets. This trend indicates that the larger the number of attributes is, the higher level of efficiency improvement can be achieved. 7 A hybrid approach for association rule mining Besides decision tree building problem, our proposed hybrid approach can be applied to some other secure two-party privacy preserving data mining problems. For example, it can also be applied to secure two-party privacy preserving association rule mining problem. In this section, we discuss how to apply the hybrid approach to the association rule mining problems. Here, the problem definition is the same as the one for decision tree building we
In the association rule mining algorithm [ 8 ], most of the computations are spent on finding the support score for each new candidate derived from the i -item set. Then the new candidates that have support scores larger than a threshold form the ( i + 1 ) -item set. The process will then repeat on the newly formed ( i + 1 ) -item set, until i = n  X  1.

This algorithm can be carried out using SMC approaches [ 18 ], but the cost is still expensive due to the large amount of interactive protocols between the two participants. The algorithm can also be carried out using randomization approaches [ 6 ], but the support scores obtained via these approaches are not accurate. As a result, some candidates with actual support scores larger than the threshold might find their scores below the threshold using randomization approaches, and vice versa. To avoid this situation, [ 6 ] proposes to lower the threshold. This solution increases the probabilities that real candidates are included, but it can also include more candidates whose actual scores are below the actual threshold. The problem is how to filter out these candidates. We can use the hybrid approach to solve this problem. The idea is that for each selected candidate, we use the SMC to compute the actual support score, and throw away those that are below the actual threshold. Because SMC operates only on the candidates selected via the randomization approaches, the amount of the SMC computations is significantly reduced compared to the solutions that use SMC alone.
Figure 3 shows an example of how the hybrid approach works for the association rule those combinations with support scores larger than the threshold 0 . 001.

First, we use the randomization approach to estimate the support score for each combina-tion, and select those whose scores are larger than 0 . 001  X  w ,where w = 0 . 0003 is used for lowering the threshold. Our goal is to use an appropriate w value, such that all the qualified combinations have high probabilities to be selected.

There are two steps in association rule mining, the first step is to find all frequent item sets (abbreviated as frequent sets from now on) with support higher than a pre-defined thresh-old, and the second step is to find association rules with confidence higher than a pre-defined we find all the frequent sets and our hybrid scheme only applies to the first step. Therefore, we focus on finding frequent item sets in datasets instead of association rules.
The experimental results are presented in the following section. 7.1 Evaluation The same Adult dataset used in previous sections for building decision trees is used in this experiment. We only consider the one-group randomization scheme in this part as the effect of multiple groups has already been presented in the previous results.

In this section, we first obtain results of SMC and Randomization-Only schemes and determine the values of parameters used in hybrid scheme and then compare different schemes. 7.1.1 SMC and randomization-only schemes First, we obtain the results of SMC and randomization-only schemes in Fig. 7 a and b, respectively.
 SMC scheme The Fig. 7 a shows the number of frequent sets found in the data set for various values of support, and it also shows the total number of sets checked and pruned for different values of support. From Fig. 7 a, we can clearly see that the difference between the number of checked and found sets, i.e., the number of pruned sets, is almost constant, except when is less than 0.2 and less when support is larger than 0.9, than the number of pruned sets in the range [ 0 . 2 , 0 . 9 ] . In this range, the number of pruned sets is almost constant.
This indicates that the value of support does not change the performance of SMC scheme too much, thus we randomly choose support to be 0.3 in the following experiments. Randomization-only scheme The Fig. 7 b shows the accuracy of the randomization-only scheme. Among the frequent sets found by randomization-only scheme, some are truly fre-quent and some are not. Also, some frequent sets may not be included in the results. As a result, we evaluate the accuracy of the finding algorithms by the quality of the found frequent sets through the following two metrics:  X  Ratio of missed sets : the ratio of number of missed frequent sets and the total number of
Figure 7 b shows the ratios of valid and missed sets with associated standard deviation (from 100 repeated experiments), for Randomization-Only scheme with different values of  X  , the randomization parameter. Since our data set is binary, the values  X  and 1  X   X  are equivalent, so we only consider the range [ 0 , 0 . 5 ) .

From the figure, we can clearly see that the quality of the found frequent sets is pretty good when  X &lt; 0 . 45. When  X &gt; 0 . 45, the missed ratio increases dramatically and the valid ratio drops dramatically. Theoretically, when  X  = 0 . 5, the valid ratio is very close to 0 since we are simply randomly guessing, and the missed ratio is very high for the same reason.
The closer of  X  to 0.5, the better the privacy of the disguised data set, so we want to use  X  values close to 0.5 to preserve more privacy. Figure 7 b shows that we can obtain pretty good accuracy when  X  is between 0 and 0.45, which eliminates the necessity of our hybrid scheme.

Therefore, we only need to consider the range of  X  , [ 0 . 45 , 0 . 5 ) , in our experiments. This range is desirable because it preserves the privacy of disguised data set very well. Without loss of generality, we choose value 0.495 in the following comparisons. 7.1.2 Accuracy comparison and performance comparison In this section, we evaluate the accuracy improvement of hybrid scheme over Randomiza-tion-Only scheme and performance improvement of hybrid scheme over SMC scheme, while changing tolerance values and fixing  X  = 0 . 495 and support to 0.3. The results are shown in figure 8 .
 Accuracy comparison The Fig. 8 a shows the missed and valid ratios for randomization-only and hybrid schemes. Note that we ignore the valid ratio for hybrid scheme since it is guaranteed to be 1.0 by SMC component.
 The Fig. 8 a clearly shows that the hybrid scheme does not change missed ratio much from Randomization-Only scheme, but it does increase valid ratio from less than 0.7 to always 1.0. This is reasonable since SMC component is applied after randomization-only component at each iteration step, so the missed frequent sets by randomization step can not be re-found by SMC component; but SMC component can guarantee the final sets are all truly frequent sets.
Therefore, hybrid scheme improves the accuracy of Randomization-Only scheme by mak-ing sure that all the found sets are truly frequent, i.e., no false positive.
 Performance comparison The Fig. 8 b shows the performance of randomization-only scheme and hybrid scheme. The number of checked sets by randomization-only scheme becomes very high when tolerance is 0.3 (close to 20,000) and it is ignored in the figure.

The Fig. 8 b shows that approximately same number of sets are checked by randomization component and SMC component in hybrid scheme, respectively. Due to the relatively much higher cost of SMC operations than randomization operations, we can ignore the cost of randomization component and only focus on the cost of SMC component.
From Fig. 7 a, we see that the number of sets checked by pure SMC scheme is about 130 when support is 0.3, while in Fig. 8 b, we see that the number of sets checked by SMC compo-nent in hybrid scheme increases gradually from about 80 to 130 when tolerance increments from 0 to 0.3.

This indicates that the performance of hybrid scheme over pure SMC scheme is almost fixed and not very flexible as in the hybrid decision tree building algorithm. Here the perfor-mance improvement is between 38 and 23% when tolerance is between 0 and 0.25. 8 Conclusions and future work We have presented a hybrid approach and a multi-group randomization approach for pri-vacy preserving data mining problems over vertically-partitioned data. The hybrid approach combines the strength of the SMC approach and the randomization approach to achieve both high accuracy and efficiency. Our experiments show that the hybrid approach achieves sig-nificantly better accuracy compared to the randomization-only approach, and at the same time, it is much more efficient than the SMC-only approach. Our multi-group randomiza-tion approach allows data miners to control the trade off between privacy and data mining accuracy.

We have also applied our hybrid scheme to decision tree building and association rule min-ing problems and our experiments show that the hybrid scheme indeed combines the desired features of SMC approach and randomization approach and gives us a trade off between accuracy, privacy, and performance.

For the hybrid approach, we only used a fixed window size throughout the entire deci-sion tree building process. In the future, we will investigate whether a dynamic window size can help further improve the performance. That is, the window size for different tree nodes might be different, depending on the randomization results. We will also investigate the effectiveness of the hybrid approach on other data mining computations.
 Appendix A: Proof of Theorem 1 Let each E i be represented by the following equation: and where i ( k ) represents the k -th bit of number i .

Recall that P ( E i  X  E j ) represents the probability that an expression E i in the original data becomes an expression E j in the disguised data. Because each group is randomized independently, we have where in the disguised data is the bitwise-opposite of that in the original data; in other words, the
If we let u represent the number of the common bits between the two numbers i and j , we get Appendix B: Proof of Theorem 2 There are K = 2 m different sub-patterns for m groups, denote them as e 1 ,..., e k . In forming matrix A , we treat these K sub-patterns as an ordered list and the index of each sub-pattern determines its index in the matrix A . We can choose different permutations of these K sub-patterns. But exchanging the positions of two sub-patterns only results in exchanging two rows and two columns which does not change the value of det ( A ) . Thus we can choose any permutation out of K ! possible permutations for the proof.

We choose one permutation as the following. First let X  X  construct a complete binary tree with depth m . Label the root node with group m ,the2 nd level node with group m  X  1, ... , the leaf node with group 1. For each node with label group i , let group i  X  X  attributes disclose their true values in the node X  X  left sub-tree, and let them disclose false values in the node X  X  right sub-tree. Each path from the root to a leaf corresponds to a sub-pattern. We order these paths by the position of corresponding leaves from left to right. And we choose this ordered list to form matrix A .

Let A m denote the matrix for the case of m groups. Now let X  X  prove the theorem. We can prove a more general result and our theorem is a special case. Let different groups use differ-ent  X  values. Suppose group i uses randomization parameter  X  i . Thus, from the construction of the complete binary tree, we have the following recursion: And (suppose 0 &lt; X &lt; 1 , we know when  X  = 0or  X  = 1, matrix A is obviously invertible.) Thus, det ( A i + 1 ) = 0 if and only if  X  i + 1 = 0 . 5or det ( A i ) = 0.
 We can easily prove by induction that det ( A m ) = 0 if and only if  X  i = 0 . 5 ,  X  1  X  i  X  m . Appendix C: Proof of Theorem 3 Let R be a record in D and R satisfies F ( i ) . There are many ways that R can be transformed (using our randomization scheme) to a record that satisfies exactly j sub-patterns. We use a general way to describe all those alternatives.
 Let R t represent the set of the sub-patterns from e 1 ,..., e m that are satisfied by R ,andlet R f represent the set of the sub-patterns from e 1 ,..., e m that are not satisfied by R . Because R satisfies F ( i ) ,wehave | R t |= i and | R f |= m  X  i .

To form j satisfied sub-patterns, we can select k ( k ranges from 0 to j ) sub-patterns from and keep the rest of the sub-patterns in R f unchanged. The probability of achieving this is  X  k ( 1  X   X ) j  X  k  X  ( m  X  i )  X  ( j  X  k ) . Combining the above results together, we have References Author Biographies
