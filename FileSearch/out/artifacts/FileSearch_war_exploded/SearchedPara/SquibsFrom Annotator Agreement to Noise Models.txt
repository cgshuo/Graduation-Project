 Northwestern University Northwestern University thatissufficientlynoise-freewithhighconfidence.Unlessappropriatelyreinterpreted,agreement the annotated material. Amathematical framework is developed that allows estimation of the 1. Introduction
By and large, the reason a computational linguist engages in an annotation project is to build a reliable data set for the eventual testing, and possibly training, of an algorithm performing the task. Hence, the crucial question regarding the annotated data set is whether it is good for benchmarking.
 value of an inter-annotator agreement coefficient such as the  X  statistic (Cohen 1960;
Siegel and Castellan 1988; Carletta 1996). If agreement is high, the whole of the data set is good for training and testing; the remaining disagreements are typically adjudicated by an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju,
Badulescu, and Moldovan 2006) or through discussion (Litman, Hirschberg, and Swerts 2006), or, in case of more than two annotators, the majority label is chosen (Vieira and
Poesio 2000). 1 There are some studies where cases of disagreement were removed from test data (Markert and Nissim 2002; Dagan, Glickman, and Magnini 2006). If agreement is low, the whole data set is discarded as unreliable. The threshold of acceptability seems to have stabilized around  X  = 0.67 (Carletta 1996; Di Eugenio and Glass 2004). reflects the quality of the data for benchmarking purposes. We develop a model of an-notation generation that allows estimation of the level of noise in a specially constructed gold standard. A gold standard with a noise figure supports cautious benchmarking, by requiring that the performance of an algorithm be better than baseline by more than that which can be attributed to noise. Articulating an annotation generation model also allows us to shed light on the information  X  can contribute to benchmarking. 2. Annotation Noise
We are interested in finding out which parts of the annotated data are sufficiently reliable. This question presupposes a division of instances into two types: reliable and unreliable, or, as we shall call them, easy and hard , under the assumption that items that are easy are reliably annotated, whereas items that are hard display con-fusion and disagreement. The plausibility of separation into easy and hard instances is supported by researchers conducting annotation projects:  X  X ith many judgments that characterize natural language, one would expect that there are clear cases as well as borderline cases that are more difficult to judge X  (Wiebe, Wilson, and Cardie 2005, page 200).
 number of instances, k the number of annotators, and X ij stance by the j th annotator. An annotation generation model assigns a functional form to the joint distribution conditioned on the latent variable have been studied in biometrics (Aickin 1990; Hui and Zhou 1998; Albert, McShane, and Shih 2001; Albert and Dodd 2004). The main assumption is that, conditioned on the type, annotators agree on easy instances and independently flip a coin on hard ones. The joint distribution satisfies: only settled, trustworthy judgments. 2 The problem is that the fact of being easy or hard is not directly observable, but has to be inferred from the observed annotations.
In particular, some of the observed agreements will in fact be hard instances, since with a given degree of confidence (  X  ), the proportion  X  of hard instances in the agreed annotations, based on the number of observed disagreements. The value of  X  is the level of annotation noise in the gold standard comprising agreed annotations.
 classification task: assumed to be labeled by coin-flips. Let B h be the event that there are overall h hard 496 instances; some of these may be unobserved as they surface as random agreements. We note that P ( A d | B h ) = h d  X  (1  X  p ) d  X  p h  X  d for d Let X be a random variable designating the number of coin-flips. It follows that Let t 0 be the smallest integer for which P ( X &gt; t 0 | agreements, we estimate the noise level of the agreed subset of the annotations as at most  X  = t 0 3. Relation to  X  Statistic 3.1 The Case o fHigh  X  with Two Annotators
Suppose 1,000 instances have been annotated by two people, such that 900 are instances of agreement. Both in the 900 agreed instances and in the 100 disagreed ones, the categories were estimated to be equiprobable for both annotators.  X  =0.8, 4 which is usually taken to be an indicator of sufficiently agreeable guidelines, and, by implication, of a high quality data set. Our candidate gold standard is the 900 instances of agreement. What is its 95% confidence noise rate? We find, using our model, that with more than 5% probability up to 125 agreements are due to coin-flipping, hence  X  = 13.8%. 5 This scenario is not hypothetical. In Poesio and Vieira (1998) Experiment 1, the classification of definite descriptions into Anaphoric-or-Associative versus Unfa-miliar has n = 992, d = 121, p = 0.47, which, with 95% confidence, yields  X  = 15%. many disagreements could we tolerate, so that the agreed part is 95% noise-free with 95% confidence? Only 33 disagreements, corresponding to  X  = 0.93. In practice, this means that a two-annotator project of this size is unlikely to produce a high-quality gold standard, the high  X  notwithstanding. 3.2 The Case o fLow  X  with Five Annotators
Suppose now 1,000 instances are annotated by five people, with 660 agreements. With categories equiprobable in both hard and easy instances, p = 0.0625. The exact value of  X  depends on the distribution of votes in the 340 disagreed cases, from  X  =0.73 when all disagreements are split 4-to-1, to  X  = 0.52 when all disagreements are split 3-to-2. Assuming disagreements are coin-flips, the most likely measurement would be about  X  = 0.637, where the 340 observed coin-flips yielded the most likely pattern. value of  X  is considered low, yet the 660 agreed items make a gold standard within the noise rate of  X  = 5% with 95% confidence, according to our model. Hence it is possible for the overall annotation to have low-ish  X  , but the agreement of all five annotators, if observed sufficiently frequently, is reliable, and can be used to build a clean gold standard. 3.3 Interpreting the  X  Statistic in the Annotation Generation Model
The  X  statistic is defined as  X  = P A agreement expected by chance, calculated from the marginals. We use the Siegel and Castellan (1988) version, referred to as K in Artstein and Poesio (2008): where n is the number of items; m is the number of categories; k is the number of anno-that all annotators flip the same coin on hard instances, and that the distribution of the categories in easy and hard instances is the same and is given by q probability for chance agreement between two annotators is q = an estimator. Agreement on a particular instance P A i is measured by the proportion of agreeing pairs of annotators out of all such pairs, and P expected agreement across all instances. Our model assumes perfect agreement on easy instances and agreement with probability q on hard ones, so we expect to see e + q agreed instances, hence P A is an estimator of e + qh e + h is an estimator of shows that  X  is very close to this ratio when the marginal distribution over the categories is uniform, with a more substantial divergence for skewed category distributions. why  X  is not a sufficient indicator of data quality for benchmarking. For when  X  =0.8, 20% of the data are hard cases. Using all data, especially for testing, is thus potentially hazardous, and the crucial question is: Can we zero in on the easy instances effectively, without admitting much noise? This is exactly the question answered by the model. uniform,  X  can be used to address this question as well. Recall that in the two-annotator case in Section 3.1,  X  = 0.8, that is, 80% of instances are estimated to be easy. Because easy cases are a subset of agreed ones in our model, 800 of the agreed 900 instances are easy, giving an estimate of 11% noise in the gold standard. Requiring 95% confidence in noise estimation, we found  X  = 13.8%, using our model. Similarly, in the five-annotator 498 scenario in Section 3.2,  X  = 0.637 tells us that about 637 out of 1,000 instances are easy; they are captured quite precisely by the 660 agreements, yielding a noise estimate of 3.5%, again somewhat lower than the high confidence one we gave using the model. 4. Training and Testing in the Presence o fAnnotation Noise
We discuss two uses of a gold standard within the benchmarking enterprise. The data for training as well. We consider each case separately in the following sections. 4.1 Testing with Annotation Noise
The two questions one wants to answer using the data are: How well does an algorithm capture the phenomenon? For any two algorithms, which one is better? Consider the algorithm comparison situation. Suppose we have a gold standard with L items of which up to R are noise (  X  = R L ). Two algorithms might differ in performance on the easy cases, the hard ones, or both. Because we cannot distinguish between easy and hard instances in the gold standard, we are unable to attribute the difference in performance correctly. Moreover, as the annotations of the hard instances are random coin-flips, there is an expected difference in performance that is a result of pure chance. on the hard ones is as good as agreement-by-coin-flipping would allow. Thus, the difference in the number of  X  X orrect X  answers on hard instances for algorithms A and
B is a random variable S satisfying S = R i = 1 X i where X identically distributed random variables which obtain values and 1 (A  X  X rong X , B  X  X ight X ) with probability 1 4 and 0 with probability  X 
S = R 2 . By Chebyshev X  X  inequality Pr ( between the algorithms will be within 4.5  X  with 95% probability. 900 and R = 125, hence a difference of up to 35  X  X orrect X  answers (3.9% of the gold standard) can be attributed to chance. 10 tant to report the noise rate of the data set that has been produced. This would allow calibrating the benchmarking procedure by requiring the difference between the two competing algorithms to be larger than the chance difference scale.
 by Reidsma and Carletta (2008). They showed that a machine-learning classifier is sensitive to the type of noise in the data. Specifically, if the noise is in the form of category over-use (an annotator disproportionately favors a certain category), when algorithm performance is measured against the noisy data, accuracy estimates are often inflated relative to performance on the real data, uncorrupted by noise (see Figure 3(b) therein). This is because  X  X hen the observed data is used to test performance, some of the samples match not because the classifier gets the label right, but because it overuses the same label as the human coder X  (Reidsma and Carletta 2008, page 232). On the other hand, if disagreements are random classification noise (the label of any instance can be flipped with a certain probability), a performance estimate based on observed data would often be lower than performance on the real data, because the noise that corrupted it was ignored by the classifier (see Figure 2(d) therein).
 investigate the patterns of disagreements between annotators to gain insight into the po-tential of incorrect performance estimation. Although we agree on the general point that human agreements and disagreements should bear directly on the practice of estimating the performance of an algorithm, we focus on improving the quality of performance estimation. We suggest (1) mitigating the effect of annotation noise on performance with agreed items; (2) providing an estimate of the level of noise in the gold standard, which can be used to gauge the divergence between the estimate of performance using the gold standard from the real performance figure on the easy instances (i.e., on noise-free data), similarly to the algorithm comparison scenario provided herein. 4.2 Learning with Annotation Noise
The problem with noise in the training data is the potential for misclassification of easy instances in the test data as a result of hard instances in the training data, the problem we call hard case bias .
 However, annotation noise is different from existing well-understood noise models.
Specifically, random classification noise, where each instance has the same probability of
Cohen 1997; Reidsma and Carletta 2008). In annotation noise, coin-flipping is confined to hard instances, which should not be assumed to be uniformly distributed across the feature space. Indeed, there is reason to believe that they form clusters; certain feature combinations tend to give rise to hard instances. The finding reported by Reidsma and op den Akker (2008) that a classifier trained on data from one annotator tended to agree much better with test data from the same annotator than with that of another annotator exemplifies a situation where observed hard cases (i.e., cases where the annotators disagree) constitute a pattern in the feature space that a classifier picks up. tation noise (Beigman and Beigman Klebanov 2009). We show that the 0-1 loss model may be vulnerable to annotation noise for small data sets, but becomes increasingly robust the larger the data set, with worst-case hard case bias of  X  ( that learning with the popular voted-perceptron algorithm (Freund and Schapire 1999) could suffer a constant rate of hard case bias irrespective of the size of the data set. 5. Discussion 5.1 The Status o fHard Instances
We suggested that only the easy instances should be taken into the gold standard. This is not to say that hard cases should be eliminated from the researcher X  X  attention; we merely argue that they should not be used for testing algorithms for benchmarking 500 purposes. Hard cases are interesting for theory development, because this is where the theory might have a difficulty, but they do not allow for a fair comparison, as their correct label cannot be determined under the current theory. The agreed data embodies the well-articulated parts of the theory, which are ready for deployment as a gold standard for machine learning. Once the theory is improved to a stage where some of the previously hard cases receive an unproblematic treatment, those items can be added to the data set, which can make the task more challenging for the machine. Linguistic theories-in-the-making can have limited coverage; they do not immediately attain the status of medical conditions, for example, where there presumably exists a true label even for the hardest-to-diagnose cases. 11 5.2 Plausibility o fthe Model
Beyond the separation into easy and hard instances, our model prescribes certain an-notator behavior for each type. In our work on metaphor, we observed that certain metaphor markups were retracted by their authors, when asked after 4 X 8 weeks to revisit the annotations (Beigman Klebanov, Beigman, and Diermeier 2008). These were apparently hard cases, with people resolving their doubts inconsistently on the two occasions; coin-flipping is a reasonable first-cut model for such cases. The model also accommodates category over-use bias (Di Eugenio and Glass 2004; Artstein and Poesio 2008; Reidsma and Carletta 2008), as P ( X ij = b j | l i is more than one degree of hardness, and annotator behavior changes accordingly.
Another extension is modeling imperfect annotators, allowed to commit random errors on easy cases; this extension would be needed if a large number of annotators is used. should clearly be put on the community X  X  research agenda. The main contribution of the simple model is in outlining the trajectory from agreement to gold standard with a noise estimate, and indicating the potential benefit of the latter to data uti-lization (low overall agreement does not preclude the existence of a reliable subset) and to prudent benchmarking. Furthermore, the simple model helps us improve the understanding of the information provided by the  X  statistic, and to appreciate its limitations. It also allows us to see the benefit of adding annotators, as discussed in the next section. 5.3 Adding Annotators
If we want the test data to be able to detect small advances in machines X  handling of the task, we need to produce gold standards with low noise levels. The level of noise in agreed data depends on two parameters: (a) the number of agreed items, and (b) the probability of chance agreement between annotators. Although the first is not under the researcher X  X  control once the data set is chosen, the second is, by changing the number of annotators. Obviously, the more annotators are required to agree, the lower p will be, and the smaller the number of agreements that can be attributed to coin-flipping. If indeed 800 out of 1,000 items are easy, agreement between two annotators can only detect them with up to 13.8% noise. Adding a third annotator means p = 0.25.
We are most likely to observe 850 agreed instances, which would not contain more than 7.7% noise, with 95% confidence. Effectively, we got rid of about half the random agreements.
 Acknowledgments References 502
