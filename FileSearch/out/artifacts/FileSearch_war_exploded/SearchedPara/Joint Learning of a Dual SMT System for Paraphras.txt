 Paraphrasing (at word, phrase, and sentence levels) is a procedure for generating alternative expressions with an identical or similar meaning to the origi-nal text. Paraphrasing technology has been applied in many NLP applications, such as machine trans-lation (MT), question answering (QA), and natural language generation (NLG).
As paraphrasing can be viewed as a transla-tion process between the original expression (as in-put) and the paraphrase results (as output), both in the same language, statistical machine transla-tion (SMT) has been used for this task. Quirk et al. (2004) build a monolingual translation system us-ing a corpus of sentence pairs extracted from news articles describing same events. Zhao et al. (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applica-tions (Zhao et al., 2009). Performance of the mono-lingual MT-based method in paraphrase generation is limited by the large-scale paraphrase corpus it re-lies on as the corpus is not readily available (Zhao et al., 2010).

In contrast, bilingual parallel data is in abundance and has been used in extracting paraphrase (Ban-nard and Callison-Burch, 2005; Zhao et al., 2008b; Callison-Burch, 2008; Kok and Brockett, 2010; Kuhn et al., 2010; Ganitkevitch et al., 2011). Thus researchers leverage bilingual parallel data for this task and apply two SMT systems (dual SMT system) to translate the original sentences into another pivot language and then translate them back into the orig-inal language. For question expansion, Dubou  X  e and Chu-Carroll (2006) paraphrase the questions with multiple MT engines and select the best paraphrase result considering cosine distance, length, etc. Max (2009) generates paraphrase for a given segment by forcing the segment being translated independently in both of the translation processes. Context features are added into the SMT system to improve trans-lation correctness against polysemous. To reduce the noise introduced by machine translation, Zhao et al. (2010) propose combining the results of multiple machine translation engines X  by performing MBR (Minimum Bayes Risk) (Kumar and Byrne, 2004) decoding on the N-best translation candidates.
The work presented in this paper belongs to the pivot language method for paraphrase genera-tion. Previous work employs two separately trained SMT systems the parameters of which are tuned for SMT scheme and therefore cannot directly op-timize the paraphrase purposes, for example, opti-mize the diversity against the input. Another prob-lem comes from the contradiction between two cri-teria in paraphrase generation: adequacy measuring the semantic equivalency and paraphrase rate mea-suring the surface dissimilarity. As they are incom-patible (Zhao and Wang, 2010), the question arises how to adapt between them to fit different applica-tion scenarios. To address these issues, in this paper, we propose a joint learning method of two SMT sys-tems for paraphrase generation. The jointly-learned dual SMT system: (1) Adapts the SMT systems so that they are tuned specifically for paraphrase gener-ation purposes, e.g., to increase the dissimilarity; (2) Employs a revised BLEU score (named iBLEU , as it X  X  an input-aware BLEU metric) that measures ad-equacy and dissimilarity of the paraphrase results at the same time. We test our method on NIST 2008 testing data. With both automatic and human eval-uations, the results show that the proposed method effectively balance between adequacy and dissimi-larity. We focus on sentence level paraphrasing and lever-age homogeneous machine translation systems for this task bi-directionally. Generating sentential para-phrase with the SMT system is done by first trans-lating a source sentence into another pivot language, and then back into the source. Here, we call these two procedures a dual SMT system. Given an En-glish sentence e s , there could be n candidate trans-lations in another language F , each translation could have m candidates { e 0 } which may contain potential paraphrases for e s . Our task is to locate the candi-date that best fit in the demands of paraphrasing. 2.1 Joint Inference of Dual SMT System During the translation process, it is needed to select a translation from the hypothesis based on the qual-ity of the candidates. Each candidate X  X  quality can be expressed by log-linear model considering dif-ferent SMT features such as translation model and language model.

When generating the paraphrase results for each source sentence e s , the selection of the best para-phrase candidate e 0  X  from e 0  X  C is performed by: where { f } is the set of sentences in pivot language translated from e s , h m is the m th feature value and  X  m is the corresponding weight. t is an indicator function equals to 1 when e 0 is translated from f and 0 otherwise.
 The parameter weight vector  X  is trained by MERT (Minimum Error Rate Training) (Och, 2003). MERT integrates the automatic evaluation metrics into the training process to achieve optimal end-to-end performance. In the joint inference method, the feature vector of each e 0 comes from two parts: vec-tor of translating e s to { f } and vector of translating { f } to e 0 , the two vectors are jointly learned at the same time: (  X  where G is the automatic evaluation metric for para-phrasing. S is the development set for training the parameters and for each source sentence several hu-man translations r s are listed as references. 2.2 Paraphrase Evaluation Metrics The joint inference method with MERT enables the dual SMT system to be optimized towards the qual-ity of paraphrasing results. Different application scenarios of paraphrase have different demands on the paraphrasing results and up to now, the widely mentioned criteria include (Zhao et al., 2009; Zhao et al., 2010; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011): Semantic adequacy, fluency and dissimilarity. However, as pointed out by (Chen and Dolan, 2011), there is the lack of automatic met-ric that is capable to measure all the three criteria in paraphrase generation. Two issues are also raised in (Zhao and Wang, 2010) about using automatic metrics: paraphrase changes less gets larger BLEU score and the evaluations of paraphrase quality and rate tend to be incompatible.

To address the above problems, we propose a met-ric for tuning parameters and evaluating the quality of each candidate paraphrase c : iBLEU ( s,r s ,c ) =  X BLEU ( c,r s ) where s is the input sentence, r s represents the ref-erence paraphrases. BLEU ( c,r s ) captures the se-mantic equivalency between the candidates and the references (Finch et al. (2005) have shown the ca-pability for measuring semantic equivalency using BLEU score); BLEU ( c,s ) is the BLEU score com-puted between the candidate and the source sen-tence to measure the dissimilarity.  X  is a parameter taking balance between adequacy and dissimilarity, smaller  X  value indicates larger punishment on self-paraphrase. Fluency is not explicitly presented be-cause there is high correlation between fluency and adequacy (Zhao et al., 2010) and SMT has already taken this into consideration. By using iBLEU , we aim at adapting paraphrasing performance to differ-ent application needs by adjusting  X  value. 3.1 Experiment Setup For English sentence paraphrasing task, we utilize Chinese as the pivot language, our experiments are built on English and Chinese bi-directional transla-tion. We use 2003 NIST Open Machine Transla-tion Evaluation data (NIST 2003) as development data (containing 919 sentences) for MERT and test the performance on NIST 2008 data set (containing 1357 sentences). NIST Chinese-to-English evalua-tion data offers four English human translations for every Chinese sentence. For each sentence pair, we choose one English sentence e 1 as source and use the three left sentences e 2 , e 3 and e 4 as references.
The English-Chinese and Chinese-English sys-tems are built on bilingual parallel corpus contain-ing 497,862 sentences. Language model is trained on 2,007,955 sentences for Chinese and 8,681,899 sentences for English. We adopt a phrase based MT system of Chiang (2007). 10-best lists are used in both of the translation processes. 3.2 Paraphrase Evaluation Results The results of paraphrasing are illustrated in Table 1. We show the BLEU score (computed against ref-erences) to measure the adequacy and self-BLEU (computed against source sentence) to evaluate the dissimilarity (lower is better). By  X  X o Joint X , it means two independently trained SMT systems are employed in translating sentences from English to Chinese and then back into English. This result is listed to indicate the performance when we do not involve joint learning to control the quality of para-phrase results. For joint learning, results of  X  from 0.7 to 1 are listed.

From the results we can see that, when the value of  X  decreases to address more penalty on self-paraphrase, the self-BLEU score rapidly decays while the consequence effect is that BLEU score computed against references also drops seriously. When  X  drops under 0.6 we observe the sentences become completely incomprehensible (this is the reason why we leave out showing the results of  X  un-der 0.7). The best balance is achieved when  X  is be-tween 0.7 and 0.9, where both of the sentence qual-ity and variety are relatively preserved. As  X  value is manually defined and not specially tuned, the exper-iments only achieve comparable results with no joint learning when  X  equals 0.8. However, the results show that our method is able to effectively control the self-paraphrase rate and lower down the score of self-BLEU, this is done by both of the process of joint learning and introducing the metric of iBLEU to avoid trivial self-paraphrase. It is not capable with no joint learning or with the traditional BLEU score does not take self-paraphrase into consideration. Human evaluation results are shown in Table 2. We randomly choose 100 sentences from testing data. For each setting, two annotators are asked to give scores about semantic adequacy, fluency, vari-ety and overall quality. The scales are 0 (meaning changed; incomprehensible; almost same; cannot be used), 1 (almost same meaning; little flaws; con-taining different words; may be useful) and 2 (same meaning; good sentence; different sentential form; could be used). The agreements between the anno-tators on these scores are 0.87, 0.74, 0.79 and 0.69 respectively. From the results we can see that human evaluations are quite consistent with the automatic evaluation, where higher BLEU scores correspond to larger number of good adequacy and fluency la-bels, and higher self-BLEU results tend to get lower human evaluations over dissimilarity.

In our observation, we found that adequacy and fluency are relatively easy to be kept especially for short sentences. In contrast, dissimilarity is not easy to achieve. This is because the translation tables are used bi-directionally so lots of source sentences X  fragments present in the paraphrasing results.
We show an example of the paraphrase results under different settings. All the results X  sentential forms are not changed comparing with the input sen-tence and also well-formed. This is due to the short length of the source sentence. Also, with smaller value of  X  , more variations show up in the para-phrase results. 4.1 SMT Systems and Pivot Languages We have test our method by using homogeneous SMT systems and a single pivot language. As the method highly depends on machine translation, a natural question arises to what is the impact when using different pivots or SMT systems. The joint learning method works by combining both of the processes to concentrate on the final objective so it is not affected by the selection of language or SMT model.

In addition, our method is not limited to a ho-mogeneous SMT model or a single pivot language. As long as the models X  translation candidates can be scored with a log-linear model, the joint learning process can tune the parameters at the same time. When dealing with multiple pivot languages or het-erogeneous SMT systems, our method will take ef-fect by optimizing parameters from both the forward and backward translation processes, together with the final combination feature vector, to get optimal paraphrase results. 4.2 Effect of iBLEU iBLEU plays a key role in our method. The first part of iBLEU , which is the traditional BLEU score, helps to ensure the quality of the machine translation results. Further, it also helps to keep the semantic equivalency. These two roles unify the goals of optimizing translation and paraphrase ade-quacy in the training process.

Another contribution from iBLEU is its ability to balance between adequacy and dissimilarity as the two aspects in paraphrasing are incompatible (Zhao and Wang, 2010). This is not difficult to explain be-cause when we change many words, the meaning and the sentence quality are hard to preserve. As the paraphrasing task is not self-contained and will be employed by different applications, the two mea-sures should be given different priorities based on the application scenario. For example, for a query expansion task in QA that requires higher recall, va-riety should be considered first. Lower  X  value is preferred but should be kept in a certain range as sig-nificant change may lead to the loss of constraints presented in the original sentence. The advantage of the proposed method is reflected in its ability to adapt to different application requirements by ad-justing the value of  X  in a reasonable range. We propose a joint learning method for pivot language-based paraphrase generation. The jointly learned dual SMT system which combines the train-ing processes of two SMT systems in paraphrase generation, enables optimization of the final para-phrase quality. Furthermore, a revised BLEU score that balances between paraphrase adequacy and dis-similarity is proposed in our training process. In the future, we plan to go a step further to see whether we can enhance dissimilarity with penalizing phrase tables used in both of the translation processes.
