 Rank correlation statistics are useful for determining whether a there is a correspondence between two measurements, par-ticularly when the measures themselves are of less interest than their relative ordering. Kendall X  X   X  in particular has found use in Information Retrieval as a  X  X eta-evaluation X  measure: it has been used to compare evaluation measures, evaluate system rankings, and evaluate predicted perfor-mance. In the meta-evaluation domain, however, correla-tions between systems confound relationships between mea-surements, practically guaranteeing a positive and signifi-cant estimate of  X  regardless of any actual correlation be-tween the measurements. We introduce an alternative mea-sure of distance between rankings that corrects this by ex-plicitly accounting for correlations between systems over a sample of topics, and moreover has a probabilistic interpre-tation for use in a test of statistical significance. We validate our measure with theory, simulated data, and experiment. Categories and Subject Descriptors : H.3 [ Information Storage and Retrieval ]; H.3.4 [ Systems and Software ]: Performance Evaluation General Terms: Experimentation, Measurement Keywords: information retrieval, evaluation, rank correla-tion, distance measure Ranking is a ubiquitous problem in Information Retrieval. Retrieval systems rank documents by estimated relevance; evaluations like the Text Retrieval Conference (TREC) rank systems by evaluation measures; systems rank queries by predicted difficulty; users rank systems by preference. Eval-uating a ranking requires some measure of comparison be-tween two rankings. In the case of ranking documents, there are a wide variety of evaluation measures that implicitly compare a ranking of documents to a perfect ranking; for the other tasks listed above, rank correlation measures, in particular Kendall X  X   X  , have become de facto standards. Copyright 2009 ACM 978-1-60558-483-6/09/07 ... $ 5.00.
Kendall X  X   X  is appealingly intuitive: given two different rankings of the same m items, count the number of pairs that are concordant X  X n the same order in both rankings X  and discordant X  X n reverse order. If P is the concordance count and Q the discordance count,  X  ranges from -1 to 1, with 1 meaning the two rankings are identical and -1 meaning one is in reverse of the other. A  X  of 0 means that 50% of the pairs are concordant and 50% discordant. Every value of  X  maps directly to a percentage of concordant pairs (assuming no ties).

Maurice Kendall introduced  X  in 1938 and subsequently developed much of its theory in his 1948 monograph Rank Correlation Methods [7]. As Kendall explains, rank correla-tion methods are ideally suited to situations where measure-ments are subjective or difficult in practice. However, corre-lation measures are meaningful when samples are drawn in-dependently and with identical sampling distributions (i.i.d.). When samples are not i.i.d., the interpretation becomes un-clear: correlation between items in a sample can confound correlation in the measurements. Suppose, for example, we want to measure the rank correlation between two evalu-ation measures. If we calculate  X  between rankings of a system that is nearly perfect, a middle-of-the-road system, and a system with a fatal bug, it does not matter how sim-ilar the measures are. If they capture anything at all about performance, they are guaranteed to be highly correlated.
Some recent work has revealed strange behavior by Ken-dall X  X   X  when comparing rankings of systems [2, 8, 11, 13]. At a high level, much of this can be explained by correla-tions between systems: when they are ranking the same doc-uments for the same topics, they will be so highly correlated that it is unclear whether  X  has any meaning whatsoever. In Section 2 we explain the high-level problem in more detail, along with other hurdles to interpreting a  X  correlation.
If our samples are not independent and identically dis-tributed, a measure of distance between rankings should take into account the likelihood of the particular rankings. In Section 3 we develop such a measure, along with a signifi-cance test for the hypothesis that two rankings are the same. In Section 4 we present theoretical results about our mea-sure and compare our rank distance measure to Kendall X  X   X  over random rerankings as well as some classic experiments from IR evaluation studies. (c)  X  = 0 . 89; d rank = 3 . 52 .
Kendall X  X   X  and other rank correlation statistics make par-ticular assumptions that result in unclear interpretation in some of the situations they are used in information retrieval. In particular, its use in meta-evaluation studies carries some significant problems. Consider Figure 1, which shows 24 re-trieval systems submitted to the TREC 2007 Million Query Track [1] sorted by increasing MAP over 149 topics (TREC topics 701-850). Note that there are  X  X ins X  of roughly equal-quality systems. Four reorderings are shown. Visual inspec-tion suggests they are getting better going from left to right: Figure 1(a) shows systems swapped between bins (in partic-ular the eighth-ranked system has moved up to rank 12); Figure 1(b) shows smaller between-bin swaps; Figure 1(c) shows no between-bin swaps but some big within-bin swaps; and Figure 1(d) shows only small (visually almost impercep-tible) within-bin swaps. Kendall X  X   X  cannot pick up on any difference between these: each of the four rankings has a  X  of 0 . 9  X  0 . 01 with the original ranking. This highlights the first, and most obvious, problem with Kendall X  X   X  and other rank correlation statistics: they treat all pairwise swaps equally, even when the probability of a swap is low.

A related but more subtle problem is that Kendall X  X   X  treats all swaps as statistically independent : it assumes that if there is no correlation, then knowing that systems i and k swapped says nothing about whether j and k swapped. This is certainly not true of retrieval systems; for example, if systems i and j are very similar to one another, a swap between j and k is much more likely given a swap between i and k . Intuitively, very similar systems will tend to  X  X tick together X . The result of this is that  X  has a very high base-line even when there is no correlation, much higher than the  X  = 0 that it is supposed to have for uncorrelated vari-ables. This is nicely illustrated by Soboroff et al., who show that correlations around 0.4 X 0.5 are achieved even when rel-evance is assigned to retrieved documents randomly [13].
The variance in  X  over a sample of systems is seldom dis-cussed. A sample of m systems with correlation  X  between two measurements has 95% confidence interval [7]: This means that with 25 systems that have a  X  of 0.9, the true correlation is somewhere between 0 . 389 and 0 . 987 with 95% confidence. This is an unacceptably large range for the way  X  is used in IR. In general the variance is of order p 2 /m , and it is not easy to reduce it by sampling additional systems. Contrast this to measures like mean average pre-cision calculated over a set of n topics: the variance in the mean is of order p 1 /n , and in principle (if not in practice) we can reduce variance by using more topics.

We  X  X now X  the variance is actually much less than this because we consistently see similar  X  correlations in meta-evaluation studies and between different evaluation mea-sures. These observations are another consequence of the systems not being independent. We argue that the sample space we are actually interested in is the topic space: given that two rankings of systems are correlated over a particular set of topics, would they still be correlated, and would the correlation be as high, if run over a different set?
To summarize, the problems with Kendall X  X   X  (or any rank correlation statistic) in meta-evaluation studies are: 1. all pairs are treated equally; 2. pairs are assumed statistically independent; 3. sample space orthogonal to the space of interest; 4. high variance over the system sample space.
 Taken together, these make  X  difficult to use for anything but the most high-level approximation of rank similarity.
The first problem we discuss above X  X he fact that  X  treats all pairs equally X  X as probably been noticed by anyone who has used  X  to evaluate a ranking of systems. Nearly all the proposed alternatives to  X  and other rank correlation measures address this problem.

Cormack and Lynam proposed estimating power and bias of pairwise system comparisons by effectively calculating  X  over only pairs with significant differences [5] (Sakai inde-pendently did something similar, naming it  X  X iscriminative power X  [10]). Yilmaz et al. proposed a  X  X op-heavy X  vari-ant that uses average precision-like weights to make high-er-ranked systems more important [17]. Melucci proposed that  X  be replaced by a different rank correlation statis-tic, Kolmogorov-Smirnov X  X  D [8]. None of these address the problems of independence or of the sample space.

Discriminative power is the only one to take into account variance over the topic space (via its determination of sta-tistical significance). It does not, however, eliminate the system space as a source of variance. Moreover, because the pairs that are significantly different are the ones most likely to be agreed upon, and because there is even greater corre-lation among significant pairs than randomly-chosen pairs, discriminative power will be very high by default. Taking a very high baseline together with wide confidence intervals results in a statistic for which the upper bound (1.0) is al-most always within the standard error.
We consider discriminative power to be a step in the right direction: it takes the topic sample into account, and makes use of differences between systems in the form of paired sig-nificance tests. It is simply not a big enough step. Instead of comparing each pair of systems independently, we must compare all the systems simultaneously. We must eliminate any variance due to systems, since it is likely impossible to have anything resembling a random sample of systems. Fi-nally, we must be able to test hypotheses about rankings over a sample of topics.

Our goal, therefore, is to develop a measure of rank simi-larity with the following properties: 1. penalizes swaps between items that are very different 2. penalizes swaps between items conditional on swaps 3. assumes a fixed population of systems with a random Number 1 ensures that pairs are not all treated equally. Number 2 explicitly incorporates correlations among pairs. Number 3 corrects the sample space. The next section de-scribes our measure.
One way to accomplish all three criteria above is to es-timate the probability of observing a particular alternative ranking of systems given a baseline ranking based on mea-surements of system results over a sample of topics. This is what our rank distance measure does. We will present the measure first, then justify it with examples and analogies.
Suppose we have an n  X  m matrix X of average precision values for m systems over n topics. 1 Let  X  be a vector of m mean APs over n topics. Let y be a vector of m means of some other evaluation measure that provides an alternative ranking of systems. Reorder  X  and the columns of X in increasing order of y . The rank distance from  X  to y is: where  X   X  is the vector of differences between adjacent val-ues in  X  and  X   X  is the covariance matrix for differences in AP between adjacent columns in X . Essentially, we find a point  X  that has maximum probability under a distribution defined by X subject to the constraint that it preserves the same ranking as y . Algorithm 1 shows pseudocode for set-ting up and solving the minimization problem; more detail is provided below.
Consider the following example: average precision and precision@10 computed for four topics run by three systems.
AP = where rows correspond to topics and columns to systems. The system means are: We use bold letters to denote vectors and matrices. Let us refer to the systems as A,B , and C by column. Clearly system A is much worse than B or C by MAP ; in fact, with only four topics, the differences are significant by a paired t-test ( p &lt; 0 . 05). Systems B and C are similar and not significantly different. System A performs worse by precision@10 as well, though not significantly so. Systems B and C have swapped by mean precision@10.

To make our requirements concrete, we desire a distance measure that penalizes swaps between A,B or A,C much more harshly than swaps between B,C ; penalizes separation of B and C , e.g. placing A in between them; and has variance over the topic sample rather than the system sample.
Consider the Mahalanobis distance [16]: if x and y are vectors drawn from a m -variate normal distribution with mean  X  and covariance matrix  X  , then the Mahalanobis distance between x and y is defined to be: This is not a distance between rankings, as x and y are not lists of ranks. 2 It is, however, inversely proportional to the probability of observing the vector x assuming the true mean is y [6]. Note that with a one-dimensional normal distribu-tion with mean  X  and variance  X  2 , Mahalanobis distance reduces to square of the common Z -statistic ( x  X  y ) 2 / X 
If x and y are vectors of means (i.e. each value in x is the average of measurements to a random sample of n items such as topics), there is an analogue to the Mahalanobis distance in the same way the t -statistic is an analogue to the Z -statistic. It is called Hotelling X  X  T 2 : T 2 is inversely proportional to the probability of observing mean vector x assuming mean vector y is the true mean [6]. In the one-dimensional case, this becomes n ( x  X  y ) 2 / X  ( x  X  y ) 2 / (  X  2 /n ), which should be recognizable as the square of the t -statistic.
 We are interested in probabilities of rankings, so d 2 and T 2 are not directly applicable. But a ranking can be defined in terms of the signs of differences between values. Sort the columns of both matrices in increasing order of p @10 and take the difference between adjacent columns: The system mean differences are: from which we can tell that B &gt; C &gt; A by p @10. The reason for reordering the columns is that it ensures all dif-ferences in mean p @10 will be positive, and therefore that the vector has no ambiguity about the ranking.

This effectively reduces a ranking to a point in ( m  X  1)-dimensional space. Covariance matrices determine which points in that space X  X hat is, which rankings X  X ight have
If we map values in x and y to their respective ranks and set  X  to the m  X  m identity matrix, the resulting Maha-lanobis distance is proportional to Spearman X  X   X  . In other words, ignoring correlations between items produces a com-mon rank correlation statistic. This shows explicitly how a traditional rank correlation statistic ignores dependence. Figure 2: Contours of the bivariate normal distri-Points are the mean, the precision values (0 . 20 , 0 . 05) , and the minimum-distance rank-preserving point (0 . 322 , ) . high or low probability. The covariances are: Figure 2 shows a contour plot of the joint distribution of MAP C  X  MAP A and MAP B  X  MAP C . The point in the cen-ter is the mean, ( . 349 ,  X  . 028). Precision@10 is at ( . 2 , 0 . 05). The innermost contour line is the 2 . 5% confidence interval where T 2 values are lowest; the outermost is the 97 . 5% in-terval (i.e. 97 . 5% of the total probability is inside the outer-most contour). T 2 values get progressively higher for points radiating out from the mean.

Note that there are regions in which every point defines the same relative ordering of systems. In Quadrant I (upper right), we have MAP C &gt; MAP A and MAP B &gt; MAP C , and therefore the ranking is B &gt; C &gt; A . In Quadrant IV (lower right), we have MAP C &gt; MAP A and MAP B &lt; MAP C . The ranking could be B &gt; C &gt; A or B &gt; A &gt; C . The diagonal line separates the two possibilities; note that most of the probability mass is in the C &gt; B &gt; A region, representing the  X  X rue X  ranking by MAP . Quadrants II and III (upper left and lower left, respectively) are where MAP A &gt; MAP
The regions representing rankings in which pairs with sig-nificant differences have been swapped have negligible prob-ability. All of the probability mass is split between two regions in which the ranking is correct: either C &gt; B &gt; A or B &gt; C &gt; A . We want to leverage this property, which holds in general, for our distance measure.

Even though every point in Quadrant I represents an ac-ceptable ranking, and every point in the upper region of Quadrant IV represents the  X  X rue X  ranking, many of these points have very large T 2 from the mean and thus effectively zero probability. The point (0 . 2 ,  X  0 . 1), for instance, repre-sents a correct ranking but is so far outside the contours that it has negligible probability. The point (0 . 20 , 0 . 05), which is the mean of precision@10 in our example, is in an outer contour and thus has relatively low probability despite be-ing an acceptable ranking. In this respect these points are indistinguishable from points that represent entirely wrong rankings. This is a problem that needs to be resolved. Algorithm 1 Calculate d rank from a vector of means y to a vector of means  X  .
 Require: vector y of means, n  X  m matrix X of values, 1: i k  X  the index of the k th-greatest value of y . 4:  X   X   X  ( X  X   X   X   X  )( X  X   X   X   X  ) 0 / ( n  X  1) +  X  I 5:  X   X  QPsolve ( n  X   X   X  1 ,  X   X   X  ). 6: d 2 rank  X  n (  X   X   X   X  )  X   X   X  1 (  X   X   X   X  ). 7: return | p d 2 rank | .

Instead of measuring the distance from a particular point to the mean, we will find a point in the same region X  X .e. an-other point that preserves the ranking X  X hat has minimum distance to the mean. This point is unique in each region. In our example, the minimum-distance rank-preserving point for the differences in mean precision@10 is at (0 . 322 , ), where is an infinitesimal number greater than zero. Being in the inner contours, that point has fairly high probability; its T 2 distance from the mean is 0 . 65, which is quite low. Based on that, we can conclude that the ranking by preci-sion at 10 is not very different from a ranking by MAP for these three systems.

Thus we have our measure in Eq. 2. Given a vector of means y and a matrix of AP or other values X with means  X  , the rank distance from y to  X  is: where  X  is the minimum-distance rank-preserving point. The constraints  X  i &gt; 0 ensure that whatever vector of differences minimizes the distance preserves the ordering of values in y .
Algorithm 1 shows how to set up the problem. At line 2, X .j is the j th column of X , i.e. the values of AP or an-other measure by system j for n topics. Subtracting columns finds the differences in AP between systems that are adja-cent in the ranking by y , exactly as ( m  X  1) paired t -tests would. The covariance matrix  X   X  ensures that the t -tests are not performed independently. When m  X  n , the covari-ance matrix is singular and noninvertible; we work around that case by regularizing  X   X  (line 4). A simple regular-ization procedure adds a constant  X  to the diagonal [15]; we used  X   X  10  X  5 . Since covariance matrices are always positive semidefinite, the formula is convex and solvable in polynomial time using quadratic programming methods [3]. The algorithm assumes a quadratic program solver QP-solve ( A , b 0 ) that minimizes b 0 Ab w.r.t. b s.t. b &gt; b Our unoptimized implementation in R ran in 0 . 003 seconds on average, compared to 0 . 0004 seconds for R  X  X  built-in im-plementation of Kendall X  X   X  .

To conclude this section, consider the following table. ranking ABC ACB BAC CAB BCA CBA  X  -1 -1/3 -1/3 1/3 1/3 1 dsc pow 0 0 1/2 1/2 1 1 d rank 4.88 4.88 4.88 4.88 0.65 0.00
While  X  and discriminative power see differences among the first four rankings, d rank effectively considers them all so bad as to be beyond repair. The first two have swapped both significant pairs by putting A first. The second two have only swapped one significant pair, but have also vio-lated the rule that very similar items should not be sepa-rated. Discriminative power cannot capture this case and thus views these two as better than the former two. The last two, which only swap B and C , are the best, but d rank can make a subtle distinction between them while discrimi-native power cannot. Kendall X  X   X  makes an extreme distinc-tion between them, saying that swapping B and C is as bad as swapping A and B .

Our measure has an interpretation problem: in princi-ple it is unbounded, with larger values meaning greater dis-tances. What is a  X  X ood X  rank distance? Is 0.65 good? In fact, the magnitude of rank distance is highly dependent on the number of systems being ranked, the number of topics they are evaluated over, and the overall interdependence of the systems by the baseline measure. Fortunately, we can avoid worrying about a direct interpretation by introduc-ing a statistical hypothesis test for rank distance with an easy-to-interpret p value.
Statistical hypothesis tests are used to estimate the prob-ability that two measurements are different, or that a mea-surement is different from some value, given a sample of items. In our case, we wish to estimate the probability that d rank is zero given a sample of topics. If it is zero, the rank-ings are identical. If it is far from zero, the rankings are sig-nificantly different. When it is slightly greater than zero due to a few pairwise swaps, the question is whether the swaps are  X  X cceptable X  given the variance in the topic space: if we used a different sample of topics, would we see a similar result? If we could evaluate over the entire population of topics, would the rankings be identical?
Many significance tests are based on knowing the distri-bution of the test statistic under the null hypothesis. For the t -test, the t -statistic has a t distribution. Hotelling X  X  T has an F distribution. Given these distributions, we can de-termine the probability ( p -value) of a particular value of t or T 2 under the null hypothesis; if it is low, we reject the null hypothesis that two values are the same. Our rank distance measure has a distribution defined in terms of the cumula-tive density function of the multivariate normal distribution: the probability of a particular d rank is the total probability in the region corresponding to the ranking induced by y . For example, the p -value of d rank = 0 . 65 in our example above is the integral of the normal density function over all points in Quadrant I. In practice this becomes difficult to compute as m grows, but there is a simple bootstrap sampling procedure we can use to estimate it.

Our approach is similar to that of Sakai [9] and Smucker et al. [12] for pairwise tests. We sample topics with replace-ment, calculate all m system means over those topics, and then calculate d rank from the resampled means to the origi-nal means. Over B trials we gain an empirical distribution of d rank . A bootstrap p -value estimate is simply the pro-portion of the d rank s in our empirical distribution that are no less than a given value d r . If the p -value is sufficiently small, we reject the hypothesis that the rankings are the same. Algorithm 2 shows the details.
 Algorithm 2 Estimate a p -value for a given rank distance d r using bootstrap sampling.
 Require: Rank distance d r with baseline matrix X from 1: for i  X  1 to B do 2: X  X   X  an n  X  m matrix constructed by sampling rows 3:  X   X   X  column means of X  X  . 4: if d rank (  X   X  |  X  , X )  X  d r then S  X  S + 1. 5: end for 6: p  X  S/B .

In our example above, d rank = 0 . 65 with probability 0 . 21 and d rank = 0 with probability 0 . 79 in the bootstrap dis-tribution (with B = 10000 trials). The two rankings are statistically indistinguishable. The other four rankings are so rare as to have empirical probability zero; they are clearly significantly different from the baseline.
We have implemented our measure in C++ and R . The software is able to read in outputs from trec eval -q to construct the matrix X ; a separate input file provides the ranking y to evaluate. Both implementations report the rank distance and a bootstrapped p -value suitable for dis-semination. The bootstrap distribution can be saved to disk so that new p -values can be computed quickly in a series of batch experiments. We have also published complete distribution tables for common TREC datasets and mea-sures; the software and distribution tables can be found at http://ir.cis.udel.edu/ ~ carteret/dRank/ .
Because our rank distance measure is entirely new, evalua-tion is somewhat difficult. We will first provide some simple theoretical results. We then compare d rank to  X  over random rerankings of TREC systems to show how they are different, and re-evaluate past work in meta-evaluation.
We can prove some basic things about our measure that may help understand or accept it. The first result is that the rank distance is zero if and only if two rankings are identical.
Theorem 1. Let y be a real-valued 1  X  m vector and X a real-valued n  X  m matrix with column means  X  . Then d rank ( y |  X  , X ) = 0 if and only if values of y have the same ranking as values in  X  .

Proof. Let  X   X  ,  X   X  be defined as in Alg. 1. Let  X  be the vector that minimizes d rank ( y |  X  , X ).  X  : Suppose the rankings are the same. Since values of  X  are in the same order as values of y , line 3 of Alg. 1 results in a vector  X   X  that is positive in every value. All values of  X  are positive by design. It is then simple to show that because both vectors are all positive, the minimum of (  X   X   X   X  )  X   X   X  1 (  X   X   X   X  ) is zero for any  X   X  .  X  : Suppose d rank ( y |  X  , X ) = 0. Since  X  must be positive, this can only happen if  X   X  &gt; 0 as well. And if  X   X  &gt; 0, the ranking is the same.

The following results shows how our measure goes above and beyond Kendall X  X   X  by penalizing a ranking more when pairs that are  X  X ore different X  are swapped (Thm. 2) and by penalizing a ranking that  X  X reaks up X  a pair of similar systems (Thm. 3). Both theorems use the same setup. Let  X  and  X  induce identical rankings of m systems based on matrices X and Y , respectively. Let y induce an alternative ranking in which system i and i + 1 are swapped, but i + 1 and i + 2 are in the correct order (i.e. y i +1 &gt; y i &gt; y  X  Theorem 2. Let t ( x  X  y ) be Student X  X  paired t -statistic. All else being equal, if t ( X .i  X  X . ( i +1) ) &gt; t ( Y then d rank ( y |  X  , X ) &gt; d rank ( y |  X  , Y ) . The proof follows from the fact that when all else is equal, the difference in the two rank distances reduces to a dif-ference in t -statistics for systems i, ( i + 1) by measure X and i, ( i + 1) by measure Y . If we accept Student X  X  t as a measure of distance (i.e. greater t means less similar), the result follows easily. Note in particular that the smaller the t -statistic, the less different the systems are, and therefore the lower d rank is. The implication is that d rank everything about a ranking that discriminative power does.
Theorem 3. Let  X  ( x , y ) be the Pearson correlation be-tween vectors x and y . All else being equal, if  X  ( X .i X d rank ( y |  X  , X ) &gt; d rank ( y |  X  , Y ) .
 The proof is algebraic, simple to work out though not easy to transcribe; with limited space, we will omit it. The con-sequence of this theorem is that the more similar i + 1 and i + 2 are, the greater the penalty from swapping i to be in between them.
The standard datasets used in evaluation studies are the complete retrieval results of systems submitted to TREC tracks over the years. For most tracks, submitted systems retrieve up to 1,000 documents for each of a set of topics; the data consists of the full ranked list for every topic along with the relevance judgments ( qrels ) for those topics.
We present results with the 2007 Million Query Track runs (TB topic subset; 24 runs over 149 topics) and the 2004 Robust Track runs (110 runs over 249 topics).
Figure 3 shows the ranks that the 24 Million Query Track runs are  X  X llowed X  to appear at given a 0 . 9 threshold for Kendall X  X   X  or the significance threshold for the rank dis-tance (for the MQ runs, the bootstrapped critical value for  X  = 0 . 05 is 2 . 62). In these plots, each ( x = system ,y = rank) point indicates the probability of system x appearing at rank y for all rankings that have  X   X  0 . 9 or d rank  X  2 . 62. Lighter values mean higher probability; points are colored on a logarithmic scale. Note that  X   X  X llows X  each system a distribution over about 5 ranks, centered on its actual rank. Rank distance, on the other hand, constrains some systems to staying where they are (in particular, systems that are significantly different from those they are adjacent to), but others to move considerably. The group of 7 systems num-bered 15 X 21 in particular have nearly free rein (with some constraints) by virtue of there being no significant differ-ences between them.

The fact that statistical significance is not transitive pro-duces some interesting effects. The fourth system is allowed Figure 3: Kendall X  X   X  and d rank constrain the possible rankings of systems in very different ways. On the left, the distribution of possible ranks when  X   X  0 . 9 is identical for each system. On the right, the distri-bution of possible ranks when d rank  X  2 . 62 ( p &gt; 0 . 05 ) does not allow significantly different pairs to swap. by rank distance to appear at ranks 4 through 8, while the fifth is only allowed to appear at ranks 4 through 6. This is because there is a significant differences between systems 5 and 7, but not between 4 and 7.

Kendall X  X   X  and d rank disagree quite a bit on whether a ranking is  X  X ood X . Over two very large samples of re-rankings (one sampled using d rank  X  X  sampling distribution, one sampled using  X   X  X ), over 25% of rankings with  X  &gt; 0 . 9 are significantly different by d rank from the true ranking. Conversely, rankings with  X  s as low as 0 . 8 can still be sta-tistically indistinguishable from the true ranking.
Incidentally, the plot for discriminative power with thresh-old 1, which is not shown, has very nearly the same con-tours as the plot for rank distance, but the blocks within those contours are solidly colored. Discriminative power cannot capture the finer constraints imposed by dependen-cies among systems and allows for instance systems 19 and 20 (which are equal in MAP to four decimal places) to ap-pear at any rank from 15 to 21 with up to 5 systems sepa-rating them. Rank distance insists that these two systems always be ranked next to each other, though they may be in any order relative to one another.
It is well-known that different evaluation measures cor-relate highly. Voorhees and Harman give  X  correlations between rankings by different evaluation measures for the TREC-7 data [14]. What we would really like to know is whether different measures actually measure different sys-tem attributes despite being highly correlated. Since they are highly correlated by default, rank correlations cannot tell us much about differences in system rankings between measures. Our measure can.
 Table 1(a) shows  X  correlations between rankings of 110 Robust systems over 249 topics by traditional TREC evalu-ation measures along with the 95% confidence interval cal-culated by Kendall X  X  formula (1). There is quite a bit of overlap in the confidence intervals.

Table 1(b) shows the rank distances between the same rankings. As explained above, rank distance is not symmet-ric: it depends on which measure is chosen as the  X  X aseline X . This table presents distance from the measure on the row (the baseline) to the measure on the column and the boot-strap p -value of the hypothesis test. Note that these dis-tances are substantially higher than the MQ critical value of 2 . 62 and the maximum distance of 4 . 88 in our example in Section 3.1. The values of d rank depend heavily on the number of systems and topics as well as the degree of inter-dependence between systems. Thus the bootstrap p -value is often better-suited for reporting; this is shown as well. Rankings that are not significantly different are bolded.
The table shows that rankings by bpref and MAP and rankings by bpref and R-prec are indistinguishable from each other; bpref does a very good job of approximating both MAP and R-prec. A ranking by MAP is not signifi-cantly different from a baseline ranking by R-prec, but the converse is not true, probably because MAP is more tightly constrained than R-prec. Overall, because nearly all the rank distances are significant, this table shows that apart from bpref &amp; MAP and bpref &amp; R-prec, different measures really do measure different system attributes, and rankings by different measures can capture systems that are designed to optimize one attribute over another. This is much more enlightening than the  X  table, in which all the values are pos-itive and significant, and whose confidence intervals overlap with each other substantially. Shallow Pools. It is commonly understood that evaluation over shallow judgment pools correlates highly to evaluation over a deep pool. Again, this should not be surprising given what we now know about rank correlation methods.

Table 2 shows  X  correlation and rank distance between a baseline ranking of Robust systems by official MAP and the ranking by MAP calculated over a shallow pool. While  X  reaches nearly 0.9 with a depth 10 pool, the rank distance test rejects the hypothesis that the rankings are the same at that point. It takes a deeper pool before the ranking are statistically indistinguishable.
 Incomplete Judgments. Buckley and Voorhees intro-duced the bpref measure for evaluation with incomplete test collections [4]. To test it, they compared bpref, MAP , R-prec, and P10 over increasingly incomplete sets of relevance judgments. We duplicated their experiment on the Robust collection, calculating d rank and the rank test p -value in ad-dition to  X  between the official ranking of systems by each measure and the ranking of systems by the same measure depth judgments  X  (95% c.i.) d rank , p -value Table 2: Kendall X  X   X  and rank distance to official ranking when evaluating over shallow pools. Bolded rank distances indicate rankings that are not signif-icantly different. calculated over the reduced set. (For full details of the methodology we refer the reader to the original paper.)
Figure 4(a) shows  X  correlation between a measure calcu-lated with a reduced qrels set and the same measure over all the qrels. bpref seems to be more robust to missing judgments, having a higher  X  with very incomplete sets; R-precision seems to be least robust to missing judgments. This agrees with the results of Buckley and Voorhees.
Figure 4(b) tells a slightly different story. This shows rank distance decreasing as the qrels becomes more com-plete. While the relative performance of the four measures is roughly the same, bpref X  X  strong performance with  X  turns out to be somewhat misleading: it requires nearly twice as many judgments to achieve a good d rank (one that is not sig-nificantly different from 0) than a  X  X ood X   X  of 0.9. However, bpref is still more robust to missing judgments than any of the other measures.

Finally, Figure 4(c) shows the p -value of the rank hypoth-esis test increasing as qrels completeness increases. Recall that a low p -value means that the ranking is significantly different; higher p -value means more similar to the  X  X rue X  ranking. Here we see that bpref very quickly  X  X oses X  signif-icance at about 30% qrels. The other three measures lose significance in the interval 0 . 4  X  0 . 7, meaning that more than half the full collection was needed before the rankings were indistinguishable. All four measures very quickly transition from  X  X ignificant X  to  X  X ot significant X ; this suggests there is a sort of  X  X hase transition X  due to the number of judgments, at which point the variance constrains the ranking enough that it is unlikely to change drastically. No Relevance Judgments. Soboroff et al. examined rank-ing systems by, in effect, random judgments [13]. They ran-domly sampled subsets of documents to be labeled relevant. They then used those pseudo-relevance judgments ( X  X seudo-rels X ) to evaluate and rank systems. They found a positive and significant  X  correlation to the true ranking.
 We duplicated the experiment on the Robust systems. Over 50 sets of pseudo-rels that were assembled (sampling from the pool without duplicates), the average rank distance was 26.99 with a standard deviation of 1.58, on the same or-der as the distance between MRR and other measures. The average  X  correlation was 0.656 with a standard deviation of 0.021. The hypothesis that the ranking by pseudo-rels was equivalent to the ranking by the qrels was rejected ( p  X  0). By contrast, the  X  correlation indicated that the ranking was significantly correlated X  X  noninformative result.
We have introduced a new measure of the distance be-tween rankings that solves the problems with using Ken-dall X  X   X  over a sample of correlated items. We have demon-strated its power by analogy to powerful statistical methods like the t -test and Hotelling X  X  T 2 , through theoretical results about relative increases, through direct comparison to Ken-dall X  X   X  over simulated rerankings, and through examples well-known in the IR literature.

We grant that our measure is not intuitive. However, the problems with using Kendall X  X   X  in meta-evaluation are so severe that the intuition it offers is a false hope. At best ,  X  can only provide the loosest guide to interpreting ranking results; at worst it is entirely misleading in both directions: a high  X  does not necessarily indicate a good ranking, and a  X  X ow X   X  is far from a guarantee of a bad ranking.
Furthermore, the assumptions required by our method are much weaker than those required by Kendall X  X   X  . On the surface our analogies to Mahalanobis distance may give the appearance that we are assuming measures like average precision are normally distributed over topics. The actual assumption is substantially weaker: only that the vector of means is normally distributed over samples of n topics. This is a consequence of multivariate generalizations to the Central Limit Theorem, which say that the distribution a sum of random variables converges to normal (regardless of whether the random variables are scalars or vectors). Ken-dall X  X   X  , on the other hand, requires the assumption that systems are independently sampled, which is most certainly not true even in approximation. Furthermore, the failure of  X   X  X  assumption imposes much greater error on the statistic than the failure of any assumption made by d rank .
We believe our method can be adapted to comparing rank-ings of documents as well, and possibly used as an objective function in learning to rank. We will be investigating this further in the future.
