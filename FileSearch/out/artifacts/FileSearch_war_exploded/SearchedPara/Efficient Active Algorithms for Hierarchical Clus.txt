 Akshay Krishnamurthy akshaykr@cs.cmu.edu Sivaraman Balakrishnan sbalakri@cs.cmu.edu Min Xu minx@cs.cmu.edu Aarti Singh aarti@cs.cmu.edu Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213 Clustering is a ubiquitous task in exploratory data analysis, data mining, and several application do-mains. In clustering, we assign each object to one or more groups so that objects in the same group are very similar while objects in di ff erent groups are dissimilar. In a hierarchical clustering, the groups have multiple resolutions, so that a large cluster may be recursively divided into smaller sub-clusters. There exist many e ff ective algorithms for clustering, but as modern data sets get larger, the fact that these algorithms require every pairwise similarity between objects poses a se-rious measurement and/or computational burden and limits the practicality of these algorithms. It is there-fore practically appealing to develop clustering algo-rithms that are e ff ective on large scale problems from both a measurement and a computational perspective. To achieve both measurement and computational im-provements, we focus on reducing the number of sim-ilarity measurements required for clustering. This approach results in immediate reduction in measure-ment overhead in applications where similarities are observed directly, but it can also provide dramatic computational gains in applications where similarities between objects are computed via some kernel evalu-ated on observed object features. The case of internet topology inference is an example of the former, where covariance in the packet delays observed at nodes re-flects the similarity between them. Obtaining these similarities requires injecting probe packets into the network and places a significant burden on network infrastructure. Phylogenetic inference and other bio-logical sequence analyses are examples of the latter, where computationally intensive edit distances are of-ten used. In both cases our algorithms are dramati-cally faster than many popular algorithms.
 In this paper, we propose a novel framework for speed-ing up hierarchical clustering algorithms through ac-tivization  X  X reating active versions of the algorithms where only a small number of informative similarities are measured. Our framework allows the user to spec-ify various levels of activeness and we provide theo-retical analysis that quantifies the resulting trade-o ff between measurement overhead and computation time on one hand, and statistical accuracy on the other. As a detailed example, we apply our framework to spectral clustering. Spectral clustering is a very pop-ular clustering technique that relies on the structure of the eigenvectors of the Laplacian of the similarity matrix. These algorithms have received considerable attention in recent years because of their empirical suc-cess, but they su ff er from the fact that they require all n ( n  X  1) / 2 similarities between the n objects to be clustered and must compute a spectral decomposition, which on large datasets can be computationally pro-hibitive. Our active algorithm avoids this limitation by subsampling few objects in each round and only com-puting eigenvectors of very small sub-matrices. By appealing to previous statistical guarantees ( Balakr-ishnan et al. , 2011 ), we can show that this algorithm has desirable theoretical properties, both in terms of statistical and computational performance. There is a large body of work on hierarchical and par-titional clustering algorithms, many coming with var-ious theoretical guarantees, but only few algorithms attempt to minimize the number of pairwise similari-ties used ( Eriksson et al. , 2011 ; Balcan &amp; Gupta , 2010 ; Shamir &amp; Tishby , 2011 ). Along this line, the work of Eriksson et. al. ( 2011 ) and Shamir and Tishby ( 2011 ) is closest in flavor to ours.
 Eriksson et. al. ( 2011 ) develop an active algorithm for hierarchical clustering and analyze the correctness and measurement complexity of this algorithm under noise model where a small fraction of the similarities are inconsistent with the hierarchy. They show that for a constant fraction of inconsistent similarities, their algorithm can recover hierarchical clusters up to size  X  (log n )using O ( n log 2 n ) similarities. Our analysis for ActiveSpectral yields similar results in terms of noise tolerance, measurement complexity, and res-olution, but in the context of i.i.d. subgaussian noise rather than inconsistencies. Our algorithm is also com-putationally more e ffi cient.
 Another approach to minimizing the number of simi-larities used is via perturbation theory, which suggests that randomly sampling the entries of a similarity ma-trix preserves many of its important properties, such as its spectral norm ( Achlioptas &amp; McSherry , 2001 ). With this result, the Davis-Kahan theorem suggests that spectral clustering algorithms, which look at the eigenvectors of the Laplacian associated with the sim-ilarity matrix, can succeed in recovering the clusters. This intuition is formalized by Shamir and Tishby ( 2011 ) who analyze a binary spectral algorithm that randomly samples b entries from the similarity matrix. Their results imply that as long as b =  X  ( n log 3 / 2 n ) their algorithm will find flat k -way clusters of size  X  ( n ) with high probability. Our work, translated to the flat clustering setting improves this guarantee; Theo-rem 2 implies that O ( n log n ) similarities are needed to recover the clustering. Furthermore, we can give guarantees on the size of smallest cluster  X  (log n ) that can be recovered in a hierarchy by selectively sampling similarities at each level.
 Recently ( Voevodski et al. , 2012 ) proposed an active algorithm for flat k -way clustering that selects O ( k ) landmarks and partitions the objects using distances to these landmarks. Theoretically, the authors guaran-tee approximate-recovery of clusters of size  X  ( n )using O ( nk ) pairwise distances. This idea of selecting land-marks bears strong resemblence to the first phase of our active clustering algorithm and also has connec-tions to the Landmark MDS algorithm of de Silva and Tenenbaum ( 2002 ). These approaches are tied to spe-cific algorithms, while our framework is much more general. Moreover, we guarantee exact cluster recov-ery (under mild assumptions) rather than approximate recovery, which translates into guarantees on hierarchi-cal clustering.
 A related direction is the body of work on e ffi cient streaming and online algorithms for approximating the k -means and k -medians objectives (See for example ( Charikar et al. , 2003 ; Shindler et al. , 2011 )). As with ( Voevodski et al. , 2012 ), the guarantees for these algorithms do not immediately translate into an ex-act recovery guarantee, making it challenging to trans-form these approaches into hierarchical clustering al-gorithms. Moreover, the success of spectral clustering in practice suggests that an e ffi cient spectral algorithm would also be very appealing. While there have been advances in this direction, the majority of these re-quire the entire similarity matrix or graph to be known apriori ( Frieze et al. , 2004 ). Apart from ( Shamir &amp; Tishby , 2011 ), we know of no other spectral algorithm that optimizes the number of similarities needed. Before proceeding with our main results, we first clar-ify some notation and introduce a hierarchical cluster-ing model that we will analyze. We refer to A as any flat clustering algorithm, which takes as parameters a dataset and a natural number k , indicating the num-ber of clusters to produce. Throughout the paper, k will denote the number of clusters at any split, and we will assume that k is known and fixed across the hierarchy. We let n be the number of objects in a datasets and define s to be a parameter to our algo-rithms, influencing the number of measurements used by our algorithm, where smaller s implies fewer mea-surements. The parameter s reflects a tradeo ff be-tween the measurement overhead and the statistical accuracy of our algorithms; increasing s increases the Algorithm 1 ActiveCluster ( A ,s, { x i } n i =1 ,k ) if n  X  s then return { x i } n i =1 Draw S  X  { x i } n i =1 of size s uniformly at random. C 1 ,...C k  X  A ( S, k ).

Set C 1  X  C 1 ,...C k  X  C k . for x i  X  { x i } n i =1 \ S do end for output { C j , ActiveCluster ( A ,s,C j ,k ) } k j =1 robustness of our method, albeit at the cost of requir-ing more measurements. Finally, our algorithms em-ploy an abstract, possibly noisy similarity function K , which can model both cases where similarities are mea-sured directly and where they are computed via some kernel function based on observed object features. Definition 1 A hierarchical clustering C on ob-jects { x i } n i =1 is a collection of clusters such that C { x i } n i =1  X  C and for each C i ,C j  X  C either C i  X  C ,C j  X  C i or C i  X  C j =  X  . For any cluster C ,if  X  C with C  X  C , then there exists a set { C i } k i =1 of disjoint clusters such that k i =1 C i = C . Every hierarchical clustering C has a parameter  X  that quantifies how balanced the clusters are at any split. Formally,  X   X  max splits { C each split is a non-terminal cluster, partitioned into { C i } k i =1 .  X  upper bounds the ratio between the largest and smallest clusters sizes across all splits in C .This type of balancedness parameter has been used in pre-vious analyses of clustering algorithms ( Eriksson et al. , 2011 ; Balakrishnan et al. , 2011 ), and it is common to assume that the clustering is not too unbalanced. For clarity of presentation, we will state our results assum-ing  X  = O (1), although our proofs contain a precise dependence between the level of activeness s and  X  . 3.1. An Active Clustering Framework Our primary contribution is the introduction of a novel framework for hierarchical clustering that is e ffi cient both in terms of the number of similarities used and the algorithmic running time. To recover any single split of the hierarchy, we run a flat clustering algorithm A on a small subset of the data to compute a seed clustering of the dataset. Using this initial clustering, we place each remaining object into the seed cluster for which it is most similar on average. This results in a flat clustering of the entire dataset, using only similarities to the objects in the small subset. By recursively applying this procedure to each clus-ter, we obtain a hierarchical clustering, using a small fraction of the similarities. In this recursive phase, we do not observe any measurements between clusters at the previous split, i.e. to partition C j , we only ob-serve similarities between objects in C j .Thisresults in an active algorithm that focuses its measurements to resolve the higher-resolution cluster structure. Pseudocode for ActiveCluster is shown in Algo-rithm 1 . As a demonstration, in Figure 1 ,weshow the sampling pattern of ActiveCluster on the first, and second splits of a hierarchy (top row), in addition to the patterns at the end of the computation (bot-tom right). Only the similarities shown in white are needed. As is readily noticeable, the algorithm uses very few similarities yet can recover this hierarchy. We are now ready to state our main theoretical contri-bution which characterizes ActiveCluster in terms of probability of success in recovering the true hierar-chy (denoted C ), measurement and runtime complex-ity. In order to make these guarantees we will need to place some mild restrictions on the similarity function K , which ensure that the similarities agree with the hierarchy (up to some random noise): K1 For each x i  X  C j  X  C and j = j : K2 For each object x i  X  C j , a set of M j objects of K1 states that the similarity from an object x i to its cluster should, in expectation, be larger than the the similarity from that object to any other cluster. This is related to the Tight-Clustering condition used in ( Eriksson et al. , 2011 ) and less stringent than earlier results which assume that within-and between-cluster similarities are constant and bounded in expectation ( Rohe et al. , 2010 ). Moreover, an assumption of this form seems necessary to ensure that even in expec-tation one could identify the clustering. Lastly, K2 enforces that within-and between-cluster similarities concentrate away from each other. This condition is satisfied, for example, if similarities are constant in ex-pectation, perturbed with any subgaussian noise. We emphasize that K2 subsumes many of the assumptions of previous clustering analyses (for example ( Balakr-ishnan et al. , 2011 ; Rohe et al. , 2010 )). Theorem 1 Let { x i } n i =1 be a dataset with true hier-archical clustering C , let K be a similarity function satisfing assumptions K1 and K2 and consider any flat clustering algorithm A with the following property: A1 For any dataset { y i } m i =1 with clustering C where Then ActiveCluster ( A ,s, { x i } n i =1 ,k ) : R1 recovers all clusters of size at least s with proba-R2 uses O ( ns log n ) similarity measurements. R3 runs in time O ( nA s + ns log n ) where A on a At a high level, the theorem says that the clustering guarantee for a flat, non-active algorithm, A , can be translated into a hierarchical clustering guarantee for an active version of A , and that this active algorithm enjoys significantly reduced measurement and runtime complexity. The only property needed by A is that it recovers a flat clustering with very high probability. While the probability of success seems strangely high, we will show that for a fairly intuitive model, a simple spectral clustering algorithm enjoys this kind of guar-antee. Verifying that the model satisfies the conditions K1 and K2, immediately results in a guarantee for the active version of this spectral algorithm.
 Before delving into the proof of the theorem, some re-marks are in order. First, by plugging in the lower bound for s into the upper bound on the measure-ment complexity, we see that ActiveCluster needs O ( n log( nk ) log n ) similarities, which is considerably less than the O ( n 2 ) similarities required by a non-active algorithm. Second, at the lower bound for s , we see that unless A runs in exponential time, ActiveCluster runs in  X  O ( n ), which is significantly faster than any clustering algorithm that observes all of the similarities and must take  X  ( n 2 )time. We now turn to the proof of R1. Due to space limita-tions, we defer many details and technical lemmas to the appendix. The proofs for R2 and R3 are straight-forward, involving counting arguments on trees, and are also available in the appendix.
 Proof for R1: We study the sampling, clustering and averaging phases of ActiveCluster in turn. In the sampling phase, we demonstrate that choosing s objects at random does not result in a highly unbal-anced subset. Using bernoulli concentration inequali-ties and a union bound we show that the balance fac-tor across all splits is at most 2  X  + 1 with probability  X  1  X  o ( ne  X  c  X  ,  X  s ), which goes to 1 under Equation 1 . For the clustering phase, Lemma 3 (in the appendix) shows that the total number of calls to A is at most k  X  1 and each time we call A we have a probability of success  X  1  X  o ( k e c 1 s ) by assumption A1. It is now easy to see that the probability of A failing at any split is o ( n exp {  X  s } ), which is o (1) under Equation 1 . In the averaging phase, we need to show that for each split of the hierarchy and object x i , the sample aver-age within cluster similarity is larger than the sample average between cluster similarity. Under assumption K1 and K2, we know that these quantities concentrate away from each other. Via a union bound across all objects and all levels of the hierarchy, we can conclude that the probability of making a mistake in any averag-goes to zero as long as s satisfies Equation 1 . Algorithm 2 SpectralCluster ( W )
Compute Laplacian L = D  X  W , D ii = n j =1 W ij v 2  X  smallest non-constant eigenvector of L .
C 1  X  { i : v 2 ( i )  X  0 } , C 2  X  { j : v 2 ( j ) &lt; 0 } output { C 1 ,C 2 } . 3.2. Active Spectral Clustering To make the guarantees in Theorem 1 more concrete, we show how to translate this into real guarantees for a specific subroutine algorithm A . In this section, we turn a simple spectral algorithm (See pseudocode in Algorithm 2 ) into an active clustering algorithm, us-ing the analysis from ( Balakrishnan et al. , 2011 ). The algorithm operates on hierarchically structured sim-ilarity matrices refered to as the noisy Hierarchi-cal Block Matrices (again from ( Balakrishnan et al. , 2011 )). These are defined as follows: Definition 2 Asimilaritymatrix W is a noisy hier-archical block matrix (noisy HBM) if W A + R where A is ideal and R is a perturbation matrix:  X  An ideal similarity matrix is characterized  X  A symmetric ( n  X  n ) matrix R is a perturba-To apply Theorem 1 , we need to verify that the assumption K1 and K2 are met and Spectral-Cluster succeeds with exponentially high probabil-ity. Checking that these conditions hold as long as  X  = O (1) results in the following guarantees for Ac-tiveSpectral , the active version of SpectralClus-ter . Proof of this theorem is deferred to the appendix. Theorem 2 Let W be a noisy HBM with  X  = O (1) and  X  = O (1) . Then, ActiveSpectral succeeds in recovering all clusters of size s with probability 1  X  o (1) as long as Equation 1 holds. Moreover, ActiveSpectral uses O ( ns log n ) measurements and runs in O ( ns 2 log s + ns log n ) time.
 The results of this theorem quantify one extreme of the tradeo ff between statistical robustness and measure-ment complexity for hierarchical spectral algorithms. In particular, it states that ActiveSpectral can tol-erate a constant amount of noise while using only O ( n log 2 n ) measurements. At the other end of this spectrum is the result of Balakrishnan et. al. ( 2011 ), showing that using O ( n 2 ) measurements, one can tol-erate noise that grows fairly rapidly with n . Varying s allows for interpolation between these two extremes. 3.3. Active k -means clustering It is also possible to activize the popular k -means al-gorithm in our framework, but we cannot prove sta-tistical performance guarantees since it is unknown whether k -means satisfies assumption A1. Activizing k -means helps illuminate the di ff erences between ob-serving similarities directly and computing similarities from directly observed object features. Convention-ally, k -means fits into the latter framework. Here, the active version does not enjoy a reduced measure-ment complexity, because all of the objects must be observed, but it can reduce the number of similarity computations from nkT to skT +( n  X  s ) kT ,sincethe iterative subroutine runs on only s objects for T itera-tions. In cases where the similarity function is expen-sive to compute, such as edit distance, this can lead to gains in running time.
 A less traditional way to use k -means is to represent each object as a n -dimensional vector of its similar-ity to each other object. Here, we can apply k -means to a n  X  n similarity matrix, much like we can ap-ply SpectralCluster and this algorithm can be ac-tivized using our framework. While we cannot develop theoretical guarantees for this algorithm, which we call ActiveKMeans , our experiments demonstrate that it performs very well in practice. 3.4. Some Practical Considerations Our algorithm as stated has some shortcomings that enable theoretical analysis but that are undesirable for practical applications. Specifically, the fact that k is known and constant across splits in the hierarchy, and the balancedness condition are both assumptions that are likely to be violated in any real-world setting. We therefore develop a variant of ActiveSpectral , called HeurSpec , with several heuristics.
 First, we employ the popular eigengap heuristic, in which the number of clusters k is chosen so that the gap in eigenvalues  X  k +1  X   X  k of the Laplacian is large. Secondly, we propose discarding all subsampled ob-jects with low degree (when restricted to the sample) in the hopes of removing underrepresented clusters from the sample. In the averaging phase, if an object is not highly similar to any cluster represented in the sam-ple, we create a new cluster for this object. We expect that in tandem, these two heuristics will help us re-cover small clusters. By comparing the performance of HeurSpec to that of ActiveSpectral ,weindi-rectly evaluate these heuristics.
 In this section we present experiments that verify our theoretical results. By Theorem 2 ,weexpect Ac-tiveSpectral to be robust to a constant amount of noise  X  , meaning that it will recover all su ffi ciently large splits with high probability. In comparison, Bal-akrishnan et. al. ( 2011 ), show that SpectralClus-ter can tolerate noise growing with n . We constrast these guarantees by plotting the probability of success-ful recovery of the first split in a noisy HBM as a func-tion of  X  for di ff erent n in Figure 2 . 2(a) demonstrates that indeed the noise tolerance of SpectralClus-ter grows with n while 2(c) demonstrates that Ac-tiveSpectral enjoys constant noise tolerance. Fig-ures 2(b) and 2(d) suggest that similar guarantees may hold for k -means and ActiveKMeans .
 Our theory also predicts that increasing the active-ness parameter improves the statistical performance of ActiveSpectral . To demonstrate this, we plot the probability of successful recovery of the first split of a noisy HBM of size n = 256 as a function of s for fixed noise variance. We compare three algorithms, ActiveSpectral , ActiveKMeans , and Algorithm 1 from ( Shamir &amp; Tishby , 2011 ), which subsamples en-tries of the similarity matrix. In theory, ActiveSpec-tral requires  X  ( n log n ) total measurements to re-cover a single split, whereas ( Shamir &amp; Tishby , 2011 ) show that their algorithm requires  X  ( n log 3 / 2 n ). Fig-ure 2(e) demonstrates that this improvement is also noticeable in practice. ActiveKMeans seems to en-joy an even more favorable dependence on s . The simulations in Figures 2(a) -(e) only examine the ability of our algorithms to recover the first split of a hierarchy, while our theory predicts that all su ffi ciently large clusters can be reliably recovered. One way to measure this is the outlier fraction metric between the clustering returned by an algorithm and the true hierarchy ( Eriksson et al. , 2011 ). For any triplet of ob-jects x i ,x j ,x k we say that the two clusterings agree on this triplet if they both group the same pair of objects deeper in the hierarchy relative to the third object and disagree otherwise. The outlier fraction is simply the fraction of triplets for which the two clusterings agree. In Figure 2(f) , we plot the outlier fraction for six algorithms as a function of  X  on the noisy HBM. The algorithms are: Hierarchical Spectral (SC), Sin-gle Linkage (SL), HeurSpec (HSC), ActiveSpec-tral (ASC), Hierarchical k -Means (KM), and Ac-tiveKMeans (AKM). These experiments demon-strate that the non-active algorithms (except single linkage) are much more robust to noise than the cor-responding active ones, as predicted by our theory, but also that the heuristics described in Section 3.4 have dramatic impact on performance.
 Lastly, we verify the measurement and run time com-plexity guarantees for our active algorithms in com-parison to the non-active versions. In Figure 2(g) and 2(h) , we plot the number of measurements and running time as a function of n on a log-log plot for each algorithm. The three non-active algorithms have steeper slopes than the active ones, suggesting that they are polynomially more expensive in both cases. To demonstrate the practical performance of the Ac-tiveCluster framework, we apply our algorithms to three real-world datasets and one additional synthetic dataset. The datasets are: The set of articles from NIPS volumes 0 through 12 from ( Roweis , 2002 ), a subset of NPIC500 co-occurence data from the Read-the-Web project ( Mitchell , 2009 ) which we call RTW, a SNP dataset from the HGDP ( Pemberton et al. , 2008 ), and a synthetic phylogeny dataset produced us-ing phyclust ( Chen , 2010 ). We refer the reader to the appendix for additional details on these datasets. In the phylogeny and SNP datasets, we have access to a reference tree that can be used in our evaluation. In these cases we can report the outlier fraction, as we did in simulation. However, the other datasets lack such ground truth and without it, evaluating the perfor-mance of each algorithm is non-trivial. Indeed, there is no well-established metric for this sort of evaluation. For this reason, we employ two distinct metrics to eval-uate the quality of hierarchical clusterings. They are a hierarchical K -means objective (HKM) ( Kauchak &amp; Dasgupta , 2003 ) and an analogous hierarchical ratio-cut (HRC) objective, both of which are natural gen-eralizations of the k -means and ratio cut objectives respectively, averaging across clusters, and removing small clusters as they bias the objectives. Formally, let C be the hierarchical clustering and let  X  C be all of the clusters in C that are larger than log n . For each C  X   X  C let x C be the cluster center. Then: In Table 1 and 2 , we record experimental results across the datasets for our algorithms. On the read-the-web dataset, we were unable to run the non-active algo-rithms. On the SNP and phylogeny datasets, we in-clude computing similarities via edit distance in the running time of each algorithm, noting that comput-ing all pairs takes 6500 and 15000 seconds respec-tively. The immediate observation is that these algo-rithms are extremly fast; on the SNP and phylogeny datasets where computing similarities is the bottle-neck, activization leads to significant performance im-provements. Moreover, the algorithms perform well by our metrics; they find clusterings that score well according to HKM and HRC, or that have reasonable agreement with the reference clustering 1 .
 We are also interested in more qualitatively under-standing the performance of these algorithms. For the NIPS data, we manually collected a small sub-set of articles and visualized the hierarchy produced by ActiveKMeans restricted to these objects. The hierarchy in Figure 3 is what one would expect on the subset, attesting to the performance ActiveKMeans . On the other hand, this same kind of evaluation on the RTW data demonstrates that active algorithms do not perform well on this dataset, while the non-active al-gorithms do. We suspect this is caused by the RTW dataset consisting of many small clusters that do not get sampled by the ActiveCluster framework. For the SNP and phylogeny datasets, the permuted heatmaps are clear enough to be used in qualitative evaluations. These heatmaps are shown in Figure 4 , and they suggest that all three active algorithms per-form very well on these datasets. Heatmaps for the remaining datasets are less clear, but for completeness we include them in the appendix. Our results in this paper, showing that a family of active hierarchical clustering algorithms have strong performance guarantees, raise several interesting ques-tions. We showed that ActiveSpectral enjoys rea-sonable statistical performance, but can other algo-rithms be activized while retaining statistical proper-ties? Second, are there principled ways to circumvent a balancedness condition, even when objects are sub-sampled? Finally, is there a theoretically justified ap-proach for estimating the number of clusters, k ? Another direction relates not toward clustering, but toward the recently popular matrix completion prob-lem. On hierarchically structured matrices, our results imply that an active algorithm can recover high-rank (rank n/ log n ) matrices using O ( n log 2 n ) similarities, an improvement over non-active approaches. Active algorithms may therefore yield impressive guarantees for matrix completion and related problems, and we hope to explore this direction in the future. This research is supported in part by AFOSR un-der grant FA9550-10-1-0382 and NSF under grant IIS-1116458. AK is supported in part by a NSF Graduate Research Fellowship.

