 In the pattern mining literature, the attention has shifted from local to global pattern mining [1,10] or from individual patterns to pattern sets [5]. Local pattern mining is traditionally formulated as the problem of computing Th( L , X , D )= {  X   X  L |  X  (  X , D )istrue } ,where D isadataset, L a language of patterns, and  X  a constraint or predicate that has to be satisfied. Local pattern mining does not take into account the relationships between patterns; the constraints are evalu-ated locally , that is, on every pattern individually, and if the constraints are not restrictive enough, too many patterns are found. On the other hand, in global pattern mining or pattern set mining , one is interested in finding a small set of rel-evant and non-redundant patterns. Pattern set mining can be formulated as the problem of computing Th ( L , X , X , D )= {  X   X  Th( L , X , D ) |  X  (  X , D )istrue } , where  X  expresses constraints that have to be satisfied by the overall pattern sets. In many cases a function f is used to evaluate pattern sets and one is then
Within the data mining and the machine learning literature numerous ap-proaches exist that perform pattern set mining. These approaches employ a wide variety of search strategies. In data mining, the step-wise strategy is common, in which first all frequent patterns are computed; they are heuristically post-processed to find a single compressed pattern set; examples are KRIMP [16] and CBA [12]. In machine learning, the sequential covering strategy is popular, which repeatedly and heuristically searches for a good pattern or rule and imme-diately adds this pattern to the current pattern-(or rule-)set; examples are FOIL [14] and CN2 [3]. Only a small number of techniques, such as [5,7,9], search for pattern sets exhaustively, either in a step-wise or in a sequential covering setting.
The key contribution of this paper is that we study, evaluate and compare these common search strategies for pattern set mining. As it is infeasible to perform a detailed comparison on all pattern set mining tasks that have been considered in the literatur e, we shall focus on one prototypical task for pattern set mining: boolean concept-learning. In this task, the aim is to most accurately describe a concept for which positive and negative examples are given.Within this paper we choose to fix the optimisation measure used to accuracy ;ourfocus is on the exploration of a wide variety of search strategies for this measure, from greedy to complete and from step-wise to one-step approaches.

To be able to obtain a fair and detailed comparison we choose to reformulate the different strategies within the common framework of constraint program-ming. This choice is motivated by [4,13], who have shown that constraint pro-gramming is a very flexible and usable approach for tackling a wide variety of local pattern mining tasks (such as clos ed frequent itemset mining and discrim-inative or correlated items et mining), and recent work [9,7] that has lifted these techniques to finding k -pattern sets under constraints (sets containing exactly k patterns). In [7], a global optimization approach to mining pattern sets has been developed and has been shown to work for concept-learning, rule-learning, redescription mining, conceptual clustering as well as tiling. In the present work, we employ this constraint programming framework to compare different search strategies for pattern set mining, focusing on one mining task in more detail.
This paper is organized as follows: in Section 2, we introduce the problem of pattern set mining and its benchmark, concept-learning; in Section 3, we formu-late these problems in the framework of constraint programming and introduce various search strategies for pattern set mining; in Section 4, we report on ex-periments, and finally, in Section 5, we conclude. The benchmark task on which we shall evaluate different pattern set mining strategies is that of finding boolean concepts in the form of k -term DNF expres-sions. This task is well-known in computational learning theory [8] and is closely related to rule-learning systems such as FOIL [14] and CN2 [3] and data mining systems such as CBA [12] and KRIMP [16]. It is  X  as we shall now show  X  a pattern set mining task of the form arg max  X   X  Th ( L , X , X , D ) f (  X  ).
In this setting, one is given a set of positive and negative examples, where each example corresponds to a boolean variable assignment to the items in I , the set of possible items. Thus each example is an itemset I x  X  X  . Positive examples will belong to the set of transactions T + , negatives ones to T  X  .The pattern language is the set L =2 I . Hence each pattern corresponds to an itemset I  X  X  and represents a conjunction of item s. The task is then to learn a concept description (a boolean formula) that covers all (or most) of the positive examples and none (or only a few) of the negatives. This can be measured using the accuracy measure, defined as: where p and n are the number of positive, resp ectively negative, examples cov-ered, and P and N are the total number of positive, resp. negative, examples present in the database. Concept descriptions are pattern sets, where each pat-tern set corresponds to a disjunction of patterns (conjunctions). Following [7,15], we shall focus on finding pattern sets that contain exactly k patterns. Thus the pattern sets correspond to k -term DNF formulas. An exa mple is considered cov-ered by the pattern set if the example is a superset of at least one of the itemsets in the pattern set.

Thus the task considered is an instance of the pattern set mining task arg max  X   X  Th ( L , X , X , D ) f (  X  ), where f is the accuracy, D = T = T +  X  X   X  ,and L =2 I ;  X  can be instantiated to a minimum support constraint (requiring that each pattern covers a certain numb er of examples), a minimum accuracy constraint (requiring that each pattern is individually accurate), or to true ,a constraint which is always true and allows any pattern to be used which leads to an accurate final set.  X  states that |  X  | = k .

Finding a good pattern set is often a hard task; many pattern set mining tasks, such as the task of k -term DNF learning, are NP complete [8]. Hence, there are no straightforward algorithms for solving such tasks in general, giving rise to a wide variety of search algorithms. The pattern set mining techniques they employ can be categori zed along two dimensions.
 Two-Step vs One-Step: in the two step approach, one first mines patterns Exact vs Approximate: exact methods provide strong guarantees for find-In the next section we will consider the instantiations of these settings for the case of concept learning. However, first we will introduce the constraint programming framework within which we will study these instantiations. Throughout the remainder of this paper we shall employ the constraint program-ming framework of [4] for representing and solving pattern set mining problems. This framework has been shown 1) to allow for the use of a wide range of con-straints, 2) to work for both frequent and discriminative pattern mining [13], and 3) to be extendible towards the formulation of k pattern set mining, cf. [7,9]. These other papers provide detailed descriptions of the underlying constraint programming algorithms and technology, including an analysis of the way in which they explore the search tree and a performance analysis. On the other hand, in the present paper  X  due to space restrictions  X  we need to focus on the declarative specification of the constraint programming problems; we refer to [4,13,7] for more details on the search strategy of such systems. 3.1 Constraint Programming Notation Following [4], we assume that we are given a domain of items I and transactions T , and a binary matrix D . A key insight of the work of [4] is that constraint based mining tasks can be formulated as constraint satisfaction problems over the variables in  X  =( I,T ), where a pattern  X  is represented using the vectors I and T , with a boolean variable I i and T t for every item i  X  X  and every transaction t  X  X  . A candidate solution to the constraint satisfaction problem is then one assignment of the variables in  X  which corresponds to a single itemset. items 1 and 3, and covers transactions 1, 2 and 5. Following [7], a pattern set  X  of size k simply consists of k such patterns:  X  = {  X  1 ,..., X  k } ,  X  p =1 ,...,k :  X  p =( I p ,T p ). We now discuss the different two-step and one-step pattern set mining approaches. 3.2 Two-Step Pattern Set Mining In two step pattern set mining approaches, one first searches for the set of local patterns Th( L , X , D ) that satisfy a set of constraints, and then post-processes these to find the pattern sets in Th ( L , X , X , D ).
 Step 1: Local Pattern Mining. Using the above notation one can formulate many local pattern mining problems, such as frequent and discriminative pattern mining. Indeed, consider the following constraints, introduced in [4,13]: In these constraints, the coverage constraint links the items to the transactions: it states that the transaction set T must be identical to the set of all transactions that are covered by the itemset I .The closedness constraint removes redundancy by ensuring that an itemset has no supe rset with the same frequency. It is a well-known property that every non-closed pattern has an equally frequent and accurate closed co unterpart. The minimum frequency constraint ensures that itemset I covers at least  X  transactions. It can more simply be formulated as separately (observe that t  X  X  T t D ti counts the number of t in column i of binary matrix D for which T t = 1). This so-called reified formulation results in more effective propagation; cf. [4]. Finally, to mine for all accurate patterns instead of all frequent patterns, the minimum accuracy constraint can be used, which ensures that itemsets have an accuracy of at least  X  . The reified formulation again results in more effective propagation [13].

To emulate the two step approaches that are common in data mining [12,16,1], we shall employ two alternatives for the first step: 1) using frequent closed patterns , which are found with the coverage , closedness and minimum fre-quency constraints; 2) using accurate closed patterns , found with the cov-erage , closedness and minimum accuracy constraints. Both of these approaches preform the first step in an exact manner. They find the set of all local patterns adhering to the constraints.
 Step 2: Post-processing the Local Patterns. Once the local patterns have been computed, the two step approach post-processes them in order to arrive at the pattern set. We describe the two main approaches for this.
 Post-processing by Sequential Covering (Approximate). Themostsimpleap-proach to the second step is to perform g reedy sequential covering, in which one iteratively selects the best local pattern from Th( L , X , D )andremovesallof the positive examples that it covers. This continues until the desired number of patterns k has been reached or all positive examples are already covered. This type of approach is most common in data mining systems. Whereas in the first step the set Th( L , X , D ) is computed exactly in these methods, the second step is often an iterative loop in which patterns are selected greedily from this set. Post-processing using Complete Search (Exact). Another possibility is to per-form a new round of pattern mining as described in [5]. In this case, each pre-viously found pattern in P =Th( L , X , D ) can be seen as an item r in a new database; each new item identifies a pattern. One is looking for the set of pat-tern identifiers P  X  X  with the highest accuracy. I n this case, the set is not a conjunction of items, but a disjunction of patterns, meaning that a transaction is covered if at least one of the patterns r  X  P covers it. This can be formulated in constraint programming after a transformation of the data matrix D into a matrix M where the rows correspon d to the transactions in T and the columns to the patterns in P .Moreover M tr is 1 if and only if pattern r covers trans-action t and 0 otherwise. The solution s et is now represented using  X  =( P, T ), where P is the vector representation of the pattern set, that is, P r =1iff r  X  P . The formulation of post-processing using complete search is now: To obtain a reified formulation of the accuracy constraint we here use L tr = max( T t , M tr )= M tr +(1  X  X  tr ) T t . The column for pattern r in this matrix represents the transaction vector if the pattern r would be added to the set P .
The first constraint is the disjunctive coverage constraint . The second con-straint is the minimum accuracy constraint , posted on each pattern separately and taking the disjunctive coverage into account. Lastly, the set size constraint limits the pattern set to size k .

This type of exact two-step approach is relatively new in data mining. Two notable works are [11,5]. In these publications, it was proposed to post-process a set of patterns by using a complete searc h over subsets of patterns. If an exact pattern mining algorithm is used to compute the initial set of pattern in the first step, this gives a method that is overa ll exact and offers strong guarantees on the quality of the solution found. 3.3 One-Step Pattern Set Mining This type of strategy, which is common in machine learning, searches for the pattern set Th ( L , X , X , D ) directly, that is, the computation of Th ( L , X , X , D ) and Th( L , X , D ) is integrated or interleaved. This can remove the need to have strong constraints with strict thresholds in  X  . There are two approaches to this: Iterative Sequential Covering (Approximate). In the iterative sequential covering approach that we investigate h ere, a beam search is employed (with beam width b ) to heuristically find the best pa ttern set. At each step during the search a local pattern mining algorithm is used to find the top-b patterns (with the highest accuracy) and uses these to compute new candidate pattern sets on its beam, after which it prunes all but the best b pattern sets from its beam. This setting is similar to 2-step sequential covering, only that here, at each iteration, the most accurate pattern is mined for directly, instead of selecting it from a set of previously mined patterns. Mining for the most accurate pattern can be done in a constraint programming setting by doing branch-and-bound search over the accuracy threshold  X  . In the experimental section, we shall consider different versions of the approach, corresponding to different sizes of the beam. When b = 1, one often talks about greedy sequential covering .

Examples of one-step greedy sequentia l covering methods are FOIL and CN2; however, they use greedy algorithms to identify the local patterns instead of a branch-and-bound pattern miner. In data mining, the use of branch-and-bound pattern mining algorithms was recen tly studied for identifying top-b patterns; see for instance [2]. Global Optimization (Exact). The last option is to specify the problem of finding a pattern set of size k as a global optimization problem. This is possible in a constraint programming framework, thanks to its generic handling of con-straints, cf. [7]. The formulation, searching for k patterns  X  p =( I p ,T p ) directly, is as follows: Each pattern has to cover the transactions (Eq. 2) and be closed (Eq. 3). The canonical form constraint in Eq. 4 enfor ces a fixed lexicographic ordering on the itemsets, thereby avoiding to find equivale nt but differently ordered pattern sets. In Eq. 5, the variables B t are auxiliary variables representing whether transaction t is covered by at least one pattern, corresponding to a disjunctive coverage.
The one-step global optimization approaches to pattern set mining are less common; the authors are only aware of [7,9]. One could argue that some iterative pattern mining strategies will find pattern sets that are optimal under certain conditions. For instance, Tree 2 [2] can find a pattern set with minimal error on supervised training data; h owever, it neither provides guarantees on the size of the final pattern set nor provides guarantees under additional constraints. We now compare the different approaches to boolean concept learning that we presented and answer the following two questions:  X  Q1: Under what conditions do the different strategies perform well?  X  Q2: What quality/runtime trade-offs do the strategies make? To measure the quality of a pattern set, we evaluate its accuracy on the dataset. This is an appropriate means of evaluation, as in the boolean concept learning task we consider, the goal is to find a concise description of the training data, rather than a hypothesis that generalizes to an underlying distribution.
The experiments were performed usi ng the Gecode-based system proposed by [4] and performed on PCs running Ubuntu 8.04 with Intel(R) Core(TM)2 Quad CPU Q9550 processors and 4GB of RAM. The datasets were taken from the website accompanying this system 1 . The datasets were derived from the UCI Machine Learning repository [6] by discretising numeric attributes into eight equal-frequency bins. To obtain reason ably balanced class sizes we used the majorityclassasthepositiveclass.Experimentswererunonmanydatasets, but we here present the findings on 6 diverse datasets whose basic properties are listedinthetop3rowsofTable1. 4.1 Two-Step Pattern Set Mining The result of a two-step approach obviously depends on the quality of the pat-terns found in the first step. We start by investigating the feasibility of this first step, and then study the two-step methods as a whole.
 Step 1: Local Pattern Mining. As indicated in Section 3.2, we employ two alternatives: using frequent closed pa tterns and using accurate closed patterns. Both methods rely on a threshold to influence the number of patterns found.
Table 1 lists the number of patterns found on a number of datasets, for the two alternatives and with different thresholds. Out of practical considerations we stopped the mining process when more than 25 million patterns were found. Using this cut-off, we can distinguish pattern poor data (data having less than 25 million patterns when mining unconstrained) and pattern rich data. In the case of pattern poor data, one can mine using very low or even no thresholds. In the case of pattern rich data, however, one has to use a more stringent threshold in order not be overwhelmed by patterns. Unfortunately, one has to mine with different thresholds to discover how pattern poor or rich an unseen dataset is. Step 2: Post-processing the Local Patterns. We now investigate how the quality of the global pattern sets is influenced by the threshold used in the first step, and how this compares to pattern sets found by 1-step methods that do not have such thresholds. Post-processing by Sequential Covering (Approximate). This two-step approach picks the best local pattern from the s et of patterns computed in step one. As such, the quality of the pattern set depends on whether the right patterns are in the pre-computed pattern set. We use our generic framework to compare two-step sequential covering to the one-step approach.

For pattern poor data for which the set of all patterns can be calculated, such as the mushroom, vote and hepatitis dataset, using all patterns obviously results in the same pattern set as found by the one-step approach. Figure 1 shows the prototypical result for such data: low thresholds lead to good pattern sets, while higher thresholds gradually worsen the solution. For this dataset, starting from K=3, no better pattern set can be found. The same is true for the mushroom dataset, while in the vote dataset the sequential covering method continues to improve for higher K. Also note that in Figure 1 a better solution is found when using patterns with accuracy grea ter than 40%, compared to patterns with accuracy greater than 50%. This implies that a better pattern set can be found containing a local pattern that has a low accuracy on the whole data. This indicates that using accurate local patterns does not permit putting high thresholds in the first step. With respect to question Q2, we can observe that using a lower threshold comes at the cost of higher runtimes. However, for pattern poor datasets such as the one in Figure 1, these times are still manageable. The remarkable efficiency of the one-step seq uential covering method is thanks to recent advances in mining top-k d iscriminative patterns [13].

On pattern rich data such as the german-credit, australian-credit and kr-vs-kp dataset, similar behaviour can be obs erved. The only difference is that one is forced to use more stringent thresholds. Because of this, the pattern set found by the one-step approach can usually not be found by the two-step approaches. Figure 2 exemplifies this for the australian-credit dataset. Using a frequency threshold of 0 . 1, the same pattern set as for the one-step method is found for up to K=3, but not so for higher K. When using the highest thresholds, there is a risk of finding significantly worse pattern sets. On the kr-vs-kp dataset, when using high frequency thresholds significantly worse results were found as well, while this was not the case for the accuracy threshold. With respect to Q2 we have again observed that lower thresholds lead to higher runtimes for the two-step approaches. Lowering the thresholds further to find even better pattern sets would correspondingly come at the cost of even higher computation times. Post-processing using Complete Search (Exact). When post-processing a col-lection of patterns using complete sear ch, the size of that collection becomes a determining factor for the success of the method. Table 2 shows the same datasets and threshold values as in Table 1; here the entries show the largest K for which a pattern set could be found, up to K=6, and the time it took. A general trend is that in case many patterns are found in step 1, e.g. more than 100 000, the method is not able to find the optimal solution. With respect to Q1, only for the mushroom dataset the method found a better pattern set than any other method, when using all accurate patterns with threshold 0.4. For all other sets it found however, one of the 1-step methods found a better solution. Hence, although this method is exact in its second step, it depends on good patterns from its first step. Unfortunately finding those usually requires using low threshold values with corresponding disadvantages. 4.2 One-Step Pattern Set Mining In this section we compare the differen t one-step approaches, who need no local pattern constraints and thresholds. We investigate how feasible the one-step exact approach is, as well as how close th e greedy sequential covering method brings us to this optimal solution, and whether beam search can close the gap between the two.

When comparing the two-step sequenti al covering approach with the one-step approach, we already remarked that the latter is very efficient, though it might not find the optimal solution. The one-step exact method is guaranteed to find the optimal solution, but has a much higher computational cost. Table 3 below shows up to which K the exact method was able to find the optimal solution within the 6 hours time out. Comparing these results to the two-step exact approach in Table 2, we see that pattern sets can be found without constraints, where the two-step approach failed even with constraints.

With respect to Q1 we observed that only for the kr-vs-kp dataset the greedy method, and hence all beam searches with a larger beam, found the same pattern sets as the exact method. For the mushroom and vote dataset, starting from beam width 5, the optimal pattern set was found. For the german-credit and australian-credit, a beam width of size 15 was necessary. The hepatitis dataset was the only dataset for which the complete method was able to find a better pattern set, in this case for K=3, within the timeout of 6 hours.

Figure 3 shows a representative figure, in this case for the german-credit dataset: while the greedy method is not capable of finding the optimal pat-tern set, larger beams successfully find the optimum. For K=6, beam sizes of 15 or 20 lead to a better pattern set than when using a lower beam size. The exact method stands out as being the most time consuming. For beam search methods, larger beams clearly lead to larger runtimes. The runtime only increases slightly for increasing sizes of K because the beam search is used in a sequential covering loop that shrinks the dataset at each iteration. We compared several methods for finding pattern sets within a common con-straint programming framework, where we focused on boolean concept learning as a benchmark. We distinguished one step from two step approaches, as well as exact from approximate ones. Each method has its strong and weak points, but the one step approximate approaches, which iteratively mine for patterns, provided the best trade-off between ru ntime and accuracy and do not depend on a threshold; additionally, they can easily be improved using a beam search. The exact approaches, perhaps unsurprisingly, do not scale well to larger and pattern-rich datasets. A newly introduced approach for one-step exact pattern set mining however has optimality guara ntees and performs better than previ-ously used two-step exact approaches. In future work our study can be extended to consider other problem settings in pattern set mining, as well as other heuris-tics and evaluation metrics; furthermore, even though we cast all settings in one implementation framework in this paper, a more elaborate study could clarify how this approach compares to the pattern set mining systems in the literature. Acknowledgements. This work was supported by a Postdoc and project  X  X rin-ciples of Patternset Mining X  from the Research Foundation X  X landers, as well as a grant from the Agency for Innovation by Science and Technology in Flanders (IWT-Vlaanderen).

