 1. Introduction
Text summarization is a data reduction process. The use of text summarization allows a user to get a sense of the content of a full-text, or to know its information content, without reading all sentences within the full-text. Data reduction increases scale by (1) allowing users to find relevant full-text sources more quickly, and (2) assimilating only essential information from many texts with reduced effort. Text summarization is particularly useful in the biomedical domain, where oncologists must continuously find clinical trial study information related to their specialty, evaluate the study for its strength, and then possibly incorporate the new study information into their patient treatment efforts ( Brooks &amp; Sulimanoff, 2002; Jaques, 2002 ). The US National Institutes of Health Clinical Trials database contains over 13,500 clinical trials ( United States
National Library of Medicine, 2005a ), while PUBMED contains in excess of 16 million citations from over 4800 journals ( United States National Library of Medicine, 2006a ). Large and continuously-updated informa-tion sources such as these impede a physician X  X  ability to improve their treatment efforts.

The contribution of this work is to present three novel extractive summarization methods (BioChain for concept chaining, FreqDist for frequency distribution, and a hybrid of the two, ChainFreq) using domain-them along with several other publicly-available summarizers. We show that our semantic-based summarizer (BioChainSumm) is effective at identifying thematic sentences, while our frequency-distribution summarizer (FreqDistSumm) removes information redundancy. The best performance is achieved when the two methods, BioChain and FreqDist are combined to form a third hybrid summarizer, ChainFreqSumm.

The paper is organized as follows. Section 2 provides related work and background on the need for biomedical text summarization and the methods used for concept annotation. Section 3 presents our three concept-based algorithms. Section 4 describes the evaluation methodology and Section 5 discusses the results of the evaluation and presents a physician X  X  observations on the mismatch between abstracts and full-text of several selected papers. Section 6 provides concluding remarks and identifies future work. 2. Background and related work 2.1. Need for biomedical text summarization
Clinical trial studies and other scientific publications usually supply a summary of the paper in the form of an abstract produced by the author(s) of a study. We have identified at least five reasons for wanting to gen-erate text summaries from a full-text source even in the presence of the author X  X  abstract. (1) There exists no  X  X deal X  summary. An ideal summary is dependent on each user, including factors such as information need and domain background. An author X  X  abstract is one view of an ideal summary, but users may want alternative summaries. (2) The abstract may be missing content from the full-text ( Cohen &amp; Hersh, 2005 ). (3) Customized summaries can be useful in question-answering systems where they provide personalized information. (4) The use of automatic or semi-automatic summary generation by commercial abstract services may allow them to scale the number of published texts they can evaluate. (5) The generation and evaluation of summaries allows for evaluation of sentence selection methods that may be useful in multi-document summarization. 2.2. Biomedical domain concepts and automated concept annotation
The main idea of this work is to use domain-specific (biomedical) concepts to identify important sentences within a biomedical text which can be extracted to form a summary. To achieve text summarization through the use of concepts, three resources are required: (a) a list of domain-specific concepts, (b) one or more syn-onymous phrases which occur in text and are associated with each domain-specific concept, and (c) an auto-mated method for identifying concepts within a text. Concept annotation of each paper in the evaluation corpus was performed using the UMLS MetaMap Transfer application ( United States National Library of
Medicine, 2005b ). Summary generation using the discovered concepts then takes place in two stages: (1) bio-medical concept annotation of the source text, and (2) summary generation from the concept-annotated text using the discovered concepts.

In the biomedical domain, the National Library of Medicine ( http://www.nlm.nih.gov/ ) provides resources for identifying concepts and their relationships under the framework of the Unified Medical Language System (UMLS) ( United States National Library of Medicine, 2005c ). UMLS contains many sub-components, but we use only three: Metathesaurus, Semantic Network, and MetaMap Transfer. The UMLS Metathesaurus, derived from over 100 different biomedical vocabulary sources, contains concepts and real-world instances of the concepts, including a concept name and its synonyms ( United States National Library of Medicine, 2006b ). The UMLS Semantic Network organizes the Metathesaurus concepts into semantic types ( United States National Library of Medicine, 2004 ).
 For automated concept annotation, the MetaMap Transfer (MMTx) ( United States National Library of
Medicine, 2005b ) application maps free-form biomedical text to Metathesaurus concepts. MMTx performs text-to-concept mapping by first identifying noun phrases in each sentence, generating term variants of the phrase, finding candidate concepts from the generated phrase variants, and scoring each candidate concept.
The highest scoring concept and its semantic type are then returned. It is possible for a noun phrase to map to more than one concept. In this case, no disambiguation is performed, and MMTx returns multiple concepts and their semantic types. 2.3. Related work
We mainly investigate previous text summarization work related to lexical chaining and frequency meth-odologies because our work largely involves these two approaches.

Lexical chaining is a method for determining lexical cohesion among terms in a text ( Barzilay &amp; Elhadad, 1997 ), and has been used for many years for text summarization. Lexical cohesion is a property of text that in computational text understanding for two major reasons: (1) providing term ambiguity resolution, and (2) for determining the  X  X  X boutness X  X  of a discourse segment, without fully understanding the discourse. A basic assumption is the text must explicitly contain semantically-related terms identifying the main concept. Lexical chains for text summarization were first introduced by Morris and Hirst (1991) . Their initial work described the approach, but did not implement it because electronic versions of a thesaurus were not available at the time. A thesaurus is used to relate words semantically; for example, through synonymy and hypernym/ hyponym relationships. A machine implementation by Barzilay and Elhadad (1997) showed the theoretical work by Morris/Hirst could be practically realized for document summarization. While Barzilay/Elhadad proved the feasibility of computing lexical chains, their algorithm runs in exponential time. A linear time algorithm was later defined and implemented by Silber and McCoy (2002) . A more recent implementation focuses on improving word sense disambiguation based on the idea of one sense per discourse ( Galley &amp;
McKeown, 2003 ). All of these implementations use WordNet ( Fellbaum, 1998 ) as the knowledge source for identifying semantic relationships between terms. A computational model for semantic relationships between terms was developed by Fellbaum (1998) . To our knowledge, no biomedical text summarization work has been done using lexical chaining with UMLS.

Term frequency was first used in extractive text summarization in the late 1950s ( Luhn, 1958 ). A follow-up study of an analysis of five term frequency methods showed high agreement in sentence selection among the methods ( Rath, Resnick, &amp; Savage, 1961 ). Subsequent research using frequency methods focused on the use of frequency as one feature among many for identifying important sentences, such as cue phrases ( Edmundson, 1999; Pollock &amp; Zamora, 1975 ). Summarization using larger units of text has also been researched. The LAKE system uses keyphrases for summarization ( D X  X vanzo, Magnini, &amp; Vallin, 2004 ). The SUMMARIST system for generalizing concepts for topic interpretation (e.g., {pear, apple} ! fruit). Most recently, the SumBasic algorithm uses term frequency as part of a context-sensitive approach to identifying important sentences while reducing information redundancy ( Nenkova &amp; Vanderwende, 2005 ). The use of frequency as a feature in locating important areas of a text has been proven useful in the literature ( Edmundson, 1999; Luhn, 1958; tant information in several different ways, in order to reinforce main points ( Sparck Jones, 1999 ). 3. Summarization methods
In extractive text summarization, the task is to identify sentences in a source text which are relevant to the user while simultaneously reducing information redundancy. Sentences are scored based on a set of features.
The top-n highest scoring sentences in a text are then extracted, using n as an upper bound, and presented to the user in their order of appearance in the original source text. 3.1. Concept chaining (BioChain)
The BioChain method ( Reeve, Han, &amp; Brooks, 2006 ) applies the concepts and methods of lexical chaining to biomedical text using concepts rather than terms. Previous lexical chaining approaches use linkages among word instances to identify semantically-related terms, and the resulting linkages are used to identify the themes of a text. In the BioChain method, automatically identified concepts in the source text (see Section 2.2 )are chained based on their UMLS semantic type(s). Each concept chain contains a list of concepts belonging to a particular UMLS semantic type. If a concept belongs to multiple UMLS semantic types (i.e., multiple concept chains), the concept appears in multiple concept chains. Fig. 1 shows the concept-chaining summari-zation process. The source text is first processed by the UMLS Metamap Transfer application to identify noun phrases, which are then automatically mapped into UMLS concepts as well as UMLS semantic types. The summarizer (BioChainSumm) then takes all discovered concepts and constructs chains of concepts which are related by their UMLS semantic type.
 Once all concepts in the text have been chained, the strongest concept chains are identified through scoring.
The original lexical chaining paper defines three strong chain features: reiteration, density, and length ( Morris &amp; Hirst, 1991 ). Reiteration is repetition of concepts throughout a text. Density suggests concepts closer together are more likely to be related. Length is the number of concept instances within a concept chain.
We score concept chains by multiplying the frequency of the most frequent concept in the concept chain by the number of distinct concepts in the concept chain ( Barzilay &amp; Elhadad, 1997; Doran, Stokes, Dunnion, &amp;
Carthy, 2004 ). Once all concept chains are scored, the strongest concept chains are identified. Lexical chaining 1997 ), and we follow that method. Strong concepts within each strong concept chain are then identified using two different methods: (1) using the most frequent concept within each concept chain (ties result in multiple strong concepts for a concept chain) (called MostFrequentStrongChainConcept), and (2) using all concepts within a concept chain (called AllStrongChainConcepts). Sentences are scored based on the number of strong concepts they contain. A subset of the original source text sentences is then extracted based on their score and then presented in their original presentation order to form a summary. The size of the generated summary is user-controlled.

Fig. 2 shows an example concept chain which links together concepts found within a biomedical source text. In this example, the concept chain is for UMLS semantic type #81, which has a description of Quanti-tative Concept . The list of concept chain member phrases are phrases automatically found by the MetaMap
Transfer application. Each phrase also shows the UMLS concept which maps to it, as well as the sentence and section (paragraph) in which it occurs. This information is also automatically generated by the MetaMap
Transfer application. 3.2. Frequency distribution (FreqDist)
When using frequency as the only feature for identifying salient sentences, unit items (e.g., word, concept, or phrase) are counted and then each sentence is given a score based on the frequency count of each unit item in the sentence. A key issue in generating summaries is reducing redundancy. Each new sentence in the sum-mary should add new information. Using the highest frequency terms will likely result in the same information repeatedly being selected. In the SumBasic context sensitive approach ( Nenkova &amp; Vanderwende, 2005 ), a
This is also related to the idea of Maximal Marginal Relevance (MMR), where marginal relevance is finding relevant sentences which contain minimal similarity to previously selected sentences ( Carbonell &amp; Goldstein, 1998 ). We use a context sensitive approach to scoring sentences based on a frequency distribution model rather than term probability. The rationale of our approach is that the frequency distribution of terms or con-cepts in the source text and the generated summary should be as similar as possible.

Our FreqDist method ( Reeve, Han, Nagori et al., 2006 ) uses a frequency distribution approach with two of the source text are counted to form a frequency distribution model of the text, and a pool of sentences from the source text is created. A summary frequency distribution model is then created from the unit items found in the source text, and their frequency counts are initialized to zero. In the Summary Generation stage, new sentences are selected to be added to the summary. Identifying the next sentence to be added to the summary is accomplished by finding the sentence which most closely aligns the frequency distribution of the summary to the frequency distribution of the original source text. For each sentence in the sentence pool, a candidate sum-mary is first initialized to the summary generated so far, and then the sentence is added to the candidate sum-mary. The candidate summary frequency distribution is then compared for similarity to the original source text frequency distribution. This similarity score (see next paragraph) is assigned to the sentence. That is, the similarity calculation step is applied to each sentence in the sentence pool one-by-one. After all sentences from the sentence pool have been evaluated for their contribution to the candidate summary, the highest scor-ing sentence is added to the summary and removed from the sentence pool. This process of identifying the next sentence to be added to the summary is iterative, and repeats until the desired length of the summary is reached. Fig. 3 shows the complete FreqDist method implemented in the FreqDistSumm summarizer.

We compared five similarity functions to find which type of function worked best to evaluate a candidate summary X  X  frequency distribution to the original source text frequency distribution ( Reeve, Han, Nagori et al., 2006 ). Each frequency distribution (candidate summary and original source text) is modeled as a vector of unit items. Similarity functions are then applied to the two vectors. The five similarity functions used are: (1) tance; (4) vector subtraction ( Subhash, 1996 ); and (5) vector model comparison considering only unit item fre-quency ( Lee, Chuang, &amp; Seamons, 1997 ). We found that Dice X  X  coefficient, which looks at the number of common terms between the two vectors, performed the best ( Reeve, Han, Nagori et al., 2006 ). 3.3. Hybrid method (ChainFreq)
The BioChain and FreqDist methods use different approaches to identify relevant sentences for building an extractive summary. A problem not addressed in the current BioChainSumm summarizer ( Reeve, Han,
Nagori et al., 2006 ) is reducing information redundancy. Sentences containing the strongest concepts in the text are extracted without a complimentary method for reducing redundancy from sentences already selected.
To overcome this limitation, we propose combining the BioChain and FreqDist methods to form a hybrid method, called ChainFreq. The hybrid ChainFreq method first uses the BioChain method to identify candi-date sentences containing strong concepts. The candidate sentences ( Sc ) and their corresponding concepts ( Cc ) are then passed to the FreqDist method, which produces a set of summary sentences from the candidate sen-tences. That is, only Sc are used as a pool of sentences in the FreqDist method in Section 3.2 . A summary frequency distribution model is then created from the Cc , and their frequency counts are initialized to zero.
The FreqDist method then selects sentences containing concepts in the same distribution as the original source text with respect to Cc which reduces redundancy to the same proportion it exists in the source text.
Fig. 4 shows how the two summarization methods, BioChain and FreqDist, are combined to form the new hybrid summarizer, ChainFreqSumm. First, all source sentences with their corresponding concept annota-tions are collected and passed to the BioChain method. The BioChain method takes advantage of domain-specific knowledge, specifically UMLS semantic types, to find sentences which are important in the domain.
There is no limit on the number of sentences generated by the BioChain method. The subset of source-text sentences identified by the BioChain method are then passed to the FreqDist method. The FreqDist method then finds a further subset of sentences whose concept distribution best aligns with the concept distribution of the source text. A user-defined summary size limits the number of sentences output at this stage. Both the
BioChain method and the FreqDist method work together to (a) find the important sentences according to the domain (using the BioChain method), and (b) reduce redundancy by further reducing the number of important sentences based on how well their concept distribution aligns with the source text X  X  concept distri-bution (using the FreqDist method), which has the effect of reducing redundancy. 4. Evaluation
The purpose of the evaluation is to evaluate the usefulness of concept frequency as a singular feature for identifying salient sentences for extractive text summarization. The evaluation was done by first asking three domain experts to manually generate extractive summaries from 24 biomedical texts (see Section 4.1 ). A series of automated summarizers (see Section 4.4 ) then generated summaries of the biomedical texts. The output of each summarizer is automatically compared using an automated tool called ROUGE ( Lin, 2005 ) (see Section 4.2 ). The results are given in Section 5 . The rest of this section details the evaluation implementation. 4.1. Corpus A corpus of 24 biomedical texts was generated from a citation database of oncology clinical trial papers.
The database contains approximately 1200 papers physicians feel are important to the field ( Brooks &amp; Suli-manoff, 2002 ). Of the 1200 papers cited, 24 were randomly selected. The PDF versions of these papers were then obtained and converted to plain-text format. The papers were manually processed to remove graphics, into an abstract text and a full-text source text (without the abstract). The number of papers chosen (24) was based on the minimum requirements of the ROUGE summary evaluation tool ( Lin, 2004 ) as well as the resources available to complete the manual processing of each paper. 4.2. Rouge
The ROUGE (Recall-Oriented Understudy for Gisting Evaluation ) tool (version 1.5.5) ( Lin &amp; Hovy, 2003 ) developed by the Information Science Institute at the University of Southern California was used. ROUGE is an automated tool which compares a generated summary from an automated system with one or more ideal summaries. The ideal summaries are called models. ROUGE uses N-grams to determine the overlap between a summary and the models. ROUGE was used in the 2004 and 2005 Document Understanding Conferences (DUC) (National Institute of Standards and Technology (NIST), 2005 ) as the evaluation tool. We used parameters from the DUC 2005 conference. ROUGE-2 and ROUGE-SU4 recall scores are used to measure each summarizer. ROUGE-2 evaluates bigram co-occurrence while ROUGE-SU4 evaluates  X  X  X kip bigrams, X  X  which are pairs of words (in sentence order) having intervening word gaps no larger than four words. 4.3. Model summaries
To compare summaries generated automatically from systems, we used four models (i.e., four ideal sum-maries) for each of the 24 papers. The models represent different versions of ideal summaries. The first model is the abstract of the paper (author X  X  summary). In addition, three models from three different domain experts were generated. The domain experts are medical students in their final year. Each was given the task of per-forming extractive text summarization by selecting 20% of the sentences within a paper which formed the best summary for that paper. Selecting a summary size was problematic. The news summarization domain typi-cally selects a size of less than five sentences, which represents about 20% of the size of a typical news story ( Goldstein, Kantrowitz, Mittal, &amp; Carbonell, 1999 ). It has been generally thought that a summary should be no shorter than 15% and no longer than 35% of the source text ( Hovy, 2005 ). 4.4. Additional summarizers
In this evaluation, eight additional extractive summarizers were randomly selected based on the type of summarization method and availability. There are roughly four categories of summarizers selected: baseline, frequency-based, multiple feature, and redundancy-sensitive, and we selected two summarizers in each cate-gory. The two baseline summarizers are Baseline-Lead, which sequentially selects the first 20% of sentences in the source text, and Baseline-Random, which randomly selects 20% of the sentences in the source text.
The frequency-based summarizers are AutoSummarize in Microsoft Word ( Microsoft Coporation, 2002 ) and Open Text Summarizer (OTS) ( Rotem, 2003 ). AutoSummarize is a feature of the Microsoft Word ( Micro-soft Coporation, 2002 ) word processing software, and although exact details of the algorithm are not docu-mented, online help for the product indicates sentences using frequently-used words are given a higher score than sentences containing low frequency words. OTS is an open source project where stemming can also be performed to eliminate word variations. The two summarizers using multiple features to identify sentences are SweSum ( Dalianis, 2000 ) and MEAD ( Radev et al., 2004 ). SweSum is a multi-lingual summarizer for
Swedish and English text using features such as sentence position and numerical data identification. MEAD is a single and multi-document summarizer using features such as position of sentence within the text, overlap of each sentence with the first sentence, sentence length, and a centroid method based on a cluster of related documents. Finally, the two summarizers which reduce information redundancy are Lemur Maximal Mar-ginal Relevance (MMR) ( The Lemur Project, 2006 ) and SumBasic ( Nenkova &amp; Vanderwende, 2005 ). Lemur
MMR iteratively selects sentences having a high query similarity to an automatically-generated query, and which are also maximally dissimilar to sentences already included in the summary. SumBasic uses a probabil-ity distribution of terms in the text, and reduces term probability as sentences containing the terms are selected. SumBasic was also adapted to use concepts as the input source, in addition to terms. 5. Results 5.1. Summarization method performance The ROUGE results for all summarizers is shown in Tables 1 and 2 . The following sections describe the
ROUGE performance of the various summarization methods. 5.1.1. BioChainSumm summarizer For the BioChainSumm summarizer, the most frequent strong chain concept (MostFrequentStrongChain-
Concept) scoring method outperforms the use of all strong chain concepts (AllStrongChainConcepts) accord-ing to both ROUGE-2 and ROUGE-SU4. The use of AllStrongChainConcepts does not have the effect of chain are used to score sentences. While at first the broad coverage may seem desirable for summarizing a com-a summary. By using the MostFrequentStrongChainConcept, the effect is to use the UMLS semantic type to find the general idea of a text, and then the main concept within the chain to further refine the idea. Both ver-sions of BioChainSumm outperform the two baseline summarizers Baseline-Lead and Baseline-Random, as well as the generic summarizers AutoSummarize, OTS, SweSum, and MEAD. This indicates the use of domain-specific concepts for selecting sentence is an improvement over the use of terms which lack domain-specific knowledge. However, neither version of BioChainSumm outperforms SumBasic, Lemur (MMR), FreqDist-
Summ, nor the hybrid ChainFreqSumm. All of these summarizers have a redundancy-removal component as part of their design. This shows that while BioChainSumm is effective at identifying important sentences within a domain, it falls short of removing redundancy, which is an important part of text summarization. 5.1.2. FreqDistSumm summarizer The term and concept versions of FreqDistSumm outperform all other summarizers except for the hybrid ChainFreqSumm (BioChain method plus the FreqDist method) (see Section 5.1.3 ). In both ROUGE-2 and
ROUGE-SU4, the term version of FreqDistSumm outperforms the concept version. Interestingly, this is also true for the SumBasic summarizer. We believe this is due to the fact that concepts map multiple instances of an expression to a single concept, and in single document summarization there is not enough variation in lan-guage to allow concepts to outperform terms. For example, in two articles discussing lung cancer, one article may use  X  X ung cancer X  repeatedly, whereas the second article uses  X  X ulmonary carcinoma. X  Using concepts, these two instances will be merged. When using terms, the different instances will remain unique. The FreqDist method keeps redundancy in check by only allowing it to occur in the same degree as the source text. Reiter-ation is a technique often used by authors to emphasize important points. The FreqDistSumm summarizer attempts to mirror the source-text in a reduced form, and so the reiteration will also be expressed in the gen-erated summary. This is a different approach than BioChainSumm, which finds all important sentences using domain-defined criteria, but does not then reduce any redundancy in the important sentences. In contrast,
FreqDistSumm will find a subset of source-text sentences using domain-defined criteria, but then eliminates some of the sentences from the subset if they emphasize main points more than the source text does. 5.1.3. ChainFreqSumm summarizer
As can be seen from the Baseline-Random summarizer, randomly picking sentences performs well. This is an indication that biomedical texts contain a large amount of redundancy. As can also be seen from the Bio-ChainSumm evaluation (see Section 5.1.1 ), redundancy can decrease summarizer performance. The hybrid
ChainFreqSumm summarizer (BioChain method plus the FreqDist method) is an attempt to find a subset of the most important sentences using domain-specific criteria, and then remove redundancy from the subset.
The ChainFreqSumm summarizer performs best when all concepts in the strong chains are used, which is the opposite of what occurs when the BioChain method is used alone. This is most likely because using all strong concepts results in a larger pool of sentences for the FreqDist method to select from. Using the ROUGE-SU4 metric, the hybrid ChainFreqSumm summarizer is the best performer, but is slightly outperformed by the
FreqDistSumm term method when the ROUGE-2 metric is used. The result in combining the two approaches is that the use of concept approaches for finding salient sentences is improved over the individual methods of
FreqDist and BioChain. We believe that a summarizer which (a) first identifies a subset of important sentences based on domain-specific criteria, and (b) then prunes the subset by removing redundancy leads to an effective domain-specific summarizer. 5.2. Physician evaluation
We provided a practicing oncologist our corpus of evaluation papers and asked him to select several papers and give his observations on the abstract and the full-text. He chose three papers based on their chaining per-formance in the original BioChainSumm evaluation ( Reeve, Han, Nagori et al., 2006 ). The intent is to sub-jectively evaluate the usefulness of abstracts of a full-text in the practice of oncology treatment. The following observations were made about each paper:
Paper #1 ( Riethmuller et al., 1998 ):  X  The abstract and the full-text had different information regarding the number of patients.  X  Information about the study design was not complete in the abstract, causing a misleading conclusion.
Paper #2 ( Perry et al., 1987 ):  X  The abstract gives conclusions without providing context of the results, such as follow-up time and response criteria.

Paper #3 ( Thomas et al., 1998 ):  X  The abstract makes a statement that is not made in the source text.

The physician concludes that the abstracts are a good starting point, but that they may miss critical infor-mation necessary for evaluating the results or overstate the conclusion. The physician also believes some bias is inherent in author-generated abstracts, and that additional ways of producing summaries are needed to address different needs of medical practitioners.

We followed up the physician X  X  evaluation to see if our FreqDistSumm-generated summaries addressed the shortcomings of the abstracts. For Paper #1, complete study design information was included. In Paper #2, the summary included additional context information, such as patient eligibility and stratification and ran-domization. In Paper #3, the full-text did not contain the information in the abstract and so could not possibly be extracted. Based on this short evaluation, we note that automatically generated summaries addressed abstract incompleteness in two out of three cases. 6. Conclusion We presented three novel semantic-based methods for extractive text summarization. The first method (Bio-
Chain) chains together semantically-related concepts, and then extracts sentences having concepts in the stron-gest of the chains. The second method (FreqDist) uses a frequency-distribution approach, where a summary is gradually constructed by adding new source sentences so that the summary and source text have similar con-cept frequency distributions. For single document summarization, we show that these two concept-based approaches are competitive with existing term-based approaches. In addition, we combine the two approaches (BioChain and FreqDist) to improve performance above all existing summarizers. The use of concepts can be more useful than terms for generating personalized summaries and multi-document summarization. An envi-sioned system allows a user to select domain-specific concepts important to the user, and then have the sum-marizer generate a summary where those concepts are more highly weighted than the concepts appearing in the source text.
 References
