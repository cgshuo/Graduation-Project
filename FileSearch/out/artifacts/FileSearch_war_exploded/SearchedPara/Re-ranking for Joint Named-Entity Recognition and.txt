 Recognizing names and linking them to structured data is a funda-mental task in text analysis. Existing approaches typically perform these two steps using a pipeline architecture: they use a Named-Entity Recognition (NER) system to find the boundaries of men-tions in text, and an Entity Linking (EL) system to connect the mentions to entries in structured or semi-structured repositories like Wikipedia. However, the two tasks are tightly coupled, and each type of system can benefit significantly from the kind of in-formation provided by the other. We present a joint model for NER and EL, called N ERE L, that takes a large set of candidate men-tions from typical NER systems and a large set of candidate en-tity links from EL systems, and ranks the candidate mention-entity pairs together to make joint predictions. In NER and EL exper-iments across three datasets, N ERE L significantly outperforms or comes close to the performance of two state-of-the-art NER sys-tems, and it outperforms 6 competing EL systems. On the bench-mark MSNBC dataset, N ERE L provides a 60% reduction in error over the next-best NER system and a 68% reduction in error over the next-best EL system.
 I.2.7 [ Natural Language Processing ]: Text analysis; I.3.1 [ Content Analysis and Indexing ]: Linguistic processing Named Entity Recognition; Entity Linking; Entity Disambiguation
Entity Linking (EL) [1, 3] is the task of identifying when a name that appears in text refers to a known entity in a reference set of named entities, such as a relational database or the set of articles in Wikipedia. Existing EL systems depend on knowing where the set of names in a text appear. Nearly all EL systems therefore use a pipeline architecture: they run a Named-Entity Recognition (NER) [6, 17] system on the text to identify the boundaries of names. They then use the names that were identified by the NER system, typi-cally called mentions , as the input set of names for the EL task Unfortunately, as is common with pipeline architectures for proba-bilistic models, errors from the NER system propagate to the EL system. Any mention that goes undetected by the NER system clearly cannot be linked correctly by the downstream EL system. Other common errors from NER systems include incorrectly split-ting a multi-word name into multiple mentions; joining together two mentions that happen to appear near one another; or leaving off one or more words from a multi-word mention. Each of these errors can cause the downstream EL system to generate too many, too few, or wrong links.

Consider the following phrase that has been incorrectly processed by a pipeline of a state-of-the-art NER system [17] and our own baseline EL system: The word  X  X he X  commonly precedes certain mentions, as in  X  X he [Rocky Mountains], X  so it is reasonable but incorrect for the NER system to place the mention boundaries around just  X  X iew X  in this example. Likewise, it is reasonable but incorrect in this context for an EL system to predict that  X  X iew X  refers to Pay _ Per _ V iew , a common method of watching certain television programs, since most of the surrounding text is related to television programming.
We develop a joint NER and EL model that we use to re-rank candidate mentions and entity links produced by base NER and EL models. Our base NER techniques would produce candidate mentions like ABC, View, and The View from the above phrase. Our base EL system would then propose several promising entity links for each candidate mention, including Pay _ Per _ V iew and The _ V iew (an American television program on a broadcast chan-nel called ABC ). The reranking model then chooses among the set of all possible mention and entity link labelings for the whole phrase to determine the best choice. Since the reranking model considers labels for  X  X BC X  and  X (The) View X  simultaneously, it can use features for known relationships between the television channel ABC and the television program The _ V iew to encour-age these as outputs. For efficiency, such features are not available to the pipeline models, which consider only one mention at a time. We use the pipeline models to prune the set of all possible candidate mentions and entity links to a manageable size while maintaining high recall. The reranking model can then use more sophisticated features for collective classification over this pruned set.
Previous work [3, 15, 9, 11] has considered collective classifi-cation for entities, but never for the combination of NER and EL. Ratinov et al. [18] argue that collective classification for entity link-ing provides only a small benefit over a purely-local model. How-ever, we find that by combining NER and EL, the set of correct mentions that are available to be linked improves drastically, and EL performance improves as a result.
For this paper we ignore type labels like Person, Location, or Or-ganization during NER, since we will link to a relational database with a finer-grained ontology.
The remainder of this paper is organized as follows: The next section discusses related work. Section 3 describes the Freebase database, and why it is suited to the EL task. Section 4 presents our joint model for NER and EL. Section 5 describes our methods for finding candidate mentions and entities, including a pipeline EL system for Freebase. Section 6 gives our experimental results, and Section 7 concludes.
All but one previous entity linking system that we are aware of uses a pipeline architecture. Ratinov et al. [18] point out that find-ing mentions using NER is a non-trivial task, and hence their exper-iments report linking performance using gold-standard mentions, as do several other recent EL systems [20, 4, 8]. Nevertheless, in practice NER performance limits the performance of these EL sys-tems. Our work relies on pipeline NER and EL techniques [1, 14, 22, 5, 18, 12, 13, 10, 20] to provide high-quality candidate men-tions and entity links. We then subject these candidates to further processing by a joint NER and EL model that outperforms state-of-the-art NER systems and state-of-the-art EL systems on their respective tasks.

Guo et al. [7] are the only authors we are aware of that consider joint mention detection and entity disambiguation, but their tech-niques were developed for, and are best-suited for, microblog text. Because microblog texts are so short, Guo et al. can use computa-tionally expensive machinery; they use a structural SVM algorithm which requires NP-hard inference. Their technique can consider nearly every token as part of a candidate mention, again because of the short length of microblog texts. In contrast, our techniques are better suited for longer documents. We use linear maximum-entropy models to re-rank a set of candidate mentions and entities provided by efficient NER and EL base models. A more minor dif-ference is that Guo et al. link to Wikipedia; our technique links to both Wikipedia and Freebase X  X  large, user-contributed, relational database. Also, Guo et al.  X  X  techniques cannot identify mention boundaries that have no corresponding Wikipedia entries, whereas our techniques can identify mentions with no corresponding en-tity in our reference set; we follow the Text Analysis Conference X  X  (TAC) guidelines in linking such mentions to the special symbol NIL .
Most existing large-scale entity linkers use Wikipedia titles as a reference set. We chose to use a combination of Freebase and Wikipedia as our reference set because it provides us all of the in-formation in the Wikipedia articles, as well as a well-developed ontology of types and binary relations available in Freebase. Free-base is a freely-available, user-contributed repository of structured data containing almost 40 million unique entities. In comparison, Wikipedia contains 4.2 million entities and DBpedia contains 3.77 million entities (all numbers are for the English versions). The Freebase schema is divided into domains , each of which contains its own set of types (such as people , books , bridges , etc. ) and properties , like date of birth for a person or latitude and longitude for a location. Freebase commons, which we use in our experiments, contains 86 different domains, including such diverse areas as schools, geography, sports, and astronomy. Ev-ery entity in Freebase is given a unique ID , and separately a set of possible names that can refer to the entity, through the built-in name and alias relations. Many Freebase entities also have links to their corresponding Wikipedia pages, when available. Fur-thermore, Freebase includes a built-in, nontrivial API for finding candidate entity links for a given name, which is based on the user-contributed name and alias relations.
While EL systems can go wrong even when provided correct mentions, we have found that in practice a large number of EL errors are caused by poor mention boundaries. Table 1 shows ex-amples of three common errors by NER systems that propagate to EL systems. Other types of errors include completely-missed men-tions, and correct mentions but incorrect links. Joint NER and EL can help with many of these problems by allowing for features that, for instance, favor joining two mentions that link to the same entity over having the same entity repeated twice. Labeled data for joint NER and EL consists of a set of documents D , and for each document d  X  D , a set Entities ( d ) of triples of the form ( l,r,e ) . Here, l and r indicate the left and right boundaries of the mention of some named entity, and e indicates the unique identifier of the named entity in a reference set. Given a labeled training set T of this form, the task is to learn a model that can predict Entities ( d ) for new documents d .
Classification over all possible sets of (mention, entity)-pairs for a document is generally intractable. Typical approaches to EL have addressed this by decomposing the task into a pipeline: first, iden-tify the set of mentions using a standard NER model. Second, determine a local model for P ( E | d,l,r ) , where E is a random variable over entity ids e . Many recent EL approaches have in-vestigated a third step, which is to select up to K tuples of entity ids ( e 1 ,...,e n ) , one e i for each mention in the document found by the NER model, and then build a model to rank these tuples of entity ids [1, 3]. However, Ratinov et al. [18] argue that this kind of global model provides a relatively small improvement over the purely-local approach, and Sil et al. [20] dispense with the global classification step entirely.

We instead adopt a different approach to decomposing the task, which offers a way to jointly classify mention boundaries and en-tity links. Rather than assume that the first-stage NER model pro-duces the correct mention boundaries for linking, we assume that it provides a high-recall set of potentially-overlapping candidate mentions. One way to produce this set is to develop a probabilistic NER model P ( M | d,l,r ) , where M is a binary random variable in-dicating whether l and r are the exact boundaries of a mention. By selecting a relatively low threshold for this distribution, the model can produce a high-recall set of candidate mentions for d that have some reasonable probability of being correct. In practice, we have found that an alternative approach of getting candidate mentions (described in Sec. 5.1), which uses an NP chunker and an exist-ing NER system, to be efficient and effective. Let A be the set of all sequences a i = ( m 1 ,...,m n i ) of mentions m such that each m is taken from the candidate set, and no two mentions in the same sequence have overlapping boundaries. As in the pipeline model, we also develop a local model P ( E | d,l,r ) , and select sev-eral promising candidates from the local model for each candidate mention. The joint classification task is over all tuples B of the form b i = ( m 1 ,e 1 ,...,m n i ,e n i ) , where each e j the local EL model X  X  candidate set for m j . We call these entity-mention tuples .

The cost of high recall in the set of candidate mentions is that the set of candidate mentions is large, and as a result B contains far too many candidate tuples for tractable classification. To decompose the problem, we observe that most words in (non-microblog) text are not part of named-entities. We adopt a simple heuristic of sep-arating all candidate mentions that are three or more words apart. In all three of the corpora in our experiments, this simple heuristic results in a partitioning of the mentions into small sets of mentions that appear near one another. We refer to these sets as the con-are shown in [square brackets], and entity links are indicated by  X  . nected components of d , or CC ( d ) . We perform classification over the set of entity-mention tuples B ( cc ) that are formed using can-didate mentions and entities within the same connected component cc  X  CC ( d ) . As an example, the phrase  X  X BC X  X  The View X  would constitute a connected component, since several candidate men-tions (ABC and View) are separated by three or fewer tokens. Two of the entity-mention tuples for this connected component would be ([ABC], ABC , [View], Pay-Per-View ) and ([ABC], ABC , [The View], The View ).

In over 98% of our training documents, all of the connected components contained tuples b i with n i  X  10 mentions. The few remaining large connected components were from documents de-scribing the results of cricket matches, where long lists of player names were separated only by one or two statistics each. For such long sequences, we heuristically divide them into chunks of a length that is the maximum length that our implementation can handle, which is 15. With relatively low thresholds for selecting candidate mentions and entities, the number of tuples in each connected com-ponent in our training data was 31.02 on average, while still allow-ing a maximum possible recall of 96%. In contrast, the maximum possible recall for a pipeline EL system using a standard NER sys-tem in our datasets is 82%. The joint model adds an extra layer of processing on top of the pipeline model, but in experiments the re-ranking step has nearly identical running time as the combination of our pipeline NER and EL models. Overall, this simple decom-position leads to a tractable joint NER and EL classification task.
We use a maximum-entropy model to estimate P ( b | d,cc ) , the probability of an entity-mention tuple b for a given connected com-ponent cc  X  CC ( d ) . The model involves a vector of real-valued feature functions f ( b,d,cc ) and a vector of real weights w , one weight per feature function. The probability is given by
We use L2-regularized conditional log likelihood (CLL) as the objective function for training: where ( b,d,cc )  X  T indicates that b is the correct tuple of entities and mentions for connected component cc in document d in train-ing set T , and  X  is a regularization parameter. We used LBFG-S for gradient-based convex optimization.
The primary advantage of the discriminative re-ranking approach is that it provides the flexibility to include many novel features into NER and EL. For instance, we include features that penalize NER boundaries that oversegment a mention by counting how often an entity-mention tuple b links to the same entity multiple times in a row. We also include features that help to learn and encourage common sequences of entity types, like (job title, employee name) and (city, country or state). These features help both to prevent un-dersegmentation in NER and to improve linking accuracy. Details of our features are below.

Token-level features: Given b , d , and cc , we define features that find the minimum ( MIN ), maximum ( MAX ) and the average ( AVG ) number of tokens inside the mentions in b , and a NUM feature for the total number of mentions in b . We also use fea-tures that measure how many tokens are not part of any mention in b , but are part of a mention in some other b Non _ Mention _ Tokens ( b,cc ) be the set of all such tokens, and All _ Tokens ( cc ) be the set of all tokens for cc . We construct the N
ON -M ENTION -C OUNT feature as | Non _ Mention _ Tokens ( b,cc ) | ,
Capitalization: Similar to [7], we include one binary feature to check if all candidate mentions in b are capitalized, one binary feature to check if all words in all mentions in b are capitalized, and one binary feature to check if there is any mention that has capitalized, non-mention tokens to its immediate left or right.
Features from entity links: We include a number of features that make use of the entity links in b , as well as the types and known relations between entities, as stored in Freebase. The most basic versions of these features include: C OUNT -N IL , which counts the number of entities that link to NIL , to help penalize tuples b that are poor matches with Freebase; C OUNT -E XACT -M ATCH , which counts the number of mentions whose surface form matches ex-actly with one of the names for the linked entity stored in Freebase; A LL -E XACT -M ATCH , which is true if all mentions in b match a Freebase name exactly; and A CRONYM -M ATCH , if the mention X  X  surface form is an acronym for a name or alias of the linked entity in Freebase.

The D ISTINCT -L INKS -P ER -M ENTION measures the ratio of distinct entities in b to mentions in b . This feature helps to penalize over-segmented phrases. This is because two mentions that should have been joined together often have high probability for links to the same entity, as in a bracketing of  X  X Home] [Depot] X  that splits this name for a business into two parts, each of which has a high probability of linking to the company Home Depot . Typically, any value of less than 1 for this feature is an indication that b is oversegmented, as it is uncommon to refer to the same entity twice in the short span of text that makes up one of our connected com-ponents, although appositives are an obvious exception.

To further establish the relation between mention candidates and their links, we define two features that mimic the Normalized Google Distance (NGD) as in [7, 18, 2]. For every consecutive pair of enti-ties ( e,e 0 ) that belongs to mentions in b , the NGD-F REEBASE fea-ture computes X sion, NGD-W IKIPEDIA , computes X where Wiki ( e ) denotes the set of tokens in the Wikipedia page of e , excluding stop-words. Two features, E NTITY -T YPE -PMI and E NTITY -T YPE -P RODUCT -PMI , make use of Freebase X  X  type system to find patterns of enti-ties that commonly appear next to one another. Let T ( e ) be the set of Freebase types for entity e . We remove common Freebase types which are associated with almost every entity in text, like /location/location or /people/person , since they have lower discriminating power. From the training data, the system first computes pointwise mutual information (PMI) [21] scores for the Freebase types of consecutive pairs of entities, ( e 1 ,e PMI ( T ( e 1 ) ,T ( e 2 )) = where the sum in the numerator is taken over consecutive pairs of entities ( e,e 0 ) in training data. The feature E PMI adds these scores up for every consecutive ( e 1 ,e 2 We also include another feature E NTITY -T YPE -P RODUCT -PMI which does the same, but uses an alternative product variant of the PMI score.

The B INARY -R ELATION -C OUNT feature encourages entity tu-ples b where the entities have a known relationship with one an-other. For instance, for the text  X  X ome Depot CEO Robert Nardelli X , we wish to encourage the tuple that links  X  X ome Depot X  to the en-tity id of the company Home Depot , and  X  X obert Nardelli X  to the entity id of the person Nardelli . Freebase contains a relation called organization_board_memberships that indicates that Nardelli is a board member of Home Depot . B INARY -R every consecutive pair of entities in b . This feature helps not just to improve entity linking accuracy; it also helps to prevent under-segmented phrases. For instance, an entity-mention tuple b that contains just a single mention for the whole phrase  X  X ome Depot CEO Robert Nardelli X  would have a lower count for this feature than the correctly-segmented version, even if the incorrect mention was linked to Nardelli .
 Finally, we include a similar version of this feature that uses Wikipedia rather than Freebase. The W IKIPEDIA -C OOCCURRENCE -C
OUNT feature computes, for every pair of consecutive entities ( e,e 0 )  X  b , the number of times that e 0 appears in the Wikipedia page for e , and vice versa . It adds these counts up to get a single number for b .
Previous work on EL systems [18, 7] have argued that EL sys-tems often suffer because of the mention detection phase. The impact is mostly because of false negative mentions: any missed mention by the NER system is also a missed entity for EL. To im-prove on the pipeline model, our joint system requires overgener-ation of candidate mentions: that is, a high-recall set of mentions with just enough precision to prevent the number of false positives from overwhelming subsequent processing.

Rather than modify an existing NER system to overgenerate men-tions, we adopt an approach of combining multiple existing sys-tems and heuristic techniques. We use the mentions found by the state-of-the-art UIUC NER system. In addition, we included strings matching the following heuristic tests as candidate mentions:
Let mention m = ( d,l,r ) consist of a document d and the left and right boundaries of the mention, and let S be a reference set containing entity ids and the special symbol NIL , which indicates an entity that does not match any id in the reference set. The task of a base (or local) EL model is to predict P ( E | m ) , where random variable E ranges over S . We call our base EL model B ASE B ASE EL, like our re-ranking model, is a maximum entropy model. Using Freebase and Wikipedia, we construct a vector of real-valued feature functions f ( m,s ) describing the degree to which m and s  X  S appear to match one another. The model includes a vector of real weights w , one weight per feature function.

As with the joint model, we use L2-regularized CLL as the ob-jective function, and LBFG-S for gradient-based convex optimiza-tion. We use only gold-standard mentions for training the base EL model. Our manually annotated data contains only positive exam-ples for entity ids. To generate negative examples, we use Free-base X  X  candidate generation technique (described in Section 3) to identify a set of potentially confusing bad matches.
 B ASE EL uses a combination of information from Wikipedia and Freebase to judge the similarity between a mention m and Free-base entity s . As a starting point, we leverage the four domain-independent features previously described by Sil et al. [20], adapted to Freebase. Intuitively, these features measure the number of at-tribute values and attribute names for entity s in Freebase that ap-pear in the context of m 2 .We add in Freebase features that take ad-vantage of its type system. One such feature, TF (m, s), counts, for each type t that entity s belongs to, the number of mentions in the context of m that have any candidate entities with the same type. The feature then adds up these counts for all the types of s . Ad-ditional features normalize these counts in various ways, and still other features make use of cosine similarity between the Wikipedia page for s and the context of m .
We evaluate the performance of N ERE L first on the NER task against 2 state-of-the-art NER systems, and second on the EL task against 6 state-of-the-art EL systems.

Datasets: We use 3 standard datasets for EL as our test sets: the ACE dataset, as annotated by Ratinov et al. [18] for Wikipedia links; the MSNBC dataset, as collected and annotated by Cucerzan [3] for mention boundaries and Wikipedia links; and the CoNLL-2003 NER dataset (testb), with YAGO2 and Freebase IDs added by Hoffart et al. [10]. Table 2 provides key statistics on these datasets. For datasets annotated with Wikipedia links, we automatically ex-tracted the corresponding Freebase IDs using Freebase X  X  built-in links to Wikipedia. In less than 1% of the examples for all the 3 test sets, Wikipedia contained a correct entity, but Freebase did not. We left the Wikipedia page as the correct label for these examples, and our B ASE EL and N ERE L systems always gets these examples wrong. In 10% of the MSNBC examples, Wikipedia had no entry, and the examples were marked NIL (ACE had no NIL entries). We left these NIL cases even when Freebase does contain a correct entity, to provide a fairer comparison with wikifiers. The MSNBC data consists of news articles covering 10 domains, 2 articles from each domain: Business, Entertainment, Politics, Science, etc, and is thus more diverse than either the CoNLL or ACE datasets.
The ACE dataset includes hand-labeled entity links for only the first nominal mention of each annotated coreference chain, and there are many correct mentions and entities which have not been labeled. This gold standard allows us to estimate the recall of an NER system, but not the precision, since many of the system X  X  guesses that don X  X  match the gold standard may still be correct. To
For a full list of features see [19]. Table 2: Number of mentions, average number of candidate referents, % of mentions that are NIL , and % of mentions that are in Freebase (and Wikipedia) in our datasets. We train our system on the Wikipedia dataset. estimate precision for NER on this dataset, we took a random sam-ple of 100 examples of each system X  X  guesses, and manually judged whether they were correct or not. For EL, we measured precision and recall using the gold-standard mentions in the dataset.
Our training dataset consists of 10,000 random Wikipedia pages, where all of the phrases that link to other Wikipedia articles are treated as mentions, and the target Wikipedia page is the label. The dataset is made available by Ratinov et al. For training, we disre-gard mentions for which there is no Freebase entity that links to the target Wikipedia page; for the remaining mentions, we use the corresponding Freebase entity as the label.

Evaluation Metric: For NER evaluation, we measure precision, recall and F1 scores. To evaluate EL accuracy, we report on a B ag-o f-F reebase ids (BOF) evaluation analogous to the Bag-of-Titles (BOT) F1 evaluation as introduced by [15, 18]. In BOF-F1, we compare the set of IDs output for a document with the gold set of IDs for that document (ignoring duplicates), and utilize standard precision, recall, and F1 measures. We separately computed exact-match accuracy, but the results were very similar to the BOF-F1 score, and we chose to report BOF-F1 as it allows for compari-son with previous systems. For more details on BOT-F1, see [18]. Note that by construction, BOT-F1 and BOF-F1 provide identical numbers for our experiment.

Competitors: We compare N ERE L with 2 state-of-the-art NER systems: the current state-of-the-art NER system for the bench-mark CoNLL 2003 test set, the UIUC NER system [17]; and the Stanford NER system [6]. The former uses a regularized averaged perceptron model and external gazetteers for strong performance. The latter uses Conditional Random Fields and Gibbs sampling to incorporate long-distance dependencies into its NER model; the constraints ensure that the same phrase should be labeled consis-tently within the same document. We downloaded these two sys-tems and ran the configurations that were reported to perform best on the CoNLL dataset. We found the results to be almost the same as reported by their authors.

The six EL competitors for N ERE L include: 1) Cuc07 [3], which was the first system to use collective classification of entity links. 2) MW08 [15], which uses the set of unambiguous mentions in the text surrounding a mention to define the mention X  X  context. MW08 uses Normalized Google Distance to compute the simi-larity between this context and the candidate Wikipedia entry. 3) Kul09 [11] is an extension of both Cuc07 and MW08. It uses a hill-climbing approach to compute the joint probability distribu-tion of all entity link assignments for a document. 4) Rat11 [18], the current state-of-the-art wikifier, has both a local and a global component. The local model uses techniques similar to traditional EL systems [14, 1] and similar to the Wikipedia-features used in our B ASE EL model. The global component uses the predictions of the local model for all mentions in a document as the context of a mention, and they use both NGD and PMI [21] to compute the similarity between this context and a candidate Wikipedia page. 5) Hof11 [10], a state-of-the-art database EL system, links mentions to the YAGO2 knowledge base. Their technique uses a dense co-herence graph algorithm to measure mention-entity similarity. 6) Sil12 represents our re-implementation of Sil et al.  X  X  [20] domain-independent database EL system, which we used as a starting point Table 3: N ERE L outperforms two state-of-the-art NER systems on two out of three datasets, and outperforms one of them on the third dataset.
 Table 4: N ERE L outperforms all competitors on the ACE and MSNBC datasets. We obtain the numbers for MW08 and Rat11 from [18], which does not report precision and recall. * indicates that the EL results are based on tests where gold-standard mentions are given as input. for our B ASE EL model. The four wikifiers and Hof11 use a col-lective classification framework, but only for collectively classify-ing sets of entity links, not for combining NER and EL. Sil12 and B ASE EL are purely local models.

Parameter Settings: N ERE L includes two important parame-ters:  X  , the regularization weight; and the number of candidate en-tities we select from B ASE EL. We set the value of  X  by trying five possible values in the range [0.1, 10] on held-out data. We chose 5 random documents as development data from the CoNLL-2003 training set. We found  X  = 0 . 5 to work best for our experiments. We chose to select 3 candidate entities from B ASE EL for each can-didate mention (or fewer if B ASE ELhad fewer than 3 links with nonzero probability), as higher thresholds led to a computationally intractable number of entity-mention tuples. As mentioned previ-ously, using 3 candidate links per mention resulted in a maximum possible recall for N ERE L of 0.96 across our test sets, which is significantly higher than the maximum possible recall of 0.82 for pipeline models.

Results: Table 3 shows the performance of N ERE L on the NER task. N ERE L achieves the highest recall and F1 scores on two of the three datsets, and second-highest on the third dataset (CoNLL). The Stanford NER system has the best precision on all three datasets, and the UIUC system has the best overall performance on CoNLL. On MSNBC, N ERE L outperforms both the state-of-the-art UIUC and Stanford NER systems by 0.14 and 0.07 gains in F1 score respectively. On the other datasets, differences are smaller, but N
ERE L consistently performs very well. We suspect that the gazetteers in the UIUC system help significantly on the CoNLL dataset. An-other problem for N ERE L on CoNLL was the presence of several long chains of 20 or more entities, which occurred in several arti-cles reporting the results of cricket matches. N ERE L tended to have much lower accuracy on these long chains of entities.
 Table 4 compares N ERE L with previously reported results by MW08 and Rat11 on the ACE and MSNBC datasets, as well as the results of our implementations of Sil12 and B ASE EL. N Table 5: N ERE L outperforms all previously-tested competitors on the CoNLL-2003 (testb) test set. Precision is shown at the highest recall achievable by each system. All competitors to N ERE L use only the correct mentions detected by the Stanford NER system as input, and thus their recall is limited by the recall of the Stanford NER system. achieves an F1 score of 85.9 on ACE and 84.6 on MSNBC, clearly outperforming all competitors. Compared with the state-of-the-art Rat11 wikifier, N ERE L improves by 0.086 and 0.097 F1 on ACE and MSNBC respectively. Part of this is due to better linking ac-curacy, probably resulting from our use of Freebase X  X  attributes, relations, and types for constructing features. Even the B and the relatively simple Sil12 system (ported to Freebase) outper-form the Rat11 wikifier, despite the fact that the Rat11 wikifier is given gold-standard mentions as input, and B ASE EL is given the output of the UIUC NER system.

However, another important aspect of N ERE L X  X  performance is its ability to correct poor mention boundaries that propagate to er-rors in linking. Compared with B ASE EL, N ERE L achieves sig-nificantly higher recall and F1, similar to the differences between N
ERE L X  X  NER performance and the performance of the UIUC NER system that B ASE EL uses in its pipeline. Overall, N proves by 0.052 and 0.071 F1 over B ASE EL on these two bench-mark datasets.

Table 5 illustrates the EL performance of N ERE L on the CoNLL dataset. In order to compare with Hoff11 [10], we replicate the ex-perimental evaluation that Hof11 consider most important: systems are evaluated according to their precision at their maximum possi-ble recall level. Furthermore, no mention candidates with NIL links are considered; in Hoff11 X  X  experiments, this resulted in the removal of 20% of the mention candidates. In Hoff11 X  X  evalua-tion, each system was supplied with the mentions from the Stan-ford NER system, and thus their recall was limited by the recall of the Stanford NER system. Hoff11 do not report this recall, but in our experiments we found that the Stanford system had a re-call of 0.783 on the CoNLL test set, or 0.801 when excluding NIL mentions. As before, N ERE L uses its overgeneration techniques to generate mention candidates. Its recall on this test set is 0.826; thus N ERE L outperforms the next-best system, Hoff11, in both pre-cision and recall.
Much of the previous research on entity linking has gone into improving linking accuracy over gold-standard mentions, but we observe that many of the common errors made by entity linkers in practice have to do with the pipeline architecture, which propagates errors from named-entity recognition systems to the entity linkers. We introduce a re-ranking model that performs joint named en-tity recognition and entity linking. The discriminative re-ranking framework allows us to introduce features into the model that cap-ture the dependency between entity linking decisions and mention boundary decisions, which existing models do not handle. Further-more, the model can handle collective classification of entity links, at least for nearby groups of entities. The joint NER and EL model has strong empirical results, outperforming a number of state-of-the-art NER and EL systems on several benchmark datasets while remaining computationally inexpensive.
 This material is based upon work supported by the National Science Foundation under Grant No. IIS-1218692. [1] R. Bunescu and M. Pasca. Using encyclopedic knowledge [2] R. Cilibrasi and P. Vitanyi. The google similarity distance. [3] S. Cucerzan. Large-scale named entity disambiguation based [4] A. Davis, A. Veloso, A. S. da Silva, W. Meira Jr, and A. H. [5] P. Ferragina and U. Scaiella. Tagme: on-the-fly annotation of [6] J. R. Finkel, T. Grenager, and C. D. Manning. Incorporating [7] S. Guo, M.-W. Chang, and E. K X c X man. To link or not to link? [8] X. Han, L. Sun, and J. Zhao. Collective entity linking in web [9] X. Han and J. Zhao. Named entity disambiguation by [10] J. Hoffart, M. A. Yosef, I. Bordino, H. Furstenau, M. Pinkal, [11] S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti. [12] T. Lin, Mausam, and O. Etzioni. Entity Linking at Web [13] P. N. Mendes, M. Jakob, and C. Bizer. Evaluating DBpedia [14] R. Mihalcea and A. Csomai. Wikify!: Linking documents to [15] D. Milne and I. H. Witten. Learning to link with wikipedia. [16] V. Punyakanok and D. Roth. The use of classifiers in [17] L. Ratinov and D. Roth. Design challenges and [18] L. Ratinov, D. Roth, D. Downey, and M. Anderson. Local [19] A. Sil. Exploring Re-ranking Approaches for Joint [20] A. Sil, E. Cronin, P. Nie, Y. Yang, A.-M. Popescu, and [21] P. D. Turney. Thumbs up or thumbs down? semantic [22] Y. Zhou, L. Nie, O. Rouhani-Kalleh, F. Vasile, and
