 Recent advances in music retrieval and recommendation al-gorithms highlight the necessity to follow multimodal ap-proaches in order to transcend limits imposed by methods that solely use audio, web, or collaborative filtering data. In this paper, we propose hybrid music recommendation algo-rithms that combine information on the music content , the music context , and the user context , in particular, integrat-ing location-aware weighting of similarities. Using state-of-the-art techniques to extract audio features and contextual web features, and a novel standardized data set of music lis-tening activities inferred from microblogs ( MusicMicro ), we propose several multimodal retrieval functions.

The main contributions of this paper are (i) a systematic evaluation of mixture coefficients between state-of-the-art audio features and web features, using the first standard-ized microblog data set of music listening events for retrieval purposes and (ii) novel geospatial music recommendation approaches using location information of microblog users, and a comprehensive evaluation thereof.
 Information systems [ Information search and retrieval ]: Music retrieval; Human-centered computing [ Collaborative and social computing ]: Social media mining Hybrid Music Recommendation, Evaluation
The field of Music Information Retrieval (MIR) is seeing a paradigm shift, away from system-centric perspectives to-wards user-centric approaches [3]. In this vein, incorporating user models and addressing user-specific demands in music retrieval and music recommendation systems is becoming more and more important.

We present several approaches that combine music con-tent , music context , and user context aspects to build a hy-brid music retrieval system [12]. Music content and music context are incorporated using state-of-the-art feature ex-tractors and corresponding similarity estimators. The user context is addressed by taking into account musical prefer-ence and geospatial data , using a standardized collection of listening behavior mined from microblog data [11].
 We make use of the best feature extraction and similarity computation algorithms currently available to model music content and music context . We then integrate these similar-ity models as well as a user context model into a novel user-aware music recommendation approach that encompasses all three modalities important to human music perception [12].
The main contributions of this paper are: (i) a systematic evaluation of combining audio-and web-based state-of-the-art approaches to music similarity measurement and (ii) two approaches to incorporate geospatial information into music recommendation algorithms.

The remainder of the paper is organized as follows. Sec-tion 2 details the acquisition of the raw music (meta-)data, which serves as input to the feature extraction and data rep-resentation techniques presented in Section 3. In Section 4, we construct different hybrid (music content and music con-text) models and systematically evaluate their mixture co-efficients. Section 5 proposes two methods to incorporate geospatial information into music recommendation models. These extended models are evaluated and compared to the models without geospatial data and to a random baseline. Section 6 briefly reviews related literature. Eventually, Sec-tion 7 draws conclusions and points to further directions.
The only standardized public data set of microblogs, as far as we are aware of, is the one used in the TREC 2011 and 2012 Microblog tracks 1 [4]. Although this set contains ap-proximately 16 million tweets, it is not suited for our task as it is not tailored to music-related activities, i.e. the amount of music-related posts is marginal.

We hence have to acquire multimodal data sets of mu-sic items and listeners , reflecting the three broad aspects of human music perception ( music content , music context , and user context ) [12]. Whereas the music content refers to all information that is derived from the audio signal it-self (such as ryhthm, timbre, or melody), the music context covers contextual information that cannot be derived from http://trec.nist.gov/data/tweets the actual audio with current technology (e.g., meaning of song lyrics, background of a performer, or co-listening re-lationships between artists). The user context encompasses all information that are intrinsic to the listener. Examples range from musical education to spatiotemporal properties to physiological measures to current activities.
 Only very recently a data set of music listening activities inferred from microblogs has been released [11]. It is en-titled MusicMicro and is freely available 2 , fostering repro-ducibility of social media-related MIR research. This data set contains about 600 , 000 listening events posted on Twit-ter 3 . Each event is represented by a tuple &lt; twitter-id, user-id, month, weekday, longitude, latitude, country-id, city-id, artist-id, track-id &gt; , which allows for spatiotemporal identi-fication of listening behavior.
 Based on the lists of artist and song names in the MusicMi-cro collection, we gather snippets of the songs from 7dig-ital 4 . These serve as input to the music content feature extractors (cf. Section 3).
 To capture aspects of human music perception which are not encoded in the audio signal, we extract music-related web pages that represent such contextual information. Following the approach suggested in [13], we retrieve the top 50 web pages returned by Bing 5 for queries comprising the artist name 6 and the additional keyword  X  X usic X , to disambiguate the query for artists such as  X  X ush X  or  X  X iss X .

In summary, we gathered raw data covering each of the three categories of perceptual music aspects [12]: music con-tent (audio snippets), music context (related web pages), and user context (user-specific music listening events with spatiotemporal labels).
To represent the music content , we use state-of-the-art audio features proposed in [7]. These algorithms won three times in a row (since 2010) the annually run benchmarking activity Music Information Retrieval Evaluation eXchange (MIREX):  X  X udio Music Similarity and Retrieval X  task 7 They hence constitute the reference in music feature extrac-tion for similarity-based retrieval tasks. More precisely, we extract the auditory music features proposed in [7], which combine various rhythmic features derived from the audio signal, e.g.,  X  X nset patterns X  and  X  X nset coefficients X  (note onsets), with timbral features, e.g.,  X  X el Frequency Cepstral Coefficients X  (coarse description of the amplitude envelop). The eventual output is pairwise similarity estimates between songs, which are later aggregated to the artist level.
We again employ a state-of-the-art technique to obtain features reflecting the music context . To describe the mu-http://www.cp.jku.at/musicmicro http://www.twitter.com http://www.7digital.com http://www.bing.com
Please note that issuing queries at the song level is not reasonable, as doing so typically yields only very few results. http://www.music-ir.org/mirex/wiki/2012:Audio_ Music_Similarity_and_Retrieval sic items at the artist level, we follow the approach pro-posed in [13]. In particular, we model each artist by creat-ing a  X  X irtual artist documents X , i.e. we concatenate all web pages retrieved for the artist. In accordance with findings of [10], we then use a dictionary of music-related terms (genres, styles, instruments, and moods) to index the resulting doc-uments. From the index, we compute term weights accord-ing to the best feature combination found in the large-scale experiments of [13]: TF_C3.IDF_I.SIM_COS , i.e. computing term weight vectors and artist similarity estimates according to Equations 1, 2, and 3, respectively for tf , idf , and cosine similarity ; f d,t represents the number of occurrences of term t in document d , N is the total number of documents, D t is the set of documents containing term t , F t is the total number of occurrences of term t in the document collection, T d is the set of distinct terms in document d , and W d is the length of document d .
All components of the data set used in this paper are publicly available. The sole exception is the actual audio content of the songs under consideration. We cannot share them due to copyright restrictions. However, we provide identifiers by means of which corresponding 30-second-clips can be downloaded from 7digital . If you are interested in the data sets, please contact the first author.
A main research question is how to ideally combine audio and web features for music retrieval. Although a few MIR researchers suggest such a combination [2, 1, 3, 12, 5], a sys-tematic evaluation of combining state-of-the-art similarity estimators is still missing, hence provided here.
In a preprocessing step, we aggregate the audio features on the artist level, as they are computed on single tracks. To obtain audio similarities asim ( i,j ) between two artists i and j , we compute the minimum of the distances between all pairs of tracks by i and j as the minimum yielded the best results in preliminary experiments similar to the ones described later in this section. Web similarities wsim ( i,j ) are already defined on the artist level. Both, audio and web similarites, are normalized using the global distance scaling method Mutual Proximity [14].

Linear combinations of web similarities and audio simi-larities yield a hybrid similarity function sim ( i,j ) between artists i and j . It is given in Equation 4, where  X  is the mix-ture coefficient, i.e., the weight of the audio part, different values of which we systematically evaluate.
As gold standard we use genre information and assess re-trieval performance via the overlap between the genres as-signed to the query artist and those assigned to his K nearest Table 1: Overlap scores for different mixture coeffi-cients  X  between web and audio features. neighbors according to the similarity function under investi-gation. This is a standard evaluation approach in MIR. We gather genre information by (i) retrieving the top tags for each artist via the Last.fm API 8 and (ii) using the top 20 main genres from allmusic 9 to index the sets of tags.
To evaluate retrieval performance, we use a Jaccard-like overlap measure, shown in Equations 5 and 6, where i is the query artist, Genres i is the set of genres assigned to i , K is the number of i  X  X  nearest neighbors to consider, and A is the number of all artists in the data set. The range of the performance measures is [0 , 1], i.e., they are 1 . 0 if the genres of the seed artist i  X  X  K nearest neighbors perfectly overlap with those of i .
Performance scores for the hybrid retrieval function for different mixture coefficients  X  are shown in Table 1, to-gether with results for a random baseline. Although using only web features (  X  = 0 . 0) yields better results than us-ing audio only (  X  = 1 . 0), adding a small amount of content features to web features (or vice versa) boosts performance considerably. Adding a small amount of a complementary similarity component thus proves highly beneficial. Overall, values of  X  around 0 . 15 perform best. We hence use Equa-tion 7 as hybrid (audio and web features) music model (MU) for subsequent experiments. Building recommendation systems requires a user model. In our case, each user u is modeled by the set of artists UM ( u ) he listened to. Based on this simple model, we im-plement the following recommendation strategies: (i) the hybrid music retrieval model (MU) elaborated in the previ-ous section and (ii) a standard collaborative filtering (CF) http://www.last.fm/api http://www.allmusic.com Abbreviation Description BL random baseline MU hybrid music model (Equation 7) CF collaborative filtering model
CF-GEO-Lin CF model: geospatial user weighting
CF-GEO-Gauss CF model: geospatial user weighting
Table 2: Overview of recommendation models. model. In the MU model, the hybrid music similarity func-tion (Equation 7) is used to determine the artists closest to UM ( u ), which are then recommended. In the CF model, the users closest to u are determined (using the Jaccard index between the user models), and the artists listened to by these nearest users are recommended. For comparison, we further implemented a random baseline model (BL) that randomly picks K users from the filtered user set (via the parameter  X  , see below) and recommends the artists they listened to. To integrate geospatial information into the CF model, we first compute a centroid of each user u  X  X  geospatial listening dis-tribution  X  u [  X , X  ] 10 . We then use the normalized geodesic distance gdist ( u,v ) (Equation 8) between the seed user u and each other user v to weight the distance based on the user models. To this end, we propose two different weight-ing schemes: linear weighting and weighting according to a Gaussian kernel around  X  u [  X , X  ]. We eventually obtain a geospatially modified user similarity sim ( u,v ) by adapting the Jaccard index between UM ( u ) and UM ( v ) via geospatial, linear or Gauss weighting, according to Equation 9 (GEO-Lin) or Equation 10 (GEO-Gauss), respectively. We recom-mend the artists listened to by u  X  X  nearest users v . Table 2 summarizes all investigated recommendation algorithms. gdist ( u,v ) = arccos ( sin(  X  u [  X  ])  X  sin(  X  v [  X  ]) + cos(  X 
In order to ensure sufficient artist coverage of users, we evaluate our models using different thresholds  X  for the min-imum number of unique artists a user must have listened to in order to include him in the experiments. We vary  X  be-tween 50 and 150 using a step size of 10. Denoting as U  X  number of users in the MusicMicro data set with equal or more than  X  unique artists, we perform U  X  -fold leave-one-out cross-validation for each value of  X  .
Figure 1 shows accuracies for K = [3 , 5] nearest neighbors and  X  = [50 ... 150]. We can see that all approaches sig-nificantly outperform the random baseline. Comparing the MU approach with the CF approaches, it is evident that CF generally works better for data sets with high numbers of users (smaller  X  ), while content-based MU outperforms CF
It is common to denote longitude by  X  and latitude by  X  . when the number of users is restricted. This finding suggests a combination of MU and CF, which will be addressed as part of future work. As for geospatial weighting, a similar observation comparing the linear weighting with the Gauss weighting can be made. The more active the users (higher  X  ), the better the performance of the linear weighting ap-proach, and the worse the Gauss kernel approach. An expla-nation for this may be that very frequent users of Twitter typically live in agglomerations, whereas occasional twitter-ers live in less densely populated areas. For these users in rural areas, a Gauss weighting is seemingly beneficial as very nearby users frequently know each other and share common music tastes (which is not true for highly populated areas). The models that integrate geospatial information outper-form the standard CF model for high  X  values, indicating again that this kind of information is beneficial for  X  X ower users X , who typically live in densely populated areas.
Specific related work on geospatial music retrieval is very sparse, probably due to the fact that geospatially annotated music listening data is hardly available. Among the few works, Park et al. [6] use geospatial positions and suggest music that matches a selected environment, based on aspects such as ambient noise, surrounding, or traffic. Raimond et al. [8] combine information from different sources to derive geospatial information on artists, aiming at locating them on a map. Zangerle et al. [15] use a co-occurrence-based approach to map tweets to artists and songs and eventually construct a music recommendation system. However, they do not take location into account.

On a more general level, this work relates to context-based and hybrid recommendation systems, a detailed re-view of which is unfortunately beyond the scope of the pa-per. A comprehensive elaboration, including a decent liter-ature overview, can be found in [9].
We presented the first systematic evaluation of hybrid mu-sic retrieval approaches (combining the currently best per-forming audio/music content and web/music context fea-tures), using a recently published, standardized data set of music listening activities mined from microblogs. Experi-ments showed that a linear mixture coefficient of 0 . 15 for Figure 1: Accuracy plots for different values of K and  X  . the audio part and 0 . 85 for the web component performed best, overall. Interestingly, adding only a very small amount of audio-based information to web features (or vice versa) considerably improves results.
 To the best of our knowledge, this is also the first work that integrates geospatial information into music recommenda-tion algorithms . Experiments indicate that including geospa-tial information is particularly beneficial for music recom-mendation when users listen to many different artists. The collaborative filtering approach (CF) outperforms the hybrid music retrieval model (MU) when the data set comprises a high number of users who listen to less artists, overall.
Future work will include considering more diverse data about the user context, such as demographics, listening time (hour of day, working day versus weekend), or gender. In addition, we plan to combine the MU and the CF models, including geospatial weighting. As a further usage scenario, we target users frequently traveling around the world and wanting to listen to music tailored to their current location, but also complying to their music taste. We will look into adapting our approaches accordingly.
 This research is supported by the Austrian Science Fund (FWF): P22856, P24095, P25655, and the EU FP7: 601166.
