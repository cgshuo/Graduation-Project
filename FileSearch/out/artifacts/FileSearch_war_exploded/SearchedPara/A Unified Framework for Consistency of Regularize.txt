 CSAIL, MIT, Cambridge, MA 02139, USA Several problems in machine learning can be modeled as the minimization of an empirical loss , which is computed from some available training data. Assuming that data sam-ples come from some unknown arbitrary distribution, we can define the expected loss as the expected value of the empirical loss. The minimizers of the empirical and ex-pected loss are called the empirical minimizer and the true hypothesis , respectively.
 One of the goals in machine learning is to infer properties of the true hypothesis by having access to a limited amount of training data. One largely used property in the context of classification and regression is loss consistency which mea-sures the generalization ability of the learning algorithm. Loss consistency guarantees are usually stated as an upper bound on the difference between the expected loss of the empirical minimizer and that of the true hypothesis. An-other set of properties relates to the ability to recover the true hypothesis. Norm consistency measures the distance between the empirical minimizer and the true hypothesis. Sparsistency refers to the recovery of the sparsity pattern (i.e. support recovery) of the true hypothesis, while sign consistency refers to the recovery of the signs of the true hypothesis. We expect these guarantees to become stronger as we have access to more data samples.
 In many settings, these guarantees are made possible by the use of a regularizer in the learning process. Consis-tency guarantees are now available for several specific reg-ularized loss minimization problems. We can hardly do justice to the body of prior work, and we provide a few references here. The work on linear regression includes the analysis of: the sparsity promoting ` 1 -norm (Wain-wright, 2009b), the multitask ` 1 , 2 and ` 1 , 1 -norms (Ne-gahban &amp; Wainwright, 2011; Obozinski et al., 2011), the multitask ` 1 , 2 -norm for overlapping groups (Jacob et al., 2009), the dirty multitask regularizer (Jalali et al., 2010), the Tikhonov regularizer (Hsu et al., 2012), and the trace norm (Bach, 2008). The analysis of ` 1 -regularization has also been performed for: the estimation of exponential fam-ily distributions (Kakade et al., 2010; Ravikumar et al., 2008; Wainwright et al., 2006), generalized linear mod-els (Kakade et al., 2010; van de Geer, 2008; Yang et al., 2013), and SVMs and logistic regression (Rocha et al., 2009; van de Geer, 2008). These works have focused on norm consistency and sparsistency, with the exception of (Jalali et al., 2010; Obozinski et al., 2011; Ravikumar et al., 2008; Rocha et al., 2009; Wainwright, 2009b; Wainwright et al., 2006) which also analyzed sign consistency, and (Hsu et al., 2012; Kakade et al., 2010) which also analyzed loss consistency. We refer the interested reader to the arti-cle of (Negahban et al., 2012) for additional references. There has been some notable contributions which char-acterize general frameworks with theoretical guarantees. Loss consistency for bounded losses and different no-tions of stability of the learning algorithm was analyzed in (Bousquet &amp; Elisseeff, 2002; Mukherjee et al., 2006; Rakhlin et al., 2005; Shalev-Shwartz et al., 2010). Sta-bility follows from the use of regularization for many dif-ferent problems (Bousquet &amp; Elisseeff, 2002). A two-level framework for loss consistency of bounded losses was provided by (van de Geer, 2005): a regularized outer-minimization is performed with respect to a set of model classes, while an unregularized inner-minimization is done with respect to functions on each class. Norm consistency for restricted strongly convex (i.e. strongly convex with respect to a subset of directions) losses and certain type of regularizers was analyzed in (Lee et al., 2013; Loh &amp; Wainwright, 2013; Negahban et al., 2009; 2012; Yang &amp; Ravikumar, 2013). In (Lee et al., 2013; Negahban et al., 2009; 2012; Yang &amp; Ravikumar, 2013) the loss is differen-tiable and convex, and the regularizer is a mixture of de-composable norms; while in (Loh &amp; Wainwright, 2013) the loss is differentiable and nonconvex, and the regular-izer is coordinate-separable, symmetric and nondecreasing, among other technical requirements. The work of (Bous-quet &amp; Elisseeff, 2002; Mukherjee et al., 2006; Rakhlin et al., 2005; Shalev-Shwartz et al., 2010; van de Geer, 2005) focus on loss consistency and requires an everywhere bounded loss. On the other hand, the work of (Lee et al., 2013; Loh &amp; Wainwright, 2013; Negahban et al., 2009; 2012; Yang &amp; Ravikumar, 2013) focus on norm consis-tency and requires a differentiable loss. The framework of (van de Geer, 2005) requires a measure of complexity for each model class as well as over all classes (which are infinity for the problems that we analyze here). Finally, the availability of independent and identically distributed sam-ples is a requirement for all these previous works. In this paper, we characterize a family of regularized loss minimization problems that fulfill three properties: scaled uniform convergence, super-scale regularization and norm-loss monotonicity. We show loss consistency, norm con-sistency, sparsistency and sign consistency. We show that several problems in the literature fall in our framework, such as the estimation of exponential family distributions, generalized linear models, matrix factorization problems, nonparametric models and PAC-Bayes learning. Similarly, several regularizers fulfill our assumptions, such as sparsity promoting priors, multitask priors, low-rank regularizers, elastic net, total variation, dirty models, quasiconvex reg-ularizers, among others. Note that our theoretical results imply that loss consistency, norm consistency, sparsistency and sign consistency hold for any combination of losses and regularizers that we discuss here. Many of these com-binations have not been previously explored. We first characterize a general regularized loss minimiza-tion problem. To this end, we define a problem as a tuple  X  =( H , D , b L n , R ) for a hypothesis class H , a data dis-tribution D , an empirical loss b L n and a regularizer R clarity of presentation, we assume that H is a normed vec-tor space.
 Let  X  be a hypothesis belonging to a possibly unbounded hypothesis class H . Let b L n (  X  ) be the empirical loss of samples drawn from a distribution D . We do not assume ei-ther independence or identical distribution of the samples. Let R (  X  ) be a regularizer and n &gt; 0 be a penalty param-eter. The empirical minimizer is given by: We relax this optimality assumption by defining an  X  -approximate empirical minimizer b  X  n with the following property for  X  0 : b
L n ( b  X  n )+ n R ( b  X  n )  X   X  +min Let L (  X  )= E D [ b L n (  X  )] be the expected loss. The true hypothesis is given by: In this paper, we do not assume boundedness, convexity or smoothness of b L n , R or L , although convexity is a very useful property from an optimization viewpoint.
 In order to illustrate the previous setting, consider for in-stance a function ` ( x |  X  ) to be the loss of a data sample x given  X  . We can define b L n (  X  )= 1 the empirical loss of n i.i.d. samples x (1) ,..., x ( n ) drawn from a distribution D . Then, L (  X  )= E x  X  D [ ` ( x |  X  )] the expected loss of x drawn from a distribution D . Our framework is far more general than this specific example. We do not require identically distributed samples. In order to gain some theoretical understanding of the general problem defined above, we make some rea-sonable assumptions. Therefore, we extend the prob-lem tuple with additional terms related to these assump-tions. That is, we define a problem as a tuple  X  0 = ( H , D , b L n , R , " n, ,c,r,b ) for a hypothesis class distribution D , an empirical loss b L n , a regularizer R form convergence rate " n, and scale function c , a regular-izer lower bound r and norm-loss function b . In what fol-lows, we explain in detail the use of these additional terms. 2.1. Scaled Uniform Convergence First, we present our definition of scaled uniform conver-gence, which differs from regular uniform convergence. In both uniform convergence schemes, the goal is to find a bound on the difference between the empirical and ex-pected loss for all  X  . In regular uniform convergence such bound is the same for all  X  , while in scaled uniform con-vergence the bound depends on the  X  X cale X  of  X  . For finite and infinite dimensional vector spaces as well as for func-tion spaces, we can choose the scale to be the norm of  X  . For the space of probability distributions, we can choose the scale to be the Kullback-Leibler divergence from the distribution  X  to a prior  X  (0) . Next, we formally state our definition.
 Assumption A (Scaled uniform convergence) . Let c : H ! [0; + 1 ) be the scale function. The empirical loss b L n is close to its expected value L , such that their abso-lute difference is proportional to the scale of the hypothesis  X  . That is, with probability at least 1 over draws of n samples: where the rate " n, is nonincreasing with respect to n and . Furthermore, assume lim n ! + 1 " n, =0 for 2 (0; 1) . In settings with a bounded complexity measure (e.g. em-pirical risk minimization with finite hypothesis class, VC dimension, Rademacher complexity), the regular uniform convergence statement is as follows ( 8  X  2 H ) | b L n (  X  ) L (  X  ) |  X  " n, . This condition is sufficient for loss consis-tency and regularization is unnecessary. This also occurs with a bounded hypothesis class H , since in that case we can relax Assumption A to ( 8  X  2 H ) | b L n (  X  ) L (  X  ) |  X  " 2.2. Super-Scale Regularizers Next, we define super-scale regularizers . That is, regular-izers that are lower-bounded by a scale function. Assumption B (Super-scale regularization) . Let c : H ! [0; + 1 ) be the scale function. Let r : [0; + 1 ) ! [0; + 1 ) be a function such that: The regularizer R is bounded as follows: Note that the above assumption implies that c (  X  )  X  R (  X  ) We opted to introduce the r function for clarity of presen-tation. 2.3. Norm-Loss Monotonicity Finally, we state our last assumption of norm-loss mono-tonicity . For clarity of presentation, we use the ` 1 -norm in the following assumption. (The use of other norm that upper-bounds the ` 1 -norm, modifies the norm consistency result in Theorem 2 with respect to the new norm, and leaves the sparsistency and sign consistency result in The-orem 3 unchanged.) Assumption C (Norm-Loss monotonicity) . Let b : [0; + 1 ) ! [0; + 1 ) be a nondecreasing function such that b (0) = 0 . The expected loss L around the true hypothesis  X  Furthermore, we define the inverse of function b as: As we discuss later, nonsmooth strongly convex functions,  X  X inimax bounded X  functions (which includes some con-vex functions and all strictly convex functions) and some family of nonconvex functions fulfill this assumption. Note that Assumption C is with respect to the expected loss and therefore it holds in high dimensional spaces (e.g. for domains with nonzero Lebesgue measure). On the other hand, it is trivial to provide instances where strong convex-ity with respect to the empirical loss does not hold in high dimensions, as shown by (Negahban et al., 2009; 2012; Bickel et al., 2009) in the context of linear regression. 3.1. Loss Consistency First, we provide a worst-case guarantee of the difference between the expected loss of the  X  -approximate empirical minimizer b  X  n and that of the true hypothesis  X   X  . Theorem 1 (Loss consistency) . Under Assumptions A and B, regularized loss minimization is loss-consistent. That is, for  X  1 and n =  X " n, , with probability at least 1 : (See Appendix A for detailed proofs.) Given the above result, one would be tempted to make R (  X  )= c (  X  ) in order to minimize the upper bound. In practice, the function c (  X  ) is chosen in order to get a good rate " n, in Assumption A. The regularizer is chosen in or-der to obtain some desired structure in the empirical mini-mizer b  X  n .
 3.2. Norm Consistency Here, we provide a worst-case guarantee of the distance between the  X  -approximate empirical minimizer b  X  n and the true hypothesis  X   X  .
 Theorem 2 (Norm consistency) . Under Assumptions A, B and C, regularized loss minimization is norm-consistent. That is, for  X  1 and n =  X " n, , with probability at least 1 : As mentioned before, we use the ` 1 -norm for clarity of presentation. (The use of other norm that upper-bounds the ` 1 -norm in Assumption C, modifies Theorem 2 with respect to the new norm.) 3.3. Sparsistency and Sign Consistency Next, we analyze the exact recovery of the sparsity pattern (i.e. support recovery or sparsistency) as well as the signs (i.e. sign consistency) of the true hypothesis  X   X  , by using the  X  -approximate empirical minimizer b  X  n in order to infer these properties. A related problem is the estimation of the sparsity level of the empirical minimizer as analyzed in (Bickel et al., 2009; Kakade et al., 2010). Here, we are interested in the stronger guarantee of perfect recovery of the support and signs.
 Our approach is to perform thresholding of the empiri-cal minimizer. In the context of ` 1 -regularized linear re-gression, thresholding has been previously used for obtain-ing sparsistency and sign consistency (Meinshausen &amp; Yu, 2009; Zhou, 2009). (See Appendix B for additional discus-sion.) Next, we formally define the support of a hypothesis and a thresholding operator. The support S of a hypothesis  X  is the set of its nonzero elements, i.e.: A hard-thresholding operator h : H  X  R ! H converts to zero the elements of the hypothesis  X  that have absolute value smaller than a threshold  X  . That is, for each i we have: In what follows, we state our sparsistency and sign consis-tency guarantees. The minimum absolute value of the en-tries in the support has been previously used in (Ravikumar et al., 2008; Tibshirani, 2011; Wainwright, 2009a;b; Zhou, 2009).
 Theorem 3 (Sparsistency and sign consistency) . Under Assumptions A, B and C, regularized loss minimization followed by hard-thresholding is sparsistent and sign-consistent. More formally, for  X  1 , n =  X " n, and  X  = b ( " n, (  X  R (  X   X  )+ c (  X   X  )) +  X  ) , the solution e  X  = h ( has the same support and signs as the true hypothesis  X   X  bility at least 1 :
S ( e  X  )= S (  X   X  ) , ( 8 i 2 S (  X   X  )) sgn( e  X  i ) = sgn(  X  Our result also holds for the  X  X pproximately sparse X  setting by constructing a thresholded version of the true hypothe-sis. That is, we guarantee correct sign recovery for all i such that |  X   X  4.1. Losses with Scaled Uniform Convergence First, we show that several problems in the literature have losses that fulfill Assumption A.
 Maximum Likelihood Estimation for Exponential Fam-ily Distributions. First, we focus on the problem of learning exponential family distributions (Kakade et al., 2010). This includes for instance, the problem of learn-ing the parameters (and possibly structure) of Gaussian and discrete MRFs. While the results in (Kakade et al., 2010; Ravikumar et al., 2008) concentrate on ` 1 -norm regulariza-tion, here we analyze arbitrary norms.
 Claim i (MLE for exponential family) . Let t ( x ) be the suf-ficient statistics and Z (  X  )= function. Given n i.i.d. samples, let b T n = 1 and T = E x  X  D [ t ( x )] be the empirical and expected suf-ficient statistics, respectively. Let b L n (  X  )= h b T n log Z (  X  ) and L (  X  )= h T ,  X  i + log Z (  X  ) be the empiri-cal and expected negative log-likelihood, respectively. As-sumption A holds with probability at least 1 , scale func-tion c (  X  )= k  X  k and rate " n, , provided that the dual norm fulfills k b T n T k  X   X  " n, .
 For sub-Gaussian t ( x ) , we can obtain a rate " n, 2 O ( nite variance, we can obtain a rate " n, 2 O ( Appendix D.) Generalized Linear Models. We focus on generalized linear models, which generalizes linear regression when Gaussian noise is assumed. This also includes for in-stance, logistic regression and compressed sensing with exponential-family noise (Rish &amp; Grabarnik, 2009). For simplicity, we chose to analyze the fixed design model. That is, we analyze the case in which y is a random variable and x is a constant.
 Claim ii (GLM with fixed design) . Let t ( y ) be the suf-ficient statistics and Z (  X  )= function. Given n independent samples, let b L n (  X  )=
P tors h x ( i ) ,  X  i . Let L (  X  )= E tion A holds with probability at least 1 , scale function c (  X  )= k  X  k and rate " n, , provided that the dual norm fulfills k 1 For sub-Gaussian t ( y ) , we can obtain a rate " n, 2 O ( nite variance, we can obtain a rate " n, 2 O ( Both cases hold for bounded k x k  X  . (See Appendix D.) Matrix Factorization. We focus on two problems: exponential-family PCA and max-margin matrix factoriza-tion. We assume that the hypothesis  X  is a matrix. That is,  X  2 H = R n 1  X  n 2 . We assume that each entry in the ran-dom matrix X 2 R n 1  X  n 2 is independent, and might follow a different distribution. Additionally, we assume that the matrix size grows with n . That is, we let n = n 1 n 2 . First, we analyze exponential-family PCA, which was in-troduced by (Collins et al., 2001) as a generalization of the more common Gaussian PCA.
 Claim iii (Exponential-family PCA) . Let t ( y ) be the suf-ficient statistics and Z (  X  )= tion function. Assume the entries of the random matrix X 2 R n 1  X  n 2 are independent. Let n = n 1 n 2 and let b L cal negative log-likelihood of x ij given  X  ij . Let L (  X  )= E bility at least 1 , scale function c (  X  )= k  X  k and rate " n, , provided that the dual norm fulfills k 1 E For sub-Gaussian t ( x ij ) , we can obtain a rate " n, 2 finite variance, we can obtain a rate " n, 2 O ( (See Appendix D.) Next, we focus on max-margin matrix factorization. This problem was introduced by (Srebro et al., 2004) which used a hinge loss. We analyze the more general case of Lipschitz continuous functions, which also includes for instance, the logistic loss. Note however that the following claim also applies to nonconvex Lipschitz losses.
 Claim iv (Max-margin factorization with Lipschitz loss) . Let f : R ! R be a Lipschitz continuous loss func-tion. Assume the entries of the random matrix X 2 { 1 , +1 } n 1  X  n 2 are independent. Let n = n 1 n 2 and let b L ing the binary values x ij 2 { 1 , +1 } by using sgn(  X  ij with probability 1 (i.e. =0 ), scale function c (  X  )= k  X  k and rate " n, 0 2 O ( 1 / n ) .
 By using norm inequalities, Assumption A holds with probability 1 for other matrix norms besides ` 1 . Nonparametric Generalized Regression. Next, we an-alyze nonparametric regression with exponential-family noise. The goal is to learn a function, thus the hypothesis class H is a function space. Each function is represented in an infinite dimensional orthonormal basis. One instance of this problem is the Gaussian case, with orthonormal basis functions that depend on single coordinates, and a ` 1 -norm prior as in (Ravikumar et al., 2005). In our nonparametric model, we allow for the number of basis functions to grow with more samples. For simplicity, we chose to analyze the fixed design model. That is, we analyze the case in which y is a random variable and x is a constant.
 Claim v (Nonparametric regression) . Let X be the do-main of x . Let  X  : X ! R be a predictor. Let t ( y ) be the sufficient statistics and Z (  X  )= the partition function. Given n independent samples, let b L empirical negative log-likelihood of y ( i ) given their pre-dictors  X  ( x ( i ) ) . Let L (  X  )= E ,..., 1 : X ! R be an infinitely dimensional or-thonormal basis, and let ( x )=( 1 ( x ) ,..., 1 ( x )) . Assumption A holds with probability at least 1 , scale function c (  X  )= k  X  k and rate " n, , provided that the dual norm fulfills k 1 " Let 2 (0; 1 / 2 ) . For sub-Gaussian t ( y ) , we can obtain a ples and O ( e n 2 ) basis functions. While for finite variance, basis functions. Both cases hold for bounded k ( x ) k  X  (See Appendix D.) Nonparametric Clustering with Exponential Families. We consider a version of the clustering problem, where the number of clusters is not fixed (and possibly infinite), and where the goal is to estimate the exponential-family pa-rameters of each cluster. Thus, the hypothesis class H is an infinite dimensional vector space. An analysis for the Gaussian case, with fixed covariances and number of clus-ters (i.e. k -means) was given by (Sun et al., 2012). In our nonparametric model, we allow for the number of clusters to grow with more samples. For simplicity, we chose to an-alyze the case of  X  X alanced X  clusters. That is, each cluster contains the same number of training samples.
 Claim vi (Nonparametric clustering) . Let  X  ( j ) be the pa-rameters of cluster j . Let  X  =(  X  (1) ,...,  X  ( 1 ) ) be the concatenation of an infinite set of clusters. Let t ( x ) the sufficient statistics and Z (  X  )= partition function. Given n i.i.d. samples, let b L n (  X  )=
P ter. Let L (  X  )= E x  X  D [ b L n (  X  )] . Let X be the do-main of x . Assumption A holds with probability at least 1 , scale function c (  X  )= " n, , provided that for all partitions X (1) ,..., X ( 1 ) of the dual norm fulfills ( 8 j ) k 1 E For sub-Gaussian t ( x ) , we can obtain a rate " n, 2 O ( clusters. For finite variance, we were not able to obtain a decreasing rate " n, with respect to n . (See Appendix D.) PAC-Bayes Learning. In the PAC-Bayes framework,  X  is a probability distribution of predictors f in a hypothe-sis class F . Thus, the hypothesis class H is the space of probability distributions of support F . After observing a training set, the task is to choose a posterior distribution b  X  . PAC-Bayes guarantees are then given with respect to tween PAC-Bayes learning and Kullback-Leibler regular-ization (Bousquet &amp; Elisseeff, 2002; Germain et al., 2009). The following theorem applies to tasks such as classifica-tion as well as structured prediction.
 Claim vii (PAC-Bayes learning) . Let X and Y be the do-main of x and y respectively. Let f : X ! Y be a predic-tor and d : Y  X  Y ! [0 , 1] be a distortion function. Let  X  be a probability distribution of predictors. Given n i.i.d. samples, let b L n (  X  )= 1 prior distribution. Assumption A holds with probability at least 1 , scale function c (  X  )= KL (  X  ||  X  (0) )+1 and rate " n, 2 O ( 4.2. Super-Scale Regularizers In what follows, we show that several regularizers com-monly used in the literature fulfill Assumption B. We also provide yet unexplored priors with guarantees.
 Norms. Norms regularizers (i.e. R (  X  )= k  X  k ) fulfill As-sumption B for c (  X  )= k  X  k and r ( z )= z . These regular-izers include: the sparsity promoting regularizers, such as the ` 1 -norm (e.g. Ravikumar et al. 2008) and the k -support norm (Argyriou et al., 2012), the multitask ` 1 , 2 and ` norms for overlapping groups (Jacob et al., 2009; Mairal et al., 2010) as well as for non-overlapping groups (Negah-ban &amp; Wainwright, 2011; Obozinski et al., 2011), and the trace norm for low-rank regularization (Bach, 2008; Srebro et al., 2004).
 Functions of Norms. The Tikhonov regularizer (i.e. and r ( z )= z 2 + 1 / 4 . We can define some instances that have not been explored yet, but that have theoretical guarantees. Consider, for instance a polynomial bound insensitive bound r ( z ) = max(0 ,z )+ , a logistic bound r ( z ) = log (1 + e z ) , an exponential bound r ( z )= e z 1 , as well as an entropy bound r ( z )= z log z +1 . Mixture of Norms. The sparse and low-rank prior (Richard et al., 2012) of the form R (  X  )= k  X  k 1 + k  X  k fulfills Assumption B by making either c (  X  )= k  X  k 1 or c (  X  )= k  X  k tr , and r ( z )= z . The elastic net (Zou &amp; Hastie, 2005) of the form R (  X  )= k  X  k 1 + k  X  k 2 fills Assumption B by making c (  X  )= k  X  k 1 and r ( z )= z or c (  X  )= k  X  k 2 , r ( z )= z 2 + 1 / 4 .
 Dirty Models. The dirty multitask prior (Jalali et al., 2010) of the form R (  X  )= k  X  (1) k 1 + k  X  (2) k 1 , 1 where and r ( z )= z . (See Appendix C.) Other Priors. The Kullback-Leibler regularizer (Bous-quet &amp; Elisseeff, 2002; Germain et al., 2009) fulfills As-sumption B for c (  X  )= KL (  X  ||  X  (0) ) and r ( z )= z , where  X  R (  X  )= k  X  k + f (  X  ) where f (  X  ) 0 , fulfills Assump-tion B with c (  X  )= k  X  k and r ( z )= z . This includes the mixture of norms, and the total variation prior (Kolar et al., 2010; 2009; Zhang &amp; Wang, 2010). Since f is not required to be convex, quasiconvex regularizers of the form R (  X  )= k  X  k 1 + k  X  k p for p&lt; 1 , fulfill Assumption B. 4.3. Norm-Loss Monotonicity Here, we show some specific conditions on the expected loss in order fulfill Assumption C. We also provide yet unexplored cases with theoretical guarantees.
 Strong Convexity. First, we show that strongly convex expected losses are a special case in our framework. We consider strongly convex functions that are not necessarily smooth.
 Several authors have shown different flavors of strong con-vexity for specific problems. Almost strong convexity with respect to a small neighborhood around the true minimizer  X  hood estimation of exponential family distributions. Strong convexity of SVMs and logistic regression for Gaussian predictors was proved in (Rocha et al., 2009). Restricted strong convexity (i.e. strong convexity with respect to a subset of directions) was shown in (Negahban et al., 2009; 2012) for generalized linear models under sparsity, group-sparsity and low-rank promoting regularizers. For simplic-ity, we focus on the regular form of strong convexity. Claim viii (Strong convexity) . Assumption C holds for b ( z )=  X  2 z 2 provided that the expected loss L is strongly convex with parameter  X  . Moreover, if L is twice contin-uously differentiable, Assumption C holds if the Hessian of
L is positive definite, i.e. if there is  X  &gt; 0 such that Note that the function b ( z )=  X  its inverse function is b  X  ( z )= b (0) = 0 , Theorem 2 guarantees exact recovery of the true hypothesis in the asymptotic case with exact minimization. That is, since we require lim n ! + 1 " n, =0 in Assump-tion A and for  X  =0 , we have lim n ! + 1 k b  X  n  X   X  k 1 The constant  X  has a problem-specific meaning. In linear regression,  X  is the minimum eigenvalue of the expected covariance matrix of the predictors (Wainwright, 2009b). In the estimation of Gaussian MRFs,  X  is the squared min-imum eigenvalue of the true covariance matrix (Ravikumar et al., 2008).
 Minimax Boundedness. Next, we provide an approach for creating a  X  X inimax X  lower bound for an arbitrary ex-pected loss. Note that we consider functions that are not necessarily smooth or convex. On the other hand, any strictly convex function is a  X  X inimax bounded X  function. Our constructed lower bound resembles a cone without apex. First, we create a flat disk of a prescribed radius cen-tered at the true hypothesis  X   X  . Then, we create a linear lower bound (i.e. linear in k  X   X   X  k 2 ) with minimum slope across the maximum over all possible directions. This lin-ear function is the lower bound of the expected loss outside the flat disk region.
 Claim ix (Minimax boundedness) . Let  X  1 &gt; 0 be a fixed radius around the true hypothesis  X   X  . Let M (  X  )= sup 0 { |L (  X  ) L (  X   X  ) ( k  X   X   X  k 2  X  1 ) } be the maximum slope for a linear lower bound of the ex-pected loss L in the direction of  X   X   X  . Let  X  2 = all possible directions. Assumption C holds for b ( z )=  X  max(0 ,z  X  1 ) provided that  X  2 &gt; 0 .
 Note that the function b ( z )=  X  2 max(0 ,z  X  1 ) is not strictly increasing for z 2 (0;  X  1 ) , and its inverse function is b  X  ( z )= z  X  orem 2 only guarantees recovery of the true hypothesis up to a small region in the asymptotic case, even with exact optimization. That is, for lim n ! + 1 " n, =0 and  X  =0 , Other Types of Nonconvexity. By using our previ-ous results, one can devise some yet unexplored settings for which our framework provides theoretical guarantees. Functions such as the square root (i.e. b ( z )= p z ) and the logarithm (i.e. b ( z ) = log(1 + z ) ) can be used for noncon-vex problems.
 We construct a lower bound of the expected loss as follows. First, we define a  X  X ransformed X  expected loss e L (  X  )= b ( L (  X  ) L (  X   X  )) . Then, we invoke strong convexity (Claim viii) or minimax boundedness (Claim ix) for the  X  X ransformed X  expected loss e L . Thus, by using the square root function (i.e. b ( z )= p z and b  X  ( z )= z 2 ), we define the family of  X  X quared strongly convex X  and  X  X quared min-imax bounded X  functions. By using the logarithmic func-the  X  X xponential strongly convex X  and  X  X xponential mini-max bounded X  functions. As expected, in order to obtain theoretical guarantees, scaled uniform convergence has to be shown with respect to the  X  X ransformed X  expected loss e L . 4.4. Rates and Novel Results Table 1 shows the rates for the different losses and regu-larizers. We have not optimized these rates. All the rates follow mainly from the ` 1 -norm regularizer and norm in-equalities. Thus, while we match some rates in the litera-ture, some are not better. Note that our framework is more general and uses less assumptions than previous analyses for specific problems.
 New results include the four types of consistency of MLE of exponential family distributions, and GLMs for other priors besides ` 1 . We prove the four types of consistency of regularizers that are a norm plus a nonnegative function (elastic net, total variation, dirty models, sparsity and low-rank), of relatively new regularizers ( k -support norm, mul-titask priors with overlapping groups), and of a proposed quasiconvex regularizer. The analysis of matrix factoriza-tion problems is novel and without the i.i.d assumption. Our analysis of max-margin matrix factorization does not assume convexity. We provide a new analysis of nonpara-metric models, such as generalized regression and cluster-ing. All the problems above have unbounded hypothesis class and unbounded loss. Finally, we show a connection between PAC-Bayes learning and Kullback-Leibler regu-larization. There are several ways of extending this research. While we focused on the exact recovery of the entire sparsity pat-tern, approximate sparsistency should also be studied. The use of a surrogate loss and theoretical guarantees with re-spect to the original loss is a challenging open problem. Most consistency results, including ours, do not provide a data-dependent mechanism for setting n . Extending the results on cross-validation for ` 1 -penalized linear regres-sion (Homrighausen &amp; McDonald, 2013) is one of our fu-ture goals. We provided examples for the i.i.d. and the independent sampling settings. We plan to analyze exam-ples for the non-i.i.d. setting as in (London et al., 2013; Mohri &amp; Rostamizadeh, 2010).
 Argyriou, A., Foygel, R., and Srebro, N. Sparse prediction with the k-support norm. NIPS , 2012.
 Bach, F. Consistency of trace norm minimization. JMLR , 2008.
 Bickel, P., Ritov, Y., and Tsybakov, A. Simultaneous anal-ysis of lasso and dantzig selector. Annals of Statistics , 2009.
 Bousquet, O. and Elisseeff, A. Stability and generalization. JMLR , 2002.
 Collins, M., Dasgupta, S., and Schapire, R. A generaliza-tion of principal component analysis to the exponential family. NIPS , 2001.
 Germain, P., Lacasse, A., Laviolette, F., Marchand, M., and
Shanian, S. From PAC-Bayes bounds to KL regulariza-tion. NIPS , 2009.
 Homrighausen, D. and McDonald, D. The lasso, persis-tence, and cross-validation. ICML , 2013.
 Hsu, D., Kakade, S., and Zhang, T. Random design analy-sis of ridge regression. COLT , 2012.
 Jacob, L., Obozinski, G., and Vert, J. Group lasso with overlap and graph lasso. NIPS , 2009.
 Jalali, A., Ravikumar, P., Sanghavi, S., and Ruan, C. A dirty model for multi-task learning. NIPS , 2010. Kakade, S., Shamir, O., Sridharan, K., and Tewari, A. Learning exponential families in high-dimensions: Strong convexity and sparsity. AISTATS , 2010.
 Kolar, M., Song, L., and Xing, E. Sparsistent learning of varying-coefficient models with structural changes. NIPS , 2009.
 Kolar, M., Song, L., Ahmed, A., and Xing, E. Estimat-ing time-varying networks. Annals of Applied Statistics , 2010.
 Lee, J., Sun, Y., and Taylor, J. On model selection consis-tency of M-estimators with geometrically decomposable penalties. NIPS , 2013.
 Loh, P. and Wainwright, M. Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for lo-cal optima. NIPS , 2013.
 London, B., Huang, B., Taskar, B., and Getoor, L. Col-lective stability in structured prediction: Generalization from one example. ICML , 2013.
 Mairal, J., Jenatton, R., Obozinski, G., and Bach, F. Net-work flow algorithms for structured sparsity. NIPS , 2010.
 Maurer, A. A note on the PAC-Bayesian theorem. ArXiv , 2004.
 Meinshausen, N. and Yu, B. Lasso-type recovery of sparse representations for high dimensional data. Annals of Statistics , 2009.
 Mohri, M. and Rostamizadeh, A. Stability bounds for sta-tionary ' -mixing and -mixing processes. JMLR , 2010. Mukherjee, S., Niyogi, P., Poggio, T., and Rifkin, R. Learn-ing theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. Advances in Computational Mathematics , 2006.
 Negahban, S. and Wainwright, M. Simultaneous support recovery in high dimensions: Benefits and perils of block ` 1 / ` 1 -regularization. IEEE Transactions on Informa-tion Theory , 2011.
 Negahban, S., Ravikumar, P., Wainwright, M., and Yu,
B. A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. NIPS , 2009.
 Negahban, S., Ravikumar, P., Wainwright, M., and Yu, B. A unified framework for high-dimensional analysis of
M-estimators with decomposable regularizers. Statisti-cal Science , 2012.
 Obozinski, G., Wainwright, M., and Jordan, M. Support union recovery in high-dimensional multivariate regres-sion. Annals of Statistics , 2011.
 Rakhlin, S., Mukherjee, S., and Poggio, T. Stability results in learning theory. Analysis and Applications , 2005. Ravikumar, P., Liu, H., Lafferty, J., and Wasserman, L. Spam: Sparse additive models. NIPS , 2005.
 Ravikumar, P., Raskutti, G., Wainwright, M., and Yu, B.
Model selection in Gaussian graphical models: High-dimensional consistency of ` 1 -regularized MLE. NIPS , 2008.
 Richard, E., Savalle, P., and Vayatis, N. Estimation of si-multaneously sparse and low rank matrices. ICML , 2012. Rish, I. and Grabarnik, G. Sparse signal recovery with exponential-family noise. Allerton , 2009.
 Rocha, G., Xing, W., and Yu, B. Asymptotic distribu-tion and sparsistency for ` 1 -penalized parametric M-estimators with applications to linear SVM and logistic regression. TR0903, Indiana University , 2009.
 Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridha-ran, K. Learnability, stability and uniform convergence. JMLR , 2010.
 Srebro, N., Rennie, J., and Jaakkola, T. Maximum-margin matrix factorization. NIPS , 2004.
 Sun, W., Wang, J., and Fang, Y. Regularized k-means clus-tering of high-dimensional data and its asymptotic con-sistency. Electronic Journal of Statistics , 2012. Tibshirani, R. Regression shrinkage and selection via the lasso: a retrospective (comments from B  X  uhlmann). J.
Royal Statistical Society , 2011. van de Geer, S. A survey on empirical risk minimization.
Oberwolfach Reports , 2005. van de Geer, S. High-dimensional generalized linear mod-els and the lasso. Annals of Statistics , 2008.
 Wainwright, M. Information-theoretic limits on sparsity re-covery in the high-dimensional and noisy setting. IEEE Transactions on Information Theory , 2009a.
 Wainwright, M. Sharp thresholds for high-dimensional and noisy sparsity recovery using constrained quadratic pro-gramming (lasso). IEEE Transactions on Information Theory , 2009b.
 Wainwright, M., Ravikumar, P., and Lafferty, J.
High dimensional graphical model selection using ` 1 -regularized logistic regression. NIPS , 2006.
 Yang, E. and Ravikumar, P. Dirty statistical models. NIPS , 2013.
 Yang, E., Tewari, A., and Ravikumar, P. On robust esti-mation of high dimensional generalized linear models. IJCAI , 2013.
 Zhang, B. and Wang, Y. Learning structural changes of Gaussian graphical models in controlled experiments. UAI , 2010.
 Zhao, P. and Yu, B. On model selection consistency of lasso. JMLR , 2006.
 Zhou, S. Thresholding procedures for high dimensional variable selection and statistical estimation. NIPS , 2009. Zou, H. and Hastie, T. Regularization and variable selec-
