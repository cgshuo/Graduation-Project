 This paper presents an unsupervised method for systemat-ically identifying anomalies in music datasets. The model integrates categorical regression and robust estimation tech-niques to infer anomalous scores in music clips. When applied to a music genre recognition dataset, the new method is able to detect corrupted, distorted, or mislabeled audio samples based on commonly used features in music information re-trieval. The evaluation results show that the algorithm outperforms other anomaly detection methods and is capable of finding problematic samples identified by human experts. The proposed method introduces a preliminary framework for anomaly detection in music data that can serve as a useful tool to improve data integrity in the future.
 Anomaly detection; music information retrieval; unsupervised
Music information retrieval (MIR) is an active research area that integrates knowledge from a number of different fields, including Electrical Engineering, Computer Science, Psychology, and Musicology [7]. Many previous studies in this area have adopted machine learning techniques and applied them to audio data to build an intelligent system that understands music. The evaluation of such systems, as described by Schedl et al. [7], requires different datasets and annotations depending on the tasks. However, since the annotation process for music data is both complex and subjective, the quality of the annotations created by human experts can vary considerably from dataset to dataset, po-tentially introducing errors into the system and adversely affecting the performance. Finding a way to enhance the correctness of these datasets is thus crucial for the further improvement of MIR systems.

A typical example of the types of problems encountered concerns the Music Genre Recognition (MGR). According to Sturm [10], the most frequent used dataset in MGR is magnitude spectrum of each block using a Hann window. The rhythmic features are extracted from the beat histogram of the entire time domain signal. The selected features are as follows (for details of the implementations, please refer to [5]):
Spectral and temporal features are aggregated in texture windows of length 0.743 s following the standard procedure as outlined in [12]. The mean and standard deviation of the features within a window are then computed to create a new feature vector. Finally, all the resulting feature vectors are reaggregated with their mean and standard deviation, gener-ating a single feature vector that represents each individual recording in the dataset.
Given N feature vectors X = { X 1 ,...,X N } and their cor-responding categories Y = { Y 1 ,...,Y N } , where each Y n  X  { C 1 ,C 2 ,...,C M } , we formulate the relation of Y and X based on a linear assumption. The input-output relationship can be represented as a regression model: where g is the categorical link function,  X  is the regression coefficient matrix, and  X  is a random variable that represents the white-noise vector of each instance. The link function g is a logit function that is paired with a category C M , i.e., ln ( P ( Y n = C m ) /P ( Y n = C M )) = X n  X  m +  X  nm . Since the probabilities of the categories will sum to one, we can derive the following modeling equations: and The coefficient vector  X  usually represents the decision bound-ary in a classification problem. Here,  X  is used to capture the normal behavior of the data. Following the convention, we assume that each  X  m obeys a Gaussian distribution with a predefined mean vector and a predefined covariance matrix  X  , i.e., In traditional regression applications, the error factor  X  is generally assumed to follow a Gaussian distribution. How-ever, a Gaussian distribution lacks tolerance for anomalies since the probability distribution is near zero at points far away from the distribution mean. In the study of robust statistics, it is suggested to assume that this random variable follows a heavy-tailed distribution in order to improve the capability of capturing anomalies [11].
 In this work, we assume that the error is a zero-mean Student-t random variable, which has been shown to be a Method P R F AUC Linear Detection 0.59 0.55 0.57 0.91 Clustering 0.23 0.23 0.23 0.74 KNN 0.27 0.26 0.27 0.78 LOF 0.26 0.25 0.25 0.74 Table 1: Average Detection Rate for Injected Data and the corresponding categories as the input, and returns a set of indices of the anomalous instances as the output.
The process starts by taking the MGR dataset D and performing a feature extraction to obtain the feature vector X and its corresponding class labels Y . Next, the anomaly detection method starts with initial values of the variables  X  0 and  X  0 . The process iteratively updates the model variables  X  and  X  using the approach introduced above (lines 3 X 7). After the variables have converged, the error variable  X  is checked to identify anomalies (lines 8 X 13). As a reasonable assumption, similar to the analysis of Gaussian distributions, instances with  X  greater than 3 times of the standard deviation are labeled anomalies.
The experiments were conducted on the popular GTZAN dataset [12]. This dataset consists of 10 music genres (blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae, and rock), each with 100 audio tracks. All tracks consist of a 30-second excerpt of complex mixtures of music.

For this study, two sets of experiments were conducted to test the performance of the proposed method and three other benchmark methods. In the first experiment, we used a purified GTZAN dataset that excludes the conspicuous misclassified and jitter music clips reported in [9]. An in-jection process was performed by randomly choosing 5% of the instances in each genre, and randomizing their genre labels to create outliers. Sturm X  X  report identifies around 50 conspicuous files, which corresponds to about 5% of the total number of files in the dataset. We generated 10 random realizations of the dataset, and evaluated the measures with the average for this 10-run batch. This experiment simulates the best-case scenario, where the dataset is clean and all genres are well separated in feature space. The results thus serve as a sanity check for all the methods.
 In the second experiment, we applied our method to the full GTZAN dataset directly. The outliers identified were then compared with the list reported in [9]. This experiment is effectively a real-world scenario in which case the automated anomaly detection methods are expected to find the outliers identified by human experts. Two sets of features represent the dataset, one is the feature set as described in Sect. 2 and the other a minimal feature set using only 13 MFCCs, as reported in the work of Hansen et al. [4].
We compared the results obtained using our approach with those of three unsupervised benchmark methods. Due to the small number of true anomaly labels and their uneven distributions across different genres, the application of a supervised anomaly classifier was deemed not feasible and thus excluded in this experiment. Method P R F AUC Linear Detection 0.52 0.40 0.45 0.87 Clustering 0.08 0.07 0.07 0.61 KNN 0.12 0.11 0.12 0.68 LOF 0.13 0.13 0.13 0.70 Table 2: Average detection rate for injected data with only MFCCs features Method P R F AUC Linear Detection 0.18 0.23 0.20 0.63 Clustering 0.08 0.07 0.07 0.41 KNN 0.10 0.09 0.10 0.50 LOF 0.10 0.09 0.10 0.51 Table 3: Detection rates for GTZAN data (Sturm X  X  anomalies) in this case. In particular, the performance of the Clustering method wa significantly dropped. This observation suggests that in MFCC feature space, the genres overlap substantially and cannot be separated by clustering.

In the second set of experiments, the anomaly detection process was applied to the full GTZAN dataset and aim to detect the misclassified music clips reported by Sturm [9]. The results are shown in Table 3. Although our method still outperformed the benchmark methods, the performance decreased noticeably compared to the injection setup. Based on the metrics utilized here, none of these methods are able to detect anomalies with high accuracy. Comparing our results with the anomalies reported in [9], we found that our method is able to detect the jitter clips ( reggae : No. 87), and some of the obviously misclassified clips, including reggae : No. 88, disco : No. 41, pop : No. 81, hip-hop : No. 31, all of which achieved high anomaly score with our method. Interestingly, we also found that neither our method nor any of the benchmark methods could successfully detect anomalies in the metal genre ( metal : Nos. 46 X 57). These music clips are in fact punk rock but are annotated as metal in the GTZAN dataset. It can be observed that the extracted features for these music clips are very similar to other metal clips. This implies that the features used for this approach are not able to sufficiently differentiate punk rock from metal . One important task for future work is therefore to improve the identification of representative features to allow for better differentiation between similar genres.

It should also be pointed out that while people mostly agree on the genre labels hip-hop and blues , they tend to disagree on the category rock [8], demonstrating the inherent ambiguity of many music genres. This trend can be observed in Table 4, where the detection rates for metal and rock are the lowest among all the genres. This is most likely due to the inherent ambiguity in these genre categories, which could result in inconsistent patterns in the feature space and thus increase the difficulty of anomaly detection. All of the methods failed to detect the alternative rock clips reported by Sturm ( metal : Nos. 96 X 99); the feature vectors of these clips are generally similar to those of other metal music clips. To the best of our understanding, these clips can also be categorized as nu-metal, which is a sub-category of metal music in the taxonomy of genre. This again highlights the natural ambiguity involved in classifying music. One potential solution is to develop a multi-class modeling method to tolerate this ambiguity and suggest alternative classes that might also be applicable to individual music clips.
