 Online controlled experiments, e.g., A/B testing, is the state-of-the-art approach used by modern Internet companies to improve their services based on data-driven decisions. The most challenging problem is to define an appropriate on-line metric of user behavior, so-called Overall Evaluation Criterion (OEC), which is both interpretable and sensitive. A typical OEC consists of a key metric and an evaluation statistic. Sensitivity of an OEC to the treatment effect of an A/B test is measured by a statistical significance test. We introduce the notion of Overall Acceptance Criterion (OAC) that includes both the components of an OEC and a statistical significance test. While existing studies on A/B tests are mostly concentrated on the first component of an OAC, its key metric, we widely study the two latter ones by comparison of several statistics and several statistical tests with respect to user engagement metrics on hundreds of A/B experiments run on real users of Yandex. We dis-covered that the application of the state-of-the-art Student X  X  t-tests to several main user engagement metrics may lead to an underestimation of the false-positive rate by an order of magnitude. We investigate both well-known and novel tech-niques to overcome this issue in practical settings. At last, we propose the entropy and the quantiles as novel OECs that reflect the diversity and extreme cases of user engagement. Categories and Subject Descriptions: H.1.2 [User/Machine Systems]: Human information processing; H.5.2 [User inter-face]: Evaluation/methodology General Terms: Measurement, Experimentation Keywords: User engagement; online controlled experiment; Overall Acceptance Criterion; A/B test; sensitivity; quality metrics; evaluation statistic; significance level; p-value
Nowadays, online controlled experiments, e.g., A/B test-ing, is the state-of-the-art approach utilized by modern In-ternet companies (such as web search engines [35, 6, 5], so-c  X  cial networks [1], etc.) to improve their services based on data-driven decisions [21]. An A/B test compares two vari-ants of a service at a time, usually its current version (con-trol) and a new one (treatment), by exposing them to two groups of users. The aim of controlled experiments is to de-tect the causal effect of service updates on its performance relying on an Overall Evaluation Criterion (OEC) [22], a user behavior metric that is assumed to correlate with the quality of the service, i.e., the value of the OEC must have a clear interpretation . When the treatment effect exists, the OEC has to detect the difference of the two versions of the service at a high level of statistical significance in order to distinguish the treatment effect from the noise observed when the effect does not exist. This property is referred to as the sensitivity of the OEC [22].

A common OEC consists of two main components [22]: (a) a key metric (e.g., the number of sessions, the number of queries, etc.), calculated for each event (entity, experimen-tal unit ) of a certain type observed in the behavioral data (e.g., a query, a user, a session, etc); and (b) the evaluation statistic of the key metric over the experimental units (e.g., the average value, the median, etc.). A third (c) compo-nent of any A/B test is the statistical significance test (e.g., t-test, bootstrap, etc.), which examines weather the evalua-tion statistic of the key metric over the two groups of users coincide. We refer to these three components (a) X (c) as an Overall Acceptance Criterion ( OAC ).

The first two components (i.e., the OEC) are responsible for the interpretation of the OAC, and all of them directly affect its sensitivity. On the one hand, one important part of existing studies on interpretation of an OEC concerns only development or improvement of key metrics that correlate with different aspects of the system quality [12, 33, 10]. On the other hand, the large set of studies devoted to the sen-sitivity of an OEC relates either to experimental units (e.g., increasing of the experiment duration or of the user popu-lation participated in the experiment [22, 35, 5]) or to the key metric as well (e.g., variation reduction techniques [22, 7, 6], metric transformation [21, 11]). At the same time, to the best of our knowledge, the statistical significance test and the statistic used in an OAC are understudied areas in online evaluation of web services.

In the current study, we focus on the impact of the statis-tics and statistical tests chosen for an OAC on the sensitivity of different key metrics. First, we show that a key metric is most effective in combination with an appropriate statistical test, which is individual for each key metric. Utilization of the usual t-test with a default p-value threshold may lead to wrong conclusions on the performance of some state-of-the-art key metrics. Second, we show that each combination of a key metric and a statistical test requires its individ-ual p-value threshold to control the false-positive rate at a predefined level.

We conduct our study for the case of user engagement metrics, since they are often considered to be most appro-priate for online evaluation. User engagement reflects how often the user solves her needs (e.g., to search something) by means of the considered service (e.g., a search engine). On the one hand, these metrics are measurable in the short-term experiment period, and, on the other hand, they are predictive of the long-term success of the company [19, 20, 21, 27]. In this study, we pay special attention to the metrics that reflect the loyalty aspect of engagement: the state-of-the-art number of sessions per user metric [19, 33] (which is accepted as the  X  X orth-star X  for A/B testing evaluation in major search engine companies like Bing [20, 21]) and the recently proposed absence time metric [12].

The state-of-the-art evaluation statistic of the key metric over experimental units is the average value, which is used in vast majority of studies on A/B testing [4, 18, 35, 19, 33, 20, 21, 5, 10, 11]. Nonetheless, there are other statis-tics that have intelligible interpretation for user engagement metrics. The median is a popular statistic within statisti-cal tests [34, 31] and it measures engagement of a typical user of the web service. Other quantiles quantify the ex-treme cases [14]: users whose behaviors are far from the mean (e.g., they are able to describe users that are engaged with the service less or more than an average user) [24, 16]. Entropy [30] measures the diversity of users against their en-gagement with a web service. Hence, these statistics could be utilized to detect the treatment effect in an evaluated up-date for such user behavior aspects as well 1 . We consider all these statistics in our OACs and compare them in terms of sensitivity. We found that each of the described above eval-uation statistics noticeably outperforms the state-of-the-art mean statistic for some metrics in terms of the treatment detection rate with a fixed false-positive rate.

The widely applicable statistical significance test is the unpaired Student X  X  t-test [7, 33, 6, 10, 11]. However, a key metric may not follow assumptions underlying this test (such as the normality of the metric X  X  distribution or the indepen-dence of experimental units). This makes the statistical test inappropriate for such key metrics, and its utilization may lead to an underestimate of the false-positive rate. In our study, we show that application of the t-test to several dif-ferent user engagement key metrics leads to the underesti-mation of this rate by an order of magnitude. Moreover, we demonstrate practical efficiency of both well-known and novel techniques (Bootstrapping, p-value adjustment, and others) to overcome this issue.

Different statistical significance tests were widely studied and compared in offline evaluation of information retrieval (IR) systems [28, 31, 38, 32, 2, 36, 29]. Unfortunately, sta-tistical tests in offline evaluation are almost always paired. Hence, the results presented in these studies could not be straightforwardly applied to the case of A/B testing. In our study, we compare 5 statistical significance tests (Student X  X  t-test, Bootstrapping, Mann Whitney U test, Tarone-Ware test, and Logrank test) and show how their application to
Some similar statistics were utilized in risk-sensitive opti-mization and offline evaluation of retrieval systems [37, 8]. different user engagement metrics results in different sensi-tivity of the OECs.

Despite the largest web services have designed special ex-perimental platforms that allow them to run A/B tests at large scale (e.g., hundreds of concurrent experiments per day) [35, 20], to the best of our knowledge, the existing studies on A/B testing evaluate their approaches on dozens real online experiment runs (e.g., the largest numbers are 21 A/B tests in [3], 32 A/B tests and 18 A/A tests in [11]). In our study, we conduct our experimental analysis on 169 A/B experiments and 98 A/A ones run on real users of Yandex (www.yandex.com) with duration from one to several weeks. This should make the results of our study more valuable for practical use in modern web companies.

To sum up, our study considers the problem, which co-incides with the emerging Internet companies X  needs : to use more sensitive and interpretable OECs in online con-trolled experiments and to get their results on a proper false-positive level. Specifically, the major contributions of our work include:
The rest of the paper is organized as follows. In Section 2, the related work on A/B testing and user engagement is dis-cussed. In Section 3, A/B testing background is described and our key metrics, evaluation statistics and statistical sig-nificance tests are presented. In Section 4, we show and discuss the results of applying OACs to the set of A/B ex-periments. In Section 5, the study X  X  conclusions and our plans for the future work are presented.
We compare our research with other studies in three as-pects. The first one relates to the common methodology of conducting online controlled experiments. The second one concerns user engagement metrics used in web services and, particularly, search engines. The third one relates to statis-tical tests used to measure significance level in information retrieval evaluation.

Online controlled experiment studies. The theoreti-cal details of the online controlled experiment methodology were widely described in the existing works [26, 22], and we conduct our experiments in accordance with them. A number of practical lessons learned from the applications of this methodology to different evaluation cases in many web companies was recently described in [18, 35, 20]. These studies concern, inter alia, experiments with different com-ponents of a web service (e.g., the user interface [18, 10, 25] and ranking algorithms [33, 10, 25]), large-scale experimen-tal infrastructure [35, 20], different aspects of user behavior and interaction with a web service (clicks [19, 21], speed [21], abandonment [21], periodicity [10, 9]), and so on. These ex-periments show that system updates of different types may affect various key metrics differently. Some of the existing works focused on the study of the trustworthiness of the re-sults of an A/B test. Various pitfalls and puzzling outcomes of online controlled experimentation were shared in [4, 19] and several  X  X ules of thumb X  were discussed in [21].
Some other studies focused on sensitivity improvement techniques for an online controlled experiment. The simplest ones [22] include increasing of the experimental time period or expanding the user population participating in the ex-periment; investigation of evaluation metric with lower vari-ance [10, 9]; and elimination of users who were not affected by the service change in the treatment group [33]. More so-phisticated sensitivity improvement techniques include the stratification and the usage of control variates that are de-fined on the basis of pre-experiment data [7]; the reduction of skewness of the evaluation metric [21] (e.g., transformation of the metric or capping its values); the sensitivity improve-ment for two-stage online controlled experiments [6]; the future user behavior prediction approach [11]; and diluted treatment effect estimation for trigger analysis [5]. To the best of our knowledge, the existing studies on A/B testing evaluated their approaches on dozens real online experiment runs (e.g., maximal are 21 A/B tests in [3], 32 A/B tests and 18 A/A tests in [11]). In our study, we experiment with a diverse and huge set of system changes: 169 large-scale A/B tests and 98 A/A experiments based on actual interactions of hundreds of thousands of real users (we use no artificially simulated data).

This allowed our study to produce more essential results on the relative sensitivity of different metrics. We believe, these results are more valuable in practice than the previous studies did, since the largest modern web companies run up to hundreds of concurrent A/B tests per day [35, 20].
Evaluation metrics. User engagement metrics are pop-ular in A/B testing practice of many companies, because user engagement reflects how often the user solves her needs (e.g., to search something) by means of the considered ser-vice (e.g., a search engine) [19, 20, 21]. Hence, on the one hand, these metrics are measurable in the short-term exper-iment period, and, on the other hand, they are predictive of long-term goals of the company [19, 20, 21]. The most well-known metrics of loyalty aspect of user engagement are the state-of-the-art number of sessions per user [33] and the absence time [12] metrics. There are also several widely used metrics of activity aspect of user engagement like the num-ber of clicks per user, shows per user, queries per user [22, 7], etc. All these metrics were utilized to evaluate different changes in search engines [3, 10, 11, 9]. However, to the best of our knowledge, these metrics were not thoroughly com-pared on the same data: e.g., the number of sessions and the absence time were studied together in [11], but their absence time is a measure over users as experimental units, while its original definition [12] considers it as a measure over peri-ods of absence between consecutive sessions of a user (as in [3]). In our work, we compare sensitivity of these and other main user engagement metrics (as in [33, 12, 3, 11]) in Overall Acceptance Criteria (OACs) with different statistics (the mean, the median, etc.) and statistical tests (T-test, bootstrap, etc.).

Statistical tests in information retrieval. Different statistical significance tests were widely studied and com-pared in offline evaluation of information retrieval (IR) sys-tems [28, 31, 38, 32, 2, 36, 29]. Unfortunately, statistical Table 1: User engagement metrics and correspond-ing experimental units used in our study.
 tests in offline evaluation are almost always paired, since the systems are compared using the same test data set. Hence, the results presented in these studies could not be straight-forwardly adopted to the case of A/B testing, where two systems are compared using different sets of users, and thus unpaired tests are needed. Our study addresses this un-derstudied area of online evaluation of web services w.r.t. user engagement metrics. We compare the Student X  X  t-test, which is the state-of-the-art in A/B testing [7, 33, 6, 10, 11]; Mann Whitney U, Tarone-Ware, Logrank tests, which are widely used in survival analysis [34]; and Bootstrap-ping whose different variants (including unpaired ones) were shown to be very useful for both evaluation of IR systems and for comparing the sensitivity of IR metrics (like nDCG or Q-measure) [28, 31, 32, 36]. Bootstrapping was applied to A/B tests in [1] and was also mentioned in [4, 21] as a method to estimate the variance, when the key metric in A/B testing is very skewed or its experimental unit does not coincide with the randomization one.
In A/B testing, users participated in an experiment, are randomly exposed to one of the two variants of a service (the control (A) and the treatment (B), e.g., the current production version of the service and its update) in order to compare their performance [22, 19, 21]. The difference be-tween the variants is quantitatively measured by an Overall Evaluation Criterion  X  ( OEC , also known as the evaluation metric, the online service quality metric, etc. [22]). In the classical methodology of A/B testing, this OEC is usually an evaluation statistic  X  =  X   X  ( X ) (e.g., the average value ) of a key metric X (  X  ) over the events ( entities )  X   X   X . More precisely, for each user group v  X  { A,B } , we have the ob-servations of the metric X over the experimental units  X  v Then, the evaluation statistics  X  v =  X   X  v ( X ), v  X  X  A,B } , are used as OEC, and their difference  X  =  X  B  X   X  A is calculated to quantify the sign and the magnitude of the change in the OEC.

Nonetheless, the quantity  X  could not serve itself as an in-dicator of positive or negative consequences of the evaluated changes of the service. The difference between the evalua-tion metrics over groups should be controlled by a statis-tical significance test (e.g., Student X  X  t-test) that calculates the probability (also known as p-value ) to observe the dif-ference under the null hypothesis , which assumes that the observed difference is caused by random fluctuations, and the variants of the system are not actually different in terms of user experience. If the p-value is lower than the threshold p val  X   X  (  X  = 0 . 05 is commonly used [22, 19, 7, 33, 21, 10, 11]), then the test rejects the null hypothesis, and the dif-ference  X  is accepted as statistically significant. We refer to the triplet (a key metric, an evaluation statistic, a statistical test) as an Overall Acceptance Criterion ( OAC ). The addi-tional details of the A/B testing framework could be found in the survey and practical guide [22]. All key metrics, eval-uation statistics, and statistical significance tests studied in this paper are specified further in the next subsections.
In this work, we study several popular engagement met-rics and their modifications, which are calculated based on actions of users 2 of Yandex, one of the most popular global search engines. Following common practice [15, 19, 12, 33, 3, 10, 11], we define a session as a sequence of user actions whose dwell times 3 are less than 30 minutes. The number of sessions S (as in [33, 10, 11]) for each user in an A/B ex-periment is our first metric. In our study, we also consider this metric for the reduced set of users that have at least 2 sessions during the experiment (such experimental unit is referred to as  X  X ser-2 X ). The time between two consecutive user sessions (i.e., the duration of an absence) is the absence time (as in [12, 3]), which is referred to as AT and is calcu-lated for each pair of consecutive sessions of each user. The experimental unit for AT is the period of absence between a pair of sessions, and, in order to match absence time to a user (as an experimental unit), we also consider the aver-age duration of user X  X  absences ATpA for each  X  X ser-2 X  4 . We logarithmically transform 5 these metrics and obtain logAT , log(ATpA) , and log(AT)pA .

All above described metrics represent the loyalty aspect of user engagement, whereas, further, we present metrics that are associated with the user activity [27]. They are: the number of queries Q [10, 11], the number of clicks C [10, 11], and the number of clicks per query CpQ (i.e., the CTR of
Users are identified by means of Cookie IDs [22] as done in other studies on user engagement [12, 23, 33, 10, 11, 9]. i.e., times between two consecutive actions of a user [10]
A user with only 1 session has no absence times AT by the definition above.
It should increase the power of OACs if the key metric has a skewed distribution [17]. the search engine result pages [11]) for each user. The pres-ence time is considered both for each user X  X  session (i.e., its duration PT in seconds) and for each user (i.e., the sum of the durations of her sessions sumPT [10, 11]). We logarithmi-cally transform these metrics and obtain logQ , logC , logPT , and logsumPT . Thereby, we study 16 key user engagement metrics in total, whose descriptions and corresponding ex-perimentation units are summarized in Table 1. The shape of distributions of 9 representative key metrics over their ex-perimental units are shown in Fig. 1 (their values are given in arbitrary units for confidentiality reasons). Short analy-sis of their persistence across time and relationships between almost all of them are studied in [10, 11].
Mean and standard deviation. The average value 6 (AVG) is the state-of-the-art evaluation statistic of the key metric over experimental units. The vast majority of studies on A/B testing [4, 18, 35, 19, 33, 20, 21, 5, 10, 11, 9] utilize it, including the fact that the mean describes the typical behavior of a web service user (an  X  X verage X  user). The second commonly used statistic in probability theory is the standard deviation (SD). It measures how the key metric X vary across the experimental units  X . In many studies on A/B testing [4, 7, 5], the standard deviation is assumed to be equal for the control variant and the treatment one, when a statistical test is applied. We consider this statistic in our analysis and show that this statement is not valid for some metrics (see Section 4).

Median and quantiles. The  X  -quantile (also known as the  X  -th population quantile [14]) is defined as the value q such that P ( X  X  q  X  )  X   X  and P ( X &gt; q  X  )  X  1  X   X  ,  X   X  [0 , 1], where P ( X ) is the distribution of the key metric X over the experimental units  X . For a finite sample of observations of the key metric, we use the traditional estimator of the  X  -quantile, presented in the equation (1) in [14] 7 . In our work, we have considered 19 different  X  -quantiles with  X  = 0 . 05 i , i = 1 ,..., 19, but, due to the space constraints, we present the results only for the 5 most representative ones. Their  X  are 0 . 5 ( the median ), 0 . 25 (the first quantile), 0 . 75 (the third quantile), 0 . 05, and 0 . 95.

The median is one of the popular statistics: its meaning is similar to the one of the mean value AVG, but the median is better suited for skewed distributions to quantify behavior of a typical user since it is much more robust and sensible (the mean value is highly influenced by outliers). There are a lot of paired [31] and unpaired [34] statistical tests specialized on the median as well. Contrariwise, a quantile with |  X   X  0 . 5 | 0 could be useful in measuring the extreme cases [24, 16]: it quantifies user behaviors that are far from the mean.
A  X  X ate X  OEC, which is a percent of experimental units with some properties over all experimental units (e.g., the percent of users who click on a link) could be formalized via averaging as well [4].
Subtler estimators (like in [14]) could be also considered, we left it for the future work. Table 2: Comparison of OACs w.r.t. the number of A/B and A/A experiments with detected treatment effect with the constant threshold  X  = 0 . 05 . Hence, such quantile may help a web service to detect the treatment effect of an evaluated update on users either with lower, or higher engagement than an  X  X verage X  user (e.g., the 0.25-quantile of the number of sessions S describes users that use the service rarely, while the 0.75-quantile of it quantifies frequent users). This may be definitely important for web companies that fight for new, rare users in a competitive environment or choose to preserve and increase engagement of loyal, regular users. The  X  -quantile is referred to as Q- X  .
Entropy. The entropy statistic (also known as the Shan-non entropy) is defined as the expectation E( X ) = E (I( X )) of the information I( X ) =  X  ln P ( X ) [30]. Entropy is a measure of  X  X haos X  or unpredictability of information content. In our case, entropy is a measure of diversity of user engagement with a web service w.r.t. the metric X over  X  (e.g., it quan-tifies the diversity of users w.r.t. their number of sessions for X = S ). Hence, the entropy statistic may help the web service to detect the treatment effect of an evaluated update in terms of such diversity: whether the update increases or decreases the variety of different types of user behavior. We propose to use the entropy as a novel OEC in A/B testing.
Unpaired two-sample Student X  X  t-test. The Stu-dent X  X  t-test (T-test) is the most popular statistical test for comparing the mean values of unpaired data samples, and, hence, it is the widely used one in A/B testing [7, 33, 6, 10, 11]. Its popularity could be explained, first, by its compu-tational costs. Indeed, this test is based on t-statistic : whose calculation takes 3 n + o ( n ) arithmetic operations, where n = max {|  X  A | , |  X  B |} . Second, despite the normal-ity of the key metric X  X  distribution is one of the assumptions underlying the test, it may be applicable in practice to some key metrics that do not follow a normal distribution even approximately. Further in our study, our experimental re-sults confirm applicability of the statistical test in the case of such metrics. On the other hand, we show that violation Table 3: Comparison of OACs w.r.t. the number of A/B and A/A experiments with detected treatment effect with the constant threshold  X  = 0 . 01 . of independence of experimental units (which is another as-sumption underlying the test) results in inapplicability of the T-test due to underestimation of its false-positive rate. Mann X  X hitney, Tarone X  X are, and Logrank tests.
 This family of rank-based statistical tests is widely used in survival analysis [34] and is based on the  X  2 FH statistic: where the values of r j,A ,r j,B ,r j ,d j,A ,d j,B ,d calculated in the following way. Given two samples of the key metrics { X  X  }  X   X   X  v , v = A,B , we merge them into one increasing sequence of values. Among these values, there are K unique: y 1 &lt; ... &lt; y K , K  X  |  X  A | + |  X  B each value j = 1 ,...,K , the following values are computed: d j,v is the number of observations equal to y j , and r the number of observations not less than y j in the sample v = A,B . The values d j and r j are defined analogously using the union of the two samples.

For  X  j = r j , we get the Gehan tests, which is equal to the Mann X  X hitney U test (MW test), also known as the Wilcoxon rank-sum test ; for  X  j =  X  r j , we get the Tarone X  Ware test (TW test); and, for  X  j = 1, we get the Logrank test 8 (LR test). Calculation of  X  2 FH usually takes O ( n log n ) operations, since one needs to sort the samples  X  A and  X  by the values of the key metric. These tests could be con-sidered as tests of medians with the additional assumption that the distributions for the samples have the same shape.
Bootstrapping. As opposed to the previous ones, this test can be applied to any evaluation statistic  X  . Therefore, the Bootstrap test is referred to as BS- X  with the statis-tic  X  whose null hypothesis is tested (e.g., BS-AVG means that the Bootstrap test is applied to the mean values of the samples). We use the Bootstrap test as described in [13,
The Logrank test is one of the main statistical tests used [34] in the Cox Model, which was applied to the absence time metric in [12]. Figure 2: The distribution of 169 A/B tests w.r.t. their duration and the size of their sample of users. Alg. 16.1] with resampling of users 9 . This statistical test is the most computational expensive among test we consider, since it takes O ( NB ) operations, where N is the number of operations needed to calculate the statistic  X  (e.g., N = O ( n ) for the mean) and B is the number of bootstrap iterations. In our study, B = 1000 as in [28, 25].

Summarizing, we study 16 key user engagement metrics and apply 26 evaluation-statistic X  X tatistical-test pairs: 2 sta-tistical tests (T-test and BS) for AVG, 4 tests (MW test, TW test, LR test, and BS) for Q-0.5 (the median), 1 test (BS) for all other considered statistics .
In our paper, we consider 169 large-scale A/B experiments conducted on real users of Yandex with duration from 7 to 30 days. The user samples used in our A/B tests are all uniformly randomly selected, and the control and the treat-ment groups are of approximately the same size (at least, hundreds of thousands of users). The distribution of these 169 A/B tests w.r.t. their duration and the size of their sample of users is presented in Figure 2. Each experiment evaluates a change in one of the main components of the search engine (including the ranking algorithm, the user in-terface, the server efficiency, etc). Each of these changes is either an artificial deterioration of a component 10 its update, which is evaluated before being shipped. In our experimentation, we also consider 98 control experiments: 70 1-week and 28 2-week so-called A/A tests that compare the same versions of the service [22, 4]. Both A/B and A/A tests were conducted during the period from January, 2013 to November, 2014.
 Additionally, we generate several thousands of synthetic A/A experiments (like in [1, 7]) by randomly splitting users from one of our control experiments. We find that the results for these synthetic A/A experiments (p-value distributions, false-positive rares, etc.) are similar to the ones for real A/A experiments. Therefore, due to the space constraints, we present the results only for our 98 real control experiments.
The straightforward way to compare sensitivity of OACs is to apply them to a series of A/B experiments and compare the number of A/B tests whose treatment effect is detected by each OAC for a selected p-value threshold p val  X   X  (as in [10, 11, 9, 25]). This number is also referred to as the success
Thus, we correctly bootstrap dependent data when the ex-perimental unit of a considered key metric is not a user [1], e.g., in the case of PT , two sessions in a sample can be of the same user, and, thus, their durations are dependent. like the swap of the second and the fourth results in the ranked list formed by the current ranking [10, 25]. Figure 3: The CDFs of the p-value of the T-test for all loyalty and activity metrics over 98 A/A tests. rate in IR offline evaluation [36]. The threshold  X  = 0 . 05 is commonly used in existing studies on A/B testing [22, 19, 7, 33, 21, 10, 11]. However, smaller thresholds are also considered (e.g.,  X  = 0 . 01 , 0 . 001 , 0 . 0001) in order to focus on strong signals [20], since this is aligned with emerging needs of modern web companies to be more confident in the effect of evaluated updates on the quality of their services. But this is a double-edged sword for OAC comparison as well, since the lower the threshold  X  (i.e., the lower the false-positive rate), the lower the success rate. We expect that comparison of OACs at different p-value thresholds may lead to different results. Hence, in order to reveal it, we present our results both for the state-of-the-art threshold  X  = 0 . 05, and for the stronger one  X  = 0 . 01.

We compare all studied key metrics (7 loyalty and 9 ac-tivity ones) applied with 5 main statistical tests (T-test, Bootstrap test for AVG, MW test, TW test, and LR test) with respect to the success rate of their OACs in Table 2 and Table 3 (the first number in a cell) for the constant threshold  X  = 0 . 05 and 0 . 01 respectively. We see that the success rates of the key metrics AT and logAT 11 with the Tarone X  X are test for  X  = 0 . 05 (77 detected treatments in 169 A/B tests, i.e., 46%) and with the Logrank test (33%) for  X  = 0 . 01 are the highest ones among all loyalty OACs, moreover, the success rates of the OACs of these metrics with T-test, MW test, TW test, and LR test dominate among other loyalty OACs by a large margin. The activity metrics CpQ , PT , and logPT (applied with almost all presented statistical tests) domi-nate other OACs w.r.t. the success rate both for  X  = 0 . 05 and 0 . 01 (with the success rates up to 57% and 47% respec-tively).

In A/B testing, correctness of an experimentation is ver-ified by control experiments (i.e., A/A tests) [22, 4]. Each of them compares two identical versions of the service and
Note that logarithmic transformation is monotone, and, hence, the p-value results for the non-transformed and log-transformed variants of the same key metric for one of MW, TW, and LR tests are the same. values over 98 A/A tests. should be failed (i.e, the treatment effect is wrongly de-tected) in not more than  X   X  100% of cases (e.g., 5% for  X  = 0 . 05), if the experimentation platform is correct and the OAC is valid. The number of failed A/A tests is referred to as the false-positive rate (also known as the type I error) and is reported as the second number in the cell for each OACs in Tables 2, 3. The cells of OACs that have inappropriate false-positive rates ( &gt; 4 and &gt; 0 for  X  = 0 . 05 and 0 . 01, re-spectively) are highlighted in red color. Such OACs should be rejected in the classical approach, since their statistical tests underestimate the standard deviation of their OEC [4]. According to this, all the previously top-rated key metrics AT , logAT , CpQ , PT , and logPT are rejected. Hence, we con-clude that the metrics log(ATpA) and C for  X  = 0 . 05 ( S , sS , C and logC for  X  = 0 . 01 ) with suitable statistical tests (their cells are highlighted in boldface ) are the best ones w.r.t. the success rate among OACs with a valid false-positive rate .
In order to understand the dramatic underestimation of the false-positive rate of several OACs by their p-values (see the previous subsection), we consider the empirical Cumula-tive Distribution Function (CDF) of the p-value of an OAC over the 98 A/A experiments. If a key metric X  X  distribu-tion satisfies the assumptions that underlay a statistical test, then the p-value of this OAC should be uniformly distributed over [0 , 1] [22, 4].

We present the empirical CDFs of the p-value of the T-test for all loyalty and activity metrics in Fig. 3. We see that only the CDFs of the key metrics AT , logAT , PT , and logPT are noticeably higher than the uniform CDF, and this becomes even more apparent near 0, where commonly used p-value thresholds (  X  = 0 . 01 and 0 . 05) are situated (see also Tables 2 and 3). These metrics are united by the fact that their experimental units do not coincide with the randomization one, i.e., a user (see Table 1). This finding is in line with the observations made in [4, 35], where Boostrapping and Delta methods are suggested to estimate the variance of non-per-user metrics.

We compare the CDFs of the p-value of the 6 statisti-cal tests (T-test, MW test, TW test, LR test, BS-AVG, and BS-Q-0.5) that are utilized for the mean and the me-Figure 5: The joint distributions of our A/A and A/B experiments w.r.t. the pairs of the p-values of two statistical tests for some representative engage-ment metrics. dian of some representative engagement metrics 12 over 98 A/A experiments (see Fig. 4). We see that the observa-tion made above for the metrics with non-user experimental units holds for all considered statistical tests except for the Bootstrapping for the mean value (BS-AVG), whose CDF is approximately uniform for all these metrics. Note that their analogs that are mapped to a user (i.e., the metrics ATpA , log(ATpA) , log(AT)pA , sumPT , and logsumPT ) do not have such convexity in their CDFs, and this results in noticeably lower false-positive rates of their OACs (see Tables 2 and 3). Therefore, we conclude that, for a non-per-user metric, its transformation to a per-user one or utilization of the Boot-strap test noticeably reduce the dramatic underestimation of the false-positive rate (up to the correct estimation) .
From Fig.3 and 4, we also make an interesting observa-tion: the metrics S , sS , Q , C , sumPT , and logsumPT have a noticeable overestimation of false-positive rate. This ob-servation prompts that their real success rates for the real
From here on in this paper, due to the space constraints, we present results only for the best or representative metrics, statistics, and statistical tests, and we pay more attention to the loyalty aspect of user engagement. Figure 6: The joint distributions of our A/B exper-iments w.r.t. the pairs of the p-values of statistical tests for two engagement metrics. false-positive rates of 5% or 1% are greater than the ones presented in Tables 2 and 3 (see the next subsection).
We also find that there is a strong relationship between the p-values of the Student X  X  t-test and the Bootstrap test for the mean (BS-AVG) for all metrics, including the metrics with significantly skewed CDFs like AT or PT . The relation-ship of the tests is not linear for such metrics and is linear for the other metrics (see the top of Fig. 5). The similarity between the Student X  X  t-test and bootstrapping is also ob-served for IR metrics in [32, 36]. The relation for each of other pairs of statistical tests (see examples of joint distribu-tions in the bottom of Fig. 5) is noticeably more chaotic than the ones described above. There is no relation between the p-value of the same test applied to different metrics as well (e.g., see Fig. 6). The relation of T-test and BS-AVG infers that the false-positive level of an OAC could be improved by utilization of the latter one instead of the former one if the metric is skewed, since the Bootstrapping has approximately uniform distribution of its p-value for such metrics.
Motivated by the observations made in the previous sub-section, we estimate the p-value threshold  X  for each OAC individually, such that its real false-positive rate is 5 failed A/A tests of the 98 ones from our set of experiments. This estimator equals to the 5-th smaller p-value from the obser-vations. It is the traditional estimator of the 5 / 98-quantile, presented in the equation (1) in [14]. The subtler estima-tors from this study could also be considered, but we left it for the future work. The obtained estimations of p-value thresholds are referred to as  X  5:98 =  X  5:98 (OAC) and are presented in Table 4. The highest values for each row are highlighted in boldface , the lowest ones are underlined , the highest and the lowest values for each column both for loy-alty and activity metrics are highlighted in green color and blue color, respectively. For instance, we see that the Mann X  Whitney U test is the frequent one, whose false-positive rate has the worst underestimation, while the Logrank test (from the same family as MW test), contrariwise, frequently shows the highest overestimation for the activity metrics.
We utilize the individually adjusted thresholds  X  =  X  5:98 to get the success rates of our OACs, that are presented in Table 5. Comparing these results with the ones from Ta-ble 2, we see that, on the one hand, the previously top-rated absence-time metrics (e.g., AT and ATpA ) have now moder-Table 4: The thresholds  X  =  X  5:98 for each OAC.
 Table 5: Comparison of OACs w.r.t. the number of A/B and A/A experiments with detected treatment effect with the adjusted threshold  X  =  X  5:98 . ate success rates (similar to or lower than the ones of other loyalty metrics). On the other hand, the metrics S and sS , whose false-positive rates have been overestimated, demon-strate now higher success rates and the best ones among the loyalty metrics (for all statistical tests except the MW one). For activity metrics, we observe decay in the success rates of PT and logPT , while CpQ remains the leadership. Hence, we conclude that the adjustment of the p-value threshold to a desired false-positive rate could significantly change the state of affairs in evaluation of Overall Acceptance Criteria: the success rates of ones can noticeably increase, other OACs can become usable in evaluation of a web service, since their false-positive rates reduce to an acceptable level .
In order to compare different evaluation statistics (consid-ered in Sec. 3.3), we select 6 most representative key metrics and present the success and false-positive rates in Table 6 for their OACs. These rates are calculated both for the con-stant threshold  X  = 0 . 05 and for the adjusted one  X  =  X  (the rates are highlighted as in the previous subsections).
Figure 7: The distributions of A/B tests w.r.t. Diff pc of some pairs of evaluation statistics for log(ATpA) . Figure 8: The CDFs of the p-values for all statistics for the metrics S , log(ATpA) , and CpQ obtained from the Bootstrap test over 98 A/A experiments.
 Since the considered statistics have different meaning w.r.t. user behavior (see Sec. 3.3), we group OACs in such a way that their statistics have similar meaning and their key met-rics have the same type of engagement (activity or loyalty). The highest success rate in each group is highlighted in blue color. In Fig. 8, we compare different statistics w.r.t. the CDFs of the p-values for several metrics.

In order to compare the meaning of our statistics, we calculate the scaled relative difference Diff pc =  X  of them for each of 169 A/B experiments. In Fig. 7, we plot joint distributions of these experiments w.r.t. Diff some pairs of OECs for the metric log(ATpA) , whose OACs demonstrate the most consistent CDFs for different evalua-tion statistics (see Fig. 8). First, the joint distribution for the mean and the median (the plot (c) in Fig. 7) reveals that they have similar meaning. Therefore, we conclude that both the mean-aware and the median-aware statistical tests could be rivals in the area of typical user behavior evaluation .
Second, the mean value also coincides with the  X  -quantile (in terms of Diff pc ) for |  X   X  0 . 5 | X  0, and this correlation re-duces while the quantile becomes far from the median (i.e., |  X   X  0 . 5 | 0). We demonstrate it by the joint distribu-tions for the mean and the  X  -quantile,  X  = 0 . 05, 0 . 25, 0 . 75, and 0 . 95, in Fig. 7 (the plots (a), (b) , (d), and (e) respec-tively). The correlation between two quantiles (in terms of Diff pc ) also depends on how far the values of their  X  are (see in Fig. 7). Therefore, we conclude that quantiles corre-spond to really independent components of user engagement that represent different extreme cases of user behavior . Note that the distributions of the metrics S , Q , and C are discrete, and a significant fraction of users has the same metric value (e.g., several percents of users have 1 session during an ex-periment). Hence, quantiles of these metrics do not change during most A/B experiments, and the p-values for the cor-responding OACs have very skewed distributions (see Fig. 8 and Table 6).

Third, we see that the entropy statistic (E) has a weak cor-relation with the standard deviation (SD), whose meaning seems similar to diversity. Hence, we conclude that entropy could be considered as a very promising statistic since it en-codes a novel independent feature of engagement (diversity) in case of A/B testing, and OACs with entropy have very high sensitivity level (see Table 6) . Finally, note that the
The factor  X  is randomly chosen once in our study in order to hide real values for confidentiality reasons. standard deviations (SD) for the control and the treatment variants of the service are significantly different for a notice-able number of A/B experiments (see the success rates for SD in Table 6). Therefore, we conclude that the assumption on equality of SD for the variants A and B (as in [4, 7, 5]) should be used carefully and should be regularly validated for each OEC .
In our work, we considered a huge set of hundreds of large-scale A/B tests. We focused on the impact of different statistics and statistical tests of an OAC on the sensitivity of different key metrics. We utilized our set of experiments to evaluate dozens of key metrics of user engagement with dozens of statistical tests. First, we demonstrated that a key metric is most effective in combination with an appropriate statistical test, which is individual for each key metric. We found that utilization of the common Student X  X  t-test with a default p-value may lead to wrong conclusions on the per-formance of some state-of-the-art key metrics. Second, we shown that each combination of a key metric and a statis-tical test requires its individual p-value threshold to control the false-positive rate at a predefined level. Third, we also proposed the entropy and the quantiles as novel OECs that quantify the diversity and extreme cases of user engagement.
Future work. First, we can extend the set of evaluation statistics or statistical tests by investigating more sophisti-cated ones. Second, we can improve the technique of adjust-ing the p-value threshold by utilizing more precise statistics. Third, we can study the sign of the treatment effect and the confidence interval for each of our OACs. The authors would like to thank Nikita Povarov and Pavel Serdyukov for useful discussions.
