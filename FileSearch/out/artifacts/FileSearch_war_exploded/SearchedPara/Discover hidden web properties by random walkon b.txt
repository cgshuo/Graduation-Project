 Abstract This paper proposes to use random walk (RW) to discover the properties of the deep web data sources that are hidden behind searchable interfaces. The properties, such as the average degree and population size of both documents and terms, are of interests to deep web crawling. We show that simple RW can outperform the uniform random (UR) samples disregarding the high cost of UR sampling. We prove that in the idealized case when the degrees follow Zipf X  X  law, the sample size of UR sampling needs to grow in the order of O ( N / ln 2 N ) with the corpus size N , while the sample size of RW sampling grows power law distribution, thus RW is better than UR sampling. On the other hand, document degrees have lognormal distribution and exhibit a smaller variance, therefore UR sampling is slightly better.
 Keywords Hidden data source Deep web Random walk Graph sampling Estimator Zipf X  X  law 1 Introduction Searchable forms are ubiquitous on the web. Many web sites, especially the large ones, have searchable interfaces such as HTML Forms or programmable web APIs. The data hidden behind a searchable interface constitute a hidden web data source (Bergman 2001 ) that can be accessed by queries only. The profiles of a hidden data source, including the Giles 1998 ), to data crawlers (Ipeirotis et al. 2001 ; Lu et al. 2008 ; Madhavan et al. 2008 ; Raghavan and Garcia-Molina 2001 ; Wang et al. 2012 ), and to virtual data integrators such business intelligence, people would like to know the number of users in Facebook, their average number of follower and their variations. In distributed information retrieval there is a need to profile the data sources before deciding where the queries should be sent to (Callan and Connell 2001 ; Shokouhi and Si 2011 ). In deep web crawling, it needs to know how many documents are there so that it can decide when to stop the crawling (Madhavan et al. 2008 ; Wu et al. 2006 ).

Discovering these properties has been a long lasting challenge (Callan and Connell 2001 ), mainly due to the unequal probability of the data being sampled, or the heteroge-neity of the data. Consequently it is difficulty or costly to obtain the uniform random (UR) samples (Bar-Yossef and Gurevich 2006 ; Bar-Yossef and Gurevich 2011 ). This paper walk (RW) on the document-term graph can perform better.

For instance, we may want to learn the average document frequencies of the terms in a data source, or the average degree of the terms in its term-document graph. Average degree can be used to derive other properties such as degree variance and population size as we will show in Sect. 4 . In turn average degree and population size reveal the total number of terms of the hidden corpus.
 average degree is
One obvious but often impractical estimation method is via UR sampling, i.e., select a of the population mean:
The sample mean estimator c h d i SM is unbiased if the terms or documents are homoge-neous , i.e., they can be selected randomly with equal probability. Unfortunately this is not the case in most practice. Popular terms have a higher probability being sampled if terms are selected randomly from a document. Similarly, large documents tend to have a higher probability of being sampled if they are selected by random queries.

To analyze such heterogeneous data where elements have unequal probabilities of being sampled, various sampling methods have been studied for hidden data sources including search engine indexes (Bar-Yossef and Gurevich 2006 ), and in related areas such as the Web (Henzinger et al. 2000 ), graphs (Bar-Yossef and Gurevich 2008 ; Leskovec and Fa-loutsos 2006 ), online social networks (Gjoka et al. 2009 ; Papagelis et al. 2011 ), and real social networks (Salganik and Heckathorn 2004 ; Wejnert and Heckathorn 2008 ). The typical underlying techniques include Metropolis Hasting Random Walk (MHRW) (Metropolis et al. 1953 ) for uniform sampling and RW (Lova  X  sz 1993 ) for unequal prob-ability sampling. MHRW is reported rather good at obtaining a random sample. However, rather high especially for hidden data sources. The samples are retrieved by queries that occupy network traffic, let alone the daily quotas impose by data providers. Thus a practical sampling method should include all the samples even if they induce bias.
Even when random samples are obtained, the sample mean estimator has a high vari-ance because the degree distribution of the terms usually follows Zipf X  X  law (Montemurro 2001 ; Zipf 1949 ). Most terms have small degrees, while a few of them have huge degrees. estimation diverge.

We propose to use harmonic mean, instead of arithmetic mean, of the sample as the estimator of the average degree of documents and terms: Here the subscript H indicates that it is the harmonic mean, and that it can be derived from the traditional Hansen X  X urwitz estimator (Hansen and Hurwitz 1943 ).

The sample for this estimator is obtained by low cost simple RW where the node selection probability is asymptotically proportional to its degree. It is rather common to use Hansen X  X urwitz related estimators when selection probabilities are not equal for elements in the population. But usually people use probability proportional to size (PPS) sampling because of the unavailability of random samples (Salganik and Heckathorn 2004 ). This samples are available X  X t has a very small bias, and the variance is smaller than the sample mean estimator for terms and is only slightly larger for the documents.

The crux of population size estimation is the heterogeneity of the data X  X ocuments and terms have unequal probabilities of being sampled. Yet the degree of the heterogeneity, ditional sampling studies where the accurate degree is hard to quantify. In our setting the degrees of sampled documents and terms are easy to obtain, thereby the average degree is ascertained accurately thanks to the estimator c h d i H . Thus, the CV can be estimated by With the knowledge of c , the population size can be obtained by Here N 0 is an estimator for homogeneous data, and n 2 1 C is one of the N 0 estimators. C is the collisions of the nodes happened during sampling.

Our main contributions in this paper can be summarized as follows:  X  For average degree estimation we show that RW sampling can outperform UR  X  We show that RW is not always better than UR sampling. We give the condition when  X  We show that average degree is an important property that can lead to the discovery of  X  We solved the open problem to correct the bias in capture X  X ecapture method. It is well
In the following we will first introduce the related work, especially the background of population size estimation. Then we model the query-based sampling as a RW on a bipartite graph. Section 4 introduces two estimators, one for average degree and the other for population size. We prove that in the idealized case where term degrees follow Zipf X  X  law with exponent one, our proposed estimator c h d i H is much better than c h d i SM when the corpus size is large. The experiments section dissects the Reuter corpus with details of the data distributions, sample distributions, and estimation results with various sample sizes. Then we give an intuitive explanation for why c h d i H can reduce the variance. 2 Related work Query based profiling of hidden data sources has been studied ever since the occurrence of web query interfaces. One of the early influential works is the estimation of search engine size (Lawrence and Giles 1998 ). The problem can be further classified by the syntax of the queries allowed and the types of data bases sitting behind. Queries can be simple key words (Broder 2006 ; Callan et al. 1999 ;LuandLi 2010 ;Shokouhietal. 2006 ; Zhang et al. 2011 ), boolean expressions (Lawrence and Giles 1998 ), or even SQL queries (Dasgupta et al. 2007 , 2010 ;Haas et al. 1995 ). The data sources can be text databases such as a collection of documents (Bar-Yossef and Gurevich 2008 ; Callan et al. 1999 ; Broder 2006 ;Shokouhietal. 2006 ), or struc-Haas et al. 1995 ). Our paper assumes simple keyword interfacing with textual database.
The background of this research is the population size estimation and sampling that have been widely studied in other disciplines (Thompson 2012 ) especially in ecology (Amstrup et al. 2005 ) and social studies (Salganik and Heckathorn 2004 ), and more recently in computer science for estimating the size of the web (Lawrence and Giles 1998 ), databases (Haas et al. 1995 ), web data sources (Broder 2006 ; Dasgupta et al. 2010 ; Henzinger et al. 2000 ; Zhang et al. 2011 ; Zhou et al. 2011 ), and online social networks (Gjoka et al. 2009 ; Katzir et al. 2011 ; Ye and Wu 2011 ). 2.1 Average degree estimation in turn CV can be used in the population size estimation; (2) it reveals the overall data size. directly available due to the restricted sampling interface as in the query-based sampling. Although there are studies to obtain the UR sample using rejection method or MHRW (Bar-Yossef and Gurevich 2008 ), the cost will be rather high. Therefore there are studies to derivation of the harmonic mean estimator can be found in (Salganik and Heckathorn sampling probability can be verified. The biased samples are taken because UR sampling is impossible. The harmonic mean estimator is derived to correct the bias, not to improve the performance of the estimation.

In the area of peer-to-peer network (Rasti et al. 2009 ) and online social network (Gjoka et al. 2011 ; Kurant et al. 2011 ), the re-weighted RW that resembles harmonic mean was used and empirically compared with MHRW, but not with UR samples.

Our work is the first to show that RW can outperform UR samples disregarding the cost of obtaining these samples. In addition, we give the conditions when RW could be better. The preliminary result was also published in our workshop paper (Lu and Li 2012 ) where the data is online social network. This paper reports our recent progresses on the following aspects: (1) we experiment on text bipartite graph instead of the non-bipartite graph rep-resenting social networks; (2) we show that RW is not always better than UR sampling, and analytically give the conditions when RW outperforms UR sampling; (3) the experiments verify that when the degree distribution follows power law, RW sampling is much better than UR sampling. When the degree distribution is log-normal, UR is slightly better than RW sampling; (4) in addition to average degree estimation, population size estimation is discussed and experimented in detail; (5) we add the intuitive explanation as for why UR can be better than RW sampling. 2.2 Population size estimation 2.2.1 Capture X  X ecapture method The starting point of population estimation is the well-known Lincoln X  X etersen estimator (Amstrup et al. 2005 ) that can be applied when there are two sampling occasions and every node has equal probability of being sampled: where n 1 is the number of nodes sampled in the first capture occasion, n 2 is the number of nodes sampled in the second occasion, d is the duplicates among two samples. The assumptions of Lincoln X  X etersen estimator can be hardly met in reality. It is extended in two dimensions: one is allowing multiple sampling occasions, the other is supporting heterogeneity in capture probability, as will be discussed in the next two subsections.
Albeit its simplicity and severe restriction, most of the existing work used the capture X  recapture sampling method and the corresponding Lincoln X  X etersen estimator in one form or another. The classic work is the estimation of the Web and the search engines described in (Lawrence and Giles 1998 ) and (Bharat and Broder 1998 ). Both approaches use queries to capture documents, and count the duplicates between the two captures. In (Lawrence therefore they presented the estimation as relative size, not the absolute size. Bharat and Broder ( 1998 ) investigated the causes for the inaccuracy, in particular the unequal prob-( 2005 ) used the same method in a larger scale, by building query lexicon from dmoz.com directory that contains 4 million pages.

Continuing in this direction, other innovative methods are proposed for those two Lincoln X  X etersen estimator, and the bias problem remains un-tackled. Both Si et al. ( 2002 ) and Kunder 1 used query frequencies to estimate search engine sizes. They take a sample set of documents with size ( n 1 ), and another sample set of documents that contain a specific query. Say the second capture is of size n 2 , which is actually the document frequency of the query, and it may be provided by search engines. The overlapping d is the the intersection documents. The advantage of this method is that n 2 does not need to be calculated by the sampler. The disadvantage is also obvious: in both sampling occasions the documents are not sampled with equal probability. What is worse, the document frequency returned by search engines are often inflated, sometimes in orders of magnitude.

In Broder et al. ( 2006 ) also use Lincoln X  X etersen estimator, but each capture is defined as the documents covered by many queries. Because the number of queries is very large, it is not possible to obtain n 1 and n 2 directly by actually submitting the queries. Instead n 1 and n 2 themselves are estimated.

Equation 6 is ubiquitous and applied in various forms. Quite often even the users may instance, ID sampling is used to estimate Facebook population by leveraging the fact that number uniformly at random in the range 1 X 10 9 , then probe the server to check whether it d . Then, according to 6, we have 10 9 = n 1 n 2 / d . When n 2 and d are available, we can use the equation to estimate n 1 , the number of valid IDs. 2.2.2 Multiple capture X  X ecapture method When there are more than two sampling occasions and each time only one sample is taken, Darroch ( 1958 ) derived that the approximate Maximum Likelihood Estimator (MLE), b N D , is the solution of the following equation: where n is the total sample size, and d is the duplications. This equation has also been used to predict the isolated nodes in random graph when edges are randomly added (Newman 2010 ). Unfortunately it does not have a simple closed form solution (Darroch 1958 ; studies, Ye and Wu ( 2011 ) used numeric method to find the solution to this estimator. Lu and Li ( 2010 ) gives an approximate solution for N that reveals a power law governing the data not sampled and the overlapping rate. In a simpler form, it states that the percentage P of the data not sampled decreases in the power of the overlapping rate R = n /( n -d ), i.e., 2.2.3 Unequal sampling probability When the data is heterogeneous, i.e., elements have unequal probabilities of being sam-pled, the estimation becomes notoriously difficult. One approach to solving this problem is to obtain the UR sample (Bar-Yossef and Gurevich 2008 ) using algorithms such as MHRW, then traditional estimators are applied on the random sample. The other approach assigned less weight to long documents being sampled; Shokouhi et al. ( 2006 ) run erogeneous data; Lu ( 2008 ) and Lu and Li ( 2010 , 2013 ) went a step further by using c , the degree of heterogeneity, to adjust the discrepancy.

The problem is that estimating c itself is equally challenging. Therefore Eq. 5 as an et al. ( 1992 ) in a reverse way to estimate c as below: where N 0 is a rough estimation for N assuming the data is homogeneous. This method was demonstrated (Chao et al. 1992 ) on small data where c 2 is typically around one. That is, the ratio between N and N 0 is around two. In our large and power law data c 2 can go up to hundreds, making the traditional estimator biased downwards by hundreds of times smaller than the real value.

In the estimation of digitalized networks such as hidden web data sources, the sampling sampling schemes where sampling probability of animals are different but the exact var-iance is impossible to quantify accurately, in the simple RW on the term-document graph we know not only the exact degree of the node being visited, but also that the sampling probability is proportional to its degree. With this knowledge, we can obtain the value of c , thereby estimator b N can be applied. Not surprisingly, Katzir et al. ( 2011 ) used a similar equation to estimate the size of online social networks: estimator.

Note that estimator b N can be approximated by equations either 7 or 8 when c = 0, sample size is small, and collisions C can be approximated by duplicates d . The approx-imation can be established by applying Taylor expansion on the right hand side of Eq. 7 . 2.3 Other size estimation methods In contrast to the traditional sampling in ecology and social studies, the diversity of the samples valid Facebook IDs from an ID space of 9 digits, utilizing the Facebook imple-mentation details that make the number of invalid IDs not much bigger than the valid ones; Zhou et al. ( 2011 ) leverages the prefix encoding of Youtube links; Dasgupta et al. ( 2010 ) depends on the negation of queries to break down the search results; Lu ( 2010 ) and Zhang use RW in query space to probe database properties, which is different from our RW in that (1) they suppose the SQL like syntax of the queries that support boolean expressions. (2) The RW is on the query space constructed by the boolean operators, not the document-term graphs in our paper. 3 Problem definition source can be modelled as a document-term bipartite graph G = ( D , T , E ), where nodes are divided into two separate sets, D the set of documents, and T the set of terms. Every edge in E links a term and a document. There is an edge between a term and a document if the term occurs in the document.
 distinct terms in document i . Let d j T denote the degree (or document frequency) of the term h d T i X  s = j T j .
 average degree for documents or terms, and N to denote the size of the population | D |or| T |. We use terms and queries interchangeably with slight different connotations: a lexeme in a document is a term. When a term is sent to a searchable interface it is called a query. Example 1 ( Bipartite graph of hidden web ) Figure 1 gives an example of a hidden data T  X f q 1 ; q 2 ; ... ; q 7 g : h d T i X  18 = 7, and h d D i X  18 = 9 :
A simple RW on the document-term graph is described in Algorithm 1. First a seed query is selected randomly from a dictionary and the list of the matched document URLs are retrieved. From the list we select randomly one of the URLs and download the cor-responding document. From the downloaded document a query is selected randomly and sent to the data source. The process is repeated until n number of sample documents and n number of sample terms are obtained. In the samples the documents or the terms can be visited multiple times. In other words it is a sampling with replacement.
During the RW process, we do not need to explore all matched documents. Instead, we can first ask for the number of matches m , generate a random number r between 1 and m , document and term at most two queries are needed, one to get the degree of the query, the other to get the page containing the r th document.
 Example 2 ( Random walk ) If the sample size n is 5 and the seed term is q 1 . A RW result can be 4 Estimators This paper focuses on two properties, the average degree and the total population size for both terms and documents. When UR samples are available, the former property can be estimated by the sample mean, the latter by capture X  X ecapture methods (Amstrup et al. 2005 ). How-ever, UR samples are not easy to obtain. It is well known that in RW large documents and queries have higher probability of being visited. Asymptotically the sampling probability of a developed for such samples whose sampling probability is proportional to their sizes.
We first develop the estimation of average degree, including the average length (number of distinct terms) of the documents and the average size (or document frequency) of terms. Based on the average degree, the estimator of population size (total number of terms and documents) is derived.

Table 1 summarizes the notations used in this paper. 4.1 Average degree Suppose that in the document-term graph there are N number of document nodes. s =
The variance r 2 of the degrees in the population is defined as (Thompson 2012 ) where h d 2 i is the arithmetic mean of the square of the degrees in the total population.
The coefficient of variation (CV, also denoted as c ) is defined as the standard deviation, or the square root of the variance, normalized by the mean of the degrees: for i  X  1 ; 2 ; ... ; n . Our task is to estimate the average degree h d i using the sample. 4.1.1 Sample mean estimator defined below: The variance of the estimator c h d i SM is (Thompson 2012 ) The problem with this sample mean estimator is that the UR sample is not easy to obtain. Moreover, its variance is too large to be of practical application if the degrees have a large variance. It is well established that the degree of the terms follows Zipf X  X  law, causing the population variance r 2 of the term degrees very large.
 sample mean estimator can be described by the following theorem Theorem 1 Suppose the degrees follow Zipf X  X  law with exponent one , i.e., d i  X  A a  X  i , where A and a are constants. The variance of the sample mean estimator is Proof See  X  X  X ppendix X  X . 4.1.2 Harmonic mean estimator When sampling probability is not equal for each unit, a common approach is to use Hansen X  X urwitz estimators (Thompson 2012 ). In the case where the sampling probability mean of the degrees: We refer to Salganik and Heckathorn ( 2004 ) for detailed derivations of the estimator in the According to Cochran ( 1977 ) the bias is on the order of 1/ n . Since the sample size n in our setting is far greater than one in general, the bias is negligible. Its variance can be derived from the variance of Hansen X  X urwitz estimator using the Delta method, resulting in: variance will be supported by our experiments in Sect. 5 .
 following theorem that can highlight the reduced variance of the estimator: Theorem 2 When the degrees follow Zipf X  X  law whose exponent is one, the variance of the estimator is Proof See  X  X  X ppendix X  X .
 c ln
N ), almost linearly with N when N is large. In other words, in order to make the variance c h d i SM , but merely ln N for c h d i H .
 Example 3 ( Degree estimation ) For our example, the harmonic mean estimation for average degree of terms is For documents the estimated average degree is 4.2 Population size estimation The population size can be estimated as follows, where c N 0 is the estimation of N if the samples are taken uniform randomly, n is the sample size, C is the number of collisions, c is CV of the degrees. Let f i denote the number of individuals that are visited exactly i times.
 The derivation of the estimator is given in Appendix 9.3. It can be also derived as a special case of Eq 3.20 in (Chao et al. 1992 ). But that equation is used to estimate c instead of N . Katzir et al. ( 2011 ) used an equivalent formula but in a very different form.
Note that this is a biased estimator as we pointed out in (Lu and Li 2013 ). The relative collisions C is large, such bias can be neglected. This paper uses the estimator in Eq. 19 to focus on the other bigger bias, i.e., the bias introduced by c . The elegance of the equation is that when the samples are uniform, c = 0, and the estimator is reduced to the traditional birthday paradox or capture X  X ecapture method. When the sample is obtained by RW, the sampling probability is not equal among all the documents or terms, resulting in c [ 0. The population size can be estimated as if the samples were taken uniformly, then multiplied by c 2 ? 1. This was not seen in literature as far as we are aware.
The reason of this formulation being overlooked may due to the challenge of deter-mining c in traditional estimation problems. In ecology and social studies the degree of a node can not be quantified accurately. For instance, it is hard to determine the friends of a drug-addict. This makes it impossible to perform a simple RW in the graph. In our setting of deep web data sources, we know exactly the number of documents a query matches, and the number of terms a document contains. Leveraging this information, the heterogeneity c degrees obtained by a RW is where p i = d i / s is the selection probability of node i . Hence where h d w i can be estimated by its sample mean and h d i can be estimated by its harmonic mean h b d i H . Combining the two equations we derive the estimator for c as follows: Example 4 ( Population size estimation ) Continuing on our example data source for terms. n = 5, f 1 = 1( q 1 ), f 2 = 2( q 6 and q 7 ), therefore 5 Experiments 5.1 Datasets Our method is tested against several datasets including Reuters newswires (Reuters 2008 ) (Reuters), newsgroups 2 (NG20), KDDCUP 2013 research papers (KDDCUP), 3 and a subset experiment a term is a sequence of letters and is case-insensitive. The term population N is the total number of distinct terms that are collected in all the documents in the corpus. The degree of a term is the document frequency of the term, i.e., the number of documents that contain the term. The degree of a document is the number of distinct terms in that document.
We list c , the normalized standard deviation of the degrees, for each dataset. It is well known that term degrees (document frequencies) follow power law, while document degrees follow lognormal law. Reflected in our datasets, c for term degree is larger than that of the document degrees. As a verification, we show distributions of Reuters, for both term degrees and document degrees in Fig. 2 . In order to show both ends of the distributions, we plot the degree against its rank in sub panel (A) and (C), as well as the frequency against its degree in sub panels (B) and (D). The former plot has a better view of the top degrees, while degrees and document degrees are different, which is the cause of different values for c , and different results on average degree estimation for terms and documents. The term degrees obviously follow Zipf X  X  law, while document degrees are more like log-normal distribution. In addition, document degrees sit in a very narrow range (min is 6, max is 1,659) compared with term degrees (1 X 434,202). Therefore the heterogeneity of those two kinds of degrees are very different. CV of term degrees is 16, and CV of document degree is merely 0.7. 5.2 Summary of the results First, we evaluate the estimators in terms of relative standard error (RSE) that is defined as below: where E  X  X  X  is the expectation of X , i.e., the mean of all the possible values, which can be approximated by the sample mean when the number of values is large. We omit bias or MSE because these estimators have negligible biases when the sample size is not small, and the degree estimator for UR sampling does not have bias. In the next subsection, we will give more detailed evaluation for Reuters data in terms of both bias and variance.
Table 3 summarizes the RSEs for the four combinations of two estimators for both estimation, the sample size varies with real data size N , approximately in the order of That is the number needed to produce some collisions.

Overall, we observe that RW outperforms UR samples, by obtaining smaller variance using the same sample size. There are also cases where RW is worse than or close to UR, for instance in document degree estimation in Reuters, NG20 and Wiki datasets. We highlight those RSEs using bold font in Table 3 . This is because c for these datasets are advantage, such as in KDDCUP data. 5.3 Average degree of Reuters Estimators are normally evaluated in terms of bias, standard error (SE), and rooted mean squared error (RSME). In the case of average degrees they are defined as Since Bias 2 ? SE 2 = RMSE 2 , and bias is negligible compared to SE according to Sect. 4 where Bias, SE and RMSE are reported, in the remaining experiments we report SE only.
Average degrees are estimated on both UR and RW samples using sample mean esti-respectively. Since the degrees of terms have a much larger CV than that of documents, RW estimator is better than the UR for terms, but slightly worse for documents. 5.3.1 Average degree of terms The two estimators are tested on the data for 10 different sample sizes ranging between 10 3 and 10 4 . For each sample size we repeat the experiment for 200 times and the results are plotted in Fig. 3 . Panel A compares UR and RW using box plots. It shows that RW has a estimations in the last box of panel A when sample size is 10 4 . It demonstrates that (1) both RW and UR estimations follow normal distribution with the same mean value (233); (2) RW has a smaller variance than UR.

Since the estimations follow normal distribution, the 95 % error bound can be calculated as roughly twice of the standard error described in Eqs. 14 and 17 . Panel C plots the error bounds along with 10 large samples, each with size up to 2 9 10 5 . Although 10 sampling processes are hardly discernible from each other in the plot, what we want to show is that mostly they are within the error bounds as predicted by Eqs. 14 and 17 . The plot validates the equations for estimated variances, and gives another perspective explaining why RW is better than UR. We will elaborate this further in Sect. 6 This plot also helps us determine how large the sample should be to achieve a satisfactory estimation.

The bias, standard error, and RSME of the two estimators are tabulated in Table 4 .It shows that indeed c h d i H has a very small bias as expected in Sect. 4.1.2 for most sample sizes except for the smallest ones. When the sample size is very small ( n &amp; 1,000), RW has dictate the outcome of c h d i H , but they can be hardly visited within a small number of RW steps. Nonetheless, even for small samples where n = 1,000 the overall indicator RMSE of RW is 117, still smaller than that of UR (132) as highlighted in bold font in Table 4 .We also experimented with sample size n = 500 where RMSE of UR is slightly better than RW. similar SEs from RW and UR methods. For instance when the sample size is 6,000, the SE of RW is 36.7005. On the other hand, UR needs more than 10,000 samples to achieve the same SE as highlighted in bold font. 5.3.2 Average degree of documents (CV = 0.7). Since the standard error of UR is already rather small, there is little chance for RW to beat the UR samples.

However, UR sampling is only slightly better than RW sampling for the same sample MHRW, the total cost shall be much higher since samples can be rejected many times. In simple RW we only need to double the sample size to achieve better result achieved by UR samples. For instance, the standard error of 3,000 RW samples is 4.1151 (in bold font), while 1,500 UR samples can achieve similar standard error. 5.4 Population size In this experiment again RW samples are compared with UR samples. For RW samples, the population size N for both documents and terms are estimated using Eq. 19 , where c is estimated using Eq. 24 . For UR samples the same estimator is used except that c = 0.
Tables 6 and 7 show the standard errors for term population and document population, sizes 500, 1,000 and 1,500 due to zero conflict (C = 0). For document population size, the smallest sample size is 2,500, no longer 500 as in other experiments because small size may not induce collision in both sampling methods. Correspondingly the number of rep-etitions of the tests in this experiment is reduced to 20.

In both term and document population size estimations, RW works better than UR in terms of standard error, even for the document size estimation where CV is not large. This is because for the same sample size n RW has larger expected collision, therefore smaller relative variance. In addition, RW needs smaller sample size to produce non-infinite estimations. In UR sampling the sample size needs to be greater than collisions can occur and the estimate is not infinite. For RW sampling, large documents have higher probability of being visited, thus smaller sample size can also induce collisions. 6 Discussions This paper shows that the biased sampling can be better than uniform sampling. In the past, such as PPS sampling only when uniform sampling is impossible (Salganik and Hecka-thorn 2004 ) or costly. The results of this paper suggest that in the context of hidden data sources, RW sampling instead of uniform sampling should be used, even when UR samples are readily accessible.

We explain this using average term degree estimation as an example. The sample distributions of the degrees are depicted in Fig. 4 . Panel A is the degree distribution for UR sample, which resembles the distribution of the population as expected. Panel B describes sampled many times. In other words, both small terms and large terms are sampled multiple times but for different reasons. Rare terms are sampled because there are large number of them, even though each term has a very small probability of being sampled. Large terms are sampled because they have higher probability of being visited, even especially the very popular words, are included by chance, in RW samples both rare words and popular words are well represented in the sample.

We elaborate this point further using a simplified fictitious example to gain an intuitive polarized scenario that contains only two kinds of nodes X  X ne million of small nodes with degree one and ten large nodes with degree one million. This mimics the scale-free graph that has many small nodes and a few very large nodes. Suppose that the sample size n is 10 4 (1 % of the population). In both UR and RW sampling, the expected estimations are close to 11, the true value.

In UR sampling, the probability of a node being visited is p &amp; 1/10 6 when one sample node is taken. When n = 10 4 samples are taken, the number of times a node is sampled follows binomial distribution B(n,p) whose expectation is np = 10 -2 . Thus the expected number of small nodes being sampled is 10 -2 9 10 6 = 10 4 , the expected number of large nodes being visited is 10 -2 9 10 = 0.1. However, we can not have 0.1 number of node. Instead, most probably a large node is sampled zero or one time. Either case the estimation deviates from the real mean greatly as shown in Table 8 . In Case 1 the large node is not sampled, resulting in the estimation 10. In Case 2 the large node is sampled once, resulting in the estimation 100 which is way larger than the expectation 11.
 portional to its degree. For a small node, the probability being sampled when one sample is larger, i.e., p l = 10 6 /(11 9 10 6 ). Thus the expected number of times a small node being sampled is np s = 1/(11 9 10 2 ), the expected total number of small nodes being sampled is 10 6 /(11 9 10 2 ) = 909. The expected number of large nodes being sampled is 10 9 np l = 9,091. Since the expected values are way larger than one, the estimates will not deviate a lot from the expected values. 7 Conclusions This paper tackles the estimations of the average degree, the degree heterogeneity, and the population size in hidden web data sources. We show that the three proposed estimators are dependent on each other X  population size is dependent on the heterogeneity, and in turn the heterogeneity relies on the average degree. Such decomposition of the estimation problem has not only the pedagogical significance, but more importantly, a large problem is divided into two smaller ones, and each sub-problem can be approached with different methods, not necessarily by RW.

The highlight of the paper is not the RW estimators. Rather, it is the comparison with the UR sampling. It shows that when the data follow Zipf X  X  law, the variance of the UR rithmically. In real graphs with moderate high CV such as term degrees in Reuters, the RW method is already much better than the UR samples, let alone the high cost of obtaining graphs, RW is orders of magnitude better than UR samples.

This paper shows that the behaviour of the RW method depends on the heterogeneity of the data. For term degrees whose variance is large (CV = 16), the RW method has a big lead over the UR method. For document degrees where the variance is small (CV = 0.7), the RW method is slightly worse than the UR samples if the cost of random sampling is excluded. In ecology studies the data reported usually have small heterogeneity whose c 2 is around one. In our big data c 2 is in the scale of hundreds or thousands. This big difference entails new methods that are drastically different from the traditional estimators such as the ones developed in (Chao et al. 1992 ).

For the population size estimation, RW is better than UR for both terms and documents in two perspectives. One is that in UR sampling the sample size needs to be greater than ffiffiffiffiffiffi 2 N p large documents have higher probability of being visited in the RW, smaller sample size can also induce collisions, consequently produce non-infinite estimates. Secondly, the standard error of RW is smaller than that of the uniform sampling because the expected collisions are larger in RW.
 Appendix Both Theorems 1 and 2 assume that the degrees follow the Zipf X  X  X  X andelbrot law (Montemurro 2001 ) which states that if the term degrees d i are sorted in descending order, then where a and A are constants. a N . All the degrees sum up to s , i.e., where we use B = ( a ? N )/( a ? 1) to make our derivations more concise. Therefore the normalizing constant A  X  s = ln B . Besides, since N is a very large number: Proof of Theorem 1 Based on Eqs. 27 and 28 , the variance of all the degrees is
Using Eq. 14 the variance of c h d i SM is Proof of Theorem 2 When nodes are sampled with simple RW, the asymptotic probability of the node i being the Hansen X  X urwitz size estimator of the population size N is (Thompson 2012 ): and the variance of c N H is (Thompson 2012 ): Replacing p i with d i / s and expand d i with A /( a ? i ), we have The Taylor expansion of c h d i H around N is By the Delta method, the variance of c h d i H is Population size estimation Nodes are selected during RW. When selecting two nodes, the probability that the same p = follows binomial distribution B ( n ( n -1)/2, p ) whose mean is The collision probability p can be translated into the heterogeneity of the data measured by c using the definition of c in Eq. 12 : Combining Eqs. 37 and 36 we obtain the expected number of collisions is: Hence the population size can be described by Since E ( C ) is unknown, it can be estimated by the observed collisions C . This gives us the estimator References
