 Informational needs behind queries, that people issue to search engines, are inherently sensitive to external factors such as breaking news, new models of devices, or seasonal changes as  X  X lack Friday X  . Mostly these changes happen sud-denly and it is natural to suppose that they may cause a shift in user satisfaction with presented old search results and push users to reformulate their queries. For instance, if users issued the query  X  X IKM conference X  in 2013 they were satis-fied with results referring to the page cikm2013.org and this page gets a majority of clicks. However, the confernce site has been changed and the same query issued in 2014 should be linked to the different page cikm2014.fudan.edu.cn . If the link to the fresh page is not among the retrieved re-sults then users will reformulate the query to find desired information.

In this paper, we examine how to detect changes in user satisfaction if some events affect user information goals but search results remained the same. We formulate a prob-lem using concept drift detection techniques. The proposed method works in an unsupervised manner, we do not rely on any labelling. We report results of a large scale evaluation over real user interactions, that are collected by a commer-cial search engine within six months. The final datasets consist of more than sixty millions log entries. The results of our experiments demonstrate that by using our method we can accurately detect changes in user behavior. The de-tected drifts can be used to enhance query auto-completion, user satisfaction metrics, and recency ranking.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Information Filtering Research was performed while the author was at Microsoft Bing.
 Algorithms, Performance, Experimentation Query reformulation, concept drift, information retrieval
Millions of users interact with search engines daily to ob-tain fresh information quickly while minimazing their ef-fort. Users issue a query Q and a search engine returns search result page ( SERP ) that is a ranked list of URLs: SERP = ( url 1 ,...,url i ,...,url n ). The order of URLs in SERP is optimized to fit a history of user interactions with a pair  X  Q,SERP  X  [3]. However, events from the outside world and time can affect user behavior on the Web [24], [25]. To illustrate this drift in the user information goals let us consider the following examples: User satisfaction with a pair  X  Q,SERP  X  can decrease dra-matically if user information needs change due to some event or decay/change of interest over time. In this paper, we an-swer the question  X  X ow can we detect a drift in user sat-isfaction with the pair  X  Q,SERP  X  using users X  interactions on the SERP ? X 
When users struggle to find an answer for Q they run a follow-up query Q 0 that is an expansion of Q . Query refor-mulation is the act of submitting a next query Q 0 to modify a previous SERP for a query Q in the hope of retrieving better results [13]. Such a query reformulation is a strong indication of user dissatisfaction [1]. We call this the re-formulation signal . Our hypothesis is that a decrease in user satisfaction with  X  Q,SERP  X  correlates nicely with the reformulation signal. In other words, the probability of re-formulating Q will grow dramatically. 0 0 . 2 0 . 4 0 . 6 0 . 8 1 Figure 1: The histogram of the probability to re-formulate query  X  X lawless X  in 2013 with one month granularity.

Let us consider the probability of reformulating a query  X  X lawless X  during year 2013. A histogram of this probability is shown in Figure 1. We can clearly see that a drift hap-pened in December. When users ran this query before Octo-ber 2013 they most probably were looking for a movie, called  X  X lawless X  . However, the singer Beyonce released her new soundtrack also called  X  X lawless X  in November 2013. Hence, this event affected dramatically the meaning of this query. As a result, if the desired song was missing in SERP a ma-jority of users reformulated the query by expanding it with the term  X  X eyonce X  .

We propose an unsupervised approach for detecting drifts in user satisfaction for pairs  X  Q,SERP  X  by applying a con-cept drift technique [33] leveraging reformulation signal. Con-cept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time [10]. Furthermore, the reformulation signal is considered to be less noisy and if reformulations are fresh and done only by users X  initiative then we can say that a reformulation signal is not biased by information coming from the search engine. Moreover, the proposed method produces: The specific contributions of this paper include: 1. A definition of query reformulation signal as an effec-2. An analysis and formulation of a drift detection in user 3. An unsupervised method for detection changes in user 4. A large scale evaluation over real user queries, showing
The remainder of this paper is structured as follows. Sec-tion 2 describes background and related work. A formal de-scription of the proposed method to detect changes in user satisfaction is presented in Section 3. Section 4 describes a research methodology for a large scale exploratory analysis of real user behavior logs from a commercial search engine. In Section 5 we describe obtained results. In Section 6 we discuss potential applications, that can benefit within pro-posed method, are described. We summarize our findings, discuss possible extensions of the current work and conclude in Section 7.
Our work examines how to model and detect changes in user satisfaction that can be a useful feature for a dynamic ranking. Huffman and Hochster [16] observed a strong corre-lation between the relevance of results and user satisfaction using navigational and non-navigational queries. Relevance is a complex concept (for a detailed review see [26], [27]). In a simplified view relevance Rel can be defined as a score for a pair of query Q and document D , where D in this case is a link URL to the web page: Rel = k X  Q,SERP  X  X  . However, it is logical to assume that Rel have an altering nature because user preferences change due to external events and passage of time. Dong et al. [7] proposed a classifier to detect re-cency sensitive queries. This classifier gives a score, called  X  X uzzines X  , to a query Q and Q is considered as a breaking-news one if its final buzzines score exceeds some threshold. Moreover, recency ranking is proposed to overcome an issue with ranking time-sensitive queries. It proposes Rel that takes a freshness of a document into account. Dong et al. [7] proposed to incorporate recency features in a ranking model. The ranking function includes recency features: (timestamp, linktime, WebBuzz, page topic) and it gives a gain for rank-ing metrics. The paper [18] suggests a temporal click feature, called ClickBuzz , that captures a spiking interest in the pair  X  Q,SERP  X  . This method helps to exploit user feedback for time-sensitive queries. The use of ClickBuzz in the ranking models leads to an improvement in NDCG 5 . Our method can be considered as a supplement to recency ranking, it detects moments when drifts happen and we need to adjust our ranking function in order to produce up-to-date results.
User satisfaction has been researched extensively. User clicks are reasonably accurate on average to evaluate user satisfaction with pairs  X  Q,SERP  X  [2], [22], using click-through information. This user satisfaction scenario is successfully applied to navigational queries. It is called query-level sat-isfaction . However, we have to take into account the fact that user clicks are biased: 1. to the page position in SERP [6],[21]; 2. to the quality of the page X  X  snippet [32], and (3) to Authors of the paper [4] claim that a search scenario for informational queries is different. Users can run follow-up queries if they are unsatisfied with the derived results. Re-formulations can lead users to an answer. This scenario is called task-level satisfaction [7]. Past research proposed dif-ferent methods for identifying successful sessions. Hassan et al. [12] used a Markov model to predict success at the end of the task. Ageev et al. [1] exploited an expertise-dependent difference in search behavior by using a Condi-tional Random Fields model to predict a search success. On the other hand, separate researches are interested in situa-tions when users are frustrated. Feild et. al [9] proposed a method for understanding user frustration with the pair  X  Q,SERP  X  . Authors gave users difficult information seek-ing assignments and evaluated their level of dissatisfaction via query log features and physical sensors. The authors demonstrated that the prediction model gets the highest quality when it is built based on query log features, de-scribed in the paper [30]. One type of user behavior that can be clearly associated with frustration is search engine switching. Authors of the paper [11] showed that one of the primary reasons users switched their search engine was due to dissatisfaction with the results on the SERP .

In our work, we consider a scenario when user satisfaction at time t i with  X  Q,SERP  X  turn into user frustration at t with the same  X  Q,SERP  X  . We associate user satisfaction using the reformulation signal. If the probability of reformu-lating query Q was close to zero at t i and grows dramatically at t i +1 , then a change happened in user satisfaction. Our scenario corresponds perfectly to a definition of real concept drift . In dynamically changing and non-stationary environ-ments, the data distribution can change over time because of the phenomenon of concept drift [28], [31]. The real con-cept drift refers to changes in the conditional distribution of the output (i.e., target variable) given the input (input features), while the distribution of the input may stay un-changed. Formally concept drift between time point t i and time point t i +1 can be defined as: where p t i denotes the joint distribution at time t i between the set of input variables X and the target variable y . In this work, we follow a lead of [5], [8], [14] and reuse methods from supervised machine learning and statistical learning theory to design and analyze suitable statistics for drift detection.
Changes in data distribution over time may manifest in different forms, as illustrated in Figure 2. The presented types of concept drift are perfectly align to the reformulation signal:
To summarize, the key distinctions of our work compared to previous efforts are: a clear and well-defined approach to detecting changes in user satisfaction using the reformu-lation signal; an in-depth analysis of changes is searchers behavior that results in accurate detection of drifts in user satisfaction. Moreover, our framework works in an unsuper-vised manner, it does not require any labelling.
In this Section we present an overview of a developed framework for detecting changes in user satisfaction due some external events and overtime decay. Our framework uses the growth of the reformulation signal as an indication of user dissatisfaction with the pair  X  Q,SERP  X  . In other words, if the probability to reformulate a query Q to Q grows dramatically then users are no longer satisfied with the pair  X  Q,SERP  X  . The desired results for the query Q has been changed and now users expect to derive SERP 0 as the answer for Q .

The proposed framework monitors user interactions and it triggers an alarm to the system when changes happen. Moreover, if indicted, our framework can produce the fol-lowing additional output per a query:
A detailed diagram of our framework is presented in Fig-ure 3. In following sections we will describe our framework in details: 1. how do we construct user behavioral logs (Section 3.1); 2. how to model the reformulation signal (Section 3.2); 3. how to detect drifts in the reformulation signal in an
In this Section, we describe how to derive user behavioral logs (in Figure 3) from search interaction logs.

We consider the following scenario: we have a stream of queries submitted to a search engine. In response to each query, the engine returns SERP . Users may decide to click on one or more URLs in SERP , reformulate their queries, or end their sessions. These types of user interactions are stored in search interaction logs.

We convert standard search interaction logs to the user behavioral logs where we store information only about re-formulations of issued queries. We use a query expansion definition from [20], [15] to detect terms which users used for reformulations.

An example of user behavioral entries is presented in Fig-ure 4. Each entry consists of four columns:
For example  X 2014 X  is a reformation term for the initial query  X  X ikm conference X  if users are looking for up-to-date information about the conference.

User behavioral logs are suitable to collect a dictionary of the reformulation terms: where K j is j th the reformulation term used to change the query Q , n is the number of the reformulation terms used for expanding the query Q .

In the next Section, the dictionary of the reformulation terms will be utilized for modelling the reformulation signal.
In this Section, we describe how to build a reformulation signal model, that is presented in our framework in Figure 3 as  X  X earn Reformulation Signal X  .

We build the reformulation signal ( RS ) of queries for a time period [ t i ,t i + w 1 ], using the user behavioral logs. RS of the query Q would be: where w 1 is the selected size of the inference window, P ( K is a joint distribution of the query Q and its reformulation term K j during the time period [ t i ,t i + w 1 ].
When time ( t i + w 1 + w 2 ) comes we rebuild the reforma-tion signal of Q for the time period [ t i + w 1 ,t i + w using Equation 3: where w 2 is the size of a test window.

The presented model for the reformulation signal will be used to detect changes in user satisfaction in the next Sec-tion.
In this section we present an algorithm for detecting a drift in user satisfaction using the reformulation signal. Our goal is to detect statistically significant changes. This action is depicted in Figure 3 as  X  X etect Changes X  .

Let us introduce a definition of a drift in the reformu-lation signal between two periods at time [ t i ,t i [ t + w 1 ,t i + w 1 + w 2 ] using Equations 3 and 4: query Q and its reformulation term K j at the time period [ t ,t i + w 1 ] .

It is important to determine what it means when the dis-tribution has changed. If the drift in the reformulation signal is statistical significant then we assume that user satisfaction with the following pair has decreased dramatically : that were generated for Q at the time period [ t i ,t i + w it is still shown during the time period [ t i + w 1 ,t i
However, users are no longer satisfied and they reformu-late Q using some drift term K j . The fact that the drift has happened at time ( t i + w 1 + w 2 ) can be a signal that improve user satisfaction.

Let us consider an example of a drift in the reformulation signal for the query  X  X ikm conference X  in Figure 5. Users were satisfied with SERP that was returned by the query  X  X ikm conference X  at time t i . However, at time t i probability to reformulate the query has been changed dra-matically and the term  X 2014 X  is the most frequent reformu-lation. Most likely users have changed their behavior due to an upcoming conference event and they could not find the right link in SERP that was optimized for clicks from the last year. Figure 5: Example of concept drift in probability to reformulate the query  X  X IKM conference X  using drift term  X 2014 X  .

The proposed algorithm DDSAT for detecting changes in the query reformulation signal to discover changes in user satisfaction is presented in Algorithm 1. We will explain how DDSAT works next.

Let us clarify which an input and an output DDSAT has:
Let us consider the method processDetecedConceptDrift () in Algorithm 1 that deals with the detected drifts. More-over, the function processDetecedConceptDrift () has two additional input parameters which show how the observed drifts influence the current system: Algorithm 1 Algorithm for detection drift in user satisfac-tion using reformulation signal ( DDSAT ).
 Require: inference window w 1 ; Ensure: drift  X  true,false 1: { RS w 1 ( Q k ) } k m =1  X  buildRefSignal( UBL [ t i ,t 3: for Q m ,K j  X  ULB do 4: if |  X  ( P w 1 ( K j ,Q m ))  X   X  ( P w 2 ( K j ,Q m )) | &gt; e then 5: drift  X  true 6: processDetecedConceptDrift ( extra,update ) 7: else 8: drift  X  false 9: end if 10: end for 11: return drift
The presented algorithm DDSAT works in an unsuper-vised way, it does not require any human labelling. It can be shown that DDSAT has a linear complexity.
The proposed framework can be used as a monitoring tool, which alarms when user satisfaction changes for a particular pair:  X  Q,SERP  X  . Our framework also gives an explanation for detected changes, it returns the list of drift terms. More-over, it suggests a possible solution for serving up-to-date SERP and returns the list of the most frequently clicked URLs after reformulating Q using the drift terms.
The ultimate goal of the presented framework is to detect changes in user satisfaction. In order to make the evalua-tion realistic we use search interaction logs of a commercial search engine, that we describe in details next. We run our framework over this dataset. In DDSAT algorithm we set to  X  X rue X  the parameters:  X  X xtra X  and  X  X pdate X  of the method processDetectedConceptDrift() . The extra statistics are used to setup a human assessment. Our evaluation scenario is described in Section 4.2, The final results is presented in Section 5.
Our experimental data comprises of search interaction logs of a commercial search engine that were collected during six months: September 2013 , October 2013 , November 2013 , December 2013 , January 2014 , February 2014 . We only in-clude log entries for US-based traffic. We derive user behav-ioral logs from the selected search interaction logs as pre-sented in Section 3.1. Each month of data consists of over 10 million records.
In this Section we describe how we organize evaluation of our framework results: 1. the derived list of detected changes with drift terms; 2. the derived list of the most clicked URLs per drift.
First, let us present a format of the presented system out-put that needs to be evaluated. Our framework returns re-sults in the form presented in Figure 6. It contains:
For the evaluation, a group of annotators were instructed to exhaustively examine the detected drift terms. The judges were well trained to understand time-related and event-related drifts in user behavioral data. They were given relevant ex-amples of drift, e.g: 1. The latest Olympic games were held in February 2014 Figure 6: Example of an output of the framework.
 Column URL is anonymized. 2. Another example would be a breaking news query such
An example of an evaluation task for the annotators is presented in Figure 7 (A). The judges were asked to decide: Can the term T be a drift term for the query Q ? The judges were allowed to use external information sources to find an-swers.

Every discovered drift, characterised by a drift term, is judged by three different annotators using binary classes:  X 0 X  = wrong and  X 1 X = right . The final score is calculated based on three judgements.

We use accuracy as a final evaluation metric that we refer as Drift Accuracy . We calculate a statistic for URLs which users clicked in SERP 0 that is derived after reformulation Q to Q 0 . If the probability of clicking on URL is greater than 0 . 5, then it is drift URL . If clicks are diverse we cannot produce any URL .

Judges, whom we asked to evaluate drift URL , answered the following question:  X  X s URL relevant for the initial query Q at the time period T ? X  In other words, we asked human annotators to evaluate a tuple  X  Q,URL,T  X  as proposed in the paper [7]
Every discovered drift URL is also judged by three dif-ferent persons using binary classes. The final score is cal-culated based on three judgements. We use accuracy as a final evaluation metric for drift URL , that we refer as URL Accuracy .

We described the large-scale evaluation of our method based on real dataset from a commercial search engine that was collected during six months. As we will show next we can precisely identify drift in user satisfaction using the re-formulation signal.
We now summarize the results for detection changes in user satisfaction using the reformulation signal. The pro-posed solution is working in an unsupervised way and can be applied to large search interaction logs. As a ground truth, we use the human judgements that are described in Section 4.2.
It is important to note that we fix a size of the inference window w 1 to one month of data. For the test window w 2 , we experiment with three different sizes: one week, two weeks, one month. As a final result, we will report for w 2 equals two weeks.

For our experimentation we use data described in Sec-tion 4.1, that can be easily transformed to user behavioral logs. The algorithm DDSAT , proposed in Algorithm 1, is running on derived data in the following way: 1. DDSAT starts at time t i (for our datasets: t i equals 2. DDSAT builds the reformulation signal ( RS ) based 3. DDSAT detects drifts in RS based on the time period 4. DDSAT reassigns t i to ( t i + w 1 ) and goes to 1. We combine all detected drifts and drift URLs and evaluate them using methodology described in Section 4.2.

However, for other domain the size of w 1 and w 2 are de-pendent on many aspects such as a volume of a traffic, type of a served content and so on. Implementers with a do-main X  X  knowledge should decide how often run our frame-work. However, we plan to extend the algorithm DDSAT so that it can determine when there was a drift on the fly.
We evaluate the discovered list of the drifts in user satis-faction to check which confidence level suits best our needs. The randomly selected part (30%) of human judgements for detected drifts (Section 4.2.1) are used to calculate the ac-curacy below. Our findings are the following: The rest of judgements (70%) are used to calculate a final accuracy for our drift detection method in Section 5.3.1.
Hence, we use the confidence value  X  = 0 . 1 for the future evaluation because it gives us the highest accuracy.
In this section we describe the experiments we conducted to evaluate the accuracy of our method DDSAT . We evaluate two types of the accuracy: 1. in Section 5.3.1, we present the accuracy of overall drift 2. in Section 5.3.2 we demonstrate the accuracy of de-
Drift accuracy is a percentage of times when the drift in user satisfaction is correctly detected using the reformula-tion signal. We calculate drift accuracy with respect to the number of users who issue the reformulation. The obtained accuracy is presented in Table 1.

Of course, the best result is characterized by the greatest amount of users. Rows in Table 1 for the number of users in a range [800 , 1000) and [500 , 800) have the lower accuracy than the accuracy rate for the number of users in the range [250 , 500) because they include smaller number of detected drifts in user satisfaction. Table 1: The accuracy of the drift detection depends on the number of users who issued reformulations.
 The metrics are calculated based on the results ob-tained with the confidence value  X  = 0 . 1 .
 Table 2: The accuracy of drift URL depending on the number of users who issued reformulations. The metrics are calculated based on results obtained with confidence value  X  = 0 . 1
Drift URL accuracy is a percentage of relevant URLs among the list of proposed drift URLs . The obtained ac-curacy is presented in Table 2. The quality of derived drift URLs is very high especially for the number of users greater than 250. We see in Table 2 the same situation as in Table 1 for the number of users in a range [800 , 1000) and [500 , 800). They have lower accuracy than the number of users in the range [250 , 500) because they include a smaller number of detected drift URLs .

Our framework includes URL into the list of drift URLs , if the probability of clicking on them after reformulating is higher than 0 . 5. Potentially, detected drift URLs can be ap-plied directly into a learned ranking function as a  X  X reshness feature X  or used for a re-ranking of current results.
To summarize our evaluation of DDSAT , we recommended to determine a confidence value for the drift detection that gives highest accuracy of the detected drift. In our case, we obtained the confidence value  X  = 0 . 1. The propose algorithm was evaluated from two points of view: 1. the accuracy of the detected drift in user satisfaction 2. the accuracy of how relevant are detected drift URLs .
In this Section we show an example of outliers in a concept drift and how to deal with this kind of anomalies.

User behavior the Web is not always reliable, it can be sometimes spurious. It is important for the algorithm DDSAT to know how to remove this anomalies from user behavioral logs data in order to return more accurate results of drift detection.
 For instance,  X  X purious behavior X  can be caused by Search Engine Optimization (SEO) that is a process of affecting the visibility of websites or web pages in search results of search engines. In general, SEO aims to push a site to a higher rank on the search results page, and more frequently a site appears in the search results list, the more visitors it will receive from the search engine X  X  users. In order to achieve the goal, SEO considers how search engines work, what people search for, the actual search terms or keywords typed into search engines and which search engines are pre-ferred by their targeted audience. Optimizing of a website may involve editing its content, HTML and associated cod-ing to both increase its relevance to specific keywords and to remove barriers for indexing activities of search engines.
While we were analysing the list of derived drifts we no-ticed abnormal drifts, e.g. the query  X  X ol mailbox sign in X  was reformulated using the drift term  X  X gnes corky X  . The reformulation was issued by more than 200 users. However, this drift did not make any sense. This behavior most prob-ably was simulated. However, we noticed that only one click happened and that clicked page referred to the website with the domain named  X  X eotest X  . Hence, we concluded that it is the anomaly.

For the final results, this kind of anomalies need to be fil-tered out. They are removed by using the following heuristic rule:  X  X f the number of users who issued reformulation: ( U ) is much greater than the number of user clicks on SERP (search results after reformulating an initial query): ( Click then the detected change is the Anomaly X :
To summarise our experimental results, the proposed tech-nique for detecting changes in user satisfaction using the reformation signal works well on real datasets. The ob-served results over large datasets (all traffic from a com-mercial search engine during 6 months) are both substantial and statistically significant. Furthermore, we have shown that results of our framework, such as lists of URLs , can be potentially useful for ranking.
In this Section we discuss potential applications where the results of the developed framework can be used.
A key component of our system is the algorithm DDSAT that is monitoring user engagement. It alarms when changes in user satisfaction happen with the pair  X  Q,SERP  X  . DDSAT alarm is a signal that user intent for Q drifted and we need to change SERP to satisfy changes in user needs. Potentially, the detected drift can be applied directly into a learned rank-ing function as a  X  X reshness feature X  or used for re-ranking. Moreover, our framework produces the list of URLs , which users prefer after reformulating the initial query. This list also can be incorporated into a ranking model.
Query auto-completion is an important feature of online search engines that enhances search experience by saving users time which otherwise would be spent on typing. A time-sensitive approach has been proposed in [29] for query auto-completion. Our framework also returns drift terms, which are reformulation terms that cause a drift in the re-formulation signal. Hence, this list can be used for time-sensitive query auto-completion.
Automatic detection of problematic queries, where search engines do not return a required result and users are dissat-isfied with their search results, has been extensively stud-ied [1], [9], [12], [23]. However, previous work largely utilise user interaction features, topical and lexical attributes to detect such underperforming queries. Time-sensitive nature of user satisfaction has not been considered.

In this paper, we propose the method to identify drifts in user satisfaction over time. The proposed framework moni-tors a system and it signals an alarm when drift in user sat-isfaction with the pair  X  Q,SERP  X  happens. Hence, when we know a problematic query we can retrain our ranker in order to improve quality of retrieved SERP . We can use the engagement on the reformulated query in order to de-rive training pairs.
In this Section we conclude and present our view of future work.
In this paper we explored the utility of incorporating the query reformulation signal in detecting changes in user sat-isfaction. We leverage concept drift techniques to detect changes in user satisfaction with the pair  X  Q,SERP  X  over time due to some events. The appearance of a drift requires a modification of the SERP to satisfy shifted user needs. We introduced a novel Drift Detection in user SATisfac-tion (DDSAT) algorithm, that accurately detects changes. The proposed algorithm works in an unsupervised manner, it does not need any labelled data. DDSAT is a part of the developed framework for detecting changes in user satisfac-tion.

We conducted a large-scale evaluation using data from a commercial search engine. The dataset was collected dur-ing six months. Our experiments show that the algorithm DDSAT works with a high accuracy. Moreover, our frame-work outputs the list of drift terms and the list of URLs , which can be used for the future re-ranking of SERP .
The algorithm of the drift detection in user satisfaction which we presented in this paper can be incorporated in many search-related applications where freshness is required, e.g. in recency ranking, query auto-completion.
We believe that the current implementation of the algo-rithm DDSAT can be improved. The algorithm uses the fixed sizes of the inference and test windows. However, it is not always suitable. For instance, the size of the test window for the sudden drift can be way shorted compared to incre-mental drifts. We anticipate that the size of the test window should be proportional to the reformulation frequency. We would like to develop a method to identify dynamically the size of the inference and test windows.

We also would like to identify the type of the detected drift in Figure 2. It is important to know in order to de-fine the lifetime. If our algorithm detects the sudden drift (e.g. breaking news queries) then its lifetime is much shorter compared to incremental or sudden drifts. We would like to develop a method to identify automatically the type of the drift.
 We thank Nick Craswell, Andreas Bode, Jonas Barklund, Paul De Bra and Mykola Pechenizkiy for fruitful discussions and their valuable suggestions, and Willi Richert for provid-ing help with infrastructure for experimentation. We also thank Ahmed Hassan for providing the code for reformula-tion detection.

This research has been partly supported by STW and it is the part of the CAPA 1 project.
 [1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it [2] E. Agichtein, E. Brill, and S. T. Dumais. Improving web [3] E. Agichtein, E. Brill, S. T. Dumais, and R. Ragno. [4] A. Al-Maskari, M. Sanderson, and P. Clough. The re-[5] A. Bifet and R. Gavald  X a. Learning from time-changing [6] N. Craswell, O. Zoeter, M. J. Taylor, and B. Ramsey. [7] A. Dong, Y. Chang, Z. Zheng, G. Mishne, J. Bai, [8] A. Dries and U. Ruckert. Adaptive concept drift detec-[9] H. A. Feild, J. Allan, and R. Jones. Predicting searcher www.win.tue.nl/  X  mpechen/projects/capa/ [10] J. Gama, I. Zliobaite, A. Bifet, M. Pechenizkiy, and [11] Q. Guo, R. W. White, Y. Zhang, B. Anderson, and S. T. [12] A. Hassan, R. Jones, and K. L. Klinkner. Beyond [13] A. Hassan, X. Shi, N. Craswell, and B. Ramsey. Beyond [14] S. Hido, T. Id  X e, H. Kashima, H. Kubo, and H. Mat-[15] J. Huang and E. N. Efthimiadis. Analyzing and evalu-[16] S. B. Huffman and M. Hochster. How well does result [17] D. B. in Web Search Samuel Ieong, D. B. in Web Search [18] Y. Inagaki, N. Sadagopan, G. Dupret, A. Dong, C. Liao, [19] Y. Inagaki, N. Sadagopan, G. Dupret, C. L. A. Dong, [20] B. J. Jansen, D. L. Booth, and A. Spink. Patterns [21] T. Joachims. Optimizing search engines using click-[22] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [23] Y. Kim, A. Hassan, R. W. White, and Y.-M. Wang. [24] A. Kulkarni, J. Teevan, K. M. Svore, and S. T. Dumais. [25] K. Radinsky, K. Svore, S. T. Dumais, J. Teevan, [26] T. Saracevic. Relevance: A review a the literature and [27] T. Saracevic. Relevance: A review of the literature and [28] J. C. Schlimmer and R. H. Granger. Beyond incremen-[29] M. Shokouhi and K. Radinsky. Time-sensitive query [30] R. W. White and S. T. Dumais. Characterizing and pre-[31] G. Widmer and M. Kubat. Learning in the presence of [32] Y. Yue, R. Patel, and H. Roehrig. Beyond position bias: [33] I. Zliobaite. Learning under concept drift: an overview.
