 1. Introduction
The general flow shop scheduling problem is a production problem where a set of n jobs has to be processed with identical flow pattern on m machines. When the sequence of job processing on all machines is the same we have the flow shop sequencing production environment known as the permutation flow shop.
Since there is no job passing permitted, the number of possible schedules for n jobs is n !.

There are a few simplifications considered on this environment; one of these assumptions is that there exists an infinite storage capacity between machines. Under this situation, jobs are allowed to wait for machines to be available for as much time as necessary.
In practice, this means that an interruption of the current proces-sing job might take place in between machines. In many scheduling environments, however, processing of some or all jobs needs to be carried out without interruptions between machines. This situation is commonly known as  X  X  X o-wait X  X . A typical example comes from steel production where the steel ingots are not allowed to wait between production stages or otherwise they would cool off. Other examples can be found in chemical and pharmaceutical industries, among many others. These types of problems are reviewed in detail in Hall and Sriskandarajah (1996) ; Framinan and Nagano (2008) and Framinan et al. (2010) .

Another widely accepted assumption is to neglect the setup times by considering them as being part of the processing times.
This assumption may be valid when setups are small, indepen-dent of the processing sequence, and inseparable from the processing times. However, this is not the case for many produc-tion environments where setup times are large and can be separated from processing times.

In this paper we tackle a flow shop scheduling problem with both no-wait and separate sequence independent setup time constraints with the criterion of total completion time minimiza-tion. This criterion is more realistic than the more common makespan minimization ( C max ) as it is known to increase produc-tivity while at the same time reduce the work-in-progress (WIP).
Following the three field notation of Graham et al. (1979) this problem is denoted as Fm = no wait , s ij = P C i for the general m machines case.

Regarding the objective of total completion time, Aldowaisan and Allahverdi (1998) deal with the F 2 = no wait , s ij = and provide sequencing rules for some special cases as well as a dominance rule and a heuristic method for the general case. The same problem is considered in Aldowaisan (2001) ,whereaglobal dominance relation along with a heuristic and a branch and bound method is provided. In a related work ( Allahverdi and Aldowaisan (2000) ) the authors extend the work of Aldowaisan and Allahverdi (1998) to three machines with an improved dominance rule and present five high performing heuristics. Recently, Shyu et al. (2004) have presented an ant colony optimization algorithm (ACO) that works with a TSP representation of the F 2 = no wait , s ij problem. The results show that the algorithm is better than the heuristic of Aldowaisan and Allahverdi (1998) . The ACO algorithm of Shyu et al. (2004) for the two machine case and the five heuristics of Allahverdi and Aldowaisan (2000) can be extended to the m machines case.

In Brown et al. (2004) an algorithm called TRIPS based on variants of the TSP is presented for Fm = no wait , s ij = Fm = no wait , s ij = C max problems.

Ruiz and Allahverdi (2007) showed an effective rule for the four machine case that can also be used for m machines. Five simple and fast heuristics were proposed along with two easy to code stochas-tic local search methods, one of them being based on Iterated Local
Search (ILS). All seven methods were compared to two recent algorithms, which are the ACO of Shyu et al. (2004) and the TRIPS of Brown et al. (2004) (TRIPS). Their results, confirmed through statistical analyses, show that the proposed methods are more effective and efficient when compared to the existing algorithms.
This paper is organized as follows. In Section 2 ,thegeneral concepts of the Clustering Search (CS) algorithm are introduced. In
Section 3 we detail a metaheuristic method that combines Genetic and Cluster Search Algorithms for the No-Wait Flow Shop problem with Setup Times. Section 4 presents the results obtained through computational experiments; furthermore, the method X  X  perfor-mance is evaluated and compared to the best method found in the literature. Finally, conclusions are presented in Section 5 . 2. Clustering search algorithm The Clustering Search algorithm generalizes the Evolutionary
Clustering Search (ECS), proposed by Oliveira and Lorena (2004, 2007) , that employs clustering for detecting promising areas of the search space. It is particularly interesting to find out such areas as soon as possible to change the search strategy over them.
An area can be seen as a search subspace defined by a neighbor-hood relationship in the metaheuristic coding space. In the ECS, a clustering process is executed simultaneously to an evolutionary algorithm, identifying groups of individuals that deserve special attention. In the CS, the evolutionary algorithm is substituted by distinct metaheuristics, such as SA, GRASP, TS and others.
The CS attempts to locate promising search areas by framing them into clusters. A cluster can be defined as a triple G  X  X  c ; r ; b  X  where c , r and b are, respectively, the center and the radius of the area, and a search strategy associated to the cluster.
The center of the cluster c is a solution that represents the cluster, identifying its location inside the search space. Initially, the centers can be obtained randomly, but progressively, they tend to fall along really promising points in the close subspace. The radius r establishes the maximum distance, starting from the center, for which a solution can be associated to the cluster. The search strategy b is a systematic search intensification, in which solutions of a cluster interact among themselves along the clustering process, generating new solutions. The CS consists of four conceptually independent components with different attributions: a Search Metaheuristic (SM); an Iterative Clustering (IC); an Analyzer Module (AM); a Local Search Module (LS).

Fig. 1 shows the four components and the CS conceptual design.
 The SM component works as a full-time solution generator.
The algorithm is executed independent of the remaining compo-nents and must be able to provide a continuous generation of solutions to the clustering process. Clusters are simultaneously maintained to represent these solutions. This entire process works like an infinite loop in which solutions are generated along the iterations.

The IC component aims to gather together similar solutions into groups, identifying a representative cluster center for them. To avoid extra computational effort, IC is designed as an online process in which the clustering is progressively fed by solutions generated through the SM module. A maximum number of clusters, NC , are an upper bound value that prevent unlimited number of clusters. A distance metric must be defined, a priori, allowing a similarity measure for the clustering process.
The AM component provides an analysis of each cluster at regular intervals, indicating a probable promising cluster. A cluster density, d , is a measure that indicates the activity level inside the cluster. For simplicity, d i counts the number of solu-tions generated by SM and allocated to the cluster i . Whenever d reaches a certain high level threshold it indicates that some information template has become predominantly generated by SM, and that information cluster must be better investigated to accelerate the convergence process on it.

Finally, the LS component is a local search module that provides the exploitation of a supposed promising search area framed by the cluster. This process is executed each time AM finds a promising cluster and the LS is applied on the center of the cluster. LS can be considered as the search strategy b associated with the cluster, i.e., a problem-specific local search to be applied into the cluster. 3. Evolutionary clustering search for a no-wait flow shop with set-up times
Inspired by the ECS proposed in Ribeiro Filho et al. (2007) to solve the Permutational Flow Shop problem with the objective of mini-mizing total flow time, we have adapted the metaheuristic that combines Genetic Algorithms (GA) and CS to the no-wait flow shop problem with separated set-up times. The ECS uses a GA to imple-ment the SM component of the CS and generate solutions that allow the exploration of promising regions by the other components of CS.
The Evolutionary Clustering Search for No-Wait Flow Shop with Setup Times (ECS_NSL) presented in this work is resumed in the following pseudo-code representation.

Procedure ECS_NSL ( ) { 1 Initialize population P; 2 Initialize clusters in set C; 3 While (stopping criteria  X  False) { 4 While (i o new_individuals) { 5Base  X  Select among the best 40% of P; 6 Guide  X  Select any individual of P; 7 Offspring  X  Recombination(Base, Guide); 8 SLS (Offspring); 9 If (random(0; 100) o  X  60) 10 LS1 (Offspring); 11 If (insert_into_P (Offspring)  X  True) 12 IC (Offspring); 13 i  X  i  X  1; 14 } 15 For each cluster c E C 16 If (AM (c)) 17 LO (c); 18 } 19 }
The ECS_NSL is now detailed beginning with their evolutionary algorithm (GA).

The chromosome representation used in GA was a vector of n components, one for each task, storing the task position in the solution schedule. The evaluation of the individuals in population was made by the minimization of the total flow time.

The quality of the individuals in the initial population is important for the GA performance ( Nagano et al., 2008 ). To ensure this quality, the initial population (line 1 in Procedure ECS_NSL) was created with a greedy heuristic which returns a fairly good solution in reasonable short times. The heuristic constructs the solution by selecting among the jobs that have not been sequenced yet and the one which is inserted into the end of the partial sequence minimizes the scheduling flow time. The very first individual inserted into the population was generated by this method followed by a Standard Local Search (SLS) procedure. The
SLS algorithm was inspired by the Local Search presented in Ruiz and Allahverdi (2007) and is defined as follows.

SLS ( current_solution ) { minimizes the partial completion time; }
The other n 1 individuals were generated by a simple perturbation of the first individual. This perturbation follows the idea presented in St  X  utzle (1998) which has two swap moves; the first consists in selecting a random task and swapping it with its following one, the other move procedure selects two tasks at random to swap. In order to avoid severe changes it is stated that these two tasks are at most max  X  n = 50 ; 30  X  positions apart and the pair of swaps movements are applied only once for each solution.
After several computing tests, ( n  X  m ) individuals were adopted as the initial population size.

The insertion routine keeps the population sorted in a non-decreasing order considering the total flow time as evaluation.
Furthermore, the routine is also responsible for maintaining only one copy of each individual in the population.

The distance between any individual and a cluster center is the number of swaps moves necessary to transform the given individual into the cluster center. Each position of the individual was compared to its corresponding position in the cluster center.
Whenever a non-coincident element was found, the rest of the individual chromosome was scanned to find its matching pair; afterwardsaswapmovetakesplacetoensurethepartialsequences are coincident. After some swaps, the individual was identical to the cluster center, and the number of swaps was considered as a distance measure.

The initial clusters in ECS_NSL (line 2 in procedure ECS_NSL) are formed scanning the population, from the best to its worst individual, creating new clusters or assimilating the individuals into clusters already created. A new cluster was created when the distance from the individual to the center of any cluster was larger than r  X  0.85 n , and the individual was used to represent the center of the new cluster. Otherwise, the individual was assimilated by the cluster with the closest center. The parameter was chosen after several tests to create a reasonable number of clusters.
The initial clusters construction ends when the whole popula-tion was scanned or when a maximum of 200 clusters were created, in this case the algorithm stops scanning the other solutions and exits straight to the method itself. The cluster radius and the maximum number of clusters are parameters tuned after several tests and considering all problem classes used for testing.
The stop criterion (line 3 in procedure ECS_NSL) used in this work is the elapsed computing time, which was an attempt to balance the computing efforts on generating the initial population and applying the method itself.

After several tests the GA method is found to work best generating 20 new individuals by iteration (line 4 in procedure ECS_NSL); for fewer new individuals as parameter the method tends to converge too early; on the other hand for more indivi-duals on each iteration the method tends to spend too much computational effort, resulting in poor quality solutions. Each of these solutions is possibly inserted into the population. The new individual generation (lines 5 X 7 in procedure ECS_NSL) was made selecting two parents at random, one from the best 40% of the population, called the base parent, and the other from the entire population, called the guide parent. A crossover process known as
Block Order Crossover (BOX) ( Fig. 2 ), presented by Syswerda (1989) , was then applied to both parents, generating a single offspring by copying blocks of genes from the parents. In this work the offspring was generated with 50% of genes coming from each parent. Several other recombination operators are studied and empirically evaluated by Cotta and Troya (1998) . Investiga-tions regarding position-oriented recombination operators are also possible in further studies.

After the crossover, the offspring went through the SLS (line 8 in procedure ECS_NSL). This strategy was applied after every recom-bination because it is a low cost procedure and it often improved the quality of the individual who is about to join the population.
Following the SLS process, the offspring had a probability of 60% to be improved by a local search procedure called LS1 (line 9 in procedure ECS_NSL) that demands more computing efforts.
This parameter was also chosen after several tests, with a smaller probability that the solutions tend to skip this improvement resulting in poor quality; on the other hand larger probability causes spending too much computational effort on the local search and is an exhaustive method.

Procedure LS1 ( current solution ){ }
This procedure uses two types of neighborhood searches: permutation and insertion. The permutation neighborhood around an individual was obtained by swapping every possible pair of positions, producing n ( n 1)/2 different individuals. The inser-tion neighborhood was obtained by removing every gene from its position, and inserting it in other positions in the chromosome, producing n ( n 1) different individuals. The new individual was then inserted into the population in the position relative to its evaluation, shifting forward the subsequent part of the population, and therefore removing the worst individual.
 The successfully inserted individuals were then processed by the IC component of ECS_NSL. The assimilation of an individual by a cluster was based on the Path Relinking procedure presented by Glover (1996) . Starting from the individual chromosome, successive swaps were made until the chromosome became identical to the cluster center. The pair of genes chosen to swap were the ones that more reduced, or less increased, the chromosome total flow time. At each swap the new chromosome configuration was evaluated. At the end of the transformation, the cluster center was moved to (replaced by) the individual, or the intermediary chromosome, that has the best evaluation better than the current center. If no such improvement was possible, the cluster center remains the same.
The IC component (line 12 in procedure ECS_NSL) searches clusters having the closest center and that are within radius r of the individual. When such cluster is found, the individual is assimi-lated; otherwise a new cluster was created having the individual as its center. New clusters were created only if the 200 clusters X  limit is not exceeded. Tests have shown that the number of cluster tends to increase at initial ECS iterations, and slowly decrease as iterations continue and the ECS removes the less active clusters.
After the generation of new individuals, its improvement and insertion into the population, the ECS_NSL executed its AM compo-nent. This cluster analysis procedure performs two tasks: remove clusters that had no assimilations in the last 5 iterations, and for every cluster that had assimilation in the current iteration it runs the following local optimization procedure, called LS2, correspond-ing to the LO component of ECS_NSL (line 17 in procedure ECS_NSL).
Procedure LS2 ( current solution ){ }
Along the ECS_NSL the center of the best cluster found so far 4. Computational results
Although some methods were mentioned in this paper, the new ECS_NSL has been compared only to the best-known existing algorithm that is based on Iterated Local Search proposed by Ruiz and Allahverdi (2007) (ILS_RA).

In the computational tests, the heuristics were coded in C and have been run on a microcomputer Intel Core 2 Quad, 2.4 GHz, 2 Gb RAM.

The computational experience was performed on two instance groups from Ruiz and Allahverdi (2007) . The first one, called  X  X  X mall X  X , comprises all combinations of problem instances where m, we have six different combinations of distributions of proces-sing and setup times. Processing times are uniformly distributed in the range [1, 10] or [1, 100]. Setup times are uniformly distributed so that the maximum setup is 50%, 100% and 150% of the maximum processing time. Zero duration setup times are allowed. This results in setup times are uniformly distributed in [0, 5], [0, 10], and [0, 15] in the case where processing times are distributed in the range [1, 10], and in [0, 50], [0, 100], and [0, 150] in the case processing times are distributed in the range [1, 100]. Considering these distributions and the combinations of n and m , we have a total of 120 different situations. A total of 25 replicates are obtained for each situation for a grand total of 3000  X  X  X mall X  X  instances.

The second set of instances is called  X  X  X arge X  X  and the same distributions of the processing an setup times are maintained. In combinations. Considering 25 replicates so the  X  X  X arge X  X  instances set comprises 2400 instances. Therefore, a total of 5400 problem instances were solved.

In the computational experience, two traditional statistics are used in order to evaluate the heuristic performances: percentage of success (in finding the best solution), and relative deviation (between the heuristic).

The percentage of success PS is given by the number of times the heuristic obtains the best total completion time (alone or in conjunction with other) divided by the number of solved instances.
 The relative deviation RD is given by RD  X  S  X  X  TCT  X  S  X  TCT  X  S n  X  where TCT  X  S  X  is the total completion time of the best sequence obtained by the heuristic S , and TCT  X  S n  X  the best total completion time obtained by the heuristics, for a given test problem.
Table 1 shows the experimental results for small size pro-blems, while Table 2 presents the results related to large size problems.

The results from Table 1 , related to percentages of success and relative deviation for the small instances set, show that the
ILS_RA is outperformed by the ECS_NSL. The general average PS is 59.5% for ECS_NSL, and 51.2% for ILS_RA.

Table 1 also shows that the solution quality, on average, for both ILS_RA and ECS_NSL are practically the same. The general average RD is 0.21% for ILS_RA, and 0.22% for ECS_NSL.
As can be noted from Table 2 , the proposed ECS_NSL clearly outperforms in solution quality.

Taking into account the PS, it is observed that the smallest and largest PS is 74% and 95% for ECS_NSL respectively. However, the largest PS for the ILS_RA is 26%. The general average PS is 87.1% for ECS_NSL, and 12.9% for ILS_RA. The general average RD substantiates the results concerning the percentages of success. The general average RD is 0.57% for ILS_RA, and 0.04% for ECS_NSL.
Tables 3 and 4 show the experimental results of ECS_NSL in relation to the best known solution for small and large instances set with a computational time limit of T  X  10 n m milliseconds.
The Best Known Solutions (BKS) along with all the instances can be downloaded from http://soa.iti.es/rruiz . These solutions have been obtained after long runs of the ILS_RA algorithm as well as during the development and testing of all methods in Ruiz and Allahverdi (2007) .

In the computational experience, two new analyses are used in order to evaluate the proposed ECS_NSL performances: wins or ties (in finding the best solution), and relative deviation for the
ECS_NSL in relation to the best known solution for small and large instances set (RD ECS_NSL ).
 The relative deviation RD ECS_NSL is given by RD where TCT ECS _ NSL  X  S  X  is the total completion time of the best sequence obtained by ECS_NSL, and TCT BKS the completion time of the best known solutions ( http://www.upv.es/gio/rruiz ), for a given test problem.

The results from Table 3 , related to wins or ties for the small instances set, show that the ECS_NSL outperforms in 87 wins, and 683 ties for a total of 3000 small instances. The largest average RD ECS_NSL is 0.73% and the smallest average RD ECS_NSL is 0.18%. The general average RD ECS_NSL is 0.436%.

The results from Table 4 , related to wins or ties for the large instances set, show that the ECS_NSL is outperformed in 1354 wins, and 1 tie for a total of 2400 large instances. The largest average RD ECS_NSL is 0.53% and the smallest average RD ECS_NSL is 0.48%. The general average RD ECS_NSL is 0.04%.

The general wins of the ECS_NSL substantiate the results concerning the general average RD ECS_NSL . In the new computa-tional experience, 1441 new best solutions are obtained for instances set (small and large) for a total of 5400 instances. 5. Final remarks
The main objective of this paper was applying CS to the no-wait flow shop problem with set-up times of original and unedited form.
The experimental results of the simulation from the given tables lead us to the following major conclusions: (i) Computational results lead us to an experimental conclusion (ii) The ECS_NSL method obtained superior performance compared (iii) Finally, after more exhaustive computer processing, new
The research reported in this paper shows that when we are dealing with large sized problems, the CS procedure in which the neighborhood is completely examined is not an efficient usage of computational resources. However LS1 and LS2 procedures lead to significant improvements in the quality of the solution to problems of the large instance set. This can be justified by the possibilities of neighboring sequences that are generated and evaluated by LS1 and LS2 procedures for obtaining the best solution.

The research related in this paper was motivated by the above considerations, which have tried to rescue the essential charac-teristics of metaheuristic methods, and balances between solution quality and computational efficiency, as well as simplicity and implementation easiness.
 Acknowledgments
This research was supported by National Council for Scientific and Technological Development  X  CNPq, Brazil (Grant no. 554546/2009-4, 476753/2011-2, 303000/2010-4, 300692/2009-9 and 470813/2010-5). The authors would like to thank unanimous referees for their helpful recommendations which improved the quality of the paper significantly.
 References
