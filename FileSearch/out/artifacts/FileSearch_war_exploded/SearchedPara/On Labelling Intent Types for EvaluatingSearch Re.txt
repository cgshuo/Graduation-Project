 Web search engine companies such as Google and Microsoft are addressing the problem of search result diversification for ambiguous and underspecified queries [1,10]. For example, by  X  X ffice, X  the user could mean a workplace or a Microsoft product (ambiguity); by  X  X arry potter, X  the user could mean the Harry Potter books, the films, or Harry Potter the main character, and so on (lack of specificity). Given such a query , search result diversification aims to satisfy different user intents with a single, short ranked list of web pages.
Unlike traditional information retrieval where relevance is all that matters, the evaluation of search result diversification, where relevance and diversity need to be balanced, is still an open problem. To ev aluate a diversified search result, test collections are constructed in which each topic (i.e. query) has several possible in-tents, and each intent has its own set of (graded) relevant documents [5,7,8,17]. When utilising such data, we believe that it is important to take into account whether each intent can be regarded as navigational or informational [2]: while a navigational intent requires (say) one particular website, an informational intent requires as much relevant information as possible, and therefore the distinction should be useful for search engines to determine how much space within the search result page should be allocated to each intent. We call this approach (intent-) type-sensitive evaluation. For example, if one of the possible intents for  X  X arry potter X  is  X  X  want to visit pottermore.com , X  a search engine that allocates five URLs for this navigational intent within its top ten results should be considered far from op-timal, since exactly one URL is required to satisfy the intent. The search engine should allocate more space to other intents, especially informational ones.
The TREC Web track diversity test coll ections [5,7,8] appear to be useful for conducting type-sensitive diversity evaluation, as their intents (or  X  X ubtopics X ) already have informational and navigational labels. However, having carefully examined the 628 intents for topics 1-150 from the three TREC rounds, we found that the labels provided there are not always consistent. Table 1 shows some examples of possible incon sistencies within each ro und of TREC. For example, in the TREC 2009 topic file, while 28-4 (Topic 28 Intent 4)  X  X  X  X  looking for InuYasha fan forums and websites X  is labelled as navigational, 38-2  X  X ake me to the homepage of the Humane Society. X  is labelled as informational. Thus, for conducting reliable type-sensitive evaluation, we probably need a set of clear criteria for labelling the intent types appropriately and consistently. In this study, we address the following main Research Questions: RQ1 How can we systematically label informational and navigational intents for RQ2 How do different methods for labelling the intent types affect the evaluation More specifically, we explore both top-down labelling , which labels each intent as either navigational or informational by just looking at the intents, and bottom-up labelling , which labels each intent based on whether a  X  X av X -relevant docu-ment [8] has actually been found in the d ocument collection during relevance assessment 1 . Here, a document is nav-relevant if the TREC assessor judged that the search intent is to obtain this exact document (and probably nothing else). Thus, top-down labelling is designed to reflect the user X  X  expectations, while bottom-up labelling is designed to investigate what the search engines can do given a web corpus. Our results suggest that reliable type-sensitive diversity evaluation can be conducted using the top-down approach with a clear intent labelling guideline, while ensuring that the desired URLs for navigational intents make their way into relevance assessments. There are two main venues that evaluate sea rch result diversification systems: the TREC web track diversity task [5,7,8] and the NTCIR INTENT Document Rank-ing subtask [14,17]. The properties of the data used in this study are summarised in Table 2: TR2009Dg is the data from TREC 2009, with graded relevance as-sessments later added by Sakai and Song [15]; TR2010D is the data from TREC 2010 with binary relevance assessments; TR2011Dg is the data from TREC 2011 with official graded relevance assessments , which contained nav-relevant docu-ments [8]. INTENT1J is the NTCIR-9 INTENT Japanese Document Ranking data (discussed in Appendix A). Note that we denote per-intent relevance levels as L 0 ,...,L 4, where L 0 means  X  X udged nonrelevant. X  More details on the data sets can be found in the aforementioned TREC and NTCIR overview papers.
Most of the existing diversity evaluation metrics (e.g.  X  -nDCG [6], ERR-IA [3] and D -nDCG [15]) are type-agnostic : they do not consider the intent type la-bels. In light of this, Sakai [13] proposed diversity metrics called DIN-nDCG and P+Q , both of which are type-sensitive in that they utilise the intent type labels. DIN-nDCG is a simple generalisation of D-nDCG [15]: the difference is that the former ignores redundant relevant documents for each navigational intent; P+Q is a generalisation of the intent-aware approach to diversity evaluation [1]: it uses a graded-relevance metric called Q for each informational intent and an-other called P + for each navigational intent [13]. Moreover, DIN -nDCG and P+Q can be obtained by simply averaging the raw metrics with intent recall (I-rec), i.e. the proportion of known int ents covered by the system X  X  ranked list. We use these type-sensitive metrics (along with the type-unaware D( )-nDCG) to address the aforemention ed two research questions 2 . Recently, Chen et al. [4] have proposed a family of type-sensitive metrics that gener alises DIN-nDCG, but their metrics require a greedy approximation of the ideal list just like  X  -nDCG [6]. Throughout our experiments, we use the document cutoff of 10 as we are interested in diversifying the first search engine result page.

The original web search query taxonomy of Broder [2] had a third category, namely transactional , where the  X  X ntent is to perform some web-mediated activ-ity. X  Rose and Levinson [11] and Jansen, Booth and Spink [9] have independently refined the taxonomy. However, the focus of the present study is to evaluate search result diversification by considering whether each intent ideally requires one URL slot or more, rather than to provide a comprehensive taxonomy to cover all web search queries. We first explore the top-down labelling approach for type-sensitive diversity eval-uation, i.e., labelling each intent as either navigational or informational by just looking at it and referring to a guideline. Recall that this approach is designed to reflect the user X  X  expectations for each intent.

AstheofficialintenttypelabelsfromTREC(whichwecall Official )arenot altogether reliable (See Table 1), we decided to set up a guideline for labelling each intent, and then re-labelled the intent types from scratch for all 150 TREC 2009-2011 topics, as described below. Since what matters is whether each intent ideally requires exactly one URL slot or more, we adopted a broader definition of a navigational intent compared to Broder X  X  [2]: a navigational intent is a one item search intent for a particular website, entity or object, or for a unique answer that satisfies a specific question . An example of entity would be a particular person name; an example of object would be a particular pdf file. Based on this definition, the first author of this paper devised the following guideline to examine its effect on diversity evaluation: Test 1: Expected Answer Uniqueness. Is the intent specific enough so that Test 2: Expected Answer Cohesiveness. If the desired item is not unique,
Following the above guideline, the first author labelled all (243+217+168 =) 628 intents of the 150 topics: we refer to this set of labels as TD1 (where  X  X D X  stands for  X  X op-down X ). Then, the second author of this paper read the same guideline and independently labelled the intents: we refer to this set of labels as TD2 . Furthermore, to merge the two sets of labels and thereby reduce subjectivity, we took the conservative approach of taking the intersection of their  X  X avigational X  labels: we refer to this as TD1+2 . We preferred this conservative approach because, once an intent has been labelled as navigational, the type-sensitive metrics will completely ignore  X  X edundant X  relevant documents for this intent. Note that we do not claim that our intent type labelling guideline is the best possible there could be: we merely want to examine the effect of using a guideline on the outcome of diversity evaluation.

We compared TD1 , TD2 and TD1+2 with Official . For example, while 97-2  X  X ind maps of South Africa X  was labelled as navigational in Offcial and informational in TD1+2 , 35-6  X  X ind a street-level map of Hoboken, NJ X  was labelled as informational in Offcial and navigational in TD1+2 .Thetwola-bellers judged that the former intent is too vague, and that a single map of South Africa may not satisfy the intent entirely; in contrast, they judged that the latter intent is quite specific and therefore that a single street-level map will serve the purpose. While there will always be grey areas, the guideline helps boost the inter-labeller agreement, as discussed below.
 Table 3 shows Cohen X  X  kappa values for comparing Official , TD1 , TD2 and TD1+2 . For example, Row  X  X LL X  Column (c) shows that while Offcial has 52+127 = 179 navigational labels out of 628 (29%), TD1+2 has 88+127 = 215 (34%), and the kappa between the two label sets is .484. A two-sided z -test sug-gests that these two sets actually agree with each other, even though Official does not rely on our guidelines. Thus, even though Official lacks consistency, the general understanding of what a navigational intent is appears to be sim-ilar to ours. However, the kappa values in Column (c) suggest that while the Official labels were quite consistent with our guidelines in 2009, they gradually deviated from them. According to z -tests for the differences between the kappa values in Column (c), the difference between TREC 2009 and 2010 is significant at  X  =0 . 05; that between 2009 and 2011 is significant at  X  =0 . 01; that between 2010 and 2011 is not significant. More importantly, by comparing across the columns in Table 3, it is clear that the agreement between TD1 and TD2 is higher than the agreements with Official . In Row  X  X LL, X  the difference in kappa between Columns (a) and (d), that between (b) and (d), and that between (c) and (d) are all statistically significant at  X  =0 . 01.

Among the 628 intents we labelled, 44, 17 and 4 intents from each TREC round lacked relevant documents. We therefore use only (199+200+164 =) 563 intents henceforth for evaluating the runs. Table 4 breaks down the lost intents by intent type: for example, according to Official ,asmanyas35ofthe44lost intents are navigational. In fact, all of the 35 intents are named page finding intents such as 34-5  X  X o to Nokia X  X  home page. X  Unfortunatley, these named pages were not captured through pooling [5]. This analysis suggests that, when constructing a type-sensitive diversity test collection using a top-down approach, it is important to ensure that the named pages for the navigational intents are actually included in the relevance data.

Table 5 compares the TREC div ersity run rankings based on Official and that based on TD1+2 for each type-sensitive metric, in terms of Kendall X  X   X  and symmetric  X  ap , which is more sensitive to the changes near the top than  X  [18]. Here, only topics that contain at l east one navigational intent according to either Official or TD1+2 are used for ranking the runs: 30, 40 and 42 topics are used, respectively. It can be observe d that, while the effect of revising the intent type labels is small for DIN -nDCG and P+Q (as they reflect the property of the type-agnostic I-rec), it is not negligible for the raw DIN-nDCG and P+Q. For example, comparing the actual ranked run lists for TR2010D with DIN-nDCG (where it is shown in the table that  X  = . 861) reveals that runs ranked at1and4accordingto Official trade places according to TD1+2 ,amongst other rank changes. The results demonstrate that how the intents are labelled in a top-down manner affects type-sensitive d iversity evaluation considerably, and suggests that a labelling guideline may be useful. Next, we explore the bottom-up labelling approach, i.e., labelling each intent as navigational only if at least one nav-relevant document exists for that intent. Here, we use the TR2011Dg data set only, as the other data sets lack nav-relevant judgments. Recall that this approach is designed to investigate what the search engines can do given a web corpus. We refer to the set of labels derived using the bottom-up approach as BU .

Table 6 compares BU with the aforementioned TD and Official labels in terms of Cohen X  X  kappa. It is clear that BU is completely different from the TD labels. Note that, of the 168 intents (including the four lost ones), only 12 of them were unanimously labelled as navigational in TD1+2 and BU .Also, BU has only 36 navigational labels, while TD1+2 has 64 (and Official has 38). Thus, even though we have tried to identify navigational intents from the user X  X  viewpoint to construct TD1+2 , the test collection lacks the nav-relevant doc-ument for many of them. For example, while 150-3  X  X ome page of the Tennesee highway Patrol X  is navigational according to both Official and TD1+2 ,itis labelled as informational in BU due to lack of a nav-relevant document. Note that TR2011Dg was created via depth-25 pooling [8] (See Table 2): for both TD and BU approaches, a better mechanism for capturing nav-relevant documents is probably necessary. A simple solution would be to include manual runs in the pools that aim for high recall for the navigational intents.

On the other hand, we had ten cases labelled as navigational in BU even though they were labelled as informational in both Official and TD1+2 .An example is 130-1  X  X ind information about the planet Uranus. X  Thus, a TREC assessor found a nav-relevant document for this intent, but it is unlikely that a user looking for general information on Uranus will be satisfied by reading just one document, unless it covers all po ssible relevant pieces of information. This analysis suggests that, for BU-based diversity evaluation, we need a better guideline that clearly defines what a nav-relevant document is.

Table 7 compares the run ranking based on TD1+2 and that based on BU for each type-sensitive metric, in terms of  X  and  X  ap . Only 40 topics that have at least one navigational intent according to either TD1+2 or BU are used for ranking the runs. TD1+2 and BU produce somewhat different rankings: for example, we found that the top two runs according to P+Q with BU are ranked at 2 and 4 by the same metric with TD1+2 (  X  = . 853). Reliable evaluation environments are an absolute necessity for advancing the state of the art. Through experiments with existing diversity test collections, we addressed two research questions o n type-sensitive diversification.
RQ1: How can we systematically label informational and navigational intents for type-sensitive diversity evaluation? RQ2: How do different methods for la-belling the intent types affect the evaluation outcome?
We first explored top-down intent type labelling by devising a guideline. The inter-labeller agreement with our new labels was statistically significantly higher than that between the official TREC labels and our new labels, and our new labels produced type-sensitive evaluation results that are somewhat different from those based on the official labels. These results suggest that our guideline is useful. On the other hand, our analysis with the top-down labels showed that some of the desired URLs for the navigational intents are missing in the existing diversity test collections.

We have also explored the possibility of bottom-up labelling which relies on nav-relevant document labels. This approach looks at evaluation from the sys-tem X  X  side, to show the best search engines can do given a set of (incomplete) rel-evance assessments. However, our analys is showed that the bottom-up labels are completely different from the top-down ones. This is due to two reasons. First, as was mentioned above, the TREC 2011 diversity test collection lacks nav-relevant documents for many intents that are clearly navigational from the user X  X  point of view, due to the incompleteness of rel evance assessments. Second, we found that some of the nav-relevant document label s in the test collection are not appropri-ate. The bottom-up approach may benefit from a clear guideline for document relevance assessments .

We observed some differences in the eva luation outcomes based on top-down and bottom-up labels. A practical recommendation for type-sensitive diversity evaluation would be to take the top-down approach with a clear intent labelling guideline and to ensure that the desired URLs for navigational intents make their way into relevance assessments. While we do not deny the possibility of bottom-up labelling with a clear document labelling guideline and a recall-enhancing mechanism to capture nav-relevant documents, this may still result in intent labels that are counterintuitive from the user X  X  viewpoint, and therefore coun-terintuitive space allocation for different intents in the search result page. If the user feels that one intent is navigational while another is informational, per-haps this view deserves to be respected in the evaluation. For example, given the query  X  X arry potter, X  a diversified search engine probably should not use up many URL slots for the  X  pottermore.com  X  X ntent. To investigate the feasibility of type-sensitive evaluation with the NTCIR INTENT test collections, we also applied the guideline described in Section 3 to the 100 Japanese topics of INTENT1J from NTCIR, which originallydo not have the intent type labels. The first author of this paper labelled the 1,091 intents. We could not hire a second Japanese-speaking assessor for this data set, but our aim was to see if trends that are similar to TREC can be observed rather than to construct a highly reliable data set. Unlike the TREC case, the intents from the INTENT1J data are generally not natural languages sentences. Rather, they are subtopic strings [17,14] such as  X  X arry Potter and the Deathly Hallows author X  (in English translation). Hence we used an additional test for selecting navigational intents: Test 3: Navigational Need Reconstructability. From the intent string, is it For example, the aforementioned subtopic string was reconstructed into  X  X ho wrote Harry Potter and the Deathly Hallows? X  (in English translation) and was labelled as navigational. A total of 130 intents (12%) were labelled as naviga-tional in this way; there are 75 intents that lack known relevant documents, so only 1,016 intents are used in our evaluation experiments. Only eight of the 75 lost intents were navigational according to the first author X  X  labels.
Appendix B compares several diversity ev aluation metrics using the top-down labelled version of INTENT1J as well as the TREC data.
 This appendix compares the diversity metrics discussed in this paper in terms of discriminative power (ability to detect statistically significant differences given a topic set size and a confidence level) [12] and the concordance test (how often the metric in question agrees with simple and intuitive  X  X old-standard X  metrics) [13]. The algorithms for these experiments follow those of Sakai [13]; the gold-standard metrics used for the concordance tests are I-rec, EfP (effective precision, i.e. precision computed by ignoring redundant relevant documents for navigational intents) and the combination of both.
 The discriminative power results with top-down labelling for the TREC and NTCIR data (Figures 1 and 2) genera lise Sakai X  X  finding with TR2009Dg [13]: DIN -nDCG and P+Q are as discriminative as D -nDCG, but the raw P+Q, a generalised intent-aware metric, is substantially less discriminative 3 .
Tables 8-10 summarise the concordance test results for the TREC data (top-down labelling), the NTCIR data (top-down labelling), and the TR2011Dg data (bottom-up labelling). For example, Table 8(c) Column  X  X R2009Dg X  presents the following information: (1) There are 173 pairs of ranked lists for which D-nDCG and DIN-nDCG disagree with each other; DIN-nDCG is consistent with both I-rec and EfP for 73% of these disagreements, while D-nDCG is consistent with both I-rec and EfP for only 61% of them. (These percentages include cases where the ranked list pair is tied according to either the metric in question or the gold standard [13].) This difference is statistically significant according to the sign test (  X  =0 . 05), and in this sense, DIN-nDCG is more  X  X ntuitive X  than D-nDCG. (2) Similary, D-nDCG outperforms P+Q (  X  =0 . 01). (3) Similarly, DIN-nDCG outperforms P+Q (  X  =0 . 01). The overall result is that DIN-nDCG outperforms D-nDCG, which in turn outperforms P+Q.

Based on the results of our extensive discriminative power and concordance test experiments, we recommend the use of DIN( )-nDCG for type-sensitive diversity evaluation. Note that DIN( )-nDCG reduces to D( )-nDCG when intent type labels are not available (i.e. when all intents are treated as informational).
