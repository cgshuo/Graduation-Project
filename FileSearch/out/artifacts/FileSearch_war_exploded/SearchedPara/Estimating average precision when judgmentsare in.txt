 Emine Yilmaz  X  Javed A. Aslam Abstract We consider the problem of evaluating retrieval systems with incomplete rele-vance judgments. Recently, Buckley and Voorhees showed that standard measures of retrieval performance are not robust to incomplete judgments, and they proposed a new measure, bpref , that is much more robust to incomplete judgments. Although bpref is highly correlated with average precision when the judgments are effectively complete, the value of bpref deviates from average precision and from its own value as the judgment set degrades, especially at very low levels of assessment. In this work, we propose three new evaluation measures induced AP , subcollection AP ,and inferred AP that are equivalent to average precision when the relevance judgments are complete and that are statistical estimates of average precision when relevance judgments are a random subset of complete judgments. We consider natural scenarios which yield highly incomplete judgments such as random judgment sets or very shallow depth pools. We compare and contrast the robustness of the three measures pro-posed in this work with bpref for both of these scenarios. Through the use of TREC data, we demonstrate that these measures are more robust to incomplete relevance judgments than bpref, both in terms of how well the measures estimate average precision (as measured with complete relevance judgments) and how well they estimate themselves (as measured with complete relevance judgments). Finally, since inferred AP is the most accurate approximation to average precision and the most robust measure in the presence of incomplete judgments, we provide a detailed analysis of this measure, both in terms of its behavior in theory and its implementation in practice.
 Keywords Evaluation  X  Incomplete judgments  X  Robustness  X  Average precision  X  bpref  X  infAP 1 Introduction Evaluation of retrieval systems is an important area of research in information retrieval and machine learning. Evaluation measures are used not only to compare relative quality of dif-ferent retrieval systems but also to optimize retrieval systems themselves. Therefore, much research have been devoted to evaluation measures. [ 15 , 17  X  19 ].

We consider the evaluation of retrieval systems when the relevance judgments are incom-plete. The most commonly used methodology in evaluating retrieval systems is the test collection methodology , also known as the Cranfield Paradigm [ 21 ]. In the test collection methodology, a document collection is formed and a set of information needs (topics) are determined. Finally, the set of documents that are relevant to these information needs are identified, and the search engines are evaluated based on these relevance judgments.
The test collection methodology is based on the assumption that the relevance judgments are complete , i.e., for each topic, all documents that are relevant to this topic are identified.
In reality, due to the need for extensive human effort, obtaining relevance judgments is an expensive process and in most settings it is not possible to judge the entire document col-lection. Hence, many greedy techniques that identify most of the relevant documents using fewer judgments have been proposed. Cormack et al. [ 11 ] and Aslam et al. [ 3 ] describe techniques for identifying documents that are likely to be relevant, hence, decreasing the judgment effort needed. However, evaluations using such methods tend to produce biased or unprovable estimates of standard retrieval measures, especially when the total number of judgments is limited.

The most popular technique used in creating relevance judgments is depth pooling adopted by TREC. In the case of depth k pooling, only the top k documents retrieved by the systems are judged and the rest of the documents in the collection are assumed to be nonrelevant. In standard settings in TREC, k = 100 is found to be a viable choice and depth-100 pools tend to contain most of the relevant documents. Hence, depth-100 pooling is shown to be a reason-able way to evaluate retrieval systems while avoiding judging the entire collection [ 13 , 24 ]. While depth-100 pool is considerably smaller than the document collection, it still engenders a large assessment effort: in TREC8, for example, 86,830 relevance judgments were used to assess the quality of the retrieved lists corresponding to 129 system runs in response to 50 topics [ 22 ]. Table 1 shows the relationship between pool depth and the number of judgments required per topic on average for various TRECs.

In some situations even by using depth-100 pooling, it is not possible to identify all the relevant documents. One such situation is the case of dynamic document collections. In the case of dynamic document collections, the document collection is constantly changing and in time, new documents can be added to/deleted from the collection. In usual settings it is impractical to judge the documents as they are added to the collection, and the retrieval sys-tems are usually evaluated using relevance judgments that do not contain the newly added documents. Since the new documents are randomly added to the collection, we can not know how the set of available judgments compare with the set of complete judgments. Hence, the set of available judgments are a random subset of complete judgments.

Another situation when the relevance judgments can be incomplete is the case of large document collections. In the case of large document collections, since there are many relevant documents, even at very high ranks the systems may continue to retrieve relevant documents. Hence, it is not possible to identify all the relevant documents using depth pooling without judging documents retrieved at very high depths. Usually, a depth pool of documents are formed and judged, and even if this pool may not contain all the relevant documents, the systems are evaluated using this pool. Hence, the systems are again evaluated using incom-plete relevance judgments. However, this case differs from the case of dynamic document collections in the sense that the relevance judgments are not a random subset of complete judgments (i.e., the relevance judgments are created so that they will identify most of the relevant documents).

Recently, Buckley and Voorhees [ 6 ] showed that standard evaluation measures such as average precision and R-precision are not robust to incomplete relevance judgments. Hence, new evaluation measures that are (1) robust to incomplete relevance judgments and (2) evaluating an important aspect of retrieval effectiveness are needed. Buckley and Voorhees proposed a new measure, bpref , which is highly correlated with average precision when relevance judgments are complete and is also more robust to incomplete judgments than average precision. Once proposed, bpref became a commonly used evaluation measures in many tracks in TREC (e.g., Terabyte track [ 10 ] and the HARD track [ 1 ]).

One drawback of bpref is that as the judgments become more and more incomplete, bpref deviates from average precision, both in terms of value and in terms of the rankings of systems induced by the two measures. Hence, when the judgments are incomplete, bpref is evaluat-ing a different aspect of retrieval effectiveness than average precision. If average precision is considered as the  X  X old standard X  one would like to compute, evaluation measures that are both robust to incomplete judgments and are estimates of average precision are desired.
In a recent work [ 23 ], we proposed three new evaluation measures, induced AP , subcollec-tion AP ,and inferred AP , which are exactly equivalent to average precision when complete judgments are present, and they are estimates of average precision when relevance judgments are a random subset of complete judgments. After these measures were proposed, inferred AP became a standard measure used in the TREC Terabyte [ 7 ] and the TREC VID tracks [ 16 ] in 2006.

In this paper, we give a detailed explanation of the three measures and we analyze the robustness of the measures when the assumption that the relevance judgments are a random subset of complete judgments is highly violated. Since in most settings the data will be gen-erated according to depth pooling (or a method that aims at identifying most of the relevant documents), we mainly focus on the case when the incomplete relevance judgments are gen-erated according to depth pooling. We show that even when the judgments are generated according to depth pooling, the proposed measures are still reasonable estimates of average precision in the sense that they rank systems very similar to average precision. Finally, we focus on inferred AP since it is the most accurate estimate of average precision, and we exten-sively analyze the behavior of inferred AP when the relevance judgments are incomplete.
In the following sections, we first give two different definitions of robustness with respect to incomplete judgments. Then, we describe the cases where measures robust to incomplete judgments are needed and describe how we form the cases on TREC data. We then show the behavior of average precision with respect to incomplete judgments and describe bpref, a commonly used measure when the judgments are incomplete. After describing bpef, we discuss the three proposed measures in detail and use TREC data to show how these mea-sures perform in the case of dynamic and very large collections (or when there are limited number of judgments). In particular, we show that the proposed measures are more robust to incomplete judgments according to both definitions of robustness. Finally, we provide a detailed analysis of inferred AP, focusing on the effect of smoothing in inferred AP and how it is used in TREC Terabyte track [ 7 ] and provide a summary of the three measures. 2 Robustness of a measure to incomplete judgments The robustness of a measure with respect to incomplete relevance judgments can be defined in different ways. When Chris Buckley and Ellen Voorhees [ 6 ] first proposed bpref, they defined the robustness of a measure with respect to incomplete judgments as how well the measure tracks itself when the relevance judgments are incomplete. Hence, they compare bpref com-puted when the relevance judgments are incomplete with bpref using complete judgment set (actual bpref). They show that when compared to other standard measures such as average precision, R-precision, precision-at-cutoff k , when judgments are incomplete bpref is better correlated with actual bpref both in terms of value and in terms of ranking purposes than the other measures are correlated with themselves, concluding that bpref is more robust to incomplete judgments than any of these standard measures.

It is important that an evaluation measure is robust to incomplete relevance judgments; however, more importantly, this measure should be evaluating some interesting aspect of retrieval effectiveness. Hence, an evaluation measure should have both properties. Buckley and Voorhees show that bpref is evaluating an important aspect of retrieval effectiveness by showing that bpref is highly correlated with average precision when relevance judgments are complete.

When we first proposed the three new evaluation measures, we used a slightly different definition of robustness than Buckley and Voorhees did. Assuming that average precision is the gold standard in evaluation of retrieval systems, we defined the robustness of a measure with respect to incomplete judgments as how well it estimates average precision when the judgments are incomplete [ 23 ].

Note that both of these definitions could reasonably be used as the definition of robust-ness. For the measures that we propose, these two definitions of robustness are identical since all three measures are equivalent to average precision itself when the relevance judg-ments are complete. However, the two definitions may differ for bpref since although bpref is highly correlated with average precision when the relevance judgments are complete, the two measures differ from each other.

In this paper, for comparison purposes, we use both of these definitions and we show that even when a measure is compared with itself, the proposed three measures are more robust to incomplete judgments than bpref. 3 Use cases and experimental setup We consider two scenarios which give rise to the problem of evaluating retrieval systems with incomplete relevance judgments: (1) dynamic document collections and (2) very large collections and/or limited judgment budgets. In the sections that follow, we discuss these use cases in detail and describe a set of experiments designed to test the robustness of retrieval measures in each of these scenarios. 3.1 Dynamic document collections In the case of dynamic document collections such as the World Wide Web, documents are added to and deleted from the document collection over time; however, relevance judgments are typically collected at one instance in time, and it is impractical to continuously update the collected relevance judgments to reflect the current state of the document collection. Thus, a future retrieval run evaluated against the fixed relevance assessments gives rise to two possible inconsistencies: (1) The future collection may contain new documents not present (or judged) in the original collection; this inconsistency in the fixed relevance assessments is referred to as incomplete judgments . (2) The fixed relevance assessments may contain judged documents which no longer exist in the future collection; this inconsistency is referred to as imperfect judgments . Recent work has shown that most measures of retrieval performance are robust in the presence of imperfect judgments, but this is not the case in the presence of incomplete judgments [ 6 , 23 ]; thus, in what follows, we concentrate on the case of incomplete judgments.
 Experimental setup To simulate the effect of an incomplete judgment set for a dynamic collection, one can take a fixed collection and corresponding effectively complete judgment set and subsample the judgment set in order to obtain incomplete judgments for the fixed collection. For the TREC collections we consider, we employ a sampling strategy effec-tively identical 1 to the one proposed by Buckley and Voorhees [ 6 ]. For each TREC, we form incomplete judgment sets by randomly sampling from the entire depth-100 pool. This is accomplished by selecting p % of the complete judgment set uniformly at random for each topic, for various values of p ranging from 1 to 100%. Note that especially for very small sampling percentages, the random sample may not contain any relevant documents. In this case, we remove the entire random sample and pick another p % random sample until a random sample with at least one relevant document is obtained. 3.2 Very large collections and limited judgment budgets Incomplete judgments also arise in the case of very large collections and/or limited judgment budgets. For example, in the early TREC collections a depth-100 pool was determined to be sufficiently complete for the purposes of evaluation [ 24 ], and while depth-100 pooling dic-tated large numbers of required judgments (86,830 documents were judged in TREC8), this total assessment effort was within the capabilities of the governmental sponsors of TREC. However, such assessment efforts are outside the capability of most non-governmental orga-nizations, and for the more recent very large collections such as those used in the TREC Terabyte tracks [ 7 , 10 ], depth-100 pooling is no longer effectively complete. In these cases, even if the collection is fixed and one can reasonably determine the scope of an effectively complete pool, one is effectively forced to use an incomplete judgment set due to the size of the effectively complete pool and/or one X  X  limited judgment budget. This scenario differs from that of dynamic document collections in a fundamental way: In the case of dynamic document collections, the incomplete (and imperfect) judgments are effectively dictated by the random document additions and deletions over time; however, in the case of very large collections and/or limited judgment budgets with a fixed collection (and effectively complete pool), one can choose the incomplete judgment set. Natural choices include (1) random sampling from the effectively complete pool (thus matching the dynamic document collection scenario), (2) shallow depth pools, and (3) mixed shallow pools and random sampling. 2 In this work, we consider the incomplete judgment sets which arise from shallow depth pools, and for one of our proposed measures ( infAP ), we also consider the case of mixed shallow pools and random sampling.
 Experimental setup To simulate the effect of incomplete judgments sets which arise as a result of very large collections and/or limited judgment budgets, we fix an effectively com-plete pool and consider shallower pools and/or random sampling. For the TREC collections we consider, we assume that the given depth-100 pool is effectively complete, and we form incomplete judgments by generating shallower depth-k pools, for various values of k ranging from 1 to 100. For the case of mixed pools, we form incomplete judgments by generating a depth-k pool and then adding documents randomly chosen from those documents within the depth-100 pool but outside the given depth-k pool. 4 Evaluation with incomplete judgments In the following sections, we first show the behavior of average precision with respect to incomplete judgments. After that, we describe bpref since it is a commonly used measure when the relevance judgments are incomplete and is known to be more robust to incom-plete judgments than any of the standard measures. Then, we describe the three proposed measures in detail and experimentally show that if the document collection is dynamic (rele-vance judgments random subset of complete judgments) or very large (relevance judgments non-random subset of complete judgments) or if the evaluation is to be done with a limited budget, then the proposed measures are more robust to incomplete judgments than bpref. 4.1 Average precision with incomplete judgments Buckley and Voorhees [ 6 ] show that commonly used evaluation measures such as aver-age precision, R-precision and precision-at-cutoff 10 are not robust to incomplete relevance judgments. In this section, we focus on average precision and show the behavior of average precision with respect to incomplete judgments.

The average precision of the output of a retrieval system is the average of the precisions at each relevant document; the precisions at unretrieved relevant documents are assumed to be zero, by convention. It is known that average precision is an approximation to the area under the precision-recall curve.

Buckley and Voorhees [ 6 ] show that when the relevance judgments are a random subset of complete judgments, as the number of judgments is reduced, the average precision value decreases and the rankings of the systems based on average precision also change. This can be explained by the fact that all unjudged documents are assumed to be nonrelevant by average precision in a typical evaluation setting such as TREC. Therefore, as the number of judgments is reduced, the number of relevant documents retrieved before a relevant document, hence the precision at a relevant document, is also reduced. Therefore, average precision, the average of the precisions at relevant documents, is reduced.

This behavior can be seen in Fig. 1 when the relevance judgments are a random subset of complete judgments as in the case of dynamic document collections. The three plots show the value of mean average precision obtained using 30, 10, and 5% of the entire judgment set versus the value of mean average precision using the entire judgment set (actual average precision). Each plot in the figure reports the root mean squared (RMS) error, the Kendall X  X   X  correlation, and the linear correlation coefficient  X  . The plots also include the line y = x for comparison purposes.

Figure 2 shows how the value of average precision changes when the judgment set is not a random subset of complete judgments but is generated using the depth pooling method. The figure shows the value of mean average precision obtained when the same number of judgments as the ones in Fig. 1 are used to generate the judgment set using the depth pooling method. The three plots show the value of mean average precision when the relevance judgments are generated using depth-30 (  X  30% of complete judgments), depth-8 (  X  10% of complete judgments), depth-4 (  X  5% of complete judgments) versus the value of mean aver-age precision using the entire judgment set (depth-100 pool in TREC 8). It can be seen that when the relevance judgments are generated according to depth pooling, average precision is a better estimate of actual average precision. However, the value of average precision and the ranking of systems based on average precision still deviates from actual average precision as the judgments become more incomplete. One important problem associated with average precision computed on shallow depths is that the performance of the best systems are consis-tently underestimated, e.g., the best systems using depth-4 or depth-8 pooling are different than the best systems according to actual average precision.

We have shown that average precision is highly affected by the incompleteness in the relevance judgments, hence is not robust to incomplete judgments. In the following sec-tion, we first describe bpref, a measure recently proposed by Buckley and Voorhees; and we propose three new evaluation measures that are more robust to incomplete judgments than both average precision and bpref. 4.2 bpref Since standard measures of retrieval performance are not robust to incomplete judgments, Buckley and Voorhees [ 6 ] proposed another measure named bpref which is more robust to incomplete relevance judgments than any of the standard measures.

Given a query with R relevant documents, the bpref value of a ranked list is calculated as where r is a relevant document and n is a judged nonrelevant document within the first R judged nonrelevant documents in the output of the retrieval system.

Note that bpref does not make any assumptions about the unjudged documents since it is only dependent on the judged documents. One drawback of this measure is that when the number of relevant documents is small, the evaluation is based on very few documents. Hence, when the number of relevant documents is small, a variation of the measure named bpref-10 is preferred. The bpref-10 measure is calculated as where n is a judged nonrelevant document within the top 10 + R judged nonrelevant docu-ments in the output of the system.
 When the relevance judgments are a random subset of complete judgments, Buckley and Voorhees [ 6 ] show that the rankings of systems obtained using bpref-10 are highly correlated with the rankings of systems produced by bpref-10 using complete judgments.

This behavior can be seen in Fig. 3 when the relevance judgments are a random subset of complete judgments as in the case of dynamic document collections. The plots show the value of mean bpref using 30, 10, and 5% of complete judgments versus the value of mean actual bpref. For comparison purposes, the 30, 10, and 5% random judgments used in this plot are the same as the ones used in Fig. 1 . It can be seen that in all cases bpref-10 better cor-related with actual bpref-10 than the correlation between average precision with incomplete judgments and actual average precision, showing that bpref is more robust to incomplete judgments than average precision. However, bpref starts deviating from actual bpref when the judgments become more and more incomplete.

Figure 4 shows how the value of bpref changes when the judgment set is not a random subset of complete judgments but is generated using the depth pooling method for depths 4, 8 and 30. It can be seen that when the size of the depth pool gets smaller, the value of bpref deviates from the value of actual bpref. When we compare this figure with Fig. 2 in terms of ranking purposes, we can see that bpref compared with actual bpref has similar behavior with average precision compared with actual average precision.

Based on these plots, one can conclude that bpref is more robust to incomplete judgments than average precision, mainly when relevance judgments are a random subset of complete judgments. However, one potential drawback of bpref (or bpref-10) is that the theoretical interpretation of the measure is unclear , as opposed to average precision (an approximation to the area under the precision-recall curve) and R-precision (the break-even point in the precision-recall curve), etc. Buckley and Voorhees [ 6 ] show that when the entire judgment set is used, the value of bpref is closely related to the value of average precision. However, as the relevance judgment sets become more and more incomplete, the value of bpref deviates from the value of average precision computed using the entire judgment set. Hence, when the relevance judgments are incomplete, bpref is evaluating a different aspect of retrieval effectiveness than average precision.

This behavior can be seen in Fig. 5 when the relevance judgments are a random subset of complete judgments as in the case of dynamic document collections and in Fig. 6 when the judgment set is generated using the depth pooling method with a shallow (small) depth. It can be seen in both cases that bpref starts deviating from actual average precision when the judgments become more and more incomplete.

Ideally, one might well prefer a measure that is both robust to incomplete judgments and that has longstanding usage, a theoretical basis, and/or exhibits a high correlation to standard measures of retrieval effectiveness. In the following sections, we describe three measures based on average precision that (1) are exactly equivalent to average precision when com-plete judgments are available, (2) are approximations to average precision itself and (3) are more robust to incomplete relevance judgments [ 23 ] than bpref. We show that when the relevance judgments are a random subset of complete judgments, the proposed measure are highly accurate estimates of average precision. We further show that when the relevance judg-ments are generated according to depth pooling (not a random subset of complete judgments), although the value of the proposed measures deviate from the value of average precision, they still rank systems very similar to actual average precision. 4.3 Induced AP (indAP) We have seen that as the judgments become more incomplete, since all the unjudged docu-ments are assumed to be nonrelevant, average precision deviates from actual average precision both in terms of value and ranking purposes. However, one can obtain a different version of average precision, which we call induced AP ( indAP ), that does not make any assumption about the unjudged documents. For a query with R relevant documents, induced AP can be calculated in exactly the same way as average precision with a slight difference: in induced AP, the documents that are unjudged are removed from the list and are not considered in evaluation.
 Once the unjudged documents are removed from the retrieval system X  X  output, induced AP can be calculated in exactly the same way as traditional average precision. Induced AP has the nice property that it is an approximation to the area under the precision-recall curve of the output of a retrieval system when only the judged documents are considered.
Given a query with R judged relevant documents, induced AP can be calculated as where r is a relevant document and rank ( r ) is the rank of a document when only judged documents are considered. Note that the above formula can be written as a preference based measure: where n is a nonrelevant document retrieved above a relevant document when only judged nonrelevant documents are considered.

Note the similarity between induced AP and bpref Eq. 1 . In bpref, at each relevant doc-ument, the number of nonrelevants above a relevant document is scaled by a factor of 1 / R (or with 1 /( R + 10 ) in the case of bpref-10), where R is the total number of judged relevant documents, whereas in the case of induced AP, it is scaled by a factor of 1 / rank ( r ) ,where rank ( r ) is the rank of the relevant document when only the judged documents are considered. Also, while calculating the number of judged nonrelevant documents above a relevant doc-ument, bpref only considers the top R (or R + 10) judged nonrelevant documents, whereas induced AP considers all such documents. Due to this similarity, bpref can be viewed as an approximation to induced AP, which is an approximation to average precision.

We note that induced AP is an available (though seldom, if ever, used) evaluation measure in TREC X  X  trec_eval [ 5 ] program. However, the robustness of induced AP with respect to incomplete relevance judgments has never been analyzed. In the following experiments, we describe the behavior of induced AP when the relevance judgments are incomplete. 4.3.1 Application of induced AP to TREC data In the case of TREC, the documents can be divided into two parts: documents in the depth-100 pool (set of documents that are judged) and the documents that are not in the depth-100 pool (set of documents that are unjudged but assumed to be nonrelevant). These two sets form the set of complete judgments and actual average precision is computed using these two sets.

When induced AP [ 23 ] was proposed, since the documents that are not in the depth-100 pool are also unjudged, these documents were removed from the output of the search engine. Because of this, even if all documents in the depth-100 pool were judged, the value of induced AP would be different than the value of actual average precision as defined by TREC (average precision computed by judging the the entire depth-100 pool and assuming the rest of the documents are nonrelevant).

In this paper, we define a variation of induced AP that makes a distinction between doc-uments in the depth-100 pool and the documents not in the depth-100 pool. Given this distinction, induced AP also assumes that documents that are not in the depth-100 pool are nonrelevant. Hence, this version of induced AP is exactly equivalent to actual average precision as defined by TREC when the entire depth-100 pool is judged. 4.3.2 Induced AP for dynamic document collections Figure 7 shows how this new version of induced map (induced AP averaged over all topics) computed using 30, 10, and 5% of the judgments compares with actual map calculated using the complete relevance judgment set. It can be seen from this figure that induced AP is closely approximating actual AP even when the judgments are 30% of complete judgments. How-ever, as the set of available judgments become smaller and smaller (10 and 5% of complete judgments), induced AP tends to overestimate the value of average precision. When com-pared to bpref using Fig. 5 , it can be seen that induced AP is always a better approximation to actual AP as seen by the RMS errors in the plots. Also, as seen by the Kendall X  X   X  value, induced AP ranks systems closer to the ranking obtained from actual AP than bpref does.
When the estimates of induced AP as defined in this paper (assume documents not in depth-100 are nonrelevant) are compared with the estimates of induced AP (no assumptions about the documents that are not in the depth-100 pool) [ 23 ], it can be seen that this version of induced AP is a much better approximation to average precision. Hence, in general research settings, it is reasonable to assume that documents retrieved very low are nonrelevant and use the version of induced AP as is defined in this paper.

In Figure 8 , for different percentages of random sampling, we demonstrate the behavior of induced AP in terms of all three statistics. In these experiments, we produced ten different runs (samples) for each sampling percentage and for each retrieval system, we calculated the induced map averaged over all queries. We also calculated the bpref-10 measure in the same way and using the same sample for comparison purposes. Then, we calculated all three statistics for each run and reported the average of these three statistics for each percentage.
It can be seen from the plot on the left that the ranking obtained by induced AP is very close to the ranking of systems using actual AP and the Kendall X  X   X  value of induced AP is almost always better than that of bpref. The second plot shows that induced AP is highly correlated with actual AP in terms of linear correlation coefficient. The rightmost plot shows via the RMS error that the value of induced map is close to the value of actual mean aver-age precision, even when very few relevance judgments are used. Note that even when the robustness of a measure with respect to incomplete judgments is defined as how well it tracks itself (bpref vs. actual bpref compared to induced AP vs. actual AP), it can be seen in the plots that induced AP is more robust to incomplete judgments than bpref. 4.3.3 Induced AP with very large collections and limited judgment budgets We have shown that when relevance judgments are a random subset of complete judg-ments, induced AP is more robust to incomplete judgments than bpref. However, usually the relevance judgments are not just a random subset of complete judgments and they are generated according to a particular method. The most commonly used such method is depth pooling.

Figure 9 shows how the value of induced map changes when judgments are generated according to (left to right) depth-30 (  X  30% of complete judgments), depth-8 (  X  10% of complete judgments), and depth-4 (  X  5% of complete judgments) pooling. Note that when the judgments are generated according to depth-30 pooling, induced AP is a very good approximation to average precision. When the judgment set becomes smaller, the estimates of induced AP are biased upwards but still the ranking of systems using induced AP is highly correlated with ranking of systems using actual average precision. Even with as few as 5% of complete judgments (depth-4 pooling), the Kendall X  X   X  correlation of induced AP with actual AP is 0.8992 (two rankings are generally considered as equivalent when the Kendall X  X   X  correlation is more than or equal to 0.9 [ 20 ]). Note that using the equivalent number of judgments, the correlation of induced AP using depth pooling with actual AP is higher than the correlation of induced AP computed with random judgments with actual AP in terms of ranking purposes. It can also be seen via comparison with Fig. 2 and 6 that the ranking of systems obtained by induced AP is better than ranking of systems using average precision of bpref with depth pooling. It can be seen that even using depth-4 pooling, induced AP is much better at identifying the best systems as opposed to average precision using depth-4 or depth-8 pooling.
Figure 10 shows how the correlations between induced AP and average precision change when the size of the depth pool becomes smaller. The figure also shows the correlations of bpref compared to actual bpref actual average precision. It can be seen through the three statistics that induced AP is more robust to incompleteness than bpref according to the two different definitions of robustness. Hence, one can conclude that even when the relevance judgments are not a random subset of complete judgments and are generated according to depth pooling, induced AP is more robust to incomplete judgments than bpref. 4.4 Subcollection AP (subAP) Hawking and Robertson [ 14 ] show that average precision is highly stable with respect to random sampling from the entire document collection. Hence, if the available document col-lection was a subset of the complete document collection and average precision was computed on this available document collection, the value of average precision would be an accurate estimate of the value of average precision using the complete document collection.
When the incomplete relevance judgments are a random subset of complete judgments, one can use this fact to derive a measure that is also robust with respect to incomplete rel-evance judgments. In most evaluation settings, the document collection can be divided into two parts: the set of complete judgments and the set of judged nonrelevants .Thesetof complete judgments is usually the set of documents that have may or may not be relevant. These are the documents that are retrieved high in the output of a search engine. The set of judged nonrelevants are the documents that are retrieved low in the rankings by all search engines, hence they can safely be assumed to be nonrelevant without being judged. In the case of TREC, the set of complete judgments is the depth-100 pool and the set of judged nonrelevants corresponds to documents that are not in the depth-100 pool.

If one knows the proportion p of the incomplete judgment set to the complete judgment set, then one could also sample from the set of judged nonrelevants with probability p ,in effect forming a new document collection that is p % random subset of the entire document collection. Since average precision is stable with respect to random sampling from the entire document collection, if average precision were computed on this new set, we should obtain an unbiased estimate of the average precision using full judgment set. Based on this fact, we derive a new evaluation measure, subcollection AP (subAP), which is also an approximation to average precision. 4.4.1 Application of subcollection AP to TREC data Note that in computing induced AP, we considered the judged documents together with all documents that are not in the depth-100 pool. Subcollection AP in TREC setup can be described as follows: While calculating the average precision estimate for a p % judgment set, instead of keeping all documents that are not in the depth-100 pool together with all documents that were judged from the depth-100 pool, one could keep the documents that are not in the depth-100 pool with probability p , randomly and independently for each such document. Note that this has the effect of sampling from the depth-100 pool while also sam-pling from the non-depth-100 pool, which is equivalent to forming a p % subcollection from the entire document collection. Once this p % subcollection is formed, we compute average precision using only the documents that are in this subcollection (by removing the rest of the documents from the output of the search engines), obtaining subcollection average precision of the system.

Note that the computation of the above description of subcollection AP requires that one randomly samples documents from non-depth-100 pool. Instead of computing a nonde-terministic sample of non-depth-100 pool, one can deterministically compute the value of subcollection AP as follows: let the rank of the document at which we compute the precision be k . Let the total number of judged relevant documents up to (and including) this rank be r , judged nonrelevant documents up to (and including) this rank be n , and the total number of documents up to (and including) this rank that are not in the depth-100 pool be nd. Then, the expected value of the precision at rank k can be computed as:
Note that nd i p i ( 1  X  p ) ( nd  X  i ) corresponds to the probability of sampling i documents from the non-depth-100 pool and r /( r + n + i ) corresponds to the precision at rank k if i documents were sampled from the non-depth-100 pool, hence the above formula corresponds to the expected precision at rank k . Since average precision is the average of the precisions at relevant documents, once we compute the precision at a relevant document, then we can simply average these precisions to compute average precision.

Note that subcollection AP has the appealing property that when the complete relevance judgment set is available, it is exactly equivalent to average precision. We further note that to employ subcollection AP in practice, one needs knowledge of p . In the case of a dynamic (growing) collections, this is the relative size of the original vs. current collection and in the case of evaluation with limited judgments or evaluation in very large collections, this is the proportion of the number of available judgments to the size of the complete judgment set. 4.4.2 Subcollection AP for dynamic document collections Using the same setup for induced AP and the same randomly generated judgment sets, Figs. 11 and 12 show how correlations of subcollection AP with respect to actual AP change as the judgment sets become more incomplete. Figure 11 illustrates that subcollection AP is compa-rable with induced AP in terms of RMS error (induced AP is a slightly better approximation to actual AP in terms of value). However, when compared to induced AP, subcollection AP is a better approximation to actual AP in terms of Kendall X  X   X  and linear correlation coefficient.
Figure 12 shows the change in behavior of subcollection AP as the relevance judgments become more incomplete. The figure shows in terms of all three statistics that subcollection AP is more robust to incomplete judgments than bpref based on the two different defini-tions of robustness (subcollection map vs. actual map compared with bpref vs. actual map or subcollection map vs. actual map compared with bpref vs. actual bpref). 4.4.3 Subcollection AP with very large collections and limited judgment budget Note that the computation of subcollection AP is highly dependent on the assumption that the relevance judgments are a random subset of complete judgments. Figures 13 and 14 show the behavior of the measure when the relevance judgments are not a random subset of complete judgments but are generated according to depth pooling.

Figure 14 shows that when the randomness assumption is violated, subcollection AP is over estimating actual AP. However, for ranking purposes, subcollection AP is still a highly applicable measure; even by using depth-4 pooling (  X  5% of complete judgments), the mea-sure has a Kendall X  X   X  value of approximately 0 . 9. When compared with bpref and average precision computed using depth pooling (Figs. 2 , 6 , it can be seen that subcollection AP ranks systems more similar to actual average precision. Hence, in terms of ranking purposes, subcollection average precision is more robust to incomplete judgments generated with depth pooling than both of these measures.

Figure 14 shows how the correlations between subcollection AP and average precision change as the size of the depth pool gets smaller. It can be seen that for ranking purposes, the measure is highly robust to incomplete judgments generated using depth pooling. It can also be seen that using equivalent number of judgments, subcollection AP using depth pooling has a higher rank correlation with actual AP than subcollection AP using random judgments (Fig. 12 ). The figure also shows that subcollection AP is more robust to incomplete judgments generated using depth pooling than bpref according to the two definitions of robustness. 4.5 Inferred AP (infAP) In order to derive our final robust measure of retrieval effectiveness, we consider the follow-ing random experiment whose expectation is average precision. Given a ranked list returned with respect to a given topic: 1. Select a relevant document at random from the collection, and let the rank of this relevant 2. Select a rank i at random from among the set { 1 ,..., k } . 3. Output the binary relevance of the document at rank i . In expectation, steps (2) and (3) effectively compute the precision at a relevant document, and in combination step (1) effectively computes the average of these precisions. One can view average precision as the expectation of this random experiment, and in order to esti-mate average precision, one can instead estimate this expectation using the given sampled relevance judgments.

The computation of this expectation is highly dependent on the distribution according to which the incomplete relevance judgments are generated. For example, in order to compute the first step of this random experiment, one needs to know the induced distribution among the relevant documents. If the relevance judgments are a random subset of the complete set of judgments (as in dynamic document collections), the induced distribution over relevant documents is also uniform, as desired. However, if the relevance judgments are generated according to a distribution other than uniform (e.g., using depth pooling), the induced dis-tribution among the relevant documents is not random (e.g., the relevant documents towards the top of the list are more probable to be picked than the relevant documents retrieved lower in the list) and this distribution is unknown. The estimation of the induced distribu-tion among the set of relevant documents is a nontrivial subject and is a subject of future research.

In this paper, we assume that the relevance judgments are a random subset of the set of complete judgments and we compute the expectation of the given random experiment, i.e., inferred AP, based on this assumption. We further demonstrate the applicability of this mea-sure when this randomness assumption is highly violated (when the incomplete relevance judgments generated via depth pooling).

Given the assumption that relevance judgments are a uniform random subset of the set of complete judgments, consider the first part of this random experiment, picking a relevant document at random from the collection. Assuming that the incomplete relevance judgments are a random subset of the set of complete judgments, the induced distribution over relevant documents is also uniform, as desired.

Now consider the expected precision at a relevant document retrieved at rank k (steps 2 and 3 of the random experiment). When computing the precision at rank k by picking a document at random at or above k , two cases can happen. With probability 1 / k ,wemay pick the current document, and since this document is known to be relevant, the outcome is 1, by definition. Or we may pick a document above the current document with probability ( k  X  1 )/ k , and we calculate the expected precision (or probability of relevance) within these documents. Thus, for a relevant document at rank k , the expected value of precision at rank k can be calculated as:
Now consider the computation of expected precision above rank k . While computing this expectation, we pick a document at random from among these k  X  1 documents and report the binary relevance of this document. Since we assume that the incomplete relevance judg-ments are a random subset of the set of complete judgments, each document above rank k has equal probability to be picked and we can use the judged documents to obtain an unbiased estimate of precision above rank k .Let r be the total number of judged relevant documents and n be the total number of judged nonrelevant documents above rank k . Then, the expected precision above rank k can be computed as r /( r + n ) . Hence, the expected precision at rank k can be computed as:
Note that it is possible to have no documents sampled above rank k ( r + n = 0).Toavoid this 0 / 0 condition, we employ Lindstone smoothing [ 9 ] where a small value is added to both the number of relevant and number of nonrelevant documents sampled. Then, the above formula becomes:
To be compatible with the version of inferred AP in trec_eval package [ 5 ], we set to be 0 . 00001.

Since average precision is the average of the precisions at each relevant document, we compute the expected precision at each relevant document rank using the above formula and calculate the average of them, where the relevant documents that are not retrieved by the system are assumed to have a precision of zero. We call this new measure that estimates the expected average precision inferred AP (infAP). 4.5.1 Application of inferred AP to TREC data In the case of TREC, the expected precision at rank k can be computed as given in Eq. 2 . However, we can obtain a better estimate of the precision above rank k by using the documents that are not in the depth-100 pool as follows:
Within the k  X  1 documents above rank k , there are two main types of documents: docu-ments that are not in the depth-100 pool, which are assumed to be nonrelevant, and documents that are within the depth-100 pool. Let the number of documents that are not in the depth-100 pool be nd and the number of documents in the depth-100 pool be d . For the documents that are within the depth-100 pool, there are documents that are unsampled (unjudged), documents that are sampled (judged) and relevant, and documents that are sampled and non-relevant. Let the number of sampled and relevant documents be r and the number of sampled and nonrelevant documents be n .

While computing the expected precision within these k  X  1 documents, we pick a docu-ment at random from these k  X  1 documents and report the relevance of this document. With probability nd /( k  X  1 ) , we pick a document that is not in the depth-100 pool and the expected precision within these documents is 0. With probability d /( k  X  1 ) , we pick a document that is in the depth-100 pool. Within the documents in the depth-100 pool, we estimate the precision using the sample given. Thus, the expected precision within the documents in the depth-100 pool is r /( r + n ) . Therefore, the expected precision above rank k can be calculated as: Thus, if we combine these two formulae, the expected precision at a relevant document that is retrieved at rank k can be computed as:
As mentioned above, since it is possible to have no documents sampled above rank k ,to avoid the 0 / 0 condition when we employ Lindstone smoothing, the above formula becomes:
Finally, we can compute the average of these expected precisions to compute the expected value of average precision, i.e., inferred AP. Note that the above formula has the advantage that it is a direct estimate of average precision. 4.5.2 Inferred AP for dynamic document collections When the relevance judgments are a random subset of complete judgments, inferred AP as computed using the above formula should give an unbiased estimate of actual average precision. Figure 15 shows how the inferred map compares with the actual map when the relevance judgments are a random subset of complete judgments. It can be seen that with as few as 5% of the complete relevance judgments, inferred map is a reasonable approximation to actual map. When 30% of the relevance judgments are available, inferred map is a highly accurate approximation to actual map as seen by the RMS error in the plot. Also, based on the RMS error, one can see that inferred map is a better approximation to actual map than all previous measures.

Similarly, Fig. 16 shows that the ranking of the systems using inferred map is very close to the ranking obtained using actual map (left plot), that inferred map is highly correlated with actual map (middle plot) and that inferred map is a very close approximation to actual map (rightmost plot). Note that even at very small percentages when the judgment sets are very incomplete inferred map is still a very close approximation to actual map. 4.5.3 Inferred AP with very large collections and limited judgment budgets Note that the computation of inferred AP is highly dependent on the assumption that the relevance judgments are a random subset of complete judgments. If this assumption is true, then inferred AP is an unbiased estimator for actual average precision. When this assump-tion is violated, the value of inferred AP may also deviate from the value of actual average precision. However, in the field of information retrieval, it is very important that one can rank systems correctly relative to each other even if the value of a measure is not an unbiased estimate of average precision.

In this section, we show how the value of inferred AP is affected by the violations in the randomness assumption by using data generated using the method of depth pooling.
Using the same setup we did before, Fig. 17 shows how the value of inferred map changes when judgments are generated according to (left to right) depth-30 (  X  30% of complete judgments), depth-8 (  X  10% of complete judgments), and depth-4 (  X  5% of complete judg-ments) pooling. It can be seen that when the relevance judgments are generated according to depth-30 pooling, inferred map is a highly accurate estimate of actual map both in terms of value and ranking purposes. When the judgment set becomes smaller, the estimates of inferred map tend to bias upwards but the ranking of systems using inferred map is still very close to the rankings using actual map. Even when relevance judgments are as few as 5% of complete judgments (judgments generated using depth-4 pooling), the rankings of systems using inferred map is almost identical to the ranking of systems using actual map (Kendall X  X   X  value of 0.9002), much better than average precision or bpref with shallow depth pools.

One important result that can be seen by comparing Figs. 15 and 17 is that using equiv-alent number of judgments, if the relevance judgments are a random subset of complete judgments, inferred AP is a better approximation to average precision than inferred AP computed using judgments generated according to depth pooling. However, the rankings of systems via inferred AP computed using judgments from depth pooling is closer to rankings of systems using judgments that are random subset of complete judgments.

Figure 18 shows how the correlations between inferred AP and average precision change when the size of the depth pool becomes smaller. The figure also shows the correlations of bpref with itself and bpref with average precision. It can be seen that for all depths the correlation of inferred AP with actual average precision is better than both bpref vs actual bpref and bpref vs actual average precision based on all three statistics. 4.5.4 Effect of smoothing in inferred AP We have shown that inferred AP is computing the expectation of the random experiment defined by average precision. Theoretically, when the relevance judgments are a random subset of complete judgments, inferred AP should produce an unbiased estimator of average precision. However, we saw that inferred AP consistently underestimates average precision (Fig. 15 ). In this section, we show that smoothing has a considerable effect in inferred AP and that the main reason of this underestimation is due to smoothing.

Note that the random experiment that defined inferred AP has two main parts: (a) Pick-ing a relevant document at random (step 1 of the random experiment) and (b) Computing the expected precision at this relevant document (steps 2 and 3 of the random experiment). Hence, the bias (underestimation) associated with inferred AP is either because we are not picking a relevant document at random from the ranked list or because we are not correctly computing the expected precisions at this relevant document.

In order to test the effect of the former, we compute inferred AP using the actual precisions at the sampled relevant documents (i.e., for each sampled relevant document in the output of a search engine, we compute the actual precision at this relevant document using complete judgments and compute the average of these precisions over all sampled relevant documents). The top left plot in Fig. 19 shows the result of this experiment. The plot shows that inferred AP computed with actual precisions is an unbiased estimate of average precision, as expected. Hence, the bias associated with inferred AP can not be due to picking a relevant document at random; hence it should be due to the estimation of precisions at relevant documents.
Now consider the estimation of precision at a relevant document. We hypothesize that the problem associated with estimation of the precisions is due to the effect of smoothing. We can rewrite the general formula used by inferred AP for estimation of precisions at relevant documents as follows: where k is the rank of the relevant document, r and n are the total number of judged rel-evant/judged nonrelevant documents above this relevant document respectively. When we defined inferred AP, we used c = 2, corresponding to Lindstone smoothing. Consider the effect of this smoothing parameter. The smoothing parameter c has the most effect when computing the precision at the top relevant document. If there are no sampled documents above the top relevant document, the formula above assigns a value of 1 / c to the precision above this document. This top document has the highest weight in computing the value of inferred AP, hence inferred AP is highly affected by the way smoothing is done.
The top right, bottom left and bottom right plots in Fig. 19 show the effect of smoothing parameter in inferred AP when relevance judgments are 5% of complete judgments. The figure shows that when c = 1, inferred AP is overestimating average precision (upper right plot) and when c = 2 . 5, similar to c = 2, inferred AP is underestimating average precision (lower right plot). Note that this is an expected result; when c = 1, when there are no sampled documents above a relevant document, the precision above the relevant document is replaced by 1; whereas when c = 2 . 5, this precision is replaced by 0 . 4. When c = 1 . 5, inferred AP estimates are very close to average precision. The estimates have no (or little) bias (bottom left plot) and they are very close to the estimates obtained by computing inferred AP with actual precisions (upper left plot). As expected, inferred AP with c = 1 . 5 has more vari-ance than inferred AP with actual precisions due to the variance imposed by estimation of precisions at relevant documents.

Note that when inferred AP underestimates average precision ( c = 2or c = 2 . 5orso on), the underestimation is more severe for better systems (systems with higher average precision). This is expected since for better systems, the precision above the first relevant document is usually close to 1 but when c = 2, we are replacing this value with 1/2, a much smaller value. Similarly, when c is 1, the value of average precision is overestimated mainly for bad systems (systems with lower average precisions).

For TREC 7, 8 and 10, Figs. 20 and 21 shows the change in Kendall X  X   X  and RMS error for various values of c as the judgments become more and more incomplete. Smoothing has the most effect when the judgments are more incomplete (as also seen in the plots) since there are more cases where there are no sampled documents above the first relevant docu-ment. Therefore, each plot in the figures contains an inset plot corresponding to the region of interest (when available relevance judgments are less than 20% of complete judgments). Note that as the value of c increases, there is a consistent increase in the Kendall X  X   X  value of the estimates (Fig. 20 ). When the value of c is 1 . 5, inferred AP is the closest approximation to average precision for TREC 7 and 8; whereas for TREC 10 c = 2 . 5 produces the best result (Fig. 21 ). 3
As a conclusion, Fig. 19 shows that inferred AP underestimates average precision due to the smoothing parameter c used. Figures 20 and 21 show that one could change the value of this parameter or use more advanced smoothing techniques to obtain better estimates of average precision. 4.5.5 Inferred AP in TREC terabyte Out of the three proposed measures, inferred AP has become the most commonly used one. Inferred AP was used in TREC VID and Terabyte tracks in 2006 [ 7 , 16 ]. In this section, we mainly focus on the usage of inferred AP in TREC Terabyte 2006. We first describe the general setup in TREC Terabyte 2006 and discuss some issues with how inferred AP was used in Terabyte.

In TREC Terabyte 2006, three different sets of relevance judgments were formed. Only two of these sets were used for evaluation purposes. The first set of judgments correspond to a traditional depth-50 pool. The main reason why these judgments were created was to obtain an idea of the average precision of the systems. The second set is a random sample of documents such that there are more documents judged from topics that are more likely to have more relevant documents. In Terabyte track, the size of the document collection is very large. Hence, the systems may continue to retrieve relevant documents even at high ranks (deeper in the list). This set of judgments is created to obtain an estimate of average precision if complete judgments were present. To compute these average precision estimates, inferred average precision is used as the evaluation measure.
 Note that in TREC Terabyte 2006, some information is lost when computing inferred AP. Even though the entire depth-50 pool was judged, inferred AP was computed only using the random sample of judgments (second set) without completely utilizing the judgments from the depth-50 pool. The main reason for this is due to the fact that the definition of inferred AP assumes that the relevance judgments are a random subset complete judgments.
In this paper, we have shown that even though this randomness assumption is highly vio-lated (when the relevance judgments are generated according to depth pooling), inferred AP is still very effective in estimating the ranking of systems according to actual AP. When equiv-alent number judgments are used, inferred AP computed on a depth pool ranks systems better than inferred AP computed on a random set of judgments. Hence, if the goal is to compare the relative performance of retrieval systems (to obtain a ranking of systems), inferred AP could still reasonably be used with depth pooling. The violation of the randomness assumption only affects the estimated value of average precision.
In the case of Terabyte track, if we combine the two sets of judgments to utilize all the available information, we have a scenario in between the two ends we have discussed in the paper. Some of the relevance judgments are generated according to depth pooling (depth-50), and the remaining judgments are a random subset from deeper ranks. Instead of computing inferred AP using only the random judgments as done in TREC Terabyte 2006, one could then compute inferred AP on this combined set.

In order to test the robustness of inferred AP when some judgments are generated using depth pooling and the rest of judgments are a random subset of complete judgments, we can-not use the TREC Terabyte data since we do not know the value of actual average precision using complete judgments. Instead, we use data from TREC 8 to test whether inferred AP could reasonably be used in this setup.

Figure 22 shows the result of this experiment. In order to form the pools that are a com-bination of depth pooling and random judgments, we first form different depth-k pools where k  X  X  1 , 2 , 3 , 4 , 5 , 10 , 20 , 30 , 40 , 50 } . For each depth k, we compute the total num-ber of documents that are in the depth-k pool and we randomly sample equivalent number of documents from the documents that are in the complete judgment set but are not in the depth-k pool (documents in depth-100 pool but not in depth-k pool). We then combine the judgments of these documents and compute inferred AP using these combined judg-ments. For comparison purposes, the plots also contain inferred AP computed using depth pooling and inferred AP computed using random judgments. The x axis in the plots is the percentage of the judgments used when compared to complete judgments. It can be seen that the ranking of systems obtained using the combined judgments is better corre-lated with average precision than ranking of systems obtained using random judgments. When equivalent judgments are used, inferred AP computed on judgments generated using depth pooling ranks systems better than inferred AP using the two other judgment sets. However, as expected, the value of inferred AP is closest to the value of actual AP when random judgments are present. This figure shows that in order to rank systems better, inferred AP on the combined set of judgments could be used (should be preferred) in TREC Terabyte.

The experiments provided in this paper raise an important question: Inferred AP on depth pooling produces better rankings and inferred AP on random judgments produces better estimates in terms of value. Can we form a variant of inferred AP that uses depth pooled judgments together with random judgments but is very good at both ranking systems and esti-mating the value of average precision? Such a measure would be very applicable to evaluation settings such as the Terabyte track and is a subject of future research. 5 Discussion and summary We proposed three different evaluation measures induced AP, subcollection AP and inferred AP for evaluation using incomplete relevance judgments. The proposed measures have the appealing property that they are exactly equivalent to average precision when complete judg-ments are present. They are estimates of average precision when the relevance judgments are a random subset of complete judgments and are robust to incomplete relevance judg-ments. When the incomplete relevance judgments are non-random (e.g., generated accord-ing to depth pooling), the value of the proposed measures deviate from the value of average precision but the ranking of systems is still similar to the ranking of systems according to aver-age precision. In this section, we compare the proposed measures with respect to incomplete relevance judgments and summarize.

Figure 23 shows the comparison of the proposed three evaluation measures and bpref in terms of Kendall X  X   X  (first row), linear correlation coefficient (second row) and RMS error (third row) when the relevance judgments are a random subset of complete judgments. The plots show that based on both definitions of robustness, the three proposed measures are more robust to incomplete relevance judgments when compared to bpref. The same behavior can be seen in Fig. 24 when the incomplete relevance judgments are generated using depth pooling. It can also be seen that even though the measures do not accurately estimate average precision as the judgments become more and more incomplete (RMS error), they continue to rank systems similar to average precision (Kendall X  X   X  ).

Among the three measures proposed, induced AP is the simplest one. It uses the same underlying data and information as bpref together with the information about documents that are not in the depth-100 pool, and it is more robust to incomplete relevance judgments. It can be seen in the plots in the first and second row of Figs. 23 and 24 that induced AP has a better Kendall X  X   X  and linear correlation with actual AP than bpref has. Induced AP is also a better approximation to actual AP than bpref. This can be seen in the plots in the last row of the same figure. For all percentages, induced AP has a lower RMS error than bpref. Even when robustness with respect to incomplete judgments is defined as how a measure tracks itself, it can be seen through these plots that induced AP is more robust to incomplete judgments than bpref.
 Subcollection AP is slightly more complicated than induced AP. Compared to induced AP and bpref, together with the documents that are not in the depth-100 pool, subcollection AP requires the additional knowledge of the proportion p of the incomplete judgments to complete judgments (depth-100 pool in case of TREC). Overall, subcollection AP has a similar behavior with induced AP with respect to incomplete judgments. When relevance judgments are a random subset of complete judgment, the rankings of systems according to subcollection AP is more correlated with average precision than induced AP (Fig. 23 , first row). However, the value of induced AP is a better approximation to actual AP than subcollection AP (Fig. 23 , third row).

Out of the three measures proposed, inferred AP is the most complex, yet it is the closest approximation to actual average precision. In order to compute inferred AP, one needs the additional knowledge of the documents in the depth-100 pool. However, it does not require knowledge of the percentage p of the depth-100 pool that is judged as subcollection AP does. In terms of Kendall X  X   X  and linear correlation coefficient  X  , inferred AP has the best (sometimes similar to induced AP) performance when compared to the other two measures (plots in the first and second rows of Figs. 23 , 24 ). Inferred AP consistently has much less RMS error compared to all the rest of the measures when relevance judgments are a random subset of complete judgments. Even when the number of judgments is very small (even as small as 1%), inferred AP has an RMS error less than or equal to 0.05, which means that inferred AP is a very close approximation to actual average precision (Fig. 23 , third row). As in the case of subcollection AP, inferred AP is exactly equivalent to actual AP when complete relevance judgments are present, except for a slight difference due to the effect of smoothing.

Considering these differences among the proposed three measures, one might prefer to use induced AP if the aim is to have a simple measure that is both an approximation to actual AP and is also robust with respect to incomplete relevance judgments. If one is looking for a simple but better approximation to actual AP in terms of ranking purposes, and one knows (or can estimate) the proportion of the size of incomplete judgment set as compared to the complete judgment set, then subcollection AP might be preferred. If the aim is a measure that is both robust and is a very close approximation to average precision, then inferred AP might be preferred. 6 Conclusions We consider evaluation of retrieval systems with incomplete relevance judgments. When doc-ument collections are large or dynamic, it is more difficult to evaluate the retrieval systems since obtaining complete relevance judgments becomes more and more difficult. Therefore, evaluation measures that are robust to incomplete relevance judgments are needed. Buck-ley and Voorhees [ 6 ] show that most commonly used evaluation measures such as average precision, R-precision and precision-at-cutoff k are not robust to incomplete relevance judg-ments, and they propose another measure, bpref , which is more robust to incomplete rele-vance judgments. After bpref was proposed, it became a commonly used evaluation measure in TREC, such as in the Terabyte and Hard tracks.
In this paper, propose three different evaluation measures, namely induced AP , subcol-lection AP ,and inferred AP that are exactly equivalent to average precision when com-plete judgments are present. We describe natural scenarios which yield highly incomplete relevance judgment sets, mainly focusing on the cases where relevance judgments are a random subset of complete judgments or when relevance judgments are generated accord-ing to depth pooling. We show that for both of these cases, all of these measures are more robust to incomplete relevance judgments than bpref in terms of both predicting precision.

Finally, we focus on inferred AP, the most robust measure to incomplete judgments, and provide a detailed explanation of the measure. We further discuss the setup inferred AP is used in TREC Terabyte 2006, and describe a different setup where inferred AP could better be used by utilizing all the available relevance judgments.
 References Authors Biography
