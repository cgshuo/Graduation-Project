
Computer Science Department, University of Oviedo, Spain Energy Department, University of Oviedo, Spain 1. Introduction
This research arises from the necessity of forecasting the thermal performance of a power plant is de fi ned by the interchange heat between the steam from the turbine and the refrigerated water of the condenser. However, among the operators of the power plant it is empirically known that this indicator exchanger tubes [16]. This fouling depends on the design of the condenser, on the operating conditions and on the physicochemical characteristics of the circulating water. During operation, fouling can be resumed from the cleanliness factor [1,16]. Hence, both the heat transfer rate ( Q ) and the cleanliness factor ( FC ) are considered to be good indicators of the thermal performance of the condenser.
The condenser operates by condensing the steam of two low-pressure turbine sections. Besides the steam, there are other important drains: drainage-in from the low-pressure closed-feedwater heater No.1, steam ejector, auxiliary condensers, the gland steam condenser, reserve feed water tank, drip and drain tanks and the fl ash box; as well as drainage-out of total condensate and non-condensing gases. Circulating seawater introduced by gravity from a load chamber to two water-boxes is used for the condenser cooling. Each water-box is divided obtaining two tube passes. The level of the load chamber temperature variations.

Up to 48 measured physicalvariables make in fl uence over the heat transfer rate and over the cleanliness factor, according to a deep study carried out by physicists and engineers [1,16]. Hence, these are the variables registered by sensors in the power plant. The physical measured properties in the power plant were mass fl ow rates, pressures and temperatures taken from circulating water system, the condenser, the steam of the turbine and other auxiliary de vices. The performance of the condenser could be estimated from all these variables using different physical polynomial relationships based on mass and heat balances in the condenser based on the mass and energy conservation principles. However, the drawback of this approach is that it requires making several assumptions and to manually obtain or obtain an estimation of the heat transfer and cleanliness factor from the measured variables without using any of the estimated intermediate variables and to select at the same time those that are really useful. This will allow to decide which sensors are necessary or not and to plan cleanliness campaigns (with the corresponding economic cost reduction for the power plant). Under the above-mentioned principles, engineers expect a polynomial relationship between the physical measured variables and heat transfer rate and cleanliness factor. These physical variables are commonly called attributes among Machine Learning (ML) researchers, while the dependent ones -in this case the heat transfer rate and the cleanliness factor-are called categories.

The task of fi nding such hypothetical polynomial presents several challenges. First it is necessary to since the number of possible monomials is excessively high, which leads to a NP-hard and NP-complete problem. In this framework, this paper proposes a method that produces a polynomial whose degree is automatically obtained. Although, it is well known that n data would be perfectly fi tted with a polynomial of degree at most n  X  1 and hence the degree would be automatically obtained; therefore, estimating the category for unseen data. Furthermore, only a limited number of promising monomials admissible. In addition, this method not only performs a dimensionality reduction in the space of all compose each monomial. This allows to decide which variables have to be collected and hence which sensors are necessary to invest in order to be installed in the power plant.

The paper is structured as follows. Firstly, some related research is presented. Then, the proposed are shown next. Finally, the conclusions of the paper and future research lines are presented. 2. Related work
The task of determining a function of a set of independent variables (attributes) that best fi tasetof observations of a dependent variable (category) [3] is known as regression analysis. Least squares [2] is a statistical technique commonly adopted in order to solve regression problems. This method determines a function that closely approximates the data. That function is supposed to adopt a particular form containing some parameters that need to be estimated. If the function is a polynomial, then it is linear be determined, since in this case it is necessary to solve a general unconstrained optimization problem (for instance using methods as Gauss-Newton or Levenberg-Marquardt [14] algorithms). An implicit requirement for Least squares method to work properly is that the noise must be randomly distributed with mean zero.
 Support Vector Regression (SVR) [20] is a method for regression based on Support Vector Machines. Using different kernels it is possible to adjust several kinds of models. For example, using a polynomial splines and solves linear operator equations [20].

There are other systems coming from the ML environment that can deal with regression, but they do not generate polynomials. Methods of this kind can be based on the nearest neighborhood [5], on neural networks [13] or on decision trees [21].

Regardless of the method adopted to perform regression, it is well known [10] that not all features are useful or relevant to predict the category, but they could introduce noise or be redundant as they do not provide information to predict the category. Hence, dimensionality reduction becomes commonly helpful. Among the different ways of performing this task, Feature Selection (FS) is a technique that dimensionality reduction technique where the resultant features are transformations or combinations of second [11].
 could be extended to polynomialsofa certain degree d greaterthan 1,since itcan be applied to any problem The drawback of this extension is that it implies considering all possible monomials of a polynomial, which makes the problem computationally unaccepta ble. Other techni ques for dimensi onality reduction also suffer from this drawback. This happens, for instance, for those techniques that detect attribute dependencies to remove redundancy [7] and hence for performing dimensionality reduction. Even other faster fi lters of this kind are not suitable in this case. For instance, the algorithm proposed in [23] proceeds in two steps. First it performs a relevance analysis ordering features according to symmetrical uncertainty. Secondly, it carries out redundancy analysis to detect attribute dependencies computing again the symmetrical uncertainty between pairs of features.

Other alternatives are, for instance, the ones proposed in [8]. This paper deals with feature selection to apply these methods here because we do not know the number of features in advance, as the degree of the polynomial is unknown. Even though a degree is fi xed and hence, the number of monomials is known, such number is even excessively high.

Also, if the quality measure to optimize is submodular (a function F : A  X  V  X  /V = { 1 , 2 ,..., | V |} is submodular if  X  A, B  X  V : F ( A )+ F ( B ) F ( A  X  B )+ F ( A  X  B ) ) and ac-cording to Das and Kempe that in [4] demonstrate that the variance reduction in Gaussian models is submodular under certain conditions on the covaria nce, then Queyranne X  X  algorithm [18] can minimize In addition to the drawbacks mentioned above for t he other methods, this one requires the function to minimize to be submodular (we will demonstrate later in this paper that this property does not hold for our target function, that is, the cross validation error).

A simple and possible solution to reduce the order of the number of monomials is, for instance, considering only univariate monomials. Nevertheless, the drawback of choosing the degree of the polynomial beforehand is not tackled and it is a too simple approach regarding the complexity of a condenser power plant operation.

The solution proposed in this paper is more general, since it overcomes the disadvantages mentioned above. It is based on a stepwise regression and the concepts of relevance and redundancy. 3. Greedy polynomial stepwise regression
The method proposed in this paper is called Greedy Polynomial Stepwise Regression (GPSR). It is based on the stepwise regression technique mentioned above.

Let be X the set of variables { x 1 ,...,x monomials built from such variables, that is Y = { x i 1 1 ...x i n natural numbers.

The aim of GPSR is twofold. First, it performs FS over the set of features Y and secondly it carries out full linear regression over the space of selected features. Not performing a previous FS allows to and NP-complete if the regression is tackled over Y a factorial number of monomials of maximum degree d with regard to the number of variables of X . Indeed, such number is the sum of all k d combinations with replacement of n elements taken in groups of k elements, that is A possibility of performing FS would consist on min imizing a submodular function using Queyranne X  X  algorithm [18] under the cond ition demonstrated in [4 ]. But taking into account that the number of monomials available is factorial according to the number of variables and that the number of evaluations would lead to compute a cubed factorial number of evaluations regarding the number of evaluations. Besides, the function to minimize would typically be the error estimated by cross validation, which in
Assuming that the function to optimize ( F ) is the error estimated by a 3 -folds cross validation, the following values are obtained.

In this table the fi rst column concerns taking just attribute A and the second one means that only attribute B is used. The third and fourth ones are when both ( A  X  B in third column) and none ( A  X  B in fourth column) attributes are taken. The values of the fi fth and sixth column show that when the category y 1 is predicted, then F is not submodular. The conditio n of submodularity reaches equality when predicting category y 2 . Finally, the condition holds when predicting category y 3 . This last case also shows that the functions neither  X  F nor 1 /F are not submodular.

As an alternative, GPRS uses a stepwise regression that includes a greedy search in order to avoid the election of a maximum degree d of the polynomial beforehand. This leads to explore only promising monomials of the in fi nite set Y . This stage is detailed below. 3.1. Stepwise regression
The stepwise regression [3] method basically consists of solving a sequence of problems P each one involving the following three steps: 1. Choosing the relevant feature f 2. Checking wether feature f 3. Performing a linear regression between f
The process begins being r 0 = z and ends when | r
In this paper, relevance [23], mentioned in step 1., stands for the relationship degree between a feature the capab ility of a certain feature to imp rove performance according to a certain quality measure. This improvement is considered with regard to a set of features not containing that feature. Redundancy is another concept related to relevance and usefulness that is involved in this context. is removed from the category (residual) to obtain the category of the next problem.

Hence, GPSR iteratively detects monomials relevant to the category (relevance), but with lower dependency with the previous selected monomials (redundancy).

A schema of the algorithm can be found in Fig. 1. Let us now comment the details of the algorithm in depth. 3.1.1. Relevant feature search
As it has been already commented, a search is performed to fi nd the feature (monomial) with highest election is motivated by the fact that there is a linear relation between monomials and categories. Since it is not computational acceptable to explore all possible monomials, a greedy forward search [12] is performed, which explores a polynomial number of features in relation to the number of variables or attributes, particularly, it explores number of features (monomials) where n is the number of variables.
In the greedy forward search the m onomial is initially 1. Then, the m onomial is sequentially multiplied by a variable. In each step, the variable that makes the monomial get the highest relevance is selected. The algorithm allows a variable to be included more than once in the monomial. This enables to obtain the degree d all degrees of all variables that form it. The algorithm ends when adding a variable to the candidate monomial does not make such monomial be more relevant. Notice that this algorithm avoids indicating the degree of the monomial beforehand and imposing constraints over its maximum degree.
It is possible that in complex problems or problems with great amount of noise, adding a variable which variable could be erroneously added to the monomial. This situation could lead to obtain an over fi tted model. In order to avoid this drawback, a variable is taken if such increase exceeds a certain percentage  X  , which establishes a balance between over fi tting and performance. 3.1.2. Usefulness of a feature
In the previous stage, a relevant monomial is selected. Now, the algorithm will check if it is useful for the fi nal regression or not. The quality measure adopted in this paper for this purpose is the common regression.
 4. Experiments
This section describes the data sets and the experiment settings. It also discusses the results obtained. 4.1. Data sets description
Different data sets have been taken to perform the experiments. On the one hand, we used the real data experiments with arti fi cial polynomials and with arti fi cial continuous not polynomial functions. This allows to study the behavior of the systems under controlled situations. Although GPSR is designed to perform polynomial regression, we also carry out experiments over regression data sets of the UCI repository in order to test the performance of GPSR in more general problems. 4.1.1. Arti fi cial polynomials
Different data sets have been obtained varying the following parameters:
Each variable takes values according to a uniform distribution in the interval [ i, i + r ] where i takes
The values of each coef fi cient of the polynomial also come from a uniform distribution in the set reduces the number of monomials with high weight in the polynomial.

In order to obtain the degree and the variables that appear in a monomial, a set of k d relevant variables are selected with replacement, where d is the maximum degree of the polynomial. These monomials speci fi ed in v) are obtained.

The noise in each variable is generated adding to each value another value generated with the same to add.

The noise in the category is simulated adding the percentage of noise multiplied by a value generated by a uniform distribution in the interval [ min, max ] where min and max are the minimum and maximum values that the category reaches.
 Each time a training set is obtained, a test set is generated with 1000 examples without noise.
In order to reduce the variance of the error estimation, each experiment is repeated several times. It is known that a sample of 30 executions is suf fi cient to estimate the population average from the sample as the average of all the 30 errors.

When a parameter varies, the rest adopt default values showed in Table 1. 4.1.2. Arti fi cial RBF
Since the Weierstrass approximation theorem states that the space of polynomial is dense in the continuous function space, we introduce continuous not polynomial functions data sets to compare the systems. They are similar to the ones described above, but Radial Basis Functions (RBF) are used instead of polynomials.
 data sets. The corresponding value of the category is obtained according to a uniform distribution in the interval [0 , 1] . Then, a RBF is obtained using SVR with RBF kernel, which produces a continuous function. The parameter  X  was fi xed to 1 . After that, we completed the number of examples generating the variable values in the same way and the category using the RBF previously obtained. Finally, irrelevant attributes are added. Keeping the original set of examples in the complete data set, we expect that SVR with the complete data set learns the same RBF if the irrelevant attributes and the examples added were removed.

The parameters we vary are the fi nal number of examples, the number of variables, the number of relevant variables and the number of examples to generate the RBF model by SVR. When a parameter varies the rest adopt default values shown in Table 2. 4.1.3. UCI repository This is a repository of databases, domain theories and data generators that are commonly used by the ML community for the empirical analysis of ML algorithms [15]. The data sets considered here are those used for regression and selected by Torgo. 1 The number of examples of these data sets goes from 43 to 40768 , while the number of variables varies from 2 to 60 . 4.1.4. Condenser data set
A total of 48 physical variables do have an in fl uence on the heat transfer rate and on the cleanliness factor, according to physicists and engineers [1,16]. The study of these variables is divided into two correspond to temperatures or pressures of each box and the rest to the whole process. Hence, 43 variables are used to estimate the heat transfer rate and the cleanliness factor for each box, Q and FC
The period of time considered was from January 1st to June 30th 1999. 4.2. Experiment settings
The system has been implemented in Matlab encapsulated in a Spider 2 object. The thresholds of the search were empirically chosen to be  X  =1%.

The system is compared with several methods. All of them use the 48 physical variables in case of the condenser data set. One of them relies on performing a multivariate linear regression considering as features all possible monomials of one variable until certain degree. This is computationally acceptable as the number of possible monomials is the product of the maximum degree and the number of variables. We will call this approach Univariate Monomial Regression (UMR). The degree of the polynomial was For the rest of data sets, a set of several degrees ranging from 1 to 5 were explored in the experiments, choosing the ones that have the best performance.

The SVR with polynomial kernel is another method included in the comparison. The implementation considered was that of Joachims [9], called SVM-Light. Another alternative is the Lib-SVM, but it is unable to perform all the experiments especially the largest ones. In SVR, it is important to select an adequate value of the parameter C , which relates the error and the margin. Another clue parameter is  X  , average of 1 parameters were taken into account, considering the best model to annotate its error and computational time. The kernel used for the Arti fi cial Polynomials data sets was the polynomial one, taking as degree the same as the one of the polynomial we wish to predict. In case of the RBF data set, we adopted a RBF and the condenser data sets, we proceeded adopting polynomial kernel and choosing the degree in the same way as for the UMR method.
Finally, we compared our method with M5 X  [21], which is a general purpose regression approach that induces decision trees and is a rational reconstruction of Quinlan X  X  M5. The implementation used in the experiments is that provided by Weka [22].

The performance of each system is evaluated using the Relative Medium Average Deviation (RMAD), which is the Medium Average Deviation (MAD) divided by the MAD of the system that always predicts the average. The expression of the MAD is where z is the category to be predicted and f ( x ) is the prediction obtained by the model. This measure is commonly adopted because it removes the effect of the dimension of the category.
 4.3. Analysis of the results
This section discusses the results for each of the data set considered in the experiments. 4.3.1. Arti fi cial polynomials Table 3 shows the RMAD for the Arti fi cial Polynomials data set when the parameters mentioned in Section 4.1.1 are varied. This table shows that GPSR outperforms the rest, especially with regard to SVR and UMR. Only M5 X  performs similarly, but almost always below GPSR. Figure 2 indicates the performance of the methods when the parameters vary. Each branch represents a parameter where the closer to the center of the fi gure the smaller the value of the parameter is. As it can be seen, GPSR performs better than the rest. Only M5 X  improves the performance for certain parameters and certain values of them (see the circles in the graph), in any case, this performance is highly punctual.
Table 4 presents the computational time of each method. According to it, M5 X  is clearly the most time consuming method, followed by GPRS and then SVR and UMR. Only the number of attributes and examples seem to affect the time, increasing as the values of these parameters increase. However, the time increasing in SVR with regard the number of attributes is not so clear. The explanation of these behavior maybe because it is a kernel based method. Also, UMR and GPSR do not increase the computational time so much as the number of examples increases. 4.3.2. Arti fi cial RBF
Table 5 shows the RMAD forthe Arti fi cialRBF data setwhen the parameters mentionedin Section 4.1.2 are varied. In this case, GPSR, UMR and M5 X  X re considerably betterthan SVR in allsituations. However, the behavior of GPSR with regard to UMR and M5 X  is slightly different depending on the parameter varied. For the number of examples, UMR and M5 X  are slightly better than GPSR, especially M5 X  when the number of examples is high. In relation to the number of variables, GPSR improves M5 X , but UMR offers a bit less RMAD. Finally, varying the number of relevant variables and the number of RBF examples, GPSR yields a better performance especially when these parameters increase.
 Figure 3 presents a summary of the performance of the different methods when the parameters vary. For this kind of functions, there is more variability, as commented in the last paragraph.
Table 6 presents the computational time of each method. The conclusions are quite similar to those is faster than the rest and SVR is in turn faster than GPSR, but the differences can be hardly noticed. Again, the methods are clearly variable with regard to the number of examples and the attributes and steady with regard to the number of relevant attributes. However, the number of RBF only affects GPRS, decreasing the time as the values of the parameter increases. This behavior may be caused because the complex the problem becomes, the simpler solutions GPRS provides, avoiding over fi tted models. 4.3.3. UCI repository
Table 7 shows the RMAD and computational time for the UCI repository averaged over all data sets that it contains. In these data sets M5 X  is the best method but GPSR improves both SVR and UMR. However, taking into account the computational time of M5 X , our method could be a good alternative. For general purpose regression problems, as these data sets, the best solution could be no continuous and GPSR could not fi t this kind of functions. GPSR would be more suitable for fi tting polynomial functions and/or in real-time environments. 4.3.4. Condenser data set
Table 8 presents the RMAD and computational time for the Condenser Data Set. GPSR reaches the best performance for each heat transfer rate and cleanliness factor. In fact, only GPSR always obtains less errors than the method that predicts the average ( RMAD &lt; 100 ). Notice that a simple univariate polynomial is not enough to reproduce that relationship. Also, it is noteworthy that only the methods that are able to generate more general polynomials (GPSR and SVR) reach the best performance. This Notice that the domain expert knowledge included in the systems is the set of original variables and all the systems bene fi t from this information equally. It is also remarkable that the computational time of GPSR is below the period of data sampling, which is 10 minutes. This does not happen with neither M5 X  nor SVR. Hence, it would be possible to develop an on-line learning system in the power plant based on GPSR. 4.4. Dimensionality reduction of the method This section discusses the degree of dimensionality reduction of GPSR.

Table 9 shows the number of monomials and the number and percentage of variables selected by GPSR This table also presents the number of monomials and the number and percentage of variables selected by GPSR for the Arti fi cial RBF data set when the number of RBF used to generate the data set varies. Table 10 shows the averaged number of monomials and the averaged number and percentage of variables selected by GPSR for all data sets. The number of monomials selected by GPSR is considerably low in relation to the number of all possible ones. The number of variables selected is also low. Besides, it is especially remarkable that the number of variables selected by GPSR keeps steady, even when the number of variables of the data sets increases. This considerable reduction on both the number of monomials Regarding the Arti fi cial RBF data set, it is shown that as the problem becomes more complex (when the number of RBF increases), the number of monomials and variables selected by GPSR decreases. 5. Conclusions and future work
This paper proposes a greedy forward method based on stepwise regression and the concepts of relevance and redundancy that selects either the variables or the monomials formed using such variables order to avoid prede fi ning the degree of the polynomial beforehand.

This allows to cope with a real regression problem: fi nding the relationship between a set of physical variables measured in a power plant and the performance of its condenser, estimated in terms of the heat out on some arti fi cial data sets and on some UCI repository data sets.
 The results prove that our model yields better performance than other regression methods, such as Support Vector Regression (SVR), when the conditi ons enable to assume a continuous relationship. However, other methods not based on polynomial regression, like M5 X , offer better effectiveness over for considerably increasing the computational time. Besides, the number of monomials and variables that our method selects keeps steady, independently of the original number of variables. and improve its effectivenessin the presenceof a data set where the assumption of continuous relationship could not be taken. Acknowledgements This research has been partially supported by the Spanish Ministry of Science and Innovation grants TIN2007-61273 and TIN2008-06247 and by the Foundation for the Promotion of Applied Scienti fi c Research and Technology (FICYT), Asturias, Spain, under grants PC06-039 and IB09-059-C2. References
