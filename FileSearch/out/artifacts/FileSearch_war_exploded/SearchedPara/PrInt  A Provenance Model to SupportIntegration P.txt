 In some integration applications, users are allowed to import data from heterogeneous sources, but are not allowed to up-date source data directly. Imported data may be inconsis-tent, and even when inconsistencies are detected and solved, these changes may not be propagated to the sources due to their update policies. Therefore, they continue to provide the same inconsistent data in the future until the proper au-thority updates them. In this paper, we propose PrInt ,a model that supports user X  X  decisions on cleaning data to be automatically reapplied in subsequent integration processes. By reproducing previous decisions, the user may focus only on new inconsistencies originated from source modified data. The reproducibility provided by PrInt is based on logging, and by incorporating data provenance in the integration pro-cess.

Data integration has been the focus of a lot of attention in both academia and industry [5]. At instance level, data integration aims at solving inconsistencies on data imported from heterogeneous sources, which may contain information on the same entity in the real world, but that differ on the value of their attributes. Although there are a number of approaches proposed in the literature that investigate data integration and data provenance separately, few of them con-sider both in the same setting [8, 3, 2, 6, 1]. In this paper, we address the use of provenance to support instance level data integration.

Data provenance is the set of metadata that allows for the identification of sources and transformations applied to data, since its creation to its current state [7, 4]. There are several advantages of incorporating data provenance into in-tegration processes, such as the ability to provide curated data back to external sources. However, there are a num-ber of integration applications in which updates on external sources are not allowed, for instance, due to lack of per-mission. In this setting, even when the integration process identifies that two pieces of information refer to the same entity in the real world, and corrects them on local copies, these updates may not be propagated to the sources. There-fore, the same erroneous data continue to be provided until the proper authority decides to update them.

Example 1. Suppose that a research center periodically generates a list of publications of its personnel by import-ing data from their home pages and also from digital li-braries, such as DBLP and ISI Web of Knowledge. Three researchers, John , Jack and Mary , are co-authors of a pa-per, but the imported data are inconsistent, as shown in Fig. 1a. Here, a paper is composed of attributes title , year and venue , and papers are identified by the value of the at-tribute title .Each paper also contains a set of authors ,each of them with attributes name and citationOrder .Wecon-sider that in the context of a paper ,each author is identified by its name . A user integrating these sources may perform the following actions: 1. the value of paper X  X  title for John is manually edited from  X  Integrating...  X  X o X  Integration...  X ; 2. paper X  X  year is copied from John to Jack ;3.author X  Bob  X  X s removed from Mary ;4. venue is copied from John to Jack ; and 5. author  X  Mary  X  X rom Jack is inserted into John .The remaining actions (i.e., 6 to 9) represent copies, which are similar to actions 2 and 4. After performing these actions, the imported data become consistent and are stored as three local copies, each one containing the integrated paper shown in Fig. 1b. Due to lack of permission, corrections made in local copies may not be propagated to the sources. Thus, in subsequent integration processes the user must reprocess the imported data, by taking the same decisions, in addition to those over new detected inconsistencies.

Repeating manual interventions on integration processes has two main drawbacks. First, it is error-prone, and may lead to contradicting solutions among integration processes. Second, it is time consuming, and it tends to get worse as the volume of data increases. Therefore, another advantage of incorporating data provenance into data integration pro-cesses is to allow user X  X  decisions to be reproduced in future iterations of the process.

The few provenance-based data integration systems that have been proposed in the literature [8, 3, 2, 6, 1] do not aim at reproducing integration decisions in applications where the integration process cannot update external data sources. In order to address this issue, a provenance model must guar-antee that all decisions taken by the user in previous inte-grations processes are solved automatically in subsequent integration processes. For reproducing a sequence of oper-ations and maintaining the same semantics, the provenance model has to consider the effect of transitive and overlap-ping operations, since they may mistakenly affect the final result of the integration process. Furthermore, it must be checked whether user X  X  decisions remain valid. We say that a decision is valid if the context in which it has been made remains unchanged. That is, if the data sources continue to provide the same data used as basis for the decision in a previous integration process.
 In order to achieve the aforementioned goals, we propose PrInt , a novel data provenance model that supports in-stance level data integration processes. The model focuses on applications in which data sources are read-only, i.e. users are allowed to import data, but are not allowed to update them. The goal of PrInt is the reproduction of user X  X  decisions among distinct integration processes. Using our model, all actions performed by the user in Example 1 are reapplied automatically, guaranteeing the proper man-agement of transitive and overlapping operations while per-forming validation. As a result, the user will only solve new inconsistencies generated from source modified data.
This paper is organized as follows. Section 2 gives an overview of the properties satisfied by the PrInt model, which are detailed in Sections 3 to 5. Section 6 concludes the paper.
The integration scenario considered by the PrInt model is depicted in Fig. 2. In this scenario, a first integration pro-cess, denoted as IntProc 1 , is executed by receiving as input several heterogeneous sources (Fig. 2a). It uses an integra-tion tool to identify and solve inconsistencies. As the user manually decides how to solve the inconsistencies (Fig. 2b), these actions are mapped to operations stored in a reposi-tory (Fig. 2c). Consistent copies of the imported data are locally stored, since direct updates on heterogeneous sources are not allowed (Fig. 2d).

In a subsequent integration process, which we denote as IntProc 2 , the local copies from IntProc 1 are replaced by up-to-date versions of the imported data. However, in IntProc 2 , the sources may or may not continue to provide the same inconsistent data (Fig. 2e). Thus, before reap-plying the operations defined by IntProc 1 , we first need to check whether the source have been updated. That is, we validate the repository of operations with respect to the sources X  current version (Fig. 2f). This step is followed by the reapplication of all valid operations on the imported data. This process reproduces the user X  X  integration deci-sions taken in IntProc 1 (Fig. 2g). As a result, manual in-tervention for solving inconsistencies among data sources in IntProc 2 are limited to those new inconsistencies originated from source updates since IntProc 1 (Fig. 2h and b).
Performing a  X  validation and reapplication  X  X re-processing on imported data based on a repository of operations can drastically reduce the number of manual interventions in an integration process. As a result, integra-tion becomes less error-prone and less time-consuming. To support this process, the PrInt model has been designed to satisfy two properties, as follows.
The motivation for the first property is to guarantee that there are no two operations that determine the final value of a data item. In order to achieve consistency, relationships between operations must be considered. For instance, if in an integration process the user takes a decision that over-rides a previous one, then operations related to the older decision must be managed properly. The management of relationships between operations is described in Section 4.
The second property, described in Section 5, refers to the idea that if a decision has been made based on a set of data values V , we can only apply the same strategy if these val-ues remain unchanged. For instance, suppose that two data sources provide distinct values, v 1 and v 2 , for the same data item, and the user chooses v 1 over v 2 . Once one of them, say v 2 is modified to v 3 , it is not clear whether the decision of choosing v 1 over v 3 is correct. In this case, we do not reap-ply the corresponding operations so that inconsistencies are not introduced in local copies without the user X  X  consent. Before describing the PrInt  X  X  properties, we provide in Section 3 details on operations supported by our model.
External data sources may keep information on distinct data formats. After importing these source data into PrInt entities are represented as objects, which are composed of attributes and subobjects. We consider that each object can be uniquely identified by the value of a subset of its attributes, called key attributes.

In PrInt user X  X  integration decisions are mapped to a se-quence of operations, which are stored in a repository .We consider four operations: insert , remove , edit ,and copy . Insert and remove are operations on objects, while edit and copy are attribute-level operations. Observe that these operations do not directly reflect user actions for integrat-ing data as the ones described in Example 1. That is, user actions or decisions can be expressed in a high-level lan-guage, and be mapped to a sequence of basic operations in the repository. For instance, consider action 5, which inserts author  X  Mary  X  X n John  X  X  data, based on the values provided from Jack . The insert action is mapped to a sequence of operations consisted of an insert operation to create a new author subobject for John , followed by a copy operation from Jack to John on attribute citationOrder . Similarly, ac-tion 3 for removing author  X  Bob  X  X rom Mary consists of an edit operation for setting  X  null  X  to the non-key attribute ci-tationOrder , followed by a remove operation. On the other hand, edit and copy actions are mapped to edit and copy operations, respectively.

A repository R is an array of records, where each record contains information on a basic operation. The array or-der reflects the temporal execution order of the operations. Each record in R is composed of the following attributes: (i) id : integer that identifies an operation in R ( id  X  op : the type of the operation, cp,ed,in,rm denoting copy , edit , insert and remove , respectively; (iii) origin :source that provides the correct value; (iv) target :sourcethatis the target of the operation; (v) objKey : values of key at-tributes of the object; (vi) objAtt : attribute name involved in op ; (vii) originValue : origin  X  X  attribute value; (viii) tar-getValue :original target  X  X  attribute value before the execu-tion of op . Given a record a in R ,wedenoteby a.att the value of attribute att of a .
There are two types of relationships between operations in a repository that may affect its consistency: transitive and overlapping operations, which are described in Section 4.1. In Section 4.2, we define a strategy, called the redo policy, for detecting and managing inconsistencies among operations.
When an operation b uses the result of another operation a , b is transitive to a . Transitive operations can be direct ( a  X  b ) or indirect ( a  X  n b ), and capture the idea of prop-agating operations results to subsequent ones.

We also consider operations that overwrite previous re-sults, which we denote as overlapping operations and dis-tinguish between origin ( a  X  o b ) and target ( a  X  t b )over-lapping. When a  X  o b or a  X  t b , operation a is said to be the overlapped operation and operation b is said to be the overlapping operation .Intuitively, a  X  t b if both operations writes on the same data item, and a  X  o b if operation a uses the value of a data item that is later overwritten by operation b .

Given the notions of transitive and overlapping opera-tions, a repository is consistent if transitive operations do not create cycles and there are no overlapped operations. Transitive operations may propagate a value back to its ori-gin, creating a cycle, which is not desirable because it could generate an infinite number of operations to be stored in the repository. This problem is avoided in PrInt by not allow-ing copy operations to have the same value on its originValue and on its targetValue , i.e., in a given copy operation a , a . originValue must be different from a . targetValue .There-fore, cycles of transitive operations never occur in PrInt
Inconsistencies among operations are derived from over-lapping operations, and are propagated to others by transi-tivity. Managing the repository consistency consists of de-veloping policies for repairing inconsistencies generated from overlapping operations. These actions take place during the integration process. In this section, we propose the redo policy for keeping the repository consistency. This policy is maintained by modifying and reordering operations stored in the repository.

Consider first origin overlapping. If a  X  o b then oper-ation a uses the value a.originV alue for updating an at-tribute value of another object on data source a.target ; a.originV alue is later overwritten by operation b with a different value b.originV alue . In this case, we infer that the user X  X  decision for applying operation a is based on the fact that she chooses the value provided by a.origin over that provided by a.target . In order to maintain this decision, given that a.originV alue is updated by operation b with b.originV alue , this new value should also be propagated to a.target (i.e., b  X  a becomes true). Therefore, the redo pol-icy for solving this inconsistency is to modify a by changing a.originV alue to b.originV alue , and moving a to the end of the repository. Operations that are direct or indirect tran-sitive to a also use a.originV alue . Thus, the same strategy is applied to these operations. Operations modified by the redo policy must be imediatelly executed using their new originV alue .

Now, consider target overlapping operations. Recall that if a  X  t b then both operations update the same data item in some source (i.e., a.target = b.target ). Since operation b is executed after a , in the end result the outcome of a  X  X  execution has no effect on the integration process. Thus, the redo policy removes a from the repository. Nevertheless, operations that are transitive to a must be adjusted. This is because given an operation c ,if a  X  c , c takes as input the value written by a . Since the final value of this data item is now given by operation b , c.originV alue is changed to b.originV alue ,and c is moved to the end of the repository.
The main goal of the PrInt model is to provide a means for reapplying operations that reflect the same decisions made by the user in previous integration processes. Since data sources are autonomous, they can be updated between two integration processes. Thus, we need to validate the operations before reapplying them. Validating an operation consists of verifying if both origin and target attributes still store the same value on the data item involved. Intuitively, we can only assume that the user would take the same ac-tion for solving the conflict if the data remain unchanged. Otherwise, the model would require additional user input.
Validation has the property that if an operation is invalid , then all its transitive operations are invalid as well. Another property is that validation is executed at attribute level. Copy and edit actions satisfy this condition, but insert and remove actions do not, as they are defined over objects. In order to perform validation, insert actions are mapped to insert and copy operations, and remove actions are mapped to remove and edit operations, as described in Section 3. Thus, an insert action is valid if its corresponding insert and copy operations are valid, and a remove action is valid if its corresponding remove and edit operations are valid. To reapply operations, we propose VRT ( Validate and Reapply in Tandem ), a method which consists of the fol-lowing steps. It first checks whether each operation in the repository is valid with respect to the origin . If this condi-tion is true, then it also checks for validity with respect to the target . Given that the operation is valid both on origin and target , it is reapplied on target . After processing all operations in the repository, the integrated data sources are available to be used in the new integration process, with all previous decisions made by the user already in place.
In this paper, we propose PrInt , a provenance model that supports instance level data integration processes. The model focuses on systems in which the integration process is not allowed to update heterogeneous sources directly accord-ing to user X  X  integration decisions. Therefore, it updates lo-cal copies of data sources, and keeps a consistent repository of operations for reproducing user X  X  decisions in subsequent integration processes automatically.

We are analyzing new policies for the management of over-lapping operations to support a wider range of integration scenarios. We also plan to extend PrInt to support schema level integration [5]. Another future work is to deal with con-current schedule of operations in multiuser environments. Acknowledgments. This work has been supported by the following Brazilian research agencies: FAPESP, CNPq, CAPES and FINEP. [1] D. W. Archer, L. M. L. Delcambre, and D. Maier. A [2] O. Benjelloun, A. Das Sarma, A. Halevy, M. Theobald, [3] P. Buneman, A. Chapman, and J. Cheney. Provenance [4] J. Freire, D. Koop, E. Santos, and C. T. Silva. [5] A. Y. Halevy, A. Rajaraman, and J. Ordille. Data [6] Z. G. Ives, T. J. Green, G. Karvounarakis, N. E. [7] A. Kementsietsidis and M. Wang. Provenance query [8] N. Shiri and A. Taghizadeh-Azari. Lineage tracing in
