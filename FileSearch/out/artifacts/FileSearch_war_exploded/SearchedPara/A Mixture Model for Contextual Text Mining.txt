 Contextual text mining is concerned with extracting topical themes from a text collection with context information (e.g., time and location) and comparing/analyzing the variations of themes over different contexts. Since the topics covered in a document are usually related to the context of the doc-ument, analyzing topical themes within context can poten-tially reveal many interesting theme patterns. In this paper, we propose a new general probabilistic model for contextual text mining that can cover several existing models as special cases. Specifically, we extend the probabilistic latent seman-tic analysis (PLSA) model by introducing context variables to model the context of a document. The proposed mixture model, called contextual probabilistic latent semantic anal-ysis (CPLSA) model, can be applied to many interesting mining tasks, such as temporal text mining, spatiotempo-ral text mining, author-topic analysis, and cross-collection comparative analysis. Empirical experiments show that the proposed mixture model can discover themes and their con-textual variations effectively.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Text Mining General Terms: Algorithms Keywords: Contextual text mining, context, mixture model, EM algorithm, theme pattern, clustering
A text document is often associated with various kinds of context information, such as the time and location at which the document was produced, the author(s) who wrote the document, and its publisher. The contents of text doc-uments with the same or similar context are often corre-lated in some way. For example, news articles written in the period of some major event all tend to be influenced by the event in some way, and papers written by the same researcher tend to share similar topics. In order to reveal in-teresting content patterns in s uch contextualized text data, it is necessary to consider context information when ana-Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. lyzing the topics covered in such data. Indeed, there have been several recent studies in this direction. For example, the time stamps of text documents have been considered in some recent work on temporal text mining [9, 16, 14, 4]. Also, author-topic analysis is studied in [17], and cross-collection comparative text mining is studied in [18]. All these studies consider some kinds of context information, i.e., time, authorship, and subcollection.

Time, authorship, and subcollection are by no means the only possible context information of a document. In fact, any metadata entry of a document can indicate a context and all documents with the same value of this metadata en-try can be considered as in the same context. For example, the source of a news article, the author X  X  age group, occu-pation, and location of a weblog article, and the citation frequency of a research paper, are all reasonable context information. Moreover, a document may belong to multi-ple contexts, and any combination of its metadata entries makes a  X  X omplex X  context. By analyzing the variations of topics over these contexts, a lot of interesting text mining tasks can be addressed, such as spatiotemporal text mining, author-topic evolutionary analysis over time, and opinion comparison over different age groups and occupations.
However, existing techniques are usually tuned for some specific tasks, and are not applicable to consider other kinds of contexts. For example, one cannot directly use the tem-poral text mining techniques to model the occupation of authors. This indicates a serious limitation of existing con-textual analysis of themes: every time when a new combina-tion of context information is to be considered, people have to seek for solutions in an ad hoc way.

Therefore, it is highly desirable to introduce a general text mining problem, contextual text mining, which is abstracted from a family of text mining tasks with various types of contextual analysis. It is desirable to derive a model that is highly general to conduct the common tasks of these specific contextual text mining problems, and easy to be applied to each of them with appropriate regularization.

In this work, we define the general problem of Contex-tual Text Mining (CtxTM) and its common tasks, which is abstracted from a family of specific text mining problems. We extend the probabilistic latent semantic analysis (PLSA) model to incorporate context information, and develop a contextual probabilistic latent semantic analysis (CPLSA) model to facilitate contextual text mining in a general way. By fitting the model to the text data to mine, we can (1) discover the global salient themes from the collection of doc-uments; (2) analyze the content variation of the themes in any given view of context; and (3) analyze the coverage of themes associated with any given context.

These tasks are general and can be easily applied to differ-ent specific contextual text mining problems. In this paper, we show that many existing contextual theme analysis prob-lems can be defined as special cases of CtxTM ,andcanbe solved with regularized versions of the mixture model we proposed, corresponding to the context information and the mining tasks it involves. Although it may not be the only possible model for contextual text mining, the model is quite flexible to adapt different assumptions.
Given a collection of documents with context information, we assume that there is a set of topics, or themes in the col-lection which vary over different contexts. Our goal is gen-erally to conduct context-sensitive analysis of these themes. As stressed in previous work [14], a theme in a contextual-ized text collection D is a probabilistic distribution of words that characterizes a semantically coherent topic or subtopic. Without loss of generality, we will assume that there are al-together k major themes in our collection,  X  = {  X  1 ,...,
To model the context of a document, we introduce a con-cept called context feature , which is defined as any meta-data of a document (e.g., the time stamp in temporal text mining or authorship in author-topic analysis). The context that a document belongs to can be indicated by the context fea-tures of this document, which is formally defined as follows:
Definition 1 (Context) Let F = { f 1 ,f 2 , ..., f | F | } a set of context features. A Context c in a document collection is decided by any combination of context fea-tures in F , formally c  X  2 |F| . The whole set of possi-ble contexts is denoted as C = { c 1 , ..., c n } . Suppose { ( document D i is a sequence of words from a vocabulary set V = { w 1 , ..., w | V | } ,and C i  X  X  is a set of context features which are associated with the document D i .Adocument D i belongstoacontext c iff. C i  X  c . This tells us that a document can belong to multiple contexts. In another word, the contexts are possible to overlap.

In contextual text mining, our goal is to analyze the top-ics/subtopics in such a text collection in a context-sensitive way. Specifically, we would like to model the k major themes and how they vary according to different contexts, and would also like to model the coverage of different themes in a doc-ument or documents that share certain kinds of context.
To accommodate context-sensitive theme analysis, we con-sider variations of these k themes over different contexts. For example, if the context we are interested in is time, we will assume that there is a potentially distinct  X  X ersion X  of the k themes in each different time period; different such  X  X er-sions X  model the variations of themes across time stamps. We formally define such a variation as a View of themes.
Definition 2 (View) A view of themes in a contextu-alized text collection D is a sequence of themes  X  i 1 , ...,  X  where  X  il is the variation of theme  X  l accordingtoview v
We will assume that there are n views in our collection, v , ..., v n , each corresponds to a context c i . Therefore, a doc-ument is assumed to potentially have multiple views; pre-cisely which views are taken depends on the document and its context. Each view v i is assumed to be taken in any doc-uments in the context c i , which can also be overlapping.
Definition 3(Context Support) The support of a con-text c i , s ( c i )isthesetofdocumentsincontext c i , i.e., s ( c i )= { D j | C j  X  c i } . Since each context is associated with a view, we also call s ( c i )asthe support of the view v
To analyze the strength of themes, we further model the variable coverage of different themes in a document. For ex-ample, some documents would favor some particular themes and thus would have a larger coverage of them.

Definition 4 (Coverage) A coverage of themes in a document (  X  j ) is a distribution over the themes Clearly, k l =1 p ( l |  X  j )=1.

We will assume that there are m distinct theme coverages in our collection,  X  1 , ...,  X  m . For example, if we assume that each document has a potentially distinct theme coverage, then m = |D| . In general, however, a document can cover themes according to multiple coverages. For example, if we are interested in modeling theme coverage associated with time stamps, we may assume that the actual theme coverage in a document would be a mixture of the document-specific theme coverage and another theme coverage associated with the time context of the document.

We use c (  X  j ) to denote the contexts where the coverage  X  is applicable, and we also define the support of a coverage in the same way as we define that of a context.

Definition 5 (Coverage Support) The support of a coverage  X  j , s (  X  j ) is the set of documents in which the cov-erage  X  j is taken, i.e., s (  X  j )= { D i | X  c  X  c (  X  j
The latent structure of themes, views and coverages in a contextualized document collection is illustrated in Figure 1. Figure 1: The theme-view-coverage structure in a text collection With these definitions, the task of Contextual Text Mining (CtxTM) can be defined as to recover the n views, v =(  X  i 1 , ...,  X  ik ), i =1 , ..., n ,andthe m theme coverages  X  , ...,  X  m from the collection D , and to analyze them in a context-sensitive way. There are many different ways to an-alyze the views and theme coverages. Below we discuss a few interesting cases. 1. Theme extraction: We may extract the global salient themes. Although each theme  X  l varies in different contexts, it is also beneficial to have an explicit model for  X  in a global view. Basically, this will give us the common information that is shared by all the variations of  X  l in all dif-ferent contexts. In practice, we always include a global view v , which corresponds to a global context c 0 = F . Clearly, all documents D i  X  X  belong to c 0 since C i  X  c 0 . 2. View comparison: We may compare the n views. The comparison of a theme  X  l from different views usually represents the content variation of  X  l corresponding to dif-ferent contexts. By comparing  X  il for each view v i which corresponds to context c i , we can analyze the influence of the context c i on the contents of  X  l . 3. Coverage comparison: We may compare the m cov-erages. The variations of p ( l |  X  j ) can tell us how likely covered by the documents in the coverage support s (  X  j ). By associating p ( l |  X  j )withincontexts c (  X  j )that  X  j ble, we can analyze how closely a theme is associated to a context, or how context-sensitive a theme is. 4. Others: With contextual text mining, we can also analyze other problems such as the influence of an individ-ual context feature on the theme coverage, e.g., the theme-location distribution in spatiotemporal theme analysis.
Among these cases, 2 and 3 are the most important, which distinguish contextual text mining from the traditional theme extraction work, and the application of them facilitate other types of analysis.

With the definition of the general problem of contextual text mining (CtxTM), we can show that some specific con-textual text mining problems are special cases of CtxTM. For example, in temporal text mining, each context feature is a time stamp. Therefore, a context is either a time stamp or a set of consecutive time stamps, or time period. A view of themes is taken in all the documents in the corresponding time. The goal of temporal text mining is mainly to compare the coverage variation over different contexts (e.g., theme life cycles in [14]), and sometimes also the content variation of themes over different views, (e.g., evolutionary theme pat-tern in [14]). In author-topic analysis, each context feature is an author, and each context is either an author or a set of authors. Each view is then taken in the document with the same author or authors. We are interested in comparing the content variations over different views (authors) [17].
In this section, we propose an extension of the Probabilis-tic Latent Semantic Analysis (PLSA) model [7, 8], called Contextual Probabilistic Latent Semantic Analysis (CPLSA) model, for contextual text mining. Our main idea is to al-low a document to be generated using multiple views and multiple coverages. The views and coverages actually used in a document usually depend on its context, which could be the time or location where the document is written, the source from which the document comes, or any other meta-data. We first propose the general CPLSA model, and then introduce two simplified versions of this model that are es-pecially suitable for two representative tasks of contextual text mining. In CPLSA, we assume that document D (with context C ) is generated by generating each word in it as follows: (1) Choose a view v i according to the view distribution p ( v i | D, C ). (2) Choose a coverage  X  j according to the cov-erage distribution p (  X  j | D, C ). (3) Generate a word using  X  .

Formally, the log-likelihood of the whole collection is The parameters are the view selection probability p ( v i the coverage distribution selection probability p (  X  j | D, C the coverage distribution p ( l |  X  j ), and the theme distribu-tion p ( w |  X  il ).

As a mixture model, we have a total of n  X  k multino-mial distribution component models. Each set of k multino-mial distributions,  X  i 1 , ...,  X  ik , represents a potentially dis-tinct view of the topics that we are interested in. How-ever, while we can potentially use all the views to generate a document, often the generation of a particular document D in a particular context C only involves a subset of these views. This is because in any interesting context mining scenario, different views generally have different supporting documents, though it is also common for the views to over-lap in some supporting documents.

More specifically, the view selection distribution p ( v i determines which views will actually be used when generat-ing words in document D . This distribution would assign zero probabilities to those views that are not selected. For example, if the views that we are to model correspond to the temporal context of a document and we have one global view spanning in the entire time period, then a document at time point t i would be generated using two different views  X  the view corresponding to time point t i and the global view, which is applied to all the documents.

Orthogonal to the choice of views, we also assume that we have choices of theme coverage distributions. The differ-ent coverage distributions are to reflect the uneven coverage of topics in different context and to capture the common coverage patterns. For example, if we suspect that the cov-erage may vary depending on the location of the authors, we can associate a particular coverage distribution to each location, which will be shared by all the documents in the location. After we learn such coverage distributions, we can then compare them across different locations. Once again, exactly which coverage distributions to use would depend on the context of the document to be generated.

The mixture model can be fit to a contextualized collec-tion D using a maximum likelihood estimator. The EM al-gorithm [5] can be used in a straightforward way to estimate the parameters; the updating formulas are as follows: p (
However, since the model has many parameters and has a high-degree of freedom, fitting it with a maximum like-lihood estimator, in general, would face a serious problem of multiple local maxima. Fortunately, in contextual text mining, we almost always associate them with appropriate partitions of context. As a result, the model is often highly constrained. For example, if all we are interested in is to compare non-overlapping views across different time, then p (  X  j | D, C ) becomes a delta function, i.e., p (  X  j | D, C if and only if  X  j is the coverage distribution for the time context of D ,and p (  X  j | D, C ) = 0 for all other  X  j  X  X .
Unfortunately, even with such constraints, the model may still have many free parameters to estimate. One possibil-ity is to add some parametric constraint such as assuming all coverage distributions are from the same Dirichlet distri-bution as done in LDA [2], which would clearly reduce the number of free parameters; indeed, we can easily generalize our model in the same way as LDA generalizes PLSA [8]. However, one concern with such a strategy is that the para-metric constraint is artificial and may restrict the capacity of the model to extract discriminative themes, which is our goal in contextual text mining. Another approach is to fur-ther regularize the estimation of the model by heuristically searching for a good initial point in EM; specific heuristics would depend on the particular contextual text mining task. This approach is adopted in our experiments and will be fur-ther discussed in Section 3.2.

In order to model the noise (e.g., common English words) in the text, we could designate the first theme as modeling such noise. That is, all  X  1 j  X  X  will be set to model the noise. We may further tie all of them so that we have just one common background unigram language model  X  1 .Thiscan also be regarded as applying an infinitely strong prior on the first theme in all views.
Considering that the most important tasks of contextual text mining are view comparison and theme coverage com-parison across contexts, as discussed in Section 2, we in-troduce two special cases of CPLSA, which are particularly useful to do these two tasks.

We first introduce the special version of CPLSA to facili-tate view comparison. In some cases, we are only interested to model the content variation of themes across contexts, e.g., when we are analyzing the theme evolutions over time [14], or comparing the common themes and corresponding specific themes across subcollections [18]. In these cases, we can fairly assume that the theme coverage over contexts is fixed, thus does not depend on the contexts that a docu-ment is in. Under this assumption, the m j =1 p (  X  j | D, C the model will be simplified as m j =1 p (  X  j | D ). If we further assume that there is only one coverage  X  applicable to each document, the log-likelihood function can be written as where  X  D is the coverage associated with the document D . We call this simplified version of model as fixed-coverage contextual mixture model (FC-CPLSA) .Ifwehave three views, where one is the global view and the other two correspond to subcollections, it will allow us to compare the common themes and specific themes in the two views, as discussed in [18]. If each view corresponds to a time stamp, this model will allow us to analyze the content evolutions of themes over time, as discussed in [14].

In some other cases, we are only interested to model the variation of theme coverage over contexts, e.g., when we are analyzing the life cycles (i.e., strength variations over time) of themes. In these cases, we are not interested in the content variation of local themes, and thus make the assumption that different views of themes are stable. With this assumption, we can simplify the model likelihood as where p ( w |  X  l ) is the global word distribution of theme which does not vary across contexts. We call this simplified model as fixed-view contextual mixture model (FV-CPLSA) . If the only context feature is time, we have two types of coverage distributions  X  D and  X  T ,where  X  D is the coverage distribution corresponding to each document and  X 
T is the theme coverage for each time period. This will allow us to model the theme life cycles, as introduced in [14]. If we have two context features, time and location, and each context is a combination of time stamp and location, we also have two groups of theme coverage distributions,  X  D and  X 
TL . This will allow us to analyze the spatiotemporal theme distributions in a spatiotemporal text mining framework.
With these two special simplified versions, the CPLSA model can be applied to solve a broad family of text mining problems with contextual analysis.
We apply the general CPLSA model presented in Section 3 to three different datasets and text mining tasks. Empirical results show that this model can model the themes and their variations across differe nt contexts effectively. In this experiment, we evaluate the performance of the CPLSA models on author-topic comparative analysis. If two authors have similar research interest, we assume that there is a set of common themes which can be found in their publications. Since different author has different preferences and focuses, the content of these themes will also vary cor-responding to each author. Previous work on author-topic analysis only consider the authorship of documents as the context [17]. Intuitively, however, the topics that an au-thor favors also evolute over time. We add another type of context information, i.e., publication time, to test the effec-tiveness of our model on handling multiple types of contexts.
We collect the abstracts of 282 papers published by two famous Data Mining researcher s from ACM Digital library. We split the whole time line into three spans: before the year 1993, from 1993 to 1999, and after the year 1999. This will give us 12 possible views as in Table 1. Since we are not interested in analyzing the coverage variations across contexts (i.e. time and authors), we assume the coverage of themes only depends on documents but not on the contexts. Table 1: Possible Views in Author-Topic Analysis
Therefore, we use the FC-CPLSA model presented in Sec-tion 3.2 to model the themes and their views corresponding to different contexts. Our goal is thus to estimate all the parameters in the regularized model, and compare p ( w |  X  over different view v j .

To avoid the EM algorithm being trapped in suboptimal local maximums, we need to make associations between each  X  jl to its corresponding global view  X  l . We achieve this by selecting a good starting point for the EM algorithm. Specif-ically, we begin with a prior of a large p ( v 0 | D, C )toview0, which is the global view. This ensures us to get the strong signal of global themes instead of local biased themes. In the following iterations, we gradually decay this prior and terminate the EM algorithm early when the average view distribution for view 0 (i.e., D  X  X  p ( v 0 | D, C ) / | D | under a threshold, say 0.1. This gives us a good starting point for the EM algorithm. Then, we do this procedure again for multiple trials and select the best start point (i.e., the one with the highest likelihood). Finally, we run the EM algorithm beginning with this selected start point until it converges. The results for this experiment are selectively presented in the following table.

In Table 2, we see that the content of this selected theme varies over different views. From the global view, in which all documents are included, we can tell that this theme is talking about frequent pattern mining. From the view of Au-thor A, we see specific frequent pattern mining techniques such as database projection, apriori, prefixspan, and closet. From the view of Author B, we see that he is not as deep into techniques of mining frequent patterns, but rather more as-sociated with introductional and innovated work of frequent pattern mining. From the view of the years before 1993, the corresponding theme barely has any connection to fre-quent pattern mining. This is reasonable however, since the first and most influential paper of frequent pattern mining was published in 1993. From the view of year 1993 to 1999, we see that this theme evolutes to talk about association rules, which is perhaps the most important application of frequent pattern mining at that time. Specific techniques, such as fdm (Fast Distributed Mining of associate rules) ap-pears high in the word distribution. From the view of the years after 1999, it is interesting to see the appearance of more new applications of frequent pattern mining, such as graphs and web. The terms corresponding to specific tech-niques of mining graph patterns and sequential patterns, e.g., gspan and bide, are with high probabilities in the theme word distribution. In the view corresponding to a combined context (Author A and after 1999), the top terms include  X  X lose X ,  X  X op-k X , and  X  X p-tree X , which well reveal the pref-erences of author A in frequent pattern mining. The view specific theme for the combined context  X  X uthor B after 1999 X  is not well associated with the global theme again, which is consistent to the fact that Author B is not activate in frequent pattern mining any more after 2000.

This experiment shows that the CPLSA model can ex-tract and compare the theme variations over different views effectively.
In this experiments, we show the effectiveness of CPLSA models on spatiotemporal analysis of themes. The context features we consider in this experiment is time stamps and location information of documents. The tasks of this spe-cific contextual text mining problem are: (1) extract global themes from the collection, which are shared by different time and locations; (2) for each time stamp, compute the distribution of theme and locations, from which we can draw the theme distribution snapshots over locations; and (3) compare the views of themes across contexts.

It is interesting to see that the second task is not a com-mon task of CtxTM. Let a context C be denoted as ( t, l ) where t and l refer to time and location, the task is to esti-mate p (  X  | D, ( t, l )) for each  X  ,and P (  X , l | t ) for any
We collect 9377 MSN Space docu ments with a time-bounded query submitted to Google blogsearch, with the keywords  X  X urricane Katrina X . In this dataset, 7118 documents pro-vide explicit location information, and the locations of oth-ers are tagged as  X  X nknown X . We segment the time stamps into six weeks, extract and compare the common themes over different locations in United States.

Each combination of the 50 States and six week consists a unique  X  X ontext X , which gives us 50  X  6 = 300 contexts. Since there are many contexts, it is difficult to estimate all the views precisely. Since we are only interested in the strength variations of global themes over all the contexts, it is reason-able to simplify the model by assuming that the content of the global themes does not vary over contexts. Therefore, we use the FV-CPLSA model presented in Section 3.2 to model the global themes and their coverage variations over time and locations. We further assume that p (  X  C | D, C constant that controls the impact of the context on selecting the coverage of themes. By estimating the free parameters, our goal is to compute the theme-location coverage: p (  X , l | t )= p (  X , l, t ) where p (  X  t,l | t, l )=1, p ( t, l ) can be computed from the word count in time period t at location l divided by the total word count in the collection.

With p (  X , l | t ) computed, we can visualize the theme-location coverage by fulfill p (  X , l | t ) in a snapshot map. In Figure 2, we show one of the 10 global themes we extracted from the blog dataset and its theme-location coverage at different time. From the top terms in this theme, we can infer that this theme is talking about aid and donations that were made to the hurricane affected areas. Figure 2 well demonstrates the evolution of theme-location coverage over different time periods. A detailed description of theme variation over time and location can be found in [13].

The next task is similar to the experiment in Section 4.1, which is to compare the views of themes across contexts. Specifically, we partition the states into four groups: Af-fected States; Peripheral States; Coast States; and Inland States. We partition the time line into spans with the length of two weeks. Then we use the FC-CPLSA model to com-pare the views of themes corresponding to different contexts. The results are selectively shown in Table 3.

It is easy to see that from the view of  X  X eriphery States X , the content of the theme  X  X onation X  is quite similar to the common theme extracted in Figure 2. People tend to talk about donations and supplies with food. However, from the view of  X  X ffected Areas X , which corresponds to the hur-Table 3: Comparison of the content of the theme  X  X id and Donation X  over different views ricane affected states such as Louisiana, people care more about medical aid and hospital cares. In the first two weeks, the view of this theme is still quite similar to the common theme. However in the last two weeks, we can notice that the  X  X elps X  become more about rebuilding and helping the returning evacuees.

This group of experiments show that our general model is effective to analyze spatiotemporal theme patterns.
In many scenarios, a collection of documents are usually associated with a series of events. For example, weblogs usually reflect people X  X  opinions about the events happen-ing. The research topics covered by scientific literatures are also likely to be affected by the influential related events, such as the invention of WWW, and the proposing of a new research direction. The impact of such event can usually be analyzed by comparing the themes in the documents pub-lished before versus after the event. In this experiment, we apply CPLSA on the problem of event impact analysis. Since each event gives a possible segmentation of the time line, this analysis also provides an evaluation of CPLSA on modeling overlapping views that are not orthogonal to each other. Although the experiments in previous sections also covers some overlapping views (e.g., a view corresponding to a location and a view corresponding to a time stamp), these overlaps are caused by different types of, or orthog-onal context features (e.g., time and location). In reality however, the overlapping views with the same type of con-text feature is desirable. For example, a business analyzer may need to analyze and compare the customers X  opinions in the first week, in the first month, in the first season, or in the first year after a new product is released. One strength of our model is that we allow the analysis views that overlap with each other. In this experiment, we evaluate our model on event impact analysis and overlapping view analysis.
We collect the abstracts of 1472 papers published in 28 years X  SIGIR conferences from ACM Digital Library. We select two influential events to the Information Retrieval community in the 90s. One is the beginning of Text RE-trieval Conferences (TREC) in 1992, which provide large-scale standard text datasets and judgements for many re-trieval problems. The other is the introduction of language model into Information Retrieval in 1998, which began a genre of research and led to a lot of publications. Our goal is to use the CPLSA model to reveal the impact of these two events in IR research, i.e., how the content of research topics change after the two events.

To achieve this, we assign the abstracts in SIGIR proceed-ings into four contexts, each corresponds to a time span. The first context includes all the documents were published be-fore 1993, in which is the first SIGIR conference after the start of TREC. The second context contains documents pub-lished on or after that. The third context includes abstracts before the year 1998, in which the first paper of language model in information retrieval was published. The fourth context contains all abstracts published on or after 1998. It is clear that there are overlaps between these contexts. We also include a global view, which corresponds to all the abstracts in SIGIR proceedings.

We use the same strategy as presented in Section 4.1 to avoid the EM algorithm to be trapped in unexpected local maximums. We extract 10 salient global themes from this collection and present the most interesting one.

From the global view in Table 4, we see that this theme is talking about retrieval models, especially term weight-ing and relevance feedback. The content of this common theme varies from different views. From the Pre-Trec view, which corresponds to the time before 1993, we see that vec-tor space model dominates, and boolean queries are men-tioned frequently. In the Post-Trec view, however, we no-tice that XML retrieval model has been paid more attention to. Also, we see specific types of data (email) and other terms related to the nature of TREC (e.g., collect, judge-ment, rank). It is more interesting when comparing the view  X  X re-Language Model X  and  X  X ost-Language Model X . We see that before 1998, the retrieval models are dominated by probabilistic models. After 1998, however, it is very clear that language model dominates the theme. The top ranked terms have changed to indicate language models, parame-ter estimations, likelihood and probability distributions, and language model smoothing. This is consistent with our prior knowledge. The overlapping views, for example Pre-LM and Pre-Trec, do share some content but clearly with different focuses. Pre-Trec, which is more faraway, emphasizes vector space model while Pre-LM emphasizes probabilistic models. This experiment shows that our method is effective to ana-lyze event impact and model the overlapping views.
The most relevant work is the Probabilistic Latent Se-mantic Analysis model (PLSA) proposed by Hofmann [7, 8], which models a document as a mixture of aspects, where each aspect is represented by a multinomial distribution over the whole vocabulary. Our CPLSA model is a natural ex-tension of PLSA to incorporate context. To avoid overfitting in PLSA, Blei and co-authors proposed a generative aspect model called Latent Dirichlet Allocation (LDA), which could also extract a set of themes from a document collection [2]. LDA, however, does not model context either. Although we have not explored it, one can also make LDA contextualized in the same way as we have done to PLSA in this paper. Re-cently, some extensions of this work have considered some specific types of context. For example, temporal context is considered in [6, 16, 4, 14]. Multi-collection context is an-alyzed in [18]. Author-topic analysis is proposed in [17]. Li et al. proposed a probabilistic model to detect retrospec-tive news events by explaining the generation of  X  X our Ws from each news article [11]. Our work is a generalization of these studies of specific context and provides a general prob-abilistic model which can be applied to all kinds of context.
Temporal context is also addressed in Kleinberg X  X  work on discovering bursty and hierarchical structures in streams [9] and some work on topic/event/trend detection and tracking (e.g., [1, 3, 12, 10, 15]). However, most of this work assumes one document only belongs to one topic and cannot be easily generalized to analyze other contexts.
In this paper, we present a study of the general problem of contextual text mining. We formally defined the basic tasks of contextual theme analysis, and proposed a novel prob-abilistic mixture model to extract themes and model their content and coverage variations over different, possibly over-lapping contexts. The problem definition and the proposed model are quite general and cover a family of specific contex-tual theme analysis problems and methods as special cases. Empirical experiments on three different datasets show that the proposed model is effective for extracting the themes and comparingtheviewsandcoveragesofthemesacrossquite different contexts.

Our work is an initial step toward a general model for con-textual text mining. An important future research direction is to further study how to better estimate the proposed mix-ture model as discussed in Section 3.1. Another important future research direction is to c reate evaluation criteria and judgements so that we can quant itatively evaluate different contextual text mining approaches. This work was in part supported by the National Science Foundation under award numbers 0425852, 0347933, and 0428472. [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, who, when, where and what (keywords) [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] S. Boykin and A. Merlino. Machine learning of event [4] C. C. Chen, M. C. Chen, and M.-S. Chen. Liped: [5] A. P. Dempster, N. M. Laird, and D. B. Rubin. [6] T.L.GriffithsandM.Steyvers.Fidingscientific [7] T. Hofmann. Probabilistic latent semantic analysis. In [8] T. Hofmann. Probabilistic latent semantic indexing. [9] J. Kleinberg. Bursty and hierarchical structure in [10] A. Kontostathis, L. Galitsky, W. M. Pottenger, [11] Z. Li, B. Wang, M. Li, and W.-Y. Ma. A probabilistic [12] J. Ma and S. Perkins. Online novelty detection on [13] Q. Mei, C. Liu, H. Su, and C. Zhai. A probabilistic [14] Q. Mei and C. Zhai. Discovering evolutionary theme [15] R. Nallapati, A. Feng, F. Peng, and J. Allan. Event [16] J. Perkio, W. Buntine, and S. Perttu. Exploring [17] M.Steyvers,P.Smyth,M.Rosen-Zvi,and [18] C. Zhai, A. Velivelli, and B. Yu. A cross-collection
