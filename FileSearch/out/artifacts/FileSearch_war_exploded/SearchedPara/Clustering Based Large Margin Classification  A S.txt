 This paper presents a novel Second Order Cone Program-ming (SOCP) formulation for large scale binary classifica-tion tasks. Assuming that the class conditional densities are mixture distributions, where each component of the mixture has a spherical covariance, the second order statistics of the components can be estimated efficiently using clustering al-gorithms like BIRCH. For each cluster, the second order moments are used to derive a second order cone constraint via a Chebyshev-Cantelli inequality. This constraint en-sures that any data point in the cluster is classified correctly with a high probability. This leads to a large margin SOCP formulation whose size depends on the number of clusters rather than the number of training data points. Hence, the proposed formulation scales well for large datasets when compared to the sate-of-the-art classifiers, Support Vector Machines (SVMs). Experiments on real world and syn-thetic datasets show that the proposed algorithm outper-forms SVM solvers in terms of training time and achieves similar accuracies.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier Design and Evaluation Performance Gaussian Mixture Models, BIRCH, Scalability, large margin classification
In recent times, there has been an explosive growth in the amount of data that is being collected in the business and Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. scientific arena. As a result, many real-world binary classifi-cation applications involve analyzing millions of data points. Intrusion detection, web page classification and spam filter-ing applications are a few of them. Classification of such large datasets is a challenging task, as they may not fit into memory. Most of the existing classification algorithms are not attractive as they perform multiple passes over data.
Support Vector Machines [15] (SVMs) are one of the most successful classifiers that achieve good generalization in prac-tice. SVMs (soft-margin SVMs) pose the classification prob-lem as a convex quadratic optimization problem of size m + n +1, where m is the number of training data points and n is their dimensionality. The optimization problem has a quadratic objective function and 2 m linear inequalities. The main contribution of the present paper is to pose the classifi-cation problem as a convex optimization problem, whose size is not dependent on the training set. SVMs have emerged as useful tools for classification in practice, primarily because of the availability of efficient algorithms like SMO [13] and chunking [7], which solve the dual of the SVM formulation. However, these algorithms are known to be atleast O ( m 2 in running time (see [13, 16]) and hence not scalable to large datasets.

Clustering before computing the classifier is an interesting strategy for large scale problems. CB-SVM [16] is an iter-ative, hierarchical clustering based SVM algorithm, which handles large datasets. The algorithm thrives on the fact that the SVM optimization solution depends only on a small set of data points called support vectors that lie near the optimal classification boundary. The authors, in their pa-per, show that the algorithm gives accuracies comparable to SMO with a very small run-time. The proposed classi-fication method also uses clustering to classify data points. However, the method does not proceed in an iterative fash-ion and does not require hierarchical clustering of the train-ing set. The proposed classifier scales well for very large datasets and gives accuracies comparable to that of SVMs. The class conditional densities are assumed to be modeled using mixture models with spherical covariances. A scalable clustering algorithm is employed to estimate the second or-der moments of components of the mixture models. Using Chebyshev X  X  inequality and the moments of component den-sities, the misclassification error on each of the components incurred by the hyperplane classifier is bounded. Introduc-ing slack variables, the bounds can be relaxed to allow for non-separable cases. The relaxation is penalized by minimiz-ing the sum of the slack variables. Additionally an upper bound on w 2 is put in order to maximize the generaliza-tion of the classifier. The resulting optimization turns out to be a Second Order Cone Programming (SOCP) problem, which can be efficiently solved using fast interior points al-gorithms [12].

The SOCP problem formulated as above has k linear in-equalities and one SOC constraint, where k = k 1 + k 2 . k are the number of components in the mixture model of the i th class. Note that the number of inequalities is indepen-dent of m , unlike the case of SVMs, where the number of linear inequalities is 2 m . Thus, the size of the optimiza-tion problem does not increase with the number of data points. Estimation of the moments of the component distri-butions is done using an efficient clustering scheme, such as BIRCH [17]. BIRCH, in a single pass over the data con-structs a CF-tree (Cluster Feature tree), given a limited amount of resources. CF-tree consists of the sufficient statis-tics for the hierarchy of clusters in the data. BIRCH also handles outliers effectively as a by-product of clustering.
The remainder of the paper is organized as follows. In section 1.1, a brief review of the classification formulations of SVMs is provided. Main contributions of the paper are presented in section 2. Section 3 presents the experiments on synthetic and real world datasets. Section 4 concludes the paper by discussing some future directions of work.
Let D = { ( x i ,y i ) | x i  X  R n ,y i  X  X  X  1 , 1 } ,i =1 ,...,m be the training set. x i represents a data point whereas, y i represents the corresponding class label. The original SVM formulation [15] solves the problem of linear binary classification. It uses w x  X  b = 0 as the discriminating hyperplane between the two classes. The idea is to mini-mize the training set error, by constraining most of the data points to lie on either side of the set of canonical hyperplanes w x  X  b =  X  1, and upper bounding the complexity of the classifier  X  which in machine learning terms corresponds to achieving good generalization [15]. Bounding complex-ity, in case of such linear classifiers, gives the constraint w 2  X  W ,where W is some positive real number. Geo-metrically this corresponds to having a lower bound on the distance between the set of canonical hyperplanes, called margin (= 2 w 2 ). Thus SVM formulation can be written as (  X  j are slack variables): The problem (1) is an instance of Second Order Cone Programming problem. An SOCP problem is a convex op-timization problem with a linear objective function and sec-ond order cone constraints (SOC). An SOC constraint on the variable x  X  R n is of the form c x + d  X  Ax + b 2 where b  X  R m , c  X  R n , A  X  R m  X  n are given. SOCP prob-lems can be efficiently solved by interior point methods for convex non-linear optimization [12]. As a special case of convex non-linear optimization, SOCPs have gained much attention in recent times. For a discussion of further effi-cient algorithms and applications of SOCP see [9].
The problem (1) can be equivalently written as the fol-lowing convex quadratic programming problem: (2) is the famous SVM soft-margin formulation. The pa-rameters C and W are related. However, W has the elegant geometric interpretation as a lower bound on the margin. This section presents the clustering based classifier. Let Z 1 and Z 2 represent the random variables that generate the data points of the positive and negative classes respectively. Assume that the distributions of Z 1 and Z 2 can be modeled using mixture models, with component distributions having spherical covariances. Let k 1 be the number of components in the mixture model of positive class and k 2 be that in the negative class. Let k = k 1 + k 2 .Let X j ,j =1 ,...,k 1 resent the random variable generating the j th component of the positive class and X j ,j = k 1 +1 ,...,k represent that generating the ( j  X  k 1 ) th component of the negative class. Let X j have the second order moments (  X  j , X  2 j I ). The prob-ability density functions (pdfs) of Z 1 and Z 2 can be written as f Z 1 ( z )= where,  X  j are the mixing probabilities (  X  j  X  0, and correctly estimate the second order moments of the compo-nents. BIRCH is one such clustering algorithm, that scales well for large datasets. Given these estimates of second or-der moments, an optimal classifier that generalizes well must be built.

Let w x  X  b = 0 be the discriminating hyperplane and w x  X  b =1, w x  X  b =  X  1 be the corresponding set of supporting hyperplanes. As discussed in section 1.1, the constraints w Z 1  X  b  X  1and w Z 1  X  b  X  X  X  1 ensure that training set error is low. Since Z 1 and Z 2 are random vari-ables, the constraints cannot be always satisfied. Thus, we ensure that with high probability, the events w Z 1  X  b  X  and w Z 1  X  b  X  X  X  1 occur: 1 : where,  X  is a user defined parameter.  X  lower bounds the classification accuracy. Since the distribution of Z i is a mix-ture model, in order to satisfy (3), it is sufficient that each of the components satisfy the following constraints: It can be easily seen that the constraints (4) are consistent only if the means of the components are linearly separable. Thus, in order to handle the case of outliers and almost linearly separable datasets, the constraints in (4) can be re-laxed using some slack variables (  X  i ) and suitably penalizing the relaxation. This leads to the following large margin clas-
Z i  X  f Z i means Z i has the pdf given by f Z i sification formulation (similar to (1)): min s.t. P ( w X j  X  b  X  1  X   X  j )  X   X , j =1 ,...,k 1 ,
The constraints in the optimization problem (5) are prob-abilistic. In order to solve the optimization problem (5), the constraints need to be written as deterministic constraints. To this end, consider the following multivariate generaliza-tion of Chebyshev-Cantelli inequality [3, 10, 2]. Theorem 1. Let X be an n dimensional random vector. The mean and covariance of X be  X   X  R n and  X   X  R n  X  n . Let H ( w ,b )= { z | w z &lt;b, w , z  X  R n ,b  X  R } be a given half space, with w =0 .Then where s =( b  X  w  X  ) + , ( x ) + = max ( x, 0) .
 Applying theorem 1 (see also [8]), the constraints for positive class can be handled by setting P ( w X j  X  b  X  1  X   X  j ): which results in the constraint where,  X  = negative class can be obtained.

Let y j ,j =1 ,...,k represent the labels of the components (clusters). Note that y i =1for k 1 components and y i =  X  for the other k 2 components. Using this notation, (5) can be written as the following deterministic optimization problem: min s.t. y j ( w  X  j  X  b )  X  1  X   X  j +  X  X  j w 2 ,j =1 ,...,k One can derive tighter bounds on the probabilities in (5), by assuming that the component distributions in mixture model are Gaussian. In other words, assume that the distri-butions of Z 1 and Z 2 are modelled using Gaussian Mixture Models (GMMs). With such an assumption, one can write the constraints in (5) as deterministic constraints using:
P ( w X j  X  b  X  1  X   X  j )= X  where  X  is the distribution function of univariate normal dis-tribution with mean 0, unit variance. Thus, the constraints in (4) can be written as: where,  X  = X   X  1 (  X  ). Note that, the final form of the con-straints with (7) or without (9) the assumption of Gaussian components are the same. In the following text,  X  is as-sumed to be  X   X  1 (  X  ) if Gaussian components are assumed and
The constraints in (8) involving w 2 canbewrittenas: Thus, the optimization problem (8) can be written in the following equivalent form: The classification formulation (10) is an SOCP problem. This problem can be solved to obtain the optimal values of w and b . The classification algorithm employed is sum-marized as follows:
Observe that when  X  ij = 0, the SVM formulation (1) and the present formulation are same. In other words, if each data point is considered to be a cluster, then both the for-mulations are same. Also, note that the number of linear inequalities in (1) is 2 m , whereas in the proposed formu-lation it is k . Thus, the proposed formulation is expected to scale very well to large datasets. The time-complexity of clustering algorithm like BIRCH is O ( m ) and that of the op-timization is independent of m . Thus, the overall algorithm is expected to have a training time of O ( m ).
The constraints in (10) have an elegant geometric inter-pretation. In order to see this, consider the following prob-lem. Suppose B ( c ,r )= { x | ( x  X  c ) ( x  X  c )  X  r 2 } set of data points lying in the sphere B with center c and radius r . Assume that all points of set B belong to positive class. Consider the problem of classifying the points lying in B (  X ,  X  X  ) correctly (allowing for slack variables): (11) has infinite number of constraints, but can be posed as a single constraint as shown below: Geometrically, the constraints in (11) say that all points that belong to B (  X ,  X  X  ) lie on the positive half space of the hyperplane w x  X  b =1  X   X  . This geometric picture (also see [4]) immediately shows that all the constraints (11) can be satisfied just by ensuring that the point in B (  X ,  X  X  )whichis nearest to the hyperplane w x  X  b =1  X   X  lies on the positive half space. This idea is stated as equation (12). Finding the minimum distant point on a sphere to a given hyperplane is simple. Drop a perpendicular to the hyperplane from the sphere X  X  center. The point at which the perpendicular intersects the sphere gives the minimum distant point ( x Note that x  X  is the optimum solution of (12). Using this geometrical argument, x  X  can be calculated using: x  X   X   X  =  X   X  w , x  X   X  B (  X ,  X  X  ). This gives x  X  =  X   X   X  X  w w 2 .Now, (11) is satisfied if w x  X   X  b  X  1  X   X  . Thissaysthat 2 , Note that this equation is of the same form as (9). Hence, geometrical interpretation (see also [5]) of the constraints of (10) is to restrict the discriminating hyperplane to lie such that most of the spheres B (  X  j , X  X  j ) are classified correctly. Figure 1 shows this geometric picture. Note that in the figure except the sphere at (5 , 5), all the spheres satisfy the constraint with  X  j =0.
 It is interesting to study the dual of the formulation (10). Using the dual norm characterization w 2 =sup u 2  X  1 u w and the Lagrange multiplier theory, the dual can be written as: max and the necessary and sufficient Karush-Kuhn-Tucker (KKT) conditions can be written as: X  X  j  X  j =0 , X  ( w u  X  W )=0 , X  j  X  0 , X  j  X  0 , X   X  0 (15) where  X  j , X  j , X  are the Lagrange multipliers. Suppose 0 &lt;  X  j &lt; 1and  X &gt; 0 then, from the KKT conditions it can be seen that  X  j =0, w 2 = W and y j ( w  X  j  X  b )= 1+  X  X  j w 2 . This says that the supporting hyperplanes are tangent to B (  X  j , X  X  j ). Extending the terminology used in case of SVMs, such spheres may be called as non-bound support spheres. Similarly one can define the bounded sup-port spheres as spheres with  X  j = 1. Also, note that  X  j 1  X   X  j &gt; 0. In figure 1, the spheres marked with  X  X  X  are non-bound support spheres and hence are tangent to the supporting hyperplanes.
 Note that the dual involves dot products of data points. This is because, The estimate of  X  2 j is 1 m j x k are the m j data points that belong to j th cluster. As the formulation (14) involves only the dot products of the data points, it can be extended to arbitrary feature spaces by using Mercer kernels [11].

Assuming that the given dataset is linearly separable, one can write an equivalent of the hard-margin classifier for the
The same constraint can be derived more rigorously using optimization theory Figure 1: Illustration showing the geometric mean-ing of the constraints. Clusters marked with  X   X   X  have positive labels and those marked  X   X  X aveneg-ative labels. The radius of clusters is proportional to  X  X  j . proposed formulation (10): Interestingly, the dual of the problem (16) turns out to be the problem of finding distance between the convex hulls formed by the negative and positive spheres ( B (  X  j , X  X  This is analogous to the case of SVMs, where dual is the problem of finding distance between the convex hulls formed by the negative and positive data points [1].
In this section, we present experimental results on syn-thetic and real world data sets. The results show that the accuracies achieved by SVM and the proposed classifier are comparable and that the proposed classification algorithm scales well for large datasets. In all cases, BIRCH was used to cluster the positive and negative training data points. The original BIRCH implementation by Zhang et.al. [17] was used for clustering. SeDuMi [14], a publicly available SOCP solver was used to solve the optimization problem (10) in all experiments. The performance of the proposed Clustering Based Classifier ( CB-SOCP ) was compared to that of SVM (using linear kernel) implemented by LIBSVM [6] (denoted by SVM ) 4 . All experiments were carried on Pen-tium 4 2.4GHz machines with 1GB memory. A  X   X   X  X nthe tables 1,2 and 3 represents the failure of the corresponding classifier to complete training.
The parameter C (see (2)) of SVM was tuned for each dataset separately. The main parameters for BIRCH algo-rithm were chosen to be the default values as given in [17]. Since the values of k 1 and k 2 are not known for the real world datasets, they were chosen to be the number of leaf CF en-tries in the CF-tree for positive and negative data points respectively. However, in case of synthetic datasets since k
Proof not provided due to space restrictions
CB-SVM is not used for comparison of performance due to non-availability of its implementation and k 2 are known, k 1 and k 2 were used as the number of clusters for positive and negative data points. The values of  X  and W were fixed to be 0 . 8 and 500. The values were not tuned for each dataset. However, in general, tuning of these parameters specifically for a dataset can give better results.
In this section, experiments on two large, almost linearly separable synthetic datasets D 1 and D 2 are presented. D is a synthetic dataset with m =4 , 500 , 000 and n =2. D 1 was generated using 9 Gaussian distributions with  X  =0 . 5 and centers on a 3  X  3 square grid. Each grid point is sepa-rated from the neighbor by 5 units. Equal number of points (500 , 000) were generated from each cluster. The labels were assigned as shown in the figure 1. As seen from the figure, the dataset is linearly separable if the label of the cluster at the center of the grid is inversed. Along with the training set D 1 a testset was also generated using the same Gaus-sian distributions. The size of testset was 450 , 000 (10% of the training set size). D 2 is a synthetic dataset with m =4 , 500 , 000 and n = 38 such that the projection of D on plane formed by first two dimensions gives D 1 . Similarly the testset for D 2 was also generated.
 The results comparing the performance of the methods on D 1 and D 2 are shown in Table 1. In case of both datasets, the SVM classifier failed to complete training, whereas CB-SOCP gave high testset accuracy with small training time. In order to evaluate the growth of training time as a func-tion of training set size, scaling experiments were performed on the datasets. Table 2,3 shows the scaling experiment results. The results show that the proposed algorithm is scalable and that the training time with CB-SOCP grows almost linearly with respect to sample size (see figure 2). In the tables,  X  X -Rate X  represents the fraction of training set,  X  X -Size X  represents the size of the sampled training set, t represents the time for clustering the training data in sec-onds, t 2 represents the time for solving (10) in seconds and t represents the total time in seconds for training. Note that the time taken for solving the optimization problem (10) was 0 . 85 sec in all cases. This is as expected, since the com-plexity of the optimization problem grows with number of clusters k rather than with the number of data points m . In this section, results on three large real world datasets  X  Web-Page, IJCNN1 and Intrusion detection are presented. The web-page dataset 5 has 49 , 749 data points in 300 di-mensions. The classification task is  X  X ext categorization X : classifying whether a web page belongs to a category or not. The IJCNN1 dataset 6 has 49 , 990 data points in 22 dimen-sions. The intrusion detection dataset 7 has 4 , 898 , 430 data points in 41 dimensions. The classification task is to build a network intrusion detector, a predictive model capable of distinguishing between  X  X ad X  connections, called intrusions or attacks, and  X  X ood X  normal connections. This dataset has 7 categorical features and 3 of them take string values. Since the proposed classifier and the SVMs work for numeri-
Training and testset available at http://research. microsoft.com/~jplatt/smo.html
Training and testset available at http://www.csie.ntu. edu.tw/~cjlin/libsvmtools/datasets/binary.html
Training and testset available at http://www.ics.uci. edu/~kdd/databases/kddcup99/kddcup99.html Table 2: Comparison of training times ( t sec) with CB-SOCP and SVM on D 1 Table 3: Comparison of training times ( t sec) with CB-SOCP and SVM on D 2 Table 4: Comparison of training times ( t sec) with CB-SOCP and SVM on Intrusion dataset cal data, these three features were removed from the training data. Hence, the final training data has 38 dimensions.
The results comparing the performance of the methods on the real world datasets are shown in Table 1. In the case of web-page and IJCNN1 datasets, the accuracies obtained us-ing CB-SOCP classifier are comparable to those obtained with SVM classifier. However, the proposed algorithm re-quires much less training time than the SVM classifier. The SVM classifier did not complete training with intrusion de-tection dataset. Whereas, CB-SOCP with small training time achieved high accuracy. Table 4 8 shows the scaling experiment results on the intrusion detection dataset. The results show that the proposed algorithm is scalable and that the training time with CB-SOCP grows almost lin-early with respect to sample size (see figure 2).
A classification method which is scalable to very large datasets has been proposed, using SOCP formulations. As-suming that the class conditional densities of positive and negative data points can be modeled using mixture models, the second order moments of the components of mixture are estimated using a scalable clustering algorithm like BIRCH. Using the second order moments, an SOCP formulation is proposed which ensures that most of the clusters are classi-
Notation used is described in section 3.2 Figure 2: Graph showing that the training time of CB-SOCP grows almost linearly with m . Solid line, dashed line and dotted line represent Intrusion, D 1 and D 2 datasets respectively. fied correctly. The geometric interpretation of the formula-error as possible. Experiments on synthetic and real world datasets show that the proposed method achieves good ac-curacy with O ( m ) training time.

As pointed in section 2.1, the optimization formulation can be extended to non-linear classifiers. However, a scal-able clustering algorithm that clusters data points in feature space needs to be built. In future, we would like to explore such clustering schemes. We would also like to explore the possibility of extending the SMO algorithm to solve the dual (14) of the proposed optimization problem. Another direc-tion of future work is to explore the possibility of designing fast nearest point algorithms to solve the dual of the hard-margin formulation (16).
The first author is supported by DST (Department of Sci-ence and Technology, Government of India) project DSTO /ECA/CB/660. [1] K. P. Bennett and E. J. Bredensteiner. Duality and [2] D. Bertsimas and J. Sethuraman. Moment problems [3] C. Bhattacharyya. Second order cone programming [4] C. Bhattacharyya, P. K. Shivaswamy, and A. J. [5] J. Bi and T. Zhang. Support vector classification with [6] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [7] T. Joachims. Making large-scale SVM learning [8] G. R. Lanckriet, L. E. Ghaoui, C. Bhattacharyya, and [9] M. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. [10] A. W. Marshall and I. Olkin. Multivariate chebychev [11] J. Mercer. Functions of positive and negative type and [12] Y. Nesterov and A. Nemirovskii. Interior Point [13] J. Platt. Fast training of support vector machines [14] J. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for [15] V. Vapnik. Statistical Learning Theory . John Wiley [16] H. Yu, J. Yang, and J. Han. Classifying large data sets [17] T. Zhang, R. Ramakrishnan, and M. Livny. Birch: An
