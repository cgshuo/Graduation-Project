 Large document repositories need to be organized, summarized and labeled in order to be used effectively. Cluster labels are essential for users to efficiently get a high-level sense of what the clusters contain, and for use as conceptual  X  X andles X  to the clusters. Without such labels, users will need to browse many documents in the clusters to get that sense. Human labeling of clusters is not viable when clustering is performed on demand or for few users. It is desirable to automatically generate cluster labels, or succinct and informative cluster de-scriptions (CDs), so that users can get that sense about the clusters by just examining the CDs. Such CDs can also be used as hints for producing final cluster labels by humans.

Much research has been done on document clustering. However, previous clus-tering algorithms mainly focused on cluster formation, and paid little attention to producing CDs. Even when CDs were generated [1, 2, 3, 4, 5], they were often just by-products of the clustering process: [1, 2, 3] use the most frequent terms as CDs, [6, 7, 4] use  X  X escriptive X  or centroid-like terms as CDs, [4] use  X  X is-criminating X  terms as CDs, and [5] use terms and their frequency distributions as CDs. Except [5], these approaches did not treat CDs as primary product to generate. Furthermore, none of them addressed the diversity factor on the terms in CDs, and the quality of CDs has not been thoroughly addressed, to the best of our knowledge. While there are approaches that produce a short summary for multiple documents by extracting some key phases or sentences [8, 9, 10], our study is focused on succinct and informative CDs consisting of a set of terms. WebelievethatsuchCDsismor e useful for cluster labeling.

We propose a CD-based classification for simulating how to interpret CDs; the corresponding classifier only uses the CDs and their associated interpretation in making classification decisions. We then propose to use the F-score of the classification to measure CD quality. This classification approach also allows us to resolve cluster competition in the interpretation process.

Using F-score directly to search for high quality CDs is too expensive. We need some  X  X urrogate X  measures of F-score for efficient search. In this paper we consider the CDD measure which combines the three factors of coverage , disjoint-ness between terms across CDs for different clusters, and diversity among terms within the CD of one cluster. Notice that diversity measures overlap among terms in the CD of one cluster, whereas disjointness measures overlap among terms in CDs of different clusters. We will argue that diversity is important in captur-ing the different flavors of a given cluster. Diversity has not been considered explicitly in previous work on CD construction.

We give a search algorithm, namely PagodaCD , for constructing CDs. Pago-daCD is a layered improvement-based replacement algorithm, and it uses the CDD surrogate quality measure. We also preselect a set of candidate terms to reduce computation cost. Experimental evaluation on subsets of the Reuters collection shows that the PagodaCD algorithm is efficient, and it can produce high quality CDs. CDs produced by PagodaCD also has the monotone quality behavior, giving higher quality CDs when more terms are in the CDs.

Organizationally, Section 2 discusses related works. Section 3 defines cluster description. Section 4 introduces our CD quality evaluation methodology. Section 5 discusses the CDD surrogate measure. Section 6 presents the PagodaCD search algorithm. Section 7 describes our experimental evaluation. Section 8 concludes. Roughly speaking, we study CD in the form of small term sets for document clusters, and address the issues of how to measure the quality of CDs and how to construct high quality CDs. Related works can be categorized as follows: Frequent-terms as CDs. Reference [3] uses frequent terms to represent clus-ters for browsing. References [1, 2] use frequent term-sets to produce a hierarchy of clusters and those frequent terms can be considered as CDs.
 Descriptive or Centroid-like CDs. In [7, 4], each cluster is described by a descriptive CD, consisting of a set of terms whose corresponding values in the centroid vector 1 are above a user-given threshold. Reference [6] describes a cluster by k objects located near the center of the cluster.
 Discriminating CDs. The Cluto clustering toolkit [4] also generates discrimi-nating CDs, which are selected from those terms that are  X  X ore prevalent in the cluster compared to the rest of the objects X (here objects mean documents). COBWEB CDs. In COBWEB [5], a conceptual clustering algorithm, each cluster is summarized by a list of attributes and associated probabilities.
Notice that these term-based approaches did not address the diversity factor on the terms in CDs. The quality of CDs as cluster labels has not been thoroughly addressed, to the best of our knowledge.
 Others. There are approaches that try to produce a short summary for multiple documents by extracting some key phases or sentences [8, 9, 10]. In contrasts, our study is focused on succinct and informative CDs consisting of a set of terms. Some of the other approaches extract information from documents based on certain pre-defined templates [11]. The filled templates can be considered as some kind of CDs. This approach involves the use of NLP and Information Extraction (IE) techniques, which is different from our term-based approach.
There are also other approaches to describing clusters for non-textual data. [12] uses  X  X ounding boxes X  plus some statistics to represent clusters; [13] uses multiple representatives in a cluster to represent the cluster; CLIQUE [14] gen-erates CDs in the form of DNF expressions. Let D be a given collection of documents. A document is a set of terms and is not treated as a bag or sequence. A clustering 2 K consists of a number L of clusters, C 1 ,C 2 , ..., C L , of all the documents in D . Roughly speaking, a CD is intended to be used as a succinct cluster label. Formally, we have: Definition 1. A cluster description (CD) for a cluster C is a set of k terms. A clustering description for a clustering K consists of L cluster descriptions CD 1 , ..., CD L ,oneforeachcluster C i .
 To allow easy interpretation, k should be a fairly small number. Constraints can be imposed on the terms in a CD. For example, we can require a CD to contain only terms that occur in its cluster. While we consider document clusters only here, one can also consider CDs for non-document clusters.

Although previous studies also considered using sets of terms as CDs, as discussed in Section 2, they have not considered the following important issues: (i) how to interpret CDs, (ii) how to measure the quality of CDs, and (iii) how to produce high-quality CDs. We will address those issues in the rest of the paper. To be useful as descriptive  X  X abels X  to clusters, CDs should allow users to get a rough picture of the contents of the clusters; they should get such a picture by looking at the CDs (but not the actual contents of the clusters) and mentally interpreting them in some natural manner. The interpretation can be viewed as a mapping from CDs to the interpreted clusters; the interpreted clusters contain what users believe are in the clusters. The amount of difference between the interpreted and the original clusters can then measure the quality of the CDs. We formalize the interpretation process and consider the CD quality below. 4.1 Interpretation Via CD-Based Classification Suppose the original clusters are C 1 , ..., C L , and their corresponding CDs are CD 1 , ..., CD L . The interpretation can be illustrated in Figure 1. The initial clus-ters are only provided to show the entire picture; users do not need to examine them during interpretation.

CD interpretation can be formalized in different ways. We believe that a natural way is the following: a user combines his/her understanding or interpre-tation of the individual terms in the CDs to form a rough picture of the clusters X  contents. We capture user interpretation of individual terms as follows. Definition 2. The interpretation of a term t w.r.t. an underlying universe S of documents, denoted as INT S ( t ), is the set of documents in S containing the term t : INT S ( t )= { d | d  X  S such that t  X  d } . We will omit S when S is the collection D of all documents under consideration.
 While INT S ( t ) is semantically the same as the concepts of tid-set, cover or SAT previously used in the literature, we use the notation of INT to emphasize that these sets are the basis of users X  perception of the terms. Notice that one can also consider other factors such as synonyms of terms when defining INT S ( t ).
When interpreting CDs, users form virtual or interpreted clusters by assigning documents to clusters based on their intuition and some  X  X ough mental reckon-ing X . Since a term t can occur in different clusters, there is competition in the interpretation of t with respect to the  X  X ight X  cluster. On the other hand, since a document d can contain terms from CDs of multiple clusters, there can be com-petition regarding which cluster to assign d to: if d contains a term t 1  X  CD 1 and a term t 2  X  CD 2 , then competition occurs since t 1 indicates that d should belong to C 1 and t 2 indicates that d should belong to C 2 .

We combine the interpretation of the terms in CDs and resolve the competi-tion to form interpretations for all clusters by using the CD-based classification approach. Here, we use the terms in the CDs as a classifier to classify documents into interpreted clusters as follows: Algorithm 1. The CD-based classification 1. For each document d and each cluster C i ,let Score ( d, C i ) be defined 3 as membership of d in C i ,where t  X  d  X  CD i . By using the union of INTs, this score uses the signal contained in any given document d exactly once. 2. A document d is assigned to the interpreted cluster C i if d has the highest highest score is zero, then d is assigned to the unknown cluster. We break ties by assigning d to the first cluster (in some fixed order) having the highest score. Collectively, the interpreted clusters C 1 , ..., C L will be referred to as the inter-pretation of the CDs using the CD-based classification approach. While Score combines terms using roughly the OR, other logical connectives can also be used.
 Example 1. We now use the example given in Table 1 to illustrate. Suppose we are given two clusters C 1 = { d 11 ,d 12 } and C 2 = { d 21 ,d 22 } ,andtwoCDs CD 1 = { a, c } and CD 2 = { g, i } (See (a)). To evaluate the quality of the given CDs, we apply our CD-based classification approach to those documents. The interpreted clusters formed by this process are shown in (b). Consider document d 22 .It contains c in CD 1 and i in CD 2 .Both d 11 and d 12 contain c ,so score ( d 22 ,C 1 )= |{ are calculated based on the contents of the original clustering. 4.2 F-Score as Measure of Quality We measure the quality of CDs by using the amount of difference between the original and the interpreted clustering. We measure the difference using F-score [15], also called F-measure .

Suppose the original clustering is K = { C 1 , ..., C L } , the corresponding CDs For each i ,the F-score for C i and C i ,denotedby F ( C i ,C i ), is defined as interpreted clustering and the original clustering is defined as the weighted av-where D =  X  L i =1 C i .Weuse F ( K , K ) as our measure of CD quality. Using F-score to directly search for good CDs is too expensive (The detailed analysis is omitted due to the space limitation). So we need to give efficient surrogate quality measures for use in the search process. In this section, we introduce one such measure, namely the CDD measure, which combines the three factors of coverage , disjointness ,and diversity .

Intuitively, coverage is used to encourage the selection of terms with high frequency (matching large number of documents) in a given cluster, disjointness is used to discourage the selection of terms with high inter-cluster overlap, and diversity is used to discourage the selection of terms with high intra-cluster overlap. Consequently, the three factors help us to capture the quality measure discussed in Section 4. 5.1 Three Factors We now discuss the three factors of coverage , disjointness ,and diversity . While the disjointness is defined on CDs for one clustering, the other two are on CDs for one cluster.
 To describe the contents of the clusters well, a CD must cover the cluster well: A good CD for a cluster C is a term set T where Cov C ( T ) is large. Definition 3. The coverage of a CD = T for a cluster C measures how well a term set T covers C , and is defined by Cov C ( T )= | t  X  T INT C ( t ) | | C | . To avoid the adverse impact of competition, the CDs for different clusters should have minimal competition against each other: good CDs for a clustering C 1 , ..., C L is a set of CDs such that Dis ( CD 1 , ..., CD L ) is large.
 Definition 4. Let CD 1 , ..., CD L be a CD for a given clustering C 1 , ..., C L . Dis-jointness measures overlap between terms in different CDs, and is defined by Dis ( CD 1 , ..., CD L )= 1 The terms in a good CD should be as different as possible (less overlap among INT s): A good CD for a cluster C is a term set T such that Div C ( T ) is large. Definition 5. The diversity of a CD = T for a cluster C measures overlap among terms of T , and is defined by Div C ( T )= 1 To see why Div C ( T ) is important, consider the cluster C depicted in Figure 2. Suppose further that overlap in (Figure 2.a) is much larger than overlap in (Figure 2.b). Metaphorically speaking, a term t can be viewed as the centroid of INT C ( t ). The centroids are much closer to each other in Figure 2.a than in Figure 2.b. As a consequence, it is much harder to synthesize the whole picture of the entire cluster using T than using T .

In general, when the centroids are close to each other, it is hard to synthesize the whole picture of the entire cluster; in contrast, when they are more widely and evenly distributed, they can be combined to offer better picture of the whole cluster. The importance of diversity can also be seen from an analogy: diversity is important [16] for the performance of classifier ensembles [17, 18], and the terms in a CD play a similar role for the collective interpretation of the CD as the committee-member classifiers in the collective classification. 5.2 The CDD Measure We now define the CDD surrogate measure in terms of the three factors. For use in the search process, we are interested in comparing two CDs, a new and an old, to determine the quality improvement offered by the new over the old. We will first define improvement for the factors, and then combine them to form improvement of the CDD measure.
 two (an old and a new) CDs for the clustering. We require that the new be obtained from the old by modifying 4 just one of the CD o i  X  X , keeping the others unchanged; let CD o j be the CD o i that is modified.

The improvement of the factors are defined as:  X  ( Cov )=  X  ( Div )= Observe that  X  ( Cov )and  X  ( Div ) are defined in terms of the cluster CD being modified, whereas  X  ( Dis ) is defined in terms of the entire clustering CDs. The CDD measure is defined in terms of the three factors. For the old CD CD o 1 , ..., CD o L and new CD CD n 1 , ..., CD n L ,the CDD improvement is defined by Observe that in the formula we took the sum of the individual improvements for the three factors and insisted that each improvement is non-negative. We can also replace  X  X um X  by  X  X ultiply X , or drop the non-negative improvement requirement; however, experiments show that these do not perform as well.
When combining multiple factors to form a quality measure, trade-off among the factors occurs. In the above formula each factor carries a constant and equal weight; one may also use different and adaptive weights. We now consider how to efficiently construct succinct and informative CDs. We will present the PagodaCD Algorithm, which is a layer-based replacement algorithm using the CDD surrogate quality measure.

A natural but naive approach to searching good CDs is to repeatedly perform the best single-term replacement among all clusters and candidate terms, until no good replacement can be found. Our experiments indicated that this method suffers from two drawbacks: it is still quite expensive, and it does not necessarily produce better CDs when the CD size increases. These drawbacks motivate us to introduce the PagodaCD Algorithm.

Roughly speaking, our PagodaCD Algorithm divides the search process into multiple major steps, working in a layered manner. Each major step corresponds to the iterative selection of some k s new terms for each CD i ; it does not replace terms selected at earlier steps. This process is level by level, and in each level all clusters are considered together. This is why the algorithm is called PagodaCD . Algorithm 2. The PagodaCD Algorithm Inputs :Clusters C 1 , ..., C L ; k (CD size); baseSize , incSize , m in Im p ; Outputs :CDs Method : 1. For each i ,set CD i to  X  ,andlet CP i consist of the most frequent 50 + k 2. IterReplace ( CP , CD , m in Im p , baseSize ); // CP and CD are vectors 3. For j =1 to k  X  baseSize i n cSize do IterReplace ( CP , CD , m in Im p , incSize ); 4. Return ( CD 1 , ..., CD L ) Parameter baseSize is the number of terms to be obtained for each CD i in the first major step, and incSize is the number of terms to be added for each CD i in each subsequent major step. Parameter m in Im p is a user given minimum quality improvement threshold.

The IterReplace procedure is used to select stepSize new terms for each CD i , while keeping the terms selected in previous levels unchanged. It first selects the most frequent stepSize unused terms from the candidate term pools (CDTL), and then use the CDD measure to repeatedly select the best replacement terms. For each iteration, it finds the best replacement term among all clusters and all terms for the current major step. This is repeated until no replacement term with significant quality improvement is found.
 Method 1. IterReplace( CP , CD , m in Im p , stepSize ) 1. For each i ,let CD T L i = { the most frequent stepSize of terms in CP i  X  2. Repeat until no replacement is found: Notice that PagodaCD uses IterReplace to do the replacement only in a lo-cal one-layer-at-a-time manner. This leads to both faster computation and the monotone-quality behavior (getting higher F-scores when CDs become larger). Due to the space limitation, we omit the complexity analysis here.

We conclude this section with some remarks on preselection of candidate terms. For large document collections, the number of unique terms can be very large. Constructing CDs from all those terms is expensive. Moreover, some terms will not contribute much to quality CDs, especially when some terms only been appeared in few documents. To address these concerns, it is desirable to select and use only a subset of terms for constructing the CDs. In this paper, we preselect a number of the most frequent terms for each C i as candidate terms. Notice that the choice of the number of candidate terms involves a trade-off between quality and efficiency. Here, we choose to have that number be  X  = 50 + k ,where k is the desired description size (or CD size) for each cluster. In this section, we present an empirical evaluation of various CD construction algorithms, including ours. The goals of the experiments are (1) to demonstrate the superior quality of CDs produced by our algorithms than those produced by other algorithms, and (2) to validate the claims that coverage , disjointness and diversity are important factors for constructing succinct and informative CDs. 7.1 Experiment Setup In this paper, we only consider CDs and assume that a clustering is given by other algorithms. We used the Cluto [4] toolkit to generate the clusterings; the clustering algorithm we used is repeated bisecting , which was shown to outper-form the basic k-means and UPGMA algorithms [19]. Below, all data sets are divided into 10 clusters, unless indicated otherwise.

We evaluate the following CD construction approaches, in addition to Pago-daCD . The  X  X escriptive CD X  and  X  X iscriminating CD X  were described in Section 2, and were generated using the Cluto package. The  X  X requency-based CD X  were simply the most frequent terms from each cluster. Finally, the  X  X OBWEB-like CD X  approach is also considered, which uses the utility category [20, 5, 21] as the search criterion and uses our PagodaCD strategy to search. 7.2 Data Sets Our experiments were performed on the Reuters-21578 [22] documents collec-tion. The collection contains 21578 news articles, distributed in 22 files. We con-structed five subsets, Reuter2k , Reuter4k , Reuter6k , Reuter8k and Reuter10k , containing 2k, 4k, 6k, 8k and 10k documents respectively, in the following man-ner: The 22 files were first concatenated in the order given. We then eliminated those documents contain little or no meaningful textual content. Finally, we got the desired number of documents from the concatenation starting from the be-ginning, i.e. Reuter2k contains the first 2000 documents from the concatenation, Reuter4k the first 4000 documents, and so on. All documents were preprocessed by removing stop-words and stemming, following common procedures in docu-ment processing. 7.3 CD Quality PagodaCD vs. Existing Approaches. We compare the CD quality of our ap-proach with other existing approaches. Figure 3 shows the average F-score of different approaches for different CD-Sizes in the Reuter8k data set. We can see that PagodaCD outperforms the Descriptive approach, which is the best among others, by at least 15% relative (or 8% absolute) percent for all description sizes. Figure 4 shows the average F-score of different approaches in Reuter2k, 4k, 6k, 8k and 10k data sets, with the description size fixed at 8. Again, the PagodaCD Algorithm outperforms the Descriptive approach by at least 10% relative (or 7% absolute) percent. For other data se ts and description sizes, the performance comparison is similar.

Interestingly, when the description size increases, the average F-score of CDs produced by PagodaCD and COBWEB-like CD also increases. However, this is not true for other approaches. Figure 3 indicates that the F-score of other approaches jumps up and down, and it even deteriorates in some cases when the description size increases.
 Table 2 shows some description terms produced by different approaches in Reuter4k when the description size is 4. We selected 2 cluster s from total of 10 clusters to save space. Although terms are in their root or abbreviated form, we can still sense that cluster 4 is about  X  X arge-scale X  bank financing and cluster 7 is about stocks. This will be more obvious to domain experts. PagodaCD and Descriptive CDs give us better sense about these topics. For Discriminating CDs, there are duplicated terms in both clusters, namely net and shr .For Frequency-based CD, inc and compani in cluster7 give redundant information.
 Impact of Clustering Quality on CD Quality. Clustering quality has big impact on CD quality. High clustering quality means that documents in a cluster are very similar to each other, but are very different from those in other clusters. It turns out that CDs constructed from high quality clusterings tend to have high quality, and those constructed from low quality clustering tend to have low quality. To demonstrate the effect of clustering quality, we produced different clusterings (5, 10, 15, 20-way) from Reuter4k. We measured the clustering quality by the weighted sum of the difference between the internal similarity and external similarity of each cluster. Interestingly, the clustering quality deteriorates when the number of clusters increases for this dataset. Figure 5 indicates that CD quality also deteriorates when clustering quality deteriorates. 7.4 Importance of the Three Factors Experiments confirmed that the three factors of coverage , disjointness and diver-sity are very important for constructing informative CDs. Indeed, if we leave any of them out, the CD quality is not as good as when all three are used. Figures 6 and 7 show the importance of different factors in terms of relative loss or gain of average F-score . Because the candidate terms are frequent terms, coverage is less important than diversity . In other experiments we observed that, when coverage is less important, the other two factors, especially diversity , are very important. We argued that constructing succinct and informative CDs is an important com-ponent of clustering process, especially for managing large document reposito-ries. We believe that succinct and informative CDs can help users quickly get a high-level sense of what the clusters contain, and hence help users use and  X  X igest X  the clusters more effectively.

We discussed and formalized how to interpret the CDs and how to resolve perception competition. We introduced a CD-based classification approach to systematically evaluate CD quality. We identified a surrogate quality measure for efficiently constructing informative CDs. We gave a layer-based replacement search method called PagodaCD for constructing CDs. Experimental results demonstrated that our method can produce high quality CDs efficiently, and CDs produced by PagodaCD also exhibits a monotone quality behavior.

For future research, we would like to do the following: (1) performing clus-tering and constructing informative CDs at the same time in order to get high quality CDs and clusterings, (2) giving the three factors different weights in dif-ferent situation, and considering new surrogate quality measures, (3) considering synonyms and taxonomy in forming CDs, (4) involving human evaluation efforts to further validate the understandability of CDs, and (5) adapting previous ideas on the use of emerging patterns and contrasting patterns for building classifiers [23, 24, 25, 26] to construct succinct and informative CDs.

