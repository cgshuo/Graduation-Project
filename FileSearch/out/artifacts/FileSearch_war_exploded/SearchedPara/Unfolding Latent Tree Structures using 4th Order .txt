 Nuclear and Frobenius norms:  X  Let  X  i be the singular values of A .Then  X  Kronecker and Khatri-Rao products: Tensor operations: l  X  L are defined as di ff erent order.
 From P to A, B, C : in our derivations is the following We can derive the formula for A starting from the element-wise formula ( 1 ) can be done using Khatri-Rao products in the following way correspond to X 2 and X 4 . We have The expression for C is derived in a similar way.
 Other representations of A, B, C : the following additional formulas, Rank properties of A, B, C : In this section we prove the rank properties used in Section 3.2 of the paper. From the lemma, it follows that i.e., generically, Algorithm 3 T next =QuartetTree( T 1 , T 2 , T 3 , X 4 ) Require: Leaf( T ): leaves of a tree T ; 1: for j =1to3 do 2: X i  X  Randomly choose a variable from Leaf( T i ) 3: end for 4: i  X   X  Quartet( X 1 , X 2 , X 3 , X 4 ), T next  X  T i  X  Algorithm 4 T =Insert( T , , T ,X i ) 1: if | Leaf( T ) | =1 then 2: T  X  Form a tree with root R connecting Leaf( T )and X i . 3: else 4: T next  X  QuartetTree(Left( T ), Right( T ), , T , X i ) 5: if T next =Left( T ) then 6: T  X  Insert( T next , Right( T )+ , T , X i ) 7: else if T next = Right( T ) then 8: T  X  Insert( T next ,Left( T )+ , T , X i ) 9: end if 10: end if 11: T  X  T + , T Algorithm 5 T = BuildTree( { X 1 ,...,X d } ) 1: Randomly choose X 1 , X 2 , X 3 and X 4 2: i  X   X  Quartet( X 1 , X 2 , X 3 , X 4 ) 4: for i =5to d do 6: if T next =Left( T ) then 7: T  X  Insert( T next , Right( T ) + Middle( T ), X i ) 8: else if T next = Right( T ) then 9: T  X  Insert( T next ,Left( T ) + Middle( T ), X i ) 10: else if T next = Middle( T ) then 11: T  X  Insert( T next , Right( T )+Left( T ), X i ) 12: end if 13: end for P want to bound the di ff erence | ! A  X  !  X   X ! A !  X  | .Wehave P Analogously, B  X  = P k .
 Therefore, we get the following upper and lower bound: If we require that then we will have ! A !  X   X ! B !  X  .
 We can derive similar condition for the relationship ! A !  X   X ! C !  X  . Let We thus obtain an upper bound on the allowed perturbation: and and thus bound on  X  depends only on pairwise marginal distributions. guaranteed to be smaller than the maximum allowed perturbation on a nedge. Let 12.1. Quartets Corresponding to a Single Edge have full rank. Let  X  min = min quarter q  X  q .From( 26 ), we have 12.2. Quartets Corresponding to a Path is where we have used P " M P end points in a path H  X  M 1  X  M 2  X  X  X  M l  X  G also has full rank. We have The reason why we do not need to perturb the term diag( P M either 1 "  X  = 0 " or  X  1 = 0 .
 below by  X  min , i.e. , Then we have The perturbation  X  on the path H  X  M 1  X  M 2  X  X  X  M l  X  G is bounded by  X  if  X  l to be small compared to the smallest excessive dependence  X  min . 13.1. Simple quartets in case of violated (A1) X equations ( 5 ), ( 6 )and( 7 ), we can conclude that and generically Thus 13.2. General quartets with hidden variables having di ff ere nt number of states be k min and let ( A4 ) all hidden variables have less than k 2 min states. X 13.3. Rank vs. nuclear norm condition succeed even if rank( A )  X  min(rank( B ) , rank( C )). Lemma 4 becomes of di ff erent number of hidden states. the finite sample nuclear norm deviates from its true quantity by % := 2 First, let q = {{ i 1 ,i 2 } , { i 3 ,i 4 }} and Then, for su ffi ciently large m , we can bound the error probability by
 Mariya Ishteva mariya.ishteva@vub.ac.be ELEC, Vrije Universiteit Brussel, 1050 Brussels, Belgium Haesun Park, Le Song { hpark,lsong } @cc.gatech.edu Discovering the latent structures from many ob-served variables is an important yet challenging learning task. The discovered structures can help better understand the domain and lead to poten-tially better predictive models. Many local search heuristics based on maximum parsimony and maxi-mum likelihood methods have been proposed to ad-dress this problem ( Semple &amp; Steel , 2003 ; Zhang , 2004 ; Heller &amp; Ghahramani , 2005 ; Teh et al. , 2008 ; Harmeling &amp; Williams , 2010 ). Their common draw-back is the lack of consistency guarantees. Further-more, the number of hidden states often needs to be determined in advance or estimated by time consum-ing cross-validations.
 We focus on latent variables models whose condi-tional independence structures are trees (called latent tree models; an example for stock data is shown in Fig. 1 ). For these models, e ffi cient algorithms with provable performance guarantees have been explored in the phylogenetic tree reconstruction community. One popular algorithm is the neighbor-joining (NJ) algorithm ( Saitou &amp; Nei , 1987 ), where pairs of vari-ables are joined recursively according to a certain dis-tance measure. The NJ algorithm is consistent when the distance measure satisfies the path additive prop-erty ( Mihaescu et al. , 2009 ). For discrete random vari-ables, the additive distance is defined using the deter-minant of the joint probability table of a pair of vari-ables ( Lake , 1994 ). However, this definition only ap-plies when the observed and the latent variables have the same number of states. If the latent variables rep-resent simpler factors with smaller number of states, the NJ algorithm can perform poorly.
 Another family of provably consistent methods is the quartet-based methods ( Semple &amp; Steel , 2003 ; Erd  X os et al. , 1999 ). These methods first resolve a set of latent relations for quadruples of observed variables (quartets), and subsequently, stitch them together to form a latent tree. A good quartet test plays an essen-tial role in these methods, as it is called repeatedly by the stitching algorithms. Recently, Anandkumar et al. ( 2011 ) proposed a quartet test using the leading k sin-gular values of the joint probability table, where k is the number of hidden states. This approach allows k to be di ff erent from the number of observed states. However, it still requires k to be given in advance. Our goal is to design a latent structure discovery al-gorithm which is agnostic to the number of hidden states, since in practice we rarely know this number. The proposed approach is quartet based, where the quartet relations are resolved based on rank prop-erties of 4th order tensors associated with the joint probability tables of quartets. The key insight is that rank properties of the tensor reveal the latent struc-ture behind a quartet. Similar observations have been reported in the phylogenetic community ( Eriksson , 2005 ; Allman &amp; Rhodes , 2006 ), but they are con-cerned about the cases where the number of hid-den states is larger or equal to the number of ob-served states. We focus instead on the cases where the number of hidden states is smaller, representing simpler factors. Furthermore, if the joint probability tensor is only approximately given (due to sampling noise) the main rank condition has to be modified. In Allman &amp; Rhodes ( 2006 ) such condition is missing and in Eriksson ( 2005 ) the condition is heuristically translated to the distance of a matrix to its best rank-k approximation. In contrast, we propose a novel nu-clear norm relaxation of the rank condition, discuss its advantages, and provide recovery conditions and finite sample guarantees. Our quartet test is easy to compute since it only involves singular value decom-position of unfolded 4th order tensors.
 Using the proposed quartet test as a subroutine, the latent tree structure can be recovered in a divide-and-conquer fashion ( Pearl &amp; Tarsi , 1986 ). For d observed variables, the computational complexity of the algo-rithm is O ( d log d ), making it scalable to large prob-lems. Under mild conditions, the tree construction algorithm using our quartet test is consistent and sta-ble to estimate given a finite number of samples. In simulations, we compared to alternatives in terms of resolving quartet relations and building the entire la-tent trees. The proposed approach is among the best performing ones while being agnostic to the number of hidden states k . The latter is an important improve-ment, since cross validation for finding k is expensive while leading to similar final results. We also applied the new approach to a stock dataset, where it discov-ered meaningful grouping of stocks according to indus-trial sectors, and led a latent variable model that fits the data better than the competitors. We focus on discrete latent variable models whose con-ditional independence structures are trees. We assume all d observed variables, { X 1 ,...,X d } , are leaves of the tree, having the same number of states, n . We also as-sume all d h hidden variables, { X d +1 ,...,X d + d the same, but unknown ,numberofstates, k ,( k  X  n ). 1 We use uppercase letters for random variables ( e.g. , X ) and lowercase letters their instantiations ( e.g. , x i Factorization of distribution. The joint distribu-tion of all d + d h variables in a latent tree model is a multi-way table (tensor) P of order d + d h . Although the tensor has O ( n d k d h ) entries, they can be computed from just a polynomial number of parameters due to the latent tree structure. That is, P ( x 1 ,...,x d + d ! i =1 P ( x i | x  X  i ), where each P ( X i | X  X  i ) is a condi-tional probability table (CPT) of a variable X i and its parent X  X  to a significant saving in terms of representation pa-rameters: we can represent exponential number of en-tries by just O ( d h k 2 + dnk )parametersfromtheCPTs. Throughout the paper, we assume that (A1) all CPTs have full column rank, k , and the marginal distribu-tions of all variables have full support. This assump-tion is needed for the identifiability of the latent vari-able models, and is common in latent tree recovery literature ( Anandkumar et al. ( 2011 )). Furthermore, it is needed only for the later rank conditions but not for the nuclear norm relaxation.
 Structure learning. Determining the tree topol-ogy is an important and challenging learning problem. The goal is to discover the latent structure based just on samples of observed variables. For simplicity and uniqueness of the topology ( Pearl , 1988 ), we assume that (A2) every latent variable has exactly 3 neigh-bors. This assumption can potentially be relaxed (  X  5 ). Quartet. A quadruple of observed variables from a latent tree T is called a quartet (Fig. 2 (a)). Given condition (A2) ,thereare3waystoconnectaquar-tet, X 1 ,X 2 ,X 3 , X 4 , using 2 latent variables H and G (Fig. 2 (b)). However, only one of the 3 quartet re-lations is consistent with T . The mapping between quartets and the tree topology T is captured in the following theorem ( Buneman , 1971 ): Theorem 1 The set of all quartet relations Q T is unique to a latent tree T ,andfurthermore, T can be recovered from Q T in polynomial time.
 Quartet-based tree reconstruction. Motivated by Theorem 1 , a family of latent tree recovery algorithms has been designed based on resolving quartet relations. These algorithms first determine one of the 3 ways how 4 variables are connected, and then join together all quartet relations to form a consistent latent tree. For a model with d observed variables, there are O ( d 4 )quar-tet relations in total (taking all possible combinations of 4 variables). However, we do not necessarily need to resolve all these quartet relations in order to recon-struct the latent tree. A small set of size O ( d log d ) will su ffi ce for the tree recovery, which makes quartet based methods e ffi cient even for problems with large d ( Pearl &amp; Tarsi , 1986 ; Pearl , 1988 ). In this paper, we design a new quartet based method. Our main con-tribution compared to previous approaches is that our method is agnostic to the number of hidden states, k , which is usually unknown in practice. In this section, we develop a test for resolving the la-tent relation of a quartet when the number of hidden states is unknown. Our approach uses information from the joint probability table of a quartet, which is a 4th order tensor. Suppose the quartet relation of X 1 ,X 2 ,X 3 and X 4 is {{ 1 , 2 } , { 3 , 4 }} , then the tensor X  X  entries are specified by P ( x 1 ,x 2 ,x 3 ,x 4 )= This factorization suggests that there exist some low rank structures in the 4th order tensor. To study the rank properties of P ( X 1 ,X 2 ,X 3 ,X 4 ), we first re-late it to the conditional probability tables, P ( X 1 | H ), ity table, P ( H, G ) (we abbreviate them as P 1 | H , P 2 | H P 3 | G , P 4 | G and P HG , respectively). Using tensor alge-bra, we have P ( X 1 ,X 2 ,X 3 ,X 4 )=  X  T 1 , T 2  X  3 , where I H and I G are 3rd order diagonal tensors of size k  X  k  X  k with diagonal elements equal to 1. The mul-tiplication  X  i denotes a tensor-matrix multiplication with respect to the i -th dimension of the tensor and the rows of the matrix, and  X   X  ,  X   X  3 denotes tensor-tensor multiplication along the third dimension of both ten-sors 3 (see illustration in Fig. 2 (c)). Next we will char-acterize the rank properties of P and then exploit them to design a quartet test for latent structure discovery. 3.1. Unfolding the 4 th Order Tensor Now we consider 3 di ff erent reshapings A, B and C of the tensor into matrices ( X  X nfoldings X ). These un-foldings contain exactly the same entires as P but in di ff erent order. A corresponds to the grouping {{ 1 , 2 } , { 3 , 4 }} of the variables, i.e. ,therowsof A cor-respond to dimensions 1 and 2 of P , and its columns to dimensions 3 and 4. B corresponds to the grouping Using Matlab  X  X  notation (see  X  A 8 for further expla-nation), Next we present useful characterizations of A, B and C , essential for understanding their connection with the latent structure of a quartet. The Kronecker prod-uct of two matrices M and M ! is denoted as M  X  M ! , and if they have the same number of columns, their Khatri-Rao product (column-wise Kronecker product), is denoted as M &amp; M ! .Then(see  X  A 9 for proof), Lemma 2 Assume that {{ 1 , 2 } , { 3 , 4 }} is the correct latent structure. The matrices A , B and C can be factorized respectively as (see Fig. 3 for illustration) The factorization of A is very di ff erent from those of B and C . First, in A , P 2 | H &amp; P 1 | H is a matrix of size n 2  X  k , and the columns of P 2 | H interact only with their corresponding columns in P 1 | H . However, in B , P 3 | G  X  P 1 | H is a matrix of size n 2  X  k 2 ,andevery column of P 1 | H interacts with every column of P 3 | G respectively (similarly for C ). Second, in A , the middle factor P HG has size k  X  k , whereas in B , the entires of P
HG appear as the diagonal of a matrix of size k 2  X  k 2 (similarly for C ). These di ff erences result in di ff erent rank properties of A, B and C which we will exploit to discover the latent structure of a quartet. 3.2. Rank Properties of the Unfoldings Given condition (A1) that all CPTs have full column rank, the factorization of A , B and C in ( 5 ), ( 6 )and( 7 ) respectively suggest that (see  X  A 9 for more details) where nnz(  X  ) denotes the number of nonzero elements. We note that the equality is attained if and only if the relationship between the hidden variables G and H is deterministic, i.e. , there is a single nonzero element in each row and in each column of P HG . In this case, the grouping of variables in a quartet can be arbitrary, and we will not consider this case in the paper. More specifically, we have Theorem 3 Assume P HG has a few zero entries, then k ' k 2  X  nnz( P HG ) and thus The above theorem reveals a useful di ff erence between the correct grouping of variables and the two incor-rect ones. Furthermore, this condition can be easily verified: Given P we can check the rank of its ma-trix representations A, B and C and thus discover the latent structure of the quartet. 3.3. Nuclear Norm Relaxation In practice, due to sampling noise, all three matri-ces A, B and C would be full rank, so the rank condition cannot be applied directly. To deal with this, we relax the rank condition using nuclear norm ) M )  X  = values of an ( n  X  n ) matrix M .Insteadofcompar-ing the ranks of A, B and C , we look for the matrix with the smallest nuclear norm and declare the latent structure corresponding to it. This simple quartet al-gorithm is summarized in Algorithm 1 .
 Note that Algorithm 1 works even if the number of hid-den states, k , is a priori unknown. This is an important advantage over the idea of learning the structure based on additive distance ( Lake , 1994 ), where k is assumed to be the same as the number of states, n ,oftheob-served variables, or over a recent approach based on Algorithm 1 i  X  =Quartet( X 1 , X 2 , X 3 , X 4 ) 1: Estimate $ P ( X 1 ,X 2 ,X 3 ,X 4 )fromasetof 2: Unfold $ P in three di ff erent ways into matrices $ A , quartet test ( Anandkumar et al. , 2011 ), where k needs to be specified in advance.
 In our current context, nuclear norm has a few use-ful properties. First, it is the tightest convex lower bound of the rank of a matrix ( Fazel et al. , 2001 ). This is why 4 it is meaningful to compare nuclear norms instead of ranks. Second, it is easy to com-pute: a standard singular value decomposition will do the job. Third, it is robust to estimate. The nu-clear norm of a probability matrix $ A based on sam-ples is nicely concentrated around its population quan-tity ( Rosasco et al. , 2010 ). Given a confidence level 1  X  2 e  X   X  , an estimate based on m samples satisfies Fourth, the nuclear norm can be viewed as a measure of dependence between two pairs of variables. For instance, A corresponds to grouping {{ 1 , 2 } , { 3 , 4 }} , and ) A )  X  measures the dependence between the com-pound variables { X 1 ,X 2 } and { X 3 ,X 4 } .Inthecom-munity of kernel methods, A is treated as a cross-covariance operator between { X 1 ,X 2 } and { X 3 ,X 4 } , and its spectrum has been used to design various de-pendence measures, such as Hilbert-Schmidt Indepen-dence Criterion, which is the sum of squares of all sin-gular values ( Gretton et al. , 2005a ), and kernel con-strained covariance, which only takes the largest singu-lar value ( Gretton et al. , 2005b ). Intuitively, our quar-tet test says that: if we group the variables correctly, then cross group dependence should be low, since the groups are separated by two latent variables; however if we group the variables incorrectly, then cross group dependence should be high, since similar variables ex-ist in the two groups. Since nuclear norm is just a convex lower bound of the rank, there might be situations where the nuclear norm does not satisfy the same relation as the rank. That is, it might happen that rank( A )  X  rank( B )but ) A )  X   X ) B )  X  . Next, we present su ffi cient conditions under which nuclear norm returns successful test. When latent variables H and G are indepen-dent ,rank( P HG ) = 1, since P HG = P H P % G ( P ( h, g )= tet relation. We can obtain simpler characterizations of the 3 unfoldings of P ( X 1 ,X 2 ,X 3 ,X 4 ), denoted as A  X  , B  X  and C  X  respectively. Using Lemma 2 and the independence of H and G (see appendix, ( 27 ) X ( 28 )), A B and rank( A  X  )=1 ' rank( B  X  ) which is consistent with Theorem 3 . Furthermore, since A  X  has only one nonzero singular value, we have ) A  X  )  X  = ) A  X  ) F = ) B  X  ) F  X ) B  X  )  X  (using ) M ) F  X ) M )  X  for any matrix M ). Similarly, C  X  = P 43  X  P 12 and ) A  X  )  X   X ) C  X  )  X  Then we know for sure that the nuclear norm quartet test will return the correct topology.
 When latent variables H and G are not inde-pendent , we treat it as perturbation  X  away from the independent case, i.e. , &amp; P HG = P H P % G +  X  . The size of  X  quantifies the strength of dependence between H and G . Obviously, when  X  is small, e.g. ,  X  = 0 , we are back to the independence case and it is easy to discover the correct quartet relation; when it is large, e.g. ,  X  = I  X  P H P % G , H and G are determin-istically related and the di ff erent groupings are indis-tinguishable. The question is how large can  X  be while still allowing the nuclear norm quartet test to find the correct latent relation.
 First, from the definition of  X  , we have  X  1 = 0 , and  X  % 1 = 0 ,where 1 and 0 are vectors of all ones and all zeros. Thus, the perturbation  X  does not a ff ect the marginal distributions P H and P G , since &amp; P {{ 1 , 2 } , { 3 , 4 }} is the correct quartet relation,  X  does not a ff ect the pairwise marginal distribution P 12 nei-ther, since P 12 = P 1 | H diag( P H ) P % 2 | H and the marginal P
H is the same before and after the perturbation. Sim-ilar reasoning also applies to P 34 = P 3 | G diag( P G ) P We define excessive dependence of the correct and in-correct groupings as It quantifies the changes in dependence when we switch from incorrect groupings to the correct one (in the case when H and G are independent). Note that  X  is measured only from pairwise marginals ( 11 )-( 12 ), P 12 and P 34 . Using matrix perturbation analysis we can show that (see appendix  X  11 for proof) Lemma 4 If )  X  ) F  X   X  k 2 + k ,theminimumof ) A )  X  , ) B )  X  and ) C )  X  will reveal the correct quartet relation. Thus, if the excessive dependence  X  is large compared to the number of hidden states, the size of the al-lowable perturbation can be correspondingly larger. In other words, if the dependence between variables within the same group is strong enough compared to the dependence across groups, we allow for larger  X  and stronger dependence between hidden variables H and G (which is closer to the indistinguishable case). It is di ffi cult to directly compare our recovery con-ditions with previous work, since we are addressing the more di ffi cult case where latent state k is un-known. Our recovery condition constrains the corre-lation between hidden variables based on observable quantity  X  and the number of latent states k , while those of Anandkumar et al. ( 2011 )assumetheunob-served correlation  X  between latent variables is given. Last, under the recovery condition in Lemma 4 ,and given m i.i.d. observations, we can obtain the following guarantee for the quartet test (see appendix,  X  14 for proof). Let  X  = min { ) B )  X   X ) A )  X  , ) C )  X   X ) A )  X  Lemma 5 With probability 1  X  8 e  X  1 32 m  X  2 ,Algorithm 1 returns the correct quartet relation. We can use the resolved quartet relations (Algo-rithm 1 ) to discover the structure of the entire tree via an incremental divide-and-conquer algo-rithm ( Pearl &amp; Tarsi , 1986 ; Pearl , 1988 ), summarized in Algorithm 2 (further details in appendix  X  10 ). Join-ing variable X i +1 to the current tree of i leaves can be done with O (log i ) tests. This amounts to performing O ( d log d ) quartet tests for building an entire tree of d leaves, which is e ffi cient even if d is large. Moreover, this algorithm is consistent ( Pearl &amp; Tarsi , 1986 ). Tree recovery conditions and guarantees. When a quartet is taken from a latent tree, each edge of the quartet corresponds to a path in the tree in-volving a chain of variables (Fig. 2 (a)). We need to bound the perturbation to each single edge of the tree such that joint path perturbations satisfy edge perturbation conditions from Lemma 4 .Foraquar-tet q = {{ i 1 ,i 2 } , { i 3 ,i 4 }} corresponding to a single edge between H and G , denote the excessive depen-dence by  X  q . By adding perturbation  X  q of size smaller than  X  q k 2 + k to P H P % G we can still correctly Algorithm 2 T = BuildTree( X 1 ,...,X d ) 1: Connect any 4 variables X 1 , X 2 , X 3 , X 4 with 2 2: for i =4 , 5 ,...,d  X  1 do { insert ( i +1) -th leaf X 3: Choose root R that splits T into sub-trees 4: Choose any triplet ( X i 1 ,X i 2 ,X i 3 ) of leaves from 5: Test which sub-tree should X i +1 be joined to: 6: Repeat recursively from step 3 with T := T i  X  . 7: end for recover q .Let  X  min := min quartet q  X  q .Ifwere-quire )  X  q ) F  X   X  min k 2 + k , all such quartet relations will be recovered successfully. If we further restrict the size of the perturbation by the smallest value in a marginal probability distribution of a hidden variable,  X  antee that all quartet relations corresponding to a path between H and G can also be successfully recovered by the nuclear norm test (see appendix  X  12 ). The intu-itive interpretation of  X  min is that if a hidden state rarely occurs (small probability), samples for the ob-served variables contain very little information about the hidden variable. It becomes harder to identify the latent structure in this case and hence smaller pertur-bation away from independence is allowed. Therefore, all quartets q in a tree.
 Theorem 6 Given condition (A1) X (A3) and poplu-ation quantities, algorithm 2 returns the correct tree topology.
 The recovery conditions guarantee that all quartet re-lations can be resolved correctly and simultaneously. Then a consistent algorithm using a subset of the quar-tet relations should return the correct tree structure. We note that condition (A2) could be relaxed to allow hidden variables to have more than 3 neighbors. In this case, instead of using the minimum of the the nuclear norm of A , B and C for quartet tests, we may need to consider their di ff erences, e.g. , ) A )  X   X ) B )  X  , to decide whether to join the observed variables with one or with two variables, as in Anandkumar et al. ( 2011 ). This is left as our future work. Last, given m i.i.d. sam-ples, we have the following statistical guarantee for the Algorithm 2 (see appendix,  X  15 for proof). Let  X  Theorem 7 Given condition (A1) X (A3) and m samples, Algorithm 2 recovers the correct tree topol-ogy, with probability 1  X  8  X  c  X  d log d  X  e  X  1 32 m  X  2 We note that the conditions needed for our re-sults are stronger than those used by previous work ( Anandkumar et al. ( 2011 )). This is partly due to the fact that our method deals with a more di ffi cult case where we do not know the number of hidden states. Another reason is that our analysis relies on the simple reconstruction algorithm by Pearl &amp; Tarsi ( 1986 ). There are better quartet based algorithms for building latent trees with stronger statistical guaran-tees, e.g. Erd  X os et al. ( 1999 ). We can adapt our nu-clear norm based quartet test to those algorithm as well. However, this is not the main focus of the paper. We choose the divide-and-conquer algorithm due to its simplicity, ease of analysis and it illustrates well how our quartet recovery guarantee can be translated into a guarantee for latent tree reconstruction. We compared our algorithm with the neighbor-joining algorithm (NJ) ( Saitou &amp; Nei , 1987 ), a quar-tet based algorithm of Anandkumar et al. ( 2011 ) (Spectral@ k ), the Chow-Liu neighbor Joining algo-rithm (CLNJ) ( Choi et al. , 2011 ), and an algorithm of Harmeling &amp; Williams ( 2010 )(HW).
 NJ proceeds by recursively joining two variables that are closest according to an additive distance defined as d ij = 1 2 log det diag P i  X  log | det P ij | + 1 2 log det diag P where  X  X et X  denotes determinant,  X  X iag X  is a diago-nalization operator, P ij denotes the joint probability table P ( X i ,X j ), and P i and P j the probability vector P ( X i )and P ( X j ) respectively ( Lake , 1994 ). When P ij has rank k&lt;n , log | det P ij | is not defined, NJ can perform poorly. Spectral@ k uses singular values of P ij to design a quartet test ( Anandkumar et al. , 2011 ). For instance, if the true quartet configu-ration is {{ 1 , 2 } , { 3 , 4 }} as in Fig. 2 (b), then the quartet needs to satisfy max { Based on this relation, a confidence interval based quartet test is designed and used as a subroutine for a tree reconstruction algorithm. Spectral@ k can handle cases with k&lt;n , but still requires k as an input. We will show in later experiments that its performance is sensitive to the choice of k . CLNJ first applies Chow-Liu algorithm ( Chow &amp; Liu , 1968 ) to obtain a fully observed tree and then proceeds by adding latent variables using neighbor joining algorithm. The HW algorithm is a greedy algorithm to learn binary trees by iteratively joining two nodes with a high mutual information. The number of hidden states is automatically determined in the HW algorithm and can be di ff erent for di ff erent latent variables. 6.1. Resolving Quartet Relations We compared our method to NJ and Spectral@ k in terms of their ability to recover the quartet relation among four variables. We used quartet with three dif-ferent configurations for the hidden states: (1) k H =2, k
G = 4 (small di ff erence); (2) k H =2, k G = 8 (large di ff erence); and (3) k H = k G = 4 (no di ff erence). In all cases, the number of observed states was fixed to n = 10. We always started from P HG corresponding to H  X  G ,but P X cally related X i and H (similarly for P X turbed them using the following formula P ( a = i | b )= drawn from Uniform[0 , X  ]. We then drew random sam-ple from the quartet according to these CPTs. We studied the percentage of correctly recovered quartet relations as we varied the sample size across S = { 50 , 100 , 200 , 300 , 400 , 500 , 750 , 1000 , 1500 , 2000 } and un-der two di ff erent levels of perturbation (  X  = { 0 . 5 , 1 } ). We randomly initialized each experiment 1000 times and report the average quartet recovery performance and the standard error in Fig. 4 .
 The proposed method compares favorably to NJ and Spectral@ k . The performance of Spectral@ k varies a lot depending on the chosen number of singular val-ues k . Our method is free from tuning parameters and often stays among the top performing ones. Es-pecially when the number of hidden states are very di ff erent from each other ( k H =2and k G =8), our method is leading the second best by a large gap (Fig. 4(b) and 4(e) ). When both hidden states are the same ( k H = k G = 4), the Spectral@ k achieves the best performance when the chosen number of sin-gular values k is the same as k H . Note that allowing Spectral@ k to use di ff erent k resembles using cross val-idations for finding the best k . It is expensive while our approach performs almost indistinguishable from Spectral@ k even it choose the best k . 6.2. Discovering Latent Tree Structure We used di ff erent tree topologies and sample sizes in this experiment. We generated tree topologies by ran-domly splitting 16 observed variables recursively into two groups. The recursive splitting stops when there are only two nodes left in a group. We introduced a hidden variable to join the two partitions in each recursion and this gives a latent tree structure. The topology of the tree is controlled by a single splitting parameter  X  which controls the relative size of the first partition versus the second. If  X  is close to 0 or 1, we obtain trees of skewed shape, with long path of hidden variables. If  X  is close to 0 . 5, the resulting latent trees are more balanced. In our experiments, we experi-mented with skewed latent trees  X  =0 . 2 and balanced trees  X  =0 . 5. We first generate di ff erent random k between 2 and 8 for the hidden states, and then gen-erate the probability models for each tree using the same scheme as in our previous experiment. Here we experimented with perturbation level  X  = { 0 . 2 , 0 . 5 , 1 } . We varied the sample size across S = { 50 , 100 , 200 , 500 , 1000 , 2000 } ,andmeasuredthe error of the constructed tree using Robinson-Foulds metric ( Robinson &amp; Foulds , 1981 ). This measure is a metric over trees of the same number of leaves. It is defined as ( a + b )where a is the number of partitions of variables implied by the learned tree but not by the true tree and b is the number of partitions of the variables implied by the true tree but not by the learned tree (similar spirit to precision and recall). The tree recovery results are shown in Fig. 5(a) -5(f) . Again we can see that our proposed method com-pares favorably to existing algorithms. Throughout the 6 experimental conditions, the tensor approach and spectral@2 performed the best with su ffi ciently large sample sizes. Note that we tried out di ff erent k for Spectral@ k which resembles using cross valida-tions for finding the best k . Even in this case, our approach works comparably without having to know k . Harmeling-William X  X  algorithm performed well in small sample sizes, while CLNJ does not perform well in these experimental conditions. 6.3. Understanding Latent Relations of Stocks We applied our algorithm to discover a latent tree structure from a stock dataset. Our goal is to under-stand how stock prices X i are related to each other. We acquired closing prices of 59 stocks from 1984 to 2011 (from www.finance.yahoo.com), which provides us 6800 samples. The daily change of each stock price is discretized into 10 values, and we applied our al-gorithm to build a latent tree. A visualization of the learned tree topologies and discovered groupings are shown in Fig. 1 .
 We see nice groupings of stocks according to their in-dustrial sectors. For instance, companies related to petroleum, such as CVX (Chevron), XOM (Exxon Mobil), APA (Apache), COP (ConocoPhillips), SLB (Schlumberger) and SUN (Sunoco), are grouped into a subtree. Pharmaceutical companies, such as MRK (Merck), PFE (Pfizer), BMY (Bristol Myers Squibb), LLY (Eli Lilly), ABT (Abbott Laboratories), JNJ (Johnson and Johnson) and BAX (Baxter Interna-tional), are all grouped into a subtree.
 We also compared di ff erent algorithms in terms of held-out likelihood. We first randomized the data 10 times, and each time used half for training and half for computing the held-out likelihood. Then we estimated the latent binary tree structures using di ff erent algo-rithms. Finally, we fit latent variable models to the discovered structures. The number of the states for all hidden variables, k , were the same in each latent vari-able model. We experimented with k =2 , 4 , 6 , 8 , 10 to simulate the process of using cross validation to se-lect the best k . The results are presented in Table 1 . Note that Harmeling-William X  X  algorithm automati-cally discovers k , so it does not use the experimental parameter k . The Chow-Liu tree does not contain any hidden variables and hence just one number in the ta-ble. CLNJ and Neighbor-joining assume the states for the hidden and observed variables are the same during structure learning. However, in parameter fitting, we can still use di ff erent number of hidden states k .In this experiment, the structure produced by our tensor approach produced the best held-out likelihood. We propose a quartet-based method for discovering the tree structures of latent variable models. The prac-tical advantage of the new method is that we do not need to pre-specify the number of the hidden states, a quantity usually unknown in practice. The key idea is to view the joint probability tables of quadruple of variables as 4th order tensors and then use the spectral properties of the unfolded tensors to design a quartet test. We provide conditions under which the algorithm is consistent and its error probability decays exponen-tially with increasing the sample size. In both simu-lated and a real dataset, we demonstrated the useful-ness of our methods for discovering latent structures. While in this study we focus on the properties of the 4th order tensor and its various unfoldings, we believe that properties of tensors and methods and algorithms from multilinear algebra will allow to address many other problems arising from latent variable models. Allman, E. S. and Rhodes, J. A. The identifiability of tree topology for phylogenetic models, including covarion and mixture models. Journal of Computa-tional Biology , 13(5):1101 X 1113, 2006.
 Anandkumar, A., Chaudhuri, K., Hsu, D., Kakade,
S., Song, L., and Zhang, T. Spectral methods for learning multivariate latent tree structure. In Neural Information Processing Systems ,2011.
 Buneman, P. The recovery of trees from measures of dissimilarity. In Hodson, F.R., Kendall, D.G., and
Tautu, P. (eds.), Mathematics in the archaeological and historical sciences , pp. 387 X 395. Edinburgh Uni-versity Press, 1971.
 Carroll, J. and Chang, J. Analysis of individual di ff er-ences in multidimensional scaling via an N-way gen-eralization of  X  X ckart-Young X  decomposition. Psy-chometrika , 35(3):283 X 319, 1970.
 Choi, M., Tan, V., Anandkumar, A., and Willsky, A. Learning latent tree graphical models. Journal of Machine Learning Research , 12:1771 X 1812, 2011. Chow, C., and Liu, C. Approximating discrete prob-ability distributions with dependence trees. IEEE
Transactions on Information Theory , 14:462 X 467, 1968.
 Erd  X os, P. L., Sz  X ekely, L. A., Steel, M. A., and Warnow., T. J. A few logs su ffi ce to build (almost) all trees:
Part II. Theoretical Computer Science , 221:77 X 118, 1999.
 Eriksson, N. Tree construction using singular value de-composition. In Pachter, L. and Sturmfels, B. (eds.),
Algebraic Statistics for Computational Biology ,pp. 347 X 358. Cambridge University Press, 2005. URL http://dx.doi.org/10.1017/CBO9780511610684 .
 Fazel, Maryam, Hindi, Haitham, and Boyd, Stephen P.
A rank minimization heuristic with application to minimum order system approximation. In American Control Conference ,pp.4734 X 4739,2001.
 Grasedyck, L. Hierarchical singular value decomposi-tion of tensors. SIAM J. Matrix Anal. Appl. ,31(4): 2029 X 2054, 2010.
 Gretton, A., Bousquet, O., Smola, A. J., and Sch  X olkopf, B. Measuring statistical dependence with
Hilbert-Schmidt norms. In Jain, S., Simon, H. U., and Tomita, E. (eds.), Proceedings of the Interna-tional Conference on Algorithmic Learning Theory , pp. 63 X 77. Springer-Verlag, 2005a.
 Gretton, A., Herbrich, R., Smola, A. J., Bousquet, O., and Sch  X olkopf, B. Kernel methods for measuring in-dependence. Journal of Machine Learning Research , 6:2075 X 2129, 2005b.
 Harmeling, S. and Williams, C. Greedy learning of binary latent trees. IEEE Transactions on Pattern
Analysis and Machine Intelligence ,pp.1087 X 1097, 2010.
 Harshman, R. A. Foundations of the PARAFAC pro-cedure: Model and conditions for an  X  X xplanatory X  multi-mode factor analysis. UCLA Working Papers in Phonetics , 16(1):1 X 84, 1970.
 Heller, K. A. and Ghahramani, Z. Bayesian hier-archical clustering. In Proceedings of the Interna-tional Conference on Machine Learning ,pp.297 X  304, 2005.
 Lake, J.A. Reconstructing evolutionary trees from dna and protein sequences: paralinear distances. Pro-ceedings of the National Academy of Sciences ,91 (4):1455, 1994.
 Mihaescu, R., Levy, D., and Pachter, L. Why neighbor-joining works. Algorithmica , 54(1):1 X 24, 2009.
 Oseledets, I. V. Tensor-train decomposition. SIAM
Journal on Scientific Computing , 33:2295 X 2317, 2011.
 Parikh, A., Song, L., and Xing, E. P. A spectral al-gorithm for latent tree graphical models. In Pro-ceedings of the International Conference on Machine Learning ,2011.
 Pearl, J. Probabilistic Reasoning in Intelligent Sys-tems: Networks of Plausible Inference .Morgan Kaufman, 1988.
 Pearl, J. and Tarsi, M. Structuring causal trees. Jour-nal of Complexity , 2(1):60 X 77, 1986.
 Robinson, D.F. and Foulds, L.R. Comparison of phy-logenetic trees. Mathematical Biosciences ,53(1-2): 131 X 147, 1981.
 Rosasco, L., Belkin, M., and Vito, E.D. On learning with integral operators. Journal of Machine Learn-ing Research , 11:905 X 934, 2010.
 Saitou, N. and Nei, M. The neighbor-joining method: a new method for reconstructing phylogenetic trees.
Molecular Biology and Evolution , 4(4):406 X 425, 1987.
 Semple, C. and Steel, M.A. Phylogenetics , volume 24. Oxford University Press, USA, 2003.
 Teh, Yee Whye, Daume, Hal, and Roy, Daniel. Bayesian agglomerative clustering with coalescents.
In Advances in Neural Information Processing Sys-tems 22 ,2008.
 Zhang, N. L. Hierarchical latent class models for clus-ter analysis. Journal of Machine Learning Research ,
