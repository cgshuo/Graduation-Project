 With the development of Internet and Web 2.0, users have to face the informa-tion overload problem when searching for what they are interested in from the massive data. Recommender systems can benefit users by presenting personal-ized recommendations based on their preferences and the available information. Furthermore, a successful recommender system facilitates the interaction of cus-tomers in online communities and promotes the development of e-commerce. In recent decades, many recommendation algorithms have been proposed in dif-ferent application scenarios [ 1  X  3 ]. Among the existing methods, collaborative filtering (CF) [ 4 ] approaches utilize users X  implicit or explicit feedbacks to give customized recommendations without domain knowledge, while content-based [ 5 , 6 ] approaches need to use the profiles of items and users. CF methods have become one of the most widely used recommendation approaches.
 Typically, CF methods can be classified into two main categories. The first, referred to as neighborhood-based methods [ 7  X  10 ], are centered on computing the similarities between users/items. And subsequently missing ratings are pre-dicted based on these similarities. However, with the increasing number of users and items, the complexity is too high to make online recommendations. The second, referred to as model-based methods [ 11  X  13 ], employ a machine learning algorithm to build a model which transforms features of both items and users to the same latent factor space.
 In real world scenarios, users typically provide feedbacks to only a few items out of thousands of items. So the user-item rating matrix is quite sparse and the available few feedbacks are particularly significant. Therefore, it is not easy for traditional methods like ItemKNN to find the transitive relations between items which have not been co-rated by at least one user. Meanwhile, the model-based methods like NSVD alleviate this problem by projecting the matrix onto a low dimensional space. However, there is an inherent limitation that they fail to capture the accurate inferences of the users by treating all rated items equally without discrimination. As a result, they ignore the differences among items and weaken the impact of items that have unique characteristics, which may lead to poor performance.
 In general, items can be classified into two main categories for a certain user, namely the favourite ones and the disliked ones. Correspondingly in recommen-dation system, we can divide all the rated items into two different parts for one user, i.e., positive examples and negative examples, depending on whether the rating value is above the average rating of the user or not. By distinguishing the positive examples from the negative examples, the rare feedbacks can be fully utilized and the user X  X  interest can be excavated more accurately. The positive feedbacks of a user are the examples of items which this user will prefer to. On the contrary, the negative feedbacks are the representive items which this user will dislike. In other words, for a target user, the predicted rating of an item should be boosted if it is similar to most of the user X  X  positive feedbacks and diminished if it is similar to most of the user X  X  negative feedbacks. Based on the above analysis, in this paper, we propose a novel approach that combines both the positive and the negative feedbacks to learn latent item-item similarity matrix. This method is a new variant of item-oriented CF along with matrix factorization techniques. In our approach, for a target user, the predicted rating of an unrated item is computed based on the learned similarities of this item with other positive examples and negative examples. To evaluate the performance of our proposed method, we conduct extensive experiments on MovieLens and EachMovie datasets. The experimental results shows that it X  X  a most convenient and effective way to infer user preferences by utilizing classified feedbacks. It also confirms that our method is superior to the state-of-the-art methods not only in ratings predictions but also in top-N recommendation. negative examples to learn latent item-item similarities. In this way, it can more accurately capture the user X  X  preference and make the favorite items rank in the top especially on a small scale. factor model to excavate the transitive relations between items. The item-item similarity matrix can be learned as an inner production of two low dimensional item latent factor matrices. verify the effect of classified examples. Then, we investigate and analyze the impact of some parameters referred to similarity agreement and dimension. the baseline estimate method. Section 3 provides a detailed description for the modeling, learning and prediction process of the proposed model. We give a brief review of the datasets and evaluation metrics in section 4 . The experimental results are reported in section 5 . Finally, we conclude the paper and present some directions for future work in section 6 . In recommender systems, some users tend to give higher ratings and some items receive higher ratings than others. As a result, typical CF data exhibit enlarges the effects of those users and items. In order to adjust the data by accounting for these effects, it is customary to encapsulate them in the baseline estimate function [ 8 ] as follows: biases. The overall average rating is denoted by  X  . The parameters b the user and item biases, respectively, which indicate the observed deviations of user u and item i from the average. For example, if we want to estimate the baseline for a movie X  X  rating like Gone with Wind by user Smith, we get the value of  X  (the average rating over all movies) as 3.3 stars at first. Because Gone with Wind is a better movie, its rating is higher than the average with 0.8 stars. Regarding to personal preference, Smith prefers to rate 0.4 stars lower than the average as a critical user. Above all, we will estimate the baseline for Gone with Wind by Smith 3.7 stars through calculating 3.3 -0.4 + 0.8 in terms of Eqution 1 . Hence, the parameters of this model are estimated to solve the least squares problem: where R is the set of observed ratings and r ui is the ground truth value. The first term aims to find the best fit values (e.g., b u , b The latter is the regularization term that can avoid overfitting by penalizing the magnitudes of these parameters. In this section, we propose a new item-based latent factor model which can learn latent item correlations by considering users X  positive feedbacks and negative feedbacks. For convenience, we named the proposed method PNSM (Combining P ositive and N egative Feedbacks with Factored S imilarity M atrix for Recom-mender Systems). 3.1 Combining Positive Feedbacks with Factored Similarity Matrix (PNSM 1 for short) In reality, the user-item rating matrix is very sparse, since users usually pro-vide feedbacks to only a handful of items out of thousands or millions of items. Traditional methods based on similarity rely on so few neighborhood relations that cannot capture the dependencies between items especially which have not been co-rated by at least one user. To overcome this problem, methods based on matrix factorization like NSVD thereby implicitly learn better relationships between items. However, these methods treat all rated items as the positive examples equally. But in fact, for one user, her rated items can be sorted into the favorite items and the dislike ones (corresponding to the positive and the negative examples respectively) depending on whether the rating value is above the average or not. Naturally the feedbacks (the positive feedbacks and the nega-tive feedbacks) of these two kind of items should have a different effect on rating prediction.
 To further explore the different influence between feedbacks, first we only take the positive feedbacks into consideration in modeling process and then introduce how it can benefit recommender systems. The positive feedbacks can facilitate the user X  X  preference, therefore for an item the predictive rating can be boosted if it is similar to most of the positive examples (items that ratings are higher than the average of the user). The estimated value for a given user u on item i is computed as: Importantly, the b ui is derived as explained in Sec. 2 . R ( u ) is the set of items rated by user u . R ( u ) + is the set of the positive examples whose ratings are higher than the average rating of the user u and the number of items in this set inner product can be regarded as the similarity to the item whose rating is being estimated. Specially, p j is the positive feedback learned from the positive item space matrix where item X  X  rating is higher than the average rating of the user. The parameter  X  is a user specified parameter between 0 and 1, and it controls the number of neighborhood items that are similar to the positive examples. Afterwards, the term | R ( u ) + |  X   X  can represent the degree of agreement to their similarity weight.
 on an item i is calculated as an aggregation of the positive examples with the corresponding product of p j latent vectors from R ( u ) + method is named PNSM 1 for short. The essential difference between PNSM NSVD is that a flexible coefficient  X  is proposed to compromise the influence of their similarity, as the domain is focus on the positive examples rather than mixed records.
 regularized optimization problem: where  X  r ui is the estimated value for user u and item i (as in Equation 3 ). The regularization terms are used to avoid overfitting and  X  is the regularization weight for latent factor vectors, user bias vector and item bias vector. 3.2 Combining Positive and Negative Feedbacks with Factored In PNSM 1 , we only integrate the positive feedbacks in rating prediction. How-ever, as referred to before, the negative feedbacks also make a difference in recommendation process. In other words, the item can receive a lower score if it X  X  most similar to the negative examples (items that the user rated lowly). In general terms, the positive feedbacks can facilitate the prediction while the neg-ative feedbacks can weaken it on the contrary. There is an assumption that for a certain user if the item is similar to most of the positive/negative examples, the user appreciates/dislikes this item as well. Hence to take full advantages of both feedbacks and discriminate them, we exploit a more comprehensive approach basedonPNSM 1 . Such method can explain the recommendations in terms of all items previously rated by users and we abbreviate it to PNSM provide accurate evaluation, the prediction of the rating that user u gives to item i can be inferred as: where y k is the negative feedback learned from item space matrix where item X  X  rating is lower than the average rating of the user, while p ratings are lower than the average rating of user u and the number of items in this set is denoted by | R ( u )  X  | . In addition, parameter  X  is similar to  X  and both of them are user specified parameters between 0 and 1. Their values are dependent on the properties of datasets and the best performing values can be determined empirically.
 In a sense, Equation 5 provides a two tier model for prediction. The first tier (i.e., b ui =  X  + b u + b i ) generally describes attributes of users and items, without taking account of any involved interactions. The rest tier illustrates the interaction among item profiles and can excavate implicit transitive relations. It can also be split up into two parts in detail.
 Formally, the former part, | R ( u ) + |  X   X  j  X  R ( u ) + tion that the positive examples provide. More specifically, given an item i and taking any positive example j , their inner product ( q T the similarity weight to boost the prediction. Therefore, the item will receive a high score by adding the aggregation of positive similarity weights to the formula. Namely, we can make the favorite items rank as top as possible and recommend the most likely items to users. In comparison, the latter part, |
R ( u ) tive examples. Concretely, as a negative similarity weight, the inner product of a certain item i and a negative example j can diminish the prediction. In this case, we subtract the cumulative values of these similarity weights to weaken the influence of negative items in the model. Besides, we use |
R ( u )  X  |  X   X  to control the degree of agreement to the positive similarity weights and the negative similarity weights respectively. This is also one of the important differences between PNSM and NSVD.
 To better understand, the former part are the cumulative similarities between item i and all of the positive examples in R ( u ) + when  X  = 0. Under the cir-cumstances, item i can be rated high, even though only one positive example is similar to it. Considering another case in which  X  = 1, these part amount to the average similarities between i and the positive examples in R ( u ) i can receive a high score when almost all of the items in R ( u ) it. As far as we know, these are two extreme cases with different settings. In principle, it is usually that the right choice will be somewhere in between. So do the parameter  X  and the latter part.
 Model parameters are estimated at a pre-processing stage. In general, the values of involved parameters are determined by minimizing the associated reg-ularized squared error function: The residual sum of squares, ( r ui  X   X  r ui ) 2 , measures how well the linear model fits the training data. The regularization terms are used to avoid overfitting and  X  is the regularization weight for biases and several latent factor vectors. The optimization problem can be solved by employing a stochastic gradient descent method. The derivatives of the parameters are as follows: where e ui In each iteration, we modify the parameters by moving in the opposite direction of the gradient.
 we can see, latent factor vectors are all initialized with small random values that are non-negative. We would repeat the iteration until the number of iterations has reached a predefined threshold or the error on the validation set tends to convergence. 4.1 Datasets In this research, we evaluated the performance of the recommender algorithms on two different real datasets, including MovieLens 1 , EachMovie subset of data obtained from the MovieLens research project. It is an available movie rating dataset and all collecting ratings are in the scale of 1-to-5 star. It contains 100000 ratings by 943 users on 1682 different items. Each user had rated at least 20 movies. For further research, we chose a subset of users who rated more than 100 items from EachMovie dataset and it contains 1196106 ratings by 7471 users on 1619 items. For rating prediction, we perform 5-fold cross validation to measure MAE and RMSE evaluators in our experiments. In each fold,, we use 80% of data as the training set and the remaining 20% as the test set on ML100K dataset. For ranking accuracy, we exactly select 10 rating records per user to fill the test set when evaluate the performance of methods on NDCG values. 4.2 Evaluation Metrics Originally, we have followed a common practice to evaluate rating prediction and the first evaluation are frequently-used based on error metrics, namely Mean Algorithm 1. PNSM-ModelLearning Absolute Error (MAE) and Root Mean Square Error (RMSE), which are defined as: where  X  r ui denotes the predicted rating user u gives to item i and N is the number of tested ratings. Since above-mentioned measures focus on the error between ground truth and predicted rating, they are not a natural fit for evaluating the top-N recommendation task. Rather, we employ an alternative prediction eval-uator named Normalized Discounted Cumulative Gain (NDCG) [ 14 ] to directly measure top-N performance.
 It is notable that the measure significantly sharpens the difference between the approaches over what a traditional accuracy measure could show. The NDCG metric is evaluated over the top-n items on the ranked item list and defined as: where T is the set of users in testing dataset and R(u,p) is the p -th item rating returned by the recommender on the ranked list for user u . Z factor calculated so that the NDCG of the optimal ranking has a value of 1. The lower ranked items in predicted list that should be higher in ground truth can be penalized with the discounting factor log (1 + p ). So the NDCG is very sensitive to the ratings of the highest ranked items and it is highly beneficial to measure the ranking quality in recommender systems where the relevance of items at the top positions are far more significant than those at low. 4.3 Comparison Algorithms We compare the performance of our models against the achieved by baseline estimate[ 8 ], ItemBased[ 15 ], RSVD (Regularized SVD [ 16 ]), NSVD [ 7 ] and FISM (Factored Item Similarity Methods [ 17 ]). We also compare the performance of two proposed models PNSM 1 and PNSM 2 . We present the best performing model with the explored parameter space for each method. In this section, we present the quality of the recommender algorithms mentioned before. The experimental evaluation is made up of two parts. At first, we discuss the comparison results with other competing methods on all the datasets. Then, we study the effect of various parameters of PNSM on the recommendation performance. 5.1 Comparison with Other Approaches Here shows the overall results of comparison partners for the recommenda-tion task on datasets. For each method, we explored the following parameter space to benefit the best performing model. Empirical analysis, the learning rate decreased at a rate of 0.97 after one iteration. For itembased CF, we used cosine similarity to measure item-item correlations. We set the dimension of latent feature to be 160 in all SVD-based methods. For PNSM set the regulation parameter  X  =0 . 01,  X  =0 . 5and  X  =0 . 6. In addition, as to evaluate NDCG, we have zoomed in on N in the range [1...10].
 RMSE values on Movielens dataset. From the result, firstly, we can see that ItemBased method outperforms the baseline with great ascension, which means neighborhood similarity contributes to rating prediction. Secondly, all of the referred SVD-based models make a significant improvement compared to base-line, which indicates that the decomposition of matrix is necessary to capture the underlying interactions between items. Obviously, considering all these together, our models perform better than the other methods with the positive and nega-tive feedbacks. As can be seen, PNSM 1 is the second best method both on MAE and RMSE. We believe that the positive feedbacks can facilitate predictions. We also observe that PNSM 2 yields best performance in most cases. It has a 4.77 percent improvement on MAE and 4.16 percent improvement on RMSE compared with the baseline on average. This verifies that the negative feedbacks and the positive feedbacks supplement each other and help to find more accurate inferences of users.
 Table 1 and Table 2 show the experimental results of NDCG values on ML100K and EachMovie datasets respectively. In this experiment, we take the different top-n numbers with a step size of 2. For each algorithm, we report the NDCG values with the different top-N numbers denoted by NDCG@n, which takes the mean of the NDCG value over the multiple permutations of top-n .For each column, we have highlighted the performance achieved by the best method. The values shown in the bottom row are the performance improvements achieved by PNSM 2 over the baseline. From the results, we observe that PNSM 1 and PNSM 2 yield better per-formance under most of the evaluation conditions. It verifies the assumption that recommendation can be improved if we consider the positive and negative feedbacks separately. In particular, PNSM 1 provides a better top-N recommen-dation since the positive feedbacks can help stand out the most relative items. Moreover, because the negative feedbacks help to filter out irrelevant items and denoise data, PNSM 2 outperforms the other methods in most cases compared to PNSM 1 . It manifests that not only the positive but also the negative feed-backs play an indispensable role in top-N recommendation. Viewed from the row both in Table 1 and Table 2 , all the methods perform better along with the increase of the top-n number. This is because that it can be easier to give a suf-ficient ranking with more items so that the preferences of users can be estimated more accurately. We also observe that the percentage of improvement based on NDCG@n values of PNSM 2 over baseline is inversely proportional to top-n num-ber. That is, it can be noted PNSM 2 can better recommend top popular items even with a small n number. In addition, compared to ML100K, we can find that our methods achieve fairly good performance on EachMovie dataset which is much denser. It confirms that the more sufficient the positive and negative feedbacks there are, the better recommendation the method provides. other comparison partners on MAE, RMSE and NDCG values, which verifies the assumption that it is highly effective to separate the positive feedbacks from the negative ones both in rating prediction and top-N recommendation. 5.2 Impact of the Dimension As stated previously, we utilized an inner product of two low dimensional latent factor matrices to learn item-item similarities. When the dimension varies, the performance of latent factor models is different with keeping other parameters in constant. We conduct experiments on ML100K dataset with the dimension from 20 to 160 and plot the results of MAE in Figure 2 . As can be seen from the figure, our recommendation performance becomes better as the value of dimension increases. When the dimension is 160, we have the best MAE measure. 5.3 Impact of Neighborhood Agreements  X ,  X  In this study, one of the main advantages is that our method incorporates the positive and the negative feedbacks to provide recommendations. Accordingly, another two important parameters  X  and  X  respectively control the positive and negative neighborhood agreement between the items rated by users. Figure 3 presents how  X  and  X  affect the performance of PNSM 2 on ML100K dataset. In the experiments, different values of  X  and  X  are set from 0 to 1 with a step size of 0.1 as the dimension is 100. We vary them to better understand the roles they played in the optimal recommendation and compare the impact of neighborhood agreement on the performance.
 In the figure, they have similar trends as  X  and  X  increase. When  X  and  X  are small, they have little effect on the performance because both the positive and the negative implicit correlations of items are ignored. When they increase to 1, these implicit feedbacks overwhelm the rating information and cause the descendence of performance. We can also see that no matter how  X  varies, the performance tends to be steadily poor when  X  is close to zero. It is the same to the case in which  X  is small. This result is consist with our expectation because the positive and negative feedbacks supplement each other while only taking either aspect into account have little effect. In addition, as  X  /  X  is around 1, there is a quadratic curve in the figure and MAE value is minimum on the bottom. Specially when  X  is 0.5 and  X  is 0.6, we have the best performance. That is, on average, for both the positive examples and negative examples, a substantial number of neighborhood items need to have a high similarity value. This is confirmed that it X  X  necessary to integrate two kind of feedbacks and give them a separation to make recommendations. In this paper, we proposed an extended method that combines both the positive and negative feedbacks of users while learning latent item-item correlations. It X  X  a new variant of item-oriented collaborative filtering algorithm based on matrix factorization techniques. To exploit effective features in recommendation pro-cess, we separated the positive examples from the negative ones and treated them differently to capture accurate preference of users. Further, we employed some parameters to control the agreement degree of the neighborhood items, and it X  X  helpful to filter out useless information and denoise data. To evaluate the performance of our proposed method, we conducted a comprehensive set of experiments on ML100K and EachMovie datasets in terms of MAE and NDCG. The method combining both the positive and negative feedbacks had a statisti-cal significant improvement than other comparison partners, which implies that the classified feedbacks contribute more to recommendation performance. The experimental results showed that our method is significantly effective in both rating prediction and top-N recommendation.
 more relevant factors, such as social trusts, etc.

