 than with score s. We then plot s versus P(cls(x) = s). scores are equal to the empirical probability. is large compared to the number of available test examples, so we cannot calculate reliable empirical probabilities for each possible score value. In this case, we can resort to discretizing the score space. But because the scores are not uniformly distributed, we have to carefully choose bin sizes so that there are enough examples to calculate reliable empirical probability estimates for each bin. 
In Figure 1 we show reliability diagrams for two well-known datasets: Adult and TIC (see Section 5 for information on these datasets), where the score space has been discretized into bins of size 0.1 and 0.15, respectively. As we can see in the graphs, al-though tending to vary monotonically with the empirical probabil-ity, naive Bayes scores are not well-calibrated because many of the points do not fall into the x = y line. 
For each test example x, an SVM classifier outputs a score that is the distance ofx to the hyperplane learned for separating positive examples from negative examples. The sign of the score indicates if the example is classified as positive or negative. The magnitude of the score can be taken as a measure of confidence in the prediction, since examples far from the separating hyperplane are presumably more likely to be classified correctly. 
Although the range of SVM scores is [-a,a] (where a depends on the problem), we can map the scores into the [0,1] interval by re-scaling them. If f(x) is the original score, then a)/2a is a re-scaled score between 0 and 1, such that if then s(x) &gt; 0.5 and if f(x) &lt; 0 then s(x) scores tend to not be well-calibrated since the distance from the separating hyperplane is not exactly proportional to the chances of membership in the class. 
In Figure 2 we show reliability diagrams for re-scaled SVM scores using the Adult and TIC datasets, where the score spaces are dis-cretized into bins of size 0.08 and 0.15, respectively. We see that 
SVM scores vary monotonically with the empirical probability, but are not well-calibrated. labels. In this case we can assume that P(clx ) examples and P(clx ) = 0 for negative examples. If we apply the classifier to those examples to obtain scores s(x), function mapping scores s(x) into probability estimates the learning method does not overfit the training data, we can use the same data to learn this function. Otherwise, we need to break the training data into two sets: one for learning the classifier and the other for learning the mapping function. a mapping function that does not generalize well to new data. One possible regularization criteria is to impose a particular parametric shape for the function and use the available data to learn parameters such that the function fits well the data according to some measure. 
The parametric approach proposed by Platt [18] for SVM scores consists in finding the parameters A and B for a sigmoid function of the form P(c[x) = T~ mapping the scores s(x) bility estimates P(c]x), such that the negative log-likelihood of the data is minimized. 
This method is motivated by the fact that the relationship be-tween SVM scores and the empirical probabilities P(c[x) to be sigmoidal for many datasets. This is the case for the Adult dataset, as can be seen in Figure 3, where we show the learned sig-moid using the training data and the empirical probabilities for the test data, for Adult and TIC. Platt has shown empirically that this method yields probability estimates that are at least as accurate as ones obtained by training an SVM specifically for producing accu-Figure 5: Using the PAV algorithm to map naive Bayes and SVM scores into probability estimates. pair-adjacent violators, because they violate the isotonic assump-tion. The values of g(xi-l~ and g(xi) are then replaced by their average, so that the examples xi-1 and xi now comply with the iso-tonic assumption. If this new set of n -1 values is isotonic, then g*(xi-1) = g*(xi) = (g(xi-1 + g(xi))/2, and g*(xj) = g(xj) other-wise. This process is repeated using the new values until an isotonic set of values is obtained. The computational complexity of this al-gorithm is O(n). An implementation of PAV in MATLAB is made available by Lutz Dtirnbgen [10]. 
When we apply this algorithm to the problem of mapping scores into probability estimates, we first sort the examples according to If the scores rank the examples perfectly, then all negative xi come before the positive xi and the values of g are not changed. The new probability estimate g* is 0 for all negative examples and 1 for all positive examples. On the other hand, if the scores do not give any information about the ordering of the examples, g* will be a constant function whose value is the average of all values of g(xi), which is the base rate of positive examples. 
In the general case, PAV will average out more examples in parts of the score space where the classifier ranks examples incorrectly, and less examples in parts of the space where the classifier ranks them correctly. We can view PAV as a binning algorithm where the position of the boundaries and the size of the bins are chosen according to how well the classifier ranks the examples. 
PAV returns a set of intervals and an estimate g(i) for each in-terval i, such that g*(i+ 1) &gt; g*(i). To obtain an estimate for a test example x, we find the interval i for which s(x) is between the lowest and highest scores in the interval and assign g* (i) as the probability estimate for x. 
In Figure 5, we show the result of applying PAV to the Adult dataset, for both naive Bayes and SVM. The line shows the function that was learned on the training data, while the stars show empirical probabilities for the test data. 
The notion of calibration introduced in Section 2 can be read-ily applied to multiclass probability estimates. Suppose we have a mnlticlass classifier that output scores s(ci[x) for each class ci and each example x. The classifier is well-calibrated if, for each score value s(ci Ix) = s, as the number of examples classified goes to infinity. 
However, the calibration methods discussed in Section 3 were designed exclusively for two-class problems. Mapping scores into probability estimates works well in the two-class case because we are mapping between one-dimensional spaces. In this setting, it is easy to impose sensible restrictions on the shape of the function being learned, as it is done with the sigmoidal shape or the mono-tonicity requirements. 
In the general multiclass case, the mapping would have to be from (k -1 ) -dimensional space to another (k-1 )-dimensional space. In this case, it is not clear which function shape should be imposed to the mapping function. Furthermore, because of the curse of di-mensionality, non-parametric methods are not likely to yield ac-curate probabilities when the number of classes grows. For these reasons, we do not attempt to directly calibrate multiclass probabil-ity estimates. Instead, we first reduce the multiclass problem into a number of binary classification problems. Then we learn a clas-sifier for each binary problems, and calibrate the scores from each classifier. Finally, we combine the binary probability estimates to obtain multiclass probabilities. 
Two well-known approaches for reducing a multiclass problem to a set of binary problems are known as one-against-all and all-pairs. In one-against-all, we train a classifier for each class using as positives the examples that belong to that class, and as negatives all other examples. In all-pairs, we train a classifier for each possi-ble pair of classes ignoring the examples that do not belong to the classes in question. 
Allwein et al. [1 ] represent any possible decomposition of a mul-ticlass problem into binary problems by using a code matrix M E ber of binary problems. If M(c, b) = + 1 then the examples belong-ing to class c are considered to be positive examples for the binary classification problem b. Similarly, if M(c, b) = -1 the examples belonging to c are considered to be negative examples for b. Fi-nally, if M(c, b) = 0 the examples belonging to c are not used in training a classifier for b. 
For example, in the 3-class case, the all-pairs code matrix is 
These code matrices are a generalization of the error-correcting output coding (ECOC) scheme [8]. The difference is that ECOC does not allow zeros in the code matrix, meaning that all examples are used in each binary classification problem. 
For an arbitrary code matrix M, we have an estimate rb(x) for each column b of M, such that where ! and J are the set of classes for which M(.,b) = -1, respectively. We would like to obtain a set of prob-abilities P(clx ) for each example x compatible with the subject to ~iP(cil x) = 1. Because there are k-1 free parameters and l constraints, and we generally consider matrices for which exact solution. 
Two approaches have been proposed for finding an approximate solution for this problem. The first is a least-squares method with non-negativity constraints proposed by Kong and Dietterich [15]. They have proposed this method for the original ECOC matrices, but it can easily be applied to arbitrary matrices. They test it on binary probability estimates from decision trees classifiers learned using C4.5, which are known not to be well-calibrated [25, 19]. Using synthetic data, they show that this method produces better estimates than multiclass C4.5. 
The alternative method is called coupling, an iterative algorithm that finds the best approximate solution minimizing log-loss instead of squared error [23]. This method was proposed as an extension to the pairwise coupling method [14], which only applies to all-pairs matrices. The algorithm was tested using boosted naive Bayes [11] as the binary learner, whose scores tend to be even less calibrated than naive Bayes scores because they are more extreme. 
It is an open question which of the two existing methods for com-bining binary probability estimates yields the most accurate multi-class probability estimates. A desirable property for such a method is that the better calibrated the binary estimates are, the better cal-ibrated the multiclass estimates should be. In the next section, we compare these methods experimentally on two multiclass datasets. 
Here we present results of the application of the methods dis-cussed in the previous sections to a variety of datasets. Since the methods used for learning the classifiers do not overfit the training data for these datasets, in all experiments we use the same data for learning both the classifier and the calibration functions. 
As the primary metric for assessing the accuracy of probability estimates, we use the mean squared error (MSE), also known as the Brier score [6]. For one example x, the squared error (SE) is defined as ~c(T(c[x) -P(c[x)) 2 where e(c[x) is the probability es-actual label ofx is c and 0 otherwise. We calculate the SE for each example in the training and test sets to obtain the MSE for each set. 
DeGroot and Fienberg [7] show that the MSE can be separated into two components, one measuring calibration and the other mea-suring refinement. If the classifier is well-calibrated the first com-ponent is zero. For two classifiers that are well-calibrated, the one for which the probability estimates P(clx ) are closer to 0 or 1 is said to be more refined, because it makes predictions that are more confident. If the two classifiers are well-calibrated, the one with the lowest MSE is more refined, and thus, preferable. 
Although MSE can be applied in general, it is more sensible to evaluate the quality of probability estimates in practical situations using a domain-specific metric. For example, in direct mailing, we should evaluate how good the probability estimates are by the profit obtained when we mail people according to a policy that uses the estimates. MSE tends to be correlated with profit [24], so when we do not have domain-specific information to calculate profit we can use MSE to evaluate our methods. 
The first dataset we use is the KDD-98 dataset, which is available in the UCI KDD repository [3]. The dataset contains information about persons who have made donations to a certain charity. The decision-making task is to choose which donors to request a new donation from. The data is divided in a standard way into a training and a test set. The training set consists of 95412 records for which it is known whether or not the person made a donation and how much the person donated, if a donation was made. The test set consists of 96367 records from the same donation campaign. Our choice of attributes is fixed and based informally on the KDD-99 winning submission of [13]. 
The optimal mailing policy is to solicit people for whom the ex-pected return P(donation[x)y(x) is greater than the cost of mailing a solicitation, where y(x) is the estimated donation amount. Since this paper is not concerned with donation amount estimation, we tMethod ITraining Test I Err X rRateTraining Test ] NB 0.25112 0.25198 0.17100 0.17321 I I SigmoidNB 0.21530 0.21515 0.15270 0.15190 PAV NB 0.20312 0.20452 0.14665 0.14831 SVM 0.28719 0.28684 0.15190 0.14968 SigmoidSVM 0.20980 0.20962 0.15156 0.14993 PAV SVM 0.20815 0.20924 0.15115 0.15113 error rate is slightly increased when we apply the correction meth-ods. This indicates that although the SVM scores are uncalibrated, the threshold used for classification is optimal. When the calibra-tion methods are used, the error rate is increased because the re-finement of the classifier is ~lightly reduced. Surprisingly, even though the shape of the function mapping SVM scores to empirical probability estimates has a distinctive sig-moidal shape (Figure 3), the PAV method performs slightly better than the sigmoid fitting method. 
We also compared PAV to binning with bin sizes varying from 5 to 50 for both naive Bayes and SVM, and found that binning is al-ways worse than PAV. This indicates that, for this dataset, by using a fixed number of examples per bin we cannot accurately model the mapping from SVM and naive Bayes scores into calibrated probe-bility estimates. 
The first multiclass dataset we consider is Pendigits, available in the UCI ML Repository [5]. It consists of 7494 training exam-ples and 3498 test examples of pen-written digits (10 classes). The digits are represented as vectors of 16 attributes which are integers ranging from 0 to 100. 
For these experiments, we use a one-against-all code matrix. We use both naive Bayes and boosted naive Bayes as the binary learn-ers, and apply PAV to calibrate the scores. As we mentioned in Section 4 there are two methods for combining binary probability estimates into multiclass probability estimates for arbitrary code matrices: least-squares and coupling. For one-against-all, however, there is another possible method: normalization. Because in this case each binary classifier i outputs an estimate of simply normalize these estimates to make them sum to 1. 
Table 4 shows MSE and error rate when we apply each of the methods to naive Bayes, PAV naive Bayes, boosted naive Bayes and PAV boosted naive Bayes. When we calibrate the probability estimates before combining them using any of the methods, both the MSE and the error rate are lower than when we use raw scores. However, it is not clear which of the methods for combining the binary estimates is to be preferred. When the calibrated estimates are used it makes less difference which method is used. For this reason, we recommend using simple normalization for one-against-all, which is the simplest method. 
The second multiclass dataset we use is 20 Newsgroups, which was collected and originally used by Lang [16]. It contains 19,997 text documents evenly distributed across 20 classes. Because there is no standard training/test split for this dataset, we randomly select 80% of documents per class for training and 20% for testing. We conduct experiments on 10 training/test splits and report mean and standard deviation. 
Previous research [20] found that one-against-all performed as we again restrict our experiments to one-against-all. We calibrate the naive Bayes scores using PAV, and apply each of the methods for obtaining multiclass probability estimates to both the raw naive Table 4: MSE and error rate on Pendigits (test set) 
We have presented simple and general methods for obtaining ac-
For two-class problems, we recommend using the PAV algorithm 
For many domains, however, using more sophisticated code ma-binary: A unifying approach for margin classifiers. Journal of Machine Learning Research, 1:113-14 I, 2000. empirical distribution function for sampling with incomplete information. Annals of Mathematical Statistics, 26(4):641--647, 1955. Computer Sciences, University of California, lrvine, 2000. ht:tp : //kdd. ies .uei. edu/. estimates. Technical Report CMU-CS-00-155, Carnegie Mellon University, 2000. databases. Department of Information and Computer Sciences, University of California, Irvine, 1998. Method I MSE I Error Rate NB Norm 0.01625 (::1: 0.00049) I 0.15836 (:t= 0.0067) NB LS 0.01720 (=1: 0.00045) ~ 0.15530 (=t: 0.0064) NB Coup 0.01585 (=1: 0.00041) 0.16066 (:t: 0.0075) PAV NB Norm 0.01220 (:t: 0.00038) ~ 0.15305 (::h 0.0060) PAV NB LS 0.01419 (=1: 0.00029) I 0.15299 (:t: 0.0057) PAV NB Coup 0.01415 (::h 0.00029) 0.15422 ( X  0.0060) 
Table 5: MSE and error rate on 20 Newsgroups. [6] G. W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78:1-3, 1950. [7] M. H. DeGroot and S. E. Fienberg. The comparison and evaluation of forecasters. Statistician, 32(1):12-22, 1982. [8] T. G. Dienerich and G. Bakiri. Solving multiclass learning problems via error-correcting oatpfft codes. Journal of Artificial Intelligence Research, 2:263-286, 1995. [9] E Domingos and M. Pazzani. Beyond independence: Conditions for the optimality of the simple Bayesian classifier. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 105-112. Morgan Kaufmann Publishers, Inc., 1996. [10] L. Dilmbgen. Statistical software (MATLAB), 2000. Available at [11] C. Elkan. Boosting and naive bayesian learning. Technical Report CS97-557, University of California, San Diego, 1997. [12] C. Elkan. Magical thinking in data mining: Lessons from coil challenge 2000. In Proceedings of the Seventh International Conference on Knowledge Discovery and Data Mining, pages 426-.-431. ACM Press, 2001. [13] J. Georges and A. H. Milley. KDD'99 competition: Knowledge discovery contest report. Available at 1999. [14] T. Hastie and R. Tibshirani. Classification by pairwise coupling. In Advances in Neural Information Processing Systems, volume 10. MIT Press, 1998. [15] E. G. Kong and T. G. Dietterich. Probability estimation using error-correcting output coding. In Int. Conf : Artificial Intelligence and Soft Computing, 1997. [16] K. Lang. Newsweeder: Learning to filter netnews. In Proceedings of the Twelfth International Conference on Machine Learning, pages 331-339, 1995. [17] A. Murphy and R. Winkler. Reliability of subjective probability forecasts of precipitation and temperature. Applied Statistics, 26(1):41-47, 1977. [18] J. Platt. Probabilistic outputs for support vector rnachines and comparison to regularized likelihood methods. In Advances in Large Margin Classifiers. MIT Press, 1999. [19] E Provost and E Domingos. Well-trained PETs: Improving probability estimation trees. CDER Working Paper #00-04-IS, Stem School of Business, New York University, NY, NY 10012, 2000. [20] J. Rennie and R. Rifkin. Improving multiclass text classification with the support vector machine. Technical Report AIM-2001-026.2001, MIT, 2001. [21] R. Rifkin. SvmFu 3, 2001. Available at [22] T. Robertson, P. Wright, and R. Dykstra. Order Restricted Statistical Inference, chapter 1. John Wiley &amp; Sons, 1988. [23] B. Zadrozny. Reducing mniticlass to binary by coupling probability estimates. In Advances in Neural Information Processing Systems (NIPS*2001), 2002. To appear. [24] B. Zadrozny and C. Elkan. Learning and making decisions when costs and probabilities are both unknown. In Proceedings of the Seventh International Conference on Knowledge Discovery and Data Mining, pages 204-213. ACM Press, 2001. [25] B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 609-616. Morgan Kaufmann Publishers, Inc., 2001. 
