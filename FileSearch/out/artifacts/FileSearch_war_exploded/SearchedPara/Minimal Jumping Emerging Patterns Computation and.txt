 Contrast set mining is a well established data mining area [ 14 ] which aims at distributions across groups. This area gathers many techniques such as subgroup contrast sets are highly useful in supervised tasks to solve real world problems in many domains [ 1 , 7 , 12 ].
 Let us consider a dataset of objects partitioned into several classes, each ing patterns (EPs) are patterns whose frequency strongly varies between two datasets. A Jumping Emerging Pattern (JEP) is an EP which has the notable property to occur only in a single class. JEPs are greatly valuable to obtain a JEP where none of its proper subsets is a JEP. Minimal JEPs are of great interest because they capture the vital information that cannot be skipped to characterize a class. Using more attributes may not help and even add noise in a time consuming process. Current methods require either a frequency thresh-results of this paper is to be able to compute the whole set of minimal JEPs. duce an efficient method to obtain all minimal JEPs . A key idea of our method is to introduce an alternative definition of a minimal JEP which stems from the differences between pairs of objects, each of a different class. A backtrack algorithm for computing all minimal JEPs is detailed and the related proofs are provided. Our method does not require either a frequency threshold or a number of patterns to extract. It provides a general approach and its scope encompasses old) and the k most supported minimal JEPs [ 16 ] which constitute the state of the art in this field. Second, taking into account the absence of attributes may provide interesting pieces of knowledge to build more accurate classifiers as experimentally shown by Terlecki and Walczak [ 15 ]. We address this issue. Our method integrates the absence of attributes in the process by adding their negation. It produces the whole set of minimal JEPs both with the present and study are given. We analyze the computation of the minimal JEPs, including the absence of attributes and comparisons with essential JEPs and top-k minimal JEPs. Finally, we experimentally assess the quality of minimal JEPs, essential JEPs and top-k minimal JEPs as correlations between a pattern and a class. in Section 3 . Section 4 presents the experiments. We review related work in Section 5 and we round up with conclusions and perspectives in Section 6 . Let G be a dataset , a multiset consisting of n elements, an element of being an atomic proposition which may hold or not for an object. The finite set of all the attributes occurring in G is denoted by M . In the remainder of this description of an object.
 denoted P ( M ). A pattern is included in the object g if description of g : p  X  g .The extent of a pattern p in G , denoted to the set of the objects that include p : p G = { g  X  X  : supported if it is included in at least one object of the dataset. Moreover, we define a relation, I ,on G X P ( M ) as follows: for any object p , gIp  X  X  X  p  X  g .
 binary descriptions, the absence of an attribute can be explicitly denoted by adding the negation of this attribute in order to build patterns conveying this information. We integrate this idea in this paper by adding the negation of absent attributes and thus the description of an object always mentions every attribute either positively or negatively. In other words, of an attribute (cf. Table 1 as an example).
 Minimal Jumping Emerging Pattern. We now suppose that the dataset partitioned into two subsets G + and G  X  , every subset of such a partition is usually named a class of the dataset. We call an object of andanobjectof G  X  a negative object . We say that a supported pattern JEP if it is never included in any negative object: p G = AJEPis minimal if it does not contain another JEP as a proper subset. The set of the minimal JEPs is a subset of the set of the JEPs which groups all the most general JEPs. As a JEP contains at least one minimal JEP, when an object includes a JEP then it includes a minimal JEP.
 Table 1 displays a dataset of 6 objects partitioned in two datasets: { g ,g { g } and p G  X  =  X  and { 1 } and { X  2 } are not JEPs, p is thus a minimal JEP. vides a new definition of a minimal JEP. The latter is the support of our algo-rithm for extracting minimal JEPs which is detailed and proven in Section 3.2 . 3.1 A Relation Between the Minimal JEPs and the Differences Between Objects Let
G be a dataset partitioned into two subsets G between an object i and an object j groups the attributes of satisfied by j : D i,j = i \ j = { m  X  X  : iIm and  X  jIm } on a negative object j ,the gathering of the differences for a negative object corresponds to the union of the differences between i and negative object 4 is D  X  4 = D 1 , 4  X  X  2 , 4 = { 1,  X  2 } X  X  X  The following lemma is a direct consequence of the definition of the gathering of the differences for a negative object.
 Lemma 1. Let j be a negative object and p be a pattern. If is not included in j :  X  ( jIp ).
 differences for a negative object and, thanks to Lemma 1 , in any negative object, thus p is a JEP. We now reason by contraposition and we suppose that a supported pattern p does not intersect with the gathering of the differences for one negative object j 0 : D  X  j 0  X  p positive object i 0 ,as D  X  j 0  X  p =  X  implies D i 0 ,j 0 j .Thus p cannot be a JEP.
 in every D  X  j ,for j a negative object. Proposition 1 follows: Proposition 1. A supported pattern p is a JEP if D  X  j  X  p 2 ): D  X  g 3  X  p = { 1 ,  X  2 } , D  X  g 4  X  p = { 1 ,  X  2 } minimal JEPs.
 Proposition 2. AJEP p is a minimal JEP if, for every attribute G  X  such that p  X  X   X  j = { a } .
 between p and a D  X  j (for j a negative object) corresponds to { 3 } does not play a necessary part in the discriminative power of not a minimal JEP.
 Proof (of Proposition 2 ). Let p be a JEP.
 that  X  j  X  X   X  ,q  X  X   X  j =  X  , it ensues that  X  j  X  X   X  ,p  X  X  can state that, if p is not minimal, then p contains one attribute  X  j  X  X   X  ,p  X  X   X  j = { a } .
 D  X  j = { a } .As p isaJEP,Prop. 1 ensures that D  X  j  X  p = that,  X  j  X  X   X  , D  X  j  X  p \{ a } =  X  . By applying Prop. 1 , cannot be minimal.
 the negative objects and where every attribute is necessary to exclude (at least one) object. It follows: Consequence of Prop. 2 . Let p be a minimal JEP for the dataset g  X   X  X   X  .If p is not a minimal JEP for the dataset G +  X  X   X  dataset G +  X  X   X  \{ g  X  } . 3.2 Calculation of the Minimal JEPs We now introduce a structure designed to generate all the minimal JEPs for a dataset: a rooted tree whose  X  X alid X  leaves are in a one-to-one correspondence with the minimal JEPs. We suppose here that for  X  j  X  X   X  , D minimal JEP. We also assume that an arbitrary order is given on the negative objects: for two negative objects j and j , j  X  j if j is accounted before Rooted Tree. A rooted tree ( T,r ) is a tree in which one node, the root called a leaf .If { u, v } is an edge of a rooted tree such that from the root to v , then v is a child of u .An ancestor of path from the root to u .If u is an ancestor of v , then v we write u v ;if u = v , we write u&lt;v .
 A Tree of the Minimal JEPs. We create the tree ( T,r ) as a rooted tree in which each node x , except the root r , holds two labels: an attribute, negative object l obj ( x )  X  X   X  .Foranode x of ( T,r ), Br that occur along the path from the root to x : Br Br ( x ) indicates the pattern considered at x . For any node x of at the level of x and whose exclusion is due to the sole presence of crit ( a, x )= { j l obj ( x ): D  X  j  X  Br ( x )= { a }} .
 Definition 1 (A tree of the minimal JEPs (ToMJEPs)). A rooted tree (
T,r ) is a tree of the minimal JEPs for G if: i) any node x , except the root r , holds two labels: an attribute label,
M , and a negative object label, l obj ( x )  X  X   X  . ii) if x is an internal node then: a) the children of x hold the same negative object label: b) every child of x holds a different attribute label, c) the union of the attribute labels of the children y of iii) x is a leaf if it satisfies one of the following conditions: a)  X  z x such that crit ( l attr ( z ) ,x )=  X  , b)  X  j  X  X   X  , D  X  j  X  Br ( x ) =  X  .
 A leaf which satisfies the criteria iii)a) is named dead-end leaf , otherwise it is named a candidate leaf .
 Figure 1 depicts a ToMJEPs for the dataset of Tables 1 and 2 . The nodes with a dashed line are the dead-end leaves, the nodes surrounded by a solid line the candidate leaves. A candidate leaf surrounded by a bold plain line is associated to a supported pattern: it represents a minimal JEP. For example, the node x such that Br ( x )= { 1 ,  X  2 } is associated to a minimal JEP while the node y by the dataset. The node z such that Br ( z )= { 3 ,  X  2 }  X  j  X  X  g raised by Prop. 2 ,thus crit (3 ,z )=  X  .
  X  X upported X  candidate leaves of a ToMJEPs and the minimal JEPs. The follow-ing lemma is an immediate consequence of the definition of a ToMJEPs, together with the application of Prop. 1 and 2 .
 Lemma 2. Let ( T,r ) be a ToMJEPs and x be a node of T , different from a dead-end leaf. If there exists i  X  X  + such that i I Br ( x ) then JEP for the dataset G = G +  X  X  j  X  l obj ( x ) } .
 Proof. By definition of a ToMJEPs, for a node x ,wehave Br l  X  l obj ( x ). Thanks to Prop. 1 , it follows that Br ( l x, crit ( l attr ( z ) ,x ) =  X  ,thus  X  a  X  Br ( x ) ,  X  j  X  X  X { j  X  l D G Lemma 3. Let ( T,r ) be a ToMJEPs. Let p be pattern. If p for the dataset G +  X  X   X  then there exists a unique candidate leaf Br ( x )= p .
 here the set of the negative objects as { 1 ,...,k } with k  X  1 ,j  X  j +1.
 negative object), we have D  X  1 = { l attr ( x ): x is a child of by definition of a ToMJEPs, crit ( l attr ( x ) ,x ) =  X  , no child of leaf. Thus, associated to any pattern p which is a minimal JEP for the dataset G Br ( x )= p . Let us now suppose that, considering any minimal JEP p for with l&lt;k , there exists a unique node x , different from a dead-end leaf, such that Br ( x )= p . When we consider a pattern q , minimal JEP for the dataset G  X  X  1 ,...,l,l +1 } , two cases arise:  X  X f q is a minimal JEP for G +  X  X  1 ,...,l } , then, thanks to the induction hypothesis, there exists a unique node x q such that Br (  X  Otherwise, thanks to the consequence of Prop. 2 , there exists one attribute a such that D  X  l that q \{ a } is minimal JEP for G +  X  X  1 ,...,l } . Thanks to the induction hypothesis, there exists a unique node x , different from a dead-end leaf, such of x , such that Br ( q )= x .As q is a minimal JEP, x is not a dead-end leaf. Prop. 3 is a consequence of Lemmas 2 and 3 : Proposition 3 (One-To-One correspondence). Let ( T,r ) be a ToMJEPs. There is a one-to-one correspondence between the set of the candidate leaves such that Br ( x ) is a supported pattern and the set of the minimal JEPs. Prop. 3 ensures that we can generate the minimal JEPs by simply performing a depth first traversal of a ToMJEPs and output the candidate leaves such that Br ( x ) is a supported pattern. Note that it is not necessary to compute and store the entire ToMJEPs. A depth first traversal only requires to store the path from the root to the node currently visited.
 The sketch of implemention provided in Section 4.1 gives information about minimal JEPs that are inferred from a ToMJEPs. This section provides and comments results from a study conducted on 13 bench-mark datasets. We investigate the computation of the JEPs according to running correlation between a JEP and a class. In the following, a JEP denominates a supported pattern with respect to any class. 4.1 Material and Methods The datasets. The study is conducted on 13 usual datasets described in Table 3 . All the datasets are available from the UCI Machine Learning repository [ 10 ]. We selected these datasets because they have been used, at least once, in an into a binary valued format by applying a sanctioned method [ 6 , 11 ]whichis available at Frans Coenen X  X  website 1 . Implementation. Our algorithm partially explores a ToMJEPs in a depth first manner, it outputs every candidate leaf whose associated pattern is a supported one. We implemented two solutions to ensure to only output supported pat-leaves and then checks whether their extent is empty or not. The second one, named maintaining extent solution, integrates the computation of the extents with the calculation of the child of an internal node of a ToMPJEPs. It enables to backtrack as soon as the extent is empty.
 ing extent solution is straightforwardly adapted to improve the computing of the derived from the cardinality of the extent. For the same reason, this solution also enables to compute the top-k minimal JEPs [ 16 ] when a value for vided. Moreover, the pruning strategy becomes more and more efficient during the mining step because the minimal frequency threshold to belong to the top-minimal JEP only increases during the mining.
 Protocol. In order to compute all the minimal JEPs whatever the positive class performed on a server using Ubuntu 12.04 with 2 processors Intel Xeon 2.80 GHz and 512 gigabytes of RAM. 4.2 Results and Discussions Computation of the Minimal JEPs. We computed all the minimal JEPs on the Moreover, essential JEPs are computed with two minimum frequency thresholds (1% and 5%), and the top-k JEPs with k =10and k = 20. Table 4 gives the cardinalities of the sets of the minimal JEPs and the running times. For comput-ing all the minimal JEPs, the maintaining extent solution always operates faster than the post-filtering solution, by a factor varying from 1 the results for the essential JEPs and top-k minimal JEPs, one notes that the Minimal JEPs as Rules to Express Correlations. A JEP expresses a correlation between the occurrence of a pattern and one class of objects. This part provides large part of the objects? Are they confident enough? We have also performed experiments to evaluate the usefulness of the explicit description of the absent attributes by adding their negations.
 The study has been conducted by using a leave-one-out framework: every object has been successively discarded from the dataset. For every object minimal JEPs have been extracted by considering G\{ g } as the dataset and the resulting rules have been applied on g .
 Table 5 provides results obtained by applying minimal JEPs, essential JEPs, or top-k minimal JEPs as association rules. No Negated attributes designates the descriptions which do not explicitly take into account the absence of attributes whereas With Negated attributes points the descriptions that explicitly consider the absence of attributes. The column Cov denotes the coverage of the set of has applied). The column Con refers to the average confidence (i.e., the ratio between the number of correct applications of the rules over the whole number whith the No Negated attributes description, 47 . 78% of the objects contain at least one minimal JEP, this coverage raises to 49 . 33% of the objects when the descriptions With Negated attributes are accounted. With the same dataset, by a minimal JEP apply on an object of the proper class ; this average confidence First of all, the JEPs often apply on a large portion of the objects: for 7 datasets among the 13 datasets, more than 80% of the objects contain at least one JEP. Note that this coverage increases when the description turns from No Negated attributes to With Negated attributes ,upto8%forthe hepatitis dataset. The average confidences indicate that minimal JEPs often point a reliable asso-ciation between a pattern and a class, even when no frequency constraint is set. By paying the price of a lower coverage, setting a minimum frequency threshold  X  as it is done for the essential JEPs or, indirectly, for the top-average confidence levels reached by the two descriptions, No Negated attributes and With Negated attributes , are very comparable.
 However, the minimal JEPs extracted with the With Negated attributes descriptions cover a wider range of objects than the minimal JEPs extracted with the No Negated attributes descriptions, but with a longer running time. ing emerging patterns and contrast sets. However, there are very few attempts to tackle the discovery of minimal JEPs. Fan and Ramamohanarao have pro-posed an algorithm extracting the minimal JEPs whose frequency of occurrence lecki and Walczak have designed a computational method based on a CP-Tree to get the k most supported minimal JEPs, named top-k minimal JEPs [ 16 ]. These methods require either a frequency threshold or a given number of expected pat-terns. On the contrary, our method is free from these parameters and computes the whole set of minimal JEPs. Terlecki and Walczak [ 15 ] have experimentally shown that taking into account the absence of attributes may provide interest-ing pieces of knowledge to build more accurate classifiers. We have dealt with this issue since our method extracts minimal JEPs including the negation of the attributes which are absent.
 In addition, JEPs can be associated to version space [ 13 ]. A version space other class. Therefore a version space corresponds to the JEPs that match all space since a JEP corresponds to all descriptions of objects that match at least one object of one class and no object for the other classes. In Formal Concept in others). We have introduced an efficient method to extract the whole set of minimal JEPs. a frequency threshold or a given number of expected patterns. Our method is also able to straightforwardly extract the essential JEPs and the k most supported minimal JEPs. Moreover it enables the integration of negated attributes that can be precious for a classification purpose. We have experimentally analyzed the computation of these JEPs, together with the reliability of the correlations between a JEP and a class.
 The structure of tree of the minimal JEPs constitutes a framework for design-ing and expressing algorithms to compute the minimal JEPs from a dataset. In orderings on the attributes or on the objects. Another direction is to produce patterns correlated to one class to a lesser extent and mine emerging patterns with high growth-rate values. Beyond this work, we plan to use minimal JEPs in the design of an advanced rule-based classifier.

