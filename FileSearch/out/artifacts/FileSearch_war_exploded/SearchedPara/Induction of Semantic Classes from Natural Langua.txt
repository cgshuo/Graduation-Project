 Many applications dealing with textual information require classification of words into semantic classes (or concepts). However, manually constructing semantic classes is a tedious task. In this paper, we present an algorithm, UNICON, for UNsupervised Induction of CONcepts. Some advantages of UNICON over previous approaches include the ability to classify words with low frequency counts, the ability to cluster a large number of elements in a high-dimensional space, and the ability to classify previously unknown words into existing clusters. Furthermore, since the algorithm is unsupervised, a set of concepts may be constructed for any corpus. Many applications dealing with textual information require classification of words into semantic classes (or concepts). Text mining systems often convert text into a set of features, many of which are defined in terms of semantic classes. In information extraction and question answering, many of the pattern matching rules make use of semantic classes such as management positions, expenditures, art work, etc. [7][20]. Manually constructing semantic classes is a tedious task. Most attempts to automatically construct semantic classes have relied on the Distributional Hypothesis [8] that words that appear in similar contexts are semantically similar. Typically, these algorithms output a similarity matrix that can be used to retrieve the most similar words of a given word as well as the similarity values. There are several drawbacks of such similarity matrices. than the threshold are considered to be 0. Since the threshold is separate good similar words from bad ones. For example, the most similar words of Beethoven, obtained from [13], are (the number following each word is its similarity to Beethoven): A human editor will probably select the words up to Wagner as fail on other lists of similar words. applications such as information extraction and question answering, concept grouping is critical. Finally, the distributional word similarity algorithms tend to break down on infrequent words. Such words have few features. Often, words that accidentally share these features will be considered its similar words. Clustering a large number of elements in a high-dimensional space presents a serious challenge. For example, CLIQUE clustering algorithm specifically designed to handle high-dimensional spaces. The experiments in [1] involved 10,000 elements in a 100-dimension space. In one of our experiments, we are faced with 146,290 elements and 1,639,996 dimensions. In a two-step clustering algorithm, McCallum et al. [15] first use a computationally inexpensive similarity measure to create a set of overlapping canopies. A canopy is a subset of elements that similar to a central element. The assumption is that two elements not belonging to the same canopy do not belong in the same cluster. Using a traditional clustering algorithm (e.g. Greedy Agglomerative Clustering or K-means) each canopy may then be separately clustered. In this paper, we present an unsupervised algorithm, UNICON, which overcomes the former limitations. It can be used for We iteratively apply a clustering algorithm, called CLIMAX, merging clusters by computing and comparing their eentroids. Since the algorithm is unsupervised, a set of concepts may be constructed for any corpus. The remainder of this paper is organized as follows. In the next section, we review previous work in automatic thesaurus construction. Section 3 describes the collocation database required by our system and in Section 4, we present our algorithms. The evaluation of our concept induction algorithm is shown in Section 5. Finally, we conclude with a discussion of future work. modified-by adjectives object-of verbs There have been several approaches to automatic thesaurus construction, mostly for information retrieval. Many algorithms are based on Harris' Distributional Hypothesis [8] and rely on the similarity between terms by constructing a similarity matrix. Salton et al. [19] developed a term dependence model based on relevance judgements targeted for information retrieval systems. Term probabilities are estimated using their frequencies in relevant and non-relevant documents. This model was later extended to use term discrimination values to compute the similarity matrix and cluster terms [5]. Low-frequency terms in clusters were then used to generate the thesaurus classes. These methods are unsuitable for our problem since relevance judgements are unavailable. Bayesian networks have also been used to discover patterns in term usage. Park [17] modelled the similarity distribution among terms using a Bayesian network built from local term dependencies. Compared to previous approaches, this system had the advantage of handling low-frequency terms. Jing and Croft [11] proposed a thesaurus construction algorithm using co-occurrence frequencies (lexical associations) and text feature recognition such as terms and parts of speecb. Using only syntactic information, Grefenstette [6] used a weighted Jaccard measure and Lin [13] proposed an information-theoretic similarity measure to compute the similarity matrix. Chen et al. [411 proposed a three-step algorithm that performs automatic indexing and cluster analysis. The input to our algorithms includes a collocation database and a similarity matrix, both available on the Internet ~. 1Available at www.cs.ualberta.ca/-lindelddemos.htm. A dependency relationship [9][10][16] is an asymmetric binary relationship between a word called bead, and another word called modifier. The structure of a sentence can be represented by a set of dependency relationships that form a tree. A word in the sentence may have several modifiers, but each word may modify at most one word. The root of the dependency tree does not modify any word. It is also called the head of the sentence. For example, the following diagram shows the dependency tree for the sentence "John found a solution to the problem". The links in the diagram represent dependency relationships. The direction of a link is from the head to the modifier in the relationship. Labels associated with the links represent types of dependency relations. We define a collocation to be a dependency relationship that occurs more frequently than predicted by assuming the two words in the relationship are independent of each other. In [12], we described a method to create a collocation database by parsing a all the dependency relationships involving w and the frequency counts of the dependency relationships. Table 1 shows excerpts of the entries in the collocation database for the words duty and responsibility. For example, in the corpus from which the collocation database is constructed, fiduciary duty occurs 319 times and assume [the] responsibility occurs 390 times. The entry of a given word in the collocation database can be viewed as a feature vector for that word. Similarity between words can be computed using the feature vectors. Intuitively, the more features that are shared between two words, the higher the similarity between the two words. This intuition is captured by the Distributional Hypothesis [8]. 
Table 2. The top 20 most similar words of duty and eat as given by OLin, 1998b). WORD SINnLAR WORDS (WITH SIMILARITY SCORE) 
DUTY responsibility 0.182, obligation 0.138, job 0.127, 
EAT cook 0.127, drink 0.108, consume 0.101, feed 0.094, Features of words are of varying degree of importance. For example, while almost any noun can be used as object of include, very few nouns can be modified by fiduciary. Two words sharing the feature object-of-include is less indicative of their similarity than if they shared the feature modified-by-fiduciary. The similarity measure proposed in [13] takes this into account by computing the mutual information between two words involved in a dependency relationship. Using the collocation database, Lin [13] used an unsupervised method to construct a similarity matrix. Given a word w, the matrix returns a set of similar words of w along with their similarity to w. For example, the 20 most similar words of duty and eat are shown in Table 2. of feature vectors. Each unique word corresponds to a vector and each distinct dependency relationship that involves the word corresponds to a feature. For the newspaper corpus used in our experiments, we collected over 146,290 unique words and 20,173,092 features (1,639,996 unique). Although there are many clustering algorithms that take feature vectors as input, none seems to be able to handle the high dimensionality and the large number of elements. to 20) subsets, which may overlap. We first use CLIMAX, a heuristic maximal-clique algorithm, to find clusters for each subset. We then apply the UNICON algorithm (Section 4.2) to merge and cluster the non-overlapping clusters returned by CLIMAX. Figure 1 outlines the CLIMAX algorithm. In Step 1, a sequential greedy heuristic [3] is used to compute Ce since finding the maximum clique is an NP-complete problem [2]. Because we use a heuristic and, more importantly, since we limit the number elements to be clustered at any given point, the maximum-clique-based clustering algorithm can be executed efficiently. For example, in our experiments, the set E contained about 20,000 Input: Step 1: Step 2: Step 3: Output: words. Step 1 took less than a minute to run on a Pentium III 700Mhz processor. The output of CLIMAX is a set of small clusters. Many of them are closely related. For example, two of the clusters returned by CLIMAX are: Nq34 and Nq184 are automatically generated names for the clusters. The number after each word in the clusters is the similarity between the word and the centroid of that cluster. The UNICON algorithm computes the centroids of the clusters and merges clusters whose centroids are very similar. The sets of clusters to be merged are identified by applying the CLIMAX algorithm to the similarity matrix of the centroids. Figure 2 outlines UNICON, our algorithm for unsupervised induction of concepts. Output: C, a list of clusters. In Step 2, we compute the centroid of each cluster. A centroid the words in the cluster. The averages are weighed by the frequency of the words so that the centroid can be treated as a pseudo-word. We can then filter out those features whose mutual information is below a fixed threshold (0.5 was used in our experiments). Sometimes, a very frequent word within a cluster may hijack the other words). Consider the following cluster in the output of CLIMAX: {degree, master's degree, doctorate, bachelor's degree, law degree, Ph.D., MBA, M.B.A.} The word degree is much more frequent than the other words. 
Consequently, the centroid for this cluster represents a mixture of all different meanings of degree. After removing the word the regenerated centroid represents academic degrees and has a higher total similarity to the members in the cluster. 
In Step 2, we recompute the centroid after all possible removals of members of a cluster. If the total similarity between the centroid and the members of the cluster decreases, the removed matrix, M', is obtained by creating a new collocation database consisting of the eentroids (pseudo-words). This matrix is used in Step 4 to determine which clusters should be merged. After adding the centroids to the original collocation database in words. The purpose of Step 8 is to expand the coverage of the clusters. For example, in one of our experiments, at the end of Step 8, the number reaches 89,226. Many words added in Step 8 to its centroid higher than a fixed threshold. For example, the wordpizza is added to the cluster {gin, bourbon, whiskey, vodka, rum, brandy, scotch} because it shares the following features with the cluster: producer-of, market-for, object-of-buy, object-of-enjoy, object-of-like, object-of-market, object-of-order, object-of-sell, object-of -serve .... One problem that plagued previous word similarity methods was that one always had to rely on an arbitrarily chosen threshold to determine similarity boundaries. In our approach, the responsibility of the threshold is reduced. If a very good cluster for a word w is found, it is used to remove membership ofw from other clusters to which it is less similar. For example, the highest similarity to the cluster: Consequently, pizza is removed from the hard liquor in the previous section. To classify a previously unknown word into a cluster, we simply have to compare the feature vector for this word with the cluster centroids. For example, the centroid for the cluster of news agencies, {adn, Tanjug, PAP, CTK, Xinhua, MTI, Prensa Latina, IRNA, Islamic Republic News Agency, Xinhua News Agency, Tehran Radio, Notimex, Yonhap, Press Association, Kyodo, Interfax, 
Excelsior }, contains features such as correspondent-for, object-of-monitor, object-of-quote, subject-of-report, modified-by-official, modifies-news-service, modifies-commentary, modifies-news-agency, modified-by-national, modified-by-state-run .... When other words have many of these features, they can be classified as a news agency. Since the centroid of a cluster is computed by averaging the frequency counts of features of the members, the more important features for the cluster tend to have higher frequency counts and mutu~ information. This makes it easier to deal with words th~ have a very smfll number of ~atures. Consider the word Venpres 2. It occurred in our corpus four times and has the following ~atures: subject-of-say, modified-by-news-agency, modified-by-official, modified-by-Venezuelan, modified-by-the The top ten similar words of Venpres obtained by [ 13] are: State Statistical Institute State Statistics Bureau Ministry for Economic Affairs DPA National Institute for Statistics National Steel Manufacturers Assoc. Bangladesh Sangbad Sangstha Statistics Department Central Bureau for Statistics Orbe Only two of the similar words, DPA and Orbe, are news agencies. The other words have high similarity to Venpres because they have the features subject-of-say and modified-by-the. When comparing the similarity between Venpres and all centroids, the most similar cluster is the news agency cluster shown in the previous subsection. of our system. We used two corpora for evaluating UNICON: NEWS consisting of a 1GB general newspaper corpus and MEDLINE consisting of a 54-million word corpus of Medline abstracts. We used Minipar 3, about 500 words per second on a PIII-750 with 500MB memory. Table 3 shows the running-time of the algorithm (not including the construction time for the collocation database and word similarity matrix) and some results from our UNICON algorithm. We evaluate the capacity of the UNICON algorithm to classify words into existing clusters and the quality of the cluster centroids. We sorted the words in the NEWS and MEDLINE corpora that occurred a minimum of 100 times and selected every 150 th (for NEWS) and every 75 th (for MEDLINE) word. For each test word, we computed the similarity between it and all the clusters. Up to two of the most similar clusters were extracted. Each cluster is represented by up to 5 words in the cluster. We then gave the output to human judges for evaluation. Below is a sample of the test data given to judges. Cluster: {River lake creek ocean stream} 
Cluster: {Giants A's dodger warrior brave} 2 Venpres is a Venezuelan news agency. 3Available at www.cs.ualberta.ca/-lindek/minipar.htm. Table 3. Description of our experiments with the NEWS and MEDLINE corpus. Two judges inspected the NEWS corpus. For the MEDL1NE corpus, two medical doctors performed the evaluation as a team. Each evaluator assigned a score between 1 and 5. Below, we describe each classification and provide an example. 1. The cluster is non-sensical and no determination for the test 2. The test word does not fit well in the cluster. 3. Undecided 4. The test word fits with the general sense of the cluster. 5. The test word fits perfectly with the cluster. The judges also had another category for unknown examples. Table 4 illustrates the result of the experiment. The average difference is the average absolute difference between each classification from the two human judges. In our experiments, 913 concepts were induced by UNICON. Table 5 shows three of them. The first column contains the pseudo-words representing the names of the concepts and the third column shows the members of each concept. The members For the sake of space, we omit the similarity values. We presented an unsupervised algorithm, UNICON, for inducing previous distributional approaches. The main contributions of our algorithm are:  X  we output a set of concepts instead of a similarity matrix; * we are able to deal with a large number of elements in a  X  we are able to classify words with few features; and 
Table 4. Evaluation of cluster centroids. Two judges evaluated the NEWS corpus while a team of two medical doctors evaluated the MEDLINE corpus. Total Examples 155 132 Average Score 4.26 3.37 
Average Difference 0.34 N/A  X  we are able to classify previously unknown words into 
We plan to use the automatically induced concepts to automatically generate verb usage templates. For example, for the word express, we may want to generate a template such as: Nq4 is a cluster of events, and Nq198 is a cluster of media. UNICON does not generate a hierarchy among its output concepts. However, manual inspection of the output does show that there are many interesting semantic relationships among the clusters of general person names but also clusters of movie stars, U.S. senators and representatives, baseball players, names of well-known criminals, U.S. Justices, and government officials. Also, there are clusters of general company names, but we find clusters of high-tech companies, automakers, U.S. banks and international banks. It would be very interesting to automatically discover such relationships between these concepts. The authors wish to thank the reviewers for their helpful comments. This research was partly supported by Natural Sciences and Engineering Research Council of Canada grant OGP 121338 and scholarship PGSB207797. 
