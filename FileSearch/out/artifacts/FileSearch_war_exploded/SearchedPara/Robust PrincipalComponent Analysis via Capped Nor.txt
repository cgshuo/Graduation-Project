 In many applications such as image and video processing, the data matrix often possesses simultaneously a low-rank structure capturing the global information and a sparse com-ponent capturing the local information. How to accurately extract the low-rank and sparse components is a major chal-lenge. Robust Principal Component Analysis (RPCA) is a general framework to extract such structures. It is well studied that under certain assumptions, convex optimiza-tion using the trace norm and  X  1 -norm can be an e ective computation surrogate of the dicult RPCA problem. How-ever, such convex formulation is based on a strong assump-tion which may not hold in real-world applications, and the approximation error in these convex relaxations often can-not be neglected. In this paper, we present a novel non-convex formulation for the RPCA problem using the capped trace norm and the capped  X  1 -norm. In addition, we present two algorithms to solve the non-convex optimization: one is based on the Di erence of Convex functions (DC) frame-work and the other attempts to solve the sub-problems via a greedy approach. Our empirical evaluations on synthetic and real-world data show that both of the proposed algo-rithms achieve higher accuracy than existing convex formu-lations. Furthermore, between the two proposed algorithms, the greedy algorithm is more ecient than the DC program-ming, while they achieve comparable accuracy.
 H.2.8 [ Database Management ]: Database applications-Data Mining Algorithm Non-convex optimization, DC programming, ADMM, low-rank, sparsity, trace norm, image processing
In many applications, we encounter very high-dimensional data such as images, texts, and genomic data. Analysis of such data is challenging due to the curse of dimensionality. One promising approach is to exploit the special structures of the data and it has recently achieved great success in many applications [2, 8, 14, 19, 24]. Two particularly inter-esting structures are the low-rank structure and the sparse structure. For example, the images of the same scene may be taken from di erent illusions, thus the shadows represent the sparse component and the scene relates to the low-rank part [1]. For a collection of text documents, the low-rank component could capture common words used in all the doc-uments while the sparse component may capture the few key words that best distinguish each document from others [6]. Robust Principal Component Analysis (RPCA) [6], or Sta-ble Principal Component Pursuit (SPCP) [1] is an ecient tool for such analysis and has received increasing attentions in many areas [5, 6, 17, 18, 20, 22].

The most general form of the RPCA problem can be for-mulated as follows: where we assume that the data matrix A 2 R m n is the summation of a low-rank matrix X and a sparse component Y . We minimize the rank of X as well as the number of non-zero entries in Y . Due to the discrete nature of the rank function, such a problem has been proven to be NP-hard. Therefore computing a global optimal solution of (1) is a challenge. Recently, one proper relaxation with theoret-ical guarantees has been proposed in [6]. In particular, the authors approximate the original problem by minimizing the sum of trace norm of X and  X  1 -norm of Y with an equality constraint: where the trace norm  X  X  X  is de ned as the sum of all singular values of X , and  X  Y  X  1 = of absolute values of all entries in Y . Notice that (2) is a convex optimization problem, therefore a global optimal can be computed. Moreover, the authors of [6] also provide an ecient algorithm for (2). It is well known that the trace norm and the  X  1 -norm are capable of inducing low-rank and sparse structures [20], achieving our desired goal. Furthermore, it has been shown in [6] that with the balance param eter equal to 1 p answer.

Problem (2) can recover a low-rank matrix only with sparse corruptions. In practice, as discussed in [5], it is necessary to consider the observed data matrix under more realistic conditions. Particularly, the given data may not only be corrupted by impulse noise which is sparse and large, but also by Gaussian noise which is small but dense. To deal with this more realistic scenario, a modi ed model with tol-erance of both Gaussian noise and impulse noise has been proposed in [20], by changing the constraint from equality to inequality: where denotes the level of the Gaussian noise. The authors proposed to apply the augmented lagrangian approaches to eciently solve problem (3). Based on the same model, an Augmented Lagrange Method of Multipliers (ADMM) is proposed in [13] to solve the optimization problem, which iteratively solves X and Y using soft-thresholding. In [1], the authors developed a non-smooth augmented lagrange method to eciently solve problem (3). There are also many other approaches which aim to solve a similar problem. For example, in [7], the authors develop a notion of rank-sparsity incoherence and uses it to characterize both of the funda-mental identi ability and the sucient conditions for exact recovery. In [23], the authors presented an approach to re-cover the correct column space of the uncorrupted matrix, rather than the matrix itself.

To our best knowledge, most of the recent research focuses on solving RPCA via solving a convex problem with certain constraints [5, 17, 18, 22]. By contrast, this paper tack-les this interesting problem from a di erent point of view: non-convex optimization using a mixture of capped trace norm and capped  X  1 -norm. We propose two algorithms to solve the non-convex formulation. First, we apply the Dif-ference of Convex functions (DC programming) framework to iteratively solve the non-convex formulation, and apply ADMM to solve the sub-problem. In our second approach, instead of using ADMM to solve the sub-problem, we present a greedy algorithm to solve the sub-problem, based on which we propose a fast alternating optimization algorithm. Both low-rank part X and sparse component Y can be recovered by our algorithms with higher accuracy than most of the existed work.
 The RPCA formulation has applications in many areas [6]. In this paper, we evaluate the proposed algorithms on syn-thetic data and two real-world applications: background de-tection in surveillance video and shadow removal from illu-minated portraits. In both synthetic experiments and real-world applications, our proposed algorithms achieve better recovery of X and Y than existing convex formulations. In particular, our proposed algorithms can capture the sparse locations with higher accuracy than existing methods.
The rest of the paper is organized as follows. In Section 2, we present our non-convex formulation for the RPCA prob-lem. In Section 3, we present a DC framework to solve our formulation. In Section 4, we develop a fast alternating op-timization algorithm to solve the formulation. In Section 5, we evaluate our algorithms on both synthetic data and real-world applications. We conclude the paper in Section 6.
In this section, we formulate the RPCA problem as a non-convex minimization problem via capped norms.

Given a data matrix A , the goal of RPCA is to extract a low rank X and a sparse component Y from A . Follow-ing (1), we consider a non-convex formulation of RPCA with an inequality constraint: where A 2 R m n is the observed matrix, 2 is the level of Gaussian noise and &gt; 0 is a trade-o parameter between the low rank and the sparse component. Here, the  X  X  X  0 norm is the number of non-zero entries of a matrix. In [1, 20], the trace norm and the  X  1 -norm were used to approximate the rank function and  X  0 -norm to convert the non-convex problem into a convex one. It is noteworthy that the con-vex relaxation may not be a good approximation of (4) in real-world applications. The main motivation of the refor-mulation to be presented in the next subsection is to reduce the approximation error using non-convex formulations.
We rst introduce the capped norms for matrices and vec-tors, which are the surrogates of the rank function and the  X  -norm. Let p = min( m; n ). We can approximate the rank function and the  X  0 -norm by: for so me small parameters 1 ; 2 &gt; 0. We can observe that if all the singular values of X are greater than 1 and all the absolute values of elements in Y are greater than 2 , then the approximation will become equality.

The smaller 1 and 2 are, the more accurate the capped norm approximation would be. We can control the recovery precision via making use of 1 and 2 . By carefully choosing and 2 , we can recovery X and Y more accurately than the trace norm and the  X  1 -norm approximation.
With the aforementioned capped norms, we propose to solve the following non-convex RPCA formulation: minimize subject to  X  A X Y  X  2 F 2 ; where P 1 ( X ) = max( j Y ij j 2 ; 0) :
Clearly, the new objective function in problem (5) is not convex due to the last two terms being concave functions. Howev er, the intrinsic structure of the formulation naturally leads to use of Di erence of Convex (DC) Programming [21]. DC programming treats a non-convex function as the di er-ence of two convex functions, and then iteratively solves it on the basis of the combination of the rst convex part and the linear approximation of the second convex part. Obvi-ously, the trace norm and the  X  1 -norm of matrices are con-vex, and the summation of maximum is also convex. Thus problem (5) exhibits the DC structure. The details of DC programming to solve the RPCA problem are presented in the next section.
In this section, we detail the DC programming framework for solving problem (5). In each iteration of the framework, the rst-order approximation is used to substitute the non-convex part. To generate the rst-order approximation of P ( X ) and P 2 ( Y ) in our formulation, we need to compute the subdi erential of a capped trace norm, as summarized in the following lemma: Lemma 1. The subdi erential of is given by: where U and V are the left and right singular vectors of X , respectively, and and p is the rank of X .

Proof. First, we can denote X = U X V T as the SVD of matrix X where U and V are unitary matrices. Then we de ne auxiliary matrices B = U B V T and C = U V T , where B = Diag( b ) ; b i 2 f 0 ; 1 g and = Diag( ). For simplicity, we denote and Using the notations above, P 1 ( X ) can be written as where E = f s 2 R p : s i 2 f 0 ; 1 gg . We can see that the maximum can be achieved if and only if: The subdi erential should be the convex hull of B [16]: where B = Diag( B ) and This completes the proof of the lemma.

In ad dition, it can be easily shown that: @P 2 ( Y ) =
By denoting U = 1 mulation (5) can be rewritten as: minimize subject to  X  A X Y  X  2 F 2 ; where  X  U; X  X  = inal non-convex problem by solving a series of convex prob-lems. In each iteration, we approximate problem (5) by the sub-problem (8) at the current X k and Y k . The key sub-problem is to solve the convex problem (8).
Here, we apply the Augmented Lagrange Method of Mul-tipliers (ADMM) [3] to solve the sub-problem (8). ADMM has been applied successfully to solve many sparse learning problems. We introduce an auxiliary variable S = X and rewrite the problem (8) as: The augmented lagrangian function of (9) is:
L ( S; X; Y; ) = 1 where is the lagrangian multiplier and is the step size of dual update.

The general approach of ADMM consists of the following iterations: Next, we present the details for updating each variable in (10). The up date of S involves the following problem: wh ich is the proximal operator of the trace norm. It has an analytical solution as summarized in the following lemma [4]:
Lemma 2. The proximal operator associated with the trace norm, i.e., the minimizer of the following problem: is giv en by: where = ( 1 ; : : : ; p ) are the singular values of A and A = U Diag ( ) V T is the SVD of A .
 From Lemma 2, if we denote the SVD of matrix X k + k as U s s V T s , where s = Diag( ), and p i = max( i 1 1 ; 0), then S k +1 ca n be obtained by S k +1 = U s Diag( p ) V T
The update of X and Y amounts to solving: By introducing a lagrangian multiplier for the inequality constraint, the lagrangian function is given by: Taki ng partial derivatives of L with respect to X and Y results in: { where D Y is the subdi erential of  X  Y  X  1 . Setting both par-tial derivatives to 0, we have where C is a constant. It is easy to verify that (13) is pre-cisely the rst-order optimality condition for the following problem: which has a closed-form solution summarized below [9]:
Lemma 3. The proximal operator associated with the  X  1 -norm, i.e., the minimizer of the following problem: is given by: where the  X  1 -norm of a matrix is given by the summation of the absolute value of all elements.
 From Lemma 3, the solution of problem (13) can be obtained And from the relation between X and Y , we can get X k +1 Since there is no direct way to calculate , we use the binary search to nd the proper .

Thus, we obtain the solution of sub-problem (9), which is one iteration of DC framework. In the DC framework, a series of sub-problem (9) are solved iteratively. The details are summarized in Algorithm 1.
 Algori thm 1 Robust PCA via DC programming Requi re: X 0 , Y 0 , 1 , 2 , Ensure: an optimal solution X and Y 1: while not converge do 2: Calculate P 1 ( X ) = 3: Compute @P 1 and @P 2 according to (6) and (7) 4: Apply ADMM to solve problem (9): 5: for j = 1 to M axIter do 6: update S using (11) 7: update X and Y using (14) 8: update using (10) 9: end for 10: end while
The DC programming is known to have a slow conver-gence rate. In this section, we propose an algorithm based on alternating optimization which is practically much faster than DC programming. Notice that problem (5) is equiva-lent to: minimize sub ject to  X  A X Y  X  2 F 2 : In the proposed algorithm, we iteratively x one variable and compute the other one.
When X is xed, computing the optimal Y amounts to solving the following sub-problem after dropping constants and changing variables: where Z = A X . It is easy to see that we only need to consider the situation that Z 0. Moreover, notice that Y = Z i we can always reach another feasible solution with equal or less objective value. Therefore we can assume that the optimal Y satis es Y Z . In addition, we may further assum e Y and Z are both represented in a vector which is formed by stacking all columns of the matrix.

It is not hard to see that the objective function of prob-lem (16) is non-convex and it is usually very dicult to nd a globally optimal solution. We rst present our method in Algorithm 2. Then we will show that, given any feasible solution of (17), we can always improve it to get a better local solution through our Algorithm 2.
 Algo rithm 2 An approximate algorithm for solving (16) Requi re: Y , Z , Ensure: an optimal solution Y 1: if  X  Z  X  2 F 2 then 2: return 0 3: end if 4: Initialize Y by Z . 5: Sort Y in increasing order 6: i = 1 7: while 2 &gt; 0 do 8: if &gt; Y i then 9: = 1 0: else 11: = 0 ; Y i = Y i . 12: end if 13: i = i + 1 14: end while 15: return Y
An intu itive example is presented in Figure 1. We rst initialize Y by Z and sort Y such that the elements of Y form a non-decreasing sequence. The key idea behind our algorithm is as follows: among all the solutions of (16), there must be one Y such that the elements of Y preserves the order of Z .

Lemma 4. Let Y be one feasible solution of (16) such that there exist indices i and j satisfying Z i &lt; Z j and Y Y . There always exists another local solution ^ Y such that ^ Y i ^ Y j and ^ Y k = Y k for all k  X  = i; j .

Proof. If there exist indices i and j such that Z i &lt; Z and Y i &gt; Y j . Let ^ Y i = Y j , ^ Y j = Y i and ^ Y k k  X  = i; j , then we have:  X  Y Z  X  2 =  X  Y Z  X  2 =  X  Y Z  X  2 =2 Y j Z i + 2 Y i Z j 2 Y j Z j 2 Y i Z i =2( Y i Y j )( Z j Z i ) 0 : The above result essentially shows that exchanging Y i and Y j will not violate the constraint and clearly the objective remains unchanged. Therefore we nd an alternative feasi-ble solution that preserves the order of elements in Z .
Lemma 5. L et Y be one feasible solution of (16) such that there exists an index i satisfying 0 &lt; Y i &lt; Y There always exists another feasible solution ^ Y such that ^ Y k = Y k for all k  X  = i; i +1 and either ^ Y i = 0 or ^ Y holds.
 Lemma 4 essentially states that the optimal solution pre-serves the order in Z and Lemma 5 shows that we can ob-tain a solution such that there exists an index i such that Y j = 0 for all j &lt; i and Y j = Z j for all j &gt; i . Then it is straight-forward to show that our algorithm provides a better local solution.
 Figure 1: Illustration of Algorithm 2. Y is initialized as Z . We rst sort the entries of Y to form a non-decreasing sequence f Y i g , then reduce Y as much as possible sequentially within the threshold , nally put Y back in the original order.
When Y is xed, the optimal X can be obtained via solv-ing the following equivalent problem: Let Z = U V T be the SVD of Z . For any feasible solution X , let ~ U ~ ~ V T be its SVD. We can observe that: where the second equation follows from the fact that the Frobenius norm is unitary-invariant and we use the Von Neu-mann's trace inequality [15] to obtain the rst inequality. The conclusion above essentially shows that the optimal X shares the same left and right singular vectors with Z . Us-ing the unitary-invariant property of the Frobenius norm we can conclude that (17) is equivalent to: which is exactly in the same form as (16) and therefore can also be computed via Algorithm 2. In this section, we compare our proposed algorithms with ALM [13] and NSA [1] on synthetic data and real-world data sets. The ALM algorithm formulated RPCA as a convex problem with an equality constraint and applied augmented lagrange multiplier method to solve it. The NSA algorithm formulated the RPCA as a convex problem with an inequal-ity constraint, and investigated a non-smooth augmented lagrange algorithm to solve it.
 Our experiments were executed on a PC with Intel Core2 Quad Q8400 2 : 66G CPU and 8G RAM. The ALM code was downloaded from the Perception and Decision Laboratory, University of Illinois ( http://perception.csl.illinois. edu/ ), and the NSA code was kindly provided to us by the author. We implemented both of the DC framework and fast alternating algorithm in MATLAB.
In this experiment, we compare di erent algorithms on synthetic data. We generate the rank-r matrix X 0 as a product of LR T , where L and R are independent n r ma-trices whose elements are i.i.d. random variables sampled from standard Gaussian distributions. We generate Y 0 as a sparse matrix whose support is chosen uniformly at ran-dom, and whose non-zero entries are i.i.d. random variables sampled uniformly in the interval [ 100 ; 100]. We set the standard Gaussian noise level at to simulate N (0 ; 2 ). The matrix A = X 0 + Y 0 + is the input to the algorithms.
In our empirical study, we set 1 = 2 = 0 : 01 and 2 =  X  lize the NSA results to initialize X and Y in our algorithms. Note that it is very common (in fact it is recommended) to use convex relaxation solutions as initial solutions for non-convex formulations [10, 25].

There are three key parameters in the process of gener-ating A , namely, n , and c r : n is the dimension of A ; c represents the ratio between the rank and the dimension of X in Y 0 , i.e.,  X  Y  X  0 = c p n . To verify the ecacy and robust-ness of di erent approaches, we proceed our experiments in three directions, by varying di erent parameters:
Following the aforementioned three directions, the exper-imental results of di erent approaches are presented in Ta-ble 1, Table 2 and Table 3. In each table, comprehensive results of the recovery errors related to X and Y as well as the accuracy of capturing the sparse location are demon-strated. In particular, the computation time with respect to dimensions is summarized in Table 1.

Under all these conditions, our alternating algorithm out-performs ALM and NSA in terms of the rank of X and the sparsity of Y . In particular, the rank of X produced by our alternating algorithm is consistent with the value we gen-erated through c r in the case of = 0 : 001. Moreover, for capturing the sparse locations in matrix Y , our DC algo-rithm and alternating algorithm both perform much better than ALM and NSA. Especially, in the case of = 0 : 001, n = 100, compared to ALM (20 : 35%) and NSA (52 : 57%), we obtain 99 : 45% (DC) and 98 : 73% (alternating) accuracy which nearly capture all of the sparse locations in Y . Fur-thermore, our alternating algorithm achieves 81 : 42% accu-racy in the case of = 0 : 01, showing its robustness to noise.
We can observe that the computation time of our DC framework is longer than ALM and NSA. However, our al-ternating algorithm has comparable execution time and is much faster than DC framework. Therefore, in the follow-ing real-world applications, we only focus on our alternating algorithm.
In this experiment, we apply di erent approaches on the background separation for an airport surveillance video [12]. The dataset contains a sequence of 201 grayscale frames of size 144 176 during a time period. To form the matrix A , we stack the columns of each frame into a vector and con-catenate the vectors together. We manually add the Gaus-sian noise by assuming only impulse noise contained in the video. The Gaussian noise is set at 20 dB signal-to-noise ra-tio (SNR) and =  X  A  X  F p is generated through A  X  = A + G , where G 2 R 25344 201 and each entry of G is generated i.i.d. from the standard Gaussian distribution.

The results of di erent algorithms are presented in Ta-ble 4. In addition, we show the results of three frames of the video in Figure 2. Each of Figure 2(a), Figure 2(b), Figure 2(c) represents the results of ALM, NSA and our al-gorithm respectively. In each gure, the rst row represents the 15-th, 150-th and 200-th frame after adding noise, the second row represents background and the third row relates to the people in the video. Notice that there is one person who is identi ed as part of background; this is due to the fact that he/she did not move at all during the period we focus on.

From Figure 2, we conclude that all of the three algo-rithms can successfully extract background from surveillance video. Even though the visual qualities of background and foreground are similar among di erent algorithms, the nu-merical measurements in Table 4 demonstrate that our alter-nating approach performs better than both ALM and NSA in terms of the low rank and sparsity. In particular, the rank of our X is 67, which is about half of the rank of X computed by ALM or NSA. Table 2: Synthetic results for varying with xed n = 100 and c than ALM and NSA in terms of capturing sparse locations in Y . to illumination are shown in the right column.
 Table 4: Recovery results of airport surveillance. Our algorithm produces X with a lower rank and Y with a smaller  X  0 -norm, while keeping the relative error comparable with ALM and NSA.

Another interesting application of RPCA is to remove shadows and specularities from face images [6]. Often times, portraits are taken under di erent illuminations which intro-duce errors to face recognition. If we have enough images under di erent illuminations of the same face, we can apply RPCA to extract the features of the face and remove the illumination errors.
 We compare di erent algorithms on the YaleB face data [11]. The images of the dataset are of size 192 168, and there are 64 illuminations for each subject. Illuminations vary from both azimuth and elevation, so the shadows for each subject are di erent in both location and intensity. Consid-ering one subject, the matrix A is constructed by concate-nating 64 images under all illuminations together. We add 20 dB signal-to-noise ratio which is similar to surveillance video. We obtain a noisy A  X  through A  X  = A + G , where G 2 R 32256 64 and each entry of G is generated i.i.d. from the standard Gaussian distribution. In this application, the low rank part would represent human face, while the sparse component is the shadow induced by di erent illuminations.
Considering Subject01, Subject05 and Subject10 in the dataset, the results of all three algorithms are shown in Fig-ure 3. For better comparison, each sub gure represents one subject. And the three columns represent observation im-age, low-rank recovery and sparsity illumination from left to right. Though the visual quality of di erent algorithms are similar, our algorithm obtains a lower rank of X and a smaller  X  0 -norm of Y than ALM and NSA, while keeping the relative error comparable, as demonstrated in Table 5. Table 5: YaleFaceB recovery results. Our algorithm produces X with a lower rank and Y with a smaller  X  -norm, while keeping the relative error comparable with ALM and NSA.
 Sub ject01 Sub ject05 Sub ject10
This paper investigates a non-convex formulation for Ro-bust Principle Component Analysis (RPCA) by the capped trace norm and the capped  X  1 -norm. We develop a DC framework as well as a fast alternating algorithm to solve the non-convex formulation. In the DC framework, ADMM is applied to solve the sub-problem, while in our fast alter-nating algorithm, a greedy algorithm for solving the sub-problem is developed based on the combinatorial optimiza-tion. We have performed extensive experiments on both synthetic and real-world datasets. Results show that our proposed approaches perform better in recovering the low-rank part and the sparse component of a given matrix than existing work.

The current work assumes that the complete data is given, i.e., there are no missing entries in the data. However, in many real applications the data may come with missing val-ues. We plan to extend our proposed algorithms to solve the RPCA problem with missing entries in the future. In addition, we plan to study the theoretical properties of the proposed non-convex formulation.
The authors are grateful to Professor Necdet Serhat Aybat from Pennsylvania State University for providing the NSA code for comparison. This work was supported in part by NIH (LM010730) and NSF (IIS-0953662). [1] N. Aybat, D. Goldfarb, and G. Iyengar. Fast [2] L. Beno^t, J. Mairal, F. Bach, and J. Ponce. Sparse [3] S. Boyd, N. Parikh, E. Chu, B. Peleato, and [4] J. Cai, E. Candes, and Z. Shen. A singular value [5] J. Cai, E. Candes, and Z. Shen. A singular value [6] E. Candes, X. Li, Y. Ma, and J. Wright. Robust [7] V. Chandrasekaran, S. Sanghavi, P. Parrilo, and [8] J. Chen, J. Liu, and J. Ye. Learning incoherent sparse [9] D. Donoho. De-noising by soft-thresholding. IEEE [10] J. Fan, L. Xue, and H. Zou. Strong oracle optimality [11] A. Georghiades, P. Belhumeur, and D. Kriegman. [12] L. Li, W. Huang, I. Gu, and Q. Tian. Statistical [13] Z. Lin, M. Chen, and Y. Ma. The augmented lagrange [14] J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. [15] L. Mirsky. A trace inequality of john von neumann. [16] Y. Nesterov. Introductory lectures on convex [17] Y. Peng, A. Ganesh, J. Wright, W. Xu, and Y. Ma. [18] B. Recht, M. Fazel, and P. Parrilo. Guaranteed [19] A. Singer and M. Cucuringu. Uniqueness of low-rank [20] M. Tao and X. Yuan. Recovering low-rank and sparse [21] P. Tao and L. An. Convex analysis approach to dc [22] K. Toh and S. Yun. An accelerated proximal gradient [23] H. Xu, C. Caramanis, and S. Sanghavi. Robust pca via [24] J. Ye and J. Liu. Sparse methods for biomedical data. [25] T. Zhang. Analysis of multi-stage convex relaxation
