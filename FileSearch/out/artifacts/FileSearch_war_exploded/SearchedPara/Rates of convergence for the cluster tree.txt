 however, this kind of analysis has proved more elusive.
 this procedure is run on points X an interesting or desirable quantity, in settings outside o f vector quantization? an (infinite) hierarchy called the cluster tree (Figure 1).
 the parameters of interest. B ( x, r ) be the closed ball of radius r around x . 2.1 The cluster tree S components . We say S  X  X  is connected if it has a single connected component. occasionally call a subpartition of X . Write  X ( X ) = { subpartitions of X} . Definition 1 For any f : X  X  R , the cluster tree of f is a function C Any element of C For any  X  , C Lemma 2 Pick any  X   X   X   X  . Then: 2.2 Notion of convergence and previous work Suppose a sample X [5] provided a very natural notion of consistency for this se tting. Definition 3 For any sets A, A  X   X  X  , let A components of { x : f ( x )  X   X  } (for some  X  &gt; 0 ), P ( A It is well known that if X sup x | f n ( x )  X  f ( x ) |  X  0 ), then the cluster tree C f The big problem is that C Lane [14] have an efficient procedure that tries to approxima te C that A  X  X [7] closes the gap and shows fractional consistency wheneve r this ratio is &gt; 1 . proof of convergence. Thus it is unclear how to set k , for instance. consistency of this method has not yet been established.
 makes possible an analysis that has minimal assumptions on t he data. Figure 3: Algorithm for hierarchical clustering. The input is a sample X suggested  X  = 1 and larger k . k  X  d log n , which we conjecture to be the best possible, if  X  &gt; (  X  = 1 , k  X  d log n ) yields consistency. 3.1 A notion of cluster salience  X  around the clusters.
 Definition 4 For Z  X  R d and  X  &gt; 0 , write Z An important technical point is that Z between them. To keep things simple, we X  X l use the same  X  as a separation parameter. exists S  X  X  (separator set) such that: Under this definition, A is zero. However, S 3.2 Consistency and finite-sample rate of convergence 1  X   X   X  2 and k  X  (2 / X  ) d d log n .
 run the algorithm on a sample X and some  X  &gt; 0 ), and if where v take k 3.3 Analysis: separation The cluster tree algorithm depends heavily on the radii r k neighbors lie (including x itself). Thus the empirical probability mass of B ( x, r show that r set of basic inequalities that we use repeatedly.
 Lemma 7 Assume k  X  d log n , and fix some  X  &gt; 0 . Then there exists a constant C probability &gt; 1  X   X  , every ball B  X  R d satisfies the following conditions: Here f P Lemma 8 Pick 0 &lt; r &lt; 2  X / (  X  + 2) such that (recall that v P at least k neighbors within radius r . Likewise, any point x  X  S and thus, by Lemma 7, has strictly fewer than k neighbors within distance r . For (2), since points in S edge across S Definition 9 Define r (  X  ) to be the value of r for which v To satisfy the conditions of Lemma 8, it suffices to take k  X  4 C 2 Figure 4: Left: P is a path from x to x  X  , and  X  ( x distance r of x perpendicular to x 3.4 Analysis: connectedness We need to show that points in A (and similarly A  X  ) are connected in G Lemma 10 Suppose 1  X   X   X  2 . Then with probability  X  1  X   X  , A  X  X whenever r  X  2  X / (2 +  X  ) and the conditions of Lemma 8 hold, and  X  to be as small as possible. A more refined argument shows that  X   X   X  2 is enough. Theorem 11 Suppose  X  2  X  2(1 +  X / A  X  X n is connected in G r whenever r  X   X / 2 and the conditions of Lemma 8 hold, and P a unit direction u , and is the indicator function of the set entirely in A has VC dimension 2 d ), we can then conclude (as in Lemma 7) that if (  X / 4) v Pick any x, x  X   X  A  X  X x 0 = x, x 1 , x 2 , . . . This will confirm that x is connected to x  X  in G The sequence x The process eventually stops because each  X  ( x formally, P  X  1 (  X  ( x the function P , there are points further along the path (beyond  X  ( x must terminate, so the sequence { x Each x successive points is: where the second-last inequality comes from the definition o f slab. To complete the proof of Theorem 6, take k = 4 C 2 (Definition 9) then translates into algorithm reaches roughly ( k/ (  X v we need this radius to be at most  X / 2 ; this is what yields the final lower bound on  X  . The number of samples it requires to capture clusters at dens ity  X   X  is, by Theorem 6, therefore, is in constants involving d .
 Theorem 12 Pick any  X  in (0 , 1 / 2) , any d &gt; 1 , and any  X ,  X  &gt; 0 such that  X v there exist: an input space X  X  R d ; a finite family of densities  X  = {  X  X such that A the following additional property.
 Consider any algorithm that is given n  X  100 i.i.d. samples X probability at least 1 / 2 , outputs a tree in which the smallest cluster containing A from the smallest cluster containing A  X  P regions: a cylinder X probability mass. Let B radius  X  . The cylinder X We will construct a family of densities  X  = {  X  appendix): it is  X ((log |  X  | ) / X  ) , for  X  = max The family  X  contains c  X  1 densities  X  Here is a sketch of  X  For any i 6 = j , the densities  X  Now define the clusters and separators as follows: for each 1  X  i  X  c  X  1 , Thus A that A X and the National Science Foundation for support under grant IIS-0347646. [2] T. Cover and J. Thomas. Elements of Information Theory . Wiley, 2005.
