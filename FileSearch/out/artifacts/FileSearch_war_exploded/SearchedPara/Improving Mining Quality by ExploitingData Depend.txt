 The usefulness of knowledge models produced by data mining methods critically de-pends on two issues. (1) Data quality : Data mining tasks expect to have accurate and complete input data. But, the reality is that in many situations, data is contaminated, or is incomplete due to limited bandwidth for acquisition. (2) Model adequacy :Many data mining methods, for efficiency consideration or design limitation, use a model in-capable of capturing rich relationships embedded in data. The mining results from an inadequate data model will generally need to be improved.
 data samples. For example, the readings of nearby sensors are correlated, and pro-teins interact with each other when performing crucial functions. Data dependency ploited to remedy the problems mentioned above. We study this in several typical scenarios.
 Low Data Quality Issue. Many data mining methods are not designed to deal with noise or missing values; they take the data  X  X s is X  and simply deliver the best results obtainable by mining such imperfect data. In order to get more useful mining results, contaminated data needs to be cleaned, and missing values need to be inferred. Data Contamination. An example of data contamination is encountered in optical character recognition (OCR), a technique that translates pictures of characters into a machine readable encoding scheme. Current OCR algorithms often translate two adja-cent letters  X  ff  X  into a  X #  X  sign, or incur similar systematic errors. tify and correct the errors. This is doable because the errors are introduced according to certain patterns. The error patterns in OCR may be related to the shape of individual characters, the adjacency of characters, or illumination and positions. It is thus possible to correct a substantial number of errors with the aid of neighboring characters. Data Incompleteness. A typical scenario where data is incomplete is found in sensor networks where probing has to be minimized due to power restrictions, and thus data is incomplete or only partially up-to-date. Many queries ask for the minimum/maximum values among all sensor readings. For that, we need a cost-efficient way to infer such extrema while probing the sensors as little as possible.
 latter basically learns a predictive model using available data, then uses that model to predict the missing values. The model training there does not consider data correla-tion. In the sensor problem, however, we can leverage the neighborhood relationship, as sensor readings are correlated if the sensors are geographically close. Even knowl-edge of far-away sensors helps, because that knowledge can be propagated via sensors deployed in between. By exploiting sensor correlation, unprobed sensors can be accu-rately inferred, and thus data quality can be improved.
 Inadequate Data Model Issue. Many well known mining tools are inadequate to model complex data relationships. For example, most classification algorithms, such as Naive Bayes and Decision Trees, approximate the posterior probability of hidden variables (usually class labels) by investigating on individual data features. These discriminative models fail to model the strong data dependencies or interactions.
 known to interact with some others to perform functions, and these interactions connect genes to form a graph structure. If one choose to use Naive Bayes or Decision Trees predict unknown protein functions, he is basically confined to a tabular data model, and thus have lost rich information about interactions.
 tation for structuring complex relationships, and thus a solution for handling proba-bilistic data dependency. In addition, efficient techniques are available to do inference on Markov networks, including the powerful Belief Propagation [15] algorithm. The power in modeling data dependency, together with the availability of efficient inference tools, makes Markov networks very useful data models. They have the potential to en-hance mining results obtained from data whose data dependencies are underused. Our Contributions. The primary contribution of this paper is that we propose a unified approach to improving mining quality by considering data dependency extensively in data mining. We adopt Markov networks as the data model, and use belief propagation for efficient inference. This paper may also contribute to data mining practice with our investigations on some real-life applications.
 Outline. We describe Markov networks in the next section. Also discussed there are pairwise Markov networks, a special form of Markov network. Pairwise Markov net-works not only model local dependency well, but also allow very efficient computation by belief propagation. We then address the above-mentioned examples in sections 3 and 4. 1 We conclude the paper with related work and discussion in Section 5. Markov networks have been successfully applied to many problems in different fields, such as artificial intelligence [10], image analysis [13] and turbo decoding [7]. They have the potential to become very useful tools of data mining. 2.1 Graphical Representation The Markov network is naturally represented as an undirected graph G =( V, E ) , where V is the vertex set having a one-to-one correspondence with the set of random variables X = { x i } to be modeled, and E is the undirected edge set, defining the neighbor-hood relationship among variables, indicating their local statistical dependencies. The local statistical dependencies suggest that the joint probability distribution on the whole graph can be factored into a product of local functions on cliques of the graph. A clique is a completely connected subgraphs (including singletons), denoted as X C . This fac-torization is actually the most favorable property of Markov networks.
 potential function  X  X C ( x C ) is a function on the possible realization x C of the clique X C . Potential functions can be interpreted as  X  X onstraints X  among vertices in a clique. They favor certain local configurations by assigning them a larger value. where Z is a normalizing constant: Z = { x } C  X  X   X  X C ( x C ) 2.2 Pairwise Markov Networks Computing joint probabilities on cliques reduces computational complexity, but still, where our interest involves only pairwise relationships among the variables, we can use use pairwise Markov networks . A pairwise Markov network defines potentials functions only on pairs of nodes that are connected by an edge.
 ables. In the text denoising example discussed in Section 1, for example, the underlying segments of text are variables, while the segments in the noisy text we observe are evi-dence. These observed external evidence will be used to make inferences about values of the underlying variables. The statistical dependency between x i and y i is written as a from the external field.
 compatibility function  X  ij ( x i ,x j ) which captures the  X  X nternal binding X  between two neighboring nodes i and j . An example of pairwise Markov networks is illustrated in Figure 1(a), where the white circles denote the random variables, and the shaded circles denote the evidence. Figure 1(b) shows the potential functions  X  () and  X  () . the overall joint probability of a graph configuration in Eq.(1) is approximated by where Z is a normalization factor, and the product over ( i, j ) is over all compatible neighbors. 2.3 Solving Markov Networks Solving a Markov network involves two phases:  X  The learning phase, a phase that builds up the graph structure of the Markov net- X  The inference phase, a phase that estimates the marginal posterior probabilities or pends on specific applications to define the random variables, the neighborhood rela-tionships and further the potential functions. We will look at the learning phase in detail with concrete applications in Sections 3-4.
 [6], mean-field annealing [11], etc. These methods either take an unacceptably long time to converge, or make oversimplified assumptions such as total independence between variables. We choose to use the Belief Propagation method, which has a computation complexity proportional to the number of nodes in the network, assumes only local dependencies, and has proved to be effective on a broad range of Markov networks. 2.4 Inference by Belief Propagation Belief propagation (BP) is a powerful inference tool on Markov networks. It was pi-oneered by Judea Pearl [10] in belief networks without loops. For Markov chains and Markov networks without loops, BP is an exact inference method. Even for loopy net-works, BP has been successfully used in a wide range of applications[8]. We give a short description of BP in this subsection.
 passed between neighboring nodes only, ensuring the local constraints, as shown in Figure 2. The message from node i to node j is denoted as m ij ( x j ) , which intuitively of the same dimensionality as x j .
  X  SUM-product rule , that computes the marginal posterior probability.  X  MAX-product rule , that computes the maximum a posterior probability.
 or the MAX-product rule, neighbor nodes of i except node j .
 puted. With the SUM-product rule, b ( x i ) approximates the marginal probability p ( x i ) , defined to be proportional to the product of the local compatibility at node i (  X  ( x i ) ), and messages coming from all neighbors of node i : where N ( i ) is the neighboring nodes of i .
 ability: In sensor networks, how to minimize communication is among the key research issues. The challenging problem is how to probe a small number of sensors, yet to effectively infer the unprobed sensors from the known. Cost-efficient sensor probing represents a category of problems where complete data is not available, but has to be compensated by inference.
 and use BP to do inference. Each sensor is represented by a random variable in the Markov network. Sensor neighborhood relationships are determined by spatial posi-tions. For example, one can specify a distance threshold so that sensors within the range are neighbors. Neighbors are connected by edges in the network.
 and Oregon [9]. The sensor recordings were collected during 1949-1994. We use 167 sensor stations which have complete recordings during that period. 3.1 Problem Description and Data Representation The sensor recordings were collected in past decades over two states along the Pacific Northwest. Since rain is a seasonal phenomena, we split the data by week and build a Markov network for each week. order to use belief propagation. One can use Gaussian or its variants to compute the are overwhelmed by zeroes, while non-zero values span a wide range. Clearly Gaus-sian is not a good choice for modeling this very skewed data. Neither are mixtures of gaussian, due to limited data. Instead, we prefer to use discrete sensor readings in the computation. The way we discretize data is given in section 3.3.
 x . It is natural to use the likelihood function: 3.2 Problem Formulation A theoretical analysis of the problem will that the problem fits well into the maximum a posterior (MAP) estimation on a Markov chain solvable by belief propagation. Objective: MAP Let X to be the collection of all underlying sensor readings, Y the collection of all probed sensors. Using Bayes X  rule, the joint posterior probability of X given Y is: maximizing the posterior probability to be maximizing the joint probability Likelihood In a Markov network, the likelihood of the readings Y depends only on those variables they are directly connected to: where m is the number of probed sensors.
 Prior Priors shall be defined to capture the constraints between neighboring sensor readings. By exploiting the Markov property of the sensors, we define the prior to involve only the first order neighborhood. Thus, the prior of a sensor is proportional to the product of the compatibility between all neighboring sensors: Solvable by BP By replacing Eqs.(11) and (12) into the objective Eq.(10), we have the joint probability to be maximized: that the objective function is of the form: where Z is a normalizing constant.
 Markov network is factorized into products of localized potential functions. Therefore, it is clear that the problem can be solved by belief propagation. 3.3 Learning and Inference The learning part is to find the  X  () and  X  () functions for each sensor, as defined in Eqs.(7) and (8). The learning is straight-forward. We discretize the sensor readings in the past 46 years, use the first 30 years for training and the rest 16 years for testing. In the discrete space, we simply count the frequency of each value a sensor could pos-sibly take, which is the  X  () , and the conditional frequencies of sensor values given its neighbors, which is the  X  () .
 for each sensor. The first bin is dedicated to zeroes, which consistently counts for over 50% of the populations. The 11 bins are assigned in a way that give roughly balanced number of readings in each bin. This very simple discretization method has been shown to work well in the sensor experiments. More elaborated techniques can be used which may further boost the performance, such as histogram equalization that gives balanced bin population with adaptive bin numbers.
 posterior distribution, as there are loops in the Markov network. However, loopy belief propagation still gives satisfactory results, as we will see shortly. 3.4 Experimental Results We evaluate our approach using Top-K queries. A Top-K query asks for the K sensors with the highest values. It is not only a popular aggregation query that the sensor com-munity is interested in, but also a good metric for probing strategies as the exact answer requires contacting all sensors.
 local maximum a posterior probability computed by belief propagation, as follows. BP-based Probing: 1. Initialization: Compute the expected readings of sensors using the training data. As 2. Probe the selected sensors. 3. True values acquired in step 2 become external evidence in the Markov network. 4. Again, pick the top sensors with the highest expectations for further probing, but 5. Iterate steps 2-4, until beliefs in the network converge. 6. Pick the top K with the highest expectations according to BP MAP estimation. ing strategy as follows: Naive Probing: 1. Compute the expectations of sensors. Pick the top 25% sensors. 2. Probe those selected sensors. 3. Pick the top K .
 On each diagram, the bottom curve shows the probing ratio, and the two curves on the top show the recall rates for raw values and discrete values, respectively. We use the standard formula to compute recall rate. Let S denotes the top-K sensor set returned, and T the true top-K set. then: T using raw values, or discrete values. Discrete recall demonstrates the effectiveness of BP, while raw recall may be of more interest for real application needs. As can be seen from Figure 3, raw recall is lower than discrete recall. This is due to error introduced in the discretization step. We expect raw recall to be improved when a more elaborated discretization technique is adopted.
 in terms of both recall rates, while requiring less probing. On average, the BP-based approach has a discrete recall of 88% and a raw recall of 78 . 2% , after probing only 17 . 5% sensors. The naive recall has a discrete recall of only 79 . 3% , a raw recall of only 64 . 6% , after probing 25% sensors.
 remains the same for other values K =20 , 30 , 40 .
 iterations, and further discussions on how belief propagation works. Local data dependency can not only help infer missing values, as in the sensor example, but can also be exploited to enhance mining results. Many data mining methods, for efficiency consideration or design limitation, use a model incapable of capturing rich relationships embedded in data. Most discriminative models like Naive Bayes and SVM belong to this category. Predictions of these models can be improved, by exploiting local data dependency using Markov networks. The predictions are used as the likelihood proposal, and message passing between variables refines and reinforces the beliefs. Next we show how to improve protein function predictions in this way. 4.1 Problem Description Proteins tend to localize in various parts of cells and interact with one another, in order to perform crucial functions. One task in the KDD Cup 2001 [2] is to predict protein functions. The training set contains 862 proteins with known functions, and the test-ing set includes 381 proteins. The interactions between proteins, including the testing genes, are given. Other information provided specifies a number of properties of indi-vidual proteins or genes that encodes the proteins. These include the chromosome on which the gene appears, phenotype of organisms with differences in this gene, etc. crucial how to learn from interactions. According to the report of the cup organizers, most competitors organized data in relational tables, and employed algorithms that deal with tabular data. However, compared with tables, graphical models provide a much more natural representation for interacting genes. With a Markov network model, inter-actions can be modeled directly using edges, avoiding preparing a huge training table. together.
 by simply leveraging local dependency. The classifier we use is Naive Bayes, which is learned from the relational table. We build a Markov network, in which genes with interactions are connected as neighbors. The  X  () function prediction comes from Naive Bayes, and the  X  () are learned from gene interactions. 4.2 Learning Markov Network We separate the learning of each function, as focusing on one function a time is easier. There are 13 function categories, hence we build 13 Markov networks. To prepare the initial beliefs for a network, we first learn a Naive Bayes classifier, which output a prob-ability vector b 0 () , indicating how likely a gene will perform the function in question. But this way the Naive Bayes classifier is over trusted, make it harder to correct the misclassifications. Instead, we adopt a generalized logistic function, shown in Eq.(16), to blur the margin between the belief on two classes, yet still keeping the prediction decision. In the experiments, we set a =0 . 75 , b =0 . 125 ,  X  =6 , and  X  =0 . 5 . by the correlation between the expression levels of the two encoding genes. At first we tried to related the functions of two genes in a simple way: a positive correlation indi-cates that with a fixed probability both or neither genes perform the function, while a negative correlation indicates that one and only one gene perform the function. This will leads to a simple fixed  X  () function for all interacting genes. But, a close look at the in-teraction tells that 25% of the time this assumption is not true. In reality, sometimes two genes participating in the same function may be negatively correlated; a more influen-tial phenomena is that genes may participate in several functions, hence the correlation is a combined observation involving multiple functions.
 teractions, separately: (a)FF: a group for protein pairs that both perform the function, (b)FNF: a group for pairs that one and only one performs the function, and (c)NFNF: a group for protein pairs that neither performs the function. Thus, the potential function  X  i,j defines how likely to observe a correlation value given for genes x i and x j , under different cases where x i and x j each has the function or not. In our technical report, we plot the distributions of correlation values learned for two functions. The distribution histograms show that correlation distributions differ among the three groups, and are specific to functions as well. 4.3 Experiments Naive Bayes does not perform well on this problem, because it does not model the gene interactions sufficiently, and thus cannot fully utilize the rich interaction information. Taking the average predictive accuracy of all classifiers, one per function, the overall accuracy of Naive Bayes is 88% . Belief propagation improves this to 90% . a subgraph of genes in Figure 4. The white circles represent genes(variables), and the shaded circles represent external evidence. Only training genes have corresponding ex-ternal evidence. The 1 X  X  or 0 X  X  in the circles tell whether a gene has the function in question or not. For interested readers, we also put the gene ID below the circle. The subgraph contains four training genes and five testing genes. All these testing genes were misclassified by Naive Bayes. After receiving strong beliefs from their neighbor-ing genes, four out of five testing genes were correctly classified. The other test gene  X  X 230291 X  was misclassified by both, but Naive Bayes predicted 0% for it to have the function (which is the truth), while belief propagation increased this belief to 25% . KDD cup [2]. First we picked out all the functions we predicted for a gene. If more functions are predicted than the true number (which is actually the number of duplicates of that gene in the test table provided), we remove the ones with the smallest confidence. The final score is the ratio of correct predictions, including both positive and negative predictions. Our final score is 91 . 2% , close to the Cup winner X  X  93 . 6% . Although the winner scored reasonably high, they organized data in relational tables and didn X  X  fully explore gene interactions. We expect that their method could perform better if integrated with our approach to exploit local dependencies between genes.
 complex relationships. To make up for this, they manually created new features, such as computing  X  X eighbors X  within k ( k&gt; 1 ) hops following neighbor links. Even so, these new features can only be treated the same as the other individual features. The rich relationship information in the original graph structure was lost. Graphical models, on the other hand, are natural models for complex relationships. Markov networks together with belief propagation provides a general and powerful modeling and inference tool on problems satisfying local constraints, such as protein function prediction. Data dependency is present in a wide spectrum of applications. In this paper, we propose a unified approach that exploits data dependency to improve mining results, and we approach this goal from two directions: (1) improving quality of input data, such as by correcting contaminated data and by inferring missing values, and (2) improving mining results from a model that ignores data dependency.
 a wide range of problems caused by noise and missing data. For better information retrieval from text, data is usually filtered to remove noise defined by grammatical er-rors [12]. In data warehouses, there has been work on noisy class label and noisy at-tribute detection based on classification rules [16] [14], as well as learning from both labeled and unlabeled data by assigning pseudo-classes for the unlabeled data [1] using boosting ensembles. All this previous work has its own niche concerning data qual-ity. Our work is more general in that it exploits local data constraints using Markov networks.
 of cost-efficient probing. However, their method relies on a global multivariate Gaussian distribution. Global constraints are very strict assumptions, and are not appropriate in many practical scenarios.
 mining quality by considering data dependency extensively in data mining. This paper may also contribute to data mining practice with our investigations on several real-life applications. By exploiting data dependency, clear improvements have been achieved in data quality and the usefulness of mining results.
 We would like to thank Zhenyu Liu and Ka Cheung Sia for preparation of the sensor data and helpful discussions about the probing problem.

