 H.1.0 [ Information Systems ]: Models and Principles Algorithms, Theory Topic Models, Textual Errors
Probabilistic topic models are stochastic models for text documents, which explicitly model topics in the document corpus. As generative models, they describe a procedure for generating documents using a series of probabilistic steps. Since it was introduced in 2003 [1], the latent Dirichlet allo-cation (LDA) model has quickly become a powerful tool for statistical analysis of text documents. LDA assumes that text documents are mixtures of hidden topics and applies Dirichlet prior distribution over the latent topic distribu-tion of a document having multiple topics. Also, it assumes that topics are probability distribution of words and words are sampled independently from a mixture of multinomials. Therefore, LDA is a widely used Bayesian topic model which
P artially supported by NSF DUE-0817376 and DUE-0937891 awards.
 Figure 1: Three examples of erroneous OCR output of a poor quality typewritten text (taken from [2]). Erroneous outputs are underlined. can model the semantic relations between topics and words for document corpora.

LDA requires accurate counts of the occurrences of words in order to estimate the parameters of the model, Therefore, it assumes that the entire document corpus is clean in order to ensure correct calculation of frequencies of words. How-ever, as text data become available in massive quantities, textual errors are appearing inevitable in large-scale docu-ment corpora. These textual errors include typos, spelling errors, transcription errors caused by text or speech recogni-tion tools, digitization errors of Google Books and Internet Archives, etc. For example, Walker et al. [2] point out that although researchers are having increasing levels of success in digitizing hand-written manuscripts, error rates remain significantly high. Figure 1 shows an example of typewritten documents and output by three Optical Character Recogni-tion (OCR) engines. Even on clean data, LDA will often do poorly if the very simple feature selection steps of removing stop-words is not performed first. It is shown that the per-formance in terms of accuracy declines significantly as word error rates increase [2], which highlights the importance of taking into account the noisy data issue in document mod-eling.

Motivated by the above observations, in this paper, we introduce our new model to tackle the issue of noisy data. In particular, we propose a new LDA model termed as TE-LDA to take into account textual errors in the document generation process. We compare the performance of our new model against the traditional LDA model and report promis-ing results of our proposal in terms of perplexity. Through extensive experiments, the efficacy of our proposed models is validated using both real and synthetic data sets.
Probabilistic document modeling has recently received tremen-dous attention in the data mining community. A series of probabilistic models including the Naive Bayesian model and the Probabilistic Latent Semantic Indexing (PLSI) model have been introduced to simulate the document generation process. The LDA model has become most popular in the data mining and information retrieval community due to its solid theoretical statistical foundation and promising per-formance. A wide variety of extensions of LDA model have been proposed for different modeling purposes in different contexts. For example, the correlated LDA model learns topics simultaneously from images and caption words [3].
However, topic modeling techniques require clean docu-ment corpus. This is to prevent the model from confusing patterns which emerge in the noisy data. Recent work by Walker [2] is the first comprehensive study of document clus-tering and LDA on synthetic and real-word Optical Char-acter Recognition data. The character-level textual errors introduced by OCR engines serve as baseline document cor-pora to understand the accuracy of document modeling in erroneous environment. The study shows that the perfor-mance of topic modeling algorithms degrades significantly as word error rates increase. In this section, we give a brief overview of the LDA model. Blei et al. [1] introduced it as a semantically consistent topic model, which attracted a considerable interest from both the statistical machine learning and natural language processing communities. LDA models documents by assuming that a document is composed by a mixture of hidden topics and that each topic is characterized by a probability distribu-tion over words. The model is shown as a graphical model in Figure 2(a). The notation is shown in Table 1.  X  d denotes a T -dimensional probability vector and represents the topic distribution of document d .  X  t denotes a W -dimensional probability vector where  X  t;w specifies the probability of gen-erating word w given topic t . M ulti ( . ) denotes multinomial distribution. Dir ( . ) denotes Dirichlet distribution.  X  is a T -dimensional parameter vector of the Dirichlet distribu-tion over  X  d , and  X  is a W -dimensional parameter vector of the Dirichlet distribtion over  X  t . The process of generating documents is shown in Algorithm 1.
 Al gorithm 1 : LDA Model .

F or each of the T topics t , sample a multinomial distribution  X  t from a Dirichlet distribution with prior  X  ;
For each of the D documents d , sample a multinomial distribution  X  d from a Dirichlet distribution with prior  X  ;
For each word w d;i in document d , sample a topic z d;i from the multinomial distribution  X  d ;
Sample word w d;i from the multinomial distribution
To overcome the constraints of the above traditional LDA topic model, in this section, we propose a new LDA model Figure 2: Comparison between our model vs. LDA model. termed as TE-LDA (LDA with T extual E rrors) to take into account noisy data in the document generation process. In this model, we distinguish the words in the documents and separate them as tokens and typos. Given a document, each word has a probability to be an error and we want to capture this probability structure in the term generation process. In order to reflect the nature of textual errors in the generative model, we adopt a switch variable to control the influence of errors on the term generation. The proposed model is illustrated in Figure 2(b). N d is the total number of words in document d (with N d = N term + N typo , the sum of all the true terms and typos).  X  ,  X  and  X   X  are Dirichlet priors,  X  d is the topic-document distribution,  X  t is the term-topic distribution.  X  typo is the term distribution specifically for typos. We include an additional binomial distribution  X  with a Beta prior of  X  which controls the fraction of errors.
For each word w in a document d , a topic z is sampled first and then the word w is drawn conditional on the topic. The document d is generated by repeating the process N d times. To decide if each word is an error or not, a switch variable X is introduced. The value of X (which is 0 or 1) is sampled based on a binomial distribution  X  with a Beta prior of  X  . When the sampled value of X equals 1, the word w is drawn from the topic z t which is sampled from the topics learned from the words in document d . When the value of X equals 0, the word w is drawn directly from the term distribution for errors. Overall, the generation process for TE-LDA can be described in Algorithm 2. We omit the derivation of parameter estimation due to space limit.
We trained our new model as well as the traditional LDA model on both synthetic and real text corpora to compare the generalization performance. Each model was trained on 90% of the documents in each data set and the trained model was used to calculate the estimates of the marginal log-likelihood of the remaining 10% of documents. Al gorithm 2 : TE-LDA Model .

F or each of the D documents d , sample  X  d Dir(  X  )and  X  d Beta(  X  );
For each of the T topics t , sample  X  t Dir(  X  );
Sample  X  typo Dir(  X   X  ); foreach N d words w d;i in document d do endfch Fi gure 3: Perplexity of different models in Unlv data set. From left to right, the subsets are Business , Magazine , Legal , Newspaper , Magazine2 . In our experiment, we used three benchmark data sets TREC AP , NIPS , and Reuters-21578 in the document mod-eling literature. The TREC Associate Press (AP) data set 1 contains 16333 newswire articles with 23075 unique terms. The NIPS data set 2 consists of the full text of the 13 years of proceedings from 1988 to 2000 Neural Information Process-ing Systems (NIPS) Conferences. The data set contains 1740 research papers with 13649 unique terms. The Reuters-21578 data set 3 consists of newswire articles classified by topic and ordered by their date of issue. The data set con-tains of 12902 documents and 12112 unique terms. For all the above data sets, we synthetically generated noisy text data to simulate different levels of Word Error Rates (WER). We also conducted experiments on one real OCR data set Unlv 4 with five subsets, namely Business , Magazine , Le-gal , Newspaper , Magazine2 . The average document WER generated by the OCR engine is around 30%.
We calculated the perplexity of a held-out test set to eval-uate the models. A lower perplexity score corresponds to better generalization performance of the document model. Formally, for a test data of D test documents the perplexity http ://www.daviddlewis.com/resources/testcollections/trecap/ http://www.cs.nyu.edu/ roweis/data.html http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html http://code.google.com/p/isri-ocr-evaluation-tools/updates/list score is calculated as follows [1]. Note that the probability p ( w d j z k ) is learned from the training process and p is estimated on the test data based on the parameters  X  ,  X  and  X  learned from training data.
We first compare the performance of our proposed model with the baseline LDA model on the real OCR dataset. Fig-ure 3 show the perplexity of TE-LDA as a function of the number of hidden topics in the five subsets of Unlv corpus. At different levels of WER for each subset, our TE-LDA model consistently outperforms the traditional LDA model.
We then systematically compare the performance of our model with LDA on the synthetically generated noisy cor-pora. In this experiment, we simulate different levels of word error rates (WER= 0.01, 0.05, 0.1). Figures 4(a)-(c) show the perplexity of TE-LDA model as a function of the num-ber of hidden topics in the document corpus on AP data set. As we can see from Figures 4(a)-(c), at different levels of WER, our TE-LDA model consistently outperforms the traditional LDA model. Furthermore, as WER increases, the margin of improvement increases. This is due to explicit modeling of textual errors during the generation of terms in the document modeling process. In Figures 4(d)-(f), we fix the number of topics T and demonstrate how the different models perform as the word error rates increase. An inter-esting finding here is that the perplexity of LDA increases as the word error rates increase while the perplexity of TE-LDA models decreases as the word error rates increase. This is because LDA does not consider the textual errors in the term generation where the accuracy of calculation of word frequencies is affected significantly in the noisy text envi-ronment. In summary, our TE-LDA outperforms LDA and the margin of improvement increases as the word error rates increase. Figures 4(g)-(l) and Figures 4(m)-(r) show similar patterns on NIPS data set and Reuters data set respectively.
In this paper, we extend the traditional LDA model to account for noisy text data in latent document modeling. Our TE-LDA adopts a switching mechanism to explicitly determine whether the word is generated from the topic-document distribution through the general topic generation route or from a special word distribution through the typo processing route. We show that our proposed model achieves better generalization performance than the LDA model. [1] D. M.Blei et al., Latent Dirichlet Allocation , In [2] D. D.Walker et al., Evaluating Models of Latent [3] X. Chen et al., Probabilistic Models for Topic Learning
