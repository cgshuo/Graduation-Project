
Dipartimento di Automatica e Informatica, Politecnico di Torino, Torino, Italy Institute for Cancer Research, Candiolo, Italy 1. Introduction
Social networks are becoming one of the most commonly used resources to communicate news or share documents, photos, and videos with a large community. Social networks sites, such as Facebook or Twitter, are accessed by m illions of people every da y. The huge amount of data generated by social network users represents a powerful source of knowledge that is worth considering in the analysis of online social communities and their u ser activities. Indeed, the appl ication of well-founded data min-ing techniques to the online community user-generated content (UGC) is definitely an appealing and challenging research topic. Recently, many research efforts have been devoted to defining user profiles and behaviors, mining opinions about products, and generating models to represent the shared knowl-edge [14]. For instance, knowledge provided by online communities has been profitably exploited in social behavior analysis [26,28], Web object categorization [39], and service recommendation [25,32, 38]. In the last years, Twitter has become one of th e most popular micr o-blogging and social network Web sites. Thus, the analysis of Twitter UGC has captured the interest of the research community. For instance, TwitterMonitor [28] extracted contextual knowledge from Twitter streams to detect most com-mon topic trends, while, in [15], topic trends are discovered, by using information retrieval techniques, to support analyst decision-making.

This paper presents the TweM (Tweet Miner) framework that addresses the extraction of hidden and high level recurrences, in the form of generalized association rules, from messages posted by Twitter users. The analysis of Twitter posts is focused on two different but related features: their textual content and their submission context (e.g., the place and the submission timestamp). TweM is based on three-step process: (i) taxonomy generation, (ii) generalized rule mining from tweet collections, and (iii) rule querying, based on the characteristics of the discovered rules. A taxonomy, i.e., a is-a hierarchy gener-ated over keywords and contextual data, is used to drive the extraction of generalized association rules. A novel generalized association rule miner, namely the EGP M INER (Evolving Generalized Pattern M I ner), exploits the historical evolution of patterns in the sequence of tweet collections.
The taxonomy generation task is accomplished by discovering and selecting strong associations over the analyzed data items (e.g., keywords, times, places) suitable for driving the generalization process. Relationships holding among contextual tweet features (e.g., the time and the geographical coordinates) are derived by means of Extraction, Transformation and Load (ETL) procedures, while an association-based approach is proposed to infer reliable implications among tweet content keywords. In particular, high-quality (i.e., frequent and high-confidence) associations involving couples of tweet keywords are selected and organized in a hierarchical fashion. The proposed selection strategy prefers high-quality aggregations well spread across different abstraction levels and, thus, combines knowledge aggregations so that they provide meaningful and not too general concepts. For instance, consider the following high-quality (i.e., frequent and high-confidence) rules Obama  X  President, President  X  Person ,and Obama  X  Person stating that the corresponding relationships among keywords Obama, President , and Person hold. By generalizing keywords based on the discovered associations the most specialized keywords (e.g., Obama ) are generalized into concepts at a higher abstraction level (e.g., President ). To aggregate the low level concept Obama at a higher level of abstraction, the two-step rule chain Obama  X  President  X  Person is preferred to the single aggregation rule Obama  X  Person .

Taxonomies are exploited to discover correlations among tweets in the form of generalized association rules [33]. The mining process is performed on a sequence of tweet collections. Differently from tradi-tional approaches, to guarantee the selection of the most relevant and persistent knowledge, the history of the already extracted patterns is exploited to drive the generalization process. More specifically, patterns that are frequent in a number of previously analyzed tweet sets (e.g., in a number of previous days) are expected to be of interest even in the current one (e.g., in the current day). Thus, as soon as they become infrequent, the discarding of their covered knowledge is, possibly, prevented by triggering their corre-sponding generalization. The TweM framework allows the investigation of recurrent trends and spatial correlations in the evolution of most relevant tweet topics. For instance, a correlation between a news-worthy topic and a geographical location, e.g., ( Keyword, Obama )  X  ( Location, New Y ork City ) , may be pointed out as frequent in a couple of days, but infrequent in the next day. The EGP M INER algorithm triggers its generalization when it performs generalized rule mining from tweets posted in the last day, e.g., by aggregating the term Obama in President and the city in the corresponding state. Thus, the generalized rule ( Keyword, President )  X  ( Location, New Y ork State ) is extracted.
This paper is organized as follows. Section 2 overviews most relevant related works concerning data mining from user-generated content. Section 3 presents the architecture of the proposed framework and describes its main blocks. Section 4 assesses the effectiveness of TweM in extracting hidden information from tweets as well as describes examples of real-life use-cases. Finally, Section 5 draws conclusions and presents future developments.
 2. Related work
Since the birth of social network sites on, many research efforts have been devoted to investigating the structure of online communities and identifying patterns relevant for characterizing the dynamics behind community user/group behavior. For instance, authors in [27] investigated the evolution of online communities by means of the maximum a posteriori (MAP) estimation, while, in [8] click-stream data is analyzed to identify most common Web user activities, such as universal searches, message sending, and community creation. Differently, in [19] the characteristics of the lifetime of the user-generated content (UGC) are investigated.

The application of data mining techniques to discover relevant social knowledge from the UGC has shown a steady growth in recent years [14]. Several data mining approaches, based on UGC analysis, are focused on (i) developing new recommendation systems to enhance the quality of product pro-motions [38], (ii) improving the understanding of online resources [6,26,39], and (iii) building query engines that take advantage of the emerging semantics in social networks [7,22]. The UGC analysis can be also useful for identifying most notable topic trends. For instance, in [15] trend patterns ex-tracted from Twitter are exploited to support analyst decision-making. More specifically, they focused on discovering Twitter users who contribute towards the discussions on specific trends. A number of pre-vious approaches addressed the discovery of association rules from user-generated content. For example, in [10] the authors compare several data mining techniques, among which association rule mining, to discover user patterns from Facebook data. Differently, the classification and link prediction method presented in [13] exploits association rules to discover correlations among data features related to the major user interests. However, to the best of our knowledge, the discovery of generalized rules in the presence of taxonomies constructed over both the tweet content and context of publication has never been investigated so far.

This paper address the usage of a well-founded data mining technique, i.e., generalized association rule mining, to perform knowledge discovery from Twitter posts. The problem of generalized association rule mining has been first introduced in [33] in the context of market basket analysis as an extension of the traditional association rule mining task [1]. Several data mining approaches are focused on proposing more efficient generalized itemset mining algorithms (e.g., [3,20,29,40]). They all try to avoid exhaustive taxonomy evaluation by preliminary pruning uninteresting patterns. Among them, authors in [3] exploit a support-driven approach to itemset generalization, i.e., they generalize an itemset only if has at least an infrequent descendant according to the given taxonomy. A similar approach has been also exploited to support context-aware user and service profiling [4]. However, all the aforementioned approaches do not consider the evolution of patterns extracted in different mining sessions to drive the generalization process. Active data mining [2] was the first attempt to represent and query the history of the discovered association rule quality i ndexes by incrementally updating a co mmon rule base. More recently, other approaches focused on detecting changes in itemset and rule quality indexes (e.g., support and confi-dence), based on either objective and subjective measures [5,11,12,36]. However, these methods either do not address rule generalization or drive the generalization process based on the characteristics of the current time period solely. Differently, the approach proposed in this paper discriminates and generalizes patterns based on their frequency of occurrence in both the current and the previous time periods. More specifically, patterns that are frequent in a number of past mining sessions are expected to be of interest even in the current one. Thus, as soon as they become infrequent, rare knowledge discarding is possibly prevented by triggering their corresponding generalizations.
 The generalized association rule mining process is typically driven by analyst-provided taxonomies. Differently, TweM integrates a taxonomy generation step to ease the domain expert X  X  task. A number of approaches have been devoted to building or automatically inferring taxonomies from data by exploiting well-known data mining techniques. Most of them propose to exploit hierarchical clustering algorithms to organize concepts [16,18,23]. However, taxonomies extracted by means of clustering approaches may provide weakly informative results [21]. In the last years, a few attempts to support taxonomy inference from the user-generated content by means of association discovery has been done. For instance, the analysis of folksonomies [30] and Web resource tags [17,31] has been recently conducted by using asso-ciation discovery techniques. In [17], the authors analyzed the correlations among context-aware tags to perform taxonomy generation. Association rules, extracted by means of Apriori algorithm [1], are mod-eled in a graph-based representation and exploited to represent semantic relationships among concepts. TweM adopts, similarly to [17], an association-based approach to discover meaningful aggregations over the tweet keywords. Unlike previous approaches, it entails (ii) discriminating among potentially relevant aggregations based on their suitability to drive the generalized rule mining process, and (ii) coping with heterogeneous data features (i.e., contextual data and content keywords). 3. The T we M framework
The TweM (Tweet Miner) framework is a data mining environment that focuses on supporting domain experts in the discovery of relevant recurrences from the user-generated Twitter posts. To address this issue, it analyzes the evolution of the most significant patterns hidden in a sequence of tweet collections. Figure 1(a) reports the TweM framework architecture. It is composed of the following main blocks:  X  User-generated content representation. Tweets are modeled as records (i.e., set of items) that de- X  Taxonomy generation. This block addresses the generation of taxonomies built over the tweet con- X  Evolving generalized pattern miner. A novel generalized association rule mining algorithm is ex-
A more detailed description of the main TweM blocks is presented in the following sections. 3.1. User-generated content representation A suitable user-generated content representation is needed to successfully accomplish the mining task. Twitter (http://twitter .com) posts can be accessed by means of the Search Appl ication Programming Interfaces (APIs). Data returned by the Twitter APIs is stored in the JSON (Java Script Object Notation) format, which is an XML-based standard for client-server data exchange. A simplified example of two tweet messages in the JSON format is reported in Fig. 2. Tweets are characterized by short textual messages enriched by several contextual information (e.g., publication place, city, date, hour). Some of the available contextual features are peculiar characteristics of the context in which tweets are posted (e.g., the GPS coordinates), while others are just high level aggregations of the previous ones (e.g., the city).

Consider the textual message and the low level contextual features first. Couples ( attribute, value ) , where attribute is the textual message or the description of the contextual feature (e.g., the date) and value is the collected information (e.g.,  X  X his is a message by Obama X , 2010-10-10), are denoted as items in the following. A tweet could be represented as a set of items, called record, in which each attribute occurs at most once. Each record is characterized by a level l that identifies the collection, in the sequence of tweet sets, to which the record (tweet) belongs to. A set of records (tweets), all characterized by a common level l , is denoted as relational tweet set of level l .
 Definition 1. Relational tweet set of level l . Let T = { t 1 ,t 2 ,...,t n } be a set of attributes, which describes the main data features and  X = {  X  1 ,  X  2 ,...,  X  n } the corresponding domains. Let r be a set of pairs ( t i , value i ) , called record, where value i  X   X  i and each t i appears at most once in r .A relational tweet set D of level l is a collection of records, where each record r is characterized by level l . Since Twitter posts do not comply with the relational tweet set format, a preprocessing phase is needed. A data cleaning procedure is exploited to discard useless or redundant information and correctly man-age missing values. Furthermore, the cleaned data is modeled as a relational tweet set (Cf. Definition 1), in which the records represent (i) the most representative keywords belonging to the tweet (see Sec-tion 3.2.2), and (ii) the tweet contextual features. For each tweet, the following pieces of information are considered:  X  Location : GPS coordinates  X  Time : publication date and time stamp  X  Content :keywords
For instance, suppose to partition the retrieved tweets in 2-hour time intervals and sort them in order of increasing time interval. Tweets reported in Fig. 2 belong to level 7 as their time stamps are in the range [12 p.m., 14 p.m. ) . Their relational tweet schema is reported in Fig. 3, where the relation primary key (i.e., the Tweet ID attribute) is printed in bold. 3.2. Taxonomy generation
This block aims at generating taxonomies, tailored to the analyzed data, that are suitable for effectively driving the generalized association rule mining process. A taxonomy is a hierarchical representation of the main concepts within a domain and the is-a relat ionships holding among them. It is composed of ag-gregation hierarchies, namely the aggregation trees, built over the domains of the source data attributes. Definition 2. Aggregation tree. Let t i be an attribute and  X  i its domain. An aggregation tree AT i is a tree representing a predefined set of aggregations over values in  X  i . AT i leaves are all the values in  X  . Each non-leaf node in AT i is an aggregation of all its children. Node  X  aggregates all values for attribute t i .

Let T = { t 1 ,t 2 ,...,t n } be a set of attributes and  X  = { AT 1 ,...,AT m } a set of aggregation trees defined on T . We define a taxonomy  X   X   X  as a set of aggregation trees. For the sake of simplicity, in the following we will consider taxonomies that contain at most one aggregation tree AT i  X   X  for each attribute t i  X  X  . A portion of an example aggregation tree built over the date attribute items is reported in Fig. 4.

TweM adopts different taxonomy generation strategies to construct aggregation trees over the tweet content and its contextual features. In the following sections, they will be discussed separately. 3.2.1. Taxonomy generation over context data
Taxonomies over contextual data features (e.g, the spatial and the temporal information) can be derived by means of aggregation functions based on a hierarchical model. The hierarchical model represents the relationships between different levels of aggregation. Similarly to what usually done in data warehous-ing [24], this information is extracted by means of Extraction, Transformation and Load (ETL) pro-cesses, called here aggregation functions. For example, in the relational tweet representation, aggrega-tion functions may define either aggregations among different contextual attributes (e.g., City  X  State ) or aggregations over a singular contextual attributes (e.g., Date  X  Semester ) which could be derived by simply parsing the corresponding attribute domain values. By applying aggregation functions, the analyst may generate taxonomies over the contextual data without a prior knowledge about the analyzed data distribution.

Given a set of aggregation functions built over the tweet contextual features, we associate with each item the corresponding set of generalizations, organized in a hierarchical fashion. For instance, consider a temporal contextual feature (e.g., Month ) that represents a high level abstraction of another one (e.g., Date ). A conceptual hierarchy of aggregations may be devised by mapping the two attribute domains by means of the corresponding aggregation function (e.g., Date  X  Month ). Consider again the Date attribute and its high level aggregation Semester . Although the corresponding higher level attribute does not exist yet, the mapping may be simply derived by parsing the lower level Date domain values (e.g., 2010-10-10) and, thus, generating the upper level concepts (e.g., 2 nd Semester 2010 ) according to the corresponding aggregation function (i.e., Date  X  Semester ). In Table 1 the aggregation func-tions exploited in the experiments (see Section 4) for the generation of the taxonomies over temporal and spatial contextual data features are resumed. However, the TweM framework allows the usage of different and more complex aggregation functions as well. Algorithm 1 Keyword taxonomy generation 3.2.2. Taxonomy generation over tweet keywords
Taxonomies over the tweet keywords are generated by following a two-step procedure:  X  Association rule graph extraction. The bag-of-word (BOW) representation of the tweet content is  X  Graph partitioning and pruning. The association rule graph is visited and pruned to generate tax-
The pseudo-code of the keyword taxonomy generation procedure is reported in Algorithm 1. In the following, each algorithm step is described in detail.

Association rule graph extraction. This first step focuses on building an association rule graph [17] that captures and represents strong (i.e., frequent and high-confidence) correlations among couples tweet content keywords. Each tweet textual content is modeled by means the BOW representation [35]. A stemming algorithm is exploited to remove stop-words, numbers, and website URLs to avoid noisy information and retrieve the stems of the processed terms. Since the goal is to identify the stems that represent concepts rather than just term root forms, we selected, among the ones available in literature, a stemming algorithm based on WordNet [9].

The BOW vector associated with each tweet could be represented as a transaction (i.e., a set of items), in which each WordNet stem represents an item. The Apriori algorithm [1] is exploited to discover hidden correlations among the transactional tweet content representation (see line 3 in Algorithm 1). Association rule mining is constrained by both (i) minimum support and confidence thresholds to dis-cover strong rules (i.e., frequent association rules that hold in most cases), and (ii) a maximum rule length, which stops the iterative Apriori itemset mining loop when itemsets of length greater than two are considered. Extracted rules are then combined in a graph-based representation, namely the associ-ation rule graph (lines 6 X 11). Association rule graph vertexes are the tweet content stems, while edges reflect the implications stated by the extracted rules. A more formal definition of association rule graph follows.
 Definition 3. Association rule graph. Let T = { T 1 ,T 2 ,...,T n } be a tweet content collection in which and min _ conf be, respectively, the minimum support and confidence thresholds. Let R be the set of association rules t i  X  t j such that  X  r  X  X  : (i) sup( r ) min _ sup ,(ii) conf ( r ) min _ conf , and (iii) stems t i  X  T j  X  X  .Anorientededge e ij belongs to ARG and connects vertex t i to vertex t j if and only if exists r : t i  X  t j such that r  X  R .

Suppose that, for instance, the following six association rules, satisfying all mining constraints, have been extracted:  X  Obama  X  President  X  Clinton  X  President  X  Obama  X  Democratic  X  Clinton  X  Secretary  X  Democratic  X  President  X  Barack  X  Democratic According to Definition 3, the corresponding asso ciation rule graph is reported in Fig. 5(a).
Graph partitioning and pruning. This step aims at reducing the association rule graph to a tree-based hierarchy (i.e., a taxonomy) by pruning less relevant edges. The problem of selecting a subset of graph edges such that (i) reduces ARG to a taxonomy and (ii) maximizes the significance of the reduced graph according to a given cost function is known to be NP-hard. Indeed, we propose an heuristics that parti-tions vertexes in aggregation levels and keeps adjacent levels connected so that keywords are maximally spread across the aggregation levels. Notice that the quality of the selected relationships is preliminary guaranteed by the mining constraints enforced during the association rule mining step. By maximizing the spread of the selected edges among the identified aggregation levels more specialized aggregations are preferred and used to drive the generalization process to reduce bias at higher abstraction levels. Con-sider, for instance, the following concepts: Football , Sport ,and Activity . The generalization of keyword Football in either the higher level concept Sport or directly in Activity are both sound and, thus, ac-ceptable aggregations. However, knowledge discovery driven by the generalization process may become more effective when a chain of more specialized aggregations is provided, e.g., if Football is generalized performed: (i) vertex labeling and (ii) edge pruning. (i) Vertex labeling. To each vertex v belonging to ARG , an aggregation level al v , which represents the level of abstraction of the corresponding stem (keyword) in the taxonomy, is assigned (lines 13 X 16). Let L be the set of vertexes belonging to ARG characterized by no incoming edges (i.e., in-degree equal to 0 ). Let v and x be two arbitrary graph vertexes and let h ( x, v ) be the maximum number of hops on ARG between vertexes x and v . Without any loss of generality, we set h ( x, v )=0 if the two nodes are disconnected. The aggregation level al v of an arbitrary vertex v belonging to ARG is defined by:
Thus, all vertexes belonging to L have aggregation level equal to 0 and could be selected as leaf nodes of the resulting keyword taxonomy. In Fig. 5(a) the aggregation level associated with each vertex is put in brackets. Vertexes Obama, Clinton ,and Barack are taxonomy leaves (i.e., aggregation level 0 ), while Secretary and Democratic are aggregations of level 1 ,and President has aggregation level 2 . (ii) Edge pruning. This step prunes the set of available edges so that (i) all vertexes keep connected and (ii) all vertexes, except for the root nodes (i.e., the nodes having no outgoing edges), have out-degree equal to one. To this aim, for each not-leaf node a sub-graph including all its descendants is built. Each subgraph is characterized by the aggregation level of its root node. Starting from the subgraphs with lowest aggregation level, a top-down depth-first visit is performed. To avoid vertex isolation, the procedure visits subgraphs/nodes in order of ascending in-degree, and, on equal terms, in lexicographical order. For each subgraph, once a node is visited, all its outgoing edges belonging to ARG that are not connected to its ancestor in the corresponding subgraph are pruned.

Consider, for instance, the labeled graph reported in Fig. 5(a). We identify three different subgraphs whose root nodes are, respectively, Secretary, Democratic ,and President . Among the nodes with the same aggregation level, the one with minimum in-degree is considered first (e.g., Secretary at aggregation level 1 ). Once a descendant is visited (e.g., Clinton ), its outgoing edges connected with the ancestors not belonging to the subgraph (e.g, the edge from Clinton to President ) are pruned from ARG . The resulting keyword taxonomy is reported in F ig. 5(b). Note that Secretary becomes a root node and its aggregation level corresponds to the generated aggregation tree height.

Since the vertexes belonging to the keyword taxonomy represent the most frequent stems in the tweet collection, keywords included in each tweet record are an ordered selection of the most representative content stems. For instance, consider the tweet set reported in Fig. 2. According to the keyword tax-onomy reported in Fig. 5(b), records belonging to the corresponding relational tweet set (see Fig. 3) include, respectively, the items ( Keyword, Obama ) and ( Keyword, Clinton ) , whose item values are nodes of the corresponding keyword taxonomy. Finally, the generated taxonomy is validated by the domain expert, who is in charge of assessing its semantic soundness. 3.3. Evolving Generalized Pattern Miner
This block focuses on discovering strong correlations, in the form of generalized association rules, from a sequence of tweet collections. The generalization process exploits the previously inferred tax-onomies to discover correlations at higher abstraction levels. The rule mining process is typically ad-dressed by means a two-step process [33]: (i) generalized itemset mining and (ii) generalized association rule generation. This section is organized as follows. Section 3.3.1 provides preliminary definitions and the related notation. Section 3.3.2 thoroughly describes the EGP M INER algorithm itemset mining step. Finally, Section 3.3.3 describes the generalized rule generation step.
 3.3.1. Preliminary definitions
The first step of the generalized association rule mining process entails the discovery of generalized and not generalized itemsets from the analyzed data.
 Definition 4. (Generalized) itemset. Let T be a set of attributes,  X  the corresponding domain, and  X  a taxonomy defined on values value i  X   X  i . A not generalized itemset is a set of items ( t i , value i ) in which each attribute t i may occur at most once. A generalized itemset is an itemset that includes at least a generalized item ( t i , value i ) such that value i  X   X  .

For instance, according to the aggregation functions reported in Table 1, { ( Place, New Y ork ) , ( date, October 2010) } is a generalized itemset of length 2 (i.e., a generalized 2 -itemset). A (general-x  X  X are either included in r , or ancestors of items i  X  r (i.e.,  X  i  X  leaves ( x ) | i  X  r ). The support of a (generalized) itemset X in a relational tweet set D l of level l is given by the number of tweets r  X  D l covering X divided by the cardinality of D l . Consider the example relational tweet set D 7 of level 7 reported in Fig. 3. The generalized itemset { ( Place, New Y ork ) , ( date, October 2010) } has support 50% in D 7 as it covers half of the records belonging to the tweet set. A descendant is associated with similar itemsets at different aggregation levels. We denote a (generalized) itemset X as a descendant of a generalized itemset Y if (i) X and Y have the same length and (ii) for each item y  X  Y there exists at least an item x  X  X that is a descendant of y . Consider the itemset { ( Place, New Y ork ) , ( date, 2010-10-10 ) } . According to the aggregation tree generated from the aggregation functions reported in Fig. 4, it is an example of descendant of the generalized itemset { ( Place, New Y ork ) , ( date, October 2010) } .
The second mining step focuses on generating generalized association rules from the set of extracted (generalized) itemsets. A generalized association rule is an implication X  X  Y ,where X and Y are disjoint generalized or not generalized itemsets, as stated by the following definition.
 Definition 5. Generalized association rule. Let A and B be two (generalized) itemsets such that attr(A)  X  attr(B) =  X  ,where attr ( X ) is the set of attributes belonging to itemset X . A generalized association rule is represented in the form A  X  B ,where A and B are, respectively, the body and the head of the rule.
 Generalized association rules are usually characterized by support and confidence quality indexes. The rule support sup is the support of the (generalized) itemset A  X  B , while the rule confidence conf 3.3.2. The EGP M INER itemset mining
The EGP M INER (Evolving Generalized Pattern M I ner) itemset mining step extracts, from each relational tweet sets (Cf. Definition 1) of level l , all frequent not generalized itemsets and the set of frequent generalized itemsets having at least an infrequent descendant at level l that is frequent in the previous history _ size levels. We formalize the problem addressed by EGP M INER as follows. Definition 6. EGP M INER itemset mining problem statement. Given a set of relational tweet sets D = { D 1 ,D 2 ,...,D n } of increasing levels 1 , 2 ,...,n , a taxonomy  X  built over D , a minimum support and confidence thresholds min _ sup and min _ conf , and a maximum history size history _ size ,the EGP M INER itemset mining step extracts from each relational tweet set D l , 1 l n .

A) The set of all not generalized itemsets satisfying the minimum support threshold min _ sup in D l , Algorithm 2 Evolving Generalized Pattern M I ner itemset mining step
B) The set of all generalized itemsets having at least a descendant (with respect to  X  ) such that If history _ size =0 then condition (B) could be ignored.

Algorithm 2 reports the pseudo-code of the itemset mining step of the EGP M INER algorithm. It it-eratively performs generalized itemset mining sessions from tweet collections D i of increasing level i by adopting an Apriori-like level-wise approach [1]. At an arbitrary step k ,EGPM INER accomplishes the following tasks: (i) k -itemset generation from dataset D i (line 20), (ii) support counting and gener-alization of (generalized) k -itemsets that are infrequent in D i but frequent in all D j such that j 1 , i  X  history _ size j i (lines 6 X 17), (iii) infrequent candidate pruning, and (iv) generation of k -candidate (generalized) itemsets of length k +1 by joining k -itemsets (line 20). The relational data format (Cf. Definition 1) allows preventing the generation of candidates including two items belonging to the same attribute. The generalized itemset generation procedure is lazily invoked only when itemsets infrequent in D i but frequent in all previous D j are considered. Given a (generalized) itemset c and a taxonomy  X  built over D , the taxonomy evaluation procedure generates a set of generalized itemsets by applying on each item ( t j , value j ) of c the corresponding aggregation tree AT j  X   X  (see Defini-tion 2). All the itemsets obtained by replacing one or more items in c with their generalized versions are generated and included into the Gen set (line 12). Finally, their support is computed by performing a dataset scan (line 16). The EGP M INER algorithm ends the mining loop on each D i  X  D when the set of candidate itemsets is empty (line 21). 3.3.3. The EGP M INER rule generation
This step generates the generalized association rules satisfying both the minimum support threshold min _ sup and the minimum confidence threshold min _ conf . Since the confidence of rules generated from the same itemset has the anti-monotone property, candidate rules of length k are generated by merging two ( k  X  1) -length rules that share the same prefix in the rule consequent [1]. Discovered rules are sorted based on confidence and support quality indexes to better support in-depth analysis. However, the TweM framework allows easily integrating other quality indexes as well (e.g., lift [34]).
The domain expert may query the ordered rule set based on either their schema or content, i.e., the attributes and/or the items to appear in the rule body or head. An example of query based on the rule schema is: { ( Keyword,  X  ) } X  X  ( Place,  X  ) } . It selects all 2-length rules that include, respectively, an item characterized by attribute Keyword in the rule body and attribute Place in the rule head. For instance, the generalized rule { ( Keyword, Sport ) } X  X  ( Place, U.S. ) } ) satisfies the requested schema. Differently, an example of query on the rule content is: { X  X   X  { ( Place, U.S. ) } . It selects all rules that contain the item ( Place, U.S ) as rule consequent. Rule { ( Keyword, Sport ) } X  X  ( Place, U.S. ) } ) satisfies also the item constraint. 4. Experimental results
We evaluated the TweM framework by means of a large set of experiments by addressing the fol-lowing issues: (i) the usefulness of the mined generalized rules in different examples of use cases (see Section 4.1), (ii) the performance of the TweM framework (see Section 4.2), and (iii) the characteristics of the aggregation rules (see Section 4.3). The experiments have been performed on 3.0 GHz Pentium IV system with 4 GB RAM, running Ubuntu kernel 2.6. 4.1. Examples of T we M use-cases
In this section, we evaluate the effectiveness of the TweM framework in discovering valuable cor-relations among tweets in different real-life use-cases. In each scenario the analyst may perform only the selection of an initial set of tweets of major interest (e.g., the top-tweets) and the definition of the constraints to categorize the tweets in different collections. The TweM framework fully supports the fol-lowing steps: (i) generation of taxonomies over textual and context data, and (ii) generalized association rule mining from an analyst-provided sequence of tweet sets. To test our framework in real application scenarios we exploited a tweet crawler to retrieve and categorize the tweets based on the constraints defined by the analyst. Some examples of real-life use-cases follows.
 Use-case 1: Content propagation analysis. This application scenario allows analysts to discover most significant spatial and temporal correlations about knowledge propagation in Twitter. For instance, start-ing from the top-tweets, which are subsets of tweets of major interest, the analyst may categorize the tweets retrieved by the crawler based on their propagation level in the chain of answers/citations to the initial set. Then, the EGP M INER algorithm is exploited to provide to the analysts a set of potentially meaningful implications, in the form of generalized association rules. Finally, the analyst can query the rules based on either their schema or content (e.g., ( Keyword,  X  )  X  ( Place,  X  ) ).
 Use-case 2: Spatial correlation analysis in message posting. This application scenario allows analysts to discover most relevant recurrences hidden in tweets posted from a delimited geographical area. For instance, correlations among tweets collected in faraway places may highlight social, political, or eco-nomical linkages. The order in which tweet collections are analyzed by EGP M INER should reflect the expected way of knowledge propagation. For instance, if some topical news are matter of contention in the U.S.A., it may be worth investigating their spatial propagation from the U.S.A. to the other countries. Use-case 3: Temporal evolution analysis. The third application scenario analyzes the temporal tweet propagation by considering the time stamp at which messages are posted. The analyst may partition tweets in distinct collections using the time stamp information (e.g., 1-day time period). The discovered rule may represent unexpected trends in the evolution of relevant tweet topics. For instance, analysts may wonder how breaking news are matter of contention on Twitter in consecutive days. The achieved results strictly depend on the granularity of the selected time periods. 4.1.1. Examples of extracted rules
In this section, we report some examples of mined generalized association rules relative to the sec-ond and the third TweM use-cases. To investigate both spatial and temporal knowledge propagation in Twitter posting, we crawled, by means of Twitter APIs, two relational tweet sets (Cf. Definition 1). The first dataset collects tweets posted within a 2,500 km radius far from New York (i.e., the lands along the Eastern American coastline), while the second one includes messages posted within a 2,500 km radius far from London (i.e., the North-West of Europe). The tweet submission dates are uniformly distributed in the time period [2011/03/20 X 2011/03/24]. Consider, for instance, the spatial evolution of the con-tent published in tweets posted in the U.S.A. Extracted rules, among which the ones below have been selected, highlight that both American and English users are interested in the recent conflict in Libya. Relational tweet set New York (ii) ( Keyword 1 , Congress ) , ( Place, Washington, D.C. )  X  X  ( Date, 2011 / 03 / 22) } ( sup = Relational tweet set London (iii) ( Keyword 1 , Obama )  X  X  ( Keyword 2 ,Libya ) } ( sup =3 . 6% , conf = 94% ) (iv) ( Keyword 1 , Obama )  X  X  ( Place, United Kingdom ) } ( sup =3 . 5% , conf = 82% )
Rules have been extracted by enforcing a minimum support threshold equal to 1% and a minimum confidence threshold equal to 80%. A particular attention is paid to the foreign policy undertaken by president Obama and the American Congress, which has been under discussion in the past meeting held in the United States Capitol Washington, D.C. (USA) on March, 22 nd 2011. To delve into the impact of breaking news coming from the United States Capitol in the very next days, we reorganized and sorted crawled tweets in order of submission date. By setting history _ size =1 ,theEGPM INER algorithm is exploited to perform generalized rule mining from tweet sets posted in consecutive days. Pattern generalization prevents the discarding of knowledge that is expected to be of analyst X  X  interest, like the one reported the following example.
 Relational tweet set March, 22 nd (v) { ( Keyword 1 , Obama ) , ( Keyword 2 ,Libya ) } X  X  ( Place, Washington, D.C. ) } ( sup = Relational tweet set March, 23 rd (vi) { ( Keyword 1 , Obama ) , ( Keyword 2 ,Libya ) } X  X  ( Place, U.S.A. ) } ( sup =2 . 5% , conf = Keywords Obama and Libya , which have been frequently posted on March, 22 nd in Washington, D.C. due to the Congress meeting, become infrequent, in the same place, the day after. However, the lazy generalization adopted by EGP M INER allows figuring out that the same topic is still of interest in the U.S. country.
 4.2. T we M performance evaluation
We evaluated the performance of the TweM framework by addressing the following issues: (i) the number of generalized and not generalized itemsets extracted by the EGP M INER algorithm (Sec-tion 4.2.1), and (ii) the scalability, in terms of the extraction time, of both the taxonomy generation procedure and the generalized association rule mining steps (Section 4.2.2). 4.2.1. EGP M INER algorithm performance
We analyzed the performance of the EGP M INER algorithm, in terms of the number of extracted item-sets, by comparing it with our implementation of the two following generalized frequent itemset miners: (i) Cumulate [33] and (ii) G EN IO [3]. Unlike EGP M INER , they do not consider the history of the pre-viously extracted patterns to drive the generalization process. While Cumulate performs an exhaustive taxonomy evaluation by generating all the possible frequent combinations of generalized and not gener-alized itemsets, G EN IO generates a higher level itemset only if it has at least an infrequent descendant in the current mining session. The proposed EGP M INER algorithm generalizes the G EN IO algorithm in a dynamic context by considering eligible for generalization exclusively the itemsets that are infrequent in a given time period k but frequent all the previous history _ size ones [ k  X  history _ size,...,k  X  1 ].
We evaluated the pruning selectivity of the EGP M INER algorithm in terms of the number of gener-ated itemsets on synthetic datasets generated by means of the TPC-H generator [37]. The TPC-H data generator consists of a suite of business-oriented ad-hoc queries. The queries and the data populating the database have been chosen to have broad industry-wide relevance. By varying the scale factor parame-ter, files with different sizes could be generated. We generated a dataset starting from the line item table by setting a scale factor e qual to 0.075 (i.e., around 450,000 record s). To partition the whole dataset in three distinct time-related data collections, we queried the source data by enforcing different constraints on the shipping date value (attribute ShipDate ). More specifically, we partitioned line items shipped in the three following time periods: [1992-01-01, 1994-02-31], [1994-03-01, 1996-05-31] [1996-06-01, 1998-12-01]. For the sake of brevity, we will denote the corresponding datasets as data-1, data-2, and data-3 in the rest of this section.

Since the minimum support threshold enforced during the itemset mining step significantly affects the number of extracted itemsets and rules, we performed different mining sessions, by varying the minimum support threshold, for all combinations of algorithms and datasets. In Figs 6 X 8 we plotted the number itemsets mined, respectively, from data-1, data-2, and data-3. To better highlight the pruning selectivity on generalized itemsets, we differentiated generalized from not generalized itemsets.
To test the EGP M INER algorithm, we considered the generated datasets in increasing order of ship-ment date. Since data-1 represents the first collection of the sequence, results obtained by the EGP M
INER algorithm and G EN IO are the same (see Fig. 6(b)). For all the tested algorithms and datasets and for most of the minimum support values, the percentage of extracted frequent itemsets including at least one generalized item is significant (i.e., at least 65%). For both Cumulate and G EN IO, the number of mined (generalized) itemsets significantly increases for lower minimum support values (e.g., 1%). Hence, it may become difficult to look into the extracted patterns. Moreover, most of the discovered patterns neither represent relevant knowledge nor highlight a significant trend in the sequence of tweet collections. In G EN IO, infrequent items are aggregated during the extraction process regardless of the past mining results thus the percentage of generalized itemsets increases when higher support threshold are enforced. When high support thresholds (e.g., 7%) are enforced, most of the extracted itemsets are generalized itemsets whose dataset coverage is too wide to provide interesting knowledge. Oppositely, when lower support thresholds are enforced (e.g., 2%), the mining algorithm generates a larger amount of patterns, possibly including a subset of itemsets of interest.

The EGP M INER algorithm generalizes the infrequent itemsets that have been frequent in the previous history _ size mining steps. Figures 7(c) and 8(c) show that the proposed approach to itemset generaliza-tion significantly reduces the number of generated generalized itemsets (e.g., 6% reduction with respect to G EN IO at minsup = 1% on data-3). Figures 9(a) and (b) show the selectivity of the minimum support threshold on the set of not generalized itemsets extracted by EGP M INER , respectively, from data-1 and data-2. In particular, Figs 9(a) and (b) report the number of not generalized itemsets mined, respectively, from data-0 and data-1 that became infrequent in the next mining steps at different minimum support values. The above itemsets are the ones over which the EGP M INER itemset lazily triggers the general-ization process. By setting history _ size =2 , only those infrequent itemsets in data-3 that are frequent in both data-1 and data-2 are generalized. Thus, the pruning effectiveness is maximal and the amount of generated generalization becomes significant only when lower support thresholds are enforced (e.g., 1%). Nevertheless, by setting history _ size =1 a significant amount of generalized itemsets have been extracted even at medium support thresholds. In summary, the lower are the values of the history set size history _ size and the minimum support threshold min _ sup the more significant is the impact of the generalization process. 4.2.2. T we M extraction time
We also analyzed, on synthetic datasets, the time spent by TweM in each mining phase, i.e., taxonomy generation, frequent generalized itemset mining, and generalized association rule generation. To perform the analysis, we exploited the same TPC-H lineitem tables data-1, data-2, and data-3 presented in the previous section.

In Fig. 10, we compared the time spent by the EGP M INER algorithm in generalized itemset mining with the one spent by Cumulate and G EN IO, by varying the minimum support threshold value. The extraction time is mainly affected by the generalization process in all the three algorithms. The impact of generalization in EGP M INER significantly decreases with respect to Cumulate and G EN IO when lower minimum support values are enforced (e.g., 1%). This effect is partially counteracted by the higher time spent in checking whether itemsets are eligible or not for generalization (i.e., the history set look back). This makes the extraction time comparable to G EN IO when higher support threshold values are enforced.

We also analyzed the impact of the TweM taxonomy and rule generation procedures separately. As expected, most of of the TweM extraction time (from 70% to 90%) is spent in the generalized frequent itemset mining phase. For all datasets and for any combination of support and confidence thresholds, the rule generation step never takes more than 7% of the whole extraction time. Time spent in keyword taxonomy generation ranges from 5% to 20% of the whole extraction time and its impact is mostly due to the stemming algorithm processing time. For instance, by considering the TPC-H dataset composed of around 450,000 records, the stemming algorithm takes around 267 seconds, the association rule graph creation (by setting minimum support threshold 1% a nd minimum confidence 50%) takes 1.5 seconds, while the graph pruning phase takes around 0.038 seconds. 4.3. Characteristics of the aggregation rules
In this section we analyzed the characteristics of the rules used to aggregate tweet keywords into higher level ones and their suitability to drive the generalization process. To evaluate the quality of the discovered aggregations, we compared the aggregation rules selected by our approach with the ones se-lected by a recently proposed approach [17] that also adopts association discovery to perform taxonomy generation in a different context (i.e., Web tag analysis).

We retrieved, by means of Twitter APIs, two examples of tweet collections concerning two very fa-mous persons, i.e., the president of the United States of America, Barack Obama, and the theoretical physicist, Albert Einstein. To perform a fair comparison, we exploited the same quality index used in [17]. In particular, we evaluated the local quality index, first defined in [17], that measures the average significance, in terms of the confidence quality index, of the selected rule set. Figures 11(a) and 12(a) plot the values of local quality for both datasets by varying the minimum confidence threshold. The quality of the rules selected by our approach is slightly worser than that achieved by the ones selected in [17] at lower confidence thresholds, while the quality gap disappears when higher confidence thresh-olds are enforced. Nevertheless, the selected aggregations are better spread across the taxonomy levels. We define the spread as the average number of hops across the taxonomy  X  to move from an arbitrary non-root node t i  X  V to its corresponding root t j  X  R :
In Figs 11(b) and 12(b) we plot the local quality in terms of its corresponding spread value by setting any confidence threshold. Our approach yields a significant improvement, in terms of local quality, at higher spread values. Thus, aggregation rules discovered by TweM are almost as accurate as the ones selected in [17] and better spread across the different abstraction levels. Indeed, they may deemed more suitable for being exploited to drive the rule generalization process.
 5. Conclusions and future work
This paper presents the TweM framework that focuses on discovering hidden and high level correla-tions among Twitter user-generated content. It invest igates the use of taxonomies t o drive the association rule mining process from a sequence of tweet collections. Experimental results show both the effective-ness and the efficiency of the TweM framework in performing knowledge discovery from Twitter user-generated content. The system is proved to be very effective in supporting the analysts in the discovery of the most notable temporal and spatial topic trends. Monitoring the evolution of the user-generated content and its related context is crucial for enhancing advanced expert analysis and reactively suiting the decision making process to the actual online community expectations.
 We focused our framework on the analysis of the information published by the Twitter community. However, our approach can be successfully applied to any application scenario and domain in which tex-tual and contextual pieces of information are available. As future work, we will scale up the system with the integration of different Web sources like news, blogs, and other social network posts. Furthermore, we will address the incremental updating of both the generated taxonomies and the extracted rules. Fi-nally, the study of novel metrics to evaluate the authority of a user based on both his posted messages and his relationships with the other users and groups is another interesting future research direction. References
