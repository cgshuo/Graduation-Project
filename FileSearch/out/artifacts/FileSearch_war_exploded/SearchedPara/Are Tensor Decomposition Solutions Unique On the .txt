 Tensor based dimension reduction has recen tly been extensively studied for data mining, pattern recognition, and machine learning applications. Typically, such approaches seek subspaces such that the information are retained while the discarded subspaces contains noises. Most tensor decomposition methods are unsupervised which enable researchers to apply them in any machine learning applications including unsupervised learning and semi-supervised learning. In such applications, one of the central focuses is the uniqueness of the solution. For example, in missing value completion problem, such as social recommenda-tion system [1], tensor decompositions are applied to obtain optimal low rank approximation [2]. Since the missing value problem requires iteratively low rank decomposition, the convergence of each iteration is crucial for the whole solution. Other real world applications also highly rely on the stability of the decomposi-tion approaches, such as bioinformatics[3], social network [4], and even marketing analysis [5].

Perhaps High Order Singular Value Decomposition (HOSVD) [6] [7] and Par-allel Factors (ParaFac) are some of the most widely used tensor decompositions. Both of them could be viewed as extensions of SVD of a 2D matrix. HOSVD is used in computer vision by Vasilescu and Terzopoulos [8] while ParaFac is used in computer vision by Shashua and Levine [9]. More recently, Yang et al. [10] pro-posed a two dimensional PCA (2DPCA) Ye et al. [11] proposed a method called Generalized Low Rank Approximation of Matrices (GLRAM). Both GLRAM and2DPCAcanbeviewedinthesameframeworkin2DSVD(two-dimensional singular value decomposition) [12], and solved by non-iterative algorithm [13]. Similar approaches are also applied as supervised feature extraction [14,15]. The error bounds of HOSVD have been derived [16] and the equivalence between tensor K -means clustering and HOSVD is also established [17].

Although tensor decompositions are now widely used, many of their properties so far have not been well characterized . For example, the tensor rank problem remains a research issue. Counter examples exist that argue against optimal low-dimension approximations of a tensor.

In this paper, we address the solution uniqueness issues 1 . More precisely, non-unique solutions due the existence of large number of local solutions. This problem arises because the tensor decomposition objective functions are non-convex with respect to all the variables and the constraints of the optimization are also non-convex. Standard algorithms to compute these decompositions are iterative improvement. The non-convexity of the optimization implies that the iterated solutions will converge to different solutions if they start from different initial points.

Note that this fundamental uniqueness issue differs from other representa-tion redundancy issues, such as equivalence transformations (i.e. rotational in-variance) that change individual factors ( U, V, W ) but leaves the reconstructed image untouched. These representation redundancy issues can be avoided if we compare different solutions at the level of reconstructed images, rather in the level of individual factors.
 The main findings of our investigation are both surprising and comforting. On all real life datasets we tested (we te sted 6 data sets and show results for 3 data set due to space limitation), the HOSVD solutions are unique (i.e., different initial starts always converge to an unique global solution); while the ParaFac solution are almost always not unique. Furthermore, even with substantial ran-domizations (block scramble, pixel scram ble, occlusion) of these real datasets, HOSVD converge to unique solution too.

These new findings assure us that in most applications using HOSVD, the solutions are unique  X  the results are repeatable and reliable.

We also found that whether a HOSVD solution is unique can be reasonably predicted by inspecting the eigenvalue distributions of the correlation matrices involved. Thus the eigenvalue distributions provide a clue about the solution uniqueness or global convergence. We are l ooking into a theoretical explanation of this rather robust uniqueness of HOSVD. 2.1 High Order SVD (HOSVD) subspace U, V, , W and core tensor S such that the L 2 reconstruction error is minimized, explicit index, In HOSVD, W, U, V are required to be orthogonal: With the orthonormality condition, setting  X  X  1 / X  X  =0,weobtain S = U T  X  1 V
T  X  2 W T  X  3 X, and J 1 = X 2  X  S 2 . Thus HOSVD is equivalent to maximize where Standard HOSVD algorithm starts with initial guess of of ( U, V, W )andsolve Eqs(3,4,5) alternatively using eigenvectors of the corresponding matrix. Since F, G, H are semi-positive definite, || S || 2 are monotonically increase (non-decrease). Thus the a lgorithm converges to a local optimal solution. HOSVD is a nonconvex optimization problem: The objective function of Eq.(2) w.r.t. ( U, V, W ) is nonconvex and the orthonormality constraints of Eq.(2) are nonconvex as well. It is well-known that for nonconvex optimization prob-lems, there are many local optimal solutions: starting from different initial guess of ( U, V, W ), the converged solutions are different. Therefore theoretically, solu-tions of HOSVD are not unique.
 2.2 ParaFac Decomposition ParaFac decomposition [18,19] is the simplest and also most widely used decom-position model. It approximates the tensor as W =( w (1) ,  X  X  X  , w ( R ) ). ParaFac minimizes the objective We enforce the implicit constraints that columns of U =( u (1) ,  X  X  X  ,u ( R ) )are linearly independent; columns of V =( v (1) ,  X  X  X  ,v ( R ) ) are linearly independent; and columns of W =( w (1) ,  X  X  X  ,w ( R ) ) are linearly independent.
Clearly the ParaFac objective function is nonconvex in ( U, V, W ). The linearly independent constraints are also nonconvex. Therefore, the ParaFac optimization is a nonconvex optimization.
 Many different computational algorithms were developed for computing ParaFac. One type of algorithm uses a sequence of rank-1 approximations [20,21,9]. However, the solution of this heuristic approach differ from (local) optimal solutions.

The standard algorithm is to compute one factor at a time in an alternat-ing fashion. The objective decrease mono tonically in each st ep, and the itera-tion converges to a (local) optimal solution. However, due to the nonconvexity of ParaFac optimization, the converged solution depends heavily on the initial starting point. For this reason, the ParaFac is often not unique. In this paper, we investigate the problem of whether the solution of a tensor decomposition is unique. This is an important problem, because if the solutions is not unique, then the results are not repeatable and the image retrieval is not reliable.

For a convex optimization problem, there is only one local optimal solution which is also the global optimal solution. For a non-convex optimization problem, there are many (often infinite) local optimal solutions: converged solutions of the HOSVD/ParaFac iterations depend on the initial starting point.

In this paper, we take the experimental approach. For a tensor decomposition we run many runs with dramatically different starting points. If the solutions of all these runs agree with each other (to computer machine precision), then we consider the decomposition has a unique solution.

In the following, we explain the (1) The dramatically different starting point for ( U, V, W ). (2) Experiments on three different r eal life data sets. (3) Eigenvalue distributions which can predict the uniques of the HOSVD. In this section, we describe a natural starting point for W . Consider the T1 decomposition [6] C, W are obtained as the results of the optimization This decomposition can be reformulated as the following: reason is the following. Let A =( a 1 ,  X  X  X  ,a n ) be a collection of 1D vectors. The corresponding covariance matrix is AA T and Gram matrix is A T A . Eigenvectors of A T A are the principal components. Coming back to the T1 decomposition,  X  H is the Gram matrix if we consider each image X ( k ) as a 1D vector. Solution for W are principal eigenvectors of  X  H , which are the principal components. For both HOSVD and ParaFac, we generate 7 different initializations: (R1) Use the PCA results W as explained in  X  4. Set V to identity matrix (fill zeros in the rest of the matrix to fit the size of n 2  X  m 2 ). This is our standard initialization. (R2) Generate 3 full-rank matrixes W and V with uniform random numbers of in (0 , 1). (R3) Randomly generate 3 rank deficient matrices W and V with proper size. For first initialization, we randomly pick a column of W and set the column to zero. The rest of columns are randomly generated as in (R2) and the same for V . For second and third initializations, we randomly pick two or three columns of W and set them to zero, and so on. Typically, we use m 1 = m 2 = m 3 =5 10. Thus the rank-deficiency at m 3 =5isstrong.

We use the tensor toolbox [22]. The order of update in the alternating updat-ing algorithm is the following: (1) Given ( V, W ), solve for U (to solve Problem 4); (2) Given ( U, W ), solve for V (Problem 5); (3) Given ( U, V ), solve for W (Problem 6); Go back to (1) and so on.
 For each dataset with each parameter setting, we run 10 indepedent tests. For each test, we run HOSVD iterations to convergence (because of the difficulty of estimating convergence criterion, we run total of T=100 iterations of alternating updating which is usually sufficient to converge).

For each independent test, we have 7 different solutions of ( U i ,V i ,W i )where i =1 , 2 ,  X  X  X  , 7 for the solution starting from the i -th initialization. We use the following difference to verify whether the solutions are unique: where we introduce the HOSVD iteration index t ,and U t i ,V t i ,W t i are the solution in t -th iteration.

If an optimization problem has a unique solution, d ( t ) typically starts with nonzero value and gradually decrease to zero. Indeed, t his occurs often in Figure 2Thesooner d ( t ) decreases to zero, the faster t he algorithm converges. For example, in the 7th row of Figure 2, the m 1 = m 2 = m 3 = 5 parameter setting, the algorithm converges faster than the m 1 = m 2 = m 3 = 10 setting.
In our experiments, we do 10 differen t tests (each with different random starts). If in all 10 tests d ( t ) decreases to zero, we say the optimization has a unique solution (we say they are globally convergent).

If an optimization has no unique solution (i.e., it has many local optima), d ( t ) typically remains nonzero at all times, we say the solution of HOSVD is not unique. In Figure 1, we show the results of HOSVD and ParaFac on a random tensor. One can see that in each of the 10 tests, shown as 10 lines in the figure, none of them ever decrease to zero.

For ParaFac we use the difference of reconstructed tensor to evaluate the struction tensor in the t -th iteration with the i -th starting point. ParaFac algo-rithm converge slower than HOSVD algorithm. Thus we run 2000 iterations for each test. In these figures, the eigenvalues of F , G ,and H are calculated using Eqs.(7a,7b,7c), but setting all UU T ,VV T ,WW T as identity matrix. The ma trices are centered in all indexes. The eigenvalues are sorted and normalized by the sum of the all the to the average along the corresponding index. The first image dataset is WANG [23] which contains 10 categories and 100 images for each category. The original size of the image is either 384  X  256 or 256  X  384. We select Buildings, Buses, and Food categories and resize the images into a 100  X  100 size. We also transform all images into 0-255 level gray images. The selected images form a 100  X  100  X  300 tensor. The second dataset is Caltech 101 [24] which contains 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Li, Andreetto, and Ranzato. The size of each image is roughly 300  X  200 pixels. We randomly pickup 200 images, resize and transform them into 100  X  100 0-255 level gray images to form a 100  X  100  X  200 tensor. Three types randomization are considered: block scramble, pixel scramble and occlusion. In block scramble, an image is divided into n = 2, 4, 8 blocks; blocks are scrambled to form new images (see Figure 2).

In pixel sample, we randomly pick up  X  = 40% , 60% , 80% of the pixels in the image, and randomly scramble them to form a new image (see Figure 2). We also experimented with occlusions with sizes up to half of the images. We found that occlusion consistently produce smaller randomization affects and HOSVD results converge to the unique solution. For this reason and the space limitation, we do not show the results here. From results shown in Figure 2, we observe the following: 1. For all tested real-life data, ParaFac solutions are not unique, i.e., the con-2. For all tested real-life data, HOSVD solutions are unique, although theoreti-3. For even heavily rescrambled (randomized) real-life data, HOSVD solutions 4. For very severely rescrambled real -life data and pure randomly generated 5. The HOSVD solution for a given dataset may be unique for some parameter 6. Whether the HOSVD solution for a given dataset will be unique can largely We found Empirically that the eigenvalue distribution help to predict whether the HOSVD solution on a dataset with a parameter setting is unique or not.
For example, in AT&amp;T dataset HOSVD converges in all parameter settings except in 8  X  8 block scramble with m 1 = m 2 = m 3 = 5. This is because the ignored 3 eigenmodes have very similar e igenvalues as the first five eigenvalues. It is ambiguous for HOSVD to select which of the 8 significant eigenmodes. Thus HOSVD fails to converge to a unique solution.

But when we increase m 1 ,m 2 ,m 3 to 10, all 8 significant eigenmodes can be selected and HOSVD converges to a unique solution. This also happens in the other two datasets (see the forth rows in top part of Figure 2.
 For 80% pixel scramble in dataset WANG, when m 1 = m 2 = m 3 =5 , 10, HOSVD is ambiguous as to select eigenmodes because there are a large number of them with nearly identical eigenvalues around the cutoff. However, if we reduce the dimensions to m 1 = m 2 =2 ,m 3 =4or m 1 = m 2 = m 3 = 3, this ambiguity is gone: HOSVD clearly selects the top 2 or 3 eigenmodes. converges (see the last row of the top panel in Figure 2). This same observation also applies to Caltech 101 dataset at 80% pixel scramble in 101 (see the last row of the top part of Figure 2).

For random tensor shown in Figure 1, the eigenvalues are nearly identical to each other. Thus for both parameter setting ( m 1 = m 2 = m 3 =5and m 1 = m 2 = m 3 = 10), HOSVD is ambiguous to selection eigenmodes and thus does not converge.

We have also investigated the solution uniqueness problem of the GLRAM tensor decomposition. The results are very close to HOSVD. We skip it due to space limitation. Theoretical analysis of the convergence of HOSVD is difficult due to the fact that U, V, W are orthonormal: they live in Stiefel manifold. Thus the domain of U, V, W are not convex which renders the standard convex analysis not applicable here.
In spite of the difficult, we present two analysis which shed some lights on this global convergence issue.
 We consider HOSVD with m 3 = n 3 which implies W = I .Furthermore,let X =( X 1  X  X  X  X n 3 ) and we restrict that X m  X  r  X  r is symmetric. In this case, V=U, and HOSVD is simplified to where U  X  r  X  k and S m  X  k  X  k .
At first glance, due to U T U = I . it is hard to prove the global convergence of this problem (convexity). However, we c an prove the global convergence using a slightly modified approach.

We can easily show that S m = U T X m U, and the optimization becomes Now let Z = UU T .Westudytheconvexityof We now prove Theorem 1. The optimization of Eq.(14) is convex when X m is semi-positive definite(S.P.D.).
 Proof. We have  X  X  2  X  X  is To see if H is s.p.d., we evaluate Now, every spd matrix X m can be decomposized into X m = B T m B m .Thuswe have Therefore, H is s.p.d. and the optimization of J 1 ( U ) is a convex problem. Indeed, even when X m are random s.d.p. matrices, the standard HOSVD algo-rithm convergence to a unique solution no matter what is the starting point. Next, we consider a nonsymmetric HOSVD problem of Eqs.(4,7) with F = XV V T X T , i.e., we solve We can similary prove Theorem 2. The optimization of Eq.(15) is convex. Indeed, even when X are random s.d.p. matrices, the standard HOSVD algo-rithm convergence to a unique solution no matter what is the starting point.
In the simplified HOSVD problems of Eqs.(14,15), we avoided the orthogonal-ity constraints, and thus can prove rigorously the convexity of the optimization problem. In generic HOSVD, the orthogonality constraints cannot be removed and thus the problem is much harder to deal with. We are currently looking into other techniques to analyze the global convergence of HOSVD. In summary, for all real life datasets we tested, the HOSVD solution are unique (i.e., different initial starts always converge to an unique global solution); while the ParaFac solution are almost always not unique. These finding are new (to the best of our knowledge). They also surprising and comforting. We can be assured that in most applications using HOSVD, the solutions are unique  X  the results are reliable and repeatable. In the rare cases where the data are highly irregular or severely distored/randomized, our results indicate that we can predict whether HOSVD solution is unique by inspecting the eigenvalue distributions. Dijun Luo was supported by NSF CNS-0923494, NSF IIS-1015219, UTA-REP. Chris Ding and Heng Huang were supported by NSF CCF-0830780, NSF CCF-0939187, NSF CCF-0917274, NSF DMS-0915228.

