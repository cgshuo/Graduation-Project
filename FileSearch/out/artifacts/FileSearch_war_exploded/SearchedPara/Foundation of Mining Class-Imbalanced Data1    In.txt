 In data mining, datasets are often imbalanced (or class imbalanced); that is, the number of examples of one class (the rare class) is much smaller than the number of the other class (the majority class). 1
This problem happens often in real-world applications of data mining. For example, in medical diagnosis of a certain type of cancer, usually only a small number of people being diagnosed actually have the cancer; the rest do not. If the cancer is regarded as the positive class, and non-cancer (healthy) as negative, then the positive examples may only occur 5% in the whole dataset collected. Besides, the number of fraudulent actions is much smaller than that of normal transactions in credit card usage data. When a clas-sifier is trained on such an imbalanced dataset, it often shows a strong bias toward the majority class, since the goal of many standard learning algorithms is to minimize the overall prediction error rate. Thus, by simply predicting every example as the majority class, the classifier can still achieve a very low error rate on a class-imbalanced dataset with, for example, 2% rare class.

When mining the class-imbalanced data, do we always get poor performance (e.g., 100% error) on the rare class? Can the error of the rare class (as well as the majority class) be bounded? If so, is the bound sensitive to the class imbalance ratio? Although the issue of class imbalance has been received extensive studies [9,3,2,7,4,5], as far as we know, no previous works have been done to answer those questions.

In fact, PAC learning (Probably Approximately Correct Learning) [8,6] is an appro-priate model to study the bounds for classifi cation performance. The traditional PAC learning model studies the learnability of the general concept for certain kinds of learn-ers (such as consistent learner and agnostic learner), and answers the question that how many examples would be sufficient to guarantee a low total error rate. However, pre-vious works [9] point out that accuracy or tota l error rate are inappropriate to evaluate the classification performan ce when class is imbalanced, since such metrics overly em-phasize the majority class and neglect the rare class which is usually more important in real-world applications. Thus, when class is imbalanced, better measures are desired. In our paper, we will use error rate on the rare (and majority) class and cost-weighted error 2 to evaluate the classification performance on class-imbalanced data. The error rate on the rare (and majority) class can reflect how well the rare (and majority) class is learned. If the misclassification cost of the class is known, we can adopt another common measure (cost-weighted error) to d eal with imbalanced data. By weighting the error rate on each class by its associated cost, we will get higher penalty for the error on the rare class (usually the more important class).

In our paper, we attempt to use the PAC-learning model to study, when class is im-balanced, how many sampled examples needed to guarantee a low error on a particu-lar class (the rare class or majority class) an d a low cost-weighted error respectively. A bound on cost-weighted error is necessary since it would naturally  X  X uppress X  er-rors on the rare class. We theoretically derive several upper bounds for both consistent learner and agnostic learner. Similar t o the upper bounds in traditional PAC learning, the bounds we derive are generally quite loose, but they do provide a theoretical guar-antee on the classification performance when class-imbalanced data is learned. Due to the loose bounds, to make our work more practical, we also empirically study how class imbalance affects the performance by using a specific learner. From our experi-mental results, some interesting implications can be found. The results in this paper can provide some theoretical foundations for mining the class-imbalanced data in the real world.

The rest of the paper is organized as follows. We theoretically derive several up-per bounds on the sample complexity for both consistent learner and agnostic learner. Then we empirically explore how class imbalan ce affects the classification performance by using a specific learner. Finally, we draw the conclusions and address our future work. In this section, we take advantage of PAC-learning theory to study the sample complex-ity when learning from the class-imbalanced data. Instead of bounding the total error rate, we focus on the error rate on a particul ar class (rare class or majority class) and the cost-weighted error. 2.1 Error Rate on a Particular Class First of all, we introduce some notations for readers X  convenience. We assume that the examples in training set T are drawn randomly and independently from a fixed but un-the rare class (the positive class) in D . For the class-imbalanced training set, p can be very small (such as 0.001). The number of total training examples is denoted as m and the number of positive and negative training examples are denoted as m + and m  X  re-negative training error, respectively, of h .

Given  X  (0 &lt;  X  &lt; 1) and  X  (0 &lt;  X  &lt; 1), the traditional PAC learning provides up-per bounds on the total number of training examples needed to guarantee e D ( h ) &lt;  X  with probability at least 1  X   X  . However, it guarantees nothing about the positive er-ror e D + ( h ) for the imbalanced datasets. As we discussed before, the majority classifier would predict every example as negative, resulting in a 100% error rate on the positive (rare) examples. To have a lower positive error, the learner should observe more posi-tive examples. Thus, in this subsection, we study the upper bounds of the examples on a particular class (say positive class here) needed to guarantee, with probability at least 1  X   X  , e
We first present a simple relation between the total error and the positive error as well as the negative error, and will use it to derive some upper bounds.
 0 .  X   X  p, then e Proof. To prove this, we simply observe that, Thus, Therefore, if e D ( h ) &lt;  X  +  X  p , e D + ( h ) &lt;  X  + .
 e
Theorem 1 simply tells us, as long as the total error is small enough, a desired positive error (as well as negative error) can always be guaranteed. Based on Theorem 1, we can  X  X euse X  the upper bounds in the traditional P AC learning model and adapt them to be the upper bounds of a particular class in the class-imbalanced datasets. We first consider consistent learner in the next subsection. Consistent Learner. We consider consistent learner L using hypothesis space H by assuming that the target concept c is representable by H ( c  X  H ). Consistent learner always makes correct prediction on the training examples. Let us assume that UB (  X  ,  X  ) is an upper bound on the sample size in the tr aditional PAC-learning, which means that, at least (1  X   X  ), e D ( h )  X   X  . The following theorem shows that we can adapt any upper bound in the traditional PAC-learning to th e bounds that guarantee a low error on the positive class and negative class respectively.

For any upper bound of a consistent PAC learner UB (  X  ,  X  ) , we can always replace  X  in UB (  X  ,  X  ) with  X  +  X  p or  X   X   X  ( 1  X  p ) , and consequently obtain a upper bound to guarantee the error rate on that particular class.
 Theorem 2. Given 0 &lt;  X  + &lt; 1 , if the number of positive examples then with probability at least 1  X   X  , the consistent learner will output a hypothesis h having e D + ( h )  X   X  + .
 0 output a hypothesis h having e D ( h )  X   X  .
 Here, we simply substitute  X  in UB (  X  ,  X  ) with  X  +  X  p , which is still within (0, 1). any consistent learner will output a hypothesis h having e D ( h )  X   X  +  X  p . According to Theorem 1, we get e D + ( h ) &lt;  X  + .

Also, m = m + p , thus we know, m  X  UB (  X  +  X  p ,  X  ) equals to Thus, the theorem is proved.

By using the similar proof to Theorem 2, we can also derive the upper bound for the negative class. Given 0 &lt;  X   X  &lt; 1, if the number of negative examples m  X   X  UB (  X   X   X  ( 1  X  p ) ,  X  )  X  ( 1  X  p ) , then, with probability at least 1  X   X  , the consistent learner will output a hypothesis h having e D  X  ( h )  X   X   X  .

The two upper bounds above can be adapted to any traditional upper bound of con-sistent learners. For instance, it is well known that any consistent learner using finite hypothesis space H has an upper bound 1  X   X  ( ln | H | + ln 1  X  ) [6]. Thus, by applying our new upper bounds, we obtain the following corollary.
 Corollary 1. For any consistent learner using finite hypothesis space H, the upper bound on the number of positive sample for e D + ( h )  X   X  + is and the upper bound on the number of negative sample for e D  X  ( h )  X   X   X  is
From Corollary 1, we can discover that when the consistent learner uses finite hy-pothesis space, the upper bound of sample size on a particular class is directly related to the desired error rate (  X  + or  X   X  ) on the class, and the class imbalance ratio p does not affect the upper bound. This indicates that, for consistent learner, no matter how class-imbalanced the data is (how small p is), as soon as we sample sufficient examples in a class, we can always achieve the desired error rate on that class.
 Agnostic Learner. In this subsection, we consider agnostic learner L using finite hy-pothesis space H ,whichmakes no assumption about whether or not the target concept c is representable by H . Agnostic learner simply finds the hypothesis with the mini-mum (probably non-zero) training error. Given an arbitrary small  X  + , we can not ensure e to happen with probability higher than 1  X   X  ,forsuch h with the minimum training error. To prove the upper bound for agnostic learner, we adapt the original proof for agnostic learner in [6]. The original proof regards drawing m examples from the distri-bution D as m independent Bernoulli trials, but in our proof, we only treat drawing m + examples from the positive class as m + Bernoulli trials.
 amples observed then with probability at least 1  X   X  , the agnostic learner will output a hypothesis h, such that e D + ( h )  X  e T + ( h )+  X  + Proof. For any h , we consider e D + ( h ) as the true probability that h will misclassify a randomly drawn positive example. e T + ( h ) is an observed frequency of misclassification over the given m + positive training examples. Since the entire training examples are drawn identically and independently, drawing and predicting positive training exam-ples are also identical and independent. Thus, we can treat drawing and predicting m + positive training examples as m + independent Bernoulli trials.
 Therefore, according to Hoeffding bounds, we can have, According to the inequation above, we can derive, This formula tells us that the probab ility that there exists one bad hypothesis h making e will hold true with the probability at least 1  X   X  . So, solving for m + in the inequation |
H Thus, the theorem is proved.

In fact, by using the similar procedure, we can also prove the upper bound for the number of negative examples m  X  when using agnostic learner: 1
We can observe a similar pattern here. The upper bounds for the agnostic learner are also not affected by the class imbalance ratio p .

From the upper bound of either consistent learner or agnostic learner we derived, we learned that when the amount of examples on a class is enough, class imbalance does not take any effect. This discovery act ually refutes a common misconception that we need more examples just because of the m ore imbalanced class ratio. We can see, the class imbalance is in fact a data insufficiency problem, which was also observed empirically in [4]. Here, we further confirm it with our theoretical analysis.
In this subsection, we derive a new relation (Theorem 1) between the positive error and the total error, and use it to derive a general upper bound (Theorem 2) which can be applied to any traditional PAC upper bound for consistent learner. We also extend the existing proof of agnostic learner to derive a upper bound on a particular class for agnostic learner. Although the proof of the theorems above may seem straightforward, no previous work explicitly states the same conclusion from the theoretical perspective.
It should be noted that although the agnostic learner outputs the hypothesis with the minimum (total) training error, it is possible that the outputted hypothesis has 100% error rate on the positive class in the training set. In this case, the guaranteed small dif-in 100% true error rate on the positive class. If the positive errors are more costly than the negative errors, it is more reasonable to assign higher cost for misclassifying positive examples, and let the agnostic learner minimize the cost-weighted training error instead of the flat training error. In the following part, we will introduce misclassification cost to our error bounds. 2.2 Cost-Weighted Error In this subsection, we take misclassification cost into consideration. We assume that the misclassification cost of the class is known, and the cost of a positive error (rare C
FP to represent the cost of misclassifying a positive example and a negative example, respectively. 3 And we denote r as the cost ratio, C FN C of error, named cost-weighted error . Definition 1 (Cost-Weighted Error). Given the cost ratio r, the class ratio p, e D + as the positive error on D, e D  X  as the negative error on D, the cost-weighted error on D can be defined as, By the same definition, we can also define the cost-weighted error on the training set T ratio and misclassification cost. The rp is the weight for the positive class and 1  X  p is the weight for the negative class. In our definition for the cost-weighted error, we use the normalized weight.

In the following part, we study the upper bounds for the examples needed to guaran-tee a low cost-weighted error on D . We give a non-trivial proof for the upper bounds of consistent learner, and the proof for the upper bound of agnostic learner is omitted due to its similarity to that of the consistent l earner (but only with finite hypothesis space). Consistent Learner. To derive a relatively tight upper bound of sample size for cost-weighted error, we first introduce a property. That is, there exist many combinations of positive error e D + and negative error e D  X  that can make the same cost-weighted error e upper bound to be the least required sample size among all the combinations of positive error and negative error that can make the desired cost-weighted error.
 of examples observed then, with probability at least 1  X   X  , the consistent learner will output a hypothesis h such that the cost-weighted error c D ( h )  X   X  .
 Proof. In order to make c D ( h )  X   X  , we should ensure, transformed into Xe D + +( 1  X  X ) e D  X   X   X  . To guarantee it, we should make sure, According to Corollary 1, if we observe, we can also ensure e D  X  ( h )  X  in order to have e D + on positive class, we also need to observe, To guarantee Formula (2) and (3), we need to sample at least m examples such that m
However, since e D + is a variable, different e D + will lead to different e D  X  , and thus affect m . In order to have a tight upper bound for m , we only need, When 1 e creasing function of e D + ,butwhen 1 e ing function of e D + . Thus, the minimum value of the function can be achieved when function, Therefore, as long as, then with probability at least 1  X   X  , the consistent learner will output a hypothesis h such that c D ( h )  X   X  .
 We can see that the upper bound of cost-weighted error for consistent learner is related to p and r . By performing a simple transformation, we can transform the above upper p decreases within (0, 0.5), the upper bound increases. It means that the more the class is imbalanced, the more examples we need to ach ieve a desired cost-weighted error. In this case, class imbalance actually affects the cl assification performance in terms of cost-weighted error. If we make another transformation to the upper bound, we can obtain, the upper bound also increases. It shows that a higher cost ratio C FN C more examples for training. Intuitively sp eaking, when class is imbalanced, the cost-weighted error largely depends on the error on the rare class. As we have proved before, to achieve the same error on the rare class, we need the same amount of examples on the rare class, thus more class-imbalanced data requires more examples in total. Besides, higher cost on the rare class leads to higher cost-weighted error, thus to achieve the same cost-weighted error, we will also need more examples in total.
 Agnostic Learner. As mentioned before, the hypothesis with the minimum training error produced by agnostic learner may still lead to 100% error rate on the rare class. Hence, instead of outputting the hypothesis w ith minimum training error, we redefine weighted error on the training set. Generally, with higher cost on positive errors, the agnostic learner is less likely to produce a hypothesis that misclassifies all the posi-tive training examples. The following theorem demonstrates that, for agnostic learner, how many examples needed to guarantee a small difference of the cost-weighted errors between the distribution D and the training set T .
 of examples observed then, with probability at least 1  X   X  , the agnostic learner will output a hypothesis h such that c D ( h )  X  c T ( h )+  X  .
 The proof for Theorem 5 is very similar to that of Theorem 4, thus here we omit the detail of the proof. Furthermore, we can also extract the same patterns from the upper bound here as found for the upper bound in Theorem 4: more examples are required when the cost ratio increases or th e class becomes more imbalanced.

To summarize, in this section we derive several upper bounds to guarantee the error for both consistent learner and agnostic learner. We found some interesting and useful patterns from those theoretical results: the upper bound for the error rate on a particular class is not affected by the class imbalance, while the upper bound for the cost-weighted error is sensitive to both the class imbalance and the cost ratio. Although those pattern may not be so surprising, as far as we know, no previous work theoretically proved it before. Such theoretical results would be m ore reliable than the results only based on the empirical observation.

Since the upper bounds we derive are closely related to the hypothesis space, which is often huge for many learning algorithms, they are generally very loose (It should be noted that in traditional PAC learning, the upper bounds are also very loose). In fact, when we practically use some specific learners , to achieve a desired error rate on a class or cost-weighted cost, usually the number of examples needed are much less than the theoretical upper bounds. Therefore, in the next section, we will empirically study the performance of a specific learner, to see how the class imbalance and cost ratio influence the classification performance. In this section, we empirically explore the patterns found in the theoretical upper bounds. We hope to see, in practice, how the class imbalance and cost ratio affect the actual ex-amples needed and whether th e empirical results reflect our theories. Those empirical observations can be useful for practical data mining or machine learning with class-imbalanced data. In our following experiments, we will empirically study the perfor-mance of unpruned decision tree (consistent learner) on class-imbalanced datasets. 4 3.1 Datasets and Settings We choose unpruned decision tree for our empirical study, since it is a consistent learner in any case. It can be always consistent with the training data of any concept by building up a full large tree, if there are no conflicting examples with the same attribute values but different class labels. For the specific implementation, we use WEKA [10] and select J48 with the pruning turned off and the parameter MinNumOb j = 1.

We create one artificial dataset and select two real-world datasets. The artificial dataset we use is generated by a tree function with five relevant attributes, A 1  X  A 5, and six leaves, as shown in Figure 1. To simulate the real-world dataset, we add another 11 irrelevant attributes. Therefore, with 16 binary attributes, we can generate 2 16 = 65 , 536 different examples, and label them with the target concept (28,672 positive and 36,864 negative). We also choose two UCI [1] real-world datasets ( Chess and Splice ). In order to make the unpruned decision tree with all the training examples, the conflicting exam-during the pre-process.
 3.2 Experimental Design and Results To see how class imbalance affects the error rate on a particular class (here we choose positive class), we compare the positive error under different class ratios but with the same number of positive examples in the training set.

Specifically, we manually generate different data distributions with various class ra-tios where training set and test set are drawn. For example, to generate a data distribu-tion with 10% positive proportion, we simply set the probability of drawing a positive example to be 1 / 9 of the probability of drawing a negative example, and the probabil-ity of drawing examples within a class is uniform. According to the data distribution, we sample a training set until it contains a certain number of positive examples (we set three different numbers for each dataset), and train a unpruned decision tree on it. Then, we evaluate its performance (positive error and cost-weighted error) on another sampled test set from the same data distribution. Finally, we compare the performance under different data distributions (0.1%, 0.5%, 1%, 5%, 10%, 25%, 50%) to see how class imbalance ratio affects the performance of the unpruned decision tree. All the results are the average value over 10 independent runs.

Figure 2 presents the positive error on three datasets. The three curves in each sub-graph represent three different numbers of positive examples in the training set. For the artificial dataset, since the concept is easy to learn, the number of positive examples chosen is smaller than that of the UCI datasets. We can see, generally the more the positive examples for training, the flatter the curve and the lower the positive error. It means, as we have more positive examples, class imbalance has less negative effect on the positive error in practice. The observation is actually consistent with Corollary 1.
To see how class imbalance influences the cos t-weighted error, we compare the cost-weighted error under different class ratios with fixed cost ratio. To explore how cost ratio affects the cost-weighted error, we comp are the cost-weighted error over different cost ratios with fixed class ratio. For this part, we only use the artificial dataset to show the results (see Figure 3). We can see, gene rally, as the class becomes more imbalanced or the cost ratio increases, the cost-weighted error goes higher. It is also consistent with our theory (Theorem 4).
 We have to point out that, our experiment is not a verification of our derived theories. The actual amount of examples we used in our experiment is much smaller compared to the theoretical bounds. Despite of that, we s till find that the empirical observations have similar patterns to our theoretical results. Thus, our theorems not only offer a theoretical guarantee, but also has some useful implications for real-world applications. In this paper, we study the class imbalance issue from PAC-learning perspective. An important contribution of our work is that, we theoretically prove that the upper bound of the error rate on a particular class is not af fected by the (imbalanced) class ratio. It actually refutes a common mis conception that we need more examples just because of the more imbalanced class ratio . Besides the theoretical theorems, we also empirically explore the issue of the class imbalance. The e mpirical observations reflect the patterns we found in our theoretical upper bounds, which means our theories are still helpful for the practical study of class-imbalanced data.

Although intuitively our results might seem to be straightforward, few previous works have explicitly addressed these fundamental issues with PAC bounds for class-imbalanced data before. Our work actually confirms the practical intuition by theoretical proof and fills a gap in the established PAC learning theory. For imbalanced data issue, we do need such a theoretical guideline for practical study.

In our future work, we will study bounds for AUC, since it is another useful measure for the imbalanced data. Another common heuristic method to deal with imbalanced data is over-sampling and under-sampling. We will also study their bounds in the future.
