 One guiding intuition in parsing, and data-driven NLP more generally, is that, all else equal, it is ad-vantageous to memorize large fragments of training examples. Taken to the extreme, this intuition sug-gests shortest derivation parsing (SDP), wherein a test sentence is analyzed in a way which uses as few training fragments as possible (Bod, 2000; Good-man, 2003). SDP certainly has appealing properties: it is simple and parameter free  X  there need not even be an explicit lexicon. However, SDP may be too simple to be competitive.

In this paper, we consider SDP in both its pure form and with several direct modifications, finding a range of behaviors. In its pure form, with no prun-ing or approximation, SDP is neither fast nor accu-rate, achieving less than 70% F1 on the English WSJ task. Moreover, basic tie-breaking variants and lexi-cal augmentation are insufficient to achieve compet-itive accuracies. 1 On the other hand, SDP is dramat-ically improved in both speed and accuracy when a simple, unlexicalized PCFG is used for coarse-to-fine pruning (and tie-breaking). On the English WSJ, the coarse PCFG and the fine SDP together achieve 87% F1 with basic treebank annotation (see Table 2) and up to 90% F1 with richer treebank an-notation (see Table 4).

The main contribution of this work is to analyze the behavior of shortest derivation parsing, showing both when it fails and when it succeeds. Our final parser, which combines a simple PCFG coarse pass with an otherwise pure SPD fine pass, can be quite accurate while being straightforward to implement. The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each frag-ment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima X  X n, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token X i is given its own unique grammar symbol.

We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG re-duction. Bansal and Klein (2010) use a carefully pa-rameterized weighting of the substructures in their grammar in an effort to extend the original DOP1 model (Bod, 1993; Goodman, 1996a). However, for SDP, the grammar is even simpler (Goodman, 2003). In principle, the implicit SDP grammar needs just two rule schemas: CONTINUE ( X p  X  Y q Z r ) and SWITCH ( X p  X  X q ), with additive costs 0 and 1, respectively. CONTINUE rules walk along training trees, while SWITCH rules change between trees for a unit cost. 2 Assuming that the SWITCH rules are in practice broken down into BEGIN and END sub-rules as in Bansal and Klein (2010), the grammar is linear in the size of the treebank. 3 Note that no lexicon is needed in this grammar: lexical switches are like any other.

A derivation in our grammar has weight (cost) w where w is the number of switches (or the num-ber of training fragments minus one) used to build the derivation (see Figure 1). The Viterbi dy-namic program for finding the shortest derivation is quite simple: it requires CKY to store only byte-valued switch-counts s ( X p , i, j ) (i.e., the number of switches) for each chart item and compute the derivation with the least switch-count. Specifically, in the dynamic program, if we use a SWITCH rule X p  X  X q , then we update If we use a continue rule X p  X  Y q Z r , then the up-date is where k is a split point in the chart. Using this dynamic program, we compute the exact shortest derivation parse in the full all-fragments grammar (which is reduced to a PCFG with 2 rules schemas as described above). SDP in its most basic form is appealingly simple, but has two serious issues: it is both slow and in-accurate. Because there are millions of grammar symbols, exact SDP parsing takes more than 45 sec-onds per sentence in our implementation (in addition to being highly memory-intensive). Many methods exist for speeding up parsing through approxima-tion, but basic SDP is too inaccurate to merit them. When implemented as described in Section 2, SDP achieves only 66% F1 on the WSJ task (dev set,  X  40 words).

Why does SDP perform so poorly? One reason for low accuracy may be that there are many short-est derivations, i.e. derivations that are all built with the fewest number of fragments, and that tie break-ing could be at fault. To investigate this, we tried various methods for tie-breaking: FIRST / LAST (pro-cedurally break ties), UNIFORM (sample derivations equally), FREQ (use the frequency of local rules). However, none of these methods help much, giv-ing results within a percentage of F1. In fact, even oracle tie-breaking, where ties are broken to favor the number of gold constituents in the derivation achieves only 80% F1, indicating that correct deriva-tions are often not the shortest ones. Another rea-son for the poor performance of SDP may be that the parameter-free treatment of the lexical layer is particularly pathological. Indeed, this hypothesis is partially verified by the result that using a lexicon (similar to that in Petrov et al. (2006)) at the termi-nal layer brings the uniform tie-breaking result up to 80% F1. However, combining a lexicon with oracle tie-breaking yields only 81.8% F1.

These results at first seem quite discouraging, but we will show that they can be easily improved with information from even a simple PCFG. The additional information that makes shortest derivation parsing work comes from a coarse un-lexicalized PCFG. In the standard way, our PCFG consists of the local (depth-1) rules X  X  Y Z with probability P ( Y Z | X ) computed using the count of the rule and the count of the nonterminal X in the given treebank (no smoothing was used). Our coarse grammar uses a lexicon with unknown word classes, similar to that in Petrov et al. (2006). When taken from a binarized treebank with one level of parent annotation (Johnson, 1998) and horizontal markovization, the PCFG is quite small, with around 3500 symbols and 25000 rules; it achieves an accu-racy of 84% on its own (see Table 2), so the PCFG on its own is better than the basic SDP, but still rela-tively weak.

When filtered by a coarse PCFG pass, how-ever, SDP becomes both fast and accurate, even for the basic, lexicon-free SDP formulation. Summed marginals (posteriors) are computed in the coarse PCFG and used for pruning and tie-breaking in the SDP chart, as described next. Pruning works in the standard coarse-to-fine (CTF) way (see Charniak et al. (2006)). If a particular base symbol X is pruned by the PCFG coarse pass for a particular span ( i, j ) (i.e., the posterior marginal P ( X, i, j | s ) is less than a certain threshold), then in the full SDP pass we do not allow building any indexed symbol X l of type X for span ( i, j ) . In all our pruning-based experiments, we use a log posterior threshold of  X  3 . 8 , tuned on the WSJ development set.

We also use the PCFG coarse pass for tie-breaking. During Viterbi shortest-derivation pars-ing (after coarse-pruning), if two derivations have the same cost (i.e., the number of switches), then we break the tie between them by choosing the deriva-tion which has a higher sum of coarse posteriors (i.e., the sum of the coarse PCFG chart-cell pos-teriors P ( X, i, j | s ) used to build the derivation). The coarse PCFG has an extremely beneficial in-teraction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (see Model F1 EX F1 EX B&amp;K2010 pruned 88.4 33.7 88.5 33.0 B&amp;K2010 unpruned 87.9 32.4 88.1 31.9 Table 2). In addition, the speed of parsing and memory-requirements improve by more than an or-der of magnitude over the exact SDP pass alone.
It is perhaps surprising that coarse-pass pruning improves accuracy by such a large amount for SDP. Indeed, given that past all-fragments work has used a coarse pass for speed, and that we are the first (to our knowledge) to actually parse at scale with an implicit grammar without such a coarse pass, it is a worry that previous results could be crucially de-pendent on fortuitous coarse-pass pruning. To check one such result, we ran the full, weighted AFG con-struction of Bansal and Klein (2010) without any pruning (using the maximum recall objective as they did). Their results hold up without pruning: the re-sults of the unpruned version are only around 0.5% less (in parsing F1) than the results achieved with pruning (see Table 1). However, in the case of our shortest-derivation parser, the coarse-pass is essen-tial for high accuracies (and for speed and memory, as always). We have seen that basic, unpruned SDP is both slow and inaccurate, but improves greatly when comple-mented by a coarse PCFG pass; these results are shown in Table 2. Shortest derivation parsing with a PCFG coarse-pass (PCFG+SDP) achieves an accu-racy of nearly 87% F1 (on the WSJ test set,  X  40 word sentences), which is significantly higher than the accuracy of the PCFG or SDP alone. 5 When the coarse PCFG is combined with basic SDP, the majority of the improvement comes from pruning with the coarse-posteriors; tie-breaking with coarse-posteriors contributes around 0.5% F1 over pruning.
Figure 2 shows the number of fragments for short-est derivation parsing (averaged for each sentence length). Note that the number of fragments is of course greater for the combined PCFG+SDP model than the exact basic SDP model (which is guaranteed to be minimal). This result provides some analysis of how coarse-pruning helps SDP: it illustrates that the coarse-pass filters out certain short but inaccu-rate derivations (that the minimal SDP on its own is forced to choose) to improve performance.
 Figure 3 shows the parsing accuracy of the PCFG+SDP model for various pruning thresholds in coarse-to-fine pruning. Note how this is differ-ent from the standard coarse-pass pruning graphs (see Charniak et al. (1998), Petrov and Klein (2007), Bansal and Klein (2010)) where only a small im-provement is achieved from pruning. In contrast, coarse-pass pruning provides large accuracy benefits here, perhaps because of the unusual complementar-ity of the two grammars (typical coarse passes are designed to be as similar as possible to their fine counterparts, even explicitly so in Petrov and Klein (2007)).

Our PCFG+SDP parser is more accurate than re-cent sampling-based TSG X  X  (Post and Gildea, 2009; Cohn and Blunsom, 2010), who achieve 83-85% F1, and it is competitive with more complex weighted-fragment approaches. 6 See Bansal and Klein (2010) for a more thorough comparison to other parsing work. In addition to being accurate, the PCFG+SDP parser is simple and fast, requiring negligible train-ing and tuning. It takes 2 sec/sentence, less than 2 GB of memory and is written in less than 2000 lines of Java code, including I/O. 7 5.1 Other Treebanks One nice property of the parameter-free, all-fragments SDP approach is that we can easily trans-fer it to any new domain with a treebank, or any new annotation of an existing treebank. Table 3 shows domain adaptation performance by the re-sults for training and testing on the Brown and German datasets. 8 On Brown, we perform better than the relatively complex lexicalized Model 1 of Collins (1999). For German, our parser outperforms Dubey (2005) and we are not far behind latent-variable parsers, for which parsing is substantially Annotation F1 EX F1 EX S TAN -A NNOTATION 88.1 34.3 87.4 32.2 B ERK -A NNOTATION 90.0 38.9 89.5 36.8 more complex. 5.2 Treebank Annotations PCFG+SDP achieves 87% F1 on the English WSJ task using basic annotation only (i.e., one level of parent annotation and horizontal markoviza-tion). Table 4 shows that by pre-transforming the WSJ treebank with richer annotation from previ-ous work, we can obtain state-of-the-art accuracies of up to 90% F1 with no change to our simple parser. In S TAN -A NNOTATION , we annotate the treebank symbols with annotations from the Stan-ford parser (Klein and Manning, 2003). In B ERK -A
NNOTATION , we annotate with the splits learned via hard-EM and 5 split-merge rounds of the Berke-ley parser (Petrov et al., 2006). Our investigation of shortest-derivation parsing showed that, in the exact case, SDP performs poorly. When pruned (and, to a much lesser extent, tie-broken) by a coarse PCFG, however, it is competi-tive with a range of other, more complex techniques. An advantage of this approach is that the fine SDP pass is actually quite simple compared to typical fine passes, while still retaining enough complementarity to the coarse PCFG to increase final accuracies. One aspect of our findings that may apply more broadly is the caution that coarse-to-fine methods may some-times be more critical to end system quality than generally thought.
 We would like to thank Adam Pauls, Slav Petrov and the anonymous reviewers for their helpful sug-gestions. This research is supported by BBN un-der DARPA contract HR0011-06-C-0022 and by the Office of Naval Research under MURI Grant No. N000140911081.

