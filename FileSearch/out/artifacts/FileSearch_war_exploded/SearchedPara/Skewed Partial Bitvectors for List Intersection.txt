 This paper examines the space-time performance of in-memory conjunctive list intersection algorithms, as used in search engines, where integers represent document identifiers. We demonstrate that the combination of bitvectors, large skips, delta compressed lists and URL ordering produces superior results to using skips or bitvectors alone.

We define semi-bitvectors, a new partial bitvector data structure that stores the front of the list using a bitvector and the remainder using skips and delta compression. To make it particularly e  X  ective, we propose that documents be ordered so as to skew the postings lists to have dense regions at the front. This can be accomplished by grouping documents by their size in a descending manner and then re-ordering within each group using URL ordering. In each list, the division point between bitvector and delta compression can occur at any group boundary. We explore the perfor-mance of semi-bitvectors using the GOV2 dataset for var-ious numbers of groups, resulting in significant space-time improvements over existing approaches.

Semi-bitvectors do not directly support ranking. Indeed, bitvectors are not believed to be useful for ranking based search systems, because frequencies and o  X  sets cannot be included in their structure. To refute this belief, we propose several approaches to improve the performance of ranking-based search systems using bitvectors, and leave their verifi-cation for future work. These proposals suggest that bitvec-tors, and more particularly semi-bitvectors, warrant closer examination by the research community.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation (e ciency and ef-fectiveness) ;H.2.4[ Database Management ]: Systems X  Query Processing Algorithms; Performance Information Retrieval; Algorithms; Database Index; Perfor-mance; E ciency; Optimization; Compression; Intersection
We examine the space-time performance for algorithms that perform in-memory intersection of ordered integer lists. These algorithms are used in search engines where the in-tegers are document identifiers and in databases where the integers are row identifiers. We assume that the lists are stored in integer order, which allows for fast merging and for compression using deltas.

Intersecting multiple lists can be implemented by inter-secting the two smallest lists, then repeatedly intersecting the result with the next smallest list in order. This set-versus-set (svs) or term-at-a-time (TAAT) approach is fast because of its sequential memory access, but it requires extra memory for intermediate results. Alternatively, a non-svs, document-at-a-time (DAAT) approach can be used, requir-ing less space for intermediate results, but having a slower random access pattern. We use the faster svs approach, but our work can also be applied to non-svs intersection.
Our experiments use data and queries from the search engine domain, so the lists of integers are usually encoded using bitvectors or compressed delta encodings such as vari-able byte (vbyte), PForDelta (PFD), and simple16 (S16), often with list index structures (skips) that allow jumping over portions of the compressed lists. Using skips results in significant performance gains, and so does using bitvectors for large lists. We have found that combining bitvectors with large skips gives the best results, regardless of the encoding, so we use this combination in our performance tests.
The integers in our lists are document identifiers assigned by the search engine. Typically, these values are assigned based on the order that the documents were indexed, re-ferred to as the original order. Changing the order of the documents can produce smaller and faster systems. Many orderings have been examined in the literature, such as doc-ument size, content clustering, TSP, URL, and hybrid or-derings. A detailed review of such reordering techniques is presented in Section 2.2. The URL ordering is easy to compute and produces comparable performance to the best approaches [23], so we use this as our basis of comparison.
The goal in this paper is to improve on the space-time performance of the combined bitvectors+skips algorithm ex-ecuted on a URL ordered index. This is accomplished using partial bitvectors via the following contributions:
We apply these techniques to the TREC GOV2 dataset and queries, and the end result is a significant improve-ment in runtime together with a small improvement in space. When compared to algorithms using only skips, the improve-ment is very significant, on top of the benefits from using URL ordering.

The remainder of the paper describes related work in Sec-tion 2, experimental setup in Section 3, partial bitvectors in Section 4, ranking in Section 5, partitioning in Section 6, extensions in Section 7, and conclusions in Section 8.
In this section, we present related work in list intersec-tion and illustrate various algorithms X  performance using space-time graphs, for which the experimental setup uses the GOV2 corpus and is described in Section 3. We also present related work exploring the e  X  ect of document reordering on the performance of search algorithms.
There are many algorithms available for intersecting un-compressed integer lists. For a broad performance compari-son see Barbay et al. [4]. Many of these algorithms are fast, but their memory use is very large (e.g., storing each integer in 32 bits) and probes into the list can produce wasted or ine cient memory access.

There are a large variety of compression algorithms avail-able for sorted integer lists. They first convert the lists into di  X  erences minus one (deltas or d-gaps) to get smaller val-ues, removing the ability to randomly access elements in the list. Next, a variable length encoding is used to reduce the number of bits needed to store the values, often grouping multiple values together to get word or byte alignment.
We examine the standard variable byte (vbyte) encoding and some of the top performers: PForDelta (PFD) [32] and simple16 (S16) [30]. The S16 encoding uses a variable block size, but we combine these small blocks into larger fixed sized blocks [30] for a clearer comparison with PFD. The vbyte encoding is byte aligned, storing seven bits of a delta with one bit indicating additional data. As with the simple9 [1] encoding it is based upon, the S16 encoding is word (4-byte) aligned, using 4 bits to allocate the remaining 28 bits to fit a few deltas. The PFD encoding combines multiples of 32 deltas into a block (padding lists with zeros to fill out the blocks) to become word aligned. The block of deltas are stored in the same number of bits, with any that cannot fit (up to 10%) stored separately as exceptions using a linked list to indicate their location and then storing the values at the end of the encoding. Lists smaller than 100 use normal vbyte encoding (to avoid the expense of padding lists with zeros to fit into blocks), thus producing a hybrid algorithm. We do not examine other variations of these encodings, such as the VSEncoding (VSE) algorithm [26] which has dynam-ically varying block sizes, storing deltas in each block using the same number of bits without exceptions. Recent work has improved decoding and delta restore speeds for many al-gorithms using vectorization [18], with some optimizations using more space. Another recent approach first acts on the values as monotone sequences, then incorporates some delta encoding more deeply into the compression algorithm [28]. While such approaches can be combined with our work, we do not explore this here.

List indexes are included to jump over values and thus avoid decoding, or even accessing, portions of the lists. A simple list index algorithm groups by a fixed number of el-ements storing every X th element in an array [20], where X is a constant, and we refer to it as skips . Variable length skips are possible, but the di  X  erences are not important here. Another approach groups by a fixed size document identi-fier range into segments [20], allowing array lookups into the list index. This segment approach has similar performance to skips for the large jump points we are using and it con-flicts with block based encodings, so it is not examined here. List index algorithms can be used with compressed lists by storing the deltas of the jump points, but the block based structure causes complications if the jump points are not byte or word aligned. To prevent non-aligned jump points, skips over block based encodings choose the block size to be equal to the skip size X [16]. Similarly, using compression algorithms that act on large blocks can obstruct the runtime performance of skips.

For list intersection, the compression algorithms produce much smaller encodings, but they are slow. Uncompressed lists are larger, but random access makes them fast. Com-bining list indexes with the compressed algorithms adds lit-tle space, and this targeted access into the lists allows them to be faster than the uncompressed algorithms.

When using a compact domain of integers, as we are, the lists can be stored as bitvectors, where the bit number is the integer value and the bit is set if the integer is in the list. For our dataset, such an encoding uses large amounts of space but can have very good runtime performance. To alleviate the space costs, lists with document frequency less than F can be stored using vbyte compression, resulting in a hybrid bitvector algorithm [11]. This hybrid algorithm, vbyte+bitvectors, is faster than non-bitvector algorithms, and some settings result in smaller configurations, since very large lists can be more compactly stored as bitvectors. We found similar improvements in runtime, with smaller im-provements in space, when combining bitvectors with more compact compression algorithms such as PFD and S16.
We have found that bitvectors combined with large skips perform better than either skips or bitvectors separately, re-gardless of the compression algorithm. Large skips of size 256 give a good space-time tradeo  X  when combined with bitvectors, even though that size is not the fastest when skips are used by themselves. Since the best algorithms in terms of space and runtime use bitvectors for large lists, future work in this area should remove large lists from con-sideration when comparing compression algorithms.
We compare performance of various algorithms using a graph of space (bits per posting when encoding the entire dataset of lists) versus time (milliseconds per query when running the entire workload sequentially). The space-time performance of various configurations of an algorithm are connected to form a performance curve. These configura-tions are from the X settings for the skips algorithm and the F settings for the bitvector algorithm. The bitvector+skips algorithm uses a fixed X value of 256 and various F settings.
A comparison of skips, bitvectors and bitvectors+skips using PFD encoding under the original document ordering is shown in Figure 1 (top), where points further left and down use less space and less time, respectively. Clearly, the combination of PFD+bitvectors+skips is much faster than PFD+bitvectors or PFD+skips, especially when the configurations use small amounts of space.

For a more detailed background of integer list compression and list intersection, we recommend the survey of search engine techniques written by Zobel and Mo  X  at [31]. The performance of many of these algorithms have previously been compared in other experimental settings [4, 11, 20, 30].
Intersecting lists of integers applies to search engines when the integers are document identifiers assigned by the system, giving a compact domain of values and small deltas that are compressible. This assignment of identifiers can be changed to produce space and/or runtime benefits, and we refer to this process as reordering the documents.

Reordering can improve space usage by placing documents with similar terms close together in the ordering, thus re-ducing the deltas, which can then be stored using smaller amounts of space. This reduces the index space as well as the amount of data being accessed per query. We have found that a reduction in data access per query does not improve the runtime of in-memory intersection, because data trans-fer is not the bottleneck in such systems. Note, reordering can also improve compression in other areas, such as the term frequencies embedded in the lists [29].

Reordering can improve runtime performance by produc-ing data clustering within the lists [29], as well as query re-sult clustering within the document identifier domain [17]. This gives larger gaps in the document domain during query processing, which causes list indexes (skips) to work better and the optimal skip size to increase. This also causes fewer cache line loads within bitvectors, making them more e -cient. These runtime improvements are seen not just in list intersection performance, but with frequency, ranking, and even dynamic pruning, where knowledge of the ranking algo-rithm is used to avoid processing some parts of the lists [27]. Tuning the compression algorithms to an ordering can also give a better space-time tradeo  X  [29].

The runtime benefits of reordering come from using skips and bitvectors to avoid accessing portions of the lists, rather than from reading more compressed data. Clearly, space im-provement and decoding time are not the only metrics that should be considered when comparing document orderings. Below we present various document ordering techniques: Random: If the documents are ordered randomly (rand), there are no trends for the encoding schemes or the inter-section algorithms to exploit, so this is a base of comparison for the other orderings.
 Original: The dataset comes in an original order (orig), which may be the order in which the data was crawled. This Figure 1: Space vs. time graphs for intersection algorithms with skip size X and bitvector cuto  X  frequency F . ordering could be anything, so it may not a good base. We have found that using the original order of the GOV2 dataset gives only small improvement over random ordering. Rank: Reordering to approximate ranking allows the engine to terminate early when su ciently good results are found. A global document order [19], such as PageRank or result occurrence count in a training set [15], can be used. Individ-ual lists could be ordered independently, as done in impact ordering [2], increasing space usage and requiring accumula-tors to process queries. These techniques essentially prune portions of the lists from the calculation. Thus they sacri-fice space in order to improve runtime performance, while the remaining types of ordering exploit information from the dataset to improve both space and runtime performance. Matrix: Reordering by manipulating the document vs. term matrix can produce improvements in space by grouping doc-uments with high frequency terms [21], producing a block diagonal matrix [5], or creating run-length encodable por-tions of the matrix [3]. Manipulating the matrix for large datasets can be expensive, and merging subindexes can be di cult, so these techniques have not been widely used. Document Size: The simple method of ordering docu-ments by decreasing number of unique terms in the docu-ment (td) produces index compression [9] and runtime per-formance improvements [27], while requiring no additional information about the documents and very little processing at indexing time. Ordering by terms-in-document is approx-imately the same as ordering by the number of tokens in the document or by document size. The improvements obtained from terms-in-document ordering are not as large as occurs with other orderings, so it has been mostly ignored. Content Similarity: Ordering by content similarity uses some similarity metric that is applied in various ways to produce an order. Ordering using normal content clustering techniques [7] or a travelling salesman problem (TSP) [22] formulation can produce space improvements. However, even with various improvements [6, 13, 24, 25], these approaches are too slow to be used in practice. In addition, these tech-niques must start from scratch when subindexes are merged, although not for subindex compaction.
 Metadata: Ordering lexicographically by URL provides similar improvements in space usage as obtained from or-dering by content similarity [23], and it improves runtime substantially when using skips [29]. URL ordering is es-sentially using the human-based organization of website au-thors, which often groups the documents by topic, to pro-duce content similarity in the ordering. Using other meta-data makes this technique broadly applicable, but the e  X  ec-tiveness can vary greatly based on the dataset and density distribution of the data within the chosen domain. This approach is simple to compute at indexing time and can support fast merging of subindexes.
 Hybrid: Ordering by terms-in-document is not as e  X  ective as ordering by URL, but these two can be combined to get a slightly better result. For example, one hybrid approach groups the documents by URL server name, then subdivides each into 5 document size ranges, and finally orders by URL within each subdivided group [13]. This approach is similar to what we present later in this paper, but the reasoning and final result are quite di  X  erent.

Ordering documents lexicographically by URL is fast to calculate, just as good as any of the other approaches [23], and especially e  X  ective for the GOV2 dataset. URL order-ing achieves this performance by placing documents with similar terms close together. Such tight clustering reduces the delta sizes substantially, with approximately 69.8% of the deltas having the value one in URL ordering vs. ap-proximately 20.4% for random ordering. This suggests that there is limited room for additional improvement from new ordering methods.

A summary of the papers on ordering documents is shown in Table 1, where the last two columns indicate if the paper is examining the space and/or runtime benefits from ordering the documents.
The runtime performance of skips has been shown to im-prove by approximately 50% when the index is ordered by URL [29]. Our bitvectors+skips algorithm is, however, still superior to bitvectors or skips separately, regardless of the compression algorithm or document ordering. As a result, we use the bitvectors+skips algorithm for the remainder of this paper. The performance of the bitvectors+skips algo-rithm using the PFD encoding under the random, original, terms-in-document and URL orderings is shown in Figure 1 (middle). Furthermore, experiments using vbyte and S16 en-codings produce similar runtime performance improvements. The terms-in-document ordering produces benefits over orig-inal and random orderings, but URL ordering is clearly much better than the others in terms of both space and time.
The amount of compression from ordering by URL has been shown for various datasets, and most uses produce significant improvements in space. The rate of improve-ment varies considerably for di  X  erent encodings, for exam-ple, vbyte compression improves space by 8.1%, PFD im-proves space by 24.7%, and S16 improves space by 43.1% for our GOV2 index. This di  X  erence in improvement rate makes S16 much smaller than PFD under the URL ordering, as shown in Figure 1 (bottom). (The performance using the S16 and PFD encodings is presented, but the vbyte encod-ing is omitted because it is dominated by PFD.) The URL ordering and resultant compression does not, however, pro-duce runtime improvements for list intersection until skips or bitvectors are added. In fact, these runtime improvements are not proportional to the space savings, suggesting that memory transfer time is not the bottleneck.
Clearly, the URL ordering is better than the others and the bitvectors+skips algorithm is fast. While the S16 encod-ing gives smaller results, the PFD encoding is faster and still requires small amounts of space. As a result, in the next few sections we use the faster PFD+bitvectors+skips(url) algo-rithm as our basis of comparison, and we show how to im-prove upon this base by skewing the distribution of postings and creating partial bitvectors over certain dense regions. Please note that the more compact S16 based algorithm can also be similarly improved.
We use the TREC GOV2 corpus, indexed by Wumpus 1 without stemming to extract document postings. The cor-pus size is 426GB from 25.5 million documents, giving 9 billion document postings and 49 million terms.

Our workload is a random sample of 5000 queries chosen by Barbay et al. [4] from the 100,000 corpus queries, which we have found to produce very stable results. These queries are tokenized by Wumpus, giving an average of 4.1 terms per query. Query statistics are summarized in Table 2, including averages of the smallest list size, the sum of all list sizes, and the result list size over all queries with the indicated number of terms for the entire corpus. For our runtime calculations, we remove the single term queries. terms queries % smallest all result
Our experiments simulate a full index: we load the post-ings lists for query batches, encode them, flush the CPU cache by scanning a large array, then execute the conjunc-tive intersection of terms to produce the results. Each query has its own copy of its encoded postings lists, so performance is independent of query order and shared terms. Intersection runtimes per step are recorded, and overall runtimes are the sums over all steps for all queries. Space and time values ignore the dictionary, positional information, and ranking.
Our code was run on an AMD Phenom II X6 1090T 3.6Ghz Processor with 6GB of memory, 6mb L3, 512k L2 and 64k L1 caches running Ubuntu Linux 2.6.32-43-server with a single thread executing the queries. The gcc com-piler was used with the -O3 optimizations to produce high performance code. The query results were visually verified to be plausible and automatically verified to be consistent for all algorithms.

We used the C++ language and classes for readability, but the core algorithms use only non-virtual inline functions, allowing a large range of compiler optimizations. We encode directly into a byte array for each list, and then include http://www.wumpus-search.org/ Figure 2: Terms-in-document distribution for the GOV2 dataset, cuto  X  s for three groups are marked.
 Figure 3: Schematic of a three-group semi-bitvector index. decode time in our runtimes to produce more realistic and repeatable measurements. The code was tuned to minimize memory access and cache line loads.
We develop our approach to representing postings lists in two steps. First, we introduce partial bitvectors over grouped lists in terms-in-document ordering. After that, we show that ordering by URL within groups outperforms other representations.
The URL and clustering based orderings place documents with similar terms close together, producing tight clustering within the postings lists. The terms-in-document ordering, however, does not place documents with similar terms to-gether in the ordering. Instead, the ordering by decreas-ing number of terms-in-document packs more postings into lower document identifiers, meaning that the density of the postings lists tends to decrease throughout the lists. This front-packing results in many smaller deltas, which can be more easily compressed. The front-packing also means that values in the postings lists are denser for lower document identifiers, giving skewed clustering with more e  X  ective skips at the end of the lists. This skewing of postings to lower doc-ument identifiers can be clearly seen in the distribution of terms-in-document values, as shown in Figure 2. The dot-ted lines split the index into three groups containing equal numbers of postings, meaning that the largest 10.9% of doc-uments contain 33.3% of the postings.

In addition, the likelihood of a document occurring in the intersection of multiple lists increases as the number of lists containing the document identifier increases, which is ex-actly the number of terms in the document. This causes the result list to be even more skewed towards lower doc-ument identifiers than the input lists. The result lists are indeed skewed in our query workload: the largest 10.9% of the documents contain 58.2% of the intersection results. Such a skew of the result list is similar to what would be ex-pected from ordering by the document X  X  usage rate in a set of training queries, where the usage rate could be measured by the number of times a document occurs in the postings lists or the result lists of the queries [15]. Preliminary ex-periments indicate that the terms-in-document ordering has similar performance to such ordering by usage, but these results are not presented here.

We can exploit the skewed nature of the terms-in-document ordering by using partial bitvectors. In particular, we use bitvectors for the denser front portion of a postings list, and then normal delta compression and skips for the rest of the list. We call this front partial bitvector structure a semi-bitvector , and the highest document identifier in the bitvec-tor portion of a postings list is called the cut point . The semi-bitvector intersection algorithm must first order the lists ascending by their cut points, then execute in a pair-wise set-versus-set manner. Each pairwise list intersection has three (possibly empty) parts that are executed in order: bitvector-to-bitvector, sequence-to-bitvector, and sequence-to-sequence. In general, the end result contains a partial bitvector that must be converted to values, followed by a se-quence of values. The intersection of two semi-bitvectors is defined in Algorithm 1 using basic intersection subroutines acting on bitvectors and sequences of integers.
 Algorithm 1 intersect semi-bitvector 1: function SemiBV (M,N) 2: r {} 3: s M. bitvSize 4: t N. bitvSize . assert ( s  X  t ) 5: b bvand ( M. bitv ,N. bitv ,s ) 6: if N. isLastList then 7: r r [ bvconvert ( b, s ) 8: r r [ bvcontains ( M. seq . select ( value &lt;t ) ,N. bitv ,t ) 9: r r [ merge ( M. seq . select ( value t ) ,N. seq ) 10: if N. isLastList then 11: return r 12: return new semi -bitvector ( b, r, s )
Our implementation of semi-bitvector intersection applies various optimizations: The bvand and bvconvert algorithms (lines 5 and 7) are executed in a single pass on the last in-tersection and the bitvector b is not created if the query contains only two lists. The restrictions on M. seq applied by the select calls (lines 8 and 9) are executed as a single pass on M . Also the two conditionals from the select call ( value &lt;t ) and the loop through M. seq (line 8) are com-bined when possible (i.e., first find the end point t in an uncompressed sequence or the nearest skip point before t in a compressed sequence, then use that location as a single conditional check). The result set r can be reused between pairwise steps (i.e., as input from the last step and out-put of the current step), except on the final step where the bitvector-to-bitvector portion is added (line 7). Figure 4: Space vs. time graph for semi-bitvectors using cut-o  X  frequency F and td-g1024-td ordering compared to the bitvectors+skips algorithm using cuto  X  frequency F ,skip size 256 and either URL or terms-in-document ordering.
Our semi-bitvector structure allows more postings to be stored in bitvectors for the same amount of memory used. Since the performance of bitvectors is much faster than other approaches, better use of bitvectors can produce a significant improvement in runtime performance, allowing the overall system to be more e cient.

We pick the semi-bitvector cut points so that the bitvec-tor portion of each list will have at least frequency F .We make the cut point calculation faster by splitting the docu-ment domain into groups and only allowing semi-bitvectors to have cut points at group boundaries. A schematic of a semi-bitvector index using three groups (and thus four po-tential cut points) with lists ordered by their cut points is shown in Figure 3. The cut point for a list is the highest group boundary where the group itself is above the density threshold F , and the bitvector portion (from the start of the list to the end of the group) is also above F . This definition allows a large number of groups to be used without degrad-ing the index with too few or too many bitvector regions.
We choose group boundaries so that each group contains the same number of postings. (Other approaches could be used to determine the group boundaries, but this is not rele-vant here.) When we run this semi-bitvector structure with many groups using the terms-in-document ordering, we see significant performance improvement. The performance of semi-bitvectors for terms-in-document ordering using 1024 groups (td-g1024-td) is shown in Figure 4. The improve-ment means that PFD+semi-bitvectors(td-g1024-td) domi-nates PFD+bitvectors+skips(td), and it is faster than our previously best URL based approach for configurations us-ing larger amounts of memory. However, it is still slower for small configurations, and no configuration is as small as what can be achieved with URL ordering.
The terms-in-document ordering gives skewed clustering towards the front of the lists, while URL ordering and the other approaches give tight clustering throughout the lists. We would like to combine these two orderings into a hybrid ordering to produce the benefits of both skewed clustering and tight clustering. url td td-g3-url Figure 5: Plots of delta counts across the document domain for various document orderings.
 Terms-in-document ordering was previously combined with URL ordering by Ding et al. [13] in the form of url.server-td-url, which splits into chunks by url.server, then groups into five parts by terms-in-document, then orders by URL. This hybrid ordering gives slight benefits in terms of space, but the e  X  ect on runtime performance was not tested. While the method of determining group separations (i.e., the bound-aries for each terms-in-document group) was not specified, the url.server portion of the hybrid ordering will split the index into many small pieces. As a result, the skew from the subsequent td ordering is spread out across the entire document range. This means that the skew cannot be easily exploited through grouping as we did in Section 4.1. For our hybrid combination of terms-in-document and URL ordering, we first group the documents by their terms-in-document value, then reorder within each group using the URL ordering. We will refer to it as td-g#-url, where # is the number of groups. (Since the GOV2 dataset covers only one section of the Web, we can relate this new approach to the previous hybrid approach as being in the form of url.server.su x-td-url.) Increasing the number of groups of documents will reduce the tight clustering from the URL or-dering, but increase the skewed clustering of the data. This means that as the number of groups increases, the perfor-mance will trend towards the grouped terms-in-document performance and thus degrade  X  X oherence. X 
The td-g3-url ordering results in some skewing of deltas towards the front of the lists, as shown in Figure 5 (bottom). It also reduces the delta sizes as compared to URL ordering, with approximately 71.9% of the deltas having the value one for this ordering. Figure 6: Entropy vs. number of terms-in-document groups.
We measure the compressibility of the data using zero order Shannon entropy H on the deltas d (which assumes deltas are independent and generated with the same proba-bility distribution), where p i is the probability of delta i in the data: Lower values of entropy indicate that more compression is possible. The td-g-url approach can improve entropy com-pared to the URL ordering, as shown in Figure 6, where four groups is the optimal setting. Surprisingly, even with one hundred groups, the entropy has not significantly degraded, even though the entropy of the (pure) terms-in-document ordering is 5.07, which is much higher, and we expect that splitting the index into many groups will degrade perfor-mance towards terms-in-document ordering.

The actual space-time performance for di  X  erent numbers of groups and di  X  erent F values is shown in Table 3. Using four groups produces the smallest configuration with F = 1 but for other F values, using eight groups is better than using four groups in both space and time. As a result, we use td-g8-url as our optimal configuration.
 td-g2-url 5.32 1.77 6.28 1.33 8.11 1.08 td-g4-url 5.16 1.65 6.15 1.22 7.92 0.98 td-g8-url 5.23 1.56 6.14 1.17 7.79 0.96 td-g12-url 5.28 1.54 6.23 1.16 7.94 0.96 td-g16-url 5.34 1.55 6.28 1.15 7.98 0.96 Table 3: Space (bits/posting) and time (ms/q) performance of PFD+semi-bitvectors for various numbers of groups and cuto  X  values F .

When we compare our semi-bitvector approach using td-g8-url ordering to the bitvectors+skips algorithm using URL ordering, we see a significant improvement in performance. For the same amount of memory, the semi-bitvectors pro-duce a speedup of at least 1.4x compared to the best URL ordering approach, and a small reduction in space usage is also possible, as shown in Figure 7. Interestingly, running the bitvectors+skips algorithm using the new td-g8-url or-dering produces very little improvement, and running semi-bitvectors without grouping by terms-in-document also pro-duces little improvement. Clearly, both the grouping and Figure 7: Space vs. time graph and improvement graph for semi-bitvectors using cuto  X  frequency F and td-g8-url order-ing compared to the bitvectors+skips algorithm using cuto  X  frequency F , URL ordering and skip size 256. semi-bitvectors are needed to produce the performance im-provements of our approach.

In addition, the space-time benefits of semi-bitvectors for the terms-in-document ordering (td-g1024-td vs. td) are sim-ilar to the benefits of semi-bitvectors for the URL ordering (td-g8-url vs. url). This suggests that our td-g-url approach is combining the benefits of tight clustering found in the URL ordering with the benefits of skewed clustering found in the terms-in-document ordering.

Most of the publicly available search systems do not use bitvectors or combine bitvectors with skips. As a result, a more appropriate comparison is between our semi-bitvectors and simple skips, where we found a speedup of at least 2.4x, as shown in Figure 8. This comparison also shows that a sig-nificant space improvement is possible. These benefits are in addition to the performance gains from using URL ordering rather than some other ordering. Such ine cient orderings may be common in existing installations. Incredibly, our semi-bitvector approach has a speedup of at least 6.0x com-pared to skips using the original ordering, while using the same amount of space, although significant improvements to space are possible.

Similar types of runtime improvements would occur with any compression algorithm, since the benefits come from using bitvectors in dense regions where they are much faster than any compression algorithm.
For comparison, we provide performance numbers from various papers using the GOV2 dataset (Table 4). Clearly, our approach using bitvectors can answer a conjunctive query much faster than the existing ranking based systems, while using much less space than a full index. The runtime per-formance di  X  erences are much bigger than any hardware or Figure 8: Space vs. time graph and improvement graph for semi-bitvectors using cuto  X  frequency F and td-g8-url or-dering compared to the skips algorithm using URL ordering and skip size X . query di  X  erences could produce. Indeed, our results are at a disadvantage, because we have indexed much more data, including the HTTP header information (6.8 billion [13] vs. 9.0 billion document level postings).

We have demonstrated that executing conjunctive queries with semi-bitvectors can be done using small amounts of space to produce extremely fast runtimes compared to rank-ing based search systems. These characteristics suggest that ranking-based systems can benefit from judiciously incor-porating semi-bitvector data structures. We introduce five possible approaches below: Pre-filter: Use semi-bitvectors to produce the conjunc-tive results, then process the ranking structures restricted to these results, as suggested in previous work [11]. This may require reordering of conjunctive results if the ranking structures use a di  X  erent document ordering. The ranking structures could use non-query based information, such as PageRank, or normal ranking structures, thus duplicating some postings information. Having the conjunctive results can make the ranking process more e cient by exploiting skips in the first list, or limiting the number of accumu-lators. Using conjunctive results to pre-filter proximity or phrase queries is the natural implementation approach. Us-ing this type of pre-filtering, however, prevents the use of non-AND based processing such as Weak-AND [8].
 Sub-document pre-filter: Use semi-bitvectors in a pre-filtering step as a heuristic to limit the results to high quality or highly ranked documents by exploiting the correlation of query term proximity to query relevance [10]. This is accom-plished by splitting the documents into (potentially over-lapping) sub-document windows, then building the semi-bitvector structures over these windows. This reduces the number of results that must be ranked, while the results be-ing ranked will be highly relevant because the query terms Lucene (vbyte) 26.0 42.1 text docID+o  X  sets AND+counts N [28] Exhaustive AND 6.56 4.5 text docID+freq. BM25 Y [14] appear close together. The conjunctive results will need to be mapped from window IDs to document IDs before executing the ranking step. The windows could be imple-mented as half-overlapping windows to guarantee proximity of query terms within half the window size. Clearly, this approach needs more examination to determine if signifi-cant filtering can be achieved without adversely a  X  ecting ranking e  X  ectiveness. If this approach can produce signif-icant filtering, the ranking step could be implemented by directly storing the tokens of each window for quick rank-ing/proximity/phrase processing.
 High density filters: High density terms have low value for ranking, with the extreme case being stopwords. How-ever, they can still act as a filter and be processed more e ciently using semi-bitvectors. In fact, high density re-gions of a postings lists may act similarly, but a constant ranking value may be needed to smoothly integrate filtering regions with ranking regions in a single postings list. Based on our results, even using semi-bitvectors for postings lists with document frequency F 1 8 can result in significant performance benefits.
 Query specific filter: The terms that could be imple-mented as filters may be query specific. To improve the processing e ciency of these filtering terms, duplicate struc-tures can be introduced: a semi-bitvector structure for filter-ing and a separate structure suitable for ranking. In fact, ad-ditional information about the user, such as topics of inter-est, can be included in the ranking algorithm. This may re-duce the e  X  ect of query terms in the ranking, allowing more query terms to be executed as filters using semi-bitvectors. Guided processing: Semi-bitvector structures can be used to produce conjunctive results that will provide statistics on the query, and these statistics can guide subsequent process-ing of the query. For example, the statistics can indicate whether ranking should be done using conjunctive process-ing or some form of non-conjunctive processing, such as a Weak-AND implementation. These statistics can also indi-cate how to adapt this processing to the specific query terms, perhaps by identifying the specific query term that causes the conjunctive processing to be overly restrictive. Process-ing the conjunctive results for a subset of the documents may be enough to produce e  X  ective statistics. Such adap-tive query processing techniques deserve close examination.
Previously, we were able to capitalize on the postings list skew resulting from terms-in-document ordering by us-ing partitioning [17]. Like semi-bitvectors, this partitioning mechanism, in conjunction with URL ordering, allows e  X  ec-tive use of bitvectors and skips. It was argued that parti-tioning by document size would be valuable in a distributed environment. When we ran experiments using three parti-tions, URL ordering within each partition, and the bitvec-tor+skips algorithm, the overall space and runtime perfor-mance was similar to our semi-bitvectors with td-g8-url.
To gain these benefits in a single machine environment, all of the partitions must run on the same machine. The multi-partition approach, however, has several limitations: the costs to manage multiple partitions, the overheads per query for each partition, and the wasted space in duplicated dictionary entries for the terms found in multiple partitions. Determining when the benefits of partitioning outweigh the limitations of running multiple partitions on a single ma-chine may be highly specific to the situation.
Our hybrid td-g-url ordering has been described as group-ing by terms-in-document, followed by ordering within each group by URL, but it is equivalent to sieving documents from the URL ordering based on their terms-in-document values. A more detailed combination of URL X  X  tight cluster-ing and terms-in-document X  X  skewed clustering could pro-vide a better combined ordering, and we leave such explo-ration for future work.

Alternative hybrid orderings could be computed by com-bining groups using terms-in-document ordering with some other second ordering. This allows the exploitation of bet-ter general ordering techniques or orderings tuned to the workload and dataset. As such, our grouping by terms-in-document approach acts to boost the performance of another document ordering technique.

In addition, the combination of grouping by terms-in-document with a second ordering could reduce the amount of time needed to calculate the second ordering, because the secondary ordering acts only on the documents within each group, rather than on the documents in the entire in-dex. This could be a big advantage for ordering techniques that do not scale well, such as content similarity based al-gorithms.

If a search system is unable to use document ordering tech-niques, perhaps because the system has a very high update rate, the documents could still be grouped (or partitioned) by their terms-in-document size to produce some benefits. Indeed, any partial ordering that can exploit some amount of tight clustering or skewed clustering may have large benefits for such systems.
We have shown how groups of documents defined by the skewed terms-in-document ordering, when combined with URL ordering and partial bitvectors, can be used to make list intersection more e cient. This is accomplished by form-ing varying densities within grouped portions of the postings lists, reordering within the groups by URL ordering, and then storing them as semi-bitvectors, which encode dense front portions of the lists as bitvectors. Essentially, this al-lows us to store more postings in bitvectors for a given space budget, and these bitvectors are much faster than other ap-proaches. This combination gives most of the benefits of tight clustering in URL ordering, while also gaining the ben-efits of skewed clustering for e  X  ective use of semi-bitvectors.
This multi-ordered configuration (td-g-url) gives signifi-cant space-time improvements, when combined with semi-bitvectors. When compared to a fast and compact configura-tion that combines bitvectors, large skips and URL ordering, we get a speedup of at least 1.4x. When compared to using only skips with URL ordering, we get a speedup of at least 2.4x. While the overall improvement will depend on the size and type of the data, as well as the number of groups used, we expect significant benefits for most large datasets.
To expand the applicability of semi-bitvectors, we have described various methods for using them to improve rank-ing based search systems. These proposals warrant further investigation.
This research was supported by the University of Water-loo and by the Natural Sciences and Engineering Research Council of Canada. We thank the researchers at WestLab, Polytechnic Institute of NYU for providing their block based compression code [30]. [1] V. N. Anh and A. Mo  X  at. Inverted index compression [2] V. N. Anh and A. Mo  X  at. Simplified similarity scoring [3] D. Arroyuelo, S. Gonz  X alez, M. Oyarz  X un, and [4] J. Barbay, A. L  X opez-Ortiz, T. Lu, and A. Salinger. An [5] I. C. Baykan. Inverted index compression based on [6] R. Blanco and A. Barreiro. TSP and cluster-based [7] D. Blandford and G. Blelloch. Index compression [8] A. Z. Broder, D. Carmel, M. Herscovici, A. So  X  er, and [9] S. B  X  uttcher, C. Clarke, and G. V. Cormack. [10] S. B  X  uttcher, C. L. Clarke, and B. Lushman. Term [11] J. S. Culpepper and A. Mo  X  at. E cient set [12] C. Dimopoulos, S. Nepomnyachiy, and T. Suel. [13] S. Ding, J. Attenberg, and T. Suel. Scalable [14] S. Ding and T. Suel. Faster top-k document retrieval [15] S. Garcia, H. E. Williams, and A. Cannane.
 [16] S. Jonassen and S. E. Bratsberg. E cient compressed [17] A. Kane and F. W. Tompa. Distribution by document [18] D. Lemire and L. Boytsov. Decoding billions of [19] X. Long and T. Suel. Optimized query execution in [20] P. Sanders and F. Transier. Intersection in integer [21] L. Shi and B. Wang. Yet another sorting-based [22] W.-Y. Shieh, T.-F. Chen, J. J.-J. Shann, and C.-P. [23] F. Silvestri. Sorting out the document identifier [24] F. Silvestri, S. Orlando, and R. Perego. Assigning [25] F. Silvestri, R. Perego, and S. Orlando. Assigning [26] F. Silvestri and R. Venturini. VSEncoding: e cient [27] N. Tonellotto, C. Macdonald, and I. Ounis. E  X  ect of [28] S. Vigna. Quasi-succinct indices. In WSDM , pages [29] H. Yan, S. Ding, and T. Suel. Inverted index [30] J. Zhang, X. Long, and T. Suel. Performance of [31] J. Zobel and A. Mo  X  at. Inverted files for text search [32] M. Zukowski, S. Heman, N. Nes, and P. Boncz.
