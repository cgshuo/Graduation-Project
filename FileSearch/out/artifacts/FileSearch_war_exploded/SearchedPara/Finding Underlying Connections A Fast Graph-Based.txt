 Andrew Moore AWM @ CS . CMU . EDU David Cohn COHN +@ CS . CMU . EDU Many techniques in the social sciences, criminology, and graph theory deal with the problem of analyzing patterns found in the underlying structure and associations of a group of entities. A typical query may examine an entity X  X   X  X mportance X  by looking at its relations to others and thus its position within the underlying structure (Wasserman &amp; Faust, 1994). However, in many real world situations true structure may not be known. Instead the relevant informa-tion may be buried within large amounts of noisy data. In this paper, we examine the problem of learning and querying a graph-based model of the underlying structure. The model is learned from link data, where each link is a single noisy input observation, specifically a set of entities observed to share some relation. As with most sources of real world data, we assume this data is noisy, possibly con-taining many irrelevant links, as well as links with spurious or missing members. Below, we present cGraph, a learning method that approximates the underlying structure. We ac-count for different types of links, which may vary in their frequency and extent of noise. We also incorporate infor-mation about when a link occurred, allowing us to account for changes in the underlying structure over time. We also introduce the  X  X riends X  problem, which examines how strongly two entities are connected and attempts to predict which entities will co-occur in future links. We use this query to test the effectiveness of the cGraph algorithm as compared to a variety of other algorithms.
 The motivation for this research is the increasing number of analytical applications that are trying to exploit mas-sive amounts of noisy transactional and co-occurrence data. This kind of data is important to insurance analysts, intelli-gence analysts, criminal investigators, epidemiologists, hu-man resource professionals, and marketing analysts. Of the many questions such analysts might like to ask, one general question is  X  X hat are the underlying relationships among people in this organization? X  and, at a more focused level:  X  X ho are the other people most likely to be directly inter-acting with this person? X  For example, if a key employee leaves a corporation, HR needs to know who is most likely to be impacted. Or if an individual is caught transporting illegal materials, law enforcement may need to rapidly de-duce who are possible accomplices. Graphs, or social networks, are a common method for rep-resenting the relationships among a set of entities. Nodes represent the entities and edges capture the relations be-tween them. For example, a simple graph representing the  X  X riends X  relation may consist of a graph with an undirected edge between any two entities that are friends.
 We use a directed weighted graph to capture the underlying relations between entities, but restrict the weights to form a multinomial distribution over the outgoing edges. In other words for W AB , the weight of an edge from A to B , we have: Thus W can be interpreted as a transition matrix. It should be noted that an edge is a weighted ordered pair represent-ing a true underlying connection and is different from the input links. An example collaborative graph is shown in Figure 1. Such a graph-based structure is simple, capturing only pair wise relationships, but is fast to approximate and query and is easy to interpret. The model for the N P entities is learned from a set of N observed links. A link is a single noisy input observation, The observations are noisy -some relations that exist in truth may not be observed, and some observations may not be the result of actual relations. A given link may also have spurious or missing members. The word link should not be interpreted too narrowly. A link can be used to capture a range of different relations such as: direct communication, co-occurrence, or even sharing a common attribute. 3.1. Link Generation Below we present several models for link generation. These models serve both to provide a model of link gen-eration (for MLE graphs and creating artificial data) and also as motivation for the approximations presented below. The random walk model of the link generation considers a link as a random non-self-intersecting walk on the under-lying graph. This walk continues for a set number of prop-agations or until it reaches a dead-end. Formally, given a link L j  X  1 that contains j  X  1 entities such that A last last entity added to the link, the next entity for the link is chosen as: The link is terminated if there is no A i with P ( A i | A L While this process can be viewed as a random walk, the transition probabilities are not independent. Since the walk must be non-self-intersecting, the probability of transition-ing to the A i depends not only on A last but also on all of the other entities already in the link. The random tree model is a more realistic model of link generation, where the j th entity can be added to the link by any of the previous members of the link. A  X  X eed entity X  starts the link. Each new entity is then added to the link by a random entity that is already in the link. Formally, given a link L j  X  1 that contains j  X  1 entities and a randomly chosen  X  X nviter X  from the link, A inv  X  L j  X  1 , the next entity for the link is chosen as: The link is terminated if there is no A i and A inv such that P ( 3.2. Maximum Likelihood Estimate Graphs It is possible to find the MLE of the underlying graph given the generative model and the set of observed links by just using a simple optimization scheme to search the graph weights. This optimization would include at least N 2 P real valued parameters, making the search expensive for large numbers of entities. Unfortunately since the ordering of the links is unknown, the step of evaluating the likelihood of the data may also be computationally expensive. Assum-ing the random walk generative model, simply calculating the probability of a link with N entities requires examining up to N ! different orderings. The first and most important task for cGraph is to learn the underlying graph from the link information. If the learned graph does not reflect the true underlying structure, future queries to it may return incorrect results. 4.1. Collaborative Graph Approximation The cGraph algorithm learns the underlying graph by using weighted counts of co-occurrences to approximate the edge weights. These counts can be accumulated during a single scan of the data. The resulting edge weights can then be computed directly from these counts.
 We can motivate this approximation by interpreting W as P ( B | A ) : the probability that if we randomly chose a link containing A and randomly chose a second entity in the link, this entity would be B . This simple interpretation leads to the approximation: where the second line follows from the assumption that both the link containing A and the second entity were cho-sen uniformly.
 We can extend this sampling interpretation of edge weights further to account for noise. We assume that the links of different types have different and unknown probabilities of being relevant and noise free, denoted by U ( L . ty pe ) larly, links that occur at different times have different prob-abilities of being relevant to the current graph, denoted by T (
L . time , t ) . Thus the edge weights represent the probabil-ity of choosing B from randomly selected relevant (both in type and time) and noise free link containing A . Given the above probabilistic interpretation, we can derive a closed form approximation of the edge weights as: where | L | is the size of the link, U ( L . ty pe ) is the typical weighting of a link of type L . ty pe , and T ( L . time , temporal weighting of a link at time t where the link oc-curred at time L . time . The choice of weighting functions is described in the following section. This approach is simi-lar to the methods presented by Newman (2001) and Kautz et. al. (1997), but accounts for additional information con-tained in the links and is normalized to ensure a probabilis-tic interpretation.
 The weighted counting approximation given in (4) can also be justified in the context of the above generative mod-els. In both models the second entity is sampled directly as P (
B 2 | A 1 ) , where B i indicates that entity B was the i th en-tity added to the link. Although we do not know the order that entities were added to the link, if we treat each possi-ble link ordering as equally probable, we can approximate the weight of an edge from A to B as a percentage of pos-sible link orderings in which A 1 and B 2 . A combinatorial argument on either model leads to the same approximation: We can then approximate the overall probability of A 1  X  B 2 as the average probability of A 1  X  B 2 in the links in which A appeared, which results in an approximation iden-tical to that in (4). The difference between the approximate and exact model is that we are no longer accounting for the fact that no entity can be added to a link twice. 4.2. Temporal and Typical Weighting The temporal weighting of a link, T ( L . time , t ) , determines the extent to which older links are counted as relevant. The intuition behind using a temporal weighting function is that we expect recent links to be more indicative of the current graph than links that occurred some time ago. Thus we choose T ( L , t ) to be a exponential decay function: where  X  is a parameter that determines the decay rate of the exponential. The choice of  X  = 0 makes all links that occurred on or before t equally likely.
 The typical weighting of a link type determines how much a link of that type should be counted compared to links of other types. Thus we can restrict the weights to lie in the range of [ 0 , 1 ] , where a weight of 0 results in the cGraph algorithm effectively ignoring all links of that type. 4.3. Time Complexity and Scaling To build the graph we only need to scan through the data one time and accumulate weighted counts of the number of occurrences and co-occurrences. If | L i | is the length of link if |
L max | is the length of the largest link, accumulating the counts has an upper bound of O ( N L  X  X  L max | 2 ) . In addition we need to normalize the outgoing edge weights. If we let B max be the largest number of outgoing edges (branch-ing factor) from a single node, the entire approximation re-quires time O ( N L  X  X  L max | 2 + N P  X  B Max ) . Table 1 illustrates how this algorithm scales. The entry t build is the time in milliseconds (or 0.0 if less than 0.1 ms) it takes to approximate the graph once. Figure 2 shows how the algorithm scales with the number of links (from a given data set) used to build the model. Even for large numbers of links and entities this approximation is reasonably fast. The approximation presented above contains several pa-rameters, such as the link type weighting and the tempo-ral decay parameter. While it may often be possible to set these parameters a-priori using domain specific knowledge, a more appealing approach is to learn these parameters. If the parameters are unspecified, then the cGraph algorithm attempts to learn them using cross validation. The perfor-mance is measured by how closely the learned graph corre-sponds to the normalized counts from the cross validation set. Unless otherwise noted, the experiments below learn type weightings using cross validation and use  X  = 0 . One of the primary goals of the cGraph algorithm is to an-swer such queries as:  X  X ist A  X  X  top 10 current/future/new collaborators? X  We call these queries the friend identifica-tion problem and consider two versions. The All Friends problem asks:  X  X iven the data who are the K entities with whom A is most likely to appear future links? X  The New Friends problem asks:  X  X iven the data seen so far, who are the K entities with whom A has never appeared in a link and is most likely to appear with in future links? X  These are predictive questions about co-occurrences for each en-tity as opposed to more standard web link analysis queries such as identifying authoritative pages (e.g. hubs and au-thorities (Gibson et al., 1998)).
 Another way to look at the friendship problem is as a dis-tance or proximity measure where two entities that are  X  X lose X  are more likely to collaborate in the future. The simplest proximity function is just W AB . Although this method is computationally cheap, it is unable to general-ize past co-occurrences it has already seen.
 We use a distance measure based on a nonself-intersecting walk that is able to capture the concept of  X  X  friend of a friend. X  Specifically, we define the proximity between A and B to be the probability of a fixed length, random, non-self-intersecting walk from A will include B . Formally: where V ( A , B , D ) is the set of all nonself-intersecting walks of length less than or equal to D from A to B and e i  X  v is a directed edge in v . We approximate this metric by only considering paths of length at most three steps. All nodes which are not reachable from A in at most three steps are given an arbitrarily low proximity.
 This measure uses a depth first search over the graph and thus may be computationally expensive. Formally we can place an upper bound on the running time of O ( min { B D max , N p } ) . As this expression shows, the friend-ship query is independent of the number of links and scales polynomially with the branching factor of the graph. Table 1 and Figure 2 again illustrate how this operation scales to different size data sets; t Q indicates the average time of a friendship query on the learned graph. 7.1. Data Sets To test the cGraph algorithm we used a variety of real world data sets, described below and summarized in Ta-ble 1. These data sets will be available (in the same form used here) at: http://www.autonlab.org/ . 1. The Lab Data consists of co-publication links for 2. The Institute Data is a set of links of three differ-3. The Citeseer Data is a collection of co-publication 4. The Drinks Data Set consists of popular bar tending 5. The Manual (Webpages) Data is a set of links created 7.2. Competing Algorithms The performance of the cGraph algorithm on the friends identification task was tested against a variety of other algorithms. Simple baseline and strawman comparisons were given by the Random , (Co-occurrence) Counting , and Popular (Entity) algorithms. The Random algorithm re-turns unique random entities as an entity X  X  friends. The (Co-occurrence) Counting algorithm returns those entities that have most often co-occurred with an entity as its top friends. Finally, the Popular (Entity) algorithm counts up the number of links in which each entity has occurred and ranks the entities according to this count. The predicted friends of any entity are just the most  X  X opular X  entities (excluding the entity itself) according to this ranking. Spectral Clustering algorithms include a broad range of techniques based on eigenvalue analysis of feature vectors that have been shown effective in the field of network anal-ysis and bibliometrics (Garfield et al., 1978; Gibson et al., 1998; Ng et al., 2001). This analysis can be viewed as a form of factoring: it begins with a matrix A , where each row i corresponds to a link and each column j corresponds to an entity. The value of A ( i , j ) indicates whether entity j participated in link i . A singular value decomposition (SVD) decomposes A into sets of left and right singular vectors (Press et al., 1993). The left singular vectors cor-respond to canonical sets of entities, which may be inter-preted as latent groups, while the right singular vectors cor-respond to the  X  X ixing proportions X  in which these groups should be combined to best approximate the observed links. Spectral clustering consists of assigning each observed link to the canonical set that dominates its mixture.
 The mixing proportions may also be used to estimate sim-ilarities between entities in one of a number of ways. In this paper, we estimate the  X  X ffinity X  of two entities as the Euclidean distance between the vectors representing their mixing proportions. Friends are predicted as entities with the highest  X  X ffinity X  to a given entity.
 In this paper, we examined both traditional SVD-based spectral clustering (labeled as  X  X pectral-S X ) and a variation based on non-negative matrix factorization (NNMF) (Lee &amp; Seung, 2001), labeled as  X  X pectral-N X  below. NNMF produces a decomposition similar to the SVD, but adds the constraint that an entity X  X  membership in the canonical sets must be strictly non-negative. 7.3. Cross Validation Tests We used sequential cross validation on the above data sets to compare the performance of the algorithms. The perfor-mance of an algorithm is the percentage of guessed friends with which it actually co-occurred in the test set. The num-ber of friends requested for each entity was equal to the true number of test set co-occurrences. It is important to appre-ciate that this test is very similar to a real world use of this algorithm. Given all data up to this time, we would like to make queries about possible connections in this time step. Table 2 shows the performance of various algorithms on the above data sets. The performance is given as the mean success rate for 10 fold sequential cross validation. The runs were treated as independent and results marked with a * are significantly different (using  X  = 0 . 05 ) from those that are not marked. If no entry in the column is marked then the performance of most algorithms on that test were not significantly different.
 Table 2 illustrates that there is no one algorithm that always outperforms the other algorithms. It is important to appre-ciate though that on all of the data sets and metrics cGraph is among the best performers. On the Institute, Citeseer, and Drinks data it is significantly better than many of the other algorithms. Further, even when it is not the single best performer, its performance is very often practically close to that of the best algorithm. Another interesting trend shown in the table is the good performance of the Popular algo-rithm. This success suggests a possible extension to the cGraph algorithm by incorporating priors for the entities. 7.4. Effect of Beta Term To demonstrate temporal weighting X  X  effect on perfor-mance sequential cross validation were run on the 1606 publication links in the Institute data set. These links were time stamped with the year of the publication. The goal was to predict collaborations in the K th subset given the previ-ous subsets. Results for several values of  X  are shown in Table 3. For both metrics, the performance of cGraph with  X  = 0 . 5 was significantly better than with  X  = 0 . 0 with a significance level of  X  = 0 . 05 .
 Initially increasing  X  can lead to increased performance at predicting future collaborations by placing more weight on recent collaborations. But as  X  continues to increase, the performance decreases as more recent information is dis-counted. In fact when  X  grows too large all information before the current time is effectively ignored. Another important test of cGraph is whether it actu-ally learns the  X  X orrect X  graph or a good approximation. Queries to an incorrect graph are at best unreliable. As a test, we compared learned graphs to known graphs that generated the link data.
 In addition to evaluating the performance of the cGraph al-gorithm, we compared its performance to that of several other learning algorithms. The first algorithm was a simpli-fied version of the cGraph algorithm using fixed, equal link type weights. This algorithm counts links weighted solely by the size of the link and thus is similar to the approaches presented by Newman (2001) and Kautz et. al. (1997). The second algorithm was an unweighted link counting ap-proach, which did not even take into account link size. The third algorithm used a hill-climbing search to find the MLE graph and weights as described in Section 3.2. The un-derlying generative model was assumed to be the random walk model on all of the tests. It is important to appreci-ate that we do not expect MLE to necessarily perform bet-ter, because the structure with the highest likelihood may not be the true underlying structure. The fourth algorithm also used a hill-climbing search to find the MLE graph for the random walk model, but assumed fixed, equal link type weights. Finally, to provide an approximate baseline, the fifth algorithm simply generated a random graph. 8.1. Data Generation for  X  X nown Graph X  Tests At the start of each test, a ground truth graph was randomly generated. Edges added between two nodes were added in both directions, but often had different weights. A set of 2,000 links, including noise links, was then created using one of the generative models. A link was created by first choosing a link type and then choosing to make the link a completely random with probability 1  X  weight ( linkty pe All tests used three different link types with fixed weights of 0.9, 0.9, and 0.5. Thus while one link type contained significantly more noise links than the others, none of the types were completely noise free.
 8.2. Results on Known Graphs Table 4 summarizes the resulting performance of the algo-rithms. Each column consists of the average error over 50 tests using the indicated generative model and number of entities. The error metric itself is the squared distance be-tween the learned and known graphs: where W i AB is the weight of the edge from A to B in graph i . This metric provides a reasonable approximation of the distance between two graphs and is computationally effi-cient to calculate. The entries in Table 4 marked with a * are statistically significantly (with  X  = 0 . 05 ) better than all unmarked entities in their column.
 The results show a clear advantage to weighting the dif-ferent link types and an advantage to using the cGraph algorithm over the MLE as the number of entities grows. Although the MLE algorithm outperformed the cGraph al-gorithm for a small number of entities, this difference is comparatively small compared to the difference between cGraph and the other algorithms. More significantly, the cGraph algorithm has a distinct advantage on data sets with more links and larger links. The link sizes above were lim-ited to at most 5 entities. For more links or links of larger sizes, simply evaluating the probability for MLE may not be computationally feasible. As illustrated by several of our data sets and examples above, our work is similar to the analysis of co-citations, referral webs, and web structure. Within the areas of bib-liometrics and web analysis eigenvector methods, such as spectral clustering, have been shown effective (Garfield et al., 1978; Gibson et al., 1998; Ng et al., 2001). These techniques presented a promising complement/alternative to our research and thus were considered in the above tests. It is important to appreciate that while our work is similar to the areas above, we are attempting to infer different infor-mation. Specifically, we wish to infer the underlying direct connections between entities. Co-citations and linked web-sites imply only relatively weak and indirect connections between the entities, such as common research topic. The use of probabilistic models is another recent approach to the above problems (Cohn &amp; Hofmann, 2001; Friedman et al., 1999; Getoor et al., 2001; Kubica et al., 2002). Cohn and Hofmann (2001) presented a model of document gen-eration where they assume that documents are generated from a mixture of sources and that the contents of the docu-ment (citations and terms) are probabilistically determined from these sources. Similarly Kubica et. al. (2001) pre-sented a model of link generation where links are gener-ated from a single underlying group and then have noise added. These models differ significantly from ours in that we do not assume that a mixture of sources, but rather prob-abilistically determine an entity X  X  membership in the link by which entities are already members of the link. Getoor et. al. (2001) present a technique that uses a probabilistic relational model to classify web pages. Here again there is an important difference in the type of information we are attempting to infer. We are not attempting to classify the entities or to create a full probabilistic model, but rather to predict the underlying direct connections between entities. Our approach is similar to techniques presented by New-man (2001) and Kautz et. al. (1997). Both Newman and Kautz et. al. describe methods for approximating and querying a social network given link data. Although both present approaches based on a weighted count of co-occurrences, these differ from our own. Both works use heuristics to choose link weighting and neither work con-siders link reliability factors such as type or when the link occurred. Further, both works attempt answer questions different from the friendship problem. For example, Kautz et. al. attempt to make referrals by finding chains be-tween two entities. While this is similar to our problem it lacks the important characteristic that multiple paths imply a stronger, although indirect, connection.
 Many other techniques and fields, such as social network analysis, complement our research since they require that an underlying graph structure is known or can easily be in-ferred from data. For example p*, creates predictive mod-els for relational variables (such as our friendship ques-tion) from a collection of explanatory variables extracted from demographics and a known social network (Ander-son et al., 1999; Wasserman &amp; Pattison, 1996). Another example is the work of Schwartz and Wood (1992) where a graph is used to find subgraphs of people with related interests. While Schwartz and Wood estimate the graph us-ing link information, they limit this estimation to Boolean edges indicating the presence of any communication. We examined the problem of quickly inferring an under-lying graph structure to capture relationships between enti-ties. This structure is learned from a collection of noisy link data, which for many problems may be easier to obtain than the true underlying graph. In conjunction, we introduced the  X  X riends X  problem, which examines how strongly two entities are connected even if they have never appeared in a link together. These problems have promise for applica-tions in areas such as: sociology, marketing, security, and the study of artificial networks.
 There are several natural extensions that we leave as future work. First is the incorporation of the concept of  X  X opular-ity of entities X  into the model. Motivated by both a natural extension to the generative models and the success of the Popular algorithm on the new friends problem, the incor-poration of priors may boost performance. Second is an extension beyond positive pairwise relationships to include such relationships as A , B and C are all likely to appear together in pairs, but never as a triple.
 Jeremy Kubica is supported by a grant from the Fannie and John Hertz Foundation. This research is supported by DARPA under award number F30602-01-2-0569. The au-thors would also like to thank Steve Lawrence for making the Citeseer data available.
 Anderson, C. J., Wasserman, S., &amp; Crouch, B. (1999). A p* primer: Logit models for social networks. Social Net-works , 21 , 37 X 66.
 Cohn, D., &amp; Hofmann, T. (2001). The missing link -a probabilistic model of document content and hypertext connectivity. Advances in Neural Information Process-ing Systems 13 (pp. 430 X 436). Cambridge, MA: MIT Press.
 Friedman, N., Getoor, L., Koller, D., &amp; Pfeffer, A. (1999).
Learning probabilistic relational models. Proceedings of the Seventeenth International Joint Conference on Artifi-cial Intelligence (pp. 1300 X 1309). San Francisco: Mor-gan Kaufmann Publishers.
 Garfield, E., Malin, M. V., &amp; Small, H. (1978). Citation data as science indicators. In Y. Elkana, J. Lederberg, R. K. Merton, A. Thackray and H. Zuckerman (Eds.),
Toward a metric of science: The advent of science indi-cators . New York: John Wiley and Sons.
 Getoor, L., Segal, E., Taskar, B., &amp; Koller, D. (2001). Prob-abilistic models of text and link structure for hypertext classification. IJCAI01 Workshop on Text Learning: Be-yond Supervision . Seattle, Washington.
 Gibson, D., Kleinberg, J., &amp; Raghavan, P. (1998). Inferring web communities from link topology. Proceedings of the Nineth ACM Conference on Hypertext and Hypermedia . New York: ACM Press.
 Kautz, H. A., Selman, B., &amp; Shah, M. A. (1997). The hidden web. AI Magazine , 18 , 27 X 36.
 Kubica, J., Moore, A., Schneider, J., &amp; Yang, Y. (2002).
Stochastic link and group detection. Proceedings of the Eighteenth National Conference on Artificial Intel-ligence (pp. 798 X 804). New York: ACM Press.
 Lee, D. D., &amp; Seung, H. S. (2001). Algorithms for non-negative matrix factorization. Advances in Neural In-formation Processing Systems 13 (pp. 556 X 562). MIT Press.
 Newman, M. E. J. (2001). Who is the best connected scien-tist? a study of scientific coauthorship networks. Phys. Rev. , 64 .
 Ng, A. Y., Zheng, A. X., &amp; Jordan, M. I. (2001). Link anal-ysis, eigenvectors and stability. Proceedings of the Sev-enteenth International Joint Conference on Artificial In-telligence (pp. 903 X 910). San Francisco: Morgan Kauf-mann Publishers.
 Press, W. H., Flannery, B. P., Teukolsky, S. A., &amp; Vetter-ling, W. T. (1993). Numerical recipes in c : The art of scientific computing . Cambridge, England: Cambridge University Press. Second edition.
 Schwartz, M., &amp; Wood, D. (1992). Discovering shared interests among people using graph analysis of global electronic mail traffic. Communications of the ACM , 36 , 78 X 89.
 Wasserman, S., &amp; Faust, K. (1994). Social network anal-ysis: Methods and applications . Cambridge, England: Cambridge University Press.
 Wasserman, S., &amp; Pattison, P. (1996). Logit models and logistic regression for social networks: I. an introduction
