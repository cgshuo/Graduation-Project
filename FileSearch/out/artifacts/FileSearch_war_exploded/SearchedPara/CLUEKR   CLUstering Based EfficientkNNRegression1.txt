 The problem of regression is to estimate the value of a dependent variable based on the values of one or more independent variables, e.g., predicting price in-crease based on demand or money supply based on inflation rate etc. Regression analysis helps to understand how the typical value of the dependent variable changes when any one of the independent variables is varied, while the other independent variables are held fixed. Regression algorithms can be used for pre-diction (including forecasting of time-seri es data), inference, hypothesis-testing and modeling of causal relationships.

In statistics, Regression is considered as collection of statistical function-fitting techniques. These techniques are classified according to the form of the function being fit to the data. Regression analysis has been studied extensively in statis-tics, but there have been only a few studies from the data mining perspective. Majority of this study resulted into algorithms that fall under the following broad categories -Linear Regression [12], Nearest Neighbor Algorithms [5], De-cision Trees [3], Support Vector Machines [4], Neural Networks [7] and Logistic Regression [8]. However most of these algorithms were originally developed for classification purpose, but have been later modified for regression.

In the recent past, a lot of research centered at nearest neighbor methodology has been performed. Instead of being computationally expensive k NN algorithm is very simple to understand, accurate, requires only a few parameters to be tuned and is robust with regard to the search space. Also k NN classifier can be updated at a very little cost as new training instances with known classes are presented. A strong point of k NN is that, for all data distributions, its probability of error is bounded above by twice the Bayes probability of error[10]. However one of the major drawbacks of k NN is that, it is a lazy learner i.e. it uses all the training data at runtime. However for majority of the datasets, we perform as accurate as k NN. In this paper, we propose a no vel, efficient and accurate, clustering based k NN regression algorithm CLUEKR, which instead of searching for nearest neighbors directly in the entire dataset, first find the cluster in which the query point has maximum likelihood of occurrence. We first hierarchically cluster the data in the pre-processing step, then a recursive search starting from root node of the hierarchy is performed. For current search node in the hierarchy, we select a cluster among its child, in which the query point has maximum likelihood of occurrence and then a recursive search is applied to it. Finally we find the k nearest neighbors of query points in the obtained cluster and return the weighted mean of their response variable as result.

The organization of rest of the paper is as follows. In section 2, we throw light on related, and recent, work in the lit erature. Section 3 deals with problem formulation. We explain the modified algorithm in Section 4. In Section 5, ex-perimental results are presented together with a thorough comparison with the state-of-the-art algorithms. Finally, in Section 6, conclusions are drawn. Traditional Statistical Approaches follow a methodology that requires the form of the curve to be specified in advance. This requires regression problems in each special application domain be studied and solved optimally for that domain. Another problem with these approaches is outlier (extreme cases) sensitivity. The most common statistical regression appr oach is linear regression which assumes the entire data to follow a linear relatio nship between the response and feature variables. But this assumption is unlikely to hold on variety of application.
Segmented or piecewise regression [11] is a method in regression analysis in which the independent variable is partitioned into intervals and a separate line segment is fit to each interval. It is is essentially a wedding of hierarchical clus-tering and standard regression theory. It can also be performed on multivariate data by partitioning the various independent variables. Segmented regression is useful when the independent variables, clustered into different groups, exhibit different relationships between the variables in these regions. The boundaries between the segments are breakpoints. Most of the Regression approaches in data mining falls under these categories -Nearest Neighbor, Regression trees, Neural Networks, and Support Vector Ma-chines. Regression trees are a variation o f decision trees in which the predicted outcome is a real number. Neural Networks and SVM techniques are quite com-plex and an in-depth analysis of results obtained is not possible.

One of the oldest, accurate and simplest method for pattern classification and regression is K -Nearest-Neighbor ( k NN) [5]. k NN algorithms have been identified as one of the top ten most influential data mining algorithms [14] for their ability of producing simple but powerful classifiers. It has been studied at length over the past few decades and is widely applied in many fields. Despite its simplicity, the k NN rule often yields competitive results. However one of the major drawbacks of k NN is that, it is a lazy learner i.e. it uses all the training data at the runtime.
 A recent work on prototype reduction, c alled Weighted Distance Nearest Neighbor (WDNN) [9] is based on retaining the informative instances and learn-ing their weights for classification. The algorithm assigns a non negative weight to each training instance tuple at the training phase and only the training in-stances with positive weight are retained (as the prototypes) in the test phase. However the algorithm is specifically designed for classification purpose and can-not be used for regression.
 In another recent work [13], a Paramet erless, Accurate, Generic, Efficient NN-Based Regression algorithm PAGER is proposed, which is based on the as-sumption that value of the dependent variable varies smoothly with the variation in values of independent variable. For each dimension in the search space, the authors construct a 1-dimensional predictor as a line passing through two closest neighbor of the query point. For each of the predictor obtained, they determine the mean error occurred if the predictor w as used in prediction of the k-nearest neighbors. A weight inversely proportional to the mean error is assigned to each predictor. Finally a weighted sum of the value output by individual predictors for the query instance is assigned to it.

Saket and others propose a k NN based regression algorithm BINER : BINary search based Efficient Regression [2], which instead of directly predicting the value of response variable recursively narrows down the range in which the re-sponse variable lies. In the pre-processi ng step training data is sorted based on the value of response variable and is hierarchically structured. At each level in the hierarchy, data is divided into three p arts, one containing elements from first to middle, other contains elements from middle to last and the third contains elements from middle of first portion to middle of second portion. The algorithm then finds the portion in which the query point has maximum likelihood to lie and finally k NN algorithm is applied to that portion.

Qi Yu and others in one of their recen t work [15] proposed a methodology named Optimally Pruned K-Nearest Neighbors (OP-k NNs) which builds a one hidden-layer feedforward neural network using K-Nearest Neighbors as kernels to perform regression. The approach performed better compared to state-of-the-art methods while remaining fast.
 In this section, we present the problem o f regression and notation used to model the dataset.

The problem of regression is to estimate the value of a dependent variable (known as response variable) based on the values of one or more independent variables (known as feature variables). We model the tuple as { X , y } where X is an ordered set of attribute values like { x 1 , x 2 , ..., x d } and y is the numeric variable to be predicted. Here x i is the value of the i th attribute and there are d attributes overall corresponding to a d -dimensional space.

Formally, the problem has the following inputs:  X  An ordered set of feature variables Q i.e. { q 1 , q 2 , ..., q d }  X  Asetof n tuples called the training dataset, D ,= { ( X 1 , y 1 ), ( X 2 , y 2 ), ..., The output is an estimated value of y for the given query Q . Mathematically, it can be represented as where parameters are the arguments which the function f () takes. These are generally set by user and are learned by trial and error method. We describe our proposed algorithm in th is section. Our algorithm proceeds in two steps :  X  It first find the cluster in the hierarchy, in which the query point has maxi- X  k NN is applied to points present in the cluster, and weighted mean of the k Size of the obtained cluster is less compared to the size of entire dataset, in this way our algorithm reduce the search space for K -Nearest Neighbor algorithm. Now we explain in detail about the clustering phase (pre-processing step) first and later throw light on the actual algorithm. 4.1 Pre-processing In the pre-processing step, data is hierarchically clustered and mean value for each of the cluster is calculated and stored. Each node in this hierarchy consist of clusters containing data point, which are recursively divided into three child clusters as we move down the hierarchy. We make use of the fact that similar instances have similar value of response variable (basic analogy on which k NN works), while selecting the cluster center. Each cluster node is sorted based on the value of response variable, then n/ 4and3 n/ 4 ranked instance are selected as the center for two of the child clusters. For the third cluster, mean of the other two cluster X  X  center is taken as center. All the points present in the cluster node are divided into child cluster, based on to which child cluster center the point is closest.

We aim to divide each cluster nodes is such a fashion, that each child node contains half the data present in the parent node. However at each level we have added an extra child cluster to make the division of points smooth, this cluster also contain points belonging to other cluster that lies at its boundary with other cluster. This will help to properly classify the query instance that lie at the boundary of clusters. Boundary points to third cluster are defined as, points whose distance ratio from other cluster center to third cluster center is between 0.9to1.0.

We recursively divide the cluster nodes , till we get a cluster node which contain less that 2  X  k points. The limiting size of 2  X  k was chosen in order to keep a margin for selection of k nearest neighbors. 4.2 Actual Algorithm Pseudo code for the actual algorithm is provided in Algo. 1. The recursive search starts from the root node and goes down the hierarchy. In order to find the cluster among the child nodes (for current node in the search) in which the query point has maximum likelihood of occurrence, distance of query point from mean value of all the child cluster of current node is calculated (lines 1-3). If the two closest distances comes out to be similar ( Confidance returns false ) then we can X  X  say with confidence, that which child cluster to pick and hence k NN algorithm is applied to the current cluster (line 5), else recursive search continues on the child cluster which is closest to the Query point (lines 7-8) i.e distance from mean of that cluster is least.

We say that two distances, d i and d j are similar if min ( d i /d j ,d j /d i )isgreater than 0.90. The value of 0.90 was selected by experimentations and it works well on most of the datasets as shown in the experimental section.
 Algorithm 1. CLUEKR : Pseudo code 4.3 Complexity Analysis We perform complexity analysis for both pre-processing step and actual algo-rithm in this section.  X  In the pre-processing step, hierarchy cons isting of cluster node is constructed,  X  At runtime our algorithm first performs a search for the cluster in the hier-5.1 Performance Model In this section, we demonstrate our experimental settings. The experiments were obtained on a wide variety of real life datasets obtained from UCI data repos-itory [1] and Weka Datasets [6].A short description of all the datasets used is provided in Table 1. We have compared our performance against the follow-ing approaches: K Nearest Neighbor, Isotonic , Linear Regression(Linear Reg.), Least Mean Square (LMS) algorithm, Radial Basis Function Network (RBF Net-work), Regression Tree (RepTree) and D ecision Stump(Dec Stump). Most of the algorithms are available as part of the Weka toolkit. All the results have been obtained using 10-fold cross validation technique.
 We have used two metrics for quantifying our results, namely, Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). MAE is mean of the abso-lute errors (actual output -predicted output). RMSE is square root of mean of squared errors. We have used euclidean distance matrix to calculate the distances in our algorithm. 5.2 Results and Discussion Table 2 compares the results obtained for our algorithm with other existing state of art approaches, top two results are highlighted in bold. As can be seen from the results, for majority of the datasets, we perform as accurate as k NN, however for some of the datasets, our algorithm also outperform k NN. Also our algorithm more than often outperforms other existing state-of-the art algorithms. However for concrete dataset in which the response variable is highly non-linear function of its attributes, we do not perform as good as for other datasets. Table 3 compares average size of cluster obtained by our algorithm with original dataset size.Ratio column in the table shows the percentage ratio of the cluster size obtained by our algorithm to the original dataset size. It is clear from the data, that our algorithm reduces the search space for k NN significantly, however the degree of reduction varies depending on the type of dataset. In this paper, we have proposed a novel clustering based k nearest neighbor regression algorithm which is efficient and accurate. Our work is based on reduc-ing the search space for nearest neighbors for any given point. We hierarchically cluster the data in pre-processing step and then search is performed to find the cluster in the hierarchy, in which the query point has the maximum likeli-hood of occurrence. We have also evaluated our approach against the existing state-of-the-art regression algorithms. As shown in the experimental section, our approaches reduces the search space for k NN significantly and is yet accurate.
