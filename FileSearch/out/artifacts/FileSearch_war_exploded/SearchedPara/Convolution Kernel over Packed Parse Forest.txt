 Parse tree and packed forest of parse trees are two widely used data structure s to represent the syntactic structure information of sentences in natural language processing (NLP ). The stru c-tured features embedded in a parse tree have been well explored together with different m a-chine learning algorithms and proven very useful in many NLP appli cations ( Collins and Duffy, 2002 ; Moschitti, 2004 ; Zhang et al., 2007 ) . A forest (Tomita, 1987) compactly encodes an e x-ponential number of parse trees. In this paper, we study how to effectively explore struc tured fe a-tures embedded in a forest using convolution kernel ( Haussler, 1999 ) .

A s we know, feature -based machine learning methods are less effective in model ing highly structured objects ( Vapnik, 1998 ) , such as parse tree or semantic graph in NLP . This is due to the fact that it is usually very hard to represent stru c-tured objects using vectors of reasonable dime n-sions without losin g too much information. For example, it is computational ly infeasible to en u-me rate all sub tree features (using subtree a fe a-ture) for a parse tree into a linear feature vector. Kernel -based machine learning method is a good way to overcome this problem . Kernel methods employ a kernel function , that must satisf y the properties of being symmetric and positive , to measure the similarity between two objects by com put ing implicitly the dot product of certain features of the input objects in high (or even i n-fin ite) dimensional feature spaces without en u-merating all the features (Vapnik, 1998) . 
Many learning algorithms, such as SVM (Vapnik, 1998) , the Perceptron learning alg o-rithm (Rosenblatt, 1962) and Voted Perceptron (Freund and Schapire, 1999), can work dire ctly with kernels by replacing the dot product with a particular kernel function. Th is nice property of kernel methods , that implicitly calculat es the dot product in a high -dimensional space over the original representations of objects , has made kernel me t hod s a n effective solution to modeling structured objects in NLP.

In the context of parse tree, c onvolution tree kernel ( Collins and Duffy, 2002 ) defines a fe a-ture space consisting of all subtree type s of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 200 2 ), semantic role labeling ( Moschitti, 2004 ; Zhang et al., 2007 ), relation extraction ( Zhang et al. , 200 6 ) , pronoun resolution (Yang et al., 2006) , q uestion classification ( Zhang and Lee , 2003 ) and machine translation (Zhang and Li , 2009) , where the tree kernel is used to co m-pute the similarity betwe en two NLP application instances that are usually represented by parse trees . However, in tho se studie s, the tree kernel only covers the features derived from single 1 -best parse tree. This may largely compromise the performance of tree kernel due to par s ing error s and data sparseness .

To address the above issues, this paper co n-struct s a forest -based convolution kernel to mine structured features directly from packed forest. A packet forest compactly encodes exponential number of n -best parse trees, and thus containing much more rich structured features than a single parse tree. This advantage enables the forest ke r-nel not only to be more robust against parsing errors , but also to be able to learn more reliable feature values and help to solve the data spars e-n ess issue that exist s in the traditional tree kernel. We evaluate the proposed kernel in two real NLP applications, r elation extraction and semantic role labeling. Experimental results on the benchmark data show that the forest kernel si g-nificantly outperforms the tree kernel.
 The rest of the paper is organized as follows. Section 2 reviews the convolution tree kernel while section 3 discusses the proposed forest kernel in details. Experimental results are r e-ported in section 4. Finally, we conclude the p a-per in section 5. Convolution kernel was proposed as a concept of kernels for discrete structures by Haussler (1999) and related but independently conceived ideas on string kernels first presented in (Watkins, 1999) . The framework defines the kernel function b e-tween input objects as the convolution of  X  X ub -kernels X , i.e. the kernels for the decompositions (parts) of the input objects. 
The parse tree kernel (Collins and Duffy, 200 2 ) is an instantiation of convolution kernel over syntactic parse trees. Given a parse tree, its fe a-tures defined by a tree kernel are all of its subtree types and the value of a given feature is the number of the occurrences of the subtree in the parse tree. Fig. 1 illustrates a parse tree with all of its 11 subtree features covered by the convol u-tion tree kernel. I n the tree kernel, a parse tree T is represented by a vector of integer counts of each subtree type ( i.e., subtree regardl ess of its ancestors , descendant s and span covered ):  X   X  ( # subtree type where # subtree type i ( T ) is the occurrence number of the i th subtree type in T . The tree kernel counts the number of common subtrees as the syntactic similarity between two parse trees. Since the number of subtrees is exponential with the tree size, it is computationally infeasible to directly use th e feature vector ( ) T  X  . To solve this co m-putational issue, Collins and Duffy ( 200 2 ) pr o-posed the following tree kernel to calculate the dot product between the above high dimensional vectors implicitly.
K T T T T where N 1 and N 2 a re the sets of nodes in trees T and T 2 , respectively, and ( ) that is 1 iff the subtree type i occurs with root at node n and zero otherwise , and number of the common subtree s rooted at n 1 and n , i.e., ( , ) n n  X  can be computed by the following recu r-sive rules: Rule 1 : if the productions (CFG rules) at Rule 2: else if both Rule 3: else , ( , ) (1 ( ( , ), ( , )))  X   X   X   X   X   X  , where the j th child of node n and  X  ( 0&lt;  X   X  1) is the d e-cay factor in order to make the kernel value less variable with respect to the sub tree sizes ( Collins and Duffy, 200 2 ) . T he recursive Rule 3 holds because given two nodes with the same children, one can construct common subtrees usin g these children and common sub trees of further offspring. The time complexity for computing this kernel is
As discussed in previous sec tion, when conv o-lution tree kernel is applied to NLP applications, its performance is vulnerable to the errors from the single parse tree and data sparseness. In this paper, we present a convolution kernel over packed forest to address the above issues by e x-ploring structured features embedded in a forest. In this section, we first illustrate the concept of packed forest and then give a detailed discussion on the covered feature space, fractional count , feature value and the forest kernel function itself. 3.1 Packed forest of parse trees Informally, a packed parse forest, or (packed) forest in short, is a compact representation of all the derivations (i.e. parse tree s ) for a given se n-tence under context -free grammar (Tomita, 1987; Billot and Lang, 1989; Klein and Manning, 2001) . It is the core data structure used in natural language parsing and other downstream NLP applications, such as syntax -based machine tran s lation ( Zhang et al., 2008; Zhang et al., 2009 a ) . In parsing, a senten ce corresponds to exponential num ber of parse trees with different tree probabilities , where a forest can compact all the parse trees by sharing thei r common subtrees in a bottom -up manner . Formally, a packed fo r-est  X  can be described as a triple: where  X  is the set of non -terminal nodes,  X  is the set of hyper -edges and  X  is a sentence represented as a n ordered word sequence . A h y-per -edge  X  is a group of edges in a parse tree which connects a father node and its all child nodes , repre senting a CFG rule . A non -terminal node in a forest is represented as a  X  X abel [start, end] X , where the  X  lab e l  X  is its syntax category and  X  X start, end] X  is the span of words it covers. As s hown in Fig. 2 , these two parse trees (  X  1 and  X  2 ) can be represented as a single forest by sharing their common subtrees ( such as NP[3,4] and PP[5,7] ) and merging common non -ter minal nodes covering the same span (such as VP[2,7] , where there are two hyper -edges attach to it ) .
Given the definition of fores t, we introduce the concepts of inside probability  X  . and ou t-side probability  X  ( . ) that are widely -used in parsing ( Baker , 1979 ; Lari and Young , 1990 ) and are also to be used in our kernel calculation .  X   X   X  ,  X  =  X  (  X   X   X  [  X  ] )  X   X   X  ,  X  =  X   X   X   X   X   X   X  (  X  ) = 1  X   X   X  ,  X  =  X   X   X   X   X   X   X   X   X  where  X  is a forest node,  X  [  X  ] is the  X   X   X  word of input sentence  X  ,  X  (  X   X   X  [  X  ] ) is the probability of the CFG rule  X   X   X  [  X  ] ,  X   X   X   X  ( . ) returns the root node of input structure, [  X   X  ,  X   X  ] is a sub -span of  X  ,  X  , being covered by  X   X  , and  X   X  is the PCFG probability of  X  . From these definitions, we can see that the inside probability is total probability of generating words  X   X  ,  X  fro m non -terminal node  X   X  ,  X  while the outside probability is the total probability of generating node  X   X  ,  X  and words outside  X  [  X  ,  X  ] from the root of forest. The inside probability can be ca l-culated using dynamic programming in a bottom -up fashion whi le the outside probability can be calculated using dynamic programming in a top -to -down way. 3.2 Convolution forest kernel In this subsection, we first define the feature space covered by forest kernel, and then define the forest kernel function. 3.2.1 Feature space , object space and fe a-The forest kernel counts the number of common subtrees as the syntactic similarity between two forests. Therefore, in the same way as tree kernel, its feature space is also defined as all the possible subtree types that a C FG grammar allows. In a forest kernel, forest  X  is represented by a vector of fractional counts of each subtree type (subtree regardless of its ancestors, descendant s and span covered): where # subtree type i ( F ) is the occurrence number of the i th subtree type ( subtree type i ) in forest F , i.e., a n -best parse tree lists with a huge n .
Although the feature spaces of the two kernels are the same, their object spaces (tree vs. forest) and feature values (integer counts vs. fractional counts) differ very much. A forest encodes exp o-nential number of parse trees, and thus contai n-ing expon ential times more subtrees than a single parse tree. This ensures forest kernel to learn more reliable feature values and is also able to help to address the data sparseness issues in a better way than tree kernel does. Forest kernel is also expected to yi eld more non -zero feature va l-ues than tree kernel. Furthermore, different parse tree in a forest represents different derivation and interpretation for a given sentence. Therefore, forest kernel should be more robust to parsing errors than tree kernel.

In tree kernel, one occurrence of a subtree co n tributes 1 to the value of its corresponding feature ( subtree type ), so the feature value is an integer count. However, the case turns out very complicated in forest kernel. In a forest, each of its parse trees, when enumerated, has its own probability. So one subtree extracted from diffe r-ent parse trees should have different fractional count with regard to the probabilities of different parse trees. Following the previous work (Cha r-niak and Johnson, 2005; Huang, 2008), we d e-fine the fractional count of the occurrence of a subtree in a parse tree  X   X  as  X   X   X   X   X   X   X   X  ,  X   X  =  X   X   X   X   X   X   X   X   X   X  . Then we define the fractional count of the occurrence of a subtree in a fore st f as  X   X   X   X   X   X   X   X  ,  X  =  X   X   X   X   X   X   X   X  |  X  ,  X  where  X   X   X   X   X   X   X   X   X   X  is a binary function that is 1 ii f the  X   X   X   X   X   X   X   X   X   X  and zero otherwise. O b-viously, it needs exponential time to compute the above fractional counts . However, due to the property of forest that compactly represents all the parse trees, the posterior probability of a subtree in a for est,  X   X   X   X   X   X   X   X  |  X  ,  X  , can be eas i-ly computed in an Inside -Outside fashion as the product of three parts: the outside probability of its root node, the probabilities of parse hyper -edges involved in the subtree , and the inside probabilities of its le af nodes ( Lari and Young , 1990 ; Mi and Huang, 2008 ). w here a nd where  X  . and  X  ( . ) denote the outside and i n-side probabilities. They can be easily obtained using the equations introduced at section 3.1. 
Given a subtree , we can easily compute its fra c tional count (i.e. its feature value) directly using eq. (3) and (4) wit h out the need of enum e-ra t ing each parse trees as shown at eq. (2) 1 . Noneth e less, it is still co m putatio n ally infeasible to directly use th e feature vector  X  (  X  ) (see eq. (1)) by explicitly enumerating all subtrees a l-though its fractional count is easily calculated. In the next subsection, we p resent the forest kernel that implicitly calculates the dot -product between two  X  (  X  ) s in a polynomial time. 3.2.2 Convolution forest kernel The forest kernel counts the fractional number s of common subtrees as the syntactic similarity between two forests. We d efine the forest kernel function  X   X   X  1 ,  X  2 in the following way.  X   X   X  1 ,  X  2 = &lt;  X   X  1 ,  X   X  2 &gt; (5) = #  X   X   X   X   X   X   X   X   X   X   X   X  (  X  1 ) . #  X   X   X   X   X   X   X   X   X   X   X  =  X   X   X   X   X   X   X   X   X   X  1 ,  X   X   X   X   X   X   X  2 where  X  =  X   X   X   X   X   X   X   X   X   X  1 ,  X   X   X   X   X   X   X  2
We next show that  X   X   X  1 ,  X  2 can be computed recursively in a polynomial time as illustrated at Algorithm 1. To facilitate discussion, we temp o-rarily ignore all fra ctional counts in Algorithm 1. Indeed, Algorithm 1 can be viewed as a natural extension of convolution kernel from over tree to over forest. In forest 2 , a node can root multiple hyper -edges and each hyper -edge is independent to each other. Therefore, Algorithm 1 iterates each hyper -edge pairs with roots at  X  1 and  X  2 (line 3 -4), and sums over (eq. (7) at line 9) each recursively -accumulated sub -kernel s cores of subtree pairs extended from the hyper -edge pair  X  1 ,  X  2 (eq. (6) at line 8). Eq. (7) holds because the hyper -edges attached to the same node are independent to each other. Eq. (6) is very similar to the Rule 3 of tree kernel (see section 2) except its inputs are hyper -edges and its further expa n-sion is based on forest nodes. Similar to tree ke r-nel (Collins and Duffy, 2002), eq. (6) holds b e-cause a common subtree by extending from (  X  1 ,  X  2 ) can be formed by taking the hyper -edge (  X  1 ,  X  2 ) , together with a choice at each of their leaf nodes of simply taking the non -terminal at the leaf node, or any one of the common subtrees with root at the leaf node. Thus there are 1 +  X   X   X   X   X   X   X  1 ,  X  ,  X   X   X   X   X  2 ,  X  poss i ble choices at the j th leaf node. In total, the re are  X  ing from (  X  1 ,  X  2 ) and  X   X   X  1 ,  X  2 (eq. (7)) co m-mon subtrees with root at  X  1 ,  X  2 . 
Obviously  X   X   X  1 ,  X  2 calculated by Algorithm 1 is a proper convolution kernel since it simply counts the number of common subtrees under the root  X  1 ,  X  2 . Therefore,  X   X   X  1 ,  X  2 defined at eq. (5) and calculated through  X   X   X  1 ,  X  2 is also a proper convolution kernel. From eq. (5) and A l-gorithm 1, we can see that each hyper -edge pair (  X  1 ,  X  2 ) is only visited at most one time i n co m-puting the forest kernel. Thus the time complex i-ty for computing  X   X   X  1 ,  X  2 is  X  ( |  X  1 |  X  |  X  where  X  1 and  X  2 are the set of hyper -edges in forests  X  1 and  X  2 , respectively. Given a forest and the best parse trees, the number of hyper -edges is only several times (normally &lt;= 3 after pruning ) than that of tree nodes in the parse tree 3 . Same as tree kernel, forest kernel is running more efficiently in practice since only two nodes with the same label needs to be further processed (line 2 of Algorithm 1).

Now let us see how to integrate fractional counts into forest kernel. According to Alg o-rithm 1 (eq. (7)), we have (  X  1 /  X  2 are attached to  X  /  X  2 , respectively)
Recall eq. (4), a fractional count consists of o utside, inside and subtree probabilities. It is more straightforward to incorporate the outside and subtree probabilities since all the subtrees with roots at  X  1 ,  X  2 share the same outside probability and each hyper -edge pair is only v i-sited one time. Thu s we can integrate the two probabilities into  X   X   X  1 ,  X  2 as follows.
 w here , following tree kernel , a decay facto r  X  ( 0 &lt;  X   X  1 ) is also introduced in order to make the kernel value less variable with respect to the sub tree sizes ( Collins and Duffy, 200 2 ) . It fun c-tions like multiplying each feature value by  X  in  X   X   X   X   X   X   X   X  .
 Algorithm 1.
 Input: Notation: Output :  X   X   X  1 ,  X  2 1.  X   X   X  1 ,  X  2 = 0 2. if  X  1 .  X   X   X   X   X   X   X  2 .  X   X   X   X   X  exit 3. for each hyper -edge  X  1 attached to  X  1 do 4. for each hyper -edge  X  2 attached to  X  2 do 5. if  X   X   X   X  1 ,  X  2 = = 0 do 6. goto line 3 7. else do 9.  X   X   X  1 ,  X  2 + =  X   X  X   X  1 ,  X  2 (7) 10. end if 11. end for 12. end for
The inside pro bability is only involved when a node does not need to be further expanded. The integer 1 at eq. (6) represents such case. So the inside probability is integrated into eq. (6) by replacing the integer 1 as follows.
 where in the last expression the two outside probabilities  X   X   X   X   X   X  1 ,  X  and  X   X   X   X   X   X  2 ,  X  are removed . This is because  X   X   X   X   X  1 ,  X  and  X   X   X   X   X  2 ,  X  are not root s of the subtree s of being explored (only outside probabilities of the root of a subtree should be counted in its fractional count ), and  X   X   X   X   X   X   X  1 ,  X  ,  X   X   X   X   X  2 ,  X  already contains the two outside probabilities of  X   X   X   X   X  1 ,  X  and  X   X   X   X   X  2 ,  X  .

Referring to eq. (3), each fractional count needs to be normalized by  X   X  (  X   X   X   X   X  ) . Since  X   X  (  X   X   X   X   X  ) is independent to each individual fractional count , we do the normalization outside the recursive function  X   X  X   X  1 ,  X  2 . Then we can re -f ormulize eq. (5) as
Finally, since the size of input forests is not constant, the forest kernel value is normalized using the following equation.
From the above discussion, we can see that the proposed forest kernel is defined together by eqs. (11), (10), (9) and (8). Thanks to the compact representation of trees in forest and the recursive n ature of the kernel function, the introduction of fractional counts and normalization do not change the convolution property and the time complexity of the forest kernel. Therefore, the forest kernel  X   X   X  1 ,  X  2 is still a proper convol u-tion kernel with qu adratic time complexity. 3.3 Comparison with previous work To the best of our knowledge, this is the first work to address convolution kernel over packed parse forest.

Convolution tree kernel is a special case of the proposed forest kernel. From feature explor ation viewpoint, although theoretically they explore the same subtree feature spaces (defined recu r-sively by CFG parsing rules), their feature values are different. Forest encodes exponential number of trees. So the number of subtree instances e x-tracted fr om a forest is exponential number of times greater than that from its corresponding parse tree. The significant difference of the amount of subtree instances makes the param e-ters learned from forests more reliable and also can help to address the data spar seness issue. To some degree, forest kernel can be viewed as a tree kernel with very powerful back -off mecha n-ism. In addition, forest kernel is much more r o-bust against parsing errors than tree kernel.
 Aiolli et al. (2006; 2007) propose using Direct Acycli c Graphs (DAG) as a compact represent a-tion of tree kernel -based model s . This can largely reduce the computational burden and storage r e-quirements by sharing the common structures and feature vectors in the kernel -based model . There are a few other previous works done by generalizing convolution tree kernel s (Kashima and Koyanagi, 2003; Moschitti, 2006; Zhang et al., 2007 ). However, all of these works limit themselves to single tree structure from modeling viewpoint in nature .

From a broad viewpoint, as suggested by one reviewer of the paper , we can consider the forest kernel as an alternative solution proposed for the general problem of noisy inference pipelines (eg. speech translation by composition of FSTs, m a-chine translatio n by translating over 'lattices' of segmentations ( Dyer et al ., 2008 ) or using parse tree info for downstream applications in our ca s-es) . Following this line, Bunescu ( 2008 ) and Finkel et al. ( 2006) are two typical related works done in reducing cascadin g noisy . However, our works are not overlapped with each other as there are two totally different solutions for the same general problem. In addition, the main m o-tivation of this paper is also different from theirs. Forest kernel has a broad ap plication potential in NLP. In this section, we verify the effective ness of the forest kernel on two NLP applications, semantic role labeling (SRL) (Gildea, 2002) and relation extraction (RE) (ACE, 2002 -2006).

In our experiments, SVM ( Vapnik , 1998) is selected as our classifier and the one vs. others strategy is adopted to select the one with the largest margin as the final answer. In our impl e-mentation, we use the binary SVMLight (Jo a-chims, 1998) and borrow the framework of the Tree Kernel T ools ( Moschitti, 2004) to integrate our forest kernel into the SVMLight. We modif y Charniak parser (Charniak , 200 1 ) to output a packed forest. Following previous forest -based studies (Charniak and Johnson, 2005), we use the marginal probabilities of hyper -edges (i.e., the Viterbi -style inside -outside probabilities and set the pruning threshold as 8 ) for forest pruning. 4.1 Semantic r ole labeling G iven a sentence and each predicate (either a target verb or a noun) , SRL recognizes and maps all the constituents in the sentence into their co r-responding semantic arguments (roles, e.g., A0 for Agent , A1 for Patient ... ) of the predicate or non -argument. We u se the CoNLL -2005 shared task on Semantic Role Labeling (Car reras and M a rquez, 2005) for the evaluation of our forest kernel method . To speed up the evaluation process , the same as Che et al. (2008), w e use a subset of the entire training corpus ( WSJ sections 02 -05 of t he entire sec tions 02 -21 ) for training , section 24 for development an d section 23 for test , where t here are 35 roles including 7 Core (A0  X  A5, AA), 14 Adjunct (AM -) and 14 Refe r-ence (R -) arguments .

The state -of -the -art SRL methods (Car reras and M a rquez, 2005) use constituents as the lab e-ling units to form the labeled argum ents. Due to the errors from automatic pars ing , it is imposs i-ble for all argument s to fi nd their matching co n-stituent s in the single 1 -best parse trees. Statistics on the training data shows that 9 . 7 8% of arg u-ments have no matching constituents using the Charniak parser ( Charniak , 200 1) , and the nu m-ber increases to 11. 76 % when using the Collins parser ( Collins , 1999 ). In our method, we break the limitation of 1 -best parse tree and regard each span rooted by a single forest node (i.e., a sub -forest with one or more roots ) as a candidate a r-g u ment. This larg e ly r e duces the unmatched a r-g u ments from 9 . 7 8% to 1. 31 % after forest pru n-ing. Ho w ever, it also r e sults in a very large amount of a r gument ca n d i dates that is 5 . 6 times as many as that from 1 -best tree. Fortunately, after the p re -processing stage of argument pru n-ing ( Xue and Palmer , 2004 ) 4 , although the amount of u n matched a r gument increases a little bit to 3 . 1 %, its genera t ed total candidate amount decreases substantially to only 1. 31 times of that from 1 -best parse tree. This clearly shows the advanta g es of the forest -based method over tree -based in SRL.

The best -reported tree kernel method for SRL  X  1 ) , proposed by Che et al. (2006) 5 , is adopted as our baseline kernel. We implemente d the  X   X   X   X   X   X   X  in tree case (  X   X   X   X   X   X   X   X   X  , using tree kernel to compute  X   X   X   X   X  and  X   X   X  ) and in forest case (  X  and  X   X   X  ).  X   X 
Table 1 shows that the forest kernel significan t-ly outperforms (  X  2 test with p=0.01) the tree ke r-nel with an absolute improvement of 2 . 3 2 (73 .76 -71. 4 2 ) percentage in F -Score, representing a rel a-tive error rate reduction of 8 . 19 % ( 2 . 3 2 /(100 -71.64)) . This convincingly demonstrates the a d-vantage of the forest kernel over the tree kernel. It suggests that the structured features represented by subtree are ver y useful to SRL. The perfo r-mance improvement is mainly due to the fact that forest encodes much more such structured features and the forest kernel is able to more effectively capture such structured features than the tree ke r-nel. Besides F -Score, both pre cision and recall also show significantly improvement (  X  2 test with p=0.01). The reason for recall improvement is mainly due to the lower rate of unmatched arg u-ment ( 3 . 1 % only) with o nly a little bit overhead (1. 31 times) (see the previous discussion in t his section). The precision improvement is mainly attributed to fact that we use sub -forest to represent argument instances, rather than sub -tree used in tree kernel, where the sub -tree is o n-ly one tree encoded in the sub -forest .
 4.2 Relation extraction As a subtask of information extraction, relation extraction is to extract various semantic relations between entity pairs from text. For example, the sentence  X  X ill Gates is chairman and chief sof t-ware arc hitect of Microsoft Corporation  X  co n-veys the semant ic relation  X  X MPLO Y-MENT.exec utive  X  between the entities  X  X ill Gates X  ( person ) and  X  X icrosoft Corporation X  ( company ). We adopt the method reported in Zhang et al. (2006) as our baseline method as it reports the state -of -the -art performance using tree kernel -based composite kernel method for RE. We replace their tree kernels with our forest kernels and use the same experimental settings as theirs. We carry out the same five -fold cross v a-lidation experiment on the same subset of ACE 2004 data (LDC2005T09, ACE 2002 -2004) as that in Zhang et al. (2006). The data contain 348 documents and 4400 relation instances. 
In SRL, constituents are used as the labeling units to form the labeled arguments. However, previous work (Zhang et al., 2006) shows that if we use comp lete constituent (MCT) as done in SRL to represent relation instance, there is a large performance drop compared with using the path -enclosed tree (PT) 6 . By simulating PT, we use the minimal fragment of a forest covering the two entities and their internal words to represent a relation instance by only parsing the span co v-ering the two entities and their internal words .
Table 2 compares the performance of the fo r-est kernel and the tree kernel on relation extra c-tion. We can see that t he forest kernel significan t-ly outperforms (  X  2 test with p=0.05) the tree ke r-nel by 1.1 point of F -score. This further verifies the effectiveness of the forest kernel method for modeling NLP structured data. In summary , we further observe the high precision improvement that is consistent with the SRL experiments. Ho w-ever, the recall improvement is not as significant as observed in SRL. This is because unlike SRL, RE has no un -matching issues in generating rel a-tion instances. Moreover, we find that the perfo r-mance improvement in RE is not as good as that in SRL. Althou gh we know that performance is task -dependent, one of the possible reasons is that SRL tends to be long -distance grammatical structure -related while RE is local and semantic -related as observed from the two experimental benchmark data. Many NLP applications have benefited from the success of con volution kernel over parse tree. Since a packed parse forest contains much richer structured features than a parse tree, we are m o-tivated to develop a technology to measure the syntactic s imilarity between two forests.

To achieve this goal, in this paper, we design a convolution kernel over packed forest by gener a-lizing the tree kernel. We analyze the object space of the forest kernel, the fractional count for feature value computing and de sign a dynamic programming algorithm to realize the forest ke r-nel with quadratic time complexity. Compared with the tree kernel, the forest kernel is more r o-bust against parsing errors and data sparseness issues. Among the broad potential NLP applic a-tions, the problems in SRL and RE provide two pointed scenarios to verify our forest kernel. E x-perimental results demonstrate the effectiveness of the proposed kernel in structured NLP data modeling and the advantages over tree kernel. 
In the future, we w ould l ike to verify the forest kernel in more NLP applications. In addition, as suggested by one reviewer, we m ay consider re s-caling the probabilities ( exponentiat ing them by a constant value) that are use d to compute the fractional counts. We can sharpen or fla tten the distribution s . This basically says "how seriously do we want to take the very best derivation" compared to the rest. However, the challenge is that we compute the fractional counts together with the forest kernel recursively by using the Inside -Ou tside probabilities. We cannot differe n-tiate the individual parse tree  X  X  contribution to a fractional count on the fly. One possible solution is to do the probability rescaling off -line before kernel calculation. This would be a very interes t-ing research topic of our future work.
