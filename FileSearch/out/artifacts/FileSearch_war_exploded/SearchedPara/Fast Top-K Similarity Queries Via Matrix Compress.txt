 In this paper, we propose a novel method to efficiently compute the top-K most similar items given a query item, where similar-ity is defined by the set of items that have the highest vector inner products with the query. The task is related to the classical k-Nearest-Neighbor problem, and is widely applicable in a number of domains such as information retrieval, online advertising and collaborative filtering. Our method assumes an in-memory representation of the dataset and is designed to scale to query lengths of 100,000s of terms. Our algorithm uses a generalized H X lder X  X  inequality to upper bound the inner product with the norms of the constituent vectors. We also propose a novel compression scheme that computes bounds for groups of candidate items, thereby speeding up compu-tation and minimizing memory requirements per query. We conduct extensive experiments on the publicly available Wikipedia dataset, and demonstrate that, with a memory overhead of 21%, our method can provide 1-3 orders of magnitude improvement in query run-time compared to naive methods and state of the art competing methods. Our median top-10 word query time is 25  X  s on 7.5 million words and 2.3 million documents.
 E.1 [ Data ]: Data Structures; H.3.3 [ Information Systems ]: Infor-mation Search and Retrieval X  Search Process Top K, Inner Product, Nearest Neighbor
The task of computing the K most similar objects given a query object where similarity is described as distances in a metric space is well known as the k-nearest-neighbor (k-NN) procedure [4]. This procedure is applicable to a wide variety of regression and classi-fication tasks. However, a naive implementation suffers from high computation demands, requiring N distance evaluations for each  X  This work was done while interning at Microsoft Research. test data point. To accelerate this procedure, various space partition-ing methods such as KD-Trees [3] (among many others) have been proposed, which provide fast exact K-nearest neighbor retrieval.
However, when similarity between objects are described by inner products, fast exact top-K retrieval is a much less understood task. Such tasks are common in collaborative filtering (finding similar movies or similar users), similarity queries (search for similar im-ages given a query image) and on-line advertising (display ads that are most textually similar with the current page). Effective solutions here can also generalize to several tasks where kernels are used to define similarity between objects. For instance: graph kernels can be used for similarity search in a molecule database or a gene regulatory network database [13, 12]. Sub-string kernels can be used for document analysis [2] or biological sequence analysis. In these cases, the hash kernel method described in [10] can be used to con-struct an explicit representation of the feature space thus mapping the task into the top-K inner product regime.

The top-K inner product problem also shares similarities with the top-K query retrieval problem explored heavily by the Informa-tion Retrieval (IR) community. Top-K query retrieval techniques include various TAAT (Term-At-A-Time) or DAAT (Document-At-A-Time) procedures [7, 5, 9, 11, 6], which rely on skip-ahead heuristics to quickly iterate through the index and inverted index of the document-word matrix, maintaining an upper bound on each can-didate. However, the top-K inner product task we are exploring in this paper differentiates itself from from the query retrieval problem since the query object is itself a datapoint, and thus can have an un-bounded number of terms . The assumption that the query length is small is not justifiable and the emphasis is thus on scalability to arbitrarily long query lengths without loss of performance.
In this paper we will use text documents as the running example: letting a collection of text documents be represented as a matrix, where each row represents a document and each column a word. The ( i, j ) -th entry is the count of the number of times the j -th word appears in the i -th document.

Our running example in this paper focuses on finding the exact top-K largest inner products given a query word. Such a system for instance, could be used to find  X  X elated words X  in a user search. The rationale for using words as query objects instead of documents, is to provide a much wider range of query lengths, allowing us to benchmark the system on both extremely short and extremely long queries. We define the similarity function between words w w 2 as the inner product of the two column vectors w 1 and w Letting D denote the document-word matrix, the task of query-ing the K most similar words to a query word v is equivalent to computing the K largest elements of the row vector v T D . Figure 1: A document collection is represented as a sparse ma-trix containing arbitrary non-negative values (counts / tf-idf scores / etc). The matrix is then compressed into a smaller ma-trix were each element in the smaller matrix represents a block in the original matrix.

We focus strictly on the in-memory setting, exploring the per-formance characteristics of different matrix representation formats. Our method augments the matrix with an additional data structure containing compressed summaries of the document-word matrix. We demonstrate that our technique scales well to extremely long query lengths, and provides 1-3 orders of magnitude performance gains over competing methods for all query lengths. Our method incurs a memory overhead that depends on the statistics of the un-derlying dataset and a tunable algorithm parameter, allowing the user to balance between query speed and memory utilization. For the query time results previewed above, we observe a 21% memory overhead over a set of 1 million Wikipedia articles. On the full dataset of 2.3M articles and 7.5M words, our median top-10 query time for a randomly selected word query is 25  X  s. 1
Given a query word v , our goal is to quickly compute a list of its top-K most similar words, i.e., the K words with the largest inner product (co-occurrence count) with v . (Note that the popular cosine similarity measure is a variant of inner product similarity and can thus be computed using the same algorithm.) A naive algorithm for doing so may proceed thus: first compute an upper bound of co-occurrence for every word with every other word in D . Then, at run-time, sort the words by upper bounds, refine the upper bound for the top candidate, re-insert it back into a max-heap, and repeat. The algorithm terminates when the first K elements in the heap are exact co-occurrence counts as opposed to upper bounds. A template for this algorithm is shown in Alg. 1.

Making this algorithm efficient requires three key conditions. 1. Ideally, the upper bound should be tight, i.e., it should be as 2. The upper bound should be significantly faster to compute 3. The maintenance of a large W sized heap in Alg. 1 is ineffi-4. The procedure must scale well with query length.
 Our solution, which satisfies all key conditions of efficiency, in-volves constructing a  X  X ompressed X  auxiliary matrix containing upper bounds of blocks of the document-word matrix. The upper bounds are established via an extension of H X lder X  X  inequality to the case of generalized matrix norms. We call the algorithm HComp, for H X lder Compression. A long version of this paper is published as a technical report[8]. Algorithm 1: T op-K Algorithm Template
H X lder X  X  inequality upper bounds the inner product of two vectors by the product of their norms. Specifically, for any vectors a and b and scalars p and q such that 1  X  p, q  X   X  and 1 /p + 1 /q = 1 , H X lder X  X  inequality states that where || x || p = ( P i | x i | p ) 1 /p is known as the p -norm of x .
The solution we propose in this paper is to compress the document-word matrix into a smaller matrix, each element of which is an upper bound for an entire block of the original matrix (Fig. 1).
Let A  X  R m,n + be a matrix of m rows and n columns whose elements are non-negative real numbers. Let a ij denote the ( i, j ) -th element of A (i.e., the element at the i -th row and the j -th col-umn). We define the u -v mixed norm of the matrix A as a function L  X , X  ( A ) . Where  X   X  1 ,  X   X  1 , Essentially, L c  X , X  ( A ) computes the  X  -norm of each column, then computes the  X  -norm of the result footnoteEven though these defi-nitions require  X ,  X  &lt;  X  , analogous forms for the  X  -norm (max-norm) can be defined..
 Then, the fundamental result:
T HEOREM 2.1. Given a query vector v and a matrix A . Then for any column vector a j in A and for any p, q satisfying the condi-tions of H X lder X  X  inequality: The proof for this theorem as well as the choice of L c q,  X   X  =  X  ) can be found in [8].

Essentially Theorem 2.1 allows us to compress the entire ma-trix A to yield an upper bound. Clearly, the quality of the bound degrades when when the matrix is larger, resulting in a trade-off between bound quality and computation efficiency. Furthermore, the procedure permits multiple levels of compression, allowing the compressed matrix can be further compressed into an even smaller matrix while retaining the upper bounding property. The following corollary summarizes the results of this section.

C OROLLARY 2.2. Given query vector v and document-word matrix D , let  X  v denote the p -compression of the vector v , and Algorithm 2: CompressMatrix(D, lvls, q, r, s): Hierarchical Compression of matrix A
Algorithm 3: CompressVector(w, lvls, q, r): Hierarchical Com-pression of query column vector w denote the q -compression of the matrix D . Let d j denote the j -th column of D , and  X  D ( j ) the corresponding compressed blocks. Then Moreover,  X  v and  X  D can be further compressed to yield looser but more computationally efficient upper bounds.
We now provide a complete definition of the complete HComp algorithm. Firstly, a pair of p, q that satisfies H X lder X  X  inequal-ity are chosen. Next, a row compression factor r and a column compression factor s are chosen. The optimal choice of r and s depends heavily on the both the dataset and the properties of the underlying matrix representation. Then, given an input matrix D , the CompressMatrix function in Alg. 2 is used to generate the compression hierarchy.

At query time, the query word/vector v is similarly hierarchically compressed using the CompressVector function in Alg. 3. Then the top-K algorithm in Alg. 4 is called. The algorithm in Alg. 4 begins by using the coarsest compression of the original matrix to provide upper bounds on ranges of words which are then stored in a max-heap. Elements are then popped from the heap, and if the element is a range, the range is refined by expanding it to the next finer compression level, splitting the range into a series of smaller ranges. If the element is a single word, it must be larger than all other upper bounds and therefore belong in the top K set.
We observe that to implement the HComp algorithm requires only a matrix representation with the ability to compute v T D D ( c ) is a contiguous range of columns in D . In the next section, we explore two different in-memory representations with this property.
We implemented the HComp algorithm above under two different matrix representation formats, the Jagged Column Store as well as the Jagged Row and Column Store . The implementation is written in C++, using standard STL containers. Both documents Algorithm 4: HComp(): HComp Top-K Algorithm and words are identified by sequential 32-bit integers. Values in the matrix are also represented as 32-bit integers. libboost  X  X  unordered_map is used as a hash table when needed.
 We make the choice of ( p = 1 , q =  X  ) for the HComp algorithm. This choice is partly motivated by the result in [8] that suggests that the (1 ,  X  ) pair is optimal for binary data.
Jagged Column Store: In the Jagged Column Array representa-tion, the document-word matrix is represented as a vector of word vectors, where each word vector is a sorted vector over documents containing the word. We will use the acronym CS to identify algo-rithms implemented with the Jagged Column Store.

Jagged Row And Column Store: In this representation, the document-word matrix is represented in both column format and row format. The row representation essentially acts as an inverted index. We will use the acronym R&amp;CS to identify algorithms implemented with the Jagged Row and Column Store.
We assume that the data is organized so that ingress into memory can be performed one document at a time. Both matrix representa-tions are computationally efficient and permit linear time insertion of documents. The hierarchical compression of the matrix is performed as a final post-processing pass. We implement the following procedures:
Naive Top-K Algorithm: As a baseline for both matrix represen-tations, we first implement the naive approach to computing top-k word co-occurrence. Given the document-word matrix D and a query vector v , the naive approach simply computes the entire ma-trix vector product v T D , returning the top-K entries in the resultant vector. To computing v T D in the CS representation we simply eval-Unique Documents Per Word 35.35 1,011.02 1 uate inner products of v against every column, while In the R&amp;CS representation we use the reverse index to iterate through documents containing the query word accumulating a weighted sum. mWand: We also implement the Wand algorithm described in Broder et al. [5] with the in-memory mWand optimization in Fon-toura et al. [7]. While not designed for long query lengths, it pro-vides a reliable evaluation baseline. The Wand Iterator uses the inverted index and uses a clever upper bounding strategy to skip and ignore some of entries in the inverted index. For the purposes of this evaluation, we configure Wand to provide exact top-K.

HComp Top-K Algorithm: As noted in Sec. 2.2, the HComp algorithm only requires the matrix representation to provide the ability compute inner products against contiguous ranges of columns in D . While this is trivially provided by the CS representation, this restriction operation in R&amp;CS requires an additional binary search to locate the start of the range. To evaluate HComp, we use a set of 1 million randomly selected Wikipedia articles with common stop words removed. The statistics of our 1M wikipedia dataset are shown in Table 1.

Compared to the dataset evaluated in Fontoura et al. [7], which has 454M non-zero elements, the Wikipedia dataset is roughly half the size. However, since we are solving the transposed problem, the problem statistics are significantly different. In particular, we have a smaller average number of unique terms per candidate (35.35 vs 130.33), but with a much larger standard deviation (1011.02 vs 103.86); our task has much larger variation in candidate sizes.
We generate a query set by extracting 10,000 random columns from the matrix. The statistics of the query set are shown in Table 1. Most notably, we observe that we have queries lengths ranging from 1 term to over 200,000 terms. Our evaluation task is to return the exact top-10 other words co-occuring with the words in the query set. We compare against naive strategies for both CS and R&amp;CS matrix representations as well as the mWand procedure.
Since the effect of varying the compression block size and shape can be dataset dependent, we first evaluate the optimal choice of compression block shape. We do so by computing the median runtime of random queries on a 5% subsample of the documents varying the size and shape of the compression block (using only one compression level).

In Fig. 2(a) and Fig. 2(b), we plot the median runtime of varying both compression block size and compression block shape, respec-tively for CS and R&amp;CS representations. We observe from Fig. 2(a) that the choice of compression factor does not impact performance under the CS representation). However, the R&amp;CS representation is significantly faster with smaller compression block sizes and wider block shapes (more words than documents). Smaller block sizes improve performance by improving the quality of the upper bound, while wider block sizes decreases the number of binary searches re-quired by maximizing the number of expanded words in each search ( each restriction operation incurs a binary search. See Sec. 3.3).
For the remaining experiments, we pick a compression ratio of 1000:1, mapping 1 doc  X  1000 words into a single entry X  X he fastest parameter setting for both CS and R&amp;CS representations.
For both matrix representation strategies, we evaluate the effec-tiveness of using hierarchical compression. Fig. 2(c) plots the query performance as the number of compression levels are increased when the CS matrix representation is used. Fig. 2(d) provides the same but for the R&amp;CS representation. We observe that, for the CS representation, adding the first level of compression provides significant performance gains, but adding the second level provides almost no performance gain for short queries, and only a small gain for large queries. On the other hand, the R&amp;CS representation demonstrates consistent uniform performance gains across all query lengths as the compression hierarchy is increased.

Finally, in Fig. 3(a) we plot the combined performance of all ma-trix representations for all algorithms: Naive, HComp and mWand. We summarize our observations here: mWand vs Naive CS: We observe that the mWand algorithm is faster than Naive CS for short queries, but when the query length exceeds 1000, the overhead of the mWand algorithm starts to be-come evident and ends up slower than the Naive algorithm. For short queries (less than 100), mWand does provide about a factor of 4 speedup over the naive CS implementation which is inline with the performance gains observed in Fontoura et al. [7].

HComp CS vs Naive CS: The HComp CS algorithm provides a small performance gain on small queries (about the same as the mWand algorithm), but as query length increases, the performance gain widens to 2-3 orders of magnitude for queries with length exceeding 10,000.

HComp R&amp;CS vs Naive R&amp;CS: HComp provides a consistent 1 order of magnitude performance gain over all query lengths above the Naive method using the R&amp;CS representation. This gain is consistent on the transposed problem (see [8]).

In summary, HComp provides between 1-3 orders of magnitude performance gain depending on query length and matrix representa-tion. The performance figures in Fig. 3(a) demonstrates an incredi-ble 4 orders of magnitude of performance differences between the fastest and the slowest algorithms, with HComp R&amp;CS consistently being the fastest algorithm.

Finally, to demonstrate the performance of the HComp algorithm, we evaluate on 10,000 randomly generated word queries on the full Wikipedia dataset comprising of 2,312,594 documents and 7,574,761 unique words. The median top-10 query time is 25  X  s , and 95% of all queries complete within 200  X  s .
A concern with large datasets is the amount of memory required to complete a given query. For instance, implementations based directly off of the algorithm template in Alg. 1 will require O ( W ) memory per query which can be extremely large. The HComp algorithm compacts the heap size by letting each heap element represent the equivalent of a range of words, expanding the range only when necessary. This strategy is extremely effective in practice:
