 Books only represented by brief metadata ( book records ) are particularly hard to retrieve. One way of improving their retrieval is by extracting retrieval enhancing features from them. This work focusses on scientific (physics) book records. We ask if their technical terminology can be used as a retrieval enhancing feature. A study of 18,443 book records shows a strong correlation between their technical terminology and their likelihood of relevance. Using this finding for retrieval yields &gt; +5% precision and recall gains. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Book Records, Technical Terminology
Information retrieval (IR) systems often can rely on hav-ing full texts available for processing. However, there are cases when full text is not available, e.g. in commercial on-line bookstores or traditional libraries where material may only be available in print, or where using optical character recognition is difficult. Access to these materials is primarily realised through the supplier or the library catalogue, where documents are represented by short book records of meta-data information, e.g. author, title, etc. The problem is that book records provide very little information, and hence they are very hard to retrieve. As a consequence, the acces-sibility of potentially relevant books is restricted for users. This work focuses on such books from the physics domain.
We ask whether we can increase the retrievability of physics book records by focussing on the special language in this scientific domain, as used by searchers and authors. Specif-ically, we model separately the technical/non-technical ter-minology of physics book records, motivated by the intu-ition that technical terminology may make a good retrieval enhancing feature. Our intuition that this modelling may benefit book record retrieval is experimentally confirmed: we find a strong correlation between the technical terminol-ogy contained in book records and their likelihood of rele-vance. Applying this to the retrieval of book records yields notable improvements in retrieval precision and recall.
Preprocessing. We use a collection of 18,443 physics book records with 53 queries and relevance assessments (qrels) from the iSearch dataset 1 . These book records contain ba-sic Machine-Readable Cataloging information, e.g. title, key phrases. To identify technical terms, we part-of-speech (POS) tag the collection (including queries) with the Tree-Tagger 2 . We extract all terms tagged as nouns, verbs, adjec-tives and participles, which are the most salient POS classes, hence the most likely to be technical terms. This results in a list of 12,548 terms, which we submit to Amazon Mechan-ical Turk (AMT) as isolated tokens (without any context) for classification into technical/non-technical terms. Using 3 AMT users per annotation (  X  95% approval rate, paid ap-proximately $0.33 per hour), 34.7% of all terms were anno-tated as technical, and 65.3% as non-technical, with strong inter-annotator agreement (Fleiss X   X   X  0 . 8).

Technical terminology density analysis. We count the number of technical terms in each book record (referred to as document henceforth) normalised by their length -this gives the document X  X  terminological density .Wesortalldoc-uments by their terminological density, and we divide them into 34 bins: 33 equal-sized bins of 300 documents each, and 1 bin of the remaining 253 documents. We estimate the probability that a randomly selected relevant document be-longs to a certain bin as: p ( d  X  b i | rel. )= | rel. d probability that a randomly selected retrieved document be-longs to a certain bin as: p ( d  X  b i | ret. )= | ret. d Figure 1: Normalised number of technical terms per document vs. probability of relevance (LEFT) &amp; re-trieval (RIGHT), with rank correlation coefficients. ( rel./ret. ) d is a (relevant 3 /retrieved 4 )document, b i th bin, and | X | denotes cardinality.

Fig. 1 shows the plots of these probabilities (y axis) vs. the average terminological density in a bin (x axis). We see that p ( d  X  b i | rel. ) varies non-randomly across bins -ter-minological density is positively correlated to probability of relevance (  X  =0 . 9098). Hence, boosting the ranking of doc-uments of higher terminological density may boost retrieval performance. The differences between how p ( d  X  b i | rel. )and p ( d  X  b i | ret. ) vary across bins shows the document groups for which the retrieval model underperforms: the retrieval model has a weaker correlation to the documents X  termino-logical density (  X  =0 . 8773), with some instability in its per-formance for documents of low terminological density, and fails for documents of the highest terminological density.
Ranking model. If a user submits the query design of biexcitonic models ,andweknowthat biexcitonic is a technical term, we hypothesise that boosting the weight of documents that contain biexcitonic will improve retrieval performance. We implement this boosting using Indri 5  X  X  combination of the Language Modeling (LM) and inference network approaches [1], which allows assigning degrees of belief to different parts of the query. This belief can be drawn from any suitable external evidence of relevance -in our case the knowledge that a search term is technical terminology. Using the #weight and #combine operators for combining beliefs, the relevance of a document d to a query q is computed as the probability that d generates q : p ( q | d )= t  X  q p ( t | d ) w t W ,where W = t  X  q w t w t is the term X  X  belief weight. The higher w t is, the higher the rank of documents containing t . We apply the above equation separately for non-technical common query terms with belief weight w com , and for technical query terms with boost the ranking of documents containing technical terms, we increase w tec at the expense of w com .

Experiments. The baseline matches the documents to queries without any treatment of technical terminology us-ing LM with Dirichlet smoothing. Our approach boosts the weight of technical terms using the same retrieval model but enhanced with belief weights as described above (TEC). We also use a pseudo-relevance feedback (FB) baseline (Indri X  X  default FB implementation), against which we compare our approach combined with FB (TEC+FB). We measure per-according to qrels in top 1000 for any query by the baseline (see Experiments ) Table 1: Retrieval of our method (TEC) vs. the baseline (BASE) &amp; pseudo-relevance feedback (FB). +% shows percent difference from the baseline. formance with the standard TREC metrics shown in Table 1, averaged over all queries for the top 1000 results (apart from the number of relevant retrieved documents (REL.RET.) which is summed). For each metric we tune: Dir X  X   X   X  { 100 , 500 , 800 , 1000 , 2000 , 3000 , 4000 , 5000 , 8000 , 10000 belief weights w com ,w tec  X  X  0 . 1  X  0 . 9 } in steps of 0.1 with w com + w tec = 1 at all times; FB X  X  number of feedback documents  X  X  1 , 2 , 5 , 10 , 20 } and number of feedback terms  X  X  3 , 5 , 10 , 20 , 40 } .

Table 1 shows that boosting the weight of technical ter-minology improves retrieval at all times 6 . The biggest im-provement is for REL.RET., indicating that our approach introduces to the ranking relevant documents that neither the baseline nor FB retrieve. This finding is positive, con-sidering that our approach does not add new terms to the query -it just boosts the weight of existing query (technical) terms. Average precision benefits more when non-assessed documents are ignored in the ranking (BPREF) than when using graded relevance assessments (NDCG), possibly be-cause NDCG gives a lower score to relevant documents that occur in the low ranks (and hence  X  X enalises X  non-relevant documents or non-assessed documents that occur in the high ranks). Our approach benefits early precision for both the top 100 retrieved documents (P@100) and the first relevant retrieved document (MRR). We can also report that our approach outperforms the baseline and FB across the whole tuning range of  X  and for w tec =0 . 1  X  0 . 5 (plots not included for brevity) without any outliers. The values w tec =0 . 1 practically correspond to applying a moderate boost to the weight of technical terminology.
We asked whether the retrieval of scientific books repre-sented only by limited metadata can be improved by us-ing their technical terminology, motivated by the empirical finding that the proportion of technical terms they contained was positively correlated with their probability of being rele-vant. To our knowledge this is a novel finding. We integrated this finding into the retrieval model successfully by boost-ing the ranking of documents containing technical terms, hence showing that our approach benefits the retrieval of book records. Future work includes using the technical term annotations to train an automatic classifier, comparing our approach against an automatic way for determining term relevance (e.g. ontologies, wikipedia pages), and testing the generality of our approach on more scientific domains. [1] D. Metzler and W. B. Croft. Combining the language
Results were not stat. significant when the t-test was used.
