 We consider a general workflow setting in which input data sets are processed by a graph of transformations to produce output re-sults. Our goal is to perform efficient selective refresh of elements in the output data, i.e., compute the latest values of specific out-put elements when the input data may have changed. We explore how data provenance can be used to enable efficient refresh. Our approach is based on capturing one-level data provenance at each transformation when the workflow is run initially. Then at refresh time provenance is used to determine (transitively) which input ele-ments are responsible for given output elements, and the workflow is rerun only on that portion of the data needed for refresh. Our contributions are to formalize the problem setting and the problem itself, to specify properties of transformations and provenance that are required for efficient refresh, and to provide algorithms that apply to a wide class of transformations and workflows. We have built a prototype system supporting the features and algorithms pre-sented in the paper. We report preliminary experimental results on the overhead of provenance capture, and on the crossover point be-tween selective refresh and full workflow recomputation. H.4.1 [ Information Systems Applications ]: Office Automation X  workflow management Algorithms, Management
Consider a workflow in which input data sets are fed into an acyclic graph of transformations to produce output data sets. Ex-amples include data warehouse ETL ( extract-transform-load ) pro-cesses [19], scientific data analyses [4, 11, 18], and information ex-
T his work is supported by the National Science Foundation under grants IIS-0414762 and IIS-0904497 and by a KAUST research grant.
 traction pipelines [6]. Suppose the input data sets have been mod-ified since the workflow was run, but the workflow has not been rerun on the modified input (for reasons outlined below). However, suppose we would like to selectively refresh one or more elements in the output data, i.e., compute the latest values of particular output elements based on the modified input data.

Selective refresh can be useful in any workflow that involves ex-pensive, batch operations, for the following reasons:  X  Eager propagation , i.e., rerunning the workflow whenever the  X  Eager propagation may be overkill: Users may not care about With selective refresh, we can avoid the expense of recomputing an entire output data set frequently, when all we need is a few up-to-date output values after the input has undergone some changes.
Our challenge is to perform selective refresh efficiently in a gen-eral setting. To avoid unnecessary work during the refresh of an output element, we must understand where the element came from in the input data, and how to rerun just the relevant computation to refresh the element. At the same time, we want to support refresh for workflows constructed from a wide class of possible transfor-mations. Thus, we do not limit ourselves to well-understood trans-formations such as relational queries, or specific types of data el-ements. Rather, we define and consider data-oriented workflows , where all we require is some understanding of the behavior and re-lationships of individual data elements constituting the inputs and outputs of each transformation.

Clearly, data provenance (also called lineage ) is highly relevant to our problem, since it captures where data came from and how it was derived [1, 4, 7, 18]. There has been a large body of work in lineage and provenance over the past two decades (Section 1.1), but we are unaware of any previous work that formalizes the refresh problem or exploits provenance for selective refresh in a general workflow environment. In this paper, we explore how provenance can be used to enable correct and efficient refresh. Our contribu-tions are as follows:  X  In Section 2, we present a formal foundation for individual  X  In Section 3, we formalize the refresh problem in terms of  X  In Section 4, we extend our formalization and refresh procedure  X  I n Sections 5 and 6, we extend our formalism and algorithms to  X  In Section 7, we discuss how refresh can still be performed  X  In Section 8, we describe the prototype system we have built  X  In Section 9, we report preliminary experimental results on the The remainder of this section covers related work and introduces a running example. In Section 10 we conclude and describe future work.
There has been a large body of work in lineage and provenance over the past two decades. Surveys are presented in, e.g., [4, 7, 18], and formal models for provenance are presented in, e.g., [1, 2, 5, 9, 10, 13]. Provenance in the context of schema mappings is studied in [8, 12, 20]. None of these papers exploits provenance for selective refresh in a general workflow environment.

There also has been a large body of work in incremental view maintenance : the efficient propagation of base data modifications, usually in a relational setting [3, 14]. Our work considers gen-eral workflows, rather than relational views. Also, in contrast to the view-maintenance problem, selective refresh considers ef-ficiently computing the up-to-date value of individual output ele-ments, rather than keeping the entire view up to date by propagating changes made to the base data.

Reference [15] provides a framework to explain  X  X issing X  an-swers in queries. There is some high-level similarity between how explanations provided by their framework are created and how we support efficient refresh using data provenance. However, the de-tails are quite different, and their framework supports SQL queries rather than general workflows.

Reference [12] considers the problem of  X  X pdate exchange X  be-tween data peers linked by mappings. A subproblem they address is determining when a derived data element is no longer valid, but they do not provide a means to selectively refresh out-of-date val-ues. Also, transformations in [12] are restricted to those that can be expressed in Datalog.
We present a running example, designed to illustrate challenges and solutions throughout the paper. Consider Jen , a genetic coun-selor who runs a workflow, shown in Figure 1, to calculate her pa-tients X  genetic risk profiles. The workflow X  X  input data sets are two lists of URLs: PatientURLs and DNAURLs . PatientURLs point to the patients X  genetic test results, recording the patients X  DNA sequences at certain DNA locations. DNAURLs identify XML documents describing disease risks associated with specific DNA location-sequence combinations. The workflow involves the fol-lowing transformations:  X  Transformations PatientDL and DNADL download files lo- X  Transformations PExtract and RiskExtract extract data from  X  Transformation Join joins tables PatientDNA and DNARisks  X  Transformation Filter selects from PatientRisks those records Figure 2 shows sample input data sets along with all intermediate data, and finally output table HighPatientRisks . (The figure also includes provenance predicates and forward filters , described in Sections 2 and 5, respectively.) The starred data elements are for reference in the following scenario.
 Before seeing a patient, Jen refreshes relevant records in table HighPatientRisks . As an example, we show how our approach efficiently refreshes Denise X  X  heart disease record, i.e., element #2 in table HighPatientRisks . There are two main steps: (1) Backward-tracing: To refresh a given output element, first one-level provenance is used to trace transitively from the output element to its relevant input elements. Starting with HighPatien-tRisks element #2, provenance enables tracing backward one step to obtain element #3 in table PatientRisks . From this element, another step is traced backward, resulting in PatientDNA element #4 together with DNARisks element #2. This process continues, to elements Denise.xml in RawPatientData and 2-aga.xml in RawDNAData , and finally to input elements PatientURLs #3 and DNAURLs #2. Details of how provenance supports backward-tracing in general will be presented in Sections 3 X 6. (2) Forward-propagation: Now that the relevant input elements have been found, they are propagated forward. Transformations PatientDL and DNADL are rerun on input elements PatientURLs #3 and DNAURLs #2 respectively to download the latest data from the web. The resulting elements are then sent through transformations PExtract and RiskExtract . Suppose that when RiskExtract is run on the latest downloaded data, the risk value for DNARisks element #2 has changed from 0.8 to 0.6. Further forward-propagation through transformations Join and Filter sets Denise X  X  new heart-disease risk in HighPatientRisks to 0.6. Note if the new value were  X  0 . 5 , then after the Filter transformation Denise X  X  record would disappear, a situation that is also captured by refresh. Details of when forward-propagation works correctly, and how, will be covered in Sections 3 X 6.

The remainder of the paper formalizes the basic building blocks and techniques demonstrated in this example, covering a wide class of data types, transformations, and workflows.
Let a data set be any set of data elements . We are not concerned about the types of individual data elements; we treat them simply as members of a data set. A transformation T is any procedure that takes one or more data sets as input and produces one or more data sets as output. As can be seen in the running example, we do not limit ourselves to transformations expressible in relational algebra or SQL. For now, we will consider transformations that take a single data set as input and produce a single output set; we 1 genetic.com/dl/00501 2 genetic.com/dl/00502 *3* genetic.com/dl/00503 4 genetic.com/dl/00504 1 Bob 1 aaa p : file =  X  X ob.xml X , f : loc = 1 2 Carl 2 ttt p : file =  X  X arl.xml X , f : loc = 2 3 Carl 3 ggg p : file =  X  X arl.xml X , f : loc = 3 *4* Denise 2 aga p : file =  X  X enise.xml X , f : loc = 2 5 Earl 2 ata p : file =  X  X arl.xml X , f : loc = 2 6 Earl 3 gcc p : file =  X  X arl.xml X , f : loc = 3 1 Bob heart 0.6 p P D : name= X  X ob X   X  loc=1  X  seq= X  X aa X , p 2 Carl liver 0.4 p P D : name= X  X arl X   X  loc=2  X  seq= X  X tt X , p
DR : loc=2  X  seq= X  X tt X  *3* Denise heart 0.8 p P D : name= X  X enise X   X  loc=2  X  seq= X  X ga X , p 4 Earl lung 0.7 p P D : name= X  X arl X   X  loc=3  X  seq= X  X cc X , p 1 Bob heart 0.6 p : name =  X  X ob X   X  disease =  X  X eart X  *2* Denise heart 0.8 p : name =  X  X enise X   X  disease =  X  X eart X  3 Earl lung 0.7 p : name =  X  X arl X   X  disease =  X  X ung X  *2* dnarec.com/2/aga *2* 2 aga heart 0.8 p : file =  X 2-aga.xml X  will generalize to multi-input transformations in Section 6. ( Multi-output transformations do not introduce any interesting challenges, and are thus omitted.) For any input data set I , we say that the application of T to I resulting in an output set O , denoted T ( I ) = O , is an instance of T .
In general a transformation may inspect the entire input data set to produce each element in the output set, but in most cases there is a more fine-grained relationship between the input and output data elements: an output data element may have been derived from a small subset of the input data elements (maybe only one). Given a transformation instance T ( I ) = O and an output element o  X  O , provenance identifies the input data elements that contributed to o  X  X  derivation.

There has been a great deal of work on defining and capturing provenance for specific understood types of transformations [5, 7, 10], and some work describing properties of  X  X paque X  transforma-tions and how they relate to provenance [9]. Our goal in this paper is not to expand or improve upon any of that work, but rather to de-velop a formal framework and algorithms for the refresh problem that can exploit those definitions and techniques.

For generality, we only require that provenance for each output data element can be obtained by applying a predicate on the input: transformation instance T ( I ) = O . We require that each output element o  X  O b e annotated with a provenance predicate p . The elements of I satisfied by predicate p , i.e., the result of tracing query  X  p ( I ) , constitute o  X  X  provenance for instance T ( I ) = O . 2 We do not impose any restrictions on provenance predicates, except that they can be applied to choose elements from data sets. Also note that our use of the relational notation  X  p ( I ) for tracing queries is for convenience and familiarity only; data set I need not be a conventional relation.

Predicates give us a very general notion of provenance, and a full treatment of how provenance predicates are obtained is beyond the scope of this paper. Note, however, that provenance in the form of pointers to specific data elements, as in [2] and other work, can be specified as predicates selecting on identifiers or keys. (In this case  X  X valuating a tracing query X  would likely be implemented as simple pointer-chasing.) Provenance for basic relational operators [5, 7, 10] is naturally expressed as predicates, e.g., for a group-by aggregation operator, provenance predicates select relevant input elements based on grouping value. Most of the numerous trans-formation types covered in [9] are amenable to the provenance-predicate approach. Although it X  X  possible some less conventional forms of provenance may not be captured by predicates, we believe this formalization is a useful and general starting point.
Formally, we now assume that the output of each transfor-mation instance produces a set of pairs: T ( I ) = O = {h o 1 , p 1 i , . . . , h o n , p n i} , combining output elements and their provenance predicates.

E XAMPLE 2.1. Our running example data in Figure 2 includes provenance predicates, denoted p . Table PatientDNA includes ad-ditional predicates labeled f , which will be explained in Section 5. Table PatientRisks includes pairs of provenance predicates, since there are two input data sets to its transformation (Section 6). All of the remaining tables have a single provenance predicate for each output element, according to our definition. In all of them it can be seen easily that the predicate p associated with each output element o , when applied to the input table for o  X  X  transformation, produces the appropriate provenance for o . 2
In our running example, it happens that provenance predicates al-ways select a single input element, but this property is not required in our approach. In general, provenance predicates may select any number of input elements, up to the entire input data set.
In this section, we specify a provenance-based refresh procedure for single transformations, and we identify two properties of the transformations and their provenance that are required for the pro-cedure to work correctly. These properties also yield insights into the meaning of refresh in the presence of provenance.

But first, ignoring provenance for a moment, consider refresh for a single transformation T . Suppose input I has been modified to I new , and we would like to refresh an output element o that was produced by T ( I ) . The refreshed value of o should be the element o in T ( I new ) that  X  X orresponds X  to o , if one exists. (In this paper we assume refresh of a single output element produces either a sin-gle refreshed element or no element at all, but not several elements. We intend to relax this assumption in future work.) Note that for refresh to even be well-defined, we need to formalize when an ele-ment o  X  in T ( I new ) corresponds to the element o being refreshed.
One way to make refresh well-defined is to declare one or more output attributes as an immutable key. Then, given an output el-ement o in T ( I ) , the refreshed value of o is the element o T ( I new ) with the same key, if one exists.
 E XAMPLE 3.1. Consider output directory RawDNAData in Figure 2. Intuitively, the file name is an immutable key. To refresh a file in RawDNAData , we could rerun transformation DNADL on list DNAURLs , then look for the output file whose file name matches the file we wish to refresh. 2 While immutable keys make for a convenient definition, unfortu-nately performing refresh based on immutable keys typically re-quires full recomputation of the output data set in order to find the refreshed element. Our goal is to avoid unnecessary computation while performing selective refresh.

Consider as an alternative the following refresh procedure based on provenance. For the remainder of the paper we assume each transformation T has a (possibly infinite) input domain I fying T  X  X  allowable input sets.
 Consider transformation instance T ( I ) = O , and suppose input data set I  X  I T has been modified to I new  X  I T . To refresh an output element h o, p i  X  O there are two steps: 1. Backward-tracing: Run tracing query  X  p on I new to find the 2. Forward-propagation: Apply T on  X  p ( I new ) to compute the
E XAMPLE 3.2. Suppose we wish to refresh RawDNAData el-ement 2-aga.xml . Instead of rerunning transformation DNADL on the entire input data set as suggested in Example 3.1, Procedure 3.1 first finds the provenance of the element being refreshed: Prove-nance predicate p ( url contains  X 2/aga X ) is applied to the input set DNAURLs to obtain DNAURLs element #2. Next transformation DNADL is applied to the selected element only, which yields the refreshed RawDNAData element. 2 Example 3.2 is more efficient than Example 3.1, but does Procedure 3.1 always work? We identify two requirements on transformations and provenance under which it does.

The first restriction requires that each provenance predicate in-deed captures the relevant input data subset for its output element. Consider any transformation instance T ( I ) = O for I  X  I any h o, p i  X  O . Then T (  X  p ( I )) = {h o, p i} . 2 For many-one (and therefore one-one ) transformations and their provenance, this requirement is natural. We will adapt the require-ment to also capture many-many transformations, but for presenta-tion purposes we defer this topic to Section 5.

All of the transformations in our running example satisfy Re-quirement 3.1, with two exceptions: PExtract is a many-many transformation that requires the additional machinery introduced in Section 5, and Join is a multi-input transformation requiring some (minimal) additional machinery covered in Section 6.

The second requirement is more subtle, and more central to our approach: transformation instance T ( I ) = O for I  X  I T , and any h o, p i  X  O . Then for any I  X   X  I T , if T (  X  p ( I  X  )) 6 =  X  , then T (  X  {h o  X  , p i} for some o  X  , and h o  X  , p i  X  T ( I  X  ) . 2 This requirement intuitively states that when input changes, even if there is a new value for an old output element based on its prove-nance, the provenance predicate remains unchanged. Not only does this condition enable efficient refresh, it also identifi es what it means for an output element based on new input to be the refreshed version of an old output element: In our running example, and in other workflows we have looked at [9, 10], it is natural for transformations to output provenance pred-icates such that Requirement 3.2 is satisfied, and for provenance to act as an immutable key.

The following property, which follows directly from Require-ment 3.1, further solidifies the key analogy by observing that prove-nance predicates are unique (under set semantics) within each out-put data set:
P ROPERTY 3.1 (U NIQUE P ROVENANCE ). Consider any transformation instance T ( I ) = O for I  X  I T , and any h o, p i  X  O . There is no h o  X  , p i  X  T ( I ) with o  X  6 = o . 2
Now let us return to our refresh procedure and see how Require-ments 3.1 and 3.2 guarantee that Procedure 3.1 works correctly, under the provenance-as-key approach. Recall, to refresh an output element h o, p i  X  T ( I ) after input I has been modified to I Procedure 3.1 computes T (  X  p ( I new )) .  X  First suppose T (  X  p ( I new )) produces an empty result. Then  X  Now suppose T (  X  p ( I new )) is non-empty. Requirement 3.2
Consider any transformations T 1 and T 2 . As usual, each trans-formation takes a data set from its domain as input, and it produces as output a data set annotated with provenance predicates. The composition T 1  X  T 2 of the two transformations first applies T an input data set I 1  X  I T 1 to obtain intermediate data set I applies T 2 to the data portion (omitting provenance predicates) of I to obtain output data set O . We assume transformations T T 2 are only composed when the data elements output by T 1 are guaranteed to satisfy the input domain of T 2 .

Composition is associative, so we denote the linear composition of n transformations as T 1  X  T 2  X   X  T n . We refer to such a com-position as a data-oriented workflow , or workflow for short. In Sec-tion 6, we extend our formalism and algorithms to cover workflows where transformations may have multiple input data sets. (As men-tioned in Section 2, transformations with multiple output sets do not introduce any complexities, and therefore are omitted from this paper.) Our running example in Figure 1 is a workflow composed of six transformations, with one of them taking multiple inputs.
Consider a workflow T 1  X  T 2  X  . . .  X  T n . A workflow instance is the application of the workflow to an input I 1  X  I T 1 . Let I T ( I i ) for i = 1 ..n . We assume I i +1  X  I T i +1 for i = 1 ..n  X  1 . The final output data set is I n +1 . We denote this workflow instance as ( T 1  X  T 2  X  . . .  X  T n )( I 1 ) = I n +1 .

Our goal is to selectively refresh an output element of an arbi-trary workflow instance, when input data sets may have been mod-ified. The following workflow refresh algorithm is a recursive ex-tension of the single-transformation refresh Procedure 3.1. workflow instance ( T 1  X  T 2  X  . . .  X  T n )( I 1 ) = I n +1 has been modified to I new 1  X  I T 1 . Algorithm workflow_refresh recursively refreshes output element h o, p i  X  I i +1 : workflow_refresh ( h o , p i  X  I i +1 ) : if i = 1 then return T 1 (  X  p ( I new 1 )) else { S =  X  p ( I i ) ;
E X AMPLE 4.1. We revisit the original example from Section 1.2, now using workflow_refresh to refresh (Denise,heart,0.8) (element #2) in HighPatientRisks . The else branch of the algo-rithm traces backward one step by applying provenance predicate p ( name = X  X enise X   X  disease = X  X eart X ) to table PatientRisks , yield-ing element #3. This element is then refreshed recursively: The algorithm traces backward another step by applying provenance predicates to PatientDNA and DNARisks (requiring our extension for multi-input transformations; see Section 6), yielding elements #4 and #2 respectively. The recursive refresh continues until the initial transformations PatientDL and DNADL are reached, for which the if branch selects elements #3 and #2 from input sets Pa-tientURLs and DNAURLs respectively.

As the recursion unwinds, the algorithm forward-propagates each refreshed element. Transformations PatientDL and DNADL are run on PatientURLs element #3 and DNAURLs element #2, yielding refreshed values for elements Denise.xml in RawPatient-Data and 2-aga.xml in RawDNAData . PExtract and RiskEx-tract are then run on these refreshed elements. Suppose, as in Sec-tion 1.2, the refreshed value for DNARisks element #2 now has risk =0.6. After the unwinding recursion runs Join and Filter on refreshed elements, we finally get the refreshed value of HighPa-tientRisks element #2, with Denise X  X  heart disease risk set to 0.6. 2
In Section 3 we identified properties of transformations and provenance required for the single-transformation refresh proce-dure to work correctly. Are the same properties sufficient for the recursive algorithm to work correctly? It turns out we need to re-strict the composition of transformations and their provenance to ensure correctness; we call this requirement workflow safety . We first provide intuition for workflow safety, then formalize it. Consider T 1  X  T 2 applied to input I 1  X  I T 1 . Let I 2 and O = T 2 ( I 2 ) . Suppose I 1 has been modified to I new Let I new 2 = T 1 ( I new 1 ) , i.e., I new 2 is what would be produced by running transformation T 1 on the entire new input set. Consider any element h o, p i in the original output set O . Safety requires that the set of elements in o  X  X   X  X ew provenance, X   X  p ( I new the set obtained by refreshing each element in o  X  X   X  X ld provenance, X   X  ( I 2 ) .

R EQUIREMENT 4.1 (W ORKFLOW S AFETY ). Consider any workflow instance ( T 1  X  T 2  X  . . .  X  T n )( I 1 ) = I n +1 be safe with respect to T i  X  1 , i = 2 ..n , defined as follows. Consider we must have [
With extensions to the safety definition to be introduced in Sec-tions 5 and 6 for many-many and multi-input transformations, the workflow in our running example satisfies Requirement 4.1. How -ever, there are some reasonable workflows where safety is not sat-isfied, as illustrated by the following example.

E XAMPLE 4.2. Consider a workflow T 1  X  T 2 that takes an in-put set I 1 with attributes salesperson , city , and sales_in_Euros . T 1 converts sales_in_Euros to sales_in_Dollars , with output provenance predicates selecting on salesperson . T 2 then sums sales_in_Dollars grouped by city , with output provenance predi-cates selecting on city .
 Suppose the original input I 1 has two salespeople from Paris, Amelie and Jacques , selling 10 Euros each. The out-put I 2 of transformation T 1 contains (Amelie,Paris,13) and (Jacques,Paris,13) (at generous 2010 exchange rates), and the final output is (Paris,26) . Now suppose I 1 is modified to I with an additional salesperson (Marie,Paris,20) . Safety requires equality of the following two procedures: 1. Compute the provenance of (Paris,26) in intermediate data set 2. Update the intermediate data set to I new 2 = T 1 ( I new Intuitively, a workflow is unsafe if, in some intermediate data set I , full forward-propagation of modified input would cause  X  X nser-tions X  into a subset of I that comprises the provenance of a data element in the next data set. These insertions will be missed when we perform backward-tracing, since we only refresh existing ele-ments in intermediate data sets.

Provenance predicates for relational transformations typically yield safe workflows, but aggregation is an example of a transfor-mation that can cause a workflow to be unsafe, if it is not the first transformation in a workflow, or if groups can grow as a result of input modifications. In Section 7 we discuss one way of handling unsafe workflows that allows us to retain some of the advantages of selective refresh without compromising correctness.

We now show that workflow_refresh correctly refreshes output elements, when Requirements 3.1, 3.2, and 4.1 all hold. The argu-ment hinges on the following theorem.
 Consider a workflow instance ( T 1  X  T 2  X  . . .  X  T n )( I satisfying Requirement 4.1. Given any element h o, p i  X  I i  X  1 , workflow_refresh ( h o, p i ) = T i (  X  p ( I new Proof. We prove the theorem by induction on i . For the base case of i = 1 , consider any h o, p i  X  I 2 = T 1 ( I 1 ) . By the first line ( if case) of Algorithm 4.1, workflow_refresh ( h o, p i  X  I T
Now suppose the theorem holds for i = k  X  1 , k &gt; 1 ; we show it holds for i = k . Consider element h o, p i  X  I k +1 . workflow_refresh computes the following two sets: S =  X  p ( I k ) = {h o 1 , p 1 i . . . h o m , p m i} and S workflow_refresh ( h o i , p i i ) . By the inductive hypothesis, each workflow_refresh ( h o i , p i i ) = T k  X  1 (  X  p i ( I new hand side of the last expression is equal to  X  p ( I new k line of workflow_refresh returns T k ( S  X  ) , workflow_refresh returns T (  X  p ( I new k )) , which completes the proof. 2 To understand what the theorem is saying, consider a workflow in-stance ( T 1  X  T 2  X  . . .  X  T n )( I 1 ) = I n +1 and a final output element h o, p i  X  I n +1 . Suppose I 1 is updated to I new 1 . Theorem 4.1 says that running workflow_refresh on h o, p i is equivalent to computing I n by pushing input I new 1 through every transformation except the last one, then running T n on o  X  X   X  X ew provenance, X   X 
With this theorem, we see that the same arguments given in Sec-tion 3 for the correctness of single-transformation refresh carry over to the general workflow case: Running workflow_refresh logi-cally reduces to running single-transformation refresh of T new input set I new n . Since T n satisfies Requirements 3.1 and 3.2, the arguments in Section 3 show that workflow_refresh returns the correct refresh of an element: If it returns empty, there is no ele-ment in the new output set with provenance predicate p . If it returns an element h o  X  , p  X  i , then p  X  = p and h o  X  , p i is the unique element in I new n +1 with predicate p .
So far we have required T (  X  p ( I )) = {h o, p i} for any h o, p i  X  O in any transformation instance T ( I ) = O (Requirement 3.1). This requirement effectively limits us to transformations that are many-one or one-one. We now weaken this requirement, only insisting h o, p i  X  T (  X  p ( I )) . Let us see how weakening the requirement captures more transformations (specifically allowing one-many and many-many transformations) but complicates the picture.
E XAMPLE 5.1. In our running example, PExtract is a one-many transformation. Consider refreshing (Earl,3,gcc) (element #6) in PatientDNA through transformation PExtract . Using Re-fresh Procedure 3.1 for single transformations, provenance predi-cate file = X  X arl.xml X  is applied to RawPatientData , producing in-put element Earl.xml . Suppose when transformation PExtract is then run on Earl.xml , two elements are produced, (Earl,2,aaa) and (Earl,3,ttt) (indicating corrected DNA sequences at locations 2 and 3), both with provenance predicate file = X  X arl.xml X . How do we know which of these elements, if any, corresponds to the one we are trying to refresh? 2
To solve the problem illustrated in this example, we require that for many-many (and therefore one-many) transformations, output elements include not only provenance predicates, but also forward filters . The forward filter for an output element o is applied after forward-propagating o  X  X  provenance, to select from multiple output elements the one corresponding to o . In Example 5.1, a suitable forward filter for output element #6 is loc =3, capturing the fact that element #6 describes Earl X  X  DNA sequence at location 3. Note that all of the forward filters for table PatientDNA (denoted f in Figure 2) select on attribute loc , since locations are unique within each set of elements for a given name .

It is not hard to generalize our entire framework to support many-many transformations using forward filters. We require each trans-formation instance to produce triples instead of pairs: T ( I ) = all f i = True for many-one transformations, our extension is fully  X  X ackward compatible X  with everything in the paper thus far.) To refresh an element h o, p, f i  X  O , we add a third step to Proce-dure 3.1 that applies forward filter  X  f to the result from Step 2, i.e., the overall refresh operation is  X  f ( T (  X  p ( I new ))) .
All of the formalism and intuitive arguments in Sections 3 and 4 extend quite easily to incorporate forward filters, generally replac-ing h o, p i with h o, p, f i and T (  X  p ( I )) with  X  f that in Requirement 3.2 ( Provenance as Key ), by extending each output pair to include a forward filter f , we are effectively treating provenance predicates and forward filters together as immuta ble keys, i.e., only the p -f pairs need be unique, not provenance pred-icates alone. Full details of the extension for many-many transfor-mations are given in an online technical report [16].
For presentation purposes, so far we have assumed each trans-formation has one input data set. The extension for transforma-tions with multiple input sets is straightforward and intuitive: Each output element carries a separate provenance predicate for each of its transformation X  X  input sets. Now a transformation instance is T ( I 1 , I 2 , . . . , I m ) = O , and O consists of extended triples: {h o 1 , ( p 1 1 , . . . , p m 1 ) , f 1 i , . . . , h o n full  X  X ackward compatibility X  with everything in the paper thus far by setting m = 1 .) Refresh proceeds in a similar manner as before, except during backward-tracing m provenance predicates are eval-uated on their corresponding input data sets, and during forward-propagation the transformation is run on the m input subsets.
E XAMPLE 6.1. From our running example, to refresh (Carl,liver,0.4) (element #2) in PatientRisks , provenance pred-icate p P D ( name = X  X arl X   X  loc =2  X  seq = X  X tt X ) is applied on PatientDNA to obtain element #2, and provenance predicate p ( loc =2  X  seq = X  X tt X ) is applied on DNARisks to obtain element #3. These elements are forward-propagated as the two inputs to transformation Join , yielding the refreshed value of PatientRisks element #2. 2
We can easily adapt all of the formalism from Sections 3 X 5 to handle multi-input transformations. In general we replace p with p , . . . , p m , and T (  X  p ( I )) with T (  X  p 1 ( I 1 ) , . . . ,  X  quirement 3.2 ( Provenance as Key ), we now treat the entire combi-nation of p 1 , . . . , p m , f as the immutable key. In Requirement 4.1 ( Workflow Safety ), we require each transformation to be safe with respect to all of its predecessor transformations in concert. Full details are given in an online technical report [16].
In Section 4 we introduced safe workflows (Requirement 4.1), and our refresh algorithms thus far require workflow safety. For now, we suggest one simple mechanism that allows refresh in un-safe workflows to still make some use of our algorithms.
Consider a workflow instance ( T 1  X  T 2  X  . . .  X  T n )( I (Our argument generalizes easily to workflows with multi-input transformations.) Suppose the workflow is unsafe at T k for some k , 1 &lt; k  X  n , but the rest of the workflow is safe. (More gener-ally, consider the largest k such that the workflow is unsafe at T Suppose I 1 has been modified to I new 1 . To support correct refresh of output elements in I n +1 , we can first bring intermediate data set I up-to-date by sending the entire modified input I new 1 through all transformations up to T k  X  1 , producing I new k . Then we can treat I k as if it were the first input data set (and T k the first transfor-mation), performing refresh as normal: backward-trace from I to I k , then forward-propagate through to I n +1 .
 In this setting, we would certainly want to keep track of when I k is up-to-date, perhaps even propagating input modifications through the first k  X  1 transformations eagerly. Combining eager propagation with on-demand refresh is an interesting direction of future work.
We have built a prototype system that implements all features and algorithms presented in this paper. This system, built initially t o support refresh, has been evolving into a more ambitious sys-tem we call Panda (for Provenance and Data ), supporting several other aspects of provenance and data in addition to refresh [17]. In this paper we focus our system description on features relevant to refresh. For the time being, all data sets handled by Panda are encoded in relational tables, but as we have seen, our formal under-pinnings and algorithms do not rely on the relational model.
The high-level architecture of the Panda system is shown in Fig-ure 3. The main backend is a SQLite server, storing all data sets, relational (SQL) transformations, provenance, and workflow infor-mation. The Panda system supports  X  X paque X  transformations pro-grammed in Python; they are stored separately in files.
Users interact with Panda through a simple command-line inter-face; we intend to build a GUI in the near future. There are three types of user commands: (1) Creating or modifying input data sets; (2) Creating transformations that generate newly-defined data sets from existing ones, to build up workflows; (3) Refresh commands. The Panda Layer processes all user commands: It stores workflow graphs and their transformations, creates and maintains auxiliary provenance tables, generates provenance predicates and forward filters for output elements, and runs the refresh algorithms.
In the remainder of this section we briefly discuss how Panda handles transformations specified in Python, transformations spec-ified as SQL queries, and finally Refresh commands.
The Panda system currently supports Python transformations that output provenance predicates with each output element. As an example, consider adding transformation PatientDL as we build up our running example workflow (Figure 1). The user writes a Python script patientdl.py that takes as its argument a list of URLs, and returns the set of XML documents located at the URLs, with cor-responding provenance predicates. The user then appends transfor-mation PatientDL to the workflow with the following command: Create Dataset RawPatientData As Python  X  X atientdl.py X  on PatientURLs In response to this command, the Panda Layer: (1) Creates the new data set RawPatientData ; (2) Inserts a record into the Workflow table connecting PatientURLs to RawPatientData via Python script patientdl.py ; (3) Creates a provenance-predicate table Raw-PatientData_PP for RawPatientData ; (4) Runs patientdl.py on PatientURLs , inserting the resulting elements and correspond-ing provenance-predicate records into tables RawPatientData and RawPatientData_PP , respectively.
For many-many transformations, the Python script also needs to create a forward filter for each output element. Then, step (3) above also creates a forward-filter table, and step (4) also inserts a record for the forward filter. The Panda system also supports transformations specified as SQL queries, including queries/transformations involving multi-ple input tables. Provenance predicates are created automatically for SQL transformations, following known definitions and tech-niques [5, 7, 10]: Single-table Select statements are one-one, so their output provenance predicates can select on declared keys from the input data set. Multi-table Select statements generate prove-nance predicates for each input table separately as described in Sec-tion 6, again relying on declared keys. Finally, Group-by queries generate provenance predicates based on the grouping attribute(s).
The command to create a new SQL transformation is similar to the command shown in Section 8.1, except As is followed by a SQL query, whose From clause must refer to already-defined ta-bles. The steps performed by the Panda Layer are also similar to those outlined in Section 8.1; forward filters are never needed since SQL queries cannot produce many-many transformations.
When workflows are created and run, Panda stores everything needed to support selective refresh: provenance predicates and in-termediate data sets for backward-tracing; transformations and for-ward filters for forward-propagation. The Panda system assumes that all transformations, provenance, and workflows satisfy the re-quirements specified in this paper. Automatically detecting when the requirements are satisfied X  X articularly the most interesting re-quirement of workflow safety X  X s an important area of future work.
Under the assumption of all requirements being satisfied, Panda supports selective refresh using the exact algorithms given in this paper. When an output element o is refreshed, if a new value o produced then Panda automatically replaces o with o  X  . If the refresh results in an empty set, then o is deleted. However, we leave a visible  X  X ombstone X  for o with its associated provenance predicate (and forward filter if present). Then, if desired we can refresh the tombstone at a later time and possibly discover that further input modifications have created a new value for o .
The primary goal of our empirical study was to determine, for varying workflow characteristics, when it is advantageous to per-form selective refresh as opposed to rerunning the entire workflow. Specifically, how many refreshes can we perform before their ag-gregate running time X  X ncluding the extra time spent to capture provenance X  X xceeds that of complete recomputation?
While eventually we plan to experiment with a suite of work-flows, for this preliminary empirical study we used our running example workflow described in Section 1.2, with fabricated data. Transformations Join and Filter use SQL, while transformations PatientDL , DNADL , PExtract , and RiskExtract were coded in Python. We indexed the intermediate data sets so that our prove-nance predicates could be evaluated efficiently. All of our experi-ments were run using the Panda system (Section 8) on a MacBook Pro laptop (2.4 GHz Intel Core 2 Duo, 4 GB memory, 150 GB storage, Mac OS X 10.4). To study how different workflow char-acteristics impact the overhead of provenance capture and the per-formance of refresh, we ran experiments for varying data sizes and varying transformation costs.

Our performance results are summarized as follows:
Figure 4: Time overhead of provenance capture, vary input siz e.
Figure 5: Space overhead of provenance capture, vary input si ze.  X  The time and space overhead of provenance capture is propor- X  Not surprisingly, the relative time overhead of provenance cap- X  In our experiments, we were able to refresh between 52% and
Figure 4 shows the time overhead of provenance capture for varying input data sizes. Comparing the running times of work-flow computation with and without provenance, we see that the time overhead is roughly proportional to the amount of input data, ranging from 27% to 34%
Figure 5 shows the space overhead of provenance capture for varying input data sizes. We totaled all data involved in the work-flow, including intermediate data, with and without provenance. Storing provenance, which includes provenance predicates and for-ward filters, incurred a 56% overhead across all input data sizes. Note the space overhead would be considerably lower for  X  X ider X  data elements; our fabricated data elements are relatively small.
Next we measured the impact of transformation cost on time overhead of provenance capture. We modified the two transforma-tions that download data from the web, PatientDL and DNADL , to Figure 6: Time overhead of provenance capture, vary transfor ma-tion cost. perform local downloads instead, and we instrumented them with a configurable delay. Figure 6 shows the time overhead of prove-nance capture, varying the costs of (i.e., the delays in) transforma-tions PatientDL and DNADL . We can see that the larger the trans-formation costs, the lower the relative time overhead of provenance capture; e.g., time overhead is 42% at 50ms vs. 26% at 200ms. Intuitively, when transformations are more expensive, provenance capture plays a smaller role.
Our next experiments identify the crossover point between se-lective refreshes (including provenance capture) and full recompu-tation (without provenance capture). For starters, the two lines in Figure 7 plot the running times of: 1. Recompute: Time required to fully (re)compute the workflow. 2. Refresh: Total time to selectively refresh a varying fraction of
In this experiment we used an input size of 20 MB and the orig-inal transformation costs for PatientDL and DNADL . From Fig-ure 7 we see that we can capture provenance and then refresh over 60% of the data elements in the output data set before the total running time exceeds that of recomputation. Obviously, selective refresh is most advantageous when only a small subset of the out-put elements are refreshed, however in our experiment a significant fraction of output elements needed to be refreshed before we ob-served no advantage at all.

Figure 8 shows the crossover point (in terms of percentage of refreshed elements) between selective refresh and full workflow recomputation when we vary the input size. This line is approx-imately constant, indicating that the relationship between selec-tive refresh and workflow recomputation is independent of input data size. Figure 9 shows the impact of transformation costs on the crossover point. The input data size is 20 MB, and the trans-formation costs are varied as described above for Figure 6. Here the crossover point becomes more favorable for selective refresh as transformations get more expensive: we can refresh 52% at 50ms versus 70% at 200ms before refresh time exceeds recomputation. Like with provenance capture, as transformations get more expen-sive, we are not surprised to see a decrease in the relative cost of provenance tracing.
All of the reported experiments were conducted using our run-ning example, which is a safe workflow. Although we have not yet run experiments on unsafe workflows, until we develop more sophisticated techniques for handling them, we don X  X  expect any
Crossover point Figure 8: Crossover point between selective refresh and work flow recomputation, vary input size.
Crossover point Figure 9: Crossover point between selective refresh and work flow recomputation, vary transformation cost. significant surprises or insights. Recall from Section 7 that cur-rently we propose handling unsafe workflows with a simple hybrid approach: perform workflow computation for the unsafe portion of the workflow, and selective refresh for the rest. Since this so-lution combines the two computations we X  X e measured, we would expect a hybrid result: the crossover point for unsafe workflows will be less favorable towards selective refresh, with the amount of crossover  X  X hift X  determined by the percentage of the workflow that is unsafe.
We presented a formal foundation and algorithms for efficient selective refresh of output elements in data-oriented workflows. We identified properties of transformations, provenance, and work-flows that are required for the algorithms to perform refresh cor-rectly, and we discussed how the algorithms can be adapted to h an-dle unsafe workflows. We described the prototype system we have built that supports the features and algorithms presented in the pa-per, and we reported some experimental results. There are several directions for future work:  X  Verifying requirements: Currently we rely on workflow cre- X  Relaxing requirements: We would like to offer solutions  X  Automatic generation of provenance predicates: As dis- X  Integrating refresh with eager propagation: In its current  X  Stability guarantees: Currently we assume that input data  X  Special settings: In this paper we have considered a very gen-[1] The Open Provenance Model  X  Core Specification (v1.1). [2] O. Benjelloun, A. Das Sarma, A. Halevy, and J. Widom. [3] J. A. Blakeley, P.-A. Larson, and F. W. Tompa. Efficiently [4] R. Bose and J. Frew. Lineage retrieval for scientific data [5] P. Buneman, S. Khanna, and W.-C. Tan. Why and where: A [6] C.-H. Chang, M. Kayed, M. R. Girgis, and K. F. Shaalan. A [7] J. Cheney, L. Chiticariu, and W.-C. Tan. Provenance in [8] L. Chiticariu and W.-C. Tan. Debugging schema mappings [9] Y. Cui and J. Widom. Lineage tracing for general data [10] Y. Cui, J. Widom, and J. L. Wiener. Tracing the lineage of [11] S. B. Davidson and J. Freire. Provenance and scientific [12] T. J. Green, G. Karvounarakis, Z. G. Ives, and V. Tannen. [13] T. J. Green, G. Karvounarakis, and V. Tannen. Provenance [14] A. Gupta and I. S. Mumick. Maintenance of materialized [15] M. Herschel and M. A. Hern X ndez. Explaining missing [16] R. Ikeda, S. Salihoglu, and J. Widom. Provenance-based [17] R. Ikeda and J. Widom. Panda: A system for provenance and [18] Y. L. Simmhan, B. Plale, and D. Gannon. A survey of data [19] P. Vassiliadis, A. Simitsis, and S. Skiadopoulos. Conceptual [20] Y. Velegrakis, R. J. Miller, and J. Mylopoulos. Representing
