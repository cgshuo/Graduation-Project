 1. Introduction
Emotions play an essential role in social interactions ( Russell, Bachorowski, &amp; Fernandez-Dols, 2003; Scherer, 2003; San-der et al., 2005 ), perform important regulatory and utilitarian functions within human body and brain, and facilitate rational decision making and perception ( Damasio, 1994 ). Evidence from recent neurological studies underlines the importance of emotions in human cognition and perception ( Picard, 2001 ). Yet, the disciplines that study human X  X omputer interaction have only recently started to investigate this phenomenon and gain understanding of its causes and effects ( Julien, McKech-( Norman, 2004 ) refer to the integration of emotions in the design of computer systems in an attempt to make them more natural for humans to understand and use ( Picard, 2003 ). Some progress has been made in developing  X  X  X ffective systems X  that are capable of recognising and appropriately responding to human emotions, and ultimately making human X  X omputer interaction experiences more effective and pleasurable.

This article aims to aid current and future studies of emotions in human X  X omputer interactions by reviewing the defini-tions and theories of emotions, methods for studying emotions and, surveying current state of emotion research in library and information science (LIS), information retrieval (IR) and human X  X omputer interaction (HCI).  X 
After reviewing and carefully evaluating the theories and methods employed in the field of emotion research, we ob-served varying strengths and weaknesses in all of them and settled on a neutral stance in our review. The field of emotion research is young and rapidly developing. In our opinion, the reviewed methods have had a similar level of exposure and validation by our peers. Our approach has been to present the existing methodologies to our readers in a way that will enable them to evaluate these approaches against their specific research objectives. 2. Theories of emotions
Despite the long history of inquiry into the nature of emotion there is an apparent lack of consensus and uniformity with-in the scientific community on what emotions are and how we can represent them. Kleinginna et al. (2005) collected more than 90 definitions of emotions. Emotions have been defined as states of emotional feeling ( Johnson-Laird et al., 1989 ), as feeling states involving positive or negative affective valence ( Ortony, Clore, &amp; Collins, 1988 ), as states of automatic arousal ( Schachter &amp; Singer, 1962 ), or changes in the activation of action dispositions ( Frijda, 1986 ). Moreover, the indiscriminate application of the term  X  X  X motion X  has led to the vague differentiation between the terms  X  X  X motion X ,  X  X  X eeling X ,  X  X  X ood X ,  X  X  X tti-tude X , and others.

There is no agreement about the nature of emotion and its relationship to the emotion stimuli. Theories of emotion can be grouped into two main categories. The first category invokes cognition as a necessary element of emotion and tries to explain the subjective manifestations of emotional experiences. The cognitive theories of emotion argue that the cognitive activity can be conscious or unconscious, intentional or unintentional and take a form of a judgement or a thought. This activity is also known as cognitive appraisal ( Folkman, Lazarus, Gruen, &amp; DeLongis, 1986 ) and refers to the evaluation of a particular encounter with the environment, as well as the determination of its relevance to one X  X  well-being. The major proponent of the cognitive theory of emotion was Lazarus (1984) , who stressed the importance of cognitive evaluations in establishing the meaning of stimuli and the way of coping with it. Another example of a cognitive approach is the work of Frijda (1994) , who defined emotion as reaction to affectively important event that consist of affect, awareness of an emotional object and further appraisal of that object, action readiness and automatic arousal. The componential theory of emotion ( Scherer, 2005 ) is another example of the modern cognitive theory that treats emotion as a synchronisation of many different bodily, per-ceptual and cognitive processes.

The second category of emotion theories emphasises somatic factors and seeks to describe emotional expressions and perceptions of emotional expressions ( Zajonc, 1984 ). Somatic theories argue that bodily responses, and not cognitive judge-ments, cause emotional reactions. Major proponents of the somatic approach include Silvan Tomkins, Robert Plutchik and
Paul Ekman. Tomkins (1984) views affect system as the primary motivation system that can amplify other physical and bod-ily functions (e.g., interference with breathing causes terror that leads to the struggle for air). Plutchik (1980) stresses the evolutionary link of emotion with instinctive behaviour in animals. Ekman (1984) , who shares a similar viewpoint, regards emotions as psychosomatic states that have evolved over time due to their adaptive value in dealing with prototypical life tasks. Ekman suggests that emotions X  primary function is to mobilise an organism to respond quickly to prototypical events, similar to those that were encountered in the past.
 Both categories of emotion theories are implicitly or explicitly used in the studies of affective aspects of information use.
For example, studies that require participants to explain their feelings assume existence of the evaluative component of emotional reaction ( Bilal &amp; Bachir, 2007; Gwizdka &amp; Lopatovska, 2009; Kuhlthau, 1991; Lopatovska &amp; Mokros, 2008 ), while studies that measure spontaneous bodily responses to emotional stimuli follow the rationale of somatic theories of emotion ( Arapakis, Konstas, &amp; Jose, 2009; Arapakis et al., 2009; Mooney, Scully, Jones, &amp; Smeaton, 2006; Ren, 2009; Soleymani, Cha-nel, Kierkels, &amp; Pun, 2008a, Soleymani, Chanel, Kierkels, &amp; Pun, 2008b; Smeaton &amp; Rothwell, 2009 ).
There is lack of consensus regarding the structure and manifestations of emotion. The two dominant views on emotions X  structure are the discrete and continuous approaches. Discrete emotion theorists, following Darwin X  X  work, suggest the exis-tence of six or more basic emotions (happiness, sadness, anger, fear, disgust, and surprise), which are universally displayed and recognised ( Darwin, 2005; Ekman, 1992; Ekman, 1999a, chap. 6 ). The arguments for the existence of basic emotions in-clude cross-cultural universals for facial expressions and antecedent events, and presence of these emotions in other prima-tes. Experiments in many countries, including countries isolated from media, show that people express and recognise basic emotions the same way ( Ekman &amp; Friesen, 1975 ). There is no agreement on which emotions qualify as basic, but the list typ-ically includes fear, anger, disgust, happiness, sadness, and surprise ( Ekman, 1992; Plutchik, 1980 ). Other emotions are seen as combinations of these basic emotions or as socially learned variants of these emotions, e.g., grief, guilt and loneliness are all variants of basic sadness, ( Bower, 1992 ). In the context of HCI research, the theory of basic emotion implies that: (i) emo-tional experiences can be measured on all stages of human interaction with systems, (ii) accurate translations of emotional expressions and predictions based on these translations can be made, and (iii) systems can incorporate characters depicting basic emotions that users can recognise accurately.

The continuous approach assumes the existence of two or more dimensions that describe and distinguish between dif-ferent emotions ( Barrett et al., 1999; Russell, 1994; Russell &amp; Mehrabian, 1977; Russell &amp; Steiger, 1982 ). Support for the dimensional emotion theories comes from physiological correlates of emotional stimuli, such as heart rate and skin conduc-tance levels. The first dimensional model was developed by Wundt (1904) , who applied both introspective and experimental methods to study subjective experiences. The model has been supported by other research, which revealed that people tend to perceive all kind of meaning in terms of valence (positive vs. negative) and activation (active vs. passive) ( Scherer, 2002 ).
Russell (1994) proposed the use of independent bipolar dimensions of pleasure X  X ispleasure, arousal, and dominance X  X ub-missiveness, rather than a small number of discrete emotion categories. In a dimensional taxonomy all emotion categories vary quantitatively ( Russell &amp; Steiger, 1982 ) and are mapped within a bipolar dimensional space.

Examples of studies that rely on the continuous approach include the work of Chan et al. (2005), Hanjalic and Xu (2005), and Soleymani et al. (2008a) . Chan et al. (2005) propose an approach towards the annotation of the emotional dimension of multimedia content using low-level feature analysis. Affective features extracted from multimedia audio content were ana-lysed in terms of arousal and valence, and they were combined in a two-dimensional frame to form the affective curve. Sim-ilarly, Soleymani et al. (2008a) present an approach to affective ranking of movies based on the emotions experienced by the viewers and affective characterisation of the content. Using a range of sensory data, in combination with audio-and video-based analysis, the levels of arousal and valence were estimated for each scene and cross-checked with the viewers self-assessments. Finally, Hanjalic and Xu (2005) suggest a framework of video content representation that follows the dimen-sional approach. A two-dimensional emotion space of arousal and valence was used to map the affective video content, using low-level feature analysis.
 Examples of studies that follow the discrete approach are Arapakis, Jose, and Gray (2008), Arapakis et al. (2009),
Lopatovska and Cool (2008), Smeaton and Rothwell (2009), and Ren (2009) . Arapakis et al. (2008) examined the effects of search task difficulty on users emotions. The study employed questionnaire as well as affective data, derived from automatic facial expression analysis. Both sources of evidence were presented using the discrete approach. In Lopatovska and Cool (2008) facial expression analysis was used to study digital libraries search. The observed emotions were studied using the discrete approach. Smeaton and Rothwell (2009) recorded people X  X  physiological reactions as they view films in a controlled, cinema-like environment. The annotation and affective tagging of the films was performed using emotion categories, such as fear, anger, shame, happiness, and other. Arapakis et al. (2009) applied automatic facial expression analysis to determine the topical relevance of viewed videos, based on users affective responses. The accumulated data was interpreted in terms of the seven basic emotion categories discussed earlier. Ren (2009) developed general-purpose agents that can recognise human emotion and create machine emotion. The user emotions were observed using a range of sensory information (brain waves, speech analysis, facial expressions, etc.). The affective information derived from the facial expressions was again analysed in terms of seven basic emotion categories.

It is our belief that none of the reviewed theories stands out as significantly better or worse than the others. The theories describe different aspects of emotion, and attempt to explain its different aspects. Since all theories have their strengths and weaknesses and none have been recognized as dominant in their native field, we leave it up to the reader to pass the judg-ment on their individual merits and applicability to the information research domain.

The next section will review some of the methods of emotion inquiry that were developed based on the theories de-scribed above. 3. Methods of emotion research
Considering the countless definitions of emotion, one should not expect a single standard method of emotion measure-ment. This section reviews methods that are currently used by the disciplines that study emotions.

Scherer (2005) suggests that due to the component nature of the phenomenon only the assessment of all components involved can offer a comprehensive and accurate depiction of an emotion episode. This suggestion entails that in the ideal study, all of the following emotion components should be measured: (i) changes in the appraisal processes (at all levels of the nervous system), (ii) responses produced in the neuroendocrine, autonomic, and somatic nervous system, (iii) motiva-tional changes brought by the appraisal process, (iv) facial, vocal and bodily indications, and (v) nature of the subjectively experienced emotional state that relates to the above component changes. Though such accurate measurement of emotion has not been accomplished yet, significant progress has been made in measuring its individual components. We will review neuro-physiological signal processing, observer and self-report methods that are used in the studies of emotional experi-ences ( Larsen &amp; Fredrickson, 1999 ). 3.1. Neuro-physiological signal processing methods
One of the emotion components is physiological arousal, which corresponds to the physiological changes (e.g., respiratory and cardiovascular accelerations and decelerations, muscle spasms, etc.) that often occur during emotion episodes. Neuro-physiological methods involve monitoring body responses to emotional stimuli. Researchers can infer the presence of emo-tion by collecting brain activity images, pulse rate, blood pressure or skin conductance readings. The procedures for collect-ing neuro-physiological measures vary between a simple sensor on a finger for monitoring pulse rate and skin conductance to more invasive sensors, such as electrocardiograph (ECG), blood pressure monitoring and electroencephalogram (EEG).
Proponents of the neuro-physiological methods argue that, while this approach requires the use of physical sensors, the sen-sors do not invade user X  X  privacy and can capture short-term changes not measurable by other means ( Scheirer, Fernandez,
Klein, &amp; Picard, 2002 ). Another benefit of the neuro-physiological measures is the detection of responses that cannot be cap-tured by any other sensory channels ( Bamidis, Papadelis, Kourtidou-Papadeli, Pappas, &amp; Vivas, 2004 ).

The method is criticised for limiting participants X  mobility and causing distraction of emotional reactions. Natural ageing and unanticipated changes in physiological characteristics (due to accidents or surgery) can introduce noise in the measure-ment of neuro-physiological signals ( Chandra &amp; Calderon, 2005 ). Additional limitations include the inability to map neuro-physiological data to specific emotions (e.g., frustration), difficulties in translating temporal micro-resolutions (milliseconds) to temporal units relevant to emotional responses and reliance on non-transparent measurement instruments (e.g., sensors that constrain movements) ( Bamidis et al., 2004 ). In addition, such methods require special expertise and the use of special, often expensive, equipment.

An example of a neuro-physiological instrument for detecting emotions is a LifeShirt sensor system that can be used for monitoring cardiovascular, respiratory, metabolic and other physiological effects of physical or emotional stress ( Wilhelm,
Pfaltz, &amp; Grossman, 2006 ). The system collects comprehensive set of physiological measures, is wearable and relatively unobtrusive. The system might help to identify specific emotion signatures and accurately infer affective states from phys-iological signals (especially when the system is individually calibrated). Picard, Vyzas, and Healey (2001) used a combination of neuro-physiological and other methods to classify a single subject X  X  emotional reactions to eight emotional stimuli over the period of time based on facial muscle tension, blood volume pressure, skin conductance, and respiration data. Partala,
Surakka, and Vanhala (2006) detected positive and negative emotions by using electrodes to capture activity of two facial muscles. Partala and Surakka (2003) investigated pupil size dilation during auditory emotional stimulation. The findings indicated that positive and negative sounds caused participants X  pupils to dilate, while neutral sounds did not impact the pupil size. Scheirer et al. (2002) investigated user X  X  physiological and behavioural changes associated with frustration and found that blood volume pressure decreased, while skin conductivity and number of mouse clicks increased during frustrat-ing episodes. Mooney et al. (2006) used a range of peripheral physiological metrics (galvanic skin response, skin tempera-ture, and other) to examine the role of searchers X  emotional states in the process of data indexing during the search process. The study provided evidence in favour of using physiological data processing for studying searchers X  emotions. More specifically, it demonstrated that we can observe measurable features in response to events in movies and within computer mediated tasks. 3.2. Observer methods
In addition to neuro-physiological measures, observer methods offer a way to study emotions through the observation of facial, vocal and gesture cues to emotional stimuli.

Research indicates that emotions are primarily communicated through facial expressions rather than bodily gestures ( Ek-man &amp; Friesen, 1975 ). Facial cues (smiles, chuckles, smirks, frowns, etc.) are an essential aspect of social interaction ( Russell et al., 2003 ); help to clarify the focus of attention ( Pantic et al., 2000a ) and regulate human interactions with the surrounding environment ( Fig. 1 ). Most of the studies done in the area of emotion recognition from facial expressions have been largely inspired by Darwin X  X  pioneering work ( Darwin, 2005 ). More recent research on recognising emotions through facial expres-sions was conducted by Paul Ekman. Ekman X  X  classification system of discrete emotion categories ( Ekman, 1999a, chap. 6; Ekman, 1999b, chap. 16; Rosenberg &amp; Ekman, 1993 ), as well as his work on the Facial Action Coding System (FACS), have provided a good foundation for automatic extraction and validation of emotional cues through the analysis of users X  facial expressions.

Facial expressions are the result of facial muscle contractions, which induce movements of the facial skin and temporary deformations of the facial features, such as eyebrows, nose, and mouth. FACS is based on recognising facial expressions of six universally distinguished emotions: fear, surprise, sadness, happiness, anger, disgust, and their combinations. The intensity of the emotion can be determined indirectly by the presence and degree of changes in all facial regions associated with it. For example, sadness is usually expressed through the brow, eye and mouth areas. In sadness, the inner corners of brows are drawn up, skin below the eyebrow is triangulated with the inner corner up, upper eyelid inner corner is raised, corners of lips are down or the lip is trembling ( Ekman, 2003 ).

The benefits of the FACS method include: (i) high reading accuracy rates, (ii) use of non-obtrusive and common laboratory equipment, such as video camera, and (iii) high validity that is confirmed by correlations with physiological measures (e.g., increased heart rate that coincides with surprise and disgust). Another benefit of FACS is the fact that it can be programmed into computer systems to automatically recognise user emotions ( Cohn &amp; Kanade, 2006; Picard, 1997 ).

Most of the automatic facial expression analysis systems directly interpret the observed facial expressions and classify them into 6 basic emotion categories ( Essa &amp; Pentland, 1997; Hong, Neven, &amp; von der Malsburg, 1998; Kimura &amp; Yachida, 1997; Lanitis, Taylor, &amp; Cootes, 1997; Wang, Iwai, &amp; Yachida, 1998; Zhang, Lyons, Schuster, &amp; Akamatsu, 1998 ). Some sys-tems describe facial expressions by noting facial muscles relative positions (vector space) without assigning labels ( Black &amp; Yacoob, 1997; Pantic et al., 2000b ). Since most of the automatic emotion analysers have been designed using static images of faces without facial hair or glasses, taken under good illumination conditions during extreme emotional episodes, they cannot draw accurate inferences on observed emotions that are expressed during longer episodes, which occur in more nat-uralistic settings. Additional limitations include the inability to perform a context-dependent interpretation of the observed facial expressions ( Jaimes &amp; Sebe, 2007 ). This suggests that, unless the emotion stimuli or current focus of attention is iden-and Luettin (2003) argue that facial expression recognition should not be confused with human emotion recognition since facial expressions can also be influenced by non-emotional mental and physiological activities. Furthermore, the face is one of the many channels (e.g., voice, gesture, gaze) that reveals emotion. The authors emphasize that emotion recognition re-quires interpretive analysis of the context and understanding of the situation in which emotion occurs. For a more detailed discussion on the facial expression analysis methods see Fasel and Luettin (2003), Jaimes and Sebe (2007), and Pantic and Rothkrantz (2003) .

Several studies that investigated the role of emotions in online search behaviour used facial expression analysis method for obtaining emotion readings. In Arapakis et al. (2009) the authors applied real-time facial expression analysis to inform the design of the video recommender system and suggest meaningful recommendations of unseen videos. While investigat-ing relationships between search task difficulty and emotions, Arapakis et al. (2009) used facial expression analysis, among other sensory channels, to aggregate information on user affective behaviour and develop models capable of determining topical relevance of documents and videos, without the aid of explicit judgments. Automatic facial expression analysis was also applied in a study of Google search ( Lopatovska, 2009a ). The author found that during the search, surprise was the most frequently expressed emotion, followed by neutral, sad, fear and happy. The study also found that specific search behaviours, such as left mouse clicks or wheel scrolls up, were associated with unique patterns of emotional expressions pre-ceding and following these behaviours. Facial expression analysis was used in a study of digital libraries X  search Lopatovska and Cool (2008) . The authors found that during the search most of the positive emotions corresponded with the time when an assistant entered the room. The study also found a wide variation in individual levels of emotional expressivity (e.g., one subject X  X  face expressed 57 intense emotions, while other subject X  X  face expressed only 9 emotions during the same period of search time).

Observer methods for studying emotions also include analysis of verbal communication. The empirical investigation of the effect of emotions on voice begun in the early 20th century ( Mendoza &amp; Carballo, 1999 ). Since then, scientific interest in the vocal attributes of moods, affective and cognitive states has increased ( Scherer, 2003 ). In recent years, research on vo-cal expression patterns of the naturally occurring emotions has produced a substantial number of theories and models of speech communication. Brunswik X  X  functional lens model of perception is an example of one such model ( Mitroff, 1974 ).
The model suggests that vocal communication of emotion is initiated with an encoding, or expression, of an emotional state by certain voice and speech characteristics, which are susceptible to objective signal measurement. The assumption made here is that that there are some acoustic correlates of emotion in the acoustic parameters (e.g., respiration, phonation) that can provide insightful cues about the speaker X  X  affect state.

According to Pantic and Rothkrantz (2003) , the auditory features that are most often extracted from the speech signal are: (i) pitch, (ii) intensity, (iii) speech rate, (iv) pitch contour, and (v) phonetic features. Pitch corresponds to the rate at which vocal cords vibrate and determines the frequency of the acoustic signal, while intensity refers to vocal energy. Variations in voice pitch and intensity usually have a linguistic function, such as over-stressing or under-stressing certain words ( Cowie et al., 2001 ). When, for example, a person is experiencing anger, fear or joy, the sympathetic nervous system becomes aroused, resulting in a heart rate and blood pressure increase that produces mouth dryness and occasional muscle tremors.
Speech is then characterised by loudness, increased speech rate and strong, high frequency energy ( Breazeal, 2001 ). Speech rate represents the number of spoken words within a time interval. Finally, pitch contour corresponds to pitch variations described in terms of geometric patterns, and phonetic features of all types of sounds involved in a speech (e.g., vowels, consonants and their pronunciation). Table 1 illustrates correlations between emotion and acoustic parameters from the Murray et al. (1993) review.
 Systems that analyse speech, along with other representations of affect, are described in Corradini, Mehta, Bernsen, and Martin (2003), Go, Kwak, Lee, and Chun (2003), Schapira and Sharma (2001), Schuller, Lang, and Rigoll (2002), Song, Bu, Chen, and Li (2004), Sebe, Bakker, Cohen, Gevers, and Huang (2005), Schuller, Arsic, Wallhoff, and Rigoll (2006), Yoshitomi,
Kim, Kawano, and Kilazoe (2000), and Zeng et al. (2006) . These systems achieved an accuracy rate of 72 X 85% when detecting one or more basic emotions from noise-free audiovisual input ( Jaimes &amp; Sebe, 2007 ). These accuracy rates outperform the equivalent human emotion recognition skill that achieves an accuracy rate of 55 X 70% in neutral content speech ( Pantic &amp; Rothkrantz, 2003 ).

Despite the high accuracy rates in detecting emotions from the vocal stream, the method is not extensively used due to a number of limitations. Jaimes and Sebe (2007) list four limitations of the existing vocal affect analysers: (i) singular classi-fication of input audio signals into a few discrete emotion categories, (ii) context-independent analysis of the input audio signal, (iii) analysis of vocal expression information only on short time scales (thus inferences about moods and attitudes are almost impossible to obtain), and (iv) assumptions about the quality of the test data (noise-free recordings, short sen-tences with intermediary pauses, clear speech, etc.).

Examples of the studies that used audio data to infer presence and quality of emotions can be found in Chan et al. (2005), and Hanjalic and Xu (2005) .In Chan et al. (2005) , a set of affective features was extracted from multimedia audio content and was annotated using a set of labels with predetermined affective semantics. The audio features that consisted of speech, mu-sic, special effects and silence, were analysed in terms of the affective dimensions of arousal and valence. Similarly, in Han-jalic and Xu (2005) the authors modelled video content using a selection of low level audio (signal energy, speech rate, inflection, rhythm duration, voice quality) and visual features (motion). The framework used in the study was based on the dimensional approach to emotion, where video content represented a set of points in a two-dimensional affect space (arousal and valence) and reliably depicted expected transitions of viewers emotional perception during the video.
Whether body movements or gestures are indicative of specific emotions is a subject under debate. Some studies suggest that the latter are indicative of the intensity of emotion, but not its type. Other studies ( Boone &amp; Cunningham, 1998; de Mei-jer, 2005; Wallbott, 1998 ) provide evidence that associate certain body movements with specific emotions. This approach follows Darwin X  X  view of human beings X  genetic predisposition to exhibit certain patterns of bodily movements during the expression of affect states ( Darwin, 2005 ). Body movements, and specifically hand gestures ( Chen, Fu, &amp; Huang, 2003;
Castellano, Villalba, &amp; Camurri, 2007; Caridakis et al., 2007 ), have recently attracted the attention of the HCI community ( Ambady &amp; Rosenthal, 1992 ). Gunes and Piccardi (2007) fused facial expressions and body gestures for bimodal emotion rec-ognition. In their study they provided a list of expressive gestures and their correlations to the emotion categories. A list of emotions recognised by the changes that occur in the body are presented in Table 2 .

Most of the recent work in hand gesture recognition can be grouped into: (i) glove-based, and (ii) vision-based ( Chen et al., 2003 ). Glove-based gesture recognition requires the user to wear an unobtrusive hand glove device, which communi-cates gestures to a computer system through a set of wires. This approach is based on the 3-D spatial description of hands.
Vision-based gesture recognition method relies on appearance of hands in images and applies appearance-based techniques, while glove-based recognition applies model-based techniques for gesture analysis. McAllister, McKenna, and Ricketts (2002b) outlined the major difficulties in using gesture recognition techniques, which are related to the following factors: (i) the hand X  X  jointed physical structure often results in self-occlusion, which makes it harder to model, (ii) many gesture recognition applications cannot track hands under poorly controlled and varying lighting conditions, (iii) tracking both hands at the same time demands a solution to the temporal matching (data association) problem, and a method for dealing with the temporary occlusion of one hand by the other. Finally, gesture recognition can be hindered by clothing or other objects. Examples of the studies that used gesture recognition include Brewster, Lumsden, Bell, Hall, and Tasker (2003), Chen et al. (2003), Camurri, Lagerl X f, and Volpe (2003), Gunes and Piccardi (2007), Isbister, H X  X k, Sharp, and Laaksolahti (2006),
Kapoor, Picard, and Ivanov (2004), Kaliouby and Robinson (2004), Kirishima, Sato, and Chihara (2005), Lics X r and Szir X nyi (2005), McAllister, McKenna, and Ricketts (2002a), Morency and Darrell (2006), Ng et al. (2002), Morency, Sidner, Lee, and Darrell (2007), Tsalakanidou, Malassiotis, and Strintzis (2007) .

HCI and IR research offers a unique observer method of inferring emotions from interactive behaviours captured in com-puter log files. Kapoor, Burleson, and Picard (2007) used log data to predict frustration. Fox, Karnawat, Mydland, Dumais, and
White (2005) found a correlation between certain search behaviours, such as the time spent on the search result page and number of result pages visited, with searchers satisfaction and dissatisfaction. Lopatovska (2009a) correlated various search behaviours with emotion and mood data and found association between certain types of clicks, number of search activities, search session duration, and happy expressions. Use of a log file data for inferring emotional states can be useful in designing systems that are capable of recognising, and possible, responding to emotions. However, we feel that this research area is still in its infancy and more work is needed to verify prior findings and find new correlations between information behaviours and emotions. 3.3. Self-report methods
While physiological response patterns and expressive behaviour can be observed and used to infer the affective state of a person, self-report methods rely on simply asking participants to describe the nature of their experience. The self-report methods rely on the assumption that individuals are able and willing to recognise and report their emotions. The reliability and validity of the measures are evident from the high correlations of self-reports with the quality of the physical stimuli and neurological activities of the brain ( Kahneman, 2000 ). Momentary reports are considered the most accurate; however, the accuracy of retrospective reports can also be improved by the use of special techniques. While they may be subject to par-ticipant X  X  bias, self-report methods are efficient and easy techniques for obtaining emotion data. We will review several self-report methods in more detail.

The two major self-report methods are based on the discrete and dimensional approaches described in the previous section. The discrete approach relies on the semantics-based categories that correspond to unique emotion patterns. In most cases, a list of emotion terms is provided to a respondent who must determine which term better describes his/ her emotional experience, rate the intensity of emotion and, finally, state how long that emotion has been experienced.
While ensuring efficiency and standardisation of data collection, discrete emotion self-reports have several disadvantages, including: (i) the possibility that one or several response alternatives may bias the respondent to choose them, (ii) the situation when a respondent wishes to refer to a category that is not provided on the list, or (iii) the situation when a respondent may be unfamiliar with the labels chosen by a researcher ( Scherer, 2005 ). Russell and Steiger (1982) argue that, when using natural language, people classify emotions by means of a taxonomy, but cannot explicitly describe it.
Thus, the taxonomy is implicit in the same sense that the syntactic rules of language are implicit. Lastly, while the infor-mation obtained from this approach appears to be intuitive and easily interpretable, there are issues of comparability of results between studies that employed different lists of emotion labels. Examples of the studies that rely on the discrete emotion include Klein, Moon, and Picard (1999), and Scheirer et al. (2002) who investigated the effects and manifestations of frustration, a discrete emotion.
 The use of dimensional approach for describing emotional states, was established by Wilhelm Wundt ( Scherer, 2005;
Sander, Grandjean, &amp; Scherer, 2005 ) who suggested the use of a three-dimensional space formed by the valence (positive X  negative), arousal (calm X  X xcited), and tension (tense X  X elaxed) dimensions to describe emotion. Given this approach, a respondent can report his/her subjective experience by simply indicating emotion X  X  coordinates in the three-dimensional space. Due to the difficulty of consistently identifying a third dimension from arousal or excitation, researchers often apply only two of the three dimensions, thus forming a two-dimensional surface (arousal X  X alence space). This approach is quite straightforward, simple and provides interval data that can be readily used in statistical processing ( Russell &amp; Steiger, 1982 ). However, the results lack the intuitiveness of the discrete emotions approach and are limited to degrees of positive or negative valence or arousal ( Fig. 2 ). Another shortcoming of the dimensional approach is ambiguity. For example, it is often not clear whether a valence judgement is indicative of the appraisal of the emotional stimulus or a feeling induced by it. Most importantly it is very difficult, if not impossible, to distinguish the intensity of an emotion from bodily excitation. As a result, extremely intense happiness may be characterised by high arousal, while intense sadness may be accompanied by very low arousal. Peter and Herbon (2006) advocate the use of dimensional approach in HCI research and investigate emotions in at least 2-dimensional spaces: arousal and valence. The authors suggest adopting this view for the use in HCI, since it allows an automated classification of different emotional states within arousal X  X alence space without labelling them. Example of the study that used dimensional approach can be found in Partala and Surakka (2004) , who investigated effects of pre-pro-grammed positive and negative interventions in human X  X omputer interaction.

To mitigate disadvantages of self-report methods, researchers often choose to use a free-response report, which allows participants to express experienced emotions using words or expressions that best represent their experiences. This tech-nique provides a high level of specificity, which can be useful in studies where accuracy and explicitness are considered important. However, according to Scherer (2005) it is difficult to analyse free-response data in a quantitative, statistical man-ner. Such data collection techniques, including journals, think-aloud protocols and interviews are popular in LIS research.
One of the earliest studies that identified searchers X  emotional states during the information search process was Kuhlthau (1991) research on students X  information seeking behaviour. The emotion data was collected by asking participants to record their feelings and thoughts related to information seeking in a journal. Analysis of the data collected through journals and questionnaires led to development of the six stage information seeking model that identified relationships between the search process, participants X  feelings, thoughts, and actions. Meghabghab (1995) observed inexperienced school librarians learning to search online databases. Subjects were asked to document their actions, feelings and thoughts on log sheets and work activity sheets. In the study that examined developmental steps of acquiring expertise with the search engine ( Nahl, 1998 ), participants were asked to keep logs of their cumulative searches and provide weekly self-ratings on satisfac-tion scales. James and Nahl (1996) examined the semester-long affective development of senior college students who were learning to use the internet. The students were asked to record their cognitive and affective information processing activities in  X  X  X elf-witnessing X  reports. Affective states experienced by children and graduate students X  during online search were com-pared in a study by Bilal and Kirby (2002) . Self-report data was collected through journals (for students) and interviews (for children).
 Think-aloud methods were used in several LIS studies that investigated the role of affective variables in search behaviour.
Nahl and Tenopir (1996) studied searching behaviour of novice database users by recording their think-aloud reports, including interactions with the study monitor, and using screen logging software to record their search activities. Wang and Soergel (1998) examined document selection criteria, including evaluations of document X  X  emotional values. Partici-pants, 25 self-selected faculty members, were asked to think aloud while selecting documents. Tenopir, Wang, Zhang, Sim-mons, and Pollard (2008) observed how academic users interact with the ScienceDirect system by collecting think-aloud protocols capturing participants X  affective and cognitive verbalisations.

Another popular technique of studying affect in LIS is interview. In most of the reviewed above studies, interviews with participants were conducted before and after participants X  engagement in a search activity. While most of the studies con-ducted one-on-one interviews, a few studies used group interviews to collect data on users X  emotional experiences. In the longitudinal study of uncertainty involved in information seeking, Wilson, Ford, Ellis, Foster, and Spink (2002) administered pre-and post-search interviews. Bilal and Bachir (2007) conducted individual pre-search interviews to generate children X  X  profiles, including their demographic information, prior experience, reading habits and preferences. Participants X  affective states were captured in the exit one-on-one interviews in several studies of children X  X  use of a search engine ( Bilal, 2000;
Bilal, 2002; Bilal &amp; Kirby, 2002 ). Julien (2007) examined library customers X  experiences with internet public stations using interview data. In a study of anxiety and perception of research, Kracker (2002) used the critical incident technique that re-quired students to recall specific research assignments and describe their feeling and thoughts associated with the research process.

Other self-report methods for investigating affective variables include questionnaires. Pre-and post-search question-naires about users X  affective states are frequently used in LIS research in conjunction with other methods in the above-men-tioned research of students X  information seeking behaviour ( Kuhlthau, 1991 ), participants completed questionnaires about their perception of the six areas of library use in addition to keeping journals of their information seeking experience. Mentis (2007) examined memories of frustrated search experiences by administering open-ended online questionnaire were partic-ipants were free to define and describe their frustrated experiences.

Standardised tests for measuring affect are also frequently used in LIS research. Lopatovska (2009b) reported using Posi-tive Affect and Negative Affect Scale (PANAS) to measure searchers X  affect between search tasks. The PANAS ( Watson, Clark, &amp; Tellegen, 1988 ) comprises of two 10-item scales that measure positive affect (extent to which a person feels enthusiastic, active, alert, etc.) and negative affect (extent to which a person experiences subjective distress, including anger, contempt, disgust, guilt, fear, nervousness, etc.). The study showed that affect did not change significantly during the course of the search and is not significantly influenced by the search process. A study that investigated subjective variables of the infor-mation search process found that better mood before the search and during the search correlates with better mood after the search, but also correlates with the worse search outcomes and lower satisfaction, suggesting that, perhaps, it pays off to feel some  X  X  X ain X  during the search in order to  X  X  X ain X  quality outcomes ( Gwizdka &amp; Lopatovska, 2009 ).

In a comprehensive study of web use Wang, Hawk, and Tenopir (2000) asked 24 graduate students to fill out a pre-search questionnaire identifying their web experience, the State Trait Anxiety Inventory (STAI, forms Y1 and Y2) to measure affec-tive states and the Embedded Figure Test to measure cognitive styles. STAI consists of two forms: S-anxiety, which measures individual general tendency of feelings, and T-anxiety, which measures individual X  X  current feelings. High scores on the tests indicate high levels of anxiety, the scores range from a minimum of 20 to a maximum of 80. Kracker (2002) researched stu-dent anxiety and perceptions of research using State Trait Anxiety Inventory test (STAI Y-1) and the critical incident tech-nique that required students to recall specific research assignment and describe their feelings and thoughts associated with the process. Form STAI Y-1 was used to clarify statements given by participants about their most memorable or the most recent research assignments.

In a study that examined relationships between search performance, anxiety levels and research achievement ( Onwuegbuzie et al., 2004 ) student participants were asked to fill out several questionnaires prior to engaging in the search task. The questionnaires were designed to collect data about participants X  emotional states, and included Library Anxiety
Scale, Hope Scale, Procrastination Assessment Scale, Multidimensional Perfectionist Scale and others. In the study of effects of emotion control on the web search behaviour and performance ( Kim, 2006 ), participants were asked to take a Problem-Solving Inventory test prior to engaging in the search tasks.

In a study of affect measurement tools for the design of haptic interfaces, Swindells, MacLean, Booth, and Meitner (2006) presented the results from two experiments, where self-reports and physiological measurement techniques were compared and contrasted. Self-reports produced better results than the biometric measures. However, the authors argue that this could be attributed to a very subtle changes in affect experienced by users during the studies.

This section outlined various methods of emotion research. It also provided examples of the studies that used described techniques to investigate the role of emotion in various information use contexts. All the reviewed methods have advantages and disadvantages (see Table 3 ), and should be chosen based on the study objectives. For example, studies that are mainly interested in the users X  perceptions and explanations of the felt emotions should consider self-report methods; studies that are interested in affect-based systems that are capable of capturing human emotions should consider neurophysiological or observer methods that can be detected automatically. A researcher should always consider using several methods in order to: (i) ensure the accuracy and consistency of the collected data, (ii) increase reliability of findings, and (iii) create a compre-hensive representation of the users X  affective states. 4. Information systems and emotions
A lot of what we know about emotions in HCI is attributed to the research done under the umbrella of  X  X  X ffective com-puting X . Rosalind Picard (1997, p. 3) defined affective computing as  X  X  X omputing that relates to, arises from, or deliberately influ-ences emotions  X . The affective computing agenda includes giving a computer the ability to recognise and intelligently respond to humans emotions ( Hudlicka, 2003; Picard &amp; Klein, 2002; Picard, 2003 ).

A number of HCI articles describe efforts to design  X  X  X ffective computers X , or systems capable of recognising and respond-ing to users X  feelings. Klein, Moon, and Picard (2002), and Scheirer et al. (2002) proposed prototypes of interactive computer systems that can detect and respond to certain human emotional expressions. The design of such interactive systems is based on the assumptions of the Media Equation theory ( Reeves &amp; Nass (1996) ), as cited in Klein et al. (2002) . The theory suggests that people exhibit a propensity for interacting with machines as if they were other people, and respond to computer X  X  praise and criticism in the same manner they respond to praise and criticism of other people.

Not everyone agrees with the idea of giving computers the power to respond to human emotions. Several studies outline the benefits of giving users means of changing computer interfaces in order to improve emotions. Blom and Monk (2003) suggests that the reasons users personalise their computers include emotional aspects, such as improved feelings of control, ownership, fun, and relieve from boredom. Tractinsky (2004) reviewed the studies that showed the link between persona-lised aesthetic interfaces and improved perceptions of usability and satisfaction. However, the link between personalisable interfaces and improved emotions can be questioned in light of findings reported by Ward and Marsden (2004) . The authors did not find statistically significant differences between user X  X  physiological reactions on well or poorly designed websites, which might suggest that aesthetic differences between systems might not sufficiently impact user emotions.
 Klein et al. (2002) described an experiment that simulated a computer game in which users were intentionally frustrated.
The experiment showed that participants who received supportive messages from the system were able to better recover from negative emotional states and chose to stay in the game longer. The authors suggest that while it is not always possible to build systems that do not cause frustration, it is possible to design computers that try to mitigate the effects of frustration.
Role of frustration in HCI was also investigated by Scheirer et al. (2002) who frustrated users by occasionally  X  X  X reezing X  mouse movements. The study found correlation between blood volume pressure, skin conductivity, number of mouse clicks and frustrating episodes. The findings suggest practical ways of designing systems capable of recognising user X  X  affective states.

Klein et al. (2002), and Scheirer et al. (2002) research is discussed in several follow up articles. Lindgaard (2004) suggests that addressing immediate negative emotions might be detrimental to achieving long-term goals. The author outlined the difficulties in determining causal relationships from behavioural observations (participants who received sympathetic mes-sages from a system in Klein et al. (2002) study might have chosen to work with a system longer out of curiosity, and not because of a sympathetic message).

Muller (2004) also critiques Klein et al. (2002), and Scheirer et al. (2002) studies for the misuse of hypothesis-testing methods. In Scheirer et al. (2002) , the authors suggest that the change of physiological measures was attributed to changing levels of frustration, while in fact it can only point to the changing levels of arousal. The author suggests expanding affective computing research to include ethnographic research methods and draws parallels between HCI and human emotional attachments to other social objects (e.g., cars and boats). Oatley (2004) suggests moving from system X  X  acknowledgement of user frustration to system X  X  attempt to repair frustration X  X ausing problems. One of the hypothetical ways of doing that would be soliciting user X  X  feedback, improving systems and giving improved products back to users for free.

Several studies have tested systems that communicate affect by incorporating emotionally expressive computer agents (e.g., emoticons and avatars). Tzeng (2004) examined effects of apologies for the failed computer game expressed through text and emoticons. Apologetic feedback and emoticons were found to enhance the aesthetics of game interaction and short-en the psychological distance between the game and the player. Apologetic (sad) emoticons were found to communicate emotions more effectively than pure text.

Brave, Nass, and Hutchinson (2005) examined effects of emotionally expressive computer agents on user X  X  perception of computer game. The study used the game of black jack where dealers were represented by a human photograph and a blob text. The nature of the photograph and a text represented neutral, sad and happy emotions directed towards participants X  wins or looses. Empathetic agents were perceived as likeable, carrying, trustworthy, supportive, and overall had positive ef-fect on participants. The authors suggest modelling affective computers after people in service roles who are trained to ex-press happiness and empathy regardless of their actual feelings.

Kleinsmith, De Silva, and Bianchi-Berthouze (2006) examined cross-cultural differences in recognising affect from com-puter animated avatars. Avatars expressing fear, anger, happiness and sadness were designed based on the body postures of predominantly Japanese and a few Sri Lanka and Caucasian American models. Participants recruited from the same ethnic groups were asked to recognise emotional expressions from the avatar postures. Three culture groups indicated medium le-vel of agreement in judging emotions from avatar postures. Significant differences were found between groups X  judgements of emotional intensity. The findings point to the challenges in designing universally recognised computer representations of emotions.

A number of studies focused on the use of emotion data for improving information systems. In Arapakis et al. (2009) , the authors presented a novel video search environment that applies real-time facial expression analysis to aggregate informa-tion on users X  affective behaviour. The collected information was used to classify the topical relevance of the perused videos and, additionally, enrich the user profiles. The benefits of such system include combination of different modalities (facial expression data, interaction data, etc.), integration of affective features into the employed profiling techniques, and, finally, facilitation of meaningful recommendations of unseen videos.

Mooney et al. (2006) examined the role of searchers X  emotional states in an attempt to improve data indexing for and within the search process. Users physiological responses to emotional stimuli were recorded using a range of biometric mea-surements (GSR, skin temperature, etc.). The study provides initial evidence that users exhibit measurable biometric behav-iour when watching movies and engaging in interactive tasks. It also examines how this data can be used for affective indexing of data within the search process.

In Soleymani et al. (2008a) the authors proposed an approach to affective ranking of movie scenes, which is based on viewers X  affective responses and content-based features. The study found that important events that characterise every scene are correlated with the viewers X  self-assessed arousal and valence. Furthermore, the provided evidence suggests that periph-eral physiological signals can be used to characterise and rank video content.

A number of IR studies are focusing on emotions as descriptors of information and are forming a new area of emotional information retrieval research. Certain information objects, such as music and images, induce emotions that help to describe and ultimately retrieve these objects. Lee and Neal (2007) developed a system for tagging emotions and their intensity in-duced by music. Schmidt and Stock (2009) used similar method for tagging images using basic emotions X  tags: anger, disgust, fear, happiness and sadness. The study findings suggest that collective emotion tagging can provide a new way to describe and retrieve information.

Reviews of additional studies related to the design of affective computers can be found in Brave and Nass (2003), Hudlicka (2003), Picard (2003), Pantic, Sebe, Cohn, and Huang (2005), and Eckhardt et al. (2009) . 4.1. Emotions in online searching
This section describes several studies where emotions were examined in the context of online searching. The included studies landed themselves into the two categories: (i) studies that investigated causes of various emotions experienced dur-ing the search, and (ii) studies that investigated effects of emotions on search behaviours.

A number of studies that focused on exploring the causes of certain emotions experienced during the online search found that positive emotions were usually associated with satisfactory search results ( Tenopir et al., 2008 ), successful search com-est in the process and documents ( Kracker, 2002; Kracker &amp; Wang, 2002; Lopatovska &amp; Mokros, 2008 ), or documents X  stylistic features ( Lopatovska &amp; Mokros, 2008 ). Negative emotions were associated with frustrating aspects of systems, uncertain search tasks and confusing search strategies ( Tenopir et al., 2008 ), software failures ( Bilal, 2000 ), uncertainty prior 2002; Meghabghab, 1995 ). We will focus on a few studies that investigated the causes of positive and negative emotions in more detail.

While examining relationships between affective and cognitive variables during the online search, Tenopir et al. (2008) showed that positive feelings were reported more frequently than negative feelings, and were associated with the thoughts about search results. Negative feelings co-occurred more often with the thoughts related to system, search strategy and task.
Similar findings were reported in the study of children X  X  use of a search engine ( Bilal, 2000 ). The study found that young par-ticipants identified more positive than negative feelings during the search. Positive feelings were associated with the use of a system in general, and ease of use, keyword search option, availability of graphics and fun in particular. Negative feelings of confusion and frustration were associated with software failures. However, negative feelings did not have significant impact on children X  X  persistence and patience in web searching.

An investigation into the role of emotions in the information seeking process ( Arapakis et al., 2008 ) has provided evidence of the effect of the search task on users X  emotions. More specifically, the findings indicated a progressive transition from po-sitive to negative valence as the degree of task difficulty increases.

In the study of children X  X  interaction with the International Children X  X  Digital Library, Bilal and Bachir (2007) discovered that positive feelings were associated with the use of the digital library in general and easiness of use and effective naviga-tion in particular. Negative feelings were associated with the limited size of collection and uncertainty prior to searching the system. Bilal and Kirby (2002) compared the internet search behaviour of adults and children and showed that while adults (graduate students) performed the search tasks more effectively and efficiently, they experienced the same feelings as young searchers. Both groups experienced satisfaction and comfort with the successful completion of task, and frustration due to difficulties in finding the answer and inadequate knowledge of a system.
 Kracker (2002), and Kracker and Wang (2002) focused on the effects of educating students about Kuhlthau X  X  Information
Search Process (ISP) model. Authors discovered that emotions related to anxiety, uncertainty and difficulty were mentioned more frequently than positive emotions related to confidence and positive perceptions of the process. Positive emotions were mostly correlated with the interest in the process and documents. The findings of this study are not consistent with
Tenopir et al. (2008), and Bilal (2000) , who found that participants reported more positive than negative feelings. Inconsis-tencies in findings might point to the lack of comparable definitions and methods as well as the need for further research.
Lopatovska and Mokros (2008) asked searchers to rate their feelings about individual web documents they reviewed. The authors found that positive and negative feelings were caused primarily by documents X  stylistic properties (e.g., easy to read), followed by the personal interest in the information contained in a document.

Studies that examined effects of emotions on various online search variables have shown relationships between emo-tions/affect and search strategies ( Nahl &amp; Tenopir, 1996 ), performance ( Nahl &amp; Meer, 1997; Nahl, 1998; Wang et al., 2000 ) and satisfaction ( Nahl, 2004a; Nahl, 2004b ). Early studies on the effects of emotions in information seeking were per-formed in a library environment. While investigating the effects of the library anxiety, Mellon (1988) found that negative emotions impeded information seeking and learning. The author suggested mitigating the negative effects of library anxiety by offering library instruction programs that are attuned to students X  emotional needs. More recently, Onwuegbuzie et al. (2004) also examined college students X  library anxiety and showed that it had a negative effect on research paper quality.
A series of studies by Diane Nahl, Carol Tenopir, Dania Bilal and others identified emotions experienced during the online search and investigated their effects on search behaviour. Nahl and Tenopir (1996) explored affective and cognitive aspects of searching behaviour of novice users and found that hesitation, desire to confirm, fear, surprise and other feelings affected search strategies. Wang et al. (2000) also examined cognitive and affective aspects of the search behaviour on the web and found reciprocal relationships between affect and search performance. The study findings showed that positive feelings sup-ported subsequent interactions while negative feelings hindered the search. The findings also indicated that successful search performance reduced negative feelings, such as anxiety.
Nahl (2004b) investigated the effects of affective variables on search behaviour and found that self-efficacy and optimism counteracted the effects of negative emotions (e.g., irritation and frustration associated with uncertainty and time pressure), and increased user support and acceptance of the system. In a study of affective motivation during on-line information search, Nahl (2004a) found positive correlation between self-efficacy and optimism, and motivation for completing the task.
The author found that higher self-efficacy and optimism were associated with higher satisfaction. The effects of self-efficacy on search behaviour were also studied by Nahl and Meer (1997) . The authors found positive correlation between students X  self-efficacy and search performance.
 Kim (2008) examined relationships between search tasks, user emotion control and performance during the web search.
The experiment involved completion of different search tasks of varying scope (specific task vs. general task), and reporting users X  expectations of problem solving. The study results indicated that both tasks and emotion control impact users X  search behaviour. The author suggested ways of releasing cognitive and affective burden on the searcher by offering information literacy education and improving interface design.

Nahl (1998) reviewed information behaviour literature covering cognitive and affective components of searching and found evidence of the effect of affective variables on search motivation, performance and satisfaction. In conclusion, the re-viewed studies found that affect influences search strategies, performance, satisfaction, motivation to continue search, acceptance and support of a system.

Table 4 summarises causes, effects and correlates of emotions examined in the mentioned above studies. 5. Discussion and conclusions
Recent scientific findings suggest that emotion is a ubiquitous element of any human X  X omputer interaction ( Brave et al., 2005 ) and should be considered when designing usable and intelligent systems ( Karat, 2003 ). Emotions not only regulate our social encounters but also influence our cognition, perception and decision-making through a series of interactions with our intentions and motivations ( Damasio, 1994; Scherer, 2001 ). This article reviewed theories of emotions and illustrated how emotions have been studied in the context of computer-related tasks.

We examined the two major categories of classical emotion theories: (i) cognitive, which stresses the importance of cog-nitive evaluation (appraisal) in establishing the meaning of stimuli and ways of copying with it, and (ii) somatic, which emphasises somatic factors and describe expressions and perceptions of emotional experiences in terms of bodily responses.
We, furthermore, discussed the two dominant views on emotion structure, namely: the discrete and dimensional. The for-mer supports the existence of six or more basic emotion categories, which are universally displayed and recognised, while the latter suggests the representation of emotions in terms of a multi-dimensional space of arousal, valence and other.
Even though no accurate and holistic measurement of emotion exists, we presented several approaches for measuring individual components of this phenomenon. The neuro-physiological methods examine the physiological changes that occur during an emotion episode. These methods require the use of special equipment and are considered accurate at detecting short-term changes that cannot be captured by other means. However, they are considered obtrusive and are prone to noise introduced by unanticipated changes in the physiological characteristics. The observer methods (facial expression and speech analysis, gesture recognition, etc.) are considered less obtrusive and do not require the use of special laboratory equipment. They can be used for the analysis of temporal changes but are also noise-prone. Finally, self-report methods rely on the assumption that a person can accurately assess and report his/her emotional experiences. This category of methods offers insights into the subjective experiences, however it has its own limitations, such as reliance on  X  X  X mperfect X  language to communicate emotional experiences. It is up to the individual researcher to weight all the advantages and disadvantages of particular methods and select one that best addresses the study objectives.

We provided examples of the studies that used various methods to investigate the role of emotions in human X  X omputer interaction, including online searching. We have also reviewed several studies that integrated emotion data in the informa-tion systems design. It is the authors X  opinion that the state of the knowledge about the role of emotions in HCI is still in its infancy. Very often, studies that investigate affective variables, such as emotions, feelings, affect or mood, do not define these concepts, which leads to the misuse of the terms and the presentation of incomparable findings. We also found lack of inter-disciplinary cross-referencing in the reviewed studies, which points to the lack of awareness about relevant research in the adjacent disciplines that study emotions. Our review tried to educate the reader about multiple emotion theories and meth-ods that have been developed to aid emotion inquiries; and survey the work that has been done to date. We feel that it is time for the emerging affective paradigm to become broader and richer, and to encourage research that reaches across the boundaries of the narrowly defined fields.
 Acknowledgements
Part of the research leading to this paper was supported by the European commission, under the contract FP6-033715 (MIAUCE Project). We would like to thank Nicholas J. Belkin and Joemon M. Jose for the fruitful discussions and encourage-ment, and our anonymous reviewers for their constructive critique and suggestions.
 References
