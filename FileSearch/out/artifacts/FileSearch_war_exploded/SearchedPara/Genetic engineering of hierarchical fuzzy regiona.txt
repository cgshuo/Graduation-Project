 ORIGINAL PAPER Christian Gagn  X  e  X  Marc Parizeau Abstract This paper presents a genetic programming based approach for optimizing the feature extraction step of a handwritten character recognizer. This recognizer uses a simple multilayer perceptron as a classifier and operates on a hierarchical feature space of orientation, curvature, and center of mass primitives. The nodes of the hierarchy represent rectangular sub-regions of their parent node, the tree root corresponding to the character X  X  bounding box. Within each sub-region, a variable number of fuzzy features are extracted. Genetic programming is used to simultane-ously learn the best hierarchy and the best combination of fuzzy features. Moreover, the fuzzy features are not prede-termined, they are inferred from the evolution process which runs a two-objective selection operator. The first objective maximizes the recognition rate, and the second minimizes the feature space size. Results on Unipen data show that, us-ing this approach, robust representations could be obtained that out-performed comparable human designed hierarchical fuzzy regional representations.
 Keywords On-line character recognition  X  Handwriting  X  Evolutionary computations  X  Fuzzy logic  X  Unipen dataset 1 Introduction Pattern recognition systems are classically modeled as a pro-cessing pipeline made up of raw input sensing, segmenta-tion, feature extraction, classification, and post-processing [ 7 ]. Except for classification where many well-known generic methods exist, all of these steps are mostly problem specific. The design of a recognition system thus requires a thorough understanding of the recognition task and, most often, involves some form of a trial and error process before adequate system performance can be reached. In particular, the segmentation and feature extraction steps are very crit-ical because no matter how powerful a classifier may be, it can never fully compensate for noisy or non-discriminating features.
 traction components of an on-line handwritten character recognition system [ 20 ], and on how the development of these two steps can be partly automated using Genetic Programming (GP) [ 3 , 14 ]. GP is a machine intelligence technique involving the simulation of natural evolution for the automatic programming of computers. It is a generic problem-solving method applicable whenever solutions can be represented by a computer program and evaluated by an objective function; the so-called  X  X itness X  function. Popula-tions of programs X  X nitially random programs X  X volve over time through a sequence of processes that include (natural) selection, crossover operations to exchange genetic mate-rial between two programs, and mutation operations to ran-domly modify parts of the evolved programs. In the end, the fittest individual (program) is chosen as  X  X he X  solution to the problem and, although GP systems do not guarantee conver-gence to an optimal solution, they have been shown in prac-tice to outperform other techniques as well as human experts for several hard problems [ 15 ].
 feature extraction module, i.e. the character representation, is possibly the most important key for the development of higher performance systems [ 29 ]. In this paper, we focus on a specific on-line representation and, using GP, we try to improve upon it with techniques that involve minimal hu-man intervention. The organization of the paper is as fol-lows. In Sect. 2 , the original  X  X uzzy-regional X  handwriting representation [ 13 ] is first summarized and previously ob-tained results are recalled for baseline comparison purposes. Results are also given in this section for a new hierarchical variation of the original uniform grid configuration. Then, Sect. 3 introduces a new data-driven approach where the hi-erarchical grid segmentation of the character is no longer static, but now depends on the spatial distribution of hand-writing strokes. Next, in Sect. 4 , the method for finding the optimal genetically engineered hierarchical representa-tion is presented, together with experimental results. Finally, Sect. 5 concludes the paper with a discussion on how its ac-tual contributions can be generalized for the design of fea-ture extraction components for other kinds of pattern recog-nition systems. 2 Fuzzy-regional representations The on-line fuzzy-regional representation [ 13 , 18 ] starts with a stroke decomposition approximated by a sequence of circular arcs, as described in [ 17 ]. A character is thus repre-sented by a sequence of circular arc strokes where each arc s i = ( p 0 , p 1 , l , c ) is described by four pa-rameters: p 0 and p 1 are respectively the starting and ending points of the arc, l is its curvilinear length, and c its curva-ture. A stroke orientation  X  is also determined from the angle of vector digits and their circular arc stroke reconstruction. The circu-lar arc segmentation algorithm is based on the local extrema and inflection points of the on-line curvature signal. 2.1 Feature extraction The feature extraction module decomposes the character into a fixed number of rectangular regions in the charac-ter X  X  bounding box. Within each of these regions, a fuzzy vector is extracted from the orientation and curvature of the strokes. Figure 2 shows the fuzzy sets that are used to quantify the segmented strokes. The four characteristic func-tions of Fig. 2 a enable the fuzzyfication of the stroke angle  X   X  X  X  180 , 180 ] , and the three functions of Fig. 2 b are for the stroke curvature c  X  X  X  X  X  ,  X  X  . A fuzzyfied stroke can thus be considered a more or less horizontal ( H ), vertical ( V ), oblique with positive slope ( O + ), or oblique with neg-ative slope ( O  X  ), depending on the stroke angle larly, a stroke can be more or less rectilinear ( R ), have a positive curvature ( C + )oranegativecurvature( C  X  ). As only one fuzzy vector is associated to each region of a char-acter, the maximum fuzzy membership value in each fuzzy set is considered when several strokes occur in the same re-gion. For example, if one region contains two strokes, one very C + (thus not very C  X  ) and the other very C  X  (thus not very C + ), then the resulting fuzzy vector will be both very C+ and very C  X  . The complete feature vector space is constructed by simple concatenation of the different re-gional fuzzy vectors. This fuzzyfication process allows the mapping of different regional characteristics extracted from a variable number of strokes into a fixed-length vector with values in the [ 0 , 1 ] X  IR interval.
 orientation is global and does not give precise information about the local orientation of the strokes within each re-gion. A refinement proposed originally in [ 13 ] is to crop the strokes at the region boundaries before computing the  X  angles. Thus, the end points used for computing this ori-entation angle are never outside the region boundaries. An-other refinement was to normalize each fuzzy membership by multiplying it with the ratio of the length of the associ-ated stroke over the diagonal length of the region. These two refinements are used throughout the paper for every experi-ment.
 new features are also extracted from each region: the hor-izontal (MX) and vertical (MY) centers of mass of the strokes after cropping. These new features are also normal-ized within the [ 0 , 1 ] interval, and represent the coordinates of a relative position in the region extent; ( 0 , 0 ) is the lower left corner of the region, and ( 1 , 1 ) its upper right corner. 2.2 Grid topologies In previous papers [ 13 , 18 ], it was proposed to divide the character bounding box into a uniform grid with, for exam-ple, 3  X  2or3  X  3 regions, as shown respectively in Fig. 3 a and b. In this paper, it is also proposed to use a hierarchi-cal grid topology, starting from the character bounding box, subdividing the region into four regions (quad tree) or five regions (quin tree), recursively. Figure 3 c shows the five re-gion decomposition for a two-level quad tree, while Fig. 3 d shows the six-region decomposition for a two-level quin tree. Three-level quad tree (RFR-quad3) and three-level quin tree (RFR-quin3) representations are also tested as standard-grid topologies. These hierarchical quad and quin tree rep-resentations are inspired from the work of Park et al. [ 19 ]. 2.3 Experimental results All classification results presented in this paper have been obtained using a MultiLayer Perceptron (MLP) classifier trained with standard on-line back-propagation and momen-tum [ 12 ], with a single hidden layer of 50 neurons. The learning rate and momentum are fixed respectively at 0 . 1and 0 . 25. Training was controlled using a cross-validation proce-dure where 67% of the learning set is used for training and 33% for validation. The MLP was trained for a minimum of 35 epochs and the total number of training epochs varied from 65 to 125. The decision strategy (the post-processing module) is simply to classify data according to the maxi-mum output of the MLP (no rejection).
 its) of the Unipen data set [ 11 ]. Training data are from Unipen Train-R01/V07, consisting in 15 953 characters, while testing data are from DevTest-R02/V02, consisting of 8 598 characters. Experiments are conducted using the com-plete data sets, except for those character samples that have zero width and zero height, as reported in [ 18 ] (four in the training set and 34 in testing set).
 engineered topologies are reported in Table 1. Experiments are conducted on the six standard grid topologies presented in Sect. 2.2 , extracting for each region the nine features de-scribedinSect. 2.1 . Thus, the feature set size for each topol-ogy is the number of regions multiplied by nine. The  X  X ean Rec. Rate X  column is the average recognition rate on the test set for ten different training runs of MLPs, the  X  X ax. Rec. Rate X  column is the best recognition rate achieved over these ten runs, and the  X  X D Rec. Rate X  is the standard deviation around the average. The  X  X ean Shift RFR-3  X  2 X  and  X  X ax. Shift RFR-3  X  2 X  columns are respectively the average and maximum recognition rate improvements over the mean rate of RFR-3  X  2 which serves as a baseline for this study. the baseline RFR-3  X  2. From this baseline representation, improvements are achieved mostly by representations with many more features (up to five times): + 0 . 25% for RFR-3 3, + 0 . 73% for RFR-quad3 and + 0 . 77% for RFR-quin3. On the other hand, the two-level quad and quin tree represen-tations show a  X  0 . 24% decrease in performance for RFR-quin2 and  X  0 . 87% for RFR-quad2. These results demon-strate that the grid topology may have an important effect on performance using such regional representations. The RFR-quin3 topology performs the best with an improve-ment of + 0 . 77%, on average, and with a best case scenario of + 1 . 06%. The RFR-quad3 performs a little worse than RFR-quin3 on average ( + 0 . 73% vs. + 0 . 77%), but this may not be statistically significant. However, it uses 90 less fea-tures than its RFR-quin3 counterpart, which is a significant advantage.
 found in the literature is not easy. In a survey compiled by Ratzlaff [ 23 ], it seems that most recognition rates reported for Section 1a of Unipen were obtained using subsets of Train-R01/V07, not with DevTest-R02/V02 which appears to be significantly harder. In reference [ 18 ], we have reported an average recognition rate of up to 96 . 9% (compared with 96 . 37% for the best result of Table 1) using similar regional representations, but with additional global features. To the best of our knowledge, the only other report for the com-plete DevTest-R02/V02 was made by Ratzlaff who obtained 98 . 1% using a different representation and a different clas-sifier [ 22 ]. But he also states in his paper that he was forced to manually label the data as he did not have access to the official class labels. 1 Thus, it is probable that this manual re-labeling of DevTest-R02/V02 has cleaned up the data to some extent, possibly correcting any mislabeling. Moreover, through a recognition experiment conducted with 10 human readers, it has been shown previously [ 13 ] that about 1% of the digits found in section 1a of DevTest-R02/V02 are totally unrecognizable. This suggests that this 1% of characters may be mislabeled.
 optimal recognition system per se, as this would probably imply the use of multiple classifiers and representations, but rather to explore different topologies in the context of region based fuzzy representations, and to experiment with GP for genetically engineering such topologies. 3 Data-driven representations The fuzzy-regional representation has several interesting as-pects, such as the mapping of an arbitrary on-line script into a fixed-length normalized feature vector. On the other hand, the use of a static grid topology may be sub-optimal, especially for deformed or slanted characters. Inspired by the work of Park et al. [ 19 ], we now study the use of data-driven hierarchical topologies where regions are recursively defined around the center of mass of strokes, instead of the absolute center of the parent region. For example, in the case of the quin tree topology, the four corners of the central region correspond to the centers of the four other regions, which are themselves defined by the center of mass of the whole character, as illustrated by Fig. 4 . For the three-level quad and quin tree representations, the position of each re-gion at the third level is computed using the center of mass of its parent region at the second level. For the data-driven 3  X  2and3  X  3 grid topologies, the width and height of each region are adjusted according to the center of mass of the character relative to its geometrical center.
 are already used to determine the region boundaries, the cor-responding features (MX and MY) are removed from the fuzzy representation, and replaced by a region height/width ratio (HW). The final representation length is thus eight times the total number of regions. 3.1 Experimental results Ta b l e 2 shows the recognition results for the data-driven rep-resentations, with experiments conducted in the same con-ditions as those exposed in Sect. 2.3 . Results show that, contrary to expectations, data-driven representations mostly tend to decrease performance, especially for the quad and quin tree topologies. This illustrates the fact that feature extraction can be counterintuitive. Only the DDR-3  X  seems to benefit a little from the data-driven approach, up to + 0 . 51% better than the baseline, and + 0 . 26% better than its RFR equivalent. Figure 5 gives a bar graph comparing the mean recognition rate shift over the baseline for the different RFR and DDR representations. 4 Genetical engineering of representations Up until now, a fixed set of predetermined hand-crafted static and data-driven topologies were tested, and we have shown experimentally that some are better than others. Also, a fixed set of features was systematically extracted from ev-ery region, sometimes generating large representations that may suffer from the well-known curse of dimensionality. Finally, the features themselves have been predefined by a fixed set of hard-coded characteristic functions. The objec-tive of this section is to establish an automatic procedure based on genetic programming to explore more diverse topologies with per region specialized features.
 use of Evolutionary Computations (EC) for feature selec-tion [ 30 ], feature space transformation [ 24 ] and feature con-struction [ 16 , 25 , 26 ]. In the context of document recogni-tion, Teredesai and Govindaraju [ 27 , 28 ]haveusedGPto design active classifiers for off-line script recognition. Their system is based on Park X  X  hierarchical representations [ 19 ], with an implicit feature selection. Another relevant work is from Radtke et al. [ 21 ] who use a multiobjective memetic algorithm to design off-line character representations in a manner similar to ours. But their approach uses genetic al-gorithms instead of genetic programming. 4.1 Evolution of representations Genetic Programming (GP) [ 3 , 14 ] is an EC technique [ 1 , 2 ] that allows automatic programming of computers by heuris-tics inspired from natural evolution principles, using genetic operations of crossover and mutation to alter computer pro-grams, and natural selection to choose the fittest programs. Classical GP represents programs as acyclic and undirected graphs (trees), where each node is associated to an elemen-tary operation specific to the problem domain, and where the data type processed and returned by these primitives is usu-ally the same for all nodes. A crossover operation is usually done by exchanging randomly chosen subtrees between two individuals, while a mutation operation consists of replacing a randomly chosen subtree with a new one, also randomly generated.
 imize (or minimize) a single objective function. However, evolutionary multi-objective optimization [ 4 ] has emerged in recent years as an important sub-field of EC. Modern population-based multi-objective optimization techniques are based on the concept of Pareto optimality, where solu-tions are ranked according to a dominance criterion. A solu-tion is said to dominate another if at least one of its objective values is better than the corresponding one for the domi-nated solution, and all others are at least equal. The Pareto front of a population is defined by the set of non-dominated solutions.
 have implemented a two-objective tree-based GP that merges elements from both the static and data-driven fuzzy regional representations. The data processed by tree nodes consist of two coordinate pairs for the lower-left and upper-right corners of a rectangular region. Table 3 enumerates the building blocks available to the evolutionary process. This set of functional primitives (tree nodes) can be classified into two categories. The first is region-modifying primitives (S2H, S2V, S3H, S3V, S4, S5, D2H, D2V, D3H, D3V, D4, D5, and ZM) that split the current region into sub-regions, or that modify the extent of the current region (ZM only). The second category is feature-extraction primitives (OR, CU, MX, MY, and HW) that extract a given type of features from the current region without modifying its definition. The cur-rent region for a given node is always defined by its parent node and the root node simply inherits the full bounding box of the character. There is one additional terminal primitive ( T ) which has no effect other than closing the tree struc-ture. A weight is associated with each primitive in order to bias its selection probability during initialization and mu-tation operations. This bias is useful for building an equi-librium between region-modifying and feature-extraction primitives.
 The first type splits the parent region at predetermined width/height fractions (S2H, S2V, S3H, S3V, S4, and S5), much like it was done in Sect. 2 . It also includes the ZM primitive that changes the scale of the current region by applying a randomly generated zoom factor between 20 and 200%. This zoom value is generated randomly during initial-ization or mutation, acting as an ephemeral random constant [ 14 ] (its value is set to a new random number each time the primitive is mutated). The second type of region modifying primitives (D2H, D2V, D3H, D3V, D4, D5) split the parent region according to the centroid of the strokes found in the parent region, in a way similar to the data-driven representa-tions of Sect. 3 .
 by adding a new feature to the output representation with-out affecting the current region which is simply passed on unchanged to its child node. The feature-extraction primi-tives are also of two types. The first is composed of orien-tation and curvature primitives (OR and CU) that extract re-spectively a degree of orientation and curvature possessed by the strokes contained in the current region. Both prim-itives include three randomly generated parameters (cen-ter, core, and boundary) that specify a symmetric trapez-ium fuzzy set, as illustrated in Fig. 6 . As with the zoom-ing primitive, these three parameters are in fact ephemeral random constants generated during initialization or muta-tion. The ranges of values for the OR primitive are center in [ X  90  X  , 90  X  ] ,corein [ 0  X  , 30  X  ] , and boundary in Each of these three values is discrete with 5  X  increments in their respective domains. The fuzzy shape for orienta-tion extraction is repeated two times, that is at  X  180  X  + 180  X  from the original center position. For the CU primi-tive, the center is in [ X  0 . 160 , 0 . 160 ] ,thecorein [ and the boundary in [ 0 . 005 , 0 . 160 ] , all three with discrete increments of 0 . 005. The second type of feature-extraction primitives is composed of the horizontal and vertical center of mass (MX and MY), and the height/width ratio of the re-gion (HW). These primitives do not make use of ephemeral random constants. 4.2 Experimental results The experimental protocol used to evolve a population of genetic handwriting representations requires a double cross-validation procedure to avoid overfitting the data during both the evolutionary process and the training of our neural net-work classifier. The Train-R01/V07 set is thus first randomly decomposed into a fitness evaluation set used to estimate the fitness of individual representations, and a validation set used to retrieve the best-of-run representation. Then, the fit-ness evaluation set is sub-divided again into a fitness eval-uation training set used to update the weights of the MLP classifier, and a fitness evaluation validation set used to halt learning when the neural network starts overfitting the data. In this way, the final validation set is completely independent of the evolutionary process and favors the selection of the in-dividual representation that exhibits the best faculty of gen-eralization; we call this representation the best-of-run repre-sentation. Finally, the best-of-run individual is re-evaluated on the complete DevTest-R02/V02 set to produce the recog-nition rates reported below.
 sizes for the validation set: 33% of the whole training set for the first two experiments, 20% for the last two. In each case, the fitness evaluation set (respectively 67 and 80% of the to-tal) was randomly divided into two equal parts: half for the fitness evaluation training set, and half for the fitness eval-uation validation set. Moreover, this random sub-division is re-shuffled for each fitness evaluation.
 evolutionary process. The first objective is to maximize the recognition rate during the training phase. The parameters used for the MLP network are: one hidden layer of 50 neu-rons, a learning rate of 0 . 1, a momentum of 0 . 25, a mini-mum of 25 training epochs and a maximum of 100, and ter-mination of training after 5 epochs without recognition rate improvements on the fitness evaluation validation set. The second objective is to minimize the total number of features in the representation.
 als over 100 generations, using a crossover probability of 0 . 9, mutation (standard, swap, and shrink) probabilities of 0 . 05 each, and a NSGA-II [ 5 ] multi-objective selection op-erator. Minimum and maximum initial tree depths are set to 3 and 7, respectively, while tree depth is limited to a maxi-mum of 17 levels during evolution. The GP implementation is done in C ++ using the Open BEAGLE framework [ 8 , 9 ] and distributed on a 26 nodes Beowulf cluster of 1.2GHz AMD Athlons using Distributed BEAGLE [ 6 , 10 ]. Recog-nition rates obtained using the best-of-run representations for the four evolutions are given in Table 4 . Each best-of-run individual was tested ten times, using the same training methodology as in Sects. 2.3 and 3.1 .
 features, the best genetically engineered representation was able to match the recognition rate of the best human de-signed representation of Sect. 2.3 (mean rates of 96 versus 96 . 37%, respectively). Moreover, the standard devia-tions of these mean rates are systematically lower (0 . 15% versus 0 . 21% for the above best case). Another interest-ing observation is that the evolution process is quite stable. Comparable results are obtained for the four distinct evo-lution runs, even if the larger validation set of the first two (Evol1 and Evol2) appears to have produced representations with fewer features.
 ing the region modifying primitives as ellipses, and the fea-ture extraction primitives as rectangular boxes (see Table 3 for a description of the primitives). When several features are extracted within a given region, they are enumerated in a single box without their parameters. For example,  X 2OR, 1CU X  means that two orientation primitives and one cur-vature primitive were extracted from the region defined by the parent node. Terminal primitives are omitted unless their parent is region modifying.
 converged to a mixture of the static and data driven segmen-tation strategies. It is interesting to note that the highest re-gion modifying node of all four representations is always data driven. So the data driven strategy may not be such a bad idea after all, as long as it is not systematic. For one of the representations (Evol2), the root node is not region modifying. It thus extracts global features. Finally, we can observe that the hierarchical segmentation is mostly two and three-level deep, but sometimes goes up to four-levels. tion of the average and maximum individual fitness over the first 100 generations. This graph shows that average conver-gence is quite rapid. Each evolution took about 2 weeks of execution time on our small cluster. Most of this time is used for training the MLP networks.
 our evolved character recognizer, we have looked at every badly classified character in the test set. Many of them are badly written or badly segmented, and some are obviously mislabeled. Figure 9 gives a subset of these characters. They are shown in their circular arc stroke reconstruction form, which corresponds to the input of the GP feature extraction module. We assume here that the filtering process that pro-duces this stroke decomposition preserves all of the discrim-inant information contained in the script, even though we fully recognize that this is a strong hypothesis. The reader should recall that the objective of this paper is to expose a new method for optimizing the feature extraction component of a character recognizer, not to report overall best perfor-mances.
 fied by at least three out of our four classifiers. They repre-sent about 2 . 2% of DevTest-R02/V02, section 1a. Examples of incomplete characters are found mostly for digits 0 and 4. For instance, a slash segmented without its associated 0, or the vertical bar of a 4 without its angle part. Some characters are dot like or dash like. Either they are parts of badly seg-mented characters, or they have been mislabeled and should not even be part of section 1a. Some scripts are completely unrecognizable, like the last instance of characters 0 and 2. Others contain two characters like the first instance of char-acter 7. Others are obviously mislabeled like some instances of characters 2, 3, 4, 6, and 7. Overall, we see that this test set is quite difficult even if some badly classified charac-ters also appear to be recognizable. This may imply that our set of primitives should be augmented with new types of features. 5Conclusion We have seen in this paper how counter-intuitive the design of discriminant features can be. For example, contrary to what we expected, the systematic use of the data-driven seg-mentation introduced in Sect. 3 has resulted in lower recog-nition rates than its static counterpart of Sect. 2 . More impor-tantly, we have shown in Sect. 4 that genetic programming can be used to at least partly automate the trial and error pro-cess that most often surround the development of the feature extraction component of pattern recognition systems. This was demonstrated for a particular case of handwriting char-acter recognition, but the general approach is in no way lim-ited to this application.
 part of the burden from expensive human development to cheap machine computations, which can be performed non-stop, 24 h per day. Of course, the problem must be appro-priately formulated to allow convergence toward interesting solutions. This was done here by the use of a tree-based hier-archical partitioning of the character bounding box, the use of some high-level feature extraction primitives, and a two-objective process that tries to both minimize the number of features while maximizing the recognition rate. This multi-objective genetic programming can be generalized to many other feature extraction systems, simply by adapting the ba-sic building blocks to the problem at hand, in order to allow extraction of discriminant information.
 if it did not produce significantly higher recognition rates for our Unipen test data set, the genetic engineering of hand-writing representations was able to drastically reduce the feature set size while favoring a more stable classifier train-ing process.
 References
