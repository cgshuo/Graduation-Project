 We have explored the usefulness of incorporat-ing speech and discourse features in an automatic speech summarization system applied to meeting recordings from the ICSI Meetings corpus. By an-alyzing speak er acti vity , turn-taking and discourse cues, we hypothesize that such a system can out-perform solely text-based methods inherited from the field of text summarization. The summariza-tion methods are described, two evaluation meth-ods are applied and compared, and the results clearly sho w that utilizing such features is adv anta-geous and efficient. Ev en simple methods relying on discourse cues and speak er acti vity can outper -form text summarization approaches. The task of summarizing spontaneous spok en di-alogue from meetings presents man y challenges: information is sparse; speech is disfluent and frag-mented; automatic speech recognition is imper -fect. Ho we ver, there are numerous speech-specific characteristics to be explored and tak en adv antage of. Pre vious research on summarizing speech has concentrated on utilizing prosodic features [1, 2]. We have examined the usefulness of additional speech-specific characteristics such as discourse cues, speak er acti vity , and listener feedback. This speech features approach is contrasted with a sec-ond summarization approach using only textual features X  X  centroid method [3] using a latent se-mantic representation of utterances. These indi-vidual approaches are compared to a combined ap-proach as well as random baseline summaries. tion scheme for automatic summaries of meeting recordings, using a weighted precision score based on multiple human annotations of each meeting transcript. This evaluation scheme is described in detail belo w and is moti vated by pre vious find-ings [4] suggesting that n-gram based metrics lik e ROUGE [5] do not correlate well in this domain. In the field of speech summarization in general, re-search investigating speech-specific characteristics has focused lar gely on prosodic features such as F0 mean and standard deviation, pause information, syllable duration and ener gy. Koumpis and Re-nals [1] investigated prosodic features for summa-rizing voicemail messages in order to send voice-mail summaries to mobile devices. Hori et al. [6] have developed an inte grated speech summariza-tion approach, based on finite state transducers, in which the recognition and summarization compo-nents are composed into a single finite state trans-ducer , reporting results on a lecture summariza-tion task. In the Broadcast Ne ws domain, Mask ey and Hirschber g [7] found that the best summariza-tion results utilized prosodic, lexical, and structural features, while Ohtak e et al. [8] explored using only prosodic features for summarization. Mask ey and Hirschber g similarly found that prosodic fea-tures alone resulted in good quality summaries of Broadcast Ne ws.
 pus), Murray et al. [2] compared text summariza-tion approaches with feature-based approaches us-ing prosodic features, with human judges favoring the feature-based approaches. Zechner [9] inves-tigated summarizing several genres of speech, in-cluding spontaneous meeting speech. Though rel-evance detection in his work relied lar gely on tf.idf scores, Zechner also explored cross-speak er infor -mation linking and question/answer detection, so that utterances could be extracted not only accord-ing to high tf.idf scores, but also if the y were link ed to other informati ve utterances.
 utterances that may not be detectable according to lexical features or prosodic prominence, but are nonetheless link ed to high speak er acti vity , decision-making, or meeting structure. The follo wing subsections give detailed descrip-tions of our two summarization systems, one of which focuses on speech and discourse features while the other utilizes text summarization tech-niques and latent semantic analysis. 3.1. Speech and Discourse Featur es In pre vious summarization work on the ICSI cor -pus [2, 4], Murray et al. explored multiple ways of applying latent semantic analysis (LSA) to a term/document matrix of weighted term frequen-cies from a given meeting, a development of the method in [10 ]. A central insight to the present work is that additional features beyond simple term frequencies can be included in the matrix before singular value decomposition (SVD) is carried out. We can use SVD to project this matrix of features to a lower dimensionality space, subsequently ap-plying the same methods as used in [2] for extract-ing sentences.
 cluded features of speak er acti vity , discourse cues, listener feedback, simple keyw ord spotting, meet-ing location and dialogue act length (in words). cating which speak er spok e the dialogue act and whether the same speak er spok e the preceding and succeeding dialogue acts. Another set of features indicates how man y speak ers are acti ve on either side of a given dialogue act: specifically , how man y speak ers were acti ve in the preceding and succeeding five dialogue acts. To further gauge speak er acti vity , we located areas of high speak er interaction and indicated whether or not a given dialogue act immediately preceded this region of acti vity , with the moti vation being that informa-tive utterances are often pro vocati ve in eliciting re-sponses and interaction. Additionally , we included a feature indicating which speak ers most often ut-tered dialogue acts that preceded high levels of speak er interaction, as one way of gauging speak er status in the meeting. Another feature relating to speak er acti vity gives each dialogue act a score ac-cording to how acti ve the speak er is in the meeting as a whole, based on the intuition that the most ac-tive speak ers will tend to utter the most important dialogue acts.
 back, and keyw ord spotting were deliberately su-perficial, all based simply on detecting informati ve words. The feature for discourse cues indicates the presence or absence of words such as decide , dis-cuss , conclude , agree , and fragments such as we should indicating a planned course of action. Lis-tener feedback was based on the presence or ab-sence of positi ve feedback cues follo wing a given dialogue act; these include responses such as right , exactly and yeah . Keyw ord spotting was based on frequent words minus stopw ords, indicating the presence or absence of any of the top twenty non-stopw ord frequent words. The discourse cues of interest were deri ved from a manual corpus analy-sis rather than being automatically detected. cording to their position in the meeting, with di-alogue acts from the middle to later portion of the meeting scoring higher and dialogue acts at the be-ginning and very end scoring lower . This is a fea-ture that is well-matched to the relati vely unstruc-tured ICSI meetings, as man y meetings would be expected to have informati ve proposals and agen-das at the beginning and perhaps summary state-ments and conclusions at the end.
 ture moti vated by the fact that informati ve utter -ances will tend to be longer than others.
 ing sentences using an LSA sentence score. The matrix of features is decomposed as follo ws: where U is an m  X  n matrix of left-singular vectors, S is an n  X  n diagonal matrix of singular values, and V is the n  X  n matrix of right-singular vectors. Using sub-matrices S and V T , the LSA sentence scores are obtained using: where v ( i, k ) is the k th element of the i th sen-tence vector and  X  ( k ) is the corresponding singular value.
 meetings sho wed that reduction to between 5 X 15 dimension was optimal. These development ex-periments also sho wed that weighting some fea-tures slightly higher than others resulted in much impro ved results; specifically , the discourse cues and listener feedback cues were weighted slightly higher . 3.2. LSA Centr oid The second summarization method is a textual ap-proach incorporating LSA into a centroid-based system [3]. The centroid is a pseudo-document representing the important aspects of the docu-ment as a whole; in the work of [3], this pseudo-document consists of keyw ords and their modi-fied tf.idf scores. In the present research, we tak e a dif ferent approach to constructing the centroid and to representing sentences in the document. First, tf.idf scores are calculated for all words in the meeting. Using these scores, we find the top twenty keyw ords and choose these as the basis for our centroid. We then perform LSA on a very lar ge corpus of Broadcast Ne ws and ICSI data, using the Infomap tool 1 . Infomap pro vides a query language with which we can retrie ve word vectors for our twenty keyw ords, and the centroid is thus repre-sented as the average of its constituent keyw ord vectors [12 ] [13 ].
 sented in much the same fashion. For each dia-logue act, the vectors of its constituent words are retrie ved, and the dialogue act as a whole is the av-erage of its word vectors. Extraction then proceeds by finding the dialogue act with the highest cosine similarity with the centroid, adding this to the sum-mary , then continuing until the desired summary length is reached. 3.3. Combined The third summarization method is simply a com-bination of the first two. Each system produces a ranking and a master ranking is deri ved from these two rankings. The hypothesis is that the strength of one system will dif fer from the other and that the two will complement each other and produce a good overall ranking. The first system would be expected to locate areas of high acti vity , decision-making, and planning, while the second would lo-cate information-rich utterances. This exempli-fies one of the challenges of summarizing meeting recordings: namely , that utterances can be impor -tant in much dif ferent ways. A comprehensi ve sys-tem that relies on more than one idea of importance is ideal. All summaries were 350 words in length, much shorter than the compression rate used in [2] (10% of dialogue acts). The ICSI meetings themselv es average around 10,000 words in length. The rea-sons for choosing a shorter length for summaries are that shorter summaries are more lik ely to be useful to a user wanting to quickly overvie w and bro wse a meeting, the y present a greater summa-rization challenge in that the summarizer must be more exact in pinpointing the important aspects of the meeting, and shorter summaries mak e it more feasible to enlist human evaluators to judge the nu-merous summaries on various criteria in the future. scripts and speech recognizer output. The unit of extraction for these summaries was the dialogue act, and these experiments used human segmented and labeled dialogue acts rather than try to detect them automatically . In future work, we intend to incorporate dialogue act detection and labeling as part of one complete automatic summarization sys-tem. 4.1. Cor pus Description The ICSI Meetings corpus consists of 75 meetings, lasting approximately one hour each. Our test set consists of six meetings, each with multiple hu-man annotations. Annotators were given access to a graphical user interf ace (GUI) for bro wsing an indi vidual meeting that included earlier human annotations: an orthographic transcription time-synchronized with the audio, and a topic segmen-tation based on a shallo w hierarchical decompo-sition with keyw ord-based text labels describing each topic segment. The annotators were told to construct a textual summary of the meeting aimed at someone who is interested in the research being carried out, such as a researcher who does similar work else where, using four headings: The annotators were given a 200 word limit for each heading, and told that there must be text for the general abstract, but that the other headings may have null annotations for some meetings. An-notators who were new to the data were encour -aged to listen to a meeting straight through before beginning to author the summary .
 mary , annotators were ask ed to create an extracti ve summary , using a dif ferent GUI. This GUI sho wed both their textual summary and the orthographic transcription, without topic segmentation but with one line per dialogue act based on the pre-e xisting MRD A coding [14 ]. Annotators were told to ex-tract dialogue acts that together would con vey the information in the textual summary , and could be used to support the correctness of that summary . The y were given no specific instructions about the number or percentage of acts to extract or about redundant dialogue acts. For each dialogue act ex-tracted, the y were then required in a second pass to choose the sentences from the textual summary supported by the dialogue act, creating a man y-to-man y mapping between the recording and the textual summary . Although the expectation was that each extracted dialogue act and each summary sentence would be link ed to something in the op-posing resource, we told the annotators that under some circumstances dialogue acts and summary sentences could stand alone.
 scripts as well as automatic speech recognition (ASR) output. The AMI-ASR system [15 ] is de-scribed in more detail in [4] and the average word error rate (WER) for the corpus is 29.5%. 4.2. Ev aluation Framew orks The man y-to-man y mapping of dialogue acts to summary sentences described in the pre vious sec-tion allo ws us to evaluate our extracti ve summaries according to how often each annotator link ed a given extracted dialogue act to a summary sen-tence. This is some what analogous to Pyramid weighting [16 ], but with dialogue acts as the SCUs. In fact, we can calculate weighted precision, recall and f-score using these annotations, but because the summaries created are so short, we focus on weighted precision as our central metric. For each dialogue act that the summarizer extracts, we count the number of times that each annotator links that dialogue act to a summary sentence. For a given dialogue act, it may be that one annotator links it 0 times, one annotator links it 1 time, and the third annotator links it two times, resulting in an aver-age score of 1 for that dialogue act. The scores for all of the summary dialogue acts can be calculated and averaged to create an overall summary score. tween human abstracts and automatic extracts, were also calculated for comparison [5]. ROUGE-2, based on bigram overlap, is considered the most stable as far as correlating with human judgments, and this was therefore our ROUGE metric of inter -est. ROUGE-SU4, which evaluates bigrams with interv ening material between the two elements of the bigram, has recently been sho wn in the con-text of the Document Understanding Conference (DUC) 2 to bring no significant additional informa-tion as compared with ROUGE-2. Results from [4] and from DUC 2005 also sho w that ROUGE does not always correlate well with human judg-ments. It is therefore included in this research in the hope of further determining how reliable the ROUGE metric is for our domain of meeting sum-marization. The experimental results are sho wn in figure 1 (weighted precision) and figure 2 (ROUGE-2) and are discussed belo w. 5.1. Weighted Pr ecision Results For weighted precision, the speech features ap-proach was easily the best and scored significantly better than the centroid and random approaches (ANO VA,p &lt; 0.05), attaining an averaged weighted precision of 0.52. The combined approach did not impro ve upon the speech features approach but was not significantly worse either . The ran-domly created summaries scored much lower than all three systems.
 tures approach compared to the LSA centroid method closely mirrors results on the ICSI devel-opment set, where the centroid method scored 0.23 and the speech features approach scored 0.42. For the speech features approach on the test set, the best feature by far was dialogue act length. Re-mo ving this feature resulted in the precision score being nearly halv ed. This mirrors results from Mask ey and Hirschber g [7], who found that the length of a sentence in seconds and its length in words were the two best features for predicting summary sentences. Both the simple keyw ord spotting and the discourse cue detection features caused a lesser decline in precision when remo ved, while other features of speak er acti vity had a neg-ligible impact on the test results.
 ASR were not significantly worse for any of the summarization approaches. In fact, the centroid approach scored very slightly higher on ASR out-put than on manual transcripts. In [17 ] and [2] it was similarly found that summarizing with ASR output did not cause great deterioration in the qual-ity of the summaries. It is not especially surpris-ing that the speech features approach performed similarly on both manual and ASR transcripts, as man y of its features based on speak er exchanges and speak er acti vity would be unaf fected by ASR errors. The speech features approach is still signif-icantly better than the random and centroid sum-
Figure 1: Weighted Precision Results on Test Set maries, and is not significantly better than the com-bined approach on ASR. 5.2. ROUGE Results The ROUGE results greatly dif fered from the weighted precision results in several ways. First, the centroid method was considered to be the best, with a ROUGE-2 score of 0.047 compared with 0.041 for the speech features approach. Second, there were not as great of dif ferences between the four systems according to ROUGE as there were according to weighted precision. In fact, the ran-dom summaries of manual transcripts are not sig-nificantly worse than the other approaches, accord-ing to ROUGE-2. Neither the combined approach nor the speech features approach is significantly worse than the centroid system, with the combined approach generally scoring on par with the cen-troid scores.
 on ASR output. ROUGE-2 has the random system and the combined system sho wing sharp declines when applied to ASR transcripts. The speech fea-tures and centroid approaches do not sho w de-clines. Random summaries are significantly worse than both the centroid summaries (p &lt; 0.1) and speech features summaries (p &lt; 0.05). Though the combined approach declines on ASR output, it is not significantly worse than the other systems. each meeting in the test set we left one human ab-stract out and compared it with the remaining ab-stracts. The result was an average ROUGE-2 score of .086. icant dif ferences between the centroid and speech features approaches. 5.3. Corr elations There is no significant correlation between macroa veraged ROUGE and weighted precision scores across the meeting set, on both ASR and manual transcripts. The Pearson correlation is 0.562 with a significance of p &lt; 0.147. The Spear -man correlation is 0.282 with a significance of p &lt; 0.498. The correlation of scores across each test meeting is worse yet, with a Pearson correlation of 0.185 (p &lt; 0.208) and a Spearman correlation of 0.181 (p &lt; 0.271). 5.4. Sample Summary The follo wing is the text of a summary of meeting Bed004 using the speech features approach: Though the speech features approach was consid-ered the best system, it is unclear why the com-bined approach did not yield impro vement. One possibility relates to the extreme bre vity of the summaries: because the summaries are only 350 words in length, it is possible to have two sum-maries of the same meeting which are equally good but completely non-o verlapping in content. In other words, the y both extract informati ve dia-logue acts, but not the same ones. Combining the rankings of two such systems might create a third system which is comparable but not any better than either of the first two systems alone. Ho we ver, it is still possible that the combined system will be better in terms of balancing the two types of im-portance discussed abo ve: utterances that contain a lot of informati ve content and keyw ords and utter -ances that relate to decision-making and meeting structure.
 weighted precision scores, a result that adds to the pre vious evidence that this metric may not be reli-able in the domain of meeting summarization. approaches in general seem immune to the WER of the ASR output. This confirms pre vious find-ings such as [17 ] and [2], and the speech and structural features used herein are particularly un-affected by a moderately high WER. The reason for the random summarizaton system not suf fering a sharp decline when applied to ASR may be due to the fact that its scores were already so low that it couldn X  t deteriorate any further . The abo ve results sho w that even a relati vely small set of speech, discourse, and structural features can outperform a text summarization approach on this data, and there are man y additional features to be explored. Of particular interest to us are features relating to speak er status, i.e. features that help us determine who is leading the meeting and who it is that others are deferring to. We would also lik e to more closely investigate the relationship between areas of high speak er acti vity and informati ve ut-terances.
 these features into a machine-learning frame work, building support vector models trained on the ex-tracted and non-e xtracted classes of the training set.
 AMI corpus [18 ] and create summaries of compa-rable length for that meeting set. There are lik ely to be dif ferences regarding usefulness of certain features due to the ICSI meetings being relati vely unstructured and informal and the AMI hub meet-ings being more structured with a higher informa-tion density . The results presented abo ve sho w that using fea-tures related to speak er acti vity , listener feedback, discourse cues and dialogue act length can outper -form the lexical methods of text summarization ap-proaches. More specifically , the fact that there are multiple types of important utterances requires that we use multiple methods of detecting importance. Le xical methods and prosodic features are not nec-essarily going to detect utterances that are rele vant to agreement, decision-making or speak er acti vity . This research also pro vides further evidence that ROUGE does not correlate well with human judg-ments in this domain. Finally , it has been demon-strated that high WER for ASR output does not significantly decrease summarization quality . Thanks to Thomas Hain and the AMI-ASR group for speech recognition output. This work was partly supported by the European Union 6th FWP IST Inte grated Project AMI (Augmented Multi-party Interaction, FP6-506811, publication AMI-150). [1] K. Koumpis and S. Renals,  X  X utomatic sum-[2] G. Murray , S. Renals, and J. Carletta,  X  X x-[3] D. Rade v, S. Blair -Goldensohn, and [4] G. Murray , S. Renals, J. Carletta, and [5] C.-Y . Lin and E. H. Ho vy,  X  X utomatic [6] T. Hori, C. Hori, and Y. Minami,  X  X peech [7] S. Mask ey and J. Hirschber g,  X  X ompar -[8] K. Ohtak e, K. Yamamoto, Y. Toma, S. Sado, [9] K. Zechner ,  X  X utomatic summarization of [10] Y. Gong and X. Liu,  X  X eneric text sum-[11] J. Steinber ger and K. Je X  zek,  X  X sing latent [12] P. Foltz, W. Kintsch, and T. Landauer ,  X  X he [13] B. Hache y, G. Murray , and D. Reitter ,  X  X he [14] E. Shriber g, R. Dhillon, S. Bhagat, J. Ang, , [15] T. Hain, J. Dines, G. Garau, M. Karafiat, [16] A. Nenk ova and B. Passonneau,  X  X v aluat-[17] R. Valenza, T. Robinson, M. Hick ey, and [18] J. Carletta, S. Ashby , S. Bourban, M. Flynn,
