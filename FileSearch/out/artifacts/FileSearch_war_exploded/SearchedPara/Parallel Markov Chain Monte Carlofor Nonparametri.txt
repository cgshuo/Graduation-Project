 Sinead A. Williamson sinead@cs.cmu.edu Avinava Dubey akdubey@cs.cmu.edu Eric P. Xing epxing@cs.cmu.edu Models based on the Dirichlet process (DP, Ferguson, 1973) and its extension the hierarchical Dirichlet pro-cess (HDP, Teh et al., 2006) have a number of appeal-ing properties. They allow a countably infinite num-ber of mixture components a priori , meaning that a finite dataset will be modeled using a finite, but ran-dom, number of parameters. If we observe more data, the model can grow in a consistent manner. Unfortu-nately, while this means that such models can theoret-ically cope with data sets of arbitrary size and latent dimensionality, in practice inference can be slow, and the memory requirements are high.
 Parallelization is a technique often used to speed up computation, by splitting the computational and memory requirements of an algorithm onto multiple machines. Parallelization of an algorithm involves exploitation of (conditional) independencies. If we can update one part of a model independently of an-other part, we can split the corresponding sections of code onto separate processors or cores. Unfortunately, many models do not have appropriate independence structure, making parallelization difficult. For exam-ple, in the Chinese restaurant process representation of a Dirichlet process mixture model, the conditional distribution over the cluster allocation of a single data point depends on the allocations of all the other data points.
 In such cases, a typical approach is to apply approxi-mations that break some of the long-range dependen-cies. While this can allow us to parallelize inference in the approximate model, the posterior estimate will, in general, be less accurate. Another option is to use a se-quential Monte Carlo approach, where the posterior is approximated with a swarm of independent particles. In its simplest form, this approach is inherently par-allelizable, but such a naive implementation will run into problems of variance overestimation. We can im-prove the estimate quality by introducing global steps such as particle resampling and Gibbs steps, but these steps cannot easily be parallelized.
 In this paper, we show how the introduction of aux-iliary variables into the DP and HDP can create the conditional independence structure required to obtain a parallel Gibbs sampler, without introducing approx-imations. As a result, we can make use of the powerful and elegant representations provided by the DP and HDP, while maintaining manageable computational requirements. We show that the resulting samplers are able to achieve significant speed-ups over existing inference schemes for the  X  X xact X  models, with no loss in quality. By performing inference in the true model, we are able to achieve better results than those ob-tained using approximate models.
 The Dirichlet process is a distribution over discrete probability measures D = P  X  k =1  X  k  X   X  ably infinite support, where the finite-dimensional marginals are distributed according to a finite Dirich-let distribution. It is parametrized by a base proba-bility measure H , which determines the distribution of the atom locations, and a concentration parameter  X  &gt; 0, which acts like an inverse variance. The DP can be used as the distribution over mixing measures in a nonparametric mixture model. In the Dirichlet pro-cess mixture model (DPMM, Antoniak, 1974), data { x i } n i =1 are assumed to be generated according to While the DP allows an infinite number of clusters a priori , any finite dataset will be modeled using a finite, but random, number of clusters.
 Hierarchical Dirichlet processes extend the DP to model grouped data. The HDP is a distribution over probability distributions D m ,m = 1 ,...,M , each of which is conditionally distributed according to a DP. These distributions are coupled using a discrete com-mon base-measure, itself distributed according to a DP. Each distribution D m can be used to model a collection of observations x m := { x mi } N m i =1 , where for m = 1 ,...,M and i = 1 ,...,N m . HDPs have been used to model data including text corpora (Teh et al., 2006), images (Sudderth et al., 2005), time series data (Fox et al., 2008), and genetic variation (Sohn &amp; Xing, 2009).
 A number of inference schemes have been developed for the DP and the HDP. The most commonly used class of inference methods is based on the Chi-nese restaurant process (CRP, see for example Aldous (1985)). Such schemes integrate out the random mea-sures ( D in Eq. 1; D 0 and { D m } M m =1 in Eq. 2) to obtain the conditional distribution for the cluster allocation of a single data point, conditioned on the allocations of all the other data points. A variety of Gibbs sam-plers based on the CRP can be constructed for the DP (Neal, 1998) and the HDP (Teh et al., 2006). Unfor-tunately, because the conditional distribution for the cluster allocation of a single data point depends on all the data, this step cannot be parallelized without introducing approximations.
 An alternative class of inference schemes involve ex-plicitly instantiating the random measures (Ishwaran &amp; James, 2001). In this case, the observations are i.i.d. given the random measures, and can be sampled in parallel. However, since the random measure de-pends on the cluster allocations of all the data points, sampling the random measure cannot be parallelized. Among existing practical schemes for parallelizable in-ference in DP and HDP, the following three are the most popular: 2.1. Sequential Monte Carlo methods Sequential Monte Carlo (SMC) methods approximate a distribution of interest using a swarm of weighted, sequentially updated, particles. SMC methods have been used to perform inference in a number of mod-els based on the DP (Fearnhead, 2004; Ulker et al., 2010; Rodriguez, 2011; Ahmed et al., 2011). Such methods are appealing when considering paralleliza-tion, because each particle (and its weight) are up-dated independently of the other particles, and need consider only one data point at a time. However, a naive implementation where each particle is propa-gated in isolation leads to very high variance in the resulting estimate. To avoid this, it is typical to in-troduce resampling steps, where the current swarm of particles is replaced by a new swarm sampled from the current posterior estimate. This avoids an explosion in the variance of our estimate, but the resampling can-not be performed in parallel. 2.2. Variational inference Variational Bayesian inference algorithms have been developed for both the DP (Blei &amp; Jordan, 2004; Kuri-hara et al., 2007) and the HDP (Teh et al., 2007; Wang et al., 2011). Variational methods approximate a pos-terior distribution p (  X  | X ) with a distribution q (  X  ) be-longing to a more manageable family of distributions and try to find the  X  X est X  member of this family, typ-ically by minimizing the Kullback-Leibler divergence between p (  X  | X ) and q (  X  ). This also gives us a lower bound on the log likelihood, log p ( X ). A typical ap-proach to selecting the family of approximating distri-butions is to assume independencies that may not be present in the true posterior. This means that varia-tional algorithms are often easy to parallelize. How-ever, by searching only within a restricted class of models we lose some of the expressiveness of the model, and will typically obtain less accurate results than MCMC methods that asymptotically sample from the true posterior. 2.3. Approximate parallel Gibbs sampling An approximate distributed Gibbs sampler for the HDP is described by Asuncion et al. (2008). The ba-sic sampler alternates distributed Gibbs steps with a global synchronization step. If there are P processors, in the distributed Gibbs steps, each processor updates 1 /P of the cluster allocations. To sample the cluster allocation for a given observation, rather than use the current allocations of all the other data, the sampler uses the current cluster allocations for the observations stored on the same processor, and for all other obser-vations it uses the allocations after the last synchro-nization step. In the synchronization step, the clus-ter counts are amalgamated. This can lead to prob-lems with cluster alignment. In particular, there is no clear way to decide whether to merge a new cluster on processor p with a new cluster on processor p 0 . An asynchronous version of the algorithm avoids the bot-tleneck of a global synchronization step; however in practice it obtains slower convergence. The key to developing a parallel inference algorithm is to exploit or introduce independencies. In the se-quential Monte Carlo samplers, this independence can lead to high variance in the resulting estimate. In the other algorithms described, the independence is only achieved by introducing approximations.
 If observations are modeled using a mixture model, then conditioned on the cluster allocations the obser-vations are independent. The key idea that allows us to introduce conditional independence is the fact that, for appropriate parameter settings, Dirichlet mixtures of Dirichlet processes are Dirichlet processes . In The-orems 1 and 2, we demonstrate situations where this result holds, and develop mixtures of nonparametric models with the appropriate marginal distributions and conditional independence structures. The result-ing models exhibit conditional independence between parameters that are coupled in Eq. 1 and Eq. 2, al-lowing us to perform parallel inference in Section 4 without resorting to approximations.
 Theorem 1 (Auxiliary variable representation for the DPMM) . We can re-write the generative process for a DPMM (given in Eq. 1) as
D j  X  DP for j = 1 ,...,P and i = 1 ,...,N . The marginal dis-tribution over the x i remains the same.
 Proof. We prove a more general result, that if  X   X  Dirichlet(  X  1 ,..., X  P ) and D j  X  DP(  X  j ,H j ), then A Dirichlet process with concentration parameter  X  and base probability measure H can be obtained by normalizing a gamma process with base measure  X H . Gamma processes are examples of completely random measures (Kingman, 1967), and the superposition of P completely random measures is again a completely random measure. In particular, if G j  X  GaP(  X  j H j ), j = 1 ,...,P , then G := P j G j  X  GaP( P j  X  j H j ). Note that the total masses of the G j are gamma-distributed, and therefore the vector of normalized masses is Dirichlet-distributed. It follows that, if  X   X  Dirichlet(  X  1 ,..., X  P ) and D j  X  DP(  X  j ,H j ), then which is well known in the nonparametric Bayes com-munity, is explored in more depth in Chapter 3 of Ghosh &amp; Ramamoorthi (2003). An alternative proof is given in the supplementary material.
 The auxiliary variables  X  i introduced in Eq. 3 intro-duce conditional independence, which we exploit to develop a distributed inference scheme. If we have P processors, then we can split our data among them according to the  X  i . Conditioned on their processor allocations, the data points are distributed according to independent Dirichlet processes. In Section 4, we will see how we can combine local inference in these independent Dirichlet processes with global steps to move data between processors.
 We can follow a similar approach with the HDP, as-signing each observation x mi in each collection x m to one of P groups corresponding to P processors. However, to ensure the higher level DP can be split in a manner consistent with the lower level DP, we must impose some additional structure into the gen-erative process described by Eq. 2  X  specifically, that  X   X  Gamma(  X  ). 1 We can then introduce auxiliary variables as follows: Theorem 2 (Auxiliary variable representation for the HDP) . If we incorporate the requirement that the concentration parameter  X  for the bottom level DPs { D j } M j =1 depends on the concentration parameter  X  for the top level DP D 0 as  X   X  Gamma (  X  ) , then we can rewrite the generative process for the HDP as: for j = 1 ,...,P , m = 1 ,...,M , and i = 1 ,...,N m . The marginal distribution over the x mi is the same in Eq.s 2 and 4.
 Proof. If  X  j  X  Gamma(  X /P ), then  X  := P j  X  j  X  Gamma(  X  ). Since D 0 j  X  DP(  X /P,H ), it follows that G GaP(  X H ). If we normalize G 0 , we find that D 0 := P If we write G mj  X  GaP( G 0 j ) = GaP(  X  j D 0 j ), then we can see that G 0 j =  X  mj D mj , where  X  mj  X  Gamma(  X  j ), and D mj  X  DP(  X  j ,D 0 j ).
 If we normalize the G mj , we find that as required by the HDP. Since the  X  mj only ap-pear as a normalized vector, we can write  X  m = (  X  A more in-depth version of this proof is given in the supplementary material.
 Again, the application to distributed inference is clear: Conditioned on the  X  mi we can split our data into P independent HDPs. The auxiliary variable representation for the DP intro-duced in Theorem 1 makes the cluster allocations for data points where  X  i = j conditionally independent of the cluster allocations for data points where  X  i 6 = j . We can therefore split the data onto P parallel pro-cessors or cores, based on the values of  X  i . We will henceforth call  X  i the  X  X rocessor indicator X  for the i th data point. We can Gibbs sample the cluster alloca-tions on each processor independently, intermittently moving data between clusters to ensure mixing of the sampler. 4.1. Parallel inference in the Dirichlet process We consider first the Dirichlet process. Under the con-struction described in Eq. 3, each data point x i is as-sociated with a processor indicator  X  i and a cluster indicator z i . All data points associated with a single cluster will have the same processor indicator, mean-ing that we can assign each cluster to one of the P processors (i.e., all data points in a single cluster are assigned to the same processor). Note that the j th pro-cessor will typically be associated with multiple clus-ters, corresponding to the local Dirichlet process D j . Conditioned on the assignments of the auxiliary vari-ables  X  i , the data points x i in Equation 3 depend only on the local Dirichlet process D j and the associated parameters.
 We can easily marginalize out the D j and  X  . As-sume that each data point x i is assigned to a proces-sor  X  i  X  { 1 ,...,P } , and a cluster z i residing on that processor. We will perform local inference on the clus-ter assignments z i , and intermittently we will perform global inference on the  X  i . 4.1.1. Local inference: Sampling the z i Conditioned on the processor assignments, sampling the cluster assignments proceeds exactly as in a normal Dirichlet process with concentration parameter  X /P . A number of approaches for Gibbs sampling in the DPMM are described by Neal (1998). 4.1.2. Global inference: Sampling the  X  i Under the auxiliary variable scheme, each cluster is associated with a single processor. We jointly resam-ple the processor allocations of all data points within a given cluster, allowing us to move an entire cluster from one processor to another. We use a Metropolis Hastings step with a proposal distribution that inde-pendently assigns cluster k to processor j with prob-ability 1 /P . This means our accept/reject probability depends only on the ratio of the likelihoods of the two assignments.
 The likelihood ratio is given by: where N j is the number of data points on processor j , and a ij is the number of clusters of size i on pro-cessor j . In fact, we can simplify Eq. 5 further, since many of the terms in the ratio of factorials will cancel. A derivation of Eq. 5 is given in the supplementary material.
 The reassignment of clusters can be implemented in a number of different manners. Actually transferring data from one processor to another will lead to bot-tlenecks, but may be appropriate if the entire data set is too large to be stored in memory on a single ma-chine. If we can store a copy of the dataset on each machine, or we are using multiple cores on a single ma-chine, we can simply transfer updates to lists of which data points belong to which cluster on which machine. We note that the reassignments need not occur at the same time, reducing the bandwidth required. 4.2. Parallel inference in the HDP Again, we can assign tokens x mi to one of P processors according to  X  mi . Conditioned on the processor assign-ment and the values of  X  j , the data on each processor is distributed according to an HDP. We instantiate the processor allocations  X  mi and the bottom-level DP pa-rameters, plus sufficient representation to perform in-ference in the processor-specific HDPs. We assume a Chinese restaurant franchise representation (Teh et al., 2006)  X  data points in the lower-level Dirichlet pro-cesses are clustered into  X  X ables X , and in the upper-level Dirichlet process, these  X  X ables X  are clustered and each cluster is assigned a  X  X ish X . 4.2.1. Local inference: Sampling the table Conditioned on the processor assignments, we simply have P independent HDPs, and can use any exist-ing inference algorithm for the HDP. In our experi-ments, we used the Chinese restaurant franchise sam-pling scheme (Teh et al., 2006); other representations could also be used. 4.2.2. Global inference: Sampling the  X  mj We can represent the  X  j as  X  j :=  X  X  j , where  X   X  Gamma(  X , 1) and  X  := (  X  1 ,..., X  P )  X  Dirichlet(  X /P,..., X /P ). We sample the  X  mj and the  X  jointly, and then sample  X  , in order to improve the acceptance ratio of our Metropolis-Hastings steps. Again, we want to reallocate whole clusters rather than independently reallocate individual tokens. So, our proposal distribution again assigns cluster k to processor j with probability 1 /P . Note that this means that a single data point does not necessar-ily reside on a single processor  X  its tokens may be split among multiple processors. We also propose  X   X   X  Dirichlet(  X /P,..., X /P ), and accept the result-ing state with probability min(1 ,r ), where A derivation of Eq. 6 is given in the supplementary ma-terial. As before, many of the ratios can be simplified further, reducing computational costs.
 As with the Dirichlet process sampler, we can either transfer the data between machines, or simply update lists of which data points are  X  X ctive X  on each machine. We can resample  X  after sampling the  X  j and  X  mj using a standard Metropolis Hastings step. Our goal in this paper is to employ parallelization to speed up inference in the DP and HDP, without in-troducing approximations that might compromise the quality of our model estimate. To establish whether we have achieved that goal, we perform inference in both the DP and HDP using a number of inference methods, and look at appropriate measures of model quality as a function of inference time. This captures both the speed of the algorithms and the quality of the approximations obtained. 5.1. Dirichlet process mixture of Gaussians We begin by evaluating the performance of the Dirich-let process sampler described in Section 4.1. We gen-erated a synthetic data set of one million data points from a mixture of univariate Gaussians. We used 50 components, each with mean distributed according to Uniform(0 , 10) and fixed variance of 0 . 01. A synthetic data set was chosen because it allows us to compare performance with  X  X round truth X . We compared four inference algorithms:  X  Auxiliary variable parallel Gibbs sampler  X  Sequential Monte Carlo (SMC)  X  a basic se- X  Variational Bayes (VB)  X  the collapsed varia- X  Synchronous approximate parallel DP In each case, we ran the algorithms on one, two, four and eight processors on a single multi-core machine 3 , with one processor for the AVparallel method corre-sponding to the regular Gibbs sampler. We initialized each algorithm by clustering the data into 80 clusters using K-means clustering, and split the resulting clus-ters among the processors.
 We consider the F1 score between the clusterings ob-tained by each algorithm and the ground truth, as a function of time. Let P ( g ) be the set of pairs of ob-servations that are in the same cluster under ground truth, and let P ( m ) be the set of pairs of observations that are in the same cluster in the inferred model. Then we define the F1 score of a model as the har-monic mean between the precision  X  the proportion the model that also co-occur in the true partition  X  and the recall  X  the proportion |P ( g )  X  P ( m ) | / |P of true co-occurrences that are co-clustered by the model. This definition of F1 is invariant to permuta-tion, and so is appropriate in an unsupervised setting (Xing et al., 2002).
 Figure 1(a) shows the F1 scores for our auxiliary vari-able method over time, using one, two, four and eight processors. As we can see, increasing the number of processors increases convergence speed without de-creasing performance. Figure 1(b) shows the F1 scores over time for the four methods, each using eight cores. While we can get very fast results using variational Bayesian inference, the quality of the estimate is poor. Conversely, we achieve better performance (as mea-sured by F1 score) than competing MCMC algorithms, with faster convergence. Figure 1(c) shows the time taken by each algorithm to reach convergence, for vary-ing numbers of processors. AV parallel and Synch per-form similarly. Figure 1(d) shows the relative time spent sampling the processor allocations (the global step) and sampling the cluster allocations (the local step), over 500 iterations. This explains the similar scaleability of AVparallel and Synch: In AVparallel the majority of time is spent on local Gibbs sampling, which is implemented identically in both models. 5.2. HDP topic model Next, we evaluate the performance of the HDP sampler on a topic modeling task as described by Teh et al. (2006). Our dataset was a corpus of NIPS papers 4 consisting of 2470 documents, containing 14300 unique words and around 3 million total words. We split the dataset into a training set of 2220 documents and a test set of 250 documents, and evaluated performance in terms of test set perplexity. We compared three inference methods:  X  Auxiliary variable parallel Gibbs sampler  X  Variational Bayes (VB)  X  the collapsed varia- X  Synchronous approximate parallel HDP Again, we ran each method on one, two, four and eight processors, and initialized each document to one of 80 clusters using K-means.
 Figure 2(a) shows the perplexity obtained using our auxiliary variable method over time, using one, two, four and eight processors, and Figures 2(b) and 2(c) compare the performance of the three inference meth-ods. As with the DPMM, while the variational ap-proach is able to obtain results very quickly, the qual-ity is much lower than that obtained using MCMC methods. The AVparallel method achieves much bet-ter perplexity than the approximate Synch method  X  the difference is much more striking than that seen in the DPMM. Note that, in the synthetic data used for the DPMM model, the true clusters are of similar size, while in the real-world data used for the HDP experiment there are likely to be many small clusters. We hypothesise that while the errors introduced in the synchronous approximate method have little effect if the clusters are large, they become more significant if we have small clusters. Again, we find (Figure 2(d)) that the majority of time is spent in the local Gibbs sampler, meaning we can obtain a good rate of in-crease of speed by increasing the number of processors (Figures 2(a) and 2(c)). We have shown how alternative formulations for the DP and HDP can yield parallelizable Gibbs samplers that allow scalable and accurate inference. Our exper-iments show that the resulting algorithms offer better performance and scalability than existing parallel in-ference methods.
 Since the assignments of clusters to processors is ran-dom, and since clusters vary in sizes, the loads as-signed to each processor will vary. In addition, since each cluster must reside on a single processor, the scal-ability of our algorithms is limited by the size of the largest cluster. An interesting avenue for future de-velopment is to investigate approximate methods for splitting large clusters onto multiple processors, to en-able better allocation of resources.
 We note that a related parallel MCMC method for the Dirichlet process has been developed concurrently and independently by Lovell et al. (2012). This work uses the same auxiliary variable representation of the Dirichlet process, and describes a MapReduce algo-rithm; the hierarchical Dirichlet process is not con-sidered. Their preliminary results using the MapRe-duce framework suggest that the speed-ups obtained in this paper in a multi-core environment will carry over to multi-machine architectures. Our next goal is to develop and publish code appropriate for multi-machine architectures, and to extend our approach to other nonparametric models.
 This research was supported by AFOSR FA9550010247, NIH R01GM087694 and DARPA XDATA FA87501220324. We would like to thank Iain Murray for helpful comments on a draft.
 Ahmed, A., Ho, Q., Teo, C. H., Eisenstein, J., Smola,
A. J., and Xing, E. P. Online inference for the infi-nite topic-cluster model: Storylines from streaming text. In AISTATS , 2011.
 Aldous, D. J. Exchangeability and related topics. 1985.
 Antoniak, C. E. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. Ann. Statist. , 2(6):1152 X 1174, 1974.
 Asuncion, A., Smyth, P., and Welling, M. Asyn-chronous distributed learning of topic models. In NIPS , 2008.
 Blei, D. M. and Jordan, M. I. Variational methods for the Dirichlet process. In ICML , 2004.
 Fearnhead, P. Particle filters for mixture models with an unknown number of components. Statistics and Computing , 14:11 X 21, 2004.
 Ferguson, T. S. A Bayesian analysis of some nonpara-metric problems. Ann. Statist. , 1(2):209 X 230, 1973. Fox, E. B., Sudderth, E. B., Jordan, M. I., and Will-sky, A. S. An HDP-HMM for systems with state persistence. In ICML , 2008.
 Ghosh, J. K. and Ramamoorthi, R. V. Bayesian Non-parametrics . Springer, 2003.
 Ishwaran, H. and James, L. F. Gibbs sampling meth-ods for stick-breaking priors. JASA , 96(453):161 X  173, 2001.
 Kingman, J. F. C. Completely random measures. Pa-cific Journal of Mathematics , 21(1):59 X 78, 1967. Kurihara, K., Welling, M., and Teh, Y.-W. Collapsed variational Dirichlet process mixture models. In IJ-CAI , 2007.
 Lovell, D., Adams, R. P., and Mansingka, V. K. Par-allel Markov chain Monte Carlo for Dirichlet pro-cess mixtures. In Workshop on Big Learning, NIPS , 2012.
 Neal, R. M. Markov chain sampling methods for
Dirichlet process mixture models. Technical Re-port 9815, Dept. of Statistics, University of Toronto, 1998.
 Rodriguez, A. On-line learning for the infinite hid-den Markov model. Communications in Statistics -Simulation and Computation , 40(6):879 X 893, 2011. Sohn, K.-A. and Xing, E. P. A hierarchical Dirichlet process mixture model for haplotype reconstruction from multi-population data. Ann. Appl. Stat. , 3(2): 791 X 821, 2009.
 Sudderth, E. B., Torralba, A., Freeman, W. T., and
Willsky, A. S. Describing visual scenes using trans-formed Dirichlet processes. In NIPS , 2005.
 Teh, Y.-W., Jordan, M. I., Beal, M. J., and Blei,
D. M. Hierarchical Dirichlet processes. Journal of the American Statistical Association , 101(476): 1566 X 1581, 2006.
 Teh, Y.-W., Kurihara, K., and Welling, M. Collapsed variational inference for HDP. In NIPS , 2007. Ulker, Y., Gunsel, B., and Cemgil, A. T. Sequen-tial Monte Carlo samplers for Dirichlet process mix-tures. In AISTATS , 2010.
 Wang, C., Paisley, J., and Blei, D. M. Online vari-ational inference for the hierarchical Dirichlet pro-cess. In AISTATS , 2011.
 Xing, E. P., Ng, A. Y., Jordan, M. I., and Russell, S.
Distance metric learning, with application to clus-
