 Since the website is one of the most important organizational structures of the Web, how to effectively rank websites has been essential to many Web applica tions, such as Web search and crawling. In order to get the ranks of websites, researchers used to describe the inter-connectivity among websites with a so-called HostGraph in which the nodes denote websites and the edges denote linkages between websites (if and only if there are hyperlinks from the pages in one we bsite to the pages in the other, there will be an edge between these two websites), and then adopted the random walk model in the HostGraph. However, as pointed in this paper, the random walk over such a HostGraph is not reasonable because it is not in accordance with the browsing behavior of web surfers. Theref ore, the derivate rank cannot represent the true probability of visiting the corresponding website. of visiting a website by the random web surfer should be equal to the sum of the PageRank values of the pages inside that website. Nevertheless, since the number of web pages is much larger than ranks of websites on the calculation of PageRank. To tackle this problem, we proposed a novel method named AggregateRank rooted in the theory of stochastic complement, which cannot only approximate the sum of PageRank accurately, but also have a lower computational complexity than PageRank. Both theoretical analysis and experimental evalua tion show that AggregateRank is a better method for ranking webs ites than previous methods. H.3.3 [Information Search and Retrieval]: Search process Algorithms, Performance, Theory, Experimentation Keywords: AggregateRank, Stochastic Complement Theory, Coupling Matrix In the traditional view, there are two kinds of essential elements that make up of the Web. One is the web page and the other is the hyperlink, corresponding to the content and structure of the Web respectively. In recent years, many researchers have realized that the website, another element of the Web, has played a more and more important role in the Web search and mining applications [1][2][3][4][5]. As compared to the individual web page, the website can sometimes provide plen ty of semantic information in the following aspects. First, pages in the same website may share the same IP, run on the same web server and database server, and be authored / maintained by th e same person or organization. Second, there might be high correlations between pages in the same website, in terms of conten t, page layout and hyperlinks. Third, from the topological point of view, websites contain higher density of hyperlinks inside them (about 75% according to [7]) and lower density of edges in be tween [6]. These properties make websites semantically important to understand the whole picture of the Web. 
Actually, the ranking of websites have been a key technical component in many commercial sear ch engines. On the one hand, web pages because those pages from important websites tend to be important as well. On the othe r hand, in the crawler part, it can help crawl pages from those impor tant websites first, or help determine a quota (number of pages) for each website in the index total, search engines can achieve the best tradeoff between index coverage and index quality. 
In the literature of website ranki ng, people used to apply those technologies proposed for ranking web pages to the ranking of websites. For example, the fa mous PageRank algorithm, which was proposed by Brin and Page in 1998 [8] was used to rank websites in [9] and [10]. As we know, PageRank uses a random walk model in the Web graph to describe the behavior of Web surfers. That is, a random surfer is supposed to jump from a page to another following the hyperlinks with a probability of  X  , or jumps to a random page with a probability of 1- X  . In order to apply PageRank to the ranking of websites, a HostGraph was constructed in these works, in which the nodes denote websites and the edges denote linkages be tween websites (if and only if there are hyperlinks from the pages in one website to the pages in the other, there will be an e dge between these two websites). According to different definiti ons of the edge weights, two categories of HostGraphs were used in the literature. In the first category, the weight of an edge between two websites was defined by the number of hyperlinks between the two sets of web pages in these sites [11]. In the second category, the weight of any edge was simply set to 1 [12]. For the sake of clarity, we refer to the two categories as weighted HostGraph and na X ve HostGraph respectively. Figure 1 and 2 show how these two categories of HostGraphs can be constructed. 
Figure 2. Illustration for (a ) weighted HostGraph and (b) 
After constructing the HostGraph, the similar random walk was conducted. That is, a random surfer was supposed to jump between websites following the edges with a probability of  X  , or jump to a random website with a probability of 1- X  . In such a way, one can obtain the HostRank, whic h is the importa nce measure of websites. At the first glance, the above random walk model over the HostGraph seems to be a natural extension of the PageRank algorithm. However, we want to poi nt out that it is actually not as reasonable as PageRank because it is not in accordance with the browsing behavior of the Web surfers. As we know, real-world web surfers usually have two basic ways to access the web. One is to type URL in the address edit of the web browser. And the other is to click any hyperlink in the current loaded page. These two manners can be well de scribed by the parameter  X  used in PageRank. That is, with a probability of 1- X  , the web users visit a random web page by inputting its URL (using favorite folder can also be considered as a shortc ut of typing URL), and with a probability of  X  , they visit a web page by clicking a hyperlink. Nevertheless, as for the random wa lk in the HostGraph, we can hardly find the same evident correlation between the random walk model and real-world user behaviors. For example, even if there is a edge between two websites A and B in the HostGraph, when a web surfer visits a page in website A , he may not be able to jump to website B because the hyperlink to website B may exist in another page in website A which is even unreachable from the page that he is currently visiting. In other words, the HostGraph is only a kind of approximation to the web graph: it loses much transition information, especially as for the na X ve HostGraph . As a result, we argue that the rank values derived from the aforementioned HostGraph are not convincing enough, and a more reasonable way to define th e ranks of a website should be the probability that the users vis it a random page in that website. Actually, it can be proved that this probability is equal to the sum of the PageRanks of all the pages in that website (denoted by PageRankSum for ease of reference). 
However, it is clear that using PageRankSum to calculate the ranks of websites is not yet a f easible solution, especially for those applications that only care a bout the websites (i.e. those site ranking services such as http://www.alaxa.com). The reason is that the number of web pages (usually measured in billion. Google announced that it indexed over 8 billions pages, and Yahoo! claimed an even larger index of 20 billions [13]) is much larger than the number of websites (usually measured in million. According to [14], there are around 70 million websites by the end of 2005). Therefore, it is much more complex to rank web pages than to rank websites, and it is almost impossible for small research groups or companies to afford such kind of expensive computations. 
To tackle these aforementione d problems, we propose a novel method based on the theory of st ochastic complement [15] to calculate the rank of a website that can approximate the PageRankSum accurately, while the corresponding computational complexity is lower than PageRankSum. We name this algorithm by AggregateRank. Experiments demonstrated the effectiveness and efficiency of this algorithm. 
The rest of this paper is organi zed as follows. In Section 2, we give the specific probability explanation for AggregateRank with the random walk model, and then conduct convergence rate analysis and error bound analysis. Then in Section 3, some experiments with a real web page collection are shown in details. Conclusions and future work are discussed in Section 4. As aforementioned, website ranking is a very important technology for Web search. However, traditional approaches on calculating website ranks lost some transition information of the random surfer. To tackle this problem, we will propose our solution in this section. In partic ular, we will first discuss how to define a more reasonable website rank, and then propose a method for efficient calculation of this rank. After that, we will also discuss the error bound and convergence rate of the proposed method. 
In this subsection, we will discuss what the probability of visiting a particular website is, under the framework of Marokv random walk. For this purpose, we need to define the transition probability matrix, called coupling matrix, between websites. And then use the stationary distribution of the coupling matrix to denote the probabilities that the random surfer visits each website. 
For the sake of the convenience of conduction, we intend to describe the random surfer model in mathematics at the beginning. The random surfer model can be formally described with a pages. The current state of the Markov chain is P evolution of the Markov chain represents the surfing behavior of the random surfer from one Web pa ge to another. Suppose there are N sites, n pages, and r hyperlinks in total. Note that according probability transiting from P i to P j is equal to otherwise, where d i denotes the out-degree of page i . Usually, this transition probability matrix 1 of the surfing Markov chain is denoted by damping factor. The PageRank is the stationary distribution of [16]. 
For the next step, we will discuss how to induce the transition probabilities between websites from the aforementioned page-level Markov chain { X k } k  X  0. For this purpose, we proceed as follows. 
As each page belongs to some determinate site, we rearrange the transition probability matrix P (  X  ) to another n X n transition probability matrix Q (  X  ), which can be partitioned into N X N blocks according to the N sites and has the following form, where elements in each diagonal block denote the transition probabilities between pages in the same website, and elements in each off-diagonal block denote the transition probabilities between pages in different we bsites. The diagonal blocks Q are square and of order n i ( n i is the number of pages in website i ), and PageRank, is given by where e is the column vector with all elements equal to 1. Partitioning  X  (  X  ) conformably with Q (  X  ), we get  X  (  X  )={  X  i =1, ..., N . 
Till now, we just get a rearranged PageRank vector. To move forward, we will turn our atten tion to the probability that the i =1, ..., N ), when the page-level surfing Markov chain attains equilibrium after long-time evolution. We assume that the Markov chain { X k } k  X  0 is started with the stationary probability vector  X  (  X  ) [17]. Then, the one-step transition probability from S to S j is defined by In fact, there are some details in PageRank computation [16], such as how to handle dangling nodes. 
By the properties of conditional probability and the definition of c ij (  X  ), we can get whose elements represent the transition probabilities between matrix [15], so that it possesses a unique stationary probability vector. We use  X  (  X  ) to denote this stationary probability, which can be got from 
One may have realized that the above computation can also be regarded as being carried out with a certain HostGraph. However, the edge weight of this ne w HostGraph is not decided heuristically as in previous wo rks [9][10], but determined by a sophisticated formulation (6). Be sides, the transition probability surfer jumps from any page in S i to any page in S transition steps. Therefore, the tr ansition in this new HostGraph is in accordance with the real behavior of the Web surfers. In this regard, the so-calculated rank from the coupling matrix C (  X  ) will be more reasonable than those previous works. 
After describing how the probability of visiting a website can validate that  X  (  X  )={||  X  1 (  X  )|| 1 , ||  X  2 (  X  )|| matrix, its stationary distribution is unique. So,  X  (  X  )={||  X  ||  X  probability of visiting a website is equal to the sum of PageRanks our intuition. 
Based on the above discussions, the direct approach of computing the ranks of websites is to accumulate PageRank values (denoted by PageRankSum). However, this approach is unfeasible because the computation of PageRank is not a trivial task when the number of web pages is as large as several billions. Therefore, efficient computation becomes a significant problem . In the next subsection, we will propose an approximate algorithm for this purpose, which can be mo re efficient than PageRankSum with very little accuracy loss. aforementioned probabilities of visiting a website in an efficient manner. First of all, we will discuss how to use the theory of stochastic complement to speed up the calculation of PageRankSum, and then we will discuss the convergence speed and the error bound of the proposed algorithm. || x || 1 = || in the row vector x . cQe from one site to the other in te rms of random surfer model; and its stationary distribution  X  (  X  ), which is equal to PageRankSum, is regarded as a reasonable rank of websites. 
It is clear that the construction of the coupling matrix asks for the calculation of PageRank over th e whole Web graph. To avoid this time-consuming st ep, it is necessary to invent a new method stochastic complement [15] gi ves a good solution to approximate C (  X  ) without PageRank values. To illustrate this, we take a simple web graph for example. Suppose th e web graph only contains two websites, and the transition probability matrix of this Web graph (rearranged according to website information) can be denoted by And its stationary distribution is  X  (  X  )={  X  1 (  X  ),  X  complement. For example, the stochastic complement of Q defined as below, The stochastic complement is also a stochastic matrix, each row of which is summed up to 1. It can be proved that  X  1 (  X  )/||  X  is the unique stationary probability vector for the stochastic complement S 11 (  X  ), i.e. the stochastic complement S ii (  X  ) , i.e. 
Intuitively, the computation of the stationary distribution of each S ii (  X  ) will be cheaper than that of PageRank directly because number in this site (we will discuss this in more detail in the next subsection). However, it is time consuming to compute the exact stochastic complement since we should compute a contradict matrix is very expensive. T hus, we prefer an approximate approach to get the stationary distribution of each stochastic complement instead. According to [ 15], there is such an efficient method. It does not use (9) to aggregate the stochastic complement directly. Instead, it only modifies each diagonal block Q ii (  X  ) by a little to get a new matrix with the same dimension as S ii (  X  ). The details are given as follows. 
For the first step, we modify the original diagonal block Q each row in the original diagonal block Q ii (  X  ) is always less than 1. To make it a transition probability matrix, we simple adjust the diagonal elements of Q ii (  X  ) (added or subtracted by a small value) to make the sum of each row equal to 1. Letting Q * ii (  X  ) denote the distribution u i (  X  ) as follows, According to [15], we can prove that From the description above, Q * ii (  X  ) is very easy to get from Q Moreover, it can even be stored sparsely like original Q (  X  ). Thus, formulation (13) means that we can get each  X  i (  X  )/||  X  efficiently. 
Utilizing the result of (13), we can obtain an approximate coupling matrix C * (  X  ) as below, 
Consequently, the stationary distribution  X  * approximate coupling matrix can be regarded as a good approximation to  X  (  X  ). We name the aforementioned algorithm by AggregateRank, which detailed algorithm flow is shown as in Figure 3. N websites. 2. Construct the stochastic matrix Q * ii (  X  ) for Q the diagonal elements of Q ii (  X  ) to make each row of Q summed up to 1. 3. Use power method to calculate u i (  X  ) from 4. Construct the approximation coupling matrix C * (  X  ) as follows 5. Use power method to get  X  * (  X  ) 
To sum up, the proposed algorithm improves the efficiency in stochastic complement, to compute each  X  i (  X  )/||  X  the whole transition probability ma trix. Second, it uses a easy-to-construct sparse matrix to replace the stochastic complement to be implemented in parallel than PageRankSum. The reason is that for PageRank, if we want to im plement it in parallel, we must take care of the information exchange between different servers since there are hyperlinks which sources and destinations are not in the same server. While, for our method, we do not need the exchange of information if we put a website at most in one sever website and is independent of othe r part of the whole web graph. AggregateRank algorithm, we divide the Web graph into websites, and conduct power-method iterations within each website. After that, we apply power method once again to the coupling matrix C (  X  ). It is easy to understand that, in this way, we can save some memory and the corresponding algorithm is easier to be implemented in parallel. When we deal with the Web graph with billions of pages, this advantage will become very meaningful. 
However for the computational complexity, it is not obvious whether the proposed method can be more efficient. The reason is that PageRank has a complexity of O ( r ). Considering that 75% of the hyperlinks connect pages in the same website [7], dividing the Web graph into websites can only save 25% propagations along hyperlinks and thus the complexity is still around O ( r ). Furthermore, for the computation in Step 5, it is not obvious whether C * (  X  ) is also a sparse matrix, thus its computational complexity might be as high as O ( N 2 ) in the worse case. All of this can be a big issue. 
In this subsection, we will discuss the aforementioned problems in detail. Specifically, we will prove in Subsection 2.2.2.1 that although the overall complexity of one iteration of power method can be significantly faster than PageRank over the whole Web graph. And then we will prove in Subsection 2.2.2.2 that C actually also a sparse matrix, and there are only O( N ) non-zero elements in this matrix. Therefore, the computation of calculating for the PageRank matrix. In order to understand the convergence speed of the power method applied to Q * ii (  X  ), we need to review the following Lemma at first. As we know, the convergence speed of the power method is determined by the magnitude of the subdominant eigenvalue of the transition probability matrix [18]. Lemma 1 just tells us the relationship between matrix P (  X  ) and its eigenvalues [16]. Lemma 1 (Langville and Meyer) Given the spectrum of the stochastic matrix P as {1,  X  2 the spectrum of the primitive stochastic matrix P (  X  ) = is {1,  X  X  2 ,  X  X  3 , ...,  X  X  n } , where v T is a probability vector. 
In order to use Lemma 1 to analyze the convergence speed of the power method applied to Q * ii (  X  ), we transform Q following form where *
Given the eigenvalues of * 1, we can get that the eigenvalues of Q 1, , , ,  X   X   X   X   X   X  . Since the convergence speed of the power method is determined by the magnitude of the subdominant eigenvalue of Q * ii (  X  ), we can conclude that the convergence rate of the power method applied to Q * ii (  X  ) is around the rate at which 2 ( ) 0 k  X   X   X  . 
As we know, the convergence speed of PageRank is around the rate at which  X  k  X 0 . So whether we can be more efficient than PageRank is determined by how small 2  X  could be. According to the following discussions, we know that 2 1  X  . 
As we know, the web link graph has a natural block structure: the majority of hyperlinks are intra-site links [19]. Therefore, the random walk on the web with the transition matrix Q ( viewed as a nearly completely decomposable Markov chain. According to [15], we know that when the states in a nearly completely decomposable Markov chain are naturally ordered, and when the transition matrix is partitioned into N closely coupled subclasses in the natural way, the underlying transition matrix of the Markov chain has exactly N -1 non-unit eigenvalues clustered near  X  =1 (There are pathological cases, but they are rare in practical work [15]). Thus Q (  X  ) has exactly N -1 non-unit eigenvalues clustered near  X  =1. Since Q * ii (  X  ) is an irreducible stochastic matrix, the Perron-Frobenius theorem [20] guarantees that the unit eigenvalue of each Q * ii (  X  ) is simple. Because Q * ii (  X  )  X  Q ii the eigenvalues, the non-unit eigenvalues of Q * ii (  X  ) must be rather far from the unit eigenvalue of Q * ii (  X  ). Otherwise the spectrum of Q (  X  ) would contain a cluster of at least N non-unit eigenvalues positioned near  X  =1. As a result, we can come to the conclusion that 2 1  X   X  , and then 2 1  X  . That is, the convergence speed of power method applied to Q * ii (  X  ) is much faster than that of PageRank. As mentioned at the beginning of this section, the sparseness of the matrix C * (  X  ) is a critical factor that influences the computational complexity of our proposed AggregateRank algorithm. To understand this, we conduct the following discussions. 
First of all, we transform Q (  X  ) into the following form where Q is the transition matrix whose element q probability of moving from webpage i to webpage j in one step following the hyperlink structure of the Web Graph, a is a vector whose element a i =1 if row i of Q corresponds to a dangling node, and 0, otherwise,  X  is damping factor, and e is a column vector of all ones. Then we investigate the construction process of C follows, where ( ) U According to this decompositi on, in Step 4, we actually only need to compute A =: U (  X  ) QV . The corresponding count of multiplications is O ( r ). Note that we do not need any iteration here, so the complexity of Step 4 is actually much lower than PageRank that will take tens or even hundreds of iterations to converge. 
Then it is clear that the computational complex of each iteration in Step 5 depends on the number of non-zeroes in A . Because a ij equals to the linear combin ation of the elements in block Q ij , we have a ij =0 when every element is 0 in Q that the average number of sites th at a particular website links to is  X  , then A has  X  non-zeroes in each row and the number of non-constant which is tens or so [11], we can come to the conclusion that the computational complex of one iteration in Step 5 is O ( N ) &lt;&lt; O ( r ). As one can see, the proposed AggregateRank algorithm is an approximation to PageRankSum. In this subsection, we will discuss the error bound of this approximation. 
According to the theory of stochastic complement, the approximation as shown in (13) requires the matrix to be nearly completely decomposable. This c ondition is well satisfied in real Web applications, because about 75% of the hyperlinks connect pages in the same website [7], and it is reasonable to treat the transition probability matrix Q (  X  ) as a nearly completely decomposable matrix. According to the discussions in Section 2.2.2, Q (  X  ) has exactly N-1 non-unit eigenvalues that are very close to the unit eigenvalue. Thus, the approximati on in (13) has an upper error bound according to [21], which is determined by the number of pages n , the number of sites N , the size of each site n condition number of the coupling matrix  X  ( C (  X  )), the deviation from complete reducibility  X  3 and the eigen structure of the probability transition matrix Q (  X  ). Theorem 1 If Q(  X  ) has exactly N-1 non-unit eigenvalues close to the unit eigenvalue, then there are the following error bounds: Not as popular as other concep ts, the deviation from complete reducibility is defined in [21]. From Theorem 1, we can see that the upper error bound of the AggregateRank algorithm principally depends on the scale of the largest website. Evidently, the number of pages in the largest website is much smaller than the size of the Web, i.e. m = o ( n ). In this regard, we can say that the corresponding error bound is well controlled. In our experiments, the data corpus is the benchmark data for the Web track of TREC 2003 and 2004, which was crawled from the .gov domain in the year of 2002. It contains 1,247,753 pages in total. Before testing our proposed method, we need to partition the Web graph into websites. For this purpose, we follow the rules as below. Because the URLs in the .gov domain are very regular, we can easily decide the site that a page belongs to. After removing slash can be considered as the s ite name. However, because there maybe some subsites, we only use the adjacent word before .gov as the identifier for the site . For example, http://aaa.bbb.gov/xxxx belongs to the website bbb . 
After this preprocess, we get 731 sites in the .gov dataset. The largest website contains 137,103 web pages while the smallest websites is shown in Figure 4. It nearly follows a power law and is consistent with previous res earch on the sizes of websites [22]. In our experiment, we validated whether the proposed AggregateRank algorithm can well approximate PageRankSum. For comparison, we also inve stigated other two HostRank algorithms, which work on the weighted HostGraph and na X ve HostGraph respectively [9]. The differences between PageRankSum and the algorithms under investigation are shown in Table 1, in terms of Euc lidean distance between the rank vectors. 
From Table 1, we can see that the AggregateRank algorithm has the best performance: its Euclidean distance from PageRankSum is only 0.0057, while the ranking results produced by the other two algorithms are farther from PageRankSum, with Euclidean distances of 0.1125 and 0.1601 respectively. Table 1. Performance Evaluation based on Euclidean Distance 
In addition to the Euclid dist ance, we also used another similarity measure based on the Kendall X  X   X  distance [23] to evaluate the performance of th e ranking results. This measure ignores the absolute values of the ranking scores and only counts the partial-order preferences. Therefore, it can better reflect the true ranking performance in real applications. This similarity between two ranking lists s and t is defined as follows. where K ( s , t ) is the Kendall X  X   X  distance , which counts the number of pair-wise disagreements between s and t , and is defined as below. According to the definition, the larger Sim ( s , t ) is, the more similar two lists are. If the two ranking lists are consistent with each other, their Sim measure is equal to 1. 
We list the performance evaluation of the aforementioned algorithm based on Kendall X  X   X  distance in Table 2. From this table, once again we can see that the AggregateRank algorithm is the best approximation to Pa geRankSum. Furthermore, the advantage of the AggregateRank algorithm over the reference algorithms becomes even more obvious if we look at the top-k PageRankSum, and obtained their order O p . Then, we got their relative orders according to other ranking algorithms, i.e. O and O n which correspond to the AggregateRank algorithm, the weighted HostRank algorithm a nd the na X ve HostRank algorithm respectively. We plot the similarity based on the Kendall X  X   X  distance between these top-k ranking results in Figure 5. 
Table 2. Performance Evaluation of Ranking Algorithms Figure 5. Similarity between PageRankSum and other three ranking results. 
After the comparison on similarity, we compare these ranking algorithms on complexity as well. As discussed in Section 2, the AggregateRank algorithm can converg e faster than PageRank. To verify this, we use the L 1 -norm of the difference between the current ranking list and the last one to measure whether the power method converges. When this difference is less than 10 regard the computation as converg ed and the computation process is terminated. The running time of each algorithm is shown in Table 3. 
From Table 3, we can see that the proposed AggregateRank method is faster than PageRankSum, while a little more complex than the HostRank methods. This is consistent with the theoretical analysis in the previous section. The fast speed of AggregateRank mainly comes from the fast convergence speed. And the fast speed of HostRank comes from the low dimension of the HostGraph. 
In summary, by taking the effectiveness and efficiency into consideration at the same time, we consider the proposed AggregateRank algorithm as a bette r solution to website ranking. Considering that website is an important organizational structure of the Web, we discussed how to rank websites in this paper. In particular, we pointed out that the widely-used approach which used a simple HostGraph to represent the inter-connectivity between websites and adopt the random walk model to get the rank of the websites is not in accordance with the browsing behavior of real Web surfers. As a result, the so-calculated rank of the websites cannot reflect the probability of visiting the corresponding website. In order to solve this problem, we proposed a novel algorithm named AggregateRank, and proved mathematically that it can well approximate the probability of visiting a website. We also inve stigate the convergence speed and error bound of this proposed method, which indicates that it is not only effective but also very efficient. Experiments on the benchmark dataset for TREC 2003 Web track verified the aforementioned theoretical findings. [1] Despeyroux, T. Practical Semantic Analysis of Web Sites [2] Hasan Davulcu, Srinivas Vadrevu, Saravanakumar [3] Kristina Lerman, Lise Getoor, Steven Minton, Craig [4] Yuangui Lei, Enrico Motta, Domingue. Modelling Data-[5] Tao Qin, Tie-Yan Liu, Xu-D ong Zhang, Guang Feng, Wei-[6] M. Girvan and M. E. J. Newman. Community structure in [7] Henzinger, M.R., Motwani, R., Silverstein, C. Challenges in [8] L. Page, S. Brin, R. Motwani, and T. Winograd. The [9] N. Eiron, K. S. McCurley, and J. A. Tomlin. Ranking the [10] Jie Wu, Karl Aberer. Using S iteRank for P2P Web Retrieval. [11] Krishna Bharat, Bay-Wei Cha ng, Monika Henzinger, and [12] S. Dill, R. Kumar, K. Mc Curley, S. Rajagopalan, D. [13] http://www.researchbuzz.co m/2005/09/google_celebrates_7_ [14] http://www.netcraft.com [15] C. D. Meyer. Stochastic complementation, uncoupling [16] Amy N. Langville and Carl D. Meyer. Deeper inside [17] John G Kemeny and J.Laurie Sn ell, Finite Markov Chains [18] William J. Stewart. Introduction to the Numerical Solution of [19] S. Kamvar, T. Haveliwala, C. Manning, a nd G. Golub. [20] F. R. Gantmacher, Matrix Theory (Chelsea, 1959) Vol. II, [21] G. E. Cho and C. D. Meyer. Aggregation/Disaggregation [22] R. Albert and A.-L. Barabasi . Statistical mechanics of [23] M. Kendall and J. Gibbons. Rank Correlation Methods. 
