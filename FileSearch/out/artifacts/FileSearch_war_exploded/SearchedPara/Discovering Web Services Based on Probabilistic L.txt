 Web services have emerged as one of distributed computing technologies and sparked and modular applications. Because web serv ices adopt open standard interfaces and the Internet. With web services, business organizations can build their applications by outsourcing some other services published on the Internet. As an ever-increasing service users to discover desired serv ices that match their requirements. 
The main processes of discovering and matching services involve several activities which are performed in the collaboration between clients and web services databases. service registry like Universal Description, Discovery and Integration (UDDI) [17] to would create a request/ response invocation on the matched services described by the Web Services Description Language (WSDL). 
At present, one of the dominating industrial techniques for web services discovery is to use UDDI registry. The UDDI is an online electronic registry, where web services are registered and described as core type of information: white pages with contacting details, yellow pages containing classification information based on standard taxonomies and allows syntactically search and category-based match web services. In addition, a service requester can use the Inquiry API provided in UDDI for retrieving services via submitting instructions like find_service ().

However, the keywords-based mechanism supported by UDDI and most of the existing service discovering and matching approaches [10, 21, 22] show some user types inaccurate keywords for searching services, he either receives numerous responses that may be totally irrelevant to his needs or get no answers at all. Second, ineffective and time-consuming. Another drawback of the most existing approaches is that they take into account only the keywords in users X  requirements and textual behind the descriptions of web services. 
To address these issues, we need an effective mechanism by which the real intention of service users can be associated to the advertisements of web services and an effective approach by which a user X  X  quer y is conceptually matched to the contents advertised in web services available. 
In this paper, we present a novel approach for discovering and matching web services. Based on the current dominating mechanisms of discovering and describing web services with UDDI and WSDL, the proposed method utilizes Probabilistic Latent Semantic Analysis to capture the semantic concepts hidden behind the words in queries and the advertisements in services so that services matching is expected to be carried out at concept level. problems and related research work. In Section 3, we briefly discuss our probabilistic semantic discovering approach. The detailed matching principle, probabilistic model and matching algorithm are introduced in section 4. The preliminary experiment found in Section 6. haystack [3]. This discovery process mainly involves locating the relevant services requirements of users with a set of services and recommending desired services to the consumers. 
To effectively discover and match web services, it is a necessary to establish some kinds of the correlations between a user and potential services available, which can be achieved through two steps. On the client side, a service user can express his require-ments described in the form of nature language and then use a service search engine to interact with a set of potential web services. On the side of services providers, on the other hand, they advertise services X  capabilities through some descriptions such as the web services X  names, the operations X  descriptions and the operations X  names with the agreement and how to associate the users X  requirements to the advertisements of web services would have a critical impact on discovering web services. Therefore, locating desired services might be difficult. 
A commonly used approach on discovering and matching web services is to directly shown in Fig. 1.). For example, a user types keywords in a web service search engine [24] to look for the desired web services. If the typed words are included in or identical to the descriptions of some services, the return services might be relevant to his need. Nevertheless, this approach based on the term frequency analysis is insufficient in the majority of cases. For one thing, syntactical different words may have similar semantics (synonyms), which results in low recall. For another, semantically different concepts could possess the identical representation (homonyms), thus leading to low precision. In short, this discovery mechanism fails to contemplate the semantic concepts hidden behind the words in a query and the descriptions in web services. Fig. 1.). This relies on finding common semantic concepts between the terms in a query and services X  advertisements. Then the similarity between a query and services concepts in the descriptions of web services has been implemented in [16]. The element in a vector is assigned a TF-IDF weight. With this method, returned m value decomposition (SVD) of the matrix is employed to discover the associated patterns between the words and their corresponding concepts. Thus, a commonly used cosine measure of the similarity between two vectors can conceptually represent how close they are in a semantic space even if a service doesn X  X  contain terms in a query. Although this approach shows some advantages compared to the keywords-based one, further applications. 
More recently, ontology-based approaches [13, 15] have been seeking to use ontology to annotate the elements in web services. Such techniques, based on a theory on existence, organize domain knowledge into the categories by virtue of objects X  components and their semantic connectives so that the suggested approaches aim to not only capture the information on the structure and semantics of a domain, but maintaining ontology may involve huge amount of human effort [9]. We leave this interesting issue to be addressed in the near future. Some other recent work can also be fond in [11, 4]. 
We propose to extend the SVD of the matrix approach of matching web services [16] with a different methodology called Probabilistic Latent Semantics Analysis better performance. introduce our probabilistic latent factor discovering approach. 3.1 Services Descri ption and Specification Since the WSDL and UDDI are currently the dominating mechanism for web services description and discovery, we focus on discovering and matching web service in this context, rather than using ontology to annotate elements in web services. 
Normally, a web service can be described by WSDL as a collection of network endpoints. The description consists of two main parts: the abstract definition of interfaces and the concrete implementations of network. In the abstract definition, interfaces and a set of operations are defined by portType element and operation element respectively. Besides, each operation may contain input/output messages that are defined by message element. On the other hand, the concrete implementations include particular binding protocols like SOAP and network address. Similarly, a set details. the interface definition from the network implementation and making it possible to multiple deployments on the identical interface. Moreover, it would facilitate the reuse of services and an example of WSDL file for a CargoShipping service is shown in Figure 3. 3.2 Overview of Our Probabilistic Approach Our approach is based on our observation of uncertainty on the usage of web services words to indicate semantic concepts because of the dictionary problem [2]. Furthermore, as mentioned, homonyms and synonym also have negative impact on effectively discovering and matching web services. On the other hand, different service providers services are priori unknown [12], which makes the discovery of services more challenging. the intention of a user to the advertisements in web services by applying Probabilistic Latent Semantics Analysis, which is expected to capture the semantic concepts hidden in the descriptions in web services. As a result, web services can be matched against a query at concept level. 
Figure 4 illustrates the outline of the proposed probabilistic latent semantic approach. To begin with, the approach will filter out those web services whose types available. Then the Probabilistic Latent Semantic Analysis is used to the match semantic similarity between a query and web services. Finally, the Quality of Services (QoS) measure will be combined with the proposed semantic measure to produce a final score that reflects how semantically close the query is to available services. 4.1 PLSA Introduction Our probabilistic approach is based on the PLSA model that is called aspect model [5]. PLSA utilizes the Bayesian Network to model an observed event of two random objects with a set of probabilistic distributions. In the text context, an observed event corresponds to occurrence of a word w occurring a document d . The model indirectly associates keywords to their corresponding documents through introducing an observation of a word w in a document d . In our context, each observation corresponds to a service user accessing to services by submitting a query for locating desired web services. Thus, the generative probabilistic model is expected to infer the common semantic concepts between a query and services. PLSA model works like this:  X  Select a document i d from a corpus of documents with probability ) ( i d P  X  Select a latent factor f z with probability ) ( i f d z P
Based on the assumption that a document and a word are conditionally independent obtained from the probabilistic model is shown as following: 
Where of parameters. To simplify the computing procedure, an iterating approach is adopted. model presents a new estimation on the previous iteration. In our context, an objective function based on the whole data collection is: Thus, a log likelihood function of an observation is defined as following: Where ) , ( j i d w m denotes the frequency of a word i w occurring in a document j d . PLSA uses the Expectation-Maximization (E M) [5] algorithm to learn the model. The whole learning process starts with ra ndomly assigning initial values to the steps: E-step and M-step. In E-step, based on the current estimation of the parameters, () , j i w d . In M-step, the parameters are updated based on the probabilities computed in the previous E-step. 
This learning process can be summarized as following: value occurrence of j w occurring in i d is computed as:  X 
And in M-step, according to the previous values, the parameters are updated for the conditional likelihoods of the observation: 
PLSA was originally used in text context for information retrieval and now has been used in web data mining [19]. In this paper, we utilize PLSA for discovering and matching web services. 4.2 Web Services Information Processing The overall process of discovering web services includes information collecting, data processing, data representation and similarity matching (see Section 4.3). The information collecting: The main consideration on the information source in our discovering approach is based on the current specification of WSDL description and UDDI discovery mechanism. As each web se rvice has its associated WSDL file such as name and textual description in the WSDL file. This kind of information will be used to decide whether a web service X  X  category is relevant to a user X  X  query. 
The data processing stage consists of transforming raw web service information the commonly used approaches for the words processing are applied. As descriptions and names are likely concatenated by a sequence of strings where individual word are separated so that each token conveys some meaning. Other methods of data processing include the word stemming and the stopwords removing. The former intends to remove common term suffixes while the latter eliminates very frequently used words. The data representation: In the PLSA model, the pre-processing information relationship between them is ignored. A ccording to this, each web service can be represented as a vector. Definition 1. (Service Document) A service document (SD) is defined as a the document and m denotes the size of a vocabulary. The words in SD are extracted from service name, service description, operation name and input/output names in the WSDL file.  X  Suppose we have a corpus of N web services with } ,..., , { 2 1 n i ws ws ws WS ws =  X  and a Based on this, we can define a service matrix as following: Definition 2. (Service Matrix) A service corpus is defined as an M by N Service Matrix(SM), where ) , ( j i d w n denotes the number of occurrences of a word i w appearing in the document j d.  X  4.3 PSMA  X  Probabilistic Semantic Matching Algorithm variables, then the similarity of a query in respect to the services in its relevant group can be computed in a smaller size of collection of web services. The learnt latent variables can be used to characterize web services. From formula First, the right-hand side of the formula indicates a matrix decomposition, that is, the aspect model expresses dimensionality reduction by mapping a high dimensional term document matrix into a lower dimensional one(k dimension) in a latent semantic label for this service. What is more, in a dimension-reduced semantic space, each dimension represents a semantic concept and the services with similar semantic concepts are projected to be close each othe r. Based on the discussion, we employ the following formula to infer the relationship between a web service and hidden factors. An algorithm of the category matching is shown as following: 
CategorizingServices(SM, K) begin: Input: service matrix SM = { n sm sm sm ,..., , 2 1 } k: the number of latent factors output: k service communities: SC = { k sc sc sc ,..., , 2 1 } Repeat for each service i sm in SM { for each hidden variable j hv ,j=1,...,k } Return SC end. 
After clustering the services into thei r corresponding concept groups, we will by finding the query X  X  correlated concept group. As a new query may be outside the model, in this case, it needs to be added to or folded in the model through the iterative EM steps, where the probabilistic distribution ) | ( z w P over the words conditioned on computed. With this way, the web services whose types are not compatible to a user X  X  On the next step, we can utilize commonly used cosine measure to decide how semantically similar a query is to each of services in the group. Finally, we believe that service selection should comply with the standard of Quality of Services (QoS). Therefore, in our application, the process of selecting services involves two steps:  X  Step 2: combine QoS with semantic matching to produce a final score. produce a final ranking for the specific web service: Preliminary experiments were carried out on the corpus of web services whose WSDL files can be accessed via the service collections published in XMethods [20] General Information, Location Finder, Translation Services and Business Services. We extract related information such as names and textual descriptions in the WSDL files and all extracted information is proce ssed with the approaches introduced in the previous section. Thus, we obtain a corpus of services consisting of 77 services which are divided into two data sets: training data and testing data. 
The extracted training data are used to fit the PLSA model. We train the model categories. 
In order to evaluate the outcome, we compute precision and recall, and apply semantic concepts hidden behind the words in a query and the advertisements in services. match users X  requirements. In this paper, we studied the current issues of the existing methods for discovering web services and proposed a novel approach to deal with the issue. Based on the current industrial standards, our approach is to indirectly associate the intention of users to the advertisements in web services by applying Probabilistic Latent Semantics Analysis, which is expected to capture the semantic concepts hidden in the descriptions in web services. Consequently, web services can be matched against a query at concept level in a dimension-reduced semantic space. 
We also showed a matching algorithm based on the probabilistic model and preliminary evaluation indicates that the pr oposed approach improve the recall of web service discovering. 
Finally, our probabilistic semantic approach is the first step towards effectively discovering web services. The ongoing project is to investigate the unification of the proposed approach with ontology toward s effectively discovering web services. Much of the data in an enterprise is strictly-typed and thus can be meaningfully been the mainstay of the RDBMS products like DB2. 
However, this  X  X tructured X  data constitutes only a fraction of the entire information content within an enterprise, which also includes  X  X nstructured X  content like analytical reports, email, meeting minutes, web-pages etc. Due to its free-flow, untyped structure, RDBMS as the strictly-typed operational data. Such data is stored in a content manager, like the IBM Content Manager [4], which associates the unstructured data, the document. The unstructured content is then retrieved by querying the metadata. 
In any enterprise today, the structured data is managed by the database system (say, IBM DB2), the unstructured data is managed by the content manager (say, IBM Content Manager [4]) and these two exist as silos (see Figure 1). This is unfortunate since the two kinds of data are complementary in terms of information content. Due to the separation between the two kinds of data today, an application would need to similar context. 
For example, consider a stock-market info rmation system. Such a system not only assessment reports, articles, related news, etc. (unstructured data). It would be nice if the stock trader, while querying the market statistics on, say, the fastest moving stock within a given sector at the moment, would also get the related advisories and reports. If she wants to trade on the stock, then depending upon the size of the trade, she gets her making an effort to hunt for them in the content repository, or on the web  X  saving thought of in other domains also, e.g.:  X  Health: Patient specific report and medical articles,  X  Manufacturing: Defect statistics and engineering specifications,  X  Marketing: Customer transaction history and marketing documents,  X  Travel: Traveler itinerary and promotional flyers, travel advisories,  X  Management: Employee records and status reports (details in Section 2). 
The goal of this paper is to present a novel, context-oriented, loosely coupled integration of structured (DB2) and unstructured data (CM) through symbiotic  X  To enhance structured data retrieval by associating additional documents relevant  X  To enhance document contents by associating additional information derived marketing, CRM applications, etc [10]. present in DB2 with that present in CM and vice versa, as illustrated in Figure 2. This broker is loosely coupled with DB2 on one side and CM on the other, and resides as a separate entity. It is important to note that data integration products like the IBM Enterprise of access, enabling the user to write a single query that spans these sources. However, the onus of specifying appropriate context remains with the user, which is a limitation since, as shown in Section 2, the user migh t not be aware of the overall context at the point of submitting the query. example is presented in Section 2. Preliminary ideas and alternatives on the design of going forward are discussed in Section 4. Related work is discussed in Section 5, with emphasis on how this work differentiates from the same. Finally, the conclusions appear in Section 6. Consider a DB2 database containing information about employees in IBM. It details what projects an individual is working on, which group he/she is a part of and details of the organization. Additionally, the content manager contains documents on several the organization. 
The user queries DB2 for information on a project named CORTEX, and gets back document has no explicit mention of CORTEX, but contains the relevant context . tuple in the PROJECTS relation) and looked around in the neighborhood to determine the relevant context (in this example, th e context is characterized by a set of discussed in a later section). 
Next, consider another user who retrieves the document Report.doc, and gets back not only the document asked for, but also (a handle of) the fragment of DB2 relational database relevant to the document. This is shown in Figure 4. Essentially, the system extracts the context of the document (again, a set of keywords for the purpose of this data can be presented in to the user in a browse-able manner. 
Clearly, such functionality cannot be achieved by querying DB2 and CM relevant documents, while in the second case DB2 uses the context retrieved from CM to identify the relevant database fragment . preliminary ideas on the architecture of the broker. 
Recall from the discussion in Section 1 that the purpose of the broker is to correlate information content of the unstructured documents in the CM. In this initial design we keep things simple, and assume that:  X  Context is modeled as a set of keywords.  X  There is an efficient algorithm to determine the context of a DB2 query. As in the  X  There is an efficient algorithm to determine the context of a CM query. We  X  There is an efficient mechanism to find all documents relevant to a given  X  There is an efficient mechanism to find the database fragment relevant to a given Accordingly, we get the preliminary architecture shown in Figure 5. 
Here, the broker in Figure 2 has been expanded into four different entities responsible for executing the various mechanisms mentioned in the assumptions above. Specifically:  X  DB2 Context Analyzer analyses the input DB2 query and the accessed database  X  CM Context Analyzer analyses the input CM query and the retrieved documents,  X  CM Context Index determines the set of documents in CM most relevant to the  X  DB2 Context Index determines the database fragments in the DB2 database most While being an acceptable proof of concept, the preliminary architecture presented in overcome them.  X  Limitation: Context is a set of keywords . This is not very expressive. Can we do  X  Limitation: The context of a DB2 query is determined by scanning a constrained  X  Limitation: The context of a CM query is dete rmined as the union of the keywords  X  Limitation: Context of a DB2 query is mapped to one or more categories; the set  X  Limitation: Context of a CM query is mapped to one or more categories; the set  X  Limitation: The documents and database fragments are returned as unordered  X  Limitation: Entire database fragments returned in addition to the documents on these efforts in perspective of this project, and emphasize how this project differentiates from them. 
The Unstructured Information Management Architecture (UIMA) [6] is a framework for classifying, describing, developing and combining natural language components in applications for processing unstructured information. As the name implies, UIMA is to aid in further analysis of the unstructured data. As such, the UIMA framework merely works as an application using the relational DBMS as a repository of its private UIMA framework. It is not clear if the framework addresses the problem of correlating existing data in the relational DBMS with the unstructured information being analyzed, and vice versa  X  the focus of this proposal. databases [1, 2]. The idea is to consider each tuple in each relation in the database as a document, and consider the entire database as a graph with these tuples as nodes and query with multiple keywords is a set of trees with leaves as tuples containing one of more keywords. These trees are ranked on relevance to the query. As mentioned in Section 3, ideas from this work can be applied to develop a more sophisticated DB2 Context Index. 
Sapient [5] integrates data and text through an OLAP-style interaction model. The authors propose a framework for automatically extracting structured information from documents to form a  X  X ocument warehouse X  that can complement the data warehouse in business analysis. Our focus is not as much on information extraction, but on providing context-based correlation of the structured and unstructured content. 
We reiterate that this effort is different from information integration [8], which enables the user, for instance, to access structured data from a DB2 database and (the metadata of) unstructured data in a different CM database in a single query. Clearly, the context of all the information needed. 
The IBM Enterprise Information Integrator (EII) for Content [7, 11] provides unstructured content stored in content managers federated into the system. However, it does not provide any support for context-based consolidation of the structured and unstructured information, which is the focus of this work. Nevertheless, this work can build on IBM EII for Content, using it as a platform for developing the CM Context Index (Section 3). This paper presented a framework for consolidating structured and unstructured data retrieval in a novel, symbiotic manner. The problem is well-motivated and a comparison understanding, not been addressed earlier. The paper also discussed a preliminary information integration has several interesting research problems as well. Acknowledgment. We would like to thank Prof. Xuemin Lin and Prof. Jeffrey Xu Yu Wang and Mr. Di Wu for their great help in revising and formatting this paper. 
