 Significant research efforts for robust integration of infor-mation from multiple sources are being pursued at a rapid pace. However, the information in heterogeneous sources is often incomplete and hence making the maximum use of all the available information is a challenging problem. Most of the recent research on data integration have been primar-ily focused on the cases where the information is available across all the different sources and can not effectively in-tegrate sources in the presence of partial information. We develop an ensemble method that boosts the decisions made from different models on individual sources and obtain ro-bust results for the task of class prediction. We propose a heterogeneous boosting framework that uses all the avail-able information even if some of the sources do not provide any information about some objects. We demonstrate the effectiveness of the proposed framework for the problem of gene function prediction and compare to the state-of-the-art methods using several real-world biological datasets. We also show that the proposed method outperforms any kind of imputation schemes that are widely used while integrating data with partial information.
 H.2.8 [ Database Management ]: Database Applications -Data Mining; I.2.6 [ Arti cial Intelligence ]: Learning Algorithms, Measurement, Design Classification, boosting, data integration, gene function pre-diction.
Integrating information from multiple sources and mak-ing combined decisions from these sources is becoming a common task across several disciplines. Although there are several works proposed for heterogeneous data integration, there is no systematic approach that enhances the prediction performance for different applications. Different ensemble strategies [3] for the integration such as decision templates, weighted majority voting, bagging, boosting, and random forests have been extensively used for integrating different models. Most of these works assume that the information about a data object is available from each of the model. In addition, these methods typically assume that the different models are built from the same type of feature sets (or data sources).

In practice, multiple heterogeneous sources do not contain all the required information about a particular data object. Some sources deliver information about some of the data ob-jects whose information is not available from other sources. In other words, when all the sources are combined, there will be some missing information about certain data objects with respect to some sources. Most of the current research work in data integration primarily focuses on integrating informa-tion when all the sources contain the information about a data object. That is, for the sake of convenience, researchers primarily deal with common objects that are available in all the data sources. Considering only the common information will potentially harm the class prediction when there are sev-eral data objects with partial information. Some work on handling partial information is available in the kernel meth-ods literature [11] where the kernel matrix is integrated to combine the information from multiple sources. Most of these methods treat it as a missing data problem to calcu-late the missing features with the help of observed features to subsequently compute the kernel matrix.

In this paper, we primarily focus on improving the per-formance on functional classification task by utilizing mul-tiple sources of information about a set of genes. In the field of Functional Genomics, the functional classification of unannotated genes and subsequently, the improvement of the existing gene functional annotation catalogs is an impor-tant and challenging problem [12]. Functional classification plays a vital role in molecular biology due to its ability to de-tect previously unknown role of genes and their products in physiological and pathological processes. Different types of biomolecular data, ranging from expression profiles to phy-logenetic gene-specific evolution rates and many others are available to classify gene functions. Such vast amounts of data, in principle, can provide useful information for the au-tomated assessment of the functional role of genes. The ex-tent to which the classification performance can be improved significantly depends on specific type of experimental data and varies for specific gene and the particular bio-molecular process under investigation. One of the key challenges in this domain is that many sources contain only partial informa-tion and one can rarely see all the information available in a given source. Hence, it is critical in this domain to develop methods that work only with such partial information.
Several approaches for heterogeneous data integration have been proposed in the literature. One of the earlier ap-proaches was to integrate multiple sources into one com-bined dataset and perform the classification task on it. Vec-tor Space Integration (VSI) [2] is one of the popular tech-niques that fall into this category. Other such methods were based on modeling networks of functional relationships be-tween proteins where graphical models provide a probabilis-tic framework for data integration [5]. These methods are often referred as early integration . In general, these meth-ods do not make use of any class information during the integration and hence, do not yield better classification per-formance. Due to different properties and heterogeneity of the features, it is not always possible to build graphical mod-els or concatenate vector spaces. Intermediate integration is typically based on kernel fusion (KF) methods. The integra-tion is performed during the training phase itself. Individual kernels on different sources are learned and the final classifier is built on the composite kernel which is obtained after com-bining the individual ones [7]. To exploit the heterogeneity of the data, often weighted functional linkage graph is gener-ated by different sources of information [13]. Another family of successful approaches often referred as late integration , which typically model individual sources and then combine the knowledge from these individual models and builds a final classifier [9, 6]. Methods such as decision templates, different types of weighted majority voting using linear or logarithmic weight combination, and ensemble methods like bagging, boosting and random forests fall into this category [8]. The intuition for using the ensemble technique is that, when the base classifiers used in the ensemble are diverse, they are expected to make different errors. Hence, the en-semble output produced by these classifiers is expected to reduce the overall misclassifications.
Some of the popular methods used for data imputation in the context of integrating multiple sources using kernel fusion techniques are as follows: (i) Unconditional Mean Imputation (KF UMI): For a data object that is not in a particular source, the feature values are imputed with the average of the feature values of the objects that are present in that source. After getting all the feature values, the kernel matrix is aggregated to get a single matrix and and a SVM classifier if built on this matrix to make the final prediction. f value for j th feature. (ii) Weighted Summation Imputation (KF WSI): The feature values for the data are not imputed in the source. Rather, for a data object that is missing in a source, the kernel matrix entries of that object for this ker-nel are imputed as a weighted combination of the average of the entries of that particular kernel matrix and the average of the entries for that objects in other kernel matrices where the data object is present. In our experiment, we used 50% weight for both these values. (iii) Nearest Neighbor Impu-tation (KF NNI): In this method, the kernel matrix entries are directly imputed rather than imputing the feature val-ues. First, the kernel matrices for different sources are gen-erated from the data objects present in those sources. At this juncture, for a data object that is not present in a par-ticular source will not have any kernel matrix entry. Using the other sources where the feature values are present for that data object, the nearest neighbor is obtained. Then, in the source where that data object was missing, the kernel matrix entry of the nearest neighbor is replicated.
To ensure the improved accuracy when combining mod-els from multiple sources, we propose to boost the decisions from these individual sources using a modification to Ad-aBoost [4] algorithm. AdaBoost is an efficient, simple and easy to manipulate additive modeling technique that can potentially use any available weak learner. Boosting algo-rithms combine weak learning models that are slightly better than random models. It is an ensemble method that gener-ates multiple classifiers from a base learner and ensembles them for building the best classifier. In boosting algorithm, strong classifier is built as an combination of a number of weak classifiers, where each classifier is chosen at every iter-ation if its accuracy is greater than 50%. At the end of each iteration, the samples are re-weighted in such a way that the misclassified samples get a higher weight so that the next weak classifier shows a better performance on those samples that were misclassified in the previous iteration.
We propose a heterogeneous boosting based integration frame-work that will exploit all the available (including partial) in-formation from multiple data sources. To achieve this goal, we propose a novel objective criterion which will emphasize the importance of data objects with partial information com-pared to the common ones. We will also modify the re-weighting scheme in the following manner: if a data object is present in only one source out of n sources, the importance of it will be increased by n times while modeling that data ob-ject. The increase of weight of the misclassified data object will be inversely proportional to the number of data sources that contain the information about the data object. The basic intuition here is that the algorithm will give more im-portance to a data object if it is available only in one source compared to one being available in many sources . At the end of each boosting iteration, the instances are re-weighted in such a way that the misclassified objects get a higher weight so that the next weak classifier gives more importance to those objects that were misclassified in the previous itera-tion. Note that, if it is misclassified in other data sources during an iteration, then its weight is increased in that data source as well. Thus, it is unlikely that a data object is ne-glected in all the strong models thus making the stronger model more general and diverse.

Let  X  denote an indicator matrix such that  X  ki is 1 if the data object d k is in dataset D i and 0 otherwise. We define  X  as the average number of datasets in which the data from D i is present and is calculated as follows:
We modify the re-weighting scheme in AdaBoost to follow the two above mentioned criteria as follows: where  X  j is the error rate for that iteration. Hence, the We varied this increment amount based on the number of data sources that the object is present in: Algorithm 1 HETEROBOOST 1: Input: Data sets D 1 ... D N , samples d 1 ...d m and the in-2: Output: Final stronger classifier M 3: Procedure: 4: for i = 1 ..N : 5: Initialize weight w ki = 1 m 6: for j = 1 ..T : 7: (a) C ij  X  arg min C 8: (b) for k = 1 ..m : 10: (ii) w ki = w ki  X  exp [ c ij . | C ij ( d k )  X  = y k 11: (c) Normalize weights:  X  k w ki = w ki  X  ki  X  12: end for 13: strong classifier M i ( x ) = 14: end for 15: return classifier M ( d ) =
When the prediction model is used on a particular test case, it is unlikely that the test case will have information for all the sources. Our method will consider only those sources where the information about the test cases is available and then obtain a weighted ensemble model out of those sources. In other words, there will be no imputation performed in the sources that do not have information about that particular gene. The final outcome is calculated as follows: M ( d ) = accuracy or F-measure) that is being measured for the strong boosted classifier M i and d is the test case.
Since the problem of integrating information from multi-ple sources naturally occurs in biological domains, we chose different bio-molecular datasets to demonstrate the advan-tages of the proposed framework. In order to evaluate the effectiveness of the proposed integration framework, we used the genes from S.cerevisiae (yeast) which is the most widely studied model organism for which vast amounts of bio-molecular data are available. The six biological data sources used in our experiments are thoroughly described in [12]. We used functional annotations collected from the Functional Cata-logue (FunCat) database [10] to associate each of the genes in the datasets to a functional class. The dataset comprised of 1901 genes that are common across all the data sources and the rest of the 2764 other genes that are present in only fewer data sources. We performed three-fold cross validation for reporting our results on test data. We randomly divided both the common and uncommon genes into three folds. The combination of two folds is used for training and the rest for testing both for common and uncommon genes. To get the combined result, one fold of the common genes is merged with one fold of the uncommon genes to make one fold of the combined genes. For evaluation, we used F-measure metric which is a more appropriate measure for this problem be-cause it is more important to correctly associate the genes with a particular functional class than correctly detecting that the gene is not associated with other function class. Because of the class imbalance problem, the models often produce poor result for F-measure for the target class. To tackle the class imbalance issue, we pre-processed the train-ing data before the training process to under samplify the majority classes and oversamplify the minority class using Synthetic Minority Oversampling TEchnique (SMOTE)[1]. For fairness in comparison, while working with the kernel fusion methods, we used the same fold generated for the heterogeneous boosting. For KF UMI, we first imputed the missing feature values and then generated complete kernel matrices for different datasets. For the other two methods, we imputed in the kernel matrix to obtain a complete kernel matrix. In either case, we have a set of full kernel matrix weighted summation of those kernel matrices based on the individual accuracy in corresponding datasets.

Table 2 shows the results of different boosting methods on common and uncommon genes. We observe that Ensem-ble using common genes performs well on the common set of genes. However, for the uncommon genes, the results are not impressive for the ensemble model built on common genes. Using the proposed heterogeneous boosting, we observe the improvement of using all the genes during the training pro-cess on the uncommon genes. Finally, we conclude that us-ing the modi ed weighting criterion which emphasized the importance for uncommon genes compared to the common genes yields performance improvement for the uncommon genes as well as the overall result. We also compared our heterogeneous boosting algorithm with kernel fusion meth-ods with different imputation schemes (see Table 3). The result of kernel fusion methods with imputation is inferior to our proposed heterogeneous boosting method. By the use of SMOTE on the training data, which is easily applicable and natural fits the boosting methods, the result of heteroge-neous boosting significantly outperformed the kernel fusion method. and boosting with all genes and with only the common genes. E Ensemble with all genes. HBOOST -the proposed HeteroBoost method. Table 3: Comparison of F-measure values for the proposed heterogeneous boosting method and dif-ferent kernel fusion approaches
The availability of different information about the same data objects created new opportunities as well as challenges for the task of class prediction. The information in hetero-geneous sources is often incomplete and most of the recent research on data integration have been primarily focused on the cases where the information is available across all the different sources. In this paper, we developed a new frame-work that uses all the available information even if some sources do not provide any information about some objects. Our study also shows that boosting the decisions made from individual sources can obtain robust results on predicting gene functions. Furthermore, giving different weights to the uncommon genes helped in improving the predictive ability for the overall classification. We demonstrated the effec-tiveness of the proposed framework for the problem of gene function prediction and compare to the state-of-the-art en-semble methods using several real-world biological datasets. The proposed heterogeneous boosting method outperformed the standard kernel fusion based approaches for integrating multiple sources in the presence of missing information. [1] N. Chawla, K. Bowyer, L. Hall, and W. Kegelmeyer. [2] M. des Jardins, P. Karp, M. Krummenacker, T. Lee, [3] T. G. Dietterich. Ensemble methods in machine [4] Y. Freund and R. E. Schapire. Experiments with a [5] U. Karaoz, T. Murali, S. Letovsky, Y. Zheng, C. Ding, [6] L. Kuncheva, J. Bezdek, and R. Duin. Decision [7] G. Lanckriet, T. De Bie, N. Cristianini, M. Jordan, [8] R. Polikar. Ensemble based systems in decision [9] F. Roli, G. Giacinto, and V. Gianni. Methods for [10] A. Ruepp, D. Zollner, A.and Maier, K. Albermann, [11] A. J. Smola, S. V. N. Vishwanathan, and T. Hofmann. [12] G. Valentini. True path rule hierarchical ensembles for [13] X. Zhao, L. Chen, and K. Aihara. Protein function
