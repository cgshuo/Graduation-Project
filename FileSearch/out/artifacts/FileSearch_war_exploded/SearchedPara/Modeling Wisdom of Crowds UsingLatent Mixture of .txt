 In many real life scenarios, it is hard to collect the actual labels for training, because it is expen-sive or the labeling is subjective. To address this issue, a new direction of research appeared in the last decade, taking full advantage of the  X  X isdom of crowds X  (Surowiecki, 2004). In simple words, wis-dom of crowds enables parallel acquisition of opin-ions from multiple annotators/experts.

In this paper, we propose a new method to fuse wisdom of crowds. Our approach is based on the Latent Mixture of Discriminative Experts (LMDE) model originally introduced for multimodal fu-sion (Ozkan et al., 2010). In our Wisdom-LMDE model, a discriminative expert is trained for each crowd member. The key advantage of our compu-tational model is that it can automatically discover the prototypical patterns of experts and learn the dy-namic between these patterns. An overview of our approach is depicted in Figure 1.

We validate our model on the challenging task of listener backchannel feedback prediction in dyadic conversations. Backchannel feedback includes the nods and paraverbals such as  X  X h-huh X  and  X  X m-hmm X  that listeners produce as they are speaking. Backchannels play a significant role in determining the nature of a social exchange by showing rapport and engagement (Gratch et al., 2007). When these signals are positive, coordinated and reciprocated, they can lead to feelings of rapport and promote beneficial outcomes in diverse areas such as nego-tiations and conflict resolution (Drolet and Morris, 2000), psychotherapeutic effectiveness (Tsui and Schultz, 1985), improved test performance in class-rooms (Fuchs, 1987) and improved quality of child care (Burns, 1984). Supporting such fluid interac-tions has become an important topic of virtual hu-man research. In particular, backchannel feedback has received considerable interest due to its perva-siveness across languages and conversational con-texts. By correctly predicting backchannel feed-back, virtual agent and robots can have stronger sense of rapport.

What makes backchannel prediction task well-suited for our model is that listener feedback varies between people and is often optional (listeners can always decide to give feedback or not). A successful computational model of backchannel must be able to learn these variations among listeners. Wisdom-LMDE is a generic approach designed to integrate opinions from multiple listeners.

In our experiments, we validate the performance of our approach using a dataset of 43 storytelling dyadic interactions. Our analysis suggests three pro-totypical patterns for backchannel feedback. By automatically identifying these prototypical pat-terns and learning the dynamic, our Wisdom-LMDE model outperforms the previous approaches for lis-tener backchannel prediction. 1.1 Previous Work Several researchers have developed models to pre-dict when backchannel should happen. Ward and Tsukahara (2000) propose a unimodal approach where backchannels are associated with a region of low pitch lasting 110ms during speech. Nishimura et al. (2007) present a unimodal decision-tree approach for producing backchannels based on prosodic fea-tures. Cathcart et al. (2003) propose a unimodal model based on pause duration and trigram part-of-speech frequency.

Wisdom of crowds was first defined and used in business world by Surowiecki (2004). Later, it has been applied to other research areas as well. Raykar et. al. (2010) proposed a probabilistic approach for supervised learning tasks for which multiple annota-tors provide labels but not an absolute gold standard. Snow et. al. (2008) show that using non-expert la-bels for training machine learning algorithms can be as effective as using a gold standard annotation.
In this paper, we present a computational ap-proach for listener backchannel prediction that ex-ploits multiple listeners. Our model takes into ac-count the differences in people X  X  reactions, and au-tomatically learns the hidden structure among them. The rest of the paper is organized as follows. In Section 2, we present the wisdom acquisition pro-cess. Then, we describe our Wisdom-LMDE model in Section 3. Experimentals are presented in Sec-tion 4. Finally, we conclude with discussions and future works in Section 5. It is known that culture, age and gender affect peo-ple X  X  nonverbal behaviors (Linda L. Carli and Loe-ber, 1995; Matsumoto, 2006). Therefore, there might be variations among people X  X  reactions even when experiencing the same situation. To effi-ciently acquire responses from multiple listeners, we employ the Parasocial Consensus Sampling (PCS) paradigm (Huang et al., 2010), which is based on the theory that people behave similarly when interact-ing through a media (e.g., video conference). Huang et al. (2010) showed that a virtual human driven by PCS approach creates significantly more rapport and is perceived as more believable than the virtual hu-man driven by face-to-face interaction data (from ac-tual listener). This result indicates that the parasocial paradigm is a viable source of information for wis-dom of crowds.

In practice, PCS is applied by having participants watch pre-recorded speaker videos drawn from a dyadic story-telling dataset. In our experiments, we used 43 video-recorded dyadic interactions from the RAPPORT 1 dataset (Gratch et al., 2006). This dataset was drawn from a study of face-to-face narrative discourse ( X  X uasi-monologic X  storytelling). The videos of the actual listeners were manually an-notated for backchannel feedback. For PCS wis-dom acquisition, we recruited 9 participants, who were told to pretend they are an active listener and press the keyboard whenever they felt like provid-ing backchannel feedback. This provides us the re-sponses from multiple listeners all interacting with the same speaker, hence the wisdom necessary to model the variability among listeners. Given the wisdom of multiple listeners, our goal is to create a computational model of backchannel feed-back. Although listener responses vary among indi-viduals, we expect some patterns in these responses. Therefore, we first analyze the most predictive fea-tures for each listener and search for prototypical patterns (in Section 3.1). Then, we present our Wisdom-LMDE that allows to automatically learn the hidden structure within listener responses. 3.1 Wisdom Analysis We analyzed our wisdom data to see the most rel-evant speaker features when predicting responses from each individual listener. (The complete list of speaker features are described in Section 4.1.) We used a feature ranking scheme based on a sparse regularization technique, as described in (Ozkan and Morency, 2010). It allows us to identify the speaker features most predictive of each listener backchan-nel feedback. The top 3 features for all 9 listeners are listed in Table 1.
 This analysis suggests three prototypical patterns. For the first 3 listeners, pause in speech and syntac-tic information (POS:NN) are more important. The next 3 experts include a prosodic feature, low pitch, which is coherent with earlier findings (Nishimura et al., 2007; Ward and Tsukahara, 2000). It is inter-esting to see that the last 3 experts incorporate visual information when predicting backchannel feedback. This is in line with Burgoon et al. (Burgoon et al., 1995) work showing that speaker gestures are of-ten correlated with listener feedback. These results clearly suggest that variations be present among lis-teners and some prototypical patterns may exist. Based on these observations, we propose new com-putational model for listener backchannel. 3.2 Computational Model: Wisdom-LMDE The goals of our computational model are to au-tomatically discover the prototypical patterns of backchannel feedback and learn the dynamic be-tween these patterns. This will allow the compu-tational model to accurately predict the responses of a new listener even if he/she changes her backchan-nel patterns in the middle of the interaction. It will also improve generalization by allowing mixtures of these prototypical patterns.
 To achieve these goals, we propose a variant of the Latent Mixture of Discriminative Experts (Ozkan et al., 2010) which takes full advantage of the wisdom of crowds. Our Wisdom-LMDE model is based on a two step process: a Conditional Random Field (CRF, see Figure 1a) is learned for each wisdom listener, and the outputs of these expert models are used as input to a Latent Dynamic Conditional Ran-dom Field (LDCRF, see Figure 1b) model, which is capable of learning the hidden structure within the experts. In our Wisdom-LMDE, each expert cor-responds to a different listener from the wisdom of crowds. More details about training and inference of LMDE can be found in Ozkan et al. (2010). To confirm the validity of our Wisdom-LMDE model, we compare its performance with compu-tational models previously proposed. As motivated earlier, we focus our experiments on predicting lis-tener backchannel since it is a well-suited task where variability exists among listeners. 4.1 Multimodal Speaker Features The speaker videos were transcribed and annotated to extract the following features:
Lexical: Some studies have suggested an asso-ciation between lexical features and listener feed-back (Cathcart et al., 2003). Therefore, we use all the words (i.e., unigrams) spoken by the speaker. Syntactic structure: Using a CRF part-of-speech (POS) tagger and a data-driven left-to-right shift-reduce dependency parser (Sagae and Tsujii, 2007) we extract four types of features from a syntactic de-pendency structure corresponding to the utterance: POS tags and grammatical function for each word, POS tag of the syntactic head, distance and direction from each word to its syntactic head.
 Prosody: Prosody refers to the rhythm, pitch and intonation of speech. Several studies have demon-strated that listener feedback is correlated with a speaker X  X  prosody (Ward and Tsukahara, 2000; Nishimura et al., 2007). Following this, we use downslope in pitch, pitch regions lower than 26th percentile, drop/rise and fast drop/rise in energy of speech, vowel volume, pause.
 Visual gestures: Gestures performed by the speaker are often correlated with listener feedback (Burgoon et al., 1995). Eye gaze, in particular, has often been implicated as eliciting listener feedback. Thus, we encode the following contextual features: speaker looking at listener, smiling, moving eyebrows up and frowning.

Although our current method for extracting these features requires that the entire utterance to be avail-able for processing, this provides us with a first step towards integrating information about syntac-tic structure in multimodal prediction models. Many of these features could in principle be computed in-crementally with only a slight degradation in accu-racy, with the exception of features that require de-pendency links where a word X  X  syntactic head is to the right of the word itself. We leave an investiga-tion that examines only syntactic features that can be produced incrementally in real time as future work. 4.2 Baseline Models Consensus Classifier In our first baseline model, we use consensus labels to train a CRF model, which are constructed by a similar approach presented in (Huang et al., 2010). The consensus threshold is set to 3 (at least 3 listeners agree to give feedback at a point) so that it contains approximately the same number of head nods as the actual listener. See Fig-ure 1 for a graphical representation of CRF model. CRF Mixture of Experts To show the importance of latent variable in our Wisdom-LMDE model, we trained a CRF-based mixture of discriminative ex-perts. This model is similar to the Logarithmic Opinion Pool (LOP) CRF suggested by Smith et al. (2005). Similar to our Wisdom-LMDE model, the training is performed in two steps. A graphical representation of a CRF Mixture of experts is given in the Figure 1.
 Actual Listener (AL) Classifiers This baseline model consists of two models: CRF and LDCRF chains (See Figure 1). To train these models, we use the labels of the  X  X ctual Listeners X  (AL) from the RAP-PORT dataset.
 Multimodal LMDE In this baseline model, we com-pare our Wisdom LMDE to a multimodal LMDE, where each expert refers to one of 5 different set of multimodal features as presented in (Ozkan et al., 2010): lexical, prosodic, part-of-speech, syntactic, and visual.
 Random Classifier Our last baseline model is a ran-dom backchannel generator as desribed by Ward and Tsukahara (2000). This model randomly gener-ates backchannels whenever some pre-defined con-ditions in the prosody of the speech is purveyed. 4.3 Methodolgy We performed hold-out testing on a randomly se-lected subset of 10 interactions. The training set contains the remaining 33 interactions. Model pa-rameters were validated by using a 3-fold cross-validation strategy on the training set. Regulariza-tion values used are 10k for k = -1,0,..,3. Numbers of hidden states used in the LDCRF models were 2, 3 and 4. We use the hCRF library 2 for training of CRFs and LDCRFs. Our Wisdom-LMDE model was implemented in Matlab based on the hCRF li-brary. Following (Morency et al., 2008), we use an encoding dictionary to represent our features. The performance is measured by using the F-score, which is the weighted harmonic mean of precision and recall. A backchannel is predicted correctly if a peak happens during an actual listener backchan-nel with high enough probability. The threshold was selected automatically during validation. 4.4 Results and Discussion Before reviewing the prediction results, is it impor-tant to remember that backchannel feedback is an optional phenomena, where the actual listener may or may not decide on giving feedback (Ward and Tsukahara, 2000). Therefore, results from predic-tion tasks are expected to have lower accuracies as opposed to recognition tasks where labels are di-rectly observed (e.g., part-of-speech tagging).
Table 2 summarizes our experiments comparing our Wisdom-LMDE model with state-of-the-art ap-proaches for behavior prediction (see Section 4.2). Our Wisdom-LMDE model achieves the best F1 score. Statistical t-test analysis show that Wisdom-LMDE is significantly better than Consensus Clas-sifier, AL Classifier (LDCRF), Multimodel LMDE and Random Classifier.
 The second best F1 score is achieved by CRF Mixture of experts, which is the only model among other baseline models that combines different lis-tener labels in a late fusion manner. This result supports our claim that wisdom of clouds improves learning of prediction models. CRF Mixture model is a linear combination of the experts, whereas Wisdom-LMDE enables different weighting of ex-perts at different point in time. By using hidden states, Wisdom-LMDE can automatically learn the prototypical patterns between listeners.

One really interesting result is that the optimal number of hidden states in the Wisdom-LMDE model (after cross-validation) is 3. This is coherent with our qualitative analysis in Section 3.1, where we observed 3 prototypical patterns. In this paper, we proposed a new approach called Wisdom-LMDE for modeling wisdom of crowds, which automatically learns the hidden structure in listener responses. We applied this method on the task of listener backchannel feedback predic-tion, and showed improvement over previous ap-proaches. Both our qualitative analysis and exper-imental results suggest that prototypical patterns ex-ist when predicting listener backchannel feedback. The Wisdom-LMDE is a generic model applicable to multiple sequence labeling tasks (such as emotion analysis and dialogue intent recognition), where la-bels are subjective (i.e. small inter-coder reliability). This material is based upon work supported by the National Science Foundation under Grant No. 0917321 and the U.S. Army Research, Develop-ment, and Engineering Command (RDE-COM). The content does not necessarily reflect the position or the policy of the Government, and no official en-dorsement should be inferred.
