 WANXIANG CHE Harbin Institute of Technology MIN ZHANG and AI TI AW Institute for Infocomm Research CHEW LIM TAN National University of Singapore and TING LIU and SHENG LI Harbin Institute of Technology 13: 2  X  W. Che et al. 1. INTRODUCTION In recent years, there has been increasing interest in Shallow Semantic Pars-ing. It is becoming an important component in many kinds of deep natural language processing applications, such as question answering [Narayanan and Harbabagiu 2004; Moschitti et al. 2006; Shen and Lapata 2007], information extraction [Surdeanu et al. 2003], and coreference resolution [Ponzetto and Strube 2006]. As a particular case of shallow semantic parsing, Semantic Role Labeling (SRL) is currently a well-defined task with a substantial amount of work and comparative evaluation. Given a sentence, the task consists of an-alyzing the propositions expressed by some target verbs or nouns and some constituents of the sentence. In particular, all the constituents in the sen-tence which fulfill a semantic argument (role) for each predicate (target verb or noun) have to be recognized. Figure 1 shows an example of SRL annota-tion result in the English PropBank [Palmer et al. 2005]. It defines six core arguments (Arg0  X  5), where Arg0 is the Agent, Arg1 is Patient, etc. ArgMs indicate adjunct arguments, such as ArgM-LOC (Locative), ArgM-TMP (Tem-poral). Chinese PropBank (CPB) [Xue and Kulick 2003] has a similar structure to the English PropBank. Both of them are widely-used benchmark data for SRL. We will introduce them in detail in the next section.

Generally, semantic role identification and classification are regarded as two key steps in SRL. Semantic role identification involves identifying the se-mantic and nonsemantic argument among all constituents in a sentence while semantic role classification involves classifying each identified semantic argu-ment into a specific semantic role. Gildea and Jurafsky [2002] are the first to use a linear interpolation method and extracted features from a parse tree to identify and classify the constituents in the FrameNet Baker et al. [1998] with syntactic parsing results. Most of the following work focused on feature engineering [Pradhan et al. 2005; Xue and Palmer 2004; Jiang et al. 2005] and machine learning models [Nielsen and Pradhan 2004; Pradhan et al. 2005]. Some other work paid more attention to the robust SRL [Pradhan et al. 2005] and post inference [Punyakanok et al. 2004]. Besides English, the SRL tasks on other languages, such as Chinese [Sun and Jurafsky 2004; Xue and Palmer 2005], were given more attention recently.

All the above work, including most systems participating in various shared tasks, such as the Senseval-3 1 , the CoNLL-2004 and 2005 SRL shared tasks [Carreras and M ` arquez 2004, 2005], used the state-of-the-art, feature-based methods for argument identification and classification. Feature-based methods usually use a flat feature vector to represent a learning object and are known to be hard in describing the syntactic structure information explicitly. As an al-ternative to the standard feature-based methods, kernel-based methods have been proposed to implicitly explore features in a high-dimensional space by di-rectly calculating the similarity between two feature vectors, or even between two objects using a kernel function (Subsection 4.1 gives the formal defini-tion of the kernel function). Moreover, there are some machine learning algo-rithms with dual form, such as Perceptron and support vector machines (SVM) [Cristianini and Shawe-Taylor 2000], which do not require the exact presenta-tion of objects to compute their kernel functions during learning and predic-tion. Such algorithms can be used as learning algorithms in the kernel-based methods. They are named as kernel machines.

Many kernel functions proposed in the machine learning community have been applied to natural language processing tasks. In particular, Haussler [1999] and Watkins [1999] proposed the best-known convolution kernels for a discrete structure. Under the framework of convolution kernels, increasingly more kernels for restricted syntax or specific domains are proposed and ex-plored, such as string kernel for text categorization [Lodhi et al. 2002], tree kernel for syntactic parsing [Collins and Duffy 2001], kernel for relation ex-traction [Zelenko et al. 2003; Culotta and Sorensen 2004; Zhang et al. 2006b], and kernel for question answering [Moschitti et al. 2006]. Particularly, Mos-chitti [2004] and Moschitti et al. [forthcoming] proposed a Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. It is considered as the first work using kernel-based methods for SRL. The PAF regards the minimum subtree comprising a predicate and a constituent 13: 4  X  W. Che et al. structure as the feature space. However, this point of view cannot distinguish among different kinds of features well, that is, the path feature and the con-stituent structure feature.

For the reasons described above, we propose a hybrid convolution tree ker-nel for SRL. We first decompose the PAF kernel into a Path kernel and a Con-stituent Structure kernel, and then combine them into a hybrid convolution tree kernel. That means the new kernel captures the Path feature and the Con-stituent Structure feature separately. This can model the structure features more effectively. Experiments on the test sets of the CoNLL-2005 SRL shared task and the Chinese PropBank show that our hybrid kernel method outper-forms the PAF kernel significantly. In addition, in order to get the best per-formance, a composite kernel by combining our hybrid convolution tree kernel and a feature-based method extended by the polynomial kernel is presented. Experimental results show that the composite kernel outperforms each of the individual kernels. It also outperforms the best reported CoNLL-2005 shared task system which uses only one syntactic parser and the best reported system on the CPB corpus which uses gold parse trees.

The remaining of this article is organized as follows: In Section 2, we in-troduce the SRL corpora used in our experiments. In Section 3, we illustrate the state-of-the-art, feature-based methods for SRL while Section 4 introduces our method. The experimental results and discussion are shown in Section 5. Finally, our work is concluded in Section 6. 2. SEMANTIC ROLE LABELING CORPORA The PropBank [Palmer et al. 2005] is a popular corpus for SRL in English. The latest version, English PropBank I, can be obtained from LDC (LDC2004T14) 2 . Correspondingly, the Chinese PropBank (CPB) [Xue and Kulick 2003] is a Chi-nese corpus for SRL.

In the English PropBank I, predicate-argument relations are annotated for verbs in the Wall Street Journal (WSJ) section of the Penn Treebank II [Marcus et al. 1993]. The arguments of a predicate contain six core arguments (Arg0  X  5) and 14 adjunctive arguments (ArgM- X  ). Usually, Arg0 denotes the Agent of a target verb; Arg1 is the Patient, and so on. ArgMs include ArgM-LOC ( Loca-tive ), ArgM-TMP ( Temporal ), and so on. PropBank I was constructed by assign-ing semantic arguments to constituents of the hand-corrected Penn Treebank parse trees. So sometimes the parse trees can have trace nodes which refer to other nodes in the trees. Such nodes do not have any words associated with them but are also marked as arguments. For example, the trace node  X  X ONE X  is labeled as  X  X rg1 X  in this labeling result:  X  X The new plant Arg 1 ], located [-NONE-Arg 1 ] [in Chinchon ArgM  X  LOC ], ...  X . However these trace nodes cannot be reproduced by most automatic syntactic parsers.

Therefore, the CoNLL-2005 SRL shared task corpus [Carreras and M ` arquez 2005], which is a snapshot of the PropBank with some corrections, is used in our experiments. The syntactic parse trees produced by automatic syntactic parsers are used in the CoNLL-2005 corpus, including Charniak parser [Char-niak 2000] and Collins parser [Collins 1999]. In addition, the preprocessing modules include an SVM based POS tagger [Gimenez and M ` arquez 2003], and Chieu and Ng X  X  [2003] Named Entity recognizer. At the same time, the CoNLL-2005 corpus annotates discontinuous and coreferential arguments explicitly with some simple rules. The first part of a discontinuous argument is labeled as it is, while the second part is labeled with a prefix  X  X - X  appended to it (e.g., [The company A 0 ] [gained V ] [shareholder approval A 1 ] [Thursday AM  X  TMP ] [to restructure in a bid C  X  A 1 ]. ). All coreferential arguments are labeled with a prefix  X  X - X  appended to them (e.g. [Every problem A 0 ] [that R  X  A 0 ] has [hob-bled V ] [the program A 1 ] ... . ). In order to test the robustness of the systems, besides the WSJ corpus, a cross-corpora evaluation is performed using a fresh test set from the Brown corpus (ck01-03). It is provided by the PropBank team.
The standard partition of the CoNLL-2005 corpus is as follows: sections 02-21 for training, section 24 for development, and section 23 for testing. Some corpus statistics are listed in Table I.
 The Chinese PropBank (CPB) [Xue and Kulick 2003] is based on the Penn Chinese TreeBank [Xue et al. 2005], which is a Chinese corpus annotated with syntactic structures. It is created by adding the semantic roles to the appro-priate constituents of the syntactic tree. Figure 2 illustrates an example in the CPB. We note that the SRL annotation scheme of the CPB is similar to the English PropBank, that is, they use the same semantic role (arguments 13: 6  X  W. Che et al.
 and adjuncts) set. Additionally, the syntactic structure of the Penn Chinese TreeBank is also similar to that of the Penn English TreeBank. 3. FEATURE-BASED METHODS FOR SRL Feature-based methods refer to the standard methods which use a flat feature vector to represent an object. At present, most of the successful SRL systems use these methods. Their features are usually extended from Gildea and Ju-rafsky [2002] X  X  work, which used flat information derived from a parse tree. According to the literature, Gildea and Jurafsky [2002]; Pradhan et al. [2005] used the Constituent, Predicate, and Predicate-Constituent related features listed in Table II.

To find a useful feature set is usually a nontrivial task. In addition, ear-lier research [Gildea and Palmer 2002; Punyakanok et al. 2005] recognized the necessity and importance of syntactic parsing for SRL. Therefore, it is critical to effectively utilize the syntactic structure features in SRL. Unfor-tunately, feature-based methods are less effective in this respect. One of the reasons is that the standard feature-based methods intensify the data sparse-ness problem for syntactic structure features in SRL. This is because they are sensitive to small changes in the structures [Moschitti 2004]. For example, in Figure 3, the Path features between predicate and Arg1 in 3(a) and 3(b) are  X  X BN  X  VP  X  VP  X  VP  X  S  X  NP X  and  X  X BN  X  VP  X  VP  X  S  X  NP X  respectively. Although their arguments are the same and their Path features differ only in one addi-tional  X  X P  X   X , they have distinct Path features. This data sparseness problem prevents the learning algorithms from generalizing unseen data well.
To overcome this problem, Pradhan et al. [2005] tried generalizing the Path feature with some intuitive heuristics. However, the heuristic generalizing method is too restrictive to give a better coverage and is difficult to be used on other structure features. Another intuition is to represent a structure using all of its substructures. However, this generates a large amount of features which grow exponentially with respect to the size of the tree. In the next section, we will introduce our solution. 4. HYBRID CONVOLUTION TREE KERNELS FOR SRL In this section, we first introduce the principle of kernel methods in Subsec-tion 4.1 and traditional convolution tree kernels for SRL in Subsection 4.2. Then, we present our proposed hybrid convolution tree kernel for SRL in Sub-section 4.3. Finally, we discuss the related work in Subsection 4.4. 4.1 Kernel-Based Methods Kernel methods [Vapnik 1998; Shawe-Taylor and Cristianini 2004] are an at-tractive alternative to feature-based methods. The kernel methods retain the original representation of objects and use the object only via a kernel func-tion (a special kind of similarity function) computed on a pair of objects. A kernel function K is a binary function over the object space X . That is K : X  X  X  X  [0 ,  X  ] maps a pair of objects x , y  X  X to their similarity score K ( x , y ). The kernel (or similarity) function is required to be symmetric 3 and positive-semidefinite 4 . 13: 8  X  W. Che et al.

It can be shown that any kernel function implicitly calculates the dot-product of feature vectors of objects in high-dimensional feature spaces. That is, there exist features corresponding to a mapping function 8 (  X  ) = (  X  1 (  X  ) denotes the dot product of vectors a and b .

There are a number of learning algorithms that can operate using only the dot product (kernel function) of instances. We call them kernel machines . For instance, the support vector machine (SVM) is a learning algorithm that not only allows for kernel function, but also provides a rigorous rationale for re-sisting over-fitting [Vapnik 1998].

We note that, from the learning system design perspective, the kernel meth-ods shift the focus from the problem of feature selection to the problem of ker-nel construction. Since a kernel is the only domain specific component of a kernel learning system, it is critical to design a kernel that adequately encap-sulates all information necessary for learning. Next, we will show a special kind of kernel for some NLP tasks. 4.2 Convolution Tree Kernels for SRL The convolution tree kernel counts the number of common subtrees (substruc-tures) as the syntactic similarity between two parse trees. In the vector rep-resentation of a parse tree, a tree T can be represented by a vector of integer counts of each subtree type (regardless of its ancestors):
This generates a very high-dimensional feature space since the number of different subtrees is exponential to the tree X  X  size. Thus it is computationally infeasible to use the feature vector 8 ( T ) directly. To solve this problem, Collins and Duffy [2001] expanded Haussler [1999] and Watkins [1999] X  X  convolution kernel by developing a convolution tree kernel function which is able to cal-culate the dot product between the above high-dimensional vectors efficiently. The kernel function is defined as follows: where N 1 and N 2 are the sets of all nodes in trees T 1 and T 2 , respectively, and I ( n ) is the indicator function whose value is 1 if and only if there is a subtree of type i rooted at node n and 0 otherwise. Collins and Duffy [2001] showed that K ( T 1 , T 2 ) is an instance of convolution kernels over tree structures, which can be computed in O ( | N 1 | X | N 2 | ) by the following recursive definitions (Let 1 ( n 1 , n 2 ) = P i I i ( n 1 )  X  I i ( n 2 )): (1) if the production rules at n 1 and n 2 are different then 1 ( n 1 , n 2 ) = 0; (2) else if their children are the same and they are leave nodes, then 1 ( n 1 , n and  X  (0 &lt;  X  &lt; 1) is the decay factor in order to make the kernel value less variable with respect to the tree X  X  size.

Moschitti [2004] proposed to apply the convolution tree kernels to SRL. He selected portions of syntactic parse trees, which include salient substructures of predicate-arguments as the predicate-arguments feature (PAF) space, and defined the convolution kernel on the PAF space. Figure 4 illustrates the PAF kernel feature space of the predicate buy and the argument Arg0 in the en-closed substructure. Figure 5 lists all the 15 subtree features in the PAF fea-ture space. Besides the structure features, the PAF kernel covers many of the previous flat features, such as Predicate, Words, POSs. Moschitti [2004] further showed that the PAF kernel performs well on the semantic role clas-sification subtask in SRL. By nature, the PAF kernel is similar to Collins and 13: 10  X  W. Che et al.
 Duffy [2001] X  X  tree kernel except for the substructure selection strategy. More precisely, Moschitti [2004] only selected the relative portion between a predi-cate and an argument and defined the tree kernel over the selected portion. 4.3 Hybrid Convolution Tree Kernel for SRL We note that the PAF feature space consists of two kinds of features namely the parse tree Path feature and the Constituent Structure feature. These two kinds of feature spaces represent different information. The Path feature cap-tures the information between a predicate and its arguments while the Con-stituent Structure feature captures the syntactic structure information of an argument. It would be more reasonable to capture these two different kinds of features separately since they contribute to SRL in different ways. Then we can easily fuse them by a linear combination using different weights. Based on the above consideration, we propose two convolution kernels to capture the two features separately, and combine them into one hybrid convolution kernel for SRL. Figure 6 illustrates the two feature spaces, where the Path feature space is enclosed by a solid curve and the Constituent Structure feature space is enclosed by a dotted curve. We name them Path kernel and Constituent Structure kernel, respectively. Formally, the Path kernel is the tree kernel covering the smallest substructure which includes one predicate with the root node of a constituent subtree. The Constituent Structure kernel is the tree kernel covering a constituent.
 Having defined the two convolution tree kernels, namely, the Path kernel K path and the Constituent Structure kernel K cs , we now define a new kernel to combine the two individual kernels. According to Joachims et al. [2001], the kernel function set is closed under linear combination. It means that the following K hybrid is a valid kernel if K path and K cs are both valid. where 0  X   X   X  1, T 1 and T 2 are two syntactic trees.

Since the size of a parse tree is not constant, we normalize the K path ( T 1 , T 2 ) p K cs ( T 1 , T 1 )  X  K cs ( T 2 , T 2 ) respectively.

Unlike the feature space captured by the PAF kernel, the new feature space of the hybrid convolution tree kernel consists of two independent parts. Figure 7 illustrates the new feature space, where the Path feature space with six subtrees is listed above the dashed line, and the Constituent Structure fea-ture space with three subtrees is listed below. Clearly, it is different from the PAF kernel X  X  15 subtrees listed in Figure 5.

Figure 8 illustrates the differences between the PAF kernel and our hybrid convolution tree kernel. In the PAF kernel, the tree structures are identical when considering constituents rooted at NP and PRP as arguments respec-tively, as shown in Figure 8(a). However, the two constituents play different roles in the sentence for predicate buy and should not be viewed as identical. Figure 8(b) shows the computing examples with the hybrid convolution tree kernel.

Figure 9 highlights the different subtrees between the two cases in Fig-ure 8(b). Compared with the consideration of the case where the constituent rooted at NP as an argument [Figure 8(b)(1)], Figures 9(a) and 9(b) show the four additional features in the Path kernel feature space and the two ignored features in the Constituent Structure kernel feature space respectively, when considering the constituent rooted at PRP as an argument [Figure 8(b)(2)]. Therefore, the two trees could be distinguished correctly.

On the other hand, in most cases, the constituent structure feature space occupies the main part in the traditional PAF feature space. Statistics in the corpus of the CoNLL-2005 shared task shows that the size of a constituent structure is about twice the size of the path feature on average. Thus it plays a major role in the PAF kernel computation, as shown in Figure 10. Here, go 13: 12  X  W. Che et al.
 is a predicate and AM-PNC is a long subsentence. Our experimental results in Subsection 5.2 show that using the Constituent Structure kernel alone does not perform well. Since the Constituent Structure kernel dominates the PAF kernel score, the PAF kernel may not perform well, either. For example, if the final PAF kernel value is 0.9 (which is normalized by the size of the whole PAF structure), the constituent structure may contribute 0.6 whereas the path feature only contributes 0.3. Therefore, the contribution of the Path feature is not very significant in the final PAF kernel. In contrast, there is no such issue in our hybrid convolution tree kernel, since we have already normalized the Path kernel ( K path ) and Constituent Structure kernel ( K cs ) before combination. This can balance the contribution of the Constituent Structure feature and the Path feature, and therefore solve the problem found in the PAF kernel. We can also adjust the weights of the K path and the K cs to achieve an optimal performance. Still for the example above, when we consider and normalize the two features separately, both of them may be contributing 0.45 (assuming that the combination weights are equal). Thus, the value of the Constituent Structure kernel is reduced relatively. The contribution of the Path feature is enhanced.

Finally, the combination of two identical subtrees, though they are coming from two different kinds of structures, confuses the Path and the Constituent Structure. For example, assuming there is a  X  X P  X  VBD X  subtree in the Path of an instance and the same subtree occurs in the Constituent Structure of another instance, the two instances will have only one contribution in the PAF kernel. However, they should not be treated as equal as they belong to two different feature spaces. Our hybrid convolution tree kernel can overcome this problem well. In other words, our hybrid tree kernel uses different kernel functions to model different linguistic objects that describe different properties of the target linguistic phenomenon. From a machine learning perspective, we note that our hybrid kernel does not generate the substructures that tend to be less relevant for describing the target learning problem. 4.4 Related Work Moschitti et al. [2006] also noted that the PAF kernel has the drawback for SRL, especially for argument identification. They provided an improved PAF (MPAF) kernel for SRL. In the MPAF kernel, the root node of a constituent is appended with  X -B X  symbol. Therefore, for the nodes  X  NP  X  and  X  PRP  X  in Figure 8(a), they become  X  NP-B  X  and  X  PRP-B  X  respectively. Thus, the new kernel can distinguish the boundary between the path and the constituent structures. Compared with our hybrid convolution tree kernel, there remain three common features for MPAF shown in Figure 11, which fail to capture the  X  X  X  included structures, such as  X  X   X  NP VP X , which is a part of Path fea-ture and an important information for SRL. On the other hand, although the MPAF method considers the boundary between the path and the constituent structures, it still treats them as an integral structure. Therefore, like the PAF kernel, it still depends mainly on the constituent related features, and is not flexible enough to consider the contribution of the Path and the Constituent Structure features separately. We will compare the MPAF kernel with our hybrid convolution tree kernel empirically.

Finally, to our knowledge, all the previous work on convolution tree kernel-based methods and their applications in natural language processing only used 13: 14  X  W. Che et al. one tree kernel on their problems, while we use two convolution tree kernels in a single application. Although Zhang et al. [2006a] compared various tree kernel spaces for relation extraction, they did not consider combining some of the convolution tree kernels together. 5. EXPERIMENTS AND DISCUSSION The aim of our experiments is to verify the effectiveness of our hybrid convolu-tion tree kernel and its combination with the feature-based method for SRL. 5.1 Experimental Setting 5.1.1 Dataset. As mentioned in Section 2, the CoNLL-2005 SRL shared task corpus is used as our English experimental dataset. We follow the stan-dard partition using WSJ sections 02-21 for training, section 24 for develop-ment, and WSJ section 23 and the Brown corpus for testing.

As for the Chinese experiments, we use the Chinese PropBank 1.0 5 , which consists of standoff annotation on all the 931 articles (chtb 001.fid to chtb 931.fid) of the Penn Chinese TreeBank 5.1 6 . To compare with Xue and Palmer X  X  [2005] work, we follow their experimental setting, that is, dividing the CPB into the training data (661 files, chtb 100.fid to chtb 760.fid) and test data (other 99 files, chtb 001.fid to chtb 099.fid). In order to speed up the training process, we use the same parameters as the corresponding English methods tuned on the CoNLL-2005 development data. Therefore, no develop-ment data is used in the Chinese experiments. Note that the functional tags and traces are ignored for comparison with Xue and Palmer X  X  [2005] work. 5.1.2 Evaluation. The system is evaluated with respect to precision, re-call, and F  X  =1 of the predicted arguments. Precision ( p ) is the proportion of arguments predicted by a system which are correct. Recall ( r ) is the propor-tion of correct arguments which are predicted by a system. F  X  =1 computes the harmonic mean of precision and recall, which is the final measure to evalu-ate the performances of the systems. It is formulated as: F  X  =1 = 2 pr / ( p + r ). srl-eval.pl 7 is the official program of the CoNLL-2005 SRL shared task to eval-uate a system performance. 5.1.3 SRL Strategies. We use constituents as the labeling units to form the labeled arguments. Because of the errors of the automatic syntactic parser, it is impossible for each argument to find its matching constituent in all parse trees. Statistics on the training set shows that 10.08% of the arguments have no matching constituents when we use the Charniak parser [Charniak 2000]. The number increases to 11.89% when we use the Collins parser [Collins 1999]. Therefore, we select the more accurate Charniak parser as the preprocessing module in our SRL system.
In order to speed up the learning process, we use a four-stage learning ar-chitecture as follows: Stage 1: To save time, we use a pruning stage [Xue and Palmer 2004, 2005] to Stage 2: We then identify the candidates derived from Stage 1 as either argu-Stage 3: A multi-category classifier is used to classify the constituents that are Stage 4: A rule-based post-processing stage [Liu et al. 2005] is used for post 5.1.4 Classifier. Support vector machines (SVM) [Vapnik 1998] are se-lected as our classifier Stages for 2 and 3. The SVM is a binary classifier. In order to handle multi-classification problem in Stage 3, we adopt the one vs. others strategy [Rifkin and Klautau 2004] and select the label with the largest score output by a binary SVM as the final output. In addition, the strategy allows us to design a parallel training process which trains different binary classifiers at the same time. In our SVM implementation, we modified the bi-nary Tree Kernels in the SVM-Light Tool (SVM-Light-TK) 8 . It encodes the tree kernel inside the well known SVM-Light tool [Joachims 2002]. The parame-ters are tuned using the CoNLL-2005 development data set in the following experiments. 5.2 Experimental Results and Discussion In order to speed up the tuning process for choosing an optimal setting of pa-rameters, in the following experiments, we only use WSJ sections 02-05 of the CoNLL-2005 SRL shared task corpus as the training data and fine-tune the parameters on the CoNLL-2005 development set. Finally, we report the per-formance of using the entire training data set (including the CoNLL-2005 and the CPB data) and the fine-tuned parameters. In the same way as in Moschitti [2004], we also set the tree kernel decay factor  X  = 0 . 4 in the computation of convolution tree kernels.

The performance curve on the CoNLL-2005 development set with respect to  X  (the weight of hybrid convolution tree kernel in Equation (1)) is shown in Figure 12. Here, we use the default SVM parameter setting in the SVM-Light.
Figure 12 shows that when  X  = 0 . 5, the hybrid convolution tree kernel gets the best performance, F  X  =1 = 65 . 73. Both the Path kernel (  X  = 1, F  X  =1 = 59 . 21) and the Constituent Structure kernel (  X  = 0, F  X  =1 = 43 . 03) do not perform better than the hybrid one. It suggests that the two individual kernels are complementary to each other. This is the reason why we decompose the PAF kernel into a Path kernel and a Constituent Structure kernel. In addition, 13: 16  X  W. Che et al.
 the Path kernel performs much better than the Constituent Structure ker-nel. Through the tuning of parameter  X  , we can decrease the influence of the Constituent Structure kernel and optimize the contribution of different ker-nels. We set the parameter  X  = 0 . 5. The two individual kernels ( K path and K cs ) are normalized, respectively. It is different from the PAF kernel, where the Constituent Structure kernel usually dominates the final kernel value as shown in Figure 10. On the other hand, our hybrid convolution tree kernel emphasizes the contribution of the Path kernel more than the PAF kernel.
Figure 13 studies the effect of the parameter C in SVM on the hybrid convo-lution tree kernel. The parameter C controls the trade off between tolerating training errors and forcing rigid margins. It creates a soft margin on either side to allow for some misclassifications. Increasing the value of C increases the cost of misclassifying points and forces the creation of a more accurate model that may not generalize well.

We can see that the parameter C has a great influence on the performance of the hybrid convolution tree kernel. The performance improved sharply when C is increased initially. However, the rate gradually slows down until convergence. At the same time, the training time also takes longer because SVM has to find more exact hyperplanes. Therefore, considering the trade off between performance and training time, we select an optimal C = 4 . 5 for the hybrid convolution tree kernel. The optimal result (F  X  =1 = 68 . 90) is 3.17% higher than the default one (F  X  =1 = 65 . 73).

Table III compares the English PropBank performance among our hybrid convolution tree kernel, Moschitti [2004] X  X  PAF, MPAF kernel, and standard feature-based methods with linear and polynomial kernels ( d = 2 , d = 3) on the CoNLL-2005 development, and test data ( WSJ section 23 and Brown corpus). Here, the WSJ sections 02-05 is used as the training data. It is worth pointing out that the parameter C s and  X  , listed in Table III, are optimized on the CoNLL-2005 development set for each individual method, respectively.
We can see that our hybrid convolution tree kernel outperforms the PAF kernel with statistically significantly 9 (  X  2 test with p = 0 . 05) on all develop-ment and test data. In addition, although the MPAF kernel also outperforms the PAF kernel, its performance is still significantly (  X  2 test with p = 0 . 05) worse than our hybrid convolution tree kernel. This empirically demonstrates that our hybrid kernel is more effective than the PAF and the MPAF kernels for SRL. In addition, comparison of columns (1) and (4) shows that using only the hybrid convolution tree kernel method, we can achieve a comparable per-formance with the feature-based method using the linear kernel (Linear). It means that if the syntactic structure can be modeled effectively, it is suffi-ciently competitive to other methods which use a large amount of diverse fea-tures.

However, our hybrid kernel still performs worse than the standard feature-based methods which use the polynomial kernel. This is simple because our kernel only use the syntactic structure information while the feature-based method uses a large number of hand-crafted diverse features, including word, POS, voice, etc., and especially combination of these features. The feature-based method with polynomial kernel ( d = 2) achieves the best performance. It suggests that the binary combination among features implemented using the polynomial kernel ( d = 2) is very useful. Therefore, we expect that the performance would be better by combining our hybrid convolution tree kernel with the polynomial kernel. 13: 18  X  W. Che et al.

Table IV compares the Chinese PropBank (CPB) performance with different methods. They are the hybrid convolution tree kernel (Hybrid), the PAF ker-nel, the MPAF kernel, the best performance feature-based method using the polynomial kernels d = 2, a composite kernel (which will be introduced later), and the Xue and Palmer X  X  [2005] work. The Chinese flat features are imported from English.

The performance trends based on the handcrafted (gold) parse trees and the auto-parsed 10 trees are consistent, that is, our hybrid convolution tree kernel outperforms the PAF kernel and the MPAF kernel with statistical significance (  X  2 test with p = 0 . 05). For the same reason as above, our hybrid kernel per-forms worse than the standard feature-based method which uses the polyno-mial kernel ( d = 2). The standard method achieves a comparable performance to Xue and Palmer X  X  [2005] work, although the latter used a different classi-fier, maximum entropy [Berger et al. 1996], and a different feature set [Xue and Palmer 2005]. In addition, we can see that the polynomial kernel signif-icantly outperforms Xue and Palmer X  X  [2005] work based on the auto parsing, although they used a similar method, that is, feature-based method. We think that the main reason is that we use a better syntactic parser.

In order to make full use of the syntactic information and the standard flat features, we present a composite kernel to combine the hybrid convolution tree kernel ( K hybrid ) with a feature-based method with polynomial kernel ( K poly ): where 0  X   X   X  1.

The performance with respect to  X  on the development set of the CoNLL 2005 share task is shown in Figure 14. Here, note that we use the polynomial kernel ( d = 2) with a default C in SVM.

We can see that when  X  = 0 . 2, the system achieves the best performance with F  X  =1 = 70.74 . It is a statistically significant improvement (  X  2 test with p = 0 . 1) over using only the feature-based method with the polynomial kernel (  X  = 0, F  X  tree kernel (  X  = 1, F  X  =1 = 65 . 73) 11 . The main reason is that the convolu-tion tree kernel based methods can represent more general syntactic features than standard feature-based methods. On the other hand, the feature-based method captures more features than what the convolution tree kernel-based method can represent, such as Voice, Named Entity. Thus the two methods are complementary to each other.

To find the optimal parameters for the composite kernel, we tune C again and find the optimal C = 4.

Finally, we train the composite kernel and the polynomial kernel ( d = 2) using the above parameter setting (i.e.,  X  = 0 . 5,  X  = 0 . 2, and default e = 0 . 001, C s are optimized for each individual methods) on the entire CoNLL-2005 SRL shared task training set (WSJ sections 02-21). Table V compares the perfor-mance among the composite kernel, the polynomial kernel, and a CoNLL-2005 SRL shared task system Surdeanu and Turmo [2005], which ranks fifth among all participating systems in the shared task, but the best one when a single syntactic parser Charniak [2000] is used (using the same parse strategy as ours). However, they used a different classifier, AdaBoost [Schapire and Singer 1999], and a different feature set [Surdeanu and Turmo 2005].

Comparison of columns (2) and (3) shows that using the SVM classifier with the polynomial kernel (d = 2) outperforms Surdeanu and Turmo X  X  [2005] Ad-aBoost. However, the improvement is not significant. On the other hand, the composite kernel improves about 0.3%  X  0.6% over the polynomial kernel 13: 20  X  W. Che et al.
 (columns 1 vs. 2) and outperforms the best reported system in the CoNLL-2005 SRL shared task using the same parse results (columns 1 vs. 3).
 Similar to the English experiments, we train the composite kernel for the CPB using the same parameter setting as English. With the final F  X  =1 is 91.67 as shown in Table IV, the composite kernel outperforms the feature-based methods with the polynomial kernel (F  X  =1 = 91 . 13) and Xue and Palmer X  X  [2005] work (F  X  =1 = 91 . 3) respectively.

The above experiments on English and Chinese further verify the effective-ness of the hybrid convolution tree kernel method for SRL.

Finally, Table VI compares the computational time of the standard feature-based and our hybrid convolution tree kernel with the same SVM kernel machine on CoNLL 2005 dataset (2.0GHz  X  2 Xeon CPU and 4G Memory). It shows that: (1) The tree kernel is slower than the standard feature-based methods. (2) It is very time-consuming to train an SVM classifier in a large dataset. 6. CONCLUSIONS AND FUTURE WORK In this article, we have proposed a hybrid convolution tree kernel to model syn-tactic structure information for semantic role labeling (SRL). Different from the previous convolution tree kernel based methods, we distinguish the Path and the Constituent Structure feature spaces. Evaluations on the data sets of the CoNLL-2005 SRL shared task and Chinese PropBank (CPB) show that our novel hybrid convolution tree kernel significantly outperforms the previous predicate argument feature (PAF) kernel and its improved version (MPAF). Therefore, we suggest the approach of using multiple tree kernels to model dif-ferent linguistic objects in natural language processing applications. The final composite kernel between the hybrid convolution tree kernel method and the feature-based method with the polynomial kernel ( d = 2) outperforms the best reported systems on CoNLL-2005 corpus using a single parser and on the CPB corpus using correct syntactic parse results respectively.

The immediate extension for our work is to integrate more linguistic knowl-edge in convolution tree kernels. For example, we can do approximate sub-structure matching based on linguistic knowledge. We can also do feature selection under convolution tree kernel framework with linguistic knowledge. Moreover, we can also explore the hybrid convolution tree kernel method in other tasks, such as relation extraction in the future.
 We would like to thank the anonymous reviewers for their critical and insight-ful comments.
 13: 22  X  W. Che et al.

