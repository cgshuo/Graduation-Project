 Query log analysis has received substantial attention in re -cent years, in which the click graph is an important tech-nique for describing the relationship between queries and URLs. State-of-the-art approaches based on the raw click frequencies for modeling the click graph, however, are not noise-eliminated. Nor do they handle heterogeneous query-URL pairs well. In this paper, we investigate and develop a novel entropy-biased framework for modeling click graphs. The intuition behind this model is that various query-URL pairs should be treated differently, i.e., common clicks on l ess frequent but more specific URLs are of greater value than common clicks on frequent and general URLs. Based on this intuition, we utilize the entropy information of the URLs and introduce a new concept, namely the inverse query fre-quency (IQF), to weigh the importance (discriminative abil -ity) of a click on a certain URL. The IQF weighting scheme is never explicitly explored or statistically examined for any bipartite graphs in the information retrieval literature. We not only formally define and quantify this scheme, but also incorporate it with the click frequency and user frequency information on the click graph for an effective query rep-resentation. To illustrate our methodology, we conduct ex-periments with the AOL query log data for query similarity analysis and query suggestion tasks. Experimental results demonstrate that considerable improvements in performanc e are obtained with our entropy-biased models.
 Categories and Subject Descriptors: H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models, query formulation General Terms: Algorithms, Experimentation Keywords: Entropy-biased model, click graph, click fre-quency, inverse query frequency, user frequency
Recently query log analysis has been studied widely for improving search engines X  efficacy and usability. Such stud-ies mined the logs to improve numerous search engine X  X  ca-pabilities, such as query suggestion and classification, ra nk-ing, targeted advertising, etc. The click graph [5], a bipartite graph between queries and URLs, is an important technique for describing the information contained in the query logs, in which edges connect a query with the URLs that were clicked by users as a result. An example of a click graph with 4 queries and 4 URLs is depicted in Fig. 1. The edges of the graph can capture some semantic relations between queries and URLs. For example, queries  X  X ap X  and  X  X ravel X  are related to each other, since they are co-clicked with som e URLs such as  X  X ww.mapquest.com X  and so on. Therefore, how to utilize and model the click graph to represent queries becomes an interesting and challenging problem.

Traditionally, the edge of the click graph is weighted based on the raw click frequency (number of clicks) [5] from a query to a URL. The transition probability can be further deter-mined by the normalized click frequency [18, 15]. Taking the edge from  X  X ap X  to  X  X ww.mapquest.com X  in Fig. 1 as an ex-ample, the raw click frequency is 10 and the normalized click frequency is 10 / 22. However, the traditional query represen-tation for the click graph has its own disadvantages. One of these disadvantages is its robustness, i.e., a query that ha s a skewed click count on a certain URL may exclusively influ-ence the click graph, such as navigational queries. In order to avoid the adverse effect on learning algorithms, previous work presented in [14] simply identified some navigational queries and removed them from the click graph. Unfortu-nately, the deletion of such queries leads to the loss of some information. Another related problem is that the raw click frequency can be easily manipulated as it is prone to spam by some malicious clicks. To deal with these critical problems , we explore a novel entropy-biased framework which incorpo-rates raw click frequencies and other information with the entropy information of the connected URLs.

The basic idea of the entropy-biased model is that various query-URL pairs should be treated differently. Let us look at the query  X  X ap X  ( q 2 ) and its connected URLs, which is shown in Fig. 1. The click frequency from q 2 to d 3 is the same as the count (10) from q 2 to d 1 . There is a critical question when only consider the raw click frequency: Is a single click on different URLs in the click graph equally im-portant? Clearly not! In this case, at an intuitive level, one click on d 3 may capture more meaningful information, or be more important than one click on d 1 . The key dif-ference is that the connected URLs are different: One URL is  X  X ww.mapquest.com X , which is connected with 2 queries; while another URL is  X  X ww.yahoo.com X , which is connected with 3 queries. Before performing a theoretical analysis, w e first briefly review the entropy and information theory [23]. Suppose there is a URL which is commonly clicked and con-nected with most of the queries (with equal probability), th is tends to increase the ambiguity (uncertainty) of the URL. However, if the URL is clicked and connected with fewer queries, this tends to increase the specificity of the URL. A frequently clicked URL thus functions in retrieval as a non-specific URL, even though its meaning may be quite specific in the ordinary sense. Therefore, a single click on a specific URL is most likely to be more important for distinguishing the specificity of the query than another click on an ambigu-ous URL . Based on the above intuition, we introduce a new concept, denoted as the inverse query frequency , to weigh the importance of a click on a certain URL, which can be extended and used for other bipartite graphs.

Consequently, we propose a novel entropy-biased model, namely CF-IQF model, to represent the query, which simul-taneously combines the inverse query frequency informatio n with the raw click frequency. As the raw click frequency can be easily manipulated, we develop and use the number of users associated with the query-URL pair, namely the user frequency (UF model), instead of the raw click frequency (CF model) to improve the resistance against malicious clic k data. Moreover, the inverse query frequency can be incor-porated with the user frequency, as another entropy-biased UF-IQF model, to achieve better performance. To illustrate our methodology, we apply the entropy-biased models to query similarity analysis and query suggestion tasks using the real-world AOL query log data. The main concern is to increase the precision of the top-n retrieved results. For the query similarity analysis, we compare six different mod-els, including four models (CF, CF-IQF, UF and UF-IQF) based on the click graph and two models (TF and TF-IDF) based on the query terms. It is shown that CF-IQF model improves over CF model by up to 6.12%, while UF-IQF over UF by up to 5.5%. As expected, UF-IQF and UF outper-form CF-IQF and CF respectively. In addition, UF-IQF model significantly improves the traditional TF-IDF model by up to 21.89%. For the query suggestion task, evaluation results also show that the entropy-biased models outperfor m the baseline models, indicating that the improvements in ou r proposed models are consistent and promising.

In a nutshell, our contributions of this paper are: (1) the introduction of a new notion, namely the inverse query fre-quency , to weigh the importance of a click on a certain URL, which can be extended and used for other bipartite graphs; (2) the identification of a new source, called the user fre-quency , for diminishing the manipulation of the malicious clicks; (3) the framework of the entropy-biased model for the click graph, which simultaneously combines the inverse query frequency with the click frequency and user frequency information; and (4) the first formal model to distinguish the variation on different query-URL pairs in the click graph.
The rest of this paper is organized as follows. We briefly review some related work on query log analysis in Section 2. In Section 3 we present the proposed query representation models. Section 4 describes two basic applications of these models, which are the query similarity analysis and query suggestion. We then describe and report the experimental evaluation in Section 5. Finally, we present our conclusion s and future work in Section 6.
With the advance of search technologies, many approaches have been proposed to utilize and analyze query logs to enhance the search results in various aspects. A common model for utilizing query logs from search engines is in the form of a click graph [5]. Based on the click graph, many research efforts in query log analysis have been devoted to query clustering [3, 26], query suggestion [13, 15], query c las-sification [14] and user behavior understanding [19, 6, 4, 9] . The use of the click data for query clustering has been sug-gested by Befferman and Berger [3], who proposed an ag-glomerative clustering technique to identify related quer ies and Web pages. Wen et al. [26] combined query content information and click-through information and applied a density-based method to cluster queries. Craswell and Szum -mer [5] used click graph random walks for relevance rank in image search. Mei et al. [15] proposed an approach to query suggestion by computing the hitting time on a click graph. [14] presented the use of click graphs in improving query in-tent classifiers. These methods are proposed based on the click graph, while our objective is to investigate a better model to utilize and represent the click graph.

There are several approaches that have tried to model the representation of queries or documents on the click graph. Baeza-Yates et al. [1] used the content of clicked Web pages to define a term-weight vector model for a query. They con-sidered terms in the URLs clicked after a query. Each term was weighted according to the number of occurrences of the query and the number of clicks of the documents in which the term appeared. In [2], the authors introduced another vectorial representation for the queries without consider ing the content information. Queries were represented as point s in a high dimensional space, where each dimension corre-sponds to a unique URL. The weight assigned to each di-mension was equal to the click frequency. This is one of the traditional click frequency models. Moreover, Poblete et al. [17] proposed the query-set document model by min-ing frequent query patterns to represent documents rather than the content information of the documents. However, these existing methods do not distinguish the variation on different query-URL pairs.

Besides, there is a trend to explore the query logs and model queries with variation for personalization [8, 24]. D ou et al. [8] explored click entropy to measure the variability in click results, while Teevan et al. [24] proposed result entr opy to capture how often results change. In this paper, we also utilize the entropy information of the URL. Other methods are focused on personalization for different queries, while our proposed entropy-biased models are different, which are focused on the weighting scheme of various query-URL pairs.
This work is also related to the term frequency inverse document frequency (TF-IDF) model [12, 22], which has a significant effect in the information retrieval field. Our proposed method shares the key point to identify and tune the importance of a query-URL edge as TF-IDF for a term. The major difference is that the TF-IDF model is applied to find the term weight in a document, while our entropy-biased models are employed to identify the edge weight of the click graph, which can also be applied to other bipartite graphs without the content information.
As stated above, the issue of how to represent queries based on the click graph is critical to the task of effectively analyzing query logs. In this section, we first introduce the preliminaries and notations, and then investigate and ex-plore the query representation models for the click graph.
Let Q = { q 1 , q 2 , ..., q M } be the set of M unique queries submitted to a search engine during a specific period of time. Let D = { d 1 , d 2 , ..., d N } be the set of N URLs clicked for those queries. A click graph is a query-URL bipartite graph G = ( Q  X  D, E ) where every edge in E connects a vertex in the query set Q and one in the URL set D . For q  X  Q and d  X  D , the pair ( q, d ) is an edge of E if and only if there is a user who clicked on URL d after submitting the query q . For each edge ( q i , d j )  X  E , we associate a numeric weight c , known as the click frequency , that measures the number of times the URL d j was clicked when shown in response to the query q i . Let C be an M  X  N matrix, whose M rows correspond to the queries of Q and whose N columns correspond to the URLs of D , and the entry ( i, j ) contains a value c ij . The click frequency matrix of Fig. 1 is shown in Table 2(a).

Let U = { u 1 , u 2 , ..., u K } be the set of K users who sub-mitted the queries and clicked on the URLs. Now, a query instance can be made up of one or more h q, d, u i triples. It is obvious that every edge ( q i , d j ) in the click graph has a set of users associated with it, so we introduce a new no-tion uf ij , referred to as the user frequency , that measures the total number of users who submitted the query q i and clicked on the URL d j . This measurement can be a good supplement of the click frequency for a robust query repre-sentation. To further explore the information of query logs , we aggregate the number of queries that are connected with a URL d j and use n ( d j ) to denote it. Some other nota-tions are briefly shown in Table 1, and will be defined in the following subsections.
Traditionally, the edge of the click graph is weighted by the raw click frequency between a query and a URL, which we call click frequency (CF) model . Given q i  X  Q and d j D , the transition probability [5, 18, 15] from the query q to the URL d j is defined by normalizing the click frequency from the query q i as where cf ( q i ) = P j  X  D c ij , and it denotes the aggregated Symbol Meaning
C M  X  N query-URL matrix c ij Click frequency between query q i and URL uf ij User frequency between q i and d j n ( d j ) Number of queries associated with URL d j idf ( d j ) Importance of a certain URL d j p ( d j | q i ) Transition probability from q i to d j p ( q i | d j ) Transition probability from d j to q i P q 2 d An M  X  N query-URL probability matrix
P d 2 q An N  X  M URL-query probability matrix number of clicks for q i . The notation p ( q i | d j ) denotes the transition probability from the URL d j to the query q i , where cf ( d j ) = P i  X  Q c ij , and it denotes the aggregated number of clicks for the URL d j . Although the click fre-quency c ij is the same, the transition probabilities p ( q and p ( d j | q i ) are generally not symmetric because of the var-ious normalization. If there is no edge between q i and d the transition probability is equal to 0.

After calculating all these transition probabilities, we o b-tain two kinds of matrices: P q 2 d  X  R M  X  N and P d 2 q R
N  X  M . Taking the click graph of Fig. 1 as an example, we can get the transition matrix P q 2 d as shown in Table 2(b). Without considering the content information, the query q i can be represented by a vector of documents weighted as the i -th row of the matrix P q 2 d :  X  X  X  q i = h P q 2 d ( i, 1) , ..., P and meanwhile the document d j can be represented by a vector of queries weighted as the j -th row of the matrix P it can be used to measure the similarity between queries and applied to other query log analysis. According to Ta-ble 2(b), for example, the most similar queriy of q 2 ( X  X ap X ) is q 1 ( X  X ahoo X ) using the cosine similarity.
The CF model only considers the raw click frequency, and treats different query-URL pairs equally even if some URLs are very heavily clicked. More generally, a great variation in URL distribution is likely to appear, and it may thus cause the loss of important information since different query-URL pairs are not sufficiently distinguished. For example, the click frequency c 21 is equal to c 23 in Fig. 1. However, it may be more reasonable to weight these two edges differently because of the variation of the connected URLs.

In this paper, we define int ( q, d ) to be true when the query q has clicks on d at least once. Let n ( d j ) be the total num-ber of queries ( query frequency ) that are connected with the URL d j , which is defined as It is predicted that the more general and highly ranked URL would be clicked and connected with more queries than the specific URLs. Thus the less specific URLs would have a larger collection distribution than the more specific ones, which tends to increase the ambiguity and uncertainty of d the URLs in the ordinary sense. Using information theory, the entropy [23] of a URL d j is defined as Suppose that the URL d j is connected with those queries with equal probability p ( q i | d j ) = 1 n ( d tropy is transformed to Generally, the entropy of the URL tends to be proportional to the query frequency n ( d j ). In order to simplify the calcu-lation, we roughly use the maximum entropy to approximate the exact entropy in the following analysis.

It is argued that the discriminative ability of a URL should be inversely proportional to the entropy, hence a (heavily-clicked) URL with a high query frequency is less discrimina-tive overall. This motivates us to propose a novel and im-portant concept, referred to as the inverse query frequency , to measure the discriminative ability of the URL d j . Sup-pose | Q | is the total number of queries in the query log, the inverse query frequency for the URL d j is defined as, which is similar to the inverse document frequency for the term [12]. The inverse query frequency factor has several benefits. The most important one is that it can constrain and diminish the influence of some heavily-clicked URLs. This will tend to balance the inherent bias of clicks for thos e highly ranked URLs [6]. Furthermore, the inverse query frequency can be incorporated with other factors to tune the representation models as shown in the following subsection s.
In the entropy-biased model, we incorporate the inverse query frequency with the raw click frequency in a unified CF-IQF model , namely The intuition behind the CF-IQF model is that query-URL pairs are treated differently according to the inverse query frequency, so that the common clicks on less frequent yet more specific URLs are of greater value than the common clicks on frequent URLs. Figure 2 shows the surface specified by the click frequency, query frequency, cfiqf , with color specified by the cfiqf value. The color is proportional to the surface height. A high weight cfiqf is reached by a high click frequency for the query-URL pair and a low query frequency associated with the URL in the whole query log. As shown in Fig. 2, the query-URL pair A , which has the same click frequency with B , will be weighted much higher than B because of the associated inverse query frequency, Figure 2: The surface specified by the click fre-quency, query frequency and cfiqf, with color speci-fied by the cfiqf value. The color is proportional to the surface height. hence such weights tend to diminish the influence of heavily-clicked URLs.

The new transition probability from q i to d j becomes where cfiqf ( q i ) = P j  X  D cfiqf ( q i , d j ). The new matrix q 2 d of Fig. 1 is shown in Table 2(c). Based on this ma-trix, it can be calculated that the most similar query of q 2 ( X  X ap X ) is q 3 ( X  X ravel X ), which is more reasonable than the result of CF model. Currently, we only consider changing the transition probability from the query to the URL, and keeping the transition probability p ( q i | d j ) from the URL to the query as the same as that of CF model.
Another drawback of the CF model is that it is prone to spam by some malicious clicks, and it can be easily influ-enced by a single user if he/she clicked on a certain URL thousands of times. To address the problem, we introduce a new concept user frequency (UF), which denotes the num-ber of users associated with the query-URL pair, instead of the click frequency, to improve the resistance against mali -cious click data. Let int ( q i , d j , u k ) to be true if a user u submitted the query q i and clicked on the URL d j at least once, then the user frequency uf ij is defined as Based on the user frequency, we can obtain UF model similar to CF model. Intuitively, UF model reinforces the capabilit y of diminishing the effect of some manipulated clicks.
To further distinguish the performance of the model, we also incorporate the user frequency with the inverse query frequency in a unified UF-IQF model , With Eq. 8, the transition probability from q i to d j becomes where ufiqf ( q i ) = P j  X  D ufiqf ( q i , d j ).
In this subsection, we establish the connection between our entropy-biased model and the famous TF-IDF model [12, 22]. Over the years, the weighting scheme TF-IDF has been extensively and successfully used in the vector space model for text retrieval. Several researchers [20, 7, 21] have tri ed to interpret IDF based on binary independence retrieval, Pois -son, information entropy and language modeling. Although the success of the TF-IDF in the text mining is widely claimed, it has never been explored to bipartite graphs. The idea of measuring the discriminative ability of the URL by IQF is totally new, and it can be expected to produce the similar effects on click graphs as IDF on text mining. More-over, our entropy-biased model is employed to identify the edge weighting of the click graph, which can also be applied to other bipartite graphs without the content information. As the query can also be represented by the vector of terms using TF and TF-IDF models, we will compare the per-formance of these two models with our proposed models in Section 5.3.
The proposed query representation models can be applied to mine the query log in many cases, such as query-to-query similarity, query clustering, query suggestion, etc. For t he comparison of different models, we focus on two tasks: (1) the fundamental query-to-query similarity analysis, whic h is very suitable for evaluating the performance of the propose d query representation models, and (2) the popular query sug-gestion task, which is to find semantically related queries f or a given query using the graph-based random walk model.
As the query can be represented by a vector of documents (or a vector of terms), two common similarity measurements will be used to calculate the similarity between queries: on e is the cosine similarity and the other is the Jaccard coeffi-cient. The cosine similarity is a measure of similarity be-tween two vectors by finding the angle  X  between them. It is represented using a dot product and magnitude as where  X  X  X  q i denotes the vector of a query. The Jaccard coef-ficient is defined as the value of the intersection divided by the value of the union of the query vectors: where P q 2 d ( i, n ) denotes the n -th value of  X  X  X  q i . We report and analyze the query similarity results in Section 5.3.
In previous studies [5, 18, 15], the click graph has been thought of as a random walk between queries and URLs according to the transition probabilities P q 2 d and P d 2 q consider the vertices in one side, such as the query-to-quer y graph, then a new random walk can be introduced by the transition probability from q i to q j , We use P q 2 q to denote the transition matrix whose entry ( i, j ) has the value p ( q j | q i ). It is important to note that the self-transition probability exists naturally in the model .
The personalized PageRank [11, 10] is the steady-state distribution of the random walk, which is usually used to rank vertices on the graph in a query dependent way. The corresponding linear system of personalized PageRank can be shown as: where R (0) j is a personalized (or query dependent) initial val-ues for vertex j , and n is the steps of a random walk. We may set R (0) j = 1 if v j is the given query and 0 otherwise. The parameter  X  is usually set to be 0.7 in previous stud-ies. Since the objective is to show the effectiveness of our proposed models for query suggestion, we present the query suggestions ranked by personalized PageRank in Section 5.4 .
In the following experiments we compare our proposed models with other methods on the tasks of mining query logs through an empirical evaluation. We define the follow-ing task: Given a query and a click graph, the system has to identify a list of queries which are most similar or se-mantically relevant to the given query. In the rest of this section, we introduce the data collection, the assessments and evaluation metrics, and present the evaluation results .
The dataset that we study is adapted from the query log of AOL search engine [16]. The entire collection consists of 19 , 442 , 629 user click-through records. These records con-tain 10 , 154 , 742 unique queries and 1 , 632 , 789 unique URLs submitted from about 650 , 000 users over three months (from March to May 2006). As shown in Table 3, each record of the click contains the same information: UserID, Query, Rank and ClickURL (we do not show the Time properties due to the limited space). This dataset is the raw data recorded by the search engine, and contains a lot of noises. Hence, we conduct a similar method employed in [25] to clean the raw data. We clean the data by removing the queries that ap-pear less than 2 times, and by combining the near-duplicated queries which have the same terms without the stopwords and punctuation marks (for example,  X  X oogle X  X  image X  and  X  X oogle image X  will be combined as the same query). Af-ter cleaning, we get totally 883 , 913 queries and 967 , 174 URLs in our data collection. After the construction of the click graph, we observe that a total of 4 , 900 , 387 edges ex-ist, which indicates that each query has 5 . 54 distinct clicks, and each URL is clicked by 5 . 07 distinct queries. Moreover, Table 3: Samples of the AOL query log dataset.
 Figure 3: The distributions of the (a) click fre-quency, (b) user frequency and (c) query frequency. taken as a whole, this data collection has 250 , 127 unique terms which appear in all the queries.

It has been shown in [2] that the occurrences of queries and the clicks of URLs exhibit a power-law distribution. However, the properties of the user frequency and query frequency have not been well explored. Fig. 3 shows the dis-tributions of the click frequency ( c ij ) and the user frequency ( uf ij ) associated with the query-URL edges, and the query frequency ( n ( d j )) associated with the URLs. All of them exhibit power-law distributions in the figure.
It is difficult to evaluate the quality of query similar-ity/relevance rankings due to the scarcity of data that can be examined publicly. For an automatic evaluation, we uti-lize the same method used in [2] to evaluate the similarity of retrieved queries, but engage the Google Directory 1 in-stead of the Open Directory Project 2 . When a user types a query in Google Directory, besides site matches, we can also find category matches in the form of paths between direc-tories. Moreover, these categories are ordered by relevanc e. For instance, the query  X  X nited States X  would provide the hierarchical category  X  X egional &gt; North America &gt; United States X , while one of the results for  X  X ational Parks X  would be  X  X egional &gt; North America &gt; United States &gt; Travel and Tourism &gt; National Parks and Monuments X . Hence, to measure how similar two queries are, we can use a notion of similarity between the corresponding categories provid ed by the search results of Google Directory. In particular, we measure the similarity between two categories Ca i and Ca as the length of their longest common prefix P ( Ca i , Ca divided by the length of the longest path between Ca i and Ca r . More precisely, the similarity is defined as:
Sim ( Ca i , Ca r ) = | P ( Ca i , Ca r ) | / max( | Ca i where | Ca i | denotes the length of a path. For instance, the similarity between the above two queries is 3/5 since they share the path X  X egional &gt; North America &gt; United States X  and the longest one is made of five directories. We evaluate the similarity between two queries by measuring the simi-http://directory.google.com/ http://www.dmoz.org/ Table 4: Comparison of different methods by P@1 and P@10. We also show the percentage of relative improvement in the lower part.
 CF-IQF/TF-IDF 9.09% 11.57% 15.65% 19.39%
UF-IQF/TF-IDF 8.44% 13.61% 16.19% 21.89 % larity between the aggregated categories of the two queries , among the top 5 answers provided by Google Directory.
To give a fair assessment, we randomly select 300 distinct queries from the data collection, then retrieve a list of sim -ilar queries using the proposed methods for each of these queries. For the evaluation of the task, we adopt the preci-sion at rank n to measure the relevance of the top n results of the retrieved list with respect to a given query q r , which is defined as where Sim ( q i , q r ) means the similarity between q i and q our experiments, we report the precision from P @1 to P @10, and take the average over all the 300 distinct queries.
We consider the question whether our proposed method can boost the performance using the entropy-biased mod-els for the fundamental query similarity analysis tasks. We compare six different models, including four models (CF, CF-IQF, UF and UF-IQF) based on the click graph and two models (TF and TF-IDF) based on the query content information, and report the precisions from P @1 to P @10 in Fig. 4 using two similarity measurements. In this figure we can see, as expected, that our proposed entropy-biased CF-IQF model outperforms the CF model in all the metrics from P @1 to P @10. Similarly to what happens between the CF-IQF and CF models, the performance of the UF-IQF model is better than that of the UF model. The results support our intuition of the entropy-biased framework about treat-ing various query-URL pairs differently. When comparing the results of UF with CF, and the results of UF-IQF with CF-IQF, we can observe that the UF and UF-IQF models perform better than the CF and CF-IQF models respec-tively, which indicates the user frequency associated with the query-URL pair is more robust than the click frequency for modeling the click graph.

We also compare our models with the TF and TF-IDF models to see whether the improvements of CF-IQF and UF-IQF over CF and UF models are consistent with the using two different similarity measurements. improvement of the TF-IDF over TF model. According to Fig. 4, it is obvious that the TF-IDF model improves the performance of the TF model, with the same observations of our entropy-biased models. The reason is that they share the same key point to identify and tune the importance of a term or a query-URL edge. The major difference is that the TF-IDF model is used to find the weight value of a term in a document, which has a significant effect in the infor-mation retrieval field. However, our entropy-biased models are applicable in identifying the weight of the edge for the click graph, which can be extended to other bipartite graphs without the content information.

To gain a better insight into the details of the results, we show the comparison of different models using P @1 and P @10 in Table 4. The first part shows the absolute preci-sions of those models, and the second part illustrates the percentage of relative improvements. A quick scan of the first part, accompanying with Fig. 4, reveals that UF-IQF achieves the best performance in most cases. When look-ing at the relative improvements of those models (the top four lines of the lower part), we can see that CF-IQF im-proves over CF by up to 6 . 12%, UF-IQF over UF by up to 5 . 5%, and UF-IQF over CF by up to 6 . 51%. While TF-IDF improves over TF by up to 9 . 79% for P @10 using Jaccard coefficient, this is because the precision of TF is much lower than other methods, which can be easily be improved. In terms of the final four lines in Table 4, another interesting comparison is seen between the proposed models on the click graph and the traditional models on the query content in-formation. Based on the click graph, CF and UF models improve the traditional TF model significantly from 9 . 76% to 30 . 02%, while CF-IQF and UF-IQF models also improve the traditional TF-IDF model from 8 . 44% to 21 . 89%. The results reconfirm many previous studies [2, 19] that the clic k graph catches more semantic relations between queries than the query terms. According to the experimental results, we can argue that it is very essential and promising to consider the entropy-biased models for the click graph.

To test the sensitivity of the similarity measurement of our entropy-biased models, we compare the results of the Jaccard coefficient, and find that the improvements are con-sistent with the cosine similarity, which indicates that ou r entropy-biased models are independent of the similarity me a-surements. In addition, we notice that Jaccard coefficient performs better than cosine similarity using CF, CF-IQF, Table 5: Examples of query suggestions generated by two different models on click graph.
 UF and UF-IQF models on the click graph, while cosine similarity is better than Jaccard coefficient using TF and TF-IDF models on the query content information.
In this subsection, we present the comparison of sugges-tions generated using the same random walk method with CF and CF-IQF models (we do not show the comparison of UF and UF-IQF models due to space constraints and simi-lar results). To better understand the improvements of our entropy-biased models, we evaluate the performance of our methods with different number of steps (from 2 to 50). Fig-ure 5 illustrates the precisions ( P @10) of CF and CF-IQF models for different parameter n . With the increase of n , both models improve their performance, which can also con-verge quickly after about 10 steps. As shown in Fig. 5, it is very clear that the CF-IQF model always performs better than the CF model.

We selectively show the detailed results ranked by the transition probabilities in Table 5. In general, the top-4 s ug-gestions generated by the CF model and the CF-IQF model are similar, and mostly semantically relevant to the origin al query. For the first example in Table 5, these two models generate the same suggestions, since the transition probab ili-ties in both models are usually similar. From these suggeste d Figure 5: The performance of random walk model. results, we see that our models not only capture the most common sense, the  X  X merican airline X , they also successful ly predict infrequent query  X  X lcoholics anonymous X  as sugges -tion. After looking into the last two examples, one impor-tant observation is that our CF-IQF model can boost more relevant queries as suggestion and reduce some irrelevant queries. To see the suggestions for  X  X ast texas real estate X  , for example, we notice that the first suggestion X  X oogle X , pr o-vided by the CF model, is irrelevant to the original query. This is because there is a edge between the query X  X ast texas real estate X  and a heavily-clicked URL  X  X ww.google.com X , which are highly associated with the query  X  X oogle X  so as to generate the high transition probability from X  X ast texas r eal estate X  to  X  X oogle X . In the last example, the irrelevant sug -gestion  X  X uy.com X  in the CF model arises from the similar reason. Comparing with the CF model, the CF-IQF model can successfully constrain such irrelevant queries and ret urn mostly relevant suggestions (e.g., upright exercise bike) , be-cause it reduces the adverse factor in such situations by con -sidering the inverse query frequency in the click graph.
In this paper we present the novel entropy-biased mod-els for click graphs, whose basic idea is to treat various query-URL pairs differently according to the inverse query frequency. Although its fundamental concept is very sim-ple, the IQF weighting scheme is never explicitly explored or statistically examined for any bipartite graphs in the in -formation retrieval literature. We not only formally define and quantify this scheme, but also propose the new entropy-biased framework to incorporate it on the click graph for an effective query representation. We apply proposed models to mine the query log and compare with the baseline models in two popular tasks. Experimental results show that the improvements of our proposed models are consistent and promising. In future work, it would be interesting to apply this entropy-biased model to identify some noise click data . Furthermore, we would like to investigate the performance of our model in other bipartite graphs to see if the proposed method might have an impact on any bipartite graphs.
The authors would like to thank Lei Zhang for the discus-sion and comments, and the anonymous reviewers for many helpful comments on the manuscript. This work is sup-ported by two grants from the Research Grants Council of the Hong Kong SAR, China (Project No. CUHK4128/08E and Project No. CUHK4158/08E). This work is also affili-ated with the Microsoft-CUHK Joint Laboratory for Human-Centric Computing and Interface Technologies.
