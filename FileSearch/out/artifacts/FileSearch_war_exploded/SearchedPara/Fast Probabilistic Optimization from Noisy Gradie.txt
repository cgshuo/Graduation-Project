 Philipp Hennig philipp.hennig@tuebingen.mpg.de Much of machine learning involves nonlinear optimiza-tion of a negative log likelihood, loss, energy, or other objective function of the data set Z to be optimized. All efficient optimization algorithms require the gradient (shortened to large it can be impossible to evaluate the loss on the entire dataset, and subsets  X  j known as mini-batches are used instead. If the mini-batches are chosen iid from the dataset, the result is a stochastic gradient whose value corresponds to the true gradient up to some noise  X  j , which, by the central limit theo-rem, is approximately Gaussian distributed. A typical example of this setup is the training of neural networks by back-propagation (Rumelhart et al., 1986). The most straightforward, but not the most efficient use of the gradient for optimization is gradient descent  X  iteratively updating the parameters according to x t + 1 x  X  tic gradient descent , x t + 1 case,  X  t theory on setting  X  (e.g. Robbins &amp; Monro, 1951). For example, choosing convergence in the limit t efficiency. In fact, it is clear from inspection that the gradient descent learning rule is not invariant under even linear changes of measure: The units of measure of were to minimize potential energy density by skiing down a slope, a European gradient descent algorithm (  X  f tres, while it X  X  American cousin would be much more cautious on the same slope ( moving only in increments of ten-thousandths of a foot (3  X  m). Simple standardisation does not solve this fun-damental problem: what is a  X  X tandard X  gradient at initiation may turn out to be a steep or flat gradient in other regions. In particular, gradient descent can be very slow in plateau regions, and stochastic gradient descent can effectively get stuck in diffusion in such areas. The long established solution to this problem is to correct the gradient descent direction relative to a local measure of f of the right units sic, though not unique, choice is the Newton-Raphson algorithm, which corrects by the local curvature, the Hessian B This algorithm is often described as efficient , both in the sense that, if L were a quadratic, it would reach the minimum in one step, and in the sense that there is a neighbourhood around each local minimum x that, if the algorithm reaches this neighbourhood, the estimate x t converges to x &amp; Mor  X ee, 1977)  X  although this neighbourhood can be arbitrarily small. Unfortunately, Newton X  X  algorithm is too expensive for high-dimensional problems: If x constructing the Hessian has general cost inversion sue is offered by quasi-Newton algorithms (Fletcher &amp; Powell, 1963; Broyden, 1965), which are learning methods constructing low-rank estimators for B observations of only the gradient. A particularly cost-efficient member of this class is the L-BFGS algorithm (Nocedal, 1980; Broyden, 1969; Fletcher, 1970; Gold-farb, 1970; Shanno, 1970), which can construct such an estimator in ality. Unfortunately, though, quasi-Newton methods do not apply to noisy gradient observations. Several authors have proposed computationally elegant ways of re-scaling the gradient using the Hessian (Pearlmut-ter, 1994; Schraudolph, 2002; Martens, 2010; Martens &amp; Sutskever, 2011), sometimes leveraging the struc-ture (e.g. sparsity) of specific optimization problems (Bordes et al., 2009). But, again, these  X  X essian-free X  approaches do not properly account for noise. Their cost is also not usually involve sampling (Byrd et al., 2011).
 Quasi-Newton methods are touchstones of numerical analysis. Decades after their invention, they remain state of the art in nonlinear optimization. Recently, work by Hennig &amp; Kiefel (2012) has provided novel, uni-fying insight into this family of methods, casting them as an approximation to Bayesian linear regression. The cited work established that the low computational cost of algorithms like BFGS is achieved by deliberately ignoring or weakening prior knowledge, such as the symmetry of the Hessian, whose exact encoding would have at least cubic cost. These authors then focussed on extending the classic algorithms to a nonparamet-ric paradigm, which increases performance in various ways. This paper generalises these results to noisy ob-servations. This is nontrivial, because the noisy setting requires a more detailed study of the information con-tent of each observation. As Hennig &amp; Kiefel showed, classic quasi-Newton methods can be interpreted as using each observation twice, in two independent likeli-hood terms. When considering noisy observations, we thus have to make sure that these observations do not amount to double counting. This paper shows that quasi-Newton methods are in fact more cautious than necessary. The guiding insight is that they deliberately ignore structural knowledge , in exchange for low com-putational cost, to a surprisingly extreme extent: the algorithms ignore the isomorphism between vectors and co-vectors. Where much of machine learning is about using structure to increase learning efficiency, here the approach is to remove structure to decrease cost. The final result is a nonparametric Bayesian quasi-Newton algorithm for noisy observations. Its most important feature is that it is parameters to optimize. So, although the new algo-rithm is more expensive than gradient descent by a considerable factor, its cost scales like that of gradient descent. The new algorithm can thus be applied to very high-dimensional optimization problems, including the training of neural networks (Section 3.2). Since Hennig &amp; Kiefel (2012) focussed on unifying a group of classic algorithms, their nonparametric deriva-tion started from a parametric viewpoint. Here, a nonparametric model is derived from scratch, which allows a more direct exposition. The following section constructs related priors over f , B . The model is interesting less for what it encodes than for what it does not encode. 2.1. Prior The objective function f is defined over an input N -dimensional real vectors, or just as well its dual vec-tor space R 1 R . This distinction is usually irrelevant, because these spaces are isomorphic: There is a bijection between x  X  the vector space structure. The following derivations deliberately ignore this structural knowledge, and define the objective function over a 2 N dimensional vector space R 1 accidentally using the isomorphism between vectors and co-vectors, I introduce the new notation x and x over f with mean function  X  f variance functions (kernels) k k  X  x up X  and  X  x down X ) denotes two arbitrary separate N -dimensional inputs. It is a variant of the more com-monly used notation x  X  and x here to prevent confusion over the use of sub-and superscripts. The product form of the covariance as-sumes independent behaviour of the function in the two N -dimensional sub-spaces. It is thus strictly more conservative than a classic N -dimensional prior (which implicitly assumes that x  X  and x  X  are the same thing, so the function values must be perfectly correlated between those two sub-spaces). For a concrete imple-mentation, consider the popular squared exponential (SE) kernels, which contain a further independence assumption about the function X  X  form in each of the input dimensions: A weak encoding of the relation between x  X  and x  X  can be achieved by setting  X   X  Since we have artificially separated vectors and co-vectors, there are now actually two gradients: and to the elements of x  X  and x  X  , respectively. Of course, these are identical up to transposition; but the model does not encode this knowledge. The following deriva-tion is symmetric under transposition, so it suffices to consider only one case. The Gaussian family is closed the prior over mussen &amp; Williams, 2006,  X  9.4), with mean function (  X  ab is Kronecker X  X  symbol, the second line uses the explicit SE kernel of Eq. (6)) cov = Unfortunately, due to the second term in the sum, eval-uating this covariance has cost term again deliberately ignores co-variance structure. The term is strictly positive definite X  X t is the product of a linear kernel with a SE kernel and a number of cause kernels form a semi-ring (Rasmussen &amp; Williams, 2006,  X  4.2.4). Since its value at x tributes no marginal variance, only covariance. Thus, the resulting Gaussian process prior over covariance function k (with  X  tions than Eq. (8) , in the sense that it assumes strictly lower covariance between function values at differing points. The second equality introduces the Kronecker product between matrices, will appear repeatedly from here on. It is particularly useful when dealing with operator-valued functions: If we stack the elements of matrix B ij , row by row, into the vector sented as on f is symmetric under transposition, the GP prior on transposition.
 The elements of the Hessian B R tives of the elements of of x  X  . But they are also  X  x derivatives of Under the factorisation structure of the prior, these two facts are now separate, independent , statements. Using the first definition, and the same argument as Gaussian process, with mean B 0 and covariance cov Just as above we drop the quadratically expensive mixing terms, to get the strictly weaker kernel An important observation is that the second definition of B (that is, reversing the order of differentiation), yields the same kernel k B for the Hessian. This is not surprising X  X t is the reason why the Hessian is symmetric X  X ut will become relevant in Section 2.2. Together, the derivations so far provide independent Gaussian process priors for f , the 2 N gradient ele-ments, and the N 2 elements of the Hessian, each as functions over the 2 N -dimensional input space. Drop-ping correlation between pairwise different dimensions also weakens the relation between the belief over the f and its gradients. But the derivation above does retain a relation between the length scales  X  and the signal variances of gradient and Hessian: If f has large length scales, it changes slowly, so we expect smaller values for gradient and Hessian.
 If we had not split x  X  and x have encoded the fact that the Hessian is symmetric, and that gradient and Hessian are conservative fields (i.e. that line integrals over them are independent of the integration path). These aspects are now erased from the model, so the posterior will converge more conservatively than it otherwise would. In return for this weakened inference power, it will turn out that the resulting inference algorithm is only linearly expensive. 2.2. Inference Estimating the Newton descent direction quires beliefs for both gradient and Hessian, from ob-servations of only the gradients, constructed these in this section. Figure 1 gives intuition.
 For the purposes of this paper, we will assume inde-pendent Gaussian noise of standard deviation  X  on all elements of the gradient. In applications like the training of neural networks, the strength of the noise may well depend on the function value. In such cases simple heuristics may help improve performance. 2.2.1. Inference on the gradient Observing along the entire subspace spanned by x  X  . We thus observe noisy functions (not just single function values) y x  X  . The likelihood is thus p N ( confusing, but it is simply the correct encoding of an observation containing no information in the x  X  space. The joint conditional probability of the M y m , observed at M the N p ( Y Inference is analytic: The posterior over the gradient is a Gaussian process with mean function = Assuming independence between elements ( X  is diago-nal), this expression simplifies into separate expressions for each dimension n n -th row of Y , we find Evaluating this mean function once for all dimensions has cost is likely to be a good assumption for many applications, including neural networks, the cost is The posterior covariance will not feature in the algo-rithm; but it is also analytic, and independent over the N dimensions:  X  2.2.2. Inference on the Hessian The likelihood of the Hessian is somewhat more in-volved, but still allows analytic inference. The gradient is the integral over the Hessian, and integration is a linear operation. Recall that there are two equivalent, but in our model independent, ways of constructing the Hessian. To incorporate both, as in Hennig &amp; Kiefel (2012), and as in classic quasi-Newton methods, we con-struct a rank-2 M update by considering two indepen-i = along a linear path. We will combine them into an N  X  the noise on each y i is presumed i.i.d. and of variance  X  , u  X  is the difference of two Gaussian variables, thus has noise variance 2  X  2 . We will also use the matrix S with elements S im probability of U  X  can then be written as p This uses the linear operator S  X   X  is the Hadamard, or point-wise product). It returns the integral of B along the x  X  sub-space ( The posterior after this first observation is also a Gaus-sian process. Constructing it is tedious but, given the insights developed in the previous sections, it is now a relatively uncomplicated generalisation of the results of Hennig &amp; Kiefel (2012), where some details can be found. The posterior mean B  X  and covariance function  X   X  over B are with the maps B  X  R B k Assuming the prior mean function B 0 is a constant diagonal matrix, evaluating these two expressions is O ( ate Gaussian integrals. There are no analytic expres-sions for these integrals; but good (single precision), lightweight numerical approximations exist (Genz, 2004; Hennig &amp; Kiefel, 2012). The second, independent obser-vation in the dual subspace is U  X  , the corresponding likelihood is, using S  X  Once again, the presence of the kernel form in this like-lihood is simply encoding ignorance in this observation along the orthogonal subspace. By extension of the derivation for Equations (21) &amp; (22) (see also Hennig &amp; Kiefel, 2012), the posterior after both independent observations is a Gaussian process with mean function B  X  and (here unused) covariance  X   X  , with objects B  X  , k  X  , K tions (23) to (25) . In fact, given our choice of kernel and a symmetric mean function, these objects are iden-tical along the sub-space x  X  vant evaluations take place. We will thus simplify to k a symmetric matrix. This is the effect of our deliberate ignorance about the relationship between derivatives, which eliminates information about the commutativity of derivatives. It is easy, albeit somewhat ad hoc, to project the mean estimate into the space of symmetric matrices, using the projection operator  X  with the transposition operator Tx On a superficial level, the final result is a relatively simple extension of that of Hennig &amp; Kiefel. But the derivations up to here considerably clarify the infor-mation content of prior and the two likelihood terms in the inference for the Hessian. In particular, it is now clear the inference scheme does not double count observations; in fact it under -counts, by considering a doubly large input space R N 2.3. Estimating the inverse Hessian We now have a joint Gaussian belief over the elements of the Hessian. Our belief over the inverse of this matrix, which we require for the Newton-Raphson direction, is not Gaussian. But a consistent point estimator for this direction can be constructed efficiently as  X  estimate B  X  , using the matrix inversion lemma. After symmetrisation, the mean estimate can be written as
V
W Our estimate of the Newton-Raphson direction can be thus evaluated in of previous evaluations M rises over time. But the algorithm moves along a trajectory, so old observations typically become irrelevant as the algorithm moves away from them. Analogously to the L-BFGS method (Nocedal, 1980), we simply impose a memory limit M max and ignore all observations more than M max steps away (see also Hennig &amp; Kiefel, 2012). 2.4. Relation to classic quasi-Newton methods For the noise-free limit of  X  ceptual connection to classic quasi-Newton methods, as shown in Hennig &amp; Kiefel (2012): If we perform the updates in individual steps, re-setting uncertainty to the prior in every step, and set the length scales  X  _ for the Hessian (assuming the function is a quadratic), the signal variance to unit,  X   X  _ method, Equation (27) to Powell X  X  (1970) symmetric Broyden method. If we further assume the function to be convex and chose each kernel k  X  ,k  X  to be equal to B itself (a concept that does not easily generalise to non-constant Hessians), then we arrive at the DFP up-date formula (Davidon, 1959; Fletcher &amp; Powell, 1963). From there, under the bijection Y obtain the widely used BFGS formula (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970). All these methods share regularization terms which can be interpreted analogously to the derivations in this pa-per, as assuming independence between the objective X  X  dependence on row and column-vectors. So, while the idea of treating separating vectors and co-vectors as separate objects as introduced in this paper may appear far-fetched at first sight, it is in fact an old concept at the heart of some of the most widely used optimization algorithms X  X t is just not obvious in their form. Cer-tain classic algorithms are parametric, noise-free limit cases of the method described here. 2.5. Open issues and limitations The previous sections provide a principled, linear cost solution to the issue of inferring the Newton-Raphson direction under noise. There are other problems of stochastic optimization not addressed here, most im-portantly the choice of step size. Even in the noise free case, a fixed step size of 1 is not optimal. Quasi-Newton methods use a line search for this purpose, but this paradigm is nontrivial to use under noise, where bisection algorithms are not applicable. It is also clear that with rising noise the algorithm learns ever more slowly and eventually has to become more conservative in its behaviour. The experiments in Section 3 ignore this old question and leave the step size at 1 through-out. A more fundamental question is in which regime in the limit of infinite noise a quasi-Newton method cannot learn anything and will simply follow the mean of the gradient. At the other end, for zero noise, it is well known that quasi-Newton methods substantially outperform simple gradient descent. The experiments shed light on this issue. The unsurprising bottom line: the right choice depends both on the noise level and the computational cost of the objective function. Very expensive functions justify the prize of comparably ex-pensive methods like Hessian-free optimization, very cheap functions are optimized most efficiently by gradi-ent descent. The method proposed in this paper covers the, perhaps most interesting, intermediate ground. In the following two experiments, a low-dimensional toy problem provides intuition and evaluates numeri-cal precision in the convergence limit, a realistic high-dimensional problem investigates cost. 3.1. Numerical Precision: Minimizing a Consider a two-dimensional optimization problem in form of a  X  X aussian bowl X : f m unit bivariate Gaussian distribution and inverse scale P  X  3 degrees of freedom. Because this function has non-trivial gradient and Hessian (proportional to the first and second Hermitian polynomials, respectively), it is nontrivial for Newton-Raphson.

SS x  X  x
SS Figure 2 shows the performance of a number of opti-mization algorithms on the  X  X aussian bowl X  problem: gradient descent, exact Newton-Raphson, Hessian-free optimization (Martens, 2010), which is essentially an elegant numerical implementation of exact Newton-Raphson, and the nonparametric probabilistic quasi-Newton algorithm. Gradient descent used the learning rate rule  X  t in a rough grid search. Each algorithm performed one optimization without evaluation noise, and one with noise. Noise was independent Gaussian on each element of the gradient, with standard deviation 0.01, which corresponds to a relatively high signal to noise ratio of  X  100. Noise vectors were sampled once per evaluation and shared by all algorithms. As in the mini-batch setting, the noise was kept constant throughout each call to Hessian-free optimization to allow a consistent inversion using conjugate gradients. Algorithms which calculate the Hessian directly (Newton-Raphson and Hessian-free optimization) were given access to the exact Hessian even where noise was added to the gra-dient. This creates an advantage for these methods, which they were not able to use: The log-log plot shows performance over the entire dynamic range of the opti-mization, from initialisation down to single precision. The noise-free quasi-Newton method, despite having no access to the Hessian, performs almost as well as the better informed Newton methods (the rough shape towards the end of optimization is due to numerical instabilities). More strikingly, it is the only method in this comparison that is robust to noise: All three other methods saturate roughly at the noise level, while the probabilistic method descends about 10 orders of magnitude below it.
 It is not surprising that the other second-order methods can not deal well with noise, they were not designed for this purpose. And of course this experiment, with its low-dimensional setting, is not particularly close qualitative difference between stochastic gradient de-scent X  X  diffusive kind of convergence, and the kind of convergence afforded by actively modeling noise. 3.2. Numerical Cost: Training a deep belief weights of a neural network. Our implementation is based on that by Hinton and Salakhutdinov 1 (Hinton &amp; Salakhutdinov, 2006), which trains a 3-layer feed-forward network on the MNIST dataset 2 . Pre-training by contrastive divergence was run without change as in the original code. To create a reasonably manageable optimization problem that could be run numerous times using various optimization algorithms, only the top (classification) layer weights were then optimized, which creates a N It is important to point out that this limitation was introduced only for experimental convenience  X  the algorithm described in this paper does in fact scale to much higher values of N (we have anecdotal experience with optimization problems in the range N The MNIST training set consists of 60 , 000 images. Figure 3.2, top, shows that the noise created by even splitting this dataset into two  X  X ini X -batches is already considerable. Adjusting the size of the mini-batches thus allows varying both computation cost for, and evaluation noise on the objective function over wide ranges.
 Figure 3.2, middle and right, shows optimization per-formance as a function of number of evaluations and computation time, respectively. The right plot should be taken with a grain of salt, since computation time changes with the complexity of the objective function, and with implementation of the optimizer. In this par-ticular case the objective function is comparably cheap, making it harder for the nonparametric algorithm to use its conceptual advantages. To get a noise-free plot, the loss function achieved by the algorithms on smaller
SNR f [irrelevant linear scale] (noisy) batches was re-evaluated on the whole, expen-sive dataset, i.e. the plotted function values were not observed in this form by the algorithms. The figure compares stochastic gradient descent to the nonpara-metric quasi-Newton method, as well as to a  X  X egular-ized gradient descent X , which simply uses the GP mean of Eq. (17) instead of the noisy gradient. The stark difference between this method and the quasi-Newton optimizer shows that the learned Hessian is nontrivial, despite the high dimensionality of the problem and the strong factorisation assumptions in the model. The plots also show results from Hessian-free optimization, implemented using matlab  X  X  efficient symmetric LQ solver (Barrett et al., 1994).
 As anticipated (Section 2.5), these results reveal a non-trivial situation: The probabilistic method dominates stochastic gradient descent in terms of optimization performance per step, but its slightly higher cost is relevant when the objective function is very cheap. Ob-viously, in the limits of no / high evaluation cost for f , the cheapest / most elaborate optimizer always wins, between, where the additional efficiency of the prob-abilistic optimizer outweighs its slightly higher cost, while remaining cheaper than the (technically nonlin-ear) Hessian-free method. Of course, this region can be extended further by a more efficient implementa-tion than our simple matlab version (note that the Hessian-free method uses c code). Optimization methods for large-scale problems must be cheap, robust, and efficient. This paper presents a nonparametric robust extension of classic quasi-Newton methods which appears to be the first to cost, accounts for Gaussian noise, and approximates the quadratically efficient Newton Raphson method. Achieving this required a nontrivial nonparametric model, which explicitly ignores algebraic knowledge in exchange for lower cost. A simple matlab imple-mentation will be released along with this paper, at http://probabilistic-optimization.org .
 I am thankful to Martin Kiefel for ongoing helpful discussions on all aspects of probabilistic optimization. I would further like to thank an anonymous reviewer for helpful comments on a draft of this paper. Barrett, R., Berry, M., Chan, T.F., Demmel, J., Do-nato, J., J., Dongarra, Eijkhout, V., Pozo, R.,
Romine, C., and Van der Vorst, H. Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods . SIAM, Philadelphia, PA, 1994. Bordes, A., Bottou, L., and Gallinari, P. SGD-QN:
Careful quasi-Newton stochastic gradient descent. J of Machine Learning Research , 10:1737 X 1754, 2009. Broyden, C.G. A class of methods for solving nonlinear simultaneous equations. Math. Comp. , 19(92):577 X  593, 1965.
 Broyden, C.G. A new double-rank minimization algo-rithm. Notices American Math. Soc , 16:670, 1969. Byrd, R.H., Chin, G.M., Neveitt, W., and Nocedal,
J. On the use of stochastic Hessian information in optimization methods for machine learning. SIAM J. Optimization , 21(3):977 X 995, 2011.
 Davidon, W.C. Variable metric method for minimiza-tion. Technical report, Argonne National Laborato-ries, Ill., 1959.
 Dennis, J.E. Jr and Mor  X ee, J.J. Quasi-Newton methods, motivation and theory. SIAM Review , pp. 46 X 89, 1977.
 Fletcher, R. A new approach to variable metric algo-rithms. The Computer Journal , 13(3):317, 1970. Fletcher, R. and Powell, M.J.D. A rapidly convergent descent method for minimization. The Computer Journal , 6(2):163 X 168, 1963.
 Genz, A. Numerical computation of rectangular bivari-ate and trivariate normal and t probabilities. Statis-tics and Computing , 14(3):251 X 260, 2004.
 Goldfarb, D. A family of variable metric updates de-rived by variational means. Math. Comp. , 24(109): 23 X 26, 1970.
 Hennig, P. and Kiefel, M. Quasi-Newton methods  X  a new direction. In Int. Conf. on Machine Learning (ICML) , volume 29, 2012.
 Hinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science , 313(5786):504 X 507, 2006.
 Martens, J. Deep learning via Hessian-free optimization.
In International Conference on Machine Learning , 2010.
 Martens, J. and Sutskever, I. Learning recurrent neural networks with Hessian-free optimization. In Interna-tional Conference on Machine Learning , 2011. Nocedal, J. Updating quasi-Newton matrices with lim-ited storage. Math. Comp. , 35(151):773 X 782, 1980. Pearlmutter, B.A. Fast exact multiplication by the Hessian. Neural Computation , 6(1):147 X 160, 1994. Powell, M.J.D. A new algorithm for unconstrained optimization. In Mangasarian, O. L. and Ritter, K. (eds.), Nonlinear Programming . AP, 1970.
 Rasmussen, C.E. and Williams, C.K.I. Gaussian Pro-cesses for Machine Learning . MIT Press, 2006. Robbins, H. and Monro, S. A stochastic approximation method. The Annals of Mathematical Statistics , pp. 400 X 407, 1951.
 Rumelhart, D.E., Hinton, G.E., and Williams, R.J. Learning representations by back-propagating errors. Nature , 323(6088):533 X 536, 1986.
 Schraudolph, N.N. Fast curvature matrix-vector prod-ucts for second-order gradient descent. Neural Com-putation , 14(7):1723 X 1738, 2002.
 Shanno, D.F. Conditioning of quasi-Newton methods for function minimization. Math. Comp. , 24(111):
