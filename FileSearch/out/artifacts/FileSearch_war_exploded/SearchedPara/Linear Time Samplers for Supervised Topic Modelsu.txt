 Topic models are effective probabilistic tools for processing large collections of unstructured data. With the exponential growth of modern industrial data, and consequentially also with our ambition to explore much bigger models, there is a real pressing need to significantly scale up topic model-ing algorithms, which has been taken up in lots of previous works, culminating in the recent fast Markov chain Monte Carlo sampling algorithms in [10, 22] for the unsupervised latent Dirichlet allocation (LDA) formulations.

In this work we extend the recent sampling advances for unsupervised LDA models to supervised tasks. We focus on the Gibbs MedLDA model [26] that is able to simultane-ously discover latent structures and make accurate predic-tions. By combining a set of sampling techniques we are able to reduce the O ( K 3 + DK 2 + D  X  NK ) complexity in [26] to O ( DK + D  X  N ) when there are K topics and D docu-ments with average length  X  N . To our best knowledge, this is the first linear time sampling algorithm for supervised topic models. Our algorithm requires minimal modifications to incorporate most loss functions in a variety of supervised tasks, and we observe in our experiments an order of mag-nitude speedup over the current state-of-the-art implemen-tation, while achieving similar prediction performances.
The open-source C++ implementation of the proposed algorithm is available at https://github.com/xunzheng/ light_medlda .
 G.3 [ Probability and Statistics ]: Statistical Computing Algorithms, Experimentation, Performance Inference; MCMC; Topic Models; Large Margin Classifica-tion; Regression; Scale Mixtures c  X 
Bayesian methods have been extremely influential in the past twenty years, thanks to modern Markov chain Monte Carlo sampling advances that free Bayesians from making strong conjugacy assumptions and render posterior distri-butions amenable to efficient analysis. One prominent ex-ample is the topic models, such as the latent Dirichlet anal-ysis [4, LDA]. Through explicitly modeling the latent prob-abilistic relations among observed variables, topic models can effectively reduce large unstructured categorical data into semantically meaningful and interpretable low dimen-sional representations. Partly due to its ability in resolving the polysemy problem ( i.e ., the same word can have dif-ferent meanings under different context), topic models have been widely used in many practical applications, for instance genetics [17], image analysis [11], text mining [4, 8], collabo-rative filtering [5], prediction tasks [3, 23], and many more.
One significant challenge we face when applying topic models on real industrial data is the scalability issue. On one hand, technology innovations have made it possible to collect very large amount of industrial scale data at an un-precedented rate. On the other hand, the need to capture more subtle information hidden in the data requires bigger, more sophisticated mathematical models, leading eventually to more unknown parameters. Consequently, there has been a lot of recent work aiming at scaling up various topic models to very large datasets and very big models. Sampling algo-rithms, in particular, Markov chain Monte Carlo (MCMC) methods, have played an eminent role in this direction.
Since in this work we mostly focus on the LDA model (and related), and due to space limits, we can only mention a few inspiring contributions in this regime. While the original LDA model relied on the variational inference method [4], soon [8] proposed the first efficient collapsed Gibbs sampling algorithm that scales much better. Exploiting the observa-tion that only few topics appear in a certain document and few words are assigned to a certain topic, the SparseLDA [21] further reduces the sampling cost of [8]. Another signif-icant contribution is the AliasLDA [10], which made explicit the crucial insight that model parameters only change slowly during sampling. By a masterful combination of the inde-pendent Metropolis-Hastings algorithm [9, 12] with Walker X  X  alias method [20] for sampling discrete proposals in amor-tized constant time, AliasLDA was able to enjoy even bet-ter efficiency. Lastly, built on the insight of AliasLDA, the very recent LightLDA [22] accomplished the first linear time sampling algorithm for LDA. Impressive as they are, the aforementioned works suffer one common drawback: they all work on the unsupervised LDA model, hence are not able to exploit any supervised information ( e.g . labels, tags, annotations, etc .).

The Gibbs MedLDA [24], on the other hand, is a hy-brid model that combines the unsupervised LDA represen-tation and the supervised large-margin classifier under the maximum entropy principle. Compared with other super-vised LDA formulations [ e.g . 3, 23], Gibbs MedLDA often leads to better performance, and more importantly, it can directly exploit modern MCMC techniques without posit-ing any restrictive assumption on the posterior distribution. The state-of-the-art implementation of Gibbs MedLDA, in [24, 26], costs O ( K 3 + DK 2 + D  X  NK ) when there are K topics and D documents with average length  X  N . Although still faster than existing supervised LDA alternatives, the su-perlinear dependence on K prevents Gibbs MedLDA from scaling up to very large datasets or even moderately large models ( e.g . K around a few tens to hundreds).

The main goal of this work is to extend the recent fast sampling algorithms [10, 22] from the unsupervised LDA to supervised tasks. Throughout we use the Gibbs MedLDA as a running example to demonstrate the main ideas, for a). Gibbs MedLDA has already been shown to be very ef-fective in various supervised tasks [24]; b). Gibbs MedLDA is very flexible, allowing a unified treatment of binary clas-sification, multi-task learning, multi-label classification, re-gression, etc .; c). The mathematical formulation of Gibbs MedLDA imposes a fair amount of computational challenges, thus makes up an ideal model for demonstrating the tech-niques we have developed. More precisely, we make the following contributions: 1). For the unsupervised latent representation part, we extend the factorized proposal in LightLDA [22] to regularized posterior distributions. This requires building a local proposal per document and we show that its complexity can still be amortized to O (1). 2). For the supervised classifier part, we propose to apply the Gibbs sampler to draw the large-margin classifier, completely by-passing the costly need of forming and inverting the preci-sion matrix. 3). By combing a set of sampling techniques we are able to significantly reduce the complexity of Gibbs MedLDA from O ( K 3 + DK 2 + D  X  NK ) to O ( DK + D  X  without altering the posterior distribution at all. To our best knowledge, this is the first linear time sampling algorithm for supervised LDA models. 4). Building on the classical re-sult on scale-mixtures, we present a unified treatment of all common loss functions in supervised tasks. With minimal modifications this results in the fastest sampling algorithm for a variety of losses in classification, multi-task learning, regression, etc . 5). Through extensive experiments we verify that our proposed linear time sampling algorithm converges an order of magnitude faster than the current state-of-the-art implementation while achieving similar prediction accu-racy. The improvement is expected to be even larger when the model size grows.

We first collect some background material on MCMC sam-pling in  X  2.1 for later use. Then in  X  2.2 we recall the Gibbs MedLDA model and in  X  2.3 we review some recent sampling advances for LDA that inspired this work. We present in  X  3 our main result, a linear time sampling algorithm for super-vised LDA models. Extensions and experiments are given in  X  4 and  X  5, respectively. Finally, we conclude in  X  6.
We begin by briefly reviewing some MCMC background, in particular the composition principle for transition kernels. Next, we recall the Gibbs MedLDA model and review some recent fast sampling advances for LDA.
For probability density functions p (  X  ) that are (computa-tionally) hard to sample directly, the Metropolis-Hastings (MH) algorithm [9, 12] offers a very convenient alternative. It repeats the following steps using a proposal density q (  X | X  ): Of course, MH is efficient only when it is easy to draw from the proposal density q (  X | X ) and when the acceptance prob-ability A is large. The two, as defined, are at odds with each other: the proposal that provides the largest accep-tance ratio ( i.e . A  X  1) is the density q (  X | X ) = q (  X  ) = p (  X  ), which is what we avoid to draw directly in the first place. Nevertheless, by carefully balancing the heaviness of draw-ing from the proposal and the probability of accepting the proposed sample Y , we can achieve great flexibility and ef-ficiency. In this work we choose the independent Metropolis algorithm, i.e . the proposal q ( Y | X ) = q ( Y ) does not de-pend on X . Since the target density p (  X  ) appears in ratio in the acceptance probability (1 ), we need only know it up to a (multiplicative) universal constant, which can be very convenient for Bayesian posterior analysis.

Underlying the MH algorithm is a Markov chain with a specific transition kernel ( e.g . transition probability matrix) where r ( x ) = R A ( x,y ) q ( y | x )d y and  X  x denotes the Dirac point mass at x . By simulating the Markov chain the sample X will eventually follow the (unique) stationary distribution  X  (  X  ) = p (  X  ), under mild regularity conditions. More gener-ally, under the name Markov chain Monte Carlo (MCMC), one can simulate any Markov chain (not necessarily con-structed as in the MH algorithm) as long as its stationary density  X  (  X  ) coincides (uniquely) with the target density p (  X  ). Here again we face the tradeoff between the convenience of drawing from the chain K (  X  ,  X  ) and its mixing rate of con-vergence to the target density p (  X  ). The modern success of Bayesian inference, including this work, heavily relies on carefully balancing this tradeoff.

One great flexibility of MCMC is that we can compose different transition kernels, to achieve better performance. The underlying idea is extremely simple:
Theorem 1 (Composition Principle, e.g . [19]). If both transition kernels K 1 and K 2 have p (  X  ) as stationary density, so do K 1  X  K 2 and  X K 1 +(1  X   X  ) K 2 for any  X   X  [0 , 1] . The former kernel K 1  X  K 2 corresponds to drawing cyclically from K 1 and K 2 while the latter kernel  X K 1 + (1  X   X  ) K corresponds to drawing from K 1 with probability  X  or K 2 otherwise. The point is that whenever it is convenient to construct multiple good transition kernels for our problem, we do not have to make a choice among them: we use them all through composition. This can dramatically improve the mixing property of the underlying Markov chain (without complicating the sampling procedure much). It is clear that the composition principle extends immediately to more than two kernels, more precisely three in our case.

Perhaps the most famous example of the composition prin-ciple is the Gibbs sampler [7]: In order to draw from the joint density Z = ( Z 1 ,Z 2 )  X  p (  X  ) we sample cyclically from the conditional kernel using two different functions f 1 ( x 1 ,x 2 ) = x 1 ,f x , upon which we have the familiar rule: X 2  X  K 1 ( X 1 ,X 2 ) = p ( X 2 | X 1 ) ,X 1  X  K 2 ( X 2 ,X Note that neither kernel K 1 or K 2 has the target density p (  X  ) as the unique stationary density, but after composition the uniqueness is often automatic. The Gibbs sampler also opens the possibility for data augmentation [18]: Suppose we want to sample from p ( X ), which is computationally inten-sive. By augmenting with  X  X irtual X  data W we can sample the joint density p ( X,W ) using the Gibbs sampler, provided that the conditional densities p ( X | W ) and p ( W | X ) are easy to sample from. Dropping W we get the desired sample X that follows p (  X  ) after burn-in.

Our main goal is to carefully combine the above MCMC sampling techniques: (independent) MH, composition prin-ciple, Gibbs sampler, and data augmentation, so as to sig-nificantly speed up supervised topic model training on very large datasets and very big models.
Gibbs MedLDA [24] is a hybrid generative/discriminative model that jointly learns the latent topic representations (unsupervised) and large-margin classifiers for enhanced pre-diction (supervised). To set up the model, let V = { 1 ,...,V } index the V words in our vocabulary, and D = { ( w d ,y d be the labeled training set, where w d = { w di } N d i =1 of tokens appearing in document d , i.e ., each w di  X  V . For ease of presentation, y d  X  Y = { X  1 , +1 } indicates the (bi-nary) label of document d . We will extend to the multi-class and regression setting in  X  4.

Gibbs MedLDA consists of two components: a latent Dirich-let allocation (LDA) [4] likelihood model that describes the input documents W = { w d } D d =1 , and a stochastic classi-fier that takes supervising signal y = { y d } D d =1 into account. Specifically, LDA [4] posits each document as an admixture of K topics, where each topic  X  k ,k = 1 ,...,K, represents a multinomial distribution over the V words. The generative process of the d -th document proceeds as: 1. Draw topic mixing coefficients  X  d  X  Dir (  X  ); 2. For each position i = 1 ,...,N d , in the document: where Dir (  X  ) denotes the Dirichlet distribution with the hy-perparameter  X   X  R K + controlling its shape, Mult (  X  ) is the single-trial multinomial distribution, and  X  z di denotes the topic indexed by the current topic assignment z di . In a fully Bayesian treatment, topics themselves are considered as ran-dom variables and assumed to be generated from the con-jugate prior, i.e ., for all k ,  X  k  X  Dir (  X  ). Throughout we denote  X  N = 1 D P D d =1 N d as the average number of tokens appearing in documents.

Let  X  = {  X  d } D d =1 be the set of topic proportions and Z = { z d } D d =1 be the set of topic assignments, where z d = { z represents the topic assignments in document d . Since only the tokens W are observed, LDA infers the posterior distri-bution for other unobserved latent variables: where p 0 (  X  , Z ,  X  ) = p 0 ( Z |  X  ) p 0 (  X  |  X  ) p 0 distribution and p ( W | Z ,  X  ) stands for the multinomial like-lihood described above. Clearly, the posterior distribution is the unique solution of the following variational problem: where, and in the following, the minimization is performed w.r.t. all probability densities, and KL [ p k q ] measures the Kullback-Leibler divergence between density p and q . Trivial as it is, the variational form of Bayesian inference makes it possible to add regularizations to the posterior. Using this idea, Gibbs MedLDA incorporates a stochastic classifier, represented as the random variable  X  , to the objective ( 5): minimize q L q (  X  ,  X  , Z ,  X  ) + 2  X   X R q (  X  ,  X  , Z ,  X  ) , (6) in ( 5), and R ( q ) = P D d =1 E q [(1  X  y d f (  X  , z pected hinge loss induced by the stochastic linear discrim-inant function 1 f (  X  , z d ) =  X  &gt;  X  z d built on normalized topic counts  X  z d = 1 N constant that balances the two objectives in (6 ).

The regularizer R in ( 6) couples the (unsupervised) latent representation Z with the (supervised) classifier  X  , leading to more pronounced prediction power. Importantly, since R is simply a linear functional of the posterior distribution, we can still derive a closed-form solution from (6 ): where p (  X  ) is the usual posterior in ( 4), and is the extra psudo-likelihood term induced by the regularizer R . The inference of the latent variables  X  ,  X  , Z ,  X  consists of repeatedly drawing samples from the (regularized) pos-terior density (7 ). The key insight, originated from [8] for LDA, is that the usual posterior p (  X  ,  X  , Z ,  X  | W ) can be efficiently sampled using the Gibbs sampler mentioned in  X  2.1 . For Gibbs MedLDA, the extra term  X  (  X  ) in (8 ) creates additional difficulty: neither itself or its conditional given all other variables can be easily sampled. Fortunately, as shown in [15], using data augmentation with an extra scale random variable  X  , the pseudo-likelihood can be written as the marginal of the scale-mixture of normal densities: where recall that  X  d is defined in (9 ). The conditionals of the augmented density can then be easily sampled (more details below). Overall, the state-of-the-art implementation
In contrast, the original MedLDA in [23] considered the expected linear discriminant function P D d =1 (1  X  y d E q which, unfortunately, is computationally more challenging. By Jensen X  X  inequality, it is clear that the objective of Gibbs MedLDA upper bounds that of MedLDA. in [24, 26] costs O ( K 3 + DK 2 + D  X  NK ) for an entire cycle of Gibbs sampling. We will significantly bring down this complexity to O ( DK + D  X  N ), based on a careful combination of MCMC techniques and recent fast sampling algorithms for LDA, which we briefly review next.
The original LDA formulation [4] was solved using vari-ational inference, under restrictive mean field assumptions. Later on [8] provided the first efficient sampling method, which largely boosts the interest in topic models. More pre-cisely, [8] noted that the latent variables  X  and  X  , due to conjugacy, can be analytically integrated out, leaving only the topic assignment Z . Then the (collapsed) Gibbs sam-pler can be efficiently applied, leading to the (conditional) multinomial distribution 2 : where n kd counts the number of tokens in document d that are assigned to topic k , n kw counts the number of word w assigned to topic k , and n k counts the number of total words assigned to topic k . The superscript  X  di means ex-cluding the current token from the respective counts, and  X   X  = 1 V P V w =1  X  w is the average. Directly drawing from the multinomial ( 11 ) costs O ( K ). This is costly when K is large and a lot of recent work has tried to improve it.

The SparseLDA [21] decomposes the multinomial in ( 11 ) into three parts in order to exploit the sparsity in the topic counts n kd and n kw , i.e ., only few topics appear in a certain document and only few words appear in a certain topic. A significant step is taken in AliasLDA [10] towards a constant sampling cost. It used the independent MH algorithm with a proposal consisting of two parts: the first part, essentially p ( k ) in (12 ) below, can be constructed using the alias table [20] in O ( K ) time, and the second part exploits sparsity in n kd hence has smaller complexity than O ( K ). Since the first part, the word proposal p w ( k ), is shared by all documents, it can be re-used K times, leading to the amortized O (1) complexity. Overall the complexity is dominated by the av-erage number of topics appearing in any document: smaller than O ( K ) but still bigger than O (1). Finally, the recent LightLDA [22] was able to achieve O (1) complexity, based on the composition principle mentioned in  X  2.1 . In words, it considered the following factorized proposal in MH: As in AliasLDA [10], the word proposal p w ( k ) is shared by all documents hence can be sampled using alias table in amortized O (1) time. The document proposal p d ( k ) is lo-cal to each document, but can be sampled almost for free: simply picking a random token in document d and using its topic assignment takes care of the n kd term. The constant term  X  k has little influence on the effect and efficiency of the sampling procedure. Using the composition principle ( c.f . Theorem 1), LightLDA cyclically samples from the word proposal and the document proposal, and achieves amor-tized O (1) complexity.
The counts n kd for topic-document pairs and n kw for topic-word pairs are different objects, hence we use slightly differ-ent fonts for them to reduce confusion.
 We mention that another line of work tries to scale up LDA by parallelization, see e.g . [1, 14, 22]. Conceivably our sampling algorithm for Gibbs MedLDA below can also be parallelized, and will be investigated in our future work.
As mentioned above, the state-of-the-art implementation of Gibbs MedLDA in [24] costs O ( K 3 + DK 2 + D  X  NK ) in a full cycle. The superlinear dependence on K , the num-ber of topics, prevents Gibbs MedLDA from scaling to large text corpus where a moderately large K is needed to catch the long tail behavior. Moreover, a larger K , resulting in more latent features, may also be beneficial for training the large-margin classifier. Considering the excellent discrimi-native power of Gibbs MedLDA and the recent impressive advances for LDA, it is thus very desirable to develop a fast linear time sampling algorithm for the former as well. We provide such an algorithm in this section, effectively reduc-ing the complexity to O ( DK + D  X  N ), which is clearly the best possible. As we demonstrate in the experiments (  X  5), this improvement is already significant for K around a few tens to hundreds.
For ease of reference, let us first recall the regularized posterior density in Gibbs MedLDA ( c.f .  X  2.2): q (  X  ,  X  , Z | W )  X  p 0 (  X  ) where B (  X  ) is the multivariate Beta function, n kd is the num-ber of tokens in document d assigned to topic k , n { n kd } K k =1 is the topic counts of document d , n kw is the num-ber of word w assigned to topic k , and n k  X  = { n kw is the word counts of topic k . Following [8] we have used conjugacy to analytically integrate out the topic mixing co-efficients  X  and topic distribution  X  . The augmented vari-able  X  is introduced to help sampling the conditional den-sity of  X  , the large-margin classifier. Recall from ( 9) that  X  = 1  X  y d  X  &gt;  X  z d represents the margin of the classifier on document d . Lastly, we impose the normal distribution prior p (  X  ) = Q K k =1 N (  X  k ; 0 , X   X  1 ) on the classifier  X  .
As in [24], we will use the Gibbs sampler mentioned in  X  2.1 to sample from the posterior q (  X  ). The individual sampling steps for each conditional density, with substantial improve-ments upon [24], are detailed in the next three subsections.
Recall that  X  is augmented, as  X  X irtual data X , to help sam-pling the classifier  X  below. Its conditional density, given both Z and  X  , factorizes among documents with each coor-dinate following the inverse Gaussian distribution: whose mean and shape parameters are respectively 1  X  |  X  1. Using the root splitting technique in [13] we can draw from the inverse Gaussian distribution in O (1) time. This step is the same as in [24], and costs in total O ( D ) time.
This part differs substantially from [24] and consists of the first key component towards the claimed linear time sam-pling algorithm. Writing out the conditional density again: p ( z di = k | rest)  X  ( n  X  di kd +  X  k )  X  where the following definitions are adopted throughout: Like other counts, the classifier re-weighted count m  X  di be incrementally updated in O (1) time within each docu-ment d . Directly sampling the above multinomial, as is done in [24], costs O ( K ). Instead, inspired by the recent LightLDA [22], we turn to the independent MH (  X  2.1) with the ideal factorized proposal: f ( z di = k | rest)  X  (  X  n kd +  X  k ) Note the similarity with the true conditional (15 ). However, directly drawing from the ideal proposal f (  X  ) is still costly. The key is to  X  X reeze X  the proposal (explaining our tilde no-tation) so that we can amortize computation [10]. In details, we use Walker X  X  method [20] to build an alias table for the proposal f (  X  ). This takes O ( K ) time, but subsequent draw-ing from the alias table costs only O (1). Thus if we recycle the alias table for O ( K ) times the total complexity can be amortized to O (1). The independent MH is then employed to account for the  X  X rozen X  hence obsolete proposal, leaving the stationary density unchanged. After recycling the alias table for O ( K ) times, we rebuild it using the fresh counts.
The alias method we described above suffers from one drawback though. It involves all three counts: the topic-document pair  X  n kd , the topic-word pair  X  n kw , and the classi-fier re-weighted count  X  m d . Thus during sampling whenever we switch to a different document or word, using the obsolete proposal in ( 18 ) will result in low acceptance. To address this issue, we follow the approach in LightLDA [22] to split the proposal into three parts: the doc-proposal p d ( k ), the word-proposal p w ( k ), and the exp-proposal p e ( k ). We build an (independent) MH Markov chain for each proposal, and use the composition principle (Theorem 1) to combine them.
Doc-proposal : The doc-proposal p d ( k ) can be sampled in O (1) time as follows. We further split it into two parts, the  X  n kd term and the constant  X  k term. Since the sum  X  n P k  X  n kd can be incrementally maintained in O (1) time, we first flip a coin (with bias  X  n d / P k  X  k ) to decide which part to sample from. For moderately large  X  , most time we will be sampling the  X  n kd part, which is extremely simple: given the topic assignments z d , we only need to draw a random token in document d and use its topic assignment. For the constant term  X  k , we use the alias method [20] that builds the alias table in O ( K ) time but repeated re-cycling of the table amortizes the complexity down to O (1). Note that this alias table can even be shared between documents. The acceptance probability required in the independent MH ( c.f . (1)), say transitioning from state s to state t , is given by A = min which is easily evaluated in O (1) time after incrementally bookkeeping the counts and the classifier re-weighted count m d ( c.f . (16)). Thus the overall sampling time for the doc-proposal is amortized to O (1) per token.

Word-proposal : The word proposal p w ( k ) is handled using the alias method [20]. We construct its alias table in O ( K ) time but can re-use the table for drawing K samples in O (1) time each. Note that the word-proposal is shared among all documents, thus even in the very unlucky case where a certain word only appears say once in document d , its alias table can still be re-used in other documents. There-fore the O ( K ) time spent on building the table is amortized again to O (1) per token. The acceptance probability A w = min is evaluated in O (1) time similarly as that of the doc-proposal.
Exp-proposal : We use again the alias method for the exp-proposal p e ( k ). The key observations here are: 1). The classifier re-weighted count m  X  di d can be easily evaluated in O (1) time after bookkeeping m d ; 2). The alias table of the exp-proposal, while local to each document, can be re-used for other tokens in the same document, therefore the O ( K ) time spent in building the table is amortized to O (1). The acceptance probability A = min again is easily evaluated in O (1) time. Note that the ex-ponential terms above do not cancel out because the tilde terms rely on the slightly obsolete count m  X  di d .
Proposal composition : After having the three propos-als described above constructed, we use the composition principle ( c.f . Theorem 1) to combine them. In the ex-periments, we will compare the cyclic combination and the mixture combination (with equal odds). For each token, we can even iterate the composed transition kernel for a small number of times (say 3). The overall time is O ( DK + D  X  where the first factor comes from building the alias table in each document and the second factor is simply the number of tokens we must process in each full cycle. We remind that although each proposal only takes care of a part of the full conditional, its acceptance probability in MH restores stationarity, that is, we never alter the stationary density. Thus after burn-in we are still sampling the true (regular-ized) posterior. This well illustrates the flexibility of MCMC and is the key to achieve linear time sampling here.
Lastly we show how to sample the classifier weight  X  again in linear time. Since we assume isotropic Gaussian prior p (  X  ) = N  X  ; 0 , X   X  1 I , the conditional density of  X  , given all other latent variables, is again Gaussian: where the posterior mean  X   X  =  X   X  1 Zu and the precision ma-trix  X  =  X  I +  X  2 P d  X  d  X  z d  X  z &gt; d , where u  X  R entry u d =  X y d (1 +  X  X  d ). Note that forming the precision matrix  X  costs O ( K 2 D ); inverting it to get the covariance matrix costs O ( K 3 ); and sampling the Gaussian with the co-variance matrix costs O ( K 3 ). This is the approach used in [24], which is fine at the time since sampling the topic assign-ment in [24] costs already O ( D  X  NK ), dominating the overall cost. Since we have successfully reduced the latter complex-ity to O ( DK + D  X  N ) in  X  3.3, the brute-force O ( DK cost for sampling the classifier can no longer be neglected. In fact, we verified in our experiments that this step starts to dominate the training time even for moderately large K . Therefore, we need a faster sampling algorithm for the clas-sifier part.

The idea is to use the Gibbs sampler. Indeed, we have the following (univariate) conditional normal density: where the (unnormalized) mean and the precision  X  k =  X  +  X  2 P d  X  d  X  z 2 dk . The key observation here is that both the mean vector  X  and the precision vector  X  can be computed in O ( KD ) time by proper bookkeeping. Note also that we completely bypass the need of forming the precision matrix  X  . After having the mean and precision, drawing each univariate  X  k costs O (1) time. Therefore, a full iteration of all K weights costs O ( KD ). We point out that there is no need to iterate the Gibbs sampler here many times: even a single iteration would still preserve the sta-tionary density. This very flexibility of MCMC is the key to obtain our linear time sampling algorithm, without altering the target posterior density at all.
We now have all ingredients for our linear time sampling algorithm for the Gibbs MedLDA model defined in  X  2.2: We cycle through the three components presented in the above subsections: sampling augmented variable  X  in  X  3.2, sampling topic assignment variable Z in  X  3.3, and sampling classifier variable  X  in  X  3.4. We repeat the procedure M times for burn-in, after which new samples Z and  X  can be regarded as true samples from the regularized posterior. The augmented variable  X  is simply discarded. As promised, the overall time is O ( DK + D  X  N ), a significant improvement over the current state-of-the-art [24]. We point out that our improvement on sampling complexity is obtained by po-tentially slowing down the mixing rate of the underlying Markov chain X  X he point, nevertheless, is that through a more delicate balance between the two costs we can achieve greater efficiency.

Testing: For inference on the test data, we follow the same procedure as in [24]. First, we infer the topic distri-bution using the point estimate:  X   X  kw  X  n kw +  X  w . Then, given a test document w , we infer its latent topic assign-ment z by drawing samples from the conditional density: cedure is repeated until some convergence criteria is met ( e.g . the relative change of the data likelihood falls below some threshold). Finally we apply the classifier  X  (sampled during training period) on the averaged topic assignment  X  z a few samples of  X  and use their average to predict. This usually leads to more robust performance.
In this section we discuss how to extend the basic binary classification setting in  X  3 to multi-class and regression tasks.
Our algorithm easily extends to the multi-class setting where the label space Y = { 1 ,...,C } . There are at least three ways. The first one is extremely simple: we use the one-vs-all (or the one-vs-one) strategy and train a separate classifier for each class while treating all other classes as neg-ative. This results in C separate classifiers and for prediction we follow the maximally confident one: The nice part of this approach is that the C classifiers can be trained in parallel.

The second strategy is to recast the multi-class problem as an instance of multi-task learning. More specifically, we train C classifiers  X  c ,c = 1 ,...,C on the shared latent topic assignment Z . Define the regularizer for each class c : where y c d is the binary label indicating whether the d -th document belongs to the c -th class. Note that the same topic assignment  X  z d is shared among all classes. As before we can derive the regularized posterior in closed-form: q (  X  ,  X  , Z | W )  X  p 0 (  X  ) where we have again integrated out the topic mixing coeffi-cient  X  and topic distribution  X  , and introduced the aug-ment variable  X  c for each class to ease sampling its classifier  X  . Using the product prior p 0 (  X  ) = Q C c =1 Q K k =1 N (  X  we can derive the fast sampling algorithm similarly as the bi-nary setting. We omit the straightforward details but men-tion one important implementation trick: When drawing the topic assignment Z we need to evaluate the acceptance probabilities which involves the following:
X Naively evaluating the above costs O ( D  X  NL ) time in to-tal while through careful bookkeeping we can reduce it to O ( DLK + DK 2 ) time, which can be advantageous for long documents. The overall sampling time is O ( DLK + DK 2 + D  X 
N ), significantly faster than the state-of-the-art: O ( LK DLK 2 + D  X  NK ) in [24]. For prediction on the test set, we use the following rule: Comparing with the one-vs-all prediction rule (25 ), the la-tent topic assignments  X  z are shared here among all classes.
The third strategy is to use a genuine multi-class formula-tion, e.g . [6]. The resulting sampling algorithm differs sub-stantially hence we do not discuss it here.
In the above we have mainly focused on the hinge loss ` (  X  ) := (1  X  y d  X  &gt;  X  z d ) + , but our linear time sampling algo-rithm can be easily extended to other losses, thanks to the classical result on scale-mixtures of the normal density: Theorem 2 (scale-mixture of normal density, [2]).
 Let the loss function ` : R +  X  R + be continuous. Then for all  X  &gt; 0 we have the scale-mixture representation of exp(  X   X ` ( t )) w.r.t. some density function f  X  : R + i.e . exp(  X   X ` ( t )) = if and only if the function ` ( on R ++ with derivatives satisfying (  X  d d t ) n ` ( n  X  1 ,t &gt; 0 .
 When the derivative condition in Theorem 2 holds, we can find the density function f  X  (  X  ) explicitly by computing the inverse Laplace transform of the function exp(  X   X ` ( that any scale-mixture must be a symmetric function ( c.f . right-hand side of ( 29)), although extension to skewed scale-mixtures that break symmetry is straightforward, i.e ., allow-ing the normal density in (29 ) to have nonzero mean that may depend on t . The hinge loss we thoroughly discussed where exp(  X   X  | t | ) is a usual scale-mixture.

The scale-mixture of normal density is extremely useful when the prior distribution is also normal. Indeed, the reg-ularized posterior distribution q (  X  ) derived from (6 ) have the following more general form under any loss ` that admits the scale-mixture representation in (29 ): q (  X  ,  X  , Z | W )  X  where  X  d is a bi-affine function of the classifier weight  X  and the normalized latent topic assignment  X  z d , e.g .,  X  1  X  y d  X  &gt;  X  z d for binary y d or  X  d = y d  X   X  &gt; If we impose the normal prior p 0 (  X  ) = Q K k =1 N (  X  k then  X  in the posterior (31 ), when conditioned on all other variables, follows again the normal distribution hence can be efficiently sampled.

The fast sampling algorithm we developed so far extends immediately to the more general regularized posterior (31):  X  Sampling the augmented variable  X  reduces to repeatedly sample the univariate density exp(  X  1 2  X  d  X  2 d )  X  efficiency of this step of course depends on the density  X 
We have extensively discussed the hinge loss that leads to the inverse Gaussian density. We mention two more examples. For the -insensitive loss ` (  X  d ) = ( |  X  used in regression [24], we are sampling again the inverse
Gaussian, which can be done in O (1) time [13]. For the logistic loss ` (  X  d ) =  X  y d  X  d + log(1 + exp(  X  d )), we need to sample the P  X olya-Gamma distribution, which again can be done efficiently [16].  X  Sampling the topic assignment Z is almost the same as in  X  3.3, except some minor change on the acceptance proba-bility and the exp-proposal: we need to compute  X   X  d  X  2  X  d  X  k  X   X  d m  X  di d  X  k , where the coefficients  X  d , X  on the exact form of  X  d . Importantly, the classifier re-weighted count m  X  di d can be incrementally maintained in
O (1) time as before.  X  Sampling the classifier  X  needs to be done using again the Gibbs sampler as in  X  3.4. As mentioned above, the conditional posterior of  X  for any scale-mixture loss ` is again normal, but its precision matrix is computationally expensive to form. Instead, we use the Gibbs sampler and through careful bookkeeping we can draw the K classifiers in all documents in total time O ( KD ).
 Cycling through the above three steps for M burn-in steps we get samples from the true (regularized) posterior. The overall time remains O ( DK + D  X  N ).
We now present empirical results to verify the efficiency of the proposed linear time sampling algorithm, referred as LightMedLDA. Its C++ implementation is available at https://github.com/xunzheng/light_medlda . We will fo-cus on comparing against the current state-of-the-art im-plementation in [24], referred as GibbsMedLDA. As shown previously in [24], GibbsMedLDA outperforms most existing supervised topic models, and we refer the interested readers to [24] for detailed comparisons.

Datasets: We conduct experiments on three benchmark datasets 3 : the 20Newsgroups for binary and multi-task clas-sification, a hotel review dataset for regression, and a Wikipedia dataset with 1.1 million documents for multi-label classifi-cation. See Table 1 for their summary statistics.

Setup: For all experiments, if not mentioned explicitly, the tuning parameters are set as follows: the regulariza-tion constant  X  = 102 . 4; the number of inner MH steps for sampling the topic assignment (  X  3.3 ) S mh = 6 ( i.e ., ap-plying each proposal twice); the number of Gibbs sampling sub-iterations for sampling the classifier (  X  3.4 ) S We use symmetric Dirichlet priors with hyper-parameter  X  k  X  6 . 4 /K, X  w  X  0 . 01. Experimental results are aver-aged over multiple runs with the standard deviation pro-vided. Except for the large Wikipedia dataset, performance is measured on a standard desktop with a 3.30 GHz CPU. Table 1: Summary statistics for the benchmark datasets. Both 20NG and Wiki have 20 classes.
Available at: http://qwone.com/~jason/20Newsgroups , http://bigml.cs.tsinghua.edu.cn/~ningchen/data.htm , http://lshtc.iit.demokritos.gr/ , respectively. Figure 1: Classification accuracy (left) and training time (right) of LightMedLDA and GibbsMedLDA on the binary 20Newsgroups sub-dataset.
We pick two subgroups alt.atheism and talk.religion.misc from the 20Newsgroups dataset to form a binary classifica-tion task. This sub-dataset consists of 856 documents for training and 569 documents for testing. Similar to the set-tings in [24], we set  X  = 262 . 4 and the number of burn-in steps to M = 10. As shown in Figure 1 (left), when we vary the number of latent topics K from 10 to 100, both LightMedLDA and GibbsMedLDA achieved consistent pre-diction accuracies around 80% on the test set, with slightly better performance from LightMedLDA. This is expected since both algorithms are solving the same problem (whereas the slight difference may be caused by different convergence speed). Shown on the right panel of Figure 1 are the train-ing times of LightMedLDA and GibbsMedLDA. We observe that even on this small sub-dataset, the training time of GibbsMedLDA increased sharply w.r.t. the number of latent topics ( x -axis). This confirms the superlinear dependence of GibbsMedLDA X  X  complexity on the model size. In contrast, LightMedLDA converged much faster, and kept the training time under 1s even for 100 topics (the largest we tried on this small dataset).

We then tried classifying all 20 classes on the full 20News-groups dataset. We used the one-vs-all strategy mentioned in  X  4.1 to train 20 separate binary classifiers and used the prediction rule in ( 25 ). The results are shown in Figure 2, along with the multi-task results described in the next sub-section. On the left we see that again LightMedLDA and GibbsMedLDA achieved similar accuracies, with slightly bet-ter performance for LightMedLDA (due possibly to its faster convergence). We observe that the classification accuracy starts to decrease once the number of topics exceeds 150. This can be explained by: First, for larger K both algo-rithms need more iterations to converge while we capped the number of iterations to M = 25 and M = 20 for LightMedLDA and GibbsMedLDA, respectively; Second, the algorithms may start to overfit when K is large. On the right of Figure 2 we observe similar behavior of the training time when we vary the number of topics: GibbsMedLDA increases sharply due to its superlinear dependence on K while LightMedLDA is only slightly affected even when K = 400 (the largest we tried on the full dataset).
In this subsection we test the multi-task formulation de-scribed in  X  4.1 , again on the full 20Newsgroups dataset. We train all 20 classifiers simultaneously on the same latent Figure 2: Classification accuracy and training time of LightMedLDA, GibbsMedLDA, multi-task LightMedLDA and multi-task GibbsMedLDA on the full 20Newsgroups dataset. One-vs-all strategy is used for binary classifiers. Figure 3: F1-measure and training time of LightMedLDA mt and GibbsMedLDA mt on the multi-labeled Wikipedia dataset. representation, and we use the prediction rule in (28 ) to combine the classifiers. The results are shown in Figure 2, along with the previous one-vs-all results for comparison. The conclusions are similar to the one-vs-all setting: Both LightMedLDA and GibbsMedLDA achieved similar accura-cies on the test set, but LightMedLDA is much faster in terms of training time, in particular when the number of topics is large. From the right panel it is also clear that the multi-task formulation took significantly less time than the one-vs-all strategy. This is not surprising as the one-vs-all essentially repeats the computation 20 times (one for each separate classifier). Note that we did not explore paralleliza-tion in this work.

We further performed a multi-labeled prediction task on the massive Wikipedia dataset that has 1.1 million doc-uments. Due to the multi-label nature we used the F1-measure (the harmonic mean of the precision and recall) to evaluate the performance. We only considered the multi-task formulation for this dataset as the one-vs-all strategy would take too long. We set the number of inner Gibbs steps S gibbs = 4 (for drawing classifiers, see  X  3.4 ) and the number of burn-in steps M = 80 for LightMedLDA and M = 40 for GibbsMedLDA. The larger number of burn-in steps is caused by the large size of this dataset. The results are shown in Figure 3, from which we conclude that both al-gorithms consistently achieved the F1-measure around 0.55 while LightMedLDA converges substantially faster. With 200 topics, GibbsMedLDA already took 33 hours on this large dataset while LightMedLDA, with 400 topics, took only 11 hours. GibbsMedLDA with 400 topics was too slow to converge. Figure 4: Classification accuracy and training time of Light-sLDA mt and Gibbs-sLDA mt on the full 20Newsgroups dataset. Figure 5: Predictive-R 2 and training time of LightMedLDA r and GibbsMedLDA r on the hotel review dataset.
All previous experiments were performed under the hinge loss. In this part we explore other loss functions for classi-fication and regression. We tried the logistic loss for multi-task classification on 20Newsgroups and the -insensitive loss for regression on the hotel review dataset. See  X  4.2 for the algorithmic modifications needed for these losses. For the logistic loss, we set  X  k  X  5 . 6 /K ,  X  = 204 . 8, and M = 40 for both our method, denoted as Light-sLDA, and the com-petitor Gibbs-sLDA in [25]. We used the more efficient multi-task formulation. For the -insensitive loss used for regression, following [3, 24] we used the predictive-R the performance measure (the larger the better). We set = 1 e  X  3 , X  = 262 . 4, and the number of burn-in steps M = 15 for LightMedLDA and M = 10 for GibbsMedLDA.
 The results are shown in Figure 4 and Figure 5 respectively. Again, we observe that both algorithms achieved compara-ble performance while LightMedLDA consumed significantly less training time than GibbsMedLDA over the entire range of topic numbers.
In this last part of experiments we conduct a thorough sensitivity analysis of the proposed LightMedLDA algorithm w.r.t. the different proposal compositions, the number of MH steps, and the number of Gibbs sub-iterations. We record both the classification accuracy and the training time on the full 20Newsgroups dataset.

Proposal compositions: Recall from  X  3 that the regu-larized posterior is factorized into three parts, and based on each we constructed the doc-proposal, the word-proposal, and the exp-proposal. Note that each proposal, equipped Figure 6: Classification accuracy and training time of LightMedLDA mt on the full 20Newsgroups dataset with mixture of proposals, cycle of propos-als, word-proposal only, doc-proposal only, and exp-proposal only. Figure 7: Classification accuracy and training time of LightMedLDA mt on the full 20Newsgroups dataset with different number of MH steps. with its correct acceptance probability, e.g . (19), (20 ), and (21 ), respectively, are all bona fide MCMC algorithms and should lead to the same stationary density eventually. How-ever, their rate of convergence to the target posterior may be different. As verified in the left panel of Figure 6, using the mixture of all three proposals ( i.e . uniformly randomly choosing one of them for S mh = 6 times) leads to the best test accuracy, consistently on all topic numbers we tried. Us-ing the exp-proposal alone leads to the poorest result, mostly because the exp-proposal is derived from the classifier part and contains the least information for drawing the topic as-signment. Using the word-proposal alone is better than the exp-proposal but worse than the doc-proposal alone, possi-bly because the word-proposal is shared among documents hence may incur a longer lag while the doc-proposal refreshes more frequently (every time we switch documents). Interest-ingly, cyclically sampling the three proposals performs sub-stantially worse than the mixture of proposals, even though in each iteration they used each of the proposals twice (on average). Another observation we draw from Figure 6 is that when the number of topics is small, all combinations of proposals seem to perform equally well. Moreovoer, we ob-serve from the right panel of Figure 6 that the exp-proposal alone and the doc-proposal alone leads to substantially less training time.

Number of MH steps: Figure 7 shows the influence of the number of MH steps in drawing the topic assignments (  X  3.3 ). For space limits, we only present the result for the mixture of proposals (for its best accuracy verified above). It is clear that using more MH steps improves the accuracy but also increases the training time. In practice we found Figure 8: Classification accuracy and training time of LightMedLDA mt on the full 20News-groups dataset with different number of Gibbs sub-iterations. that using six MH steps (two for each proposal) seems to lead to the best tradeoff.

Number of Gibbs sub-iterations: Figure 8 shows the influence of the number of Gibbs sub-iterations in drawing the classifier (  X  3.4 ). Again we only present the result for the mixture of proposals. We observe that the prediction accuracy stays relatively constant while more Gibbs sub-iterations clearly increases the training time. In practice it seems 1 or 2 Gibbs sub-iterations would suffice.
Topic models such as LDA are excellent tools in process-ing large collections of unstructured data, and a lot of re-cent work has devoted to scaling them to large industrial data and big models. Building on these recent sampling advances for unsupervised LDA formulations, we have pre-sented the first linear time sampling algorithm for the su-pervised topic model, Gibbs MedLDA, that can exploit the large supervision information to achieve better predictions. Our algorithm easily extends to a variety of losses in binary classification, multi-task learning, multi-label classification and regression, and we observed in our experiments an or-der of magnitude speedup over the current state-of-the-art implementation, while obtaining comparable accuracy. For future work we plan to explore nonparametric extensions and parallel implementations.
 We thank the reviewers for their valuable comments. This work was supported by NIH Grants R01GM087694 and P30DA035778, and NSF Grant IIS1447676.

