 HAO ZHOU and SHUJIAN HUANG , Nanjing University HUADONG CHEN, XINYU DAI, CHUAN CHENG, and JIAJUN CHEN , Nanjing University Modern data-driven transition-based parsers parse a sentence by performing a se-quence of shift-reduce actions. Most of these transition-based parsers run in linear time, which is faster than traditional chart-based parsers [Eisner 1996; Collins 1997; Charniak 2000; McDonald et al. 2005]. Widely used in both dependency [Yamada and Matsumoto 2003; Nivre et al. 2007; Zhang and Nivre 2011; Goldberg and Nivre 2013] and constituent [Sagae and Lavie 2006; Wang et al. 2006; Zhang and Clark 2009] parsing tasks, these parsers achieve state-of-the-art parsing performance. 1
Traditional features used by shift-reduce parsers can be divided into three main categories: lexical features, POS features, and constituent label or dependency label features. More complex features can be obtained by combining these atomic features. Much work has been proposed to exploit the feature context of shift-reduce parsing. For example, high-order dependency arcs and sets of arc labels have been used for dependency parsing [Zhang and Nivre 2011]. Joint segmentation, part-of-speech (POS) tagging, and parsing [Hatori et al. 2012; Zhang et al. 2013] exploit the larger context by making using of features from the lexical analysis tasks to achieve better perfor-mance. Huang et al. [2009] propose utilizing the bilingual word alignment and ordering information to disambiguate the shift-reduce dependency parsing. Chen et al. [2012] take advantage of the dependency language model [Shen et al. 2008] to enhance the accuracy of dependency parsing and achieve significant improvements with the semisu-pervised method. These methods focus on either extracting high-order or long-distance patterns from a local part or several local parts of the tree (represented by a combina-tion of binary indicator features) or calculating lexical statistics from a large, usually unannotated, corpus. However, representing tree structure in general is difficult.
In shift-reduce constituent parsing, parser actions can be a useful representation of the general tree structure. This fact has not been fully exploited by previous work. During shift-reduce constituent parsing, parsers select a SHIFT or REDUCE action in each step. The actions used during parsing a sentence compose an action sequence. Typically, for constituent parsing, each action sequence corresponds to a unique binary-headed syntax tree (a binary tree with marked head child of each tree node), and each binary-headed syntax tree also corresponds to a unique action sequence. 2 As a result, action sequences could be regarded as linearized versions of tree structures and may be useful to parsing process disambiguation.

To exploit the distribution of parsing action sequences, we propose the action n-gram model (ANM). For a sequence, a classical modeling method is the n-gram estimation method [Chen and Goodman 1996], which has shown its simplification and effective-ness in language models and many other fields. The action n-gram model is trained on the action sequences generated from training data with the n-gram estimation method. To make use of larger context, we incorporate POS tags and lexical infor-mation to enhance the basic action sequence. We construct a log-linear interpolated model over the action sequence to represent various parsing context information dur-ing constituent parsing. With the action n-gram model, we can get the probability of each action given the former action sequence in parsing. Instead of extracting the bi-nary indicator features of the shift-reduce parsing context, the action n-gram model utilizes these action sequences generated by parsers directly to do the runtime parse disambiguation, which is a different way to exploit the context of shift-reduce parsing. The generalized LR parser also utilizes the parse action for disambiguation [Briscoe and Carroll 1993; Lavie 1996; Kentaro et al. 1998]. But the statistical generalized LR parser focuses on exploiting the state transitions, and the action n-gram model focuses on the the action sequence and syntax tree structures formed by the action sequence. And instead of in a data-driven parser framework, their work is based on a generalized LR parser.

In addition, we enhance shift-reduce parsing performance with action n-gram mod-els. Adding dense features such as action n-gram models into the parsing framework is not a trivial task. The structure perceptron [Collins and Roark 2004] works poorly when the model contains millions of binary features and only a few dense features. We propose three different methods to incorporate these action n-gram models into a state-of-the-art parsing framework. The first directly adds the scores of action n-gram models as real-valued features into the decoding process; the second adds the action n-grams into parsing as binary indicator features; the last adopts a cascaded model [Jiang et al. 2008] to incorporate action n-gram models into parsing. By experiments, we find that action n-gram models with the cascaded model method outperform the other methods in both efficiency and accuracy. The cascaded model may be a good choice to incorporate dense features into a well-tuned discriminative model.

We conduct different experiments to validate the effectiveness of action n-gram mod-els. First, we show that action n-gram model scores have similar Pearson X  X  correlation coefficients compared to the baseline discriminative model scores. This confirms that action n-gram models are able to distinguish good and bad predicated parsing tress. We also calculate the perplexities of action n-gram models on various datasets and conclude that the action n-gram model could be well trained on limited training data. We experiment with the three proposed methods to incorporate action n-gram models into parsing and compare their performance in detail. The resulting parser achieves accuracy improvements over a state-of-the-art baseline parser on three treebanks; a 0.7% absolute improvement on CTB2.0, 0.5% on CTB5.1, and 0.5% on WSJ.
 The rest of this article is organized as follows. In Section 2, we review related work. Section 3 introduces the framework of the shift-reduce constituent parser. Section 4 represents the main idea of the action n-gram model and how to obtain the action n-gram model. Section 5 introduces three strategies we adopted to incorporate the action n-gram model into parsing. Section 6 shows the advantages of the action n-gram model with experiments. Section 6.4 discusses the usage of the action n-gram model in more detail. Finally, we conclude the article and briefly outline future work in Section 7. Much work has been proposed for deeply exploiting the parsing context that achieved accuracy improvements in both constituent parsing and dependency parsing. Shen et al. [2008] proposed a dependency language model to exploit the long-distance word relations in statistic machine translation. The n-gram dependency language model pre-dicts the next child of a head based on the n-1 immediate previous children and the head itself. The dependency language model was also used by Chen et al. [2012] to enhance the accuracy of dependency parsing, achieving significant accuracy improvements. Zhu et al. [2013] used the dependency language model to capture structural relations in the shift-reduce constituent parser. Besides the dependency language model, they also employed word clustering and lexical dependencies in their system. Zhang and Nivre [2011] attempted to enhance the dependency parsing by employing a lot of high-order dependency arcs and a set of arc label features. They extended the shift-reduce depen-dency parser with many graph-based dependency parsing features.

Features from different natural language processing tasks and multilingual data are also used in parsing. For example, joint segmentation, part-of-speech (POS) tagging, and the parsing model [Hatori et al. 2012; Zhang et al. 2013] exploit a larger context by making using of features from the lexical analysis tasks to achieve better perfor-mance. Huang et al. [2009] proposed using the bilingual word alignment and ordering information as constraint features to guide the shift-reduce dependency parsing.
Reranking methods [Charniak and Johnson 2005; McClosky et al. 2006; Huang 2008] upon the generative chart-parser offer a different way to explore the effects of nonlocal features in syntax parsing. State-of-the-art reranking parsers achieved much higher accuracy than other parsers, which indicates that the nonlocal features are very helpful to syntax parsing. Briscoe and Carrol [1993] described work toward the construction of a probabilistic parsing system for natural language, based on the LR parsing technique. They proposed to associate probabilities with transitions in the automaton in a generalized LR parsing framework [Tomita 1987]. They combined parsing action with the current parsing state, lookahead item, and resultant nonterminal. The final probabilities of combined actions are derived from the set of parse histories resulting from the training phase, by counting the frequencies of combined actions and converting these to probabilities.
There are many generalized LR parsers that assign probabilities to actions [Lavie 1996; Kentaro et al. 1998]. Compared to associating the probabilities to the rules of the grammar, the methods allow the probabilistic parser to distinguish situations in which identical rules reapply in different ways across different derivations or apply with differing probabilities in different contexts.

The action n-gram model and statistical generalized LR parsers both assign prob-abilities to actions for parsing disambiguation. The statistical generalized LR parser assigns probabilities to the next actions by counting their co-occurrence frequencies with the current state, lookahead item, and resultant state. In contrast, the action n-gram model assigns probabilities to the next actions by counting the previous n actions directly. In generalized LR parsing, the action is combined with the lookahead item and resultant nonterminal in the LR parsing table. But the action of action n-gram models is based on head word, head pos-tag, and constituent label information in the parsing stack. The statistical generalized LR parser focuses on exploiting the state transitions in parsing. In contrast, the action n-gram model focuses on the action sequence and syntax tree structures formed by the action sequence.

The action n-gram model is built upon a data-driven shift-reduce parsing framework with an n-gram estimation method rather than on a generalized LR parsing framework. Compared to using the action history only, we propose to incorporate the action n-gram model into a discriminative parsing model to enhance the parsing performance. Typical shift-reduce parsers parse a sentence by performing a sequence of shift-reduce actions. The action to be performed is determined by a statistical classifier, and the parsing result is obtained by searching greedily from left to right of the sentence [Sagae and Lavie 2005]. Zhang and Clark [2009] applied global discriminative training and beam-search to obtain higher accuracies. Zhu et al. [2013] added a new action to fill the gap among action sequences with different lengths caused by unary reduce actions. In this section, we briefly review the shift-reduce constituent parsing framework. A shift-reduce parser parses a sentence from left to right and generates the whole parse tree by performing a sequence of actions. The parser starts with an initial state and makes transitions from one to another by performing actions. Every state consists of POS-tag pairs to be processed and S contains partially parsed subtrees. In each step, one of the following actions is applied:  X  SHIFT (S): push the first word and POS-tag pair of the queue onto the stack as the top node s 0 .
  X  UNARY-REDUCE-X (UR  X  X): pop the top node s 0 off the stack; generate a unary-branching node with constituent label X whose child is s 0 ; push the new node back onto the stack.  X  BINARY-REDUCE-L/R-X (BR-L/R  X  X): pop the top two nodes s 1 , s 0 off the stack; generate a binary-branching node with constituent label X whose left child is s 1 and right child is s 0 , with the left(L)/right(R) child as its head; push the new node back to the stack.  X  FINISH (F): pop the root node off the stack; no other actions can be performed except
IDLE.  X  IDLE (I): keep the current state; extract features from S and Q of the current state. The baseline parser adopts a global discriminative training method [Zhang and Clark 2009], which uses an agenda to find the output parse from the K-best possible can-didates. The agenda initially contains a start state, which consists of an empty stack and a queue that contains the full input sentence. At each step, each of the states is extended using all applicable actions, generating a set of new state items, which are put onto the agenda. After each step, the K-best states from the agenda are taken for the generation of new states in the next step. The same process repeats until the highest-scored state from the agenda is in the final state, consisting of a final parse tree on the status and an empty queue. The parse tree in the final state is taken as the final parse.
 The score of a state is the sum of the scores of every action leading to this state: Here, n is the number of actions, ( a i ) is the feature vector for i th action of state  X  ,and  X  is the model parameter. The model parameter  X  here is computed by the perceptron training algorithm described in Algorithm 1. Instead of updating model parameters immediately after performing an action in each step, the baseline parser updates with the early update method [Collins and Roark 2004]. We adopt the feature templates from Zhu et al. [2013], which are listed in Table I. In-dividual features are generated from these templates by first instantiating a template with particular labels, words, and tags and then parsing the instantiated template with a particular action.

Here, s i is the i th node on top of the stack S and q j is the j th node in the queue Q. s l , s 0 r represent the left and right child for binary branching s 0 , respectively, and s 0 u represents the single child for unary branching s 0 . Similarly, s 0 ll denotes the left child of the left child of s 0 . w , c ,and t represent the lexical head token, constituent label, and lexical head X  X  POS tag for a node, respectively.

Combined trigram features are used also (e.g., s 0 cs 1 cs 2 c ). However, describing the In Section 4, we propose the action n-gram model, which is able to model the structural context in a different way. In shift-reduce constituent parsing, each action sequence corresponds to a unique binary-headed syntax tree. A partial action sequence could be used to represent a subtree, which is useful to predict the following actions. We start with a basic ac-tion sequence and show how to train a naive action n-gram model on the basic action sequence. Then we extend the basic action sequence with richer information. In the end, we obtain the action n-gram model over the enhanced action sequences. Without constituent labels, an action sequence determines the structure (or branching) of a tree. For example, in Figure 1, 3 the tree structure in the box (Figure 2) can be represented by the following basic action sequence:
We define the probability of an action in a basic action sequence by following the standard practice of statistical language modeling. Given the preceding context, the conditional probability of each action can be defined as where C is the context of current action a i . And we represent the context by the pre-vious actions a 1 ,..., a i  X  1 . Following the n th order Markov assumption, the conditional probability can be defined as The probability could be estimated on action sequences transformed from treebanks. We can also obtain the probability of a whole tree structure by calculating the product of the conditional probability of each action in a complete action sequence: Basic action sequences contain only structural information of a tree. To capture richer context, we can add a constituent label, POS tag, and lexical head word to construct en-hanced action sequences. In an enhanced action sequence, each action can be expressed by where Act is the action, and Label , POS, and Head are the constituent labels, head POS, and the head word of the node produced by the action, respectively. If the action is SHIFT , the node generated by the action will be a leaf node that does not have a constituent label, in which case the POS tag of the head word will be used as a pseudo-constituent label instead. For example, the syntax tree in the box of Figure 1 could be represented by the enhanced action sequence in Figure 3.

The enhanced action sequence corresponds to a more specific parse tree compared to the basic action sequence. The action n-gram model upon enhanced action sequences could be obtained in the same way as the one used in the basic action sequence. Adding more information to the action sequence makes the representation more spe-cific, but also more sparse. As a result, the final action n-gram model is not trained on the enhanced action sequence directly. We factorize the enhanced action sequence into multiple fragment sequences and train different submodels on these fragment sequences. The final action n-gram model is an interpolation of these submodels calcu-lated on the fragment sequences.

These fragment sequences include Act , Act-Label , Act-Head, and Act-POS . For ex-ample, the Act fragment sequence consists of action labels as in the basic action sequence. It represents the basic shape of a tree. The Act-Head fragment sequence combines the action label and head word in the enhanced sequence. To sum up, we obtain four submodels on corresponding fragment sequences: ANM Act ,ANM Act  X  Label ,
For simplicity, we use a simple log-linear model. The score of an enhanced action sequence could be obtained by a combination of the four submodels: where a i is the current action, a ki is the i th item of the k th submodel, C ki is the corre-sponding context, and  X  k is the weight for the k th submodel. Our baseline parser employs millions of binary indicator features (with a value 0 or 1) for constituent parsing; the proposed action n-gram model produces real-valued scores. It is not straightforward to incorporate an action n-gram model into the parsing framework, and we adopt three different methods to incorporate our action n-gram models. The most intuitive way to utilize an action n-gram model in parsing is to add it as dense features directly. We add the scores of submodels in the action n-gram model to our baseline parser as dense features (Table II). However, all of the features in the baseline parser are binary indicator features (0 or 1), and the feature values of the action n-gram model (logarithm of the probability, often around -100) are much larger than the values of the binary indicator feature. To avoid excessive fluctuations of feature values, we used a simple rescaling method: dividing dense feature values by a constant Z (usually set as 100). An alternative method is to add action n-grams into parsing as binary indicator fea-tures. Compared to the traditional indicator features, action n-grams pack the syntax structures in a flat sequence other than a tree structure. Thus, adding action n-grams as binary indicator features to represent syntactic structures requires much fewer feature templates compared with traditional indicator features.

In this method, different feature templates were designed for different action n-grams (Table III). For example, Act-Head3gram binds the previous three actions and head words of nodes to act as a binary indicator feature. Act-Head3gram is activated when the current parsing state has three actions performed already. For simplicity, we only include 3-grams of action n-grams, other than 5-grams in training the action n-gram model. As a third method, we adopted the cascaded model [Jiang et al. 2008] to incorporate the action n-gram model. Jiang et al. [2008] used word and POS language models instead of word and POS n-grams to enhance the performance of joint Chinese word segmentation and POS tagging, proposing a cascaded linear model to incorporate word and POS language models into the joint model.

We adopt the cascaded linear model to incorporate the action n-gram model into shift-reduce constituent parsing. The cascaded model has a two-layer architecture, with a perceptron being the core and another linear model being the outside layer. Instead of incorporating all features into the perceptron directly, they first trained the perceptron with binary indicator features and then trained the outside layer using the outputs of the submodels, including the core model, word language model, and so forth.
Shown in Figure 4, baseline parsing is performed in the core layer. And in the outside layer, we treat both ANM scores and outputs of the core layer as dense features and train a linear model upon them. We use the minimum-error-rate training algorithm [Och 2003] as the outside-layer training algorithm. We experimented with both Chinese and English across three datasets. On the Penn Chinese Treebank 2.0 (CTB2.0), we used sections 001 through 270 as training data, sections 301 through 325 as development data, and sections 271 through 300 as evaluation data. On the Penn Chinese Treebank 5.1 (CTB5.1) [Xue et al. 2005], we used sections 001 through 270 and 440 through 1,151 as training data, sections 301 through 325 as development data, and sections 271 through 300 as evaluation data. On the Wall Street Journal (WSJ) corpus of the Penn Treebank [Marcus et al. 1993], we used sections 2 through 21 as labeled training data, section 24 as development data, and section 23 as evaluation data. For CTB2.0, gold POS tags were assigned to training data. For WSJ and CTB5.1, 10-fold jackknifing [Collins 2000] was used to automatically assign POS tags to training data.

The EVALB tool 4 was used to evaluate the performances of our parser. Evaluated scores include labeled precision (LP), labeled recall (LR), and bracketing F1.
We implemented a state-of-the-art parser following Zhu et al. [2013] as our baseline parser. The action n-gram models were trained on training data with a 5-gram language model and the MKN smoothing algorithm [Chen and Goodman 1996]. 6.2.1. Perplexities of Action N-Gram Models on CTB2.0. The action n-gram model packs the syntax tree into a flat action sequence, but the quantity of the labeled syntax tree (10,000) could be used to train action n-gram model is much less than the unlabeled sentences (billion) could be used to train n-gram language model. Perplexity is a mea-surement of how well a probability distribution or probability model predicts a sample. To confirm whether action n-gram models can be well trained on limited training data, we calculated the perplexities [Brown et al. 1992] of action n-gram models on CTB2.0. Action n-gram models are trained on the training data and tested on the development data.

The perplexities of action n-gram models on CTB2.0 are shown in Table IV. Al-though the training data of CTB2.0 is small (3,475) for normal n-gram estimation, all submodels of action n-gram model except ANM Act  X  Head achieved relatively small per-plexity on evaluation data. This may be because of their small vocabularies ( &lt; 200). But ANM Act  X  Head , which has a huge vocabulary, is not trained well on CTB2.0. In Section 6.3, we will show that with richer training data, the perplexity of ANM Act  X  Head will decrease significantly. 6.2.2. Pearson Coefficient with Bracketing F1. Pearson X  X  product-moment correlation co-efficient [Stigler 1989] is a measure of the linear correlation between variables. To explore the ability of action n-gram models to model syntax trees, we calculated the Pearson X  X  coefficient between the action n-gram model scores and the bracketing F1 scores of parse trees. The higher the correlation coefficient is, the more effective the action n-gram model is.

We output the 16 best parse trees on evaluation data of CTB2.0 by using our baseline parser and calculated action n-gram model scores and bracketing F1 scores on these trees. We conducted the experiments with action n-gram models trained on the com-bined layers of the multilayer action graph: ANM Act  X  Label ,ANM Act  X  Head ,andANM Act  X  POS . For comparison, we also calculated the Pearson correlation coefficient between the model scores of our baseline parser and bracketing F1 scores.
 Figure 5 shows the scatters of Pearson correlation coefficients on evaluation data. Table V shows the means and variances of these Pearson correlation coefficients. We could see that action n-gram models have comparable means and variances of corre-lation coefficient with a fine-tuned discriminative model score. This indicates that the action n-gram model has a good ability to distinguish good and bad predicated parsing tress. 6.2.3. Comparison of Different Strategies to Incorporate Action N-Gram Model. In this section, we compared the performance of three different methods of incorporating action n-gram models in parsing. We abbreviate these methods to be compared as d-ANs, d-ANMs, and c-ANMs. d-ANMs is the method to directly add action n-gram models into parsing as dense features. d-ANs is the method to add action n-grams into the parsing process as binary indicator features. c-ANMs is the method of using the cascaded linear model to incorporate action n-gram models.

The experiment was conducted on CTB2.0. 5 The results of three methods are shown in Table VI. Parsing accuracies of the three methods and the baseline parser by the iteration are plotted in Figure 6. We only reported the accuracy of the last iteration with the c-ANMs strategy because the training algorithm of c-ANMs is MERT, which converges in five iterations and is not comparable to the perceptron training algorithm. The method of c-ANMs obtained the best parsing performance, which is marked higher than others. The cascaded model proposed by Jiang and et al. [2008] is more efficient and effective than the other methods.
From Figure 6, we can see that compared to the baseline, d-ANMs converged faster and achieved higher accuracy, which indicates that action n-gram models are helpful in guiding the parsing process. However, d-ANs did not work. A possible reason may be that adding action n-gram models as numerical features has a smoothing prepro-cessing, which is more flexible than fixed action n-grams. We extended the experiments on CTB2.0 to CTB5.1 and WSJ for validating the effec-tiveness of the action n-gram model across different datasets and languages. 6.3.1. Perplexities of Action N-Gram Models on CTB5.1 and WSJ. In Section 6.2.1, we show that with limited training data, the action n-gram model is also able to be well trained
We calculated the perplexities on CTB5.1 and WSJ, as shown in Table VII. Compared to CTB2.0 (3,475), CTB5.1 (18,076) and WSJ (39,832) have a larger training corpus. We found that the poorly trained submodel of the action n-gram model (ANM Act  X  Head ) on CTB2.0 is well trained on CTB5.1 and WSJ. From CTB5.1 to CTB2.0, the size of training data varies from 18,076 to 3,475, and the perplexity of ANM Act  X  Head varies from 66.9 to 660.8. The result indicates that the action n-gram model can be well trained with 10,000 of training sentences.
 6.3.2. Results in Varying Granularities. To analyze the effects of different submodels of the action n-gram model, we also compared the results on different datasets by adding action n-gram models in varying granularities. For convenience, we only conducted the experiments with the best scoring method (c-ANMs) on CTB2.0. We first experimented with all submodels of the action n-gram model in Table II 6 and then experimented by removing one specified action n-gram model from them all. For example, the fifth column in Table VIII shows the results without the submodel ANM Act .

From the results, we can see that incorporating action n-gram models is helpful to enhance the parsing performance. We achieved 0.7%, 0.5%, and 0.5% accuracy im-provements on CTB2.0, CTB5.1, and WSJ, respectively. However, the most significant improvements achieved on different datasets are in different settings. On CTB5.1 and WSJ, the highest parsing accuracies were obtained by incorporating all the submodels of the action n-gram model into parsing process. On CTB2.0, parsing with all action n-gram models except ANM Act  X  Head achieved the best parsing accuracy. A likely reason is that the performance of adding all submodels on CTB2.0 suffers from poorly trained ANM Act  X  Head , due to lack of training examples. 6.3.3. Comparison of Parsing Performance with Previous Work. We also compared the final results with a large body of related work following Zhu [2013] in Table X. We grouped the parsers into three categories: single parsers (SI), discriminative reranking parsers (RE), and semisupervised parsers (SE).

Our baseline parser achieved the same parsing performance compared to Zhu et al. [2013]. After incorporating the action n-gram models, our extended parsers achieved parsing accuracies of 84.5 (+0.5%) on CTB5.1 and 90.9 (+0.5%) on WSJ. Although our extended parser still falls behind the reranking and semisupervised parsers, it is comparable with the best single parser. On CTB5.1, parsing with action n-gram models does not use external information and achieved the best reported performance among single parsers. On WSJ, our extended parser is comparable with the best single parser that is based on Tree Adjoining Grammar and dynamic programming decoding (  X  0.2% to Carreras [2008]). 6.3.4. Results of Single Action N-Gram Model in Parsing. To verify the effectiveness of action n-gram models directly, we also experimented on parsing with only the action n-gram model to explore what level of performance could be obtained using only the action n-gram model. For simplicity, we remove all the binary indicator features in the d-ANMs strategy 7 and retain only all the action n-gram model dense features in Table II. The final results on CTB5.1 and WSJ are shown in Table IX.

From Table IX, we can see that performances of parsing with only the action n-gram model are not as good as the former cascaded model. Particularly, on WSJ, the parsing performance is only about 50%. A possible reason may be that the current action n-gram model is trained with a 5-gram n-gram estimation method. With the action n-gram model, we can capture the context of last five shift-reduce actions at most. However, if the last five actions are all about the first node on the parsing stack, we can no longer observe the context of the second or third parsing node in the stack, which is a limitation. In other words, if the first node in the parsing stack has a deep structure, we may pay too much attention to this over the information of other nodes on the parsing stack. This will cause a problem in parsing disambiguation, especially when we adopt a heuristic search procedure. The traditional features can always be extracted from the first three nodes in the parsing stack, and therefore the parsing performance of combining traditional features and the action n-gram model is much better than using the action n-gram model only. We think that the action n-gram model could not replace the discriminative model, but it is a good supplement to the discriminative model.
However, it is interesting that using only the action n-gram model yields much better results on Chinese treebank (CTB5.1) compared with the English treebank. One possible reason is that Chinese syntax trees are more regular than English syntax trees in the treebanks. The action n-gram model is a different perspective to exploit the context of data-driven shift-reduce parsing. Except ANM Act  X  Head , the other submodels of the action n-gram model have a quite small vocabulary (around 100). As a result, higher-order n-grams could be applied to obtain a better estimation.

The idea of the action n-gram model is to introduce history-based features by model-ing the list of transitions. The idea can be used by various transition-based methods. For tasks where multiple action sequences correspond to the same output (i.e., arc-eager dependency parsing [Nivre et al. 2007; Goldberg and Nivre 2013]), the action n-gram model could still be used. But its benefits might be affected by spurious ambiguities. In this article, we proposed a novel way to capture the context of parsing. Action n-gram models are trained on enhanced action sequences with the n-gram estimation method and could be used to predict the next action based on previous ones or even calculate the probability of a syntax tree. Experiments show that incorporating action n-gram models can enhance parsing accuracies of two languages across three datasets.
In the future, we would like to try a semisupervised learning method to obtain much more training data. Expanding action n-gram models from constituent parsing to dependency parsing is a very interesting direction, because action n-gram models in dependency parsing may have different intuitive meanings.

