 Andrew Hardie Abstract Much electronic text in the languages of South Asia has been published on the Internet. However, while Unicode has emerged as the favoured encoding system of corpus and computational linguists, most South Asian language data on the web uses one of a wide range of non-standard legacy encodings. This paper describes the difficulties inherent in converting text in these encodings to Unicode. Among the various legacy encodings for South Asian scripts, the most problematic are 8-bit fonts based on graphical principles (as opposed to the logical principles of Unicode). Graphical fonts typically encode several features in ways highly incom-patible with Unicode. For instance, half-form glyphs used to construct conjunct consonants are typically separate code points in 8-bit fonts; in Unicode they are represented by the full consonant followed by virama . There are many more such cases. The solution described here is an approach to text conversion based on mapping rules . A small number of generalised rules (plus the capacity for more specialised rules) captures the behaviour of each character in a font, building up a conversion algorithm for that encoding. This system is embedded in a font-mapping program, outputting CES-compliant SGML Unicode. This program, a generalised text-conversion tool, has been employed extensively in corpus-building for South Asian languages.
 Keywords Unicode Font Devanagari South Asian languages/scripts Legacy text Encoding Conversion Virama Conjunct consonant Vowel diacritic 1 Introduction An increasingly critical issue in language processing, corpus linguistics and related fields is the prevalence of textual data resources that exist only in non-standard encodings. This problem can arise with regard to any writing system where there is significant variation in encoding systems; although this paper will focus on the scripts of South Asia, it is anticipated that the issues and solutions for other writing systems may be at least in part comparable to those of the South Asian scripts.

From 2000 to 2003, Lancaster University, in conjunction with partners in the UK and South Asia, 1 undertook (in the EMILLE 2 project) the construction of a 97 million word corpus in a range of South Asian languages. One of the main problems encountered during this project was identifying appropriate sources of machine-readable monolingual written data for the corpus. While a great deal of electronic text in a range of South Asian languages is available on the World Wide Web  X  typically on news websites  X  none of it was in an easily usable format. Unicode 3 is quickly becoming the text encoding of choice for corpus builders working on lan-guages that use more than the basic ASCII character set, and a key design goal of the EMILLE corpus was that it would use Unicode as its sole encoding. However, websites that publish data in South Asian languages typically do not use Unicode. 4 Hence the data that was collected was initially encoded in range of mutually incompatible 8-bit legacy encodings, discussed in detail in Sect. 2.

It was therefore of some importance that a reliable means of moving text auto-matically from these legacy encodings to Unicode be developed. 5 However, as outlined in Sect. 3, the correlation between the 8-bit encoding and the corresponding Unicode was often far from straightforward. There was no way to set up a simple character-to-character mapping. Even a more sophisticated approach allowing many-to-one, one-to-many, and many-to-many character mappings could not ac-count for all the complexities of these encodings, which require context-sensitive re-orderings of characters, context-conditional mapping, mergers of two characters separated from one another by one or more other letters, and other such compli-cated transformations.
 This problem was addressed by the creation of a dedicated software suite, called Unicodify . The basis of Unicodify is a set of mapping rules , each of which cha-racterises the behaviour of a particular type of 8-bit character in the mapping to Unicode. The purpose of this paper 6 is to outline the conceptual basis of this system. The rationale of the mapping rules is described in Sect. 4, and each rule is discussed in detail in 4.1. Sects. 4.2 and 4.3 provide some further details of the Unicodify software, which, although initially developed for the purpose of corpus building, is designed as a generalised tool for text conversion. 2 Legacy encodings for South Asian scripts A bewildering variety of formats and fonts are needed to view South Asian scripts on the web. Indeed, it was the case that practically every source of data explored on the EMILLE Project used a unique encoding that was incompatible with all the others. A standard for encoding characters in all the scripts of the languages we were interested in 7 has long been available, in the form of ISCII (Indian Standard Code for Information Interchange; see Bureau of Indian Standards, 1991 ) and its sister standard PASCII (Perso-Arabic Standard Code for Information Interchange); indeed, the Unicode standard for Indian alphabets is actually based on the ISCII layout. However, these standards are very rarely used to encode web-based docu-ments. Rather, most organisations which publish electronic documents in these languages use some  X  X ne-off X  approach to encoding the script, developed for the purpose without reference to any standard. This may take the form of a specially constructed 8-bit font, or a system that outputs image files of the electronic docu-ment, or some other non-standard-based solution.

There is a clear reason why, to date, Unicode has not been widely used to publish electronic documents in these scripts: until comparatively recently there were few Unicode-compliant word-processors that could handle them. More critical was the fact that most commercial operating systems and web browsers were not capable of successfully rendering the Unicode forms of the South Asian scripts without the aid of some specialist piece of software. Although the more recent versions of Microsoft Windows and Microsoft Internet Explorer are Unicode-enabled, even now it cannot be assumed that a text in, say, Sinhala will be rendered correctly in Windows XP, or that Urdu text will always be displayed with the correct right-to-left directionality. So even an online publisher who used Unicode-compliant software to produce their text could not, until fairly recently, have published the text online in Unicode for-mat, because their readership X  X  web browser would probably not have been able to render it. Of course, it is to be anticipated that as the implementation of Unicode in operating systems and software is now becoming more widespread, use of Unicode on the web will increase.

The practical result of the lack of standardisation in online publication of text in the languages of South Asia is that there exists a very large quantity of electronic text encoded in legacy encodings which are incompatible with the increasingly widely used Unicode standard. The problem, baldly stated, is as follows: how is such legacy text to be converted to Unicode? This is a critical issue for builders of Uni-code-compliant multilingual corpora; but it is an issue of wider relevance to anyone who wishes to make use of documents encoded in this way.

The term legacy is used with regard to these texts and encodings as a form of shorthand. Clearly, some of these encodings cannot be  X  X egacy X  encodings in the strictest sense, as they are still in active use for the production of new texts on the web. Another term that has been used for such ad hoc 8-bit encodings is  X  X endor-specific encodings X  (used, for instance, by Hussain, Durrani, &amp; Gul, 2005 ). But this term too fails to describe all the encodings addressed in this paper, as not all of them are specific to particular vendors. The term legacy encodings will continue to be used in this paper, therefore, although it should be understood as representing legacy and other ad hoc or vendor-specific non-standard encodings . In this context, non-standard effectively means  X  X ot Unicode X . It might be argued that, since the wholesale adoption of Unicode by producers of text in the languages of South Asia is not yet an accomplished fact, it is not legitimate to assume that Unicode is and will inevitably remain the standard encoding for these scripts. Rather, it might be argued, there is a chance that Unicode itself will prove to be a legacy encoding, in which case con-verting other encodings to Unicode is no more than a legacy-to-legacy conversion and, thus, no great gain. Clearly, it is impossible to predict with full certainty what encoding standards will and will not be adopted and persist in the future (although the PAN Localization Project, 8 which among other goals aims at the development of character set and other language standards for a range of Asian languages, has in many cases identified Unicode as the foremost standard for these languages: see Hussain et al. ( 2005 : 4), where the growing international popularity of Unicode is noted). However, regardless of whether or not Unicode is ultimately adopted by all producers of text in all South Asian scripts, it is very clear that Unicode has become the current standard encoding for linguistic research, particularly in corpus linguis-tics and related fields. 9 There is, then, a clear purpose and value in transferring non-Unicode  X  X egacy X  encodings to Unicode.

Electronic texts in various legacy encodings differ greatly in terms of how straightforward the character equivalences are between the legacy encoding and Unicode. For instance, at one extreme are webpage documents that rely on a ded-icated plug-in or other additional piece of software to display the South Asian script characters. This is most common with Urdu or other languages written in the Indo-Perso-Arabic 10 script, for which a separate program, such as Urdu 98, is often used to handle the display of right-to-left text and the complex rendering of the highly cursive nasta X  X iq style of Indo-Perso-Arabic script. The text format used by such programs is in most cases opaque and not amenable to analysis and conversion to another encoding.

At the other extreme are text formats such as ISCII, where the relationships between characters and their Unicode equivalents are highly transparent. ISCII defines a single character layout in the upper half of the 8-bit range which applies to a range of Indian scripts (Assamese, Bengali, Devanagari, Gujarati, Gurmukhi, Kannada, Malayalam, Oriya, Tamil and Telugu). ISCII can be converted very simply, using a one-to-one character mapping, because (as mentioned above) the Unicode standard for these alphabets, and others descended from Brahmi, is based on the character layout in the ISCII standard. However, as indicated above, texts on the web encoded as ISCII are rare to the point of vanishing. 11
As well as ISCII, some formats for encoding texts in the Indo-Perso-Arabic alphabet can be converted straightforwardly to Unicode using a set of one-to-one or one-to-many character mappings. These encodings include PASCII (see above) and the plain-text format exported by the Inpage software, a popular Indo-Perso-Arabic word processing package. It should be noted, however, that neither of these text encodings is much in evidence on the web; it is much more usual for one of the image or plug-in based solutions discussed above to be employed for Indo-Perso-Arabic text.
Between these two extremes  X  and very common on the web  X  are a large number of encodings based on 8-bit fonts devised on graphical principles rather than logical principles (a distinction which will be defined in the following section). This is the approach most commonly used for publishing texts in these scripts electronically. The relationships between the characters in the encodings and their Unicode equivalents are much less transparent than is the case for ISCII or PASCII. The remainder of this paper will be devoted to explaining the problems posed by encodings based on these fonts (Sect. 3), and outlining the solution used in the Unicodify software 12 (Sect. 4).

It should be noted, however, that the solutions outlined here are critically dependent on the font being available for analysis. This is not always the case. On the EMILLE project, several data sources were found which, although they used an 8-bit font and could thus conceivably have been converted to Unicode using the procedures discussed below, actually used an embedded font or similar technology. It was found that owners of websites that used embedded fonts were typically unwilling to give those fonts up. Without access to the font, developing the mapping rules (see Sect. 4) for text in that font is prohibitively difficult.

Even fonts which are made available by the websites that use them do not nec-essarily stay available. The website may cease operation, or switch to another method of publishing their texts. For instance, the Bengali news site Bengalnet 13 originally used the AdarshaLipi fonts to publish text online, but later switched to a system based on Macromedia Flash Player. Even where a font is widely used on the web, it can be difficult to identify the copyright holder and determine whether sites distributing that font are authorised to do so. There is in these cases therefore a legal difficulty as well, which will, however, not be discussed further here. Bearing these issues in mind, therefore, I give URL references for the fonts discussed in this paper wherever possible, but where no such reference is given, I am not aware of a current authorised internet source for that font. 3 Difficulties in mapping 8-bit fonts to Unicode 3.1 Graphical versus logical encodings The principle reason that the most widely-used 8-bit font solutions for South Asian scripts are highly difficult to map to Unicode is that they are based on a graphical approach to representing the letters of the script, rather than the logical approach used by Unicode (and ISCII before it).

Since the distinction between these two principles forms the cornerstone of the discussion that follows, it is worth taking a moment to define this distinction clearly. The two approaches to encoding are distinguished by what they aim to encode. A graphical encoding aims to encode the actual graphical glyph forms that occur in the script, i.e. the actual shapes that appear on paper (or on a screen). A logical encoding aims to encode the underlying logic of the script X  X  orthography as it is usually per-ceived by the human beings who use the script. So in a graphical encoding, a single code point is linked to a single graphical shape which recurs in text written in that script. In a logical encoding a single code point is linked to what human users of the script perceive to be a single  X  X etter X . By contrast the notion of  X  X etter X  is not needed for graphical encoding: the unit encoded by one code point might be a letter, part of a letter, or part(s) of more than one letter. Similarly, in a graphical encoding the order of the code units reflects the visual sequence of the shapes on paper or on the screen. In a logical encoding, however, the order of the code units reflects what human users typically perceive to be the order of the letters in the orthography of the languages that use that script.
 It is perhaps best to clarify this by reference to some particular examples. The Latin alphabet is an example of a script where the two approaches produce the same result  X  every letter has a single form, and it has that form regardless of the context  X  and so both logical and graphical approaches demand the same set of code points. 14 However, in other alphabets, the form of a letter in the written or printed word can vary widely depending on context. In Indo-Perso-Arabic, for instance, the character written at the start of a word as is shaped as in the middle of a word and as at the end of a word. Following the logical principle of encoding, all these glyphs would be encoded using the same character value, 15 because they are perceived as being  X  X he same letter X  and have, orthographically speaking, exactly the same value. The program rendering the text would then have to work out which variant was appropriate in any given context, and display the correct variant. Following the graphical principle, each of these three glyphs would each be allocated a different code number, and each of these code numbers would represent only a single, unvarying graphical shape. The use of the term logical to describe the former ap-proach should not be taken as implying that the latter is necessarily illogical , but rather that the logic followed by the graphical approach is not the logic of the orthography as normally perceived by human users of the script.

As indicated above, it is usually relatively easy to map to Unicode from another encoding system that uses the logical principle (as long as Unicode contains code points for all the relevant characters). This is why ISCII is easily mapped to Unicode; although none of the code points are the same, there is a one-to-one relationship between the ISCII code point and the Unicode code point. 16 It would also be easy to map from an encoding based on the graphical principle to Unicode if the graphical variants only affect the shape of the character. For example, we might imagine an Indo-Perso-Arabic font that encodes , and at different code points: it would be computationally trivial to implement a text converter that would map all three of these to the single correct Unicode character.

However, to move from an encoding for Devanagari or a similar alphabet based on the graphical principle to Unicode or another encoding one based on the logical principle is very difficult, because for these alphabets the graphical principle does not merely give rise to a variety of encodings for individual characters. It also creates situations:  X  where a single code point (graphical shape) may represent, by itself or together  X  where the mapping is many-to-many, that is, a string of characters in one  X  where the characters are actually in a different order to that which would be These problems will be illustrated in 3.2 to 3.4 below.

It is clear why 8-bit fonts using the graphical principle have been popular on the internet as a means of displaying South Asian scripts: they constitute an easy solu-tion. A South Asian script can be rendered reliably using only the same software that is used to render Latin text, 17 and a special font containing the fixed glyphs for each code point. This eliminates the need for complex rendering software altogether. 18 However, this advantage is outweighed by the fact these font-based solutions are typically compatible with nothing but themselves, having being designed with no thought for standardisation, and being, as outlined above, so intractable to conver-sion into standard logical encodings such as Unicode. I will now go on to explain and exemplify some of the difficulties. 3.2 Vowel symbols Devanagari and other alphabets descended from Brahmi represent vowel sounds in two ways (see Campbell, 1997 : 13, 47 X 48, 67, 113, 118 X 119). A vowel which follows a consonant is represented as a diacritic symbol added to the consonant letter. A vowel at the start of a word, or following another vowel, is represented as an independent letter. However, this is not solely a contextual distinction, because a consonant symbol on its own is deemed to contain an inherent vowel (what vowel this is can vary from language to language; for convenience it is usually deemed to be a schwa, as it is in Hindi, and transliterated as  X  a  X  19 ). This inherent vowel is can-celled if the consonant has a vowel diacritic, or if the special  X  X ero vowel X  diacritic  X  called virama or sometimes halant or hasant  X  is added.

So, a consonant letter with a vowel diacritic represents that consonant followed by that vowel; but a consonant letter followed by an independent vowel letter repre-sents the consonant, followed by schwa, followed by the vowel, as demonstrated below with some (non-word) strings in Devanagari (see Campbell, 1997 : 47 X 48; Nakanishi, 1980 : 48; Snell &amp; Weightman, 1989 : 5 X 19):  X  Devanagari consonant letter representing the syllable ta  X   X  Devanagari consonant ta with virama diacritic  X   X  Devanagari consonant ta with vowel diacritic e  X   X  X  Devanagari consonant ta followed by independent vowel e  X 
The distinction between the independent vowel and the vowel diacritic for e  X  is therefore not merely one of context (since both may come directly after ta ); as such, they are encoded as separate characters in both ISCII and Unicode. For instance, diacritic e  X  is U+0947 and independent e  X  is U+090F. The independent vowels are  X  X ormal X  letters and do not behave in such a way as to create difficulties for con-verting from a graphical encoding to a logical encoding. However, the vowel dia-critics do create a range of such problems.
Firstly, vowel diacritics vary in their placement relative to the consonant: they problems with the relative ordering of characters in the text stream in logical as opposed to graphical encodings, discussed in 3.4 below.

Secondly, vowel diacritics not infrequently have different glyph forms with dif-ferent consonants. In a logical encoding, these different forms are represented by the same code number, whereas in a graphical encoding each glyph shape has a different code number. This does not represent a problem if no glyph ever represents more than one vowel. However, some South Asian scripts do include instances of a single glyph indicating a different vowel, depending on which consonant the diacritic modifies. One example is the Sinhala script (see Campbell, 1997 : 112 X 113; Nakanishi, 1980 : 66), where vowels after the consonant ra behave differently to vowels else-where, as demonstrated below.

In Unicode, the same code point (U+0DD4) is used for Sinhala u regardless of the shape it may happen to have; in a font based on the graphical principle, the same code point is used for the glyph, regardless of whether it represents u or e . 20 So mapping from such a font (for instance, the DL-Manel family of Sinhala fonts) to Unicode requires contextual awareness: the glyph must map to one character after the ra consonant and to another character altogether after other consonants.
A similar case exists in the Tamil alphabet (Bright, 1998 : 67 X 71; Campbell, 1997 : 117 X 119; Nakanishi, 1980 : 58) where some of the diacritics are represented by glyphs that also do service as consonants. So the  X  glyph represents the vowel a  X  , but also represents the consonant ra in some contexts, and the  X  glyph represents both the consonant la and part of the vowel au . In these cases, as with the Sinhala vowels, an accurate mapping from a graphical encoding to a logical encoding means that the context of each instance of these glyphs must be taken into account.

Some alphabets also contain vowel diacritics that contain two components, one that stands to the left and one that stands to the right of the consonant. An example is the Bengali script (see Campbell, 1997 : 11 X 13; Milne, 1913 : 1 X 11; Nakanishi, 1980 : 56), where the diacritic for the vowel o  X  stands both to the right and left of the consonant. Each of the two glyphs that make up this diacritic is also used on its own to represent a different vowel, as shown in the examples below.  X  Bengali consonant ta  X  X  X  Bengali consonant ta with vowel diacritic e  X   X  X  Bengali consonant ta with vowel diacritic a  X 
There are two problems here. The first is an ordering one, and will be discussed with other such problems in Sect. 3.4. The other problem is that in a Bengali font based on graphical encoding, for example the AdarshaLipi font, there is a single code point for the glyph and another single code point for the glyph. These will then be used (together with the glyph for the consonant) to construct all the con-sonant-vowel combinations shown above. A logical encoding will, however, use a different single code point for each of these three vowel diacritics. So the Unicode equivalent for can only be deduced by checking for the presence or absence of in a position that may be two or more bytes distant 21 (and vice versa for ).
In summary, in all of these cases, converting a vowel diacritic in an encoding based on the graphical principle to Unicode requires taking account of the context of the character in ways that are rather complex and vary considerably from script to script. 3.3 Conjunct consonant forms As mentioned above, in theory, a consonant with no vowel after it is followed by the virama diacritic in the South Asian scripts. In practice, however, in most South Asian scripts virama is seen only rarely. Rather, whenever one would theoretically expect a consonant-virama-consonant sequence to indicate a consonant cluster, a special conjunct consonant glyph appears instead. 22
Conjunct consonants come in various forms. The most straightforward are when the first of the two consonants appears as a half-form . This typically (although not always) means that the consonant appears without the central vertical bar that many consonant characters are built around, as in the example below:  X  X  X  Devanagari consonants na (left) and sa (right)
Slightly less straightforward is the situation where half-forms are not used, but the shapes of the two component consonants are combined in some other way, e.g. by one occurring on top of the other, as in the following example:  X  X  Devanagari consonants t
Other, less transparent changes in form occur in other conjunct consonants. For example, if ra is part of a conjunct consonant, it appears as a diacritic mark on the other consonant, as shown below:  X  X  Devanagari consonants ra (left) and ka (right)
In the most extreme cases (which are comparatively rare) there is no immediately obvious correspondence between the component characters and the glyph repre-senting the conjunct. In many alphabets, including Devanagari, this is the case with the ksha conjunct:  X  X  X  Devanagari consonants ka (left) and sha (right)
In Unicode and ISCII, as indicated above, there are no code points at all for half forms, for the ra diacritics, or for the more complex conjuncts: they are represented in memory by combinations of the consonants of which they are made up, and the rendering software handles their realisation. 8-bit fonts based on the graphical principle obviously do not do this. Rather, they typically employ a mixture of two strategies to deal with conjunct consonants.

The first strategy is simply for each conjunct character to be assigned a code point of its own. This strategy is rather profligate of code points, 23 and in an 8-bit font these are in short supply. Obviously, the unpredictable transformations of clusters such as ksha and t graphical encoding. But the predictable conjuncts  X  the ones using half forms or a ra diacritic  X  are not typically stored as individual code points. Rather, another strategy is employed: the components of the predictable conjuncts are allotted code points, and then the conjunct is assembled by juxtaposing the appropriate components  X  full consonants, half-form consonants, and ra diacritics, as appro-priate. To look briefly at an example in Gujarati (Campbell, 1997 : 66 X 67; Nakanishi, 1980 : 52; St. Clair Tisdall 1892 : 19 X 25), in the word ( rajist  X  registrar + particle X ), the central consonant cluster str is built up as follows in the Gopika 24 font: Glyphs Combined glyph Character code 25 4D 78 5B Unicode equivalent Rendered as
As can be seen, a half-form of sa (whose full form is ), the full form of t diacritic form of ra are combined in order to create the overall conjunct.
So far this seems straightforward enough to convert to Unicode; the half-form or ra diacritic maps to a combination of the appropriate consonant and virama , and the full consonant simply maps to the appropriate consonant. However, there is an additional complication. The half-forms used to build conjunct consonant glyphs are in some fonts (but not all) also used to build full consonant forms. Since half-forms are often shaped like the full-form, but without the full form X  X  vertical bar, it follows that the full form can be rendered by juxtaposing the half-form glyph with a glyph containing the vertical bar. This means that it is not then necessary to allot a code point to the full form of a consonant whose half-form is shaped in this regular way: the full form, like conjuncts, can be built up from its graphical components.
An example of this is the Express font for Devanagari. This font has no glyph for the letter ( ja ). Rather, it has a glyph for the half-form of that consonant ( , character code 70) and this is used to build up full forms with the vertical-line glyph ( , character code 65), which is also the glyph used for the dependent vowel a  X  .A straightforward mapping here, treating character 70 as a half-form only, would produce ja + virama + a  X  (a badly formed sequence), rather than just ja . Again, contextual analysis is necessary to map the character correctly. 26 3.4 Character ordering As discussed above, the glyphs realising some vowel diacritics appear on the left side of the consonant, or on both the left and right sides. Logical encodings such as Unicode always have the character encoding the vowel diacritic to the right (i.e. after) the consonant, because this reflects both users X  perceptions of the orthography and the pronunciation of the letter, whereas in fonts following the graphical prin-ciple, the order of the characters always reflects the visual order of the glyphs. So, for instance, the Devanagari syllable  X  X  ( ki ) is represented as [U+0915, U+093F] in Unicode and as [72, 66] in the Webdunia font 27 for Devanagari  X  but the character that represents i is 72 (the first character) in the font and U+093F (the second character) in Unicode.
One might imagine that a simple way to deal with characters like this would be to preprocess the 8-bit text in such a way that any instances of a character representing a left-standing vowel diacritic swap places with the character that follows them. Mapping could then proceed as normal. However, this simplistic approach would inevitably fail, because the consonant to which the vowel diacritic is attached may be represented by more than one character in the 8-bit text. This might be the case if the consonant is a conjunct consonant (for instance, the Devanagari syllable nsi , ) or if it is a single consonant represented in the font by a sequence of more than one character (see 3.3 above). Both the simple and the complex cases are exemplified in the following example of a Hindi word in the Express font:
The reordering to map a word such as this is already clearly rather complicated. It only increases in complexity when dealing with those scripts where a single vowel is represented by two glyphs, one on the left and one on the right of the consonant (see the Bengali example in 3.2 above). In these cases, not only does the left-hand vowel glyph need to be moved relative to the consonant, but what Unicode character it maps to is conditional on whether or not another vowel glyph is present in the position it is moving to on the right of the consonant.

A similar, but reversed phenomenon occurs with respect to conjunct consonants where on the second component consonant. However, thi s diacritic is usually positioned above or on the upper-right of the second consonant  X  and thus, is usually encoded in the graphical fonts by a character after the character encoding the second consonant (and sometimes also after any vowel diacritics that follow the second consonant). So, for instance, in Gujurati the conjunct consonant rga is written ,where ga is and the ra + virama diacritic is positioned above the consonant. In the Gopika font, this is encoded as [64, 6F] where 6F is the glyph of the ra + virama diacritic; in Unicode, of course, the ra and the virama come before the ga , reflecting pronunciation and human perceptions of orthography.
For the same reasons noted above with regard to left-standing vowel glyphs, a simple reordering switching the ra diacritic with the consonant before it in the process of mapping to Unicode will fail, because the appropriate position in the Unicode byte stream may be two or more characters away. 3.5 A summary of the difficulties in mapping fonts to Unicode To summarise the differences between the logical principle of Unicode and the graphical principle of the font-based encodings, we might say that in Unicode, each character has simple semantics and complex display behaviour; whereas in the fonts each character has straightforward display behaviour, but its semantics are very complicated.  X  X emantics X  here refers to what orthographic letter the character represents. Unicode characters always uniquely represent a single letter, whereas in the fonts discussed here, a character may represent one letter, more than one letter, or part of a letter, and this is often dependent on the context in complex ways. Conversely, the display behaviour of the glyphs stored in a font are very simple: the glyph has a single invariant form. Rendering Unicode characters correctly, by con-trast, requires a powerful program to handle all the contextual variation exemplified in the preceding sections. The difficulty of automatically decoding the complex semantics of graphical fonts into the simple semantics of Unicode is, in summary, what makes the mapping process problematic.

It should be emphasised at this point that most of the fonts in question are deeply incompatible with one another, as well as with the ISCII and Unicode standards. There are few easy consistencies between different graphical fonts for South Asian scripts. The code points often differ  X  so for instance, the Unicode character U+092 (the Devanagiri consonant ba , ) is encoded as 63 in the Webdunia font but as 7E in the Shree-Dev-0714 font. Furthermore, the actual encoding structure differs. So while Webdunia and Shree-Dev-0714 both have code points for ba , two other Devanagari fonts (Express and Xdvng 28 ) do not. Instead they build that consonant from two characters (in Express the string is [79, 65] and in Xdvng it is [62, 61]). There are also further differences in how extensively the problems outlined above  X  with regard to conjunct consonants, vowel reordering and so on  X  impact on any given font.

This means that there can be no single solution for the font encodings, as there can be for the many different alphabets represented by ISCII. Rather, a set of general principles are needed which can be applied to all these fonts, allowing a particular solution for each font to be quickly devised. It is possible to develop such a set of general principles because, although the character encodings are not consistent from font to font, the types of mismatch that occur between a graphical and a logical system are . This is because these mismatches are for the most part limited to those discussed above, by virtue of the common structural characteristics of the scripts in question. 4 A solution using mapping rules Having extensively outlined the problems inherent in mapping text to Unicode from a legacy 8-bit font using a graphical encoding, I will now discuss the solution to these problems adopted in the design of the Unicodify software.

It is immediately clear that a mapping table, of the sort that can be used to convert ISCII or other standardised 8-bit encodings to Unicode, cannot possibly be used for these 8-bit fonts. The reordering issues, conditional mappings, and other difficulties outlined above make it simply impossible to list the Unicode character or string represented by each character in a font. Rather, in Unicodify the mapping is accomplished by a set of instructions , where for each character in each font there are one or more specific instructions affecting the Unicode output. These instructions can take into account conditional compilation, reordering issues, and so on. The overall  X  X olution X  for each font is therefore an algorithm rather than a mapping table.

As discussed above, there are a limited number of types of problem in mapping any given 8-bit font, these types being ultimately attributable to the common structural features of the South Asian alphabets. It is therefore possible to generalise many of the instructions, simplifying the task both conceptually and computation-ally. I have dubbed these generalised instructions mapping rules (henceforth simply rules ). Each character in a font has a particular rule associated with it; these rules thus represent the procedure which is followed by the mapping program every time it encounters that character in its input. Rules may take one or more arguments; each argument is a Unicode string and is referred to as a target . Targets determine the output string produced when the rule is applied. The kinds of instructions that rules may consist of are limited to the absolute workable minimum (see 4.1.1 to 4.1.6 below). This system of mapping rules constitutes the solution proposed here to the problems outlined in the paper thus far.

To keep the mapping process as simple as possible in the face of the task X  X  many complexities, it was determined that all mapping should be accomplished in one pass, and that each character should be dealt with sequentially. If reordering is necessary for a particular character, then that is accomplished by its rule, not by any preprocess or postprocess. If a sequence of input characters needs to be mapped all at once, then the instructions for this will be embodied by the rule for the first character in the string.

The mapping for each character in a font is therefore fully defined by specifying which rule it follows, and specifying the target (an exception being the characters that follow special rules  X  see 4.1.6 below). The full set of rules and targets consti-tutes the algorithm for that font. Six classes of necessary procedures are identifiable in the mapping process (though no one font requires all six). Thus, there are six rules, given the names A to G. 29 Each of these is discussed separately in 4.1.1 to 4.1.5 below. These rules have been devised on the basis of an analysis of more than a dozen fonts, covering a wide range of South Asian scripts (including all those cited in Sect. 3 above). For this reason it is unlikely, albeit possible, that further rules are yet to be discovered.
In the discussion below, rules are represented as pseudo-code in English. In practice, they were written as C code. At an early stage in the development of the software, various systems were considered whereby the rules could be written in a specially devised formalism and then read-in to the software at runtime. However, this approach was abandoned when it became clear that no such formalism could possibly be any simpler than the actual code, and thus there would be no gain in user-friendliness to be had from programming such a rule-compiler. For this reason, the mapping algorithms for each font were devised conceptually using the rules discussed below, and then programmed directly into the Unicodify software X  X  source code. 4.1 An outline of the rules 4.1.1 Rule A Rule A handles represents the simplest possible case: where a character in the 8-bit font always corresponds to a particular Unicode character or string, regardless of context, and with no need for reordering. It takes a single target as its argument. The instruction in this case is:  X  Output the  X  X arget X  string.

This rule applies to font characters that represent a single letter, be it consonant, vowel, numeral or punctuation, without any context-based conditional mapping or reordering of any kind. It also applies to font characters whose equivalent is a string of two or more Unicode characters (e.g. a whole consonant cluster, or a consonant plus vowel diacritic combination). Computationally, there are several variant implementations of Rule A depending on the length of the target string; however, conceptually all these are identical. Rule A is, essentially, the rule that would be used for every character in an approach based on mapping tables rather than mapping instructions. Indeed, a mapping algorithm based only on Rule A can be successfully employed to map encodings to Unicode that lack the complexities of the South Asian script encodings discussed here, such as non-Unicode encodings for Perso-Arabic, Greek or Cyrillic. 4.1.2 Rule B Rule B handles a large set of characters, the half-form consonants (see 3.3 above). As such it is the most general of the rules that take context into account. These half-form consonants map to the Unicode character for that consonant plus a virama when they are part of a conjunct; when they are not, they are followed by a character encoding a vertical-line glyph, and the combination of vertical line and half-form maps to the Unicode character for the consonant (with no virama ). Rule B captures this as follows:  X  If the next character in the input stream is a vertical line glyph, then output the  X  otherwise, output the target string with an appended virama character.
This rule introduces two more types of instruction: the if-else structure referring to the next (and, in other rules, previous ) character, which is at the root of how the rules handle conditional mapping, and the instruction to  X  X kip the next character X , which is vital for many-to-one and many-to-many mappings. Another feature of Rule B that differentiates it from Rule A is that, while its general form is always as stated above, it has specific forms that are particular to individual fonts. This is because the software does not know what characters are  X  X ertical line glyphs X , and the Unicode character code for virama is different for different scripts; so this information must be built into the code for Rule B individually for each font. So for instance, Rule B for the Devanagari font Webdunia is as follows:  X  If the next character in the input stream is 74, then output the target string and  X  otherwise, output the target string with an appended U+094D.

For some other fonts the rule is more complicated. For example, in the Gopika font, as well as a vertical line glyph (character code 74), there are also three characters that depict the vertical line glyph and a vowel diacritic. These characters are as follows: Code Glyph 5C Represents vertical line plus (U+0AC3) CB Represents vertical line plus (U+0AC2) FE Represents vertical line plus (U+0AC1)
If a Rule B half-form is followed by one of these characters, it has to map to the consonant character alone rather than consonant plus virama  X  but here the next character in the input stream cannot be skipped, since the informat ion in the vowel diacritic has not yet been mapped to anything. So the full Rule B for the Gopika font was as follows:  X  If the next character in the input stream is 74, then output the target string and  X  otherwise, if the next character in the input stream is 5C, CB or FE, then output  X  otherwise, output the target string with an appended U+0ACD.

It was then possible for the vertical bar plus vowel diacritic characters to be treated as if they only displayed the vowel diacritic, and converted accordingly when the mapping program moved onto them. 4.1.3 Rule C
Rule C handles vowels which, in the graphical fonts, stand to the left of their consonants, and which must be moved to the right of that consonant (or consonant cluster) in the Unicode output (see 3.4 above).

The key problem with these characters is that the destination point of the vowel is not determinable at the point when the program is dealing with the character. As discussed above, it is insufficient simply to move the vowel one character down the byte stream. Rather, it must stand after the next consonant in the output stream that is not followed by virama , since a consonant followed by virama is a non-final component of a conjunct consonant, and a vowel diacritic may only occur after the final component of a conjunct consonant.

This means that the insertion point for the reordered vowel can only be identified by checking the characters further down the output stream to see whether or not they are consonants, vowels, virama, etc. This entails allowing rules to use the glyph type of characters in the output in if-else conditions : this requires a dedicated function in the softwareprogrammed to recognisethecharact er typeof all theSouth Asian script letters in Unicode. 30 But there is a greater difficulty  X  namely that at the point when the character for the reordered vowel is being mapped, the subsequent output stream does not yet exist .
At a conceptual level this is solved by pretending that the output stream does exist, and formulating the rule accordingly. This fiction is then supported at the computational level by a system whereby vowels that need to move rightwards down the output stream are stored in a dedicated buffer and held there until enough characters have been written to output for an appropriate destination point for the vowel to be identified.

One final complication is that some Rule C characters encode the ra + virama diacritic as well as a vowel that needs to be moved rightwards. Such diacritics usually need to be moved leftwards (as described in the following section), but in this case the correct place for them is their current location. So to handle these characters, Rule C has a two targets: target.1 , which is the target to be inserted further down the output stream, and target.2 , which is the target that is to be inserted at the current point in the output stream. The full Rule C is as follows:  X  At the current location, output the target.2 string.  X  At the first location further rightwards in the output stream where  X  output the target.1 string. 4.1.4 Rule D
Rule D handles characters which need to be moved backwards in the stream  X  i.e. to the left. In practice, the only characters that require this are those representing the diacritic ra + virama combination discussed above (see 3.4).

As with Rule C, the destination of the target string is contextually variable: to reach the right location relative to conjunct consonants 31 it must be inserted before the most recent consonant in the output stream which is not preceded by virama . And as with Rule C, some Rule D characters encode additional letters which do not need to be reordered as well as the ra + virama diacritic which does (it is, for instance, not unusual for a single font character to represent not only ra + virama but also a right-standing vowel diacritic)  X  and therefore Rule D takes two targets:  X  At the current location, output the target.2 string.  X  At the first location further leftwards in the output stream where  X  output the target.1 string.
Conceptually this is precisely the same process as Rule C, only in the other direction. Computationally it is rather different because the output stream to the left of the current insertion point already exists. For this reason, while Rule C targets are put in abeyance until the context required to locate their destination has been generated, Rule D targets are inserted in the appropriate place immediately that they are encountered. This means, incidentally, that if a Rule C character is currently in abeyance when a Rule D character is mapped, resolution of the Rule D character takes precedence over resolution of the Rule C character. 4.1.5 Rules F and G
Rules F and G handle cases where a vowel is realised by two glyphs, one standing on either side of the consonant. Rule F handles combined consonant-plus-vowel characters, whereas Rule G handles characters that depict just a vowel diacritic. As such Rule G is simpler, and will be discussed first.

As illustrated in Sect. 3.2, the Bengali vowel diacritic o  X  is represented by a glyph to the left of the consonant and a glyph to the right of the consonant. The left-hand glyph, when used alone, represents the vowel diacritic e  X  . The two glyphs are encoded as separate characters in graphical fonts. To correctly map these characters, there-fore, it is first necessary for the left-hand glyph ( ) to be moved to the correct position after the consonant (as per Rule C). When it gets there, if a right-hand-glyph ( ) is present, the two need to merge together to produce the Unicode o  X  diacritic; if there is no right-hand glyph, the output should be the Unicode e  X  diacritic.
This is rather difficult to implement in Rule C, because the correct insertion point for the Rule C character is only determined after the program has processed the character encoding the right-hand glyph (since at least that much context is required to identify the insertion point). So the left-hand glyph is dealt with under Rule C as described above, just as if it were always an e  X  diacritic and never part of o  X  . Then, when the character encoding the right-hand glyph is being processed, Rule G is applied. Rule G is a conditional rule, like Rule B, but in Rule G the mapping is conditional on whether or not an e  X  diacritic character is currently held in abeyance in the Rule C buffer . If it is, then Rule G deletes the e  X  diacritic from the buffer, and outputs the merged diacritic character. If it is not, then Rule G outputs the character for the right-hand glyph alone. The rule is thus:  X  If the character representing the left half of a two-part vowel diacritic 32 is  X  otherwise, output the target.2 string
This is somewhat confusing in the abstract, so let us examine a concrete instance of this process at work in the AdarshaLipi family of fonts for Bengali. For the syllables to  X  , te  X  , ta  X  , the desired font-to-Unicode correspondences are as follows: Glyph AdarshaLipi (input) Unicode (desired output) Represents.. .
AE is mapped according to rule C; its target.1 (sent rightwards) is U+09C7 and its target.2 (inserted at the current point) is a null string. This means that, when C buffer. Then, again in both o  X  and te  X  , character 61 is mapped to U+09A4 by Rule A.
It is now that, in the case of to  X  , Rule G comes into play. Character A1 is mapped by Rule G; its target.1 is U+09CB and its target.2 is U+09BE. Rule G examines the buffer, looking for U+09C7. it finds it, so it outputs target.1  X  U+09CB, the two-part diacritic o  X  . It then cancels the operation of Rule C by emptying the Rule C buffer. The mapping process has now effectively merged together AE and A1 from the input into U+09CB in the output.

In the case of te  X  , Rule G does not come into play (as there is no Rule G character present) and Rule C completes its operation as normal, moving U+09C7 from the buffer to the output stream after the consonant character. As for ta  X  , A1 still triggers Rule G, but this time there is nothing in the Rule C buffer, so Rule G outputs character A1 X  X  target.2, which is U+09BE (the vowel a  X  ).

Rule F, used so far only for Sinhala, operates according to the same basic prin-ciple: mapping is conditional on the presence of some character in the Rule C buffer. However, Rule F is more complicated. To explain why, it is necessary to discuss the nature of Sinhala vowel diacritics.
 Sinhala has four different two-part vowel diacritics, representing e  X  , o  X  , o , and au . The left-hand glyph for all four is the symbol that, on its own, represents the vowel e . This character, like Bengali e  X  , is mapped according to Rule C. Again as in Bengali, the right-hand glyphs of these two-part vowels may also represent diacritics, when they are considered in isolation. As such they may mostly be handled by Rule G. However, the right-hand glyph of e  X  , which in isolation represents virama , 33 is problematic: it is often realised not as a separate diacritic but as a regular variation in the form of the consonant it follows, as illustrated below:  X   X   X  Sinhala consonant ka , without (left) and with (right) virama  X  Sinhala consonant kha , without (left) and with (right) virama  X  Sinhala consonant t  X  Sinhala consonant va , without (left) and with (right) virama
Some Sinhala fonts (for instance, the DL-Manel family of fonts, or MiANCL) give the forms of the consonants that are merged with virama a separate, single code point. For instance, in DL-Manel, va without virama is 6A and va with virama is F5. So in this font, character F5 must map to va plus virama if there is no preceding e -glyph, but to va plus e  X  if there is a preceding e -glyph. The dif-ference to Rule G is that the conditional mapping occurs on the consonant, because there is no separate right-hand glyph where Rule G could be invoked .
The rule which handles these merged consonant plus right-half diacritic combi-nations is Rule F. It takes a single target, which is the code point for the consonant, and outputs either that consonant followed by virama (in Sinhala, U+0DCA) or that consonant followed by e  X  (U+0DDA). Thus, for Sinhala Rule F is as follows:  X  If the character U+0DD9 34 is currently being moved rightwards in the output  X  otherwise, output the target string followed by U+0DCA 4.1.6 Special Rules
All the fonts discussed here contain at least one character whose behaviour cannot be captured in terms of the generalised rules discussed above (and, in some cases, many more than one). For these characters, a  X  X pecial rule X  must be written  X  that is, a unique set of mapping instructions to be followed solely when mapping that one character.

These  X  X pecial rules X  for particular characters are built up from the same com-ponents illustrated Rules A, B, C, D, F and G, namely if-else conditions referring to subsequent and previous characters,  X  X kip the next character X  instructions, refer-ences to characters currently being held in abeyance for Rule C, and so on. They may also include one or more of the standard rules, for example this special rule from the algorithm mapping the characters 88, 89 and 8A 35 in the MiANCL font for Sinhala:  X  If the next character in the input stream is A3, then  X  otherwise, proceed according to Rule G (target.1: U+0DDA,
A more typical special rule is a rule of the sort often used to handle independent vowel letters. These have not been discussed at length so far, since they are largely straightforward, having no contextual variants or ordering issues. However, many of the independent vowel letters are made up of discernible subcomponents. For instance, in Devanagari, the independent vowel a  X  ( ) looks like the independent vowel a (  X  ) with an a  X  diacritic ( ). Many Devanagari fonts therefore do not have a character for , instead building it up from the characters for independent a and diacritic a  X  . In Unicode and ISCII, by contrast, every independent vowel has its own code point, regardless of whether or not it appears to be decomposable. This logical-graphical mismatch is similar to the one affecting half-form consonants, but Rule B cannot be used for characters like this, since virama is never involved. Furthermore, the contextual conditioning can be much more complicated in these cases, since other independent vowels have the shape of the joined-up glyph with additional diacritic glyphs. For instance, the independent vowel o  X  (  X   X  ) has the appearance of  X  with the diacritic (and several other characters work the same way). So, not only the next character but the character after that must be taken into account  X  often leading to a long list of if-else options, as demonstrated in the rule for character 79 ( ) in the Webdunia font:  X  If the next character in the input stream is any character other than 74, then  X  otherwise, if the next character is 74 and the character after that is 69, output  X  otherwise, if the next character is 74 and the character after that is 70, output  X  otherwise, if the next character is 74 and the character after that is 75, output  X  otherwise, if the next character is 74 and the character after that is EE, output  X  otherwise, if the next character is 74 and the character after that is FC, output  X  otherwise, if the next character is 74 and the character after that is none of the Note that, as demonstrated in the examples above, special rules do not have targets. Any string that they output is part of the rule itself.

One slightly more general  X  X pecial rule X , which applies to more than one char-acter, is the  X  X kip current X  rule:  X  Output nothing.

This rule is typically used for characters which have no purpose except to provide context referred to by the rules for other characters. Such characters are often skipped over by the  X  X kip next X  procedure discussed above, but in cases where this is not possible, the  X  X kip current X  rule is employed. 4.2 The input and the output As has been indicated, the input to Unicodify is a stream of 8-bit characters encoded using one of the fonts the software is designed to encode. However, in the web-based texts that use these fonts, no text is encoded solely in the font. The texts will contain ASCII-encoded Latin alphabet text  X  at the very minimum, the HTML tags are encoded as ASCII, and the text may contain short stretches of text in English as well. This means that not all the file should be processed according to the algorithm for the main font: some should simply be converted from 8-bit ASCII characters to 16-bit UTF-16 characters 36 without any changes to the character codes.

Unicodify uses the font mark-up in the source HTML to identify which stretches of the text use the South Asian font and which do not. For this reason, the input to Unicodify must be an HTML file. Unicodify is capable of recognising font infor-mation in both  X  font  X  tags and  X  span  X  tags. However, all the HTML tags are removed from the output text.

They are replaced with a minimal SGML/XML mark-up that is compliant with the Corpus Encoding Standard, 37 using little more than the basic tags  X  p  X  and  X  head  X  (since this is all that can be reliably inferred from the original HTML tags). The output file is also given an empty CES-compliant header. These markup features are designed to meet our needs as corpus builders. However, they do not form part of the actual core algorithm described in the previous section. That algorithm consti-tutes a fully general solution to the problem of legacy texts encoded using the font-based encoding systems discussed in this paper, regardless of whether the Unicode version of a legacy text is to be used in the construction of a corpus or for some other purpose entirely.

It should be noted at this point that the use of the mapping rules described in 4.1 above has a very strong  X  X arbage in, garbage out X  effect. The rules operate on the assumption that the input stream is well-formed according to the principles of the font that is being converted. For instance, Rule B assumes that there will never be an instance of a half-form consonant that is not followed either by an appropriate full form, making up a conjunct consonant, or by a vertical bar glyph, making up a full-form consonant. If a half-form consonant did occur outside these contexts, it would be ill-formed in terms of the structure of South Asian scripts, but also ill-formed according to Unicodify expectations. If rules designed for well-formed input are applied to ill-formed input, the output too will be ill-formed. However, the precise nature of the errors in the output is not necessarily the same as the errors in the input. The text may be malformed in a wholly different way. For instance, presented with a misplaced half-form consonant character, Rule B would produce a consonant plus virama combination. Similarly, presented with a misplaced diacritic, Rules C and D will still move it, but to an unpredictable destination. In short, where the input to the rules is not well formed, the effect of applying the rules is not defined. 4.3 Extended features of the Unicodify software Unicodify is currently capable of converting to Unicode eighteen different fonts, covering six scripts (Bengali, Devanagari, Gujarati, Gurmukhi, Sinhala and Tamil). As mentioned in 2.2, other components of the software also convert the ISCII and PASCII standard encodings, and the plain text format output by the Inpage word-processing package. More prosaically it can also convert the Latin-2 encoding for Eastern European languages to Unicode. As mentioned in 4.1.1 above, its system based on mapping rules is extensible to encoding systems for any of the many writing systems (Latin, Cyrillic, Greek, IPA, etc.) that are less complex than the South Asian scripts discussed here, since the functionality required for the South Asian scripts subsumes that required to handle less complex scripts.
 Unicodify has been successfully used to generate the Unicode text of the EMILLE/CIIL Monolingual Written Corpora (see Baker et al. 2004 ), and has also been used by Peter Hook of the Malhar Project 38 to convert that project X  X  legacy text to Unicode format.

However, although Unicodify is working software, and has been made available for general use, 39 it is still a work in progress. It is extended to handle additional fonts whenever this becomes necessary to our work on South Asian text corpora. Work is also ongoing to add more powerful features to the program  X  for instance, to give the user the option of preserving the HTML tags in the input file. Work is also undertaken on a regular basis to make the algorithms for the current fonts more robust, by adding or extending special rules to handle the more common  X  X arbage in X  problems (see above) to reduce the extent to which they produce  X  X arbage out X . This has led, for instance, in text generated from the Bengali AdarshaLipi fonts, to a reduction in the number of errors in the ordering of the candrabindu diacritic (caused by typing errors in the original files). A further planned advance in the software is to develop a graphical user interface for the definition of additional mapping algorithms, getting around the problem mentioned above, that no suffi-ciently powerful formalism for expressing mapping rules is significantly simpler than the actual code for those rules.

Unicodify is unique in its implementation of the approach to text conversion described in this paper. However, this does not imply that Unicodify is necessarily unique as a system for converting legacy font encodings to Unicode. Indeed it is not; for instance, among the PAN Localization Project X  X  collection of language locali-sation resources are a number of systems that carry out some form of encoding conversion. 40 A discussion of the substantial variation in the details of such systems X  implementation would be beyond the scope of this paper, although I am not aware of any that duplicates in full Unicodify X  X  capabilities. It is sufficient to note that the strengths of the approach based on mapping rules that has been described here are its power and its generality in allowing the behaviour of a wide range of font encodings for the full set of South Asian scripts. 5 Conclusion It is clear that the legacy encodings for South Asian scripts that may currently be encountered on the Internet and elsewhere vary very greatly. Some are based on the logical principle while many others are based on the graphical principle; among the graphical encodings, there is furthermore a great variety in how different letters, diacritics and contextual variants are encoded, constrained only by the common structural features of the South Asian scripts descended from the Brahmi script.
It is conceptually and computationally relatively straightforward to map from a legacy encoding based on the logical principle to Unicode. But it is much, much harder to go from a graphical font-based encoding to Unicode, for a range of reasons explored in Sect. 3. However, as demonstrated in Sect. 4, if the conversion process is conceived in terms of mapping rules making up an algorithm, rather than direct correspondences from character to character in a mapping table, it is possible to resolve these difficulties. This has been done in the design of the Unicodify software, which as a result is now a general tool for South Asian script encoding, which also has the capacity to handle any less complex script. While at first glance many legacy encodings are so alien from Unicode that converting the one to the other may appear more trouble than it is worth, unravelling the complexities in the relationship between logical and graphical encodings of the South Asian scripts can bring a wealth of legacy text to Unicode-based computational and corpus linguistics. References
