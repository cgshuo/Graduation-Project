 ORIGINAL PAPER Alicia Forn X s  X  Anjan Dutta  X  Albert Gordo  X  Josep Llad X s Abstract The analysis of music scores has been an active research field in the last decades. However, there are no pub-licly available databases of handwritten music scores for the research community. In this paper, we present the CVC-MUSCIMA database and ground truth of handwritten music score images. The dataset consists of 1,000 music sheets writ-ten by 50 different musicians. It has been especially designed for writer identification and staff removal tasks. In addition to the description of the dataset, ground truth, partitioning, and evaluation metrics, we also provide some baseline results for easing the comparison between different approaches. Keywords Music scores  X  Handwritten documents  X  Writer identification  X  Staff removal  X  Performance evaluation  X  Graphics recognition  X  Ground truths 1 Introduction The analysis of music scores [ 19 , 22 , 31 , 35 ] is a classical area of interest of Document Image Analysis and Recogni-tion (DIAR). Traditionally, the main focus of interest within the research community has been the transcription of printed music scores. Optical Music Recognition (OMR) [ 1 , 2 , 15 ] consists in the understanding of information from digitized music scores and its conversion into a machine readable format. It allows a wide variety of applications such as the edition of scores never edited, renewal of old scores, conver-sion of scores into Braille, production of audio files, adapta-tion of existing works to other instrumentations, transposing a music sample to some other clef or key signature, producing parts from a given score or a full score from given parts, cre-ation of collecting databases to perform musicological anal-ysis. Since the first works by Prerau and Pruslin in the late 1960s [ 26 , 27 ], interest in OMR has grown in last decades, appearing several complete OMR systems for printed music (such as Aruspix, Gamera, or Guido [ 28 , 29 , 33 ]), braille mu-sic approaches [ 3 ], and even an almost real-time keyboard-playing robot (the Wabot-2 robot [ 21 ]).

Among the required stages of an Optical Music Recog-nition system, an special emphasis has been put in the staff removal algorithms [ 5 , 6 , 12 , 32 ], since a good detection and removal of the staff lines will allow the correct isolation and segmentation of the musical symbols, and consequently, will ease the correct detection, recognition, and classification of the music symbols. Staff removal is somehow related to form processing [ 18 ], where ruling lines must be removed prior to recognize the text. The main difference is that staff removal techniques can take advantage of grouping rules, in other words, the algorithm can search a group of five equidistant horizontal lines (the staff).

In the last decade, there has been a growing interest in the analysis of handwritten music scores [ 11 , 20 , 23 , 24 , 30 , 31 , 34 ]. In this context, the focus of interest is two-fold: the recognition of handwritten music scores and the identification (or verification) of the authorship of a mu-sic score. Concerning writer identification, musicologists do not only perform a musicological analysis of the composi-tion (melody, harmony, rhythm, etc.) but also analyze the handwriting style of the manuscript. In this sense, writer identification can be performed by analyzing the shape of the hand-drawn music symbols (e.g., music notes, clefs, ac-cidentals, rests, etc.), because it has been shown (see [ 10 ]) that the author X  X  handwriting style that characterizes a piece of text is also present in a graphic document. Nevertheless, musicologists must work very hard to identify the writer of a music score, especially when there is a large amount of writers to compare with. Recently, several writer identifica-tion approaches have been developed for helping musicolo-gists in such a time-consuming task. These approaches are based in many different methodologies, such as Self Orga-nizing Maps [ 20 ], Bag of Features [ 14 ], knowledge-based approaches [ 4 , 13 ], or even systems that adapt some writer identification approaches for text documents to music scores [ 11 ].

Contrary to printed music scores databases [ 6 ], there are no public databases of handwritten music scores available for the research community. For this reason, there is a need of a public database and ground truth for validating the different methodologies developed in this research field. With this motivation, in this paper we present the CVC-MUSCIMA 1 ground truth: a ground truth of handwritten music score im-ages. The database and ground truth are available in the Web site: http://www.cvc.uab.es/cvcmuscima . This dataset con-sists of 1,000 music sheets written by 50 different musicians and has been especially designed for writer identification and staff removal tasks.

In this paper, we describe the database, evaluation metrics, partitions (data subsets), and baseline results for comparison purposes. We believe that the presented ground truth will serve as a basis for research in handwritten music analysis. Moreover, we will show that the effort of generating ground truth can be reduced using color cues and by applying dis-tortions to both original images and ground truth images.
The rest of the paper is organized as follows. Section 2 describes the dataset and the staff distortions applied. Section 3 presents the evaluation partitions, metrics, and baseline results for comparison purposes. Finally, concluding remarks are described in Sect. 4 . 2 Dataset The dataset consists of 20 music pages of different compo-sitions transcribed by 50 writers, yielding a total of 1,000 music pages. All the 50 writers are adult musicians (aged from 18 to 35) in order to ensure that they have their own characteristic handwriting style. We chose the set of 50 musi-cians as much heterogeneous as possible. The musicians are from different geographic locations (different cities in Spain). The set of writers includes advanced musician stu-dents (in conservatories of music or at University), musi-cologists, music teachers, and professional musicians, but as far as we know, none of them are famous. They all have been studying music for many years, and consequently, they have their own characteristic handwriting style. Figure 1 shows some examples of handwritten music scores written by three different musicians. Having a look at the images, one can see that writer B tends to write in a rectilinear way (with very thin headnotes), while writers A and C draw very round headnotes. In addition, it can be observed that writer C tends to write short symbols (and also short slurs), whereas writers A and B draw taller music symbols and longer slurs.
Each writer has been asked to transcribe exactly the same 20 music pages, using the same pen (a black Pilot v7 Hi-Tecpoint) and the same kind of music paper (standard DIN A4 sheets with printed staff lines in blue color). The set of the 20 selected music sheets contains monophonic and poly-phonic music, and it consists of music scores for solo instru-ments (e.g., violin, flute, violoncello, or piano) and music scores for choir and orchestra. It must be noted that the music scores only contain the handwriting text considered as part of the music notation theory (such as dynamics and tempo notation), and for this reason, music scores for choir do not contain lyrics.

Furthermore, for staff removal tasks, each music page has been distorted using different transformation techniques (please refer to Sect. 2.2 for details), which, together with the originals, yields a grand total of 12,000 base images.
Next, we describe the data acquisition, the generated deformations, and the different ground truths and data formats. 2.1 Acquisition and preprocessing Documents were scanned using an flatbed Epson GT-3000 scanner set at 300 dpi and 24 bpp, as color cues were used in the original templates to ease the elaboration of the staff ground truth. Later, the images were converted to 8-bit gray scale. Care was put into obtaining a good orientation during the scanning stage, and absolutely no digital skew correction was applied once the pages were scanned.
 The staff lines were initially removed using color cues. Afterward, they were binarized and manually checked for correcting errors, specially when some segments of the staff lines were manually added by the writer (see an example in Fig. 2 ). Thus, from the gray scale images, we generated the binarized images, the images with only the music symbols (without staff lines), and finally, the images with only the staff lines. Next, we describe the distortions applied to the music scores for staff removal. 2.2 Staff distortions To test the robustness of different staff removal algorithms, we have applied a set of distortion models to our music score images. These distortion models are inspired by the work of Dalitz et al. [ 6 ] for testing the performance of staff removal algorithms in printed music scores. In [ 6 ], the authors describe nine different types of deformations for simu-lating their dataset with real-world situation: degradation with Kanungo noise, rotation, curvature, Staffline inter-ruption, typeset emulation, Staffline y -variation, Staffline thickness ratio, Staffline thickness variation, and white speckles.

In order to obtain the same effect, the deformation is simul-taneously applied to the original and the ground truth staff images, which correspond to binary images with only the staff lines. A brief description of the individual deformation models is given next:  X  Kanungo noise . Kanungo et al. [ 17 ] have proposed a noise  X  Rotation . The distortion rotation (see Fig. 3 c) consists in  X  Curvature . The curvature is performed by applying a half  X  Staffline interruptions . The Staffline interruptions con- X  Typeset emulation . This particular defect is intended to  X  Staffline y-variation and Staffline thickness variation .  X  Staffline thickness ratio . This defect only affects the (k) Staffline thickness Ratio (l) White speckles  X  White speckles . This degradation model is used to gener-
Table 1 describes the parameters of the respective mod-els. Dalitz et al. [ 6 ] have developed the MusicStaves toolkit which is available for reproducing the experiments in other datasets. However, these available algorithms for distorting the staff lines have an important drawback: they require com-puter-generated perfect artificial images, which means per-fect horizontal staff lines, equidistant, and also with the same thickness. Since our dataset contains printed and handwritten segments of staff lines (see Fig. 2 ), their algorithms cannot be directly applied to our music scores. For this reason, we have modified these algorithms to reproduce the same dis-tortion model in our handwritten music scores (where we do not assume any constraints for perfect staff lines).
For validating the staff removal algorithms, we have gen-erated a set of 11,000 distorted images by applying the nine already described distortion models, where two of them have been applied twice (see Fig. 3 ). Thus, for each original image, we have obtained 11 distorted images by applying these dis-tortion algorithms with the parameters described in Fig. 3 . As a result, the dataset for staff removal purposes contains 12,000 images (1,000 original images plus the 11,000 dis-torted ones). However, since we also provide the code of the staff distortions algorithms, the users can generate the distorted images with their desired parameters. 3 Ground truth In this section, we describe the images, evaluation parti-tions (subsets), evaluation metrics, and some baseline results. Thus, they will serve as a benchmark scenario for a fair com-parison between different approaches.

Concerning the baseline results, it must be said that since the main contribution of this work is the framework for per-formance evaluation, we include some baseline results just for reference purposes. 3.1 Images description All the images of the dataset are presented in PNG format. Each document of the dataset (1,000 original images plus the 11,000 distorted images) is labeled with its writer identifica-tion code and presented in different image flavors:  X  Original gray scale image (only for the original 1,000  X  Binary image (with staff lines).  X  Binary staffless image (only music symbols).  X  Binary staff lines image (no music symbols).

Although all this information is available for all tasks, we encourage the use of certain image flavors for different tasks. The staffless images are particularly useful for writer identification: Since most writer identification methods re-move the staff lines in the preprocessing stage, this eases the publication of results which are not dependant on the performance of the particular staff removal technique applied (seeanexampleinFig. 4 ).

Similarly, for the staff removal tasks, staff lines images without music symbols (see Fig. 5 ) may be useful, not only for the evaluation of the method but also for training pur-poses. It must be said that the ground truth images only show the pixels that belong only to staff lines. Consequently, these images contain holes, which correspond to the pixels belonging to music symbols.

Table 2 summarizes the provided images and these recommendations. 3.2 Evaluation partitions for writer identification For training and evaluation purposes, we devised two sets of ten partitions, which were especially designed for the evalu-ation of writer identification tasks:
Set A or constrained . In the first set of partitions, the training pieces of a given fold are the same for each writer, and so none of the pieces of the test set have been used during the training stage. As an illustrative example, look at Fig. 6 a. Since the first music page of one writer is in the train set of a given fold, all the first music pages of the remaining writers will also be in the train set of that particular fold.
Set B or unconstrained . In the second set of partitions, this constraint is not satisfied, and pieces that appear in the training set of one author will appear in the test set of a dif-ferent one (for example, the first music page will appear in the train set of one author and in the test set of another, as seen in Fig. 6 b).

These partitions are particularly devised to attest that we are indeed performing writer identification instead of rhythm classification. Indeed, if the method was performing rhythm classification, it is reasonable to think that, in set B, unconstrained, test pieces from one author would be matched with the exact same pieces that appear in the train set of a different author, and so the classification results would be significantly lower than on set A, where this confusion is not possible. At the same time, a writer identification rate in set B similar to the one in set A will show that the system is classifying according to the handwriting style and not being particularly affected by the kind of music notes and symbols appearing in the music sheet.

In each partition, 50% of the documents of each writer belong to the training set and the other 50% belong to the test set. Furthermore, effort has been put in guaranteeing that each piece appears approximately 50% of the time in training and 50% in test. The exact partitions can be found in the data-set pack.

It must be said that instead of the proposed partitions, other strategies (such as  X  X eave-one-out X ) can be also used. However, we encourage the use of these partitions to test whether the system is rhythm dependant or not. In any case (partitions or  X  X eave-one-out X ), the metrics described in next subsection can be applied without any modification. 3.3 Evaluation metrics and baseline results for writer In this subsection, we will describe the evaluation metrics and, as an illustrative example, some baseline results for writer identification purposes.
Metrics. Writer identification systems are evaluated considering two options: if the image has been correctly clas-sified taking into account the n-first authors or only the first writer. In our scenario, we will treat it as a binary problem, in which a music score is correctly classified only if the first nearest writer corresponds to the ground-truthed one.
Method. As we have commented, it is out of the scope of this work to make a comparison of different writer iden-tification methods in the literature. However, and only for reference purposes, we provide baseline results using a recent writer identification method for musical scores [ 14 ].
In the Bag-of-Notes approach described in [ 14 ], features are computed using the Blurred Shape Model descriptor [ 8 ]. As in the Bag-of-Visual-Words framework, a codebook is built and symbols are assigned to the vocabulary words to represent the musical scores. Finally, they are classified using aSVMtrainedina1vs.allfashion.

In that work, the authors presented a vanilla Bag-of-Notes, with the following properties:  X  Unsupervised clustering with k -means.  X  Hard assignment.  X  Linear kernel.

Afterward, they proposed the following modifications:  X  Supervised clustering (learning a different codebook for  X  Probabilistic vocabulary (learning the vocabulary with a  X  Using a RBF kernel.

Interestingly, we found that the simpler vanilla implemen-tation of the Bag-of-Notes obtained very similar results than the more complex modifications. In general, the probabilis-tic vocabularies bring little or no improvement over k -means even when tied with supervised clustering unless some adap-tation is performed [ 25 ]; besides, the increasing size of the vocabulary usually makes these approaches impractical or unfeasible as the number of classes increase.

The RBF kernel provided slightly better results than the linear one. However, RBF has an extra parameter, the band-witdh  X  , which has to be validated. Also, using a linear kernel allows us to use solvers optimized for linear problems such as LIBLINEAR [ 9 ], which makes use of the cutting-plane algorithm and drastically improves the training speed of the SVM. To set the C trade-off cost of the SVM classifier, we used the same heuristic used by the SVM light [ 16 ] suite. Given a set of N training vectors X ={ x 1 , x 2 ,..., x N } ,weset C as follows: C = 1 / k 2 , k = 1
This heuristic gave excellent classifications results, better than to those obtained by manually setting the parameter.
Because of its simplicity as well as comparatively good performance, we will report results using the vanilla imple-mentation of the Bag-of-Notes (unsupervised clustering with k -means, hard assignment, and linear kernel), without the improvements of [ 14 ].

Results. Table 3 reports mean classification accuracy and standard deviation as a function of the number of words for the two sets of partitions (please c.f. Sect. 3.2 for details on these partitions). Note that the accuracy results on both sets are quite similar, with a slight advantage for the second set; the higher accuracy and smaller standard deviation are probably caused because this set contains more variety in the training data. The fact that both sets obtain very similar results suggests that the Bag-of-Notes method is indeed performing writer identification and not rhythm identification, as would be the case if the constrained set obtained significantly better results than the unconstrained set. 3.4 Evaluation metrics and baseline results for staff removal The goal of staff removal is to delete those pixels that only belong to staff lines. If a pixel belongs to both a staff line and a musical symbol, then the pixel is labeled as belonging to the symbol. Consequently, the staff removal algorithm must be careful when removing the staff line segments, because they should not remove those pixels belonging to music symbols. Next, we describe the evaluation metrics and some baseline results for staff removal purposes.
 Metrics. Different metrics have been used in the literature. For example, in [ 6 ], the authors use the error rate, segmen-tation error, and Staffline interruptions, whereas in [ 32 ], the authors propose to use the percentage of staff lines falsely de-tected and the percentage of staff lines missed to detect. How-ever, some of these measures are not very easy to compute. For this reason, we have chosen the pixel-based evaluation metric to get the quantitative measurement of the perfor-mance of staff removal algorithms. These measures are very well known, easy, efficient, and fast to compute. In this sce-nario, we consider the staff removal problem as a two-class classification problem at the pixel level. For each of the images, we compute the number of true-positive pixels tp (pixels correctly classified as staff lines), false-positive pixels fp (pixels wrongly classified as staff lines), and false-negative fn (pixels wrongly classified as non-staff lines) by overlap-ping with the corresponding ground truth images. The Preci-sion and Recognition Rate measures of the classification are computed as: Precision = P = Recognition Rate = R =
The third metric error rate E is computed as ( # means  X  X umber of  X , sp means  X  X taff pixels X  ): E = #misclassified sp
Method. For the sake of illustration, we have chosen one of our staff removal algorithms as the baseline results. The approach proposed in [ 7 ] is based on the criteria of neigh-boring staff components. It considers a staffline segment as a horizontal linkage of vertical black runs with uniform height, and then, it uses the neighboring properties of a staffline segment to discard the false segments.

Results. Table 4 shows the results of the staff removal algorithm using the proposed evaluation metrics and applied to the 12,000 distorted images. It must be noted that it does not obtain the best results in all cases with respect to the three evaluation metrics, showing that there is still room for research in this field. It should also be noted that these results are over the whole dataset and not only on the testing set, since this method does not require any training step. 4 Conclusions In this paper, we have described the CVC-MUSCIMA data-base and ground truth, which has been especially designed for writer identification and staff removal tasks. We have also described the evaluation metrics, partitions, and baseline results in order to ease the comparison between the differ-ent approaches that may be developed. It must be said that the main contribution of this work is the framework for per-formance evaluation, and for this reason, we have included some baseline results just for reference purposes. Concern-ing ground truthing, we have shown that, although ground truth generation is a time-consuming task (specially when it is manually generated), one can reduce the effort of ground truthing using some simple methods (e.g., using color cues, applying distortions to images, and carrying the ground truth through to the distorted images).

The database can serve as a basis for research in mu-sic analysis. The database and ground truth are considered complete at the current stage. However, further work will be focused on labelling each music note and symbol of the music score images for Optical Music Recognition purposes. References
