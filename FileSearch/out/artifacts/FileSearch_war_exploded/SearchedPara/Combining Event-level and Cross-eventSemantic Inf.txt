 The task of Event Relation Detection (abbr., ERD) is defined to determine se-mantic relation between event mentions, and aggregate distributed events in text to form event relation network. ERD is comprised of two separate tasks, Event Relation Identification (ERI) and Event Relation Classification (ERC) [14]. ERI intends to determine whether two events are relevant. ERC determines what types of relations occurred between the events. In this paper, we take our re-search focus on ERC.
 is a sentence or a clause that depicts a natural event, consisted of at least the trigger of the event and the closely related participants. See the following event mentions, for example, which are respectively by the trigger words attacked and wounded and died : Causality , which inherits the main relation type Contingency . As shown in Table level. In this paper, four top-relation types wiil be considered for the evaluation of ERC systems, including Contingency , Expansion , Comparison and Temporality . semantic-level relation analysis into the perception of logical relation among real historical events. In particular, we embed cross-event semantic features, along with inner ones of a single relation sample. By combination of the features, we can deal with the relation classification for the non-adjacent event mentions and even cross-document and cross-topic samples, such as the Comparison relation between the events  X  tsunami alarm in Hawaii  X  and  X  Many planes turn back to San Francisco  X . Pattern-matching method is one of the conventional approaches on ERC. Chklovski and Pantel [4] extracts pairwise events on the basis of manual designed lexical-syntactic pattern. Pantel and Pennacchiotti [11] propose a method based on Espresso Algorithm to construct patterns automatically, which somewhat im-proves the recall of pattern-matching method.
 ence. They are mainly inherit Harris distribution assumption that words in the same context usually hold the same or similar meaning [9]. Lin and Pantel [10] propose an unsupervised method relying on Harris assumption and dependency tree. The algorithm identifies grammatical relationships between words and con-structs dependency trees formed of the relationships between words. weak in capturing semantic aspects of events.
 lation recognition, which considerably promotes the development of relation de-tection tasks. However, we find that the method isolates the inner clues between two arguments and only takes each argument into consideration, which inevitably omits the mutual influence between two arguments. Figure 1 describes the architecture of our model, which primarily involves the following 4 components: (1) word embedding learning, which reveals the embed-ding vector of words in event; (2) frame embedding generation, where we extract frames from event to generate the frame embedding vector; (3) event-level and cross-event features extraction with SCNN, which exploits the inner semantics in and cross event; and (4) relation classification, which concatenates the generated features and outputs the candidate relation with highest confidence score. 3.1 Word Embedding Learning We choose the state-of-the-art model Skip-gram to pre-train the word embed-ding [2]. In our framework, w i  X  R d corresponds to a d -dimensional vector rep-resentation of i -th word in each event, 1  X  j  X  d . Figure 1 assumes that each word has size d = 4. 3.2 Frame Embedding Generation natural language processing problems [1, 3, 12, 14]. properties. If two event-event pairs share the similar scenario (which is comprised of a series of frames), they always hold the same relation [14]. Therefore, we use frame semantics as another important information to help detect event relations. Then, the frames will be mapped to a randomly initialized vector of dimension l , let f i  X  R l correspond to a l -dimensional vector of i -th frame: 3.3 Extracting Event-level and Cross-event Features with SCNN We combine word embedding w i and frame embedding f i to generate e i , where  X  is a concatenation operator, e i  X  R d + l . An event with n words is ex-tracted as the following matrix, lutional operations in detecting discourse relations. We adopt this SCNN method and for each column c in E take the following three convolution operations to capture event-level features,  X  event-level features : each column c , we further obtain the cross-event features by exploring max , min and avg operations on matrix { E 1 ,E 2 } ,  X  cross-event features : { E 1 ,E 2 } to generate a ( E 1 ), a ( E 2 ) and a ( E 1 ,E 2 ), hyperbolic tangent tanh to generate a hidden layer, 3.4 Relation Classification At last, we apply the softmax function upon the hidden layer to predict K -class classification, between y and gold relation g , and further define the objective function:  X  = ( w,f,l, X ,b ) is parameters to be learned.
 4.1 Datasets We utilize 968 event pairs [14] annotated on FrameNet-1.5 [8], and follow their annotation metric to annotate 4459 new pairs on GIGAWORD (LDC2003T05), both of which finally make up of our experimental datasets in Table 2. 4.2 Experimental Setup Word embedding is trained on large-scale data by word2vec toolkit. We empiri-cally adopt the same parameters in our experiments. Specifically, we set d =200, l =5 and batch=128. We apply stochastic gradient descent (SGD) algorithm to minimize J (  X  ), with learning rate lr=0.1 and momentum=0.9. Finally, we choose precision (P), recall (R) and F 1 -score (F 1 ) as evaluation metrics. 4.3 Comparison with state-of-the-art methods relations between pairwise events, which assumes that events with same scenarios share the similar relations.
 Training, which improves the classification performance by expanding training corpus with higher confidence unlabelled samples.
 relation recognition, which could be also applied to ERC task. In this system, only Event-Level(EL) features are taken into account, we set e i = w i in Eq. 3 and z = a ( E 1 )  X  a ( E 2 ) in Eq. 12, z,h  X  R 6 d .
 in Eq. 3 and z = a ( E 1 ,E 2 ) in Eq. 12, only take Cross-Event(CE) features into account, z  X  R 3 d .
 event features without using frame embedding features. Specifically, we set e i = w event features together with frame embedding features, we set e i = w i  X  f i as 4.4 Results and Analysis As shown in Table 3, we found the model SCNN (EL) performs better than Cross-Scenario and Tri-Training, which suggests that the shallow structure works well and using event-level features is beneficial to ERC.
 than Tri-Training and SCNN (EL). The main reason may be that considering cross-event features only might have left out the important information in each event. However, SCNN (CE) performs better than Cross-Scenario in general, which reveals that the cross-event features is also effective to some extent. in four relations and gets highest F 1 -score 44.71% in Expansion among all models, which sufficiently suggests that combining event-level and cross-event informa-tion in relation classification gains remarkable promotion. best result in Comparison , Contingency and Temporal in SCNN (EL+CE+Frame), which indicates that frame embedding is useful to represent the deep semantics of event relation. Table 4 presents the overall performance of six models using macro average measure, SCNN (EL+CE+Frame) model also achieves the best result. In summary, according to Table 3 and Table 4, our model undoubtedly gains the best result in ERC. In this paper, we exploit a novel method for event relation classification which combined embeddings. We further concatenate these convolutional features into that our model significantly outperforms the state-of-the-art methods.
