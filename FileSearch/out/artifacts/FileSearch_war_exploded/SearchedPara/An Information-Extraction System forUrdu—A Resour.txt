 SMRUTHI MUKUND and ROHINI SRIHARI State University of New York at Buffalo and ERIK PETERSON Janya, Inc.
 1. INTRODUCTION The increase in the amount of multilingual data in the form of electronic text has ignited a slew of efforts toward monolingual and crosslingual information retrieval. Specifically, Indic and related languages have received considerable attention in the past few years. For instance, FIRE [Mitra and Majumder 2008] (Forum for Information Retrieval Evaluation) is a new forum that concentrates on showcasing research in important Indian languages. Though FIRE focuses on languages like Hindi and Bengali, it does not cover Urdu. We argue that FIRE should consider research in Urdu, as this language has a large num-ber of speakers in the world (48,062,000 speakers in India alone as of 1997 [Ethnologue 2002]). Urdu has also gained political prominence due to its close cinating language to study given the ethnic and geographical diversity of its speakers.
 able insight into the cultural, political, and social conditions of that society. To facilitate information discovery, there is a need to work on advanced multilin-gual information extraction (IE) systems. Tools for word segmentation, part of speech (POS) tagging, named entity (NE) tagging, and shallow parsing provide the basic structural information needed for further analysis. In this work we attempt to develop an end-to-end IE system that provides all the above men-tioned tools for Urdu, a less commonly taught language (LCTL) [Janus 1997]. can address the challenges, we need to address an important question that of-ten arises while discussing natural language processing for Urdu. If Urdu and Hindi (a language commonly spoken in India) are treated as similar languages [King 1995], can the NLP tools developed for Hindi be applied to Urdu? Hindi and Urdu are two standardized forms of Hindustani, an Indo-Aryan language that contains several closely related dialects of northern India and Pakistan. 1 Urdu is considered to be very similar to Hindi as it shares its phonological, mor-phological, and syntactic structure with Hindi. Both these languages evolved from Sanskrit and share the common Khari Boli dialect. They are free word order languages and follow a general SOV (Subject-Object-Verb) structure. But despite the similarities, NLP tools developed for Hindi cannot be used as is for Urdu [Russell 1996]. The three main differences that hinder the direct use of Hindi tools for Urdu are the following. (1) Script difference. Urdu has the nastaleeq and nasq style of writing that is (2) Vocabulary difference. The vocabulary of Urdu is highly influenced by (3) Missing diacritics pr oblem. The problem of mi ssing diacritics in Urdu to generate accurate results (see Sections 6 and 7). Hindi script is space de-limited and hence does not encounter the problem of word segmentation. Since word segmentation is the precursor to any NLP task, it affects the accuracy of subsequent NLP tasks. Another interesting issue is that of missing diacritics. The addition or deletion of diacritics is based on the writer and is not always consistent. To make NLP modules such as POS tagger, NE tagger, etc., more accurate, these modules are trained on d ata normalized for diacritics. However, since Hindi does not suffer from the missing diacritic problem, Hindi taggers X  which consider token level features X  X hen used on Urdu fail to produce im-pressive results. But, using Hindi reso urces to generate suitable Urdu taggers that depend on syntactic information (e.g., shallow parsers) holds promise (see Section 7).
 annotated data. For English, the Penn Treebank corpus consisting of more than 4.5 million words is used to train a state-of-the-art POS tagger [Marcus et al. 1994]. The Chinese Treebank project [Xue et al. 2005] provides a well segmented, POS tagged corpus of more than 500,000 words for the Chinese language. Arabic is also gaining significant attention in the research commu-nity and an Arabic Treebank [Maamouri et al. 2004] consisting of part of speech and morphological annotations is available. The Linguistic Data Consortium has also released the Korean Treebank [Han et al. 2002]. An excellent compi-lation of resources is available at this Web site. 2 Unfortunately Urdu is not as privileged as these languages when it comes to the availability of annotated re-sources. Though there exists the Hindi-Urdu Treebank initiative [Bhatt et al. 2009], data is still not available for research. As of today, POS annotated data for Urdu is provided by the Center for Research in Urdu Language Process-ing (CRULP), 3 IIIT Hyderabad 4 and The EMILLE Project [Hardie 2003]. But these data sets are based on different tagsets. The same is true with the two NE annotated corpora provided by IJCNLP (2008) and Computing Research Laboratory (CRL). 5 Apart from the EMILLE dataset, other datasets are very limited in terms of size. Developing good taggers that use these limited re-sources is not a trivial task. Furthermo re, incorporating all these NLP tools into a single comprehensive system is a challenging task. 2. RELATED WORK Developing better methods for segmenting continuous text into words is impor-tant for improving the automatic processing of Asian languages [Wang et al. 2007]. For tasks such as machine translation and information retrieval, the fundamental text unit is a word token. All operations such as morphological analysis, semantic reasoning, etc., are performed on this word token. Sim-ilar to Chinese, Japanese, and Korean (CJK languages) [Choi et al. 2009], Urdu too has the problem of word segmentation [Durrani 2007]. Words in Urdu are written continuously without the use of word delimiters (white space, comma, quote, and period). Trying to retrieve word tokens from such text is non-trivial. The approaches used for word segmentation can be categorized as dictionary-based [Wang et al. 2007], machine-learning based [Gao et al. 2005], and transformation-based [Xue 2003]. Approaches that combine all three have also been explored [Sproat and Shih 2002]. In this work, we propose a technique that attempts to mark word segmentation boundaries by combin-ing Hidden Markov Model (HMM) based learning with dictionary lookups (see Section 5).
 that offers great benefits. Tasks that use semantic information such as senti-ment analysis, agent-target identification, role labeling, etc., consider the sig-nificance of the content words [Nicholls and Song 2009] for analysis. Shah and Bhattacharyya [2002], Kanaan et al. [2005], and Karimpour et al. [2008] have conducted studies that evaluate the importance of parts of speech for informa-tion retrieval. The POS information associated with a word helps to differenti-ate word senses [Wilks and Stevenson 1996]. POS taggers have been developed using primarily two techniques: by relying on only linguistic rules in an unsu-pervised manner [Brill 1992], and by using machine learning algorithms such as Hidden Markov Model (HMM) [Ekbal and Bandyopadhyay 2007], Maximum Entropy (MaxEnt) [Dandapat et al. 2007; Ratnaparkhi 1996] and Conditional Random Field (CRF) [Avinesh and Karthik 2007; Lafferty et al. 2001]. The tagging technique used and the efficiency of the resulting tagger depend on the tagset that is used. The granularity of the tagset chosen, in-turn depends on the task that it assists. We develop an Urdu POS tagger using a MaxEnt model, the details of which are discussed in Section 6.
 role extraction [Shamsfard and Mousavi 2008], and semantic role labeling [Punyakanok et al. 2005] are greatly benefitted by not only POS and NE information but also shallow parse information. Shallow parsing divides text into segments that correspond to certain syntactic units. Shallow parsers are commonly used to perform clause chunking and identifying noun phrases. These shallow parsers are also used to bootstrap a complete parser. In the literature, several techniques have been applied to perform shallow parsing. Ramshaw and Marcus [1995] use transformation-based learning. Daelemans et al. [1999] use memory-based learning. In the shared task for CoNLL-2001 [Erik et al. 2001] several techniques that use HMMs, memory-based learning, boosting, etc., were presented. The chunk and clause information in a sentence are represented by means of tags. Defining these tagsets is language dependent. Our approach to develop a shallow parser for Urdu is based on the resource sharing technique. Section 7 delves into the details of this method for this work.
 ment that belong to predefined categories such as names of persons, organiza-tions, locations, date/time, etc. This NLP task is very challenging as the named entities are infinite in number and often new entities are coined over time. In information extraction systems, accura te detection and classification of named entities is very important as the named entities help to extract knowledge from texts. NE recognition also finds application in question and answering systems [Moldovan et al. 2002] and machine translation [Babych and Hartley 2003]. Different techniques have been used to develop NE recognizers: rule-based recognizers [Farmakiotou et al. 2000], statistical recognizers such as BBN X  X  HMM-based IdentiFinder [Bikel et al. 1999], and New York University X  X  Max-Ent based recognizer, MENE [Borthwick 1999]. Using CRF [McCallum and Li 2003] for NE recognition in Indian languages [Ekbal and Bandyopadhyay 2009] has also shown promising results. Section 9 describes in detail the technique used in this work to develop an NE tagger for Urdu.
 guage analysis for Urdu, including assimilation of available resources for learn-ing and creation of lexicons for lookups. Our focus is to ensure that each of the text-processing modules makes complete use of the available resources and achieves state-of-the-art accuracy for Urdu. We address the challenges at each stage of text processing and provide suit able solutions to circumvent them. Our word segmentation module achieves an accuracy of 92.64% for space omission errors. The POS tagger we developed has an average F -measure of 88.9%. An F -measure of 89.99% is obtained for the shallow parser we developed using re-sources from Hindi. The NE tagger developed based on ACE [ACE 2005] guide-lines achieves an F -measure of 72.6%. As the next step in the NLP pipeline, we attempt transliterations for person names. These transliterations can aid systems for machine translation and question-answering. To this end, we also attempt to develop a morphological analyzer that can further aid tasks such as semantic role labeling (as shown by Pandian and Geetha [2009]), and word sense disambiguation [Mishra et al. 2009]. The modules at different stages in the NLP pipeline are explained in the sections that follow. But before we look at each module and the associated challenges, let us examine the techniques used to assimilate resources on the web for Urdu. Section 3 will also give the reader an idea of the size of the lexicons that we successfully procured for our system. 3. RESOURCES A large amount of text in Urdu is available on the Web which, when orga-nized and registered in a dictionary, can be extremely valuable. Mining the Web for transliterations and English translations of Urdu words and compil-ing them into a translation dictionary will significantly and positively affect the quality of cross-lingual NLP applications, such as machine translation and cross-lingual information retrieval. We have compiled several of the resources into well-categorized lexicons. These lexicons are used to assist POS tagging, NE tagging, and transliteration modules in our NLP pipeline.
 researchers that recognize it as a source of exceptional scale and utility. Many Wikipedia entries have descriptions in several languages. The use of Wikipedia in question and answering systems [Ahn et al. 2004; Buscaldi and Rosso 2006; Ko et al. 2007], in summarization [Baidsy et al. 2008], and in parallel corpora generation [Adafre and Rijke 2006] by exploiting its multilingual property has shown promising results. Richman and Schone [2008] utilize the multilingual characteristics of Wikipedia to annotate a large corpus of text with NE tags. Wikipedia resources for Urdu, though not significant, are growing rapidly. As of November 2009, Urdu Wikipedia had more than 11,500 articles making it a potential source for mining named entities. In this work, we have exploited the multilingual property of Wikipedia and mined Urdu Wikipedia 7 to generate NE lexicons with their respective English transliterations.
 tries such as Living people, Births, and Deaths can be used as filters to ex-tract person names into lexicons. Location, Geographical Location, City, and Country, etc., can act as filters for location entities. Similar categories exist for entities that refer to organizations. 8 Links are maintained between arti-cles on the same topic written in different languages across the Wikipedia do-main. We exploited this link structure that exists between English and Urdu to mine named entities. The titles of articles on the same topic across the two languages are extracted into a lexicon, and the English title is used as the translation of the Urdu title. The categories are organized based on the standards set by ACE for entity types. For example, categories such as  X   X   X  X  X   X  (sadr),  X   X   X  X  X  X  X  X  X  X   X  (siyasatdan) are grouped as PER Individual,  X   X   X   X  X   X   X  (mulk) as GPE Nation and  X   X   X  X  X  X  X  X  X   X   X   X  X sORG Media.
 translations for all named entities in articles related to Pakistan and Islam. These translation pairs are also mined and added to the lexicons based on the category filters. We have also mined several name databases on the Web, urduseek.com and apniurdu.com , which provide the most common Islamic names along with their English transliterations. These lexicons are used to generate a statistical name transliterator as we describe in Section 10. this method for the three main NE categories: Location, Organization, and Person. Other categories such as Facilities with subtags FAC Airport, FAC Building Grounds, and FAC Path have a total of 48 entries. Our lexicons are also categorized to indicate colors, Islamic months, Islamic weeks, time, directions, and common greetings. We have also maintained lexicons with names of famous personalities such as chief justices, prime ministers, presi-dents, army generals, and politicians of Pakistan (close to 200 entries). Translations for common Urdu words along with their most probable POS in-formation are also available across di fferent sources on the Web. Waseem Sid-diqi 10 (2008) compiled the first offline Urdu-English/English-Urdu dictionary with more than 24,000 entries where each entry is assigned the most probable POS tag with which it is associated. Lexicons are also provided by Computing Research Laboratory (CRL) as a part of their machine translation corpus for Urdu. Lexicons for parts of speech such as adjectives, nouns, and verbs are obtained from the Urdu POS corpus provided by EMILLE. These resources are compiled to one single format and grouped into probable POS categories such as Ga JJ, Ga NN, and Ga VB for adjectives, nouns, and verbs, respectively. Each category has approximately 3,000 words. These lexicons aid the POS tagging model that we describe in Section 6. 3.1 Semantex TM Architecture Each of the text processing modules t hat we present in the NLP pipeline are integrated into Semantex TM , a text processing tool provided by Janya Inc. 11 Semantex TM is a customizable platform for scalable information extraction. It reflects a hybrid paradigm, support ing both grammar-based and machine-learning components in a pipelined architecture. Documents are converted to an internal tree representation known as a tokenlist ; subsequent modules (lexical look-up, grammars, or statistical components) modify the tokenlist by adding new features to tokens, creating groups of tokens or links between to-kens. The underlying grammar formalism is a tree-walking automaton; this has evolved from an earlier finite state transducer model. Grammars are typi-cally regular expressions over token lis ts, but can access any part of the parse tree through special operators. The machine learning framework supports models such as MaxEnt and CRF. The pipeline can be configured for different extraction tasks, data sources, or languages. Language porting to Urdu has, for the most part, consisted of adding ne w data resources: lexicons, grammars, and statistical models based on machine learning. An IDE for development, access to the complete English IE system, integration with Lucene/SOLR, and an academic licensing program were additional reasons for it being chosen as the integration platform.
 cons as well as Semantex TM framework for seamless integration, we proceed to explain each of the NLP modules that go into the pipeline framework. 4. PREPROCESSING MODULE The first module in the NLP pipeline that comes even before the word seg-mentation module is the preprocessing module. This module consists of two subtasks: diacritics remo val and normalization.
 marked and their usage is left to the writer X  X  discretion. Even with the same writer, the markings are not uniform. The same is the case with machine-printed Urdu text. Though the virtual keyboards provided for Urdu have op-tions to include diacritics like zer (U+0650) and zabar (U+064E), these options are not commonly used while typing. So a word such as  X   X   X   X  X  X   X  (qabil  X  having potential) which contains a zer above  X   X   X   X  to indicate the vowel  X  X  X , is often writ-ten as  X   X   X  X  X   X , without the zer diacritic. But several users exploit the fact that the virtual keyboards have the option of entering the diacritics. It is essential to maintain consistency throughout the text. One approach to attain consistency is to restore the diacritics. The process o f restoring diacritics is a very compli-cated task. Lexicons, annotated corpora used for training, and other resources contain words whose diacritics have to be restored as well. This adds to the complexity of the task. Another method is to consider complete removal of all diacritics. We chose the latter method and applied it to lexical data, training data, as well as incoming data and removed all diacritics ( zer, zabar, and pesh ) present.
 Unicode of the characters consistent. Several characters in Urdu have different orthographic forms and these variations cause discrepancies in NLP. UTF-8 standard has multiple representatio ns of several characters to accommodate cross-lingual variants. However, most writers tend to use these variants as they please without a consistent representation. For example, character  X  has two representations, Unicode value U+0622 and also Unicode values U+0627 + U+0653. There are several other ambiguous cases such as the use of  X  vs.  X  ( Yeh vs. Alif Maqsura )andtheuseof  X  vs.  X  ( Heh vs. Taa Marbuta ). It is necessary to convert different representations to one standard form. tions of data to a consistent underlying normal form. CRULP has developed their own normalization tool [CRULP 2007] that provides three different op-tions for normalization: Normalization form D (NFD), Normalization form C (NFC), and Normalization form KD (NFKD). These three options are based on the two equivalences defined by the Unicode Normalization standard: canon-ical equivalence and compatibility equivalence. The details can be found on the CRULP Web site. 12 In this work, we use a variation of the Normalization form C (NFC) where a decomposed representation of a character is replaced with its composed representation. When applied on a set of 100 Urdu articles downloaded from BBC Urdu 13 and Jang daily 14 (about 12,000 words), we found nearly 90 instances where character representations had to be normalized. We also noticed that a majority of these inconsistencies were in words that were proper nouns. This indicates that though the percentage of inconsistencies is small, normalization has to be perfor med as these errors can propagate and affect NLP tasks that come later in the pipeline (POS, NE, etc.). 5. WORD SEGMENTATION Word tokens are the fundamental blocks on which NLP tools such as POS tag-ger, NE tagger, etc., are developed. Generating these word tokens in some lan-guages essentially maps to simply using space and special characters (;, ( ) etc.) as word delimiters (e.g., English, Hindi). However, there are other languages in which determining word delimiters to generate tokens goes beyond determin-ing just the space and the special characters (Thai, Chinese, Urdu, etc.). Urdu is one such language and segmenting a sentence to generate word tokens in this language is not trivial. Urdu suffers from both space insertion and space deletion problems. These issues are solved by the reader who, while reading, groups the characters into words based on contextual knowledge. However, providing such contextual information for a statistical learning algorithm is an extremely difficult and expensive task.
 face similar segmentation challenges. Traditional techniques such as longest and maximum matching strings depend on the availability of a lexicon that holds all morphological forms of a word. Such lexicons are not readily available for Urdu. Statistical techniques apply probabilistic models (usually bigram and trigram models [Papageorgiou 1994]) toward solving the word segmentation problem. Feature-based techniques [Meknavin et al. 1997] that use POS infor-mation for tokenization, consider the context around a word for specific words and collocations. There are other models that generate segments by consider-ing word level and syllable level collocation [Aroonmanakul 2002] when devel-oping a learning model for word segmentation. Statistical models that consider character-based, syllable-based, and word-based probabilities have also been shown to perform reasonably well. The word segmentation problem for Thai was solved by Pornprasertkul [1994] using the character-based approach. to yield good results as modeling all erroneous cases is a challenge for Urdu. Space insertion problems are found generally around abbreviations, affixes, suffixes, and proper nouns and space deletion problems are found generally around verbs, proper nouns, auxiliary verbs, and pronouns, making the seg-mentation issue highly dependent on the morphological and lexical features. A mixed model that considers morphological as well as semantic features of the language facilitates better performance as shown by Durrani and Hussain [2010]. They proposed a word segmentation model that uses a lexicon for proper nouns and a statistical model that trains over the n -gram probability of morphemes. Maximum matching string technique is then used to generate word boundaries of the orthographic wo rds that are formed. The generated segments are ranked and the best one is accepted. Space insertion and space deletion problems are separately dealt with. Durrani and Hussain X  X  [2010] so-lution for space insertion issue associ ated with compound words and affixes is highly dependent on lexicons. This approach has a combined error detection rate of 85.8%.
 statistical approach [Pornprasertkul 1994] and grammar rules with lexicon lookups to generate word boundaries. A character in a word can occur in three positions: initial, medial, and final. In Urdu, there are some characters that occur only in certain positions (e.g.,  X   X   X   X  noon gunna occurs only at the end of words). The bigrams of each of these characters, based on their positions, are obtained by training over a properly segmented training set. For unknown characters, unknown character models for all the three positions of occurrence are also trained. The following probabilities are estimated and maximized at character level using the Viterbi algorithm. (1) Probability of character k being in medial position given character k-1 is in (2) Probability of character k being in final position given character k-1 is in (3) Probability of character k being in final position given character k-1 is in (4) Probability of character k being in medial position given character k-1 is in (5) Probability of character k being in initial position given character k-1 is in
Suitable word boundaries (space delimited) are generated using a combina-tion of morphological rules, lexicon lookups, bigram word probabilities, and bi-gram HMM character model. Each word t hus formed successfully is verified for morphological correctness (rule based) using a sliding window approach. If the word is not valid morphologically, then the window is moved back over three characters and at every step the valid ity of occurrence of the word is noted. Similarly, the window is moved three characters ahead and the validity of the word is verified. All words formed successfully are taken and further processed using a language model that considers the bigram occurrence for each word. The unknown word probability is also accounted for. The word with maximum probability is taken as valid in the given context. We also note the number of times transitions occur from syllable sets with consonants to syllable sets with vowels in a word. This number of transitions in a valid word cannot be more than four (decided after observing the errors in the corpus). The Viterbi algorithm used for decoding also accounts for an already existing space. here. A word boundary is formed when: (1) the word ends with  X   X   X -nun gunna , (2) the character transitions over to digits, (3) punctuations marks are encountered (-is also included), (4) if current character is  X  X lif  X  and the previous character is  X  X e X  -bari ye ,then (5) the word phonemes repeat (reduplication within the orthographic words). (1) no two  X  X e X  -choti ye come back to back; and (2) no characters occur in detached form unless they are initials or abbrevia-
In order to train the character transition model and the language model, we use the POS-tagged data set released by CRULP (see Section 6). This data set is well segmented and has a wide coverage of pronouns, verbs, and auxiliaries. We tested our technique on a fairly small data set of 50 sen-tences (  X  1,700 words) obtained from BBC Urdu. Most errors in our test set are associated with the space omission problem. Many of the auxiliary verbs are combined with the verbs. For example,  X - X   X   X  X   X   X  X   X   X  ki gayi hai (has been done) is written as  X - X  X  X  X  X   X   X   X   X  has been done .Manyof the pronouns are combined with case markers. For examples,  X   X   X   X  X   X   X  us ki (his) is written as  X   X  X  X  X   X   X  his . The space insertion problem is predom-inantly around the abbreviations [Durrani and Hussain 2010]. For example,  X   X  X   X   X  X   X   X  US is written as  X   X  X   X  X   X   X   X  US (U.S. is a country name and should be treated as one word). In order to handle these errors, we maintain POS lexicons for most common auxiliary verbs, pronouns, case markers, and deter-miners. We then apply our algorithm that generates suitable word boundaries using a combination of morphological rules, lexicon lookups, bigram word prob-abilities, and bigram HMM character model.

We report our analysis only for the space omission problem. Our test set has 136 instances of space omission problems and our algorithm was able to suc-cessfully segment 126 instances, thus achieving 92.64% accuracy. This accu-racy is close to Durrani and Hussain X  X  [2010] method (94.5%) of using bigrams for space omission. Their best approach generates 96.83%. Unfortunately we could not run our method on the same test bed as that of Durrani X  X  due to the unavailability of datasets. However, we believe that this difference in the ac-curacies is because of the following re asons. 1. There are 18 proper nouns in the test set that have the problem of space omission. For example,  X  X eneral Musharraf  X  is written as one word. Our lexicons do not account for affixes and proper nouns, and 2. In Urdu, some words can be written in more than one way. This mostly depends on the diacritics and ambiguity between bari and choti  X  X e X . These variations affect the word segmentation.
 to maintain a format in their rendering of Urdu and to eliminate many issues arising due to improper word segmentation. The Urdu news wire data from BBC clearly is fairly clean in terms of w ord segmentation and the few errors that remain can easily be removed by simple heuristic rules. But there is the issue of special characters (shown in Table II) attached to the word body (e.g.,  X ( (  X  X  X  X   X  ) X  X astobe X ( (  X  X  X  X   X  ) X   X   X (Chicago) X  , with a space between the word and the parenthesis) that has to be considered. Hence we use a simple tokenization model that delimits the words based on spaces and the characters mentioned in Table II. This forms the first stage in the Urdu NLP pipeline. 6. PART OF SPEECH TAGGER There are several words that have different meanings based on contextual in-formation. For example, the Urdu word  X   X  X  X   X  means  X  X race X  when it is a noun, and  X  X ay X  when it is a transitive verb. POS information is essential to disam-biguate the meaning of words. For any task that involves semantic analysis, assigning POS information to the token words becomes the primary task. pends on the domain of the data set used to train the model but also on the tagset used for annotation. We consider three main Urdu tagsets in our discus-sion: U1 tagset, Sajjad tagset, and CRULP tagset.
 tags. The tags are differentiated based on the tense, gender, and number infor-mation. However, the EMILLE corpus is the only corpus that uses this tagset and the size of this corpus is insufficient for simple statistical methods to learn the morphological dependencies for the large number of tags present in this tagset. Hence, we disregard the U1-tagset in our analysis.
 tagset) contains 42 tags and is designed based on the work done by grammari-ans Schmidt [1999], Haq [1987], Javed [1981], and Platts [1967]. In this tagset, the pronouns are categorized as personal, reflexive, relative, adverbial, and kaf (a pronoun that adds interrogative property in a sentence). Nouns are catego-rized as common nouns and proper nouns. There also exists a special tag for negative words (e.g., no, not, non). However, the Sajjad tagset has only one tag for verbs, except for auxiliaries that show aspect (AA) and tense (TA) informa-tion about the verb (for more details please refer [Sajjad and Schmid 2009]). CRULP [Hussain 2008; Sajjad 2007] has annotated a 110,000-word corpus us-ing this tagset. This corpus is not freely available for download. The Sajjad tagset was discarded due to lack of fine-grained analysis for verbs. the English-Urdu parallel corpus at CRULP shows promise. It follows the Penn Treebank guidelines [Santorini 1990] and has several tags that are assigned to verbs on the basis of their forms and semantics in addition to the tags for aux-iliaries for aspect and tense. It also has two shades of proper nouns that help to identify phrase boundaries of compound proper nouns. A WALA tag is assigned to every occurrence (inflected and uninfle cted) of the word  X  X ala X . A wala word often means  X  X he one X  and is influenced by the semantics of the phrase that it follows, for example,  X  X he enlightened one  X . Due to the colloquial usage of the wala word, a new tag is assigned to acco mmodate its several morpho-syntactic variations. Muaz and Khan [2009] give a more detailed explanation of the wala word and its usage. For details of the CRULP tagset, please refer to the CRULP Web site. 15 We use this tagset in our work for two reasons: (1) the availabil-ity of annotated corpus that uses the CRULP tagset, and (2) the existence of the light verb tags VBL and VBLI which is helpful for semantic role labeling [Sinha 2009]. 6.1 Using Hindi POS Tagger for Urdu Before we explain the process of generating an Urdu specific POS tagger, we would like to justify our claim that a Hindi specific POS tagger will not yield good results when applied on Urdu.
 ing approach. CRF++ tool 16 is used to facilitate this training. Hindi POS annotated data set is obtained from IIIT Hyderabad and is tagged using the tagset released as part of the AnnCorra standards [Bharati et al. 2006]. This data is represented using SSF format [Bharati et al. 2002]. Since represent-ing Urdu data in this format is not trivial, we apply a simple transliteration on the Hindi data set and generate a representation that Urdu data can also be transliterated to. For example,  X  X ka bAra kaswurabA gAMXI bahuwa bI-mAra ho gaIM X  is mapped to  X  X k bar kasturaba Gandi bahut bimar ho gain X . Urdu written in nastaliq form is also changed to a similar representation. For example,  X   X  X  X  X  X   X   X   X  X  X  X  X   X  X  X  X  X   X   X  X   X  is represented as  X  X hkaago ki gdmyn thytr myn X . Clearly this leads to a mismatch in vocabulary as Urdu suffers from the missing diacritics problem, whereas Hindi does not. We applied the dou-ble metaphone 17 algorithm on each transliterated word in both data sets and considered the pronunciation information as a feature in our learning model hoping that this in turn would compensate for the missing diacritics problem. (Results without this pronunciation information are bad with an overall F -measure of 38.6%). The CRULP tagset was reduced to AnnCorra tagset. All tags from CRULP tagset found suitable tags in the AnnCorra tagset. Most tags were just a one-one mapping and the subcategories for nouns and verbs were collapsed to NN and VM, respectively. The overall F -measure for the Hindi POS tagger on Urdu corpus is 55.5%.
 main tags: Nouns (NN), Adjectives (JJ), Verbs (VM), and Adverbs (RB). We notice that several of the nouns are spuriously marked and many are missing. Adverbs are never marked. Most adve rbs are marked as nouns (NN) or verbs (VM). The auxiliary verbs are marked as adverbial particles (RP).
 when words are considered as features, many of the open class words in Urdu become unknowns for the Hindi tagger; (2) Hindi tagset is not well defined and cannot account for a lot of morphological inflections that occur in Urdu due to the Persian influence on Urdu; (3) the missing diacritics problem in Urdu affects accurate transliterations, and (4) several nouns and adjectives in Hindi are derived from Sanskrit while this is not true for Urdu. This clearly indicates that the effort involved in developing tagsets and POS taggers specifically for Urdu is not futile. 6.2 Urdu-Specific POS Tagger Muaz and Khan [2009] trained a TnT Tagger [Brants 2000] and a Tree Tag-ger [Schmid 1994] on two corpora of Urdu tagged with different POS tagsets (Sajjad tagset and CRULP tagset). TnT tagger on Urdu corpus with Sajjad tagset has an accuracy of 93.01% that is comparable to the work done by Sajjad and Schmid [2009]. The accuracy reported on Urdu corpus with CRULP tagset using Tree tagger is 90.49%. In this work, we use two other learning algo-rithms: CRF and MaxEnt. The annotated corpus 18 contains sentences trans-lated from the Wall Street Journal (WSJ) set of the Penn Treebank corpus. This is tagged with the CRULP tagset and has a total of 3,325 sentences. A tenfold cross-validation test performed on this corpus using the CRF approach gener-ates an average accuracy of 88% with a maximum of 90.57% and a minimum of 81.1%. The MaxEnt approach also produces comparable results with an aver-age accuracy of 88.9%, a maximum of 90.62%, and a minimum of 81.7%. Out of the ten folds, the fold with the minimum accuracy has the maximum number of unknown proper nouns (equal to 62). We report all our findings on this fold as the modifications that we make show significant improvement on this set by boosting its accuracy to 84.1%.
 tences from the 01 folder of WSJ (Penn Treebank) with a total of 2825 sen-tences. The remaining sentences belong to the test set (about 500 sentences). Since the training time of the MaxEnt model is significantly lower than the training time of the CRF model and the results comparable, MaxEnt model is used to perform further experiments. A simple MaxEnt model which uses the current word, the previous word and the next two words as features, gener-ates an accuracy of 81.7% as shown in Column 2 of Table IV. Most of the er-rors involve the verb tags (VB, VBL, VBI, VBLI) and the auxiliary tags (AUXT, AUXA). The major confusion with automatic tagging is to differentiate the VBL (light verb) and the VB (verb) tags. Th ough considered complex predicates [Sinha 2009] VBL words are syntactically very similar to VB words. Several of the cardinals were tagged as nouns owing to syntactic similarity. Many of the missing NNCM (nouns after case markers) were tagged as NN (nouns). Many of these errors are due to the lack of fine-grained features in the learning model. We worked towards improving th e efficiency of this tagging model by including lexical features. The gazetteer tags (Ga NN, Ga VB, Ga JJ) from lex-icons compiled from different sources (see Section 3) are assigned to the data by performing lookups and these tags are used as features in the learning model. We also apply several heuristic rules (see Section 6.3) that help in correcting inappropriate tag assignments. The final result obtained has an F -measure of 84.1% as shown in Column 3 of Table IV. 6.3 Heuristic Rules for POS Correction (1) Our model tags all the Question Words (QW) such as  X   X   X  X   X -kya as pronoun (2) If the word is  X   X   X  X   X  X  kya and the previous tag is an adjective (JJ) and the (3) There were spurious instances where proper nouns (NNP) were tagged as (4) All valid cardinals were tagged as nouns or proper nouns by the model. This (5) All symbols are assigned the symbol (SYM) tag. (6) WALA tag is applied for all wala words.
 of VBL, VB, VBI, and AUXT tags. NN and NNCM are also marked with higher accuracy. These improvements, though minor, impact the performance of NE tagging (see Section 9).
 Section 9.1.1). Hence the next step in our NLP pipeline is to provide shallow parsing information for Urdu data. 7. SHALLOW PARSER Shallow parsing gives non-overlapping phrase boundaries that are used to identify the phrase structure in a sentence. Algorithms for shallow parsing use syntactic rules to perform detailed semantic analysis such as role label-ing [Punyakanok et al. 2005] and machine translation [Garrido-Alenda et al. 2004]. Linguistic analysis at the syntactic level is concerned with how words are arranged in a construction (sentence, phrase, etc.). The position of a word and its role in a construction offers great insight in understanding the meaning of the construction.
 a language such as Urdu. Rizvi et al. [2004] developed a rule based shallow parser (or chunker) for Urdu using the linguistic characteristics of morpho-logically closed classes. Their method uses heuristic rules that group chunks based on the closed class words. The linguistic nature of the closed class words influence the joining of neighboring words to these chunk groups. They also provide a complete parser for Urdu which is rule based and builds on the chunks that are formed. This method has not been quantitatively evaluated to determine its effectiveness. However, considering the complexity of the lan-guage and the dependency of the method on heuristic rules, formulating a complete unambiguous set of linguistic rules that cover the entire language is very challenging and involves considerable amount of effort. In the literature, shallow parsing using statistical techniques has also shown promising results. Techniques based on HMMs and CRFs have been very successful in generating shallow parsers for other languages like Chinese [Liao et al. 2006] and Hindi [Awasthi et al. 2006]. To the best of our knowledge, at the time of writing this article, there is no data freely available which is annotated with chunk information for Urdu. Since developing a statistical learning model requires access to annotated data, alternate techniques based on resource sharing is considered.
 chunk tags are language dependent as they depend on the syntactic structure of the language. Chunk tags, though not exclusively designed for Urdu, have been formalized for Indian languages as a part of the AnnCorra standards [Bharati et al. 2006]. A chunk tag depends less on a word itself and more on the POS cat-egory associated with the word. Since Hindi and Urdu share a similar syntactic structure, borrowing Hindi tags for us e with Urdu seems logical and promis-ing. This idea of adapting resources from a language that is more privileged to a language that is less privileged is not new. Singh and Surana [2007] con-ducted studies that help in estimating the cost involved in borrowing resources. They show that, based on the task involved and the kinds of similarities consid-ered between the languages, it is acceptable to depend on a linguistically close and resource rich language for analysis. Singh and Surana [2007] proposed different methods for comparing similarities between languages. Their meth-ods calculate the surface similarity, contextual similarity, and distributional similarity to compare similar languages.
 coding based on language similarities. The measures they consider range from simple log probability difference to more complicated measures like the JC measure [Jiang and Conrath 1997]. We conducted a simple experiment to jus-tify the syntactic similarity between Hindi and Urdu. To compare only the syntactic similarity (not the vocabulary), the RE measure [Singh 2006] is suffi-cient as we need to measure only the relative entropy between the POS transi-tion probability distributions. Four different corpora, each with approximately 50,000 words X  X wo for Hindi, one for Urdu and one for English X  X re used. The corpora are tagged using a condensed tagset that marks nouns, adjectives, pro-nouns, adverbs, verbs, conjunctions, interjections, and prepositions. The tran-sition probability of these syntactic categories for each corpus is calculated. Words that could not be assigned to any of the above mentioned syntactic cate-gories are assigned a probability of 1 co unt ( POS tags ) as such words influence the syntactic difference between the languages.
 POS transitions. We see that the syntactic divergence between Hindi and Urdu is only slightly greater than the syntactic divergence between Hindi and Hindi. However, the syntactic divergence between Hindi and English is much greater than that between Hindi and Hindi. We can now conclude that Urdu and Hindi have very similar syntactic structure and the chunk tags from Hindi can safely be borrowed over to Urdu. No te that this experiment only justifies the syntactic similarity between Hindi and Urdu and not the similarity over the vocabularies.
 of words: open class and closed class. Open class words, or content words, such as nouns, verbs and adjectives, are the main bearers of meaning in a language. They contain crucial semantic information and provide the building blocks needed to comprehend the overall sense of what is spoken or written in a sentence. Closed class or function words such as articles, conjunctions, and prepositions support syntactic analysis of a sentence. These words are relatively devoid of meaning and primarily serve a syntactic role in language understanding. They co-determine the syntactic relations between open class words, thereby making word combinations interpretable. We can consider this distinction between open class and closed class words as a basic reflection of the separation between semantics and syntax. This distinction motivated us to consider only the role of closed class words in proximity to POS categories of open class words in generating an effective Urdu specific chunker. 7.1 Using Hindi Chunker for Urdu The tagset for chunking as defined by the AnnCorra standards is show in Table V. An obvious approach is to apply a Hindi specific shallow parser on Urdu data. A Hindi chunker with five-fold cross-validation accuracy of 91.2% was trained using CRF learning approach on the POS and chunk annotated data provided for Hindi (IIIT Hyderabad corpus). The annotated data set con-sisting of about 20,000 words were divided into training and testing sets (ap-proximately 15,000 and 4,000 words, respectively). Features used for training are current word, current POS tag, bigram word, and POS tag probabilities. Miss  X   X  NNP  X   X  NNP UNK NNP ka  X  X  CM  X  X  CM  X  X  CM kirdar  X  X   X   X   X  NN  X  X   X   X   X  NN UNK NN When this chunker was applied on Urdu data (with transliterations similar to what is done in Section 6), the results were not impressive. Unfortunately, due to the lack of gold-standard annotated data for Urdu, the results of this chunker cannot be quantified for Urdu. However, example 1 and its explana-tion in Table VI should give the reader a fair idea of the issues associated with applying a Hindi chunker on Urdu data.

Example 1. - X  X   X  X  X   X  X  X  X  X  X  X  X   X  X   X  X  X  X  X   X  X  X   X  X  X  X   X  X  (Miss Haag Elianti ka kirdar ada karti hai).
 ple 1. Column 3 of Table VI shows the assignment of chunk tags obtained by applying a Hindi chunker on Urdu for Example 1. The discrepencies are (see Section 6.1) due to the vocabulary difference between Hindi and Urdu. Since all the word tokens were also considered as features while training the learn-ing model, several of the Urdu specific words are considered unknown words and the chunk boundaries are not marked accurately. Several of the chunks that need to be marked NP (noun chunk) are marked VGNN (gerunds) in the context of a noun POS tag followed by a verb POS tag (kirdar and ada). Chunk boundaries for proper nouns are also not accurate. These errors indicate that the Hindi chunker has to be modified to deal with Urdu related anomalies. 7.2 Urdu-Specific Chunker Chunking is highly dependent on the syntactic structure of a sentence. Closed-class words, as mentioned before, support the syntactic analysis of a sentence. Intuitively, training a learning model with only the closed class words for chunking should yield good results. Both Rizvi et al. [2004] and Stanislav [2003] state the importance of closed-class words over open-class words for the purpose of chunking. Stanislav [2003] shows that the words that commonly match between Urdu and Hindi are closed-class words. We developed a shallow parser by training a CRF learning model on a reduced corpus of Hindi shallow parsed annotated data. This model is then applied on Urdu to generate the chunk tags.
 case markers, postpositions, possession markers, conjunctions, interjections, auxiliaries, pronouns, and question markers X  X s created. Only the training its original format. We train a CRF learning model which considers word to-ken features only if the token belongs to the closed class group. The bigram probabilities of word tokens around the closed class words and the POS tags associated with them are also considered as features when training the model. The five-fold cross-validation accuracy of this model is 89.99% which is close to the accuracy obtained (91.2%) when the f eatures of open class words are also considered for training, thereby indicating the potential of this technique (see Column 3 of Table VI).
 the closed-class words in the reduced corpus were translated to Urdu using a Hindi-Urdu dictionary (in most cases, this process is a simple translitera-tion as closed-class words in Urdu are the same as in Hindi [Stanislav 2003]). However, there are some words that require a dictionary lookup. For example, Hindi word  X  X riman X  to Urdu word  X  X anab X . This was not a difficult task as the set of closed-class words is limited and hence the replacements were minimal. Another CRF learning model was trained on this set to generate an Urdu spe-cific chunker. We perform a task based evaluation of this Urdu specific chunker and show that the F -measure of an NE tagger (see Section 9) improves from 71.4% to 73% when these chunk tags are used as features in training the NE tagger (see Section 9.1.1). 8. MORPHOLOGICAL ANALYZER Morphological analysis of words is a process in which words are studied for their structure based on the different forms that they represent. Usually, mor-phological analysis is performed to aid POS tagging [Agi et al. 2008]. How-ever one cannot rule out the usefulness of detailed morphological analysis for more advanced NLP tasks like machine translation [Stymne et al. 2008]. The CRULP POS tagset used by us (in Section 6) is fairly comprehensive. It cov-ers all the top level POS categories including more detailed analysis for verbs (light verb, infinitive verb) and nouns (prepositional noun, combined noun). This tagset does not account for gender, case, number, and tense inflections for nouns, verbs, and adjectives. Information about these inflections, if present, can benefit several tasks like NE tagging and semantic role labeling. In fact, some of the heuristic rules applied during the post processing stage of the four stage NE tagger that we developed for Urdu (Mukund and Srihari [2009] and Section 9.1) use the gender information of nouns for location entity correction. Several bootstrap techniques and transfer learning techniques used for re-source poor languages can effectively use inflection information (e.g., attempts to generate annotated data for resource poor languages using cross-lingual pro-jections can yield promising results if detailed morphological inflections over verbs are considered). These benefits of detailed morphological analysis moti-vated us to integrate one such module to our NLP pipeline.
 lexical categories to which a word belongs, along with formulating rules that categorize these words to their respective lexical categories. Urdu and Hindi have very similar grammatical structure . However, there are a few significant differences that prevent the usage of Hindi morphological analyzer for Urdu. Following are some of the differences.  X  X he phonology, vocabulary, and writing styles between the two languages differ.  X  X ostpositions are treated as bound morphemes after pronouns in Hindi, but as separate words in Urdu (Wi kipedia Urdu Grammar page 19 ).  X  X ariations associated with the third person form are another grammati-cal difference between Hindi and Urdu. yah  X  X his X /ye  X  X hese X /vah  X  X hat X /ve  X  X hose X  is the literary set for Hindi while ye  X  X his, these X /vo  X  X hat, those X  is the set for Urdu and spoken (and also often written) Hindi (Wapedia Urdu Grammar, page 12).
 of the Urdu ParGram project [Butt and King 2002]. Humayoun et al. [2007] have also performed Urdu morphological analysis using functional morphology. Both methods begin with an attempt to determine the top level POS category (noun, verb, adjective, preposition, adverb) and later, based on heuristic rules, account for inflections. However, both analyzers have not been quantitatively evaluated. Providing a quantitative performance measure for a morphological analyzer is a hard task as manually annotating a corpus that accommodates all inflectional variations is difficult. Since both methods are rule based, qualita-tive analysis of the rules helps to determine the performance of the analyzers. Humayoun et al. [2007] documented all the rules used to mark the top level tags and the inflections associated with them. However, these rules produce multiple categories for each word and hence multiple inflections which require a separate model to perform disambiguation. The quality of these rules is su-perior as they are modeled based on the Urdu grammar analysis done by well known grammarians, Butt and King [2004] and Siddiqi [1971]. We used a vari-ation of the work of Humayoun et al. [2007] to develop our Urdu morphological analyzer. In our approach, the lexical categories are obtained by first running the statistical POS tagger (see Section 6) where suitable POS tags are assigned to the words. Our analyzer is only used to determine the inflections for words that already have POS labels. As our morphological analyzer is also rule based errors, if present, will be those p ropagated from the POS tagger.
 tion. The underlying automaton corresponds to a tree walking automata. The rules however are regular expressions on the tokenlist (see Section 3.1). Thegrammarrulesarewrittenbyconside ring token descriptions and combin-ing them with regular expressions. Token descriptions are collections of con-straints that individual tokens must satisfy where each word is represented by a token. The basic operations in a rule are (1) tagging: POS tagging and NE tagging, (2) chunking: shallow parsing, and (3) linking: relationship identification.

Example 2. [:: [ \ p { Alphabetic } ]+  X  X  \ b::/t (NNP | NNPC)  X  NeOrg  X  NePer &lt; NeLoc &gt; ]
The rule in Example 2 is used to change the NE tag of all those words that are marked proper nouns (NNP or NNPC) and have suffix ending with  X   X  X   X  (pur).  X  NeOrg and  X  NePer imply the removal of these tags (if they exist on the word) and assigns &lt; NeLoc &gt; tag to the word. (Words such as Nagpur, Jaipur, etc., are tagged NNP and have suffix ending  X  X ur X . These are considered Loca-tion Entities).

Srihari et al. [2008] discuss the architecture and the grammar framework of Semantex TM in detail. We have used the grammar toolkit of Semantex TM to develop the morphological analyzer for Urdu. The advantage of using this toolkit over other toolkits (XFST [Xerox Finite State Tool]) is that our analyzer can be seamlessly integrated into the Urdu NLP pipeline framework that also uses the Semantex TM platform. Humayoun et al. [2007] provide rules for de-termining nouns, verbs, adjectives, adverbs, and pronoun inflections. In the following subsections, we briefly explain the different inflections considered by Humayoun et al. [2007] in context of our implementation. 8.1 Nouns Most nouns are identified based on post-positions (clitics). Nouns are inflected in number and case. They can be singular or plural. They also inflect on gender. The Urdu POS tagger developed by us (see Section 6) uses the CRULP tagset that successfully identifies four variations of nouns: noun (e.g.,  X  X   X   X  ladka  X  boy), combined noun (  X   X  X  X   X  X   X  X  bara-i-rast  X  direct), prepositional noun (  X  X  X   X  an-dar  X  inside), and proper noun with an F -measure of  X  70%. We use heuristic rules to mark the various inflections on these nouns. Humayoun et al. [2007]
Vocative  X  X  (ae) ae + Oblique or modified  X   X   X   X  !  X   X  X  consider nine cases for nouns: nominative, oblique, ergative, accusative, da-tive, instrumental, genitive, locative, and vocative. All nine cases are identified based on the clitic information. Each case (Table VII) is programmed as a rule and applied over each word token identified as a noun by the POS tagger. how genders are identified in nouns based on letter endings. For each of the rows in the table, case information is marked based on the clitics. For a com-plete list please refer Humayoun et al. [2007]. 8.2 Adjectives Humayoun et al. [2007] account for two types of inflections in adjectives. One type of adjective inflects in case, number, and gender and the other inflects in degree (positive, comparative, and superlative). Gender information is taken into account to mark the first type of inflect ion in adjectives. No inflections are associated with the masculine form of adjectives if they do not end with  X  (alif), for example,  X  X  X  X  X  X  X  (khoobsurat  X  beautiful). Feminine forms generally end with  X  (choti ye), for example,  X  X  X   X   X   X  (sultani  X  monarchic). Masculine forms end with  X  (alif) or  X  (bari ye), for example,  X   X  X  X  X  X  X  (harabara  X  magnificent). In order to identify degree inflections, the phrase used before the adjective plays an important role. Adjectives with a positive degree do not have any intensifiers as affix phrases. However, if the word is preceded by phrases like (bohot se, thoda zyada etc.) then the degree is comparative. Affix phrases such as (sab se, bohot zyada etc.) determine the superlative form of the word. The rules that help to mark these inflections are applied on the JJ (adjective) tagged words only. 8.3 Verbs Humayoun et al. [2007] consider tense, mood, aspect, gender, and number in-flections of verbs and the rules take all of these factors into account. In our implementation we consider only the tense information. The rules used by our analyzer to identify verb inflections are shown in Table IX.
 The POS tagset that we use has two infinitive tags for verbs: VBI and VBLI. VBI is used to indicate the infinitive form of verbs and VBLI to indicate the infinitive form of light verbs: These two tags play a very important role in marking infinitive inflections for verbs (Table X). 8.4 Pronouns Pronouns inflect in number, person, gender, and case. But these inflections are not regular and sometimes may show no inflections. Hence developing general-ized rules for pronoun inflections is difficult. But Humayoun et al. [2007] have compiled case by case rules to handle pronoun inflections based on the most common pronouns.
 ye/wo X  are known to inflect in case and number. Clearly, words like mein  X  me and hum  X  we indicate first person with mein being first person singular and hum first person plural. Pronouns like tu  X  you and tum  X  you are second per-son with tu indicating a very casual relation and tum a familiar relation. ap  X  you is second person indicating a respectful relation. ye  X  him or this indicates third-person near relation and wo  X  him or that indicates third-person distant relation. Case inflections for each of these pronouns are determined based on the word endings and case markers that follow. The details can be found in Appendix A of Humayoun [2006]. Person information for these pronouns plays a very significant role in anaphoric resolution.
 our system by applying our analyzer on the examples mentioned in their work. There are 30 example sentences for noun inflections. Our analyzer successfully marked case and gender inflections for a ll 30 sentences accurately. We propose to test our analyzer using a task based approach like semantic role labeling that will eventually become part of the NLP pipeline framework that we are attempting for Urdu.
 The usefulness of our morphological analyzer can be seen in the heuristic rules used to improve the NE tagging accuracy (see Section 9.1). 9. NAMED ENTITY TAGGER NE recognizers are an important tool for almost all types of NLP applications such as information extraction, summerization, machine translation, etc. NE tagging is a process of recognizing chunks that represent person, location, or-ganization, date, time, percentage, or quantity. Accurately identifying these chunks is a nontrivial task. Several approaches, both rule-based and statistical learning-based have been used in the literature to provide high NE recognition (NER) rates across different languages. Successful approaches using machine learning techniques have been those that treat this problem as a sequence la-beling task. MaxEnt [Borthwick 1999], HMM [Bikel et al. 1999], and CRF [McCallum and Li 2003] based methods are most popular. Ekbal and Bandy-opadhyay [2010] have shown that using SVM based approach for Hindi NER also produces reasonably good results. There is limited work on NER for Urdu. The IJCNLP 2008, shared task for Named Entity Recognition has released an NE annotated data set for Urdu based on the AnnCorra standards [Bharati et al. 2006]. The best F -measure for Urdu NER on this data set is 35.47% reported by Saha et al. [2008] indicating the complexity of the task. The in-dividual recognition rates for person and location categories are also very low. This can be due to the complexity of the tag set (which is very detailed) and the small size of the annotated data (only about 35,447 tokens). Unfortunately this is the only NE annotated data set freely available for Urdu. However, CRL has released an Urdu NE annotated corpus for the machine translation task. This corpus has a fairly simple tagset and is annotated for location, organization, person, and time categories. The co rpus has about 55,000 words with a wide range of location, organization, and person names (Table XI).
 mentioned dataset, it is worthwhile to look at the NER performance for Hindi as the similarity between the two languages cannot be ignored. An NE recog-nizer for Hindi developed by Saha et al. [2008] has an F -measure of 79.03%. To overcome the problem of overfitting that occurs with using a MaxEnt classifier on a small data set, they use a clustering based approach to reduce the number of features. However the best performance obtained by using MaxEnt classi-fier is 75.6% F -measure. McCallum and Li [2003] also developed an NE tagger for Hindi using the CRF learning model with an F-Measure of 85.11%. These results show promise in using a learning model that treats NER as a sequence tagging problem.
 of reasons. For instance, Urdu does not have the concept of capitalization of characters. Also, most names of people have specific meanings associated with them and can easily be found in a dictionary as word entries. Our attempt to develop an NE recognizer for Urdu is explained in Mukund and Srihari [2009]. Below, we give a brief explanation of this technique and also continue to explain further enhancements made (adding chunk features and using the ACE tagset) to improve the performance of the model.
 We decided to use the POS information a ssociated with each word as a feature, as POS tags roughly account for the sense of the word. We consider three main categories: location, organization, and person. A 10-fold cross-validation on the CRL data set using this MaxEnt classifier has an F -Measure of 55.3%. It is obvious that all these NE tagged words should be tagged with the NNP (proper noun) POS tag. But when POS tags were generated for the NE tagged ground truth data, most of these words were either tagged as adjectives (JJ) or common nouns (NN). This adversely affe cted the NE tagger performance. We also notice that the POS tagger tagged most of the NNPs as NNs because of the sparseness of the NNP tag in the POS ground truth data set. This observation made us look at bootstrapping techniques for effective learning.
 bootstrap technique to increase the a ccuracy with which the words are tagged as NNPs. Although we report results using CRF learning approach, MaxEnt approach also generates similar results. The working of this model is briefly explained as follows. The CRULP dataset (dataset POS ) is a corpus of 150,000 words that are only POS tagged and the CRL dataset (dataset NE ) is a corpus of 50,000 words that are only NE tagged. A POS learning model (see Section 6) is first applied on dataset NE to generate POS tags. Rules to correct the POS tags associated with the named entities is then applied to generate correct POS tags for dataset NE . Another POS learning model is trained on the combined dataset -dataset NE and dataset POS . This model is now used to generate POS tags that are used as features to learn the NE tags. The final NE learning model is trained on dataset NE with the corrected POS tag information. More details of this algorithm can be found in Mukund and Srihari [2009]. A ten fold cross validation on this model has an average F -measure of 68.89% with the maximum being 69.21%.
 (1) an increase in the size of the dataset used to train the POS learning model, (2) more NNP entries that enable better learning of proper noun POS tags.  X  X ournalistic X  or  X  X ews writing X  style. 20 The articles are objective and follow a Subject-Object-Verb structure. Related information is usually presented within close sentence proximity. This makes it possible to hand-craft grammar rules for the discovery of NE tags with fine granularity. The final POS tagged and NE tagged data generated is processed using rules and lexicon lookups to fur-ther improve the overall tagging accuracy of the model. The rules used in this method are domain specific. 9.1 Rules for NE Tagging (2) Words such as  X   X  X  X  X  X   X   X  (organization),  X   X  X  X  X   X   X  are marked ORGANIZATION if (3) Lexicon look up for names of places is performed and the POS tag of the next (4) If a proper noun that is selected ends with a suffix  X  X ur X ,  X  X ad,  X  X ad X  and set from 68.89% to 74.67% and for the best from 69.21% to 71.3%. Our test sets consist of about 100 tags from each category. We also trained a MaxEnt learning model on a combined data set: CRL data set and the data set released by IJCNLP (2008). The F -measure on this data set was 34.2%, much less than the baseline of 55.3%. This could be due to the domain difference between the two data sets and also the styles of writing. Since using MaxEnt on CRL data set alone showed more promise, we proceeded with using only this data set for our experiments. prove NE recognition is not new. Several participants who achieved state-of-the-art NER scores for English in MUC-6 and MUC-7 21 NE shared tasks used chunk information [Zhou and Su 2002]. To further improve the efficiency of the proposed four stage model, we decided to use the shallow parsed informa-tion along with the parts of speech in the NE learning phase. The shallow tags were generated using the technique described in Section 7. The F -measure on the test set improved from 71.3% to 73.1% as shown in Table XII. Includ-ing chunk information shows a significant improvement in the location recog-nition rate. This is because several of the location tags consist of multiple words, that is, each location tag chunk can contain more than one word. For example, Andra Pradesh and South Afri ca contain two words in the location chunk. Though our POS tagset accommodates for a Proper Noun Continue tag (NNPC), several of these were not marked accordingly. But the shallow parsed chunk tags accommodated for this error explaining the boost in the location recognition rate. 9.2 NE Tagging Based on ACE Tagset Tasks such as machine translation and information extraction benefit from fine-grained categorization of NE tagsets. ACE [2005] NE tagset is a com-prehensive tagset that provides several NE categories. In order to achieve bet-ter clarity, we decided to represent the location NE tag in the ACE format. Table XIII shows the various subcategories under which location words can be represented.
 the sub categories for location tags. The location keywords are manually looked up in lexicons (see Section 3) to determine the subcategories. Proper nouns which do not occur in the lexicons or words that occur in more than one sub-category are further analyzed taking the surrounding context into account to determine the appropriate subcatego ry assignment. Words that cannot be as-signed to any of the ACE tags retain the location ( &lt; NeLoc &gt; ) tag. A similar four-stage model is used to train an NE system using the modified annotated data. The learning model is a MaxEnt classifier with the same set of features as shown in Table XII. This classifier is applied on the fold with F -measure 73.1% (see Table XII). The performance of the classifier is reduced to 72.6% (Mean Precision = 75.0% Recall = 70.3%) (see Table XIV) when trained on ACE tagset. This reduction is due to the small variation triggered in the person cat-egorization on account of similar syntactic structure with the location category.
A number of avenues remain to be explored to further improve the perfor-manceoftheNEmodel.Oneapproachwouldbetousethebootstrappingtech-nique for NE data as well. However, the rules required can be complicated. More hand-crafted rules and detailed lexicon lookups can result in better NE tagging. Rules that resolve the ambiguity between location and person tags need to be explored. Since POS tags are considered as features, we notice that a lot of proper nouns are still getting tagged as adjectives adding to the error. Resolving this issue will definitely boost the NE tagging accuracy. We have ex-plored the gender information of nouns and verbs that help in better location tagging. Exploiting similar morphological information such as the case infor-mation (locative, dative etc. (see Section 8) can also provide improvements. 9.3 Time Date Identification and Conversion Since we do not account for the date information in the NE tagging model, a separate process had to be considered to identify date words. Identifying time and date information in Urdu text is a challenge. There are two main formats in which this information is represented. (1) Gregorian format that follows the English calendar. (2) Islamic format that follows the lunar calendar (Hijri calendar).  X   X   X   X  (U+06BE) when used at the end or the beginning of a date representation indicates the Islamic format and  X   X   X   X  (U+0621) or nothing used at the end or the beginning of a date representation indicates the Gregorian format. Gram-mar rules are written to identify such representations and classify them ac-cordingly. The rules also identify the different Gregorian formats: dd-mm-yy or dd/mm/yy. Our POS tagset also provides a date category tag and the POS tagger has a tagging accuracy of 90.9% on this tag (Table IV).
 other so that a date can be represented in either format. Refer the Fourmilab X  X  Calendar Convertor page 22 for a detailed explanation of the algorithm used for conversion. 10. TRANSLITERATION Transliteration is an important precursor to Machine Translation and Cross Lingual Information Retrieval (CLIR). It is a process of converting text written in one writing system into another writing system. The process is challenging when two writing systems having different orthographic styles are considered. In this section we describe a method that generates translitera-tions for common Urdu names while simultaneously restoring missing diacrit-ics. We attempt to provide forward transliteration [Al-Onaizan and Knight. 2002] that involves transcribing an Islamic name written in Urdu to Eng-lish. Our proposed hybrid method [Mukund and Srihari 2009] first learns the alignment probabilities over CV* templatized words which are obtained by ap-proximate transliterations, and then applies heuristic rules to obtain the best-transliterated words. The first level of character level transliterations applied in Mukund and Srihari [2009] were based on simple observations (observations are only done on the lexicons collected for common Urdu names (see Section 3) that resulted in 40% accuracy. We further conducted experiments that involved changing this first level character mappings (for ambiguous characters). Our results show that the mappings shown in Mukund and Srihari [2009] are the most accurate.
 task because of the issue of missing diacritics. While writing in Urdu, one tends to skip writing some vowels and this makes the script ambiguous. Based on the context and the restored diacriti cs, different words may look similar but have different associated meanings. For example  X   X   X   X  X  X   X  is both  X  X alim X , and  X  X alam X  based on the vowel that is restored between  X  X  X  and  X  X  X  and both have different meanings. There are also cases where an Urdu name has more than one correct transliteration. Such issues make the problem of transliteration very similar to that encountered for Arabic.
 based on a generative model. But their method requires all pronunciations to be documented and any out-of-vocabulary pronunciations will fail to generate the word sequence. Al-Onaizan and Knight. [2002] proposed a spelling-based model where each English letter is mapped to an Arabic letter in a sequence. This method out-performed the phonetic-based method. There are other meth-ods that explore the information required for transliteration using parallel cor-pora. Samy et al. [2005] used Spanish to Arabic parallel corpora and picked the transliterated Named Entities based on sentence pair alignments. Kashani et al. [2007] proposed a three-phase algorithm for Arabic name transliteration that is based on a Hidden Markov Model approach. Their method also lever-ages lexical information to boost the overall accuracy of the system. Urdu. Urdu has a lot of aspirated consonants and retroflex sounds and these make pronunciations very different. Also the number of vowels between the two languages varies. Hence developing a unique Urdu transliteration system is beneficial. Diacritic restoration problems with respect to transliterations can be treated as a vowel restoration problem. If an Urdu word were to be translit-erated character by character to English, then the learning problem becomes a problem of learning the possible vowels between the consonant transitions. Our proposed hybrid learning model is applied only if lexicon lookups fail to generate an English transliterated match for a given Urdu name. Refer to Section 3 for lexicon creation.
 Some names starting with  X   X   X   X  [Aa] have their English transliterations starting with both  X  X a X  and  X  X  X . While  X  X a X  seems correct,  X  X  X  is not wrong either. Also the presence of aspirated consonants and  X  X  X  is not consistent. Urdu names that typically end with  X   X   X  [Heh goal] should have their English translitera-tion ending with  X  X  X . However there are many names that do not consider the presence of  X  X  X  in their transliterations. There are inconsistencies in the pres-ence of diacritics as well. Some words include the diacritics and some do not. Treating these inconsistencies as erro rs is not accurate as several letters in Urdu do have dual pronunciations [Hussain 2004].
 first level involves representing the training data in a format that helps map the consonants between the two languages efficiently. This level has three ma-jor steps. In the first step, all of the Urdu names are approximately translit-erated to English using the one-to-one Urdu to English character mappings (Table I in Section 3.1, Mukund and Srihari [2009]). These mappings were decided based on a thorough observation of the lexical resources collected for names. However, mappings used for characters shown in Table XV can trigger confusion.
 These characters can have more than one form of phoneme representation that depends mostly on their position in the word. Noon Gunna ( X   X   X ) is only for nasalization and should not be treated equivalent to  X   X   X . Do chasmey Hey ( X   X   X ) also has a similar issue which when combined with all stops and af-fricates [Hussain 2004] forms aspirated consonants but does not add additional phoneme [Hussain 2004]. These are true in the context of a text to speech sys-tem but not otherwise. But our observations reveal that several names with  X   X   X  endings have their English transliterations ending with  X  X  X . For example,  X  X hadan X , X  X hadman X ,  X  X abaan X  all end with  X   X   X . Same is the case with  X   X   X  X hich is equated to  X  X  X  in many cases. For example,  X  X ahm X ,  X  X ahbaan X ,  X  X ahrah X  all have  X   X   X .  X   X   X  X nd X   X   X  have many English equivalents based on their po-sition in the word. If they occur in the beginning and followed by only one phoneme, they are mostly equated with  X  X  X  otherwise  X  X  X . If they occur in the middle, they sometimes are  X  X e X ,  X  X i X ,  X  X i X  or simply  X  X  X . We have considered  X  X  X  to begin with and later use heuristic rules that account for the position in-formation to get the best transliteration. Similarly  X , X  can be either  X  X  X ,  X  X  X ,  X  X o X ,  X  X u X ,  X  X u X .
 mappings, we changed these mappings to nothing for  X   X   X  and again nothing for  X   X   X  if it occurred along with a consonant,  X  X  X  for  X   X   X ,  X   X   X  if the word begins with either and is followed by a phoneme and  X  X  X  for  X   X   X  (we also changed our heuristic rules to account for these characters). The results indicate that the character mappings based on our observations are the most accurate. Work done by Ahmed [2009] and the findings discussed in the paper complement our findings.
 mate transliterations. These rules help to mitigate the problem of inconsistent diacritic markings. The rules are generated based on the observations made on existing transliterations and by incorporating some mapping rules used by Ali and Ijaz [2009] in their English-to-Urdu transliteration system. Table XVI de-scribes some of these rules. For a compre hensive coverage of these rules, refer to Section 3.1 of Mukund and Srihari [2009].
 data, both actual English transliterations and approximate transliterations generated in the first step need to have the same number of consonant-vowel pair mappings. A CV* (C  X  Consonant V  X  Vowel) template is applied on every word and each pair is grouped separately. For example,  X  X abid X   X   X  X abd X  is split using the CV* template to  X  X a bi d X   X   X  X a b d X . The inconsistencies present in the data required us to handle the templatization logic as two separate cases. In Case 1, there exists a one-one correspondence between the CV* split for the actual transliteration and the approximate transliteration. In Case 2, such a one-one map does not exist. Hence correct ion symbols ( X  and $) are inserted into the actual English transliterations of such words. (see Section 3.1 of Mukund and Srihari [2009]).
 ary conditions for each word pair.  X  X - X  is added to the first character and  X -E X  is added to the last character of both the actual transliteration and the ap-proximate transliteration word pair. Words for which the correction symbols are applied are treated differently here as well.  X  X  X  X  is appended to the first character of such word pairs. 10.1 Learning Model The learning phase involves learning the possible vowels between consonants in a given word. We calculated the alignment probabilities using the gener-ated training data as parallel corpora. T hese alignment scores are generated using GIZA++. 23 The similarity score between the pairs was determined by us-ing the mapping table for potential CV* alignments. The transliteration for a new Urdu name was found by calculating most probable CV* alignments using beam search algorithm. The beam size is set to 75 and the alignment string with the highest probability was considered to be the most probable transliter-ation of the given Urdu name. 10.2 Post Processing Step Heuristic rules based on the generated approximate transliteration are applied to move most transliterations with second highest alignment probabilities to being the most probable ones. These rules improve the accuracy of the system from 23% to about 60%. The rules (see Table XVII) account for the different variations associated with  X   X   X  X nd X   X   X  X nd X   X   X . Mukund and Srihari [2009] give a more detailed explanation of the heuristic rules that are applied at the post processing stage in Section 3.3.
  X  X  X  or prune these symbols from the actual transliterated word, bigram tran-sition scores of the actual English transliterations (with the CV* splits) were calculated using the CMU language toolkit. 24 10.3 Results We tested our proposed method using two sets of 100 unique names not present in the training corpus. Neither of these sets share names between them. So our accuracies can be conside red as being reported over unknown words (not seen in the training data). The similarity scores in a way determine how many of the vowels were successfully restored. Levenshtein distance between the two strings is measured in each case. The pe rcentage similarity is computed using Equation1.
 where str 1 and str 2 are the two strings and strlen() is the length of the string. This percentage similarity is averaged over the 100 words of each test set and three cases are considered: (1) similarity between the required transliterated string and the approximate (2) similarity between the required transliterated string and the first alignment (3) similarity between the required transliterated string and the final found The scores obtained are shown in Table XVIII.

The results show a considerable improvement in the overall similarity score after the post processing step is applied, as many alignment strings with sec-ond highest probabilities (correct transliterations) are chosen to be the most likely English transliterations. Table XIX summarizes the results over the two test sets.

In order to determine the correctness of our observations and the charac-ter mappings file thus generated, a new mappings file for ambiguous charac-ters were generated as explained before. The transliterations obtained show a dip in the accuracy over the two data sets to 20% and 9% (highest probabil-ities) without the post-processing stage. Comparing the output got with the two mappings (mappings based on observations and new mappings), we see that clearly treating  X   X   X  X nd X   X   X  as treated for a pronunciation system (text to speech) is not accurate for a transliteration system. Using  X  X  X  for  X   X   X  X nd X   X   X  caused names such as  X  X anus X  and  X  X akoob X  to be transliterated to  X  X anus X  and  X  X akoob X . Using  X  X  X  instead of  X  X  X  in the first phase and later using heuristic rules to correct based on the phonetic constraint gives better results. Similar issues were found with  X   X   X  indicating that mappings used for name transliter-ations have to be dealt with separately and cannot be combined with common word pronunciations.
 and alignment scores are calculated. T he results reported in Tables XVIII and XIX were determined over test sets that contained only Islamic names. It was observed that this method performs poorly for Hindu names. The poor perfor-mance can be attributed to the following factors. (1) Our training data has no examples of Hindu names. (2) Many Hindu names have consonant groupings that are rarely found in com-(3) Most common Hindi names do not end with aspirated consonants or  X  X  X .
We also tested our method on a testset of English names.  X  X ndria X ,  X  X athew X ,  X  X ark X  were transliterated to  X  X nderia X ,  X  X asyo X , and  X  X arik X . It is observed that approximate transliterations are very close to the actual ones.  X  X ndria X  is approximately transliterated to  X  X ndrya X ,  X  X athew X  to  X  X athyw X , and  X  X ark X  to  X  X aark X . We also notice that the most ambiguous cases for Eng-lish words are in the interpretation of  X  X  X  and  X  X  X .  X  X  X  is usually changed to  X  X  X  or  X  X  X , and  X  X  X  to  X  X  X  or retained as  X  X  X . Our approximate transliterations can be heuristically modified to get transliterations close to the actual names. 10.4 IPA Standards Along with providing transliterations that are learnt using the alignment prob-abilities, we also provide another useful transliteration mechanism: translit-erations based on IPA standards [Hussain and Afzal 2001]. Though this is based on a lookup table, the transliteration is helpful to give a user who under-stands Hindi readability with Urdu. For example,  X   X   X  X   X   X   X : Arafat. We provide three types of transliterations X  English, UrduIPA, and UrduTransliterate  X  X or a word that is marked a proper noun. English form is obtained by performing lexicon lookups. UrduIPA form is obtained by applying IPA standards on the Urdu word, and UrduTransliterate form is obtained by applying the technique explained in Section 10. Table XX shows the different forms obtained for the example word  X  Arafat  X . 11. CONCLUSION Putting together a comprehensive text analysis system for a language like Urdu is a challenging task. In this work, we have developed an NLP system for Urdu that extracts basic language related information. The tagsets used for POS, NE, and shallow parser modules are standardized and our results can now be used as baseline for further improvements. The training data that we use is freely available for comparison. We have also shown that although Urdu has limited resources (required to develop statistical models), significant in-formation and data can be successfully borrowed from a syntactically similar language like Hindi. Experiments performed in Section 6 and 7 show that de-veloping an Urdu specific NLP system is a must and tools developed for Hindi cannot be directly used on Urdu despite the similarity between the two lan-guages. Our framework can be used to preprocess text and perform further analysis such as emotion detection, agent-target identification, and question opinion mining. We also provide different transliteration methods based on pronunciation and IPA standards that can improve crosslingual search and machine translation.

