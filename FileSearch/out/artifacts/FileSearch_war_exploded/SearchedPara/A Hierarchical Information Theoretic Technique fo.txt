 Discovery of alternative clusterings is an important method for exploring complex datasets. It provides the capability for the user to view clustering behaviour from different perspec-tives and thus explore new hypotheses. However, current algorithms for alternative clustering have focused mainly on linear scenarios and may not perform as desired for datasets containing clusters with non linear shapes. Our goal in this paper is to address this challenge of non linearity. In par-ticular, we propose a novel algorithm to uncover an alterna-tive clustering that is distinctively different from an exist-ing, reference clustering. Our technique is information the-ory based and aims to ensure alternative clustering quality by maximizing the mutual information between clustering labels and data observations, whilst at the same time en-suring alternative clustering distinctiveness by minimizing the information sharing between the two clusterings. We perform experiments to assess our method against a large range of alternative clustering algorithms in the literature. We show our technique X  X  performance is generally better for non-linear scenarios and furthermore, is highly competitive even for simpler, linear scenarios.
 I.5 [ Pattern Recognition ]: Clustering Algorithm Alternative Clustering, Information Theoretic Learning, Parzen-window technique
Data clustering aims at discovering novel patterns and structures from data. Its objective is to categorize similar data instances into the same classes (or clusters). However, while it may be reasonable to refer to a  X  X est X  model in su-pervised classification, it is less useful to make statements about a single, best clustering [5]. When exploring com-plex data, different clusterings can exist and they may each be reasonable. For example when analyzing a document dataset, one may find that it is possible to categorize ac-cording to either topics or writing styles; or when clustering a gene dataset, it is found that grouping genes based on their functions or structures is equally useful [6]. This challenge has recently stimulated the growing research area of alter-native clustering, where the goal is to generate different, yet high quality clusterings or groupings of a given dataset.
Several algorithms have been developed for the task of alternative clustering. Given an input, the reference clus-tering, the task is to generate another clustering, which is dissimilar from the reference one, yet it is still plausible (i.e. has high quality). It is this dual objective of achieving both dissimilarity and quality that makes the task challenging.
In this paper, we explore another aspect of the alterna-tive clustering problem. We focus on the scenario of non linearity, where cluster shapes may have unusual and non-Gaussian shapes and the border between clusters may not be linearly separable. We show that current algorithms for alternative clustering tend to underperform in this sce-nario. This motivates us to develop a new algorithm, called NACI (Non-linear Alternative Clustering with Information theory). NACI is a hierarchical technique which uses in-formation theoretic methods to optimize the dual objective functions of both quality and dissimilarity.

In particular, given a predefined (reference) clustering, the NACI algorithm aims to discover an alternative clus-tering for which i) the mutual information between its clus-ter labels and data observations is maximized, whereas at the same time ii) the mutual information between the alter-native clustering and the reference clustering is minimized. Objective i) helps to reduce the uncertainty within each clus-ter of the alternative clustering, by ensuring there is a strong (probabilistic) relationship between the cluster labels and the data instances. We later motivate this clustering objec-tive through the use of Fano X  X  inequality. On the other hand, objective ii) helps to ensure that the alternative clustering is independent (different) from the reference clustering.
The principal technical contribution of our work is the for-mulation of a well founded alternative clustering objective function, that is purely information theoretic. The advan-tage of using an information theoretic approach is that it can adapt well to the presence of non linearities. However, the technical development is not straightforward, requiring the use of Parzen windows for probability density estima-tions, as well as approximations based on quadratic mutual information.

Through an experimental analysis, we show that NACI performs particularly strongly when finding alternative clus-terings for non-linear datasets, improving over the state of the art. Furthermore, even for simpler, more linear datasets, NACI is able to discover desirable alternative clusterings, possessing high quality and high dissimilarity to the refer-ence clustering.
There are several works related to our research. The clos-est are those developed in [13, 2, 8, 10], which exploit various forms of negative information toward the desired clustering (as opposed to those using prior knowledge to improve clus-tering results [17, 22, 4]). In [13], the authors proposed a conditional information bottleneck (CIB) method, which treats class labels of a given clustering as negative infor-mation in seeking an alternative clustering. The new data partition is found by maximizing the information sharing between the cluster labels and the data features (describ-ing for data objects), but conditioned on the given refer-ence clustering. This method, though similar to our work in that both address the problem from an information the-ory viewpoint, is rather different in two important aspects. First, our approach makes no assumption regarding the data density distribution, whilst CIB requires the availability of the joint distribution information between cluster labels and the features, which is known as being hard to formalize[10]. Second, while our algorithm directly minimizes the mutual information between two clusterings to ensure their inde-pendence, the CIB approach only conditions on the refer-ence clustering in the process of encoding properties of the data features into the new clustering. In other words, it uses mutual information in a completely different way to our approach. Another approach, which exploits the effec-tiveness of pairwise constraints for data clustering [22, 3], is the COALA technique [2]. From the reference clustering, COALA generates a set of cannot-link constraints between pairs of data samples and attempts to find a different clus-tering that satisfies as many as possible of these constraints. On the other hand, a line of work developed in [8, 10] takes a rather different approach to alternative clustering by relying on the notion of orthogonality. In [8], the authors develop two techniques to find an alternative clustering using or-thogonal projections. In the first one, data is projected onto a space that is orthogonal to the space spanned by the set of mean vectors in the given clustering, while in the second technique, such a representative vectors are replaced by the feature space. An alternative clustering is then found by simply applying a clustering algorithm on this new trans-formed data. A similar approach is developed in [10] by which the transformation is applied on the distance matrix learnt from the provided clustering. In comparison, this work has an advantage compared to[8], since it avoids prob-lems when the data dimension is smaller than the number of clusters (e.g., spatial datasets).

Another series of works addressing the alternative clus-tering problems are those developed in [15] and [9]. Unlike the work above, these methods attempt to seek two alter-native clusterings at the same time. In the first work, two algorithms named Dec-kmeans and ConvEM are developed which attempt to derive two sets of mean vectors that are pairwise orthogonal, whereas in the second work [9], an al-gorithm, known as CAMI seeks two clusterings that share minimal mutual information. A clear distinction between these algorithms and ours in this paper is that our algo-rithm is not limited to spherically shaped clusterings, i.e. it is able to seek alternative clusterings for non linear datasets. We provide experimental comparisons with all the above al-gorithms in Section 5 of the paper.
In information theory, the quantity entropy plays a cen-tral role and is a measure of the uncertainty of a random variable. Mathematically, let X be a continuous random variable characterized by the probability distribution p ( x ), the Shannon entropy of X is:
When a variable is known and another is not, the remain-ing uncertainty is measured by the conditional entropy:
A related concept to the entropy is the Kullback-Leibler divergence. It is a measure of the distance between two distributions p ( x )and q ( x ) and is defined by:
Mutual information, which is of importance in our work, turns out to be a special case of the KL divergence. It mea-sures the information shared between two objects or in other words, it accounts for the amount of information that one random variable contains about another variable. In spe-cific form, let X and Y be two random variables with a joint probability density function p ( x, y ) and marginal probabil-ity density functions p ( x )and p ( y ), the mutual information I ( X ; Y ) is the KL distance between the joint distribution and the product of two marginal distributions p ( x )and p ( y ): which is obviously symmetric and non-negative. Impor-tantly, two random variables have zero mutual information if and only if they are statistically independent.
Given the above definitions, an intuitive problem state-ment is as follows:
Definition 1. Given a dataset X = { x 1 ,x 2 , ..., x n } each x i is in d -dimensional space and for which there is an existing reference clustering C  X  , find a new alternative clus-tering C + from X , such that C + is not only high quality but also as independent (i.e., different) from C  X  as possible. The independence between two clusterings can be quantified by the information sharing between them.

The number of clusters within each clustering C + and C  X  may be different. In our work, for easy of presentation, we assume that they are the same and use k to denote this number. However our methods can be easily adapted to the more general case.
For any learning algorithm, the learning process should ultimately transfer the information carried in the data sam-ples into the system X  X  parameter [20]. It is therefore natural to find an objective function that directly manipulates in-formation. Since mutual information is an essential tool to quantify the statistical relationship between any two random variables, it is intuitive to create a clustering cost function that relies on it. In the particular case of cluster analysis, we would like to find a clustering solution that has a strong probabilistic relationship with the data observation X .This implies that the clustering label variable C + has little un-certainty given the data observation X or in other words, the observations contain much of the information about the clustering label C + and we can infer the value of C + from the observations with small error. Theoretically, this ob-jective can be justified by the well-known Fano X  X  theorem from information theory, which provides a lower bound for the probability of error when guessing discrete values of a random variable C + from the random variable X .More specifically, let H ( C + | X ) be the conditional entropy of C given X , then Fano X  X  inequality states that:
Pr ( c + = c + )  X  H ( C in which c + and c + are respectively the true and guessed cluster labels of C + , after observing a sample of X .Thus, the lower bound on error probability is minimized when the mutual information between the cluster label C + and the data observation X is maximized. In this respect, it is possible to say that mutual information corresponds to the amount by which knowledge provided by the data observa-tion X decreases the uncertainty about the cluster.
Therefore, combining the use of information theory to en-sure cluster quality, with our objective of minimizing the mutual information between two clustering solutions, it is possible to form an alternative clustering objective function as follows (with a constraint of k final clusters within C
The parameter  X  regulates the relative importance of each of the clustering objectives.
When a data instance is assigned to one of several clusters, it incurs a variation on the mutual information cost. Opti-mizing this variation could be used as an evaluation function for clustering. In working toward optimizing the objective function in Eq.(6), one possible way is to employ an ag-glomerative hierarchical clustering technique. This type of algorithm typically begins by placing each data sample into its own cluster and then successively merges pairs of clusters until all samples are grouped into a single cluster. However, unlike most of the existing agglomerative techniques, where the merging between two clusters is decided based on their similarity (e.g., Euclidean distance), in our work, two clus-ters in C + are merged if such a combination makes the global mutual information I ( C + ; X ) maximally increased, while at the same time it minimizes the amount of I ( C + ; C  X  ). This evaluation requires the estimation of the mutual information at each merging step of the algorithm. In the following, we present an approach that can help to estimate mutual in-formation directly from data, while making no assumption about the data density distribution.

As mentioned in Section 3.1, mutual information defined in Shannon X  X  entropy can be viewed as the KL divergence between the joint distribution and the product of the two marginal distributions of two variables. However, computing this divergence is not an easy task in practice since it requires the availability of all variables X  probability density functions. Furthermore, the numerical integration of these functions also leads to very high computational complexity.
Fortunately, notice that our clustering objective is to op-timize the mutual information, rather than computing it ex-actly. It has been shown in [16, 20] that as long as a learning process does not require to compute an exact value of the mutual information, but rather to maximize or minimize it, then other practical divergences can be used. Importantly, the extrema of these divergences are also coincident with those of the KL divergence and therefore, the objectives of optimization are not compromised. One of such divergence is presented in [16]: D ( p || q )= 1 where  X  =0 , 1 .
 Based on this, a quadratic form of mutual information can be derived by selecting  X  = 2 and extending the equation to continuous densities (the first constant term can be omit-ted): It is easy to prove that all essential properties of the diver-gence are preserved, i.e., I R 2 is always non-negative, sym-metric and equal to 0 if and only if p ( x, y )= p ( x ) p ( y ).
When applying this quadratic form of mutual information to our alternative clustering problem, it is possible to derive the information sharing between two discrete variables C + and C  X  as follows: I and the quadratic mutual information between the continu-ous variable X and the discrete variable C + : I where the prior probabilities p ( c + i )and p ( c  X  j ) are estimated by the number of data samples in each cluster (over n ), i.e., respectively n i /n and n j /n . Similarly, the joint probabil-ity between c + i and c  X  j is estimated by the number of data samples belonging to both c + i and c  X  j , i.e., n ij /n .
Notice that computing the mutual information in Eq.(9) requires the estimation of variables X  probability density func-tion. Nonetheless, an appealing property of the quadratic mutual information is that it is possible to combine it with the Parzen window method, an effective non-parametric den-sity estimation technique, to simplify the computation. This involves placing a kernel function at each data sample. The density is accordingly evaluated by the sum of kernels. When using a Gaussian function for the kernel, it follows that: where G ( x  X  x i , X  2 )= 1 is the Gaussian in a d -dimensional space. An important property with this kernel is that the convolution of two Gaussians remains a Gaussian function:
This equation can be interpreted as the information poten-tial between x i and x j . Therefore, when combining the non-parametric density estimation method with the quadratic mutual information, the computational complexity can be greatly reduced. Specifically, according to the Parzen win-dow method: where n i is the number of data points belonging to clus-ter c + i . Followed by Bayes theorem, the joint probability between x and c + i is: The quadratic mutual information in Eq.(9) thus can be decomposed into three terms:
T T in is interpreted as the sum of all information potentials within each of clusters.
 T all can be described as the sum of all information poten-tials, regardless of their cluster and weighted by the cluster prior. And T btw is seen as the sum of information potentials between each cluster X  X  data points and all data points, weighted by the cluster prior.

Given the computations above, it is clear that the local interaction, as defined by the kernel in Eq.(11), between any two data instances needs to be computed. Therefore, a matrix G having size of n  X  n is generated in which at row i column j , the information potential term G ( x i  X  x j , 2  X  computed. Notice that, in practice, only half the number of these interactions need to be evaluated due to the symmetry. In addition to the G matrix, two other matrices D in and D btw , which respectively account for the variation in I R and I R 2 ( C + ; C  X  ) incurred by merging any pairs of clusters in C + , are utilized. Indeed, the combination of these two matrices acts as the similarity matrix in a classical agglom-erative clustering technique. Notice that, different from whose elements do not change during the clustering process, elements of these two matrices are updated regularly and their size is reduced one upon each merging step of the algo-rithm. These two matrices are initialized by computing the variation in mutual information when grouping any pair of data samples.

Under the hierarchical clustering framework, there are n  X  k iterative steps to merge clusters. Specifically, at each iterative step of this agglomerative clustering, the maximum element from the combined matrix is selected:
Subsequently, cluster c +  X  is grouped into cluster c +  X  this means that the merging leads to the maximum variation in our global objective function. Upon this union, elements locatedatcolumnandrow  X  will be removed from D in and D btw and it is necessary to update elements in column and row  X  th of these two matrices. That means a new varia-tion on the mutual information is computed if the updated cluster c +  X  is combined with any of the rest clusters in C
For simplicity, we re-use the cluster index notations and denote c +  X  for the new cluster by merging c +  X  with any cluster c  X  in C
Upon these, the variation in the mutual information can be simply computed. Specifically, the variation of mutual information with respect to each cluster c  X  j in C  X  is:  X  I = p ( c +  X  ,c  X  j )  X  p ( c +  X  ) p ( c  X  j ) 2  X  =2 p ( c +  X  ,c  X  j ) p ( c +  X  ,c  X  j )+ p ( c +  X  ) p ( c The entire variation on I R 2 ( C + ; C  X  ) is therefore:
Analogously, the variation on I R 2 ( C + ; X )canbederived by interchanging summations and integrations:
 X  I R 2 ( C + ; X ) in which: and
Notice that all these summations can be easily obtained from the G matrix.

Finally, it can be observed from the above that the varia-tion in the two mutual information values may be in different units. This is because I R 2 ( C + ; X ) is calculated between a clustering solution and the data (i.e. a discrete and con-tinuous variables), whereas I R 2 ( C + ; C  X  )iscomputedbe-tween two clustering solutions (i.e. two discrete variables). Therefore, in order to avoid this difficulty, the variation with respect to each mutual information is normalized by divid-ing it by the corresponding quadratic mutual information (Eqs.(8),(9)). This also makes it easier when regularizing the trade-off factor  X  between these two information X  X  quan-tities.
One of the key advantages in our algorithm is that it makes no prior assumption about the probability density functions and these functions are approximated directly from data using the non-parametric method. However, it should be noticed that the success of this approach is dependent on an appropriate selection for the kernel parameter, that is the standard deviation  X  . It is shown in [19] that if the kernel width  X  is annealed toward zero at a sufficiently low rate as n tends to infinity, then the Parzen window density estimator will be asymptotically unbiased and consistent. However, for most practical applications where data are fi-nite, the kernel size should be selected in such a way that it balances out the bias and variance, which essentially be-ing derived from the optimization of the mean integrated squared error between an estimator p ( x ) and the true den-sity p ( x ): MISE p ( x ) = x E [ p ( x )  X  p ( x )] 2 dx .
In our work, we choose  X  =  X  4 n (2 d +1) derived by applying the least square cross-validation and the normal reference rule [23] to minimize the generalization error above. It was also further experimentally observed that by setting  X  at this value, the interaction between samples that are far distant is still considered, while the interaction between close data samples remains emphasized. As shown in our experimental section, this  X  selection results in good clusterings for most of the datasets examined.
The proposed algorithm requires to compute the matrix G of information potentials between any pair of data samples. The complexity of this operation requires O ( dn 2 )where d is the cost of calculating the interaction according to Eq.(11), and n is the number of data observations. The calculation of mutual information X  X  variation when merging any two data samples takes O ( n 2 ). At each merging step, the maximum value is selected from the combination of two matrices D in and D btw . By using the priority queue data structure [7] that supports the search and deletion in O (log n )fromthis matrix, this step thus takes O ( n log n ). The calculation of updating information variation according to Eqs.(15) and where  X  = 1 d i  X  i and  X  i  X  X  are the diagonal elements of the sample covariance matrix (16) takes constant time given the availability of informa-tion potential matrix G . Since there are ( n  X  k )merging steps for the agglomerative clustering, the computation is O ( n 2 log n ). The overall complexity of our proposed algo-rithm is therefore O ( n 2 log n + dn 2 ). Considering that d is usually smaller than log n , it is possible to say that the final complexity is O ( n 2 log n ), which is the same as that of a conventional hierarchial clustering using group-average similarity.
In this section, we provide experimental results on both synthetic and real-world benchmark datasets. Our NACI algorithm is compared against eight methods, including five semi-supervised alternative clustering algorithms: the CIB method [13], COALA [2], two methods from [8] denoted by Algo1 and Algo2, and the ADFT algorithm[10]; and three unsupervised alternative clustering algorithms: the Dec-kmeans, ConvEM from [15], and our previous algorithm CAMI [9]. Unless otherwise indicated, we set  X  =0 . 2as the default value for NACI. For the CIB method, we im-plement the iterative version [12, 13] and its outputs are post-processed by assigning each data point to the cluster to which it has the highest probability. For ADFT, we im-plement the gradient descent method integrated with the iterative projection technique (in learning the full family of the Mahalanobis distance matrix) [24, 25]. We also use the EM technique as the background clustering technique for the approaches developed in [8, 10]. For the Dec-kmeans, Con-vEM and CAMI, we follow the heuristic method described in their work to set the trade-off factor between the clustering quality and dissimilarity. Since both NACI and COALA are developed based on agglomerative hierarchical clusterings, which are not sensitive to the initial parameters but possi-bly to the data instances X  order. Therefore, when running them, we randomly swap the order amongst data samples. We run each algorithm 20 times and report the average re-sults.
We evaluate the clustering results based on both clustering dissimilarity and clustering quality measures. For measuring dissimilarity between two clusterings, we report the values of two different measures. The first and also the most pop-ular one is the normalized mutual information[18, 21, 14, 11]: NMI ( C + ; C  X  )= I ( C + ; C  X  ) / ( H ( C + ) H ( C ing the number of shared instances between clusters c + i and c  X  j  X  C  X  . The second is the Jaccard index (JI) [2, 10]: pairs of samples in the same cluster for both C + and C  X  n 01 and n 10 are the number of samples X  pairs belonging to the same cluster in one solution, but not in the other.
For measuring clustering quality we divide into two cases: if true class labels are known, the agreement between clus-tering results and the correct labels is calculated by the F-measure: F =2 P  X  R/ ( P + R ), in which P and R are respec-tively the precision and recall. If true class labels are not known, we use the Dunn Index, similar to [2, 10]: DI ( C )= Figure 1: Alternative Clustering returned by NACI on Syn1, 2 and 3 datasets the cluster-to-cluster distance and : C  X  R + 0 is the cluster diameter measure.

Note that for the NMI and JI measures, a smaller value is desirable , indicating higher dissimilarity between cluster-ings, while for the F-measure and Dunn Index, a larger value is desirable , indicating a better clustering quality. Also, since methods like Dec-kmeans, ConvEM and CAMI do not require existing clusterings to be provided and instead seek two alternative clusterings at the same time, we try to achieve a fair comparison with them by reporting the higher values of F-measure in the case true labels are available, and averaging the Dunn Index when the class labels are not known.
Four synthetic datasets are used to evaluate the perfor-mance of our proposed clustering technique against other algorithms. For the first dataset Syn1, we use a popular one from [2, 8, 10, 15], which consists of 4 Gaussian sub-classes. Each Gaussian contains 200 points in 2-dimensional data space. The goal of using this dataset, when setting k =2, is to test whether our algorithm can discover an alternative clustering that is orthogonal to the existing one. For the sec-ond synthetic dataset Syn2, we use a more complicated one in which 6 Gaussian sub-classes are located in a ring shape. Different from Syn1, this dataset consists two equally impor-tant clusterings (with k = 3) that are not orthogonal and it is not possible to find them by simply projecting the data on either of the subspaces. We generate the third and fourth datasets by replacing a non-Gaussian shape for each sub-class in the first synthetic dataset. By using these datasets, we aim to test the ability of our algorithm in uncovering non-linear clusterings.

Figures 1(a), (b) and (c) show clustering solutions respec-tively for the first three synthetic datasets. Clusterings in the top graphs of each figure are provided as pre-existing reference solutions to each semi-supervised algorithm and in the bottom graphs, we demonstrate the alternative clus-terings returned by the NACI, which exactly match second important clusterings included in each dataset. We compare the performances of all algorithms via the results summa-rized in Table 1. As can be seen from the table, like other alternative clustering methods, our proposed algorithm can the pre-defined clustering.
 easily identify the orthogonal clustering for the first simple synthetic dataset. Its performance on the second dataset is also accurate, although the two alternative clusterings solu-tions have been deliberately designed to be non-orthogonal. Notice that, unlike Syn1, there are no dominant features that fully support any of the clusterings in this dataset. Therefore, the methods developed based on orthogonal space transformation [8, 10] or orthogonal clusters X  mean projec-tion [15] are usually less successful in discovering the second alternative clustering. Only CAMI and NACI are able to achieve high accurate results since they both adopt the ap-proach of mutual information minimization. However, dif-ferent from CAMI, which slightly suffers from the problem of initial parameters sensitivity, NACI can completely avoid this since it is designed based on the hierarchical cluster-ing technique. In addition, though we tested the strategy at which data samples were randomly swapped before each running, it was still found that NACI X  X  performance was more consistent across all trials. 2
We refer the reader to our previous work [9] for detailed jus-tification on the clustering outputs of the other algorithms.
We provide the clustering output of NACI for the Syn3 dataset in Figure 1(c) and all algorithms for the Syn4 dataset in Figure 2 (the clustering outputs of Algo2 and ConvEM are omitted since they are very much similar to those of Algo1 and Dec-kmeans, respectively). For these two syn-thetic datasets, we aim to test the algorithms X  ability in un-covering nonlinear clusterings. It is clear that methods like Algo1 and Algo2, or Dec-kmeans, ConvEM and CAMI are unable to identify the correct alternative clustering, since their core algorithms are tied to a particular spherical clus-tering technique (e.g., k-means, EM). Moreover, the data projection on a space orthogonal to the set of mean vec-tors used in Algo1 or analogously to the feature subspace used in Algo2, does not help them uncover the second al-ternative clustering, since the data is distorted and seems to be more overlapping by these transformations; whereas the clustering approaches used in ConvEM and CAMI only ensure a low value of decorrelation between two alternative clusterings, but cannot guarantee that accurate clusterings can be found for these nonlinear clustering shapes due to the EM technique. Similarly, by inverting the stretcher ma-trix, ADFT is also unable to deduce the hidden clustering structure, since elements in this diagonal matrix actually are the stretching factors along each dimension. Thus, varying any of the elements (corresponding to dimensions) does not make the alternative clusterings more easily discovered, es-pecially for the Syn4 dataset where both cluster shapes and clustering boundary are strongly nonlinear.

COALA, although essentially an average linkage algorithm, has merging criteria based on the Euclidean distance be-tween clusters. This is in contrast to our algorithm, which exploits the similarity between clusters based on the infor-mation embedded in the data. Furthermore, the dissim-ilarity between two clusterings is optimized by NACI via the global quantity of the clusterings X  mutual information, rather than the local pairwise cannot-link constraints be-tween any two data points as used in COALA.

The CIB method also approaches the problem based on information theory. However, different from our approach in which we directly minimize the mutual information between the alternative and the existing clustering, it can be noticed that CIB only conditions on this provided clustering in its process of maximizing the information between the new clus-tering and the set of data features. Its resultant alternative clusterings therefore look somewhat unnatural (as observed in Figure 2). Moreover, our approach in utilizing the mu-tual information is also rather different from the CIB X  X  ap-proach. In particular, while we approach the problem by making no assumption regarding the data distribution and exploit computational advantages of quadratic mutual in-formation combined with the reliable and non-parametric density estimation technique, CIB still relies on the mutual information using Shannon X  X  entropy and explicitly assumes the availability of the variables X  joint distribution. This can-not be guaranteed, especially for datasets with limited sizes.
The CMUFace data obtained from the UCI KDD reposi-tory [1] is an interesting dataset, since its data samples can be partitioned in several different ways (e.g. by individual, by pose, etc.). The dataset consists of images of 20 people taken at various features such as facial expressions (neutral, happy, sad, angry), head positions (left, right or straight), and eye states (open or sunglasses). Each person has 32 images captured in every combination of these features. We randomly select 3 people along with all their images. Since it is known which image comes from which person, this forms an existing partition of the dataset. We run NACI and the other algorithms with this provided clustering. As a pre-processing step, the PCA technique is applied to reduce the number of dimensions, in which we retain the number of first principal components that cover 90% of the original data X  X  variance.

In Figure 3, we show the mean vectors of the provided clustering (in the top graphs) and the mean vectors returned in the NACI X  X  alternative clustering (in the bottom graphs). Pictorially, it is possible to observe that the uncovered al-ternative clustering returned by NACI provides another dif-ferent, yet equally important clustering on the set of image data. While pictures in the first row show that they rep-resent for different individuals, pictures in the second row clearly reveal that images have been partitioned according to different poses. This obviously provides another mean-ingful interpretation about this dataset. For specific results, Figure 3: NACI X  X  results on CMUFace dataset we report NACI X  X , together with other techniques X  in Ta-ble 2. As observed from this table, COALA and CIB per-form slightly better than Algo1 and Algo2, which attempt to find alternative clusterings in an orthogonal transforma-tion space. For the methods like Dec-kmeans and CAMI which seek two alternative clusterings simultaneously, we found that they perform fairly well for the clustering based on individuals but achieve a very moderate accuracy on the clustering based on poses. Looking deeper, we also found that the clustering based on poses is quite hidden and non-linearly separable, but the configuration based on persons is very obvious and quite separated when visualizing using the first three PCs. This might explain why the methods like Dec-kmeans and CAMI work well for the first cluster-ing, but not for the second one. Our algorithm outperforms these algorithms since its clustering objective is to maximize the probabilistic relationship between cluster labels and the data, and thus is not limited to the Gaussian shapes of the clusters. We also test another strategy by which the clus-tering labels based on poses are provided as background in-formation. The clustering accuracy for the person based partitioning of all algorithms is summarized in the fourth column of the Table 2.
We further compared the nine algorithms on three real-world datasets selected from the UCI repository: Segmen-tation, Vehicle Silhouette, and Vowel. For the Segmenta-tion dataset, three attributes 5,7 and 9 are removed as they were reported to be repetitive with the attributes 4,6 and 8 [1]. Since these datasets already contain pre-defined class labels, we utilize them as an existing clustering provided. Also, as we do not have ground truth for alternative clus-terings, the Dunn Index (instead of F-measure) will be used for clustering quality comparison amongst the nine cluster-ing techniques. We report the results of all techniques on these datasets in Table 3.

Looking at this table, we see that NACI also performs well with all three datasets. Note that these datasets have a much higher degree of non-linearity, compared to the ones we have already examined. It can be noted that NACI typi-cally finds more dissimilar clusterings (as measured by both NMI and JI) compared to those of other algorithms. Its clusters found in the alternative clustering are also well sep-arated as indicated by the small values of Dunn index. This measure is only slightly larger than that of CAMI in the Vehicle dataset and ADFT in Vowel dataset. This might happen, since the Dunn Index is essentially computed by the averaging distances between pairs of points in two clus-ters over the maximum cluster diameter, and thus somewhat supports clusters returned by CAMI and ADFT. However, overall, one can observe that our algorithm tends to achieve more decorrelated (i.e., more different) clusterings, whereas its clustering quality is very competitive in all three datasets. Its performance in the Segmentation dataset is better than all other algorithms.
There are two parameters that may impact the perfor-mance of our NACI algorithm: the kernel width  X  and the regularization factor  X  . We have conducted a series of ex-periments to examine the sensitivity of the results on these parameters. For the kernel width  X  , though its variation can affect the algorithm X  X  performance, we have found that for most of cases, setting it to the value derived in Section 4.3 often leads to good and stable clustering results (as reported in the previous sections). Due to lack of space, we can only present here the main observations with respect to the pa-rameter  X  , which regularizes for the relative importance be-tween two quantities of mutual information.

Since both the variations computed in Eqs.(15) and (16) have been normalized by the corresponding mutual informa-tion, we set the  X  to be within the unit interval. In Figure 4, we show the relationship between the normalized mutual in-formation, the Dunn index and the values of  X  for three real-world datasets: Segmentation, Vehicle Silhouette, and Vowel. The results are reported when  X  is varied from 0.1 to 0.5 with a step of 0.05. As we expected, small values of  X  lead to compact clustering solutions (in terms of Dunn index), but such results seem to be quite overlapping with the given clusterings (shown by the high values of NMI). As the values of  X  increase, the clustering quality somewhat re-duces but the alternative clusterings are more decorrelated from the existing solutions. However, once  X  is above 0.3, it was observed the fluctuations happened with the NMI X  X  values. This can be explained by the hierarchical clustering approach of NACI, where it tends to combine clusters which overly support small values of the information between two clusterings at the beginning, but such decisions cannot be undone at a later time where it converges to a small number of final clusters. Likewise, it was seen that the resultant clus-ters in the alternative clusterings in this case were also very imbalanced, which made the Dunn index located at small values as well. Nevertheless, as we observed from all three graphs in Figure 4, high quality and dissimilar alternative clusterings can be achieved if  X  is set around 0.2 since the Dunn Index in this range is relatively high, whereas that value of NMI is also small. In this paper we have proposed a novel algorithm called NACI, to discover alternative clusterings, which are of high quality, yet distinctively different from a provided reference clustering. We address the problem purely from information theory, in which the clustering quality is achieved by max-imizing the mutual information between cluster labels and the data observations (this implicitly ensures a strong prob-abilistic relationship between them), whereas the dissimilar-ity between two alternative clusterings is achieved by the minimization of the mutual information between them. To fully exploit the information embedded in the data, we em-ployed the Parzen window method for probability density es-timation. Such a non-parametric technique does not impose any assumptions regarding the data distribution and further enables practical computations, when combined with the quadratic mutual information form. These features made NACI particularly suitable for the challenging scenario where datasets have non linear structures.

We evaluated the performance of our algorithm on a num-ber of synthetic and real-world benchmark datasets, com-paring against eight well known existing approaches. The experimental results show NACI is able to achieve excel-lent performance for non linear cases, and is able to obtain highly competitive performance even for simpler, more lin-ear structures. We believe that NACI is a powerful tool for alternative clustering discovery and exploration. [1] A. Asuncion and D. Newman. UCI machine learning [2] E. Bae and J. Bailey. Coala: A novel approach for the [3] S. Basu, A. Banerjee, and R. Mooney. Active [4] M. Bilenko, S. Basu, and R. J. Mooney. Integrating [5] R. Caruana, M. Elhawary, N. Nguyen, and C. Smith. [6] G. Chechik and N. Tishby. Extracting relevant [7] D. M. Christopher, R. Prabhakar, and S. Hinrich. [8] Y. Cui, X. Fern, and J. Dy. Non-redundant multi-view [9] X. H. Dang and J. Bailey. Generation of alternative [10] I. Davidson and Z. Qi. Finding alternative clusterings [11] X. Fern and W. Lin. Cluster ensemble selection. Stat. [12] D. Gondek. Non-redundant clustering. In PhD Thesis, [13] D. Gondek and T. Hofmann. Conditional information [14] D. Gondek, S. Vaithyanathan, and A. Garg. Clustering [15] P. Jain, R. Meka, and I. Dhillon. Simultaneous [16] J. Kapur. Measures of Information and their [17] K.Wagstaff and C.Cardie. Clustering with [18] M. Law, A. Topchy, and A. Jain. Multiobjective data [19] E. Parzen. On estimation of a probability density [20] J. Principe, D. Xu, and J. Fisher. Information [21] A. Topchy, A. Jain, and W. Punch. A mixture model [22] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr  X  odl. [23] M. P. Wand and M. C. Jones. Kernel [24] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance [25] L. Yang and R. Jin. Distance metric learning: A
