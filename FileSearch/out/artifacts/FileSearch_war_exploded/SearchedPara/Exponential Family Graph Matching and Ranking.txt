 The Maximum-Weight Bipartite Matching Problem (henceforth  X  X atching problem X ) is a funda-by standard methods such as the Hungarian algorithm.
 ample, in computer vision the crucial problem of finding a correspondence between sets of image features is often modeled as a matching problem [2, 4]. Ranking algorithms can be based on a matching framework [13], as can clustering algorithms [8].
 When modeling a problem as one of matching, one central question is the choice of the weight (e.g. SIFT) associated with those points.
 optimizing for agreement with data is called structured estimation [27, 29]. [27] and [4] describe max-margin structured estimation formalisms for this problem. Max-margin received careful examination recently [16, 15]. (MAP) estimator for the matching problem. The observed data are the edge feature vectors and of matches given the observed data. We build an exponential family model where the sufficient including DORM [13], an approach that only differs from our model instance by using max-margin instead of a MAP formulation. We show very competitive results on standard document ranking and [27] are likely to be preferable even in spite of their potential inferior accuracy. 2.1 Structured Prediction dictors , which are predictors of the kind annotated matches (i.e., a set { ( x n ,y n ) } N vector  X  for predictor g  X  when data { ( x 1 ,y 1 ) ,..., prediction for input x means computing y = g ( x ;  X  ) using the estimated on max-margin estimators [29, 27], and the other on maximum-likelihood (ML) or MAP estimators in exponential family models [12].
 McAllester recently provided an interesting analysis on this issue, where he proposed new upper bounds whose minimization results in consistent estimators, but no such bounds are convex [18]. The other approach uses ML or MAP estimation in conditional exponential families with  X  X truc-over the cliques of the graph (in which case they are called Conditional Random Fields, or CRFs perform inference. ML and MAP estimators in exponential families not only amount to solving an motivated the use of max-margin methods in many scenarios where such intractability arises. 2.2 The Matching Problem Consider a weighted bipartite graph with m nodes in each part, the edges. G can be simply represented by a matrix ( w ij edge ij . Consider also a bijection y : { 1 , 2 ,...,m }7 X  X  matching problem consists of computing Figure 1: Left: Illustration of an input vector-weighted bipartite graph is a vector x e associated with each edge e (for clarity only edge). Right: weighted bipartite graph G obtained by evaluating only edge ij is shown). This is a well-studied problem; it is tractable and can be solved in according to domain knowledge and subsequently solving the combinatorial problem. 3.1 Basic Goal composition of a feature vector x ij (observed) and a parameter vector data). Therefore, in practice, our input is a vector-weighted bipartite graph { X,Y } = { ( x n ,y n ) } N n =1 is available, where x n := ( number of nodes in each part of the vector-weighted bipartite graph as the observed data. We will assume f to be bilinear, i.e., 3.2 Exponential Family Model We assume an exponential family model, where the probability model is is the log-partition function, which is a convex and differentiable function of The prediction in this model is the most likely y , i.e., and ML estimation amounts to maximizing the conditional likelihood of the training set i.e., computing argmax form MAP estimation: Assuming iid sampling, we have p ( Y | X ;  X  ) = Q N where  X  is a regularization constant. ` ( Y | X ;  X  ) is a convex function of function g (  X  ) is a convex function of  X  [31] and the other terms are clearly convex in 3.3 Feature Parameterization diction of the exponential family model (5), i.e., P natural model is i.e., linear in both x and  X  (see Figure 1, right). The specific form for under the model  X  . 4.1 Basics We need to solve  X   X  = argmin  X  [31], therefore gradient descent will find the global optimum. In order to compute log-partition function is the expectation of the sufficient statistics: the above expression gives Note that the above is the expression for the permanent of matrix in definition to the determinant, the difference being that for the latter sgn( linear algebra manipulations, computing the permanent is a we have no realistic hope of computing (11) exactly for general problems. 4.2 Exact Expectation The exact partition function itself can be efficiently computed for up to about O (
M 2 M ) algorithm by Ryser [25]. However for arbitrary expectations we are not aware of any For larger graphs, we have alternative options as indicated below. 4.3 Approximate Expectation approximate the permanent of dense non-negative matrices [10]. The algorithm works by producing precisely the same form as the distribution we have here, for applications that involve larger graphs.We generate K and directly approximate (12) with a Monte Carlo estimate In our experiments, we apply this algorithm to an image matching application. 5.1 Ranking to rank . Ranking is a fundamental problem with applications in diverse areas such as document method. In this paper we focus, without loss of generality, on document ranking. We are given a set of queries { q k } and, for each query with corresponding ratings { r k retrieved document d k Training At training time, we model each query q k as a vector-weighted bipartite graph (Figure 1) where the nodes on one side correspond to a subset of cardinality document of every rating is present. Therefore M must be such that of documents { d k subset of M documents from the original set of D ( k ) documents.
 In the following we drop the query index k to examine a single query. Here we follow the con-struction used in [13] to map matching problems to ranking problems (indeed the only difference with ranking position j  X  i is dataset specific (see details below). From (10) and (16), we have proceeds as explained in Section 4.
 Testing At test time, we are given a query q and its corresponding list of We then have to solve the prediction problem, i.e., Figure 2: Results of NDCG@k for state-of-the-art methods on TD2004 (left), TD2003 (middle) and OHSUMED (right). This is best viewed in color.
 j , then (17) can be solved simply by sorting the values of words, the matching problem becomes one of ranking the values should be used. We do not solve this problem in this paper, and instead choose a fixed sets: OHSUMED, TD2003 and TD2004.
 Data sets OHSUMED contains features extracted from query-document pairs in the OHSUMED each query there are a number of associated documents, with relevance degrees judged by humans and TD2004 contain features extracted from the topic distillation tasks of TREC 2003 and TREC All datasets are already partitioned for 5-fold cross-validation. See [14] for more details. Evaluation Metrics In order to measure the effectiveness of our method we use the normalized discount cumulative gain (NDCG) measure [11] at rank position where r ( j ) is the relevance of the j th document in the list, and a perfect ranking yields an NDCG score of 1 . and max-margin. Runtimes for M = 3 , 4 , 5 are from the ranking experiments, computed by full enumeration; M = 20 corresponds to the image matching experiments, which use the sampler from [10]. A problem of size 20 cannot be practically solved by full enumeration. randomness of the sampling of the training data. We use c Optimization To optimize (8) we use a standard BFGS Quasi-Newton method with a backtracking line search, as described in [21].
 Results For the first experiment training was done on subsets sampled as described above, where linearly with M . For TD2003 we also trained with all possible subsets ( In Figure 2 we plot the results of our method (named RankMatch), for of the datasets: RankBoost [6], RankSVM [7], FRank [28], ListNet [5], AdaRank [32], QBRank implementation of DORM [13], using precisely the same resampling methodology and data for a fair comparison. RankMatch performs among the best methods on both TD2004 and OHSUMED, while on TD2003 it performs poorly (for low k ) or fairly well (for high are only reported on TD2004 and OHSUMED. RankMatch compares similarly with SortNet and StructRank on TD2004, similarly to C-CRF and StructRank on OHSUMED and similarly to the two versions of SortNet on TD2003. This exhausts all the comparisons against the methods which their performance published for the respective missing dataset.
 When compared to the methods which report results in all datasets, RankMatch entirely dominates their performance on TD2004 and is second only to IsoRank on OHSUMED. in the near future so that we are able to compare them on a fair and transparent basis. Consistency In a second experiment we trained RankMatch with different training subset sizes, starting with 0 . 03  X  D ( k )  X  M and going up to 1 . 0  X  D that, as more training data is available, RankMatch improves more saliently than DORM. will result in much slower runtimes than those typically obtained in the max-margin framework. 5.2 Image Matching For our computer vision application we used a silhouette image from the Mythological Creatures shear to the image creating 200 different images. We then randomly selected each pair i,j of points (one from each image) and learned would manually provide. In this setup, and  X  i is the Shape Context feature vector [1] for point method described in Section 4.3. Once again, the regularization constant size, our model seems to enjoy a slight advantage. NDCG@1 on the ranking dataset OHSUMED. This evidence is in agreement with the fact that our estimator is consistent, while max-margin is not. We presented a method for learning max-weight bipartite matching predictors, and applied it ex-it consists of performing maximum-a-posteriori estimation in an exponential family model, which
