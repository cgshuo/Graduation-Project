 Topic models have been widely used in tasks like information retrieval [ 1 ], text summarization [ 2 ], word sense induction [ 3 ] and sentiment analysis [ 4 ]. Popu-lar topic models include mixture of unigrams [ 5 ], probabilistic latent semantic indexing [ 6 ], and latent Dirichlet allocation (LDA) [ 7 ].
 via the top-10 words of highest probability in a given topic. For example, the multinomial word distribution feed contaminated farms company eggs animal food dioxin authorities german is a topic extracted from a collection of news articles. The model gives high probabilities to those words like feed , contami-nated ,and farms . This topic refers to an animal food contamination incident. Our research aims to generate topic labels to make LDA topics more readily interpretable.
 ture the meaning of a topic; and (2) it should be easy for people to understand. There are many ways to represent a topic, such as a list of words, a single word or phrase, an image, or a sentence or paragraph [ 8 ]. A word can be too general in meaning, while a sentence or a paragraph can be too detailed to capture a topic. In this research, we select phrases to represent topics.
 Our method consists of three steps. First, we generate candidate topic labels, then map topics and candidate labels to vectors in a vector space. Finally by calculating and comparing the similarity between a topic and its candidate label vectors, we can find a topic label for each topic.
 Our contributions in this work are: (1) the proposal of a method for gen-erating and scoring labels for topics; and (2) the proposal of a method using two word vector models and a letter trigram vector model for topic labelling. In experiments over three pre-existing corpora, we demonstrate the effectiveness of our methods. Topics are usually represented by their top-N words. For example, Blei et al. [ 7 ] simply use words ranked by their marginal probabilities p ( w model. Lau et al. [ 9 ] use features including PMI, WordNet-derived similarities and Wikipedia features to re-rank the words in a topic, and select the top three words as their topic label. A single word can often be inadequate to capture the subtleties of a topic. Some other methods use human annotation [ 10 , 11 ], with obvious disadvantages: on the one hand the result is influenced by subjective factors, and on the other hand, it is not an automatic method and is hard to replicate.
 Some use feature-based methods to extract phrases to use as topic labels. Lau et al. [ 12 ] proposed a method that is based on: (1) querying Wikipedia using the top-N topic words, and extracting chunks from the titles of those articles; (2) using RACO [ 13 ] to select candidate labels from title chunks; and (3) ranking candidate labels according to features like PMI and the Student X  X  t test, and selecting the top-ranked label as the final result.
 Blei and Lafferty [ 14 ] used multiword expressions to visualize topics, by first training an LDA topic model and annotating each word in corpus with its most likely topic, then running hypothesis testing over the annotated corpus to identify words in the left or right of word or phrase with a given topic. The hypothesis testing is run recursively. Topics are then represented with multiword expressions. Recent work has applied summarization methods to generate topic labels. Cano et al. [ 15 ] proposed a novel method for topic labelling that runs sum-marization algorithms over documents relating to a topic. Four summarization algorithms are tested: Sum basic, Hybrid TFIDF, Maximal marginal relevance and TextRank. The method shows that summarization algorithms which are independent of the external corpus can be applied to generate good topic labels. Vector based methods have also been applied to the topic labelling task. Mei et al. [ 16 ] developed a metric to measure the  X  X emantic distance X  between a phrase and a topic model. The method represents phrase labels as word dis-tributions, and approaches the labelling problem as an optimization problem that minimize the distance between the topic word distribution and label word distribution.
 Aletras and Stevenson [ 17 ], and work on learning vector representations of words semantically related in a semantic vector space. 3.1 Preliminaries Distributional vectors can be used to model word meaning, in the form of latent vector representations [ 18 ]. In order to capture correlations between a topic and a label, we map LDA topics and candidate labels to a vector space, and calcu-late the similarity between pairs of topic vectors and candidate label vectors. The candidate label which has the highest similarity is chosen as the label for that topic.
 candidate labels of topic z and Sim represents the similarity between two vectors. The symbols used in this paper to describe the top model and the topic labelling method are detailed in Table 1 .
 tors from [ 21 ], and two word vectors: CBOW (continuous bag-of-words model) and Skip-gram [ 20 ]. A letter trigram vector is able to capture morphological vari-ants of the same lemma in close proximity in a letter trigram vector space. CBOW and Skip-gram are methods for learning word embeddings based on distributional similarity. They each capture latent features from a corpus. 3.2 Candidate Label Extraction We first identify topic-related document sets according to a topic summarization method [ 15 ]. The predominant topic of a document d can be calculated by: Given a topic z , the set of documents whose predominant topic is z ply the set of documents that have z as their predominant topic. For each topic z , we then use OpenNLP 1 to full-text chunk parse each document in z chunks that contain at least words in the top-10 words in z , as candidate labels. 3.3 Vector Generation CBOW Vectors. CBOW generates continuous distributed representations of words from their context of use. The model builds a log-linear classifier with bi-directional context words as input, where the training criterion is to correctly classify the current (middle) word. It captures the latent document features and has been shown to perform well over shallow syntactic relation classification tasks [ 22 ].
 Recent research has extended the CBOW model to go beyond the word level to capture phrase-or sentence-level representations [ 22  X  24 ]. We simply apply the weighted additive method [ 23 ] to generate a phrase vector. Word vectors of candidate labels and LDA topics are generated as follows: where y cbow w j is the word vector of word w j based on CBOW . Skip-gram Vectors. The Skip-gram model [ 22 ] is similar to CBOW , but instead of predicting the current word based on bidirectional context, it uses each word as an input to a log-linear classifier with a continuous projection layer, and predicts the bidirectional context.
 shown to perform well over semantic relation classification tasks [ 22 ]. [ 23 ]. Skip-gram vectors of candidate labels and LDA topics are generated in the same manner as CBOW , based on y skip w j (i.e. word vectors from Skip-gram ). Letter Trigram Vectors. We use the method of [ 21 ] to generate vectors for the topic and its candidate labels based on letter trigrams. Each dimension in a letter trigram vector represents a letter trigram (e.g. abc or acd ). We generate a of letter trigrams from the phrase. For example, the letter trigram multiset of each dimension i in the letter trigram vector of phrase l , we assign an integer value based on the frequency of the corresponding letter trigram in the multiset, and normalize the counts to sum to one.
 top-10 LDA words, and take the union of the individual letter trigram multisets to calculate the overall letter trigram distribution for the top-10 words. We derive a vector representation for the topic based on the combined letter frequencies, and once again, normalize the counts to sum to one. 3.4 Topic Label Selection After generating vectors for candidate labels and LDA topics, we then calculate the similarity between them based on cosine similarity. 4.1 Dataset &amp; Gold Standard We use three corpora in our experiments: (1) News , (2) Twitter NIPS .The News and Twitter corpora are from [ 15 ], while the is a collection of NIPS abstracts from 2001 to 2010, commonly used for topic model evaluation.
 effect of the topic labelling method when T (the number of topics) is set to 30, 40 and 50 for each corpus. We use a within-topic entropy-based method to filter bland topics, i.e. topics where the probability distribution over the component words is relatively uniform, based on: In the News and Twitter corpora, topics with an entropy higher than 0.9 were eliminated, and in the NIPS corpus, topics with an entropy higher than 1.4 were eliminated; these thresholds were set based on manual analysis of a handful of topics for each document collection. For the Twitter further filtered topics which lack a meaningful gold-standard topic label, based on the method described later in this section. Table 2 provides details of the datasets.
 Yang [ 25 ] observed that gold standard labels from human beings suffer from inconsistency. The inter-annotator F-measure between human annotators for our task is 70 X 80 %. In an attempt to boost agreement, we developed an automatic method to generate gold standard labels to evaluate the proposed method: for each topic z , we extract chunks from titles in D z , assign a weight to each chunk according to the word frequency in that chunk, and select the chunk that has the highest weight as the label ( X  X S X ) for that topic. Our underlying motivation in this is that each headline is the main focus of a document. A phrase from a title is a good representation of a document. Therefore a phrase from a title can be a good label for the predominant topic associated with that document. Note that the News and NIPS corpora have titles for each document, while the Twitter corpus has no title information. The gold standard for the and NIPS corpora were thus generated automatically, while for the corpus  X  which was collected over the same period of time as the pus  X  we apply the following method, based on [ 15 ]: (1) calculate the cosine similarity between each pair of Twitter and News topics, based on their word distributions; (2) for each Twitter topic i , select the News the highest cosine similarity with i and where the similarity score is greater than a threshold (0.3 in this paper). The label (GS) of News topic j is then regarded as the gold standard (GS) label for Twitter topic i . 4.2 Evaluation Metrics We evaluate our results automatically and via human evaluation.
 Automatic Evaluation Method. Because of the potential for semantically directly with the GS automatically. Rather, we propose the following evalua-tion: where Lin is word similarity based on WordNet, in the form of the information-theoretic method of Lin [ 26 ]. GS and l represent the gold standard and the label generated for topic z , respectively. The Porter stemmer all words. The score is used to measure the semantic similarity between an automatically-generated and GS label.
 Human Evaluation Method. We also had six human annotators manually score the extracted labels. Each annotator was presented with the top-10 LDA words for a given topic, the gold standard label, and a series of extracted labels using the methods described in Sect. 3 . They then score each extracted label as follows: 3 for a very good label; 2 for a reasonable label, which does not com-pletely capture the topic; 1 for a label semantically related to the topic, but which is not a good topic label; and 0 for a label which is completely inappro-priate and unrelated to the topic. We average the scores from the six annotators to calculate the overall topic label score. 4.3 Baseline Methods LDA-1. Simply select the top-ranked topic word as the topic label. DistSim. This method was proposed by [ 16 ], and involves generating a word vector of candidate labels according to first-order cooccurrence-based PMI values in the original corpus. In this paper, the first-order vector is used in our vector-based method shown in Fig. 1 . 4.4 Experimental Results The word2vec toolbox 3 was used to train the CBOW and Skip-gram models. The window size was set to 5 for both models. We experimented with word vectors of varying dimensions; the results are shown in Fig. 2 , based on automatic evaluation. When the number of dimensions is 100, the result is the best on average, and this is the size we use for both CBOW and Skip-gram throughout our experiments. The dimension of the letter trigram vector is 18252. ferent numbers of topics. We can see that the results vary with the number of topics. When the topic number T is 50, the score for the corpora is the highest; and when the topic number is 40, the score for the Twitter corpus is highest.
 and our methods, over the News-50 , Twitter-40 and NIPS-50 follows: 1. Most methods perform better over NIPS than News and Twitter 2. The Skip-gram model performs much better than CBOW over 3. The letter trigram vectors perform surprisingly consistently over the three The reason might be that in this method, we compare the top-1 word with a phrase (GS). Our methods are also better than the DistSim baseline in most cases. Our result shows that trigram vectors are more suitable for topic labelling over different types of corpus. Skip-gram is better than CBOW for NIPS , while CBOW is more suitable for News . Table 3 also shows the results for human evaluation. We summarize the results as follows: 1. Similar to the automatic evaluation results, the score over than the other two corpora. The score for News is higher than the score for Twitter . Under human evaluation, labels generated using vector-based methods are on average reasonable labels for NIPS , and somewhat reasonable labels for News . Even for a corpus without title information like it can extract related topic labels. 2. Human evaluation achieves very similar results to our automatic evaluation; in fact, we calculated the Pearson correlation between the two and found it to be remarkably high at r =0 . 84. This shows that our automatic evaluation method is effective, and can potentially save manual labor for future work on topic label evaluation. 4.5 Effectiveness of Topic Labelling Method To show the effectiveness of our method, some sample topic labels from Twitter and NIPS are shown in the Table 4 . Full results over the three corpora are available for download from: http://lt-lab.sjtu.edu.cn/wordpress/wp-content/uploads/2014/ 05/topic%20label%20result.zip We have proposed a novel method for topic labelling using embeddings and letter trigrams. Experiments over three corpora indicate that all three kinds of vectors are better than two baseline methods. Based on the results for automatic and human evaluation, labels extracted using the three vector methods have reasonable utility. The results of word vector models vary across the different corpora, while the letter trigram model is less influenced by the genre of the corpus. The limitation of word vectors is that the quality of a topic label relies on the quality of the word vector representation, which in turn is influenced by the corpus size. The novelty of our work includes the use of embeddings for label ranking, the automatic method to generate gold-standard labels, and the method to automatically evaluate labels. In the future, we plan to do more experiments on different types of corpora. Letter trigram vectors do not need training, and are more suitable for different types of corpus. We also plan to do more experiments on different types of vector representations and on vector combination, and also extrinsic evaluation of the topic labels [ 8 ].
