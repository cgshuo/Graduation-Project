 1. Introduction
Bibliometric analyses and other information retrieval/data mining (IR/DM) techniques are powerful instruments for unlocking the contents of scientific publications. Via the Web, these techniques can be made available to everyone, and in-deed an increasing number of Digital Libraries and search engines offer citation-network analysis functionality. However, existing proposals often neglect a fundamental aspect of learning: that understanding and learning require an active and con-structive exploration of a domain.

In this paper, we describe a new method and a tool that use machine intelligence and interactivity to turn the typical search and retrieve dialogue, in which the user asks questions and a system gives answers, into a dialogue that also involves sense-making , in which the user has to become active by constructing a bibliography and a domain model of the search term(s). We call this activity context creation (for a search and search term).

This tool is part of an integrated authoring system covering all phases from search through reading and sense-making to writing. The goal of this system is to show scientists what they can gain from (high-quality) citation data, in order to motivate them, in the same environment , to contribute such high-quality data  X  again using techniques of IR/DM (Berendt, Dingel, &amp; Hanser, 2006 ). The goal is to create a positive-feedback loop in which more and more correct meta-data are created and used for scholarly progress. To the best of our knowledge, no other comparable tool or service ex-ists that is modelled on the whole process of scientific writing and that accompanies authors in their standard environment.

After a survey of related work in Section 2 of this paper, Section 3 describes our system X  X  architecture and the algorithmic methods and usage interface of the tool. In Section 4, we summarise two evaluation studies that assess qualitative and quan-titative quality criteria. Section 5 concludes with an outlook. 2. Motivation and related work
A central goal of our tool is to support an active construction of a search-context and domain structure by the user, based on pertinent information in the documents. The measurement of such pertinent information has been studied by the disci-tions and texts.

Citation analysis serves to detect the structure and evolution of science: generic vs. specialised authors and topics,  X  X  X pe-cialty narratives X , the changing  X  X  X rontiers of science X , and changes in paradigms (see Chen, 2003 for an overview).
Tools to tap this huge potential of citation data are therefore interface elements for repositories like http:// citeseer.ist.psu.edu/ http:/citeseerx.ist.psu.edu www.citebase.org, www.slac.stanford.edu/spires/hep , portal.acm.org , scho-lar. google.com , www.arxiv.org or http://www.thomsonreuters.com/products_services/scientific/Web_of_Science . All repos-itories offer some form of topical and metadata search. Navigation in the document network is generally based on lists of documents citing or cited-by the currently-viewed document. Links to similar documents (based on text, co-citation, or bib-liographic coupling) are offered in some repositories. Recommendations based on such local similarities have been found to be useful in suggesting new references (McNee et al., 2002 ).

Various tools show bibliometric measures like number of citations, rankings of the authors or journals of the found pub-lications, etc., or visualisations of the repositories and their semantics (Chen, 1999; Chen, 2003 ).
A problem of all these tools is that they do not support the user in constructing a domain model of a researched field. They sentation of an automatically generated model, or they help in navigating but do not support model building. Also, tools that perform overview analyses on large datasets (Small, 1994; Chen &amp; Carr, 1999; Chen, 2003; Chen, 2006 ) are generally not available to the public, or they operate on pre-processed test sets rather than on live Digital Libraries (DLs).
Recently, analysis tools have been developed that are available either for local installation and use with arbitrary datasets
Fortuna, Grobelnik, &amp; Mladenic, 2005 ). Search engines like www.clusty.com or www.kartoo.com employ similar text-based techniques on Web resources. However, the user can neither choose the level of granularity nor interactively label clusters. clustering documents. A well-known precursor of this method was proposed by Cutting, Pedersen, Karger, and Tukey (1992) .
Feng, Jeusfeld, and Hoppenbrouwers (2005) envisage future DLs in which knowledge thus learned adds a  X  X  X nowledge sub-space X  (which could be queried directly for factual information) to the usual  X  X  X ocument subspace X  (which can be queried (which do not always make sense).

Janssens, Gl X nzel, and De Moor (2008) gave an overview of studies published since the seminal work of Braam, Moed, and van Raan (1991) , which have shown that hybrid clustering methods that incorporate text and citation information can out-perform clustering methods that use only one of these components, and they proposed a new hybrid method that outper-forms both. Another form of hybrid clustering combines text-based document similarities with author similarities; this can live DLs, and they have no context-creation components.

Interactive sense-making requires tightly interrelated activities of searching and representation forming (such as context by allowing users to manually create categories. The system of Zhang, Qu, Giles, and Song (2008) relies on the structure found in the ACM subject hierarchy placement of publications. Bier, Good, Popat, and Newberger (2004) describe an interface for detail-and-context visualizations of such a structure. However, none of these tools leverage machine intelligence.
In sum, many solutions for analysing publications or documents exist, but all of them, when viewed from the perspective of our goal of active and constructive domain-model building, are only partial solutions. We encourage interactive model
Chen, 2006 ), and build on word profiles for characterising the content of document clusters (as in Braam et al., 1991; Chen, but in contrast to all existing approaches, we realize this in an architecture based on Web services that is thereby modular, configurable and easily extensible. 3. The context creation tool 3.1. Functionality and user interface
The context creation tool has two front-ends in order to help users group documents in their preferred working environ-ment: Web browser and MS Word. The first is familiar to and preferred by all users especially for searching, retrieving, and discussing with others, and popular also for PDF reading with standard browser plugins. The second is a highly popular writ-ing environment for many users.

In the context creation tool, the first user input is a search term. Outputs are generated in three stages: First, a biblio-tering has a default mode for non-expert users and configurable options for expert use. The user is encouraged to label and describe the groups, and to modify the automatically-derived grouping to both reflect and develop her perception of the sci-presented below the cluster name. The user can request a re-computation of the keywords after she has changed cluster con-tents by deletions or insertions.

Fig. 1 shows an example screenshot. In a search for literature on  X  X  X FID X , the user has already labelled the first group (as functionality: specifying the number of desired results, extending a search result by more and/or different-search-term re-sults, and deleting and moving result documents between different clusters.

Third, the user can save and reload results, include them in personal documents, and make them available to others ( dis-cussion ), via the Web in a format amenable for further processing (XML+CSS), see Fig. 2 for an example. The tool is deployed at http://www.cs.kuleuven.be/~berendt/CiteseerCluster .
 3.2. Data processing and data sources
The user interfaces interact with a a back-end layer operating on a server (via Web server for the browser front-end and via Web services for the MS Word front-end). This layer accesses local data, search-engine Web services, and a remote repos-and rich structure, and also because it offers an OAI interface.
Processing has five stages: (1) The search term is transformed into a request to the Yahoo! Search Web service (http://developer.yahoo.com/search ), (2) For each document d in the result set, two lists of IDs are compiled: all documents that cite d , and all documents that (4) A similarity matrix for D is constructed.

Various sources of similarity are considered: co-citation, bibliographic coupling, and text-based similarities. Citation mea-sures are derived from the citation matrix; text measures are derived from the abstract (a traditional choice, see e.g. Braam LSA.

The default is bibliographic coupling, which, like co-citation, is a classic type of document linkage extensively researched long enough to be cited by many sources. Bibliographic coupling (Kessler, 1963 ) can help to group especially new documents faster: A new publication will associate itself to an existing cluster of similar documents by referring to the same sources.
Both methods aggregate independent opinions about which documents are related, thus taking advantage of the robustness of  X  X ollective intelligence X .

To combine citation and text information, the user can specify a weighting factor for a linear combination between the two similarity matrices. 5
As default, we use the Jaccard coefficient. The Jaccard coefficient is a popular, proven, and scalable method of mea-suring similarity between Web documents (Haveliwala, Gionis, Klein, &amp; Indyk, 2002 ), and it has been used in co-citation (Small &amp; Greenlee, 1980) as well as bibliographic-coupling ( Bani-Ahmad, Cakmak,  X zsoyoglu, &amp; Al-Hamdani, 2005 ) analyses.

The resulting similarity values are then derived from the combination of similarity source and measure. For example, the bibliographic coupling similarity between documents d 1 ; d
Only documents that can contribute to the numerator are considered, operationalized as documents that appeared in the minimum of the publication years of d 1 and d 2 or earlier (analogously for co-citation).

D may contain  X  X solated documents X  (Small &amp; Griffith, 1974 ) that are not co-cited with anything, or that do not co-cite with anything. This can be detected by all-zero columns or rows in the citation matrix; both the row and column are deleted such that a c c similarity matrix, with c 6 r , remains. (5) The non-isolated documents from D are clustered using the toolkit CLUTO (http://www.cs.umn.edu/~karypis/cluto ).
Different clustering methods can be chosen. Their selection was the result of prior experiments with the methods imple-mented in CLUTO. The user of our system can choose between hierarchical agglomerative clustering with complete link or
UPGMA,  X  X  X irect clustering X  (a method similar to k-means, with a global optimality criterion), and RPR (repeated bisections with a global optimality criterion). The default is RPR.
 clusters specified by the user. The minimum c 1 guarantees that there is at least one two-element cluster. Alternatively, an optimal number of clusters is determined by the highest Silhouette value in the interval between 2% and 15% of the number of documents (cf. Tan et al., 2005; Janssens et al., 2008 ).

Tests showed that the clustering computation step, even if repeated, is very fast (the computation of the similarity matrix requires most time in the processing of most search queries).

If present, two additional groups are shown: isolated documents and documents whose citation links could not be ana-lyzed because they are not in the local database. This is done to avoid arbitrary assignments while respecting that citation-based clusters do not represent the entire relevant literature that covers a topic (Braam et al., 1991 ).
An example. To illustrate this process, we present the following fictitious and necessarily small example of the stages described above: The repository contains documents d 1 ; ... ; d
The search engine identifies documents D  X f d 1 ; d 2 ; d be the same for any user-specified number P 6). Since the very recent d database contains (only) the following citation relations (first document cites second document):  X  d ; d 5  X  ;  X  d 1 ; d 6  X  ;  X  d 2 ; d 1  X  ;  X  d 3 ; d 1  X  ;  X  d
To keep the example simple, we consider the exclusive use of the Jaccard coefficient for bibliographic coupling as the similarity measure. We also assume that both d 2 and d 3 have been published after d published after both d 3 and d 2 . The data yield: sim bc from D . d 1 does not co-cite with anything and is therefore the only element in the set of isolated documents. In sum, this produces c  X  4 and a 4*4 similarity matrix. (5) The user chooses hierarchical agglomerative clustering and desires to see seven clusters. The system then forms min  X  7 ; c 1  X  X  min  X  7 ; 3  X  X  3 clusters. The clustering solution is ff d 2 ; d 3 g ; f d 4 g ; f d 5 gg plus the isolated-documents group f d would be the same for any user-specified number of clusters n P 3, and the further settings of the clustering algorithm do not affect the result.) n  X  2 or a system-optimised cluster number would assemble d cluster. 4. Evaluation
We evaluated the tool with a combination of data-mining and usability quality measures. The purpose was not to eval-
Cook, 1984; Bani-Ahmad et al., 2005 ). Rather, our focus was the usefulness of the clustering and interaction for end users. 4.1. Cluster quality
Concept and measures Context creation is a knowledge-discovery task: designed to find  X  X  X alid, novel, potentially useful, are document groups.

Validity would ideally be assessed by traditional measures of cluster validity; specifically external or relative measures example, with different numbers of clusters) by indexes that combine the goals of maximizing intra-cluster similarities and minimizing inter-cluster similarities.
 by arbitrary search terms. An application of relative measures for sample datasets produced, due to the sparsity of the sim-showed us that it may be more useful to have some user-desired number of clusters in order to keep a survey view of a topic, because different information needs may make a coarser or more fine-grained clustering desirable (see Tuzhilin, 2002 for the general necessity of subjective measures of usefulness).

In bibliometrics-based work, the usual procedure for judging cluster validity and usefulness is to ask experts. The ques-tions asked are akin to external and relative measures, but they are necessarily posed qualitatively and elicit answers that involve subjective assessments. Examples are  X  X  X o clusters represent specific research topics? X  and  X  X  X o these topics differ sequent research? X  and  X  X  X ould you explain the possible nature of such connections? X  (Chen, 2006 ), the  X  X  X elatedness X  be-tween bibliographically coupled documents (Vladutz &amp; Cook, 1984 ), or the  X  X  X elevance X  of clusters as new concepts for an ontology ( Spiliopoulou, Schaal, M X ller, &amp; Brunzel, 2006 ).

Based on these proposals, we define and obtain the measure cluster integrity for a given clustering solution: We first ask aged over all clusters. In addition, we ask the expert whether and which clusters overlap strongly in content. We divide the number of overlapping clusters by the total number of clusters to obtain the measure of cluster impurity . 4.1.1. Method
We asked two domain experts to determine cluster quality. any choice of search terms for such an evaluation must necessarily be exemplary. We therefore chose 10 search terms that we considered broad and semantically ambiguous enough to produce distinct subtopics (see Table 1).

To find an approximation of a  X  X  X seful X  clustering solution, we considered cognitive capacity: It is well-known that the number of information  X  X  X hunks X  that people can handle simultaneously is limited (see the classic article by Miller (1956) and the literature following it). To use an empirically motivated value, we formed the average of the numbers of document groups that our test users settled on in their final organisation of results (see Section 4.2). The rounded average number of clusters was seven, which is also in line with Miller X  X  (1956) results.

All clusters were formed from a result set large enough to produce at least 30 non-isolated documents ( X  X  X FID X : 25, the maximum result set in CiteSeer). Total result set sizes ranged from 55 to 108, which were, by the Yahoo! ranking, also the most relevant results. The tool X  X  default settings (including bibliographic coupling) were used. 4.1.2. Results and discussion
Results are shown in Table 1 . From the given titles for the different clusters, one can see that topics range from very gen-eral collections to specialized topics. The results also illustrate commonalities and differences between raters. First, the quantitative measures, shown at the top of the table, of cluster integrity and impurity were similar, but not identical. An analogous observation can be made about the qualitative cluster labels shown at the bottom of the table (for reasons of space, they are only listed for the first four search terms). The labels show that cluster content was generally perceived in and it points to the paramount importance of treating the machine-generated clusters as a startpoint for users X  individual and interactive (re-)grouping of documents. 4.1.3. Limitations and future work
The obtained clustering results are useful, but not perfect; clusters arose whose elements could belong to other clusters as  X  X  X ot in database X , and sources such as DBLP, ACM and Google Scholar. In future work, we also intend to study systematic with this type of data better.

Larger sets of raters and clusters will be necessary to further investigate cluster quality and usefulness and inter-rater agreement, as well as possible specific fits between user groups and methods ( Spiliopoulou et al., 2006 ) or fields and methods. 4.2. Usability and cognitive support 4.2.1. Method 15 graduate students with some experience in online literature search worked with the Web-based version of the tool and answered questionnaires. 9 The questionnaire contained 21 statements to be assessed on a five-point Likert Scale (ranging were and which additional ones would be considered most helpful.
 (1990) that 3 X 5 users generally suffice for a heuristic and formative evaluation.)
Students in both groups were first given a task in which grouping was not mentioned and then a task in which grouping was encouraged (literature search for a course essay or publication without/with the instruction to present aspects and sub-areas of the search term). For about 1.5-2 hours, participants worked on two search terms from the list in Table 1 above.
Instructions were given to structure the searches and make them comparable. After the completion of each task, participants were asked to write down a mind map or list (in the following:  X  X  X ind map X ) summarizing their results. Participants were then asked to fill out the questionnaire.
 ducted. An expert judged the quality of the groupings and mind maps. 4.2.2. Results and discussion: Usability
Participants of the experimental group largely agreed that the tool was usable (median rating of all items of a usability dimension, reversing ratings of negatively-phrased items; averaged over all items of a usability dimension): satisfaction (2.5), usefulness (2.33), ease of learning (2.17), control (3.5), and efficiency (2.67). way of searching, and 50% said that grouping would be a helpful new feature. 4.2.3. Results and discussion: Groupings
All participants in the experimental condition used the clustering tool and the opportunities to delete and re-group doc-ticipants in both conditions produced (usually hierarchical) mind maps of the search term topics.

Table 2 shows results on document-group and mindmap-concept numbers and quality. Reported proportions are relative to the number of participants who produced groups and mind maps in the respective task (11 in task 1, 9 in task 2). The results indicate that grouping was used more extensively in task 2 than in task 1 and that the degree of hierarchical struc-turing increased, both within the mind maps X  concepts and between mind-map concepts and cluster-led groups. Also, the quality of both groupings and mind maps increased, and keywords and labels from the cluster-led solution were re-used more extensively for the mind maps in the second task. The re-use of keywords and labels was observed to be meaningful, especially in the second task, and the participants with good groupings also created meaningful mind maps. The changes between the tasks are evidence of learning, including a transition to using and developing the groupings as a first step to-wards a high-level domain model. 4.2.4. Limitations and future work
These results cannot establish whether people obtained better conceptual structures of the domain of their search term than they would have done without the automatic grouping. An inspection of the mind maps showed that the used method had left many degrees of freedom and introduced noise. In order to test the strong claim of tool usefulness, one would need to confront all participants with a topic about which they know little, subject them to pre-and post-tests of knowledge, and allow for significantly more time for in-depth topic researching. 5. Conclusions and outlook
In this paper, we have proposed a general system architecture and a concrete tool as part of such a system for supporting (search/retrieval and sense-making), encouraging authors to actively construct and re-construct literature lists and domain models,andto engageindiscussion.UsingYahoo!andCiteSeer,thetooloffersa groupingofliteratureusing bibliographic-cou-pling, co-citation and textual similarity which can be changed, tagged and re-used by the tool X  X  users.
Evaluation studies showed that the interactive and constructive nature was welcomed and seen as a chance to learn more about metadata, citations, and the  X  X  X eb of science X . We argued that the judgment of clusters and document groups con-structed from them must involve subjective criteria, and showed that clusters and groups represent identifiable sub-topics.
In future work, we plan to professionalize the system and develop a workflow for keeping the system and its use of other resources up to date. For example, the tool is being updated to work with CiteSeerX (http://citeseerx.ist.psu.edu ), which went online after the bulk of the work described here was done. CiteSeerX is currently (April 2009) in beta stage, and the major search engines index some combinations of CiteSeer and CiteSeerX. The functionalities relevant for our system have not changed, so a migration is rather straightforward.

In addition, we aim to extend functionality, in particular discussion. Currently, we only encourage the user to assign some shaw (2003) . The combination of personal sense-making, referential tagging, and Web2.0 tagging promises to lead to the next generation of intelligent authoring tools.
 Acknowledgements We thank Lee Giles and Isaac Councill for providing us with the CiteSeer code and many answers to our questions. References
