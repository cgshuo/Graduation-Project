 Frequent episode discovery is a popular framework for min-ing data available as a long sequence of events. An episode is essentially a short ordered sequence of event types and the frequency of an episode is some suitable measure of how often the episode occurs in the data sequence. Recently, we proposed a new frequency measure for episodes based on the notion of non-overlapped occurrences of episodes in the event sequence, and showed that, such a definition, in addition to yielding computationally efficient algorithms, has some important theoretical properties in connecting fre-quent episode discovery with HMM learning. This paper presents some new algorithms for frequent episode discov-ery under this non-overlapped occurrences-based frequency definition. The algorithms presented here are better (by a factor of N , where N denotes the size of episodes being dis-covered) in terms of both time and space complexities when compared to existing methods for frequent episode discov-ery. We show through some simulation experiments, that our algorithms are very efficient. The new algorithms pre-sented here have arguably the least possible orders of space and time complexities for the task of frequent episode dis-covery.
 H.2.8 [ Information Systems ]: Database Management X  Data mining Algorithms Event streams, frequent episodes, temporal data mining, non-overlapped occurrences  X  This work was carried out when Srivatsan Laxman was at Department of Electrical Engineering, Indian Institute of Science, Bangalore.

Frequent episode discovery [8] is a popular framework for temporal data mining. The framework is applicable on data available as a single long sequence of ordered pairs, ( E which are called as events. In each event, ( E i , t i ), E ferred to as an event type (which takes values from a finite alphabet, E ) and t i is the time of occurrence of the event. The data, which is also referred to as an event sequence (or an event stream), is ordered according the times of occur-rence. There are many applications where data appears in this form, e.g., alarm sequences in telecom networks [8], web navigation logs [1, 2], time-stamped fault report logs from manufacturing plants [6, 7], etc.

The framework of frequent episode discovery [8] can be used to mine temporal patterns from event streams. The temporal patterns, referred to as episodes , are essentially small, (partially) ordered collections of event types. For ex-ample, ( A  X  B  X  C ) denotes a temporal pattern where an event type A , is followed (some time later) by a B and a C , in that order. When events of appropriate types appear in the data sequence, in the same order as in the episode, these events are said to constitute an occurrence of the episode. For example, in the data sequence h ( A, 1), ( D, 2), ( E, 4), ( B, 5), ( D, 6), ( C, 10) i , the episode ( A  X  B  X  C ) occurs once. An episode is considered interesting if it occurs  X  X f-ten enough X  in the data. Stated informally, the framework of frequent episodes is concerned with the discovery of all episodes that occur often in the data. To do this, we need to define a frequency measure for episodes in the data. The data mining task is to find all episodes whose frequencies exceed a user-defined threshold. This paper presents a new and very efficient algorithm for frequent episode discovery.
The framework of discovering frequent episodes in event streams was introduced by Mannila, et al. [8]. They define the frequency of an episode as the number of windows (of prefixed width) on the time axis in each of which the episode occurs at least once. They propose a counting algorithm using finite state automata for obtaining the frequencies of a set of candidate episodes. The worst case time complexity of the algorithm is linear in the total time spanned by the event stream, the size of episodes and the number of candidates. The space needed by the algorithm is also linear in the size of episodes and the number of candidates. Some extensions to this windows-based frequency have also been proposed [2, 9]. There have also been some theoretical studies into this framework whereby one can estimate (or bound) the expected frequency of an episode in a data stream of a given length if we have a prior Markov or Bernoulli model for the data generation process. Thus, if sufficient training data is available, we can first estimate a model for the data source, and then, on new data from the same source, can assess the significance of discovered episodes by comparing the actual frequencies with the expected frequency [3, 4, 10].
Recently, we have proposed [6, 7] a new notion for episode frequency based on the non-overlapped occurrences of an episode in the given data sequence. In [6], we have also pre-sented an efficient counting algorithm (based on finite state automata) to obtain the frequencies for a set of candidate episodes. This algorithm has the same order of worst case time and space complexities as the windows-based counting algorithm of [8]. However, through some empirical inves-tigations, it is shown that the non-overlapped occurrences -based algorithm is much more efficient in terms of the ac-tual space and time needed, and that, on some typical data sets, it runs several times faster than the windows-based al -gorithm. It is also seen that our new frequency definition results in qualitatively similar kinds of frequent episode s be-ing discovered (as in the case of windows-based frequency) and all the prominent correlations in the data come out among the top few frequent episodes under both frequency definitions [6]. Another important advantage of the non-overlapped occurrences count is that it facilitates a forma l connection between discovery of frequent episodes and lear n-ing of generative models for the data sequence in terms of some specialized family of Hidden Markov Models [6]. This formal connection allows us to assess statistical significa nce of episodes discovered without needing any prior model es-timation step (and thus obviating the need for any separate training data). Our formal connection also allows one to fix a frequency threshold automatically and in empirical stud-ies, this automatic threshold is seen to be quite effective [6 ]. All this makes the non-overlapped occurrences count an at-tractive method for applications involving frequent episo de discovery from event streams.

In this paper, we present a new algorithm for frequent episode discovery under the frequency count based on non-overlapped occurrences. The algorithm is significantly sup e-rior to that proposed in [6] both in terms of time and space complexities. The space complexity is same as number of candidates input to the algorithm and the time complexity is linear in the number candidates and the total number of events in the data stream. Unlike the existing algorithms fo r frequent episode discovery [6, 8], the time and space com-plexities do not depend even on the size of episodes being discovered. This is because our algorithm needs only one automaton per episode, while the other algorithms need N automata per episode, where N is the size of (or number of nodes in) the episodes being counted. We believe that our algorithm attains the minimum possible order of worst case time and space complexities for the frequent episode discov -ery process. Thus, our new algorithm is a very competitive alternative to all existing algorithms for frequent episod e discovery.

The paper is organized as follows. Sec. 2 presents a brief overview of the frequent episode discovery framework. Sec. 3 presents the new frequency counting algorithms. We demon-strate the effectiveness and efficiency of our new frequency counting algorithms through some simulations in Sec. 4. In Sec. 5 we present the conclusions.
Formally, an episode,  X  , is defined by a triple, ( V  X  ,  X  g ), where V  X  is a collection of nodes,  X   X  is a partial or-der on V  X  and g  X  : V  X   X  E is a map that associates each node in  X  with an event type from a finite alphabet, E . When  X   X  represents a total order among the nodes of  X  , the episode is referred to as a serial episode, and when  X  is trivial (or empty), the episode is referred to as a paral-lel episode. Given an event sequence, h ( E 1 , t 1 ) , . . . , ( E an occurrence of episode  X  = ( V  X  ,  X   X  , g  X  ) in this event se-quence, is an injective map, h : V  X   X  { 1 , . . . , n } , such that g ( v ) = E h ( v ) for all v  X  V  X  , and for all v, w  X  V  X  v  X  w we have t h ( v )  X  t h ( w ) . Finally, an episode  X  is said to be a subepisode of  X  if all the event types in  X  appear in  X  as well, and if the partial order among the event types of  X  is the same as that for the corresponding event types in  X  .
As mentioned earlier, the episode X  X  frequency is some mea-sure of how often it occurs in the data. There are many ways to define episode frequency [2, 6, 8]. In the original frame-work of [8], the frequency of an episode was defined as the number of fixed-width sliding windows over the time axis that each contain an occurrence of the episode. In this pa-per, we consider the non-overlapped occurrences-based fre -quency definition proposed in [6]. Two occurrences of an episode are said to be non-overlapped if no event correspond-ing to one occurrence appears in between events correspond-ing to the other. Definition 1 given below, formalizes this notion of non-overlapped occurrences, using the notation that was just introduced for episodes and their occurrences in an event stream.

Definition 1. Consider an N -node episode  X  = ( V  X  , &lt; g ) where V  X  = { v 1 , . . . , v N } . Two occurrences, h 1 of  X  are said to be non-overlapped if, either (i) h 2 ( v h ( v j )  X  v j  X  V  X  or (ii) h 1 ( v 1 ) &gt; h 2 ( v j )  X  v tion of occurrences of  X  is said to be non-overlapped if every pair of occurrences in it is non-overlapped. The correspond -ing frequency for episode  X  is defined as the cardinality of the largest set of non-overlapped occurrences of  X  in the given event sequence.

The standard approach to frequent episodes discovery is to use an Apriori-style level-wise procedure. Starting wit h frequent episodes of size 1, frequent episodes of progressi vely larger sizes are obtained (till there are no more frequent episodes at some level). Each level involves two steps  X  a candidate generation step and a frequency counting step. Candidate generation in the ( N + 1) th level, takes frequent episodes of size N and combines them in all possible ways to obtain a set of potential frequent episodes (referred to as candidate episodes) of size ( N + 1). Candidate gener-ation exploits the anti-monotonicity of episode frequency , i.e. frequency of an episode is bounded above by the fre-quencies of its subepisodes. Hence, whenever an episode has frequency greater than the user-defined threshold, all its subepisodes would have also met this frequency thresh-old criterion at previous levels in the algorithm. The fre-quency counting step obtains the frequencies for the candi-date episodes (of a given size) using one pass over the data. This data pass is the main computationally intensive step in frequent episodes discovery. In the next section, we presen t some very efficient frequency counting algorithms under the non-overlapped occurrences-based frequency. Before that , we first illustrate how the choice of definition for episode frequency has a direct bearing on efficiency of the frequency counting step of the frequent episode discovery process.
Consider the following example event sequence: The occurrence of a serial episode may be recognized using a finite state automaton that accepts the episode and rejects all other input. For example, for the episode ( A  X  B  X  C ), we would have an automaton that transits to state 1 on seeing an event of type A and then waits for an event of type B to transit to its next state and so on until it transits to its final state, when an occurrence of the episode is regarded as complete. Intuitively, the total number of occurrences o f an episode seems to be a natural choice for frequency of an episode. However, counting all occurrences turns out to be very inefficient. This is because different instances of the automaton of an episode are needed to keep track of all its state transition possibilities. For example, there are a to tal of eighteen occurrences of the episode ( A  X  B  X  C ) in the event sequence (1). We list four of them here: 1. { ( A, 1) , ( B, 3) , ( C, 8) } 2. { ( A, 1) , ( B, 3) , ( C, 12) } 3. { ( A, 1) , ( B, 3) , ( C, 13) } 4. { ( A, 1) , ( B, 9) , ( C, 12) } On seeing the event ( A, 1) in the event sequence (1), we can transit an automaton of this episode into state 1. However, at the event ( B, 3), we cannot simply let this automaton transit to state 2. That way, we would miss an occurrence which uses the event ( A, 1) but some other occurrence of the event type B later in the sequence. Hence, at the event ( B, 3), we need to keep one instance of this automaton in state 1 and transit another new instance of the automaton for this episode into state 2. As is easy to see, we may need spawning of arbitrary number of new instances of automata if no occurrence is to be missed for an episode. Moreover, counting all occurrences renders candidate generation ine f-ficient as well. This is because, when using total number of occurrences as the frequency definition, subepisodes may be less frequent than corresponding episodes. For example, in (1), while there are eighteen occurrences of ( A  X  B  X  C ), there are only eight occurrences of the subepisode ( A  X  B ). So, under such a frequency definition, level-wise procedure s cannot be used for candidate generation. Hence, the ques-tion now is what kind of restrictions on the class of occur-rences will lead to an efficient counting procedure?
Recall that each occurrence, h , of episode  X  , is associated with a set of events { ( E h ( v stream. Two occurrences, h 1 and h 2 , of an episode  X  are said to be distinct if they do not share any events in the event sequence, i.e., if h 1 ( v i ) 6 = h 2 ( v j )  X  v i event sequence (1), for example, there can be at most three distinct occurrences of ( A  X  B  X  C ): 1. { ( A, 1) , ( B, 3) , ( C, 8) } 2. { ( A, 2) , ( B, 9) , ( C, 12) } 3. { ( A, 7) , ( B, 10) , ( C, 13) }
It may appear that if we restrict the count to only dis-tinct occurrences, we may get efficient counting procedures. However, while subepisodes now will certainly be at least as frequent as the episodes, the problem of needing unbounded number of automata remains. This can be seen from the following example. Consider the sequence In such a case, we may need (in principle) any number of instances of the ( A  X  B  X  C ) automaton, all waiting in state 2, since there may be any number of events of type C occurring later in the event sequence. Hence there is a need for further restricting the kinds of occurrences to cou nt when defining the frequency.

Definition 1 provides an elegant alternative for defining frequency of episodes based on non-overlapped occurrences. Two occurrences of an episode in an event sequence are non-overlapped if no event corresponding to one occurrence appears in between events corresponding to the other oc-currence. In (1) there can be at most one non-overlapped occurrence of ( A  X  B  X  C ), e.g., { ( A, 2) , ( B, 3) , ( C, 8) } (since every other occurrence of ( A  X  B  X  C ) in (1) over-laps with this one). Similarly, in (2) we need to keep track of only one of the pairs of event types A and B , since any other occurrence of ( A  X  B  X  C ) will have to overlap with this occurrence. In general, there can be many sets of non-overlapped occurrences of an episode in an event sequence. For example, in the event sequence (1), { ( A, 2) , ( B, 3) } and { ( A, 7) , ( B, 9) } are two non-overlapped occurrences of the 2-node episode, ( A  X  B ). However, if we consider the oc-currence { ( A, 2) , ( B, 9) } , then there is no other occurrence that is non-overlapped with this one in the sequence (1). This means that the number of non-overlapped occurrences, by itself, is not well-defined. For this reason, we define the frequency in Definition 1 as the cardinality of the largest set of non-overlapped occurrences.

In terms of automata, to count non-overlapped occur-rences of an episode, only one automaton is needed. Once an automaton for the episode is initialized, no new instance of the automaton needs to be started till the one that was initialized reaches its final state. In Sec. 3.1, we present t he associated frequency counting algorithm and show that, us-ing just one automaton per episode, it is possible to count the maximal (or largest) set of non-overlapped occurrences for a serial episode. Finally we note that although the ex-amples discussed above are all serial episodes, Definition 1 prescribes a frequency measure for episodes with all kinds of partial orders (including the trivial partial order case of parallel episodes). In Sec. 3.2, we present an efficient algo-rithm for obtaining the non-overlapped occurrences-based frequency for parallel episodes and indicate later in a dis-cussion how these may be extended to the case of counting episodes with general partial orders as well.
This section presents some new frequency counting algo-rithms for frequent episode discovery. In Sec. 3.1, we first present an algorithm for counting non-overlapped occur-rences of serial episodes. Then in Sec. 3.2, we show how the algorithm can be adapted to obtain the frequencies of parallel episodes.
Counting serial episodes requires the use of finite state automata. Since we must count the frequencies of several episodes in one pass through the data, there are many au-tomata that need to be simultaneously tracked. In order to access these automata efficiently they are indexed using a waits ( ) list. The automata that are currently waiting for event type A can be accessed through waits ( A ). Each element in the waits ( ) list is an ordered pair like (  X , j ), in-dicating which episode the automaton represents and which state it is currently waiting to transit into. More specifi-cally, (  X , j )  X  waits ( A ) implies that an automaton for  X  is waiting for an event of type A to appear in the data to com-plete its transition to state j . This idea of efficiently index-ing automata through a waits ( ) list was introduced in the windows-based frequency counting algorithm [8]. The list was used to manage up to N automata per N -node episode. The algorithm we present here requires just one automaton per episode, and is also time-wise more efficient.
The overall structure of the algorithm is as follows. The event sequence is scanned in time order. Given the cur-rent event, say ( E i , t i ), we consider all automata waiting for an event with event type E i , i.e., the automata in the list waits ( E i ). Automata transitions are effected and fresh au-tomata for an episode are initialized by adding and removing elements from appropriate waits ( ) lists. In this respect, a temporary storage called bag is used, if it is found necessary to add elements to the waits ( ) list over which we are cur-rently looping. We present all algorithms as pseudo code. In the algorithms, N denotes the size of the episodes whose frequencies are being counted,  X  [ j ] is the event type corre-sponding to node j of episode  X  and  X .freq is its current frequency count.

The strategy for counting non-overlapped occurrences is very simple. An automaton for an episode, say  X  , is ini-tialized at the earliest event in the data sequence that cor-responds to the first node of  X  . As we go down the data sequence, this automaton makes earliest possible transiti ons into each successive state. Once it reaches its final state, a n occurrence of the episode is recognized and its frequency is increased by one. A fresh automaton is initialized for this episode when an event corresponding to its first node ap-pears again in the data and the process of recognizing an occurrence is repeated. This way, for each episode, a set of non-overlapped occurrences is counted. Later in this secti on, we prove that this strategy yields the maximal set of non-overlapped occurrences. Algorithm 1 gives the pseudo code for counting non-overlapped occurrences of serial episode s. In the description below we refer to the line numbers in the pseudo code.

Algorithm 1 requires the following inputs: the set of can-didate episodes, the event stream and a frequency thresh-old. (Note that frequency threshold is given as a fraction of data length). The output of the algorithm is the set of frequent episodes (out of the set of candidates input to the algorithm). The waits ( ) lists are initialized by adding the pair (  X , 1) to waits (  X  [1]), for each episode  X   X  C , (lines 1-4, Algorithm 1 ). The frequencies are initialized to zero (line 5, Algorithm 1 ) and the temporary storage, bag , is initially empty (line 6, Algorithm 1 ). Basically, one automaton for each episode is set waiting for the event type corresponding to its first node. The main loop in the algorithm (lines 7-20, Algorithm 1 Non-overlapped count for serial episodes Require: Set C of candidate N -node serial episodes, event Ensure: The set F of frequent serial episodes in C 1: for all event types A do 2: Initialize waits ( A ) =  X  3: for all  X   X  C do 4: Add (  X , 1) to waits (  X  [1]) 5: Initialize  X .freq = 0 6: Initialize bag =  X  7: for i = 1 to n do 8: /  X  n is length of data stream  X  / 9: for all (  X , j )  X  waits ( E i ) do 10: Remove (  X , j ) from waits ( E i ) 11: Set j  X  = j + 1 12: if j  X  = ( N + 1) then 13: Set j  X  = 1 14: if  X  [ j  X  ] = E i then 15: Add (  X , j  X  ) to bag 16: else 17: Add (  X , j  X  ) to waits (  X  [ j  X  ]) 18: if j = N then 19: Update  X .freq =  X .freq + 1 20: Empty bag into waits ( E i ) 21: Output F = {  X   X  C such that  X .freq  X  n X  min } Algorithm 1 ) looks at each event in the input sequence and makes necessary changes to the automata in waits ( ). When processing the i th event in the data stream, namely, ( E the automata in waits ( E i ) are considered. Every automaton (  X , j ) waiting for E i is transited to its next state. This in-volves removing (  X , j ) from waits ( E i ) (line 10, Algorithm 1 ) and adding, either (  X , j + 1) or (  X , 1) to the appropriate waits ( ) list (lines 11-17, Algorithm 1 ). More specifically, if the automaton has not yet reached its final state, it waits next for  X  [ j + 1] i.e., (  X , j + 1) is added to waits (  X  [ j + 1]). If instead, an automaton has reached its final state, then a new automaton for the episode is initialized by adding (  X , 1) to waits (  X  [1]). Note that since this process of adding to the waits ( ) list is performed inside the loop over all elements in waits ( E i ) (i.e., loop starting line 9, Algorithm 1 ), it is in-appropriate to add to this list from within the loop. Hence, as was mentioned earlier, we use a temporary storage called bag . Whenever we want to add an element to waits ( E i ) it is stored first in bag which is later emptied into waits ( E after exiting from the loop (line 20, Algorithm 1 ). Finally, the episode frequency is incremented every time its automa-ton reaches the final state (lines 18-19, Algorithm 1 ). Since a new automaton for the episode is initialized only after an earlier one reached its final state, the algorithm counts non-overlapped occurrences of episodes.
At any stage in the algorithm, there is only one active automaton per episode which means that there are |C| au-tomata being tracked simultaneously. The maximum possi-ble number of elements in bag is also |C| . Thus, the space complexity of Algorithm 1 is O ( |C| ). The initialization time is O ( |C| + |E| ), where |E| denotes the size of the alphabet. The time required for the actual data pass is linear in the length, n , of the data sequence. Thus, to count frequen-cies for all episodes in the set, C , the time complexity of Algorithm 1 is O ( n |C| ).

The space required by the serial episode counting algo-rithms of both [8] and [6] are O ( N |C| ), where C is a collec-tion of N -node candidate episodes. The time complexity of the algorithm in [8] is O ( X  T N |C| ), where  X  T denotes the total number of time ticks in the data sequence, while that for the algorithm in [6] is O ( nN |C| ). Thus, both these algo-rithm suffer an increase in time complexity due to the size, N , of episodes being discovered. In addition to this, some-times, when the time span of the event sequence far exceeds the number of events in it, the windows-based algorithm would take an even longer time (since  X  T  X  n ).

Thus, Algorithm 1 , both time-wise and space-wise, is an extremely efficient procedure for obtaining frequencies of a set of serial episodes. In fact, it appears difficult to do bett er than this algorithm in terms of order complexities. This is because, at the least, we need to store and access all the candidate episodes in C , and so space required cannot be less than O ( |C| ). Similarly, at least one pass through the data is required for obtaining the episode frequencies, and in the worst case, at each event in the given event sequence, every candidate might require an update. Thus, it looks like O ( n |C| ) is the best possible worst-case time complexity that can be achieved for counting frequencies of |C| candidates in a data sequence of n events.
Algorithm 1 uses only one automaton per episode and hence we wait for the first event type again only after one complete occurrence of the episode. Thus, it is clear that th e occurrences of any episode counted by Algorithm 1 would be non-overlapped. Hence, to establish correctness of the alg o-rithm, we have to only show that it counts maximum possi-ble number of non-overlapped occurrences which is what we do in this subsection. Fix an N-node serial episode  X  . Let H be the (finite) set of all occurrences of  X  in the given event se-quence. (We emphasize that H contains all occurrences of  X  , including overlapping ones as well as non-distinct ones tha t share events). Based on the definition of episode occurrence , it is possible to associate with each occurrence, h  X  H , a unique N -tuple of integers, ( h ( v 1 ) , . . . , h ( v N the events { ( E h ( v the occurrence.) The lexicographic ordering among these N -tuples, imposes a total order, &lt;  X  , on the set H . (The notation h  X   X  g will be used to denote that either h = g or h &lt;  X  g .) This orders the elements of H such that when h  X   X  g , the occurrence times of events corresponding to these two occurrences must satisfy the following condition s. The first event corresponding to h never occurs later than that for g , i.e., h ( v 1 )  X  g ( v 1 ). Now, if h ( v 1 h ( v 2 )  X  g ( v 2 ). (If instead, h ( v 1 ) &lt; g ( v 1 ing occurrence times for h and g need not satisfy any further constraints.) Again, if h ( v 1 ) = g ( v 1 ) and h ( v 2 then h ( v 3 )  X  g ( v 3 ), and so on.

Let f be the frequency count based on non-overlapped oc-currences and let H no = { h 1 , . . . , h f } denote the sequence of non-overlapped occurrences of  X  that is counted by Algo-rithm 1 , i.e., h 1 is the first occurrence of  X  that Algorithm 1 counts, h 2 is the second, and so on. Clearly, H no  X  H and we have, h 1 &lt;  X  &lt;  X  h f . Algorithm 1 employs one automaton for  X  which makes earliest possible transitions into each of its states and a fresh automaton for  X  is initi-ated only after the current automaton reaches its final state . Thus, h 1 , which is the first occurrence of  X  counted by Al-gorithm 1 , is in fact the first occurrence possible for  X  in the data stream. Then, h 2 , the second occurrence of  X  counted by Algorithm 1 , is basically the earliest possible occurrence of  X  in the data stream after h 1 is completed. This gives us two important properties of the set, H no , that Algorithm 1 counts; A1-1 The occurrence h 1  X  H no is such that, h 1 &lt;  X  h for A1-2 For each i = 1 , . . . , ( f  X  1), the occurrence h i Given an occurrence h  X  H that appears after some h i  X  H no , we now ask the question, what can be the earliest occurrence after h that is non-overlapped with h ? Since h i &lt;  X  h , and since h i makes earliest possible transitions to its states, h i ( v N )  X  h ( v N ). Now consider a later oc-currence,  X  h , that is non-overlapped with h . We must have  X  h ( v 1 ) &gt; h ( v N ), and hence,  X  h ( v 1 ) &gt; h i overlapped with h i . But the first occurrence after h i which is non-overlapped with h i is h i +1 , because Algorithm 1 effects the earliest possible transitions for the automaton. Thus, since  X  h is also non-overlapped with h i and is later than h we must have h i +1  X   X   X  h . We state this fact as a third property of the set, H no , below: A1-3 Consider an occurrence, h  X  H , with h i  X   X  h for We use the above properties to establish maximality of H no (and consequently the correctness of Algorithm 1 ). As-sume that there is some other set of f  X  non-overlapped occur-rences of  X  in the event sequence with f  X  &gt; f . Let us denote From A1-1 , we have h 1  X   X  h  X  1 . If f &gt; 1, from A1-2 we know that h 2 is the earliest occurrence after h 1 that is non-overlapped with h 1 . Thus, since h 1  X   X  h  X  1 , and since h  X  is non-overlapped with h  X  1 , using A1-3 we have h 2  X   X  This way, by repeated application of A1-2 and A1-3 , we have h i  X   X  h  X  i for i = 1 , . . . , f . Now, since h f can be no occurrence after h  X  f that is non-overlapped with f (again, using A1-3 ), implying that, if f  X  &gt; f , then h  X  must overlap with h  X  f , which contradicts our earlier assump-tion about H  X  being a set of f  X  non-overlapped occurrences of  X  . Thus, f  X   X  f and so f is indeed the maximum number of non-overlapped occurrences possible in the data stream for episode  X  . This proves that, H no , the set of occurrences counted by Algorithm 1 , is the largest set of non-overlapped occurrences of  X  in the given event sequence.
The non-overlapped frequency definition (i.e. Definition 1 ) is applicable to episodes with all kinds of partial orders. In this section we present an algorithm for counting non-overlapped occurrences of parallel episodes.

An occurrence of a parallel episode simply requires event types corresponding all its nodes to appear in the event se-quence, with no restriction on the order in which they ap-pear. The difference when recognizing occurrences of paral-lel episodes (as compared to recognizing occurrences of ser ial episodes) is that there is no need to worry about the order in which events occur. Instead, we are interested in asking if each event type in the episode has occurred as many times as prescribed by the episode. For example, each occurrence of the 6-node parallel episode  X  = ( AABCCC ) is associated with a set of six events in the data sequence in which, two are of event type A , one is of event type B and the remaining three are of event type C (and it does not matter in which time order they appear).

Algorithm 2 , presented below, obtains the non-overlapped occurrences-based frequencies for a set of candidate paral lel episodes. As usual, we present the algorithm as a pseudo code and refer to it through line numbers in our description. Algorithm 2 takes as inputs, the set of candidates, the data stream and the frequency threshold, and outputs the set of frequent episodes. The main data structure here is once again a waits ( ) list -but it works a little differently from the one used earlier in Sec. 3.1. Each entry in the list waits ( A ), is an ordered pair like, (  X , j ), which now indicates that there is a partial occurrence of  X  which still needs j events of type A before it can become a complete occurrence. The initial-ization process (lines 1-8, Algorithm 2 ) involves adding the relevant ordered pairs for each episode  X  into appropriate waits ( ) lists. For example, episode  X  = ( AABCCC ) will initially figure in three lists, namely, waits ( A ), waits ( B ) and waits ( C ), and they will have entries (  X , 2), (  X , 1) and (  X , 3) respectively. There are two quantities associated with each episode,  X  , namely,  X .freq , which stores the frequency of  X  , and  X .counter , which indicates the number of events in the sequence that constitute the current partial occurrenc e of  X  .

As we go down the event sequence, for each event ( E i , t the partial occurrences waiting for an E i are considered for update (line 11, Algorithm 2 ). If, (  X , j )  X  waits ( E i having seen an E i now, (  X , j ) is replaced by (  X , j  X  1) in waits ( E i ) if sufficient number of events of type E i for  X  are not yet accounted for in the current partial occurrence (lines 13-15, Algorithm 2 ). Note that this needs to be done through the temporary storage bag since we cannot make changes to waits ( E i ) from within the loop. Also,  X .counter is incremented (line 12, Algorithm 2 ), indicating that the partial occurrence for  X  has progressed by one more node. When  X .counter = |  X  | = N , it means that the N events necessary for completing an occurrence have appeared in the event sequence. We increment the frequency by one and start waiting for a fresh occurrence of  X  by once again adding appropriate elements to the waits ( ) lists (lines 16-23, Algorithm 2 ).
Each waits ( ) list can have at most |C| entries and so the space needed by Algorithm 2 is O ( N |C| ) (because there Algorithm 2 Non-overlapped count for parallel episodes Require: Set C of candidate N -node parallel episodes, Ensure: The set F of frequent parallel episodes in C 1: for all event types A do 2: Initialize waits ( A ) =  X  3: for all  X   X  C do 4: for each event type A in  X  do 5: Set a = Number of events of type A in  X  6: Add (  X , a ) to waits ( A ) 7: Initialize  X .freq = 0 8: Initialize  X .counter = 0 9: Initialize bag =  X  10: for i = 1 to n do 11: for all (  X , j )  X  waits ( E i ) do 12: Update  X .counter =  X .counter + 1 13: Remove (  X , j ) from waits ( E i ) 14: if j &gt; 1 then 15: Add (  X , j  X  1) to bag 16: if  X .counter = N then 17: Update  X .freq =  X .freq + 1 18: for each event type A in  X  do 19: Set a = Number of events of type A in  X  20: if A = E i then 21: Add (  X , a ) to bag 22: else 23: Add (  X , a ) to waits ( A ) 24: Reset  X .counter = 0 25: Empty bag into waits ( E i ) 26: Output F = {  X   X  C such that  X .freq  X  n X  min } can be at most N distinct event types in an episode of size N ). To analyze the time complexity, note that, some ex-tra work needs to be done during initialization (as com-pared to the serial episode algorithms) to obtain the num-ber of times each event type in an episode repeats (lines 4-5, Algorithm 2 ). This means the initialization time com-plexity is O ( |E| + N |C| ). The main loop, as usual, is over n events in the data, and any of the waits ( ) loops, can at most be over |C| partial occurrences. Re-initialization of appropriate waits ( ) lists whenever an occurrence is com-plete (lines 16-23, Algorithm 2 ) takes O ( N ) time. This re-initialization needs to be done at most n N times for each episode. Hence, the total worst case time complexity of Al-gorithm 2 is O ( n |C| ). The space complexity of the windows-based algorithm for N -node parallel episodes is O ( N |C| ) and the time complexity is O ( X  T |C| ), where  X  T denotes the number of time ticks in the data sequence. Thus, except for the fact that, sometimes, the time,  X  T , spanned by the data sequence can be much larger than the number, n , of events in it, the time and space complexities of the non-overlapped occurrences-based algorithm and the windows-based algo-rithm are identical.
Earlier, in Sec. 3.1, we proved that Algorithm 1 always yields the maximum possible number of non-overlapped oc-currences in the data. The basic idea was that, by mak-ing earliest possible transitions in the automata, we ensur e that we track the largest number of non-overlapped occur-rences available in the data, for each (serial) episode bein g counted. As we have seen from Algorithm 2 , there is no need for any automata when tracking non-overlapped occurrences of parallel episodes. However, Algorithm 2 is similar to Al-gorithm 1 in respect of how they both track occurrences in the data by recognizing the earliest possible events for each node of an episode. This strategy ensures that we will count the maximum number of parallel episodes. Since the argu-ments needed to show this formally follow the same lines as our proof for the case of serial episodes in Sec. 3.1, for the sake of brevity, we do not explicitly prove the correctness o f Algorithm 2 here.
In this section, we have presented two new algorithms  X  one that obtains the non-overlapped occurrences-based fre -quencies for a set of serial episodes, and the other that ob-tains the same for a set of parallel episodes. Algorithm 1 , which is the counting algorithm for serial episodes, requir es just one automaton per candidate episode . This makes it an extremely efficient algorithm, both in terms of time and space, compared to all currently known algorithms [8, 6] for frequent serial episode discovery. We have also provide d a proof of correctness for the algorithm to show that Al-gorithm 1 indeed obtains the frequency of serial episodes as prescribed by Definition 1 . The algorithm for parallel episodes (i.e. Algorithm 2 ) is also very efficient. We note that this is the first time an algorithm has been reported for obtaining the non-overlapped occurrences-based frequenc ies for parallel episodes. However, its space and time complex-ities are same as that for the windows-based counting algo-rithm for parallel episodes [8].

In general, Definition 1 , is applicable to episodes with all kinds of partial orders. Any general partial order can be rep -resented as a combination of serial and parallel episodes. F or example, consider an episode having three nodes with event types, A , B and C . Let the partial order be such that both A and B must occur before C , but there is no restriction on the order among A and B . We can denote this episode as ( AB )  X  C . Such an episode is like a serial episode with two nodes, where the first node corresponds to a parallel episode, ( AB ), and the second node is C . Occurrences of such partial orders can be recognized using automata-type structures, where parallel episodes recognition needs to b e used as a subroutine. Viewed like this, it is possible, in principle, to design algorithms for counting non-overlapp ed occurrences of episodes with general partial orders. How-ever, more work is needed to transform this strategy into an efficient counting algorithm. Moreover, there is also a need to design efficient candidate generation strategies which ca n exploit the fact that the number of non-overlapped occur-rences of an episode is never greater than that of any of its subepisodes. Therefore, developing algorithms for dis -covering frequent episodes with general partial orders und er the non-overlapped occurrences-based frequency, would be a useful extension of the work presented here.
We present results obtained on some synthetic data gen-erated by embedding specific temporal patterns in varying levels of noise. The main objective of the experiments pre-sented here is to empirically demonstrate the efficiency ad-vantage of our new algorithm. The utility and effectiveness of the non-overlapped occurrences-based frequency in real applications have already been discussed in our earlier wor k [6, 7]. We had also shown through simulation experiments there, that our earlier algorithm for counting non-overlap ped occurrences is itself faster than the windows-based algo-rithm of [8]. Here, we compare Algorithm 1 with our earlier algorithm for counting non-overlapped occurrences (which was reported in [6] and to which we refer to in this sec-tion as Algorithm 0 ), as well as, with the windows-based frequency counting algorithm of [8] (which is referred to as Algorithm W in this section). We note that the frequency counts obtained for serial episodes using Algorithm 1 of this paper are identical to those obtained using Algorithm 0 , and hence, exactly the same set of frequent episodes would be output by both algorithms. However, it is not possible to directly relate the frequencies obtained using the windows -based algorithm ( Algorithm W ) with those obtained under the non-overlapped occurrences-based counting algorithm s. In our simulation experiments, we found that the sets of frequent episodes obtained under both frequency definition s are qualitatively very similar. Hence, the goal of this sec-tion is mainly to demonstrate that the gains (by a factor of N , where N is the size of episodes) in order complexities of Algorithm 1 over Algorithm 0 and Algorithm W , translate to actual run-time gains as well. For the case of parallel episodes, the space and time complexities of our algorithm is same as that of the windows-based algorithm for paral-lel episodes reported in [8]. Further, since there is no othe r algorithm for counting non-overlapped occurrences of para l-lel episodes, we do not provide any comparative results for Algorithm 2 .

By varying the control parameters of synthetic data gener-ation, it is possible to generate qualitatively different ki nds of data sets. In general, the temporal patterns that were picked up by our algorithm correlated very well with those that were explicitly inserted in the data even when these patterns are embedded in varying amounts of noise. Also, the sets of frequent episodes discovered were same as that discovered using our earlier algorithm of [6]. However, bot h in terms of memory as well as run-times our new algorithm is much more efficient.
Each of the temporal patterns to be embedded (in the synthetically generated data) consists of a specific ordere d sequence of events. A few such temporal patterns are speci-fied as input to the data generation process which proceeds is as follows. There is a counter that specifies the current time instant. Each time an event is generated, it is time-stamped with this current time as its time of occurrence. After generating an event (in the event sequence) the cur-rent time counter is incremented by a small random integer. Each time the next event is to be generated, we first decide whether the next event is to be generated randomly with a uniform distribution over all event types (which would be called an iid event) or according to one of the temporal patterns to be embedded. This is controlled by the param-eter  X  which is the probability that the next event is iid . If  X  = 1 then the data is simply iid noise with no temporal patterns embedded. If it is decided that the next event is to be from one of the temporal patterns to be embedded, then we have a choice of continuing with a pattern that is already embedded partially or starting a new occurrence of one of the patterns. This choice is also made randomly. It may Table 1: Ranks of  X  in frequency-sorted list of 4-node frequent episodes, for Algorithm 1 , Algorithm 0 and Algorithm W , on synthetic data with two pat-terns embedded in varying levels of noise. Data length is 50000 and number of event types is 50. Table 2: Ranks of  X  in frequency-sorted list of 3-node frequent episodes, for Algorithm 1 , Algorithm 0 and Algorithm W , on synthetic data with two pat-terns embedded in varying levels of noise. Data length is 50000 and number of event types is 50. be noted here that due to the nature of our data genera-tion process, embedding a temporal pattern is equivalent to embedding many episodes. For example, suppose we have embedded a pattern A  X  B  X  C  X  D . Then if this episode is frequent in our event sequence then, based on the amount of noise, episodes such as B  X  C  X  D  X  A can also become frequent.
We now present some simulation results to show that the episodes discovered as frequent by Algorithm 1 are same as that discovered by Algorithm 0 . (This indeed must be the case, since Algorithms 1 &amp; 0 are essentially frequency count-ing algorithms under the same frequency definition.) The main difference between Algorithms 1 &amp; 0 is that among any set of overlapped occurrences, Algorithm 1 tracks the earliest among them, while Algorithm 0 tracks the inner-most among them. In this section, we illustrate this aspect empirically by considering some synthetic data generated b y embedding two patterns in varying degrees of iid noise. The two patterns embedded are:  X  = ( B  X  C  X  D  X  E ) and  X  = ( I  X  J  X  K ). Data sequences with 50000 events each are generated for different values of  X  . The objective is to see whether these two patterns indeed appear among the sets of frequent episodes discovered (under both frequency counts ), and if so, at what positions. The respective positions of  X  and  X  (referred to as their ranks ) in the (frequency) sorted lists of 3-node and 4-node frequent episodes discovered are shown in Tables 1 &amp; 2. For comparison, we also show the ranks obtained using the windows-based algorithm in the tables. As can be seen from the tables, for all data sets, the ranks of  X  and  X  are identical under all three algorithms.
Size of episodes Algo 1 Algo 0 Algo W Speed-up Table 3: Run-times (in seconds) for Algorithm 1 , Algorithm 0 and Algorithm W , for a fixed num-ber of candidate episodes but with different sizes of episodes. The last column records the speed-up factor of Algorithm 1 with respect to Algorithm W . Data length is 50000, number of candidates is 500 and number of event types is 500.
 Table 4: Run-times (in seconds) for Algorithm 1 , Algorithm 0 and Algorithm W , for a fixed size of episodes but with different number of candidate episodes. The last column records the speed-up fac-tor of Algorithm 1 with respect to Algorithm W . Data length is 50000, size of episodes is 4 and number of event types is 500. Now we present some run-time comparisons to show that Algorithm 1 runs faster than both Algorithm 0 and Algo-rithm W . Recall that the worst-case time complexities of these algorithms are O ( n |C| ) and O ( nN |C| ). In this section, we empirically show that the better time complexity of Al-gorithm 1 translates to significant advantages in terms of actual run-times as well.

We first show how run-times of the algorithms vary with the size of episodes being discovered. The frequency count-ing algorithms were presented with several sets of candidat e episodes with each set containing the same number (500) of episodes. However, the size of the episodes in each set was different. Table 3 lists the comparison of run-times of the two algorithms for these sets of candidate episodes. The input data stream used was a 50000-long uniform iid event sequence over a large number (500) of event types. In such a sequence, all episodes of a given size would roughly have the same frequencies and hence the computation associated with all candidates in any given set would roughly be of the same order. It can be seen from the tables that Algorithm 1 is always faster than both Algorithm 0 and Algorithm W . It can also be seen from the table that, for Algorithm 1 , the run-times do not increase much with the size of episodes, while for Algorithm 0 , the run-times are roughly linear in the size of episodes.

Next we perform a similar experiment by varying num-ber of candidates but keeping the size of episodes fixed. We Table 5: Run-times (in seconds) for Algorithm 1 , Algorithm 0 and Algorithm W , for different lengths of data sequences. The last column records the speed-up factor of Algorithm 1 with respect to Al-gorithm W . Number of candidates is 500, size of episodes is 4 and number of event types is 500. Table 6: Total run-times (in seconds) for Algo-rithm 1 , Algorithm 0 and Algorithm W , for synthetic data with two patterns embedded in varying lev-els of noise. The last column records the speed-up factor of Algorithm 1 with respect to Algorithm W . Data length is 50000 and number of event types is 500. consider sets with different number of 4-node candidate se-rial episodes. The corresponding run-times for the three algorithms are listed in Table 4. Here again, we see that Al-gorithm 1 , both runs faster and scales better with increasing number of candidates, than Algorithm 0 and Algorithm W . Similar results were obtained when we studied the effect of data length on the run-times of the various algorithms and the results are shown in Table 5.

Finally, we compare the overall run-times for frequent episode discovery based on all these algorithms. The algo-rithms were run on synthetic data generated by embedding two patterns in varying levels of iid noise. The frequency thresholds were chosen such that roughly the same number (100) of frequent 4-node episodes are output by both algo-rithms. The results are tabulated in Table 6. In all cases, it can be seen that Algorithm 1 outperforms both Algorithm 0 and Algorithm W .
In this paper, we have presented some new algorithms for frequency counting under the non-overlapped occurrences-based frequency for episodes. The new algorithms are, both space-wise as well as time-wise, significantly more efficient than the earlier algorithms reported in [6]. These algorith ms arguably have the best space and time complexities for fre-quency counting of a given set of candidate episodes. This algorithmic efficiency, together with the theoretical prope r-ties presented in [6], make out a very strong case for us-ing the non-overlapped occurrences-based frequency for fr e-quent episode discovery in event streams.

In the frequency counting algorithms described in this pa-per we do not worry about the spread of events within an occurrence. In some applications, it may be necessary not to count an episode occurrence if the events constituting it ar e widely spread out. The windows-based frequency count of [8], for example, implements some kind of a time constraint on the occurrences of an episode, since the width of the win-dow used is basically an upper bound on the time span of the occurrences (that are considered for the episode X  X  fre-quency). However, the problem with such a scheme is that, while it eliminates widely spread out occurrences from con-tributing to the frequency count, it also artificially incre ases the frequency when occurrences are very compact.

Since our new frequency counting algorithms explicitly count episode occurrences, it is possible, when an applica-tion so requires, to incorporate an extra time constraint di -rectly on the occurrences being counted [5]. An expiry time constraint can be used to define the extent to which events of an occurrence may be spread out in the event sequence. In case of serial episodes, the expiry time constraint is an upper bound on the time difference between the events in an occurrence corresponding to the first and last nodes of the episode. Incorporating such expiry time constraints does increase the space and time complexities of the algorithm a little bit. The counting strategy we now need closely re-sembles that of the algorithm for serial episodes that we proposed earlier in [6]. Even when such time constraints are prescribed, the algorithms for counting non-overlappe d occurrences-based frequencies are both space-wise and tim e-wise very efficient, and compare favorably with the windows-based frequency counting scheme. We will address some of these issues in our future work.
 This research was partially funded by GM R&amp;D Center, Warren through SID, IISc, Bangalore. [1] M. J. Atallah, R. Gwadera, and W. Szpankowski. [2] G. Casas-Garriga. Discovering unbounded episodes in [3] R. Gwadera, M. J. Atallah, and W. Szpankowski. [4] R. Gwadera, M. J. Atallah, and W. Szpankowski. [5] S. Laxman. Discovering frequent episodes in event [6] S. Laxman, P. S. Sastry, and K. P. Unnikrishnan. [7] S. Laxman, P. S. Sastry, and K. P. Unnikrishnan. [8] H. Mannila, H. Toivonen, and A. I. Verkamo. Discovery [9] N. Meger and C. Rigotti. Constraint-based mining of [10] M. Regnier and W. Szpankowski. On pattern
