 Data stream clustering has emerged as a challenging and interesting problem over the past few years. Due to the evolving nature, and one-pass restriction imposed by the data stream model, traditional clustering algorithms are inapplicable for stream clustering. This problem becomes even more challenging when the data is high-dimensional and the clusters are not linearly separable in the input space. In this paper, we propose a non-linear stream clustering al-gorithm that adapts to the stream X  X  evolutionary changes. Using the kernel methods for dealing with the non-linearity of data sep-aration, we propose a novel 2-tier stream clustering architecture. Tier-1 captures the temporal locality in the stream, by partitioning it into segments, using a kernel-based novelty detection approach. Tier-2 exploits this segment structure to continuously project the streaming data non-linearly onto a low-dimensional space ( LDS ), before assigning them to a cluster. We demonstrate the e ff ective-ness of our approach through extensive experimental evaluation on various real-world datasets.
 Categories and Subject Descriptors: H.2 [Database Management]: H.2.8 Database Applications General Terms: Algorithms, Management, Theory.
 Keywords: Data streams, Stream mining, Stream clustering, Ker-nel methods, Dimension reduction
Recent advances in hardware technology, communications and database systems have made streaming data a possibility. Data stream clustering has received significant attention in the recent past and has been found useful in a wide range of applications such as transaction processing, network monitoring, sensor networks, text mining and web-click stream analysis.

Stream-clustering is the process of assigning to a cluster (or cre-ating a new cluster for) a data point, as it arrives on-line. A stream-clustering procedure should be one-pass, operate in limited mem-ory, and adapt to the stream X  X  evolutionary changes. For example, consider an on-line intrusion detection system. Such a system aims to monitor network tra ffi c on-line, and cluster the network connec-tions, as they become available, into normal or intrusion clusters. Since, there could be many kinds of intrusion attacks, connections relating to di ff erent intrusion types should form di ff erent clusters. This cluster structure can aid the system administrator to focus on interesting (unusual) events quickly, while investigating the intru-sion attack. Another interesting application is that of clustering newswire data. A newswire continuously receives data pertain-ing to di ff erent topics that emerge (and die) temporally. Emer-gence of new topics should create new clusters, while the old ones should fade out gradually. Clustering of such continuous data helps in its analysis, storage, and indexing. Other on-line information sources, such as e-mail and chat discussion boards, all represent personal information streams with intricate topic modulations over time. On-line clustering is a desirable tool for observing evolution-ary changes, mining and organizing the data.

In general, the number of clusters in stream clustering is not known a priori , since new classes can emerge at any time. For example, a new story might start developing on the newswire or an intrusion attack might cause a surge of unfamiliar connections on a network link. Thus, an e ff ective stream-clustering algorithm should be able to identify such events and group all such data into a new cluster. Another important consideration is that of tempo-ral decay. As new classes appear, data may cease to fall into any of the existing clusters. Maintaining such old clusters in memory might become a liability, and so the obsolete ones should slowly fade away .

While operating under the above mentioned constraints, we ad-dress the problem of stream clustering in an even more di ffi cult set-ting, where the cluster boundaries are not linearly separable and the streaming data is high-dimensional in nature (e.g., newswire text data can have as many as 10 , 000 dimensions [9]). We propose our novel, two-tier stream clustering architecture using the kernel meth-ods [20, 23], which is an e ffi cient and e ff ective technique for deal-ing with non-linear separation of data. Our architecture, thus also supports, non-linear projection of data onto a workable low dimen-sional space while adapting to the stream X  X  evolutionary changes.
Kernel methods maps input vectors non-linearly onto a higher-dimensional feature space (as feature vectors), and then applying need to develop new high-complexity algorithms that deal with non-linearity in the input space itself. Kernel methods are attrac-tive since (a) data in the feature space are more likely to be lin-early separable (and thus easier to cluster) than in the original input space [20], and (b) the implementation is e ffi cient since we can per-1 For n dimensional data, the input space is spanned by n orthogo-nal direction vectors. The feature-space representation is obtained using a non-linear mapping of the input-space data in some m di-mensions s.t. m  X  n . We discuss kernel methods and related termi-nology in more detail in Section 3. form the kernel methods without explicit usage of the feature-space vectors themselves .

Consider the toy example shown in Figure 1. The dark-blue marks represent instances from one class and the red circles those from a di ff erent class, while the separating boundary is delineated by the green ellipse. As evident in Figure 1(a), there is no linear plane separating the two clusters. Recently proposed stream clus-tering algorithms that operate in input space, such as STREAM [13], HPStream [4] and CluStream [3], are not e ff ective in such a situ-ation. However, if we have a mapping function  X  that can project data to a 3-dimensional feature space, as shown in Figure 1(b), the data becomes linearly separable. This linear separation, though is not useful if it comes at the cost of working in high-dimensional feature space (or using explicit representation of  X  ). Kernel meth-ods allow us to make operations in high-dimensional feature space using a kernel function but without explicitly representing  X  (i.e., using implicit representations in the feature space).

Furthermore, with an appropriately selected kernel function (we formally introduce the kernel methods in Section 3), data is often well-behaved in the feature space and hence higher clustering accu-racy can be achieved. A motivating example is shown in Figure 2. We used the real-world network-intrusion data stream [9], that con-sisted of the network connection details of 22 di ff erent kinds of  X  X psweep X  attack, and shows the frequency distribution of the dis-tance of all the points in the dataset, from the center point. The dark line shows the quadratic fit for the distribution curve. As ev-ident in Figure 2(a), the data is distributed haphazardly around the center and thus is di ffi cult to cluster. However, when projected ap-propriately in the feature space (we have used the Gaussian kernel function here) as shown in Figure 2(b), the distance distribution is well-behaved, and a majority of the points are close to the center. Hence, any clustering approach in the feature space is likely to be more e ff ective.

A desirable (and often essential) step for clustering high dimen-sional data is e ff ective dimension reduction. The traditional ap-proach is to project data in lower dimensions along relevant direc-tion vectors, called principal components , where the variance of data along the principal components is preserved as much as possi-ble in the original data. Principal component analysis (PCA) [15], is a widely accepted linear method for obtaining the principal com-ponents. Aggarwal et al., proposed a projected clustering method for dimension reduction of stream data in [4]. However, these methods apply to cases where data is linearly separable. Sch  X  olkopf, et al., [21], developed the kernel PCA (KCPA) as a non-linear ver-sion of PCA that gives an explicit low dimensional space such that the data variance in the feature space is preserved as much as pos-sible. Unlike PCA, where the eigenvalue decomposition ( EVD ) of the sample covariance matrix is used to calculate the principal com-ponents, KPCA uses the EVD of a kernel matrix .

Although, the KPCA-based clustering method o ff ers good clus-tering accuracy, applying it to streaming data poses two challenges:  X  It su ff ers from prohibitive computational complexity of O ( n 3 ) of the EVD of an n  X  n kernel matrix, where n is the number of data samples. While n is infinite for data streams, performing the
EVD repeatedly on sliding window of fixed size is computation-ally prohibitive.  X  Principal components evolve with time. Consider the newswire data clustering example again. The attributes are a list of the main themes or terms along with a numeric weight indicating Figure 2: Well-behaved data in the feature space (Network in-trusion data for  X  X psweep X  attack). their relative importance to the document as a whole. Now as topics evolve, so do these weights. Eventually, the EVD informa-tion becomes stale, as there is no variance along the old principal components.

Using the kernel methods, we have developed an adaptive two-tier architecture for stream clustering that has the EVD complexity of O(  X  3 ), where  X  n . At Tier-1, the temporal locality in the stream is captured in a segment , using a kernel novelty-detection approach. A segment is a group of contiguous points of the stream that are packed closely together in the feature space.

Tier-2 adaptively selects segments produced from Tier-1 to main-tain / update a low dimensional space (we call LDS ), which has the following two properties: (a) It is significantly low-dimensional in nature, and (b) implicit data in the feature space is projected explic-itly in it such that the distances in the feature space is still preserved. The adaptively selected segments at Tier-2 are called representative segments .

The high-level overview of our adaptive non-linear clustering framework is presented in Figure 3. A point x in the input space (the left side of the figure) maps to LDS as  X  x (the right side).
For cluster assignment, we employ the fading cluster technique introduced in HPStream [4], where a fading cluster X  X  recency is computed using a monotonically decaying fading function f ( t ). This allows only the frequently updated clusters to survive, as ob-solete ones gradually fade away. The fading clusters reside in the LDS . In Figure 3, the shade of a cluster represents its recency value: the darker the shade, the more recent. Once the data is explicitly projected in LDS , it is assigned to a cluster based on its distances to the center of the alive clusters.

The LDS is computed using the EVD of the covariance matrix of the feature-space means of the representative segments. We have developed kernel methods for projecting data in the input space onto LDS using the representative segments structure, and for adap-tive selection of representative segments and LDS update.
Let us briefly illustrate the clustering process and formalize the problem setting. The task at hand is to group incoming data (of possibly many dimensions) into clusters (the number of clusters is not known a priori ) such that mean purity over all the clusters is always high. Assume that we use some initial part of the stream to generate an LDS and that we operate on a fixed memory space M . M holds the representative segments and other data structures needed by the two-tiers to operate. As new data arrives, we run our kernel novelty detection algorithm to see if this new data is novel enough to create a new segment. If yes, we take the last completed segment and check whether it is significantly di ff erent from the representative segments in M . If so, we update the set of representative segments and the LDS ; otherwise we discard the segment. The data is now projected onto the LDS and assigned to an existing cluster if it is close enough to one; otherwise a new cluster is created with that point. Using a fading function f ( t ), we update the recency value of all the alive clusters.

In summary, this work makes the following significant advance-ments compared to the existing methods:  X  We use kernel methods to deal with non-linear separation of data, which existing stream clustering approaches cannot do.  X  Using our novelty-detection algorithm, we can e ff ectively parti-tion the stream into tightly packed segments, which reduces the
EVD computation complexity drastically.  X  Our architecture is adaptive, and performs the EVD step only when required, with a computational complexity of O(  X  3 ), where  X  n .  X  We have developed a kernel methods for e ffi cient and e ff ective representation of the data in LDS while tracking the evolutionary changes in the stream.

The rest of this paper is organized as follows. Related work is discussed in Section 2. Section 3 presents a brief introduction to the kernel methods which lays the foundation of our stream clustering approach. In Section 4, we present a high-level overview of our adaptive 2-tier stream clustering approach. Section 5 presents the details of the kernel methods developed for our non-linear stream clustering framework. We also analyze our methods in terms of space and time complexities in this section. In Section 6, we present a rigorous experimental evaluation of the proposed clustering frame-work on 3 real-world data streams. Conclusions and goals for fu-ture work are presented in Section 7.
One of the first research e ff orts in stream clustering was pre-sented by Guha, et al., in [13] where the objective was to minimize the sum of square distance measure (SSQ). The problem, being similar to k -means, is NP -hard. The authors provide iterative tech-niques to find good initial solutions and approximate methods (with theoretical guarantees) to address the problem in batch mode . The clusters are computed over the entire data stream with one-pass in-cremental methods. The k -median algorithm is run over a chunk of data that fits in the memory. Only the k -centers from the previous pass are kept in memory, and a weighted k-median algorithm is ap-plied on the subsequent chunk of data. Another approach for clus-tering massive datasets was proposed in BIRCH [25] which builds a hierarchical data structure to incrementally cluster incoming points. In essence, it is an e ffi cient method for clustering massive datasets, but does not fit the data stream paradigm well. In addition to operat-ing in batch mode, these approaches fail to capture the evolutionary changes in the data stream.

Recently, data-stream clustering solutions were proposed by Ag-garwal, et al., in [2, 5], which use the concept of microclusters on a pyramidal time frame model. They divide the clustering pro-cess into on-line microclustering and o ffl ine macro-clustering com-ponents. The on-line component builds fast summary statistics, and the o ffl ine component uses these statistics in a pyramidal time model to provide the clustering results. While their approach does take temporal evolution into consideration where old clusters die away, it works only for low dimensional data, cannot handle non-linear data relations and does not track the evolutionary changes in the stream. They later proposed HPStream [4], where they perform dimension reduction using projected clustering techniques. Sub-space clustering approaches have also been proposed for clustering high dimensional data [6, 1]. However, such methods might not work for all datasets and fall apart when data is not linearly separa-ble, as shown in the toy example of Figure 1.

Kernel methods have emerged as a powerful tool for data mining and machine learning because of their simplicity and computational e ffi ciency [24]. Any method whose operations depend only on in-ner products can form the kernel method. Kernel methods have been applied extensively in the fields of data classification [8], text mining [14], and clustering [7]. The KPCA technique [21], has also been used for data clustering and classification purposes. In-cremental PCA techniques [19] have also been applied e ff ectively for data clustering / classification in batch-mode. Such techniques are computationally expensive and do not work well with non-linear data relations. Gower introduced his method for analyz-ing segmented-data and projecting it in a low dimensional space in [12], using his PCO (Principal COordinate analysis) [10] tech-nique. We have developed a kernel extension of his segmented data analysis method and its extension to stream clustering.
Before proposing our work, we provide a brief introduction to the kernel methods to facilitate understanding of the core modules of our system. Kernel methods (or kernel theory) provide a power-ful and principled way of capturing non-linear relations. A kernel method has two essential components: 1) an embedding into a fea-ture space and, 2) a linear method to address the problem in the feature space. On the one hand, linear algorithms are well studied and e ffi cient but can be restrictive. On the other hand, non-linear algorithms are less restrictive but can be computationally expen-sive. The kernel trick makes it possible to operate in the high-dimensional feature space using only the inner products, without explicitly representing data in the feature space. This reduces the computational load drastically, making the overall approach simple and e ffi cient . As an example, let us revisit Figure 1. We are given a two-dimensional input space and a non-linear vector-valued map-ping  X  that embeds the input data into a three-dimensional feature space as follows: A quadratic relation in the input space corresponds to a linear rela-tion in the feature space where data boundaries become linear. The composition of the inner product can then be evaluated as: Thus, the inner product of feature vectors can be easily computed using the square of inner products in the input space. This elim-inates the need to explicitly compute the coordinates in the high-dimensional feature space. This kernel trick is a computational shortcut where inner product operations in the feature space can be computed via a positive semi-definite function  X  : X  X  X  X  X  X  &lt; s.t.:
D  X  X  X  X  X  X  X  X  X  X  X  X  X  1. A symmetric real-valued function  X  : X  X  X  X  &lt; is said to be positive semi-definite if and only if for all n , { x 1 ,..., x n } X  X and { a 1 ,..., a n } X &lt; .
In the case of a finite set X = { x 1 ,..., x n } ,  X  is positive semi-definite if and only if the corresponding Kernel matrix (also called Gram matrix ), K = [  X  ( x i , x j )] is an n  X  n positive semi-definite ma-trix.

The positive definite function  X  in Eq 2, is called the kernel function. A wide range of kernel functions is available to fit dif-ferent applications, and the choice of the kernel function is often trivial 3 . An example of a kernel function is the Gaussian ker-nel or Radial Basis Function (RBF) kernel which is defined as the bandwidth of the kernel.

We use normalized kernel functions in all our experiments, and all references to kernel functions in the rest of the paper are to nor-malized kernel functions. Any kernel function  X  ( x , z ) can be nor-malized to  X   X  ( x , z ) as follows: Furthermore, a normalized kernel function has the following two properties: (a)  X  ( x , x ) = 1, and (b) |  X  ( x , z ) | X  1.
An obvious observation from the first property is that the diago-nal elements of a kernel matrix generated using a normalized ker-derivations of the kernel methods in Section 5. Readers interested in kernel theory are referred to [20, 23].
Having introduced the kernel theory, we now present our clus-tering framework. Throughout the rest of this paper, if a data point in the input space is denoted by x , its correspondence in the fea-ture space and LDS is denoted by  X  ( x ) and  X  x respectively. The time stamp of a variable is attached to it as a superscript within braces. As discussed earlier, we use a fading cluster strategy similar to HP-attributes associated with it: (1) its recency value, R i  X  u , and (3) the mean squared distance of  X  u i to all the points in the cluster, D i .

If t is the current time, and t last is the time cluster C point assigned to it, we have: where  X  &gt; 0 is a fading factor and we shall select  X  = 0 . 1 in all our experiments. Given a user parameter  X  1 , a new point x ( t ) received from the stream, is assigned to cluster C j if: is beyond the scope of the paper. self-similarity scores.
 but otherwise a new cluster is created with that point. Once the assignment is made, the recency values of all alive clusters are up-dated as shown in Eq. 4. Clusters having a recency value below a threshold are deleted [4].

In Figure 4, we present the flowchart of our non-linear adaptive clustering framework. Any point received from the stream first en-ters Tier-1. At any time, Tier-1 maintains an active segment S , whose size is denoted by s . When a new point is received, its novelty score is computed based on its feature-space distance to the points in S . If the novelty score is high, or S has reached its maximum allowed size s max , a new segment is created; otherwise, the point becomes part of S . Creation of a new segment pushes S to Tier-2, and the new segment now becomes the active seg-ment of Tier-1. The advantage of this segmentation approach is twofold:  X  It groups together data points that are similar in the feature space, which allows reduction in computational complexities of the pro-cedures in Tier-2.  X  It keeps track of the stream evolution at a fine level and provides check points for tracking evolutionary changes.
 Creation of a new segment at Tier-1, is indicative of a possible evo-lutionary change in the stream that could potentially lead to the change in the direction vectors of the LDS . When Tier-2 receives a new segment, it checks whether the segment represents an evo-lutionary change significant enough to cause a realignment on ex-isting direction vectors of the LDS , or to require addition of new ones. If so, then the received segment is included in memory as a representative segment , otherwise it is discarded. The mean of each representative segment in the feature space is used to compute a co-variance matrix, the EVD of which provides the direction vectors of LDS . A new point is projected in LDS using its inner products with the means of the representative segments.

Although data belonging to a cluster can be distributed across the stream, it is not likely to cause frequent updates of the LDS di-rection vectors. Once a segment holding points from a cluster is chosen as a representative segment and has initiated an LDS up-date, future points belonging to the same cluster but distributed elsewhere in the stream will form new segments only at Tier-1, but these segments would not be be selected as representative segments as long as the old representative segment is bu ff ered at Tier-2.
With this overview in mind, we present the details of the two-tiers in Sections 5. We also provide an analysis of the kernel meth-ods we developed for the two-tiers in terms of space and time com-plexities. In Section 5.2.1, we present our representative selection selection method, and LDS update procedures.
In this section, we present the formulations and analysis of the kernel methods for Tier-1 and Tier-2. Here and later, we assume that  X  represents the non linear vector-valued function from an in-put space to a feature space. A point x in the input space is repre-sented as  X  ( x ) in the feature space.
Stream segmentation is based on a novelty detection algorithm in the feature space, which works on the following principle: In terms of the property of points seen so far, a new point is considered to be novel if it is too far away from the rest points in the feature space .
There are many existing algorithms for novelty detection [18, 22]. However, most of them are either incapable of operating in the feature space or have large computational complexities making them inapplicable for data streams. Our objective is to develop an e ffi cient method, which can mark the beginning / end of a segment in an on-line fashion.

At any time instant t , the active segment of Tier-1 is represented s min is the minimum number of points a segment should have be-fore we can run a novelty detection algorithm on it. If the number of points in a segment reaches s max , it is pushed to Tier-2 regardless of the novelty. While small segments can be sensitive to noise, large segments become insensitive to any variation in the data. Thus, we impose the constraint in Eq. 6, to bound the size of the active Though, it cannot be obtained explicitly, we can calculate its norm as follows: calculated as follows: The first term in Eq. 9 is unity because of the self-similarity score property of normalized kernels (Eq. 3), and the second term can be directly inserted from Eq. 8.
 is contained in a ball of radius R s.t., R = max( diag ( K ( t ) )), it has where  X  is a confidence parameter whose typical value is 0.01 [23]. d specified parameter, but will join the current active segment other-wise.
We now present an analysis of the proposed stream segmenta-tion algorithm. Eq. 10 reveals that the error in the estimation of  X   X  , is inversely proportional to the size of the segment. Hence, the size of a segment should be bounded to control the sensitivity of the system. It might appear that the novelty detection algorithm has high space complexity due to its dependence on the kernel ma-term in Eq. 9 can be computed on-line in O ( qs max q is the dimension of the input data. Hence, the computation of the segmentation procedure is O ( qs max ). Since we do not have to ity of our algorithm is O ( qs max ). Tier-1 thus provides an e ff ective method for on-line segmentation of the stream.
Tier-2 is responsible for obtaining an explicit low-dimensional representation of the streaming input data. Tier-2 generates (and updates) the LDS which has the following property: The squared distances between a pair of points in the feature space is strongly preserved in the LDS , i.e., The objective is to have e ff ective dimension reduction with reduced information loss. Our kernel method for LDS generation employs techniques used in Gower X  X  [11] grouped-data analysis method, called Principal Coordinate Analysis (PCO). Given a distance ma-trix, PCO helps analyzing its structure by expressing it in low di-mensional space. While doing so, PCO preserves the relative dis-tances between the points in the matrix as best as it can.
We cast the conventional PCO into a stream clustering frame-work by continuously operating on a distance matrix that holds the dissimilarity measures of the feature-space means of selected seg-ments received from Tier-1. These selected segments are called representative segments .

Let us briefly provide a gist of the conventional PCO. Let D = i j ] be an p  X  p distance matrix where d 2 i j is the square of dis-similarity measure between any two feature points  X  ( x i and H p = I p  X  1 p 1 p 1 0 p be an p  X  p centering matrix where I p  X  p identity matrix and 1 p is the p  X  1 vector of ones (and 1 0 transpose). Then PCO is to use matrix A u = L  X  1 / 2 to represent the points in u ( u  X  p ) dimensions where  X  and L are the eigenvalue and eigenvector matrices of  X  1 2 H p DH p such that:
Our method might appear similar to KPCA, but it has two dis-tinct advantages over KPCA:  X  Instead of computing principal components, our method com-putes principal coordinates by using PCO. That is, we project the data points into the new space directly while preserving ac-tual distances in the feature space as closely as possible, and  X  The segmentation scheme from Tier-1 helps in reducing the com-putational complexity from the order of the number of points (in KPCA) to the number of representative segments.

Tier-2 operates in limited memory M . Let us assume that at any where  X  is the maximum number of representative segments Tier-2 can hold in memory M (we discuss representative segment selec-tion procedure shortly in Section 5.2.1). Further, let R S be a set holding data of all the representative segments in M , such that: In Eq. 14, S i is a segment that was identified as a representative segment when it was received from Tier-1.

Let there be a total n data elements in all the representative seg-ments s.t. n = sentative segment S i . Further, let G be an n  X  m ( t ) indicator matrix is represented as a row of the matrix  X  R S , the feature-space mean of each representative segment is a row of the matrix  X   X  = N  X  1 G 0  X 
We can thus, formulate an eigenvalue problem similar to that shown in Eq. 13 as follows (see Appendix for details): where K R S is the kernel matrix over the elements of all the repre-trix obtained in Eq. 15. The number of dimensions of LDS can be obtained using the measure of eigenvalues. The higher the eigen-value, the greater is the variation of data along that eigenvector. If the number of significant eigenvalues is u ( u  X  m ( t ) ), then LDS is a u -dimensional space.

Once we have the EVD , we need a way to project an arbitrary data element from the stream, from the input space to LDS . Let A u = L  X  1 / 2 be a matrix such that L ,  X  are the eigenvectors and eigenvalues obtained from EVD of Eq. 15. LDS projection  X  x ( t ) , of distance vector such that d i is the squared distance between  X  ( x ( t ) ) and the feature-space mean  X   X  S distance of  X   X  S
Given R S , vector ( d  X  d 0 ) can be computed using the kernel trick as follows:
As new points are received from the stream, Eq. 17 can be used repeatedly to obtain their corresponding projections in the LDS , after which they can be e ff ectively clustered as discussed earlier in Section 4.

There could be three kinds of evolutionary changes in the stream: (a) data belonging to a some existing cluster might cease to reap-pear in the stream, (b) new classes might start appearing or, (c) both occur simultaneously. In case (a), though old clusters will die away, their dissolution might leave some spurious dimensions in the LDS 5 . Cases (b) and (c), the other hand, can adversely a ff ect the clustering performance, as a realignment or addition of more direction vectors in LDS might be required to incorporate the evo-lutionary changes. Nevertheless, any evolutionary change should update the LDS appropriately. In the next section, we present our technique for dealing with such evolutionary changes, and selecting the representative segments.
EVD is an e ff ective dimension-reduction technique used to gen-erate a lower dimensional space, such that the input data can be projected into it with minimum information loss. A vector in q -dimensional input space, has its energy split along q directions. EVD splits that energy into m eigenvectors such that m  X  q . The loss of information (or energy) in this transition is proportional to the amount of energy of a vector, which lies in a space complemen-tary to the one spanned by the eigenvectors. This vector is called the residue vector, and the loss in energy is given by its norm. If a new segment produced from Tier-1 has all the energy in the current LDS , we need not update the LDS and can safely discard this seg-ment. Otherwise, we discard old data from M to accommodate this new segment as a representative segment, and update the LDS .
For a segment S j , if  X  s j represents its mean in the current LDS and  X  y represents the mean of the data in set R S that was used to generate the current LDS , s.t.  X  y = r of segment S j is given by: where L is the u  X  u matrix of eigenvectors, as presented in Sec-tion 5.2 earlier. For all the representative segments, we maintain the mean and standard deviation of the norms of their residue vec-tors denoting them by  X  r R S and  X  R S respectively. A new segment with mean  X  s i is included in M as a representative segment if: Eq. 19 thus provides with the LDS update rule: If the norm of the residue vector of a new segment received from Tier-1, is less than one standard deviation away from the mean of those in memory M , it is selected as a representative segment and an LDS update is made .

Since the available memory is fixed, we delete the oldest segment having the least norm and update the terms  X  r R S and  X  ingly. While updating terms G , N , K R S and  X  y is trivial, updating the EVD is computationally more expensive. Although, there have been some research e ff orts devoted to e ffi cient on-line updating of the EVD , its discussion is beyond the scope of the paper, and the choice of a particular EVD update algorithm does not a ff ect the end results of our work. We use a representative approach for updating EVD ,  X  u and D i .
Having presented our projection approach, we now analyze its complexity. As evident in Eq. 15 and Eq. 17, the computation of A requires the matrix K R S , and the representative segments (in the input space) are required to compute  X  x . Since we operate in lim-5 In clustering methods using EVD , the number of significant eigen-vectors (and thus the dimensions) is usually proportional to the number of clusters in the dataset. ited memory M , let us assume that we can fit at any time only  X  representative segments in M , i.e m ( t )  X   X  . Thus, the maximum size of K R S matrix is  X   X   X  ( O (  X  2 )), and the storage space re-quired to store all the representative segments is O (  X  q ) ( q being the dimension of input data). Further, if there are k active clusters, then we have to maintain their centers, recency and mean distances to centers hence O ( k ). Therefore, the overall space complexity is O (  X  (  X  + q ) + k ). However, there is a trade-o ff between space and computational complexity here. One can store data in the input space instead of storing the kernel matrix K R S , and can compute the kernel matrix on the fly whenever the EVD is required. This comes at the computational cost of O (  X  q ). Projecting data in LDS takes O (  X  q ) time as evident from Eq. 17. The most expensive step in approaches involving EVD is that of the EVD computation itself, which is of the order O (  X  3 ) in our case 6 . This is where our ap-proach has outstanding benefits, as the computational complexity of EVD operations in our scheme depends directly on the evolu-tionary changes in the stream. If the stream is not very dynamic, tion complexities and vice versa.
In this section, the experimental analysis of our clustering method is presented. We conducted the experiments on a Pentium Xeon dual processor workstation with 1GB of memory. We tested the performance of our system on three di ff erent real-world data streams (details of the datasets are presented in Section 6.1) on four per-formance metrics, namely: (a) cumulative clustering purity (Sec-tion 6.2.1), (b) fraction of points in significant clusters (Section 6.2.2), (c) e ff ect of dimensionality (Section 6.2.3), and (d) computational overhead (Section 6.2.4).

Our approach compares best with KPCA-based clustering meth-ods. KPCA can be adapted for stream clustering by first using a training dataset to obtain the principal components in a prepro-cessing step, and then projecting the data along them as it arrives terms of performance (as we will show in the experimental eval-uations in Section 6.2), we have the overhead of multiple EVD computations. In terms of time complexity, both KPCA and our approach have the same time complexity for: (a) data projection (which is of the order of the size of the kernel matrix used), and (b) the cluster assignment and update procedure (which is of the order of the number of active clusters and dimensions of LDS ). Thus, we have overheads of segmentation ( O ( qs max )) and the EVD ( O (  X  3 )). While segmentation is not expensive, EVD can be a computational bottleneck when done frequently involving large matrices. How-ever, in our architecture, the frequency of EVD computation (and its complexity) is directly dependent on the stream X  X  evolutionary characteristics. Only when the stream is highly dynamic, the seg-ments are small and the EVD updates frequent. This o ff ers a fair trade-o ff between performance and computational complexity. As we will show in our experimental evaluation, the frequency of EVD updates is not high for real-world streams and does not incur a sig-nificant computational overhead.
We now present the details of the 3 contrasting real-world data sets which we used for the experimental evaluation. 6 Recently, EVD update algorithms have been proposed that have quadratic time complexity [16]. 7 Another approach is to recompute the EVD each time a new data point arrives in the stream, which obviously would be computation-ally impractical.
 Reuters Dataset : This newswire dataset called the Reuters-21578 [9], was originally collected and labeled by Carnegie Group, Inc. and Reuters, Ltd. in the course of developing the CONSTRUE catego-rization system. We used a version that has 3 , 829 documents with as many as 16 , 583 features. The dataset consists of documents (represented in the form of feature vectors), where each document belongs to a topic . The features are weights of frequent words in-dicating their relative importance to the document as a whole. We generated a newswire stream selecting documents from 10 di ff er-ent topics while preserving the chronological order in the original dataset. The stream pattern is shown in Figure 5. The graph was generated such that all data points belonging to the same cluster were shown by a vertical line with the same shade of gray. For ex-ample, all points with cluster ID 1 are black and those with cluster ID 10 are white. The x -axis in the graph shows time. Network-Intrusion Dataset : This dataset has also been used in the evaluation of some recently proposed stream clustering meth-ods [2, 4, 13]. The dataset, collected by the MIT Lincoln Labs, is a dump of nine weeks of TCP sequence packets flowing on a sim-ulated U.S. Air Force local-area network (LAN). The dataset was constructed for the purposes of testing cyber-attacks or network-intrusion detection systems. It has 34 continuous attributes with a total of 23 classes, 22 belonging to di ff erent intrusion types and one normal connection type. We used a version that had 10 , 422 records. The streaming pattern of the dataset is shown in Figure 6. MNIST Dataset : Since we wanted to maintain variety in our datasets, we also tested the performance of our results on an image dataset. This dataset, called MNIST, consists of the images of handwritten digits from 0 to 9. The complexity of this dataset poses a significant challenge for any clustering process; thus we selected it to maintain a rigorous evaluation testbed. The images were normalized in size and centered before obtaining the feature vectors. For more details see [17]. The dataset has a total of 576 features. We used a ver-sion that has 500 records for each digit and hence a total of 5 , 000 records. The stream pattern that we used in all our experiments involving this dataset is shown in Figure 7.
We tested the performance of our approach by comparing it with the KPCA-based clustering method. KPCA-based clustering is known for its e ff ective performance and its ability to beat input-space-based approaches. We performed KPCA on initial 120 points of the data stream and then used the principal components so obtained to project the streaming data. Once projected, the clusters are as-signed using the fading cluster strategy as discussed in Section 4. We used the same initial data for our clustering method, however; being adaptive, our method is not a ff ected by the choice of the train-ing data. To stress the adaptive nature of our method, we did not include examples from all clusters in the initial training data. These 120 points in the training data contained 40 examples each from 3 randomly selected classes for all 3 datasets.
We now present the details of the experimental setup and the evaluation results. We used the normalized quadratic kernel func-tion  X  ( x i , x j ) = ( x i  X  x j ) 2 for Reuters stream and the Gaussian ker-nel function for both Network-intrusion and MNIST streams. Ker-nel parameters were available for MNIST dataset [17], while we learned parameters for the network-intrusion stream. To ensure a fair evaluation, we used the same parameters while computing re-sults for KPCA. We always selected the top 10 eigenvectors (i.e. u = 10) while comparing the results with KPCA, as it o ff ered good performance with that choice of u for all datasets. We set  X   X  = 75,  X  2 = 0 . 05, S min = 2 and S max = 15 for all our experiments. We tested our system on a wide range of parameter choices. How-ever, except in extreme cases, we always observed similar or better performance than KPCA. Due to space constraints, we present only critical results, demonstrating performance enhancements achieved using our approach over those of KPCA. Since all the three datasets we used, had the ground truth (i.e the actual cluster ID X  X ) available, cept of cluster purity has also been used in earlier methods, where purity is defined as the average percentage of the dominant class label in each cluster [4].

In the rest of this section, we present our experimental results based on four performance metrics.
Cumulative purity at any time is the mean purity of all the dead and alive clusters seen so far in the stream. Previous methods us-ing fading clusters technique, report purities of only alive clusters (clusters that have not yet faded away completely) at only specific time instances. Our metric is more rigorous, as it does not ignore previous mistakes while reporting the purity. Once a data point is clustered incorrectly, the cumulative purity cannot reach 100%. We report cumulative cluster purity of only significant clusters . Signif-icant clusters are clusters that have more than one element in them. We present our logical reasoning for using them shortly in Sec-tion 6.2.2.

Figure 8 shows the cumulative cluster purity obtained using our method compared to that of KPCA for all the three data streams. The cluster purity for significant clusters is shown at the y -axis, ably whereas the x -axis denotes time. For the Reuters data stream, our method X  X  accuracy is almost 90% throughout the stream, as seen in Figure 8(a), whereas KPCA barely reaches 75% towards the end of the stream, falling to as low as 37% at the beginning of the stream. The reason for the degraded performance of KPCA is its inability to adapt to the stream evolution. Since the principal components were obtained using data di ff erent from the testing data, they can-not delineate boundaries between unseen clusters that evolve with the stream. However, as old clusters die and new classes are created (some of which might have been there in the training data), KPCA is able to improve the performance to some extent. Our adaptive method, on the other hand, recomputes the LDS as and when re-quired and does not allow the performance to su ff er.

Figure 8(b) exhibits similar trend in outperforming KPCA, for the network-intrusion stream. Referring back to the stream pattern in Figure 6, there exists a long stretch of one cluster at time index 1 , 253. KPCA performance rapidly falls to almost 30%, during the presence of this cluster. Analysis of the testing data revealed that this cluster (cluster ID 4 belonging to the  X  X eptune X  intrusion attack) was not used in the training data for KPCA. KPCA shows a similar dip in performance again at time index 5 , 614 for cluster ID 6 (intrusion type  X  X murf X ). Our adaptive method X  X  accuracy in contrast is more than 90% most of the time.

Performance results for the MNIST stream are shown in Fig-ure 8(c). Our incremental clustering method o ff ers consistent per-formance of more than 85% in all cases, as shown in Figure 8(c). KPCA X  X  performance is better for this stream, however our ap-proach shows a best case performance enhancement of almost 50% over KPCA.
Any cluster having more that one element in it, is called a sig-nificant cluster . In our scheme, old clusters fade away and even-tually die. Hence, there can be a many clusters consisting of just one or few elements due to: (a) an element being an outlier, or (b) more elements belonging to an alive cluster did not reappear in the stream before it faded away. It is di ffi cult to di ff erentiate between the two cases, since access to the whole stream is never available, and single-element clusters might form, simply due to the stream dynamics. However, single-element clusters can also bias the clus-ter accuracy numbers arbitrarily. A stream of n elements divided into n single-element clusters will have 100% purity. It could be argued that a cluster with two elements in it could fit into the same criterion. However, we believe that if two elements lie in the same cluster, they are unlikely to be outliers. Thus, a 2-element clus-ter is treated as a significant cluster. To avoid this possible bias of single-element clusters, we ignore all such clusters while reporting our results, and focus on significant clusters only.

The fraction of elements in significant clusters is the ratio of the number of elements falling in significant clusters, to the total num-ber of elements seen in the stream so far. The performance results on this metric are presented in Figure 9. Since single-element clus-ters are unavoidable in a streaming environment, this fraction is not likely to be close to unity. In Figure 9(a), we show the fraction of elements lying in the significant clusters for the Reuters data stream. Our method shows more graceful degradation by e ff ec-tively grouping more than 60% of the stream in significant clusters. KPCA starts well, putting initial data in significant clusters. How-ever as the stream evolves, its performance falls sharply to as low as 30%. Similar results are obtained for the network-intrusion stream as seen in Figure 9(b). KPCA cannot cluster the initial part of the stream e ff ectively due to the evolutionary changes and it thus shows poor performance. However, for the rest of the duration, the per-formance is almost as good as our approach (though the purity is significantly lower as seen in Section 6.2.1.

As we had seen in Figure 7, the MNIST stream, in general, is more dynamic (the duration of a presence of a cluster is short) in na-ture as compared to the other streams. For such a transient stream, the number of single-element clusters formed is high. Hence, the fraction of elements in significant clusters is lower for our method (around 50%), as observed in Figure 9(c). However, KPCA shows miserable performance putting only 30% of the stream data in sig-nificant clusters.
The clustering accuracy is also a ff ected by the dimensionality u , of the LDS . While using too many dimensions can cause sparsity (noise) and thus degrading the e ffi ciency, choosing too few dimen-sions can cause information loss and can degrade the accuracy. In our experiments, we observed that KPCA o ff ers poor performance (in both accuracy and fraction of elements in significant clusters) for any choice of u .

For the Reuters stream, we show the e ff ect of changing the di-mensionality of LDS on our method in Figure 10(a). The accu-racy of our method is almost 85% even when using just 4 dimen-sions, and can rise to as high as 93% when using 8 eigenvectors. Changing values of u showed an interesting trend for the network-intrusion stream. As evident in Figure 10(b), the purity is around 90% using 8 dimensions; however, it jumps up to almost 99% us-ing 4 eigenvectors. This is due to the nature of the dataset itself. Although this dataset has 23 classes in all, it is dominated by only a few major ones. For example, the frequency count of 11 out of 23 classes is less than 50 in the whole dataset. A deeper analy-sis showed presence of only 5 major types of classes in this dataset, namely,  X  X ormal X , X  X eptune X , X  X murf X , X  X atan X  and  X  X ortsweep. X  The optimal choice of u is between 4 and 6 eigenvectors. While the number of clusters appearing in a stream is not always known ad-vance, a wrong choice on u does not a ff ect the results drastically. Our method exhibits better performance compared to KPCA for any choice of the dimensionality of LDS . The variation in perfor-mance for the MNIST stream, with changing numbers of eigenvec-tors is shown in Figure 10(c). We do not see much variation in performance with changing u . In this case, a slight degradation is observed when working with 4 dimensions; however, it is still bet-ter than the 10-dimensional KPCA. The accuracy is less sensitive to u in this case because of the complex nature of the dataset. The classes in this dataset are such that the EVD s end up giving similar scores to all the eigenvalues. Thus all eigenvectors have the same amount of variation along them and a small change in u does not a ff ect the clustering purity drastically.
Although we achieve significant enhancement in clustering pu-rity, placing a high number of points in significant clusters, we have the overhead of EVD computations. On the one extreme, EVD for KPCA could be performed just once for some initial training data, which leads to performance degradation as seen in the experiments results so far. On the other extreme, the EVD could be performed repeatedly over a sliding window of data, which is computationally impractical. Our solution o ff ers an excellent trade-o ff between the two extremes.
 Though there are various methods available for implementing EVD e ffi ciently, the overall computational overhead depends on the number of EVD computations and size of the matrix that is being decomposed. Hence, we analyzed the computational overhead in-curred by our stream-clustering approach based on these measures. As discussed in Section 5.2.2, the only major overhead is the EVD update. Hence, we observed the number of EVD s performed during the length of an entire stream for each experiment. In Figure 11, we present the ratio of the number of EVD s performed to the total num-ber of points in the stream for each of the three datasets we used. In the figure, the ratio of EVD computations to the length of the stream is shown along y -axis, as we changed the LDS dimension-ality from u = 10 to u = 4. The frequency of EVD computations for all the datasets is always lower than 8% and drops even more as we decrease u . For example, for the network intrusion stream, we need only 406 EVD computations for a long stream of 10 , 422 points with u = 10. Since we set  X  = 75 in our experiments, in the worst case we are looking at 406 EVD computations of 75  X  75 matrix each, which is not computationally prohibitive. Note that this small overhead provides a significant accuracy enhancement of more than 25% over that of KPCA.
We believe that an e ff ective stream-clustering solution is one that o ff ers high cluster purity, putting most of the points in significant clusters. At the same time, the solution should be adaptive in na-ture and should not be computationally intensive. Our experimen-tal results suggest that our two-tier architecture is an e ff ective ap-proach that o ff ers excellent clustering accuracy and low computa-tional overhead. Being adaptive in nature, it outperforms KPCA as it does not require any training data.
In this paper, we have proposed a kernel-based method for clus-tering high dimensional streaming data, that is non-linearly separa-ble in the input space. We proposed a two-tier architecture where Tier-1 segments the stream on-line using a kernel novelty detection scheme, while Tier-2 projects the stream into a low dimensional space. The extensive experimental results illustrated superior per-formance over the conventional KPCA-based clustering method. Our method delivers accuracies of almost 90% for all the datasets with a high percentage of points lying in the significant clusters. In the future, we shall extend our work in the following two direc-tions:  X  Since our incremental LDS projection scheme shows high accu-racy even when working in low dimensions, it can be applied for stream visualization problems.  X  We shall also consider the application of our adaptive method in the incremental classification problem with non-linear data bound-aries. Let D = [ d i j ] be an n  X  n distance matrix of the feature-space means of n representative segments such that: where,  X   X  i is the feature-space mean of representative segment S Matrix D , can also be expressed as: where, v is an n  X  1 vector expressing diagonal elements of 1 is an n  X  1 vector of ones. For a centring matrix H n = I we have: The first two terms on the right-hand side of Eq. (iii) are zero be-
