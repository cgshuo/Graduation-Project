 The automatic consolidation of database records from many heterogeneous sources into a single repository requires solv-ing several information integration tasks. Although tasks such as coreference, schema matching, and canonicalization are closely related, they are most commonly studied in iso-lation. Systems that do tackle multiple integration prob-lems traditionally solve each independently, allowing errors to propagate from one task to another. In this paper, we de-scribe a discriminatively-trained model that reasons about schema matching, coreference, and canonicalization jointly. We evaluate our model on a real-world data set of people and demonstrate that simultaneously solving these tasks reduces errors over a cascaded or isolated approach. Our experi-ments show that a joint model is able to improve substan-tially over systems that either solve each task in isolation or with the conventional cascade. We demonstrate nearly a 50% error reduction for coreference and a 40% error reduc-tion for schema matching.
 H.2 [ Information Systems ]: Database Management; H.2.8 Algorithms, Data Integration, Coreference, Schema Matching, Canoni-calization, Conditional Random Field, Weighted Logic
As the amount of electronically available information con-tinues to grow, automatic knowledge discovery is becoming increasingly important. Unfortunately, electronic informa-tion is typically spread across multiple heterogeneous re-sources (databases with different schemas, or web documents with different structures) makin g it necessary to consolidate the data into a single repository or representation before data mining can be successfully applied. However, data in-tegration is a challenging problem. Even the task of merging two databases with similar schemas about the same real-world entities is non-trivial. An automatic system must be able to perform coreference (to identify duplicate records), canonicalization (to pick the best string representation of the duplicate record), and schema matching (to align the fieldsacrossschemas).

Coreference and other integration tasks have been stud-ied almost exclusively in isolation, yet the individual prob-lems are highly correlated. As an example, consider the two different data records of a person named John Smith in Table 1. Each data record is represented using a dif-ferent schema. In this example, knowing that the Contact attribute from schema A maps to the Phone attribute from schema B provides strong evidence that the two mentions are coreferent, indicating that schema matching is a valuable precursor to coreference. However, knowing that the two John Smith mentions are corefe rent provides strong evidence about which fields should be matched across the schemas (for example, the FirstName and LastN ame attributes of schema A should be mapped to the Name attribute of schema B ). The high correlation of these two tasks indicate that a cascaded approach, where one task must be solved before the other, is likely to l ead to gratuitous error propa-gation.

To motivate the idea further, consider the task of canoni-calization, the process of creating a single standardized rep-resentation of a record from several different alternatives. The result of canonicalization on a set of records is a single record containing a high density of information about a real-world entity. Intuitively, these canonical representations of entities contain valuable evidence for coreference. We would like exploit this entity-level information, yet canonicalization assumes coreference has already been performed.

In this paper we investigate a unified approach to data integration that jointly models several tasks and their de-pendencies. More precisely, we propose a conditional ran-dom field for simultaneously solving coreference resolution, record canonicalization, and schema matching. As described in Section 3.1, one particular feature of our model is that it automatically discovers the top level canonical schema. We use first order logic clauses for parameter tying, effectively combining logic and probability in a manner similar to [24, 7, 19]. Exact inference and learning in these models are intractable, thus we present approximate solutions to both these problems. Our approximations prove to be effective allowing us to achieve almost a 50% reduction in error for coreference and a 40% error reduction in schema matching over non-joint baselines.
Coreference is a pervasive problem in data integration that has been studied in several different domains. The ACE and MUC corpora have helped initiate a line of research on newswire coreference, beginning with approaches that exam-ine mention pairs [25, 18, 12] to more complicated models that reason over entire sets of mentions [7]. Person disam-biguation, another form of coreference, has also been studied in detail in [21, 11, 10, 26, 14, 4, 1]. However, these works only resolve coreference between objects of the same repre-sentation (e.g., database schema). The coreference problem we tackle involves objects that contain different represen-tations, making direct comparisons between these objects difficult (for example, we may not know in advance that Phone from schema A maps to Contact in schema B ). The coreference portion of our model factorizes over sets of men-tions and incorporates first order logic features making it most similar to Culotta et al. [7].
Since records can refer to the same underlying entity in multiple ways (common aliases, acronyms and abbreviations), it is often necessary to choose a single and standardized rep-resentation when displaying the result to a user, or storing it compactly in a database. Additionally, because the canon-ical record is constructed from multiple records, it contains a high density of information about the entity, making it a convenient source of evidence for coreference resolution.
Canonicalization has played an important role in systems that perform coreference and database merging. Tradition-ally, it is performed post hoc andoftenreliesonmetricsfor evaluating distances between strings. An example of canon-icalization in a database merging task is Zhu and Unger [28], who obtain positive results by learning string edit pa-rameters with a genetic algorithm. McCallum et al. [17] extend usual edit distance models with a conditional ran-dom field, demonstrating more accurate distance evaluations on several corpora; however, they do not apply their string distance model to the problem of canonicalization. Other approaches include Ristad and Yianilos [23], who use expec-tation maximization to learn the parameters of a generative model that defines a string in terms of the string edit op-erations required to create it. This work is extended and applied successfully to record deduplication by Bilenko and Mooney [2]. Recently, Culotta et al. [6] describe several methods for canonicalization of database records that are robust to noisy data and customizable to user preferences (e.g., a preference for acronyms versus full words).
Schema and ontology mapping are fundamental problems in many database application domains such as data inte-gration, E-business, data warehousing, and semantic query processing. In general we can identify two major challenges with the schema matching (and ontology mapping) problem: (1) structural heterogeneity and, (2) semantic heterogeneity. The former concerns the different representations of infor-mation where the same information can be represented in different ways. This is a common problem with heteroge-neous databases. The latter deals with the intended mean-ing of the described information. More specifically we can identify the following differences between schemas [27]: (1) structural conflicts concerned with different semantic struc-tures; (2) naming conflicts where different attribute names may be used for the same type of information, or the same name for slightly different types of information; (3) conflicts where different formats maybe used to represent the values of attributes (for example, different units, different preci-sion, or different abbreviation styles). This problem has been extensively studied primarily by the database and ma-chine learning communities [20, 13, 15, 8, 9, 27] (for a survey of the traditional approaches refer to [22]).

Our model is able to reason about all three different kinds of conflicts mentioned above, based on a set of first order logic features employed by the CRF modeling the task. Our approach also differs from previous systems in that schema matching is performed jointly along with coreference and canonicalization, resulting in a significant error reduction as we will see in Section 5. One important aspect of our model is that it will automatically discover the top level canonical schema for the integrated data as will be demonstrated in Section 3.1.
We seek a general representation that allows joint reason-ing over a set of tasks defined on a set of possibly hetero-geneous objects in the world. In our unified data integra-tion approach we aim for a representation that enables us to perform inference and learning over two different types of objects: (1) data records (in relation to the coreference reso-lution and canonicalization tasks); (2) schema attributes (in relation to the schema matching task). In abstract terms, our model finds solutions to this problem in terms of a set of partitions (clusters) of data records where all the records within a particular partition are coreferent and canonical-ized; and also a set of partitions (clusters) of the schema at-tributes across different databases, where all the attributes within a schema partition are mapped together. As we will see shortly, although data record clusters and schema clus-ters are kept disjoint in terms of the type of objects they contain, they are tied together through a set of factors that compute different features associated with each task. Thus, inference is performed jointly over all three tasks. In the next section we describe a graphical model representation of this approach in more detail.
We use a conditional random field (CRF) [16] for jointly representing schema matching, coreference resolution, and canonicalization tasks as follows:  X  Let D = {D 1 ,..., D k } denote a set of k databases. Each S records generated using the schema S i .Eachschema S i is represented by a set of attributes { a i j } n S i j =1 where a j th attribute of the schema S i .Weuse S = {S 1 ,..., S k } to represent the complete set of schemas, and A = { a i j to denote the complete set of schema attributes across all schemas.  X  Let X = { x 1 ...x n } X  X  denote a set of observed vari-ables, where x i is a data record drawn from some database D ,and A is the set of all schema attributes { a i j } . ables X constrained by the fact that all the variables in a served variables in a cluster X i are either data record objects, or schema attribute objects). For clarity we use X r i to de-note a cluster of data records, and X s j to denote a cluster of schema attributes.  X  Let Y = { y 1 ...y m } denote a set of unobserved vari-ables that we wish to predict. In this paper we focus only on a particular class of clustering models 1 where the vari-ables y i indicate some compatibility among clusters of X variables (i.e., clusters X r i and X s j ). We employ three types of Y variables: (1) variables that indicate the compatibility among instances within a cluster of X variables; (2) variables that indicate the compatibility between a pair of clusters of the same type (compatibility between two clusters of data records, or two clusters of schema attributes); (3) variables that indicate the compatibility between a pair of clusters of different types (compatibility between a data record cluster and a schema attribute cluster). Note that this represen-tation renders an exponential complexity when instantiat-ing the Y , X r i ,and X s j variables (e.g., | Y | = O (2 i | = O (2 n ), and |X r i | = O (2 | A | )).

In general the inference and learning methods could be ap-plied to a variety of model structures [5]. ables X constrained by the fact that all variables in a cluster X attribute objects). For clarity we use X r i to denote a cluster of data records, and X s j to denote a cluster of schema at-tributes. Our goal is to learn a clustering X = {X r i } X  X X s such that all the data records in X r i are coreferent, and all the schema attributes in X s j bear the correct schema match-ing.

Next, the conditional probability distribution P ( Y | X )is computed as: where Z X is the input-dependent normalizer, factor f w pa-rameterizes the within-cluster compatibility, and factor f parameterizes the between-cluster compatibility 2 .Notethat in our model there are two types of within cluster factors f w : those measuring the compatibility within a data record clus-ter (e.g., X r i ), and those measuring the compatibility within a schema attribute cluster (e.g., X s i ). Similarly, there are two types of between cluster factors f b : those measuring the compatibility between a pair of homogeneous clusters (two data record clusters, or two schema attribute clusters), and those measuring the compatibility between a pair of heterogeneous clusters (a data record cluster and a schema attribute cluster). We also employ a log-linear model of po-tential functions (i.e., f w and f b ): This model can be intuitively described as follows: every possible clustering of the data induces a different set of in-stantiations of Y variables and possibly gives different as-signments to them. The conditional distribution P ( Y | X ) gives the probability of a configuration Y measured in terms of a normalized score of how likely that configuration is. We parameterize this score with a set of potential functions that evaluate the compatibility of both within-cluster attributes and between-cluster attributes.
 Figure 1: Three example databases: each database uses a different schema to generate a set of data records. Each schema is visually represented as a collection of color-coded objects of the same shape (solid rectangles in schema A, dotted lines in schema B, and hollow rectangles in schema C).

A desirable facet of our model is that it factorizes into clusters of data rather than pairs (Equation 1). This en-ables us to define features of entire clusters using first-order
In the above equation we use the notation X ij to denote a pair of clusters X i and X j . logic features : features that can universally and existentially aggregate properties of a set of objects [5].

To further illustrate the model consider the simple exam-ple task demonstrated in Figure 1. There are three databases, where each database uses a different schema to generate a set of data records. Each schema is visually represented as a collection of color-coded objects of the same shape (solid rectangles in schema A , dotted lines in schema B , and hol-low rectangles in schema C ). Within each schema, different attributes are color-coded, and similar color across differ-ent schemas may refer to the same attribute concept. Each database consists of a number of data records generated us-ing its own schema (for example, database A contains n A data records generated using schema A ). The goal is to per-form joint inference among coreference resolution, canoni-calization, and schema matching.

Figure 2 displays a factor graph of the conditional random field modeling the above joint task. There are two levels of clustering processes:  X  Schema attribute clusters (top level): Each cluster in this level consists of a subset of the complete set of schema attributes. Note that two or more attributes of the same schema may be placed within the same cluster together with the attributes of other schemas. For example, one database may use a schema that has an attribute Name to repre-sent the full name of a person, while a second database may use a schema with two different attributes, First Name and Last Name , for representing the same concept. Intuitively, we would like to place all three in the same cluster. Some meaning that it does not match to any other schema at-tributes in the other databases. Note that the set of schema attribute clusters establishes the top level canonical schema for the integrated data (lightly shaded clusters).  X  Data record clusters (bottom level): Each cluster represents a set of coreferent data records. Note that every data record is visually represented as an encapsulation of the schema from which it was generated. For example clus-ter X 1 consists of a single data record from database A ,a single data record of database B , and two data records from database C . There may also exists clusters that contain a set of data records from the same database (for example the  X  Factors: Although the clusterings at different levels are defined over the same type of objects (data records or schema attributes), they are tied using a set of factors. We can identify three types of factors in general: (1) factors that measure compatibility among instances within a clus-ter (e.g., f 1 ,or f 4 ); (2) factors that measure compatibility between pairs of clusters of the same type (compatibility between two clusters of data records such as f 12 ,ortwo clusters of schema attributes such as f 67 ); (3) factors that measure compatibility between pairs of clusters of different types (compatibility between a data record cluster and a schema attribute cluster such as f 34 ).

Although omitted from Figure 2 for clarity, there are ad-ditional canonicalization variables for each attribute in each coreference cluster. Even though we lack labeled data for canonicalization, we set these variables using a centroid-based approach with default settings for string edit param-eters (insert, delete and substitute incur a penalty of one, and no penalty is given for copy). This method is shown in recent work by Culotta et al [6] to perform reasonably Figure 2: Factor graph representation of the model. There are two clustering processes, one at the level of schema attributes (top level), and one at the level of data records (bottom level). Different factors tie these two processes which allows for joint inference among different data integration tasks. Note that for clarity of the figure not all the factor names are represented. The top level clustering also automati-cally discovers the top-level canonical schema in the integrated data (lightly shaded clusters). well and to capture many of the desirable properties of a canonical string.

Even though we are able to achieve greater expressiveness in our model with cluster-wise first order features and high connectivity, we sacrifice the ability to apply exact inference and learning methods, since we cannot instantiate all of the Y variables. In Culotta and McCallum [5], approximate inference and parameter estimation methods operate with partial instantiations, where only the difference between two configurations are sufficient to perform learning. Building on these techniques, we briefly demonstrate how learning is performed in this model in the next section.
Both the joint model and the individual conditional ran-dom fields for each subtask are too large to be fully instan-tiated, making exact training and inference intractable. In this section we describe in detail our approximate and infer-ence methods.
To learn the parameters for coreference resolution we fix the schema matching to ground-truth and fix the canoni-calization to a reasonable default. Next, we sample pairs of clusters C i and C j and define the binary random variable ent. Given the fixed schema matching and canonicalization, and a whole set of cluster pairs with their corresponding labels, we set the coreference parameters to maximize the likelihood of the training set by performing gradient descent (and regularizing with the usual Gaussian prior). A similar Algorithm 1 Joint Inference 1: Input: 2: while Not Converged do 5: end while procedure is used to set the parameters for schema match-ing. However, in this case the coreference ground-truth is held fixed and the label indicates whether or not all the instances in two schema clusters all match to each other. Canonicalization is not used for the schema-matching task. This training method can be viewed as a piecewise psuedo-likelihood approximation.
For inference we use a standard greedy agglomerative ap-proximation to each subtask. The algorithm begins with a singleton clustering (each instance is in its own cluster) and greedily merges clusters until no merge scores are above a stopping threshold  X  . Joint inference works in rounds, per-forming greedy agglomerative first on coreference, and then is used to help schema matching in round i ,whereasaschema matching prediction from round i is used to help coreference ber of iterations.
Synthetic data: The synthetic data is generated from a small number (10) of user records collected manually from the web. These records use a canonical schema contain-ing attributes such as first name, phone number, email ad-dress, job title, institution, etc. Next, we created three new schemas derived from the canonical schema by randomly splitting, merging, or noisifying the attributes of the canon-ical schema. For example, one schema would contain a Name field whereas another would contain two fields, FirstName and LastName , for the same block of information (perhaps dropping the middle name if it existed).

In the training data we used the first two schemas and in the testing data we used one of the schemas from training, and also the third schema. This way we train a model on one schema but test it on another schema. For training we used a small number of user records that were similar in ways such that random permutations could make coreference, schema matching, and canonicalization decisions difficult. We first conformed the records to both schemas, and then made 25-30 copies of each data record for each schema while randomly permuting some of the fields to introduce noise.

The testing data was created similarly, but for a differ-ent set of data records. The random permutations included abbreviating fields, deleting an entire field, or removing a random number of tokens from the front and/or the end of a field. The result of this was a large number of corefer-ent records, but with the possibility that the disambiguat-ing fields between different records have been altered or re-moved.

Real World Data: For our real-world data we man-ually extracted faculty and alumni listings from 7 univer-sity websites, of which we selected 8 lists that had differ-ent schemas. The information in each schema contains ba-sic user data such as first name, phone number, email ad-dress, job title, institution, etc., as well as fields unique to the academic domain such as advisor, thesis title, and alma mater. For each name that had an e-mail address we used our DEX [3] system to search the Internet for that person X  X  homepage, and if found we would extract another record. The DEX schema is similar to the university listings data, bringing the total number of different schemas to 9. Of the nearly 1400 mentions extracted we found 294 coreferent entities. Table 2 shows the DEX schema and two of the faculty listing schemas. There are several schema match-ing problems evident in table 2, for example Job Depart-ment from the UPenn schema is a superset of both of the Job Title fields. Another example, which occurs numerous times between other schemas, is where the pair of attributes
First Name , Last Name from one schema is mapped to the singleton attribute Name that denotes the full name.
For each of the experiments we took all of the 294 coref-erent clusters, and randomly selected 200 additional men-tions that had no coreferent entities. This data was split into training and testing sets, where the only schema shared between training and testing was the DEX schema. This ensures that the possible schema matchings in the training set are disjoint from the possible schema matchings in the test set. The data provides us with a variety of schemas that map to each other in different ways, thus making an appropriate testbed for performing joint schema matching, coreference, and canonicalization.
The features are first-order logic clauses that aggregate pairwise feature extractions. The types of aggregation de-pend on whether the extractor is real-valued or boolean. For real-valued features we compute the minimum, the maxi-mum, and the average value over all pairwise combinations of records. For boolean-valued features we compute the fol-lowing over all pairwise combinations of records: feature does not exist; feature exists ; feature exists for the major-ity; feature exists for the minority; feature exists for all.
Table 3 lists the set of features used in our experiments. In cases where we compute feature s between records in different schemas, sometimes we only compute the feature between fields that are aligned in the current schema matching. This is indicated by the Matched-fields only column in Table 3. All of these features are used for coreference decisions, but only substring matching is used for schema matching.
We evaluate the following three systems with and with-out canonicalization for a total of six systems. Canonical-ization is integrated with coreference inference by interleav-ing canonicalization with greedy agglomerative merges (ev-ery time a new cluster is created, a new canonical record is computed for it). Canonicalization has no affect on schema matching in isolation and is only relevant when coreference is involved.

Whenever we use greedy agglomerative inference, we set the stopping threshold to  X  =0 . 5. This is a natural choice as it corresponds to the decision boundary for a binary max-imum entropy classifier. Additionally, the joint inference method described earlier is run for four rounds for the joint system.  X  Isolation (ISO): this system performs schema match-ing and coreference in complete isolation from each other. Greedy agglomerative search is used as an inference method in each task.  X  Cascaded (CASC): the cascaded approach executes schema matching and coreference in a pipelined fashion, ei-ther performing schema matching first and then coreference, or the other way around. The predicted result of the first task has the potential to influence the second task.  X  Joint (JOINT): the joint approach integrates corefer-ence and schema matching allowing predictions in both tasks to influence future predictions in the other. The system it-eratively runs the cascaded approach, allowing predictions to propagate in both directions between schema matching and coreference resolution.
We compare our joint model of coreference, canonicaliza-tion, and schema matching to the baselines of the cascaded and isolated inference (both with and without the benefits of joint canonicalization). Although we report precision, re-call, and F1 for both Pairwise and MUC evaluation metrics, the error reductions discussed in the body of text below are in terms of Pairwise F1 only. Table 4 displays the impact on coreference performance and Table 5 shows schema match-ing. As expected, joint inference between coreference and schema matching improves performance in both tasks over the baselines of performing each task in isolation or as a cascade. Overall, by performing all three tasks jointly, an error reduction of 49% is rea lized in coreference over the cascaded approach and 32.1% over coreference in isolation. For schema matching, the joint model reduces error by 40% over both the isolated and cascaded approaches.
Although the cascaded approach of performing corefer-ence followed by schema matching performs slightly better (with canonicalization) or the same (without canonicaliza-tion) as performing schema matching in isolation, the reverse is not true. First performin g schema matching and then coreference actually has a negative impact on coreference performance when compared to just performing coreference in isolation (regardless of whether or or not canonicalization is performed). This is not surprising as the schema match-ing in isolation achieves just 50.9% f1, thus passing a large amount of error to the subsequent coreference task. This result helps highlights the potential danger of error propa-gation in cascade systems.
Canonicalization has a positive impact on performance on nearly all the systems. The only system it does not affect is schema matching in isolation since it is not relevant for that task. Both cascaded systems and both joint systems improve substantially when introducing factors over canon-ical coreference records. Even the model that jointly does coreference and schema matching is improved by canonical-ization: coreference errors are reduced by 21% and schema matching errors by 6.5%.

The cascaded approach of first performing schema match-ing and then coreference actually does worse than having no schema matching information at all, highlighting the detri-mental effect of error propagation in a pipeline. The other cascade of performing coreference first has no effect on the schema matching
In addition to the real-world data experiments, we also applied both joint approach (joint schema matching and coreference resolution with no canonicalization), and the cascaded approach to our synthetically generated dataset. The models used in these experiments are slightly simpler (with canon) jointly inferred with coreference (JOINT) since they do not include the canonicalization task, but in-ference is performed in the exact same manner as described in Section 3.2.
 Figure 3: An example schema matching produced by our model using the cascaded approach on syn-thetic data. The dotted lines represents false pos-itives, while the thick lines represents the correct matches, and the thin lines represents the matchings that were missed. The cascaded approach produces a lot of wrong matches.

As expected we find that joint model on synthetic data reduces coreference resolut ion errors by 42% compared to the cascaded model. The improvements in schema-matching can be seen graphically: Figures 3 and 4 show examples of schema matching produced by the cascaded and joint ap-proaches respectively. The dotted lines represents false pos-itives, while the thick lines represents the correct matches (hits), and the thin lines represents the matchings that were missed (misses). It can be observed that the cascaded ap-proach produces a lot of false positive matches. The joint approach does not produce any false positive compared to the cascaded approach, while having a relatively low recall. Figure 4: An example schema matching produced by our model using the joint approach on synthetic data. The thick lines represents the correct matches, and the thin lines represents the matchings that were missed. The joint approach does not pro-duce any false positive compared to the cascaded approach.
In this paper we have demonstrated a successful method for performing coreference resolution, record canonicaliza-tion, and schema matching simultaneously with a condi-tional random field. Joint inference in this model outper-formed cascaded and isolated approaches on both synthetic and real-world data despite having to use approximate in-ference and parameter estimation.

We believe a ripe area of future work would be to explore less greedy inference methods such as Metropolis-Hastings, simulated-annealing, or similar stochastic search methods. Additionally, rank-based learning may be combined with Metropolis-Hastings to train an even larger model that in-cludes first-order logic clauses over entire clustering configu-rations. Finally, extending the model to include other tasks such as named entity recognition (NER) and record assem-bly is a natural next step. This could be particularly inter-esting because exact learning and inference for linear-chain CRFs (like those used in NER) is tractable, yet the other components of the model would have to be approximated.
This work was supported in part by the Center for In-telligent Information Retrieval, in part by Lockheed Martin through prime contract #FA8650-06-C-7605 from the Air Force Office of Scientific Research, in part by The Central Intelligence Agency, the National Security Agency and Na-tional Science Foundati on under NSF gr ant #IIS-0326249, in part by The Central Intelligence Agency, the National Se-curity Agency and National Science Foundation under NSF grant #IIS-0427594, and in part by the Defense Advanced Research Projects Agency (DARPA), through the Depart-ment of the Interior, NBC, Acquisition Services Division, un-der contract number NBCHD030010, and AFRL #FA8750-07-D-0185. Any opinion s, findings and conc lusions or rec-ommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. [1] J. Artiles, J. Gonzalo, and S. Sekine. The [2] M. Bilenko and R. J. Mooney. Learning to combine [3] A.Culotta,R.Bekkerman,andA.McCallum.
 [4] A. Culotta, P. Kanani, R. Hall, M. Wick, and [5] A. Culotta and A. McCallum. Tractable learning and [6] A. Culotta, M. Wick, R. Hall, M. Marzilli, and [7] A. Culotta, M. Wick, R. Hall, and A. McCallum. [8] A. Doan, P. Domingos, and A. Y. Halevy. Reconciling [9] A. Doan, J. Madhavan, P. Domingos, and A. Y.
 [10] X. Dong, A. Y. Halevy, E. Nemes, S. B. Sigurdsson, [11] O. Etzioni, M. Cafarella, D. Downey, S. Kok, [12] A. Haghighi and D. Klein. Unsupervised coreference [13] R. Ichise, H. Takeda, and S. Honiden. Rule induction [14] P. Kanani, A. McCallum, and C. Pal. Improving [15] M. Lacher and G. Groh. Facilitating the exchange of [16] J. Lafferty, A. McCallum, and F. Pereira. Conditional [17] A. McCallum, K. Bellare, and F. Pereira. A [18] A. McCallum and B. Wellner. Conditional models of [19] B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, [20] N. F. Noy and M. Musen. Anchor-prompt: Using [21] B.-W. On, D. Lee, J. Kang, and P. Mitra.
 [22] E. Rahm and P. A. Bernstein. A survey of approaches [23] E. S. Ristad and P. N. Yianilos. Learning string edit [24] P. Singla and P. Domingos. Entity resolution with [25] W.M.Soon,H.T.Ng,andD.C.Y.Lim.Amachine [26] V.I.Torvik,M.Weeber,D.R.Swanson,andN.R.
 [27] F. Wiesman and N. Roos. Domain independent [28] J. J. Zhu and L. H. Unger. String edit analysis for
