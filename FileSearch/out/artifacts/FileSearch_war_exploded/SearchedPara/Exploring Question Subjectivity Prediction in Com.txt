 In this paper we begin to investigate how to automatically determine the subjectivity orientation of questions posted by real users in community question answering (CQA) portals . Subjective questions seek answers containing private states, s uch as personal opinion and experience. In contrast, objective ques tions request objective, verifiable information, often with suppo rt from reliable sources. Knowing the question orientation would be helpful not only for evaluating answers provided by users, but also for guiding the CQA engine to process questions more in telligently. Our experiments on Yahoo! Answers data show that ou r method exhibits promising performance. H.3.5 [ Information Storage and Retrieval ]: On-line Information Services  X  Web-based services. Experimentation Subjectivity Analysis, Question Classification Although much progress has been made in automatic Q uestion Answering (QA), answering complex and realistic que stions in open domain automatically is still beyond the state -of-the-art of automatic QA systems. This is one of the reasons th at caused the emerging popularity of Community Question Answering (CQA) services  X  which allow users to post questions for other users to answer. Yahoo!Answers (http://answers.yahoo.com) no w dominates the CQA market with hundreds of millions of answers posted by millions of participants on thousands of topics. The reason for existence of these services is th at people are usually better at interpreting and answering questi ons than automatic QA systems or general-purpose web search engines. Unfortunately, QA communities are not perfect, and due to a variety of factors such as incentives, abuse, or va ndalism the provided answers may not be what the asker requeste d. Therefore, it is increasingly important to identify high-quali ty and responsive answers to questions. One approach is to automatica lly evaluate the quality of answers (and questions) provided by users of community QA systems. Another approach may be to automatically estimate  X  X orrectness X  of the provide d answers by trying to verify them against reliable sources. Our approach is complementary to the above: our goal is to understa nd the intent behind the question posted, which can then be used to infer whether proposed answers match the intent of the qu estion. We focus on one important aspect of intent detection: subjectivity analysis . We attempt to predict whether a question is subje ctive or objective. Objective questions are expected to be a nswered with reliable or authoritative information, typically pu blished online and possibly referenced as part of the answer, wher eas subjective questions seek answers containing private states su ch as personal opinion, judgment, and experience. Consider two que stions crawled from Yahoo! Answers: 
Question 1. what X  X  the difference between chemother apy and radiation treatments? ( objective ) 
Question 2. Has anyone got one of those home blood pressure monitors? and if so what make is it and do you think they are worth getting? ( subjective ) As the examples above illustrate, objective and subjective questions require different types of answers. If we could automatically predict the orientation of a question , we would be able to better identify appropriate answers for the question. For objective questions, we could try to find a few hig hly relevant articles as references, whereas for subjective ques tions, most (if not all) of answers are not expected to be found in authoritative sources. Furthermore, question orientation informat ion may be useful for evaluating user-provided answers. For ex ample, we could rank or filter answers based on whether an an swer matches the question orientation. Finally, learning how to identify question orientation is a crucial component of inferring use r intent, a long-standing problem in web search. While previous work exists on differentiating objective and subjective contents f or answer extraction [1, 2], ours is the first study on autom atically predicting question subjectivity in CQA. Compared to the questions in traditional QA researc h (e.g. TREC), questions asked by web users are prone to be ill-formatted (e.g., word capitalization may be incorre ct or missing, or consecutive words may be concatenated), ungramma tical, and include common online idioms (e.g., using  X  X  X  to me an  X  X ou X  and  X 2 X  to mean  X  X o X ). These properties make online que stions more difficult to be analyzed with current NLP technique s, even for the basic step of tokenization. Moreover, one or more a nswers may be available for these questions, and might be used to help subjectivity analysis, as good answers are assumed to have the same subjectivity as questions. We explore what que stion and answer features could be most helpful for identifyi ng subjectivity orientation of real questions. Our experiments on Y ahoo!Answers data show that our method exhibits promising perfor mance. follow this practice and focus on finding more effe ctive features for our target scenario. Features that we consider are listed in Table 1. We use case-insensitive features and chara cter n-grams to overcome spelling errors and poor formatting, and P OS features to attempt to capture simple grammatical patterns. We experi-mentted with three term weighting schemes: binary, term frequency (TF), and TF*IDF, and chose TF as the mos t effective. As questions are often accompanied by one or more a nswers, we also explore ways to obtain good performance by con sidering both questions and answers. As the classifier we us e a robust SVM implementation, LIBSVM (http://www.csie.ntu.edu .tw/ ~cjlin/libsvm), with linear kernel. Dataset: We created a labeled dataset consisting of 978 reso lved questions randomly chosen from Yahoo! Answers under the following 5 top-level categories: Arts, Education, Health, Science, and Sports. For annotation, we employed Amazon X  X  Me chanical Turk service (http://www.mturk.com). Each question was annotated by 5 Mechanical Turk workers, and we deri ved the final annotation by majority strategy with our judgment o n marginal cases. The overall average percentage agreement bet ween Mechanical Turk workers and the final annotation is 0.795. 646 questions (about 66%) are subjective, which indicat es that CQA users tend to ask subjective questions. Metrics: we use the macro-averaged F-1 (the average of F-1 f or predicting both the subjective and objective classe s). A na X ve baseline that always picks the majority subjective class would result in F-1 value of 0.392. 
Table 2 reports our experimental results. The first five rows show results using the text of the question ( question ), the text of the best answer ( best_ans ), the text of all the answers ( all_ans ), the text of both the question and the best answer ( q+bestans ), and the text of the question with all the answers ( q+allans ), respectively. Interestingly, the text of the best a nswer itself is not as effective as the text of the question, nor is us ing the text of all of the answers. One possible reason is that many be st answers (about 40%) are chosen by the community, and not th e asker him/herself, are hence do not necessarily represent the asker X  X  intent. With character 3-gram, our system achieves performance comparable with word as feature, but combining them together does not improve performance. We observe a slight g ain with more complicated features, e.g. word and POS n-gram , but the gain is not worth the increased time and space comp lexity. feature sets (Macro-averaged F-1, 5-fold cross vali dation). 
While combining question text with answer text dire ctly does not improve performance (rows 4 and 5), we do get a small, but significant gain when incorporating the answer text in separate feature spaces (rows 6 and 7), where the same term from question and answer is treated as different features. We con jecture that separately modeling distributions of terms in quest ions and answers is helpful, as we plan to explore this furt her in future work. We introduce the problem of automatically identifying subjectivity orientation of questions in QA communi ties, and explore a supervised machine learning solution with different features designed for this task. Our experiments de monstrate that case-insensitive character 3-gram feature is simple yet effective representation for this task, and that exploiting t he answers provided by other users for a question indeed can i mprove the prediction accuracy. Our method is significantly mo re accurate than a na X ve baseline, and will serve as a good sta rting point for future work. In the future we plan to explore semi-supervised learning methods, and explore further how to more e ffectively use the available answers to derive a more powerful cla ssifier. [1] Yu, H., and Hatzivassiloglou, V. Towards Answering [2] Somasundaran, S., Wilson, T., Wiebe, J., and Stoyan ov, V. [3] Zhang, D., and Lee, W.S. Question Classification Us ing [4] Pang, B., and Lee, L. A Sentimental Education: Sent iment 
