 The problem of summarizing a large collection of homo-geneous items has been addressed extensively in particular in the case of geo-tagged datasets (e.g. Flickr photos and tags). In our work, we study the problem of summarizing large collections of heterogeneous items. For example, a user planning to spend extended periods of time in a given city would be interested in seeing a map of that city with item summaries in different geographic areas, each containing a theater, a gym, a bakery, a few restaurants and a subway station. We propose to solve that problem by building rep-resentative Composite Items (CIs).

To the best of our knowledge, this is the first work that addresses the problem of finding representative CIs for het-erogeneous items. Our problem naturally arises when sum-marizing geo-tagged datasets but also in other datasets such as movie or music summarization. We formalize building representative CIs as an optimization problem and propose KFC, an extended fuzzy clustering algorithm to solve it. We show that KFC converges and run extensive experiments on a variety of real datasets that validate its effectiveness. H.3.3 [ Information storage and retrieval ]: Information Search and Retrieval X  Selection process summarization; fuzzy clustering; composite items
The problem of summarizing a large collection of items has received a lot of attention in particular in the case of geo-tagged datasets. For example, in [13], representative tags are used to summarize a large collection of Flickr photos. In the presence of several collections, each representing a dif-ferent item type (e.g., schools, restaurants, subway stations, and theaters in a city), we are faced with the question of defining an effective summarization. One could summarize one collection at a time and show the results on a map. For example, in the case of a city, each item type such as restau-rant and subway station, would be summarized separately and rendered on the same map. This one-type-at-a-time ap-proach does not necessarily guarantee that each area in the map will contain representative items of each type, nor does it ensure that items in the same area will be close to each other, i.e., cohesive . In this paper, we propose to explore the applicability of Composite Items (CIs) to this question.
CIs have been shown to be very effective in solving com-plex information needs such as planning a city tour, selecting books for a reading club, or organizing a movie rating con-test [2,3,7,9,10,12,13,15,16]. In those applications, a CI is a -POIs, movies rated by the same users) that satisfy abudget (e.g., at least two schools and one theater, at least one movie per genre). When summarizing POIs, a CI may correspond to geographically close places that have different types (e.g., theater and museum) and whose total visit time does not exceed 3 hours. When selecting books, a CI may be formed by similar books, i.e., on similar topics, written by differ-ent authors and whose total price is less than a maximum amount. When organizing a movie contest, a CI is a set of comparable movies, i.e., having common reviewers, and with different genres or release years. The budget constraint of a CI can therefore be used to glue together heterogeneous items, i.e., items with different types. In that case, we say that a CI is valid . The problem of summarizing heteroge-neous item collections can therefore be formulated as finding K valid, cohesive and representative CIs, i.e., each CI sat-isfies budget constraints, is formed of  X  X lose X  items, and the set of CIs  X  X overs X  all input items.

Forming valid, cohesive and representative CIs can be nat-urally expressed as a constrained optimization problem [2]. Existing solutions to solve this problem usually rely on two phases: in one solution, many valid CIs (i.e. satisfying the budget constraint) are built, and then the K farthest are chosen thereby resulting in representative CIs. In the other, a K -clustering is performed in the first stage to address representativity, and then one valid CI, i.e. satisfying con-straints, is picked from each cluster in order to produce K CIs overall. This process decouples budget constraint satis-faction (e.g., a CI must contain one museum and 2 restau-rants) from the optimization goal (e.g., each CI is a set of closely located POIs). As a result, we can argue that while lations are not well-adapted to achieve validity, cohesiveness and representativity simultaneously. We hence advocate the seamless integration of validity, cohesiveness and representa-tivity when building CIs. We illustrate that on the following example.

Example 1. ConsiderthecaseofMarywhosejobisto train future users of products developed by a large software company. Mary often travels to different places where she spends extended periods of time, i.e., at least 2 weeks, during which she rents an apartment. In her free time, Mary en-joys going to the theater and dining out wherever she stays. She also practices yoga and likes swimming. Mary would be interested in exploring a map with representative CIs in different areas in the city she is planning to visit. Each CI must be valid, i.e., contain at least one theater, a pharmacy, a gym, two restaurants and a subway station (at least 6 items in total with specific cardinality constraints per item type), and cohesive, i.e., contain closeby items. Figures 1a, 1b and 1c show three sets of CIs for Paris produced using the Tourpedia dataset 1 with three different methods: the one-at-a-time approach that summarizes each homogeneous item collection separately, the two-stage approach that decouples validity, cohesiveness and representativity, and an integrated approach that optimizes validity, cohesiveness and represen-tativity together. The CIs generated using the integrated ap-proach (Figure 1c) offer the best trade-off between validity, cohesiveness and representativity. Indeed, the CIs in Fig-ure 1a tend to favor representativity (coverage of the city) to the expense of cohesiveness (items in each CI are not close toeach-other).ThoseinFigure1barelocatedontheedges of the city because this two-stage approach first produces the most cohesive valid CIs, which limits their representativity in the second stage.

Our example intuitively illustrates that summarizing in-dividual lists of items and decoupling validity and represen-tativity results in finding sub-optimal CIs in case of het-erogeneous item collections. In addition, while existing ap-proaches are based on hard clustering techniques imposing that an item must belong to one CI, in the case of a hetero-geneous set of items, an item may have multiple types and should be able to belong to more than one CI. Therefore, we propose to study the applicability of fuzzy clustering [6] to find valid and representative composite items. In fuzzy clustering, an item may belong to more than one cluster. http://datahub.io/dataset/tourpedia Our algorithm solves a joint optimization problem where one part aims at identifying item representatives (related here to cluster centroids obtained through fuzzy clustering) whereas the other part ensures that the representatives cho-sen are  X  X lose X  to valid CIs (that is CIs satisfying budget constraints) and are cohesive. We consider in fact two prob-lems: a minimization problem involving distance functions, and in particular the Euclidean distance, and a maximiza-tion problem involving similarity functions, and in particular cosine similarity.

Our problem naturally arises when summarizing geo-tagged datasets but also in other datasets such as book and movie summarization. That is illustrated in our experiments that make use of real datasets: Tourpedia 2 , BookCrossing 3 , and MovieLens 4 . We compare our integrated approach with the two-phase approaches proposed in [2] and show that blending validity, cohesiveness and representativity produces higher quality CIs efficiently.

In summary, the paper makes the following contributions: http://datahub.io/dataset/tourpedia http://www.bookcrossing.com/ https://movielens.org/
Section 2 contains our formalization and general problem statement. Section 3 describes our integrated algorithm, KFC . Experiments are provided in detail in Section 4. Re-lated work and conclusion are given in Sections 5 and 6 respectively.
In this section, we first define our formal model and dis-cuss the link between clustering, validity, cohesiveness and representativity. We then formalize the problem of finding asetof K , possibly overlapping, valid, cohesive and repre-sentative CIs .
We are given a set X of items where x  X  X  is uniquely identified. X is a heterogeneous set of items each of which may have one or several types in T = { t 1 ,...,t n } .For example, the movie Titanic has two types: romance and drama. A book type could be novel or adventure and the type of a point of interest could be museum, park, etc. We use x.type to refer to the type(s) of x . We furthermore assume that an item x may have a cost, that will be denoted Foramuseum,itcouldeitherbethecostofanentryticket or the average time required to visit it.

We define a budget vector b = # t 1 ,..., # t n , # $ where each # t i specifies a cardinality for an item type t i  X  X  # $ is a total cost (e.g., maximum price a user is willing to pay for a movie or maximum time a user is willing to spend on books would represent 1 novel, 2 art books, and 1 self-help book, assuming those are the only available book types, whose total price does not exceed 90$. The same vector applied on points of interest in a city would be interpreted very differently and represent 1 gym, 2 subway stops and 1 bakery and a total time not exceeding 90 minutes.
Depending on the context, we will make use of a distance compare a pair of items ( x, x )  X  X  X X . For instance, if x and x are points of interest in a city, it is natural to use their geographic distance. If items are books, it is more ap-propriate to compare them according to content similarity, e.g. based on the cosine between their vectors; similarly, if items are movies, their similarity can be computed as the fraction of reviewers who like both x and x .
We are interested in identifying valid, cohesive and rep-resentative sets of items where each item has one or several types. The validity of a set of items is expressed in terms of the budget vector b = # t 1 ,..., # t n , # $ introduced above. The cohesiveness is the ability to identify sets of items rel-atively close to each other, whereas the representativity is the ability to cover the input dataset. The clustering lit-erature contains many proposals for finding representative points of a dataset. Indeed, representative points are typi-cally obtained, in any given dataset, as the centroids of the clusters present in that dataset: The set of clusters  X  X overs X  the whole dataset and their centroids represent a summary of the content of each cluster. In hard clustering, items are divided into distinct clusters and each item belongs to ex-actly one cluster, a framework well adapted to homogeneous items [13]. However, in the case of a heterogeneous set of items, an item may have different types and hence belong to more than one cluster. Therefore, we propose to study the applicability of fuzzy clustering [6] to the problem of finding valid, cohesive and representative items.
 The most popular fuzzy clustering algorithm is Fuzzy C-Means (FCM) [6]. FCM assigns a set of items X to a col-lection of K fuzzy clusters represented through their cen-troids v j , 1  X  j  X  K (the set of centroids will be denoted V ). More precisely, given a set of N items, X , the algo-rithm returns both the K centroids and a partition matrix W = w i,j  X  [0 , 1], i  X  [1 ,N ], j  X  [1 ,K ] where each w sents the degree to which item x i belongs to cluster j .Given a distance function d ( , ), the standard objective function of FCMisasfollows: where m is a weighting exponent, greater than one. A large value of m results in smaller memberships w ij and hence, fuzzier clusters, whereas setting m to 1 leads to hard clus-tering [6]. The problem above is typically solved through an alternate optimization process in which one fixes v (re-spectively w ) and solves for w (respectively v ). The proof that such an approach converges is given in [5]; furthermore, initialization of k-means++ [4] can also be used for the cen-troids.

Fuzzy clustering thus represents a direct way to identify clusters in a dataset and their representative points defined by their centroids. Furthermore, its fuzzy nature enables each point to be assigned to different clusters (and centroids) through membership values.
We seek to find a set of K valid, cohesive and represen-tative items. Intuitively, validity finds sets of items that satisfy a budget constraint (i.e., cardinality and/or cost) b which glues together items of different types into composite items , CIs. Cohesion and representativity intuitively try to identify those CIs formed of close items that cover the input dataset (i.e., that are close to cluster centroids). We first define a valid CIs as follows:
Definition 1. Given a set of items X andabudget b ,a valid CI, denoted { x 1 ,  X  X  X  ,x le ; x i  X  X, 1  X  i  X  le of items such that : where is an indicator function which is 1 if both arguments are equal and 0 otherwise. le is the number of items in the CIandissuchthat le  X  n ,where n is the number of type values considered. The set of all valid CIs will be denoted as V It is easy to check whether for any b and any X there exists a valid CI or not (one simply gathers the # t j cheapest items of each type t j in b and checks their total cost). We will assume that there exists at least one valid CI for the set of items and budget constraint considered. We also assume that X X  R p . This assumption is not restrictive as most data points (or items) can be transformed so as to obtain a vector representation.

We can now formulate our problem as a joint optimization problem where one part aims at identifying good summaries (i.e. cluster centroids that are representative) of the set of items whereas the other part ensures that the representa-tives chosen are  X  X lose X  to valid CIs, which are in turn cohe-sive, i.e., formed of closeby items. The closest cohesive CIs to the obtained centroids are thus valid and representative of the set of items. We in fact face two problems: a mini-mization problem involving the distance function d ( , ) and a maximization problem involving the similarity function s ( , ). Note that the weighting exponent m of the fuzzy clustering part of each problem takes values in [1 ,  X  ]forthemaxi-mization problem and in [0 , 1] for the minimization problem. This is simply due to the fact that the functions considered should be pseudo-convex in one case and pseudo-concave in the other. This leads to, when considering distances:  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X 
Distance-based formulation argmin s.t.  X  i  X  [1 ,N ] , where V denotes a set of K points (centroids) and W apar-tition matrix of size N  X  K .The Similarity-based for-mulation is obtained from the above by replacing argmin by argmax, min by max, and d ( , )by s ( , ).  X  is a parameter that controls the influence of the two aspects of the prob-lem: identifying cluster centroids that are representative of the complete dataset ( FC -Fuzzy Clustering ) while ensur-ing that the centroids obtained are close to some valid CI (
CRCI -Close Representative CI ). Minimizing the sum of the distances of all the items of the CI to the centroid in CRCI additionally ensures the cohesion of the valid CI considered. It is the compromise between these different aspects that allows one to identify valid, cohesive and representative CIs. It is important to note that the above formulation corre-sponds to an integrated approach that directly yields valid, cohesive and representative CIs. This contrasts with most previous solutions that rely on a two-step approach in which candidate CIs are first generated and then filtered [2].
Note that one could consider a more complex formulation with an explicit term to account for cohesion of items within CIs. Such a problem would however be more difficult to solve and would make us of an additional hyper-parameter. We rely on this study on the simpler form above in which cohesion is implicitly captured through the distance of all items to the centroid, as mentioned above.
 Complexity considerations The minimum sum of squared clustering problem (MSSC) is known to be NP-hard [1] (this problem is the one tackled by the classical k-means heuristic [14]). Setting  X  to 0, m to 1and d ( , ) to the Euclidean distance in Problem (1) directly corresponds to MSSC (setting m to 1 transforms the fuzzy clustering defined in FC into a hard clustering problem; the fuzzy C-means algorithm becomes standard k-means in that case [6]). Hence, were we able to solve Problem (1) in poly-nomial time, we would be able to solve MSSC in polynomial time. Problem (1) is thus NP-hard (and so is its maximiza-tion version, i.e. its similarity-based counterpart).
Furthermore, the consideration of the cost constraint in the budget may render the minimization problem in CRCI NP-hard. We thus introduce below a generalization of the the above optimization problem that allows one to partly circumvent this problem.
We present here an algorithmic solution for the optimiza-tion problem above, focusing on the Euclidean distance for used measures. Prior to that, we first introduce a slight gen-eralization that partly circumvents the minimization prob-lem in CRCI .

Given a set of items X , X X  R p , a budget constraint b and the set of valid CIs V CI ,let f be a function that associates toapoint v  X  R p a valid CI from V CI : f : R p  X  X  CI .As before, we will denote by V asetof K points (centroids) and by W a partition (weight) matrix of size N  X  K .We consider the following general minimization problem using the Euclidean distance:  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  As before, a similarity-based formulation can be ob-tained by replacing argmin by argmax and the Euclidean dis-tance by the cosine similarity. In the remainder, we will use G f . G cos is defined in the same way for the cosine similarity. It is easy to see that Problem 2 is a generalization of Problem 1 as setting f to f ( v j ) = min Problem 2 yields Problem 1. However, as mentioned above, this setting may not be always possible as it relies on a minimization problem that is NP-hard in the worst case. The general formulation provided in Problem 2 allows one to avoid this problem by considering general functions f that can be computed more easily. We will come back to thechoiceof f in Section 3.1.

If the set V is fixed and f is given, so that C j is known for 1  X  j  X  K , then G eucl ( V,W,f ) is a convex function of W and the W that minimizes it can be obtained by setting the derivative of the Lagrangian of G eucl (that integrates the constraints on W )withrespectto W to 0 and solving for W . This leads to the following update rule for W (equivalent to the standard FCM update rule [6]): where l serves to indicate that new values are computed from known (old) ones. Similarly, for fixed W and given C the function G eucl ( V,W,f )isconvexin V .Thevaluesof V minimizing G eucl are obtained by setting the derivatives of G eucl with respect to V to 0 and solving for V , leading to: where | C ( l ) j | represents the number of items in C ( l )
Forthevalidcompositeitem C j associated to the centroid v , two cases may arise depending on the function f consid-ered. Either the valid composite item provided by f for the better solution, in which case the previous valid composite item is used. This can be formalized as: The above update rules guarantee that, starting with W ( l ) V ( l ) and f , one has: as, for each update of W and V , the function G eucl is min-imized and does not decrease when updating the CIs pro-vided by f . Thus, the algorithm iterating over the update rules defined by Eq. 3, 4 and 5 convergences (as G eucl is lower bounded by 0) and provides a local minimum for the problem with the Euclidean distance.

The development for the cosine similarity is exactly the same, the convexity condition being replaced by a concav-ity one. We furthermore consider that all x  X  X  are nor-malized ( || x || 2 = 1) and add a normalization constraint on v j ( || v j || 2 = 1) so as to rely on a standard dot product ( s ( x, x )= x T x , where T denotes the transpose). The up-date rules obtained in this case are summarized below, where x Algorithm 1 Input: X , budget constraint b , K ,  X  ,step  X  , procedure f Output: Set S of K CIs 1: S  X  X  X  ;  X  =  X  ;  X  =0 2: Initialize (e.g. through random assignment) V and W 3: repeat 4: repeat 5: Update W through Eq. 3 (resp. Eq. 6) 6: Update V through Eq. 4 (resp. Eq. 7) 7: Update f ( V ) through Eq. 5 (resp. Eq. 8) 8: until G eucl (resp. G cos ) does not change 9:  X  =  X  +  X  10: until  X   X   X  11: S  X  f ( V ) (with the final f and V obtained) Algorithm 1 summarizes the steps followed. As one can note, we first set  X  to 0 and gradually increase its value. By doing so, one first identifies fuzzy centroids that are then moved towards valid, cohesive CIs.
Because the budget constraints b considered here have two parts, related respectively to type cardinality and cost (see Definition 1), we rely on two scenarios associated to two selves to budget constraints b that only contain type car-dinality constraints: b = # t 1 ,..., # t n .Inthatpartic-ular case, it is possible to efficiently compute, for any v min 1. Set C  X  X  X  2. For i =1to n ,addto C the # t i items of type t i closest 3. Return C The function f defined by the above algorithm, the com-plexity of which is O ( KN ) in the worst case, directly yields the minimizer of CRCI in Problem 1 as there is no other valid CI closer to the given point v j .

In the second scenario, we consider cost constraints in ad-dition to type cardinality constraints, leading to the general cannot directly use the above approach and we resort in this study to backtracking: we first select the closest item to a given v j with a type in b , and iteratively add the next clos-est item to v j compatible with the constraint in b .Ifthe cost constraint is violated, the process backtracks until all the constraints are satisfied. As mentioned in Section 2.3, for any b , the existence of a solution can be determined ef-ficiently; the search for a valid CI is thus only performed when the existence of a solution is guaranteed. Lastly, the backtracking process may not lead to an optimal solution in the sense of the minimization problem defined in CRCI (Problem 1); it will nevertheless yield a valid CI (close to the centroid considered), which is required to solve Prob-lem 2.
We report the results of an extensive experimental study on a variety of real-world datasets. We first examine the quality of our CIs and compare them to those produced by existing algorithms and then do an in-depth study of our integrated approach.
Our experiments confirm the superiority of the integrated approach over two-stage approaches using both problem for-mulations: distance minimization on POIs in a city and sim-ilarity maximization on movies. In summary, the CIs pro-duced from POIs using the integrated approach are charac-terized by a better coverage of the city they belong to, and the CIs produced for movies are representative of a variety of genres and release periods. We also find that the inte-grated algorithm produces better values for the objective function (distance minimization or similarity maximization) than its competitors. The second half of the experiments studies scalability (in terms of response time) of the inte-grated algorithm for different parameter values ( K and m ) and total size of items and shows that it performs very well and can hence be used to build representative CIs on the fly.
Our prototype is implemented using JDK 1.8.0. All scal-ability experiments are conducted on a 2.4 GHz Intel Core i5with8GBofmemoryonOSX10.9.5operatingsystem.
 Each result is an average of 10 runs with different random seeds.

We use three datasets with different characteristics sum-marized in Table 1. Section 3.1 defines two scenarios: with and without cost constraint. We apply the former to the first two dataset and the latter to the third dataset.
Tourpedia 5 contains a collection of heterogeneous POIs in various European cities gathered via Facebook and Four-square. The latitude and longitude of each POI is available allowing us to use Euclidean distance. We collect data for Paris, Berlin, Barcelona and Amsterdam. The heat maps Dataset #items Obj. function Tourpedia (POIs) 2 to 3000/city min geo dist.
 MovieLens (movies) 3,952 max review sim.
 BookCrossing (books) 270,000 max review sim.
 CI Year Genre Name Table 2: Example of results for u KFC with  X  =0 . 7on MovieLens showing the density of POIs in each city are depicted on Figure 2. The budget vector for this use case exploits 4 POI types: The second dataset is MovieLens 6 , a movie rating database. We rely on cosine similarity of user vectors to compute the similarity between movies. The budget used for MovieLens is defined on movie genres:
The third dataset is BookCrossing 7 which contains books and their user ratings, allowing cosine similarity computa-tion. We assign a price to each book uniformly at random between 5$, 10$ and 15$ and define a budget vector on re-lease dates:
We denote our integrated approach presented in Section 3 as
KFC . Our approach competes with two-stage approaches from the literature [2]: BOBO and CAP . BOBO is a produce-and-chose approach that first produces a large number of candi-date CIs and then chooses K with the objective to maximize the distance between them. We also designed a variant of BOBO ,called RBOBO , that optimizes representativity (i.e. the distance between each item and the closest centroid of a CI) in the second phase. Iteratively, RBOBO removes the candi-date that contributes the least to decreasing the distances of data points to their closest CI. Thus RBOBO relies on the same objective function as KFC . CAP is a cluster-and-pick ap-proach that clusters data points into K sets and selects the CI whose items are most similar to each other in each set. Since these approaches do not assign weights between data points and CIs, we only compare them against u KFC ,the hard clustering version of KFC (with m = 1). Finally, for KFC and u KFC ,wesetthe  X  parameter to 0.01.
We first compare the quality of CIs produced with u KFC against those of BOBO , RBOBO ,and CAP . Then, we study the quality of CIs produced by KFC in an independent quality evaluation.
We use Tourpedia (city = Paris) to study the behavior of our objective function with different values of  X  and K and for different algorithms (Figure 3). Given our problem definition, this objective function constitutes a proxy to as-sess the quality of the CIs generated over several executions. We then compare one instance of the CIs produced by KFC against those generated by two-stage approaches (Figure 4). Figure 3 shows the evolution of the objective function. For better understanding of the results, we normalized each score by the one obtained by u KFC . We observe that, when  X  is lower than 0 . 98, u KFC always produces better values of the objective function, which means that the CIs gen-erated offer a better compromise between cohesiveness and representativity. BOBO selects K CIs by optimizing the dis-tance between them. While it seems as it could indirectly promote representativity, in practice, BOBO tends to select outliers, and thus obtains poor results. This highlights the difference between KFC and existing approaches [2]: the ob-jective function considers all points in the dataset, while BOBO only takes the selected CIs into account. In this evalu-ation, we varied the number of candidates available to RBOBO from 20 K candidates, to CIs formed from all data points. Our experiments show that this only marginally improves the results obtained. Since we adapted RBOBO for represen-tativity, it significantly outperforms BOBO , but remains less efficient than KFC . CAP , while based on clustering, obtains worse results than most algorithms, as CIs selected, while originating from different clusters, can still be close to each other, which is detrimental to representativity. Overall, as K increases, the difference between u KFC and its competi-tor diminishes. Indeed, as more CIs are selected, each item is on average closer to a CI centroid, and thus the poten-tial difference in the objective function is reduced. When  X  nears 1, the objective function almost only takes cohesive-ness. In such configurations, algorithms based on generating huge amounts of candidate CIs obtain better results, as they have a higher probability of selecting items very close to each other.

To confirm this analysis of the algorithm X  X  behavior, we present in Figure 4 an instance of the results obtained by each algorithm on the Tourpedia (Paris) dataset. As ex-plained previously, to maximize distance between CIs, BOBO visible on Figure 4a. There are fewer points in the areas around those outliers (Figure 2a), so this achieves bad rep-resentativity. RBOBO does significantly better, thanks to the heuristic selecting K CIs among the candidates. The CIs selected are spread on the periphery of dense areas, so rep-resentativity is quite good, but not optimal. The results of CAP and u KFC exhibit some similarities. Indeed, they are both based on a clustering approach. However, CAP can still select CIs close to each other, which can be observed on the left of Figure 4c. On the contrary, CIs selected by u KFC well spread over space and achieve the best representativity. We obtain similar results on the other cities.
We conduct a similar experiment on the Movielens dataset, and present the results on Figure 5. Note that in this setup (cosine similarity), the goal is to maximize the objective function. We observe a similar trend as in the Tourpedia experiment. u KFC achieves a higher score as long as repre-sentativity is taken into account. When  X  reaches extreme values, cohesiveness becomes the main factor, and produce-and-choose approaches obtain better results. Note that on this dataset CAP performs particularly poorly.

Due to space constraints, we are unable to present exam-ples of the CIs composed by each algorithm. Hence, we will focus on the results of u KFC , presented on Table 2. We can styles and periods. For example, CI 3 showsmainlyscience fiction while CI 4 shows movies with good soundtracks.
We first examine the quality of the results obtained by u
KFC on the Tourpedia Paris dataset (Figure 4d). The CIs generated are clearly localized around some of Paris X  most famous attractions: the Eiffel tower, Montmartre, Montpar-nasse, R  X  epublique and Nation. Indeed, the presence of these attractions affects the distribution of POIs in Paris, and is able to capture this to select representative CIs.
We now consider the results obtained by KFC for Berlin (Figure 6) and Barcelona (Figure 7) for different values of m (the weight exponent that controls the centroid position-ing process). The higher m , the fuzzier representativity is. Thus, while CIs are quite spread out for low values of m , they converge towards the most central point of the dataset as the dataset contains several POIs located on the outside of the city. In practice, m could be used to control the coverage spread of the resulting CIs, while maintaining rep-resentativity.
This last set of experiments report a study of the response time scalability of KFC and RBOBO , its closest competitor, with varying values of K , m and total number of items. BookCrossing is the most challenging use case, as it com-bines a large dataset size and a total cost constraint. Fig-ure 8a shows the execution time of KFC . Overall, we observe that KFC scales linearly with both K and m . This behavior is inherited from the clustering approach underlying KFC ,and shows that while the dataset grows, convergence remains rather fast. Despite running on a laptop, KFC generates 20 composite items on a dataset of 250,000 items in less than 7 minutes.

The results for RBOBO are given on Figure 8b. RBOBO com-putes the distance between each candidate CI and each item. In the case of a large dataset, this can quickly become over-whelming. Furthermore, as the size of the dataset increases, the number of candidate CIs increases in order to preserve results quality. RBOBO iteratively removes candidates until reaching the desired number of K CIs. Hence, its execu-tion time is driven by the number of candidates, and not by K . Consequently, we were unable to execute RBOBO for more than 5000 items, as the execution was lasting over 10 minutes.
To the best of our knowledge, this is the first study focus-ing on the identification of  X  X epresentative X  composite items of a heterogeneous set of items. While there is no work that can be directly compared to ours, there are multiple related areas. We provide a brief summary of each area.

Photo Summarization. The problem of summarizing a large collection of homogeneous items has received a lot of attention in particular, for photo summarization. Differ-ent metadata was used ranging from location to temporal data to sharing information. In [13], location information was used to generate photo summaries and visualize them on a map. In [12], representative photos are generated for a given time period using patterns in photo-taking habits (later studied in http://www.hpl.hp.com/techreports/2003/HPL-2003-165.pdf ). In [11], the authors relied on event detection in personal photo collections (e.g., birthdays) which could be used for collection summarization. In [13], summaries of POIs are generated to aid visualization on a map. All cited approaches provide summaries of homogeneous item collec-tions as opposed to our work where we are able to summarize heterogeneous item collections.

Composite Items. Composite retrieval was studied with different algorithms were explored to build composite items. In [8], the authors propose a formalization for composite items (in the case of city tours) that is personalized. Most existing algorithms rely on a two-stage process that decouple constraint satisfaction (e.g., a CI must contain one museum and 2 restaurants) from the optimization goal (e.g., each CI we show that our integrated approach is more effective than a two-stage approach. The main difference however between our approach and previous ones on composite items lies in the objective functions used for the optimization. To our knowledge, our approach is the first one to fully address the problem of identifying  X  X epresentative X  composite items of a set of heterogeneous items. As discussed before (Section 2), this is achieved by relying on the identification of the cen-troids of fuzzy clusters that are close to all the items.
Constrained Clustering. We adapt the Fuzzy C-Means (FCM) Algorithm [5] to formalize our problem. More pre-cisely, we extend the standard Fuzzy C-Means formulation with an extra term aiming at  X  X ushing X  centroids towards valid CIs. It is that extra term that integrates the bud-get constraint, that guarantees the identification of valid CIs next to representative centroids, these latter ones being mainly identified as in Fuzzy C-Means. The budget con-straint is integrated in this extra term via the procedure f discussed in Section 3 that associates to any given point a  X  X lose X  and valid CI.
We explored the use of valid and cohesive composite items (CIs) to represent large collections of heterogeneous items such as POIs in a city or movies with different release dates. Validity is achieved by summarizing items with different types into a single CI. Indeed, CIs can naturally express gluing together items of different types in a budget vector such as 2 drama, 2 action, 1 comedy, $5 where each entry specifies the minimum number of each item type desired in a CI and an upper-bound of the total cost a user is willing to pay for that CI. Cohesion and representativity are achieved by finding the best K valid CIs according to an objective function. We hence formalized the problem of summarizing large collections of heterogeneous items as that of building the K most valid, cohesive and representative CIs. Our for-malization relies on two commonly-used objective functions: distance and similarity. Distance is naturally used for POIs in a city in order to find the K CIs that cover the city best. Similarity is well-adapted to representing movies in order to find the K CIs that are reviewed by similar users. We designed a new integrated algorithm that builds the K most valid, cohesive and representative CIs of an input dataset. Our algorithm integrates constraint satisfaction into fuzzy clustering in order to simultaneously optimize for validity, cohesion and representativity. Experiments on real datasets showed that the integrated approach outperforms two-stage ones resulting in CIs that achieve very good rep-resentativity of existing items.

Our immediate future plans include running more exten-sive experiments with user studies in order to refine our notions of validity and representativity in different applica-tions. We are also working on a formalization of our problem that admits adaptive constraints for validity thereby captur-ing a wide variety of interests in different item types, and on an extension in which cohesion is explicitly modeled. This work was partially funded by ANR-13-CORD-0020. [1] D. Aloise, A. Deshpande, P. Hansen, and P. Popat. [2] S. Amer-Yahia, F. Bonchi, C. Castillo, E. Feuerstein, [3] A. Angel, S. Chaudhuri, G. Das, and N. Koudas. [4] D. Arthur and S. Vassilvitskii. K-means++: The [5] J. C. Bezdek. A convergence theorem for the fuzzy [6] J. C. Bezdek, R. Ehrlich, and W. Full. FCM: The [7] H. Bota, K. Zhou, J. M. Jose, and M. Lalmas.
 [8] I.R.Brilhante,J.A.F.deMac X  edo, F. M. Nardini, [9] A. Brodsky, S. M. Henshaw, and J. Whittle. Card: a [10] M. D. Choudhury, M. Feldman, S. Amer-Yahia, [11] M. L. Cooper, J. Foote, A. Girgensohn, and [12] A. Graham, H. Garcia-Molina, A. Paepcke, and [13] A. Jaffe, M. Naaman, T. Tassa, and M. Davis. [14] J. B. MacQueen. Some methods for the classification [15] S. B. Roy, S. Amer-Yahia, A. Chawla, G. Das, and [16] M. Xie, L. V. Lakshmanan, and P. T. Wood. Breaking
