 We incorporate relevance feedback into a learning to rank framework by exploiting query-specific document similari-ties. Given a few judged feedback documents and many re-trieved but unjudged documents for a query, we learn a func-tion that adjusts the initial ranking score of each document. Scores are fit so that documents with similar term content get similar scores, and scores of judged documents are close to their labels. By such smoothing along the manifold of retrieved documents, we avoid overfitting, and can therefore learn a detailed query-specific scoring function with several dozen term weights.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Algorithms Keywords: Relevance Feedback, Manifold Learning
Relevance feedback has been shown to be an effective way of improving accuracy in interactive information retrieval. Hence, many different algorithms that can use explicit or implicit feedback have been proposed in the literature.
Relevance feedback for probabilistic retrieval models works by changing the estimation of model parameters using the information provided by the relevant documents [10]. In the case of language modeling, on the other hand, feedback doc-uments are used to alter the estimate of the query language model [7] or the relevance model [8]. Rocchio X  X  algorithm incorporates relevance feedback information into the vector space model [11]. It is based on constructing a centroid vec-tor from the feedback documents and moving the original query vector towards this centroid vector.

Recently, several machine learning techniques have been proposed to solve relevance feedback problems. Among them, global ranking using continuous conditional random fields (C-CRF) [9] is the closest to our work. C-CRF [9] exploits a ranking model defined as a function on all the judged doc-uments with respect to the query. To infer the parameters of the ranking models, C-CRF also considers similarity be-tween documents based on the contents. There are two main differences between C-CRF and our work. First, C-CRF does not make use of unjudged documents in learning. This could lead to overfitting problems, since the number of the judged documents in the case of relevance feedback is typ-ically small. Second, C-CRF uses raw document contents to compute similarity between documents, while we are us-ing query-specific content from each document for similarity computation.

During the past few years, there has been significant change in retrieval systems. Most modern search engines are now based on the learning to rank approach, which involves cal-culating a rich and diverse set of features that capture some aspect of relevance of the submitted query and a document. They then learn a combination of these features based on training data [3, 4]. Most relevance feedback algorithms are based on expanding the query by adding weighted terms from the feedback documents [11]. Even though learning to rank approaches are now popular for ranking, few relevance feedback algorithms follow that paradigm [6].

The challenge in applying the learning to rank paradigm stems from the fact that the number of given (judged) rel-evance feedback documents is very small, typically ten or less. At the same time the number of possible expansion terms is very large, on the order of vocabulary size, yielding a very large space of potential expansions to be considered. This unfavorable proportion of a small number of judged relevance feedback documents to the huge expansion space makes the problem difficult. An attempt at applying learn-ing to rank methods involves taking an initial base ranker and adapting it by training it with the relevance feedback documents for the query; however, when this is done naively, it leads to overfitting (to the labelled documents).
In this paper, we incorporate relevance feedback into a learning to rank framework by exploiting query-specific doc-ument similarities. Given a few judged feedback documents and many retrieved but unjudged documents for a query, we learn a function that adjusts the initial ranking score of each document. Scores are regularized so that documents with similar term contents get similar scores, and scores of judged documents are close to their labels. By such smoothing along the manifold of retrieved documents, we avoid overfit-ting, and can therefore learn a detailed query-specific scor-ing function. Using data from TREC and OHSUMED col-lections, we show that our algorithm produces significantly better results than relevance feedback methods trained only on judged feedback documents.
For each query: A = { a 1 ,  X  X  X  ,a m } represents the set of retrieved documents, where a i is the original representation (using features like BM25, tf-idf, etc) of document i . B { b 1 ,  X  X  X  ,b l } , represents the judgement, where b i is a ( l&lt;&lt;m ). For example, we can use  X +1 X  to represent  X  X ele-vant X ,  X -1 X  to represent  X  X on-relevant X . B is a 1  X  l matrix. The initial ranking model F 0 provides a ranking score for each document. X = { x 1 ,  X  X  X  ,x m } is a t  X  m matrix, where x is the new representation of document i . The desired ad-justment Y = { y 1 ,  X  X  X  ,y l } = { b 1  X  F 0 ( a 1 ) ,  X  X  X  ,b Y is a 1  X  l matrix.

The problem is formalized as follows: given a document set X = { x 1 ,  X  X  X  ,x m } represented over words, and the de-sired adjustment Y = { y 1 ,  X  X  X  ,y l } for the judged documents { x 1 ,  X  X  X  ,x l } ,where l&lt;&lt;m , we want to construct a map-ping function f to project any document x i to a new space, where f T x i matches x i  X  X  desired adjustment y i . In addition, we also want f to preserve the document manifold topology, such that similar documents get similar adjustments.
Notations used in the algorithm are as follows: W is a weight matrix, where W i,j = e  X  x i  X  x j 2 models the similar-ity of x i and x j . x i  X  x j stands for the Euclidean distance between x i and x j in the vector space. D is a diagonal ma-trix: D i,i = malized graph Laplacian matrix corresponding to W . I is an l  X  l identity matrix. U =  X  is a weight scalar.  X  X ap X  vector V =[ y 1 ,  X  X  X  y l , 0 ,  X  X  X  , a1  X  m matrix. () + represents pseudo inverse.
Solution to the problem is given by the mapping function f to minimize the following cost function: = = The first term of C ( f ) is based on judged documents, and penalizes the difference between the mapped result of x i its desired adjustment y i . The second term does not take label information into account. It guarantees that the neigh-borhood relationship (defined by document contents) within X will be preserved in the mapping. When x i and x j are similar, the corresponding W i,j is big. If f maps x i and different positions, f will be penalized. The second term is useful to bound the mapping function f and prevents over-fitting from happening. Here  X  is the weight of the second term. When  X  = 0, the model disregards the unlabelled data, and data manifold topology is not respected.
This formulation is similar to score regularization [6], ex-cept that (i) we learn a parametric function of x rather than a nonparametric set of regularized scores, and (ii) the func-tion represents an adjustment (an error) between the judge-ments and the initial ranking model F 0 , instead of a function that directly approximates the judgements. The latter ap-pears to be more difficult, and did not work as well in initial testing.
Our algorithm makes use of query specific document sim-ilarities. The similarities are computed using a separate dic-tionary that depends on the contents of the feedback docu-ments for each query. When we construct query specific dic-tionaries, we follow an idea that is commonly used in query expansion. In the first step, we retrieve all non-stop words from relevant judged documents. For each word, we sum up its term frequencies in all relevant judged documents result-ing in score + and its term frequencies in all non-relevant judged documents resulting in score  X  . We sort the terms using score +  X  score  X  following descending order. The top t words will be used in our query specific dictionary. We also add the query words to the dictionary if they are not already there.
The main algorithm to re-rank documents for each query is given in Figure 1. For each judged document, our query-level re-ranker provides an adjustment for the initial ranking model to close the gap between the true label and the initial ranking score. The adjustment has to be learned from the judged documents. However, given the fact that the num-ber of the judged documents is always small, the adjustment could overfit to the judged documents. To solve this prob-lem, we add a regularizer that guarantees that documents (including both labeled and unlabeled documents) with sim-ilar contents get similar adjustments.

A question that naturally arises is how to define the query-specific similarity of documents. We know that the docu-ments are represented by features like BM25, tf-idf in the initial ranking model. Such features only describe the rela-tionship between query and document. They are not good to compute similarity between documents. A more reason-able way to compute document similarity is to use document contents. In this scenario, similarity between documents also depends on the given query as the similarities are computed only using the words in the query specific dictionaries. For different queries, the contributions of the same word could be quite different. Our query-level re-ranker uses the con-tents of the judged documents to create a dictionary that only has useful words for that query. When we compute similarity using document contents, we only consider the words in that dictionary.
Our algorithm offers the following advantages: (1) The algorithm exploits unlabeled data in the form of document contents. This allows it to learn a more complex retrieval function that weights individual terms. Overfitting is controlled by using the unlabeled data for manifold regu-larization. (2) The algorithm relies on manifold learning, a rich model that does not rely on any strong distributional assumptions about the data. In contrast, other algorithms make more limiting assumptions about the data, such as Gaussianity. (3) Only two parameters need to be manually set:  X  con-trols how much we like to focus on manifold topology preser-vation. For simplicity, we use the same value for  X  in all experiments. t is the maximum size of the query specific dictionary. We set it to 200 in all the tests. (4) Closed form solution: unlike most approaches in this area, our algorithm (like [6]) provides a closed form solution of the result. The solution is global optimal regarding the cost function C ( f ).
Our main algorithm is based on manifold regularization framework [2], which goes beyond regular regression mod-els in that it applies constraints to those coefficients, such that the topology of the given data manifold will also be re-spected. A manifold is a mathematical space that on a small enough scale resembles the Euclidean space, but the global structure of a manifold may be more complicated. Manifold topology represents how instances (including both labeled and unlabeld instances) in the space is connected. Preserv-ing manifold topology can be understood as a constraint that neighbors in the manifold space will be modulated in a similar way. Computing the optimal weights in a regres-sion model and preserving manifold topology are conflicting objectives, manifold regularization provides a way to ideally balance the two goals.
 Theorem 1: f =( X ( U +  X  L ) X T ) + XUV T minimizes the cost function C ( f ).
 Proof: Given the input X , we want to find the optimal mapping function f such that C ( f ) is minimized: It is easy to verify that
X We can also verify that C ( f )=( f T XUX T f  X  2 f T XUV T + VUV T )+  X f T X L X T f. Using the Lagrange multiplier trick, we have This implies that So f =( X ( U +  X  L ) X T ) + XUV T .

Interestingly, the solution to f includes L , the normal-ized graph Laplacian matrix [5], which models data man-ifold topology. Recently, spectral graph theory combined with classical differential geometry and global analysis on manifolds forms the theoretical basis for  X  X aplacian X  tech-niques for function approximation and learning on graphs and manifolds, using the eigenfunctions of a Laplace opera-tor naturally defined on the data manifold to reveal hidden structure.
In our experiments, we use data from two different collec-tions: OHSUMED and TREC 6 X 8 ad-hoc. The OHSUMED collection contains 106 queries in total, and each query con-tains about 150 judged documents on average, with an av-erage 20% of them being relevant. The TREC collection employs a subset of Letor 3 features, as in [1]. This dataset contains 150 queries, roughly 1,500 documents are retrieved for each query, and roughly 7% of them are relevant. Both datasets are split into 5 folds, where each fold contains train-ing, validation and testing sets. In all the experiments through this paper, we set  X  = 100.

In a standard relevance feedback task, there is an initial base ranking, which is shown to a user and the user gives feedback on some documents in this initial ranking. Our base ranker is a linear regression model. One way of using relevance feedback information is to use the labels of the feedback documents for a query to train a separate ranking model for that query that fits the difference from the base ranker. We call this approach the Query-level Greedy feed-back Ranker (QGR). Another way is Positive-Negative feed-back Ranker (PNR), which is an active relevance feedback approach [12]. The PNR model was shown to outperform standard relevance feedback algorithms such as Rocchio and language modelling based relevance feedback. Hence, we mainly compare our algorithm only with the PNR model.
For each of the 5 folds in the datasets, we run our base ranking algorithm on the queries on these sets. We then assume that the feedback documents are the top k docu-ments retrieved by the base ranker (we tried k = 5 and 10 in this paper). We employ average precision (AP) to compare the quality of the manifold model with the base ranking model, the QGR model, and the PNR model. Our approach consistently outperforms the other methods, and the differences are statistically significant using a sign-rank test (Figure 2). For the TREC dataset, the results of the query greedy ranker are not included in the plot as the per-formance of the algorithm is much worse than the other three algorithms. Compared to PNR, which is implemented in the same algorithmic framework except using non-query-specific document contents to compute document similarity, mani-fold model achieves better results in all experiment settings. This is a strong indicator to show that the query-specific document contents can provide extra valuable information to query-level feedback rankers. The QGR model can be thought as a special case of a manifold model, where man-ifold topology is not respected. In these experiments, the greedy feedback ranker does not return satisfying results. The reason for this is that the greedy model can easily over-fit for the feedback data.

In Table 1, we compare greedy model and manifold model in a real example. When initial ranking scores, judgements of 5 documents and a large amount of unjudged documents (only 5 are shown) are given, both models make adjustment to the original ranking scores. The greedy approach does not respect the manifold topology, so it assigns perfect re-ranking scores to the judged documents. But the ranking scores of the unjudged documents (documents with label -1) could be dramatically changed (see the big changes in the table). We carried out another experiment (not included in the figure) of query-level feedback ranker without using the base ranker; this yielded poor results. The reason is that the number of judged documents for each query is too small to construct a good feedback ranker (without a base ranker) even when manifold topology is respected. Base rankers are valuable, since they provide prior knowledge to help the feedback rankers when the feedback is limited.
We present a novel algorithm to incorporate relevance feedback into a learning to rank framework by exploiting query-specific document contents. Given a few judged feed-back documents and many retrieved but unjudged docu-ments for a query, our algorithm provides an optimal so-lution to adjust the base ranking score of each document, such that scores of judged documents are close to their la-bels and documents with similar term contents get similar adjustments.
