 Faceted navigation is being increasingly employed as an effective technique for exploring large query results on structur ed databases. This technique of mitigating information -overload leverages metadata of the query results to provide users with facet conditions that can be used to progressively refine the user X  X  query and filter the query results. However, the number of facet conditions can be quite large, thereby increasing the burden on the user. We present the FACeTOR system that proposes a co st-based approach to faceted navigation. At each step of the navigation, the user is presented with a subset of all possible facet conditions that are selected such that the overall expected navigation cost is minimized and every result is guaranteed to be reachable by a facet condition. We prove that the problem of selecting the optimal facet conditions at each navigation step is NP -Hard, and subsequently present two intuitive heuristics employed by FACeTOR. Our user study at Amazon Mechanical Turk shows that FACeTOR reduces the user navigation time compared to the cutting edge commercial and academic faceted search algorithms. The user study also confirms the validity of our cost model. We also present the results of an extensive experimental evaluation on the performance of the proposed approach using two real datasets. FACeTOR is available at http://db.cse.buffalo.edu/facetor/ . H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  selection process and information filtering. H.5.2 [ Information Interfaces and Presentation ] : User Interfaces  X  user -centered design , graphical user interfaces. Algorithms, Experimentation, Human Factors , Performance.
 Information Overload, Quer y Interfaces, Faceted Navigation . In recent years, there has been a tremendous increase in the number and size of data bases published online, commonly referred to as the  X  X eep web X  [ 3], exposing a wide range of content including product catalogs (e.g. Amazon, eBay), bibliographies (e.g. DBLP, CiteSeer, PubMed), local businesses (e.g. Yelp) and many more. These databases are commonly queried using form s or keyword -based interfaces. When users are not familiar with the content and structure of the database, or are unable to use sophisticated search interfaces, they issue queries that are exploratory in nature and may return a large number of results. In other cases, users often issue broad (underspecified) queries in fear of missing potentially useful results. As a consequence, user s end up spending considerable effort browsing long results lists . This phenomenon, known as information overload , is a major hurdle in querying large databases.
 Information overload has been tackled from t wo directions  X  ranking and categorization. There are many recent works on ranking database results for both keyword [ 1,11] and structured queries [ 5]. Ranking is effective when the assumptions used by the ranking function are ali gned with user preferences . Ranking may not perform well for exploratory queries, since it is hard to judge which result is better than the other when the query is broad. Moreover, no summary (grouping) of the query result is provided for the user to refine her query. In categorization, query results are grouped based on hierarchies, keywords, tags, or attribute values. For instance, consider the MEDLI NE database of biomedical citations [ 16], whose articles are tagged with terms from the MeSH concept hierarchy [ 14]. Categorization s ystems propose a method for users to effectively explore the large results by navigating the MeSH sub-hierarchy relevant to the particular query result [13] . Wider adoption of such hierarchical categorization systems is limited , as building these concept hierarchies requires an intense manual effort, and automatically assigning terms to tuples afterwards is not always successful [9 ]. A popular variant of categorization, which is the focus of this paper, is faceted navigation [17] . Here, the tuples in a query result are classified into multiple independent categories , or facets , instead of a single concept hierarchy . For an example car dataset, the result for keyword query "  X  X  X  X  X  X  X  " shown in Figure 1a is categorized based on  X  X  X  X  X  X  ,  X  X  X  X  X  X  and  X  X  X  X  X  X   X  facet s, among others. Each facet is associated with a set of facet conditions , each of which appears in the number of tuples shown in parenthesis (cardinality). For instance, the  X  X  X  X  X  X  facet in Figure 1a is associated with the set {2000, 2001, ... } of facet conditions . The user can narrow down or refine this result set by selecting a facet condition (e.g.,  X  X  X  X  X  X  = 2003 ) and clicking on it. User studies have shown that faceted navigation improves the ability of users to explore large query results and identify tuples of interest when compared to single concept hierarchies [21 ]. Faceted navigation has been studied extensively by the Information Retrieval community, where the challenge is to dynamically determine the facets for a given set of documents. The drawback of these systems is the unpredictability and counter -intuitiv eness of the resulting facets [9, 10]. In contrast, faceted navigation is much more intuitive and predictable for structured databases, where each attribute is a facet describing a particular characteristic of the tuples in the dataset. The following are key concerns that need to be addressed to achieve effective faceted navigation when the number of facets and facet conditions are large : 1. Which facets and facet conditions should be suggested 2. Which facet conditions will lead to the tuples of interest in 3. The overlap of the query results among the set of suggested In this paper, we present the FACeTOR system that takes a cost -based approach to selecting the set of facet conditions to suggest to the us er at each navigation step. These facet conditions are selected using an intuitive cost model that captures the expected cost of navigating a query result. At each navigation step, FACeTOR first computes the applicable facet conditions. However, instead of showing all of them or ranking them by an ad hoc function, FACeTOR suggests a subset of them based on an intuitive navigation cost model, which considers factors including the user X  X  familiarity with the suggested conditions, their overlap, and the expect ed number of navigation steps. The suggested facet conditions are chosen such that they minimize the expected navigation cost until the tuples of interest are reached, although these are not known a priori . Recent works on faceted navigation of database qu ery results [4,17] have limitations that we address in this paper. In both works, the navigation algorithm selects one facet (or possibly multiple ones [17]) and displays all its facet conditions to the user. Instead, we suggest a mix of facet conditions from several facets, that is, our algorithm operates at the facet condition level and not the facet level. Further, our cost model more closely estimates the actual user navig ation cost. These improvements introduce novel algorithmic challenges, due to the explosion of the search space and the interactive time requirement of exploration systems. This paper makes the following contributions: 1. A complete framework for faceted navi gation of structured 2. Intuitive navigation and cost models that closely resemble 3. Two efficient and intuitive heuristics for the above problem 4. An extensive experimental evaluation with two re al datasets 5. A large -scale user study showing that FACeTOR decreases Section 8 pr esents related work, and we conclude in Section 9. The starting point of the FACeTOR framework is a result set that the user explores. Definition 1 (Result Set) A result set is a relation  X  with schema  X  = {  X  1 , ... ,  X   X  } . Each attribute  X   X   X  X  X   X  has an associated active domain  X  X  X  X  X  X  (  X   X  ,  X  ) of un-inter preted constants.  X  The initial result set  X  could be the whole database or more realistically, the result of a keyword query. In this work, we assume that the user first submits a key word query (e.g., "  X  X  X  X  X  X  X  " in Figure 1a). At each step of a faceted navigation, FACeTOR classifies the tuples of a result set  X  according to their facets . Each attribute  X   X   X  X  X   X  of  X  contributes a facet to the classification which in turn, contribut es a set of conditions . Definition 2 (Facet Condition): Given a result set  X  , a facet condition is an equality predicate  X  :  X   X  =  X   X  , where  X   X   X  X  X  X  X  X  X  (  X   X  ,  X  ) .  X  The set of all possible facet conditions for a result set  X  is  X  (  X  ) . Our ru nning example considers a cars result set  X  whose tuples are classified by their  X  X  X  X  X  X  ,  X  X  X  X  X  X  ,  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  and 37 more facets. As shown in Figure 1a, FACeTOR displays the name of each facet along with a list of facet conditions as hyperlinks, followed in parenthesis by the number of tuples in  X  satisfying the condition (cardinality).
 When the user clicks on a hyperlink corresponding to a facet condition  X   X  , FACeTOR filters the result set  X  to the tuples that satisfy  X   X  , thus yielding a new result set  X   X  navigation proceeds to the next step where  X   X  is now being classified. FACeTOR captures the progression of the faceted navigation using a query  X  . When the user clicks on a facet condition  X   X  , then the equality predicate is added conjunctively to  X  , thus forming a refined query  X  X  X  X   X  . At each navigation step, FACeTOR suggests only a subset  X   X  (  X   X  ) of all possible facet condi tions in  X  (  X   X  ) . Definition 3 (Suggested Conditions) : For a result set  X   X  (  X   X  X  X  X  )  X  X  X  X   X  (  X   X  ) =  X   X  , that is, every tuple in  X   X  satisfies at least one suggested condition.  X  In this w ork, we are interested in minimizing the overall expected navigation cost incurred by the user, by choosing the best set of suggested conditions for a given  X   X  , without making any assumptions about the user X  X  preference over the tuples in  X  navigation cost is based on an intuitive model of user navigation. The faceted navigation model of FACeTOR is formally presented in Section 3.1 and forms the basis for the navigation cost model defined in Section 3.2 . We present complexity results in Section 3.3, showing that selecting the set of facet conditions that minimize the expected navigation cost is NP -Hard. At each faceted navigation step, FACeTOR displays to the user the set of suggested conditi ons  X   X  (  X   X  ) for the current result set  X  . The user then explor es  X   X  by examining all conditions in  X  (  X   X  ) and proceeds to the next navigation step by performing one of the following actions: 1. SHOWRESULT (  X   X  ) : The user examines all tuples in the 2. REFINE (  X  ,  X  ) : The user chooses a suggested condition 3. EXPAND (  X   X  ,  X   X  ) : The user is dissatisfied with (rejects) all The formal navigation model is presented in Figure 2. It is a recursive procedure and is initially called on the entire result set  X  and the identity query  X  , and terminates when the user finds all the tuples of interest, i.e. when the user executes SHOWRESULT (  X   X  ). The set  X   X  and the suggested conditions  X  (  X   X  ) is computed at the beginning of each NAVIGATE step.
 The cost model measures the navigation cost incurred by the user when exploring a query result set  X   X  , using the navigation model described in Section 3.1 . The navigation cost is the sum of costs of the actio ns performed by the user , that is, examining suggested conditions, SHOWRESULT, REFINE and EXPAND actions.
 The cost of examining all tuples in a result set  X   X  , that is, the cost of SHOWRESULT (  X   X  ) is |  X   X  | , and the cost of examining all suggested condit ions is  X   X   X  (  X   X  )  X  .We assume that the REFINE and the EXPAND actions have a cost  X  associated with them , that is,  X  is the cost of  X  X licking X  on a suggested condition or executing an EXPAND action on the attribute  X   X   X  X  X   X  . If the exact sequence of actions followed by the user in navigating  X  were known a priori , we could accurately determine the cost of navigation. Since this sequence cannot be known in advance, we estimate the navigation cost, taking into account the inherent uncertainty in the user navigation. T o estimate the navigation cost, we introduce four probabilities:  X  SHOWRESULT Probability  X   X  X  X  (  X   X  ) is the probability  X  REFINE Probability  X  (  X  ) is the probability the user refine s  X  Attribute Preference Probability  X   X  (  X   X  ) is the probability  X  EXPAND Probability  X   X  (  X   X  ) is t he probability the user Since the navigation model is recursive, the expected navigation cost can be esti mated by the following recursive cost formula : where The first line of Equation 1 captures the fact that the user has two options , when presented with a set of suggested conditions. One is to e xecute a SHOWRESULT action with probability  X   X  X  X  (  X  cost |  X   X  | . The other is to exe cute a REFINE or EXPAND action with probability 1  X  X  X   X  X  X  (  X   X  ) . The cost entailed by this last option consists of the following parts shown in the square brackets of cost formula: 1. A fixed cost  X  of a REFINE action. 2. The user reads the suggested conditions with cost |  X  3. With probability 1  X   X   X   X  X  X   X   X  the user decides to REFINE . 4. With probability  X   X  (  X   X  ) , the user does not choose any of the The cost formula (Equation 1) quantizes the effort incurred by the user navigating the results  X   X  of the query  X  . The challenge now is to choose the set of conditions  X   X   X  X  X   X  minimizes the overall navigation cost. We prove that the problem of finding the suggested facet conditions that minimize the expected navigation cost given by Equation 1 is NP -Hard, by showing that a simplified version of the problem is also NP -Hard. The Simplified Facet Selection (SFS) problem considers a simpler navigation model than the one in Section 3.1 , called NAVIGATE -SINGLE and defined next.
 NAVIGATE -SINGLE: In this model , the system performs a single REFINE action, where the user randomly selects one of the suggested conditions, and then performs a SHOWRESULT action. The cost of NAVIGATE -SINGLE navigation is the cost to examine all suggested conditions displayed ( |  X   X  (  X  cost  X  X  X   X  X  X  X   X  of performing the SHOWRESULT action for the randomly -selected suggested condition  X  . Suppose that the dominant cost of our cost model is that of examining a suggested condition. That is, suppose the cost to examine a suggested condition is 1 and the cost of SHOWRESULT is 0. Also suppose that all attributes of  X  Boolean (0, 1) and that the suggested conditions in  X  always positive, that is, |  X  (  X   X  )| = 1 . Recall that facet conditions only specify a single attribute.
 Theorem 1: The SFS problem is NP -Hard .  X  Proof (sketch) : SFS is clearly in NP. To prove the NP -Hardness we reduce from the HITTING -SET problem. An instance of the HITTING -SET problem consists of :  X  a hypergraph  X  = (  X  ,  X  ) , where  X  is a finite set of vertices  X  a positive integer  X   X  |  X  | . The problem is to determine whether there is a hitting set  X   X  X  X  of size  X  such that  X   X   X  { 1, ...  X  } :  X  X  X  X   X   X  X  X  . We reduce HITTING -SET to SFS as follows. A node  X  becomes a facet condition  X   X  = 1 . A hyperedge  X   X   X  X  X  becomes a tuple  X   X  in the result set  X   X  .  X   X  connects the vertices corresponding to the attributes that have value 1 for the result  X   X  . The solution of HITTING -SET translates naturally to a solution to NAVIGATE-SINGLE and vice versa.  X  Our aim in this paper is to present a framework for effort based navigation of faceted query results. The problem of estimating probabilities,  X   X  X  X   X  X  X   X   X  ,  X  (  X  ) ,  X   X  (  X   X  ) and  X  the solution and can be estimated in various ways viz. information theoretic approaches such as entropy, user navigation logs etc. However, for the sake of completion and evaluation of the framework, we present a method to estimate these probabilities. Estimating  X   X  (  X   X  ) : This is the probability the user knows or likes attribute  X   X  . We estimated this probability using a survey of 10 users (students and faculty in our institutions) who rated each attribute  X   X  in the dataset on a scale from 0 to 1. These values are tak en to be the user preference  X   X  (  X   X  ) for attribute  X  Estimating  X   X  X  X  (  X   X  ) , the probability the user executes SHOWRESULT on a given result set  X   X  . We use the information theoretic measure of Entropy to estimate  X   X  X  X  behind this de cision is that the user would choose to further refine the query  X  and narrow down the result set  X   X  if the tuples in  X  are widely distributed among all possible facet conditions  X  (  X  The entropy of a result set  X   X  distributed amongst the face t conditions in  X  (  X   X  ) is given by: where  X  =  X  |  X   X  X  X  X  |  X  X  X  X  (  X  over all facet conditions. Since the value of entropy can be greater than 1, we normalize it with the maximum value of entropy for a given result set  X   X  distributed over  X   X  (  X   X  )  X  facet conditions. Entropy is maximal when  X  tuples are distributed equally amongst  X  X  X  (  X   X  )  X  facet con ditions, that is, each facet condition is satisfied by  X  /|  X  (  X   X  )| tuples. The entropy of such a system is: Estimating  X  (  X  ) :  X  (  X  ) is the probability the user executes a REFINE action on suggested condition  X  . A user would REFINE with the value of the attribute in  X  . Therefore, we used a two -pronged approach to c ompute  X  (  X  ) . To estimate the popularity of a value of a facet condition, we computed the frequency  X  X  X  X  X  X  (  X   X  .  X   X  ) of each value for each attribute in  X  . Then, we multiply each frequency with the attribute preference to obtain the attribute/value pref erences  X  (  X  :  X   X  =  X   X  ) =  X  X  X  X  X  X  (  X  which we then normalize by di viding by the maximum frequency. Given the intractability of the Facet Selection problem, we have to rely on heuristics to compute the set of suggested conditions. To develop these heuristics, we analyzed the cost model presented in Section 3.1 to determine the characteristics of suggestions that form good candidates for suggested conditions. This analysis is summarized as a brief discussion in Section 5.1. Next, we present two heuristics to efficiently compute the best set of suggested conditions. The first, ApproximateSetCover (Section 5.2), is inspired by an approximation algorithm for the weighted set cover problem [6], and attempts to find a relatively small set of suggestions that have a high probability of being recognized by users (high P(c)). The second heuristic, UniformSuggestions (Section 5.3), follows Equation 1 more closely and greedily selects facet conditions based on a heuristic assumption that is der ived from the analysis of the cost model.

Figure 3. Result Set  X   X  , All Facet Conditions  X  (  X   X  Consider a sample result set  X   X  shown in Figure 3. Also shown, are three alternative sets of suggested conditions (Figure 3a, 3b and 3c ) selected from the set of all facet conditions  X  (  X  one of the alternative set of suggestions shown in Figures 3a, 3b and 3c has the lowest cost, and therefore is more likely to be selected by the navigation cost model? The suggested conditions shown in Figure 3a are highly selective, since each one of them appears in a small number of results (low cardinality). Therefore, a large number of su ch conditions are required to cover the result set  X   X  causing the navigation cost to increase as the user now has to read all the labels before proceeding to the next navigation step.
 A set of suggested conditions where each condition has low selectivity (Figure 3b) also leads to a high overall expected navigation cost. Such conditions typically have a high overlap and do not effectively narrow down the result set and therefore, the user has to execute more REFINE actions to narrow down the result set. Fo r example, refining by either  X  X  X  X  X  =  X  X  X  X  X  X  X  or  X  X  X  X  X  X  = 2005 , in Figure 3 b, reduces the number of results from the initial six to four, and the resulting result set may need to be refined further before reaching the desired result(s). Conditions with low selectivity can potentially lead to redundant navigation steps. For example, refining by  X  X  X  X  X  X  X  =  X  X  X  does not narrow down the result but still adds to the navigation cost.
 Based on the above discussion, we observe that the facet conditions selected by the cost model as suggested ones should neither have high nor low selectivity. The suggested conditions in Figure 3c are facet conditions with such desired characteristics. The conditions  X  X  X  X  X  =  X  X  X  X  X  X  X  X  ,  X  X  X  X  X  X  = 2001 and  X  X  X  X  X  X  X  =  X  X  X  X  X  X  X  are moderately selective and thus have minimum overlap and do not require a large number of conditions to cover  X  Another factor that increases the navigation cost is the EXPAND action, since the user can potentially see a large number of conditions, thereby increasing the navigation cost. The expected cost of EXPAND is multiplied by  X  (1  X   X  (  X  ))  X  X  X  X  minimized when all the conditions in  X   X  (  X   X  ) have a high  X  (  X  ) . Given a result set  X   X  and its facet conditions  X  (  X   X  is to compute the set of suggested conditions  X   X  (  X   X  ) such that the expected navigation cost, based on our cost model, is minimal and the set  X   X  (  X   X  ) covers  X   X  , that is,  X   X   X  X  X  X  =  X  each facet condition  X  covers  X  X  X   X  X  X  X   X  results in  X  closely resembles the well -known NP -hard weighted set cover problem  X  given a set system (  X  ,  X  ) , such that  X   X  weights  X  :  X  X  X  X  + , find a subfamily  X   X  X  X  such that  X   X  and  X   X  (  X  )  X  X  X  X  is minimal. The approximation algorithm for weighted set cover [6 ] adds at every step the set  X  that maximizes the number of newly covered items divided by the weight  X  (  X  ) . In order to apply the approximation algorithm for weighted set cover to our problem, we need to define the weight function  X  :  X  (  X   X  )  X  X  X  + . By observing the cost formula in Equation 1 , each facet condition in the suggested set  X   X  (  X  high probability  X  (  X  ) of being selected for REFINEment. Otherwise, the probability that the user does not select a suggested condition and chooses EXPAND would be high, resulting in a high overall cost. To achieve this objective, we set the weight function to be  X  :  X   X  X  X  (  X   X  )  X  1/  X  (  X  ) . Note that the overlap among conditions and number of elements covered by a selected condition do not need to be part of  X  , since they are considered directly in the approximation algorithm.
 Figure 4 presents the ApproximateSetCove r heuristic , which is an adaptation of the weighted set cover approximation algorithm [ 6] using the above defined weight function, and has a running time of  X  (|  X  (  X   X  )|  X  |  X   X  |) and an approximation ratio of  X  (log (|  X  (  X   X  )|)) . Note that this approximation ratio assumes that the quantity we want to minimize is the sum of the weights ( 1 /  X  (  X  ) ) of the selected conditions. However, the real objective of ApproximateSetCover is to minimize the navigation cost, which is much harder to bound, given tha t ApproximateSetCover does not capture all the details of Equation 1. Also note that this approximation ratio can be large if the number of conditions in  X  (  X   X  ) is large. However, the number of facet conditions is generally small and this algorithm perfo rms reasonably well in practice, as demonstrated by the experiments in Section 6 . Example: Figure 3 b shows the result of the ApproximateSetCover heuristic on the result set  X   X  in Figure 3 . The algorithm requires two iterations of the while loop (lines 3 -7) before terminating with the set of suggested conditions in Figure 3 b. In the first iteration, the algorithm selects  X  X  X  X  X  =  X  X  X  X  X  X  X  , since this facet condition covers 4 results and has the maximum value of  X  (  X  )  X  |  X  3.2 amongst all the conditions in  X  (  X   X  ) and  X  is empty. In the next iteration, two results (  X  1 &amp;  X  3 ) remain uncovered and are covered by facet condition  X  X  X  X  X  X  = 2005 .  X  In this heuristic we follow the cost formula in Equation 1 more closely, which leads to a more robust heuristic. C omputing the optimal suggested conditions involves recursively evaluating Equation 1 for each combination of facet conditions in  X  (  X  This translates to a very large (in both height and width) recursion tree. UniformSuggestions replaces this recursion tree with a set of very small recursion trees , one for each condition in  X  (  X  that, we evaluate the expected cost of each facet condition independently, assuming that all future suggested conditions will have identical properties, and then select the facet conditions with minimal expected cost, until all results in  X   X  are covered. In particular, the uniform -condition heuristic assumption states that for a given condition  X   X  X  X  (  X   X  ) , evaluate the navigation cost using Equation 1, while assuming that every other condition in  X  (  X   X  ) has the same characteristics as  X  . The characteristics of  X  are (a) its probability  X  (  X  ) , and (b) the ratio  X  (  X  ) = |  X  of the uncovered results that  X  covers. This heuristic assumption reduces the search space of suggestions to  X  X  X  (  X  condition is now evaluated independently. It also allows us to simplify the cost formula in Equation 1 as follows.
 If each suggested condition in  X   X  (  X   X  ) cove rs a ratio  X  of the the results in  X   X  . Also, REFINEment by  X  narrows down  X  estimated |  X   X  |/  X  number of results. On the other hand, if the user does not select a suggested condition and instead EXPANDs an attribute  X   X  , she views an additional |  X  (  X   X  ) \  X  facet conditions. Also, in the absence of any prior knowledge about the selectivity of facet conditions in  X  (  X   X  ) , we assume that each  X  X  X  X  X  (  X   X  ) narrows d own  X   X  to an estimated  X   X  Thus, we can simplify the recursion in Equation 1 as follows: where  X   X  (  X  ) =  X  1  X  X  X  (  X  )  X   X  Observe that instead of  X  , the cost function in Equation 3 above uses  X  and  X  X  X   X   X  as arguments for this heuristic, since a cost is computed for each  X  , and only the number of results |  X  importa nt. The parameter  X  in the original cost formula (Equation 1) captured the query progression with REFINE actions, which is not required in this heuristic . In Equation 3 above ,  X  normalized probability of following one condition of type  X  . Since all  X  suggested conditions have the same  X  (  X  ) , then  X  1  X   X  . Therefore the cost component in Equation 3 for navigating all  X  suggested conditions can be rewritten as : By a similar argument, and since every facet condition  X  X  has the same characteristics as  X  in Equation 3, we can simplify the last line of Equation 3 as follows , where  X   X  is the attribute of  X  : Therefore, the cost equation (3) can now be rewritten as: The recursion terminates when the size of the result  X  X  X  below a threshold  X  . Since a navigation should be able to narrow down the result to a single tuple, we set  X  to 1 . The algorithm, based on the uniform -condition heuristic assumption is presented in Figure 5 . The algorithm computes the estimated  X  X  X  X  X  X  of each facet condition using the simplified cost formula in Equation 4 (lines 4 -9), and selects the condition with the minimum  X  X  X  X  X  X  (  X  X  X  X  X  X  ) to be added to the set of selected conditions (lines 10-11). Next, we remove from the set  X  of uncovered results the results covered by  X  X  X  X  X  X  . The algorithm terminates when all the results in  X  are covered.
 The result of applying the UniformSuggestions heuristic algorithm to the result set  X   X  in Figure 3 is shown in Figure 3c. Recall from the discussion in Section 5.1 that the cost model selects conditions with moderate selectivity and high  X  (  X  ) . Under our heuristic assumption, a facet condition  X  is evaluated under the assumption that all conditions in  X  (  X   X  ) have the same characteristics as  X  . T herefore, a condition with moderate selectivity and a high  X  (  X  ) has a lower cost when evaluated using the simplified cost formula in Equation 4 and these are just the conditions selected by the algorithm.
 In this section, we present a thorough evaluation of the algorithms and heuristics described in Section 5 and show that FACeTOR achieves a significant decrease in navigation cost compared to current approaches . The experiments , presented in Section 6.2, are based on a large-scale simulation of user navigations. The metric used is the average navigation cost as defined by the cost formula in Equation 1 . Section 6.3 measures the time requirements of our heuristics and shows that they can be used for real -time interaction. The primary goal of these experiments is to evaluate the effectiveness of the system in decreasing the user navigation cost for a set of query results. To this end, we compare the two heuristics presented in Section 5 to each other and to the current state of the art algorithm, which is the single -facet -based -search [17 ], henceforth called INDG. All experiments were conducted on a Dell Optiplex machine with 3GHz CPU and 3GB of RAM. We use d MySQL as our database and Java for algorithms.
 Datasets : We evaluate FACeTOR on two datasets, UsedCars and IMDB. We assume that numeric attributes have been appropriately discretized. The UsedCars database was downloaded from Yahoo! Auto and contains 15,191 car tuples with 41 attributes/facets . From the IMDB dataset, we extracted a total of 37,324 movies. W e only leveraged the movie, actors, directors , ratings and genre data. Note that actors, directors and genres are set -valued attributes, i.e. each attribute can have multiple values for a given movie.
 Methodology : For each dataset, IMDB and UsedCars , we select a number of keyword queries ( see Table 1 ) whose results from the initial result set  X  , and a random result tuple as the target for navigation for each query. Next, we m easure the number of navigation actions (REFINE/EXPAND actions, facet conditions displayed and results viewed) incurred before reaching the target tuple as the navigation cost for the query. In our system, the target tuple can be reached by multiple naviga tion s. For example, tuple  X  in the result set of Figure 3 can be reached by REFINEing by any one of the two conditions in Figure 3b.
 Since, the user X  X  navigation cannot be known in advance, we consider an evaluation approach that considers both these navigation paths. To account for uncertainty in user navigation, we use a guided randomized simulation of user navigation. In this simulation, we randomly select one of the facet conditions  X   X  X  X   X  (  X   X  ) for navigation . The probability that the agent selects a condition  X  is proportional to  X  (  X  ) , the probability that the user would know or likes the facet condition  X  . The simulation is target result. For example, if the agent encounters the two suggestions in Figure 3b and the target is tuple  X  3 , the simulation would choose either  X  X  X  X  X  =  X  X  X  X  X  X  X  or EXPAND. The proba bility of choosing EXPAND is  X  ( 1  X  X  X  (  X  ) )  X  X  X  X   X   X  X  X   X   X  are the suggested conditions. We execute the navigation for each query 1000 times using this simulation technique and average the cost over the individual navigations. We also report the av erage number of times each navigation action is executed. The navigation cost is sensitive to the constant  X  according to the cost function in Equation 1 . Varying this constant changes the set  X  (  X   X  ) for UniformSuggestions, but not for ApproximateSetCover, since it does not consider  X  . Intuitively,  X  denotes the patience of the user towards suggestions generated by the system. If the user sees a small number of conditions she would have to execute m ore REFINE actions to reach the result. Thus by setting  X  to a large value the user should typically see more suggestions per REFINE and vice versa.
 We experiment with different values of  X  and observe the effect on the overall navigation cost for the U sedCars query workload in Table 1 . We also compare the number of suggested conditions generated (on average) and the number of REFINE actions. We compare our approach with the current state of the art INDG algorithm [17]. This algorithm constructs a decision tree that partitions the result set  X   X  by a facet (attribute) at each level. The aim is to minimize the average depth of the tree in reaching the results. The user is presented with all the facet conditions on the attribute that forms the root of the decision tree. Since INDG generates suggestions from a single attribute, the simulation for this algorithm differs from above as follows: at each step, the agent chooses to EXPAND or REFINE by one of the suggestions of an attribute  X   X  with probability  X   X  (  X   X  ) and EXPAND action reveals all the facet conditions for a different attribute.
Figure 8. Average Navigation Cost for  X  =  X  and  X  =  X  X  X  The average navigation costs for the INDG, ApproximateSetCover and UniformSuggestions algorithms for the UsedCars queries in Table 1 are shown in Figure 6a. As seen in the graph, our approach leads to significant savings in navigating cost. Figure 6b shows some of the individual component s of the total cost for Figure 6 a, that is, the average number of REFINE actions, average number of EXPAND actions. Also show n (top of the bars) are the average numbers of suggestions per navigation step. As expected, the INDG algorithm has very few R EFINE and EXPAND actions, but reveals a large number of facet conditions, resulting in high overall cost. The INDG algorithm ignores the cost of inspecting labels and therefore produces a large number of suggestions at each navigation step.
 The average cost incurred by UniformSuggestions algorithm is less compared to ApproximateSetCover. ApproximateSetCover has a higher number of REFINE and EXPAND actions as compared to UniformSuggestions, even though the average number of suggestions at each navigation step is comparable. In each iteration, the greedy ApproximateSetCover algorithm selects a small set of facet conditions with a high value of  X  (  X  ) that also cover a large number of results. These suggested conditions therefore, have a low selectivity and tend to have a high degree of overlap (Figure 7 ), thereby reducing the effectiveness of REFINE actions. Thus , the user has to perform many REFINE actions in order to reach the target result s. Figure 8 shows the effect of increasing  X  , the cost of executing REFINE. As expected, the average overall cost increases. The UniformSuggestions adapts to a changing value of  X  , whereas the ApproximateSetCover and INDG do not. Therefore the cost of UniformSuggestions increases at a slower rate than the ot her two algorithms. This is primarily because, for a higher  X  , UniformSuggestions generates more suggestions per REFINE/ EXPAND.
 The results of IMDB workload queries in Table 1 are shown in Figure 9. As in the UsedCars workload, the UniformSuggestions outperforms ApproximateSetCover. Also, t he observations for the number of EXPAND and REFINE actions and the number of suggested conditions generated is also similar to those for the UsedCars dataset. However, the navigation cost with the UniformSuggestions alg orithm is much lower than ApproximateSetCover. A movie in the IMDB dataset can be classified into a large number of facet conditions. For example, each movie can have multiple actors or directors or genres. Therefore executing an EXPAND action reveals a very large number of facet conditions (the number on top of bars in Figure 9b), thereby significantly increasing the navigation cost. Figure 10. Average Execution Time of UniformSuggestions This experiment aims to show that UniformSuggestions is fast enough to be used in real -time. The average execution time of UniformSuggestions per REFINE action for the queries in Table 1 ( UsedCars dataset ) is shown in Figure 10. The execution time for this heuristic depends primarily on the number of facet conditions in the result set  X   X  . As the number of facet conditions decreases, as is the case towards the end of navigation, the performance of UniformSuggestions improves dramatically. In the interest of space, we omit reporting these values , as well as the results for ApproximateSetCover which, given its simplicity, is much faster. In this section, we present the results of a us er study we conducted to compare the user experience with FACeTOR and other state of the art interfaces. We measure (a) the actual time it took users to navigate using different interfaces, (b) how realistic is our cost model, by studying the relationship of the actual time (actual cost) with the estimated cost and (c) the users perception of the faceted interfaces through a questionnaire. By comparing the actual navigation time to the users X  perception, we study if lower actual time corresponds to more int uitive (cognitively easier) interfaces. We constructed 8 randomly created result sets of 1000 tuples from the UsedCars dataset and for each one we created a task that involves locating a set of target tuples (cars), which satisfy a set of attribute/value c onditions. For each one of the 8 result sets, we showed the requested conditions to the users and asked them to locate the target tuples using three interfaces: (a) FACeTOR , (b) Amazon -Style , which suggests at most 5 facet conditions with the highest cardi nality for each attribute, and (c) O ne-attribute -at -a-time INDG [17], where an attribute is selected at each step and all its conditions are displayed. We deployed our system on Amazon Mechanical Turk [2] task and collected a total of 37 responses.
Figure 11. Actual User Navigation Time for 8 Result Sets Actual Time Figure 11 shows the actual time as well the average time taken by users to navigate each of the eight result sets using the three interfaces. As shown, FACeTOR speeds up the navigation by 18% and 37% over Amazon-Style and INDG respectively, even for relat ively small result sets of 1000 tuples (Figure 12). This is primarily because users spend less time in reading suggested conditions and deciding which one to follow next, as evidenced by Figure 12. FACeTOR shows 36% fewer suggestions than Amazon-style and 57% fewer suggestions than INDG, while it requires the same number of REFINE and EXPAND actions (on average) to reach the target tuples. This is an indication of high quality suggestions provided by FACeTOR. Estimated Cost Figure 13 displays the data points of actual time vs. estimated cost, as computed by Equation 1, for the eight result sets for t he three interfaces. Based on these data points, Figure 13 also shows the trend li ne between actual time and estimated navigation cost for each interface. We observe that the actual time is linearly proportional to the estimated navigation cost for all thr ee interfaces, which shows that our cost model is realistic. Users Perception The study also included a questionnaire where we elicited the users X  opinion on various aspects of the three interfaces, including the ease of use, size and intuitiveness of suggested conditions and preferred choice of interface. The results of this surve y are shown in Figure 14. 92% of users said that they thought the suggestions presented by FACeTOR at each step made the task of locating the target tuples easier (Figure 14a), compared to 89% for Amazon-style and 40% for INDG. A large majority of users (9 2%) also said that the suggestions provided by FACeTOR had a low  X  X ognitive dissonance X  (Figure 14b) in the sense that it was very easy (45%) or easy (46%) to decide which suggestion to follow. We also asked the users if the number of suggestions provided by the interfaces were adequate (Figure 14c). A significant percentage (30%) said that FACeTOR provided too few suggestions at each navigation step, indicating that users prefer more choices even if it means an increase in absolute navigation cost  X  a situ ation that could easily be remedied by increasing the value of constant  X  . Ranking Ranking could be applied in conjunction with a faceted interface. [5] uses the unspecified attributes and apply Probabilistic Information Retrieval principles to rank the results of a database selection query. Various ranking techniques have also been proposed for ke yword search on structured databases [ 1,12] based on the size and relevance of the results.
 Faceted Search on S tructured Data Faceted search employe d by major e-Commerce web sites (Amazon, eBay) typically display s all the facet conditions applicable to the current set of query results. If too many values are available for a facet, then the most popular are displayed, and a  X  X ore X  button reveals the res t. In contrast, our approach displays only a subset of applicable facet conditions chosen to minimize the overall navigation cost. English et al. [8] was one of the first to introduce faceted search and discusses facets from a user interface perspective.
 Our work is closest to [4] and [17], which also use a cost based approach for faceted navigation. In particular, FACeTOR adopts ideas from both works and addresses their key shortcomings. In both these works, the navigation algorithm selects one attribute (or possibly multiple attributes [17 ]) and displays all the values of these attributes to the user. Alternatively, a text box could be displayed [17 ], but we believe that this is impractical, given all known values would have been in the original query. Our approach differs from these works, because we display a mix of facet conditions from several attributes, that is, our algorithm operates at the attribute value level and not the attribute level. Keyword -B ased Faceted Search and Query Refinement The GrowB ag project [7] and Sarkas et. al [18] suggest additional search terms based on the co -occurrence patterns of these terms in the query result. The GrowBag algorithm [7] computes higher order co-occurrences of terms in the document collection and suggests te rms appearing in the neighborhood of each search term as refinement suggestions whereas [18] suggests terms that co -occur with search terms and narrow down the result -set to interesting subsets using the surprise metric. Our work is also related to query r efinement systems [15,19]. [19] recommends new terms for refinement such that the recall of the resulting query is maximized, whereas [15] uses relevance judgment feedback on the results to refine the query. Our approach also suggests facet conditions to r efine the query, but we use navigation cost as metric. Our navigation model is similar to BioNav [13], which uses the ontological annotations of PubMed publications to create a navigation tree. A key difference is that in BioNav, there is a given concept h ierarchy [14], which prunes the search space. In contrast, there is not such tree in FACeTOR, which makes the selection of a set of faceted conditions harder. OLAP A faceted interface can be viewed as an OLAP -style cube over the results. [20] generates hie rarchical partitions over the query results based on a cost model for user navigation and display this hierarchy to the users. The interestingness of group-by aggregations is used to rank candidate aggregations to display. Faceted navigation is employed to reduce the information-overload experienced during navigation of query result s. The effectiveness of these interfaces is limited as they often show too many or irrelevant facet conditions . Our system addresses these problems by selectively showing a subset of the available facet conditions that are selected based on an intuitive cost -based navigation model that attempts to minimize the navigation cost by hiding uninteresting or ineffective conditions. We provide feasible solutions for this problem and demonstrate their effectiveness by a thorough experimental evaluation and a user study. Vagelis Hristidis was partly supported by NSF gran ts IIS -0811922 and IIS -0952347, and DHS grant 2009-ST -062 -000016.
