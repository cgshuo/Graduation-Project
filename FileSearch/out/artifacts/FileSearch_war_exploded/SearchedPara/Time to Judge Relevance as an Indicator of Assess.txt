 When human assessors judge documents for their relevance to a search topic, it is possible for errors in judging to oc-cur. As part of the analysis of the data collected from a 48 participant user study, we have discovered that when the participants made relevance judgments, the average partic-ipant spent more time to make errorful judgments than to make correct judgments. Thus, in relevance assessing sce-narios similar to our user study, it may be possible to use the time taken to judge a document as an indicator of assessor error. Such an indicator could be used to identify documents that are candidates for adjudication or reassessment. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval Keywords: Relevance judging, assessor error detection
We have two motivations for wanting to better understand assessor behavior and assessor error. First, knowledge of as-sessor behavior should help IR researchers in the construc-tion of more accurate relevance assessing systems. Second, while traditional retrieval metrics are relatively insensitive to assessor error [6], newer metrics that sample documents for judging, can be dramatically affected by assessor error [1, 7]. A single sampled document can represent thousands of documents in the final estimation, and a single document falsely judged to be relevant can greatly inflate the esti-mated number of relevant documents. Being able to predict which documents might have incorrect judgments could be useful for improving assessor consistency by asking assessors to judge certain documents a second time [1].

We have previously reported on relevance judging errors made by the 48 participants who took part in our user study [5], but we have not previously reported on what be-haviors correlate with errors.

In this paper, we present results for a relevance assessing scenario where there was not an incentive for users to work faster. In this scenario, where the assessor is not paid per judgment, but is paid for the time spent, we have found an interesting result: users take more time to make mistakes than they do to make correct relevance judgments.
The user study presented participants with fixed ranked lists of documents. Given a ranked list, participants were instructed to search for and save relevant documents. The user interface was web-based and consisted of two web pages. The first page showed 10 document summaries in a format similar to a web search engine X  X  results page. Clicking on a summary took the participant to a page that displayed the full document and allowed the participant to save the docu-ment as relevant. We did not require relevance judgments to be made. We treat viewing a full a document and not sav-ing it as a decision to judge the document as non-relevant. Participants could view the next 10 results by clicking on a link at the bottom of the document summaries page.
The study used 8 topics from the 2005 TREC Robust track. The documents came from the AQUAINT newswire collection. Each participant searched ranked lists with a uniform precision of 0.6 or 0.3. We use the NIST relevance judgments as truth.

Participants worked on 4 search topics for 10 minutes each. The study used a fully balanced design. Participants were instructed to work as fast as possible while making as few mistakes as possible. We use data from the second phase of the study. All participants had already completed phase 1 of the study and gained experience with relevance judging before participating in the second phase. The result lists contained near-duplicate documents. We use only the first judgment on a set of near-duplicates.

We measured the time the participants spent on the full document page before saving the document as relevant or leaving the page. It is well-known that large variations in behavior are caused by the search topic and the user. To allow comparisons of times across users and topics, we stan-dardized a participant X  X  times on a topic to have a mean of zero and a standard deviation of 1.

We average a participant X  X  times on a topic, and then average across topics for that participant to produce a par-ticipant average. We finally average across participants to produce average participant times.
When a participant judges a non-relevant document to be relevant, a false positive error is made. Likewise, when a relevant document is judged to be non-relevant, a false neg-ative error is made. Table 1 and Figure 1 show the main results. These results are based on 3614 judgments (2706 correct judgments, 511 false negative errors, 397 false posi-tive errors). 0.01 0.26  X  0.07 &lt; 0.001 0.13  X  0.08 0.02 2 39  X  4 0.03 33  X  3 0.16 errors took a statistically significant longer time (p &lt; 0.05).
 Figure 1: Average participant standardized times to judge document relevance for false positive, correct, and false negative judgments.

We found that on average, participants took significantly longer to judge documents when they made an error com-pared to when they were correct in their judgment. A priori, one might guess that participants would make errors by not taking enough time, but in contrast, we find here that time appears to indicate difficulty in making a correct judgment.
We hypothesize that the study participants make judg-ment errors according to the following process. First the participants identify a topically relevant document based on the document X  X  summary and then click on the summary to view the full document. Participants then study the docu-ment to determine if it is relevant or not to the specifics of the search topic. The more difficult it is to determine the document X  X  relevance, the longer the participant takes. At some point, the participant decides to save the document as relevant or abandon the document without saving it. False negatives result from not finding the relevant material in the document, and false positives are the result of the final decision being a guess.

There are many other factors that can cause a user to take longer to judge a document. For example, we have shown that the longer a document, the longer it takes to judge its relevance [3]. Is it just that mistakes are being made on longer documents? To investigate this, we mod-eled the probability that a judgment was an error using lo-gistic regression. For the model, we considered time, doc-ument length measured in words, and the rank of the doc-ument in the results list. We found that when both time and document length were used together in the model, doc-ument length was not a useful predictor of errors. On the other hand, when we included the interaction between time and document length, this interaction was helpful. In other words, if the time to judge is too long given the length of the document, this is an indicator of error. We also found that ranks above about 50 were predictive of making errors. Reaching a rank over 50 in 10 minutes is likely indicative of a participant who is not carefully judging documents. For ranks less than 50, rank was not predictive of errors.
The relevance assessing setup may have a significant effect on assessor behavior. In preliminary analyses of separate studies where assessors had an incentive to work faster [4] and relevant documents were highly prevalent (90% rele-vant) [2], we have seen false negatives be longerrors and false positives be shorterrors . In this paper, both types of errors were long errors. In future work we intend to analyze these studies in more detail by using this paper X  X  method of standardizing times.
We found in a relevance assessing scenario where the as-sessor is trusted and paid by time spent, and not by number of judgments made, that all things equal, the longer an as-sessor takes to make a relevance judging decision, the more likely a mistake will be made. We hypothesize that time to judge a document relative to other documents, gives an indication of the difficulty of judging the document.
David Hu wrote the software to compute the sets of near-duplicate documents. This work was supported in part by NSERC, in part by Amazon, and in part by the University of Waterloo.
