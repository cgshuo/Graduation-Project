 Abstract The experience of a user of major search engines or other web infor-mation retrieval services looking for information in the Basque language is far from satisfactory: they only return pages with exact matches but no inflections (necessary for an agglutinative language like Basque), many results in other languages (no search engine gives the option to restrict its results to Basque), etc. This paper proposes using morphological query expansion and language-filtering words in combination with the APIs of search engines as a very cost-effective solution to build appropriate web search services for Basque. The implementation details of the methodology (choosing the most appropriate language-filtering words, the number of them, the most frequent inflections for the morphological query expansion, etc.) have been specified by corpora-based studies. The improvements produced have been measured in terms of precision and recall both over corpora and real web searches. Morphological query expansion can improve recall up to 47 % and lan-guage-filtering words can raise precision from 15 % to around 90 %, although with a loss in recall of about 30 X 35 %. The proposed methodology has already been successfully used in the Basque search service Elebila ( http://www.elebila.eu ) and the web-as-corpus tool CorpEus ( http://www.corpeus.org ), and the approach could be applied to other morphologically rich or under-resourced languages as well. Keywords Search engines Web-as-corpus Basque NLP Morphological query expansion Language-filtering words 1 Motivation 1.1 Problems when searching for Basque The World Wide Web has become essential in many people X  X  everyday lives, and owing to its vastness search engines have become an indispensable tool for accessing and looking for information.

However, when using search engines to look for information in Basque, some problems arise. One of the most noticeable is that Basque is an agglutinative language. The problems that non-English languages, and agglutinative languages in particular, have with search engines have been widely addressed (Bar-Ilan and Gutman 2005 ; Lazarinis 2007 ; Lazarinis et al. 2007 ; Efthimiadis et al. 2009 ).
In the case of Basque, a given lemma produces many different surface forms, depending on the case (genitive, locative, etc.) or the number (singular, plural, etc.) for nouns and adjectives, and the person (me, he, etc.) and the time (present, past, future, etc.) for verbs. A brief morphological description of Basque can be found in Alegria et al. ( 1996 ). Quoting Wikipedia, Basque,  X  X  X s an extremely inflected language, heavily inflecting both nouns and verbs. A Basque noun is inflected in 17 different ways for case, multiplied by 4 ways for its definiteness and number. These first 68 forms are further modified based on other parts of the sentence, which in turn are inflected for the noun again. It is estimated that at two levels of recursion, a Basque noun may have 458,683 inflected forms. Verb forms are similarly complex, agreeing with the subject, the direct object and several other arguments X  X . 1 For example, the lemma lan ( X  X  X ork X  X ) forms the inflections lana ( X  X  X he work X  X ), lanak ( X  X  X orks X  X  or  X  X  X he works X  X ), lanari ( X  X  X o the work X  X ), lanei ( X  X  X o the works X  X ), lanaren ( X  X  X f the work X  X ), lanen ( X  X  X f the works X  X ), etc. This means that looking for the exact given word alone or applying some simple stemming rules of other languages (such as appending an  X  X  s  X  X  for the plural, which is what major search engines do) is not sufficient for Basque. Neither is the use of wildcards allowed by some search engines an appropriate solution, as it can return occurrences not only of conjugations or inflections of the word, but also of derivatives, unrelated words, etc. For example, looking for lan* would also return all the forms of the words lanabes ( X  X  X ool X  X ), lanbide ( X  X  X ob X  X ), lanbro ( X  X  X og X  X ), and many more.
Another major obstacle when web searching in Basque is that none of the existing search services can distinguish Basque pages in their searches. Searching in any of them for a technical word that also exists in other languages ( anorexia , sulfuroso , byte or allegro , to cite just a few examples of the many that exist) or a proper noun or a short word, will not only not yield results exclusively in Basque, but often not yield any results in Basque at all. And local (Spanish) versions of search engines do not perform better: at best, a few results in Basque might appear lost among the results in Spanish, when using the Basque UI. 1.2 Need for a web-as-corpus tool For Basque as for any other language, it is important to have corpora publicly available and searchable through the Internet. One factor that must be taken into account is that the standardization of Basque did not start until the late sixties of the last century, and that many rules, words and spellings have been changing since. Furthermore, Basque was not taught in schools until the seventies and did not become a medium of instruction at universities until around the eighties. All this has led to a scenario in which even written production abounds with misspellings, corrections, uncertainties, different versions of a word, etc. But, above all, the main problem is that there are many areas or words upon which a decision as to the correct word or spelling has not yet been taken. So writers, technical text producers, dictionary makers, translators and even academics in the field of standardization need corpora in order to avail themselves of the data upon which to base their decisions.

However, the Basque language does not have a wealth of corpora. The number and size of corpora are in direct proportion to the number of speakers and the economic resources of the language. These are the only Basque corpora that are currently available to the public:  X  Orotariko Euskal Hiztegiaren Testu -Corpusa , a non-tagged 6 million-word  X  XX. mendeko Euskararen Corpusa , 2 a 4.6 million-word balanced corpus (in  X  Ereduzko prosa gaur , 3 a 25.1 million-word corpus compiled by the University of  X  Zientzia eta teknologiaren corpusa , 4 an 8.6 million-word corpus compiled by the  X  Klasikoen gordailua , 5 a non-tagged 10.7 million-word corpus compiled by the  X  Lexikoaren Behatokia , 6 an 18.1 million-word corpus produced by the Academy
As can be seen, there are very few Basque corpora, and they are small compared with those of major languages. It is also apparent that few of them are being updated with recent texts, so their usefulness for detecting the most recently incorporated words, terms and neologisms is severely limited.

However, we do have at our disposal a huge repository of text that is constantly being updated, and this is the Internet, which contains many more texts in Basque than the other four corpora put together. Having a web-as-corpus tool, in other words, being able to query the Internet as if it were a large Basque corpus, would be a very valuable resource.

We are aware that this use poses certain major disadvantages, the main ones being the following:  X  Such systems will always have some uncertainty due to the fact that the Internet  X  They will never be able to show all the existing information, only what appears  X  The web is not as balanced as an ideal corpus should be, that is, not every genre,  X  There is a lot of redundancy in the web.

Despite these well-known problems posed by such systems, we felt that the benefits a tool like this would have for Basque far exceeded the disadvantages. Yet these tools make use of search engines, so when using them for Basque we encounter the same problems that we mentioned in the previous subsection. 2 Proposed approach and related work Many works have proposed setting up language-specific search engines that will only include pages that are in the target language and which would not index the word forms that a page contains, but its lemmas (Benczu  X  r et al. 2003 ; Langer 2001 ). Since Basque language-detection and lemmatizing were implemented long ago (Aduriz et al. 1996 ), such a solution would be technically possible (using Apache X  X  Lucene 7 for example), but it is beyond our possibilities and objectives to implement and maintain the whole infrastructure that a search engine and its crawling, indexing and serving involves (bandwidth, disk, reliability, etc.).
Instead, we leave the crawling, indexing and ranking to the major search engines, and focus on making use of them via their Application Interfaces or APIs, while applying various NLP techniques that will improve performance significantly, namely morphological query expansion and language-filtering words. These techniques are described in more detail in the following subsections. Although this approach has its drawbacks (principally, that it is very dependent on the way search engines function: their internals are obscure, the user is limited to the options offered by the API, etc.), we believe the advantages exceed the limitations.
Regarding web-as-corpus tools, similar services have already been implemented and are available for public use. Some examples are WebConc, 8 WebCorp 9 (Kehoe and Renouf 2002 ) or KWiCFinder 10 (Fletcher 2006 ). But in view of the fact that they too rely on search engines, they do not work well for Basque words. So the approach mentioned above can be used to build such a tool for Basque too. 2.1 Morphological query expansion In order to obtain a lemma-based search, when the API of a search engine is requested to supply a word, we need it to return pages that contain its conjugations or inflections, too. The way we have approached this matter is based on morphological query expansion.
The importance and use of morphology for various IR tasks has been widely documented (Ambroziak and Woods 1998 ; Krovetz 1993 ; Woods 2000 ; Woods et al. 2000 ). But morphological variation processing is usually approached by lemmatisation or stemming at the indexing stage (this is the case of the papers mentioned above), since it has been proved to be very effective. This is also the method used so far in Basque IR to deal with the agglutinative nature of the Basque language (it is the preferred method in the search boxes of Basque websites). Instead, since our intention is to use major search engines that do not apply Basque lemmatisation or stemming at the indexing stage, our approach involves the applying of morphological generation at the querying stage.

Some works do propose using query expansion for dealing with morphology (Xu and Croft 1998 ; Moreau et al. 2007 ). However, they rely on corpora and statistical co-occurrence methods or machine learning to find the morphological derivations of the words in the query. These techniques are mostly language independent, but they can expand the query not only with inflections or conjugations of the words but also with compounds, other kinds of derivatives and sometimes unrelated words even. Besides, they have not been evaluated with a highly inflectional language: the more derivations we try to get using these methods, the greater the probability of getting wrong words. By using a morphological generator based on lexica and rules, we always obtain correct inflections and conjugations. Stankovic  X  ( 2008 ) does in fact use a rule-based morphological generator for expanding queries in a highly inflectional language, but focuses on inflecting compounds and phrases correctly. The approach most similar to ours is that used by Kettunen et al. ( 2007 ) for Finnish and other morphologically rich languages, which they call FCG or Frequent Case Generation; they also use corpus-based studies to obtain the most frequent cases of each kind of word and then use morphological generators to produce the forms of these cases for the searched words.
 Specifically, we use a tool created by the IXA Group of the University of the Basque Country and which gives us all the possible inflections or conjugations of the lemma, and the search engine is asked to look for any of them by using an OR operator. For example, if the user asks for etxe ( X  X  X ouse X  X ), the search engine is asked for  X  X (etxe OR etxea OR etxeak OR etxeari OR etxeek OR etxearen OR ... ) X  X . But the APIs of search engines have their limitations with regard to search term count, length of search phrase, etc. These limitations render a proper lemmatised search for Basque impossible, as searching for all the conjugations or inflections is not feasible. So the most frequent ones are sent, and this will cover a high enough percentage of all the occurrences, as Kettunen et al. ( 2007 ) have shown.
Many previous works dealing with the query expansion problem have shown the importance of weighting the expanded words and the query original terms differently. Although this has been generally applied in order to give different degrees of importance to the original search term and its synonyms, it might also be interesting to weigh frequencies. By using the OR operator, we are giving all query terms equal weight and thus this potential benefit is lost, but we have no other choice: we are using APIs of web search engines, and they do not provide the possibility of weighting the search terms. 2.2 Language-filtering words The search engine result filtering in a given language is a well known problem in IR. There are many tools and techniques for language classifying of texts: N-gram based, trigram frequencies based, Markov models based, etc. (Padro  X  and Padro  X  2004 ). The best known among these tools is probably TextCat (Cavnar and Trenkle 1994 ). However, the one that offers the best results for Basque is LangId, a free language identifier based on word and trigram frequencies developed by the IXA group of the University of the Basque Country, and which is specialized in recognizing Basque and its surrounding languages (Spanish, French and English).
So the most obvious and straightforward approach for showing results that are exclusively in Basque would be to filter the results returned by the API by applying LangId to the snippets, since this is the method most used in the literature (Osinski et al. 2004 ; Ghani et al. 2003 ). But this does not work well in many cases: as we have already stated, searching for technical words that also exist in other languages, proper nouns or short words will often yield very few results in Basque, if any at all, so this subsequent filtering would leave almost no results.

In order to obtain results in Basque alone from the APIs, we have used an approach that we call  X  X anguage-filtering words X ; this consists of adding to the search phrase, in conjunction with an AND operator, some Basque words to act as language filters. The features these words need to share are as follows: (1) they should be very frequent, so that practically any document in the Basque language will contain them, and (2) they should be specifically Basque, so that no documents in other languages will contain them. Unfortunately, the most frequent words in Basque are short and, as such, the chances of their presence in other languages or being used as abbreviations or acronyms is quite high. In fact, the two most frequent words at least have well-known meanings in other languages. Therefore, several of these language-filtering words need to be included in the queries in order to obtain a high percentage of Basque results, although this also involves a loss in recall (some Basque pages may not be returned because they do not contain one or more of the words).

The words to be used as language-filtering words are what can be considered as stopwords (very frequent words present in almost any page that are not representative of the textual content and which are therefore discarded by search engines when indexing). However, as we have already stated, there are no Basque-aware search engines, so these words are not included in their stopwords list. 3 Implementation details and quantitative evaluation Considering the inexistence of proper search services for Basque, a first version of a search engine for Basque, Elebila, and a web-as-corpus tool for Basque, CorpEus, both of which made use of morphological query expansion and language-filtering words, were launched in autumn 2007. The implementation details (cases for the morphological expansion, which and how many language-filtering words) were chosen based on classical corpora and quite intuitively, without making any measurements of the improvement obtained.

But in order to obtain optimum performance, it is important to fine-tune certain details of the morphological query expansion and language-filtering words methodology as much as possible. The choice of how many and which language-filtering words to use, and expanding the query with the most frequent inflections of the words, are crucial for the effectiveness of our approach. For a second version of Elebila and CorpEus, these choices have been made on the basis of precision and recall studies over different corpora. Incidentally, these studies have also produced quantitative measurements of the level of improvement offered by these services. 3.1 Design of the study As stated above, the study described in this section consists of various corpus-based measurements. One of the corpora used for carrying it out is the ZT Corpus (Areta et al. 2007 ), a lemmatised Basque corpus on science and technology made up of 8.6 million words. Since the typology of the documents that form a classical corpus and those that form the WWW might differ (Sharoff 2006 ), we considered it advisable to use not only a classical corpus, but also a web corpus. So a web corpus was compiled by crawling the Basque branch of the Google Directory. 11 We downloaded the 3,000 plus pages present there and recursively followed all the links found in the pages that LangId identified to be in the Basque language. The downloading process was designed to ensure as much website variety as possible and used a breadth-first approach, by queuing the links found, prioritising different domains in each parallel downloading stage, etc. The web corpus obtained is made up of over 44,000 documents and approximately 20 million words.

The various measurements using these corpora had to be done by employing many different queries. We are aware that there exist quasi-standard query collections to evaluate IR systems, such as the TREC test questions, but we opted to use queries that people doing Basque searches really use, so we took the most frequent searches from the search logs of Elebila. This way, by optimising the tools with the results of the study, we would be maximising their performance for real-life searches. The Elebila logs we used accounted for over 400,000 searches involving over 800,000 words, which after lemmatisation made over 70,000 different words. The lemmatised queries were subsequently ordered according to decreasing frequency and the topmost ones were used for our work. All these most frequent queries are one word long, which suits our experiments well. Examples of some these queries are berri ( X  X  X ew X  X ), didaktiko ( X  X  X idactic X  X ), eoliko ( X  X  X olic X  X ), hiztegi ( X  X  X ictionary X  X ), musika ( X  X  X usic X  X ), energia ( X  X  X nergy X  X ), ikasi ( X  X  X earn X  X ), Galileo ( X  X  X alileo X  X ), Mozart ( X  X  X ozart X  X ), Egipto ( X  X  X gypt X  X ) and Bilbo ( X  X  X ilbao X  X ). 3.2 Language-filtering words 3.2.1 Choosing the words For choosing the language-filtering words, the first step was to see which the most frequent words in Basque were. Table 1 shows the 16 most frequent words of each corpora with the document-frequency of each of them.

The 16 most frequent words in both corpora are the same, but their order is different. In view of this, we chose the candidates to act as language-filtering words from the first list, as this corpus is supposedly more similar to the one to which we will apply our tools, that is, the Internet. So the candidates will be the topmost six words from the web corpus list: eta , da , ez , du , bat and ere . After that, precision and recall studies were performed on different combinations of these six candidates.
If one looks at the document-frequencies of the candidate words, it is clear which words would have been chosen if the filter had consisted of one or two words, since there are significant gaps between the frequencies of the first three words in both corpora. Choosing which should be the third and fourth words is more difficult, because the next words have quite similar document-frequencies. For these ones we can even consider OR combinations. So the combinations for which the precision and recall will be analysed in the following subsections are shown in Table 2 . 3.2.2 Loss in recall To measure the loss in recall produced by the language-filtering words, their document-frequency in the classical corpus and the web corpus were measured. The decrease in hit counts obtained by searching the web using the API of Microsoft Live Search was also measured. This was done only for words that only exist in Basque; otherwise, occurrences of the words in other languages could have distorted the results.

We are aware that hit counts are known to be an unreliable source of information (Uyar 2009 ) and that it would be better to at least average the hit counts from all the major search engines. But the studies performed in this paper involved making several thousand queries to the APIs; and using APIs other than Microsoft X  X , due to the limitations they impose on the number of queries per day, would have meant several weeks X  or even months X  work.
 The results are shown in Fig. 1 .

From the graph we can see that there is a remarkable similarity between the web corpus and hit counts series, confirming that the corpus that was crawled from the web is a good sample for predicting the behaviour of the web. Furthermore, we can observe that the recall in the ZT Corpus is significantly greater, most likely due to the fact that the type of documents of which this corpus is composed of (books and articles on science and technology) is, on average, greater in size than most web pages, which confirms our previous supposition that it was better to base our study on a corpus collected from the web.

The loss in recall from one to two filter words is significant. Also, in the groups of three or four filter words, there is a gap when passing from the combinations with an OR to those without it. The graph shows that including three or four filter words without an OR reduces recall to half, which is a significant reduction, so one or two filter words would be best if a language-precision that is sufficiently large is achieved. 3.2.3 Gain in precision The addition of more of the language-filtering words to the query leads to a gain in language-precision. For quantifying this gain the ideal procedure would be, as before, to measure it on the corpora, but this is not possible, since we would need a multilingual corpus that would have the same proportion of each language as the web does, which is very difficult, if not impossible, to obtain. So we had no other option but to measure the gain in precision by searching the web through Microsoft X  X  API and looking at the percentage of results in the Basque language. To classify the results into Basque or non-Basque we used LangId again, by applying it to the snippets returned. LangId is specialized in Basque detection and obtains an accuracy of practically 100 %, so it works very well even with such short texts.
We mentioned above that the performance of the language-filtering words method is most noticeable when the search term exists in other languages, or when it is short, or when it is a proper noun. If the word only exists in Basque, the language-filtering words might bring little benefit or even none at all. So the gain in precision was measured separately for different categories of words (the words were classified into their categories by language specialists):  X  Short words . Words with 5 characters or less. The probability of their existing in  X  Proper nouns . Proper nouns are usually the same in other languages. Some of  X  International words . Words that we know definitely exist in another language  X  Words that are likely to be found in other languages . Technical words which,  X  Basque words . Words that we are almost sure do not exist in any other language.
For the overall measure of the categories, a weighted average of them was made by taking into account the frequency of use of each category. To calculate these frequencies, we classified approximately the first 900 words (all that have a query frequency over 100) out of the more than 70,000 of the Elebila logs into one of the categories. This may not seem very much, but they do in fact account for more than 40 % of the queries. The percentage of words and queries of each category is shown in Table 3 .

The gain in precision produced by the language-filtering words for each category of word and overall is shown in Fig. 2 .

The peaks and valleys of the graph provide us with hints as to the filtering properties of the last four words ( ez , du , bat and ere ). All the valleys are combinations containing du and the highest peaks contain the word ere , so these two are, respectively, the worst and best words of the four for filtering. Between ez and bat there is not a big difference, although ez seems to behave a little better. These conclusions are logical: du is a word that is present in almost any text in a big language like French; bat is a word that, although not very frequent, exists in the language with the highest presence on the web, that is, English; and, as far as we know, ez and ere are not widely used words in at least three major languages, such as English, Spanish and French, but ere is longer and hence yields better results.
The graph also shows that the average language-precision obtained without any language-filtering words is around 15 %. This means that if we did not use language-filtering words and then filtered the results with a language classifier, we would get far fewer results. 3.2.4 Choosing the number of language-filtering words In Fig. 3 we put together the precision, recall and F-measure of the different language-filtering word combinations.

The conclusions we can draw from it are that by using 4-word combinations we can achieve very good precision (even high above 90 %), but with fairly bad recall (near to or below 50 %). So it might be more advisable to use 3-word combinations that do not include the word du , like eta AND da AND ( ez OR bat OR ere ), eta AND da AND ( ez OR ere )or eta AND da AND ( ez OR bat ), with which we can achieve a precision of 86 X 87 % and a recall of 68 X 65 %. In fact, these are the combinations with the highest F-measure. But we must take into account that for proper nouns or international words the precision would fall to around 70 %.
The most appropriate step might be to keep a list of the most searched proper nouns and international words, and when someone wishes to search for one of them, use 4-word combinations, and otherwise use 3-word ones. Or we could also prioritise precision and normally use 4 words, and if the user is not happy with the searching again by increasing the recall (using 3 words). This last option is the one chosen to implement our system. 3.3 Morphological query expansion 3.3.1 Most frequent inflections In order to maximise the performance of morphological query expansion, it is important for the inflections used to be the most frequent ones. We must take into account that search engines allow, in the worst case, up to only 18 words in the queries; to this limitation we have to subtract three or four for the language-filtering words; so in some cases we can only send 14 morphologically generated words; and if the user has requested more than one word, we have to divide the inflections by the number of words requested.

So we looked for the most frequent inflections in both of the aforementioned corpora. We took the most searched-for words of the Elebila logs and classified them into the five morphologically productive POS in Basque: nouns, proper nouns, place names, adjectives and verbs (strictly speaking, place names are not a POS, but they are inflected differently from other proper nouns). Because of the non-tagged nature of the web corpus, the words chosen had to be non-ambiguous regarding their POS. Then we looked at the document-frequency that every different surface form of the words had in both corpora, and we assigned its inflection name to each of them. By grouping them by inflection name and ordering them by decreasing frequency, we produced a list of the most frequent inflections for each POS, both in the classical corpus and the web corpus. The lists of each corpus, although similar, reveal some differences between them. Since they were to be applied in a web search application, we chose the web corpus lists. The most frequent inflections of each POS are shown in Table 4 . 3.3.2 Gain in recall Once the most frequent inflections of each POS were known, we measured the increase in recall we would obtain for each POS by including 1, 2, 3 ... of the inflections in an OR. We have performed this measurement using the same words as before. Again, both of the aforementioned corpora were used, and we also looked at the increase in hit counts returned by Microsoft X  X  Live Search API.

In a couple of cases, there were inflections of a word that formed a word that also had another completely different sense. When this happened, the recall would go up abruptly and form peaks. These exceptional cases were removed and not taken into account for the measurement.

For the overall measure, we made a weighted average according to the frequency of use of each POS, calculated again by classifying the first 900 most searched words in the Elebila logs. The percentage of words and queries of each POS is shown in Table 5 . The increase in recall obtained over the baseline for each corpus can be seen in Fig. 4 .

Attention should once again be drawn to the high level of coincidence between the web corpus and hit counts series.

With as few as 4 inflections, an increase in recall of about 35 % can be obtained, and with more inflections we can even reach an increase of 47 %. The recall obtained without applying morphological query expansion is only two thirds of what can be achieved by applying it. Thus the validity of the morphological query expansion method can be considered proven.

There is no decision to be taken as to the number of inflections that will be sent in an OR; as many as possible will be included, since there is no drawback in doing so. In the query, the word form entered by the user is sent first, and then the inflections sorted by decreasing case-frequency; nevertheless, the order does not seem to affect the results.
 The gain shown in the chart is the weighted average of the gains obtained by each POS; the individual gains for the web corpus are shown in Fig. 5 .

The differences between the various POS are obvious: some of them, namely verbs, adjectives and place names, really benefit from the query expansion while the others (nouns and proper nouns) do so to a lesser extent. The reason for this is that in these POS the base form is more frequently used than in the others, and so the baseline (the recall obtained by querying for the base form) is already higher, thus leaving less room for improvement.

Note on Fig. 5 : By looking at the Elebila logs, we have noted that for verbs, adjectives and nouns, more than one form of the word is used indistinctively when searching for the word, so the leftmost column shows an average of the recall of those inflections usually used, whereas place names and proper nouns are almost exclusively searched for using the nominative form, which is also the most frequent inflection, which explains the non-existent improvement from the baseline or leftmost column to the next for proper nouns and place names. 4 Additional problems and solutions 4.1 Subsequent language-filtering Although the language-filtering words method ensures high language-precision, a non-negligible number of pages that are not in Basque are still returned by the API (see Fig. 3 ), and a search service should filter out these results before showing them to the user. To achieve this, LangId is used once again, applied to the snippet returned by the search engine. This way, a language-precision of practically 100 % is obtained, but at the cost of returning fewer results than requested. So the performance of the language-filtering words method is very important, because the lower the precision achieved by it, the fewer the results that will be shown.
In a web-as-corpus tool, the problem is different. It is not the whole page that we want to leave or filter out, but each occurrence of the search term. With the language-filtering words method, we ensure that almost all of the pages downloaded will have Basque in them, but not that they will be exclusively in Basque. There are many bilingual pages on the web and, due to the Basque language being co-official with Spanish in the Basque Autonomous Community and in some parts of the Charter Community of Navarre, there are a great many web pages and documents in both Spanish and Basque, e.g. many local and regional government gazettes. So bilingual pages in which the search term can be in a non-Basque part are returned at times, and we need to show only the contexts of the words that are in a piece of text in Basque.

To solve this, we apply LangId to some context around each occurrence of the search term. Choosing the right length of the context was no small matter: if it was too short, the language identifier would not have enough data to decide the right language correctly; if too long, bits of text in other languages could be included. By performing some experiments we found that the best result was obtained if we tried initially with a fairly broad context; then, if LangId said that the text was not Basque, which would normally be due to parts in other languages being included, more attempts were made by reducing its length progressively until a minimal length was reached; the occurrence would be included in the result if any of the attempts said that the language was Basque. 4.2 Improving navigational and transactional searches Throughout the whole article a methodology has been described for building a web search service designed and optimized for finding textual content in Basque and, as such, it would perform best for informational queries, that is, queries aimed at obtaining information about something. But according to Broder ( 2002 ), informa-tional queries account for only 39 X 48 % of all the web queries. He also introduced the concepts of navigational queries (where the intention is to reach a particular site that the user has in mind, either because they visited it in the past or because they assume that such a site exists) and transactional queries (the purpose of which is to perform some web-mediated activity, such as shopping, downloading, accessing some data-base, etc.), and estimated that they represented more than half of the web queries. Although some other works estimate them to be less than 40 % (Rose and Levinson 2004 ), and although the language-oriented nature of the search service we are trying to build results in its most typical use being for informational queries (as a look at Elebila X  X  logs confirms), a non-negligible number of transactional and navigational queries would most likely still be made to it.

But such a search service would not work so well for these kinds of queries, because of its use of language-filtering words. The inclusion of these words in the queries causes a loss in recall, as we have already shown. Many pages are left out because they do not contain one or more of the filter words, and these are mostly short pages that do not have much textual content. The pages that are the objective of navigational queries (homepages of companies or organizations) or transactional queries (entry pages of online dictionaries, social multimedia repositories such as Flickr or YouTube, online shops, etc.) are often not rich in textual content. Besides, a user might use the Basque search service to find the homepage of a Basque company whose web page is only in Spanish, French and/or English but not in Basque, so our service would not find it.

Nevertheless, major search engines work quite well for Basque navigational and transactional queries. It is mainly informational queries that they do not handle well and this is what we are trying to improve. But when looking for the address of the main page of a company or some other site, even if it is a page in Basque, the classical ranking measures (link analysis, click-through data, having the search terms in the title or URL, etc.) usually work well, returning the desired page among the first results.

We take advantage of this fact in order to improve transactional and navigational queries. Apart from the morphologically expanded and language-filtered query, the API is also asked for the raw search terms the user entered, and the first five results are looked at to see if there are any in which the title or the URL matches the search other results.

A web-as-corpus tool would not have to deal with this problem. The language-filtering words method prioritizes pages rich in textual content, and these are also the most interesting ones from a corpus point of view, since they will usually contain more occurrences of the search terms. Moreover, when many words within an OR operator are included in a query (which is what we do for the morphological query expansion), pages containing as many as possible of them seem to be promoted, and this is perfect for the intended use of a corpus tool. 4.3 Variant suggestion We have already mentioned that the late start of Basque standardization and the only very recent introduction of the Basque language into the educational process have been responsible for the fact that written production, the Internet included, is deprecated or incorrect spelling of a word in our Basque search service and have to cope with the results without knowing that there are many more results for the correct form. Or in our web-as-corpus service, one could ask for an incorrect word and find enough evidence to consider it correct.

In the case of the web-as-corpus tool there is another problem caused by the fact that the web is not linguistically tagged. In linguistically tagged and manually disambiguated corpora, different variants of a word (old spellings, common errors, etc.) or even typing errors have their correct lemma assigned, so searching for a certain lemma would also return occurrences of the variants, but not in our tool.
We solve these problems by means of variant suggestion. Expanding the query using variants of the search term to improve the results has been suggested in the literature, either by automatic expansion (Jones and Tait 1984 ) or interactive suggestion (Belkin 2000 ). The expansion is usually done with synonyms obtained from a thesaurus or related words extracted by statistical measures over corpora, relevance feedback, etc. In our case, the query is not automatically expanded with variants; the user is informed about the existence of the variants and given the option of looking for them with a simple click. And the variants we suggest are aimed at solving the problems mentioned above: known variants, common errors, deprecated forms and old spellings. This implementation makes use of EDBL, a lexical database developed by the IXA Group of the University of the Basque Country and used by all the linguistic tools made for Basque (Aduriz et al. 1998 ). This database links each word to its linguistic variants (common errors, old spellings, deprecated words, etc.). So if the terms entered by the user have some variant or correct form in EDBL, they are suggested and can be looked for in a click. If, for example, we are interested in the collocations or terms in which the noun jarduera ( X  X  X ctivity X  X ) is the head, the system offers the possibility of also retrieving the occurrences of iharduera , a now deprecated spelling widely used until 1998, and vice versa. 5 Services and tools created 5.1 Elebila The methodology explained throughout this article has been implemented in the search service Elebila, launched in October 2007 (Leturia et al. 2007b ). It is an API-based search service, so it is easy and cheap to implement.

Elebila uses morphological query expansion for obtaining a lemma-based search but, optionally, only the exact form entered can be looked for. The user can enter more than one search term, and the lemma-based search is performed for all of them. Likewise, it can perform an exact phrase search by enclosing the search terms in double quotes, but it applies the morphological generation to the last word of the phrase, thus performing a proper lemma-based search for whole noun phrases or terms, since in Basque only the last component of a noun phrase or term is inflected.
Elebila makes use of language-filtering words for obtaining results in Basque alone. The user can also choose to look for known variants (common errors, archaic forms, etc.) of the word. 5.2 CorpEus CorpEus is a tool that makes use of the methodology explained in this article and that allows the Internet to be consulted as if it were a Basque corpus (Leturia et al. 2007a ). It also makes use of the APIs of search engines to perform a web search.
To obtain the best results for Basque, it uses the same methodology as Elebila: it obtains results in Basque only by means of language-filtering words, it performs a lemma-based search using morphological query expansion, it suggests variants of words, more than one search term can be entered, and it offers the possibility of performing an exact phrase search by enclosing the search terms in double quotes.
Once the search engine has returned its results, each of the returned pages is downloaded. For the downloading, different processes are launched concurrently and the contexts are served in the order the pages arrive, so that a slow or blocked page does not stop the complete process.

Each occurrence of the search terms in the pages is only shown if LangId, applied to some context around it, says it is in a piece of text in Basque. The KWICs can be ordered according to different criteria. In the KWICs, each form of the searched word shows its possible lemma and POS analysis in a floating box that appears if the mouse is moved over it. CorpEus can also show different charts with counts of word forms, possible lemma or POS, word before, word after, etc. 6 Conclusions In this paper we have shown that applying the combination of some NLP techniques (morphological query expansion and language-filtering words, along with some other small improvements and tweaks) to the APIs of search engines is a valid method for building cost-effective search services and web-as-corpus tools for morphologically rich and/or minority languages that will significantly improve the performance of major search engines. This has been proven both theoretically (by performing corpora-based precision and recall measurements for Basque) and practically (by building and successfully launching the Basque search service Elebila and the web-as-corpus tool CorpEus).

Moreover, we are of the opinion that the steps followed here for specifying the implementation details of the methodology and for measuring the improvements obtained with it could be very valuable for developing similar tools for other languages with similar features and problems, of which there are several. Currently, major search engines cover only about forty languages (the most widely spoken ones) appropriately, while the tools needed for implementing the methodology described in the paper (N-gram based language detection tools and lexical processing tools) exist for many others, even regional and minority languages. References
