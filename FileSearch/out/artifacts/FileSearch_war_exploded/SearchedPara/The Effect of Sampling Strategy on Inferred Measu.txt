 Using the inferred measures framework is a popular choice for constructing test collections when the target document set is too large for pooling to be a viable option. Within the framework, different amounts of assessing effort is placed on different regions of the ranked lists as defined by a sam-pling strategy. The sampling strategy is critically important to the quality of the resultant collection, but there is little published guidance as to the important factors. This paper addresses this gap by examining the effect on collection qual-ity of different sampling strategies within the inferred mea-sures framework. The quality of a collection is measured by how accurately it distinguishes the set of significantly differ-ent system pairs. Top-K pooling is competitive, though not the best strategy because it cannot distinguish topics with large relevant set sizes. Incorporating a deep, very sparsely sampled stratum is a poor choice. Strategies that include a top-10 pool create better collections than those that do not, as well as allow Precision(10) scores to be directly com-puted.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  X erformance Evaluation Test collections; Incomplete judgments; Sampling
Test collections drive much of the research in information retrieval. Obtaining human judgments regarding document relevance is the most expensive direct cost of building a test collection, so recent research has focused on developing eval-uation mechanisms to support fair comparison of retrieval results using relatively small amounts of judging effort. One such mechanism is the judgment-set-building process that supports the computation of extended inferred estimates of traditional evaluation measures using many fewer judgments than would be required to reliably compute the traditional scores directly [4, 5]. This framework has proved popular because it is convenient to use in practice and supports a variety of evaluation measures.

Empirical evaluation of the inferred measures shows the estimates can have high fidelity to their directly-computed counterparts, but the quality of the estimates depends on the sampling strategy used to select which documents to judge. Despite the importance of the sampling strategy on estimate quality, there is little guidance in the literature on how best to sample to obtain good estimates. This paper thus ex-amines the question of how the sampling strategy used in creating judgment sets affects the quality of the resulting test collection when using extended inferred measures.
A test collection is a triple containing a document set, a set of topics, and a set of relevance judgments which we will call a qrels . A run is the output of a retrieval system for each of the topics in the collection. This output is assumed to be a list of documents ranked by decreasing similarity to the topic. The quality of a run is evaluated using the mean over all topics in the collection of some metric that computes a per-topic score based on the ranks at which relevant doc-uments are retrieved. This work considers three measures, mean average precision (MAP), normalized discounted cu-mulative gain (NDCG), and precision at 10 documents re-trieved (P(10)) and their inferred counterparts.

Ideally, qrels would be complete meaning that all doc-uments are judged for all topics. Complete judgments are infeasible for all but the tiniest of document sets, however, so instead some subset of documents is judged. In pooling [2], the top K results from each of a set of runs are combined to form the pool and only those documents in the pool are judged. Runs are subsequently evaluated by assuming that all unpooled (and hence unjudged) documents are not rele-vant. The inferred measures framework uses stratified sam-pling strategies and estimates the values of the measures based on the judgments obtained on sampled documents.
Both pooling and the inferred measures framework con-struct a judgment set based on a set of runs. The frame-work focuses different amounts of assessing effort on differ-ent regions of the ranked lists (the strata) by assigning a sampling rate to each stratum. The strata are defined by disjoint sets of contiguous ranks. A document belongs to the stratum defined by the smallest rank at which the doc-ument was retrieved across the set of runs. The sampling rate is the probability of selecting a document from the stra-tum to be judged. The inferred measures use the knowledge of a stratum X  X  size, sampling rate, and number of relevant documents found from among the judged set to estimate the total number of relevant documents in the stratum, and combines the different strata X  X  estimates to compute a final global estimate of the number of relevant documents for a topic. Similar local estimates are made for the number of relevant documents retrieved by an individual run based on the number of judged documents retrieved by that run in each of the strata.

A sampling strategy is the combination of strata definition and sampling rate per stratum. Different sampling strategies applied to the same run set create different test collections since the judged set of documents depends on the sampling strategy. The quality of a sampling strategy is thus mea-sured by the quality of the test collections it induces. To measure the quality of a test collection, we use the accuracy of determining the set of significantly different run pairs as compared to a gold standard set of runs pairs (similar to the approach used by Bompada, et al. [1]).

In general, the larger the assessment budget (i.e., maxi-mum number of human judgments that can be obtained), the better a test collection will be. Thus it is important to control for the total number of documents judged when comparing sampling strategies. In practice, this means that stratum size and sampling rate must be traded off against one another: small strata can have large sampling rates but large strata must be sampled more sparsely.

We consider the following strategies in this work. pool : A single, exhaustively judged (thus shallow) stra-1stratum : A single stratum drawn to a moderate depth 2strata : An exhaustively judged small initial stratum plus 3strata : An exhaustively judged small initial stratum, a
To examine the quality of a sampling strategy, we use an existing test collection and create a set of sampled collections containing a subset of the original collection X  X  judgments. We then compare the average behavior of the sampled col-lections to that of the original collection, which we treat as gold standard behavior. We use two existing TREC collec-tions as the base collection in this work: the collection built in the the TREC-8 ad hoc retrieval task and the collection built in the TREC 2012 Medical Records track.

The TREC-8 ad hoc track collection contains about 528,000 mostly newswire documents and 50 topics. Bi-nary relevance judgments were created through pooling with K = 100. This collection was chosen for this study because it is a high quality test collection that has been used in many similar studies; evaluation scores computed on this collection are as close to Truth as we are likely to ever get. We use traditional evaluation measures computed over the official qrels as the gold standard for this collection.

The original motivation for this study was unexpectedly noisy scores that resulted from using inferred measures in the TREC 2011 Medical Records track [3]. The 2012 Medical Records track collection is similar to the 2011 collection in that it uses the same document set and similar topics, but it contains a better set of relevance judgments to use as ground truth. The document set is approximately 17,000 medical visit reports and there are 47 topics. Judgment sets were created using a two strata strategy. The first stratum contains all documents retrieved between ranks 1 X 15 from 88 runs, and the second stratum is drawn from ranks 16 X  100 with a sampling rate of 25%. Documents were judged on a three-way scale of relevant, partially relevant and not relevant. For this collection, the gold standard scores are the inferred scores computed using the entire qrels created in the track. The computation of infNDCG uses a gain value of 1 for partially relevant documents and 2 for fully relevant documents.

Call a run that contributed to the judgment sets for the base collection a judged run 1 . To test a sampling strategy, we randomly split the set of judged runs in half, and apply the sampling strategy to only the runs in one half. Once a judgment set is produced, we evaluate all runs submitted to the original TREC track using it. The entire process is repeated 50 times, each time called a trial, with each trial using a different random split of judged runs. All sampling strategies use precisely the same split of judged runs in each trial, but necessarily produce different judgment sets from that split.
Using the set of sampled judgment sets, we can compare the sampling strategies along three dimensions: the number of judged documents required, the accuracy of determining significantly different run pairs, and the accuracy of esti-mates of the total number of relevant documents, R.
As mentioned above, comparisons of sampling strategies must control for the total number of judgments a strategy requires, called the judgment set size. The mean judgment size over the 50 trials for each sampling strategy is given in Table 1.

Note that it is possible that a document selected to be judged might not actually have a judgment in the base col-lection X  X  qrels. When this happens, we ignore the selection (i.e., we mark it as not selected). To account for this effect, the sampling rate actually used in constructing the sample judgment sets was greater than the target rate such that the number of judgments obtained approximated the num-ber the target rate would produce (while always sampling from the appropriate stratum). The rates given in the def-initions of the strata are the target rates, which are also the effective sampling rates. The strata sizes and sampling rates provide only coarse control of the resulting judgment set size, however, and there are sampling strategies the base collection cannot support because there are too many miss-ing judgments in its qrels. The TREC 2012 collection cannot support a pooled strategy with a depth greater than 15, nor does it support the exploration of a 3strata strategy. We use a pool depth of both 15 and 25 for the TREC-8 collec-tion since depth 25 is a better match for the other methods X  judgment set sizes on that collection while depth 15 is used for the TREC 2012 collection.

The lack of fine control means there is a wider spread in the range of judgment set sizes across the strategies than is ideal. However, the 1stratum strategy is the strategy with the largest judgment set size, and as will be seen below, it is not a very good strategy even with the larger number of judgments.
We use the accuracy of detecting the set of significantly different run pairs as the measure of a sampled collection X  X  quality. For a given measure, we first compute the set of significantly different run pairs in the base collection using a paired t-test with  X  = 0 . 05. Using the same test for a sampled collection, we calculate the number of pairs in each of the following five categories: true positive: the run pair is significantly different in both true negative: the run pair is not significantly different in miss: the run pair is significantly different in the base col-false alarm: the run pair is not significantly different in inversion: the run pair is significantly different in both col-The accuracy of the significant pairs determination is then computed as the number of correct classifications over the sum of correct and incorrect classifications where inversions are counted as both a false alarm and a miss.

Accuracy scores are given in Table 2, which shows the mean value as well as the minimum and maximum accuracy values observed across the 50 trials.

The 1stratum strategy is consistently the worst strategy for the NDCG and P(10) measures, and is only somewhat more competitive for MAP. This strategy is the only strategy examined that does not judge all of the documents retrieved in the top 10 ranks, but instead samples uniformly from the top 100 ranks. Since all three measures strongly emphasize Table 2: Accuracy of reproducing the set of signifi-cantly different run pairs the quality of the top of the ranked list X  X hough MAP less so than P(10) and NDCG X  X njudged documents in these ranks increase the variability of the final estimated score much more than unjudged documents later in the ranked list do. Thus it is beneficial to concentrate more of the judging resources at the very top of the document ranked lists.
The 3strata strategy is the strategy used for the TREC 2011 Medical Records collection whose noisy score estimates prompted this study. Its behavior on the TREC-8 collection confirms that it is a poor choice of sampling strategy. The problem is that in this strategy the stratum for which the least information is known receives the greatest emphasis. The computation of the inferred measures uses as an esti-mate of a run X  X  number of relevant retrieved in a particular stratum, S, a smoothed fraction of the number of judged rel-evant documents in S retrieved by the run over the number of judged documents in S retrieved by the run. When a run retrieves few (or none) of the documents that were selected to be judged in the stratum but relatively many documents from the stratum, the estimate can be far afield of the true value. For example, if a run retrieves only one document in S that was judged, and that document is relevant, the estimated number of relevant retrieved is .99998 times the number of documents in S that are retrieved by the run. Any strategy that includes a large stratum that is sampled very sparsely will be affected by this behavior.

The relative quality of the pool and 2strata strategies is more complex. The 2strata strategy is more accurate for the TREC 2012 collection. The pooling strategy is generally more accurate for the TREC-8 collection, but there is a clear dependency on the pool depth. The next section will show that the 2strata strategy produces better estimates of the number of relevant documents for both collections.
Part of the computation of extended inferred measures is creating an estimate of the total number of relevant docu-ments for a topic. This estimate has a large impact on the quality of the final estimates of NDCG and MAP scores. Since NDCG is defined over a fixed set of ranks (100 in this work), while MAP considers the entire relevant set, the qual-ity of the estimate of R has a bigger impact on infAP than it does on infNDCG. Table 3: Mean correlation between estimated and gold-standard R
T able 3 gives the mean over the 50 trials of the Pear-son correlation between the per-topic estimate and gold-standard values of R, the number of relevant documents. The gold-standard value of R for the TREC 2012 collection is the estimate produced using the entire set of runs submit-ted to the Medical Records track. The estimates based on half of the runs are very highly correlated with the original estimates, though the pooling strategy produces less good estimates than the other strategies. The gold-standard value of R for the TREC-8 collection is the count of the the num-ber of relevant in the original qrels. The estimated R values are again highly correlated with true R, but the correlations are weaker for the TREC-8 collection than the TREC 2012 collection. The pooling strategies exhibit the weakest cor-relations, and the correlations get weaker as the pool depth gets smaller.

Despite a good overall correlation, the pooling strategies underestimate R for all but the smallest of relevant sets, meaning that they cannot distinguish topics that truly have a small relevant set. Figure 1 shows the estimates of R per topic for the TREC-8 collection for the pool25 (left) and 2strata (right) sampling strategies. Individual topics are plotted on the x-axis and the number of documents on the y-axis. The minimum and maximum estimates of R across the 50 trials are plotted by a line connecting the two points (which runs off the top of the graph if the maximum estimate exceeds the graph X  X  largest y-value). The mean across the 50 trials of the estimated R is plotted as an x on that line. The gold-standard value of R is plotted as a circle. For the pool25 strategy, nine topics have a mean estimated R smaller than 20. The gold-standard and estimated R values for these topics are shown below, which has a correlation of just 0.42.
 Gold: 13 22 16 6 28 13 17 17 65
Est: 12.7 17.8 15.0 6.0 13.7 9.8 17.0 17.0 16.0
The inferred measures framework can construct test col-lections that have high fidelity with their traditionally-constructed counterparts using relatively few judgments pro-vided an appropriate sampling strategy is used in the con-struction process. The framework does not work well with large, sparsely sampled strata nor with strategies that do not exhaustively judge a small top stratum. Restricting all judgments to the very top ranks (i.e., pooling) is also not a good strategy, though, because such samples are unable to accurately estimate R. Thus, what this paper called the 2strata strategy X  X sing an exhaustively judged small initial stratum coupled with a moderate depth, moderate sampling rate stratum X  X s a practical and effective sampling strategy to use for inferred measures.
