 Online reviews provide consumers with valuable information that guides their decisions on a variety of fronts: from en-tertainment and shopping to medical services. Although the proliferation of online reviews gives insights about different aspects of a product, it can also prove a serious drawback: consumers cannot and will not read thousands of reviews before making a purchase decision. This need to extract useful information from large review corpora has spawned considerable prior work, but so far all have drawbacks. Re-view summarization (generating statistical descriptions of review sets) sacrifices the immediacy and narrative structure of reviews. Likewise, review selection (identifying a subset of  X  X elpful X  or  X  X mportant X  reviews) leads to redundant or non-representative summaries. In this paper, we fill the gap between existing review-summarization and review-selection methods by selecting a small subset of reviews that together preserve the statistical properties of the entire review cor-pus. We formalize this task as a combinatorial optimization problem and show that it NP-hard both to solve and ap-proximate. We also design effective algorithms that prove to work well in practice. Our experiments with real review corpora on different types of products demonstrate the util-ity of our methods, and our user studies indicate that our methods provide a better summary than prior approaches. H.3.3 [ Information Search and Retrieval ]: Selection Process; H.2.8 [ Database Management ]: Database ap-plications X  Data Mining customer reviews, review selection, reviews
One of the most significant developments in consumer shopping in recent years has been the explosive proliferation of online reviews. This reflects the great utility and trust that individuals ascribe to first-person reviews of products and services, particularly when those reviews come from ac-tual past customers. It also reflects the fact that reviews are often highly detailed: reviews routinely cover many different aspects of a single product, allowing the reader to compile a very thorough picture of the strengths and weaknesses of any given product. Because of the utility and impact of reviews, major e-commerce sites have adopted them as a standard feature (e.g., Amazon.com ), and a number of popular web-sites have emerged whose main goal is simply hosting online reviews (e.g., Yelp.com ).

The immense value of online reviews has led to a vast body of reviews for individual items. For example, Amazon has more than 32,000 reviews for its Kindle e-reader, and there are individual restaurants in Yelp that have more than 100,000 reviews. Having a large number of reviews on the same product is desirable, since a large review-corpus is more likely to provide insights about different aspects of the item at hand. In addition, a large number of reviews increases the reader X  X  confidence in the opinions they express.
However, the vast body of reviews for individual item-s is also a serious drawback. Consumers can not and will not read thousands of reviews before making a purchase de-cision. When a body of reviews is massive, its sheer size can prevent readers from extracting its useful information. A number of researchers have therefore begun to look for methods to extract the useful information contained in large review corpora, into a form that users can absorb more eas-ily, despite having limited time or limited screen space.
For example, a significant amount of work has been devot-ed to the development of review-summarization methods [5, 20, 14, 15, 18]. The goal of these approaches is to create a summary of a review corpus that is both compact and rep-resentative of the opinions it contains. These summaries are statistical in nature, recording the number of positive and negative opinions that are expressed on each of the item X  X  features. However, the drawback of such summaries is that they lack the immediacy and narrative structure of reviews as written by real users. This sacrifices one of the prima-ry benefits of online reviews: the ability to appreciate the first-person experience of a previous buyer.

In response to this drawback of statistical summaries, an-other set of methods seek to summarize a review corpus using a subset of  X  X mportant X  reviews. These important re-views are ultimately the ones shown to the user. For exam-ple, there exists a considerable amount of work in the area of review ranking [4, 7, 9, 10, 11, 17, 19]. Those approaches first produce a score for each review. They then show the t op-k highest-scoring reviews to the user. However, these methods also suffer from a serious drawback: they do not seek coverage over the range of features that are important to users, and can therefore be highly redundant. For ex-ample, all the top reviews of a new camera may be highly informative about its long-range zoom ability, but mention nothing about how easy it is to use or carry. In other words, by evaluating each review separately, these approaches fail to consider the complementarity among reviews.

Hence, a number of recent efforts has focused on the prob-lem of review selection based on feature coverage . For exam-ple, Lappas and Gunopulos [8] select a subset of reviews that collectively comment upon of all the features of the prod-uct at hand. Similarly, Tsaparas et al. [16] select a set of reviews that collectively provide both the negative and the positive aspects of each commented feature. While these methods do diversify the reported set of reviews, they fail to accurately capture the proportion of opinions (both posi-tive and negative) in the underlying collection. For example, the summary constructed by method in [16] will always con-tain at least one positive and at least one negative opinion on each feature, even if one kind of opinion is significantly more common in the reviews of the underlying collection.
In this paper, we fill the gap between existing methods for review summarization and selection, by providing a subset of reviews that collectively present a statistically accurate summary of the entire review collection. More specifical-ly, we preserve the integrity, coherence, and immediacy of actual reviews by providing users with a subset of reviews written by real previous customers. At the same time, we also ensure that the selected set of reviews preserves the s-tatistics of the underlying corpus with respect to the positive and negative opinions expressed for different features. On a high level, the problem that we address in this paper is the following:  X  X iven a corpus of Reviews R on item, find a small subset S  X  R of reviews that accurately capture the proportion of opinions of the item X  X  features. X  To the best of our knowledge, we are the first to formally define and study this problem, which we call the Characteristic-Review Selection problem.

In our work, we provide a formalization and thorough analysis of the problem, and identify and overcome the chal-lenges that arise in efficiently finding its solution. We present novel algorithmic techniques, and show that they are effec-tive through a rigorous evaluation. We show that our meth-ods can accurately represent large review corpora, using only a small subset of reviews (e.g., from 5 to 20 reviews). Fi-nally, we confirm the utility of the reported sets through a user study in which actual users judge the relative value of our approaches compared to the state of the art.

In addition to our main contribution of formalizing and solving the Characteristic-Review Selection problem, we also address an important precursor problem. A statisti-cal summary is only meaningful if the distribution of positive and negative opinions in the corpus is stable at the time the summary is formed. If the state is unstable or immature, then the reported set could also be misleading. Taking this into consideration, we include an analysis of real review cor-pora in our experiments, and explore how the allotment of opinions in such corpora changes through time. Our analysis provides valuable insight on the converge of opinions, and allows us to identify when a body of reviews is sufficiently stable so as to serve as the basis of an informative summary. Roadmap: The rest of the paper is organized as follows. First, we review related work in Section 2. We formally de-fine our problem in Section 3, and in Section 4 we describe our algorithms for solving it. In Section 5, we present a thor-ough experimental evaluation of our methods. We conclude the paper in Section 6.
The goal of our work is to select a characteristic subset of reviews from a given corpus, in order to provide users with a compact and representative source of information. Although  X  to the best of our knowledge  X  the exact problem formu-lation we are proposing has not been considered before, our work clearly has ties with existing work in the domains of review summarization and review selection.
 Review Selection: A recent line of work has addressed the problem of selecting a subset of reviews from a given corpus. Lappas and Gunopulos [8] proposed the selection of a set that represents the majority opinion on each fea-ture. The drawback of this approach is that it overlooks the minority opinion, regardless of how significant it is. In a follow-up work, Tsaparas et al. [16] adopted an alternative formulation: given an upper bound on the number of the s-elected reviews, their goal is to select the set of reviews that covers as many features as possible. In their work, a feature is considered covered if the reported set includes at least one positive and at least one negative opinion on it. The short-coming of this formulation is that it disregards the ratio of the number of positive and negative opinions on a feature. Thus, the reported set can be misleading to the user, who cannot use this set to deduce the actual proportion of posi-tive and negative opinions across product features. The set of characteristic reviews reported by our methods does not have such drawbacks, since our goal is to accurately emulate the opinion distribution in the underlying corpus. Review Summarization: Our work has ties to the exten-sive research devoted to review summarization. The prob-lem of summarization was essentially introduced by Hu and Liu [5], who proposed a method for opinion-extraction, and applied the mined knowledge to create a statistical sum-mary that informs the user of the number of positive and negative opinions in the corpus. A similar approach to the problem was taken by Zhang et al. [20]. In that work, the authors focused on the review domain, enriching their sum-mary with opinionated sentences from the corpus. Instead of reporting the number of positive and negative opinions, Meng and Wang [14] report the terms that are used to char-acterize each feature (e.g.  X  X mall X  or  X  X xpensive X ). Another extension is proposed by Shimada et al. [15], who enrich the produced summary with objective information on the item, mined from web sources (i.e. Wikipedia). Finally, Liu et al. [9] explore ways to filter out low-quality reviews, in order to prevent them from corrupting the produced summary.
While the proposed approaches use statistical findings and isolated snippets to represent the given corpus, our own formulation asks for the selection of a characteristic set of actual reviews. We consider reviews as cohesive linguistic constructs, which provide users with an intuitive and user-friendly representation of the corpus. Nonetheless, in the context of a review-hosting website, our selection paradig-m can be easily complemented by a corresponding statisti-cal summary, in order to provide users with the maximum amount of information.
 Review Quality and Ranking A significant amount of work has been devoted to evaluating different aspects of re-view quality [1, 11, 17]. These methods assign a quality score to every review, which is then used for ranking the reviews. The most prolific line of work in this field has focused on finding ways to emulate the user-assigned helpfulness votes that are present in review-hosting websites [19, 10, 7, 19, 4]. The main shortcoming of these approaches is that they evaluate the quality of each review separately. Hence, they completely dismiss the potential of reviews to complement each other and cover as many of the item X  X  features as possi-ble. High-quality reviews can still be redundant, reiterating the same opinions on the same features, and thus convey-ing no additional information to the user. Our selection paradigm overcomes such shortcomings, by considering the complementarity among reviews. Nonetheless, our method-ology is fully compatible with quality-evaluation techniques, which can be used to filter out substandard reviews.
In this section, we describe the notational conventions that we will use throughout the paper. We also provide a formal definition of the Characteristic-Review Selec-tion problem and discuss its computational complexity.
We use R to denote the collection of n reviews of the item of interest. We also assume that every item is associated with a set of z features { f 1 , . . . , f z } . One should think of the features as the characteristics of the item that reviewers comment upon. For example, the features of a restaurant may be the food quality , ambiance , service , and so forth. A typical review only comments on a subset of these fea-tures, express either positive or negative opinions. In fact, the same review may have a positive opinion about one fea-ture and a negative opinion about another. Hence, in order to describe the opinions that appear in reviews, we form the set of binarized feature opinions, which we refer to simply as the opinions of the item. That is, for each feature f i , there are two possible opinions: positive and negative. Hence, the opinion vector of a item has cardinality m = 2 z ; we denote it by A = { a 1 , . . . , a m } .

Throughout the paper, we assume that features and opin-ions can be automatically extracted. There are a number of methods for this task. In our experiments, we use the method by Ding et al. [2]. Therefore, every review R  X  R can be represented as a subset of opinions from A ; we say that R covers all the opinions that appear in R .
Given a collection of reviews S  X  R , we use  X  ( S ) to denote the m -dimensional vector of that represents the fraction of the reviews in S that cover each opinion. That is,  X  ( S , i ) denotes the fraction of reviews in S that cover opinion a We refer to  X  ( S ) as the opinion vector of S .

In addition to the opinion vector, we also use the notion of a target vector  X  . The target vector is also defined over the set of opinions in A and, thus, it is also an m -dimensional vector. We quantify the closeness between two vectors  X  and  X  using the L 2 2 norm of their difference. We denote this by Given a set of reviews S and a target vector  X  we call the value of D (  X  ( S ) ,  X  ) the error of the collection S .
Given a collection of reviews R about a item and a target vector  X  , our high-level goal is to select a subset of reviews S  X  R such that D (  X  ( S ) ,  X  ) is small. At the same time, we also want the subset S to consist of a small number of reviews. The encoding of these two goals is incorporated in the following problem definition.
 Problem 1 (Characteristic-Review Selection (Crs)). Given a collection of reviews R , a target vector  X  and an integer number k , find S  X  R such that |S|  X  k and D (  X  ( S ) ,  X  ) is minimized.

Problem 1 would be trivial if all the reviews contained at most one opinion. Without such unrealistic restriction-s, however, the problem is hard. Specifically, our analysis below demonstrates that the Crs problem is not only NP-hard, but it is also NP-hard to approximate. In order to see this, lets consider the decision version of the problem which is defined as follows: Given a collection of reviews R , and real number L and integer K does there exist a subset S  X  R with |S|  X  k and D (  X  ( S ) ,  X  )  X  d ? We call this de-cision version of the problem Decision-Crs . The following lemma shows that this problem is NP-complete.
 Lemma 1. The Decision-Crs problem is NP-complete. Proof. We will reduce and instance of X3C ( Exact Cover by 3 Sets ) [3] to Decision-Crs . An instance of X3C consists of a universe of n items U and a set of sets C = { C 1 , . . . , C M } where | C i | = 3 for every i = 1 , . . . , M and C i  X  U . Given integer K , the decision version of the problem asks whether there exists a subset C  X   X  C such that |C | = K , U and every element in U is covered by exactly one set in C  X  .
 We transform an instance of X3C to an instance of Decision-Crs by setting the set of opinions A to be the universe U . In this case, every set C i  X  C is represented by a review R i  X  R ; review R i covers opinion a j  X  A if set C contains the j -th element of U . To complete the instance of the Decision-Crs problem, we set the target vector  X  to consist of n dimensions all equal to 1 /K , and we also set k = K and d = 0. Then, it is easy to see that the answer to the X3C decision problem is  X  X es X  if and only if there exist-s a solution to the instance of the Decision-Crs problem, which we created above.

By the fact that we used X 3C for our reduction, we can show that the problem is NP-hard even if all the reviews of the collection have exactly three opinions. Another imme-diate corollary of Lemma 1 is the following.

Corollary 1. The Crs problems is NP-hard and NP-hard to approximate. That is, it is NP-hard to find a poly-nomial -time approximation algorithm for the Crs problem.
Proof. We will prove the hardness of approximation of the Crs problem by contradiction. Assume that there exists an  X  -approximation algorithm for the Crs problem. Then if S  X  is the optimal solution to the problem and S A is the so-lution output by this approximation algorithm, it will hold that D (  X  ( S A ) ,  X  )  X   X D (  X  ( S  X  ) ,  X  ). If such an approximation algorithm exists, then this algorithm can be used to decide decision instances of the Crs problem for which d = 0. How-ever, this contradicts the proof of Lemma 1, which indicates that these problems are also NP-hard. Thus, such an ap-proximation algorithm does not exist.

So far, we have discussed the C rs problem for an arbi-trary target vector  X  . For a collection of reviews R , the most natural instantiation of the target vector is the mean opinion vector , denoted by  X  ( R ). Figure 1 shows an ex-ample of a review corpus and the optimal characteristic subset that minimizes D (  X ,  X  ( R )). The corpus consist-s of 6 reviews R 1 , R 2 , R 4 , R 4 , R 5 , R 6 . These reviews com-ment on 3 distinct features f 1 , f 2 , f 3 . A plus (resp. mi-nus) denotes a positive (resp. negative) opinion on the feature. For example, review R 1 expressed positive opin-ions on features f 1 and f 2 and a negative opinion on fea-ture f 3 . In this example, the mean opinion vector corre-sponding to the included opinions f + 1 , f  X  1 , f + 2 , f opinion vector of the selected characteristic set { R 4 , R is optimal with respect to the objective function, when the target is the mean vector.
 Figure 1: A n example of a review corpus and the opti-
This instantiation of the target vector  X  and the Crs prob-lem  X  corresponds to a very natural geometric problem: Giv-en a set of points in m -dimensional space, select a subset of them of size k whose centroid approximates the centroid of the collection as well as possible. In the case of Crs the m -dimensional vectors are also binary, but the general problem where vectors lie in R m is also interesting and  X  to the best of our knowledge  X  unexplored. Another natural target vec-tor is the distribution of features in R . However, one can observe that this is actually a simple rescaling of the mean opinion vector  X  ( R ). Although in all our experiments we use  X  ( R ) as our target vector, we note that alternative instan-tiations may also arise in practice. For example, when the goal is to provide personalized sets of characteristic reviews, then one may choose to approximate a specially-selected set of reviews (e.g. reviews written by reviewers with similar preferences as the interested user). Hence, in such cases the target vector would be tuned by the characteristics of the particular user.
Although the Crs problem is NP-hard and NP-hard to ap-proximate, our experiments with real datasets demonstrate that, in practice, there exist heuristics that work extremely well. We describe such heuristics below. With the exception of Random (described in Section 4.3), all our heuristics work for arbitrary target vectors  X  . The Greedy algorithm is an iterative algorithm for Crs . At every iteration t , the algorithm picks one review R to form subset S t . The review R is picked so that the distance D  X  S t  X  1  X  { R } ,  X  is minimized. The pseudocode of the Greedy algorithm is shown in Algorithm 1.
 Algorithm 1 T he Greedy algorithm 1: S =  X  2: for i = 1 . . . k do 3: R = arg min R  X   X  X  D (  X ,  X  ( S  X  { R  X  } )) 4: S = S  X  { R } 5: R = R \ { R } 6: end for 7: return S
For a collection R o f n reviews, the running time of Greedy is O ( knT D ), where T D is the time required to compute the distance D required in line 3 of Algorithm 1. In our case, T D requires O ( m ) time, and therefore the running time of Greedy is O ( knm ).
The Integer-Regression algorithm finds the set of k characteristic reviews by first solving a continuous version of the Crs problem, and then transforming the obtained continuous solution into the closest discrete one. This is a well known strategy that has been shown to be effective for combinatorial optimization problems.

To describe the Integer-Regression algorithm, we use matrix notation. We start with the m  X  n binary matrix R in which entry R ij = 1 iff opinion a i appears in review R . The Characteristic-Review Selection problem can then be restated as: find a 0-1 vector s such that D ( Rs ,  X  ) is minimized, and s contains at most k 1s. The fact that min s D ( Rs ,  X  ) has the form of a linear regression is what inspires our approach.

In reality, R can contain duplicate columns; these are ir-relevant for regression, so we remove them to form  X  R having size m  X  n  X  and consisting of distinct columns. However, du-plicate columns in R correspond to distinct reviews that may be useful in approximating  X  ; hence we keep track of the number of such duplicate columns by remembering for each column i of  X  R its multiplicity c i in R . The Integer-Regression algorithm works in two steps, which are repeated for all values from 1 to k : Step 1 solves a regression problem, with the constraint that the solution has no more than  X  positive coefficients. In s-tatistics, regression with this sort of constraint is referred to as a subset selection problem; in signal processing it is called sparse signal reconstruction. We adopt an algorithm from signal processing for this step, namely Nonnegative Or-thogonal Matching Pursuit ( NOMP ) [12, 13]. However, a variety of algorithms are known for this problem, and we expect that our general approach can make use of other al-gorithms as well. NOMP works iteratively; at each iteration t , it greedily selects the (not-yet selected) column from  X  has the largest dot-product with the residual of the target vector  X  . Then, the corresponding coefficients x t are com-puted via least-squares, and a new residual is computed as  X   X   X  Rx t .

Step 1 returns a nonnegative real-valued vector x such that D (  X ,  X  Rx ) is minimized. Intuitively, the entries of x en-code the  X  X roportions X  of each column of  X  R that are needed in order to accurately approximate the target vector  X  . The vector x is real-valued, and does not take into account the multiplicity of reviews c i . Hence, x cannot be used directly as a solution to the Crs problem; instead, we use x to guide us to an actual set of reviews from the original collection R . Accordingly, we transform the proportions of reviews encod-ed in x into integer values in  X s such that k  X s k 1 = k and for every element  X s i of  X s it is guaranteed that  X s i  X  c column in  X  R cannot be used more times than its multiplic-ity in R ). This translation from proportions to integers is accomplished in Step 2 of the algorithm.

The reason that we perform these steps for  X  = 1 , ..., k is that a trade-off exists between the sparsity of x and the hard limit k on the total number of reviews. When  X  is small (say, 1) then it is difficult to find an x such that  X  is well approximated by  X  Rx ; after all, x can only have  X  nonzero elements. On the other hand, when  X  is large (say, k ) then it is difficult to approximate the real-valued x with k integer-valued reviews. Intuitively, we expect the best value of  X  to be somewhere in between 1 and k ; in practice, we simply test all values of  X  between 1 and k .

As we have already discussed, Step 2 constructs the clos-est possible discrete approximation to the output of Step 1 . The problem can expressed as follows: Let the num-ber of nonzero elements in x be p . Given the nonnegative real vector v  X  R p having || v || 1 = 1, and a set of integers c , c 2 , ..., c p , output a nonnegative integer vector  X s  X  R
We use an efficient algorithm to solve the above problem in O ( Cp ) time, where C = P p i =1 c i . The basic idea is  X  X on-ditioning X  on the sum of  X s elements, i.e., ||  X s || 1 . That is, we augment the problem with the requirement that ||  X s || 1 We can solve this efficiently as follows. First, we observe that Next, we set aside the constraints c i and solve the un-constrained problem. For that, let U = P p i =1  X  N v i  X  and L = P p i =1  X  N v i  X  . If N  X  L (resp. N  X  U ) then the solution is to set each  X s i value below (resp. above) the corresponding value of N v i . If L &lt; N &lt; U , then compute X = N  X  L . Let the set L be the elements of v having the X largest val-ues of N v i  X   X  N v i  X  . For elements i  X  L , set  X s i For the other elements, set  X s i =  X  N v i  X  . In order to solve the constraint version of the problem, we first fix  X s i for all entries i such that c i &lt; N v i . Then, we solve the unconstrained version of the problem for the remaining ele-ments. For any given N , this yields the optimal  X s in O ( p ) time. Since the maximum allowable value of ||  X s || 1 given the constraints is C , we only need to run this algorithm for each N  X  1 , ..., C to find the optimal value of  X s . The computational complexity of Step 1 of the Integer-Regression algorithm requires O ( k 3 ) in general. However, since NOMP works column-wise, incremental algorithms exist that lower the complexity of the regression to O ( k 2 ). Com-bined with the outer loop over k , this brings the overall com-plexity to O ( k 3 ). Thus, for small values of k that are most appropriate for review summaries, the running time of this step is negligible. As we have already discussed, the running time of Step 2 is O ( Cp ). As C = O ( n ) and p = O ( k ) the worst-case running time of this step is O ( nk ). For the very small values of k that we consider in practice, the running time of this step is almost linear in n .
When the target vector  X  is the mean opinion vector of the collection of reviews R (denoted by  X  ( R )) we have the following observation.

Observation 1. A random sample S of R of size k guar-antees that D (  X  ( R ) ,  X  ( S )) = 0 on expectation . In other words, for a random sample of reviews S ,  X  ( S ) is an unbiased estimator of  X  ( R ). However, the algorithm that randomly samples k reviews from R , which we call Random , performs poorly in practice. This is because its results have high variance, which also leads to higher error. In order to reduce the variance, we also propose an iterative version of Random  X  which we call Iterative-Random . The Iterative-Random selects a reasonably large number of random samples from R and reports the one with the best valuation of the objective function of the Crs problem.
Here, we present a thorough experimental evaluation of our methodology using review corpora from Amazon. In all our experiments, we use the mean opinion vector of the underlying collection of reviews as our target vector
We start our experimental evaluation in Section 5.2, where we present a study of the convergence of the opinion vectors of different corpora. We then evaluate the performance of the different algorithms with respect to the objective func-tion of Crs in Section 5.3. We conclude our experimental study in Section 5.4, by presenting a user study that demon-strates the appeal of our summaries to real users.
In our experiments, we use six different datasets of re-views from Amazon.com. The data was extracted from the collection of reviews introduced by Jindal and Liu [6]. This collection, crawled in June 2006, covers a wide range of d-i fferent product from Amazon.com. It includes 5.8 million reviews, 2.14 million reviewers and 6.7 million products. For our own experiments we extract six datasets from six differ-ent domains: MP3 players, Digital Cameras, Coffee Makers, Printers, Books, and Vacuum Cleaners. We refer to these as MP3 , CAM , COF , PRINT , BOOK and VAC . These datasets in-clude reviews on 238, 233, 40, 112, 11447, and 62 different items, respectively. The mainstream nature and popularity of these domains ensures the availability of sizable review corpora. Other than that, our methods do not benefit from this selection. For the BOOK dataset, we purposefully avoid-ed introducing any further constraints on the genre or type of the book. Our goal was to evaluate our algorithms in the context of a very large dataset of ambiguous items that typically encourage subjective reviews.

For our experiments, we extracted the opinions expressed in a review using the method proposed by Ding et al. [2]. Recall that an opinion is defined as the mapping of a feature (e.g. the lens of a digital camera) to a positive or negative polarity. While the method of Ding et al. worked well in practice, our framework is compatible with any method for opinion extraction.
The Crs problem asks for a set for a set of reviews that respect the opinion vector  X  ( R ) of the underlying corpus. Such a set would only be informative and depictive of the reviewed item X  X  true qualities, if the target opinion vector has converged to a stable state. On the other hand, if the arrival of new reviews causes big variations to the opinion vector, then the resulting set can be misleading.The goal of this section is to experimentally explore whether such variations appear in practice.

In order to study this for a corpus R on a particular item, we proceed as follows. First, we sort the reviews in ascend-ing order of their submission date and set a checkpoint every 10 reviews. Using a checkpoint attached to a calendar unit (e.g. monthly or weekly) was not desirable since, for most corpora, the number of included reviews varies greatly across units. As we saw in our experiments, setting checkpoints ev-ery 10 reviews results in batches that include opinions on at least 90% of the item X  X  features. Further, 10 is the number of reviews that is typically included in a single webpage of a review-hosting site (e.g. Amazon.com ), since it is big e-nough to provided sufficient information, and small enough to respect the users X  attention span.
 Therefore, for a corpus with T checkpoints and t  X  { 1, . . . , T } we use R t to denote the set of the 10 t oldest reviews in the corpus. We measure the variation between the opinion vectors of R t  X  1 and R t by measuring
Given a sequence of T such measurements and a threshold d , we say that an item is converged with respect to d , if there exists a t such that D t  X  d . If, in addition to the above, we also observe that for every t  X   X  t D  X  t  X  d , then we say that the item is strongly converged . Therefore, the opinion vector of strongly-converged items shows little or no variation after a particular checkpoint. Finally, we refer to items that are converged but not strongly converged as relapsed items. For a given item with corpus R , and T measurements D , . . . , D T , we study the following questions: (a) Do the D values decrease as t increases? (b) How long does it take for a corpus converge? (c) How often do items relapse? Next, we address these questions and present the corresponding experimental findings.

Figure 2(a) shows the average D t values across all items as a function of the checkpoint t . The figure validates our intuition that as the size of a corpus increases, the D t decrease. For the plot, we use T = 25 for all items, since the D t values for t &gt; 25 are almost zero. The plot demonstrates that the opinion vectors of all corpora show little variation as time progresses. Starting from a maximum value around 0 . 3, the average D t value drops fast with t and for t = 25, it becomes almost zero.

To gain additional insight, we study the percentage of items that have relapsed at least once. The results for dif-are shown in in Figure 2(b). In order to properly interpret these results, we need to know the percentage of converged items per threshold; this percentage is shown in Figure 2(c). Figure 2(b) clearly demonstrates that only a minuscule per-centage of items relapse. Therefore, most of the items are actually strongly converged. The significance of this find-ing is enhanced even further by its applicability across all datasets, despite the fact that they correspond to differen-t types of products. Figure 2(c) shows the percentage of converged items for different values of d . For threshold val-ues d = 0 . 025 and d = 0 . 05, the percentage of converged items ranges from 1% ( CAM -0.025) to 33% ( MP3 -0.05). This variance demonstrates that, for small values of d , the conver-gence results are domain-specific. However, as d increases (e.g., d = 0 . 4), the percentage of converged items is equal or very close to 100% for all datasets.

Overall, our analysis demonstrates that the opinion vec-tors of review corpora converge as new reviews are added. In addition, we discovered that the vast majority of the items in our collections are strongly converged. In this section we evaluate the solutions reported by the Greedy , Integer-Regression and Iterative-Random algo-rithms for the Crs problem.
 Randomization testing: We start our evaluation by per-forming the following randomization experiment. For each corpus R in our collection we use algorithm Alg 1 to extract a set of k characteristic reviews S Alg . Then, we evaluate the error of this set, i.e., D Alg = D (  X  ( S Alg ) ,  X  ( R )). We compare the value D Alg with the corresponding error of a random set of k reviews D R . After considering N = 1000 such random sets, we report the empirical p -value of algorithm Alg to be the fraction of times for which the random set exhibited a smaller error than the set reported by Alg . Formally, such an empirical p -value is computed as follows: In the above equation, D i corresponds to the error of the i -th random sample and I ( D Alg , D i ) is an indicator variable that takes value 1 if the error of the set reported by Alg is less than the error reported by the error of the i -th ran-A lg can be any of the three Iterative-Random , Greedy or Integer-Regression algorithms. dom sample of k reviews. Small values of p Alg are desir-able, as they indicate that Alg has a very low probability of outperformed by a random set of k reviews from R . The empirical p -values for the Greedy , Integer-Regression and Iterative-Random algorithms as a function of k are shown in Table 1. The values reported in the table are averages over all the corpora of every dataset.
 Table 1: Average empirical p -values for the so-lutions reported by Greedy , Integer-Regression and Iterative-Random for all datasets; average is taken over all items of a dataset.
We observe that the empirical p -values obtained for all our algorithms are either zero or close to zero. Therefore, the low-error solutions obtained by our algorithms cannot be attributed to randomness. For all datasets, the highest p -values are observed for k = 5. This trend is particularly pronounced in the BOOK dataset. This indicates that, for this dataset, it was more challenging for the algorithms to select a characteristic set with such a small size. This can be attributed to the ambiguity of the domain, which is less entertaining to objective judgments. Such ambiguity may lead to a more diverse opinion vector (i.e. with multiple positive and negative opinions per feature), and makes it more challenging to find a very small characteristic set of reviews. Nonetheless, the results show that the empirical p -values drop sharply as k increases. In fact, they reach zero or (near-zero) value for all datasets when k  X  15. Error values: The actual error values of the solutions reported by Greedy , Integer-Regression and Iterative-Random as a function of k are also shown in Table 2. Again the reported values are averages over all corpora in each dataset. The standard deviations within every dataset (not shown in the table) are values in the range [0 , 0 . 05] for all al-gorithms, datasets and values of k . The error values indicate all three algorithms achieve low errors that are reduced to near-zero as k is increased. Further, Greedy and Integer-Regression have a consistent advantage over Iterative-Random , in terms of both the absolute error values, and their ability to utilize larger values of k ; observe that, for larger values of k , both Greedy and Integer-Regression achieve lower errors than Iterative-Random .
 Error ratios: In order to further investigate the relation-ship between Greedy , Integer-Regression and Iterative-Random , we also conduct the following experiment. First, we collapse all the items from all six datasets into a single collection. For each value of k and each item, we compute E
G (resp. E IR ) as the ratio of the error of the solution re-ported by Greedy (resp. Integer-Regression ) to the error of the solution reported by Iterative-Random . We refer to both E G and E IR as the error ratios . Figure 3 shows the average values of these ratios for different values of k ; the average is taken over all items. The results demon-strate that both ratios are consistently less than 1, indi-cating that both Greedy and Integer-Regression achieve less error than Iterative-Random . In fact, as the value of k increases the values of both ratios drop. This indicates that the advantage of Greedy and Integer-Regression becomes more pronounced as the value of k increases. Another ob-servation is that Integer-Regression slightly outperforms Greedy , especially for larger values of k . This demonstrates that this algorithm is better at utilizing the flexibility of large values of k than Greedy , which simply extends the so-lution that it has obtained for smaller values of k . Table 2: Average error of the solutions reported by G reedy , Integer-Regression and Iterative-Random for all datasets; the average is taken over all items of a dataset.
T he goal of our user study is to demonstrate that the set of reviews reported by our methods is more appealing to the average user than the current state-of-the-art. To achieve this, we we randomly select 10 items from the M-P3 dataset. For the 10 review corpora that correspond to these items, we select representative sets of reviews by us-ing three approaches: Helpfulness , GroupCover , and our own Integer-Regression algorithm. The first one ranks the review by the standard helpfulness measure that is im-plemented as a feature in major review hosting-sites such as Yelp.com or Amazon.com ; we obtain the helpfulness scores of the reviews of our corpora while crawling. The second is the review-selection method proposed by Tsaparas et al. [16]. This method tries to maximize the number of features for which there is at least one positive and one negative opinion in the reported set. We use the Integer-Regression algo-rithm to represent our own selection paradigm, since it was shown to outperform the other algorithms in our other ex-periments, and is the one that gave the least-error solutions.
For each of the 10 items, we use the above three methods to select a set of 5 reviews. We chose to select no more than 5 reviews, to ensure that the human annotators would be able to complete the task in a reasonable amount of time. We asked the human annotators to rank the three sets from best (score 1) to worst (score 3). The criterion for the rank-ing was how well each set represents the entire corpus of reviews on the item. Clearly, it was impossible for the an-notators to process the tens or even hundreds of reviews of the entire review corpus in order to make a decision. Thus, Figure 3: Average error ratios E G a nd E IR for k  X  { 5 , 10 , 15 , 20 } . The reported ratios are averages over all items in the union of all six datasets. for each item, the annotator was shown the total number of positive and negative opinions expressed on each feature ( Price, Sound Quality, Battery Life, Connectivity, Design, Screen, Menu and Radio ) in the entire corpus. We also pro-vided annotators with the original link to the review corpus on Amazon X  X  website. In this way, we provided them with all the necessary information needed to determine their rank-ings. To conduct our survey, we created a HIT (Human Intelligence Task) on Amazon X  X  Mechanical Turks platform. We hired 40 annotators to work on our HIT. The sets were shown in a randomized order to the annotators, who were thus oblivious to which set correspond to each approach. Figure 4: User study: percentage of rankings for w hich different methods were ranked first by the human annotators.

Figure 4 shows, for each of the 10 items, the percentage of workers that chose the set reported by each approach as the best one (i.e. ranked as 1). The results illustrate that the human subjects had a clear preference to the sets reported by our approach. A high percentage of the annotators con-sistently reported our approach as the best one, leading to percentages as high as 77%. In fact, our set was selected as the best for all items, with the exception of items 3 and 4 (for which our approach was tied with helpfulness).
Figure 5 shows the average rank assigned to each ap-proach, for each of the 10 items. Recall that lower values are desirable (since smaller ranks indicate higher preference of users to the results of a method). This plot illustrates that our approach outperforms the two baselines. Items 3 and 4 are the only exceptions. For these, the reviews selected us-Figure 5: User study: average ranking of different m ethods as ranked by human annotators. ing the helpfulness scores were ranked slightly higher than ours. For all the other items, the average ranking achieved by our method was significantly lower than that of Helpful-ness and GroupCover . In conclusion, the results of the user study demonstrate the validity and superiority of our selec-tion paradigm, when compared to existing ranking criteria and state-of-the-art review-selection methods.
In this paper we introduced the problem of selecting a characteristic set of reviews from a given corpus. Our se-lection paradigm is a significant improvement over previous relevant work, since it is the first to ask for a set that respects the proportion of opinions on each feature (both positive and negative), as observed in the underlying corpus. We formal-ly define the Characteristic-Review Selection problem and prove that it is NP-hard both to solve and approximate. We propose three heuristic algorithms for selecting a char-acteristic review set, which we evaluate on a wide range of review datasets from different domains. The results indicate that our algorithms are consistently able to find a compact set of reviews that yields a highly accurate approximation of the set of opinions in the corpus. To demonstrate that our problem definition and solution is an important improve-ment over previous efforts, we perform a user study using the Amazon Mechanical Turks platform. The study reveals that users consistently prefer the set of reviews selected by our methods, as opposed to those selected by previous state-of-the-art methods. Our work can be easily incorporated into any review-hosting website, in order to provide users with a compact set of real reviews, that accurately represents the opinions expressed in the entire corpus.
 This research was supported by gifts from Microsoft, Ya-hoo! and Google, by the NSF award #1017529, and by the NSF grants CNS-0905565, CNS-1018266, CNS-1012910, and CNS-1117039. [1] S. Baccianella, A. Esuli, and F. Sebastiani. Multi-facet [2] X. Ding, B. Liu, and P. S. Yu. A holistic lexicon-based [3] M. Garey and D. Johnson. Computers and [4] A. Ghose and P. G. Ipeirotis. Designing novel review [5] M. Hu and B. Liu. Mining and summarizing customer [6] N. Jindal and B. Liu. Opinion spam and analysis. In [7] S.-M. Kim, P. Pantel, T. Chklovski, and [8] T. Lappas and D. Gunopulos. Efficient confident [9] J. Liu, Y. Cao, C.-Y. Lin, Y. Huang, and M. Zhou. [10] Y. Liu, X. Huang, A. An, and X. Yu. Modeling and [11] Y. Lu, P. Tsaparas, A. Ntoulas, and L. Polanyi. [12] S. Mallat. A Wavelet Tour of Signal Processing . [13] S. Mallat, G. Davis, and Z. Zhang. Adaptive [14] X. Meng and H. Wang. Mining user reviews: from [15] K. Shimada, R. Tadano, and T. Endo. Multi-aspects [16] P. Tsaparas, A. Ntoulas, and E. Terzi. Selecting a [17] O. Tsur and A. Rappoport. Revrank: a fully [18] J. Zhan, H. T. Loh, and Y. Liu. Gather customer [19] Z. Zhang and B. Varadarajan. Utility scoring of [20] L. Zhuang, F. Jing, X. Zhu, and L. Zhang. Movie
