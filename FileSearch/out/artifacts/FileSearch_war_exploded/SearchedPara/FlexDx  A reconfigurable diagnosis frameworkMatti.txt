 1. Introduction
Reliable real-time multiple fault diagnosis of dynamical systems in the presence of noise is of fundamental importance in many applications. The diagnosis problem typically consists of detecting and isolating faulty components given available sensor and actuator signals. Important challenges are real time issues, complexity issues when dealing with multiple faults, how to handle process dynamics, and noisy measurements. The main objective of this work is to describe FlexDx a diagnosis framework that performs multiple fault isolation for dynamic noisy systems using adaptive and reconfiguration techniques to lower the computational burden of the diagnostic system. 1.1. Problem background
In consistency-based diagnosis, sensor and actuator signals are compared to a formal description of the process, a model, to detect inconsistencies indicating faults. Inconsistency detection can for example be based on local propagation of observations, like solutions based on the well known general diagnostic engine (GDE) ( de Kleer, 1987; Hamscher et al., 1992 ). However, such solutions have limitations when diagnosing dynamical systems in other hand, in the automatic control and signal processing communities there exists a large volume of research directed at detecting faults in noisy dynamic systems, see e.g. the books Gertler (1998) , Patton et al. (2000) , Blanke et al. (2006) , and Basseville and Nikiforov (1993) . These approaches are not based on local propagation, but typically on a set of pre-compiled tests, module ( Gertler, 1998 ) to reach a diagnosis decision. A key topic in these approaches has been to study residual generator design for dynamical systems in noisy environments. However, efficient handling of multiple faults has not been a central topic in these works, as in the more AI-based literature ( de Kleer, 1987; Nyberg, 2006 ).

In several works, for example Cordier et al. (2004) , Nyberg and Krysander (2003) , Ploix et al. (2003) , and Frank and K  X  oppen-Seliger (1997) , it is noted that the fault isolation techniques from the AI literature can be used together with the pre-compilation techniques from the automatic control commu-nity. These observations makes it possible to combine the techniques from the automatic control community to handle noise and system dynamics, with the fault isolation techniques from the AI-community to handle multiple fault isolation. However, due to the inherent computational complexity of the multiple fault isolation problem, there are still open problems that need to be addressed.

The computational complexity of a diagnostic system based on pre-compiled tests mainly originates from two sources: complex-ity of the process model and the number of behavioral modes considered. A high resolution capability of distinguishing between faults, especially when multiple faults are considered, requires a large number of diagnostic tests ( Krysander, 2006 ). This also follows from the well known fact that the number of required minimal conflicts, which corresponds to triggered tests in a residual based approach, to solve a multiple fault diagnosis task grows exponentially in the number of faults in the system ( de
Kleer and Kurien, 2003 ). Also, the more complex the process model is, the more computationally expensive the execution of the diagnostic tests is. For example, if the model consists of non-linear dynamic equations, a typical test involves a non-linear observer which may be computationally expensive. Finally, since dynamical systems are considered here, tests must be recom-puted sufficiently often to capture fault dynamics and to get a fast and accurate fault detection and isolation. 1.2. Solution outline
Our basic idea to mitigate the computational burden is to exploit the fact that all pre-compiled tests are not needed at all times. For example, only a subset of the available tests are needed be used to further isolate the faulty component but they are not needed when the system is in fault free operation. As will be shown, substantial reduction in the computational burden can be achieved by exploiting this observation.

This approach is similar to works coping with the complexity of multiple fault diagnosis from the AI-community. There the complexity issues related to having many tests have been avoided by applying propagation techniques directly on the process model. The propagation is initiated with the observed measure-ments and then only explore the part of the model that might be inconsistent. Here we propose a similar technique, using pre-compiled tests instead of local propagation.

The proposed approach, a reconfigurable diagnosis framework called FlexDx, chooses which tests to run at a particular instant alarm is generated, then an updated set of diagnoses is computed and the set of active tests is reconfigured. It is shown how such an approach requires controlled ways of initializing the dynamic diagnostic tests and algorithms how to select the new tests to be started when a set of diagnostic tests has generated an alarm. To facilitate a thorough analysis of the approach, linear process models are used in the paper but the framework is not based on this model assumption, but can be extended to non-linear models if non-linear test techniques are used, e.g. Frank (1994) ,
Staroswiecki and Comtet-Varga (2001) , and Persis and Isidori (2001) . 1.3. Paper outline The reconfigurable diagnosis framework is introduced in
Section 2 and related work to the different components in the framework is discussed. To illustrate the properties of the approach, linear dynamical process models are used and the theoretical diagnosis background for such systems is pre-sented in Section 3. Methods for how to determine, in a specific situation, which tests should be run are treated in Section 4. An appropriate initialization procedure for dynamic tests is described in Section 5. The complete approach is exemplified on a dynamic system in Section 6, which, in spite of its relatively small size, clearly illustrates the complexity of the problem and the computational gain of the proposed approach. The diagnosis framework is implemented using DyKnow, a stream-based knowledge processing middleware framework ( Heintz, 2009 ), which is briefly described in Section 7. Finally the paper is concluded with a summary in Section 8. 2. FlexDx: a reconfigurable diagnosis framework
The main idea of this work is to reduce the overall computa-tional burden of a diagnostic system by utilizing the observation that all tests are not needed at all times. For example, when starting a fault free system, there is no need to run tests that are designed with the sole purpose of distinguishing between faults. which may be significantly fewer compared to the complete set of tests. When a test triggers an alarm and a fault is detected, appropriate tests are started to refine the diagnosis.
FlexDx uses a consistency-based approach to diagnosis when determining if a supervised system is working correctly ( de Kleer, diagnosis is incrementally refined by adding and removing tests in an iterative manner according to the following procedure: 1. Initiate the set of diagnoses. 2. Based on the set of diagnoses, compute the set of tests to be performed. 3. Compute the initial state of the selected tests. 4. Run the tests until an alarm is triggered. 5. Compute the new set of diagnoses based on the test results, then go to step 2.
 FlexDx represents all diagnoses with the minimal diagnoses, which has been shown useful when dealing with multiple fault no conflicts and the only minimal diagnosis is the no-fault mode
NF, i.e. the set of minimal diagnoses D issetto{NF}instep1.Step2 uses a function that given a set of diagnoses D returns the set of tests T to be performed to monitor whether a fault has occurred or to further explore the possible diagnoses. Step 3 initiates each of space form. To properly initialize such a residual generator, it is performed until at least one is violated and a test result is generated in the form of a set of conflicts ( de Kleer et al., 1992;
Reiter, 1987 ). Step 5 computes the new set of diagnoses D ,given the previous set of diagnoses and the generated set of conflicts.
This step can be performed by algorithms handling multiple fault diagnoses ( de Kleer, 1987; Nyberg, 2006 ).

Steps 4 and 5 are standard steps used in diagnostic systems and will not be described in further detail. Steps 2 and 3 are new steps, needed for dynamically changing the test set T , the details are given in Sections 4 and 5, respectively.

To implement an instance of the FlexDx framework, a number of issues have to be managed besides implementing the algorithms for each step and integrating them in a system. Each test is implemented by a residual generator, computing the residual given the measurements of the system, and a monitor that checks if the residual has triggered a violation of the test.
When a potential fault is detected, FlexDx computes the last known fault free time t f and the new set of residual generators to be started at time t f . To implement this, three issues have to be handled. First, the FlexDx instance must be reconfigured to replace the set of residual generators and their monitors. Second, the computation of the residuals must begin at time t f which will be in the past. Third, at the same time as FlexDx is performing tests on historic data, system observations will keep coming at their normal rate. How these issues are solved is described in Section 7.

A key step in the reconfiguration procedure outlined above is of tests is related to the selection of new measurement points. For example, in the general diagnostic engine ( de Kleer, 1987 )a solution is proposed where measurement points are selected in a sequential way such that, on average, the actual fault can be isolated using a minimum number of measurements. This is done by dynamically ranking the possible measurement points accord-ing to the expected information obtained from the measurement.
A related approach is found in Biteus et al. (2009) and Biteus (2007) where tests are ranked and selected according to some information criterion. The approach adopted here for test selection is fundamentally different and the objective is to find running these tests provides all information available in the model and observations given the current set of diagnoses.
Several works ( Efendic, 2006; Gertler, 1998; Korbicz et al., 2004 ) have considered test selection for achieving different objectives but contrary to this work no focus has been on on-line reconfiguration. Another related approach is presented in
Struss (1994) although the models and diagnosis techniques are different. Recently, works on on-line reconfiguration of the diagnostic system have appeared. For a related work, see for example Benazera and Trave  X  -Massuy es (2007) , where Kalman-filters are reconfigured based on diagnosis decisions. 3. Theoretical background
The diagnostic systems considered in this paper include a set the threshold can be set to one without loss of generality. It is assumed that the residuals are normalized such that a given false alarm probability p FA is obtained, i.e.
 P  X j r  X  t  X j 4 1 j NF  X  X  p FA  X  1  X 
The residuals are designed using a model of the process to be diagnosed. 3.1. The model
The model class considered here is linear differential-algebraic models. Although the presentation in this paper relies on results for linear systems, the basic idea is equally applicable to non-linear model descriptions.

There are several ways to formulate differential-algebraic models. Here, a polynomial approach is adopted, but any model description is possible, e.g. standard state-space or descriptor models. The model is given by the expression
H  X  q  X  x  X  L  X  q  X  w  X  F  X  q  X  f  X  V  X  q  X  v  X  2  X  time-shift operator q . The vector x contains all unknown signals, which include internal system states and unknown inputs. The vector w contains all known signals such as control signals and measured signals, the vector f contains the fault signals, and the vector v is white, possibly multidimensional, zero mean, unit covariance Gaussian distributed noise.

To guarantee that the model is well formed, it is assumed that z A
C . This assumption assures, according to Nyberg and Frisk there exists a solution to the model equation (2). 3.2. Residual generation
Residuals are used both to detect and isolate faults. This task can be formulated in a hypothesis testing setting. For this, let f denote both the fault signal and the corresponding behavioral mode ( de Kleer and Williams, 1992 ) of a single fault. Let F be the set of all single faults in model (2).

A pair of hypotheses associated with a residual can then be stated as H 0 : f i  X  0 for all f i A C H 1 : f i a 0 for some f i A C where C D F is the subset of faults the residual is designed to detect. This means that the residual is not supposed to detect each sensitive to different subsets C of faults, fault isolation is possible. This isolation procedure is briefly described in Section 3.3.

In the literature there exists several different ways to formally introduce residuals ( Gertler, 1998; Blanke et al., 2006 ). In this paper an adapted version of the innovation filter defined in Nikoukhah (1994) is used. For this, it will be convenient to consider the nominal model under a specific hypothesis. The nominal model under hypothesis H 0 above is given by (2) with observations w , consistent with the nominal model (2) under hypothesis H 0 , it holds that lim t -1 r  X  t  X  X  0.

Now, consider again the stochastic model (2) where it is clear that a residual generated with a nominal residual generator will be subject to a noise component from the process noise v .A nominal residual generator under H 0 is then said to be a residual generator for the stochastic model (2) if the noise component in the residual r is white Gaussian noise.
 as defined above, for the stochastic model (2) can be written as R  X  q  X  X  Q  X  q  X  L  X  q  X  internal form of the residual is given by r  X  Q  X  q  X  L  X  q  X  w  X  Q  X  q  X  F  X  q  X  f  X  Q  X  q  X  V  X  q  X  v  X  3  X  Thus, the fault sensitivity is given by r  X  Q  X  q  X  F  X  q  X  f  X  4  X  and the statistical properties of the residual under H 0 by r  X  Q  X  q  X  V  X  q  X  v  X  5  X  A complete design procedure is presented by Nikoukhah (1994) for state-space models and by Frisk (2001) for models in the form (2). The objective here is not to describe a full design procedure, but it is worth mentioning that a design algorithm can be made fully automatic, that the main computational steps involve a null-space computation and a spectral factorization, and that the resulting residual generator is a basic dynamic linear filter. 3.3. Computing the diagnoses r is sensitive to the faults with non-zero transfer functions. Let C r triggers an alarm, at least one of the faults in C must have occurred and the conflict C is generated ( Reiter, 1987 ).
Now we can relate the test results to a diagnosis. Let a set of single faults b D F represent a system behavioral mode with the meaning that f i a 0 for all f i A b and f j = 0 for all f faults b will with some abuse of notation sometimes be referred to as a behavioral mode. This is also in accordance with the notation used in for example de Kleer (1987) and Reiter (1987) . The set of all system behavioral modes is then represented with the power set of F .

In short, the behavioral mode b is a diagnosis if it can explain all generated conflicts, i.e. if b has a non-empty intersection with is considered a minimal diagnosis if no proper subset of b is a diagnosis ( de Kleer, 1987; Reiter, 1987 ). Algorithms to compute all minimal diagnoses for a given set of conflicts, which is equivalent to the so-called minimal hitting set problem, can be found in for example de Kleer (1987) and Reiter (1987) . The following example illustrates the main principle.
 that residual r i is sensitive to fault f j r 1 XX r 2 XX r 3 XX
If residuals r 1 and r 2 trigger alarms, then conflicts C
C ={ f f and f 3 cannot be 0. Now, for a set of faults to be a diagnosis it must then explain both these conflicts. It is straightforward to verify that the minimal diagnoses in this case are b 1 ={ f b ={ f 1 , f 2 }. 4. Test selection
There are many possible ways to select the set of tests T given a set D of minimal diagnoses. The method used by FlexDx relies on basic principles in consistency-based diagnosis based only on the deterministic properties of (2).

A fundamental task in consistency-based diagnosis is to compute the set of consistent modes given a model, a set of possible behavioral modes, and observations ( de Kleer, 1987 ). The design goal of the test selection algorithm is to perform tests such that the set of consistent modes is equal to the set of diagnoses computed by the diagnostic system. 4.1. Consistent behavioral modes
The deterministic behavior in a behavioral mode b is described by (2) when v =0, f i a 0 for all f i A b , and f j =0 for all f observations consistent with b is consequently given by
This means that a mode b is consistent with the deterministic part of model (2) and an observation w if w A O  X  b  X  . Note that f required to be non-zero in mode b , but this is not required in the distinguish infinitely small faults f i a 0 from the case when f
To capture this property in the deterministic analysis here we include also the case when f i =0in O ( b ).
 The design goal can now be formulated in terms of the sets behavioral modes. As mentioned in Section 2, we will use minimal diagnoses to represent all diagnoses. This is possible since (6) the minimal consistent modes remain consistent when new observations are processed. 4.2. Tests for checking model consistency
Next, we will describe how tests can be used to detect if w = 2 O  X  b  X  . Let T be the set of all available tests and let r be the residual corresponding to test t i .

A residual generator checks the consistency of a part of the complete model. To determine which part, only the deterministic model needs to be considered. It can be shown that residual r set of consistent observations for tests in a similar way as for models, we define
O  X  t  X  X f w j x i  X  q  X  w  X  0 g X  7  X 
Now, we can characterize all test sets T that are capable of detecting any inconsistency between an observation w and the assumption that w A O  X  b  X  . For this purpose, only tests t means that a test set T is capable of detecting any inconsistency of w A O  X  b  X  if and only if
O  X  b  X  X 
A trivial solution to (8) is T ={ t } where O ( t )= O ( b ). 4.3. The set of all available tests
If T is not capable of checking the consistency of b , then no subset of tests will be capable of doing this either. Hence, this approach puts requirements on the entire set of tests T .By applying the approach to a model consisting of the considered set of tests, a diagnostic system with the same diagnosis capability as use two different types of test sets T fulfilling (8) for all modes b A B . These are introduced by the following example.

Example 2. Consider the model x  X  t  X  1  X  X  a x 1  X  t  X  X  w 1  X  t  X  X  f 1  X  t  X  x  X  t  X  X  x 1  X  t  X  X  f 2  X  t  X  w  X  t  X  X  x 1  X  t  X  X  f 3  X  t  X  w  X  t  X  X  x 2  X  t  X  X  f 4  X  t  X  X  9  X  where x i are unknowns, w i known variables, a a known parameter, and f i the faults. There are 2 4 modes and the set of observations consistent with each mode is
O  X  |
 X  X  w
O  X f f g X  X f w j w 2  X  t  X  X  w 3  X  t  X  X  0 g
O  X f f g X  X  O  X f f 4 g X  X  O  X f f 2 ; f 4 g X  X f w j w 1  X  t  X  X  a w 2
O  X f f g X  X f w j w 1  X  t  X  X  a w 3  X  t  X  w 3  X  t  X  1  X  X  0 g O  X  b  X  X  R 3 for the remaining modes :
The behavioral models for the 10 last modes b do not contain any redundancy and the observations are therefore not restricted, i.e. here expressed in the same form as for tests, that is with linear differential equations in the known variables only. Any set described as in (6) can be written in this form ( Polderman and Willems, 1998 ).
 behavioral model containing redundancy. For the example, T consists of four tests t i such that O  X  t 1  X  X  O  X  |  X  , O ( t w
A O  X  |  X  , two linear residuals are needed, which is the degree of redundancy of the model. These two residuals can be combined in a positive definite quadratic form to obtain a scalar test quantity.
When stochastic properties are considered, the quadratic form is chosen such that the test quantity conforms to a w 2 distribution.
Tests for models with a high degree of redundancy can be complex, and the second type of test set T 2 includes only the tests for the behavioral models with degree of redundancy 1. For the example, T 2  X f t 2 ; t 3 ; t 4 g andbynotingthat O  X  |  X  X  O  X  t consistency of w A O  X  |  X  .In Krysander (2006) it has been shown under some general conditions that T 2 fulfills (8) for all modes b 4.4. Test selection methods
We will exemplify methods that given a set of minimal b
A D . An optional requirement that might be desirable is to select such a test set T with minimum cardinality. The reason for not requiring minimum cardinality is that the computational com-plexity of computing a minimum cardinality solution is generally much higher than to find any solution.

A straightforward method is to use the first type of tests and not require minimum cardinality solutions. Since this type of test redundancy, it follows that a strategy is to start the tests corresponding to the minimal diagnoses in D .

Example 3. Consider Example 2 and assume that the set of minimal diagnoses is D  X f | g . Then it is sufficient to perform test t
If the set of minimal diagnoses are D ={{ f 2 },{ f 3 },{ f is T ={ t 3 , t 4 }.Forthisexample,thisstrategyproducestheminimum cardinality solutions, but this is not true in general.
A second method is to use the second type of tests and for example require a minimum cardinality solution. The discussion of the method will be given in Section 6 where this method has been applied to a larger example. 4.5. Relaxing the design goal
The design goal to perform tests such that the set of consistent modes is equal to the set of diagnoses is an ambitious goal that may require many tests. Three different principles for relaxing the approach will be discussed next.

First, the test selection methods are not limited to take the set of all minimal diagnoses as input. If the number of minimal incorporated to reduce the number of diagnosis and thereby also the number of selected tests. Minimal cardinality diagnosis or most probable diagnosis given some a priori probabilities are examples of possible focusing strategies.

The second type of relaxation reduces the total number of tests in T . It may require a large number of tests to fulfill (8) for all modes b A B and sometimes it is not interesting to consider unlikely modes including a large number faulty components. By considering a subset of modes B 0 B , the goal can be modified as follows. Tests should be performed such that among the modes in B 0 exactly the consistent ones are diagnoses. This means that T is When running the diagnostic system, the test selection T D to fulfill (8) for all minimal diagnoses in B 0 .

A third option is to use a given set of tests T 0 , that is not required to fulfill (8) for any modes. The goal can then be formulated as follows. Given the current minimal diagnoses, perform the tests in T D T 0 such that the diagnoses computed with the selected subset of tests would be equal to the diagnoses computed using all tests. This means that, given a set of minimal diagnoses D , the test selection T D T 0 has to fulfill for each b A D .

The three relaxations can be used individually or mixed. As an example of how the three relaxations can be combined, we can the single fault modes, and the double fault modes in B 0 , and use the minimal cardinality focusing strategy. Then, given a set of all minimal cardinality diagnoses b that belong to B 0 .Which relaxation, or which combination of relaxations, that are appro-priate for a specific application must be determined case by case. 5. Initialization
When a new test selection has been made, the new tests have to be initialized. Since information about faults sometimes are only occurred, otherwise valuable information could be missed. It is also residuals can deliver test results immediately. Therefore, the initialization following a new test selection consists of: 1. Estimate the time of the fault from the alarming test(s). 2. Estimate the initial condition for each new test.
 Both these steps require the use of historical data, which therefore have to be stored. The fault time estimation will use the historical residuals from the triggered test, while the initial condition estimation uses the measured data from the process before the fault occurred. In case not enough historical data are available, it is reasonable to use all available data. In such a case, one may expect some degradation in detection performance compared to running all tests at all times. 5.1. Estimating the fault time
There are many possibilities to estimate the fault time. See for example Page (1954) and Basseville and Nikiforov (1993) for standard approaches based on likelihood ratios. Here, a window-based test has been chosen. It should be noted, however, that for the given framework, what is important is not really to find the exact fault time, but rather to find a time-point before the fault has occurred. The estimated time-point will be denoted by t
Given a number of residuals from an alarming test, over a sliding window, i.e.
 S  X  t  X  X  1 s 2
If the residual generator is designed such that under the null can be used to test whether this null hypothesis has been rejected at different time-points by a simple w 2 test. The length  X  of the sliding window is selected according to the expected fault responses. A slowly increasing fault response requires a long time-window while short time-windows are preferable for impulses.

Since it is preferable to get an estimated time-point that occurs w 2 test should be chosen such that the null hypothesis is fairly the time-point at the beginning of the sliding window is used. 5.2. Estimating the initial condition
Having found t f , the next step is to initialize the state of the new residual generator. The method used here considers a time-
Consider the following residual generator: x  X  t  X  1  X  X  Ax  X  t  X  X  Bw  X  t  X  r  X  t  X  X  Cx  X  t  X  X  Dw  X  t  X  X  12  X  data (inputs and outputs) from the process model and v ( t ) is the
Gaussian noise. In fault free operation, there is a state sequence x ( t ), such that the output r ( t )=0if v ( t )=0, x  X  t  X  1  X  X  Ax 0  X  t  X  X  Bw 0  X  t  X  0  X  Cx 0  X  t  X  X  Dw 0  X  t  X  X  13  X  x ( t x  X  t  X  X  A k 1 x 0  X  t f k  X  1  X  X  F w
W  X  A k 1 x 0  X  t f k  X  1  X  X  F w  X  W D V V  X  X  14  X  where
F  X  X  A k 2 BA k 3 B ... ABB 0
W  X  2 6 4
V  X  2 6 4
In a similar manner, the second line of (13) gives 0  X  R x x 0  X  t f k  X  1  X  X  R w  X  W D V V  X  X  15  X  where
R  X  2 6 6 6 4 and since the residual generator is observable, it follows that R left with the full rank matrix  X  R "# we get x  X  t f k  X  1  X  X  X  R T x R x  X  1 R T x R w  X  W D V V  X  X  16  X  0  X  N that x  X  t  X  X  X  A k 1  X  R T x R x  X  1 R T x R w  X  F w  X  X  W D V V  X  X  18  X  Assuming that the distribution of V is known, say, V N  X  0 ; S (17) gives
E  X  V j W  X  S V D T V R T w N T R and this together with (18) gives the estimate ^ x  X  t
 X  X  E  X  x 0  X  t f  X j W 5.3. Determining the length k of the time-window
The choice of k is made in advance, based on the computed
Fig. 1 exemplifies how the standard deviation of an initialized residual may vary. The thick line in the bottom indicates the stationary standard deviation that the standard deviation of the initialized residual will converge to. The four curves above show the standard deviations obtained for indicated values of k . The larger k is, the more accurate initial state estimation ^ x
Hence, k can be chosen via a trade-off between minimizing the additional overhead that the computations in Section 5.2 represent and minimizing the maximum probability of false alarms during the initial time steps as follows. 0.5 1.5 2.5  X  (t)
Let s r be the maximum acceptable residual standard deviation which corresponds to a maximum acceptable probability of false
Note that k implicitly defines the matrices. Since W = W 0 the initial covariance of the estimated state ^ x 0  X  t f
S 0  X  t f  X  X  KD V S V D and the standard deviation of r ( t f )by  X  t  X  X  for t = t f .

Fig. 1 shows that the maximum standard deviation s r  X  t  X  for t covariance and residual standard deviation have to be computed for t Z t f based on
S 0  X  t  X  1  X  X  A S ^ x 0  X  t  X  A and (21). The k is selected as the smallest k such that  X  t  X  r s r  X  22  X  for all t Z t f . 6. Example
To illustrate the FlexDx framework, let us consider the simulated example system shown in Fig. 2 , where a DC-servo is connected to a flywheel through a rotational (damped) spring. The system dynamics can be described by
J  X  y 1  X  t  X  X  ku  X  t  X  a 1 _ y 1  X  t  X  M s  X  t  X 
M  X  t  X  X  a 2  X  y 1  X  t  X  y 2  X  t  X  X  X  a 3  X  _ y 1  X  t  X  _ y 2
J  X  y 2  X  t  X  X  a 4 _ y 2  X  t  X  X  M s  X  t  X  where u ( t ) is an input signal controlling the torque from the motor (with a scaling coefficient k = 1.1), y 1  X  t  X  and y angles of the motor axis and the flywheel, respectively, and M is the torque of the spring. The moments of inertia in the motor is
J = 1 and for the flywheel J 2 = 0.5. The parameters a 1  X  1 and  X  0 : 1 determine the viscous friction at the motor and flywheel, respectively, while a 2  X  0 : 05 is the spring constant and the viscous damping coefficient of the spring.

As outputs, the motor axis angle and velocity, and the angle of the flywheel are measured. We will design the diagnostic system The augmented system model becomes
J  X  y 1  X  t  X  X  k  X  u  X  t  X  X  f 1  X  t  X  X  a 1 _ y 1  X  t  X  M s  X  t  X 
M  X  t  X  X  a 2  X  y 1  X  t  X  y 2  X  t  X  X  X  a 3  X  _ y 1  X  t  X  _ y 2
J  X  y 2  X  t  X  X  a 4 _ y 2  X  t  X  X  M s  X  t  X  X  f 3  X  t  X  y  X  t  X  X  y 1  X  t  X  X  f 4  X  t  X  X  v 1  X  t  X  y  X  t  X  X  _ y 1  X  t  X  X  f 5  X  t  X  X  v 2  X  t  X  y  X  t  X  X  y 2  X  t  X  X  f 6  X  t  X  X  v 3  X  t  X  Here, v i ( t ), for i = 1,2,3, are measurement noise terms.
Since the diagnosis framework will work on sampled data, the model is discretized before designing the tests using a zero-order hold assumption. The noise is implemented as i.i.d. Gaussian noise with variance 10 3 . By using the second type of tests described in Section 4.3 for the discretized system, a set of 13 tests were needed. Their fault sensitivity is shown in Table 1 . The false alarm probability is set to 10 3 and the maximum false performance, the number of samples needed for initiating the 13 tests are, according to the method proposed in Section 5.3, 38, 86, 65, 92, 23, 40, 52, 69, 41, 42, 113, 82, and 108, respectively. The tests will in the following simulations be combined with the second test selection method described in Section 4.4. 6.1. Test reconfiguration
To show how the diagnostic system is reconfigured during a fault transient, we will describe what happens when the fault f occurs at t = 100 in a simulated scenario. The course of events is described in Table 2 .

Each row in the table gives the most important properties of one iteration in the FlexDx procedure given in Section 2. In one such iteration, the set of active tests are executed on observations collected from time t f to t a . The column Minimal Diagnoses shows a simplified representation of the minimal diagnoses during the corresponding phase. For example 25 represents the mode { f Each iteration ends when one or several of the active tests trigger an alarm. These are shown in bold type.
 Let us take a closer look at the steps of the FlexDx procedure. Step 1 initiates the set of minimal diagnoses to D = {NF}, which is shown in row 1. The degree of redundancy of the behavioral model for NF is 3, and therefore three tests are needed to check if w A
O  X  NF  X  is consistent. Step 2 computes the first, in lexicogra-phical ordering, minimum cardinality solution to (8), which is the test 5 triggers an alarm at time t a = 102.6. From the fault sensitivity of residual r 5 given in Table 1 , C ={ f 1 , f conflict which is the output of step 4. The new set of minimal diagnoses, computed in step 5, is shown in the second row.
Returning to step 2, the degree of redundancy for each of the behavioral models corresponding to minimal diagnoses is 2, and therefore at least two tests are needed to check the consistency of each of them. The minimum cardinality test set computed in step 2is T = {1,3,10,13}. This set is shown in row 2. Tests 1 and 3 check the consistency of { f 1 }, 1 and 10 the consistency of { f the consistency of { f 5 }, and 10 and 13 the consistency of { f tests T are estimated using observations sampled in a time is estimated to be 100.6 due to slow fault response in test 11. A correct estimation can be obtained by tuning the length of the time-window and the threshold of the fault time estimator (11).
One straightforward option to further decrease the computa-tional load is to modify the approach to focus on single faults, as
Table 2 are the same since the minimal diagnosis only include faults appear in the minimal diagnoses. If only single fault diagnoses are considered, only the diagnoses { f 1 } and { f
Then, by disregarding the multiple fault diagnoses, it is concluded that tests T ={1,2,8} should be started instead of the set
T ={1,2,6,7,8,11,12} which is needed when also considering the the only single fault diagnosis is { f 1 } and we have finalized the isolation procedure. From this it is clear that the number of tests needed can be further reduced by focusing on single faults. If the shifted to double faults, and so on. 6.2. Reduction of the computational burden
Let us consider a second simulated scenario, where the system = 200, f 5 is set to 0.1. The residuals computed by the diagnostic system are shown in Fig. 3 . It is noteworthy that the residuals have not been computed for all time-points and thereby the expected cost reduction has been achieved. It is difficult to quantify the reduction in computational cost. In this case, where all residual generators are linear filters, one possibility is to evaluate the computational cost by examining the number of multiplication and addition operations used to compute the residuals. Using that approach in this simulation, by comparing the computational cost for a diagnostic system running all tests at all times with the computational cost with the proposed system without using focusing, a 98% reduction of the computational cost is obtained for the simulated scenario. This number is in itself not an indication of expected computational gain in a typical application. The reduction strongly depends on for example failure rates, degree of redundancy, complexity of the system model, and fault isolation requirements. The key point is that not all tests are run at all times, and during fault free operation, typically only a few tests are needed which typically results in a significant reduction in the computational load.

The largest number of tests is performed during the fault transitions which last only a short period of time. Although the computational load decreases with the approach, during fault may still be too large with a too high computational load. Since the FlexDx framework includes the possibility to store observa-tional data and run the tests in an asynchronous way, it is straightforward to serialize the test computations. Thereby, the need for high computational power can be traded against 0 100 200 300  X 2 0 2 0 100 200 300  X 2 0 2 0 100 200 300  X 2 0 2 increased memory usage and increased time for detection and isolation. 7. DyKnow
To implement an instance of the FlexDx framework, a number of issues have to be managed besides implementing the described algorithms and integrating them in a system. When a potential fault is detected, FlexDx computes the last known fault free time t and the new set of residual generators to be monitored starting at
FlexDx instance must be reconfigured to replace the set of residual generators and their monitors. Second, the computation of the residuals must begin at time t f in the past. Third, at the same time as FlexDx is computing residuals and performing tests on the historic data, system observations will keep coming at their normal rate.

To manage these issues, FlexDx is implemented using DyKnow, a stream-based knowledge processing middleware framework for processing asynchronous streams of information ( Heintz, 2009 ).
Even though FlexDx could have been implemented with a dedicated solution with less overhead there are a number of benefits of using DyKnow. First, it provides solutions to the mentioned issues within an existing framework. Second, DyKnow provides a distributed infrastructure which allows sensors and other components to be hosted on many different computers in a network. This can be used both to collect data from distributed sensors and to distribute computations in the case were no computer is powerful enough.

DyKnow provides both a conceptual framework and an implementation infrastructure for integrating a wide variety of components and managing the information that needs to flow between them. It allows a system to incrementally process low-level sensor data and generate a coherent view of the environ-ment at increasing levels of abstraction. Due to the need for incremental refinement of information at different levels of abstraction, we model computations and processes within the knowledge processing framework as active and sustained knowl-edge processes . The complexity of such processes may vary greatly, ranging from simple adaptation of raw sensor data to controllers to diagnosis algorithms.

The system being diagnosed by FlexDx is assumed to be synchronous. At the same time the diagnosis procedure is asynchronous, jumping back and forth in time trying to figure out which fault has occurred. This requires knowledge processes to be decoupled and asynchronous to a certain degree. In DyKnow, this is achieved by allowing a knowledge process to declare a set of stream generators , each of which has a label and can be subscribed to by an arbitrary number of processes. A subscription can be viewed as a continuous query, which creates a distinct asynchronous stream onto which new data are pushed as it is generated. Each stream is described by a declarative policy which defines both which generator it comes from and the constraints on the stream. These constraints can for example specify the maximum delay, how to approximate missing values or that the stream should contain samples added with a regular sampling period. Each stream created by a stream generator can have different properties and a stream generator only has to process data if it produces any streams. The contents of a stream may be seen by the receiver as data, information, or knowledge.
A stream-based system pushing information easily lends itself available. This minimizes the processing delays, compared to a query-based system where polling introduces unnecessary delays in processing and the risk of missing potentially essential updates as well as waste resources. This is a highly desired feature in a diagnostic system where faults should be detected as soon as possible.

For the purpose of modeling, DyKnow provides four distinct types of knowledge processes: primitive processes, refinement processes, configuration processes, and mediation processes. To introduce these processes and to describe how the three issues introduced by FlexDx are solved, we will use a concrete FlexDx instance as an example. An overview of the processes and streams is shown in Fig. 4 .

Primitive processes serve as an interface to the outside world, connecting to sensors, databases, or other information sources that in themselves have no explicit support for stream-based knowledge processing. Such processes have no stream inputs but provide a non-empty set of stream generators. In general, they tend to be quite simple, mainly adapting data in a multitude of external representations to the stream-based framework. For example, in FlexDx the stream of observations of the system being diagnosed is provided by a primitive process System.

The second process type to be considered is the refinement process , which takes a set of streams as input and provides one or more stream generators producing refined, abstracted, or otherwise processed values. In FlexDx there are four refinement processes, as seen in Fig. 4 :
ResidualGenerator  X  Computes the residual for a particular test from system observations. The residual is initialized as described in Section 5.

ResidualMonitor  X  Monitors a residual and checks whether it has triggered a test. This can either be a simple threshold check or a more elaborate test which checks properties of the residual over time, such as if it has been above or below the threshold for more than five consecutive samples. If a test has been triggered the process computes the last known fault free time, which is the output of the process.

Diagnoses  X  Computes the new set of diagnoses each time a test has been triggered.

TestSet  X  Computes the new set of residual generators to be monitored when the set of diagnoses changes.
 streams as input but produces no new streams. Instead, it enables dynamic reconfiguration by adding or removing streams and processes. In FlexDx a configuration process is required to handle the first issue, to be able to reconfigure the set of residuals and tests that are computed.

CreateTests  X  Updates the set of residual generators and monitors as the set of tests changes. Each test consists of two refinement processes, one to compute the residual and one to monitor the test on the residual. In order to manage the second issue, that residuals are computed starting at the last known fault free time.
The input to a residual is a stream which begins at this time-point. This is part of the policy the configuration process uses to set up the new residual generat or process. Creating streams partially consisting of historic data is a DyKnow feature.
Finally, a mediation process generates streams by selecting or collecting information from other streams. Here, one or more of the inputs can be a stream of labels identifying stream generators to which the mediation process may subscribe. This allows a different type of dynamic reconfiguration in the case where not all potential inputs to a process are known in advance or where one does not want to simultaneously subscribe to all potential inputs due to processing costs. FlexDx uses a mediation process to collect the detected conflicts.

ConflictSetMediator  X  Subscribes to the output of each of the tests and aggregates these to a single stream. When tests are added or removed the current set of subscriptions is updated accordingly. The output of this process is a stream of pairs, each pair containing the identifier of the test that was triggered and the last known fault free time for the corresponding residual.

Without any specific termination condition FlexDx will run as long as the system produces output and there are tests that have not been violated yet. It is possible to stop earlier, for example when there is a unique consistent single fault.

To give a concrete example of a run of the system, consider the example from Section 6 as described in Table 2 . When the system is started, tests 1, 2, and 5 are created by CreateTests. These are computing the residuals and performing tests from time 0 to 102.6, when test 5 is triggered. Then the refinement process for test 5 computes the last known fault free time to 98.9. Using this information Diagnosis computes the set of minimal diagnosis to {1,3,5,6} and TestSet the new set of tests to {1,3,10,13}. The old tests 2 and 5 are removed and the new tests are added by
CreateTests. All of the tests are computed from time 98.9 until time 102.7 when test 13 is triggered, which means that they are computed from historic data until time 102.6. In this manner the set of tests is updated one more time before concluding that f the only consistent single fault.

The FlexDx algorithms are implemented in Matlab and inte-grated through code generation into DyKnow which is implemen-ted in C++ using CORBA as a communication infrastructure. 8. Conclusions
FlexDx an implemented reconfigurable diagnosis framework is proposed. It reduces the computational burden of performing multiple fault diagnosis by only running the tests that are currently needed. This involves a method for dynamically starting new tests. An important contribution is a method to select tests such that the computational burden is reduced while maintaining the isolation performance of the diagnostic system. Key compo-nents in the approach are test selection and test initialization. To illustrate that the general framework can be instantiated, specific algorithms for diagnosing linear dynamical systems have been developed for each component.
 Implementing a reconfigurable diagnosis framework such as
FlexDx introduces a number of interesting issues. First, FlexDx must be reconfigured to compute the new set of tests each time the set changes. Second, these computations must begin at the last known fault free time, which will be in the past. Third, at the same time as FlexDx is performing tests on historic data, system observations will keep coming at their normal rate. To handle these issues FlexDx is implemented using DyKnow, a stream-based knowledge processing middleware framework.

In the given example, the proposed approach has shown a significant reduction of the computational burden for a relatively small dynamical system. For systems with a high degree of redundancy, i.e. systems for which there exists many possible tests, the reduction can be expected to be even higher. Systems with low failure rate are also a class of systems where the approach can be expected to be advantageous, since then typically only a small subset of the tests are required to run continuously, rendering a significant reduction in computational burden. References
