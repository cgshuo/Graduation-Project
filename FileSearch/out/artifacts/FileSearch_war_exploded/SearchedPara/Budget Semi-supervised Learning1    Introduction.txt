 Previous studies on semi-supervised learning almost neglect the fact that although there exist abundant or even unlimited unlabeled data, the computational resource that can be used is generally not unlimited. In this paper, we propose to study budget semi-supervised learning , that is, semi-supervised learning with a resource budget. Roughly, given a labeled data set L and a large unlabeled data set U , for a concerned semi-supervised learning algorithm Algo , a computational resource  X  is needed to exploit all the data in U , yet the available resource is only  X  a which is much smaller than  X  ; trytoenable Algo to adapt to  X  a .

Here, the budget can be memory storage, computational time cost, etc. For example, assume that a storage of 10 6  X  10 6 matrix is required for Algo to exploit all avail-able unlabeled data, yet the memory stora ge available is only able to accommodate a 10 5  X  10 5 matrix. Ideally, effective budget semi-supervised learning algorithms should be able to adjust their behaviors considering the given resource budget. Roughly, the more resource, the more exploitation on unlabeled data.

There were some consideration on limited budget of computational resource in some other fields, but in the area of semi-supervised learning, to the best of our knowledge, algorithm, cluster kernel [1], as an example, we will show that the problem of budget semi-supervised learning can be tackled by a simple yet effective method which exploits advantages of known techniques and is validated in experiments.

The rest of this paper is organized as follows. Section 2 briefly introduces the cluster kernel algorithm [1]. Section 3 presents the Lank method. Section 4 reports on experi-mental results. Finally, Section 5 concludes.
 u , n = l + u . For simplicity, assume that x  X  R m and y  X  X  0 , 1 } . We can construct a Gaussian kernel matrix K by considering the pairwise affinity of the data points, where nearby data points are assigned with relatively large edge weights. Now we consider the following cluster kernel algorithm [1]: 1. Let D denote the diagonal matrix whose elements are D ii = j K ij , and construct 2. Compute the eigen-decomposition L = U X U T , and assume that the eigenvalues The transfer function  X  can take different forms. Here, the poly-step transfer function which has achieved the best performance in [1] is adopted:
For an unseen test example x , we can approximate it with a liner combination of the labeled and unlabeled training examples by where v i = K ( x , x i ) since K is Gaussian, and  X  is the feature map corresponding to K ,i.e., K ( x , x T )=(  X  ( x )  X  ( x T )) . Thus, The storage cost of the cluster kernel algorithm is O ( n 2 ) and the computational cost is roughly O ( n 3 ) . Now, suppose we do not have unlimited resource, e.g., we have a storage budget which is much smaller than O ( n 2 ) , what can we do?
The large storage is caused by the use of a full-connection graph. If we consider a k -graph where each node is only connected to its k -nearest neighboring nodes, the new [2,5]. We call this as k -approx . This method is reasonable since in most cases the affinity among neighboring examples is important while that among far examples are not very meaningful. To ensure that the resulting matrix is symmetric, we simply set the symmetric component of any non-zero component to the same non-zero value, and thus, for the worst case the storage is O (2 kn ) .

The 2nd step of the cluster kernel algorithm, however, is still expensive in storage even after using k -approx. In addition, using k -approx alone could not significantly reduce the computational cost. Note that K is large and symmetric, and assume that the m ( m n ) largest eigenvalues and the corresponding eigenvectors are sufficient. Thus, we can solve the problem through the famous Lanczos method [4] and reduce the computational cost to O ( mn 2 ) . Given a symmetric n  X  n matrix K and an n -dimensional vector  X  z having unit 2-norm, the Lanczos method works as follows: The method computes an m  X  m symmetric tridiagonal matrix T m with the property that the eigenvalues of K contains the eigenvalues of T m . The diagonal and subdiagonal altered during the entire process. If K has an average of about k non-zero values per row then approximately (2 k +8) n flops are involved in a single Lanczos step. Upon termination the eigenvalues of T m can be found using the symmetric tridiagonal QR algorithm [3]. The main computational task is the matrix-vector multiplication K u with whole Lanczos process can be implemented with just two n -vectors of storage. 3.1 Analysis The k -approx. First, by using k -approx we get a matrix K ( k ) which is different from K . If from K ( k ) and K we can solve the same eigenvectors, the remaining steps of the cluster kernel algorithm are almost untouched. So, we can analyze the influence of the use of k -approx by studying the eigenvectors solved from K ( k ) and K .
Once K is got, D is determined and L = D  X  1 / 2 KD  X  1 / 2 can be derived. Assume that the eigen-decomposition of L is given by L = U X U T = n i =1  X  i u i u T i ,where  X  =diag(  X  i ) is a diagonal matrix whose diagonal elements are the eigenvalues of L , and U =[ u 1 u 2  X  X  X  u n ] is an orthogonal matrix whose i th column is the eigenvector corresponding to  X  i . Assume that the eigenvalues are ordered as  X  1  X  X  X  X  X  X   X  n . Suppose we get K ( k ) by applying k -approx with a specific k value, the eigenvalues of L bounds for eigenvalues between L and L ( k ) : where the following matrix decompositions are used:
When k is large ( E ( k ) F is small), the left-hand of Eq. 4 is small, and thus the about the relative perturbation bounds for eigenvectors between L and L ( k ) : where rg(  X  ) expresses the relative gap,  X  ( k ) denotes diagonal matrix with canonical angles between the subspace generated by Q 1 =[ u i 1  X  X  X  u i l ] and that generated by Q 1 =[ u L = Q 1 Q 2 the corresponding partitions [7]. When k is large, the left-hand of Eq. 5 is small, and thus the canonical angles between the subspaces Q 1 and Q ( k ) 1 is small. The Lanczos Process. Given a starting vector  X  z , the Lanczos process will generate successively an orthonormal basis w 1 ,  X  X  X  , w m of a subspace. The vectors w i  X  X  form have W T m KW m = T m ,sothat T m is the matrix representation of the projection of K onto the Krylov subspace E m with respect to the basis W m =[ w 1 , w 2 ,  X  X  X  , w m ] . Specifically, the approximate solution z m of the linear system K z = v (or z = K jector onto E m , this means that the Lanczos process solves the approximate problem: we state how well the result approximates the K  X  1 v in Eq. 2 [6]: Theorem 1. Assume that K is symmetric positive definite with the smallest eigenvalue  X  n and largest eigenvalue  X  1 . Let the projected residual J m ( v  X  K  X  z ) be expressed obtained from the Lanczos-Galerkin projection process is such that v  X  K z m K  X  1 = represents the Chebyshev polynomial of the first kind of degree m .
 Theorem 1 tells us that in this case the method will provide a good accuracy when  X  1 is not too large. It is clear that when m becomes larger, the term | | becomes smaller as | j m (  X  ) | increases. On the other hand, the Lanczo s process can compute the eigenvalues  X   X  c According to the theorem, when m is large, both the approximation of eigenvalues and eigenvectors are close to the original eigenvalues and eigenvectors.
 The Combination. It can be found from Eq. 3 that the final classification is made by  X  KK  X  1 v , so we analyze the bound on  X  KK  X  1 v when the approximate eigenvalues and normalization is not considered in the analysis. Assume that the eigen-decomposition of T m is given by T m = S m  X  m S T m where  X  m is an m  X  m diagonal matrix with entries see Eq. 1. By using the Lanczos process, we know that K m and  X  K m are close to K and  X  K respectively when m is large.
 find that when k (the kernel matrix density) and m (the number of Lanczos steps) K m )( K  X  The two terms K m  X  K and  X  K  X   X  K m is small when m is large. Similarly, K ( k ) m  X  K m and  X  K m  X   X  K  X  K
Note that Eqs. 4 and 5 imply that by using k -approx, the differences between the eigenvalues/eigenvectors and those of using the full-connection graph are small when k to eigen-system approximation error. On the other hand, using the full-connection graph mal k value from those tolerated by the resource budget; otherwise it is not bad to use the largest k value tolerated by the budget.
 3.2 The Algorithm From the above analysis we have the Lank (Lan czos with k -approx) algorithm: 1. Decide the largest k and m tolerated by the given budget. 4. Apply the transfer function (e.g., Eq. 1) to the eigenvalues, and use the transferred For test example x , similar to Eq. 3, we have
Suppose each integer costs 4 bytes and each double float costs 8 bytes in storage; this is popular in current machines. Given n examples with dimensionality d , the storage for these examples is dn  X  8 bytes. After applying k -approx, the matrix K ( k ) which contains 2 kn number of non-zero entries is generated. Considering both the storage for the non-zero values and their indices, the required storage is roughly 2 kn  X  8+ 2  X  2 kn  X  4=4 kn  X  8 bytes. So, up to now the storage required is ( d +4 k ) n  X  8 bytes. Now considering the Lancozs process, for storing the m largest eigenvalues and storage for the Lanczos process is ( m + mn +4 kn )  X  8 bytes. When m n , the storage method for exploiting all the labeled and unlabeled examples is (max ( m, d )+4 k ) n  X  8 bytes. Since d and n are known, by assuming m =  X k (  X  is a parameter), we can get the estimate of the largest k tolerated by the budget. 4.1 When Original Cluster Kernel Can Work First, we study that when there are sufficien t resource for the original cluster kernel algorithm [1] (abbreviated as ClusK in the following) to exploit all available unlabeled examples, how well the Lank method approximates the performance of ClusK.
For this purpose, we run experiments on two small-scale UCI data sets, australian and kr-vs-kp . On each data set we randomly pick 10% examples to use as labeled train-ing examples and regard the remaining ones as unlabeled examples. On both data sets the resource are sufficient for ClusK to utilize all examples, and we compare ClusK and Lank in a transductive setting. According to [1],  X  of Gaussian is set to 0.55 for both algorithms. We repeat the experiments for ten times with random labeled/unlabeled partitions, and the average results are shown in Fig. 1.
To study how well the Lank method approximates ClusK, we present in Fig. 1 the performance of Lank under different k and m ( m =  X k ) configurations. Note that the number of examples in kr-vs-kp is about five times of that in australian ,andsoa k value when both k and m are small on kr-vs-kp , Lank approximates ClusK well on both data sets and on most parameter configurations. 4.2 When Original Cluster Kernel Cannot Work Next, we study how the algorithms work given a resource budget. We run experiments on the SRAA data set (http://www.cs.umass.edu/  X  mccallum/code-data.html) which con-classes; we merge the two real classes and the two simulate classes, resulting in a two-class classification task. We randomly pick 1K or 5K examples to use as labeled training examples and regard the remaining ones as unlabeled examples. In the experiments we evaluate the performances under three storage budgets, 200MB, 400MB and 600MB.
The original ClusK cannot deal with such a large data set even when the largest budget (i.e., 600MB) were allocated. So, we facilitate it with the two sampling methods described in [2]. Briefly, RandSub randomly samples some unlabeled examples to use; SmartSub first uniformly samples some unlabeled examples to get an estimate of the decision surface, and then chooses the examples near that estimated surface to use.
We use the calculation described at the end of Section 3.3 to estimate k and m . Here, we simply set  X  =0 . 5 k . ClusK facilitated with RandSub or SmartSub is denoted by ClusK+RandSub and ClusK+SmartSub , respectively; both use the largest number of unlabeled examples that can make the algorithms executable under the given storage budget. The parameters of the sampling methods are set to the values recommended in [2]. The experiments are repeated for ten times and the results are shown in Fig. 2. Note that when there are 5K labeled training examples, ClusK+SmartSub cannot work given the budget of 200MB or 400MB, because the budget is not sufficient for running the SmartSub method. Fig. 2 shows that Lank is a better choice under all budgets. This paper proposes to study budget semi-supervised learning . The key is that, given different budgets, even for the same data, the behaviors of the algorithm should be dif-ferent. Roughly speaking, the more resource, the more exploitation on unlabeled data. Considering that algorithms relying on spectral analysis suffer seriously from budget resource, we present a simple yet effective method which is able to adapt such kind of algorithms to a given budget. In order to show that the goal of budget semi-supervised learning is tractable, the presented method utilizes some well-known simple techniques. It is for sure that new elaborate methods will be attractive and lead to a better perfor-mance. This will become a fruitf ul topic in future research.
 Acknowledgements. This research was supported by NSFC (60635030, 60721002), JiangsuSF (BK2008018), HKRGC (201508) and HKBU FRGs.

