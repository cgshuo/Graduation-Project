
Idiap and EPF Lausanne A huge amount of images with accompanying text captions are a vailable on the Internet. Websites selling various items such as houses and clothing provide ph otographs of their products along with the caption. These news websites are very popular because pe ople are interested in other people, especially if they are famous (figure 1). Exploiting the asso ciations between images and text hidden in this wealth of data can lead to a virtually infinite source o f annotations from which to learn visual models without explicit manual intervention.
 The learned models could then be used in a variety of Computer Vision applications, including face recognition, image search engines, and to annotate new imag es for which no caption is available. Moreover, recovering image-text associations is useful fo r auto-annotating a closed corpus of data, e.g. for users of news website to see  X  X ho X  X  in the picture X  [6 ], or to search for images where a certain person does a certain thing.
 Previous works on news items has focused on associating name s in the captions to faces in the im-ages [5, 6, 16, 21]. This is difficult due to the correspondence ambiguity problem: multiple persons appear in the image and the caption. Moreover, persons in the image are not always mentioned in the caption, and not all names in the caption appear in the image. The techniques tackle the correspon-dence problem by exploiting the fact that different images s how different combinations of persons. As a result, these methods work well for frequently occurrin g persons (typical for famous people) appearing in dataset with thousands of news items.
 In this paper we propose to go beyond the above works, by model ing both names and action verbs jointly. These correspond to faces and body poses in the images (figure 3). The connections be-tween the subject (name) and verb in a caption can be found by w ell established language analysis techniques [1, 8]. Essentially, by considering the subject -verb language construct, we generalize the  X  X ho X  X  in the picture X  line of works to  X  X ho X  X  doing what X . We present a new generative model where the observed variables are names and verbs in the capti on as well as detected persons in the image. The image-caption correspondences are carried by la tent variables, while the visual appear-ance of face and pose classes corresponding to different nam es and verbs are model parameters. During learning, we simultaneously solve for the correspon dence and learn the appearance models. In our joint model, the correspondence ambiguity is reduced because the face and pose information help each other . For example, in figure 1b, knowing what  X  X aves X  means would r eveal who of the two imaged persons is Obama. The other way around, knowing wh o is Obama would deliver a visual example for the  X  X aving X  pose.
 We show experimentally that (i) our joint  X  X ace and pose X  mod el solves the correspondence problem better than simpler models covering either face or pose alon e; (ii) the learned model can be used to effectively annotate new images with or without captions; ( iii) our model with face alone performs better than the existing face-only methods based on Gaussia n mixture appearance models. Related works. This paper is most closely related to works on associating na mes and faces, which we discussed above. There exist also works on associating no uns to image regions [2, 3, 10], starting from images annotated with a list of nouns indicating the obj ects it contains (typical datasets contain natural scenes and objects such as  X  X ater X  and  X  X iger X ). A re cent work in this line is that of Gupta and Davis [17], who model prepositions in addition to nouns ( e.g.  X  X ear in water X ,  X  X ar on street X ). To the best of our knowledge, ours is the first work on jointly m odeling names and verbs. The news item corpus used to train our face and pose model cons ists of still images of person(s) performing some action(s). Each image is annotated with a ca ption describing  X  X ho X  X  doing what X  in the image (figure 1). Some names from the caption might not a ppear in the image, and vice-versa some imaged persons might not be mentioned in the capti on. The basic units in our model are persons in the image, consisting of their face and upper b ody. Our system automatically detects them by bounding-boxes in the image using a face detector [23 ] and an upper body detector [14]. In the rest of the paper, we say  X  X erson X  to indicate a detecte d face and the upper body associated with it (including false positive detections). A face and an upper-body are considered to belong to the same person if the face lies near the center of the upper bo dy bounding-box. For each person, we obtain a pose estimate using [11] (figure 3(right)). In add ition to these image features, we use a language parser [1, 8] to extract a set of name-verb pairs fr om each caption. Our goals are to: (i) associate the persons in the images to the name-verb pair s in the captions, and (ii) learn visual appearance models corresponding to names and verbs. These c an then be used for recognition on new images with or without caption. Learning in our model can be seen as a constrained clustering problem [4, 24, 25]. 2.1 Generative model We start by describing how our generative model explains the image-caption data (figure 2). The notation is summarized in Table I. Suppose we have a collecti on of documents D = { D 1 , . . . , D M } with each document D i consisting of an image I i and its caption C i . These captions implicitly provide the labels of the person(s) X  name(s) and pose(s) in t he corresponding images. For each caption C i , we consider only the name-verb pairs n i returned by a language parser [1, 8] and ignore other words. We make the same assumptions as for the name-fac e problem [5, 6, 16, 21] that the labels can only come from the name-verb pairs in the captions or null (for persons not mentioned in the caption). Based on this, we generate the set of all poss ible assignments A i from the n i in C i (see section 2.4 for details). Hence, we replace the caption s by the sets of possible assignments a person X  X  face to a name and pose to a verb. These take on value s from the set of name indices over all the captions and null represents unknown names/verbs and false positive person d etections. Document collection likelihood. Assuming independence between documents, the likelihood o f the whole document collection is where  X  are the model parameters explaining the visual appearance o f the persons X  faces and poses of learning is to find the parameters  X  and the labels Y that maximize the likelihood. Below we focus on P ( I i | Y i ,  X  ) , and then define P ( Y i | A i ) and P ( A i ) in section 2.4. Image likelihood. The basic image units in our model are persons. Assuming inde pendence be-tween multiple persons in an image, the likelihood of an imag e can be expressed as the product over the likelihood of each person: the face and pose appearance of a person, the conditional pro bability for the appearance of the p th person in image I i given the latent variable y i,p is: where  X  = (  X  name ,  X  verb ) are the appearance models associated with the various names and verbs. correspond to different poses such as holding the ball on the racket, tossing the ball and hitting it. Analogously,  X  u name models the variability within the face class corresponding to a name u . 2.2 Face and pose descriptors and similarity measures After detecting faces from the images with the multi-view al gorithm [23], we use [12] to detect nine distinctive feature points within the face bounding box (fig ure 3(left)). Each feature is represented by SIFT descriptors [18], and their concatenation gives the ov erall descriptor vector for the face. We use the cosine as a naturally normalized similarity measure bet ween two face descriptors: sim face ( a, b ) = We use [14] to detect upper-bodies and [11] to estimate their pose. A pose E consists of a distri-bution over the position ( x, y and orientation) for each of 6 body parts (head, torso, upper/lower Figure 3: Example images with facial features and pose estimates superimposed. Left Facial features (left left/right arms). The pose estimator factors out variation s due to clothing and background, so E conveys purely spatial arrangements of body parts. We deriv e three relatively low-dimensional pose descriptors from E , as proposed in [13]. These descriptors represent pose in di fferent ways, such as the relative position between pairs of body parts, and part-specific soft-segmentations of the image (i.e. the probability of pixels as belonging to a part). We re fer to [13, 11] for more details and the similarity measure associated with each descriptor. We nor malize the range of each similarity to the rest of this paper is dist pose ( a, b ) = 1  X  sim pose ( a, b ) . 2.3 Appearance model The appearance model for a pose class (corresponding to a ver b) is defined as: where  X  k verb are the parameters of the k th pose class (or  X  verb if k = null ). The indicator function pose class, as the face model is derived analogously.
 approach. Some previous works on names-faces used a Gaussia n mixture model [6, 21]: each name is associated with a Gaussian density, plus an additional Ga ussian to model the null class. Using functions of the exponential family like a Gaussian simplifi es computations. However, a Gaussian may restrict the representative power of the appearance mod el. Problems such as face and pose recognition are particularly challenging because they inv olve complex non-Gaussian multimodal distributions. Figure 3(right) shows a few examples of the v ariance within the pose class for a verb. Moreover, we cannot easily employ existing pose similarity measures [13]. Therefore, we represent the conditional probability using a exemplar-based likeli hood function: where Z closest class representative vector  X  k representative poses for verb k . The likelihood depends on the model parameters  X  k verb , and the distance function d pose . The scalar  X  verb represents the null model, thus poses assigned to null have correspond to any verb in the caption or they might be false de tections. By generalizing the similarity sample center vector  X  k The center vector  X  k vectors assigned to  X  k belongs to the class of  X  k version of the k-means [19] clustering algorithm. The numbe r of centers R k can vary for different verbs, depending on the distribution of the data and the numb er of samples. As we are interested only in computing the distance between  X  k the only term that needs to be computed in equation (6) is the s econd (the third term is constant for each assigned  X  k 2.4 Name-verb assignments The name-verb pairs n i for a document are observed in its caption C i . We derive from them the set of all possible assignments A i = { a i number of possible assignments L As opposed to the standard matching problem, here the assign ments have to take into account null . Moreover, we have the same constraints as in the name-face pr oblem [6]: a person can be assigned to at most one name-verb pair, and vice-versa. Therefore, gi ven a document with P i persons and W i name-verb pairs, the number of possible assignments is L i = P min( P i ,W i ) j =0 P i j is the number of persons assigned to a name-verb pair instead of null . Even by imposing the above constraints, this number grows rapidly with P i and W i . However, since different assignments share many common sub-assignments, the number of unique lik elihood computations is much lower, namely P i ( W i + 1) . Thus, we can evaluate all possible assignments for an image efficiently. Although certain assignments are unlikely to happen (e.g. a ll persons are assigned to null ), here we use an uniform prior over all assignments, i.e. P ( a i only come from A i , we define the conditional probability over the latent varia bles Y i as: The latent assignment Y i play the role of the annotations necessary for learning appe arance models. The task of learning is to find the model parameters  X  and the assignments Y which maximize the likelihood of the complete dataset { I , Y , A} . The joint probability of { I , Y , A} given  X  from equation (1) can be written as objective function over the latent variables Y and parameters  X  : Thus, to minimize J , each latent variable Y i must belong to the set of possible assignments A i . If Y would be known, the cluster centers  X   X   X  name ,  X   X   X  verb which minimize J could be determined uniquely (given also the number of class centers R ). However, it is difficult to set R before seeing the data. In our implementation, we determine the centers ap proximately using the data points and their K nearest neighbors. Since estimating the normalization con stants Z tationally expensive, we make an approximation by consider ing them as constant in the clustering process (i.e. drop their terms from J ). In our experiments, this did not significantly affect the results, as also noted in several other works (e.g. [4]).
 Since the assignments Y are unknown, we use a generalized EM procedure [7, 22] for sim ultane-ously learning the parameters  X  and solving the correspondence problem (i.e. find Y ): Input. Data D ; hyper-parameters  X  name ,  X  verb , K 1. Initialization. We start by computing the distance matrix between faces/pos es from images sharing some name/verb in the caption. Next we initialize  X  using all documents in D . For each different name/verb, we select all captions containing onl y this name/verb. If the corresponding The center vectors are found approximately using each data p oint and their K nearest neighbors of the same name/verb class. If a name/verb only appears in ca ptions with multiple names/verbs we randomly assign the name/verb to any face/pose in each ima ge. The center vectors are then initialized using these data points. The initial weights w for all data points are set to one (equation 6). This step yields an initial estimate of the model parameters  X  . We refine the parameters and assign-ments by repeating the following EM-steps until convergenc e. 2. E-step. Compute the labels Y using the parameters  X  old from the previous iteration 3. M-step. Given the labels Y , update  X  so as to minimize J (i.e. update the cluster centers  X  ). Our algorithm assigns each point to exactly one cluster. Eac h point I i,p in a cluster is given a weight Therefore, faces and poses from images with many detections have a lower weights and contribute less to the cluster centers, reflecting the larger uncertain ty in their assignments. Datasets There are datasets of news image-caption pairs such as those in [6, 16]. Unfortunately, these datasets are not suitable in our scenario for two reaso ns. Faces often occupy most of the image so the body pose is not visible. Second, the captions frequen tly describe the event at an abstract level, rather than using a verb to describe the actions of the persons in the image (compare figure 1 to the figures in [6, 16]). Therefore, we collected a new datas et 2 by querying Google-images using a combination of names and verbs (from sports and social inte ractions), corresponding to distinct upper body poses. An example query is  X  X arack Obama X  +  X  X hake hands X . Our dataset contains 1610 images, each with at least one person whose face occupie s less than 5% of the image, and with the accompanying snippet of text returned by Google-images . External annotators were asked to Figure 5: Examples of when modeling pose improves the results at learning time. Belo w the images we report extend these snippets into realistic captions when necessa ry, with varied long sentences, mentioning the action of the persons in the image as well as names/verbs n ot appearing in the image (as  X  X oise X , figure 1). Moreover, they also annotated the ground-truth na me-verb pairs mentioned in the captions as well as the location of the target persons in the images, en abling to evaluate results quantitatively. In total the ground-truth consists of 2627 name-verb pairs. In our experiments we only consider names and verbs occurring in at least 3 captions for a name, an d 20 captions for a verb. This leaves 69 names corresponding to 69 face classes and 20 verbs corres ponding to 20 pose classes. We used an open source Named Entity recognizer [1] to detect n ames in the captions and a language parser [8] to find name-verbs pairs (or name-null if the language parser could not find a verb as-sociated with a name). By using simple stemming rules, the sa me verb under different tenses and possessive adjectives was merged together. For instance  X  X  hake their hands X ,  X  X s shaking hands X  and  X  X hakes hands X  all correspond to the action verb  X  X hake hand s X . In total, the algorithms achieves precision 85 . 5% and recall 68 . 8% on our dataset over the ground-truth name-verb pair. By disc ard-ing infrequent names and verbs as explained above, we retain 85 names and 20 verbs to be learned by our model (recall that some of these are false positives rath er than actual person names and verbs). Results for learning The learning algorithm takes about five iterations to conver ge. We compare experimentally our face and pose model to stripped-down ver sions using only face or pose informa-tion. For comparison, we also implement the constrained mix ture model [6] described in section 2.3. Although [6] also originally incorporates also a language m odel of the caption, we discard it here so that both methods use the same amount of information. We ru n the experiments in three setups: (a) using the ground-truth name-verb annotations from the c aptions; (b) using the name-verb pairs automatically extracted by the language parser; (c) simila r as (b) but only on documents with multi-ple persons in the image or multiple name-verb pairs in the ca ption. These setups are progressively more difficult, as (b) has more noisy name-verb pairs, and (c) has no documents with a single name and person, where our initialization is very reliable.
 Figure 4(left) compares the accuracy achieved by different models on these setups. The accuracy is defined as the percentage of correct assignments over all det ected persons, including assignments to null , as in [5, 16]. As the figure shows, our joint  X  X ace and pose X  mo del outperforms both models using face or pose alone in all setups. Both the annotation of faces and poses improve, demonstrating paper. Figure 4(right) shows improvements on precision and recall over models using faces or poses alone. As a second point, our model with face alone also outpe rforms the baseline approach using Gaussian mixture appearance models (e.g. used in [6]). Figu re 5 shows a few examples of how including pose improves the learning results and solve some of the correspondence ambiguities. Improvements happen mainly in three situations: (a) when th ere are multiple names in a caption, as not all names in the captions are associated to action verbs ( figure 1(a) and figure 5(top)); (b) when there are multiple persons in an image, because the pose disa mbiguates the assignment (figure 1(b) different than frontal (i.e. where face recognition works l ess well, e.g. figure 5(middle)). Results for recognition Once the model is learned, we can use it to recognize  X  X ho X  X  do ing what X  in novel images with or without captions. We collected a new s et of 100 images and captions from Google-images using five keywords based on names and verbs fr om the training dataset. We evaluate the learned model in two scenarios: (a) the test data consist s of images and captions. Here we run inference on the model, recovering the best assignment Y from the set of possible assignments generated from the captions; (b) the same test images are use d but the captions are not given, so the problem degenerates to a standard face and pose recognit ion task. Figure 6(left) reports face annotation accuracy for three methods using captions (scen ario (a)): (  X  ) a baseline which randomly assigns a name (or null ) from the caption to each face in the image; (x) our face and po se model; ( ) our model using only faces. The figure also shows results for s cenario (b), where our full model tries to recognize faces (+) and poses (  X  ) in the test images without captions. On scenario (a) all mod els outperform the baseline, and our joint face and pose model im proves significantly on the face-only model for all keywords, especially when there are multiple p ersons in the image.
 Conclusions. We present an approach for the joint modeling of faces and pos es in images and their association to names and action verbs in accompanying text captions. Experimental results show that our joint model performs better than face-only mod els both in solving the image-caption correspondence problem on the training data, and in annotat ing new images. Future work aims at incorporating an effective web crawler and html/language p arsing tools to harvest image-caption pairs from the internet fully automatically. Other techniq ues such as learning distance functions [4, 15, 20] may also be incorporated during learning to improve r ecognition results.
 [1] http://opennlp.sourceforge.net/ . [2] K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D. Blei , and M. Jordan. Matching words [3] K. Barnard and Q. Fan. Reducing correspondence ambiguit y in loosely labeled training data. [4] S. Basu, M. Bilenko, A. Banerjee, and R. J. Mooney. Probab ilistic semi-supervised cluster-[5] T. Berg, A. Berg, J. Edwards, and D. Forsyth. Names and fac es in the news. In Proc. CVPR X 04 . [6] T. Berg, A. Berg, J. Edwards, and D. Forsyth. Who X  X  in the pi cture? In Proc. NIPS X 04 . [7] A. P. Dempster, N. Laird, and D. Rubin. Maximum likelihoo d from incomplete data via the [8] K. Deschacht and M.-F. Moens. Semi-supervised semantic role labeling using the latent words [9] I. Dhillon, Y. Guan, and B. Kulis. Kernel k-means: spectr al clustering and normalized cuts. In [10] P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth. Ob ject recognition as machine transla-[11] M. Eichner and V. Ferrari. Better appearance models for pictorial structures. In Proc. [12] M. Everingham, J. Sivic, and A. Zisserman. Hello! my nam e is... buffy -automatic naming of [13] V. Ferrari, M. Marin, and A. Zisserman. Pose search: ret rieving people using their pose. In [14] V. Ferrari, M. Marin, and A. Zisserman. Progressive sea rch space reduction for human pose [16] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. Au tomatic face naming with caption-[17] A. Gupta and L. Davis. Beyond nouns: Exploiting preposi tions and comparative adjectives for [18] D. Lowe. Distinctive image features from scale-invari ant keypoints. IJCV , 60(2):91 X 110, [19] J. B. MacQueen. Some methods for classification and anal ysis of multivariate observations. In [20] T. Malisiewicz and A. Efros. Recognition by associatio n via learning per-exemplar distances. [21] T. Mensink and J. Verbeek. Improving people search usin g query expansions: How friends [22] R. Neal and G. E. Hinton. A view of the em algorithm that ju stifies incremental, sparse, and [23] Y. Rodriguez. Face Detection and Verification using Local Binary Patterns . PhD thesis,  X  Ecole [24] N. Shental, A. Bar-Hillel, T. Hertz, and D. Weinshall. C omputing gaussian mixture models [25] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. Cons trained k-means clustering with
