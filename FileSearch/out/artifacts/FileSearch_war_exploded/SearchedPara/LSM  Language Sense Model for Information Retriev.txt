 Word sense disambiguation (WSD) has been studied for a long time in natural language processing. In the field of information retrieval (IR), word sense ambiguity is regarded as one of the main causes which affect the retrieval performance for two reasons:  X  Polysemy: One word may have different meanings under different contexts.  X  Synonymy: Different words may have the same meaning.

This encouraged various of work to integrate the WSD into IR. However, most of the poor results were intensively studied, which can be categorized as follows:  X  Fine-grained Sense: The words might be resolved to senses which are too specific  X  Poor WSD Results: The low accuracy of the WSD system affects the final perfor- X  Cannot Fall Back: The pure sense based IR system can not fall back to the term
An example which successfully solved the three problems above would be Kim X  X  work in 2004 which improved retrieval performance significantly[7]. Firstly, it achieved the high WSD accuracy by using the coarse-grained senses which consisted of 25 root senses in W ORD N ET 1 . Secondly, it combined the root senses and the document terms senses and terms. However, there are still some questions to be further addressed,  X  Will the integration of fine-grained senses and terms work as well?  X  Is there any other model to integrate the terms and senses besides the vector space
To answer these questions, we first implement a WSD system using the fine-grained senses in W ORD N ET . In our WSD system, only nouns and verbs are disambiguated because the nouns and verbs play important roles in IR and they are much easier to disambiguate with a comparatively higher accuracy.

Then, the fine-grained senses are utilized based on the language model[8]. Firstly, the language model on term representation (LMTR) and sense representation (LMSR) are studied. Then, a novel model, language sense model (LSM) for information retrieval, is proposed which utilizes both sense and term representations. The experimental results on the TREC collections shows that the LMSR can not bring any improvement. How-ever, the LSM outperforms both vector sp ace model and traditional language model for medium and long queries significantly (7.53% -16.90%).

The rest of the paper is organized as follows. In Section 2, some related work is discussed . In Section 3, the process and evaluation of WSD are presented. The discus-sion of the LSM is given in Section 4. The experiment results of the LSM are given in Section 5. Finally, we make a conclusion and give some future work in Section 6. In this section, the related work is surveyed in two aspects. One relates to the previous efforts on WSD for IR while the other to the previous work on language models. 2.1 Word Sense Disambiguation for Information Retrieval Most of the early work which integrated WSD into IR resulted in no improvement. A complete review of the integration of WSD and IR prior to the year 2000 can be found in the work of Sanderson [9]. In this section, we review some recent work which reported significant improvements by integrating WSD into IR.

In 2003, Stokoe represented documents and queries with sense vectors and retrieved the relevant documents using the traditi onal vector space model [10].Their experiments on TREC WT10G data collection empirically showed that their WSD system could sig-nificantly improve the retrieval performance. Howe ver, it was problematic that the ab-solute precision of the baseline and the propos ed system were too low to investigate the effect of sense-based retrieval. Compared with Stokoe X  X  work, the LSM improves the retrieval performance significantly when the baseline X  X  absolute precision is compara-tively much higher. More importantly, in the LSM, the terms and senses are integrated to achieve a better performance.
In 2004, Kim et al proposed the root sense tagging approach for information retrieval by integrating the root sense tags into the vector space model[7]. As proposed in Section 1, Kim solved the three existing problems successfully. Different from Kim X  X  work, the LSM utilizes fine-grained senses, and combines the terms and senses in the language model. 2.2 Language Model For many years, the primary consumers of statistical language models were speech recognition systems [11]. In 1998, Ponte and Croft [8] proposed a smoothed version of the document unigram model to assign a score to a query, which can be thought of as the probability that the query was generated from the document model. Since then, there emerged a great amount of research work related to language model. Most of them tried to solve the following two problems:  X  Data Sparseness: Many smoothing methods were suggested to re-evaluate the  X  Term Dependency: The unigram language model made an improper assumption Word Sense Disambiguation (WSD), which is a classical problem in Natural Language Processing (NLP), aims to improve the accuracy, namely the number of words cor-rectly disambiguated. Our approach is based on the Co-occurrence, S EMCOR 2 ,and W
ORD N ET . In order to achieve the high disambiguation accuracy, only the nouns and verbs on both the queries and the documents are disambiguated. Most of the methods are based on popular and effective techniques in [19, 7, 20].

S EMCOR 2.0 is distributed with W ORD N ET 2.0, an online thesaurus created at Prince-ton University. W ORD N ET 2.0 consists of 90,000 terms and collocates organized into Synsets. Each Synset contains words which are synonymous with each other, while the links between Synsets represent hypernymy and hyponomy relationships to form aW ORD N ET hierarchical semantic network. S EMCOR 2.0 is a manually sense tagged W tagged with part-of-speech (POS) by Brill X  X  tagger 3 . Secondly, ANNIE TAGGER 4 performs on the text to remove named entities from the WSD candidate set. In the exper-iment, only three types of named entities: LOC (location), PER (person) and ORG (or-ganization) are extracted. Thirdly, each monosemous word is identified with the unique sense it owns. In the following three subsections, we will introduce the main methods of the WSD system.

Our WSD system makes use of mutual information (MI) of the adjacent words in the text. Besides, W ORD N ET and S EMCOR information is integrated into the following procedures to identify the senses of the candidate words. We get context clues from the S
EMCOR of the occurrence of the collocation. If, in all the occurrences of the collo-cation, the word has only one sense, and the number of the occurrences is larger than a given threshold (  X  2 in our experiment), then we identify the word with the sense. We identify the sense of a word by comparing the original context of the word and the context set of the word X  X  senses at W ORD N ET and S EMCOR . The following nouns will be added to the context set of the sense: the words in the sense at W ORD N ET ,thefirst shortest noun phrase from the definition of the sense at W ORD N ET , all the nouns which occur within a window size (20 words in our experiment) with respect to the sense in S Our WSD system also integrates the hierarchical information of the synsets in W
ORD N ET .InW ORD N ET , all the words with the same POS are organized into hi-erarchies, each synset is a part of a hierarchy. Taking the noun as an example, there are 25 root senses. For two words t 1 and t 2 within a window size, if the hierarchical distance between a sense of the word t 1 and the word t 2 is equal to or less than 1, the system identifies the two words with their corresponding senses.

We trained our method on the first 300 documents of S EMCOR , and tested it on the last 52 documents. The accuracy of noun is 78.12% and accuracy of verb is 60.58%, the overall accuracy of WSD system is 72.40%, which is much higher than the previous WSD sytem applied to IR. the language model for term and sense representations is proposed. Then, the smoothing methods are discussed and a new hierarchical smoothing method is proposed. Finally, in Section 4.2, the LSM and the correspondingly parameter estimation methods are proposed to integrate the term and sense representations. 4.1 Language Model and Smoothing Methods Language Model for T erm and Sense Representations. In this paper, each document has two different representations: namely term representation and sense representation, as shown in Figure 1. Two examples from TREC Fbis corpus are given in the right.
The language model on term representations (LMTR) is the traditional approach. It documents are ranked according to the pr obability the model could generate. In this paper, the urigram language model is adopted and the equation could be represented as follows: Where q t and d t means the term representations of query q and document d respectively. q ti means the ith term of the query q t and m is the length of the query q t .
The language model on sense representations (LMSR) is similar to the one on term representations. It first generates a sense model d s for each document d using the sense q s 1 q s 2 ...q sm . The corresponding equation can be shown as follows.
 Smoothing Methods. The smoothing method plays an important role in language model due to the data sparseness problem. An empirical study of smoothing methods for the language model can be found at [21]. Table 1 shows three of them which are popularly used in language model for information retrieval [13].
 The three smoothing approaches can be applied to the LMTR and the LMSR. For the LMSR, we developed a new smoothing method, namely hierarchical smoothing, based on the W ORD N ET hierarchy as follows:
Here the Relative ( q si ) can be defined as the hypernym sense or hyponym sense of the sense q si in the W ORD N ET hierarchy.  X  h is a constant from 0 to 1 which measures the confidence of the Relative ( q si ) . 4.2 Language Sense Model (LSM) In this section, we firstly propose the language sense model (LSM) for information retrieval which utilizes both term and sense representations. Then the model parameter estimation is discussed.
 Model Description. Figure 2 shows the framework of the LSM. In the LSM, the model generates the probability of a given query from both document X  X  term representation and sense representation. The sense representation d s can be further extended to d h and d r which stand for sense X  X  hypernym sense and root sense respectively. In this paper, we senses in information retrieval. So the LSM can be shown as Equation 4: q from term representation and sense representation respectively. Note that not all the terms in q t can be disambiguated as the WSD is only conducted on the nouns and verbs. A default value will be given to the q si if q ti can not be disambiguated. To solve the data sparseness problem, the existing smoothing method (as shown in Table 1) can be integrated into the LSM. An integration example of Jelinek-Mercer smoothing into LSM can be shown as follows: P ( q | d )=
Other than the traditional smoothing methods, the hierarchical smoothing can also P h ( q si | C s ) defined in Equation 3 as follows: P ( q | d )= Parameter Estimation. To compute the query generating probability from the LSM, parameter  X  .
 resentation and sense representation generating the corresponding query term. Given a query, we estimate the optimal weights  X   X  which could maximize the likelihood of the queries. This method is similar to Zhai X  X  me thod in estimating the parameter of the two stage model[13] and Cao X  X  method in estimating the combination in NSLM [18] . Let  X   X  be the optimal weight, taking the formula 5 as an example, we have:  X   X  =argmax where N is the number of documents in the dataset, and m is the length of query q. {  X  the query. With this setting, the EM formulae to update the parameter can be shown as follows: and The EM algorithm will be terminated if the log-likelihood of the query changes within It allows to initialize the  X  i with randomized value too because the EM algorithm guar-antees the convergence with a local optimization. The EM update formula for Dirich-let and Absolute Discount smoothing can be inferred similarly. Note that there are no independently. 5.1 Experiment Setup The whole TREC FBIS collection is used in our experiment. At first, all the nouns and verbs of queries and documents of TREC FBIS corpus were disambiguated with the methods proposed in Section 3. In order to evaluate the LSM X  X  performance on different length queries, we generated three t ypes of queries, shown as in Table 2. The queries are extracted from the TREC-5 routing topic which consists of 50 queries with 40 titles, 50 descriptions and 50 narratives.

The LSM system is built based on the Lemur 3.1. The Vector Space Model is based on the BM25 formula whose term frequency component is implemented as follows [22]: where f(t,d) means the term count of t in document d. In the experiment, k and b are set to 1 and 0.3 respectively.

In the following experiment, the standard mean average precision(MAP) and the total retrieved relevant document number (Recall) are used to evaluate the retrieval per-formance. 5.2 Evaluation of LMTR and LMSR The results of language models on term and sense representations are compared on dif-ferent queries and different smoothing methods, shown as Table 3. From the table, we can see that the language model on term representation (LMTR) performs much better than language model on sense representation (LMSR) in both precision and recall. Not-ing that some terms in the term representation cannot be disambiguated, we generated a mixed document representation, where the undisambiguated terms are reserved in the sense representation. However, we got the conclusion again that the LMTR performs much better than the language model on mixed representations.

The hierarchical smoothing for LMSR is also tested with two kinds of Relative ( q si ) , namely hypernym sense and hyponym sense. However, the result of LMSR remains almost unchanged. So in the next section, the experiments of the LSM is conducted without hierarchical smoothing. 5.3 Evaluation of Language Sense Model The results of the LMTR and LSM are compared with different queries and different smoothing methods as shown in Table 4, where a diamond ( ) means the LSM using the Dirichlet smoothing. From the  X  X mproved Map X  column, we can see that the LSM outperforms both the traditional language model and vector space model (BM25) on all queries. From the  X  X mproved Recall X  column, we can see that the LSM improved the recall on the medium and long queries as well. The 11-point precision/recall curves for the LSM using the Jelinek-Mercer, Dirichlet and Absolute Discount smoothing are are LSM, LMTR, LMSR and VSM respectively. To understand whether these improve-ments are statistically significant, we performed t-tests on MAP. The p-values are shown in the  X  X ign X  column of Table 4 where an asterisk (*) means significant improvement ( &lt; 0 . 05 ). From the result, we can see that the LSM improves significantly on both medium and long queries, however, not significantly on short queries. It X  X  reasonable because that:  X  There are less nouns and verbs to be disambiguated for short queries (see Table 2).  X  It X  X  much harder to disambiguate the short queries because of the sparse context. In the work, we implement a WSD system which is designed for nouns and verbs only. Then the language model on sense representations (LMSR) and language sense model (LSM) are proposed. The LSM integrated the fine-grained disambiguated senses and terms seamlessly through an EM algorithm. The experiments show that the LSM outper-forms both vector space model (BM25) and t raditional language model s significantly on both medium and long queries (7.53%-16.90%) with various smoothing methods. From this study, we can also empirically draw that the fine-grained senses will help the information retrieval if they are properly utilized.

In the future, we will study the hierarchical smoothing using more W ORD N ET rela-tions. In addition, we will further evaluate the LSM on more corpus and study how the accuracy of WSD affects the LSM.

