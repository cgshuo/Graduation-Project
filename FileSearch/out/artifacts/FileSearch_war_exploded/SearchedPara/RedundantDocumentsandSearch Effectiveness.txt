 The web con tains a great man y documen ts that are content-equivalent , that is, informationally redundan t with resp ect to eac h other. The presence of suc h mutually redundan t doc-umen ts in searc h results can degrade the user searc h exp eri-ence. Previous attempts to address this issue, most notably the TREC novelty trac k, were characterized by diculties with accuracy and evaluation. In this pap er we explore syn-tactic techniques | particularly documen t ngerprin ting | for detecting con ten t equiv alence. Using these techniques on the TREC GO V1 and GO V2 corp ora rev ealed a high degree of redundancy; a user study con rmed that our metrics were accurately iden tifying con ten t-equiv alence. We sho w, more-over, that con ten t-equiv alen t documen ts have a signi can t e ect on the searc h exp erience: we found that 16.6% of all relev ant documen ts in runs submitted to the TREC 2004 terab yte trac k were redundan t.
 H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al| Clustering, Sele ction Process Performance, Measuremen t Novelty, duplicate detection, searc h e ectiv eness
Searc h engines are designed to presen t answ ers in a way that allo ws users to satisfy their information needs with min-imum e ort. An issue that interferes with this objectiv e is data redundancy . If a documen t is e ectiv ely iden tical to documen ts that have already been presen ted, then it is un-necessary for the user to see it; at the most they may be interested in its location or in the fact that it exists. How-ever, most mainstream information retriev al systems | and Cop yright 2005 ACM 1 X 59593 X 140 X 6/05/0010 ... $ 5.00. most metho ds for measuring them (van Rijsb ergen 1979) | are based on the assumption that the relev ance of eac h documen t is indep enden t of any consideration of other doc-umen ts in the result list.

Sev eral techniques for managing rep etition of topics in answ er lists are based on the assumption that di eren t doc-umen ts may con tain alternativ e presen tations of the same information. For example, in Scatter-Gather (Hearst &amp; Ped-ersen 1996) documen ts are group ed based on automatic clus-tering. Other approac hes to reducing redundancy have been explored in the TREC novelty trac k (Harman 2002, Sob oro &amp; Harman 2003), where the aim is to extract novel and rel-evant sen tences from an ordered list of relev ant documen ts. However, it is unclear whether it would be feasible to add any of the metho ds evaluated in the trac k to a standard searc h engine (Allan et al. 2003). Furthermore, the novelty approac h su ers from the problem that novelty as a concept is dicult to de ne, recognize and evaluate.

In this pap er we examine a more elemen tary and robust approac h to reducing redundancy in searc h results. We de ne pairs of documen ts that con tain the same informa-tion as eac h other as content-e quivalent . In man y cases con ten t-equiv alen t documen ts can be remo ved from a result list with little or no negativ e consequence. Giv en kno wledge of con ten t-equiv alence relationships within a collection, an-swer lists can be postpro cessed to presen t duplicates as a single entry, reducing the presen tation of redundan t infor-mation to the user.

We explore syn tactic techniques for iden tifying con ten t-equiv alen t pairs. Our approac h is delib erately conserv ativ e; in this application an error-prone algorithm may inadv er-ten tly conceal essen tial information from the user. Nonethe-less, we sho w that our approac hes are able to iden tify a large num ber of con ten t-equiv alen t pairs on web collections. We examine documen t ngerprin ting (Man ber 1994, Brin et al. 1995, Hein tze 1996, Bro der et al. 1997), a technique that can rapidly analyze a collection to iden tify pairs of docu-men ts that share signi can t blo cks of text. Using human assessors and a substan tial set of documen t pairs of di er-ent degrees of syn tactic similarit y, we exp erimen tally deter-mine thresholds of similarit y above whic h most documen ts are con ten t-equiv alen t.

We exp erimen tally explore the e ect that con ten t-equi-valence amongst documen ts in a collection has on the searc h exp erience. Our results on the TREC GO V1 and GO V2 documen t collections sho w that not only are there man y con ten t-equiv alen t documen ts but that this large-scale du-plication leads to signi can t redundancy in result lists re-turned by systems for real queries. We also evaluate the consequences of con ten t equiv alence for searc h and for evalu-ation of information retriev al systems. We nd, surprisingly , a high level of inconsistency in the relev ance judgemen ts for the 2004 terab yte trac k; our results sho w that a good frac-tion of the judgemen ts are wrong. Moreo ver, our results sho w | on the assumption that redundan t information is irrelev ant | that con ten t-equiv alen t documen ts are dramat-ically a ecting searc h e ectiv eness results. We sho w that re-moving con ten t-equiv alen t documen ts from result lists has the poten tial to signi can tly impro ve e ectiv eness.
The pro cess of information retriev al can be explained in terms of a hypothetical user with a speci c information need (van Rijsb ergen 1979). The user's objectiv e is to satisfy the information need; the role of the information retriev al system is to retriev e and presen t information in suc h a way that the information need can be satis ed with minim um exp enditure of e ort by the user. The expression of the in-formation need by the user, the presen tation of results by the system, and the interaction between the user and the system can all tak e man y forms. However, in the domain of text searc h, most systems use variations on a single mo del, in whic h the user presen ts a query and the system returns a rank ed list of documen ts that the user is able to bro wse. The aim is to structure the list suc h that all relev ant docu-men ts in the collection are at the beginning. The user can minimize their e ort by bro wsing the list in order until the information need is met.

The exten t to whic h an information retriev al system achi-eves these goals is quan ti ed by values suc h as recall and precision. Recall is a measure of the exten t to whic h all the available relev ant information has been presen ted to the user, while precision is a measure of the exten t to whic h the e ort required to ful ll the information need has been minimized. These measures simplify the mo del by assuming indep endence of relev ance. That is, in all cases relev ance is assumed to be a prop erty exclusiv ely of the relationship between a given documen t and an information need. The position of the documen t within a result list and its rela-tionship to other documen ts in the list is not considered.
However, as we sho w later, remo val of this assumption does a ect measured performance. In practice, it is often the case that, although a documen t is in isolation relev ant to the information need, by the time it has been view ed by the user it adds nothing new. It follo ws that for typical querying suc h a documen t should not be considered relev ant.
For some information needs, the existence or location of redundan t documen ts may be of value to the user, but suc h cases are probably rare; in most cases, it is the con ten t itself that the user seeks. In other words a documen t, in order to be relev ant, must not just be topical to the information need but must in addition con tain an elemen t of novelty .
The TREC novelty trac k (Harman 2002, Sob oro &amp; Har-man 2003) has been a forum for evaluating the performance of systems that attempt to promote novelty by remo ving re-dundan t information from the result list. The trac k focused on a sen tence-lev el retriev al task in whic h eac h sen tence was returned only if it was relev ant and con tained some novel information given the sen tences that preceded it.
There are sev eral problems asso ciated with the task de-ned by the novelty trac k. The rst of these is assessmen t. The fact that the novelty of a sen tence dep ends on every sen tence that precedes it means that the task has had to be evaluated on a small, prede ned set of documen ts pre-sen ted in a speci c order. Furthermore, in order to have a larger pool of relev ant sen tences on whic h to base the as-sessmen ts, all documen ts in the list were strongly relev ant to the query . However, this practice may have introduced a bias in the results. Allan et al. (2003) suggest that this common practice in novelty researc h of using only relev ant documen ts could mean that existing results do not predict performance in more realistic searc h environmen ts.
The consequences of system error in the novelty task are another cause for concern. Seman tic concepts suc h as nov-elty | or, indeed, relev ance | are notoriously dicult to reliably detect with a computer. This is re ected in the low recall and precision scores prev alen t in most areas of information retriev al; high accuracy is not curren tly achiev-able. The problem is that, while inaccuracy is somewhat ac-ceptable for relev ance, it is less so for novelty. Because the systems aim to highligh t only novel information, all other information | that whic h is considered to be redundan t | is e ectiv ely discarded from the searc h result. Thus, mis-classi cation of novel information as redundan t could lead to critical information being overlo oked.

Furthermore, the novelty trac k has demonstrated that not even human judges were able to agree on the novelty status of man y sen tences. Giv en the inheren t dicult y of the task, and the further dicult y we have using computers to capture seman tic prop erties suc h as novelty, it is unlik ely in the near future that systems will be able to discern novelty with a sucien t degree of accuracy to be useful in any but the most specialized applications.

Finally , there is more to meeting a user's information need than just novelty; the issue of authorit y is also a considera-tion. If a user were to disco ver an unlik ely fact (for example, that cold fusion has been achiev ed) from a single source, then they would be liable to doubt that information. However, if they came across that same information from sev eral inde-penden t sources then they would be inclined to lend more weigh t to the truth of the statemen t. Thus, elimination of seman tic redundancy may not alw ays be adv antageous.
For all of the above reasons, the techniques explored in the TREC novelty trac k are unlik ely to gain much traction amongst pro duction information retriev al systems.
We refer to a pair of documen ts as content-e quivalent if they con vey the same information as eac h other. A func-tional interpretation is that an information consumer, hav-ing view ed one documen t, would gain no new information by viewing the other.

In dev eloping a robust technique for detecting con ten t-equiv alen t pairs, we restrict ourselv es to detecting relation-ships between documen ts with high syn tactic similarit y | pairs typically referred to as duplicates or near-duplicates. In con trast to the task of analysing the seman tic con ten t of documen ts, purely syn tactic analysis is relativ ely easy , as it relies only on the sup er cial structure of the data. This is a conserv ativ e approac h; we may miss some con ten t-equiv alen t pairs, but the probabilit y of false positiv es is much lower than when using seman tic techniques.

A further adv antage of this approac h is that using syn tac-tic similarit y as criterion largely resolv es the issue of author-ity. In man y cases, rep etition of information from multiple indep enden t sources is an imp ortan t step towards meeting an information need. By con trast, duplication is rarely valu-able: further copies of the same documen t add little to the user's con dence in the information. For most scenarios, a result list need not con tain both documen ts from a con ten t-equiv alen t pair; one is sucien t, with perhaps an indication that the same documen t app ears elsewhere.

It is trivially apparen t that two iden tical documen ts are con ten t-equiv alen t. However, documen ts that are not byte-wise iden tical can still displa y enough syn tactic similarit y to be iden ti ed as con ten t-equiv alen t. A common scenario in the TREC .gov data is the presence of two iden tical docu-men ts, one in HTML and the other con verted to plain text from PDF. These documen ts are iden tical on a word-b y-word level but vary signi can tly in the way they are stored. In other cases, documen ts vary in their formatting and hy-phenation. Another kind of case is where a documen t is updated without a ecting most of the text, suc h as alterna-tive forms of the same press release or di eren t versions of the same policy documen t.

Duplication or near-duplication of documen ts has been noted as a special-case or extreme-case example of docu-men t redundancy (Zhai et al. 2003, Zhang et al. 2002). We sho w that documen t duplication and near-duplication is nei-ther special nor extreme; rather, it is widespread and has a signi can t impact on searc h results. We describ e two syn tac-tic approac hes to robust iden ti cation of con ten t-equiv alen t documen ts below.
We rst de ne retrieval equivalenc e , an easy-to-compute restricted form of con ten t equiv alence motiv ated by the op-eration of searc h engines. A set of documen ts is retriev al-equiv alen t if, once they are canonicalized | stripp ed of for-matting and other information that is not considered dur-ing indexing | they are iden tical. This means that the documen ts will be indistinguishable to a searc h engine at retriev al time. Retriev al-equiv alen t documen t sets pro vide a very strong assurance of redundancy , as their textual con-ten t is virtually iden tical.

The algorithm for detecting retriev al equiv alence follo ws from the de nition: documen ts in the collection are canon-icalized and then hashed using the MD5 algorithm (Riv est 1992). The list of hash values is sorted and all sets of doc-umen ts sharing an iden tical MD5 hash are considered to be retriev al-equiv alen t; the asymptotic cost is thus O ( n log n ) for n documen ts but in practice the dominan t cost is reading the documen ts.

We exp erimen ted with six levels of canonicalization, where eac h level adopts all the measures of previous levels: whites-pace normalized; tags remo ved; punctuation remo ved; case folded; stop words remo ved; and words stemmed. Note that the possibilit y of hash collisions is negligible: MD5 is a 128-bit hash, meaning that the space of possible values is 2 128 ; on a collection of a billion documen ts the likeliho od of a single collision is about 10 25 .

More general detection of syn tactic similarit y for non-identical documen ts is less straigh tforw ard. There are sev-eral metho ds available for determining syn tactic similarit y between documen ts in a text collection, suc h as the I-Matc h algorithm of Cho wdh ury et al. (2002) and relativ e-frequency techniques suc h as those of Hoad &amp; Zob el (2003) and Shiv-akumar &amp; Garc a-Molina (1995). We choose however to use documen t ngerprin ting techniques (Man ber 1994, Brin et al. 1995, Hein tze 1996, Bro der et al. 1997). We do this because the technology has a pro ven record of application to large documen t collections (Bro der et al. 1997, Cho et al. 2000, Fetterly et al. 2003), and because it can easily be tuned to capture di eren t levels of duplication.

Documen t ngerprin ting is based on a pro cess whereb y xed-size documen t chunks | for example, word sequences of length eigh t | are compared to eac h other. The level of similarit y between a pair of documen ts is determined by the num ber of chunks they have in common. This is an app eal-ing measure, as iden tical documen ts have all their chunks in common, and the measured similarit y degrades gracefully as the documen ts div erge as through revisions, deletions and insertions. Con ten t-equiv alen t documen t pairs are exp ected to score highly using suc h schemes. Furthermore, given an appropriate chunk length, false positiv es are unlik ely.
Retaining the full set of chunks for eac h documen t would consume a quan tity of resources sev eral times the size of the source collection. Most documen t ngerprin ting techniques reduce resource consumption by using a heuristic selection function to choose whic h chunks to retain. Furthermore, most systems also hash eac h chunk so that it tak es less space. While hashing introduces the possibilit y of false positiv es, the space of chunks is sucien tly sparse that they are rare.
Once chunks have been selected, they are inserted into a standard inverted index (Witten et al. 1999). For our appli-cation | in whic h we are interested in syn tactic similarit y between all pairs of documen ts in the collection | we can step through eac h postings list in the index and update ac-cum ulators for pairs of documen ts that co-app ear in that list. Shiv akumar &amp; Garc a-Molina (1999) describ e a re ne-men t of this approac h that reduces costs asso ciated with the quadratic expansion of postings list. We use this latter approac h in our work.

The most signi can t di erence between ngerprin ting tech-niques is in their chunk selection pro cess. Most selection techniques use simple heuristics, suc h as selecting chunks based on their hash value or some other sup er cial char-acteristic of the chunk. There are other more sophisticated techniques, suc h as the winno wing algorithm of Schleimer et al. (2003). However, in all cases these selection schemes are lossy; poten tially valuable data is discarded, rendering the algorithms less reliable and less able to distinguish between levels of syn tactic resem blance.

The spex selection algorithm (Bernstein &amp; Zob el 2004) pro vides lossless chunk selection. It pro ceeds from the ob-serv ation that a chunk that occurs only once in a collection does not a ect the calculation of similarit y scores, as the cal-culations are based on chunk co-o ccurrence. Spex uses an iterativ e hashing algorithm to select only chunks that occur more than once in the curren t collection. In most collec-tions, a great man y chunks occur once only , so this scheme pro vides signi can tly reduced index sizes without the loss of accuracy asso ciated with the schemes review ed above. For these reasons we have chosen to use spex as the chunk se-lection algorithm for our exp erimen ts.

We used the S 3 measure (Bernstein &amp; Zob el 2004) to com-pute resem blance between a pair of documen ts u and v : The S 3 score measures the num ber of chunks shared by u and v as a prop ortion of the mean num ber of chunks in u and v . This score tak es on values between 0 : 0 and 1 : 0, making it natural to interpret the score as a similarit y prop ortion or percen tage.

As we intend to use the S 3 score as a predictor of whether a pair of documen ts is con ten t-equiv alen t, we need to estab-lish the correlation between this algorithmically-generated score and the sub jectiv e seman tic notion of con ten t equiv-alence. Having determined the nature of this correlation, we can then choose a threshold S 3 value suc h that a pair of documen ts scoring higher than this value is | with a high degree of reliabilit y | con ten t-equiv alen t.
As the S 3 similarit y scores of pairs of documen ts range smo othly between 0 and 1, we used human assessors to de-termine an appropriate threshold for con ten t equiv alence. For this exp erimen t a large num ber of documen t pairs with S scores evenly distributed between 0.4 and 1 were retriev ed from the QRELS collection describ ed below; based on prior insp ection, to mak e the best use of our assessors we de-cided to classify all pairs with lower scores as not con ten t-equiv alen t. Participan ts were presen ted with a sequence of these documen t pairs in a random order, and the S 3 score was not rev ealed. The participan ts assigned eac h of these pairs into one of the follo wing categories: Level 3. The documen ts have completely equiv alent con-Level 2. The two documen ts are conditionally equiv a-Level 1. The documen ts have nearly equiv alent con ten t Level 0. The documen ts are not equiv alent ; di erences We avoid de ning the term `reasonable' too precisely; a rea-sonable query is a best-e ort attempt at expressing an in-formation need that one migh t plausibly have. A reasonable searc h engine mak es sensible use of available information in the documen ts in order to mak e a best-e ort attempt at answ ering the information need expressed in a given query .
A group of 4 people participated in a pilot study in whic h 420 documen t pairs were analyzed. The study rev ealed that there were man y pairs for whic h con ten t was equiv alen t in all but very particular circumstances. For example, two docu-men ts may have the same body but di eren t navigational links. Although the navigational links would not di er-entiate the documen ts with resp ect to most queries, some queries may directly address these navigational links. In suc h cases the documen ts would not be equiv alen t.
It was this observ ation that suggested the concept of con-ditional equiv alence, in whic h the space of queries can be partitioned into two categories: those for whic h the two documen ts are not di eren tiated, and those for whic h only one or other of the documen ts would be returned by a rea-sonable searc h engine. In other words, no queries exist for whic h both documen ts may reasonably be considered rele-vant to that query and yet at the same time not be consid-ered equiv alen t. The signi cance of conditional equiv alence is that, within a given result list, documen ts of this class can be considered con ten t equiv alen t; the presence of both documen ts in the result list implies that they are equiv alen t for that query . Figure 1: The cumulative proportion of document pairs judge d to be at each equivalenc e level, for S 3 values from 0.4 to 1. Cumulation is from right to left.

This change indicates how the de nition of equiv alence is far from absolute. We believ e that the de nitions that we have arriv ed at are useful for the particular application that we are investigating | web searc h | and that the relativ e consistency of resp onses we receiv ed from our assessors is an indication that the de nitions corresp ond to real-w orld tasks. However, this does not mean that our de nitions are necessarily suitable for a di eren t set of assumptions or circumstances, and other de nitions may be equally suitable for the web searc h domain.

For the main study , a group of 12 participan ts assessed a total of 964 documen t pairs for equiv alence using the above criteria. Figure 1 sho ws the cum ulativ e error rates as the S 3 threshold increases; that is, the values at a particular S 3 value indicate the level of accuracy for all documen ts at and above that score. The graph indicates a reason-ably strong linear relationship between the S 3 score for a documen t and the likeliho od that it is con ten t-equiv alen t. Note the relativ ely low accuracy for level-3 documen t pairs. This result means that man y documen t pairs iden ti ed by spex do have some point of di erence (alb eit a very small one in man y cases). This suggests that it may not be pru-den t to purge documen ts from a collection at index-time, as there may be particular situations in whic h a given docu-men t may be needed despite its near-equiv alence to other documen ts. However, the S 3 score sho ws high levels of ac-curacy for level 1 and level 2 documen t pairs.

The choice of a threshold is somewhat arbitrary given re-sults suc h as these. We chose a threshold for whic h 95% of documen t pairs were classi ed as equiv alen t at level 1 or above. On the data collected, this threshold at whic h level 1 accuracy exceeds 95% is 0.58. The prop ortion of documen ts at this threshold that were categorized at level 2 or above was 87%. We use this threshold value to signify conditional con ten t-equiv alence.

Our results sho w that the S 3 measure is able to accurately iden tify con ten t equiv alence between documen ts. While not failure-pro of, the metho d app ears robust; given that the documen t in a con ten t-equiv alen t pair that would be re-turned to the user is the documen t that is more highly rank ed with resp ect to the query , and that the other docu-men t in the pair should be available to the user on request, the incidence of information loss is likely to be low. Collection Year crawled Size (GB) # documen ts GOV1 2002 18.1 1,247,753 GOV2 2004 426.0 25,205,179 Table 1: Statistics for the GO V1 and GO V2 collec-tions.
 Table 2: Numb ers of duplic ate les remove d from GO V1 and GO V2 during the original crawl.
We mak e use of three documen t collections for these ex-perimen ts: GO V1, GO V2, and QRELS, whic h are collec-tions of web documen ts from the .gov domain created for TREC. The GO V1 and GO V2 collections are crawls of the .gov domain; see Table 1 for details. According to the TREC trac k information, some duplicate documen ts have already been remo ved from these collections. Table 2 sho ws the num ber of les remo ved. QRELS is describ ed later. For the GO V1 collection, we use the spex algorithm of Bernstein &amp; Zob el (2004) to determine retriev al-equiv alence and con ten t-equiv alence. For the GO V2 collection, we were only able to iden tify retriev al-equiv alence. Although spex app ears to be reasonably scalable, pro cessing sev eral giga-bytes an hour, the 426 GB GO V2 collection is too large for spex on our hardw are.
 Equivalence in GO V1. We tested the six levels of retriev al equiv alence on the GO V1 collection. We found that when only whitespace and HTML tags were remo ved, there were 21,840 sets of retriev al-equiv alen t documen ts for a total of 97,048 documen ts. When all further transformations were applied, this increased to 22,870 sets for a total of 99,227 documen ts. In other words, most of these documen ts be-came iden tical by virtue of having their HTML tags re-moved. Few additional documen ts were iden ti ed as a result of further transformations. In ligh t of this, all further exp er-imen ts applied all transformations, as a stricter de nition of retriev al equiv alence would not change the num bers much. In general, documen ts in these sets can be considered com-pletely con ten t-equiv alen t (lev el 3) and in man y cases all but one of these documen ts can be completely eliminated from the collection prior to indexing.

When we use spex on the collection, we nd a total of 215,314 documen ts that are participating in a con ten t equiv-alence relationship, or 116,087 additional documen ts com-pared to retriev al-equiv alence only . In total this means that 17.3% of documen ts were non-unique within this relativ ely small collection, already a relativ ely high gure. As collec-tions gro w larger and more comprehensiv e, one would exp ect this prop ortion to gro w.

We remark ed above that the error rate for level 3 equiv-alence was too high to recommend pruning the collection at index time. Nonetheless, keeping a record of con ten t-equiv alence relationships within the collection enables e-cien t mo di cation of result lists at query time, and gives an indication of the overall level of redundancy in the collection. equiv alence in the GO V2 corpus found 865,362 retriev al-Figure 2: Distribution of retrieval-e quivalent set sizes in GO V2, on log-lo g axes. equiv alen t sets consisting of a total of 6,943,000 documen ts. Thus, a total of 6,077,638 documen ts in the collection are entirely redundan t for retriev al purp oses, in addition to the 2,950,950 duplicate documen ts that had already been re-moved at crawl time. These 6,077,638 documen ts represen t nearly 25% of the documen ts in the ocial corpus, whic h is consisten t with previous investigations of text duplication in web crawls (Bro der et al. 1997, Fetterly et al. 2003), whic h rep orted gures in the range of 25%{30%. We speculate, based on our results for GO V1, that further large num bers of documen ts will be redundan t according to the con ten t-equiv alence measure.

The results on GO V2 sho w sev eral extremely large sets of documen ts that are retriev al-equiv alen t. The largest set encompasses 512,030 documen ts | 2% of all the documen ts in the GO V2 collection. Figure 2 sho ws the frequency of oc-currence of sets of various sizes. The linear character of the distribution on double-log axes suggests a power-la w distri-bution. This is consisten t with the results of Fetterly et al. (2003), with the graph sho wing a high degree of similarit y to graphs constructed from a crawl of 150,000,000 documen ts from the general web. There is even a similar curious arte-fact at set sizes of about 100, whic h Fetterly et al. attribute to mirroring. There were man y large sets that con tained reasonably information-ric h documen ts. An example of a documen t that occurred 300 times is in Figure 3.
The TREC terab yte trac k (Clark e et al. 2004) ran for the rst time in 2004. The terab yte trac k is intended as a forum for assessing the retriev al performance of searc h engines on a far larger (and more consisten t) web collection than had previously been available to the general researc h comm unit y.
The data set used for evaluation for the 2004 terab yte trac k consists of two comp onen ts: the collection itself | 426 GB of data | and a set of 50 topics that de ne queries on the collection. Participan ts were required to run the set of queries on the collection using their searc h system and, for eac h query , submit the top 10,000 results as rank ed by their soft ware. In order to assess precision and recall, the top 100 documen ts for eac h query from eac h run were added to an assessmen t pool. The documen ts in the assessmen t pool were given one of three relev ance values by an exp ert human judge, corresp onding to not relev ant, relev ant and highly relev ant. For the purp oses of ocial assessmen t, the Figure 3: A document that app ears 300 times in the GO V2 collection. latter two were treated as equiv alen t. Documen ts that were not in the pool were assumed to be non-relev ant.

We wished to examine the occurrence patterns of con-ten t equiv alence amongst documen ts that app eared in the relev ance judgemen ts for the 2004 terab yte trac k. To this end, we created a sub collection of the GO V2 collection, the QRELS collection, consisting exclusiv ely of the human-assessed documen ts for the 50 queries of the GO V2 corpus. It consists of 58,078 documen ts and is 2.8 GB, or nearly one-sixth of the total size of GO V1.

There are sev eral reasons why this subset of documen ts is worth investigating. Documen t access patterns are ex-tremely skewed, suc h that some documen ts are accessed ex-tremely frequen tly and others rarely or not at all (Garcia et al. 2004). For example, the documen ts that app eared in the largest sets discussed in Section 6 would be relev ant to few queries and as suc h accessed rarely . The documen ts in the QRELS collection were retriev ed in resp onse to the carefully form ulated TREC topics and as suc h have demon-strated their utilit y. The signal-to-noise ratio is exp ected to be much higher in the QRELS than amongst the collection as a whole. We note that the average size of a documen t in QRELS is 49.3 KB, or 2.8 times larger than the collection average of 17.7 KB. This is indicativ e of the higher informa-tion con ten t of these documen ts.

Furthermore, the QRELS documen ts have been assessed for relev ance by exp ert human judges. By examining the as-sessed documen ts for eac h query , we can analyze the prev a-lence of con ten t equiv alence amongst documen ts that are judged relev ant. Finally , the QRELS documen ts allo w us to observ e the incidence of con ten t-equiv alen t documen ts that have been inconsisten tly classi ed by the judges. This gives some indication of the overall qualit y of relev ance judge-men ts for the queries in this collection.

A total of 23.9% of all documen ts in the collection were found to exhibit con ten t equiv alence with at least one other documen t, but for some queries the gure was far higher. Only a minorit y of the con ten t-equiv alen t documen ts are retriev al-equivalen t; this sho ws that con ten t equiv alence has a real e ect on searc h results.

Figure 4 sho ws the degree of documen t redundancy | the prop ortion of documen ts that can be eliminated for a given query because they can be represen ted by an equiv alen t doc-umen t | on a per-topic basis. The lower histogram sho ws the percen tage of all relev ant documen ts for eac h topic that are redundan t, while the upp er histogram displa ys the same data for irrelev ant documen ts. (Topic 703 was not included in the nal TREC judgemen ts, accoun ting for the absence of results for that query .) Taken across all queries, the mean percen tage of redundan t documen ts amongst the relev ant set was 16.6%, while the mean amongst the irrelev ant set was 14.5%. If we consider only retriev al-equiv alen t documen ts, mean redundancy amongst the relev ant and irrelev ant sets is 2.3% and 4.0% resp ectiv ely.

We note that the observ ed incidence of con ten t equiv a-lence in the QRELS collection is lower than in the GO V2 collection as a whole. This is not surprising, as much of the con ten t equiv alence in the GO V2 collection occurred on low-value documen ts, suc h as the serv er-generated error pages and searc h forms that occurred in the large sets discussed above. Despite this, redundan t documen ts still represen t a signi can t prop ortion of all documen ts in both the relev ant and nonrelev ant judged sets of the QRELS collection. The fact that, on average, 16.6% of relev ant documen ts are re-dundan t means that the user will in man y cases be viewing relev ant documen ts that they have seen before. These doc-umen ts, though relev ant to the topic, are no longer relev ant to the user's information need as they are entirely lacking in novelty. This suggests that e ectiv eness gures calculated for the terab yte trac k runs could be signi can t overestimates of the user exp erience, and that the user exp erience could be substan tially impro ved by remo ving instances of redundan t documen ts from result lists.
 agged as con ten t-equiv alen t con tained inconsisten t relev ance judgemen ts; one documen t was judged relev ant while the other was judged irrelev ant. Equiv alen t documen ts should not vary with resp ect to their relev ance; thus, either the pairs were incorrectly classi ed as con ten t equiv alen t or one of the documen ts in the pair was erroneously judged. This pro vides a con venien t opp ortunit y to assess the overall con-sistency of relev ance assessmen t for this collection.
All documen ts that were connected by con ten t-equiv alence relationships were aggregated into a single group. Any group in whic h not all documen ts receiv ed the same relev ance judge-men ts were iden ti ed as con taining an inconsistency . We man ually examined 20 randomly selected groups that had been inconsisten tly judged and found that in all cases the judgemen ts ough t not have disagreed.

In total across all queries there were 465 groups where judgemen ts were inconsisten t. Within these groups, 791 documen ts were judged relev ant and 681 were judged non-relev ant. Assuming | for the sak e of argumen t | that the ma jorit y of judgemen ts in eac h groups was correct, on aver-age across all groups 3.8% of the judgemen ts (522 of 13854) were incorrect. If we consider only the set of potential ly rel-evant documen ts | that is, the set of all documen ts that are either relev ant or form part of an inconsisten t group | we have 522 erroneous judgemen ts out of 4,013 documen ts, an error rate of 13.0%.

This evidence seems incon trovertible: the same judge has inconsisten tly assessed iden tical or near-iden tical documen ts in man y cases. We have no evidence of assessmen t qualit y for the documen ts that do not have a con ten t equiv alence relationship with any other documen ts, but there is no rea-son to supp ose that accuracy for these other documen ts is any higher. dancy has a negativ e e ect on the e ectiv eness of searc h. In this section we quan tify the exten t of this e ect on the 70 ocial runs submitted to the 2004 TREC terab yte trac k.
We reiterate the novelty principle, discussed earlier: a documen t | though relev ant in isolation | should not be regarded as relev ant if it is the same as a documen t the user has already seen. In order to mo del this principle, we mo d-i ed the ocial judgemen ts for eac h run so that documen ts app earing in a result list after another documen t with whic h they were con ten t-equiv alen t were mark ed as irrelev ant, re-gardless of their judged relev ance status.

In order to discoun t the e ect of poorly performing runs we adopted the metho dology used by Voorhees &amp; Buc kley (2002) and Sanderson &amp; Zob el (2005) and discarded the bottom 25% of runs, leaving us in this case with 54 runs. We used the trec eval tool for evaluation and recorded the mean average precision (MAP) (Buc kley &amp; Voorhees 2000) for eac h of the runs.

Evaluating the TREC runs as submitted resulted in an average MAP across the runs of 0.201. When the novelty principle was mo delled as describ ed above, the average MAP fell to 0.161, a relativ e reduction in MAP of 20.2%. This is a substan tial di erence, and demonstrates that the as-sumption of indep enden t relev ance is in ating e ectiv eness scores, and that redundan t documen ts almost certainly have a signi can t impact on the user exp erience of searc h. In a scenario when we are exploring the e ect of eliminating re-dundan t documen ts, it is clear that 0.161, not 0.201, is the correct baseline for comparison.

To sim ulate an information retriev al system that is aware of con ten t-equiv alen t documen ts, we mo di ed the runs so that documen ts app earing after another documen t with whic h they were con ten t-equiv alen t were remo ved from the result list. The average MAP increase to 0.186, a relativ e 16.0% impro vemen t in MAP compared to our baseline, demon-strating that an equiv alence-a ware retriev al system is able to substan tially impro ve the user's searc h exp erience.
That the impro vemen t is, by construction, observ ed in every query with redundan t answ ers and that the degree of impro vemen t is closely coupled to the degree of redundancy observ ed in the QRELS collection does nothing to invalidate this result. Again, we note that redundan t answ ers can add no weigh t to a user's con dence in information. We also note that this impro vemen t is obtained on the same under-lying collection and on the same set of redundancy-mo delled relev ance assessmen ts.

These results are presen ted in graphical form for eac h of the individual runs in Figure 5. In all cases the ocial MAP result overstates the e ectiv eness of the run. Interestingly , the impro vemen t from remo ving duplicates was greater for runs that sho wed overall better e ectiv eness. This is an interesting phenomenon that will be studied in future work.
We have explored a class of similarit y we call con ten t equiv alence; documen ts are con ten t-equiv alen t if they con-tain the same information as eac h other. We con tend that the presence of con ten t-equiv alen t documen ts in a result list is not in general of bene t to the user. Documen t nger-prin ting was sho wn via a user study to be a robust metho d for iden tifying con ten t-equiv alence between documen ts. Us-ing our metho d, we found that over 17% of documen ts in GO V1 were non-unique under con ten t equiv alence, more than double the gure of the stricter retriev al equiv alence. Almost 25% of GO V2 was redundan t under retriev al equiv-alence; our analysis of judged documen ts and extrap olation from GO V1 suggests that as man y documen ts again may be non-unique under con ten t equiv alence, suggesting a high degree of documen t redundancy in this larger collection.
We also sho wed that con ten t-equiv alence has a signi can t impact on actual searc h results from a wide variet y of searc h metho dologies. Purging con ten t-equiv alen t documen ts from results lists impro ves novelty-based MAP on the TREC ter-abyte queries by an average of 16.0% over results generated by 54 di eren t ranking algorithms. Disturbingly , our study exp oses a signi can t degree of inconsistency in the human relev ance judgemen ts used for evaluating the performance of searc h algorithms in the TREC terab yte trac k. This work was supp orted by the Australian Researc h Coun-cil and by an RMIT VRI I gran t. Allan, J., Wade, C. &amp; Boliv ar, A. (2003), Retriev al and the run.
 Bernstein, Y. &amp; Zob el, J. (2004), A scalable system for Brin, S., Davis, J. &amp; Garc a-Molina, H. (1995), Cop y Bro der, A. Z., Glassman, S. C., Manasse, M. S. &amp; Zweig, Buc kley , C. &amp; Voorhees, E. M. (2000), Evaluating Cho, J., Shiv akumar, N. &amp; Garcia-Molina, H. (2000), Cho wdh ury , A., Frieder, O., Grossman, D. &amp; McCab e, Clark e, C., Crasw ell, N. &amp; Sob oro , I. (2004), Overview of Fetterly , D., Manasse, M. &amp; Najork, M. (2003), On the Garcia, S., Williams, H. E. &amp; Cannane, A. (2004), Harman, D. (2002), Overview of the TREC 2002 Novelty Hearst, M. A. &amp; Pedersen, J. O. (1996), Reexamining the Hein tze, N. (1996), Scalable Documen t Fingerprin ting, in Hoad, T. C. &amp; Zob el, J. (2003), `Metho ds for Iden tifying Man ber, U. (1994), Finding Similar Files in a Large File Riv est, R. (1992), `The MD5 Message-Digest Algorithm'. Sanderson, M. &amp; Zob el, J. (2005), Information Retriev al Schleimer, S., Wilk erson, D. S. &amp; Aik en, A. (2003), Shiv akumar, N. &amp; Garc a-Molina, H. (1995), SCAM: A Shiv akumar, N. &amp; Garc a-Molina, H. (1999), Finding Sob oro , I. &amp; Harman, D. (2003), Overview of the TREC van Rijsb ergen, C. J. (1979), Information Retrieval , Voorhees, E. M. &amp; Buc kley , C. (2002), The e ect of topic Witten, I. H., Mo at, A. &amp; Bell, T. C. (1999), Managing Zhai, C. X., Cohen, W. W. &amp; La ert y, J. (2003), Bey ond Zhang, Y., Callan, J. &amp; Mink a, T. (2002), Novelty and
