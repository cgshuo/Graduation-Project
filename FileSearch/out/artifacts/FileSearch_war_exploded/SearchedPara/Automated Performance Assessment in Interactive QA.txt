 In interactive question answering (QA), users and systems take turns to ask questions and provide answers. In such an interactive setting, user questions largely depend on the answers provided by the system. One question is whether user follow-up questions can provide feedback for the system to automatically assess its performance (e.g.,assess whether a correct answer is delivered). This self-awareness can make QA systems more intelligent for information seeking, for ex-ample, by adapting better strategies to cope with problem-atic situations. Therefore, this paper describes our initial investigation in addressing this problem. Our results in-dicate that interaction context can provide useful cues for automated performance assessment in interactive QA. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Search Process General Terms: Experimentation, Performance Keywords: Performance Assessment, User Behavior, In-teractive Question Answering
Interactive question answering has been identified as one of the important directions in QA research [1]. In interactive QA, users and systems take turns to ask question and pro-vide answers. In such an environment, questions formed by a user not only depend on his/her information goals, but are also influenced by the answers from the system. Because of this dependency, our assumption is that user follow-up ques-tions can provide feedback for the system to assess the status of preceding answers (e.g., whether a correct answer is de-livered). The awareness of its own performance will enable the system to automatically adapt better strategies to cope with problematic situations. To our knowledge, there has not been much work that addresses this important aspect of interactive QA. This paper describes our initial investigation on this problem. Given a question Q i and its corresponding answer A i , the specific question examined here is whether the user language behavior in the follow-up question Q i +1 and the interaction context can help the system to assess its performance at answering the preceding question ( A i ).
To address this question, we conducted a user study where users interacted with a control led QA system to find informa-tion of interest. Our studies indicate that when the system fails to deliver a desired answer, users do exhibit some lan-guage behavior (e.g., rephrase of the question) in the follow-up question to respond to this problematic situation. User behavior and interaction context can provide important cues for a QA system to automatically identify problematic situ-ations. Based on the data collected from our studies, we ex-perimented with three classifiers (Support Vector Machine, Maximum Entropy Model, and Decision Tree). Our results indicate that the Decision Tree model can detect problem-atic situations with 73.8% accuracy, which is significantly better than the baseline.
To investigate the role of interaction context in auto-mated performance assessment, we conducted a controlled user study where a human wizard was involved in the inter-action loop to control and simulate problematic situations. Users were not aware of the existence of this human wizard and were led to believe they were interacting with a real QA system. This control led setting allowed us to focus on the in-teraction aspect rather than information retrieval or answer extraction aspect of question answering. More specifically, during interaction after each question was issued, a random number generator was used to decide if a problematic sit-uation should be introduced. If the number indicated no, the wizard would retrieve a passage from a database with correct question/answer pairs. Note that in our experiments we used specific task scenarios (described later), it is pos-sible to anticipate user information needs and create this database. If the number indicated that a problematic sit-uation should be introduced, then the Lemur retrieval en-gine 1 was used on the AQUAINT collection to retrieve the answer. Our assumption is that AQUAINT data are not likely to provide an exact answer given our specific scenar-ios, but they can provide a passage that is most related to the question. The use of the random number generator was to control the ratio between the occurrence of problematic situations and error-free situations. In our initial investiga-tion, since we are interested in observing user behavior in problematic situations, we set the ratio as 50/50. As a re-sult, this simulation generated 56% error-free situations and 44% problematic situations. In our future work, we will vary this ratio (e.g., 70/30) to reflect the performance of state-of-the-art factoid QA and investigate the implication of this ratio in automated performance assessment.

Eleven users participated in our study. Each user was asked to interact with our system to complete information http://www-2.cs.cmu.edu/ lemur/ seeking tasks related to four specific scenarios. Each of the four scenarios was focused around a separate topic: the 2004 presidential debates , Tom Cruise , Hawaii ,and Pompeii .As a result of this study, a total of 456 QA exchanges from 44 interactive sessions were collected, where each answer was annotated with a binary tag to indicate whether or not the answer was problematic.
We formulate automated performance assessment as a classification problem. Given a question Q i with a corre-sponding answer A i , our goal is to decide whether A i problematic based on the follow up question Q i +1 and the interaction context. More specifically, the following set of features are used: (1) Target matching(TM) : a binary fea-ture indicating whether the target type of Q i +1 is the same as the target type of Q i . Our data show that the repeti-tion of target type may indicate a question rephrase, which could signal a problematic situation has just occurred. (2) Named entity matching (NEM) : a binary feature indicating whether all the named entities in Q i +1 also appear in the Q . If no new named entity is introduced in Q i +1 , it is likely Q i +1 is a rephrase of Q i .(3) Similarity between questions (SQ) : a numeric feature measuring the similarity between Q i +1 and Q i .(4) Similarity between content words of ques-tions (SQC) : this feature is similar to the previous feature (i.e., SQ) except that the similarity measurement is based on the content words excluding named entities. This is to pre-vent the similarity measurement from being dominated by the named entities. (5) Similarity between Q i and A i (SA) . (6) Similarity between Q i and A i only based on the content words (excluding named entities)(SAC) .
 To measure the similarity between two chunks of text T 1 and T 2 , we applied the following equation proposed by Lin [2]: where P ( w ) was calculated based on 1806 pseudo documents (i.e., question/answer pairs) from previous TREC evalua-tions.

We experimented with three classification approaches (Max-imum Entropy Model from MALLET 2 ,SVMfromSVM-Light 3 , and Decision Trees from WEKA 4 ) based on ten fold cross-validation (90% of data was used as training data and 10% as testing data in each trial). Table 1 shows the accu-racy of the three approaches on identifying problematic/error-free situations using different combinations of features. The baseline was obtained by simply assigning the most fre-quently occurring class (i.e., 56% of correct situations in our data). The best performance for each model is highlighted in bold in Table 1. The Decision Tree model achieves the best performance of 73.8% in identifying problematic situations, which is more than 17% better than the baseline perfor-mance. Different combinations of features result in different performance in all three models. In general, the feature set that considers different forms of question/answer similarity works better than those that do not consider these aspects. http://mallet.cs.umass.edu/index.php/ http://svmlight.joachims.org/ http://www.cs.waikato.ac.nz/ml/weka/ Table 1: Accuracy of automated performance as-sessment based on three approaches
Identification of problematic situations can be considered as implicit feedback. One might think that an alternative way is to explicitly ask users for feedback (for example, with a feedback button). However, soliciting feedback after each question not only will frustrate users and lengthen the in-teraction, but also it may not be possible for certain devices (e.g., PDA). Therefore, our focus here is to investigate the more challenging end of identifying problematic situations through implicit feedback. In real interaction, explicit and implicit feedback should be intelligently combined. For ex-ample, if the confidence for identifying a problematic situa-tion or an error-free situation is low, then perhaps explicit feedback can be solicited.
This paper presents our initial investigation on automated performance assessment in interactive question answering. Our studies indicate that when a problematic situation oc-curs (i.e., retrieved answer does not appear to be correct), users exhibit distinctive behavior such as rephrasing the question. Follow-up questions and interaction context can provide useful cues for the system to automatically evaluate its performance. Although our current evaluation is based on the data collected from our study, the same approaches can be applied during online processing as the question an-swering session proceeds. Such performance assessment can provide feedback directly to a QA system as to what ques-tions the system may have correctly answered and what questions the system may have trouble with. This will not only allow the system to automatically adapt better strate-gies during online processing but also provide a mechanism to automatically build databases of question answering pairs for other applications(e.g., collaborative question answer-ing). [1] J. Burger and et al. Issues, tasks and program [2] D. Lin. An information-theoretic definition of
