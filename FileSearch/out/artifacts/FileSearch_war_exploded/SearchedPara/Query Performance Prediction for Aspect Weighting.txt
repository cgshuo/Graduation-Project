 Accurate estimation of query aspect weights is an important issue to improve the performance of explicit search result di-versification algorithms. For the first time in the literature, we propose using post-retrieval query performance predic-tors (QPPs) to estimate, for each aspect, the retrieval effec-tiveness on the candidate document set, and leverage these estimations to set the aspect weights. In addition to utiliz-ing well-known QPPs from the literature, we also introduce three new QPPs that are based on score distributions and hence, can be employed for online query processing in real-life search engines. Our exhaustive experiments reveal that using QPPs for aspect weighting improves almost all state-of-the-art diversification algorithms in comparison to using a uniform weight estimator. Furthermore, the proposed QPPs are comparable or superior to the existing predictors in the context of aspect weighting.
A major challenge for search engines is satisfying its users with a few top-ranked query results in a setting where users X  search intents are inherently ambiguous or underspecified. To address this problem, a promising solution is diversifying the top-ranked results so that they are both relevant to a given query, but, at the same time, can cover its different aspects.

Approaches for search result diversification are typically categorized as either implicit or explicit [11]. Given a set of candidate documents initially retrieved for a query, implicit methods aim to discover the possible different query aspects by utilizing the content of these documents. In contrast, explicit diversification methods directly model the query as-pects, exploiting manually or automatically assigned query labels in a taxonomy [1], or query reformulations in a search log [11]. In the latter case, aspects weights that can rep-resent the importance [11], popularity [2] or likelihood [1] of each aspect for a given query is of utmost importance to optimize the quality of the final result.

In this paper, we put a new perspective on aspect weight-ing to improve the performance of explicit search result di-versification. The weight to be assigned to an aspect in a diversification method should not only depend on the as-pects X  intrinsic properties (such as those exemplified above), but it should better reflect the expected retrieval effective-ness of the top-ranked results (in the candidate set) that match to this aspect. We explain the underlying intuition as follows. In a typical explicit diversification framework, the relevance score of candidate documents for each explicit aspect is computed (using some retrieval model); and each aspect contributes its highest scoring documents to the final query result, which is typically of size 10 or 20. Thus, given an aspect (regardless of how important or likely it is for a given query), if the candidate documents with the highest matching scores to this aspect are indeed irrelevant, such an aspect cannot help improving the final result quality, and may even degrade it.

In this light, we propose leveraging query performance predictors (QPPs) to estimate the retrieval effectiveness of the query aspects over the candidate documents. To this end, we employ post-retrieval QPPs that are based on score distribution analysis, namely, weighted information gain (WIG) [14], normalized query commitment (NQC) [12] and their variants presented in [8]. The choice of these QPPs is intentional, to satisfy the demanding efficiency require-ments of online query processing. As mentioned above, all state-of-the-art explicit diversification algorithms [1, 5, 11, 9] compute the relevance of aspects to candidate documents, and hence, the input to these predictors will be created for free, without any additional cost or effort. To the best of our knowledge, no previous work employs QPPs for weighting query aspects in the context of search result diversification.
As our second contribution, we introduce three new pre-dictors that are again based on the score distribution anal-ysis and hence, directly applicable in aspect weighting sce-nario. The first one is a simple yet effective QPP that is based on the score ratios. The other two predictors are novel in that their performance estimations are based on a virtual document that yields the best possible relevance score for a given query aspect (in a similar manner to the score normalization technique presented in [9]).

We evaluate the existing and proposed QPPs in the con-text of aspect weighting using the standard TREC Diver-sity Task framework. Our experiments include a wide range of explicit diversification methods, namely, IA-Select [1], xQuAD [11] (and its variants proposed in [9]), PM2 [5], and CombSUM, a well known score-based aggregation strategy adapted to diversification problem [9]. Our findings show that, performance based weighting of query aspects consis-tently improves the result quality for these algorithms. Fur-thermore, the proposed predictors are superior to the exist-ing QPPs when applied in the context of aspect weighting.
Let X  X  assume that a given query q retrieves an initial set of N documents, i.e., so-called the candidate set D q , over a corpus C . The goal of result diversification is constructing a ranking D k q of k documents that maximizes both relevance and diversity. In case of the explicit result diversification, it is assumed that there is a set of explicitly identified query aspects (a.k.a., sub-topics, interpretations, sub-queries) de-noted as T = { q 1 ,...,q m } associated with the original query q . These aspects are usually obtained from external re-sources, such as a taxonomy or query log.

In most explicit diversification methods (as discussed in the next section), there is an aspect weight component, which may represent the likelihood, popularity or impor-tance of a given aspect q i for the query q . This aspect weight can be assigned in various ways. For instance, Agrawal et al. employ a classifier trained on the ODP taxonomy to as-sociate categories (as aspects) to the queries along with the class likelihood scores (as weights) [1]. Santos et al. apply three different methods to compute aspect weights, the sim-plest being the uniform probability assigned as a weight to each aspect [11]. They also suggest weighting methods based on the number of results retrieved by the query aspects from an external collection (e.g., using a search engine) and the local corpora C (in a similar manner to resource selection methods employed in distributed retrieval systems). In their work, the simple uniform estimator is reported to yield the best performing aspect weights, and hence, it is also adopted in the succeeding works by others (like [5, 9]).

In this paper, we propose a novel perspective for aspect weighting that is different from all the aforementioned ap-proaches. Our proposal is based on the observation that the most successful explicit diversification methods (such as [5, 11]) compute and exploit the relevance rel ( d,q i ) of each can-didate document d  X  D q to each aspect q i during the diver-sification process. Furthermore, since the ultimate goal is coming up with a final ranking D k q and there may be several aspects of a query, only the highest scoring documents for an aspect can have a chance to be selected into this final ranking. Subsequently, an aspect q i can improve the quality of the final result only if its top-p documents over the candi-date set, D p q i , is highly relevant to q i . This suggests that the effectiveness of D p q i for the aspect q i is a natural indicator of the weight that should be assigned to q i during diversifica-tion. Hence, in this paper, we propose using QPPs to assign weights to query aspects in result diversification algorithms.
Since the rankings D p q i per aspect are typically computed by the state-of-the-art diversification methods, it is a natural choice to employ post-retrieval QPPs that rely on the score distribution analysis for aspect weighting task. By doing so, we avoid additional costs that may be incurred by the pre-dictors and can satisfy the demanding requirements of on-line query processing in large-scale search engines. In what follows, we describe these baseline QPPs (in addition to sim-ple uniform estimator) adopted for query aspect weighting. Next, in Section 2.2, we introduce our own QPPs that are again based on score distributions. Uniform predictor. This is the straightforward approach employed in several earlier works [5, 11, 9]. For a query with the set of aspects T = { q 1 ,...,q m } , the aspect weights are computed as W ( q i ) = 1 /m .
 Weighted Information Gain (WIG). This predictor is originally proposed to capture the divergence between the mean retrieval score of top ranked documents and that of the entire corpus [14]. To compute WIG, we use Eq. 1 presented in [3]. Note that, rel ( C,q i ) represents the relevance score of the corpus C to the aspect q i , and it further helps to make different aspect weights comparable, i.e., serves as a normalization factor.

W ( q i ) = 1 Normalized Query Commitment (NQC). Shtok et al. propose that the mean retrieval score for the top-ranked results of a query represents the score of a possible misleader (as the result list would include some irrelevant documents besides the relevant ones) [12]. Therefore, NQC computes the standard deviation of the relevance scores over the list D i and again normalizes the result value by the relevance score of the corpus (Eq. 2).

W ( q i ) = ScoreAvg. Markovits et al. employ a simpler variant of WIG in a data fusion setting [8]. In this variant, called here ScoreAvg, instead of using rel ( C,q i ) for normalization as in WIG, the relevance scores rel ( d,q i ) are sum normalized to [0, 1] before computing their average.
 ScoreDev. This method [8] is a variant of NQC, and applies Eq. 2 without the normalization factor rel ( C,q i ). Note that, there are other works [10, 4, 12] that again make use of the standard deviation of the document scores in various ways, and not considered here for the sake of space. ScoreRatio. This predictor is motivated by the intuition that as the gap between the scores of the documents in a ranking widens, the likelihood of seeing irrelevant documents also increases. Thus, the ScoreRatio predictor computes the ratio of the scores of the first and last documents in D p VScoreAvg. In a recent work, we have shown that explicit diversification algorithms are quite sensitive to techniques that are employed for normalizing the relevance scores be-tween documents and query aspects [9]. Furthermore, we have proposed an effective score normalization technique, so-called Virtual, which we adapt here for the purposes of query performance prediction.

Our virtual-score based predictors differ from the previ-ously described QPPs in the following way. Instead of con-sidering the score of the entire corpus (as a huge single doc-ument) for normalization (as in WIG or NQC), we consider a virtual document that can yield the highest possible rele-vance score for a query aspect q i on a given corpus. More specifically, for a given aspect q i , we assume a virtual doc-ument d V that includes each term in the aspect with the frequency of the document length and no other terms, i.e., as if the document is only composed of the query terms. The length of the virtual document is set to the average docu-ment length in the corpus. Then, we compute the relevance score of this virtual document d V to q i as an upper-bound value, i.e., the score of an imaginary perfect match for this aspect. Assume that for a given q i , the virtual(-normalized) scores for each d in D p q i are defined as follows:
Then, VScoreAvg predictor computes the weight of an aspect q i as shown in Eq. 4 VScoreFirst. Inspired from the earlier approaches that use highest retrieval score as an indicator of the query perfor-mance [13], for each aspect q i , we use the virtual score of the top-ranked document in D p q i . Dataset. We use the standard framework of TREC Diver-sity Task. In particular, we employ ClueWeb09 collection Part-B that includes around 50 million English web docu-ments. We report our results for TREC 2010 topic set that includes 48 query topics. For each topic, a number of as-pects (up to 8) are described and the relevance judgments are provided at the aspect (sub-topic) level. In the experi-ments, following the common practice in previous works [5, 11], we use the  X  X uery X  field of the topic as the initial query and generate aspects using the official sub-topic descriptions. Initial retrieval model. For each query, we first retrieve top-N candidate documents, D q , and then run the diversi-fication methods to obtain the final top-k results, D k q the initial retrieval, we use our homemade IR system with the well-known Okapi BM25 metric. In our experiments, we set N = 100 and k = 20.

We also apply spam filtering by utilizing the publicly avail-able Waterloo Spam Rankings 1 that assigns a spam per-centile score to each document in the ClueWeb09 collection. During the initial retrieval, the documents with a spam score less than 60 are eliminated from the candidate set.
In this paper, we employ various explicit diversification methods that can be broadly categorized as greedy ap-proaches and aggregation-based approaches. While outlin-ing these methods we conform to their original descriptions that are typically based on a probabilistic mixture model, where P ( d | q ) ( P ( d | q i )) represents the likelihood of a docu-ment for a given query (aspect), respectively; and P ( q corresponds to the aspect weight. In our experiments, for the former probability, we employ rel ( d,q ) and rel ( d,q scores that are computed by BM25 retrieval model, after normalizing them with one of the techniques discussed later in this section. For the latter probability, aspect weight, we use the baseline and proposed QPP strategies described in the previous section. While doing so, the weights computed for the aspects of a query are sum normalized to [0, 1] so that they can replace P ( q i | q ) in the following methods. http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/ Greedy methods. These methods run in rounds. In each iteration, they score the documents in D q and select the document d with the highest score into D k q . They differ in the scoring function used in the latter stage.
 Intent Aware (IA)-select. This method aims to choose the document with the highest probability of satisfying the user given that all previously selected ones fail to do so [1]. The scoring function of IA-Select is as follows: s ( d,q ) = X where V ( d | q,q i ) is the likelihood of d satisfying q for the underlying aspect q i . As in [11], we replace the latter by P ( d | q i ), and subsequently, by rel ( d,q i ), as discussed above. xQuAD. The original xQuAD algorithm proposed in [11] has the following scoring function: s ( d,q ) = (1  X   X  ) P ( d | q ) +  X  X where  X  is the trade-off parameter between the relevance and diversity, and P (  X  D k q | q i ) denotes the probability of q not being satisfied by the documents that are already in D q . Actually, this latter probability captures the novelty and it can be represented as the product of the probabilities of each document in D k q for not satisfying q i : art xQuAD and geo xQuAD. In a recent work, we have identified that the computation shown in Eq. 5 can lead to a problem, i.e., very small numeric values for the novelty part, due to the nature of the multiplication operation [9]. As a remedy, two variants of xQuAD with superior diversification effectiveness have been introduced, namely, art xQuAD and geo xQuAD. These variants replace the product operation in Eq. 5 with arithmetic and geometric means of the probability values (1  X  P ( d j | q i )), where d j  X  D k q .
 PM2. In PM2 method [5], at a given iteration, first the winner aspect q i  X  is determined by the popularity of the as-pect in D q and number of positions in D k q that are allocated to this aspect up to this iteration (i.e., referred to as quo-tient score ). Next, for this winner aspect q i  X  , PM2 selects the document d that maximizes the following score function: s ( d,q ) =  X   X  qt [ i  X  ]  X  P ( d | q i  X  ) + (1  X   X  ) X where qt [ i ] is the quotient score. Note that, we incorpo-rate the aspect weights assigned by the QPPs into PM2 by modifying the way qt [ i ] is computed after each iteration. Aggregation-based methods. We have recently adapted score and rank(-based) aggregation methods to the diversification problem [9], and found that the most promising method is based on the well-known CombSUM [6, 7] approach. This method computes the scores of all candidate documents in a single round as follows:
In our experiments, for all the diversification strategies that employ the trade-off parameter  X  , we test all values in [0,1] range with a step size of 0.01, and report the results for the  X  values that maximize the  X  -NDCG@20 scores. Normalization techniques. We employ three alternative techniques, namely MinMax, Sum and Virtual [9], to nor-malize the relevance scores as generated by BM25, so that these scores can replace the corresponding probabilities in the diversification methods. Our results are reported for all three techniques, as diversification algorithms are shown to be sensitive to the applied normalization [9].
We evaluate the baseline and proposed QPPs by incor-porating the predicted aspect weights into each of the six diversification algorithms. Note that, for all QPPs, we set the parameter p as 10, i.e., we obtain top-10 documents (out of a candidate set of 100 documents) for each aspect and pro-vide their scores to the performance predictors. Since every query in TREC topic set has more than one aspect and the final ranking has size 20, we believe setting p as 10 would be adequate (as will be justified by the results). We report  X  -NDCG@20 scores computed with ndeval software.

From our results shown in Table 1, we draw the following conclusions: (i) We see that using QPPs for aspect weight-ing improves almost all the diversification methods (13 out of 18 cases) in comparison to assigning uniform weights to each aspect. The absolute improvements in  X  -NDCG scores reach up to 1.5%, whereas the relative improvements are up to 4% (e.g., for the xQuAD method with Sum normal-ization). (ii) Considering the baseline predictors, WIG and ScoreDev are the most effective ones. Among the proposed QPPs, the ScoreRatio predictor outperforms the other two. (iii) Comparing baseline predictors to the proposed ones, we observe that the latter are more effective as the ScoreRatio (VScoreAvg) predictor yields higher  X  -NDCG scores than all the baseline estimators in 10 (11) out of 18 cases, respec-tively. Overall, the proposed predictors yield the highest  X  -NDCG scores in 8 cases (covering all algorithms and most normalization techniques), whereas the baseline estimators yield the best effectiveness for a total of 5 cases. For the first time in the literature, we used post-retrieval QPPs in the context of aspect weighting in explicit search result diversification. To this end, we introduced three new QPPs that are based on score distributions, as well as using several others from the literature. Through extensive exper-iments, we showed that predicting the retrieval effectiveness of each individual aspect on the candidate document set is a good indicator of an aspect X  X  contribution to the quality of the final result. As a future work, we plan to combine and utilize multiple QPPs for aspect weighting.
