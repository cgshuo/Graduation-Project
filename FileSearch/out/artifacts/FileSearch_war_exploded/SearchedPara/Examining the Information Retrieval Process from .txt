 Term-weighting functions derived from various models of re -trieval aim to model human notions of relevance more accu-rately. However, there is a lack of analysis of the sources of evidence from which important features of these term weighting schemes originate. In general, features pertain -ing to these term-weighting schemes can be collected from (1) the document, (2) the entire collection and (3) the query . In this work, we perform an empirical analysis to determine the increase in effectiveness as information from these thre e different sources becomes more accurate.

First, we determine the number of documents to be in-dexed to accurately estimate collection-wide features to o b-tain near optimal effectiveness for a range of a term-weighti ng functions. Similarly, we determine the amount of a docu-ment and query that must be sampled to achieve near-peak effectiveness. This analysis also allows us to determine the factors that contribute most to the performance of a term-weighting function (i.e. the document, the collection or th e query).

We use our framework to construct a new model of weight-ing where we discard the  X  X ag of words X  model and aim to retrieve documents based on the initial physical represen-tation of a document using some basic axioms of retrieval. We show that this is a good first step towards incorporating some more interesting features into a term-weighting func-tion.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models, Search Process Experimentation, Performance Information Retrieval, Models, Term-Weighting
One main focus in research into information retrieval (IR) is the modelling and estimation of the human notion of rel-evance given an information need (query) and a large un-structured collection of information items (e.g. document collection).

Numerous models have been proposed to try and correctly model the notions underlying relevance and subsequent re-trieval. These models have been adopted with the aim of gaining new insights into retrieval and ultimately, to im-prove the effectiveness of the retrieval process. Most model s place both documents and queries into a framework in which they can be operated upon by operations inherent within the algebra of the model itself. Whether a specific model actually uncovers real truths regarding retrieval and rele -vance is open to question. In reality, the ranking functions produced from these models combine similar features in a similar manner to construct a term-weighting function. In general, these features can be collected from three differen t sources (namely the document, the collection and the query itself). Indeed, several works describe term-weighting fu nc-tions solely by the different weights applied to each of these sources using a triple representation [18].

Users typically determine relevance by reading a piece of text (i.e. document) with an information need in mind (i.e. query), equipped with a good knowledge of the language in which they are searching (i.e. collection-wide informa-tion). Some research [9, 10, 11] has aimed to more accu-rately model this process of retrieval by developing a numbe r of axioms for retrieval. These axioms are deemed valid in an inductive framework that supposes a human linearly reading a document with an information need in mind. The degree of relevance (i.e. a numeric score of some type) changes as the human reads the document, depending on whether a human encounters words/phrases that are on-topic or off-topic. This process of linearly scanning a document is usefu l in developing of a set of intuitive and useful axioms for term weighting schemes [10].

However, it is possible to adopt this incremental view to the other sources of evidence mentioned. One can equally view other sources of evidence (e.g. collection and query) i n a similar manner. In this paper, we adopt this incremental approach and iteratively increase the information availab le from the three sources of evidence (the document, the col-lection and the query) to ascertain which of these sources is more sensitive to a lack of information. We apply this method of analysing the retrieval process to a number of state of the art term-weighting functions and present sev-eral interesting findings. In general, we find that very littl e of the query and collection need to be sampled to achieve near-peak performance. Therefore, we concentrate on the process of scoring a document within the same framework. Finally, we present a first step towards creating a new type of term-weighting scheme that relaxes the  X  X ag of words X  ap-proach and instead attempts to inherently model the linear process that is undertaken when a human assesses relevance.
In summary, the contributions of this paper are three-fold:
The remainder of the paper is organised as follows: Sec-tion 2 outlines related work. Section 3 details a number of state of the art term-weighting functions and presents resu lts regarding their performance. Section 4 outlines our incre-mental approach and studies the effectiveness of different sources of evidence in the retrieval process. Section 5 out-lines how new term-weighting functions can be constructed by adopting the inductive process [10]. Section 6 outlines our conclusions and future work.
As previously mentioned there have been many different models proposed for IR. These include the Boolean model, the vector space model [13], classical probabilistic model s [12], language models [17], divergence from randomness mod -els [1] and others. Furthermore, attempts have been made to learn term-weighting functions explicitly using an evolut ion-ary model that artificially induces a  X  X urvival of the fittest  X  paradigm to find suitable term-weighting schemes [6].
Most term weighting schemes assume perfect knowledge of the entire document collection. However, there has been research in the domain of distributed information retrieva l where these assumptions do not hold. To deal with the prob-lems of source selection and results merging, attempts are made to estimate the term frequency distributions in text collections. In these scenarios, one must sample the collec -tions to generate suitable estimates which can be used to guide result fusion. Query-based sampling approaches [4] involve generating a number of queries, submitting these queries to the collections, retrieving the top N documents and then updating the term distribution estimates. These queries should be sampled in an appropriate manner [5] and at appropriate times given a dynamic collection. Related work in resource selection uses evidence gleaned from previ -ous queries to build a suitable sample [15]. Some work [16] has also studied the problem of estimating global features (e.g. idf ) for distributed IR.

However, the work outlined here is different from previous work as our aim is to show how the estimation of information from a number of sources of evidence affects the performance of a number of state of the art term-weighting functions. This process informs us about the sources of evidence and is also a study into term-weighting behaviour. A study of sources of evidence for vertical selection has also been con -ducted recently [2]. However, the task studied therein (ver -tical selection) and the data sources used are quite differen t to those studied in this paper.

A recent approach [10] to modelling the retrieval problem has been to aim to develop a number of axioms and to build up a retrieval foundation from which we can develop new term-weighting schemes. This model supposes a reader en-countering terms as he/she reads a document. This process at least more accurately reflects the process of how a human these axioms (constraints) is contained in [10]. The first constraint (C1) states that adding a new query term to a document must always increase the score of that document. The second constraint (C2) states that adding a non-query term to a document must always decrease the score of that document. The third constraint (C3) states that adding successive query terms to a document should increase the score of the document less with each successive addition. Furthermore, the axioms developed have been to shown by empirical studies to be useful estimators of term-weightin g performance [10, 8]. A fourth constraint (C4) states that adding more non-query terms to a document should de-crease the score of a document less with each occurrence. Furthermore, a proximity constraint (C5) regarding within -document term proximity has also been developed [14].
However, as yet there has been no attempt to create actual term-weighting schemes by directly modelling the inductiv e process previously outlined [10]. This work attempts to rem -edy this situation.
In this section we introduce term-weighting formulas that are derived from different models of retrieval. We perform some preliminary experiments and present the performance of these schemes on TREC data.
One of the best performing term-weighting functions, BM 25 [10], is derived from the probabilistic model of retrieval a nd is defined as follows where tf D t is the frequency of a term t in D and tf Q t frequency of the term in the query Q . dl and dl avg are the length and average length of the documents respectively. N is the number of documents in the collection and df t is the number of documents in which term t appears. k 1 and b are tuning parameters set to 1 . 2 and 0 . 75 by default.
We study a number of other state of the art term-weighting schemes developed from different models of retrieval. We also use the pivoted document length normalisation ( P IV ) [17] from the vector space model, the I ( n ) L 2 function from the divergence from randomness approach ( DF R ) [1], a rank-ing function ( ES ) developed using a evolutionary learning model [7] and a language modelling approach using dirich-let priors ( LM ) [17]. These five term-weighting functions might be constructed by incorporating eye-tracking. cover a wide range of models and are all state of the art in terms of performance. Furthermore, all of these term-weighting schemes use features that are calculated from the three different sources of evidence (i.e. collection, docum ent and query).
We outline here the data used in this work and we measure the performance of the five term-weighting functions on that data. The performance is presented so that the reader gets a general view of the performance of the schemes and can refer back to these absolute values at a later stage. For our analysis and subsequent experiments, we use the FT, FBIS, WSJ and AP collections from TREC disks 1 to 6 as test collections and topics. The LATIMES collection is used later in this work to tune certain parameters in the results section (Section 5). For each set of topics, we creat e a short query set (title field of the topics), a medium length query set (title and description fields) and a long query set (title, description, narrative and concept fields, where av ail-able). Table 1 shows some of the characteristics of the col-lections used in this research. We stemmed the collections using Porter X  X  algorithm and removed standard stop-words.
Table 2 shows the MAP and P @10 for the collections used in this research. The P IV scheme is the poorest perform-ing scheme. In terms of MAP, the best performing function tends to be the ES function, but suffers from a lack of high precision ( P @10) for medium and long queries on some col-lections. As mentioned these schemes use features from the three sources of evidence mentioned. We can confirm that many of the differences between the schemes are statisticall y the influence of each source of evidence for this set of term-weighting schemes.
The term-weighting approaches outlined in the previous section use features from each of the three different sources of information (i.e. collection, document and query). In th is section, we determine the change in effectiveness of each of these term-weighting functions as the information from the se sources becomes more accurate. We wish to determine the percentage of a collection (global information) that must b e indexed to achieve a near optimal level of performance (i.e. MAP) for a term-weighting approach. This type of infor-mation is similar to the information that humans possess regarding the semantic value of a term. Therefore, using this process we can determine the number of documents a isons between all combinations of term-weighting scheme on different datasets for this preliminary analysis. Figure 1: % MAP increase on FT collection for short queries as global information becomes more accurate Figure 2: % MAP increase on FT collection for long queries as global information becomes more accurate human needs to read to gain accurate knowledge about the semantic value of terms in the language. In general, adult reader possess a good knowledge of their language, but for specialised collections this general information may not b e as useful.

To this end, we perform a number of experiments that measures the change in performance as varying amounts of the collection are indexed (sampled). We sample the follow-ing percentages of the collection: (0.01, 0.1, 1, 5, 10, 20, 3 0, 40, 50, 60, 70, 80, 90 and 100). For the graphs that follow in this section, one can determine how quickly the performance reaches 100% (i.e. if the graph rises quickly and remains flat at around 100%, it means we only need to sample a small amount of the source to achieve 100% performance).
We also perform a similar experiment on the document source. We measure the change in performance of a term-weighting function as the information in the document sourc e becomes more accurate. This will inform us whether a hu-man can ignore much of the latter part of the document or whether one must read the complete document to deter-mine relevance. Finally, we perform a similar experiment with the query source, where we increase the number of unique query terms used in the representation and measure the performance as more keywords are added to it. For the global based experiments, we index documents as they are ordered within the TREC collection. For the document and query experiments, we increase the number of terms (infor-mation) in the order in which the document and query is written/read in natural language.
Global information has an effect on the calculation of idf type features, average document length ( dl avg ) features and collection size ( | C | ) features in the term-weighting schemes. Global information is completely accurate once the entire collection has been indexed and therefore, we measure the effectiveness of the term-weighting schemes as a percentage of the performance when all global features are entirely ac-curate.

Figures 1 and 2 show the percentage effectiveness achieved (in terms of MAP ) for all of the term-weighting functions as the number of documents indexed in a collection is increased . For the FT collection, we can see that once 10% of the collec-tion is indexed all the functions have achieved above 95% of the effectiveness that could be achieved for a term-weightin g scheme, when the entire collection is indexed. This is true for all lengths of queries (short, medium and long). The results from the AP, FBIS and WSJ collection (not shown) are very similar to those on the FT collection. While global information is important in term-weighting schemes, it doe s not take much information to obtain near optimal global es-timates. The results of these experiments when using P @10 as a measure of effectiveness are almost identical.
Furthermore, we can see that the LM and ES term-weighting functions perform very poorly when there is very little of the collection known (i.e. less than 1% of the col-lection). However, the P IV , BM 25 and DF R schemes per-form very close to their maximum performance with less global information. In general, only a small sample of the collection (language base) is needed to achieve a high level of performance. This is quite an interesting finding as it in-dicates that only a small number of documents need to be sampled to achieve a good performance (possibly useful in a filtering scenario). Furthermore, from the results of this experiment, we can also determine the usefulness of global information to each term-weighting function. For example, if we look at the short queries on the FT collection (Figure 1), we can see that for the ES and LM schemes over 40% of the performance comes from global information (as if we ignore global information, the performance of those scheme s drops to about 60%). We can also see that the contribu-tion of global information to the performance of the P IV , BM 25 and DRF schemes is much less that those of the other schemes. Figure 3: % MAP increase on FT collection for short queries as local information becomes more accurate Figure 4: % MAP increase on FT collection for long queries as local information becomes more accurate
Having studied the effect that the estimation of global in-formation has on the performance of term-weighting schemes , we can turn our attention to local information. Local infor-mation is the information within the information item that is currently being assessed (or read). Local information us u-ally consists of length information and term-frequency inf or-mation (although this is not all encompassing).

Figures 3 and 4 show the effectiveness as we encounter larger samples of the document for the FT collection. We can see that to achieve at least 90% of the peak effectiveness, we need to read 80% of each document. This indicates that there is important relevant information in much of the docu-ment. The results for the FBIS, AP and WSJ collection (not shown) are again very similar to that of the FT collection. In general, to achieve anywhere near the peak effectiveness we need to use the entire document. Again, the results of these experiment are almost identical when using P @10 as a measure of effectiveness. Figure 5: % MAP increase when the query length increases on the FT collection Figure 6: % MAP increase when the query length increases on the AP collection
We now analyse the contribution that the information in the query has on performance by measuring the performance each time we encounter a new (unseen) term in the query. We consider queries in this incremental manner measuring the performance as each new term is encountered for all queries until a query of length (measured by unique terms) 15 is reached. For the collections in this research, a query o f 15 unique terms is about 25 terms on average (i.e. a sizeable query). We can see from Figure 5 and Figure 6 that most of the performance (about 80%) can be achieved using a 4 term query (i.e 4 unique terms). For the FBIS collection (not shown), 90% of the performance can be achieved using the first 3 terms. The results on the WSJ collection are similar to those on the AP collection. This result confirms previous findings that suggests queries of length 2 to 5 are most effective [3] when balancing effectiveness and effort.
Using the results from the previous three sections, we can determine the percentage of performance that comes from various sources. We can calculate the percentage of per-formance that comes from the collection by indexing very little or no documents. For each query type (short, medium and long), we can calculate the percentage of information that comes from the query source by preventing a scheme from using query term features (i.e. within-query term-frequencies). Therefore, the remainder of the performance can be deemed as coming from the document source. It is true that information is not strictly mutually exclusive to each source. For example, term occurrences in each docu-ment affects global information. Furthermore, in large doc-uments the distribution of terms may be some way repre-sentative of the distribution across the entire collection . Re-gardless, the method chosen in this work informs us as to the effectiveness of each source in comparison with other term-weighting functions, which has not been shown before.
Figures 7 and 8 show the breakdown of the effectiveness by source on the FT and AP collections. The results on the other collections are very similar. We can see that for the ES and LM schemes a lot of the effectiveness comes from better use of the global information. We can see for short queries that there is little or no information in the query (other than term occurrence/absence). This is be-cause the lengths and within-query term-frequencies are al l small (i.e. limited information). Short queries (i.e. com-mon web type queries) have very little extra information (i.e. other than term occurrence). The results also indi-cate that the ES scheme uses less query information than the other schemes to achieve its performance (this can be seen for medium and long queries). Another interesting point is that the effectiveness of the P IV , BM 25 and DF R schemes is distributed similarly from the sources of infor-mation (although, the performance of these schemes is dif-ferent). Not surprisingly, global information is less usef ul for shorter queries (as if we consider the case of a query of length 1, we can deduce that global information is not useful at all). This global information becomes more important for longer queries. These results are also useful for one X  X  choi ce of term-weighting scheme for a particular task. For example , in situations where global information is unavailable (i.e . a cold start in a filtering system), DF R or BM 25 would be a good choice of term-weighting.
We have shown that very little global information is needed to achieve good performance for a centralised index. We have also shown that for the collections used in this work, al l of the document must be read to achieve close to maximum performance. Our query-based experiments have shown that there seems to be diminishing returns when using queries longer than four unique terms. Furthermore, we have shown that when no (or very little) global information is availabl e, the learned function ( ES ) and the language modelling ap-proach ( LM ) perform poorly. We have determined that the ES and LM scheme make better use of global information and that there is a limited amount of information in the query (especially for shorter queries). Therefore, we turn our attention to developing a better representation for ex-ploiting information within the document itself. The next section deals with this process.
We have shown that different schemes achieve their perfor-mance using information from different sources. Typically a short query (less than five terms) is suitable for specifying an information need. Therefore, there is very little extra inf or-mation that can be gathered from such a short query. Simi-larly, very little frequency information needs to be collec ted (in a global context) to inform a user about the discrimina-tion value (or resolving power) of a term. Because of this and because ultimately, it is the document representation that determines the performance of a particular ranking strateg y, we will turn our attention to the document representation and in particular, we look more closely at the linear traver-sal of a document (similar to how a person might read a document and determine relevance). In this section, we aim to construct a ranking function based on linearly scanning a document using the natural ordering of terms. To aid us in developing a term-weighting strategy we will make use of a number of constraints.
As mentioned earlier in the related work section, a num-ber of axioms have already been constructed assuming the inductive approach [10]. We now wish to construct a term-weighting scheme that can score a document using this ap-proach. Therefore our term-weighting scheme will scan thro ugh an entire document in a linear manner as the imaginary user would. The estimation of relevance as this process occurs is governed by the axioms. Furthermore, one can notice that the document remains in the same representation as it is in reality. We do not propose that this representation is indee d the true (correct) view of language or meaning for humans. However, we do submit that it is through the construction of correct axioms that apply to the original representation of a document that we will be able to infer a greater under-standing of retrieval (and possibly a truer view of relevanc e). Indeed, we as yet do not fully understand axioms for mean-ing, relevance and the relatedness between terms, although as we have noted there has been some attempts to do this [10, 11]. Nonetheless, this representation does allow us ea s-ier access to a number of interesting document features (e.g .
As a basis for construction of this new weighting scheme, we will assume perfect knowledge of the collection. The initial weight ( w ( t )) of a term is usually some type of term discrimination value or measure of resolving power. When reading a document, most adult readers already possess a good estimate of the expected frequency of terms in a global context (i.e. the semantic content of a term). Therefore, we can model the initial weight ( w ( t )) of a term as one of the following functions (keeping in mind that we do not need a large sample of a collection to get accurate values): where gw ( t ) is the global part of the ES scheme and idf ( t ) an approach will be O ( D ) instead of O ( Q ). The aim of this work it is to seek performance improvements, or at least create a more intuitive framework that more readily allows access to, and intuitive incorporation of, more features (i .e. position, proximity etc). is a simplified version of the idf factor from the P IV , BM 25 and DF R schemes.
We know from the first constraint ( C 1) that as on-topic terms (query terms) appear the estimation of relevance of a document (or score) must increase. From the second con-straint ( C 3) we also know that the increase in weight for a term must decrease with successive occurrences. Therefore , to model this aspect in our inductive term-weighting model we use the following damping factor d ( n t ): where n t in the n th occurrence of term t in the document. We can now score a document as we linearly traverse a doc-ument D as follows: The damping function, d ( n t ), acts as a term-frequency as-pect. The first time the reader encounters term t , the score of the document is increased with the full initial weight ( w ( t )) of term t (e.g. idf ( t )). When the reader next en-counters term t , the score is increased with a smaller weight. Figure 9 shows the fraction of the initial weight to add to the score of a document as a reader re-encounters a term (for different values of x ). We can see that the first time a term is encountered (i.e. n = 1 ), the full weight of a term is added to the score. On successive occurrences a fraction of the initial weight is added. We used the LATIMES collec-tion to tune this parameter (i.e. x ). We found a value of 1 . 6 (see Figure 10) to be suitable for queries of different length using both idf ( t ) and gw ( t ) as an initial weight ( w ( t )). From Figure 9 we can see that a value of x = 1 . 6 means that the initial weight of the term drops to about 1 / 3 of its initial weight when it occurs for the second time. Figure 9: The fraction of the initial weight added to the score as a query term is re-encountered
A brief analysis of this damping factor shows that it is sim-ilar to the term-weighting factor of other weighting scheme s. The term-frequency parts of both BM 25 (when k 1 = 1) and the DF R schemes can be written as T F ( n ) = n n +1 for an average length document where n = tf D t . The additional Figure 10: Tuning the damping parameter on LA-TIMES weight for each of the n th successive term occurrence can then be calculated as follows: d ( n t ) = T F ( n )  X  T F ( n  X  1) = n n + 1  X  n  X  1 n = which can be multiplied by a constant in a ranking situation to 2 n 2 + n . In Figure 9 it can be seen that this function is a very close fit to the tuned function that we use as a damping factor.
Now we attempt to satisfy constraint 2 (and constraint 4) so that non-query (off-topic) terms are penalised. We can incorporate this as follows : where  X   X  is a penalising factor. This subtracts a weight every time a non-query term is encountered. We can see from Figure 11 that this normalisation acts similarly to oth er types of normalisation (i.e. short queries require little n or-malisation, while longer queries require more normalisati on). on average, for different query lengths on the LATIMES col-lection when using idf ( t ) and gw ( t ) as initial weights re-spectively. Table 3 shows the results of applying this sort of normalisation to both idf ( t ) and gw ( t ) and is labelled  X  nd 1  X  to denote this type of normalisation. Unfortunately, this simplistic normalisation approach does not work well for medium or long queries. Therefore, we try another ap-proach.
For our second approach to normalisation, we will incor-porate the document length ( dl ) directly into the function. We set  X  to zero and normalise the initial weight of a query term as follows: where N () is some normalisation function. If N () = 0 there will be no normalisation, while an increase in N () will de-Figure 11: Tuning the normalisation parameter (lambda) on LATIMES when using idf(t) crease the w ( t ) weight more. Using this method with our damping factor ( nd 2 ()), we would expect the normalisation function to be zero (i.e. N () = 0) for an average length document and to decrease the score for documents longer than the average document. Therefore, we use the following formula: where a is a tuning factor that is similar to normalisation tuning factors in other term-weighting schemes. We take the square root of the lengths and average length of the docu-ment to adhere to constraint 4 (C4). Figure 12 shows that the parameter a varies per query length as for other weight-ing schemes. For a fair comparison we took a single value of a for all query lengths. We set a = 0 . 5 when using the idf ( t ) as the initial weight and a = 0 . 25 when using gw ( t ). The results from this approach to normalisation can be seen in Table 3. We can see that adding this type of normalisation (labelled  X  nd 2  X ) to our term-weighting schemes is compara-ble to BM 25 and ES on some collections and outperforms them on others. In general, we see a slight improvement over BM 25 and ES when using our new weighting scheme with this type of normalisation ( nd 2 ).
Now that we have defined a basic term-weighting approach that is at least comparable to our baseline functions in term s of performance, we now attempt some improvements. As one is scanning through a document in such a manner, it is very simple to incorporate other information (such as term position and term proximity). To show this, we incorpo-rate one of these heuristics (i.e. proximity) and show that we can achieve performance gains using this representation . It is intuitive that a reader has some estimate of when an on-topic term last appeared. We incorporate proximity by remembering the position of the last occurring query term as we scan through a document. For example, if a query term appears in a particular position (e.g. position 7) and we encounter another query term very soon afterward (e.g. position 9), we increase the weight of that document. In this way, the complexity of the approach does not change (as we only iterate through the document once). The scoring func-Figure 12: Tuning the normalisation parameter (a) on LATIMES when using idf(t) tion can be written as: L ( Q, D ) = X where p ( t, t 0 ) is a function which measures the distance be-tween the current query term t and the most recent previous query term t 0 when t 6 = t 0 . It is intuitive that the weight to increase the document by is proportional to the initial weight (i.e. discrimination value) of each term involved an d inversely proportional to the distance between the two term s within the segment of text. We use the proximity constraint [14] (C5), which says that the proximity curve should be convex, to instantiate p ( t, t 0 ). Therefore, we define p ( t, t 0 ) as: where t is the current query term, t 0 is the query term last en-countered, dist ( t, t 0 ) is the difference between the two query term positions (i.e. linear distance between both terms) an d y is a tuning factor. We have found that y = 1 . 6 is a good choice for this parameter. We use the geometric mean of the initial term weights to weight the proximity score. For ex-ample, if the distance between the terms is 1 (i.e. they are a bi-gram), we add a score of p w ( t )  X  w ( t 0 ) to the document. We take the maximum value of the function p ( t, t 0 ) as the document is traversed. We also experimented with the av-erage value of this function. Table 4 shows the results of the schemes that include proximity (both the maximum and av-erage for our proximity approach are included). For a fairer comparison, we compared our schemes to BM 25 and ES when a baseline proximity function [14] is incorporated. We can see that the newly developed term weighting approach outperforms the proximity based versions of BM 25 and ES on most collections and query lengths and is significant on some data sets.
In this paper we have presented a method of applying an inductive view to the sources of evidence available. By varying the amount of evidence available in these sources we gain an insight into how much of a collection one needs level.
 to index in order to estimate global collection wide infor-mation to achieve a particular level of performance for a number of weighting schemes. We show that for many well known schemes, suitably precise estimates can be calculate d while indexing only a relatively small amount of the collec-tion. We also find that in most cases with respect to the evidence present in the query, that we again do not need much evidence to achieve reasonable performance. Regard-ing the evidence present in the documents themselves (local document evidence), the majority of the document must be scanned in order to achieved satisfactory performance. Thi s indicates that there is important information, regarding r el-evance in all of the document.

Furthermore, we have introduced a term-weighting ap-proach wherein the incremental linear traversal of a docu-ment is modelled to assign a relevance score to that docu-ment. While more computationally expensive that other ap-proaches, we argue that this intuitive inductive framework for assigning relevance provides the potential to correctl y model extra within-document evidence (e.g. proximity in-formation, positional information). To illustrate the ben efit of this approach, we have presented a set of experiments and results that show that this approach is at least as effective as current approaches and worth pursuing.

Future work will entail the incorporation of more features of within-document evidence into the weighting schemes. In -terestingly, there are many other features that may possi-bly be used in this framework. Natural language processing could possibly be used to create new features. For example, different parts of speech (nouns, verbs etc) may be assigned different initial weights. Also, parse trees may be utilised to develop a different type of proximity (possibly a more re-alistic view of the proximity between terms in a sentence). It would be interesting, for future work, to automatically learn a term-weighting function within this model that can use proximity, position, normalisation and content featur es together to score a document.
Ronan Cummins is funded by the Irish Research Council (IRCSET), co-funded by Marie Curie Actions under FP7. Mounia Lalmas is funded by Microsoft Research/Royal Academ y of Engineering. [1] Gianni Amati and C. J. van Rijsbergen. Probabilistic [2] Jaime Arguello, Fernando Diaz, Jamie Callan, and [3] Leif Azzopardi. Query side evaluation: an empirical level.
 [4] Jamie Callan and Margaret Connell. Query-based [5] James Caverlee, Ling Liu, and Joonsoo Bae.
 [6] Ronan Cummins and Colm O X  X iordan. Evolving local [7] Ronan Cummins and Colm O X  X iordan. Learning in a [8] Ronan Cummins and Colm O X  X iordan. Measuring [9] Hui Fang, Tao Tao, and ChengXiang Zhai. A formal [10] Hui Fang and ChengXiang Zhai. An exploration of [11] Hui Fang and ChengXiang Zhai. Semantic term [12] K. Sparck Jones, S. Walker, and S. E. Robertson. A [13] G. Salton, A. Wong, and C. S. Yang. A vector space [14] Tao Tao and ChengXiang Zhai. An exploration of [15] Ellen M. Voorhees, Narendra K. Gupta, and Ben [16] Hans Friedrich Witschel. Estimation of global term [17] Chengxiang Zhai and John Lafferty. A study of [18] Justin Zobel and Alistair Moffat. Exploring the
