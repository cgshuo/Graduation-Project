 Diversification models for search results [14,3,7,23,20,22] have attracted much attention since they can effectively ide ntify possible aspects of the query and return documents for each aspect. In many cases, this is more useful than con-ventional search methods which focus on finding the top-k most relevant docu-ments, often favouring (near) duplicates in the top positions of the ranked list at the expense of topic diversity. Although methods for finding a diverse search result list have been well studied, they primarily address the problem from the perspective of minimizing redundancy , and promoting lists that contain docu-ments covering multiple topics. One limitation of these approaches is that they treat each document equally while overlooking the fact that some topics are more popular than others; this can result in too much prominence being given to topics that are unlikely to be interesti ng to a majority of searchers. Ideally, the number of documents from each topic should reflect the popularity degree of that topic. Consider the case of reco mmending a set of 10 musical documents in a music recommendation system wher e two topics are considered (e.g., rock and classical ) with 90% popularity voting for the topic rock and 10% for the topic classical . For most users, it would be more useful to return a list which included mainly results related to rock with less results for classical (e.g. 9 rock, 1 classical). Existing approaches to diversification would return roughly equal numbers of results for each topic (i.e. 5 rock, 5 classical), which is less than ideal.
Motivated by this, we aim to better solve the problem of diversification by con-sidering it from a different viewpoint: proportionally diversifying search results . Specifically, we study the problem of diversifying the top-k representative search results by respecting the overall popul arity degree for each topic; we acheive this by determining the number of representative documents on each topic pro-portional to the topic X  X  popularity by maximizing a novel objective function. Since the computation of this objective function is NP-hard, the final propor-tionally representative results are obta ined by using an effective greedy heuristic to approximately maximize the objective function.

We evaluate both our method and state-of-the-art approaches by conducting comparison experiments over standard metrics [7,8,6] for diversity based on re-dundancy penalization, and our proposed metric, which considers proportional diversification.
 Our principal contributions are as follows.  X  We present a novel method for diversification: proportionally diversifying  X  We show that finding the optimal diversified top-k results by our objective  X  A novel metric for diversity is prop osed to verify our technique from the The rest of the paper is organized as follows: Related work is briefly reviewed in Section 2. We formulate the problem into a combinatorial optimization problem and show its potential to find a proportionally diverse ranking list in Section 3. We present the objective function and n ear-optimal algorithm in Section 4. Then, in Section 5, we report the experi mental studies. Section 6 provides the conclusion. There has been rising interest in incorporating diversity into search results to meet the diverse requirements of users b y both covering a large range of topics as well as preserving relevance. Standard diversification techniques [3,2,21,17,16] attempt to form a diverse ranked list b y repeatedly selecting documents that are different to the selected items. On e of the typical techniques is  X  X aximal Marginal Relevance X  (MMR) proposed by Carbonell et al. [3], where each candi-date is iteratively selected by the MMR ob jective function until a given condition is met. MMR was extensively studied by Rafiei et al. [17], who aimed to find the best strategy for ordering the results such that many users would find their rel-evant documents in the top few slots by formulating a weight vector as facets of the query were discovered.

Other than the work discussed above , there are many recent works studying result diversification [22,20,1,14,19]. For instance, in [22], the authors proposed a random-walk-based approach to encourage diversity by assigning absorbing states to the items that have been sel ected, which effectively  X  X rags down X  the importance of similar unranked states. In a similar vein, a model based on a reinforced random-walk is proposed in [14] to automatically balance the rele-vance and diversity in the top returned answers. Tong et al. [20], propose to address diversity from an optimization vi ewpoint which considers relevance and diversity optimally. Although the experimental results in [20] show improved performance in terms of diversity, it is still less than ideal in applications where the awareness of proportional popularity is desirable. The work most relevant to our own is proposed in [10], where an election-based method is proposed to address the problem of diversifying searched results proportionally. The method is divided into two phases. First, it diversifies the topics of all candidates by an election-based strategy, and then the final ranked list is yielded by selecting an appropriate number of candidates for each topic. However, this method can lead to some documents in popular topics being irrelevant to the query due to the separation of topic selection and candidate selection. It aims at diversifying the topics of all candidate documents rather than the candidate documents in essence. In contrast, our technique unifies the above phases, and effectively ob-tains a diverse top-k ranked list taking into account both the popularity degree of each topic and the relevance of each document to the query.

In this paper, we propose a novel objective function where the final top-k proportionally diversifying search results are obtained by achieving the optimal set of the function. To the best of our knowledge, our work is the first to obtain an effective solution for proportionally dive rsifying search results in an optimizing environment. In this section, we formulate a description of the problem of proportionally di-versifying search result as follows. Let Q = { w 1 ,...,w n } ( n  X  1) be a set of keywords comprising a query, let T = { t 1 ,...,t m } be the set of all m topics in the result of Q , and let U denote the set of all documents. We denote p i to be the popularity degree of topic t i  X  T (1  X  i  X  m )in U .
 Definition 1. The ranked list R is a proportional representation of U iff the number of documents in R within topic t i  X  T (1  X  i  X  m ) is proportional to its popularity degree p i .Suppose N ( i ) is the number of candidates from t i in R , then we have We normally present the top-k elements of R as the result for query Q ;the proportion of documents for each topic in the query result should roughly follow the the popularity degree for that topic Note that Eq.1 shows that the number of candidates for each topic in the final ranked list is not required to exactly match the proportion of the popularity degree for that topic. This is because the relevance between query and each candidate could degener ateifwestrictly adhere to the precise proportions (this is demonstrated in section 5).
 Example 1. Consider a document collection U where we assume that 80% of the documents in U about the  X  X pple X  computer company and 20% are about the fruit  X  X pple X . In this case, U is associated with two topics. Let R =[ R 1 ,R 2 ] where R 1 denotes the set of documents about  X  X pple X  (the company) and R 2 is the set of documents about  X  X pple X  (the fruit). In a top-10 ranked result list for the query  X  X pple X , we would expect roughly 8 results from R 1 and 2 results from R .
 Challenges. There are two challenges to be solved in our framework. The first challenge is how to design an objective set function where the optimal or near-optimal set can best describe the proportionally diverse ranked list, which is proportionally representative of the d ocument set. The second challenge is de-veloping an effectiveness measure; that is, given a proportionally ranking list, how to quantify its goodness. To solve the above two challenges, we propose a novel objective set function as well as a metric, both of which are shown in section 4. In this section, we first introduce the preliminaries and then describe our novel objective set function to obtain the top-k ranked list proportionally to the pop-ularity degree of each topic. 4.1 Preliminaries As our diversification algorithm is developed based on the availability of pair-wise similarities between documents, we adop t the personalized PageRank technique to compute the values [11]. Suppose there are n documents in the database and q i is i th query. We represent q i by a n  X  1 vector q i such that q i ( i )=1 and q i ( j )=0( j = i ). The pair-wise similarities from each document d j (for 1  X  j  X  n )tothequery d are denoted by a n  X  1 vector r i . P is the row-normalized adjacency matrix (i.e. n j =1 P ( i,j ) = 1) of the similar-ities, P is the transpose of P , c is a damping factor, and r i ( j ) is the similarity of j to i .Notethat r i ( j ) is not necessarily equal to r j ( i ). For each pre-computed n  X  1 vector r 4.2 Objective Function Given a set R of k documents, we propose to measure the quality of R , regarding the relevance to a given query q i 0 and the proportional diversity based on the topic popularity, as follows. where r i 0 ( i ) is a relevance score; the more relevant each individual document d i is to the query, the higher the value of g ( R ). Nevertheless, the inclusion of d in R is penalized against the pair-wise similarities ( r i ( j )) from document d i to other documents d j in R ; that is, subtract w i r where r i ( j ) is large when d i and d j have the same topic, which will further reduce the value of g ( R ). Thus, g ( R ) is expected to capture simultaneously the high closeness and the great diversity by maximizing its value while confirming the proportionality. Thereby, we aim to efficiently retrieve a set R of k documents such that g ( R ) is maximized.

The question here is how to proportionally diversify top-k results? We argue that it is implemented by w i , which indicates the importance of discounting the pair-wise similarity to include d i in R . Herein, w i determines the topic to be selected; we call this the topic coefficient . In fact, the proportion for the number of documents in each topic is guaranteed by automatically updating the topic coefficient w i , which manages the possibility of declining d i provided that many items belonging to the same topic as d i have already been included. Specifically, we define w i as where z i denotes how many documents that belong to the same topic as d i have been included in R and u i is the number of documents with the topic t i .We assume that z i is always less then u i in our setting. It is natural to observe that the larger w i is, the heavier penalty on document d i , and it becomes more difficult for d i to be selected.

We now prove that the problem of maximizing g ( R ) is NP-hard even when all w i = 1, which is a special case of this optimization problem. To deal with this, we then propose a near-optimal algorithm with a performance guarantee (i.e., accuracy guarantee and time-complexity) regarding a general g ( R ).
 Theorem 1. The problem of retrieving a set R of k documents to maximize g ( R ) is NP-hard with respect to k even if all w i =1 in Eq. (3).
 Proof. We convert the decision problem of maximum clique to a special case of the decision problem of the optimization problem in Theorem 1.
 Decision Problem of Maximum Clique (MC) INSTANCE: Given a graph G with n vertices, and an integer k  X  n 2 .
 QUESTION: Is there a complete subgraph of G with size k ?
It is well known that the maximum clique problem is NP-hard [13]; thus, its decision problem, above, is NP-complete regarding k .
 ProofofTheorem1. For each instance (i.e. G and k ) in MC, we construct r as follows. Suppose that the query has label 0 and each vertex v i  X  G corresponds toadocument i . For each vertex v i  X  G (1  X  i  X  n ), we assign r 0 ( i )= 1 n and r (0) = 0 (note r is not always symmetric); clearly, r 0 (0) should be 1. Then, for v and v j which are not connected by an edge in G , we assign r i ( j )= r j ( i )= 1 n 2 . Then, based on a preliminary calculation, it can be immediately verified that g ( R )  X  1 with | R | = k if and only if the following two conditions hold: 1. R contains the query with label 0; and 2. R contains a complete subgraph, with ( k  X  1) vertices, of G . Note that the 4.3 Efficient Near-Optimal Algorithm Theorem 1 shows that retrieving a set of k documents to maximize g ( R )isNP-hard. The function g ( R )is submodular and [15] states that a greedy algorithm to maximize a submodular function has the approximation ratio ( e  X  1 e ). Our algorithm (see Algorithm 1) is a greedy algorithm, for which we increase the value of topic coefficient w j if some documents belonging to the same topic t j have already been included in R .Theset R resists document d j with higher value of w j while it prefers document d h (1  X  h  X  n ) if its topic coefficient is lower. Furthermore, when the maximum number of a topic is reached (e.g. z i = u i ), the corresponding topic coefficient is set to be a prohibitive value  X  . Setting w i =  X  in Eq. (3) ensures that we reject any further documents which have topic t i .A suitable value of  X  was determined via our empirical studies.

According to the Proposition 4.3 in [15], the greedy algorithm of diversifica-tion has the following accuracy guarantee.
 Theorem 2. The greedy algorithm achieves an approximation ratio of ( e  X  1 e ) for the submodular function of diversification with proportionality constraint. Proof. Omitted for brevity. Refer to [15] for details. 4.4 Proposed Metric Observing that most existing metrics measure diversity by explicitly penalizing redundancy over each returned document while maintaining relevance, we pro-pose a novel metric that considers the proportionality on the diversified search Algorithm 1. Diversification by Popularity-based Proportionality.
 results by extending the metric in [10]. The metric proposed in [10] considers the following principles: First, each document need not belong to just one aspect of the query; that is, a document might be related to multiple aspects. Second, selecting a document that is related to some topics which already have enough relevant documents should be evaluated better than a non-relevant document. In other words, non-relevant documents should not be evaluated as highly as over-selection. However, the metric in [10] ignores the importance of rank positions of documents. Therefore, another critical property should be added: non-relevant documents appearing at earlier rank positions should be evaluated worse than relevant documents in later positions.

Considering the above three principles and least square index (LSq) [12], which is a standard metric for measuring dis-proportionality , we formulate our metric as Eq.(5) for penalizing the dis-proportionality for each rank position L (1  X  L  X  k ): where u i indicates the number of documents relevant to topic t i , v i is the number of documents that are actually found for t i , Y denotes the number of non-relevant documents at positions 1 ..L . The coefficient c i on topic t i is defined as follows: We now briefly discuss how our metric satisfies the aforementioned three prin-ciples. Our metric addresses the first principle associated with metric design by penalizing a list for under-selecting ( v i  X  u i ) on some topics but not for over-selecting ( v i  X  u i ) on it. At the same time, non-relevant aspects are penalized ( Y  X  0) while over-selecting is not, which m eets the second principle. Finally, the third principle of rank positions is implemented by considering the positions that are occupied by the non-relevant documents in the top-k diverse ranked list. To make the metric comparable across queries, we normalize the proportionality measure as follows: In the end, the proportionality diversification metric for a ranked list R can be computed as follows: In this section, we conduct extensive experiments to evaluate the effectiveness and efficiency of our algorithms. The setting of experiment is introduced in section 5.1, followed by the study of parameter learning in section 5.2. Then elaborate evaluations are presented in section 5.3. 5.1 Experimental Setup Baseline Diversity Models. We implemented the model described above, along with four other diversity models as baselines. The first diversity model is MMR [3], which has been widely considered standard in diversity literature. Another model, xQuAD [18], uses a probabilistic framework which determines how well each document satisfies each topic and outperforms many others in the task of diversitfication. The third model, proposed by Dang et al. [10], is referred to as Election in our experiment, and uses an election-based approach to address the problem of search result diversification. Finally, we implemented the approach of Dragon , which captures relevance and diversity in an optimized way [20].
 Query and Topic Collection. There are 50 queries in our query set, which come from the diversity task of the TREC 2009 Web Track [5]. To obtain the relevant documents for each query, we ad opt the query-likelihood framework to conduct the relevance search [9]. The evaluation is conducted on the ClueWeb09 Category B retrieval collection 1 , which contains 50 million webpages. As our ap-proach and xQuAD require the availability of query topics and their popularity, we utilize the sub-topics provided by TREC as aspects for each query. Since the popularity of each topic is not available in TREC data, we follow the model in [18] by adopting suggestions provided by a search engine as topic representation. Evaluation Metrics. We evaluate our approach and baseline models in terms of the proportionality metric proposed in Section 4. Considering that the propor-tion metric is specialized towards our model, we also report performance using several standard metrics including  X  -NDCG[7], ERR [4] and NRBP [8]. 5.2 Parameter Learning Parameter learning aims to determine  X  X ptimal X  values for  X  and k .The  X  measure is specific to our approach, and we evaluate precision and recall using  X  values ranging from 5 to 25. Fig.1 shows that our model achieves the best results when  X  has the value of 15. The k measure applies to all algorithms, and we need to ensure that we do not choose a k value that is biassed towards any particular approach. Fig.2 shows that all approaches perform best with a value of k around 40. Thereby, we conduct the diversification search with a ranked list of 40 documents. 5.3 Performance Evaluations Metrics and Proportionality Evaluations. We first compared our proposed technique to MMR, xQuAD, Dragon and Election using our proportional metric PM ( R ) for a list of R . From Fig.3 (a), we can see that our technique outperforms the other four, which demonstrates the e ffectiveness of our method at preserving proportionality. Secondly, we conducted comparisons in terms of three standard metrics from the diversity literature:  X  -NDCG, ERR and NRBP. The results are reported in Fig.3 (b) to (d), from which we can observe the similar result as in the previous example with proportional metric. Specifically, MMR is the least effective because of its ignorance of query topics. On the other hand, our method outperforms greatly over all the other method on almost all metrics. Note that these measures are computed using top 20 documents retrieved by each model, which is consistent with the standard TREC evaluations [5].
As all algorithms rank the top-k retrieved documents according to different principles, we examine the performance in both precision and sub-topic recall. We summarize the results in Table 1, which suggest that documents returned by MMR are more relevant and Election covers more topics than others. How-ever, in terms of suggestions, i.e., the representation of popularity on retrieved documents, our model achieves better performance than the other four.
 Scalability. Fig.4 gives our evaluation on the scalability of our algorithm (using synthetic data). The number of edges are fixed when we evaluate the scalability with respect to the number of nodes and vice versa. Fig.4 shows that our model increases linearly with respect to node s and edges, which demonstrates that it can be applied to large-sized databases. In this paper, we present a novel technique to address the problem of propor-tionally diversifying search results. A novel objective function is proposed to obtain a top-k ranked list by maximizing the value of the function. We prove that obtaining the optimal maximal value with respect to the proposed objective function is NP-hard, and resolve this by proposing an efficient greedy heuristic. We also propose a metric ( PM ( R )) to measure how effectively a diversifica-tion algorithm captures proportionality. Our experimental studies, evaluated on both standard metrics and our proposed metric, validate that our algorithm is not only able to effectively balance the rel evance and diversity of search results, but is also capable of keeping approximate proportionality of the top-k search results according to the popularity degree of the various topics.
 In recent years sentiment analysis or opinion mining aiming to uncover the attitude of users from the content has drawn much attention in the NLP. But most methods usually rely heavily on an annotated corpus for training the sentiment classifier. So the sentiment corpora are con s idered as the most valuable resources for sentiment analysis. To circumvent laborious annotation efforts, developing an un supervised method for sentiment analysis is one of the goals of this paper. 
Previous methods have tackled the problem at different levels of granularity, from document level, sentence level, phrase-level, as well as the speaker level in debates. Intuitively sentiment analysis at different level of granularity can benefit from each other [11, 15]. Though some studies have been performed to analyze the sentiment of document level and sentence level simultaneously, their approaches are usually based usually are supervised or semi-supervised [11, 16, 17]. So another goal of this paper is to develop a unsupervised model that jointly classifies sentiment at sentence level and document level of granularity. 
For fine-grained sentiment analysis such as for sentence, many problems must be tackled. Negation is the main problem, especially involving long-distance dependencies such as the negation of the proposition or the n egation of the subject influenced by modality, word sense, the syntactic role of a word in the sentence and advanced NLP technologies such as dependency parsing [25]. So the third goal of this paper is to incorporate advanced NLP technologies with statistical model for sentiment analysis. 
In this study, we propose an unsupervised model to incorporate the sentiment analysis at the document level and sentence level. The model is a novel and unified hierarchical generative model combining Na  X ve Bayes and Latent Dirichlet Allocation (LDA), which we refer to as NB-LDA. But unlike LDA, our model does not assume sentiment label, the latent sentiment label is drawn from the distribution of sentiment over document, and the words or features in the sentence are generated by the latent sentiment label in a Na X ve Bayes manner. Thus the Na X ve Bayes Assumption in our model makes it easy to integrate rich features and advanced NLP technique results into it to achieve better performance. We show that this model naturally fits the task of sentiment analysis, and experimental results show the effectiveness of the proposed model. The rest of this paper is organized as follows: Section 2 introduces related work. The proposed NB-LDA model is described in detail in Section 3. Section 4 shows the experiments setup, and experiment results are des cribed in section 5. L astly we conclude this paper and discuss the future work. [20]. And these systems have tackled the problem at different levels of granularity. sentiment analysis is much harder than at document level. In order to recognize the contextual polarity in phrase-level, Wilson et al. [18] have compiled a lexicon of over approaches relay on the availability of fine-grained annotations. 
Recently topic modeling has been an area of active research [3, 7, 14]. Some models extended LDA have been proposed for sentiment analysis [10, 12, 19]. These approaches jointly learn topic and sentim ent to improve predictions at document level. But these approaches have a shortcoming that they take the documents as "bag of words" and each word has a latent sentiment variable. Mukherjee and Liu [14] proposed two latent variable models (TME model and ME-TME model) to simultaneously model and extract topics (aspects) and various comment expressions for online review. But only n-grams (up to 4-grams) were modelled as a commenting expression. 
There are, however, obvious advantages for sentiment analysis at both document level and sentence level. Pang and Lee [15] performed minimal cuts in a sentence graph to s elect subjective sentences to i mprove the performance of document sentiment classification. But these methods try to only improve document sentiment classification via selecting useful sentences. 
Some structured models have previously been used for sentiment analysis [11, 16, 17]. McDonald et al. [11] presented a stru ctured graphical model for fine-to-coarse sentiment analysis, and adopted a sequence classification with a constrained version of Viterbi for inference of the model. T X ckstr X m and McDonald [16] used latent variable structured prediction models for fine-grained sentiment analysis in the common situation where only coarse-grained supervision is av ailable. Similarly T X ckstr X m and McDonald [17] derive two structured conditional models, which combine coarse-grained supervision with fine-grained supervision for sentence-level sentiment analysis. But these approaches are supervised or semi-supervised. 
Another very relevant model is ASUM [8], as shown in Fig. 1(a), but there are some key differences between our model and ASUM. Firstly our model can use rich feature to improve sentiment analysis. For example the sequence of the negation and polarity words can be considered in our work. But ASUM can't do so. Secondly, our model is a unified generative model. Like LDA, Naive Bayes is a generative model, and they can be naturally integrated into a u nified model. Notably NB can be supervised or un supervised while other classification algorithms such as MaxEnt, SVM are discriminative only for supervised classification and difficult to integrate into LDA. Thirdly, our model focuses on sentiment classification. However ASUM is proposed mainly to discover pairs of senti-aspects. To our knowledge, some similar models have been proposed by taking the best of Na X ve Bayes and LDA models, such as Latent Dirichlet conditional Na X ve Bayes (LD-CNB) [1] and Bayesian Naive Bayes 1 . But these models are different from ours. LD-CNB assumes a Dirichlet prior with parameter  X  from which the mixing weights  X  is and x j is sampled from the corresponding component distribution. The LD-CNB process of generating is different from ours. Also there has been some work where arbitrary features were included into the LDA model, such as Dirichlet Multinomial Regression [13], MaxEnt-LDA [20] and etc. But the purposes and methods of these works are different from ours. 3.1 Motivation 
For the distribution of sentence level sentiment in each document sentiment category, T X ckstr X m and McDonald [16] h ave shown that the sentence level sentiment is aligned with the document sentiment, as shown in Table 1. That is, in positive documents, most of sentences are positive, and in negative documents, most of sentences are negative, and in neutral documents, most of sentences are neutral Obviously this distribution can be exploited in sentiment analysis. As we know, topic models like LDA use co-occurrence information to group similar words into a single topic. Intuitively sentence level sentiment co-occurrence in documents may be mined to achieve better performance like topic model. So we proposed a new model, NB-LDA, which can employ rich features and exploit the distribution of sentence level sentiment in each document sentiment category to improve sentiment analysis. Here Na X ve Bayes is to identify the sentence sentiment polarity based on a set of features in the local context, and LDA is to exploit the sentence level sentiment co-occurrence in documents globally. 
In addition, to improve the performance of unsupervised sentiment analysis, our approach employs dependency parsing and a v ariety of f eatures to iden tify the contextual polarity of the sentence inspired by [18]. 3.2 NB-LDA Model The NB-LDA model belongs to a f amily of generative models for text where a document contains a fixed number of sentences, each sentence expresses a kind of sentiment polarity represented by a latent variable z , and words in the sentence are viewed as features conditionally independent given sentiment label z . As a generative previous studies [16, 17]. As to a document, we assume that it is "bag of sentences" and is generated by a mixture distribution  X  of T sentiments, which is sampled from a prior Dirichlet distribution Dir(  X  ) . 
This generative process can be expressed more formally by defining some of documents, F be the number of features, S d be the number of sentence in document d , S be the i -th sentence in document d , and F di be the number of features in sentence S . We can parameterize the multinomial distribution over sentiment labels for each probabilities of sentiment labels in document d . Similarly the matrix  X  of size T * F denotes the distribution over features associated with each sentiment label, where  X  t stand for the probabilities of generating features from sentiment label t . These matrix distributions are assumed to be generated from symmetric Dirichlet priors with hyper parameters  X  and  X  respectively. The formal definition of NB-LDA model is the following:  X  For each document d , sample  X   X  For each label t , sample  X   X  For each sentence i in S  X  Choose a sentiment label z di ~ Multi (  X  d )  X  For each feature j in F di features of sentence s di  X  Choose a feature value f dij ~Multi( In above generation process, the most difference from LDA is that NB-LDA chooses a latent variable z for each sentence in document d and then generates the feature corresponding to this process is shown in Figure 1(b). 
Although previous studies have shown that topic and sentiment are dependent, the dependencies are usually limited in the range of one sentence. In NB-LDA, we use the NB and a s et of features produced by NLP technologies such as dependency parsing to solve the local dependencies. 3.3 Inference with NB-LDA A variety of algorithms have been used to estimate the parameters of topic models [3, 6]. In this paper, we will use Gibbs sampling [6], as it provides a simple method for obtaining parameter estimates under Dirichlet priors and allows combination of estimates from several local maxima of the posterior distribution. 
Under this generative process, the joint probability of the z assignments and the features f can be factored into the following terms: By applying Gibbs sampling, we construct a Mar kov chain that converges to the Markov chain results from repeatedly drawing z from its distribution conditioned on all other variables, summing out  X  and  X  using standard Dirichlet integrals: including the current sentence. 
For any sample from this Markov chain, being an assignment of every sentence to a sentiment label, we can estimate  X  and  X  using where probability of sentiment label k in document d . feature space. In our experiments, the hyper parameters  X  and  X  are fixed at 0.3 and 0.01 respectively [6]. 4.1 Corpus, Prior-Polarity Subjectivity Lexicon and Features We used 2 corpora to evaluate our model. The first one is the Movie Review Data 2 . The second is Fine-grained Sentiment Da taset [16]. F or pre-processing, we used Stanford's Suite of NLP Tools 3 . Here some tokens were removed according to the stop words list and POS, and "Fine-grained Sentiment Dataset" was adjusted slightly in order to adapt to Stanford's Suite of NLP Tools. 
In the experiments, we used a p ublic subjectivity lexicon as prior-polarity knowledge [18]. We compared the word tags to clues in the lexicon. If matched, the word token was marked with the corresponding "prior polarity". And the sentence was marked with the majority polarity voted by all matched words while taking negation into account. The prior information of sentence was only utilized during the corpus. If the sentence has a "prior polarity", the corresponding sentiment label is assigned to it. Otherwise, a sentiment label is randomly sampled. Table 2 lists the features in the experiments. Most of the features are used in [18]. Here we used word lemma as features instead of word tokens. Besides we added sentence position as structure features. Note there are some differences between Stanford typed dependencies and that used in [18], and we handled them. 4.2 Baselines In this work, we adopted four baselines to evaluate our model. lemma-prior: In this baseline, we only use lemma as features, but didn X  X  use any prior knowledge. In the initialization of Gibbs, all the latent variables of sentences are randomly chosen. lemma+prior: In this baseline, we only use lemma as features, and use the prior knowledge. In the experiments, we only compared the tokens or lemmas morphologi-cally with the words in subjectivity lexicon, and ignored the POS. 
ASUM+prior: ASUM is so similar to our model. So we compared ASUM model as baseline, where we use the same prior-pol arity subjectivity lexicon instead of seed words used in [8] for prior knowledge. 
NB-ASUM: In the baseline, we assumed that ASUM can generate feature values just like our model. Besides the setting of ASUM+prior, we use all features described in Table 2. 5.1 Results for Movie Review Data In Movie Review Data, the reviews are only labeled as positive or negative at sentiment labels: positive and negative. The document sentiment is classified based on label given document label given document accuracies were averaged from 10 runs with 1000 Gibbs sampling iterations. 
As can be observed from Table 3, the performance of "lemma-prior" is mediocre when no prior information was incorporated. A signif cant improvement, with 10.7%, is observed after incorporating prior information. It is also noted that NB-LDA with all features achieved 3.9% improvement over "lemma+prior", implying that the richer features can benefit sentiment analysis. So discovering more effective features is one of the future works. 
When compared to the recently proposed unsupervised approach based on a spectral clustering algorithm [4], NB-LDA achieved better performance with about 1% overall improvement. Nevertheless, the approach proposed in [4] requires users to specify which dimensions (def ned by the ei genvectors in spectral clustering) are most closely related to s entiment by inspecting a set of features derived from the reviews for each dimension, and clustering is performed again on the data to derive the fi al results. In our model studied here, no human judgment is required. Another recently proposed non-negative matrix tri-factoriza tion approach in [9] also employed lexical prior knowledge for semi-supervised sentiment classif cation. However, when incorporating 10% of labeled documents for training, the non-negative matrix tri-factorization approach performed much worse than NB-LDA, with only around 60% accuracy achieved. Even with 35% labeled documents, it still performs worse than NB-LDA. It is worth noting that no labeled documents were used in the NB-LDA results reported here. 
Another notable result is that the prior information has different effects on positive documents and negative ones. Without prior information, the positive accuracy is less than the negative accuracy. While with prior information, the positive accuracies are better than the negative ones significantly. Manually analyzing the results reveals that there are more words matching the positive clues than negative clues in the corpus, which causes the imbalance of distribution of positive prior and negative prior during the initialization. Actually we found the similar problem in the experiments for fine-grained dataset reported in the next subsection, and in the results of [10, 16, 17]. 5.2 Results for Fine-Grained Dataset There are three sentiment categories at document level in this dataset, but there are five sentiment categories at sentence level: POS, NEG, NEU, MIX, and NR. Like [16], we considered the MIX and NR categories as belonging to the NEU category. So in the experiments, T is set to 3. T he sentence sentiment polarity is classified by the latent sentiment label z after sampling. 
Table 4 shows the results for ou r model in terms of sentence and document accuracy as well as F1-scores for each sentence sentiment category. In Table 4, the results above double line come from our experiments, and that below double line are results presented in [16] and [17] for the same dataset. 
In terms of sentence accuracy, from these results it is clear that the NB-LDA model signif cantly outperform VoteFlip with quite a wide margin. Comparing to SaD and DaS, the results from NB-LDA are also very competitive. However our results are still lower about 7% ~ 12% than HCRF and Interpolated in the last three rows in terms of both sentence and document accuracy. Actually the results below double line are evaluated via 294 reviews, but with 143,580 labeled reviews as supervision. Notably NB-LDA does not use any labeled data. 
In our results, "lemma-prior" is m ediocre. But with the prior information, "lemma+prior" performs better about 6% for sentence accuracy and about 8% for document accuracy than "lemma-prior". The NB-LDA with all features and prior information achieved much better perf ormance than "lemma-prior" and "lemma+prior". The results suggest that both proper features and prior information are important for improving sentiment analysis. In fact, NB-LDA model can easily integrate rich f eatures and advanced NLP technique into it to achieve better performance. 5.3 NB-LDA vs ASUM Similar to our NB-LDA, ASUM is also a generative model for sentiment analysis, which is proposed mainly to discover pairs of senti-aspects. In this work, our model only focuses on sentiment classification. But ASUM incorporates aspect and sentiment together to model sentiments toward different aspects [8]. The differences between them have been dis cussed in motivation. Here we compared the performances between the two models, and analyzed the differences, although NB-ASUM should obtain the same results with NB-LDA in theory. 
In fact as shown in Table 3, the  X  X emma+prior X  performs better than ASUM+prior slightly. It seems that only using lemma as feature, ASUM can obtain comparable higher about 1% than that of NB-ASUM. In fine-grained dataset as shown in Table 4, we can see the similar results, where the  X  X emma+prior X  can achieve better performances than ASUM+prior, and NB-LDA is better than NB-ASUM. From the experimental results, we can see that NB-LDA is consistently better than ASUM. As we know, the generative process of ASUM is more complex than NB-LDA. In NB-ASUM we need compute the probabilities for each feature value conditioned on topic variable and sentiment variable. It means that in NB-ASUM, the distributions would be s parser than in NB-LDA. The sparsity might block the propagation of sentiment information during iterations, and could not conduct good results. In this paper we proposed a hierarchical generative model based on Na X ve Bayes and Latent Dirichlet Allocation for unsupervised sentiment analysis at sentence level and document level simultaneously, only using a public subjectivity lexicon. The idea of NB assumption at sentence level makes it possible that we can use advanced NLP technologies such as dependency parsing to improve the performance of sentiment analysis. The experiments show that our model obtained better performance than VoteFlip, a ru le-based approach and ASUM. However for n ow our model hardly reach the competitive performance to the supervised or semi-supervised approaches. 
Since unsupervised approaches hardly obtain comparable performance to supervised ones. A simple extended model could be designed based on supervised LDA [2] and Na X ve Bayes. Another extension of our model is to capture sentimental structures within the documents simultaneously, inspired by HTMM [7]. The third future work is to empirically investigate the effects of more features on more datasets. Finally we may split sentences not only by punctuations but also by conjunctions, since one sentence may contain multiple sentiment clauses. Acknowledgments. The work was funded by the National Natural Science Founda-tion of China (No. 61133012, 61173062, 61070082) and the Major Projects of Chinese National Social Science Foundation (No.11&amp;ZD189). Dong-Hong Ji is the corresponding author. In cryptography, side channel attack is a kind of attacking strategy taking advantage of the information gained from physical implementation of a cryptosystem to obtain the cryptographic keys of the device. One major advantage of side channel attack lies in its non-intrusive characteristic that allo ws the attacker to obtain side information that facilitates the deciphering of the key. It also enjoys a much lower computational complexity than cryptanalytic-theoretical attack, most of which is of super-polynomial complexity. For well-designed ciphers, side channel attack might be the only feasible way to recover the key to the device in practice. In this work we would like to introduce a machine-learning based attacking strategy for side channel attack. 
We focus on a specific type of side channel attack called power analysis, but in general the proposed technique can be applied to several other kinds of side channel attacks such as electromagnetic attacks and acoustic cryptanalysis. Power analysis is a form of side channel attack, with which the power consumption of a cryptographic device (e.g. a smart card) is analyzed to obtain the cryptographic key of the device. As some cryptographic devices are implemented using semiconductor logic gates, and current flows across the silicon substrate when change is applied to or removed from the gate, it is not hard to imagine that through examining the power consumption of the device externally it is possible to determine what kind of operations (i.e. macro-characteristics) are executed on the device chip. One can then use such information to guess the secret key that corresponds to the hypothesized operations. 
Le (2006) classified these techniques into two categories: attacks without reference devices and attacks using reference devices. With a reference device, it is possible to arrange different keys and plaintexts to f eed into the device and record the output ciphertexts and power traces for further analysis. Without the reference device, while the outputs can still be measured, we have no idea what the inputs (i.e. keys) are. Hence, attacks using reference devices is like the supervised machine learning scena-rio, where the training data are labeled with known keys, and no (input, output) rela-tion is provided for the other case. Therefore, attacking without reference devices is considered a much harder unsupervised learning problem. 
In this paper, we propose an efficient, extensible unsupervised framework of power analysis based on machine learning techniques. We model the decipherment process as identifying a key that minimizes the training error of a given time stamp, which can be done unsupervised without using any labeled training data. Besides, the approach can be viewed as parameter estimation in abstraction, where the parameter domain contains all possible key candidates. To tackle sparse-training situation, we further propose a technique to exploit the dependency of multiple round functions in the en-cryption process. Finally we perform experiments on datasets obtained from the DPA Contest to show that the proposed method outperforms the competitors significantly. 
The contributions of this paper are as follows. First, to the best of our knowledge, there has not yet been any work aiming at exploiting machine learning approaches to perform unsupervised side channel attack. Here we show that with careful design, simple machine learning techniques such as regression models can be exploited to tackle a cryptography problem. In this work, we hope to send an encouraging message to ML researchers on how the bridge between machine learning and cryptography can be established by demonstrating how the side-channel attack problem can be con-ducted from learning point of view. The concept of side channel cryptanalysis was first proposed by Kelsey (1998), which from imperfect implementation to facilitate breaking the cipher system. It is concep-tually different from traditional cryptanalysis. That is, side channel cryptanalysis uses the correlation between plaintext and ciphertext to guess what happens inside a cryp-tosystem and further infer the key of the system. Side channel attack exploits the fact that most implementations of a cryptography system are not perfect and hence could inevitably leak some side channel information. The side channel information can be in the form of, for example, the electromagnetic (EM) g auged from CMOS device 1999) such as what has been used to attack many implementations of Data Encryption Standard (DES) or Advanced Encryption Standard (AES). There are s ome other forms of power analysis, such as timing attack (Kocher, 1996), template attack (Chari et al., 2002), and acoustic attack (Backes et al., 2010). 
Analyzing the snooped data is a non-intrusive attack of a cryptographic implemen-tation, and power analysis is one of the most successful forms of such attack. The key reason to the success of power analysis lies in that the power consumption of a device generally possesses some correlation to the intermediate values that can be produced based on the cipher algorithm. In other words, maximum correlation can be obtained given correct hypothesis on the key. Below we will discuss some popular approaches based on this concept including DPA (Kocher et al., 1999), CPA (Brier et al., 2004), and BS-CPA (Komano et al., 2010). 
Differential Power Analysis (DPA) is a t ype of attack that examines the power consumption signals through exploiting statis tic measures to retrieve the correct key that has the maximum likelihood of producing the observed power consumptions. Similar to DPA, Correlation Power Analysis (CPA) is based on the linear relation between the real power consumption of the device and the intermediate values from the encipher model; it can be regarded as a form of multi-bit DPA. Messerges et al. (2002) demonstrate that CPA is just another form under DPA divided by a normaliza-tion factor. Built-In Determined Sub-Key Correlation Power Analysis (BS-CPA) is an enhancement of CPA that results in efficient trace usage. Whenever a sub-key is de-termined in each S -box, the BS-CPA can pass such information to as sist other S-boxes to decrease the signal-to-noise ratio. In DPA Contest 2008, BS-CPA has been shown to be the most effective method. We will later compare our method with it. 
In recent years, machine learning techniques have been playing an increasingly important role in attacking a cryptosystem. Hospodar (2011) proposed a supervised learning architecture to attack an AES system by side channel information. It regards the power consumption signal as an instance, divides the key bits into several binary labels and treats the problem as several binary classification tasks with Least Squares Support Vector Machine (LS-SVM) as the learner. The experiments show that LS-SVM is suitable for such purpose (Chari et al., 2002). A similar supervised approach is proposed by Lerman (2011). In practice, however, such labeled training examples are not available in most situations because it requires knowing the hidden key infor-mation in advance. Acknowledging such fact, we design an unsupervised learning approach that follows different assumptions than the previous work. In our case, only one single encrypted device with unknown key is needed.. We start by interpreting the encoding process using Shannon's Noisy Channel Model. As shown in Figure 1, the inputs X to the channel contain a set of plaintext or known while the interaction of the inputs produces the observed outcome Y={y 1 ...y n } that generally required for side channel attack) to use a variety of cipher-texts interacting with the same key to produce a set of observations. The noisy channel P(X|Y) can be considered as a black box that produces an output given an input. Given a fully ob-served Y and a partially observed X with P(X|Y) unobserved, the goal then becomes to recover the missing part of X (i.e. the secret key) using Y and C. The problem can then be mathematically formulated as argmax x P(X|Y). We can first use Baye's rule to decompose argmax x P(X|Y) into argmax x [P(Y|X)*P(X)]. This essentially tells us that a proper X should not only possess a higher chance to produce the observation Y, but also has a higher chance to occur among other X 's. Here we assume no prior know-ledge about X in a cryptography system, and consequently P(X) is uniformly distri-buted. In this problem, we are given a set of n instances as inputs X={x k =(c k ,key) , where k=1...n }, where c k is a known cipher-text with the corresponding observation y , but the secret key is unknown. Assuming the deciphering processes are indepen-dent for each cipher-text, the problem argmax x P(Y|X) can be transformed to Then it becomes obvious that with a faithful estimation of P(y k |c k .key), one can even-(denoted as key c ) shall obtain high P(y|c,key) value. Mathematically, a faithful esti-mation of P(y k |c k .key) should possess the following property: P(  X  1 |  X  1 ,  X  X  X   X  ) P(  X  2 |  X  2 ,  X  X  X   X  ) ...P(  X   X  |  X   X  ,  X  X  X   X  ) In other words, incorrect keys should possess much lower probability of producing y than the correct one. After taking log on both sides, we can obtain max The above equation is reasonable as the power signature reflects only the interaction between the correct key and the ciphertext. Therefore, we propose a t hree-stage framework to solve this problem: respond to one single key: learners ML r . The generation of such features depends heavily on the backend crypto-graphy algorithm. An example will be demonstrated in the experiment section. learners. 
Acknowledging the fact that P(y k |c k ,key) represents how likely y k results from an input {c, key} and output y are learnable (i.e. low prediction errors) for correct key, and not learnable (high prediction errors) fo r incorrect keys. Therefore, the predicta-bility of a machine learner has been used here to estimate the quality of a noisy chan-nel. The channel corresponds to the right key should contain less noise and conse-quently be more learnable. Figure 1 illustrates the process. Next we assume the rela-tion between trace sample y and features x generated from each pair of cipher-text and key candidate can be modeled simply as where n can be assumed as Gaussian noise as in (Prouff et al., 2009), and its density function can be written as where  X  is the standard deviation of the noise. As a result, the weight vector w for square (OLS). Furthermore, coefficients of determination are used to normalize dif-ferent scales at different time stamps. 
Above we have explained how a supervised learning model can be exploited to solve such an unsupervised decipherment problem. However, for power analysis, reveals apparent relationships with respect to (c, key). That is, usually only the right combination of key (denoted as key c ) and time stamp (denoted as t c ) possess higher learnability than other time stamp. Therefore, instead of building m different learners, we propose to create m*q different learners (q represents the number of samples for each time sequence) and argue that the one with the best predictability does reflect the correct key and time stamp. 3.1 Sub-Key Breaker One major concern for such key enumeration approach lies in the fact that there are exponentially many keys to try. To conquer this problem, we follow a commonly used strategy of CPA (Brier et al., 2004) to divide the key into several sub-keys according to the permutation of inputs of substitution boxes (S-boxes). S-boxes are important components in block cipher design, which perform substitution and provide nonli-nearity between ciphertexts and plaintexts. For instance, the length of key is 56 bits for DES; based on such a 56-bit key, each round a 48-bit round key is derived and certain physical meaning, and we can extract features from it given some domain knowledge. Then we can apply the method proposed in 3.1 on each sub-key indepen-dently for better efficiency. 3.2 Dual-Round Approach for Multi-round Ciphers So far we have introduced our approach to obtain the secret key from power traces. The quality of the results depends significantly on whether there are sufficient exam-ples (or traces) to learn from. Without sufficient training examples, by chance some framework. One practical method to determine whether there is sufficient trace is to draw the  X  X earning curve X  that indicates whether the deciphered key becomes stable. In our experiments, we consider the deciphering process as completed if the results do not change after 100 additional traces are added. 
The method mentioned in 3.1 might not be as effective if the number of traces is not enough to reveal the correct key (or to eliminate the false positives). Experiments signal in each round does correspond to one particular time interval. Therefore it is possible to generate multiple training instances based on information from different rounds. Remember in Section 3.1 we have described how to break a longer key into sub-keys for analysis. In general these sub-keys are organized differently in different rounds. For examples, some bits might be grouped into the same sub-key in one round and broken into several different sub-keys in another round. For each round, some sub-keys can be deciphered easily (i.e. requ iring fewer traces to converge to a steady result), while others require more training examples. Here we would like to further describe how one can fully exploit the side channel information from multiple-round such as DES, if we are given some information of dependency between sub-keys in different rounds, a modified version of our method can exploit the relations between these sub-keys to improve the deciphering performance. The intuition behind our idea ing error than most of other candidates), then we can propagate such information to other learners and group the overlapped bit using the learned values. By doing such, the search space for other harder sub-keys is reduced, which alleviates the high de-mand for training traces and reduces the false positives. 
An example is shown in Figure 2. It is known that some of the key bits such as b 0 and b 4 are used in both rounds, indicated by the connecting edges. Therefore, we can search for the key bits in these two rounds simultaneously. The difficulty of finding the correct key of the harder round (need more traces to break) can be reduced with the help of the easier one because of the increasing of signal-to-noise ratio when con-sidering all cases at the same time. In dual-round approach, during training we first identify a set of S-boxes, called S-boxes set, whose bits overlapped with each other to some extent. Then within each S-boxes set, we train each of the S-boxes independent-ly, but weighted-sum the errors of each to represent the quality of a particular assign-ment of bits. The weight is determined by the inverse of the  X  X umber of S-boxes X  in each round containing in this particular S-boxes set. That is, if an S-box set contains one S-box in round 1 and two S-boxes in round 2, then the weight for the round 1 S-box is twice as much as that of the ones in round 2. We evaluate our models using the power consumption traces of the unprotected DES crypto-processor on the SecmatV1 SoC (System on Chip) in ASIC (Application-Specific Integrated Circuits), which is provided by DPA Contest 2008 and focuses on Differential Power Analysis or s imilar attacks. Here we select the first 1000 traces from secmatv1_2006_04_0809 for experimenta tion, with each trace con taining 16 nominal DES runs. The raw signal of each trace looks like the one shown in Figure 3. To smooth the signal and eliminate apparent outliers, we take the average of ten orig-inal samples as one sample for every power consumption trace in our dataset. As the number of learners we need to create is proportional to the number of temporal sam-ples, here we choose to use only 20 samples per trace for efficiency purpose. Experi-ments show that we can still achieve high-quality results based on only 20 learners. 
This experiment tries to address two issues. First, we would like to know whether our method can accurately identify the correct key. Second, given that the correct key can be discovered, we want to compare our method with two popular non-learning based methods, CPA and BS-CPA, to see whether our method can find the correct key using fewer number of power traces (i.e. achieve the same quality using fewer data). For the second purpose, we gradually increase the training data size until the outputs become stable. Based on the rule from the DPA Contest, the attack is consi-dered successful if the correct key appears and remains unchanged for 100 consecu-tive trace additions. 4.1 Feature Generation Here we first describe the encryption process of DES. In the encryption of DES, the plaintext is first permuted and divided into two 32-bit halves, L_0 and R_0. These round, there is a 48-bit round key involved, which is a permutation of the original key and can be computed reversely. Hence, we can figure out 48 bits of the original key by revealing a round key, and then the other 8 bits can be found by exhaustive search. In every round, the right half is expanded to 48 bits and XORed with the round key. Then it is split into eight 6-bit blocks and fed to eight S-boxes, each of which has a 4-bit output and generates a 32-bit value that is then permuted again, XORed with the left half to become the new right half. For a typical implementation of the DES, the two half-blocks keep the same addresses in memory throughout the encryption. Therefore, after a round ends and before the next one begins, the register storing the right half must be replaced by a new value, which results in several bit flips and extra power consumption. By modeling the power consumption, we can derive the condi-tion of the bit flips and eventually derive the round key. Because the eight S-boxes form a parallel structure, we can attack them one at a time. Each S-box is related to 6 bits of the round key and the four output bits are stored to some known addresses. 
We take the first S-box of the first round as an example. The right half before the first round, R_0, is known and the four output bits of the target S-box can be com-puted by assuming a hypothetical 6-bit value as the relevant part of the round key. Therefore, we are able to find whether the b it flips for these four bits and can generate four features, each is 1 or 0, representing whether the bit flips or not. 
Thus, the first feature we extract is to compute the hamming distance, which meas-ures the corresponding bits flip between the old and new right half of register in the first or the last round of DES (Kocher et al., 1999). The second feature is to compute the hamming distance between the old and new left half of register in the first or the last round of DES (Almeida, 2008). The difference between left half and right half is that the left half inherits from previous right half directly and does not divide into 8 S-boxes. Therefore, when we attack different S-boxes in a round, each S-box has its hamming distance value. Since the output of each S-box has 4 bits, the first feature value of each S-box is between 0 and 4. On the other side, the hamming distance of left half register is always the same in a round because it inherits directly from the feature value of each round is between 0 and 32. In single-round approach, we extract those features in the last, or the 16th round, of DES. All traces and features are then normalized to zero mean and unit variance. 4.2 Experiment of Single-Round Approach In order to compare the efficiency of our model, we use CPA and built-in determined sub-key CPA (BS-CPA) (Komano et al., 2010) as competitive algorithms against our model. For each competitive algorithm, we add 10 traces each time until a correct and stable key is found. As mentioned previously, we adopt the sub-key breaker to attack the sub-key (or S-box) one by one. Acknowledging the fact that the order of the traces to be added can affect the results as some training inputs are more representative, here we shuffle the order of traces each time and then average results from 20 different orders to obtain the average number of traces used for each algorithm. We depict the average traces of each method in the left hand side of Figure 4. In some cases such as S-box3 or S-box7, our method gets worse results. We believe that it is caused by the noise in the current traces o r overfitting; however, our learning-based method per-forms better than others in most of the cases. 4.3 Experiment of Dual-Round Approach In real world scenario, sometimes there are only limited numbers of traces available. Therefore, we can resort to the dual-round approach that exploits extra multi-round information. For DES, we exploit the first and the last rounds. That is, we explore key dependency between the first and the last of nominal DES round. Once we obtain a possible key candidate in one round with high confidence, we can pass such informa-tion to the other round to reduce the search space of all possible key candidates. Before pursuing dual-round attack, we need to first observe the dependency of key bits, which can be derived from the encryption algorithm itself. For example, the bit0 and bit2 of the S-box6 in the last nominal DES round do not have corresponding key bits in the first round. The bit1, bit3, bit4 and bit5 of S-box6 in the last round have corresponding key bit positions 35, 29, 34 and 24 in the first nominal DES round respectively. 
The single-round results in Figure 4 show that S-box5 is the most difficult sub-key to attack, as it requires the most traces on average. If we can use the knowledge of key dependency from another round, it is possible to reduce traces required to attack S-box5. Figure 5 shows the key dependency between the first round of S-box4, S-box5 and the last round of S-box6. We have realized that there are two overlapped keys between S-box5 in the first round and S-box6 in the last round, and another two over-lapped bits between S-box6 in the last round with S-box4 in the first round. There-fore, it is possible to propagate the key bits learned in an easier S-box (e.g. S-box4) to the harder ones. We realize such idea by co nsidering the bits in these three S-boxes altogether and use the weighted sum of the errors to evaluate the quality of certain assignment. Even though not all key bits has a corresponding mapping between the first and the last round, we still need to search all possible combinations of those non-overlapped bits. The higher the key dependency, the more likely we can use fewer traces or training examples to decipher the key. 4.4 Results of Dual Round Experiment Here we compare the dual-round approach with the single-round approach. We focus on deciphering the S-boxes in the last nominal DES round using the dual-round attack technique because we can easily compare the results with our single-round approach. The right hand side of Fig. 4. shows the results of regression-based single-round approach and dual-round approach. Table 1 s hows the numbers of average traces required for each S-box. Not surprisingly, the dual-round approach has better perfor-mance than single-round approach in most situations. Such results demonstrate that dual-round approach can trim the search space to avoid the interference of some po-tential false positives because an incorrect key needs to perform well in both rounds to be selected as false positives, which is less likely to happen comparing with single-round approach. Side channel attacks play an important role in cryptography. Despite that in theory, cryptographers can design provably-secure cryptographic algorithms, these algo-rithms need to be i mplemented and carried out by computing hardware. The imple-mentations can subject to side channel attacks no matter how secure the algorithms are in theory. This is why many industrial and governmental standards such as FIPS (Federal Information Processing Standard), CC (Common Criteria), and EMV (Euro-pay, MasterCard, and VISA) require that compliant security products have various levels of countermeasure against side channel attacks. It is therefore crucial to under-stand how efficient such attacks can be with advanced techniques from, e.g., machine learning, as well as to gain some insights into how these attacks work in order to de-sign more effective countermeasures. In this paper, we introduced a novel unsuper-vised, regression-based approach to perform side-channel attack. We further extend this approach to consider information from multiple rounds with promising results. We hope this paper can serve as an encouraging example to show how machine learn-ing approaches can be carefully crafted to solve a well-known security problem. Acknowledgement. This work was supported by National Science Council, National Taiwan University and Intel Corporation under Grants NSC101-2911-I-002-001, NSC101-2628-E-002-028-MY2 and NTU102R7501. 
