 Solid-state drives are becoming a viable alternative to magnetic disks in database systems, but their performance characteristics, particularly those caused by their erase-before-write behavior, make conventional database indexes a poor fit. There have been various proposals of indexes specialized for these devices, but to make such indexes practical, we must address the issue of concurrency control. Good concurrency control is especially critical to indexes on solid-state drives, because they typically rely on batch updates, which may take long and block concurrent index accesses. We design, implement, and evaluate an index structure called FD+tree and an associated concurrency control scheme called FD+FC . Our evalu-ation confirms significant performance advantages of our approach over less sophisticated ones, and brings out insights on data struc-ture design and OLTP performance tuning on solid-state drives. H.2.2 [ Physical Design ]: Access Methods; H.2.4 [ Systems ]: Con-currency Indexing, Concurrency control, Flash-memory SSDs Solid-State Drives ( SSDs ) have become a viable alternative to mag-netic disks in database systems [8, 11, 4]. SSDs perform random reads one to two orders of magnitude faster than magnetic disks. However, a write may necessitate first erasing a large region of data (called an erase block ). This erase-before-write nature makes random writes one to two orders of magnitude slower than reads. SSDs X  fast random reads benefit tree indexes like the B+tree used extensively in database systems. However, the conventional B+tree performs random in-place writes, making it a poor fit for SSDs.
The unique characteristics of SSDs have led database researchers to new tree indexes such as BFTL [21], LA-tree [2], FD-tree [12], and SkimpyStash [7]. A foundational idea behind these new indexes is to convert the small random writes caused by index modifica-tions into large sequential writes, by somehow buffering modifica-tions and then updating the index with a batch reorganization. Such reorganizations may take long, as illustrated by Figure 1. While Figure 1: Completion time of an insertion request X  X ncluding a ny index reorganization triggered X  X or (an improved version of) FD-tree [12], over the course of two million insertions, start-ing with an empty index. The insertions are grouped into buck-ets each containing 10,000 requests; we plot the longest ob-served time per request in each bucket. they make efficient use of SSD characteristics, there is a serious issue: if the index does not employ proper concurrency control techniques, an ongoing reorganization can prevent concurrent in-dex accesses, hence causing large variance in access latency. For example, without proper concurrency control, the completion times shown in Figure 1 would translate into response times experienced by concurrent accesses: some accesses may complete in millisec-onds or less, but others blocked by a long-running reorganization can take seconds. Thus, there is a particularly pressing need for ef-ficient concurrency control for tree indexes on SSDs, which stems from potentially long-running index reorganizations X  X n issue not found in traditional indexes designed for magnetic disks.
An index with good concurrency control should do better on three crucial requirements: a) low average access latency, b) low variance across latencies, and c) low worst-case latency. While most research on SSD indexes focus on (a), requirements (b) and (c) are equally (perhaps more) important in practice. Users feel variance in performance more than they feel the average [14]. En-gineers at Web-based companies like Facebook care about mini-mizing variance and ensuring that the  X  X dge cases X  are not bad, even at the potential cost of higher average latency [1]. Contributions First, we identify concurrency as a critical issue in making SSD indexes practical. We show that straightforward con-currency control schemes are inadequate. A global readers-writer lock incurs unacceptably high variances and worst-case access la-tencies. An alternative is for each index reorganization to write a new version of the updated portion of the index and  X  X witch it in X  at the end of the organization; hence, readers can access the old copy of the index without being blocked. However, this scheme dou-bles the space requirement, making it unattractive for SSDs, which continue to be much smaller and more expensive than magnetic disks. Furthermore, as we shall see later in the paper, because cru-cial resources held by the old copy cannot be devoted to incoming Figure 2: Illustration of high-level ideas in FD+tree. Maxim um capacity of each level is delineated by dashed lines. modifications until the current organization finishes, this scheme continues to suffer from worst-case modification latency as high as using a global readers-writer lock.

We propose FD+FC , a novel indexing and concurrency con-trol scheme for SSDs. FD+FC allows concurrent accesses by both readers and writers during ongoing index reorganizations, improv-ing both response time and throughput of the index. Furthermore, FD+FC does so without the extra space requirement of a space-doubling scheme. Achieving these features requires careful design of the data structure and algorithms. At a high level, FD+FC em-ploys a reorganization procedure that sweeps a wavefront across a portion of the index, progressively converting it while ensuring that the converted and unconverted parts remain connected as a co-herent structure supporting concurrent accesses. Most parts of the index have a single sequential writer and multiple random readers; this special access pattern is exploited by FD+FC X  X  efficient con-currency control protocol. We implemented and evaluated FD+FC against alternative schemes.

Finally, the basis for FD+FC is an index called the FD+tree , which modifies and extends the FD-tree proposed by Li et al. [12]. FD+tree is a contribution in its own right because of several new features aimed at making it practical: one-pass merge makes index reorganization more efficient and simpler for concurrency control; level skipping speeds up reads by skipping small, unnecessary lev-els; level tightening makes it possible for the tree to shrink in height in the presence of deletions; and underflow-triggered merges pro-vide performance guarantees for workloads involving deletions. Overview and Intuition Before presenting our full indexing and concurrency control scheme, we need to describe its underlying in-dex, FD+tree. As mentioned earlier, FD+tree modifies and extends FD-tree [12]. Conceptually, both indexes employ the techniques of the logarithmic method [3] and fractional cascading [6].
The logarithmic method turns in-place random writes into batch sequential writes. Data reside across layers of sequential files ( X  X ev-els X ) whose maximum capacity increases geometrically from top to bottom. Modifications are not applied in place, but are instead added to the top level. Upon reaching its maximum capacity, a level is merged into a lower one; sometimes multiple levels will need to be merged so that the result level is within its maximum capacity. Figure 2 illustrates the idea.

Fractional cascading speeds up search across levels. Each level is sorted by key. By strategically placing, in each level, pointers to locations within the next level, we leverage the effort of searching a level in searching the next level, as illustrated in Figure 2. Without fractional cascading, each level would be searched from scratch.
Beyond conceptual similarities, FD+tree differs from FD-tree in significant ways, which we highlight in Section 2.4. Our overar-ching goal is to make the index more practical, with performance guarantees for workloads involving deletions, simpler and more ef-ficient index reorganization, etc. To this end, FD+tree maintains a richer set of invariants, and performs more careful bookkeeping, pre-reorganization planning, and post-reorganization adjustment. For simplicity of presentation, we assume a unique index; i.e., there is at most one record with any given key value. Extension to handle duplicates is straightforward.

An FD+tree (illustrated in Figure 3) consists of a sequence of Levels below L 0 are linked lists of disk blocks 1 compactly stor-ing sorted runs of entries. L 0 is a standard data structure (e.g., a B+tree) that supports lookup, in-place insert and delete, and or-dered scan. L 0 is small enough that we keep it in main memory; persistence can be achieved by separate logging. 2
The entries in an FD+tree have two types: data and fence . Ev-ery entry has a key and a payload . A data entry can be an insert or delete entry. For a data entry, the payload contains the record pointer or value being indexed. For a fence entry f , the payload contains a pointer to a block in the next level; all entries in that block have keys no less than f  X  X  key. The bottom level, L no fences or delete entries.

In the presence of deletions, the tree may contain multiple data entries with the same key; e.g., a delete entry can effectively  X  X an-cel out X  an insert entry with the same key in a lower level. There-fore, the tree may be  X  X loated X  in the sense that it stores more en-tries than necessary. To limit bloating, we maintain two counters, N  X  and N  X  , which respectively track the total number (across all levels) of insert data entries and that of delete data entries currently in the tree. While the total number of data entries in the tree is N  X  + N  X  , the true number of elements indexed is N  X   X  N  X  cause every delete entry cancels out exactly one insert entry.
Let  X  denote the block size , as measured by the number of entries that fit within one block. Let  X  denote the size ratio parameter that controls how fast the maximum size grows between adjacent levels (see (I3) below). Let B ( L i ) denote the actual size of level L number of blocks. An FD+tree maintains the following invariants (FD-tree maintains only the first three):  X  (I1) All-blocks-fenced : For every block of L i (for all i &gt; 0 )  X  (I2) Fence-first : The first entry in every block of L i  X  (I3) Max-size : Let  X  i denote the maximum size allowed for L  X  (I4) Min-size : If h &gt; 2 , B ( L h  X  1 ) &gt;  X  h  X  2  X  ( I5) No-underflow : N  X  N
FD+tree supports another optimization, level skipping , whose details we omit for simplicity and space limit. The intuition is illus-trated in Figure 4. After reorganization, a chain of single-block lev-els may be generated, which degrades lookup performance. Level skipping allows unnecessary intermediate blocks in this chain to be bypassed, thereby improving performance. For additional details, including its formalization in terms of data structure invariants, see the technical report version of this paper [20]. For an insertion, we simply add an insert data entry to L a deletion, we check whether L 0 contains an insert entry with the same key: if yes, we delete that entry and decrement N  X  ; other-wise, the entry being deleted is below L 0 , so we add a delete entry to L 0 and increment N  X  .

If the new entries we add to L 0 overflow it, we trigger a merge , as described later in Section 2.3. If N  X  N also trigger a merge. We call these two types of merges overflow-triggered and underflow-triggered , respectively.

Lookup for a given key proceeds top-down through the levels, starting with L 0 . On a level L i , we look for all data entries with key . The modification procedure for L 0 and the merge procedure guarantee that there are only four cases: L i has 1) no data entries with key ; 2) a single insert entry with key ; 3) a single delete entry with key ; or 4) one delete entry with key followed by one insert entry with key . In Case 3, we report that key is not found and stop. In Cases 2 and 4, we return the insert entry and stop. In Case 1, we look in L i for the fence f with the largest key no greater than key . 4 The search continues to the block in L i +1 pointed to by f . If we encounter Case 1 in L h  X  1 , we report that key is not found and stop. Figure 3 illustrates the process of lookup. A merge reads and replaces the top m + 1 levels ( L 0 , . . . , L for some m  X  1 . Levels below L m that exist before the merge are not disturbed. The last new level written by the merge, which we denote by L m , consolidates and stores all data entries from the old m + 1 levels; it also stores fences to L m +1 (if any). When the merge ends, new levels above L m s tore only fences. Figure 4 illustrates how merge works. Here, m = m = 3 . In practice, m c an be less than m when many input entries cancel each other.
Suppose the tree height is h before the merge. A merge that reads all levels is called a full merge . A full merge with m = h ( it can be shown that m  X  h ) grows the tree by one level. A full merge with m &lt; h  X  1 s hrinks the tree (possibly by more than one level).
Merge has three steps detailed below: merge-prepare determines which levels are involved; merge-execute combines the old levels into a new one, and creates fence-only upper levels as needed; fi-nally, merge-finalize adjusts the merge result by skipping unneces-sary levels and/or shrinking the tree when possible.
 Merge-Prepare We first determine m , i.e., which levels are to be replaced. For an underflow-triggered merge, we always do a full merge, and m = h  X  1 . For an overflow-triggered merge, we Figure 4: Illustration of FD+tree merge and motivation for l evel skipping. From right to left, we show the old levels, the new levels that would have been produced by merge without level skipping, and the actual new levels produced by merge (with level skipping). FD+FC directly produces the new levels on the right without going through the state in the middle. calculate m as follows. An upper bound on the size (in blocks) of the run obtained by merging L 0 , . . . , L i is: b
U ( i ) = The second summation in the case for i &lt; h  X  1 is a lower bound on the total number of fences in levels above L i , which will be ignored by the merge. In the case of i = h  X  1 , we in fact know the exact size of the run with the help of counters N  X  and N In other words, we try to merge as few levels as possible provided that the last level has enough space to accommodate the result.
Note that calculating b U ( i ) and m is computationally trivial, so merge-prepare adds no discernible cost to the overall merge. Merge-Execute To execute the merge, we read all materialized levels among L 0 , . . . , L m in key order in parallel and output the new levels L new 0 , . . . , L new m . 7 During the merge, we maintain p last , a pointer to the last block in L m +1 for which we have added a fence to L new m . This information is needed to ensure (I2) and, specifically, that the first entry in every m block is a fence.

Consider all entries in the old L 0 , . . . , L m with the smallest key key yet to be processed. If among them is a fence f pointing to L i +1 , we will add f to L new m . Fences pointing to L m or above are simply ignored. Let S 0 , . . . , S m denote the sets of data entries with key from L 0 , . . . , L m . We coalesce these data entries into a final set S to add to L new m . S captures the net effect of applying the insert and delete operations in S m , S m  X  1 , . . . , S is easy to see that 0  X  | S |  X  2 and there are just four cases as described in Section 2.2. We update N  X  and N  X  to reflect the effect of replacing S m , S m  X  1 , . . . , S 0 with S .
If the last block of L new m has enough space to accommodate f (when applicable) and S , we simply add them and move on to the next key. If f was added to L new m , we also update p last
If L new m has insufficient space, we begin a new block of L add f (when applicable) and S . To maintain (I2), the first entry for this block needs to be a fence. If we happen to have f to add, we are fine; otherwise, we add a fence with k ey and p last as the first entry of the new block.

When we begin a new block of L new m , we need to add to L fence to this block. If L new m  X  1  X  X  last block has no space left, we begin a new block for L new m  X  1 to put the fence in (which automatically satisfies (I2)). A fence to this new block must then be added to m  X  2 . Such fence additions may propagate all the way up to L merge-prepare ensures that L new 0 has enough space.
 After all entries from the old L 0 , . . . , L m have been processed, L 0 , . . . , L new m become the new L 0 , . . . , L m .
 Merge-Finalize The result of merge-execute needs further tweak-ing for several reasons. The first reason is that we need a way for an FD+tree to shrink in height, when there have been enough deletions and a merge produces a bottom level violating (I4).

The second reason is more subtle. Performance becomes sub-optimal when large merges generate long chains of single-block levels. After merge-execute, the new levels above L m store only fences. The sizes of these levels decrease rapidly at the rate of 1 / X  , so typically, all but a few levels above L m would have one block each, as illustrated in Figure 4. Hence, lookups perform poorly.
Thus, to allow an FD+tree to shrink, and to guard against in-efficiency, we take the third step of merge, merge-finalize. Based on the actual sizes of the new levels produced by merge-execute, merge-finalize skips new levels that are unnecessary, and adjusts level numbers for the remaining materialized new levels. It may modify L 0 , but no other levels X  contents. Because of space limit, we omit the details, which can be found in [20].

Figure 4 illustrates how merge-finalize improves lookup perfor-mance by reducing the effective tree height. FD+tree X  X  performance guarantees are summarized below; see [20] for the proof.
 Theorem 1. Let N denote the true number of elements indexed. The space consumption is O ( N/ X  ) blocks. The worst-case I/O cost of a lookup is O (log  X  N  X  tion or deletion is O (  X   X   X   X  l og  X  N  X 
A s mentioned earlier in this section, FD+tree differs from FD-tree [12] in significant ways. Out of FD+tree X  X  invariants presented in Section 2.1, FD-tree maintains only the first three: (I1), (I2) and (I3). More importantly, FD+tree improves practicality by address-ing the following three issues.

First, although FD-tree supports deletion, it does not tighten lev-els, and its merges are triggered by overflows only. Therefore, FD-tree provides no performance guarantee for workloads involv-ing deletions; the complexity bounds and proofs in [12] assume insertion-only workloads. In contrast, our Theorem 1 applies to any workload, and its bounds are stated in terms of the true number of elements indexed, not in terms of the number of entries stored in the tree (which would have been a weaker guarantee).

Second, FD-tree does not have level skipping; all levels are mate-rialized and fences always point to the immediate next level. There-fore, unlike FD+tree, FD-tree is susceptible to performance degra-dation by chains of single-block levels, as described in the discus-sion of merge-finalize.
 Third, as discussed at the beginning of Section 2, compared with FD+tree X  X  one-pass multi-level merge, an FD-tree merge proceeds in multiple passes, combining only two levels at a time. When L 0 overflows, FD-tree first merges L 0 and L 1 , producing a new L 1 (and a new fence-only L 0 ). In general, if the result of merg-ing L i  X  1 and L i can be accommodated by L i , FD-tree stops the merge; otherwise, FD-tree proceeds to merge L i and L i +1 ing L 0 , . . . , L i with new fences in the process. Although FD-tree and FD+tree merges cost asymptotically the same, FD+tree X  X  one-pass multi-level merge in most cases is the clear winner both in terms of actual cost and in terms of number of writes (which is im-portant to SSDs because of write wearing). Also, concurrency con-trol for the FD-tree merge is more difficult because multi-pass two-level merges have more complex read/write patterns; such a merge may rewrite a level multiple times, while a one-pass multiple-level merge writes each level once. Before presenting our full solution in Section 4, we present two other approaches which further motivate our design choices. FD+XM (FD+Tree with Exclusive Merge) FD+XM uses a sin-gle readers-writer lock for the entire FD+tree. The tree supports ei-ther multiple concurrent lookups, which must acquire shared locks (s-locks), or a single insertion or deletion request, which must re-quire an exclusive lock (x-lock). If a modification triggers a merge, the x-lock is released only after the merge completes.

FD+XM has low CPU overhead because each request makes a single lock call, and merges run uninterrupted with exclusive ac-cess to the tree. For workloads consisting of nearly all lookups or those whose modifications are issued far apart in time from other requests, we expect FD+XM to work well.

However, FD+XM offers no concurrency between lookups and modifications. As a merge x-locks the entire tree, lookups must wait until the merge completes. Since merges are long, such waits severely lengthen lookup response times to the point of impractical. FD+DS (FD-Tree with Concurrency by Doubling Space) The key idea is to trade space for concurrency. During a merge, instead of emptying the old levels as we go, we simply leave them intact while producing the new levels on the side. Meanwhile, lookups can still proceed through the old levels. When the new levels are ready, they replace the old levels in an atomic step, and the space taken by the old levels can then be reclaimed. Thus, FD+DS im-proves lookup response times because readers of the old levels are not competing with any writer; merges run faster too, because the single writer of the new levels is not competing with any reader.
While FD+DS is conceptually simple, implementing it still re-quires some care, as we have discovered from our experience. First, some concurrency control is still needed. For example, before re-claiming the space taken by the old levels, we must ensure any ongoing lookups through them have completed; we implement this check using a counting semaphore. Second, to support concurrent modifications while a merge is in progress, we need to write these modifications somewhere, and have lookups search through them in addition to the old levels, again necessitating concurrency con-trol. Our implementation of FD+DS adds these modifications to the new top level being created by merge; when searching the new top level, lookups do not follow fences (while lookups through the old levels still do). 8
One main drawback of FD+DS is that it doubles the index space while merges are ongoing. This higher space requirement is es-pecially costly for SSDs, which are still much smaller and more expensive than magnetic disks.

There is another subtle yet significant disadvantage to FD+DS X  X  simple rule of not modifying the old levels. Recall that the top level occupies premium memory space (the argument also holds if the top level resides in the locality area of SSDs, because this area is small). FD+DS cannot free space in the old top level until after a merge completes, and therefore cannot use that space to acco m-modate modifications that arrive during the merge. Thus, given limited space, FD+DS can accommodate fewer new modifications during a merge before stalling for its completion, so its worst-case modification response time suffers, as we will see in Section 5. Discussion Our main quest is to achieve the same or higher level of concurrency offered by FD+DS without its space overhead. Since merges have a regular, sequential access pattern, it should be pos-sible, with careful updates to the index, to direct lookups to the un-processed parts of the old levels as appropriate, while space from the processed parts continues to be reclaimed. The idea of trading space for concurrency is still applied, but we can limit redundant storage to data near the  X  X avefront X  of the ongoing merge.
Moreover, better space utilization makes it possible to improve modification concurrency. As a merge progresses, it consumes en-tries from the top level. By aggressively reclaiming space taken by these entries, it should be possible to process new modifications at the same rate as the merge consumes the old top level.

Next, we show how to realize these possibilities with FD+FC. FD+FC builds on the idea of weaving both unprocessed and pro-cessed parts of the index during a merge into a single coherent structure to support concurrent accesses. While this idea is con-ceptually simple, its realization is far from trivial. For example, we must consider the overhead introduced by maintaining a single coherent index during merges. Moreover, with fractional cascad-ing, cross-level pointers in FD+tree may form a graph, where one index block may be reached by multiple paths, which makes the index trickier to handle than traditional ones such as B-tree where pointers form a tree. In the following, we will start with the concep-tual overview of FD+FC, and gradually introduce implementation details and challenges such as those mentioned above. When there is no ongoing merge, FD+FC X  X  data structure is the same as FD+Tree. However, when there is an ongoing merge M involving L 0 , . . . , L m , the data structure consists of the following parts (as illustrated in Figure 5):  X  New top level ( L new 0 ). Initially empty at the beginning of M , this  X  Old top level ( L old 0 ). At the beginning of M , this part contains  X  Old upper levels ( L old 1 , . . . , L old m ). At the beginning of M , they  X  Below-merge levels ( L m +1 , L m +2 , . . . , if M is not a full merge). During M , the first entry in L old 0 is always a fence which we call the wavefront fence . This fence serves the special purpose of de-lineating the old and new parts of the tree. It always points to the current head block of L old 1 . Its key indicates how far the merge has progressed, with the following invariant:  X  (I6) Wavefront : Consider all data entries and fences for L Figure 5: Components of FD+FC during a merge of L , . . . , L m .
Conceptually, the wavefront fence partitions the top and upper levels of the FD+tree into old and new parts. M progressively pushes the wavefront forward X  X mptying the old top and upper levels, consolidating the entries, and populating the new top and upper levels. Meanwhile, modifications go directly to the new top level; lookups check the wavefront fence to determine which parts of the tree need to be searched. M empties the old upper levels in a careful way such that an old block is reclaimed as soon as no new lookups can ever go through it. Data Movement from Old to New Levels A straightforward im-plementation of merge moves one  X  X uple X  (or more precisely, all entries with the same key) at a time from L old 0 , . . . , L When removing an entry from an old level, we cannot afford to remove it on the SSD, because that would cause an expensive in-place write per entry. Caching the block in memory for searches and updates solves this problem, but there are other performance issues with this tuple-wise data movement . Every removal from a block in an old level requires not only updating the block X  X  in-memory data structure, but also x-locking appropriate parts of the tree to avoid conflicts with concurrent lookups that might be read-ing the old levels. As our performance evaluation reveals, the CPU overhead of these operations are high. Thus, we choose instead to implement block-wise data movement . We would never modify a block in an old level, either on SSD or in memory; we only reclaim a complete block when its contents are not needed. The memory requirement is only O ( h X  ) . More details are given in Section 4.4. Preemptive Level Skipping As discussed in Section 2.3, FD+tree performs level skipping in merge-finalize. Unfortunately, the tim-ing is too late for FD+FC, because concurrent lookups that arrive during M would miss this optimization and still see suboptimal tree shapes. Therefore, FD+FC performs preemptive level skip-ping during merge-prepare, so that even the intermediate tree state produced by merge-execute skips unnecessary levels.
 Dynamic Memory Sharing between Top Levels Both L old 0 and L 0 are memory-resident. A simple approach would be to allocate a fixed amount of memory to each, but we can do better at memory utilization by allowing them to share memory dynamically as M progresses. We implement L old 0 and L new 0 using a common buffer of the size allotted for L 0 . An overflow-triggered merge M begins when L 0 is close to (but below) full capacity and becomes L we always reserve  X  1 slots for L new 0 to accommodate fences to be produced by M . As M progresses, space taken by entries removed from L old 0 is given to L new 0 for new modifications; a modification may have to wait for M to make new space. At the end of M , L becomes empty, and L new 0 becomes L 0 . Locks F or disk-resident levels, we use a readers-writer lock for each block. Since L 0 (or L new 0 and L old 0 ) is in memory, we use a single mutex to control its access. As the wavefront fence is stored in L old 0 , its access is controlled by the same mutex. A modification goes to L new 0 if there is an ongoing merge; otherwise it goes to L 0 . It waits if no space is available in L new L 0 or L 0 and proceeds exactly as in Section 2.2. By design, FD+FC triggers a merge after the modification completes. If there is no ongoing merge, we process a lookup exactly as in Section 2.2. If a merge is underway, we compare the lookup key against the wavefront key:  X  If key is strictly greater, we first search L new 0 (but without going  X  Otherwise, we search L new 0 , and through it, the new upper levels In either case, lookup uses a standard tree-based locking protocol. It starts by locking L old 0 and L new 0 , and it always s-locks a block in the next level before unlocking the current. As with the non-concurrent FD+tree, the merge procedure has three steps, merge-prepare , merge-execute , and merge-finalize . As dis-cussed in Section 4.2, merge-prepare additionally estimates which result levels to skip, so merge-execute can avoid materializing un-necessary levels. On the result of merge-execute, merge-finalize further performs level tightening and skipping, in case that merge-prepare X  X  estimation turns out too conservative.

In the following, we focus on the merge-execute step, highlight-ing how it performs concurrency control. Details on the other two steps can be found in [20].
 Starting Merge-Execute At this point, the wavefront fence (the first fence in L old 0 ) has key  X  X  X  and points to the first block of 1 . The new upper levels do not have any blocks yet; m-insert , described below, will create them on demand.
 Merge-Execute Main Loop We repeat the following three steps until all entries from the old levels are processed.  X  M-stage reads entries from the old levels and puts them in key  X  M-insert moves entries from S to L new m and then adds fences to  X  M-delete updates the wavefront fence and conceptually  X  X eletes X 
M -Stage M -stage buffers in memory the contents of every block it reads from the old levels, so no block will be read more than once. The memory required for buffering is at most h X  entries. The first time it runs, m-stage reads one block from each of L old 0 In each iteration, m-stage starts with a empty staging area S , and keeps adding buffered entries in key order to S until some old level runs out of buffered entries to add. M-stage also ensures that S contains either all entries with the same key, or none at all. When it stops, m-stage hands off S to m-insert, and reads in the next block for any level that is out of buffered entries. While the actual number of entries in S varies, it is easy to see that the maximum is h X  .
M-stage acquires no locks, because when it is running there are no other writes.

M -In sert M -insert processes entries in S one group at a time, where each group contains all entries with the same key. Process-ing of each group proceeds exactly as in FD+tree X  X  merge-execute (Section 2.3). M-insert buffers new blocks in memory until they are full or it finishes writing; the memory required is h X  entries. Note that for each group, m-insert writes new levels bottom-up. Writing to a new block b requires no locking, because at this point no lookup can access b  X  X here are no fences to b yet since we write levels bottom-up. On the other hand, to write to an existing block b , m-insert x-locks b , and unlocks b when it finishes writing and before it writes a block in the level above. The timing of release is important to avoid deadlocks with lookups, who might be travers-ing down the new upper levels with the tree-based locking protocol.
M -Delete L et S 0 denote the subset of entries in S from L First, m-delete updates the wavefront fence in L old 0 : the key is set to the last key in S (not S 0 ), and the pointer is set to that of the last fence in S 0 (not S ), if any. Then, m-delete deletes all entries in S
Next, m-delete reclaims blocks in old upper levels that are no longer needed. Knowing when which blocks are safe to reclaim is tricky. It turns out that we cannot simply reclaim a block once all its entries have been processed by m-insert; this block may still be needed to direct lookups. Therefore, m-delete uses the following rule: a block can be reclaimed only when m-insert has processed the first key on the following block on its level. Remark A.1 in appendix explains the intricacies and why this rule works correctly. M-delete applies the rule to the old upper levels top-down. For the head block b of each level, if S contains the first key in b  X  X  following block, b is reclaimed.

M-delete locks L old 0 while modifying it. To reclaim a block, m-delete x-locks it and unlocks it when done; there is no need to x-lock the next block below before unlocking the current. Ending Merge-Execute After the merge-execute main loop com-pletes, we still have a chain consisting of the last blocks from the old levels. Recall m-delete X  X  rule of not reclaiming a block unless the first key on the following block has been processed; the last blocks do not have following blocks. Thus, we reclaim the chain explicitly. Starting from L old 0 , we finally delete the wavefront fence and make L new 0 the new L 0 ; after this point, the remaining old up-per levels are no longer accessible by lookups. Then, we proceed top-down to reclaim the blocks in old upper levels.

Modifying L old 0 requires locking. Then, we follow the standard tree-based locking protocol to reclaim the blocks, always x-locking a block in the next level before unlocking the current one. Tree-based locking is necessary to avoid conflict with any ongoing lookup that might still be searching the old levels top-down for a (nonexis-tent) key greater than all existing keys; such a lookup traverses on the very chain to be reclaimed. Instead of employing standard locking protocols on FD+tree in a straightforward manner, FD+FC carefully considers FD+tree X  X  spe-cial access patterns in designing correct and efficient protocols. For example, for the new upper levels, the top-down read pattern of lookup coexists with the bottom-up write pattern of m-insert, which means the standard top-down tree-based locking cannot be applied to both. As another example, for the old upper levels, since both lookup and m-delete have top-down access patterns, the stan-dard tree-based locking would work, but FD+FC instead allows m-delete to deviate by releasing its locks early (before acquiring child locks). This optimization hinges on the observation that there is a single writer of the tree levels at any time X  X he merge pro-c edure. Without this observation specific to FD+tree, early lock release would lead to deadlocks between two writers.

In conclusion, FD+FC serializes lookups and modification by the order in which they lock the top level, and is free of deadlocks. A discussion of the correctness of FD+FC can be found in [20]. In terms of space and I/O complexities, bounds established for FD+tree in Theorem 1 still hold for FD+FC. Because of block-wise data movement, a merge may use O ( h ) (logarithmic in N/ X  ) addi-tional blocks (without affecting the asymptotic space complexity). In comparison, the space-doubling FD+DS (Section 3) uses up to  X ( N/ X  ) additional blocks during a merge. We implemented FD+XM, FD+DS, and FD+FC in C++. We use two SSDs in our evaluation: Intel X25-E SLC 32GB SSD and Intel 320 Series MLC 80GB SSD, hereon referred to simply as X25-E and 320S . At the time when we ran our experiments, 12 . 5 TB had been written to X25-E, and 1 . 5 TB to 320S. Here we report only results for X25-E because 320S showed similar trends; for details see [20]. The indexes are stored on the SSD connected through SATA to a workstation with an Intel i7 8-core 2.8GHz CPU, 8GB main memory, and Linux 2.6.32 kernel running in single-user mode without GUI. We used Linux X  X  ext2 file-system, which does not have journaling. We set the file-system cache and the SSD X  X  inter-nal cache to write-through mode, as recommended by most database vendors. We also experimented with write-back mode for SSD X  X  internal cache; again, see [20] for details.
 We implemented two workload generators for the evaluation. The first generator, G R , generates a stream of lookup ( l ), insertion ( i ), and deletion ( d ) requests with a specified W l : W portion. The second generator, G T , generates the stream of requests by following the TPC-C workload characteristics. Remarks A.2 and A.3 give more details about the two generators. The workload is stored as a file from which a workload injection thread reads and populates two request queues: Q r for index lookup requests (reads), and Q w for index insertion and deletion requests (writes). We allocate T r and T w worker threads to process requests from Q and Q w respectively. If the workload injection thread finds Q Q w full, it blocks until slots become available in that request queue.
We measure performance with the following metrics:  X  R p is the time taken by a worker thread to dequeue a request  X  R q is R p plus the time spent by the request waiting in the queue.  X  R o is the overall time between request arrival and completion. As emphasized in Section 1, we measure (a) average, (b) variance, and (c) worst-case for the above metrics over each entire workload. We will focus on the R p and R q metrics in this section. Results for R o are similar and are presented in [20].

When testing with G R , we preload each index with 10 M inser-tions resulting in an index of 120 MB; we then run a workload con-taining 10 M requests (with specified W l : W i : W d ) and measure performance after a warm-up of 10 K requests. When testing with G , we use 10 districts per warehouse and 3000 customers per dis-trict. The preload step inserts 3 orders per customer.

Defaults for the size of the FD+tree top level (  X  0 , but measured in bytes), size ratio (  X  ), and the main-memory buffer cache size are 256 KB, 24 , and 15 MB respectively. Defaults for the total num-ber of queue slots ( | Q r | + | Q w | ) and number of worker threads ( T r + T w ) are 5000 and 8 respectively. The queue slots are divided between Q r and Q w according to the ratio W l : ( W i + W FD+FC and FD+XM, we set T r = 6 and T w = 2 because these indexes process insertions quickly. FD+FC runs merges in an ex-tra background thread that is woken up by one of the T w threads whenever merge is triggered (by overflow or underflow). Varying the Lookup Ratio Figure 6 compares FD+FC against FD+XM and FD+DS. We consider a spectrum of G R workloads by varying W l in the workload from 0 to 1 . The non-lookup requests in each workload are distributed equally between insertions and deletions so as to keep the total number of indexed records roughly constant throughout workload execution. For example, a workload with W l = 0 . 6 will have 20% each of insertions and deletions.
At a high-level, Figure 6 shows that 1) FD+FC significantly out-performs FD+XM on worst-case response times, as we expect by design, but as a bonus, FD+FC also performs better on other met-rics; 2) FD+FC delivers comparable or better performance than FD+DS without requiring the double amount of space as FD+DS; 3) Despite of doubling space, FD+DS X  X  worst-case insertion times are in fact as bad as FD+XM. We now delve into details below.
Figure 6(a) shows that FD+FC X  X  throughput is at least as good as FD+DS and is much better than FD+XM. For a very update intensive workload ( W l  X  0 . 2 ), FD+FC X  X  throughput is larger than FD+DS by around 15% . For other workloads, it is larger by 8  X  9 %. This is because updates have to wait longer in FD+DS. Over FD+XM, FD+FC X  X  advantage is around 27 % and 20 % when W l = 0 . 6 and 0 . 8 , respectively.

Figures 6(b) and 6(c) show the average R p for insertions and lookups (deletions are handled just as insertions by FD+trees). When W l reaches 0 . 9 , FD+FC processes insertions faster than FD+DS by 25% on average. It is faster by 3 . 8 times than FD+XM. In-sertions take little time to process by themselves, but for FD+XM and FD+DS, they wait longer when there is an ongoing merge X  FD+XM waits for the release of the exclusive lock, while FD+DS waits for the reclamation of the old top level. When we consider av-erage lookup R p , all three approaches perform equally well when tested with a read-only workload. Also, FD+FC performs as well as FD+DS except for update-heavy workloads; lock/unlock calls of frequently triggered merges slow down the lookups. FD+XM X  X  lookup performance is much worse, because lookups are blocked whenever there is an ongoing merge.

Figures 6(d) and 6(e) show the worst observed R p for insertions and lookups. While FD+FC X  X  worst R p over insertions is around a second, for FD+DS and FD+XM it is around 33 to 38 seconds X  displaying a crucial advantage of the fully concurrent FD+FC. As described in Section 3, FD+DS cannot remove entries from L after they were added to the new levels. If it does, lookups will fail. It can delete the entries only after the merge completes. When the merge involves many levels (for ex., a full merge), this scheme is obviously costly. For lookups, FD+FC and FD+DS show similar worst R p  X  X  (around 500 milliseconds), well below FD+XM.
Figure 6(f) shows equi-width histograms with a bucket length of 2 . 5 milliseconds for lookup R p  X  X . Similar trends were observed for insertions. The Y-axis is in log-scale. The X-axis shows the first 400 buckets, i.e., R p  X  1 second; requests with R p &gt; 1 second are added to the last bucket in Figure 6(f) X  X ence the (red) blip at the end of FD+XM X  X  histogram. Figure 6(g) complements Figure 6(f) by showing how the standard deviation of R p values for insertions is much lower for FD+FC than FD+DS.

Figures 6(h)-6(l) compare the schemes on the R q metric (recall it is R p plus the time spent by the request in the Q r or Q FD+FC X  X  better performance on the core R p metric translates into F igure 6: Comparison of FD+XM, FD+DS, and FD+FC on Intel X  X  X25-E SSD for G pletion time (a), average insertion R p (b), average lookup R of lookup R p  X  X  (f), standard deviation in R p (g), comparison on R (h)-(l). better performance on R q For the three concurrency schemes, av-erage insertion R q is large when W l is small. Average lookup R shows an opposite trend. The reason is, for low W l , there are so many updates that processing them becomes the bottleneck. Hence, insertion requests wait in the queue longer than lookup requests. As W l increases, the bottleneck shifts to lookup processing. Fig-ure 6(k) shows FD+DS and FD+XM have high worst-case insertion R . FD+FC X  X  starts higher, because the workload is too skewed, and there are always many insertions waiting in the queue; but as W l increases, it falls to 2.5 seconds, where as FD+DS and FD+XM still remain close to 33 seconds. Figure 6(k) shows FD+XM suf-fers high worst-case lookup R q . Figure 6(l) shows higher standard deviation for FD+DS insertion R q distribution than that of FD+FC. Varying the Initial Index Size Figure 7 shows the performance trends on X25-E as we scale the initial index size to 90 M key-value pairs totaling 1080 MB. Workload size is set to be equal to the initial index size. W l is set to 0 . 8 . The worst-case insertion R p for FD+DS jumps to 659 seconds when the database size is 90 M. But, FD+FC X  X  still remains under a second. The benefits of FD+FC X  X  full concurrency are clear when data sizes increase; its lookup and insertion R p  X  X  remain manageable. These results show that FD+FC X  X  concurrency algorithms scale as well as FD+DS. Workloads based on TPC-C Figure 8 compares the schemes for G
T workloads with TPC-C characteristics. As the number of ware-houses increases from 20 to 100 , initial index size increases from 21 MB to 105 MB. Note that the insertions constitute 91 . 3% of the requests in this workload (see Remark A.3); the remaining 8 . 7% are lookups. For such workload characteristics, Figure 8(a) shows that FD+FC X  X  throughput is higher than both FD+XM as well as FD+DS. It is higher by 14% than FD+DS when number of ware-houses is 100. Average insertion R p is slightly better for FD+FC (Figure 8(b)), around 13% less when the number of warehouses total completion time (a), average insertion and lookup R R q (d), worst-case insertion and lookup R q (e), and standard deviation in R is 100 . However, FD+DS has better average lookup R p because for high-insertion workloads, FD+FC X  X  concurrent block reclama-tions constantly interfere with lookups. Still, FD+FC has higher throughput, and as Figure 8(c) shows, has lower worst-case inser-tion R p . FD+DS insertion R p values also exhibit high variance (see Figure 8(f)). FD+FC X  X  average R q for update requests is as bad as FD+DS (Figure 8(d)), this is because the workload is very update intensive. But, FD+DS X  X  worst case R q rises linearly (Figure 8(e)). Having seen the end-to-end benefits of FD+FC, we now drill down to the benefits provided by individual features.
 Dynamic Memory Sharing between L new 0 and L old 0 FD+FC X  X  memory sharing feature allows space freed from L old 0 by a merge to be added to L new 0 immediately (Section 4.4). The plot  X  X D+FC w/o MS X  in Figure 9(a) compares the performance of FD+FC without memory sharing against  X  X D+FC w/ MS, X  the regular version of FD+FC with memory sharing. FD+DS X  X  worst insertion R p is close to that of  X  X D+FC w/o MS, X  which is around 32  X  38 seconds. Worst insertion R p for FD+FC is much smaller at around a second. Block-wise vs. Tuple-wise Data Movement FD+FC uses block-wise data movement during merges (Section 4.4). To show the ben-efit of this feature, Figures 9(b) and 9(c) compare FD+FC against FD+FC/TUP, a variant of FD+FC that uses tuple-wise data move-ment in the merge procedure. FD+FC/TUP leads to heavy CPU us-age and long-running merges that impact insertion response times severely. When W l = 0 . 9 , average insertion R p of FD+FC/TUP is 12 times worse than FD+FC. FD+FC/TUP X  X  worst-case inser-tion R p is much worse than even that of FD+XM, highlighting the importance of optimizing CPU usage when using SSDs. A number of indexes have been proposed recently to optimize for the SSDs X  fast random read and slow random write characteristics. BFTL [21], FlashDB [16], and LA-tree [2] are based on B-trees and perform some form of logging in order to postpone in-place updat-ing of B-tree blocks. SkimpyStash [7] and SILT [13] are exact-match key-value stores based on hashing. FD-tree [12] is a state-of-the-art index designed for SSDs, which we have discussed and compared with in detail in Section 2.4. All of these indexes have reorganizations as essential part of their operations. Their reorga-nization costs vary, but are consistently higher than those in their counterparts designed for magnetic disks. However, none of these past works address concurrency control. The PIO B-tree [18] tech-nique includes a basic concurrency scheme that is very similar to FD+XM (discussed in Section 3). In contrast, the FD+FC scheme we developed allows lookups concurrent access to the index while a merge is ongoing.

Index structures optimized for writes to magnetic disks have also been considered in the database literature. LSM-tree [17] maintains multiple B-trees with geometrically increasing sizes. All updates go to the smallest tree. Rolling merges , which run concurrently between each pair of neighboring levels, percolate these updates to lower levels. Our work differs from LSM-tree in many ways. First, LSM-tree X  X  design is motivated by an always active insertion workload (e.g., when indexing a growing log file), while we tar-get traditional workloads including OLTP. Second, CPU efficiency is not a concern for LSM-tree; however, since SSDs have orders-of-magnitude faster I/Os than magnetic disks, CPU costs become significant (see Section 5) and we must design for CPU efficiency. Together, these differences in design goals translate into very differ-ent choices: 1) To speed up search across levels, FD+tree uses frac-tional cascading (Section 2), which requires maintaining pointers across levels. LSM-tree does not use fractional cascading because it targets insertion-heavy workloads. 2) LSM-tree uses multiple rolling merges to increase insertion throughput. However, such an approach would consume a lot of OS resources (threads, memory, etc.) and add too much CPU overhead for OLTP workloads running on SSDs; it would also significantly complicate concurrency con-trol in the presence of fractional cascading. In contrast, FD+FC X  X  one-pass multi-level merge is more CPU-efficient and works well with fractional cascading.

The LHAM-tree [15] is conceptually similar to LSM-tree, but targets temporal databases. The bLSM-tree [19] is similar to LSM-tree, but uses bloom filters to improve lookup performance and carefully designed scheduling policies for synchronizing between rolling merges. The Stepped-Merge technique proposed in [9] is similar to LSM-tree, but maintains multiple B-trees at each level. The T1SM [10] partitions data into subindexes, each of which is an LSM-tree. For the last two techniques, concurrency control is implemented by allowing the merge to create a new version of the index or subindex, and dropping the previous version after the merge completes. Thus, this approach is analogous to FD+DS dis-cussed in Section 3, which we have evaluated and compared with FD+FC in Section 5. Like LSM-tree, none of LHAM-tree, bLSM-tree, Stepped-Merge, and T1SM supports fractional cascading. F igure 9: (a) Benefits of memory sharing between L new 0 . (b, c) Performance of FD+FC vs. FD+FC/TUP that shows the importance of FD+FC X  X  block-wise data movement. New indexes are being designed for database systems that store data on SSDs. We argue that efficient concurrency control schemes are crucial in making these indexes usable for a wide spectrum of workloads. In this paper, we have described the FD+tree index for SSDs and the associated FD+FC concurrency control scheme, which, to our knowledge, is the first of its kind. We demonstrated the performance benefits of FD+FC through extensive experimental evaluation. A promising avenue for further work is to consider crash recovery for FD+FC.
 Remark A.1 (When to reclaim a block; Section 4.4) Consider a block b 1 and its following block b 2 on the same old upper level. Let k denote the key of the last entry in b 1 and let k 2 denote the key some block c that precedes the block pointed to by the first fence of b . Suppose m-insert has just processed entries with key k where k  X  k &lt; k 2 , so all entries in b 1 have been processed. However, if we reclaim b 1 at this point, a lookup in the key range ( k, k still should be directed to the old upper levels, would not be able to reach c . On the other hand, if m-insert has just finished processing k , we can safely reclaim b 1 because the wavefront key has moved to k 2 , and lookups for keys no greater than k 2 will be directed to the new upper levels instead.
 Remark A.2 (Workload generator G R ; Section 5) Given the total number N of requests and the proportion W l : W i : W lookup, insertion, and deletion requests for a workload, G determines the number of insertion requests N i to be generated. It initializes two variables k l and k u with the smallest and largest keys that will be inserted as part of this workload: k l = s + 1 and k u = s + N i , where s is the largest key (inserted previously) in the index before the workload starts (or zero if the index is empty). G
R follows a two-step procedure to generate each request. It first determines which type of request to generate by choosing randomly with the ratio W l : W i : W d . To generate a lookup request, G picks a key from a list K that maintains all keys currently in the index. To generate an insertion request, G R picks the smallest key in [ k l , k u ] \K and a randomly chosen value; the chosen key is added to K . To generate a deletion request, G R randomly picks from K with the constraint that it was inserted at least 100 requests earlier. Remark A.3 (Workload generator G T ; Section 5) TPC-C bench-mark simulates a complete order-entry environment. The following five types of transactions execute against the database: order entry, order delivery, payment, order status check, and inventory check. While each order is entered as a single transaction, 10 orders are delivered together according to the TPC-C specification. Hence, order delivery transactions are roughly 10 times fewer than order entry transactions. The frequencies of the five transactions accord-ing to the TPC-C specification are 45 %, 43 %, 4 %, 4 % and 4 %.
The NewOrder table in TPC-C has attributes NO_W_ID (ware-house id), NO_D_ID (district id) and NO_O_ID (order id). They together form the primary key. Orders that have not been processed yet are stored in this table. Our workload generator G T generates index requests for the primary index built on the New Order table.
