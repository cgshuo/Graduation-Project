 In this paper, we propose a search-based approach to join two tables in the absence of clean join attributes. Non-structured documents from the web are used to express the correlations between a given query and a reference list. To implement this approach, a major challenge we meet is how to efficiently determine the number of times and the locations of each clean reference from the reference list that is approximately mentioned in the retrieved documents. We formalize the Approximate Membership Localization (AML) problem and propose an efficient partial pruning algorithm to solve it. A study using real-word data sets demonstrates the effective-ness of our search-based approach, and the efficiency of our AML algorithm.
 H.2.4 [ Database Management ]: Systems X  Textual Databases Algorithms, Experimentation Approximate Join, Web-based Join, Approximate Membership Lo-cation, AML
With the growing amount of information available on the web, the connections betw een entities of the worl d become exposed for everyone to see and use. The long tradition of cross-referencing information between databases using repeated information through join procedures finds its limits with the amount of information struc-tured into these databases. The objective we pursue in this paper is to leverage the non-structured information available in free text documents on the web in order to perform approximate joins in the absence of clean join attribute.

A typical example is a case where an institution needs to estab-lish the rank of the venues where a researcher X  X  papers are pub-lished given paper titles and according to a ranking of venues pro-vided by a government in the form of a database, such as the ERA conference and journal ranking list 1 provided in Australia. Such example carries a great amount of variations in the usage of venue names on the web, ranging from abbreviations to typos. The clas-sical approach to the problem is to use an existing join table with dirty attributes and match them to the clean references. However even when researchers provide venue names attached to their paper titles this information may not be used to perform a join. Paper titles can however be used as a web query for retrieving the join attribute. According to the ARC example, it means that for each publication title we must provide a score for each venue name from the ERA approximately or exactly appearing in the web results set. The score we propose is not only based on the number of mentions of each attribute in the results set but also the rank of the document they appear in and their distance to the initial attribute used as the query.

The localization of clean references in retrieved documents of dirty entity name is not trivial given the large size of the reference list and the noisy nature of web documents, where the mention of the references can be approximate and there may be mentions of non-relevant references. We formalize this problem as Approxi-mate Member Localization (AML), which targets at locating clean references approximately mentioned in a given document. In order to efficiently solve the AML problem, we propose an optimized al-gorithm which prunes a large part of overlapped redundant match substrings before generating them.

Our main contributions in this paper are summarized below: After covering related work Section 2, Section 3 presents the frame-work of our search-based methodology for web-based join, as well we the definition and algorithm for Approximate Membership Lo-calization (AML). The results of our experimental study are pro-vided in Section 4. http://www.arc.gov.au/era/era_2010.htm
Previous attempts to solve the problem focussed on retrieving one data source containing a unique and possibly dirty reference attribute in correlation with the initial attribute and match the dirty reference attribute with the attributes from the clean reference list. Most approaches focus on the textual similarity (or syntactic sim-ilarity) between strings [5]. However, when representations of the same real-world entity are syntactically far apart from each other another line of approaches is necessary, using correlation [6], say mutual information, between reference strings within the data set or across an external collection of documents. For example, if two venue names like  X  X DD X  and  X  X IGKDD X  refer to the same con-ference, they might co-occur in many documents. However, there might not be enough mutual information to leverage within the data set, and an external collection of documents which can provide suf-ficient and accurate mutual information is not always available.
In this paper, we propose Web-based join, which is an approxi-mate join using documents collected by a web search engine. An-other web search-based method extends the original reference list with IDTokenSets (IDentifying Set of Tokens) [1]. Here the exten-sion of the reference list can be done off-line, reducing the approx-imate entity matching problem to an exact entity matching prob-lem against the extended reference table, which can be processed on-the-fly. This approach is an extension of token-based similar-ity metric, and cannot step across the textual-semantic gap. There are other synonyms which do not consists of IDTokenSets such as abbreviations, typos that can not be found.
Given a list of elements T with an attribute T.X and a clean reference list R with an attribute R.A , the problem is to create from the web an intermediary table RT containing the correlations between two attributes T.X and R.A in order to perform a join between T and R . Given that the information available on the web can be dirty and noisy, RT shall contain the likelihood associated with its entries.

Based on the hypothesis that there exist web documents contain-ing elements of T.X that also contain the elements of R.A ,weuse the elements of T.X as a query for a search engine to retrieve the ranked list of documents Docs . The framework and notations are presented in Figure 1.
The values of the links to clean references of R in Docs are measured by adopting an unsupervised approach inspired from [1]. This approach provides a score that can be used by setting a thresh-old (either for the value or for the number of best matched clean references) to perform the join. For a given value T.x of T.X ,the three relevant parameters of the evaluation of correlations are for each document:
For each clean reference r in R.A , the likelihood 2 that r is cor-related to a value T.x of T.X can be measured by: where imp ( d ) is the importance of document d and score thelocalscoreof r in d .

To calculate imp ( d ) , the N documents of Doc should be parti-tioned into B ( 1  X  B  X  N ) ranges according to their rank returned by the web search engine, so that the importance imp ( d ) lows the normalized discount cumulative gain (NDCG) function [3], which is popularly used for assigning degrees of importance to web documents in a ranked list.
The score of an entity r in document d can be calculated as fol-lows: score ( r, d )= w a  X  freq where N is a normalization factor, w a is the weight given to the frequency of a reference mentioned in d ,and 1  X  w a is the weight given to the distance between each mention of the reference and the position of T.x in d .

This scoring approach requires to determine the number of times and locations where a clean reference mentioned in Docs .Given that these references may be approximately mentioned in the doc-uments, we need to find non-overlap substrings that can approxi-mately match with any clean reference in documents. When several reference-matched substrings overlap in a word position of a doc-ument, only one of them should remain: the one with the largest similarity to its matched entity. For ease of presentation, we call this matched substring a bestmatch substring in this paper.
Likelihood is here to be understood in the general sense rather than mathematical.
Figure 2: Work Flow Comparison in Solving AML Problem
We formally define the problem of finding bestmatch substrings from a document w. r. t a reference list as the Approximate Member-ship Localization problem below.
 D EFINITION 1. (Approximate Membership Localization (AML)) Given an input document M and an entity reference list R ,thetask of approximate membership localization is to find all match sub-strings m in M for each reference r in R , such that: 1) sim ( m, r ) &gt; X  ,where sim () is a given similarity function, and  X  is a given threshold between [0 , 1] . 2) there is no substring m that overlaps with m in any position of M which satisfies sim ( m ,r ) &gt;sim ( m, r ) ,where r is also a reference from R .

As illustrated by Figure 2(a), a straightforward method of AML is based on AME method. The AME method generates a large set of AME candidate results firstly, which is a superset of AML results. There are mainly two kinds of redundancies, which need to be removed later: 1. Boundary Redundancy: In a document d = { w 0 ,w 1 , ...w 2. Multi-match Redundancy: In a document d = { w 0 ,w 1 , ...w
An experimental pilot study showed that given a reasonable sim-ilarity threshold such as 0.85, about 90% elements in the candidate or result set of AME are redundancies, thus forcing the AME-based process to unnecessarily generating and verifying them. P-Prune Algorithm: A more efficient approach is to prune the potential redundant substrings before generating them, so that less time is required for generating and verifying the remaining candi-date pairs (Figure 2(b)). This method can generate almost the same results as the AME-based method. Although it obscures some po-sitions of bestmatch substrings, it reaches overall a much higher efficiency than the straightforward method, as will be reflected in the experimental study.

Divide and Conquer: We divide M into sub-documents 3 , each
Each sub-document is a consecutive substring of M and sub-documents may overlap with each other sub-document is called as a domain of M such that there is at most one bestmatch substring in each domain and all bestmatch substrings are within domains of M . Therefore, the problem of finding bestmatch substrings on M becomes a problem of finding the bestmatching substrings from each domain of M .

Step 1. Divide M into domains: Basedonthe min-signature scheme [2], if a substring m from M matches with an entity r , it should contain at least one word from the min-signature set of r . The words in the min-signature set of r are taken as r  X  X  strong words . Assume we are only interested? in substrings no longer than L 4 , such that all matched substrings of an entity r must appear within a distance L around a strong word of r . Therefore, given a 2
L  X  1 size window with a strong word of an entity r located at its center, the window is a domain of r since there is at most one bestmatch substring of r within the window. This also provides the bestmatch substrings within all window domains (short for size window domain). All window domains in a document can be classified into two categories depending on whether they overlap with other domains in position: non-overlap domains ,and over-lap domains .

Step 2. Prune domains: The window domains above are raw divisions. For very large reference lists, the number of window do-mains, especially overlapped ones, can become an issue. In order to reduce the amount of domains, three prune strategies are proposed to prune the ones that cannot contain a bestmatch substring. The size of other domains can also be shortened without changing their properties.
Step 3. Locate bestmatch substrings from domains: We lo-cate bestmatch substrings from the domains remaining after prun-ing. Match substrings of non-overlap domains never overlap with those in other domains. Hence a substring matching a reference already proves that there is a bestmatch substring in this domain, and the location of this domain will be taken as the location of its bestmatch substring. For several overlapped domains, we have to generate all match substrings for each domain, and select the non-overlap ones most similar to the entities as bestmatch substrings.
We study the search-based strategy for joining publication ti-tles with venue names from the ERA conference &amp; journal list. The ERA ranking list contains about 25k conference and journal X  X  names. The names are in average 4.6 words long. We manually extracted about 60k publication records from more than 500 re-searcher X  X  home pages in Computer Science 5 . In order to establish
L is the maximum length of substring that may match with any reference in the given reference list
This dataset has been collected by the first author and is available upon request. a comparison with string-based approaches, the venue names as-sociated to the paper titles on the researcher X  X  home page are also collected. To establish gold standard, we manually labeled 10% of the 60k publication records with the corresponding venue names from the ERA list.
We compare the effectiveness of our web-based join method with several methods below:
IDTokenSets: This is the search-based method proposed by Chaud-huri et. al. in [1], which expands the given reference list with ID-TokenSets of each of its entities.

Token-based: Here we use the idf (inverse document frequency) metric to assign weights to words, then calculate the cosine-similarity between the word sets of the two strings.

Edit-distance: We use the Monge-Elkan version of edit-distance similarity metric that can weaken the influence of word gaps in the strings.

Our web-based join requires to retrieve web documents. Through a pilot experiment, we choose to use snippets instead of web pages since it can reach better effectiveness with much less web accesses. In our experiments, snippets of the first 100 search results retrieved by Google X  X  web search engine are used for both our Web-based Join method and IDTokenSets method. The P-Prune algorithm is used for the AML component and reaches the best performance with this data set when the substring length threshold is set to L 10 and the similarity threshold to  X  =0 . 85 . In the link evalua-tion process, usually we set B =10 , and we reach the best results when we have w a =0 . 7 .The IDTokenSets method reaches its best performance with  X  =0 . 85 .

Based on the gold standard, we measure the precision and re-call of the entity matching results of the four methods for various thresholds. The results of this evaluation presented on the graph of Figure 3 demonstrate that our web-based join method reaches its highest recall and precision (recall=0.831, precision=0.873), which is far above all other methods.
Figure 3: Comparison of effectiveness of different methods
We compare the efficiency of our P-Prune algorithm to the AME-based algorithm not only based on the Pub  X  ERA data set, but also two other real-world data sets. The properties and the running time comparison between our P-Prune algorithm and the straight-forward AME-based algorithm 6 are given in Table 1. The last col-umn is the rough proportional comparison of the running time be-tween P-Prune and AME-based methods averaged over values of  X 
We choose to use EvIter based on an SIL index as the AME algo-rithm as it has proven to be state-of-art in terms of efficiency [4]. between 0.75 and 0.95. The difference between the datasets can be explained by the variations in the average length of references. For longer entity names in the reference list is more likely that domains overlap with each other.

Additional experiments have shown that the efficiency of P-prune degrades linearly for increasing values of the substring length thresh-old L , the dictionary size | R | , and the query document size
DBLP-Person
In this paper, we proposed a new web-based framework to per-form approximate joins on apparently non correlated tables. This method uses free-text documents retrieved on the web for the ele-ments of one table used as a query and extracts the elements of the other table using Approximate Membership Localization to pro-vide correlation values. Our experimental study based on the ARC publication evaluation against the ERA list, demonstrates that our method can reach a higher precision and recall than the previous search-based one proposed in [1], and previous textual-based simi-larity metrics that use a unique and dirty join attribute. We also for-malized the Approximate Membership Localization (AML) prob-lem, and proposed to solve it with an efficient P-Prune algorithm which proved to be several times faster, and sometimes even tens or hundreds of times faster than simply adapting formerly existing AME methods on several real-world data sets. [1] S. Agrawal, K. Chakrabarti, S. Chaudhuri, V. Ganti, A. Konig, [2] K. Chakrabarti, S. Chaudhuri, V. Ganti, and D. Xin. An [3] K. Jarvelin and J. Kekalainen. Cumulated gain-based [4] X. M. Jiaheng Lu, Jialong Han. Efficient Algorithms for [5] N. Koudas, S. Sarawagi, and D. Srivastava. Record linkage: [6] C. Manning and H. Schutze. Foundations of statistical natural
