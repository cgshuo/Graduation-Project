 With the sheer growth of online user data, it becomes chal-lenging to develop preference learning algorithms that are sufficiently flexible in modeling but also affordable in com-putation. In this paper we develop nonparametric matrix factorization methods by allowing the latent factors of two low-rank matrix factorization methods, the singular value decomposition (SVD) and probabilistic principal component analysis (pPCA), to be data-driven, with the dimensionality increasing with data size. We show that the formulations of the two nonparametric models are very similar, and their optimizations share similar procedures. Compared to tradi-tional parametric low-rank methods, nonparametric models are appealing for their flexibility in modeling complex data dependencies. However, this modeling advantage comes at a computational price  X  it is highly challenging to scale them to large-scale problems, hampering their application to applications such as collaborative filtering. In this pa-per we introduce novel optimization algorithms, which are simple to implement, which allow learning both nonpara-metric matrix factorization models to be highly efficient on large-scale problems. Our experiments on EachMovie and Netflix, the two largest public benchmarks to date, demon-strate that the nonparametric models make more accurate predictions of user ratings, and are computationally com-parable or sometimes even faster in training, in comparison with previous state-of-the-art parametric matrix factoriza-tion models.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Theory, Performance Nonparametric Models, Matrix Factorization, Collaborative Filtering
Recent advances in collaborative filtering have been par-alleled by the increasing popularity of low-rank matrix fac-torization methods. To discover the rich structure of a very large, sparse rating matrix, empirical studies show that the number of factors should be quite large [10]. Inspired by the success of kernel methods [11], which generalize finite-dimensional linear models to infinite-dimensional functions in RKHS, it is natural to ask  X  why do we restrict the num-ber of factors in advance? Kernel methods enable the use of infinite dimensional feature spaces; however, using kernel methods for collaborative filtering is hampered by the large scale of real-world data. To the best of our knowledge, it has only been attempted on relatively small problems [12].
Nonparametric models differ from parametric models in the sense that the model dimensionality is not specified a priori , but is instead determined from data. The term  X  X on-parametric X  is not meant to imply that such models com-pletely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance. Recent years has witnessed the successful development of such non-parametric methods as kernelized support vector machines (SVMs) [11], Gaussian processes (GPs) [7], and Dirichlet processes (DP) in modeling complex data dependencies. On the other hand, applying these models to large-scale data is always challenging. To date, optimization for large scale nonparametric models like kernel systems remains an active research area. Since collaborative filtering problems usually involve an even greater scale of observational data than clas-sification/regression problems, fast nonparametric methods for collaborative filtering is a relatively untouched area.
In this paper we investigate nonparametric matrix fac-torization models, and study together two particular exam-ples, the singular value decomposition (SVD) and proba-bilistic principal component analysis (pPCA) [14, 9]. Their nonparametric counterparts in fact both have simple and similar formulations, which we call by nonparametric SVD (NSVD) and nonparametric pPCA (NPCA), respectively. By effectively exploiting data sparsity and organizing the computation, we show that learning with such models is in fact efficient on large-scale sparse matrices. Interestingly, although learning probabilistic models is usually considered to be slow, our methods make NPCA as fast as its non-probabilistic counterpart NSVD.
We applied the two proposed algorithms to the EachMovie data matrix of size 74 , 424  X  1 , 648 (users  X  movies) and the Netflix data matrix of size 480 , 189  X  17 , 770, the two largest public benchmarks to date. Our results are, in term of both efficiency and accuracy, comparable or superior to the state-of-the-art performance achieved by low-rank matrix factor-ization methods.
We use uppercase letters to denote matrices and lower-case letters to denote vectors, which are by default column vectors. For example, Y  X  R M  X  N is an matrix, its ( i, j )-th element is Y i,j , and its i -th row is represented by an N  X  1 vector y i . The transpose, trace, and determinant of Y are denoted by Y &gt; , tr( Y ), and det( Y ), respectively, and I de-notes the identity matrix with an appropriate dimensional-ity. Moreover, k y k is the vector ` 2 -norm, k Y k F denotes the Frobenius norm, and k Y k  X  the trace norm, which is given by the sum of the singular values of Y .

We denote a multi-variate Gaussian distribution of a vec-tor y with mean  X  and covariance matrix  X  by N ( y ;  X ,  X ), or by y  X  N (  X ,  X ); E (  X  ) denotes the expectation of ran-dom variables such that E ( y ) =  X  and E [( y  X   X  )( y  X   X  ) Cov( y ) =  X .

If Y contains missing values, O denotes the indices of ob-served elements of Y , and | O | the number of observed ele-ments. ( i, j )  X  O if Y i,j is observed, ( Y ) 2 O = the sum of the square of all observed of Y . We use O i to denote the indices of non-missing elements of the i -th row y . For example, if y i  X  X  elements are all missing except the 1st and the 3rd elements, then:
In this paper we consider an M  X  N rating matrix Y describing M users X  numerical ratings on N items. A low-rank matrix factorization approach seeks to approximate Y by a multiplication of low-rank factors, namely where U is an M  X  L matrix and V an N  X  L matrix, with L &lt; min( M, N ). Throughout this paper we make the as-sumption M &gt; N , without loss of generality. Since each user rates only a small portion of the items, Y is usually ex-tremely sparse. Collaborative filtering can be seen as a ma-trix completion problem, where the low-rank factors learned from observed elements are used to fill in unobserved ele-ments of the same matrix.
Traditionally, the SVD is derived in terms of approximat-ing a fully observed matrix Y by minimizing k Y  X  U V &gt; However when Y contains a large number of missing values, a modified SVD seeks to approximate those known elements of Y according to where  X  1 ,  X  2 &gt; 0, and the two regularization terms k U k and k V k 2 F are added to avoid overfitting. Unfortunately, the optimization problem is non-convex. Gradient based approaches can be applied to find a local minimum. This algorithm is perhaps one of the most popular methods ap-plied to collaborative filtering, e.g., [4, 5, 13].
Probabilistic PCA [14, 9] assumes that each element of Y is a noisy outcome of a linear transformation where U = [ u i ]  X  R M  X  L are the latent variables follow-ing a prior distribution u i  X  N (0 , I ), i = 1 , . . . , M , and e i,j  X  N (0 ,  X  ) is independent Gaussian noise. The learning can be done by maximizing the marginalized likelihood of observations using an Expectation-Maximization (EM) al-gorithm, which iteratively computes the sufficient statistics V and  X  in the M-step. Note that the original formula-tion includes a mean of y i ; here we assume that data are centered for simplicity. Related probabilistic matrix factor-izations have been applied in collaborative filtering, e.g., [6, 10].
One way to construct a nonparametric matrix factoriza-tion model is to relax the cardinality constraint such that the approximating matrix U V &gt; is full-rank; the following simple result then holds.

Proposition 1. If U and V are both full-rank, the prob-lem (2) is equivalent to Proof: In solving the problem (2), if we fix V but minimize the cost w.r.t. U , the sub optimization problem becomes standard ridge regression By the representer theorem, we know the solution must sat-isfy U = AV , A  X  R M  X  N . Plugging this first-order condi-tion into the cost (2) and letting X = U V &gt; and K = V V because K 0, we obtain (4). 1
In a similar way, we can derive the equivalence to Though we get two equivalent formulations, in the remain-der of this paper we will mostly focus on problem (4), be-cause the kernel matrix K is smaller than  X  (i.e., M &gt; N ). An earlier version of this result has appeared in [17].
By setting the partial derivatives of the cost (4) w.r.t. K to be zero, we obtain the optimum condition Plugging the above condition back into (4), we obtain the equivalence to a convex learning problem known as max-margin matrix factorization [12] using the trace norm. Because of the convexity, the global optimum can be reached by any algorithm that seeks a local optimum.

The optimization of (6) resorts to semidefinite program-ming that can handle only small matrices [12]. Our result will show that, in contrast, (4) can be applied to much larger-scale collaborative filtering problems. Since (6) implies a parameter redundancy in  X  1 and  X  2 , in the following we let  X  =  X  1 =  X  2 . We first suggest an EM-like coordinate descent algorithm by alternatively updating X and K ; conveniently, both updates have analytical solutions: 2 Implementation of the algorithm requires only basic matrix computations. The kernel regression step (7) suggests, how-ever, the possibility that when working with  X  X nfinite di-mensions, X  the so-called  X  X ernel trick X  [11] can be applied to exploit the data sparsity. In fact, there is an even greater opportunity to improve efficiency, as we shall discuss in Sec-tion 4.
For the pPCA model (3) in the limit of  X  X nfinite factor dimensions, X  it is infeasible to directly handle either u v , but we can work on x i = [ X i, 1 , . . . , X i,N ] &gt; u v j . It is easy to see that x i follows an N -dimensional Gaussian distribution with mean and covariance
Note that the two optimization steps here are different from the standard EM algorithm for learning probabilistic mod-els. In this paper we call them by E-step and M-step, mainly for reference conveniences and showing the analogy with an-other EM algorithm that is introduced later.
 Let K = V V &gt; , and relax the rank constraint such that K is a positive-definite kernel matrix. Then the pPCA model (3) is generalized to a simple generative model where e i = [ e i, 1 , . . . , e i,N ] and The model describes a latent process X , and an observa-tional process Y , whose marginal probability p ( Y | K,  X  ) is As we can see, pPCA in fact assumes that each row of Y is an i.i.d. sample drawn from a Gaussian distribution with covariance K +  X I . If we consider maximizing the joint like-lihood p ( Y, X | K,  X  ) with respect to X and K , we obtain an optimization problem min which appears to be similar to the SVD formulation prob-lem (4). The major difference is that (11) employs the log-determinant as a low-complexity penalty, instead of using the trace norm.

However (11) is not a probabilistic way to deal with un-certainties and missing data. A more principled approach requires integrating out all of the missing elements, and aims to maximize the marginalized likelihood (10) condi-tioned on the observed elements. This is done by a canonical expectation-maximization (EM) algorithm: The EM algorithm appears to be similar to the coordinate descent in Section 3.1, because both involve a kernel regres-sion step, (7) and (12). Due to the additional computation of the posterior covariance of x i by (13), the E-step is ac-tually identical to Gaussian process regression [7]. The it-erative optimization is non-convex, and converges to a local optimum. For notational convenience, we refer to the two algorithms NSVD (nonparametric SVD) and NPCA (nonparametric pPCA), and describe their large-scale implementations in this sec-tion. The two EM algorithms share a few common compu-tational aspects: (1) Instead of estimating the latent factors U and V , they work on an approximating matrix X ; (2) the major computational burden is the E-step, which has to go over all the M users; (3) in both cases, the E-step is de-composed into independent updates of x i , i = 1 , . . . , M ; (4) for each update of x i , the kernel trick is applied to exploit the sparsity of Y . Though the last two properties ease the computation, a naive implementation is still too expensive on large-scale data. For example, on the Netflix problem, a single E-step will consume over 40 hours using NSVD 3 . Even worse, since NPCA takes into account the distribution of X and computes its second order statistics by (13), it costs an additional 4,000 hours in a single E-step. In the following, we show that the computational cost of (7) or (12) can be significantly reduced, and the overhead caused by (13) can be almost completely avoided. As a result, NSVD is as fast as low-rank matrix factorization methods, while NPCA is as fast as its non-probabilistic counterpart  X  NSVD.
We first reduce the computational cost of (7), which is the bottleneck of NSVD. The computation can be re-written as where K is the N  X  N full kernel matrix, and z i  X  R N is a vector of zeros, excepts those elements at positions O i whose values are assigned by The re-formulation of (7) suggests that (8) can be realized without explicitly computing X , since The above analysis suggests that, for every row i , we can save the multiplication of an N  X | O i | matrix K : , O i vector z O i of length | O i | , and the N 2 multiplication x (i.e., replaced by the smaller | O i | 2 multiplication z In total we get a reduction of N tiplication operations. On the Netflix data, this means a reduction of over 40 hours for one E-step, and the resulting computation takes less than 4 hours.

The next major computation is the eigenvalue decompo-sition required by (8). Since the trace norm regularizer is a rank minimization heuristic, after some steps if K is rank R , based on (16) we know that the next K has a rank no more than R . Thus at each iteration we check the rank R of K and at the next iteration compute only the leading R eigenvalues of K .

The pseudo code is described by Algorithm 1. We omit some fine details to keep the description compact, including that (1) we check if the predictions deteriorate on a small set of validation elements and quit the program if that hap-pens; (2) we keep Q and S to compute the step (11) via
Throughout this paper, computation time is estimated on a PC with a 2.66 GHz CPU and 3.0G memory.
 KBK = QS ( QBQ ) SQ &gt; , and store the resulting matrix in the memory used for K . So the largest memory consump-tion happens during the inner loop, where we need to store two N  X  N matrices B and K , with totally N ( N  X  1) mem-ory cost. The major computation is also the inner loop, which is O ( ing K , the prediction on a missing element at ( i, j ) is by X Algorithm 1 NSVD Require: Y ,  X  &gt; 0, iter max ; 1: allocate K  X  R N  X  N , B  X  R N  X  N ; 2: initialize iter = 0, K = I , R = N ; 3: repeat 4: iter  X  iter + 1; 5: reset B , i ; 6: repeat 7: i  X  i + 1; 10: until i = M ; 11: [ Q, S ] = EigenDecomposition( KBK, rank = R ); 12: S  X  Sqrt( S ); 13: R  X  min K , subject to 14: S  X  Truncate( S, R ); 15: K  X  QSQ &gt; ; 16: until iter = iter max 17: return K ; Compared to the non-probabilistic NSVD, the E-step of NPCA has one additional step (13) to compute an N  X  N posterior covariance matrix for every i = 1 . . . , M . It turns out the overhead is almost avoidable. Let B be an N  X  N matrix whose elements are initialized as zeros, then we use it to collect the local information by the M-step (14) can be realized by Therefore there is no need to explicitly perform (13) to com-pute an N  X  N posterior covariance matrix for every i , which saves N Netflix problem, this reduces over 4 , 000 hours for one E-step. The pseudo code is given in Algorithm 2. We note that, since the optimization is non-convex, we initialize K by an item-item correlation matrix based on the incomplete Y (see Section 6.3).

Comparing Algorithm 2 with Algorithm 1, we find the re-maining computation overhead of the probabilistic approach lies in the steps 10 and 11 that collect local uncertainty infor-mation for preparing to update the noise variance  X  , which costs additional each E-step. In order to further speed-up the algorithm, we propose to simplify NPCA. The essential modeling as-sumption of (9) is that Y is a collection of rows y i inde-pendently and identically following a Gaussian distribution N (0 , K +  X I ). Then our idea is, rather than modeling the noise and signal separately, we merge them by K  X  K +  X I Algorithm 2 NPCA Require: Y , iter max ; 1: allocate K  X  R N  X  N , B  X  R N  X  N ; 2: initialize iter = 0, K ; 3: repeat 4: iter  X  iter + 1; 5: reset B , Er = 0, i = 0; 6: repeat 7: i  X  i + 1; 8: G = ( K O i +  X I )  X  1 ; 9: t = G  X  y O i ; 10: Er  X  Er + 11: Er  X  Er + 13: until i = M ; 14: K  X  K + 1 M KBK ; 15:  X   X  1 | 16: until iter = iter max 17: return K ,  X  ; and directly deal with the covariance of the noisy observa-tions y i , i = 1 , . . . , M . The obtained model is conceptually simple and  X  ( X i,j ) is the distribution with a probability one if Y X i,j and zero otherwise.

In fact, the simplified model (19) is equivalent to the orig-inal model (9) that separately handles signals and Gaussian noises. This is because that in (9) there is no prior either on the covariance K or on the noise variance  X  , and thus both models can be seen as an maximum-likelihood estima-tor (MLE) of the free-form covariance of the noisy obser-vations. Though it should be straightforward to assign pri-ors or regularization on K and  X  , we present only the non-regularized approach here for keeping the simplicity. Due to the fact that M is typically very large, the MLE estima-tor works well in our experiments. For model (9) the EM algorithm is the following: The implementation is summarized by Algorithm 3. The computation at the E-step has only minor differences from that of the non-probabilistic Algorithm 1. Compared with Algorithm 2, in one E-step, the new version saves about 11.7 hours on the Netflix data, and ends up with only 4 hours. The memory cost is also the same as Algorithm 1, which is N ( N  X  1) for storing K and B . The prediction is made by computing the expectation E( Y i,j ) = K j, O i ( K O i  X 
O i )+  X  j . Due to its remarkable simplicity, we mainly apply the faster version of NPCA in the experiments.
 Algorithm 3 Fast NPCA Require: Y , iter max ; 1: allocate K  X  R N  X  N , B  X  R N  X  N , b  X  R N ,  X   X  R N 2: initialize iter = 0, K ; 3: repeat 4: iter  X  iter + 1; 5: reset B , b , i = 0; 6: repeat 7: i  X  i + 1; 9: t = G ( y O i  X   X  O i ); 12: until i = M ; 13: K  X  K + 1 M KBK ; 14:  X   X   X  + 1 M b ; 15: until iter = iter max 16: return K ,  X  ;
Low-rank matrix factorization algorithms for collabora-tive filtering can be roughly grouped into non-probabilistic [4, 1, 5, 13] and probabilistic approaches [6, 10, 18]. In the re-cent Netflix competition, low-rank matrix factorization was extremely popular among the top participants, e.g. [5, 13, 10, 15, 18, 2].

Non-probabilistic nonparametric matrix factorization un-der the trace norm regularization was introduced in [12]. The optimization was cast as a canonical semidefinite pro-gramming (SDP) problem, which has a poor efficiency and scalability. In [12], the algorithm was applied to handle only a small 100  X  100 matrix. To improve its efficiency and scal-ability, a faster approximation was subsequently proposed in [8], which was in fact a parametric (non-convex) low-rank method, similar to the formulation of SVD (2). Recently, theoretical analysis of matrix completion under trace norm regularization is becoming an active research area, e.g., [3].
The probabilistic NPCA is a nonparametric generaliza-tion of the well-known parametric pPCA [14, 9]. Although the dimensionality of latent factors goes to potentially in-finity, the resultant nonparametric model, (9) or (19), is conceptually simple  X  it is the maximum likelihood estima-tor of a (very!) high-dimensional Gaussian from incomplete data, a topic related to the recent development in nonpara-metric Gaussian process regression [7]. Though this model is more powerful in explaining data, it also demands much more computation. As a consequence, despite the fact that parametric matrix factorization has made a huge success in building modern recommender systems, probabilistic non-parametric models like NPCA have never been paid enough attention in collaborative filtering research and practice.
We note that a further development of NPCA is intro-duced in [16], where the nonparametric random effects model (NREM) can be seen as a Bayesian version of NPCA that ad-ditionally utilizes row-specific and column-specific attributes in a multi-task learning setting. We carry out a series of experiments on the entire Each-Movie data set, which includes 74424 users X  2811718 distinct Table 1: RMSE of matrix factorization methods in the EachMovie experiment Table 2: Runtime performances of matrix factoriza-tion methods in the EachMovie experiment very sparse matrix since 97 . 17% of the elements are miss-ing.

In our first setting, we randomly select 80% of each user X  X  ratings for training and use the remaining 20% as test cases. The random selection is repeated 20 times independently. We focus on two groups of algorithms: 1. SVDs : Low-rank SVDs with 20 and 40 dimensions, 2. PCAs : Low-rank pPCAs with 20 and 40 dimensions,
The mean and standard error of RMSE (root-mean-square error) averaged over all the 20 trials are listed in Table 1. We can see that for either of the two categories, SVD and pPCA, the nonparametric version consistently outperforms the low-rank counterparts. Second, the probabilistic PCAs consis-tently outperforms those SVD methods. Over all, NPCA is the winner among all the methods.

We implement all the algorithms using C++. The aver-age run time results are reported by Table 2. SVD using conjugate gradients converges very slowly. As analyzed be-fore, NSVD and NPCA have almost the same computational cost for a single EM iteration, but NSVD usually stops af-ter 5 iterations due to the detected overfitting, while NPCA often goes through 30 iterations. We observe that the per-formance of NPCA on the holdout set saturates after some Table 3: Comparison with state-of-the-art methods in the literature, in terms of NMAE Method Weak NMAE Strong NMAE URP [8] 0.4422  X  0.0008 0.4557  X  0.0008 Attitude [8] 0.4520  X  0.0016 0.4550  X  0.0023 FMMMF [8] 0.4397  X  0.0016 0.4341  X  0.0023 MMMF-A100 [4] 0.4287  X  0.0020 0.4300  X  0.0031 NSVD 0.4548  X  0.0005 0.4517  X  0.0008 NPCA 0.4257  X  0.0005 0.4255  X  0.0011 Table 4: Comparison with state-of-the-art methods in the literature in terms of runtime speed iterations, and deteriorates very little as more iterations per-formed (see Figure 2), which indicates overfitting is perhaps not a big issue for NPCA. We conjecture that NPCA is im-mune to overfitting, as long as M N , which is often the case in applications.

In our second setting, in order to directly compare with several state-of-the-art results on EachMovie, we follow [8, 4], where 36,656 users with no less than 20 ratings are used; for each user, one rating is randomly withdrawn for testing and the remaining ratings are used for training; in each ran-dom trial, 30,000 random users are selected for training, and a performance of  X  X eak generalization X  is evaluated on the training users X  withdrawn ratings; then the learned model is used to make predictions on the rest 6,656 users, and the so-called  X  X trong generalization X  is evaluated. For more details please refer to [8]. We repeat this experiment randomly for 20 times in testing each method. Following the performance metric by [8, 4], we report the average NMAE (i.e., mean ab-solute error normalized by 1.944) in Table 3, where FMMMF is the fast max-margin matrix factorization, and MMMF-A100 is an ensemble of 100 MMMF models with different random seeds for initialization. The runtime speed is re-ported in Table 4, which shows both nonparametric models work very well in terms of speed. We note that FMMMF and MMMF-A100 both aim to directly minimize the NMAE loss, while NPCA works on square loss. This perhaps ex-plains why NSVD does not give low NMAE. On the other hand, NPCA applies square loss too, but it results in the lowest NMAE in Table 3.
 Table 5: RMSE of various matrix factorization methods on the Netflix test set Table 6: Influence of Initialization of K on NPCA
Initialization Option RMSE 1. Identity Matrix 1.0873  X  0.0003 2. Emp. Covariance + Identity Mat. 1.0817  X  0.0003 3. Emp. Cov. + Identity Mat. + Bias 1.0795  X  0.0003
The Netflix data are collected representing the distribu-tion of ratings Netflix.com obtained during a period of 1998-2005. The released training data consists of 100,480,507 ratings from 480,189 users on 17,770 movies. In addition, Netflix also provides a set of validation data with 1,408,395 ratings. Therefore there are 98 . 81% of elements are missing in the rating matrix. In order to evaluate the prediction ac-curacy, there is a test set containing 2,817,131 ratings whose values are withheld and unknown for all the participants. In order to assess the RMSE on the test set, participants need to submit their results to Netflix. Since results are evaluated on exactly the same test data, it offers an excellent platform to compare different algorithms.

We run the Algorithms 1 and 3 on the training data plus a random set of 95% of the validation data, the remaining 5% of the validation data are used for the stopping crite-rion. In the following Table 5 we report the results obtained by the two models, and quote some state-of-the-art results reported in the literature by matrix-factorization methods. We note that people reported superior results by combining heterogenous models of different nature, e.g. [2]. In this pa-per we only compare with those achieved by single models. In the table, the Baseline result is made by Netflix X  X  own algorithm. BPMF is Bayesian probabilistic matrix factor-ization using MCMC [10], which produces so far the state-of-the-art accuracy by low-rank methods. NPCA achieves an even better result than BPMF, improving the accuracy by 6 . 18% from the baseline. NSVD does not perform very well on this data set, we suspect more work should be done on fine-tuning the regularization parameter. However this contrasts the advantage of NPCA that has no parameter to tune. In terms of run-time efficiency, both algorithms use about 5 hours per iteration. The NPCA result is obtained by 30 iterations. In contrast, BPMF (with 300 dimensions) takes several hundreds of iterations, 200 minutes per itera-tion, to burn in, and then needs other hundreds of iterations to compute the average.
NPCA performs very well in all of our experiments. In this section we present a closer look at some empirical behaviors of this model.

First, as the optimization is non-convex, we would exam how initialization of K can influence the outcome of the model. We initialize K in the following way where I is the N  X  N identity matrix, [ a, b, c ] linear weights taking values from [0 , 1], and C emp  X  R N  X  N a rough empir-ical covariance calculated from the training data, Figure 1: RMSEs obtained from initializing K via various ways of combining empirical covariance and identity matrix. The bar shows the relationship be-tween gray level and RMSE. where  X  0 and  X  j are the standard deviations estimated, re-spectively, from ratings on all the items and those on item j , M j is the number of observed ratings for item j , and  X  is the average of ratings on item j . Furthermore, we let x i,j =  X  j if it is missing. Following the same setting of Ta-ble 1, the performances of the following three initialization options are reported in Table 6: (1) using identity matrix only; (2) combining empirical covariance and identity ma-trix; (3) combining empirical covariance, identity matrix, and bias, where in each case the configuration of [ a, b, c ] is determined based on the first of the totally 20 training/test partitions. Table 6 shows that different initialization options lead to slightly different results, but all are better than those of competitive methods reported in Table 1. In Figure 1 we visualize the sensitivity of RMSE under various a and b with fixed c = 0 . 5. The result is based on the first partition only. We find that the performance is almost insensitive to c (the evidence is not provided here due to space limitation). Based on this figure we choose a = 0 . 3, b = 0 . 5, and c = 0 . 5 in the initialization option 3 in Table 6, which also leads to the result in Table 1.

Since all the above results are obtained by limiting the maximum number of EM iterations to be 30, one may ask whether the superior performance is attributed to an im-plicit regularization imposed by early stopping . In Figure 2 we demonstrate that, as the iterations go further, very little drop of accuracy is observed under various initialization con-ditions. Interestingly, it seems the major effect of a better initialization is the faster convergence.

Another interesting and important aspect of NPCA is its ability to output not only the mean but also the uncertainty of predictions. It follows the E-step that the predictive stan-dard deviation is calculated by We collect all the predictions whose uncertainty calculated by the above falls in to [ s  X  0 . 05 , s + 0 . 05], and measure the standard deviation of their prediction residuals. Since in Figure 3, which indicates that NPCA can give excellent assessment on the uncertainty of predictions. We believe this Figure 2: The convergence of RMSEs on test set over EM iterations Figure 3: Standard deviations of prediction residu-als vs. standard deviations predicted by NPCA aspect should be further exploited in future development of recommender systems.
In this paper we proposed nonparametric matrix factor-ization models for solving large-scale collaborative filtering problems. We considered two examples, singular value de-composition (SVD) and probabilistic principal component analysis (pPCA). This is perhaps the first work showing that nonparametric models are in fact very practical on very large-scale data containing 100 millions of ratings. More-over, though probabilistic approaches are usually believed not as efficient as non-probabilistic methods, we managed to make NPCA as fast as its non-probabilistic counterpart. In terms of the predictive accuracy, we observed that non-parametric models often outperformed the low-rank meth-ods, and probabilistic models delivered more accurate results than non-probabilistic models. Furthermore, the computa-tion tricks can be used for speeding up not only nonparamet-ric models, but also large-dimensional matrix factorization on large-scale sparse matrices.
 We thank Dr. Dennis DeCoste for a fruitful discussion, and the reviewers for constructive comments. [1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. [2] R. M. Bell, Y. Koren, and C. Volinsky. The BellKor [3] E. J. Cand`es and T. Tao. The power of convex [4] D. DeCoste. Collaborative prediction using ensembles [5] M. Kurucz, A. A. Benczur, and K. Csalogany.
 [6] Y. J. Lim and Y. W. Teh. Variational Bayesian [7] C. E. Rasmussen and C. K. I. Williams. Gaussian [8] J. D. M. Rennie and N. Srebro. Fast maximum margin [9] S. Roweis and Z. Ghahramani. A unifying review of [10] R. Salakhutdinov and A. Mnih. Bayesian probabilistic [11] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels . [12] N. Srebro, J. D. M. Rennie, and T. S. Jaakola. [13] G. Takacs, I. Pilaszy, B. Nemeth, and D. Tikk. On the [14] M. E. Tipping and C. M. Bishop. Probabilistic [15] M. Wu. Collaborative filtering via ensembles of matrix [16] K. Yu, J. Lafferty, S. Zhu, and Y. Gong. Large-scale [17] K. Yu and V. Tresp. Learning to learn and [18] Y. Zhang and J. Koren. Efficient Bayesian hierarchical
