 In the era of information explosion, people need new information to update their knowl-edge whilst information on Web is updating e xtremely fast. Multi-document summa-rization has been proposed to address such dilemma by producing a summary delivering the majority of information content from a document corpus, and the short summary is necessarily helpful to facilitate users to quickly understand the large number of docu-ments. Automated multi-document summarization has drawn much attention in recent years. In the communities of information ret rieval and natural language processing, a series of conferences on automatic text su mmarization have advanced the summariza-tion techniques and produced a couple of experimental online systems.

Graph-based ranking algorithms have been recently exploited for summarization by making use of sentence-to-sentence relationships and played an important role with the exponential document growth on the Web. In general, traditional graph summarization utilizes plain linkage among se ntences without considering higher-level information be-yond the sentence-level information, which is insufficient. Given a document set with linkage information to summarize, different sentences belong to different documents and clusters, either clustered by visible linkage (e.g., anchor texts) or invisible linkage (e.g., semantic cohesion), which enables a hi erarchical text structure. It is challenging and interesting to investigate the impacts a nd weights of source documents/clusters: different documents and clusters usually have different importance for users to under-stand the document set. Sentence from important documents/clusters are deemed more salient than the trivial ones. In brief, simultaneous consideration of three-layer hierar-chical linkage has not been investigated under a unified framework.

In order to address above insufficiency, we aim to model these three levels of hierarchical linkage, i.e., sentence-to-sen tence, sentence-to-document and document-to-cluster relationships, into traditional g raph-based summarization, and we name this approach as Hierarchical Graph Summarization (HGS). We propose a hierarchical lan-guage model to measure the sentence relationships for the ranking process in HGS. Document/cluster-level information through visible and invisible linkage is used for smoothing: neighboring text information is proved to be useful [11]. We will first in-vestigate the presence of visible and invisible linkage for clustering.
 Visible Linkage. A web document is connected to other web documents by explicit links via anchor texts, which are denoted as visible linkage.
 Invisible Linkage. A web document is connected to other web documents through implicit semantic coherence, denoted as invisible linkage.

The contributions of this paper are as follows:  X  The 1st contribution is to utilize the instinctively explicit linkage among web documents, which is a natural understanding of enormous web data organization. We distinguish such visible li nkage from invisible linkage by semantic cohesion and utilize both information into clustering.  X  The 2nd contribution is to incorporate a three-level hierarchical linkage structure into a unified language smoothing model, which is used to measure sentence relation-ships by utilizing both document-level and cluster-level information simultaneously.
We start by reviewing previous work in Section 2. In Section 3 we describe the basic graph summarization and describe our proposed HGS in Section 4. We conduct empiri-cal evaluations in Section 5, including performance comparisons and result discussion. Finally we draw conclusions in Section 6. Multi-document summarization (MDS) has drawn much attention in recent years. In general, MDS can either be extractive or abstractive. The former assigns salient scores to semantic units (e.g. sentences, paragraphs) of documents indicating the importance and then extracts top ranked ones, while the latter demands information fusion(e.g. sentence compression and reformulation). Here we focus on extractive summarization.
To date, various extraction-based methods have been proposed for generic multi-document summarization. MEAD [3] is an implementation of the centroid-based method that scores sentences based on featur es such as cluster centroids, position, and TF.IDF, etc. NeATS [6] adds new features such as topic signature and term cluster-ing to select important content. Themes (o r topics, clusters) in documents have been discovered and used for sentence selection [10,14,13].

Most recently, the graph-based ranking methods have been proposed to rank sen-tences/passages based on  X  X otes X  or  X  X  ecommendations X  between each other. TextRank [9] and LexPageRank [2] use algorithms similar to PageRank and HITS to compute sentence importance. Cluster information such as document-level information has been incorporated in the graph model to b etter evaluate sentences [12].

Generally, summarization considers content characteristics such as coverage, diversity [1,17,5], and all these characteris tics require a calculation of sentence linkage measurement. To the best of our knowledge, currently, neither the instinctively visible linkage of anchor texts from web document organizations is utilized for summarization, nor the three-layer hierarchical linkage has been investigated simultaneously in a uni-fied language model to measure sentence relationships. HGS approach can naturally and simultaneously take into account these two advantages in graph-based summarization. The basic graph summarization is essentially a way of deciding the importance of a vertex within a linkage graph based on global information recursively drawn from the entire graph, using the Markov Random Walk Model (MRW). The basic idea is that of  X  X oting X  or  X  X ecommendation X  between the v ertices, where each vertex is a sentence. A link between two vertices is considered as a vote cast from one vertex to the other vertex. The score associated with a vertex is determined by the votes that are cast for it, and the score of the vertices casting these votes.

Formally, given a document set D ,let G =( V,E ) be a graph to reflect the relation-ships between sentences in the document set, as shown in Figure 1 (Part A). V is the set of vertices and each vertex s i in V is a sentence in the document set. E is the set of edges, which is a subset of V  X  V . Each edge e ij in E is associated with an affinity weight f ( s i  X  s j ) between sentences s i and s j ( i = j ). Sentence s is generated from the language model  X  s . The affinity weight is measured by Kullback-Leibler divergence where D KL ( s j || s i ) is: W is the set of words in our vocabulary and w denotes a word. The language model of  X  s will be discussed in details later. If  X  s i and  X  s j are very close, the KL-divergence would be small and f ( s i  X  s j ) would be high, which intuitively makes sense.
Given f ( s i  X  s j ) , the transition probability from s i to s j is then defined by normal-izing the corresponding affinity weight as follows. Note that p ( s i  X  s j ) is asymmetric and it measures the affinity from s i to s j .We let f ( s i  X  s i )=0 to avoid self transition. We use the row-normalized matrix M = [ M ij ] | V | X | V | where M ij = p ( s i  X  s j ) to describe G with each entry corresponding to the transition probability and all zero elements are replaced by a smoothing factor empirically set to 1 / | V | .

Based on the matrix M , the saliency score  X  ( s i ) for sentence s i can be deduced from those of all other sentences linked with it an d it can be formulated in a recursive form as in the PageRank algorithm as follows: For implementation, the initial scores of all sentences are set to 1 and the iteration al-gorithm in Equation (3) is adopted to compute the new scores of the sentences. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any sentences falls below a given threshold (0.0001 in this study).  X  is the damping factor usually set to 0.85, as in the PageRank algorithm. We then apply the Maximum Marginal Relevance (MMR) mech-anism for redundancy removal, similar to the method used in [11].

We see that according to the KL-divergence scoring method, our main tasks are to estimate  X  s .Since s can be regarded as a short document, we can use any standard method to estimate  X  s . Here, we use Dirichlet prior smoothing [18] to estimate  X  s as follows: model used as smoothing factor. Generally p ( w | B ) is estimated by the whole document
However, note that as the length of a sentence is very short, smoothing is critical for addressing the term sparseness problem for sentences. The globalized smoothing from the whole corpus is coarse-grained. Therefore, we move on to estimate the fine-grained p ( w |  X  s ) from multiple-layers by the hierarchical graph summarization. 4.1 Overview In the basic graph summarization, all sentences are indistinguishable, i.e., the sentences are treated uniformly. As we mentioned in Section 1, there may be many factors that can have impact on the importance analysis of the sentences. This study aims to examine the impact of hierarchical linkage on graph s ummarization, by incorporating sentence-to-document relationship, as well as visible and invisible document clustering.
Besides 1) the basic pair-wise sentence-to-sentence relationship, the hierarchical graph includes 2) sentence-to-document relationship, 3) document-to-cluster relation-ship from visible linkage and 4) document-to-cluster relationship from invisible link-age by semantic clustering. We number these four types of linkage correspondingly in Fig. 1 . As can be seen, the lowest layer is just the traditional link graph between sentences that has been well studied in previous work. The upper layer represents the documents. The dashed lines between these two layers indicate the conditional influ-ence between the sentences and the documen ts: a link is established when the sentence is from the document. Documents are connected due to visible lines by anchor text arrows and are also grouped by invisible semantic clusters. 4.2 Incorporating Hierarchical Linkage Sentence-to-Document Links. To incorporate the document-level information and the sentence-to-document relationship, the document-based graph model is proposed based on the two-layer link graph including both sentences and documents in Fig. 1.(Part B): the language model of sentence s is smoothed by the source document.
 Visible Document-to-Cluster Links. Web documents are linked to each other through anchor texts and we keep such structural information. We start  X  X alking X  from a par-ticular web document to all connected web documents until all linked documents are visited. These documents are clustered together as visible clusters, and the document within the visible cluster forms a visible document-to-cluster relationship. Invisible Document-to-Cluster Links. Web documents can be clustered according to their semantic coherence, and the distance is calculated by the standard cosine similarity measurement. We use the popular clustering algorithms of K-means to produce the invisible semantic cluster. Given a document s et, it is hard to predict the actual cluster number, and thus we empirically set the number k of expected clusters as k = | D | , where | D | is the number of documents.
 Linkage Integration. After we introduce three types of hierarchical links, the estima-tion of the background language model  X  B should be based on the source document and source cluster where the sentence comes from, according to [8], the background model can be now written as: We take Equation (5) into Equation (4) and obtain the final representation:  X  c can be interpreted as our confidence on the prior of how cluster information weighs. Thus setting  X  c = | d | means that we put equal weights on the document-level and the cluster-level information.  X  c =0 yields no consideration of cluster-level information and  X  =0 yields simple consideration of plain sentence relationships.

After simple calculation, we notice that the sum of all coefficients in Equation (6) equals to 1, and hence we change Equation (6) into a more concise format of  X  ,  X  ,  X  all belong to [0,1] and  X  +  X  +  X  =1. The cluster representation of p ( w | C ) can controlled by  X  : Special Cases: (1)  X  =0 and  X  =0: only plain relationship betw een two sentences are considered; (2)  X  = 0,  X  =0: plain linkage and document-to-sentence relationship included; (3)  X  = 0,  X  =0 means no invisible clustering impact from visible linkage; (4)  X  = 0,  X  =1 means no visible clustering impact from invisible linkage. 4.3 Estimation of Document/Cluster Importance Documents and clusters are not equally important. Our assumption is that the sentences in an important document or cluster should be ranked higher and more likely to be chosen into the summary. The importance of documents (or clusters) is measured the relevance to the whole corpus. We examine such impact by incorporating the document importance and cluster importance into calculation of sentence linkage and ranking.
The function  X  ( d ) aims to evaluate the importance of document d in the document set D . The following two methods are developed to evaluate the document importance.  X  kl : It uses the transformed KL-Divergence value between the document d and the whole document set D as the importance score of the document:  X  pr : It constructs a weighted graph between documents and uses the PageRank algo-rithm to compute the rank scores of the documents as the importance scores of the documents. The link structure among documents is established by the inherent visible linkage. The equation for iterative com putation is the same with Equation (3).
The function  X  ( C ) evaluates the importance of cluster C (both visible and invisible) in the document set D . Similarly we have two methods to evaluate cluster weights.  X  kl : It uses the transformed KL-Dive rgence value between the cluster C and the whole document set D as the importance score of the cluster:  X  pr : We add the PageRank scores of all the documents within the cluster C , i.e., By incorporating document and cluster importance, Equation (7) can be rewritten as for all sentences and applied into Equation (1), (2), (3) to calculate the hierarchical sentence relationships and to rank sentences within the multiple-layer graph. 5.1 Dataset We use the data in [16] to test HGS on the real world datasets, which amounts to 5197 documents from various major news sites (such as BBC, CNN and Xi nhua News, etc.). Our data includes 4 subjects, and each belongs to a different category of Rule of Inter-pretation (ROI) [4]. Reference su mmaries are created by editors [16].
 5.2 Evaluation Metrics The ROUGE measure is widely used for evaluation [7]: the DUC contests usually offi-cially employ ROUGE for automatic summarization evaluation. In ROUGE evaluation, the summarization quality is measured by c ounting the number of overlapping units, such as N-gram, word sequences, and word pairs between the candidate summaries CS and the reference summaries RS . There are several kinds of ROUGE metrics, of which the most important one is ROUGE-N with 3 sub-metrics: precision, recall and F-score. S denotes a summary. N in these metrics stands for the length of N-gram and N-gram  X  RS denotes the N-grams in reference summary while N-gram  X  CS denotes the N-grams in the candidate summary. Count match (N-gram) is the maximum number of N-gram in the candidate summary and in the set of reference summaries. Count (N-gram) is the number of N-grams in reference s ummaries or candidate summaries.

According to [7], among all sub-metrics, unigram-based ROUGE (ROUGE-1) has been shown to agree with human judgment most and bigram-based ROUGE (ROUGE-2) fits summarization well. We report three ROUGE F-measure scores: ROUGE-1, ROUGE-2, and ROUGE-W, where ROUGE-W is based on the weighted longest com-mon subsequence. The weight W is set to be 1.2 in our experiments by ROUGE package (version 1.55). The higher the ROUGE scores, the similar the two summaries are. 5.3 Algorithms for Comparison Pre-processing. Given a collection of documents, we first decompose them into sen-tences. Then the stop-words are removed and words stemming is performed. After these steps, we implement the following widely used summarization algorithms as baseline systems. They are designed for traditional summarization without hierarchical linkage. For fairness we conduct the same preprocessing for all algorithms.
 Random: The method selects sentences random ly for each document collection.
Centroid: The method applies MEAD algorithm [ 3] to extract sentences according to the following parameters: centroid value, positional value, and first-sentence overlap.
GMDS: The plain graph MDS proposed by [11] first constructs a sentence connec-tivity graph based on cosine similarity an d then selects important sentences based on the concept of eigenvector centrality.

PGMDS: Wan et al. present a two-layer pair-wise graph summarization methods in [12], utilizing sentence-to-s entence and sentence-to-doc ument linkage without a con-sideration of simultaneous document-to-cluster links.

HGS: HGS is an algorithm with three-layer hierarchical linkage information and at the same time, both visible and invisible document clustering are performed.
RefSum: As we have used separate reference summaries from human evaluators, we not only provide ROUGE evaluations of the competing systems but also of the reference summaries against each other, which pro vides a good indicator of not only the upper bound ROUGE score that any system could achieve.
 5.4 Overall Performance Comparison We u s e a cross validation manner among 4 datasets, i.e., to train parameters on one subject set and to examine the performance on the others. After 4 training-testing pro-cesses, we take the average F-score performance in terms of ROUGE-1, ROUGE-2 and ROUGE-W on all sets. The details are listed in Tables 2  X  5.

From the results in Table 2 to Table 5, we have following observations:  X  Generally Random has the worst performance.  X  The results of Centroid are better tha n those of Random, mainly because the Cen-troid method takes into account positional value and first-sentence overlap, which facil-itate main aspects summarization. However, the flat clustering-based summarization is proved to be less useful [15].  X  The GMDS system outperforms centroid-based summarization methods. This is due to the fact that PageRank-based framework ranks the sentence using eigenvector centrality which implicitly account s for information subsump tion among all sentences.  X  In general, the PGMDS algorithm outperforms GMDS system. It indicates that the two-layer hierarchical summarization is more useful than plain graph summarization and richer linkage structure indeed facilitates graph summarization.  X  HGS under our proposed framework outperforms baselines, indicating that the overall properties we use for three layers of hi erarchical linkage are beneficial for sum-marization tasks.
Having proved the effectiveness of our proposed methods, we carry the next move to identify how different layers of information take effects to enhance the quality of a summary in parameter tuning of  X  ,  X  ,  X  and  X  . 5.5 Parameter Tuning Keeping other parameters fixed, we vary one parameter at a time to examine the changes of its performance from all 4 datasets. The first group of key parameters in our frame-work is  X  ,  X  and  X  where  X  +  X  +  X  =1. Every time we tune a parameter at a step of 0.1 and vary the other two for the best performance to achieve. Experimental results indi-cate the sentence-level relationship have stable but little impact on the summarization performance (illustrated in Fig. 2). The positive influence of documents and clusters are confirmed in Fig. 3 and Fig. 4 when  X  = 0and  X  = 0. Compared with document-level information, cluster-level informatio n has a relatively weaker influence. Excessive use of higher level information impairs performance. Over smoothing from source texts might make the language models divergent from the original ones. We set  X  =0.3,  X  =0.5,  X  =0.2 in our experiments.

Another key parameter in our framework is  X  in Equation (8) to measure the tradeoff between visible and invisible cluster information. We gradually change  X  from 0 to 1 at the step of 0.1 to examine the effect in Fig. 5. The combination of visible and invisible cluster outperforms the performance in isolation (  X  =1 or 0). It is understandable that these two clustering metrics denote separate document organization methods and intro-duce different smoothing backgrounds. In general, a larger weight from visible cluster is preferable (  X  =0.3).

Finally we examine the impact of document and cluster weights and the results are summarized in Table 6. From Table 6, we conclude that to distinguish the weight of documents and clusters is useful to measure s entence relationships because the usage of both weights brings prominent improve compared with  X  ( d ) =OFF and  X  ( C ) =OFF. web organization structure is helpful to find the centric documents within the corpus. The usage of  X  kl ( d ) has been proved in [12]. We also try different combinations of  X  ( C ) for visible and invisible clusters.  X  kl means both clusters are weighed by KL-Divergence, and  X  pr means both clusters are weighed by PageRank score.  X  kl + pr means using KL-Divergence for visible clusters and using PageRank for invisible clusters, while  X  pr + kl means using PageRank score for visible clusters and using KL-Divergence for invisible clusters. We have an interesting finding that for visible clusters organized by anchor texts, the weights measured by PageRank seems to make more sense than using semantic coherence, and vice versa. T herefore, in general, the performance of  X  pr + kl is the most plausible weighting strategy.
 In this paper we propose a Hierarchical Graph Summarization method, incorporat-graph summarization models. We utilize sentence-to-sentence relationship, sentence-to-document relationship and document-to-cluster relationship. We also investigate the web document structural information by explorations of visible and invisible doc-(  X  =0.3). Further more, we distinguish document/cluster by measuring their correspond-ing weights, calculating KL-Dive rgence and PageRank scores.

Abundant experiments are conducted on 4 real datasets, comparing 5 rival algo-rithms. Experimental results demonstrate the effectiveness of our proposed HGS. The benefits of visible and invisible clustering are also confirmed. Documents and clusters should be distinguished by their significance. We also find that the semantic coherence for invisible clustering has not shown as promising effects as visible clustering does. Acknowledgments. This work was partially supported by HGJ 2010 Grant 2011ZX01042-001-001 and NSFC with Grant No. 61073081. Xiaojun Wan was sup-ported by NSFC with Grant No.61170166, and Rui Yan was supported by the MediaTek Fellowship.

