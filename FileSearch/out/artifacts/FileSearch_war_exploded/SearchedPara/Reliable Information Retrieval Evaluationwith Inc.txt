 Information retrieval evaluation based on the pooling method is inherently biased against systems that did not contribute to the pool of judged documents. This may distort the results obtained about the relative quality of the systems evaluated and thus lead to incorrect conclusions about the performance of a particular ranking technique.
We examine the magnitude of this effect and explore how it can be countered by automatically building an unbiased set of judgements from the original, biased judgements ob-tained through pooling. We compare the performance of this method with other approaches to the problem of incom-plete judgements, such as bpref, and show that the proposed method leads to higher evaluation accuracy, especially if the set of manual judgements is rich in documents, but highly biased against some systems.
 H.3.4 [ Systems and Software ]: Performance evaluation (efficiency and effectiveness) Experimentation, Performance Information Retrieval, Evaluation, Incomplete Judgments
According to the Cranfield paradigm [7], evaluating the quality of the search results produced by a document re-trieval system requires the existence of a set of relevance judgements that define for a given document and a given search query whether the document is relevant to the query. Relevance judgements may be binary (i.e.,  X  X elevant X / X  X ot relevant X ) or graded (e.g.,  X  X xcellent X / X  X ood X / X  X oor X ). Be-cause all judgements are produced by human assessors, it Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. is not feasible to judge every document in the collection. Instead, several different retrieval systems are run on the given set of queries, and each system produces a ranked list of documents, ordered according to their predicted relevance to the query. By taking the top p documents from each rank-ing (usually 50  X  p  X  100), a set of to-be-judged documents is built and sent to the assessors for judging. This set of documentsisreferredtoasthe pool of judged documents (or simply pool ), and the technique is referred to as pooling .
Obviously, pooling can be used to perfectly evaluate the quality of the first p search results returned by each sys-tem that contributed to the pool. However, because it is so expensive to generate human relevance assessments, it is desirable to make them reusable. That is, we want to be able to accurately evaluate the quality of a retrieval system even if it did not contribute any documents to the pool.
Reusing relevance judgements can be difficult. For exam-ple, a new ranking algorithm might be developed that re-turns different documents than the systems that were used to build the pool of judged documents. In this case, it is not clear how to deal with the unjudged documents when evaluating the quality of the new algorithm. We say that the judgements are biased against the new system, because the system was not allowed to contribute anything to the pool of judged documents.

Research in the area of incomplete judgements usually fo-cuses on the case of unbiased judgements, where judgements are incomplete, but do not favor any particular system over another (see [1], [4], [15] for examples). In this paper, we address the problem of biased judgements and discuss how the bias can be removed from the judgements. Using an ex-isting set of relevance judgements, a classifier is trained and used to predict the relevance of documents returned by the retrieval system but not found in the pool of judged docu-ments. Systems are evaluated as usual, but the evaluation is based on the new, extended set of judgements instead of the original one.

Our experimental results show that the method can pro-duce highly reliable evaluation results, especially if the exist-ing set of relevance judgements is reasonably large. If used to build an unbiased set of judgements starting from a set of highly biased judgements, it produces more accurate evalu-ation results than bpref [4]. However, care has to be taken that the classifier is not used to predict the relevance of doc-uments that are beyond the depth of the pool. If classifying too many unjudged documents, the method may lead to a bias in the opposite direction.

If the set of judged documents does not match the set of documents returned by a retrie val system very well, then tra-ditional information retrieval evaluation measures, such as average precision (AP) and precision at k documents (P@ k may lead to grossly inaccurate evaluation results. Suppose that for a system S , on average, only 50% of the top 10 doc-uments returned have been judged. Then S  X  X  P@10 score can be at most 0.5, even though, on average, more than 50% of the top 10 documents might be relevant.

Although it has been argued that such a large bias against one particular system is highly unlikely (see, for instance, Voorhees [14] and Zobel [16]), researchers have started to develop novel evaluation measures, taking the incomplete nature of relevance judgements into account.
 One of the first such measures is proposed by Buckley and Voorhees [4]. Their bpref measure addresses the problem of incomplete judgements by completely ignoring search results for which no relevance information is available. The bpref score of a document ranking D ,relativetoaquery Q ,is bpref( D , Q )=1  X  where R is the set of known relevant documents and N is the set of the | R | most highly ranked known non-relevant doc-uments for the query Q . Note that bpref can use a judged document in the evaluation even if the document does not show up in the ranking. For known relevant documents not in the ranking, it assumes that they appear at rank  X  .For known non-relevant documents not in the ranking, it as-sumes that they appear at rank  X  X  X  1. Bpref now is one of the standard evaluation measures used in TREC 1 .
Buckley and Voorhees also discuss a variation of bpref, called bpref-10, in which the top | R | + 10 non-relevant doc-uments are taken into account when evaluating a ranking. Bpref-10 leads to more stable results than bpref if the num-ber of known relevant documents is very small.

Gr  X  onqvist [9] argues that bpref X  X  only taking the top | R | non-relevant documents into account is a weakness of the measure. He proposes RankEff, a measure similar to bpref, also only taking judged documents into account. The RankEff score of a document ranking D is RankEff( D , Q )=1  X  where J is the set of judged documents, R is the set of known relevant documents, and N is the set of known non-relevant documents ( R  X  N = J ).

Yilmaz and Aslam [15] propose another measure, inferred average precision (infAP), that overcomes the problem of incomplete judgements by estimating the current precision when it encounters an unjudged document in the ranking. Compared to bpref and RankEff, infAP has the advantage that it converges to the actual average precision value as the judgements become more and more complete.

One of the shortcomings of existing work on the effect of incomplete judgements is that it is limited to the case of unbiased incomplete judgements. Evaluation accuracy with incomplete judgements under a given measure is usu-ally evaluated by selecting a random subset of the judged
Text REtrieval Conference (http://trec.nist.gov/) documents and comparing the ranking produced according to the reduced set of judgements with the ranking produced according to the original judgements (cf. [1], [4], [15]). Such incomplete judgements do not favor any particular system. We extend the findings obtained for unbiased judgements and explicitly focus on the case where the set of judge-ments is highly biased against some systems, by removing its unique contributions from the pool.

The only published work we are aware of that tries to counter the effect of biased judgements on evaluation accu-racy is presented by Aslam et al. [2], who obtain reliable per-formance estimates by randomly sampling documents from the ranking produced by a retrieval system. Their method, however, does not generalize well to  X  X arly precision X  mea-sures, such as precision at k documents (for small k )and reciprocal rank.

Similar to the work presented here, Aslam and Yilmaz [3] discuss a method that can be used to infer the relevance of unjudged documents. In contrast to our work, however, they do not train a classifier on the available relevance judge-ments, but follow a relaxed integer programming approach, inferring document relevance based on the rankings pro-duced by several different systems and the average precision (or estimated average precisio n) values for those systems.
The main idea of our method is that, given a sufficiently large set of training examples in the form of judged docu-ments, it might be possible to train a document classifier that can be used to predict for any unjudged document whether the document is relevant for the given query or not. This idea is motivated by great success of automatic document classification systems, and also by the success of pseudo-relevant feedback techniques, suggesting that it might be possible to learn the concept of relevance even from a very small set of training examples.

In our experiments, we use two different text classifiers to determine whether an unjudged document is relevant or not. One is based on the Kullback-Leibler divergence (KLD) between a document and the language model defined by the judged relevant documents. The other is based on support vector machines. Given two probability distributions P and Q ,their Kullback-Leibler divergence (KLD) is: The KLD between two distributions P and Q is always non-negative. It is zero if and only if P = Q .Thus,itcanbe used to measure the distance between two distributions.
Suppose M R is the unigram language model of the rele-vant documents ( R ). Then an unjudged document D ,with language model M D , is considered relevant if where  X  is a threshold value, chosen in such a way that ex-actly | R | of the judged documents exceed the threshold (i.e., precision equals recall on the training data). The language models M D and M R are smoothed using the collection X  X 
Table 1: Summary of the TREC 2006 TB qrels. background model (language model defined by the concate-nation of all documents) via simple interpolation smoothing with  X  =0 . 2.

Choosing  X  so that precision and recall are equal is mo-tivated by the application at hand. If a set of random doc-uments contains r relevant and n non-relevant documents, then the classifier will classify r  X  recall / precision of these documents as relevant. Thus, for precision = recall ,wecan expect the number of documents classified as relevant to be about the same as the number of documents actually rele-vant, even though the intersection of the two sets might be very small. This, at least so we hope, will leave measures like P@ k close to their true value.
Support vector machines (SVMs) [8] are generalized linear classifiers, represented by a decision function of the form where used to classify any vector
Joachims [11] [12] has shown that SVMs can be success-fully used for text classification. To make the documents in thecollectionamenabletoSVM,wetransformthetextual representation of each document D into a 10 6 -dimensional TF-IDF feature vector in the following way: 1. By counting term occurrences in the entire text collec-2. All occurrences of a term T  X  X  are removed from D . 3. D  X  X  feature vector In our experiments, we use Joachims X  SVM light implementa-tion 2 , with default parameter values.
The data set used in our experiments is taken from the ad-hoc retrieval task of the TREC 2006 Terabyte track [5]. The ad-hoc retrieval task models the retrieval process associated with informational search queries on a document collection http://svmlight.joachims.org/ similar to the Web. The actual document collection used is GOV2, the result of a Web crawl of documents in the .gov domain, conducte d in early 2004 [6].
 The set of topics (i.e., search queries) used in the 2006 Terabyte track consists of 50 topics (TREC topic IDs 801 X  850). The track had 20 participating groups, submitting a total of 80 runs (a run is a ranked list of documents, the search results to a given query). Up to three runs from each group were selected to contribute to the pool. For every topic, the top 50 documents from each run were collected to form the pool of judged documents. In accordance with the standard TREC terminology, we use the term qrels to refer to this set of judged documents. All judgements in the qrels are interpreted to be binary  X  X elevant X / X  X on-relevant X  decisions, treating the TREC X  X   X  X ighly relevant X  judgements simply as  X  X elevant X . TREC 2006 Terabyte was special in that, in addition to the 61 automatic runs, 19 manual runs were submitted (a manual run is a run that involved some kind of human interaction; all other runs are automatic ).
An overview of the topics and qrels used in TREC Ter-abyte is given by Table 1. Note that, although technically only 11 manual and 27 automatic runs contributed to the pool, we found 42 runs (11 manual, 31 automatic) for which the top 50 documents were completely covered by the qrels for every topic. We thus decided to treat them all equally and to pretend that 42 instead of 38 runs actually did con-tribute to the qrels.

In all experiments involving the computation of evaluation measures, the computation was performed based on the top 10,000 documents retrieved by the respective system. This was a change introduced in TREC 2006 and is different from the evaluation based on the top 1,000 documents performed in earlier TRECs.

In most of our experiments, we build different sets of qrels and compare their behavior under different evaluation mea-sures. In this context, by original qrels we mean the pool of judged documents built by taking the top 50 documents from each run contributing to the pool. The term incom-plete qrels refers to a subset of the original qrels. Finally, the term completed qrels refers to a set of judgements that is built from a set of incomplete qrels by using a classifier to predict the relevance of unjudged documents. It covers the first 50 documents of each run submitted to the TREC 2006 Terabyte track. Thus, the co mpleted qrels reference the same set of documents as the original qrels. The relevance judgements, however, may be (and usually are) different.
The evaluation measures referred to in this paper, such as MAP (mean average precision) and MRR (mean recipro-cal rank), are used as defined by the implementation found in the trec eval evaluation toolkit 3 , the standard evalua-tion tool used at TREC. In addition to the basic measures supported by trec eval , we also use the following measures: http://trec.nist.gov/trec eval/
When we measure the similarity between two rankings, we use Kendall X  X   X  , as defined by Kendall [13]. Like Kendall, we do not pay special attention to the case where two systems are tied according to a given evaluation measure. Whenever a tie is encountered, it is assumed that the two entries are ranked in the correct order. We also look at how the raw score of a particular measure is affected as a result of chang-ing the set of qrels used to compute its value. We quantify the difference between the original value and the new value by the root mean square (RMS) error: where the o i and n i are the old and new values, respectively, for example the MAP values of all N runs in the pool.
The experiments presented in this section consist of three parts: First, we repeat the experiments with incomplete, un-biased judgements conducted by other researchers, using the TREC Terabyte data. Second, we extend those experiments by looking at biased judgements instead of unbiased ones. Third, we examine to what extent the effect of biased judge-ments can be countered by training a classifier and using it to predict the relevance of unjudged documents.
In our first series of experiments, we examine how de-creasing the set of qrels in a uniform, unbiased way affects the evaluation results. For this, we take the qrels for the 50 TREC topics from 2006 and generate random subsets com-prising 5% X 80% of the original qrels. We then evaluate all 42 runs in the original pool on these incomplete qrels and look at how reducing the qrels affects raw evaluation scores and how it affects the correlation between the systems X  ranking on the original qrels and on the incomplete qrels.
Figure 1(a) shows that reducing the size of the qrels decreases the value of all measures, except for bpref and RankEff. So far, this is consistent with earlier findings [4] [1]. What might be surprising, however, is that bpref, contrary to earlier finding, does not exhibit a dramatic increase when the qrels are reduced. The reason for this is that we use the actual bpref measure and not the adjusted bpref-10. By using bpref-10 in their experiments, Buckley and Voorhees [4] drastically increase the relative number of non-relevant documents used in the evaluation if the num-ber of known relevant documents is small. The result is a higher bpref-10 score for most runs. This is consistent with the fact that, in our experiments, the average RankEff score is greater than the average bpref score; RankEff takes more judged non-relevant documents into account than bpref. Finally, the fact that the RankEff score remains essentially constant is consistent with the results reported by Ahlgren and Gr  X  onqvist [1].

Figure 1(b) shows the correlation between the ranking produced from the original qrels and that produced from the incomplete qrels, for the same measures and the same set of topics as before. Here, the results are completely in line with earlier findings: Average precision (AP), P@ k ,and nDCG@ k lead to poor correlation; bpref achieves a higher correlation than those three; RankEff achieves a slightly higher correlation than bpref. The curve for bpref-10, not shownintheFigure,wouldmostlybebetweenthecurves for bpref and RankEff.
Experiments with unbiased incomplete qrels, like the ones presented above, gave the initial motivation to replace the traditional average precision measure by new measures, such as bpref. However, unbiased incomplete qrels only cover one aspect of the evaluation process. What is equally, or even more, important when aiming for a reusable set of relevance judgements is how well an evaluation measure can deal with biased judgements that were created by only taking docu-ments from some runs into account, while completely ignor-ing others. The reason why this is so important is because it reflects the everyday life of many researchers in the field, who use existing data and relevance judgements to evaluate their new retrieval methods. If an evaluation measure fails to generalize to runs that did not contribute to the pool, then these people may obtain highly misleading results about the quality of their new methods.
 Leave-One-Out Experiments We selected the 42 runs that contributed documents to the TREC Terabyte 2006 qrels and simulated how removing all RMS Error 0.0130 0.0207 0.0243 0.0223 0.0105 0.0346 0.0258 0.0143 runs submitted by the same group from the pool would affect the scores and rankings achieved by runs from that group. More specifically, we proceeded as follows: 1. Pick a group G . 2. For each topic T and each judged document D in the This procedure was repeated 20 times, once for each group participating in TREC TB 2006. The effect that the re-sulting biased qrels have on various evaluation measures is showninTable2.

On average, removing the unique contributions by a group from the qrels decreases the number of judged documents by 22 per topic. Thus, not very surprisingly, the effect on early precision measures like P@20 and nDCG@20 is quite substantial. According to the table, a run from the discrim-inated group on average loses 2.1 positions in the ranking of all 42 systems. In extreme cases, however, the loss can be far more extreme: 12 positions for P@20, and 14 positions for nDCG@20.

Average precision is a little less sensitive to the biased judgements than the early precision measures. The rank of a left-out run, according to average precision (AP), changes by about 1.5 positions. Interestingly, bpref does not appear more stable than AP. On average, a run submitted by the left-out group moves by 2 positions in the ranking. In 90.5% of all cases, the bpref score difference between the original qrels and the incomplete qrels, for the same run, is statis-tically significant according to a paired t -test ( p&lt; Moreover, AP and bpref affect the rank of a left-out run in opposite directions: While, according to AP, the rank of the run is usually lower with the incomplete qrels than with the original qrels (by up to 10 positions), according to bpref it is higher (by up to 14 positions).

This is an important result, because it shows that, for bi-ased judgements, bpref is no more reliable than AP. Where AP underestimates the performance of a system, bpref over-estimates it. Both phenomena are potentially dangerous and should not be taken lightly. While using AP to evaluate a run outside the pool may lead a researcher to the incorrect conclusion that a newly developed technique does not work # Judged 31,984 16,157 23,099 # Relevant 5,893 4,495 4,373 % Relevant 18.4% 27.8% 18.9% Table 4: Basic characteristics of original and biased qrels for TREC topics 801 X 850 (TREC TB 2006). very well, using bpref may lead to the equally incorrect con-clusion that it works really well when in fact it does not.
RankEff, in contrast, designed for unbiased incomplete qrels, just like bpref, behaves remarkably well. On average, the position of a discriminated run changes by only 0.857 positions X 4placesintheworstcase.

The P@20(j), defined in Section 4 for exactly this purpose, to be used in the presence of incomplete judgements, turns out to be a very unstable measure. Where P@20 under-estimates the performance of a run, P@20(j) overestimates it  X  grossly, by up to 22 positions in the ranking of the sys-tems. Why does P@20(j) give such a poor approximation of the original P@20 score? The answer lies in the distribution of relevant and non-relevant documents among the unique contributions by a given group. After removing a group X  X  unique contributions from the qrels, a run by that group on average has 3.4 unjudged documents among its top 20. Of these 3.4 documents, however, only 0.3 are relevant accord-ing to the original qrels. Thus, a unique contribution is far less likely to be relevant than what could be expected from a run X  X  P@20 score (which is usually far greater than 10%). Ignoring the unjudged documents in a system X  X  ranking im-plicitly assumes that they exhibit the same proportion of relevant and non-relevant documents as the judged docu-ments  X  an assumption that is simply wrong.
 Automatic Runs vs. Manual Runs By looking at the evaluation results more carefully, we found that, although all runs by a given group are noticeably af-fected by removing that group X  X  contributions from the qrels, the manual runs seem to be more sensitive to this than the automatic runs. This inspired us to conduct another exper-iment; instead of removing the unique contributions by a particular group from the qrels, we now removed all docu-ments that are only referenced by some of the manual runs in the pool, but not by any of the automatic runs. Run name Type Original qrels Automatic-only qrels Completed qrels (SVM) stay reasonably close to their original values.

Removing the manual runs from the pool greatly affects the size of the qrels. As can be seen from Table 4, the num-ber of judged documents is reduced by almost 28% (from 31,984 to 23,099). The number of relevant documents de-creases by 26% (from 5,893 to 4,373). This means that man-ual runs tend to retrieve documents that are different from the ones retrieved by the automatic runs. Moreover, man-ual runs are substantially better than automatic runs at finding relevant documents: 27.8% of all documents in the manual-only pool are relevant, whereas only 18.9% in the automatic-only pool are. Therefore, removing the manual runs from the pool can be viewed as an approximation of the situation where a new ranking technique is developed that produces better rankings than existing techniques, but at the same time returns a very different set of documents. The question then is: If evaluated based on the existing pool of judged documents, will this new ranking technique be judged fairly or not?
To be able to answer the question, we evaluated all runs (manual and automatic) on both the original qrels and the incomplete qrels created from the automatic runs only. This time, the difference between the two sets of qrels, and the impact it has on the systems X  rankings, is even more extreme than in the leave-one-out experiments (see Table 3).
According to P@20, the position of a manual run in the ranking of all 42 runs in the pool is lowered by more than 6 positions on average (18 positions in the worst case). AP is a little less sensitive, but a manual run still loses 5 positions on average, and 13 in the worst case. Again, for bpref the situ-ation is reversed. A manual run gains 4 position on average, and one run even moves up by 12 positions. RankEff, like before, is the most reliable measure: On average, a manual run moves up by only 1.8 positions.

We also examined how switching from the original qrels to the incomplete qrels created only from automatic runs affects the overall ranking of all runs, as measured by Kendall X  X   X  between the two rankings. For all measures evaluated, Kendall X  X   X  drops below 0.9 (i.e., more than 5% inversions), which commonly carries the interpretation that the two rankings are not equivalent. The poorest correlation is exhibited by P@20, which produces 74 out of inversions (8.6% inversions  X  Kendall X  X   X  = 0.8281).
In order to understand why bpref is doing so poorly here, it is helpful to have a look at a few concrete examples. Ta-ble 5 lists automatic and manual runs from four different groups. Switching from the original qrels to the automatic-only qrels consistently increases the bpref of all runs. This increase is due to the reduced number of known relevant doc-uments and the resulting artificial increase of the systems X  recall (on the automatic-only qrels, the mean per-topic re-call of an average run in the TB 2006 pool is 0.7018; on the original qrels, it is 0.6363). However, this effect is much larger for the manual runs than for the automatic runs. For example, the bpref score of the manual run  X  X etaman X  is increased by almost 20%, from 0.398 to 0.475. This general trend is also reflected by the relatively large RMS error for bpref of 0.0542 (cf. Table 3).

RankEff also overestimates the performance of most runs when computed on the automatic-only qrels. Unlike bpref, however, it overestimates the performance consistently, in-creasing the score of every run by about 0.04. Thus, the ranking is largely unaffected by the higher scores.
We now present the results we obtained by building a model of relevance from the training data in the qrels and predicting whether an unjudged document is relevant or not.
We first tested how well the two classifiers defined in Sec-tion 3 can predict the relevance of a document when trained on a random subset of the qrels. The results are summa-rized in Table 6. They show that, for a very small training set, the KLD classifier performs better than the SVM-based approach, while SVM outperforms KLD for larger train-ing sets. In general, however, both classifiers did a sur-prisingly poor job. An F 1 score around 0.5 is far below what researchers usually report for document classification tasks [11]. Nonetheless, the results might be good enough for our purposes.

It needs to be mentioned at this point that we used the in-ductive learner that comes with SVM light for all experiments described in this paper. For small training sets, the trans-ductive learner [12] substantially outperforms the inductive approach (on the 5% training set, for instance, F 1 increases from 0.230 to 0.373). However, due to time constraints, we performed all of our experiments with the inductive learner, which is substantially faster than the transductive one.
In our first real experiment with the classifiers, we ex-amined whether predicting the relevance of unjudged doc-uments can improve evaluation accuracy in the leave-one-out experiments. In this context, the quality of the classi-fiers is actually far better than what Table 6 suggests. The SVM classifier achieves a precision of 0.7979, and a recall of 0.6872, both macro-averaged over all 20 groups and 50 topics (KLD classifier: precision = 0.6532, recall = 0.6642). 1 measure Precision Recall F 1 measure KLD SVM  X 
Table 7 shows the results we obtained in the revised leave-one-out experiments, using a classifier to predict the rele-vance of all documents that were removed from the pool (i.e., top 50 documents from the left-out runs). When using the SVM classifier, the rank of a run from the group that was removed from the pool changes by less than 1 place on average. This holds for all measures. For P@20, the maxi-mum change in rank decreases from 18 to 7; the RMS error of the P@20 scores is reduced by 64%, from 0.0243 to 0.0088 (comparing Table 2 and Table 7).

In a last experiment, we had the classifiers predict the relevance of all documents removed from the pool in the automatic-only experiments (i.e., documents retrieved only by manual runs). Table 8 shows the results we obtained for this setting. Using the SVM classifier, the number of places a manual run is moved up or down in the ranking according to P@20 is decreased from 6.4 to 2.0 on average (comparing Tables 3 and 8). The correlation between the original rank-ing, based on the original qrels, and the new ranking, based on the SVM-completed qrels, is in excess of 0.9 for all mea-sures shown in the table, including bpref and RankEff (in the case of bpref, it is improved from 0.8676 to 0.9164; in the case of RankEff, a very slight increase from 0.8955 to 0.9071 is achieved). Following the usual interpretation of Kendall  X  values, the two rankings can be considered equivalent. Regarding the relative performance of the KLD and the SVM classifier, we can say that the SVM classifier leads to more accurate results in almos t all cases. The only exception is AP, for which the KLD classifier with its training target precision = recall (which is what AP needs for its scores to remain constant) achieves slightly better results.
Traditional evaluation measures used in information re-trieval are unable to deal with the problem of incomplete judgements. The bpref [4] measure overcomes this limita-tion by ignoring unjudged documents. This approach works well for unbiased incomplete judgements, but does not prop-erly address the problem of biased judgements. We found that bpref is not immune to incomplete and biased judge-ments. Where other measures, such as average precision, tend to underestimate the performance of a run that lies outside the pool of judged documents, bpref tends to over-estimate it  X  to a similar degree.

The effect of biased judgements, however, can be coun-tered by training a classifier on the judged documents and using it to predict the relevance of unjudged documents. In our experiments with data from the TREC 2006 Terabyte track, predicting document relevance consistently increases the Kendall  X  correlation between the ranking produced from the original set of judgements, based on all runs, and a ranking produced from a biased set of judgements, con-structed from the automatic runs only, for all measures we examined. For P@20, Kendall X  X   X  is increased from 0.8281 to 0.9512. For bpref, it is increased from 0.8676 to 0.9164. Hence, it seems to be possible to reliably evaluate the perfor-mance of retrieval systems, even if the relevance judgements used in the evaluation are incomplete and highly biased.
This does not imply that less effort should be put on the creation of manual judgements. It does, however, mean that, at least for the GOV2 collection examined in our experi-ments, it is usually a good idea to not use the original qrels built from a pool of old systems when evaluating a new ranking technique. Instead, a classifier should be trained and used to predict the relevance of documents that are re-turned by the new technique, but not found in the pool. This way, the bias inherent in most evaluation measures (either positive or negative), can be avoided, and a more reliable evaluation of the new ranking technique can be obtained.
Of course, using a classifier to predict document relevance bears the risk that a new ranking method is developed and trained to best match the classifier instead of actual human relevance judgements. Therefore, it can only be a short-term KLD SVM  X  solution, to be replaced by human relevance assessments in a later stage of the development of a new technique.
It is quite possible that the results presented here can be improved by using more sophisticated classification al-gorithms or by fine-tuning the classifier X  X  parameters. It is also possible to modify the text classifiers used in our exper-iments in such a way that, instead of performing a binary decision of the form  X  X elevant X / X  X on-relevant X , they output for each unjudged document the probability that the doc-ument is relevant. These probability values could then be used to compute estimated precision values instead of exact values. It is not unlikely that this modified version would lead to better results than the basic version presented here.
Moreover, once the classifiers have been modified to gener-ate probabilities instead of binary judgements, it is possible to combine their output with the probability values com-puted by the method proposed by Aslam and Yilmaz [3]. It is conceivable that such a combination would result in even better predictions of document relevance  X  a direction that deserves further exploration. [1] P. Ahlgren and L. Gr  X  onqvist. Retrieval Evaluation [2] J. A. Aslam, V. Pavlu, and E. Yilmaz. A Statistical [3] J. A. Aslam and E. Yilmaz. Inferring Document [4] C. Buckley and E. M. Voorhees. Retrieval Evaluation [5] S. B  X  uttcher, C. L. A. Clarke, and I. Soboroff. The [6] C. L. A. Clarke, N. Craswell, and I. Soboroff. [7] C. Cleverdon. The Cranfield Tests on Index Language [8] C. Cortes and V. Vapnik. Support-Vector Networks. [9] L. Gr  X  onqvist. Evaluating Latent Semantic Vector [10] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated Gain-Based [11] T. Joachims. Text Categorization with Suport Vector [12] T. Joachims. Transductive Inference for Text [13] M. G. Kendall. A New Measure of Rank Correlation. [14] E. M. Voorhees. The Philosophy of Information [15] E. Yilmaz and J. A. Aslam. Estimating Average [16] J. Zobel. How Reliable are the Results of Large-Scale
