 Armand Joulin armand.joulin@ens.fr WILLOW-SIERRA project-teams, INRIA -Ecole Normale Sup  X er ieure Francis Bach francis.bach@inria.fr SIERRA project-team, INRIA -Ecole Normale Sup  X erieure Discriminative supervised classifiers have proved to be very accurate data-driven tools for learning the rela-tionship between input variables and certain labels. Usually, for these methods to work, the labeling of the training data needs to be complete and precise. How-ever, in many practical situations, this requirement is impossible to meet because of the challenges posed by the acquisition of detailed data annotations. This typ-ically leads to partial or ambiguous labelings. Different weakly supervised methods have been pro-posed to tackle this issue. In the semi-supervised framework ( Chapelle et al. , 2006 ), only a small num-ber of points are labeled, and the goal is to use the un-labeled points to improve the performance of the clas-sifier. In the multiple-instance learning (MIL) frame-work introduced by Dietterich &amp; Lathrop ( 1997 ), bags of instances are labeled together instead of individu-ally, and some instances belonging to the same bag may have different true labels. Finally, in the am-biguous labeling setting ( Jin &amp; Ghahramani , 2003 ; Hullermeier &amp; Beringer , 2006 ), each point is associ-ated with multiple potential labels.
 More generally, in all these frameworks, the points are associated with observable partial labels and the im-plicit or explicit goal is to jointly estimate their true latent labels and learn a classifier based on these la-bels. This usually leads to a non-convex cost function which is often optimized with a greedy method or a coordinate descent algorithm such as the expectation-maximization (EM) procedure. These methods usu-ally converge to a local minimum, and their initializa-tion remains an open practical problem.
 In this paper, we propose a simple and general frame-work which can be used for any of the aforementioned problems. We explicitly learn the true latent label and the classifier parameters. We also propose a convex re-laxation of our cost function and an efficient algorithm to minimize it. More precisely, we use a discrimative classifier with a soft-max loss, and our convex relax-ation extends the work of Guo &amp; Schuurmans ( 2008 ). Our main contributions are:  X  a full convex relaxation of the soft-max loss func- X  a novel convex cost function for weakly supervised  X  a dedicated and efficient optimization procedure. We develop our framework for the general weakly su-pervised case. We propose results on both toy exam-ples as proof of concept of our claims, and on standard MIL and semi-supervised learning (SSL) datasets. 1.1. Related work Multiple instance learning (MIL) has received much attention because of its wide range of applica-tions. First used for drug activity prediction, it has also been used in the vision community for different problems such as scene classification ( Maron &amp; Ratan , 1998 ), object detection ( Viola et al. , 2006 ), object tracking in video ( Babenko et al. , 2009 ), and image database retrieval ( Yang , 2000 ). Many MIL methods have been developed in the past decade. For example, some are based on boosting ( Auer &amp; Ortner , 2004 ), others on nearest neighbors ( Wang &amp; Zucker , 2000 ), on neural networks ( Zhang &amp; Zhou , 2006 ), on decision trees ( Blockeel et al. , 2005 ), or the construction of an appropriate kernel ( Wang et al. , 2008 ; G  X artner et al. , 2002 ; Kwok &amp; Cheung , 2007 ). Much of the work in the MIL community has focused on the use of dis-criminative classifiers, the most popular one being the support vector machine (SVM) ( Andrews et al. , 2003 ; Chen &amp; Wang , 2004 ; Gehler &amp; Chapelle , 2007 ). In this paper, we concentrate on the logisitic loss which makes little difference with the hinge loss with the ad-ditional advantage of being twice differentiable. Note that this loss has already been used in the context of MIL ( Xu &amp; Frank , 2004 ; Ray &amp; Craven , 2005 ), but with different optimization schemes.
 Many semi-supervised learning (SSL) methods have also been proposed in the past decade (see, e.g., Chapelle et al. , 2006 ; Zhu , 2006 ). For example, some are based on maximizing the margin with an SVM framework ( Joachims , 1999 ; Bennett &amp; Demiriz , 1998 ; Xu &amp; Schuurmans , 2005 ), and others use the unlabeled data for regularization ( Belkin et al. , 2004 ) or co-training of weak classifiers ( Blum &amp; Mitchell , 1998 ).
 Discriminative clustering provides a principled way to reuse existing supervised learning machin-ery while explicitly estimating the latent labels. For example, following the SVM approach of Xu et al. ( 2005 ), algorithms using linear discriminant anal-ysis ( De la Torre &amp; Takeo , 2006 ) or ridge regres-sion ( Bach &amp; Harchaoui , 2007 ) have been proposed. These methods often fail in the multiclass case, whereas we show that the soft-max loss with intercept works well in this setting. A common issue for discrim-inative clustering is that a perfect separation is reached by assigning the same label to all of the points. In most of the previously cited methods, this issue is adressed by adding linear constraints on the size of each clus-ter. In this paper we use instead a natural cluster-size balancing term corresponding to an entropy penaliza-tion ( Chapelle et al. , 2006 ; Joulin et al. , 2010 ). The link between SSL and MIL has been widely studied in the community. For example, in the context of image segmentation with text annota-tion, Barnard et al. ( 2003 ) propose a general weakly supervised model based on a multi-modal extension to a mixture of latent Dirichlet allocation. An impor-tant issue with this family of generative models is that learning the parameters is often untractable. Another example is Zhou &amp; Xu ( 2007 ) who use the relation be-tween MIL and SSL to develop a method for MIL. The idea of using a convex cost function in the weakly supervision context has been already studied in dif-ferent contexts such as, for example, ambiguous la-beling ( Cour et al. , 2009 ) or discriminative cluster-ing ( Xu et al. , 2005 ; Bach &amp; Harchaoui , 2007 ). In this paper, we are interested in the convex relaxation of a general multiclass loss function, i.e., the soft-max loss. Guo &amp; Schuurmans ( 2008 ) propose a related re-laxation but do not consider the intercept in the linear classifier. We extend their work to the case of linear classifiers with an intercept and show in the exper-iment section, why this difference is crucial when it comes to classification. Note that by using kernels, we can use non-linear classifiers as well. Also, our ded-icated optimization scheme is more scalable than the one developed in Guo &amp; Schuurmans ( 2008 ) and could be applied to their problem as well. 2.1. Notations We suppose that we observe I bags of instances. For i and N i = |N i | is its cardinality. We denote by N = P i N i the total number of instances. In each bag i , an instance n in N i is associated with a feature x n  X  X and a label y n in L , in certain feature and label space. In this paper, we suppose that this label is common to all the instances of a same bag and explain only partially the instances contained in the bag. We are thus interested in finding a latent label z n  X  P which would give a better understanding of the data. We denote by P and L the cardinalities of P and L . We also assume that the latent label z n of an instance n can only take its values in a subset P y depends on the label y n of the bag. The variables y n and z n are associated with their canonical vectorial representation, i.e., z np = 1 if the instance n has a latent label of p and 0 otherwise. We denote by z the N  X  P matrix with rows z n .
 Instance reweighting. In many problems, a set of instances can be bigger than the other, this is the case for example in a one-vs-all classifer where the number of positive instances is often very small compared to the number of negative examples. A side-contribution of this work is to consider explicitly a reweighting of the data to avoid undesired side effects: Each point is associated with a weight  X  n  X  0 which denotes its importance compared to others. Some examples are the uniform case, i.e.,  X  n = 1 N or when bags have to be reweighted, i.e.,  X  n = 1 IN denote by  X  the vector with entries equal to  X  n . Note that  X   X  0 and P n  X  n = 1.
 This setting is very general, so let us now show how it applies to several concrete settings.
 Semi-supervised learning. Given a set of true la-bels P and N l points with known label, there are N l +1 bags, i.e., one for each labeled point and one for all the unlabeled instances. The set L is equal to P plus a la-bel for the unlabeled bag (i.e., L = P + 1). The true label of an instance in a positive bag is fixed whereas in the unlabeled bag it can take any value in P . Unsupervised learning. This is an extreme case of the semi-supervised framework with only the unla-beled bag.
 Multiple instance learning. There are two possible labels for a bag ( L = 2), i.e., positive ( y n = 1) or negative ( y n = 0). The true label z n of an instance n in a negative bag is necessarily negative ( z n = 0) and in a positive bag it can be either positive or negative ( P 1 = { 0 , 1 } ).
 Ambiguous labelling. Each bag is associated with a set of possible true labels P l . The set of partial labels is thus the combination of all possible subsets of P , i.e., each label l  X  L represents a subset of P ( L = 2 P ). 2.2. Problem formulation The goal of a discriminative weakly supervised classi-fier is to find the latent labels z that minimize the value of a regularized discriminative loss function. More precisely, given some latent label z and some feature map  X  : X 7 X  IR d (note that  X  could be explicitly defined or implicitly given through a positive-definite kernel), we train a multi-class discriminative classifier to find the parameters w  X  IR P  X  d and b  X  IR P that minimize: where  X  : IR P  X  IR P 7 X  IR is a loss function. In this paper, we are interested in the multi-class set-ting where a natural choice for  X  is the soft-max loss function ( Hastie et al. , 2001 ). Note that for a given instance n , the set of possible true labels de-pends on the the label y of its bag, our loss func-tion  X  ( z n , w T  X  ( x n ) + b ) then takes the following form:  X 
X where w T p is the p  X  X h row of w T and b p the p  X  X h entry of b .
 Cluster-size balancing term. In many unsuper-vised or weakly supervised problems, a common issue is that assigning the same label to all the instances leads to perfect separation. In the MIL community, this is equivalent to considering all the bags as neg-ative and a common solution is to add a non-convex constraint which enforces at least one point per pos-itive bag to be positive. Another solution used in the discriminative clustering community is to add con-straints on the number of elements per class and per bag ( Xu et al. , 2005 ; Bach &amp; Harchaoui , 2007 ). De-spite good results, this solution introduces extra pa-rameters and may be hard to extend to other frame-works such as MIL, where a positive bag may not have any negative instances. Another common tech-nique is to encourage the proportion of points per class and per bag to be close to uniform. An ap-propriate penalty term for achieving this is the en-tropy (i.e., h ( v ) =  X  P k v k log( v k )) of the proportions of points per bag and per latent label, leading to: Penalizing by this entropy turns out to be equivalent to maximizing the log-likelihood of a graphical model where the features x n explain the labels y n through the latent labels z n ( Joulin et al. , 2010 ). An important consequence is that the natural weight of this penalty in the cost function is 1, so we do not add any extra parameters.
 To avoid over-fitting, we penalize the norm of w , lead-ing to the following cost function: where  X  &gt; 0 is the regularization parameter and the problem thus takes the following form: where S P = { t  X  IR P | t  X  0 , t T 1 P = 1 } is the sim-plex in IR P . To avoid cumbersome double subscripts, we suppose that any instance n in a bag with a la-bel y n (which is common to the entire bag), has a latent label z n in P instead of P y In the next section we show how to obtain a convex relaxation of this problem. An interesting feature of the soft-max cost function is its link to the entropy through the Fenchel conju-gate ( Boyd &amp; Vandenberghe , 2003 ), i.e., given a P -dimensional vector t , the log-partition can be written Substituting in the loss function, the weakly super-vised problem defined in Eq. ( 1 ) can be reformulated as: complexity O ( N 2 ) O ( N 2 ) O ( N 3 ) O ( N ) where q is an N  X  P matrix with n -th row q T n , and g ( z, q ) is equal to: min Minimizing this function w.r.t. the intercept b leads to an intercept constraint on the dual variables, i.e, ( q  X  z ) T  X  = 0. The minimization w.r.t. w leads to a closed-form expression for g : where K is the positive definite kernel matrix asso-ciated with the reweighted mapping  X  , i.e., with en-tries equal to K nm =  X  n  X  ( x n ) T  X  ( x m )  X  m . The cost function is not convex in general in z since it is the maximum over a set indexed by q of concave functions in z . A common way of dealing with this issue is to relax the problem into a semidefinite program (SDP) in zz T . Unfortunately, our cost function does not di-rectly depend on zz T , but a reparametrization in terms of q inspired by Guo &amp; Schuurmans ( 2008 ) allows us to get around this technical difficulty.
 Reparametrization in q . We reparametrize the problem by introducing an N  X  N matrix  X  such that q =  X  z ( Guo &amp; Schuurmans , 2008 ). The in-tercept constraint and the normalization constraint on q (i.e., q 1 K = 1 N ) become constraints over  X , i.e., respectively  X  T  X  =  X  and  X 1 N = 1 N . Translating the addition of an intercept to a linear classifier into a simple constraint on the columns of  X  provides a sig-nificant improvement over Guo &amp; Schuurmans ( 2008 ), as shown in Section 5.1 . This reparametrization has the side-effect of introducing a non-convex term in the cost function since the entropies over q n in Eq. ( 2 ) is replaced by an entropy over the n  X  X h row of  X  z which is not jointly concave/convex in  X  and z .
 Tight upper-bound on the entropy. We show in the supplementary material that the entropy in q can be bounded by a difference of entropy in  X  and z , up to an additive constant C 0 :
X This upper-bound is tight in the sense that given a dis-crete value of z (i.e., before the relaxation), the maxi-mum of the left part among discrete values of q is equal to the maximum of the right part among correspond-ing discrete values of  X . Note also that the term in z appearing in Eq. ( 3 ) cancels out with the entropy term in Eq. ( 2 ). This relaxation leads to the minimizition of the following function of z : max where O = {  X  |  X 1 N = 1 N ,  X  T  X  =  X ,  X   X  0 } . This problem depends on z solely through the matrix zz T , and can thus be relaxed into an SDP in zz T . Reparametrization in z . With the change of vari-able Z = zz T , we have the maximum of a set of linear functions of Z , which is convex. However, the set Z of possible values for Z is non-convex since it is defined by: Let us review these constraints:  X  In practice, the piecewise-positivity constraint is  X  The rank constraint is the main source of non- X  The rest of the constraints defines the elliptope : Note that an additional linear constraint may be needed depending on the considered weakly supervised problem. We give below some examples:  X  In the case of MIL, this constraint takes the form  X   X  X ust-not-link X  constraints on the instances can In the rest of this paper, we consider the specific cases of SSL, MIL and discriminative clustering:  X  In SSL, we can reduce the dimensionality of Z :  X  In MIL, the same reduction can be done with P =  X  Discriminative clustering is similar to SSL By taking into account all of these modifications and by dropping the rank constraint, we replace the non-convex set Z by the elliptope E N imization of g ( Z ) over E N max In the next section we propose an efficient algorithm to solve this convex optimization problem. Since our optimization involves a maximization in our inner loop , it cannot be solved directly by a general-purpose toolbox. We propose an algorithm dedicated to our case. In the rest of this paper we refer to the maximization as the inner loop and the overall mini-mization of our cost function as the outer loop . 4.1. Inner loop Evaluating the cost function defined in Eq. ( 5 ) involves the maximization of the sum of the entropy of  X  and a function T defined as: We use a proximal method with a reweighted Kullback-Leibler (KL) divergence which naturally en-forces the point-wise positivity contraint in W , and leads to an efficient Bregman projection with a KL di-vergence (an I-projection to be more precise) on the rest of the constraints defining W . More precisely, given a point  X  0 , the proximal update is given by max-imizing the following function: l ( X )=tr  X  T  X  T ( X  0 )  X  X where L is the Lipschitz constant of  X  T and D  X  is a reweighted KL divergence defined as:
D  X  ( X  k  X  0 ) = The I-projection can be done efficiently with an iter-ative proportional fitting procedure (IPFP), which is guaranteed to converge to the global minimum with linear convergence rate ( Fienberg , 1970 ).
 Note that to obtain a faster convergence of the inner loop, we may take advantage of a low-rank decomposi-tion of K and R T ZR and we use an accelerated prox-imal scheme on the logarithm of  X  ( Beck &amp; Teboulle , 2009 ). To control the distance from the optimum  X   X  , we can use a provably correct duality gap which can be computed efficiently (details are in the supplementary material). 4.2. Outer loop The outer loop minimizes g ( Z ) as defined in Eq. ( 5 ) over the elliptope E N been proposed to solve this type of problems ( Goemans &amp; Williamson , 1995 ; Burer &amp; Monteiro , 2003 ; Journ  X ee et al. , 2010 ) but, to the best of our knowledge, they all assume that the function and its gradient can be computed efficiently and put the em-phasis on the projection. This is not the case in our problem, and we thus propose a method adapted to our particular setting.
 First, to simplify the projection on the E N place our cost function g ( Z ) by its diagonally rescaled version g R ( Z ) = g (diag( Z )  X  1 / 2 Z diag( Z )  X  1 / 2 that even if this function is in general non-convex, it coincides with g ( Z ) on E N this set convex. This modification allows us to rescale the diagonal of any update Z to a diagonal equal to 1 N without modifying the value of our cost function. Our minimization of g R over the elliptope is also based on a proximal method with a Bregman diver-gence to guarantee updates that stay in the feasible set. A natural choice for the Bregman divergence is the KL divergence based on the von Neumann en-tropy, i.e, the entropy of the eigenvalues of a ma-trix (see more details in the supplementary material). This divergence guarantees that each update has non-negative eigenvalues. Given a point Z 0 , its update can then be obtained in closed-form as the diagonally rescaled version of V Diag(exp(diag( 1 t E ))) V T , where V and E are the eigenvectors and the eigenvalues of  X  X  X  g R ( Z 0 )+ t log( Z 0 ) and t is a positive step size com-puted using a line-search with backtracking.
 As in the inner loop, we use a computationnally tractable provable duality gap, i.e.,  X  N R  X  min , where  X  min is the lowest eigenvalue of  X  g R ( Z ) (see details in the supplementary material). 4.3. Rounding Many rounding schemes can be applied with simi-lar performances. Following Bach &amp; Harchaoui ( 2007 ) and Joulin et al. ( 2010 ), we use k-means clustering on the eigenvectors associated with the k highest eigen-values ( Ng et al. , 2001 ) to obtain a rounded solution z . This z  X  is then used to initialize an EM procedure to solve the problem defined in Eq. ( 1 ) and obtain the parameters ( w, b ) of the classifier, leading to finer de-tails not caught by the convex relaxation.
 A specificity of the MIL framework is that strictly no point from a negative bag should be classified as pos-itive, which leads to adding to Eq. ( 1 ), the following linear constraints on the parameters of the classifier:  X  i  X  I  X  , n  X  N i , w T 0  X  ( x n ) + b 0  X  w T 1  X  ( x We add these hard constraints in the M-step (opti-mization over w and b ) of the EM procedure. The projection over this set of linear constraints is per-formed efficiently with an homotopy algorithm in the dual ( Mairal et al. , 2010 ). Implementation. Our algorithm is implemented in MATLAB and takes from 1 to 5 minutes for 500 points. Note that we can efficiently compute the solutions for different values of  X  using warm restarts. Our overall complexity is O ( N 3 ) but we can scale up to several thousands of points. The complexity of the differ-ent steps in our algorithm is given in Figure 1 . On larger datasets, we can use our relaxation on subsets of instances or on pre-clustering the instances (with k-means) and use it to initialize the EM on the complete dataset. 5.1. Discriminative clustering In this section, we compare our method to two dif-ferent discriminative clustering methods for the mul-ticlass case: the SDP relaxation of the soft-max prob-lem with no intercept ( Guo &amp; Schuurmans , 2008 ) and the discriminative clustering framework introduced by Bach and Harchaoui ( 2007 ). The latter comparison is relevant since they propose a convex cost function based on the square loss with intercept.
 We consider in Figure 2 , as a proof of concept, two toy examples where the goal is to find 3 and 5 clus-ters with linear kernels and N = 500. Even if the clusters are linearly separable, the set of values of w and b which leads to a perfect separation is very small (Figure 2 , panel (a)), making the problem challeng-ing. For fair comparison, we test different regulariza-tion parameters and show the one leading to the best performances. We show the matrix Z obtained for the three methods as well as the matrix K = xx T in Fig-ure 2 . We see that our method clearly obtains a bet-ter estimation of the class assignment compared to the others, showing the importance of both the soft-max loss and the intercept.
 In panels (a) and (b) of Figure 3 , we also show that our method works with non-linear kernels in a multi-class setting. Finally, in the panel (c) of Figure 3 , we show a comparison with k-means as we increase the number of dimensions containing only noise, following the setup of Bach &amp; Harchaoui ( 2007 ). Our setting is the 3-cluster problem shown in Figure 2 with an RBF kernel and N = 300. We see that our algorithm is more robust than k-means. 5.2. Multiple instance learning In Figure 4 , we show some comparisons with other MIL methods on standard datasets ( Dietterich &amp; Lathrop , 1997 ; Andrews et al. , 2003 ) for a variety of tasks: Algorithm Musk1 Tiger Elephant Fox Trec1
EM-DD ( Zhang &amp; Goldman , 2001 ) 84.8 72.1 78.3 56.1 85.8 mi-SVM ( Andrews et al. , 2003 ) 87.4 78.9 82.0 58.2 93.6 MI-SVM ( Andrews et al. , 2003 ) 77.9 84.0 81.4 59.4 93.9 PPMM Kernel ( Wang et al. , 2008 ) 95.6 80.2 82.4 60.3 93.3 Random init / Uniform 71.1 69.0 74.5 61.0 81.3 Tandom init / Weight 76.6 71.0 74.5 59.0 84.4 a drug activity prediction ( musk ), image classifica-tion ( fox, tiger and elephant ), and text classification ( trec1 ).
 For comparison, we use the setting described by Andrews et al. ( 2003 ), where we create 10 random splits of the data, train on 90% of them and test on the remaining 10%. We test our algorithm with and with-out the intercept and with uniform or bag-specific (i.e., to some classical MIL algorithms. Note that we have only tried a linear kernel, and we select the regulariza-tion parameter using a 2-fold cross-validation for each split. Our algorithm obtains comparable performances with methods dedicated to the MIL problem. 5.3. Semi-supervised learning For the SSL setting, we choose the standard SSL datasets and we compare with methods proposed in Chapelle et al. ( 2006 ). The benchmarks (Linear and Nonlinear) are based on a SVM formulation and the benchmark (Entropy-Reg.) uses an entropy regu-larization. We use our method with either a linear or a RBF kernel. To fix our parameters, we follow the experimental setup of Chapelle et al. ( 2006 ). Each set contains 1500 points and either l = 10 or 100 of them are labeled. We show the results in Figure 5 . As expected, since the benchmarks and our formulation are very related, the performances are mostly similar when l = 100. However, when l = 10, our method is more robust and its performances get significantly higher showing that a convex relaxation is less sensible to noise and poorly labeled data. In this paper, we propose a convex relaxation of a general cost function for weakly supervised problems. We show the importance of a tight convex relaxation compared to relaxation where either the related linear classifier has been approximated (absence of intercept) or the loss function (square-loss instead of the soft-max loss). Our comparison with standard non-convex methods for MIL and SSL shows the importance of the initialization for robustness of the approach. We believe that convex relaxation is a powerful tool to ob-tain good initializations to non-convex problems. The trade-off is that these methods are usually not scal-able which suggest to use them on subsets of points or after a quantization step to initialize a more efficient algorithm, such as EM.
 Acknowledgements. This paper was partially sup-ported by the European Research Council (SIERRA and VIDEOWORLD projects).

