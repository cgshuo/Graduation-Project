 Community question answering (cQA) has become an important service due to the popularity of cQA archives on the web. This paper is concerned with the problem of question search. Question search in cQA aims to find the historical questions that are seman-tically equivalent or similar to the queried questions. In this paper, we propose a faster and better retrieval model for question search by leveraging user chosen category. After introducing the ques-tion category, we can filter certain amount of irrelevant historical questions under a wide range of leaf categories. Experimental re-sults conducted on real cQA data set demonstrate that the proposed techniques are more effective and efficient than a variety of baseline methods.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval; H.3.5 [ Information Systems and Applications ]: On-line Informa-tion Services Algorithms, Experimentation, Performance Question Search, Question Category, Translation Model, Language Model
Over the past few years, large-scale question and answer archives have become an important information resource on the web. To make use of the large-scale archives of question-answer pairs, it is critical to have functionality of helping users to retrieve previous answers [9]. Therefore, it is a meaningful task to retrieve the se-mantically equivalent or similar questions to the queried questions, and then these answers of the retrieved questions will be used to answer the queried questions.
 Recently, question search in cQA has gained a wide interest in NLP and IR communities. A series of conferences (ACL, EMNLP, COLING, SIGIR, WWW, and CIKM) have advanced the question search techniques and proposed several different retrieval models, such as the vector space model (VSM) [7, 11], the Okapi BM25 model [7, 11], the language model (LM) [6, 7, 9, 11], the trans-lation model (TR) [1, 7, 11, 13], the translation-based language model (TRLM) [26], phrase-based translation model [20, 28], and the statistical machine translation enriched model [29, 30]. Exper-imental results consistently reported that the translation-based lan-guage model (TRLM) achieved the state-of-the-art performance for question search [26]. However, all these approaches focus on how to improve the performance of question search while ignoring the efficiency (computational cost of average running time for a search query). Efficiency is important for question search since question answer archives are huge 1 and they keep growing [6]. We note that applying these existing methods for question search suffers from the following problem:
As a result, the irrelevant historical questions would increase the computational cost of running time and hinder the efficiency of question search, rather than contribute to the performance of ques-tion search. Consider an example shown in Table 1. The estimated similarity of d 1 is higher than that of d 2 and d 3 to the queried ques-tion q by using the existing methods (VSM, BM25, LM, TR and TRLM). However, d 1 is irrelevant to q .
 cQA usually organizes questions into a hierarchy of categories. When a user asks a question, the user is typically required to choose Y ahoo! Answers has more than 1 billion resolved questions as of May 1, 2010. a category for the question from a predefined hierarchy of cate-gories. The available categories of queried questions can be used to filter irrelevant questions in the archives and thus improve the ef-ficiency of question search. Moreover, we note that not all the rel-evant questions come from the same category with the category of the queried question. The relevant questions under the similar cat-egories might be exploited for further improving the performance of question search. For example in Table 1, the category "Non-Alcoholic Drinks" of q will filter the irrelevant historical questions d , and the relevant questions d 2 and d 3 under the same and simi-lar categories will be obtained relative higher ranks.

Although it appears natural to exploit the existing category in-formation for question search, we are aware of only three pub-lished studies [6, 7, 5] on utilizing category information for ques-tion search. Cao et al. [6] employed classifiers to compute the probability of a queried question belonging to different categories. The performance of question search highly depends on the accu-racy of classifiers. However, question classification in cQA is chal-lenged by the large-scale hierarchical classification problem classification error leads to the retrieval results improvement only slightly [7]. Cao et al. [7] proposed a category enhanced retrieval model and computed the global relevance score with regard to the entire collection of questions, which greatly hinders the efficiency of question search, as we will show in the experiments. Cai et al. [5] incorporated the category information into TRLM for fur-ther improving the performance while ignoring the efficiency.
In this paper, we aim to balance between effectiveness ( better performance) and efficiency (lower computation cost) for question search by leveraging user chosen category information. Compared to [6, 7, 5], our proposed method is much faster and better. To the best of our knowledge, it is the first work to give a thorough analysis between effectiveness and efficiency in studies of question search in cQA.

The contribution of this paper is expected in the following two aspects:
The rest of this paper is organized as follows. Section 2 describes the related work. In section 3, we give a brief a introduction of the existing retrieval models for question search. Section 4 presents our proposed faster and better retrieval models for question search by leveraging the user chosen question category information. Ex-perimental results are presented in section 5. Finally, we conclude with ideas for future work in section 6.
In cQA, there are more than 1,200 leaf categories organized into a hierarchical structure, the large-scale top-down hierarchical clas-sification approach used in [6] suffers from the following problems as discussed in [25]: (1) misclassification at a parent or ancestor category may force a question to be excluded from the child cat-egories; (2) the classification over high-level categories may fail easily since some of the categories are too general and thus harder to discriminative. Cao et al. [6] reported the Micro F 1 -score of hierarchical question classification was only 45.59%. Therefore, the performance of question search is hindered by the classification error.
The research of question search has been further extended to the cQA data. The major challenge for question search in cQA is the word ambiguity and lexical gap problems. Jeon et al. [11] pro-posed a word-based translation model for automatically fixing the lexical gap problem. Xue et al. [26] proposed a word-based trans-lation language model for question search. The results indicated that word-based translation language model further improved the retrieval results and obtained the state-of-the-art performance. Sub-sequent work on word-based translation models focused on provid-ing suitable parallel data to learn the translation probabilities. Lee et al. [13] tried to further improve the translation probabilities based on question-answer pairs by selecting the most important terms to build compact translation models. Bernhard and Gurevych [1] pro-posed to use as a parallel training data set the definitions and glosses provided for the same term by different lexical semantic resources. Cao et al. [7] explored the category information into the word-based translation model for question search.

In order to improve the word-based translation model with some contextual information, Riezler et al. [20] and Zhou et al. [28] pro-posed a phrase-based translation model for question and answer retrieval. The phrase-based translation model can capture some contextual information in modeling the translation of phrases as a whole, thus the word ambiguity and lexical gap problems are somewhat alleviated. Singh [21] addressed the lexical gap issues by extending the lexical word-based translation model to incorpo-rate semantic information (entities).

Recently, Zhou et al. [29] argued that the effectiveness of the above translation models were highly dependent on the availabil-ity of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they were troubled by noise issue. Therefore, Zhou et al. [29] proposed an alternative way to address the word ambiguity and word mismatch problems by taking ad-vantage of potentially rich semantic information drawn from other languages. Furthermore, Zhou et al. [30] integrated the semantic knowledge drawn from other languages with matrix factorization in order to solve the data sparseness and noised introduced by sta-tistical machine translation.

Besides, some other studies model the semantic relationship be-tween the queried questions and the candidate answers with deep question analysis or a learning-to-ranking strategy. Duan et al. [9] proposed to conduct question search by identifying question topic and question focus. Surdeanu et al. [19] proposed an approach to rank the answers retrieved by Yahoo! Answers with multiple fea-tures. Wang et al. [24] aimed to rank the candidate answers with only word information instead of the combination of different kinds of features.

However, all these existing approaches focus on how to improve the performance of question search while ignoring the efficiency (computational cost of average running time for a search query). In this paper, we aim to balance between effectiveness ( better perfor-mance) and efficiency (lower computation cost) for question search by leveraging user chosen category information. Although some studies utilized category information for question search [6, 7, 5], they focused on improving the performance of question search while hindering the efficiency. To the best of our knowledge, it is the first work to give a thorough analysis between effectiveness and effi-ciency in studies of question search in cQA.
In this section, we describe some existing retrieval models for question search.
The vector space model (VSM) has been widely used for ques-tion search [11, 7]. Given a queried question q and a historical question d in the archives, the similarity score function can be com-puted as follows: where w q ;t denotes the IDF (inverse document frequency) of term t in the collection, and w d ;t denote the TF (term frequency) of term t in d . N is the number of questions in the whole collection, f is the number of questions containing the term t , and tf frequency of term t in d .
BM25 model takes into account the question length to overcome the shortcoming of VSM [16]. Following [7], the similarity score function between a queried question q and a historical question d in the archives can be computed as follows: where k 1 , b , and k 2 are parameters that are set to 1.2, 0.75, and respectively. j d j is the question length of d and W A is the average question length in the collection.
The unigram language model (LM) is often used and assumes that each term is generated independently. It concerns only the probabilities of sampling a single word by the maximum likeli-hood. To avoid zero probability, we use Jelinek-Mercer smoothing [27] due to its good performance and cheap computational cost. So the ranking function for the query likelihood language model with Jelinek-Mercer smoothing can be written as: where C is background collection, is smoothing parameter. #( w; d ) is the frequency of term w in d , j d j and jCj denote the length of d and C , respectively.
Previous work [7, 11, 26] consistently reported that the transla-tion model (TR) yielded superior performance for question search. This model exploits the word translation probabilities in a language modeling framework. Following [11, 26], the ranking function can be written as: Figur e 1: An example of category hierarchy in Yahoo! An-swers.
 P
T R ( w j d ) = (1 ) where P ( w j t ) denotes the translation probability from word t to word w . Jeon et al. [11] assume that the probability of self-translation is 1, meaning that P ( t j t ) = 1 .
Xue et al. [26] proposed to linearly mix two different estima-tions by combining the language model with the translation model into a unified framework, called translation-based language model (TRLM). It is shown that this model gains better performance than both the language model and the translation model [26]. The model can be written as: where parameter controls the impact of translation component.
As discussed above, the previous works on question search fo-cus on how to improve the performance while ignore the efficiency (computational cost of running time) of question search. For a given queried question q , all historical questions in the archives are involved in similarity computation, although certain amount of historical questions under a wide range of categories irrelevant to q . These irrelevant historical questions will increase the computa-tional cost rather than improve the performance of question search. To solve the above problem, we propose a faster and better retrieval model by leveraging question category information to filter the ir-relevant historical questions for question search. In cQA, all ques-tions are usually organized into a hierarchy of categories. Figure 1 shows an example of part of category hierarchy of Yahoo! An-swers. When a user asks a question, the user typically required to choose a category label for the question from a predefined hierarchy of categories. Hence, each question in cQA has a category label. Let C = f c 1 ; c 2 ; ; c n g denote all leaf categories. The basic category-sensitive (BCS) question search is defined as follows: where c i is the category of queried question q , and c ( d ) is the cat-egory of historical question d . P BCS ( q j c i ; d ) is category-specific question search, which can be calculated using the existing meth-ods (VSM, BM25, LM, TR and TRLM) with some minor modifi-cations, as we will describe in subsection 3.3. By introducing the question category information, only a small number of the histor-ical questions under the same leaf category with the category of a queried question are employed, we have
According to equation (14), the computational cost of running time for a search query is substantially reduced and the efficiency of question search is greatly enhanced.
In subsection 4.1, the basic category-sensitive retrieval model in equations (11) and (12) is based on the same leaf category as-sumption, with potential relevant questions under the similar leaf categories being omitted. As shown in Figure 1, there exists sev-eral similar leaf categories under one main category. For example, "Programming &amp; Design" and "Software" are two similar leaf cate-gories under the main category "Computers &amp; Internet". Questions under the leaf category "Programming &amp; Design" may also be rel-evant with questions under the leaf category "Software". Based on these observations, we propose a related category-sensitive (RCS) retrieval model by taking into account the relevant questions under the similar leaf categories: P RCS ( q ; c i j d )= 1 where controls the relative impact between the original leaf category and other similar leaf categories. If we set a large value of , the importance of the original leaf category is emphasized. Related ( c i ) denotes the set of similar leaf categories which are related to (similar to) c i , and R ( c j ! c i ) represents the similar probability from category c j to category c i . 3
In this paper, we define Related ( c i ) as follows: where is a threshold between 0 and 1.

To estimate the similar probability between two categories, answerer-based and content-based methods used in [14] can be naturally em-ployed. However, we observe that some leaf categories consist of only a small number of questions, which may lead to the data
In cQA (e.g., Yahoo! Answers), when a user asks a new question, the user has to choose a particular category for the question. The cQA system allows the askers to choose only one leaf category for each question. So the most similar category is not allowed to manually assign to each category when adding it in cQA system. This motivates us to automatically calculate the category similarity. sparseness. In this paper, we propose to leverage topic models for inferring similar probability between two categories. The basic as-sumption is that two categories are similar because their probabili-ties of belonging to the same latent topic are similar. For example in Figure 1, leaf categories "Monitors", "Scanners" and "Printers" are similar because they all belong to the latent topic Computer Hardware .
To leverage topic model for inferring category similarity, we use the widely studied topic model Latent Dirichlet Allocation (LDA) [3] to identify the latent topic information from the large scale question-answer collection. LDA models each document as a mixture of underlying topics and generates each word from one topic. To identify the topics that each leaf category is about us-ing LDA, we aggregate all questions under the same leaf category into a big document. Thus, each document essentially corresponds to a leaf category. After utilizing LDA, each leaf category c can be represented as a Z -dimension vector topic distribution P ( z where Z is the topic number. Thus, the task of inferring cate-gory similarity probability is converted to calculate the distance between two leaf category vectors. In this paper, we propose to use normalized Kullback Leibler (KL) divergence [12], which is an asymmetric measure for measuring category similarity. The KL-divergence from c j to c i is computed by P KL ( c j jj c ( z j categories c j to c i using Jensen Shannon divergence, which shows the superior performance than others. Thus, we have
The larger R ( c j ! c i ) 2 [0 ; 1] , the more similar c After calculating the similarity between each pair of leaf categories, we can obtain the similarity matrix M C = f m ji = R ( c j where i , j 2 [1 ; n ] , n is number of leaf category. Figure 2 shows an example of generating the category similarity matrix. From M we can easily find the similarity between two leaf categories.
In subsection 4.1 and 4.2, we describe a faster and better retrieval model for question search by leveraging question category infor-mation. Now we focus on how to calculate the category-specific retrieval model P BCS ( q j c ( d ) ; d ) in equation (14) by using the ex-isting question search models.
Computing the category-specific question search with regard to only the category containing the historical question appears to be preferable. This observation motivates us to compute the category-specific question search differently from the standard VSM. The category-specific retrieval model P BCS ( q j c ( d ) ; d ) is computed by using equation (1) with the following modifications: N is replaced by N c ( d ) , the number of questions in category c ( d ) ; f by f t;c ( d ) , the number of questions containing the term t in cate-gory c ( d ) . We modify w q ;t as follows:
Similar to VSM, the category-specific retrieval model P BCS is computed by using equation (2) with the following modifica-tions: N is replaced with N c ( d ) ; f t and W A are computed with re-gard to the category c ( d ) and are replaced by f t;c ( d ) respectively, where f t;c ( d ) is the document frequency of t in cate-gory c ( d ) , and W A;c ( d ) is the average question length in the cat-egory c ( d ) . Specifically, compared to equation (2), the following modifications are made: The smoothing in LM, TR and TRLM plays an IDF-like role. Therefore, we compute the smoothing value with regard to the cate-gory rather than the whole collection, i.e., we use P ml ( w the smoothing instead of P ml ( w jC ) when computing the category-specific retrieval model P BCS ( q j c i ; d ) in equations (4), (7), and (10), where C c i denotes all questions in category c i .
The performance of the translation model and the translation-based language model will rely on the the word-to-word transla-tion probabilities. In our experiments, question-answer pairs are used for training, and the GIZA++ toolkit is used to learn the IBM translation model 1. IBM model 1 is a widely used word alignment algorithm which does not require linguistic knowledge for two lan-guages 4 .

IBM model 1 assumes each translation pair should be of com-parable length. However, an answer is usually much longer than the corresponding question. It will hurt the performance if we will fill the length-unbalanced pairs for training. To address the length-unbalanced problem, we propose a word sampling method for each answer to make it comparable to the length of the corresponding question. Suppose the lengths of an answer and the corresponding question are j a j and j q j , respectively. For answer a , we first build a bag of words b a = f ( w i ; e i ) W a i =1 g , where W unique words in a , and e i is the weights of word w i in a .
We use TF*IDF scores as the weights of words. Using b a , we sample words for j q j times with replacement according to the weights of words, and finally form a new bag with j q j words to rep-resent answer a . In the sampling result, we keep the most important words in answer a . We can thus construct a question-answer pair with balance length. W e have also employed more sophisticated algorithms such as IBM model 3 for this task. However, these methods do not achieve better performance than the simple IBM model 1. Therefore, we only demonstrate the experimental results using IBM model 1 in this paper.
 Table 2: Number of questions in each first-level category
Gi ven the length-balanced question-answer pairs, IBM model 1 can be trained using Expectation-Maximization (EM) algorithm [8] in an unsupervised fashion. Using IBM model 1, we can obtain the translation probabilities of two language-sides, i.e., P ( t P ( w j t ) , where w is a word in answer side and t is a word in ques-tion side.

IBM model 1 will produce one-to-many alignments from one side to another side, and the trained model is thus asymmetric. Hence, we can train two different translation models by assigning translation pairs in two directions, i.e., (answer ! question) and (question ! answer). We denote the former model P a 2 q and the latter as P q 2 a . We define the final translation probability P ( t the harmonic mean of the two models: where is the harmonic factor to combine the two models. When = 1 : 0 or = 0 : 0 , it simply uses model P a 2 q or P q 2 a spondingly. We collect the questions from Yahoo! Answers and use the get-ByCategory function provided in Yahoo! Answers API 5 to obtain cQA threads from the Yahoo! site. More specifically, we utilize the resolved questions and the resulting question repository that we use for question search contains 2,288,607 questions. Each resolved question consists of four parts: "question title", "question descrip-tion", "question answers" and "question category". For question search, we only use the "question title" part. It is assumed that the titles of the questions already provide enough semantic infor-mation for understanding the users X  information needs [9]. There are 26 categories at the first level and 1,262 categories at the leaf level. Each question belongs to a unique leaf category. Table 2 shows the distribution across first-level categories of the questions in the archives. To learn the word-to-word translation probabili-ties, we use the GIZA++ alignment toolkit 6 trained on one million question-answer pairs from another data set. 7
We use the same test set in previous work [6, 7]. This set contains 252 queried questions and can be freely downloaded for research http://de veloper.yahoo.com/answers http://www-i6.informatik.rwth-aachen.de/Colleagues/och/software/ GIZA++.html
The Yahoo! Webscope dataset Yahoo answers compre-hensive questions and answers version 1.0, available at http://reseach.yahoo.com/Academic_Relations. communities. 8 F or each method, the top 20 retrieval results are kept. Given a returned result for each queried question, an annota-tor is asked to label it with "relevant" or "irrelevant". If a returned result is considered semantically equivalent to the queried question, the annotator will label it as "relevant"; otherwise, the annotator will label it as "irrelevant". Two annotators are involved in the an-notation process. If a conflict happens, a third person will make judgement for the final result. In the process of manually judging questions, the annotators are presented only the questions.
We evaluate the performance of our approach using the follow-ing metrics: Mean Average Precision (MAP) and Precision@N (P@N), as they are widely used in evaluation the performance of retrieval result [15].
 where 1 ( S ) is an indicator function which returns 1 when notes the number of relevant questions among the top j ranked list returned by M for queried question q , and N M q denotes the total number of relevant questions of queried question q returned by a method M , and M q ;j is the j -th question generated by method for queried question q . MAP rewards methods that return relevant questions early and also rewards correct ranking of the results. where N M q ;N denotes the number of relevant questions among the top N ranked list returned by a method M for queried question q .
The experiments use several parameters. The first two are smooth-ing parameters and ; the third , controls the relative importance between the original leaf category and other similar leaf categories; the fourth parameter , is a threshold between 0 and 1; the last pa-rameter , controls the translation directions.

In the traditional LM in equation (4), the traditional translation model (TR) in equation (7), and the traditional translation-based language model (TRLM) in equation (7), we follow the previous work presented in [6, 7, 11, 26, 27] to select the parameters and finally empirically set = 0 : 2 and = 0 : 8 .

For parameters and used in the category-specific retrieval models, different models need different combination values. We do an experiment on a small development set of 50 questions to determine the optimal values among 0.1, 0.2, , 0.9 in terms of MAP for each category-specific retrieval model. This set is also extracted from the Yahoo! Answers data set, and it is not included
The data set is a vailable at http://homepages.inf.ed.ac.uk/gcong/qa/ Table 3: Computational cost of different methods by using the category information to filter the irrelevant questions, where running time (in seconds) is on a PC with 4G of memory and a 2.5Ghz CPU with Java programming language. 10 TR 2,288,607 0.655 11 BCS_TR 2,413 ( # 99.89%) 0.072 ( # 89.01%) 12 RCS_TR 101,466 ( # 95.57%) 0.075 ( # 88.55%) 13 TRLM 2,288,607 0.672 14 BCS_TRLM 2,413 ( # 99.89%) 0.077 ( # 88.54%) 15 RCS_TRLM 101,466 ( # 95.57%) 0.083 ( # 87.66%) in the test set. As a result, we set = 0 : 3 and = 0 : 7 for category-specific LM, TR, and TRLM, respectively.

For parameter , we do an experiment on a small development set of 50 questions to determine the best value among 1, 2, , 9 in terms of MAP. As a result, we set = 4 in the experiments empirically as this setting yields the best performance.
The key success is how to balance the effectiveness and effi-ciency in the best way. In this paper the balance is controlled by parameter , we do an experiment on this development set to de-termine the best value among 0.05, 0.1, , 0.45 in terms of the computational cost and the performance (MAP) of question search. Figure 3(a) shows the influence of on the computational cost of average running time for a search query, while Figure 3(b) shows the influence of on the performance (MAP) of question search. Finally, we set = 0 : 25 in the experiments as this setting yields the best way to balance between the the effectiveness and efficiency.
For parameter , we do an experiment on a small development set of 50 questions to determine the best value among 0.1, 0.2, , 0.9 in terms of MAP. As a result, we set = 0 : 7 in the experiments empirically as this setting yields the best performance. Besides, we use LDA to infer the category similarity probability. In this paper, we set Dirichlet priors  X  = 50 =Z , and  X  = 0 : 05 as Griffiths and Steyvers [10]. We run LDA with 200 iterations of Gibbs sampling. After trying a few different number of topics, we empirically set Z = 150 . We choose these parameter settings because they give coherent and meaningful topics for our data set.
Efficiency is important for question search since historical ques-tions in the archives are huge and they keep growing. By introduc-ing the category information, we can filter certain amount of his-torical questions under a wide range of leaf categories, which are less likely to contain relevant questions to the queried questions. In other words, the different retrieval models will search questions only in the categories which are the same or similar to categories of the queried questions. This can greatly reduce the computational cost and save running time for a search query. of question search.

We first look into how well the proposed method benefits the efficiency of question search. Table 3 shows the impact of av-erage number of historical questions and the running time for a search query by using the category information to filter the irrele-vant questions. The experiments are conducted on a PC with 4G of memory and a 2.5Ghz CPU with Java programming language. When question search is based on the same leaf category assump-tion (queried questions and the historical questions come from the same leaf category), the average number of historical questions for a search query is reduced by 99.89%, with a significant decrease of search space by limiting search in a specific leaf category (row 1 vs. row 2; row 4 vs. row 5; row 7 vs. row 8; row 10 vs. row 11; row 13 vs. row 14).

A leaf category usually consists of only a small number of ques-tions, thus search in a leaf category will be much more efficient than in the whole collection. 9 However, this simple assumption is not good in terms of performance. We note that not all relevant questions come from the same leaf categories with the categories of the queried questions. So we also consider the relevant questions under the similar leaf categories with the categories of the queried questions. This improved filtering strategy prunes the search space by limiting the search in a several similar leaf categories, with the average number of historical questions being reduced by 95.57% (row 1 vs. row 3; row 4 vs. row 6; row 7 vs. row 9; row 10 vs. row 12; row 13 vs. row 15).

Turn to the computational cost of average running time for a search query, we find that our proposed methods are more time ef-ficient than the traditional methods and thus make question search faster (saving more than 80% time). Also, we find that BCS_TRLM (e.g., BCS_VSM, BCS_BM25, BCS_LM, BCS_TR) spends less time than RCS_TRLM (e.g., RCS_VSM, RCS_BM25, RCS_LM, RCS_TR) because the latter considers the historical questions un-der the similar leaf categories with categories of the queried ques-tions. Although the latter is more time consuming than the former, it is possible to reduce the running time by using the parallel com-puting since the category similarity calculation between the leaf categories is independent with each other . We will leave it for fu-ture work.
There are some clear trends in Table 4 showing the performance of question search. First, note that the different methods using the question category information to filter irrelevant questions under a wide range of categories consistently outperform the baseline meth-ods. Some of these improvements can be quite large; for example, the MAP of BCS_VSM and RCS_VSM increase that of VSM by
In our data set, the number of questions in a leaf category is usu-ally not exceeding 1% of the whole collection.
 Table 4: The performance of question search by leveraging the question category information performed on a PC with 4G of memory and a 2.5Ghz CPU with Java programming language. Improvements over baseline methods are shown in parentheses. 16.53% and 26.03% (row 1 vs. row2 and row 3). Second, when considering the relevant questions under the similar leaf categories with the categories of queried questions, the performance is further improved (row 2 vs. row 3; row 5 vs. row 6; row 8 vs. row 9; row 11 vs. row 12; row 14 vs. row 15). 10 To sum up, the results in Table 4 demonstrate the effectiveness of exploiting user chosen category information (the same or the similar leaf category information) for question search.

In order to have a better understanding why the proposed method can significantly outperforms the traditional methods, we manually check each queried question in the test set shown in Figure 4, where X axes represents the number of the similar leaf categories that the relevant questions come from, and Y axes represents the proportion of the relevant questions relative to the number of the similar leaf categories.

The results in Figure 4 show that the relevant questions come from the one leaf category only 42%, that is to say, more than half percentage questions come from the similar leaf categories with the category of queried questions. The figure validates why our pro-posed related category-sensitive methods significantly outperforms the basic category-sensitive methods (BCS_VSM vs. RCS_VSM; BCS_BM25 vs. RCS_BM25; BCS_LM vs. RCS_LM; BCS_TR vs. RCS_TR; BCS_TRLM vs. RCS_TRLM).
These comparisons are statistically significant at p &lt; 0 : 05 Figur e 4: The proportion of the relevant questions relative to the number of the similar leaf categories.
 Table 6: Comparison with Cao X  X  work. All these comparisons are based on the same setting (a PC with 4G of memory and a 2.5Ghz CPU with Java programming language), where y in-dicates the statistical significance over the baseline LM + QC with p &lt; 0 : 05 , indicates the statistical significance over the baseline VSM + TRLM with p &lt; 0 : 08 . # Methods T ime (in Seconds) MAP P@10 1 LM + QC 0.071 0.408 0.247 2 RCS_LM 0.064 0 : 453 y 0 : 267 y 3 VSM + TRLM 0.445 0.456 0.269 4 RCS_TRLM 0.083 0 : 482 0 : 275
Example: T o get a better understanding the effectiveness of our proposed method, Table 5 gives part of the results of an example queried question "How do i find out what kind of bird i have by looking on the internet?" that is originally in the category "Pets ! Birds". The questions in bold are labeled as "relevant". After using the user chosen category, we can filter the irrelevant question under the category "Internet". Moreover, the ranks of the relevant questions under the specific category can be promoted (from 4th to 1st and 15th to 5th, respectively). Finally, when considering the relevant questions under the similar leaf categories (e.g., Other-Pets), some relevant questions under the the similar leaf categories can be found, thus the overall performance can be further improved. This example validates the effectiveness of our proposed method.
We are aware of only two published studies [6] and [7] on utiliz-ing category information for question search. In this subsection, we compare with two studies in term of computational cost of average running time for a search query and the performance of question search.
 Cao et al. [6] employed classifiers to compute the probability of Table 7: Effect of length-balanced translation pair by using two measures MAP and P@10.
 a queried question belonging to different categories, and then in-corporated the classified categories into language model for ques-tion search. We denote this method as LM + QC shown in row 1 in Table 6. Cao et al. [7] computed the global relevance score with regard to the entire collection of questions, and then computed the local relevance with regard to each category of the historical questions. Cao et al. [7] introduced the different combinations to compute the global relevance and local relevance, the combination VSM + TRLM showed the superior performance than others. In this paper, we compare the proposed method with the combination VSM + TRLM shown in row 3 in Table 6 according to two mea-sures MAP and P@10.

From Table 6, we can see that the proposed method is much faster and better than Cao et al. [6] (row 1 vs. row 2) and Cao et al. [7] (row 3 vs. row 4). To investigate why Cao et al. [6] fails to give the satisfactory results, we check the hierarchical classifi-cation model and some wrong examples. We find that the Micro F -score of the classifier is only about 45%, if the queried question is not correctly classified, then the retrieval results are poor since we search in a wrong category. For [7], the global relevance and local relevance are computed in a pipeline way, so it is difficult to employ the algorithm (e.g., parallel computing) to reduce the com-putational cost of running time. In this paper, we balance the ef-fectiveness and the efficiency of question search by leveraging user chosen question category, which is simpler, faster and better than Cao X  X  works.
In this paper, question-answer pairs collected from Yahoo! An-swers are used as a type of parallel corpus. IBM alignment tool assumes that each translation pair should be of comparable length. However, the answer part is usually much longer than the question part. It will hurt the performance if we fill the length-unbalanced pairs for learning the translation probabilities. In this subsection, we look into how much the length-balanced translation pair ben-efits the question routing. We introduce the baseline method (de-noted as LU) to denote the translation probability with the length-unbalanced question-answer pairs, which is similar to [26]. Our proposed length-balanced question-answer pairs (denoted as LB) is also used for comparison. T able 8: Sample word translation probabilities using the length-unbalanced pairs (left) and the length-balanced pairs (right). Note that words are stemmed. Figur e 5: The effect of translation directions with length-balanced translation pair for question search by using the mea-sure MAP.

Table 7 shows the comparison. From this Table, we see that using the length-balanced question-answer pairs significantly out-performs the method using the length-unbalanced translation pairs (row 1 vs. row 2; row 3 vs. row 4). Significant test using t -test show the difference between the two methods are statistically sig-nificant ( p &lt; 0 : 05 ).

To better understand the length-balanced and the length-unbalanced pairs, Table 8 shows a historical question word together with ten most probable queried question words that it will translate to by both LU and LB methods. The table shows that the related words for word "car" in case of the length-balanced question-answer pairs are more reasonable and specific than for words learned via the length-unbalanced pairs.
The harmonic factor controls the weight of the translation models trained in two directions, e.g., P a 2 q ( t j w ) and P shown in equation (22). In this subsection, we look into the effect of the harmonic factor for the performance.

In Figure 5, we show the MAP curve of the RCS_TRLM with the length-balanced question-answer pairs for question search when harmonic factor ranges from 0.0 to 1.0 stepped by 0.1. From the figure, we observe that the best performance is obtained when = 0 : 7 , which indicates that "answer ! question" is more impor-tant than "question ! answer".
In this paper, we propose a faster and better retrieval model for question search by leveraging user chosen question category. The proposed method not only considers the relevant questions under the same leaf categories, but also considers the relevant questions under the similar leaf categories with the categories of queried ques-tions. Experimental results conducted on large-scale real cQA data set demonstrate that the proposed techniques are more effective and efficient than a variety of baseline methods. To the best our knowl-edge, it is the first work to extensively address both the effective-ness and the efficiency of question search in cQA.
 This work opens to several interesting directions for future work. First, it is necessary to include the related category-sensitive infor-mation into other studies (e.g., learning-to-rank techniques [2, 4, 18, 19], analogical reasoning-based approach [23], syntactic struc-tures of questions [22], and phrase-based SMT [20, 28]) for ques-tion search. Second, question structures should be considered, so it would be interesting to combine the proposed method with other question search methods (e.g., Duan et al. [9]) to further improve the performance.
This work was supported by the National Natural Science Foun-dation of China (No. 61303180, No. 61070106, No. 61272332 and No. 61202329), the National High Technology Development 863 Program of China (No. 2012AA011102), the National Basic Research Program of China (No. 2012CB316300), CCF Opening Project of Chinese Information Processing, and also Sponsored by CCF-Tencent Open Research Fund. We thank the anonymous re-viewers for their insightful comments. [1] D. Bernhard and I. Gurevych. 2009. Combining lexical [2] J. Bian, Y. Liu, E. Agichtein, and H. Zha. 2008. Finding the [3] D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet [4] R. Bunescu and Y. Huang. 2010. Learning the relative [5] L. Cao, G. Zhou, K. Liu, and J. Zhao. 2011. Learning the [6] X. Cao, G. Cong, B. Cui, C. Jensen, and C. Zhang. 2009. [7] X. Cao, G. Cong, B. Cui, and C. Jensen. 2010. A generalized [8] A. P. Dempster, N. M. Laird, D. B. Rubin. 1977. Maximum [9] H. Duan, Y. Cao, C. Lin, and Y. Yu. 2008. Searching [10] T. Griffiths and M. Steyvers. 2004. Finding scientific topics. [11] J. Jeon, W. Croft, and J. Lee. 2005. Finding similar questions [12] S. Kullback and R. A. Leibler. 1951. On information and [13] J. Lee, S. Kim, Y. Song, and H. Rim. 2008. Bridging lexical [14] B. Li, I. King, and M. Lyu. 2011. Question routing in [15] C. Manning, P. Raghavan, and H. Schtze. 2008. Introduction [16] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and [17] G. Salton, A. Wong, and C. S. Yang. 1975. A vector space [18] Y. -I. Song, C. -Y. Lin, Y. Cao, and H. -C. Rim. 2008. [19] M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
 [20] S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. [21] A. Singh. 2012. Entity based q&amp;a retrieval. In [22] K. Wang, Z. Ming, and T-S. Chua. 2009. A syntactic tree [23] X. Wang, X. Tu, D. Feng, and L. Zhang. 2009. Ranking [24] B. Wang, X. Wang, C. Sun, B. Liu, and L. Sun. 2010. [25] G. Xue, D. Xing, Q. Yang, and Y. Yu. 2008. Deep [26] X. Xue, J. Jeon, and W. Croft. 2008. Retrieval models for [27] C. Zhai and J. Lafferty. 2001. A study of smooth methods for [28] G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-based [29] G. Zhou, K. Liu, and J. Zhao. 2012. Exploiting bilingual [30] G. Zhou, F. Liu, Y. Liu, S. He, and J. Zhao. 2013. Statistical
