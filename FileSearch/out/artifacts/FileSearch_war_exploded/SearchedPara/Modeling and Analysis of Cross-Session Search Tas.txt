 The information needs of search engine users vary in complexity, depending on the task they are trying to accomplish. Some simple needs can be satisfied with a sing le query, whereas others require a series of queries over a longer period of time. While search engines effectively satisfy many simple needs, searchers receive little support when their information needs span sessions. In this work, we propose methods for modeling and analyzing user search behavior that extends over multiple search sessions. We focus on two problems: (i) given a user query, identify all related queries from previous sessions that the user has issued, and (ii) given a multi-query task for a user, predict whether the user will return to this task in the future. We model both problems within a classification framework that uses features of individual queries and long-term user search beha vior at different granularity. Experimental evaluation of the proposed models for both tasks indicates that it is possible to effectively model and analyze cross-session search behavior. Our fi ndings have implications for improving search for complex information needs and designing search engine features to support cross-session search tasks. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process, selection process Algorithms, Experimentation, Human Factors Cross-session search tasks, machine learning, user behavior. Web searchers perform a broad range of information seeking tasks, from figuring out how to spell a word to researching cancer treatment options. Co rrespondingly, the information needs of search engine users vary in complexity. Some simple information needs, like finding a person X  X  home page or navigating to a social networking site, can be unambi guously expressed as keyword queries and have distinct answers. Other, more complicated needs, like planning a wedding or vacatio n, have multiple aspects and cannot be satisfied by the results shown on a single search result page. Addressing complex information needs requires a user to issue a series of queries, potentially spanning a long period of time and multiple search sessions. In doing so, searchers may collect, filter, and summarize information from many Web pages. In previous work [9], it has been estimated through manual analysis of query logs that appr oximately 10% of search sessions include queries on such longitudinal tasks and 25% of the overall query volume corresponds to th is type of search task. While modern search engines effectively serve many of the individual queries that correspond to simple information needs, users get little or no help when their information needs transcend the boundary of a single search session. A search session , as defined by Boldi et al. [5], is a sequence of queries issued by a single user within a specific time limit . In this work, we model and analyze complex, multi-session information needs, which we call cross-session search tasks . Cross-session tasks are related to research missions and goals [9][16], as we describe in more detail below, but we explicitly fo cus on tasks that extend across sessions. We assume that an indi vidual cross-session task consists of a series of queries that co rresponds to a distinct high-level information need. The queries related to the task are not necessarily consecutive, and a single search session may contain interleaved queries from multiple cross-session tasks, as well as shorter, within-session tasks. Cross-session tasks may evolve over time, with users starting with only a general idea of what they are searching for and progressively refine their need over time. During a single session, a user may find some results of interest, disregard others and continue exploring, try related queries, or interleave one task with other tasks. They may then drop the task before returning to it at a later time [17][19][20]. Since Web search is currently stateless, the cognitive burden of keeping track of complex search tasks is placed on the searcher. Incorporating the analysis and pred iction of long-term user search behavior into search engine infrastructure could improve the search experience for searchers in many ways. For example, Web search traditionally only considers a user X  X  current query when identifying relevant search results. If a search engine were able to identify past queries and interactions related to the searcher X  X  current long-term intent, this information could be used to improve search quality. Past info rmation could also be retained and displayed to help the user re-establish the context of a long-term search task, relieving the user from the burden of recalling past queries and pages visited (e.g ., [9]). Similarly, if a search engine could predict that a user was going to return to a task that has only been temporarily suspe nded, the search engine could help support future related searches by, for example, pre-identifying and pre-caching relevant documents, or soliciting user assistance to archive the current search session for future use. In this paper, we explore how effectively cross-session search tasks can be modeled. Specific ally, we focus on two related problems: (i) identify all previous queries by a user on the same search task, and (ii) given a search task for a user, predict whether the user will return to this task in the future. After a discussion of recent work in modeling user search behavior, we formally define the specific aspects of the challenge of modeling and analyzing cross-session search tasks that we address. We next discuss the formulation of the two research problems as classification tasks, and describe the classification m odels used. We then present the experimental setup and results for both problems. Finally, we summarize our findings and discuss future work. User search behavior has been actively studied in recent years, primarily using search logs as a valuable resource to understand people X  X  interactions with Web s earch engines. Earlier work on understanding search behavior focu sed on methods for classifying queries into high-level search goals, such as informational, navigational and transactional [8][18][29]. Downey et al . [10] studied the relationship betwee n information needs and their formulation as search queries. Recent work indicates that, in addition to queries themselves, long-term and short-term search contexts can be effectively leverage d to predict user interests [34], search success [14], and improv e search result ranking [1]. Search behavior can be analyzed over time to identify queries that express the same underlying info rmation need. Most previous work has focused on search behavior analysis and prediction within a single search session. Related queries within a session have been referred to as being part of a query chain [26] or search goal [14][16][24]. He et al . [15] proposed an algorithm to segment a query stream into sessi ons by detecting topical shifts between the queries. Hassan et al . [14] modeled session-level search goals using hidden Markov models. They experimentally demonstrated that models that took into account users X  search behavior were more predictive of session success than those that relied on document re levance. Piwowarski et al . [24] used a layered Bayesian network to model a hierarchy of user search actions, with the goal of identify ing distinct patterns of user search behavior that correspond to th e latent states of the Bayesian network. They used a classifier to learn the mapping from the distribution of latent states for a clicked document to the relevance assessment of that document in the absence of document content models. Mei et al . [21] proposed a general framework to study sequences of search activities and focused on simple prediction and classification tasks, ranging from predicting if the next click will be on an algorithmic result to segmenting the query stream into goals and missions. Single-session analyses have also been used for various search-related tasks such as query suggestion [6][7], interactive feeback [30], and query disambiguation [22]. In this paper we focus on tasks that extend across sessions. There has been some work on characterizing such tasks using log or survey data, and on automatically identifying queries on the same task. Teevan et al . [32] showed, via query lo g analysis, that nearly 40% of queries were attempts to re-find previously encountered results. Using a survey methodology, Aula et al . [2] studied the search and information re-access strategies of experienced Web users. They found that people often have difficulty remembering the queries they used originally to discover information of interest. In a field study of 21 people, MacKay and Watters [20] explored a variety of Web-based information seeking tasks. They found that information gathering tasks accounted for 13.4% of tasks, and that 58.8% of these tasks continued across sessions. Information gathering tasks we re complex and people used a variety of browser tools and actions to help complete these tasks. Liu and Belkin [19] examined the structure (parallel or dependent) of tasks that extend across diffe rent sessions. Jones and Klinker [16] proposed methods to partition a query stream into research missions and goals, where each mission corresponds to a set of rela ted information needs and may include multiple search goals. This work is closely related to ou r problem of identifying previous queries on the same search task . However, we do not decompose tasks into a hierarchical structure of missions and goals, we examine tasks that extend over a longer time period (up to a week) and we study an order of magnitude more users. We also propose many new features and experiment with different classification models. Several algorithms and tools have been developed to support the resumption of search tasks. Morris et al . [23] developed SearchBar , a system that proactively a nd persistently stores query histories, browsing histories, and users X  notes and ratings. SearchBar supports multi-session investigations by assisting with task context resumption and info rmation re-finding. However, instead of determining the search tasks automatically, SearchBar requires users to explicitly identi fy them, thus creating additional user overhead. Donato et al . [9] developed SearchPad , a system which automatically identifies research missions and presents a search workspace comprising previ ous queries and results related to the mission. SearchPad uses measures of topic coherence between pairs of consequtive queries and user engagement to identify such research missions. Although Donato and colleagues describe a method and architec ture for detecting research missions, the system evaluation presented in [9] focused primarily on systems issues (triggering w ithout influencing latency and using online evaluation to set application-specific parameters). They did not compare alternative classification algorithms or consider the problem of predicting whether users will return to the same task in the future. The research presented here addr esses two important problems in modeling cross-session information needs: (i) identifying all previous queries in a user X  X  search history on the same task as the current query, and (ii) predicting whether a user will return to the task in future sessions. We formalize these problems as classification tasks within a supervised learning framework. Our work differs from prior work in several ways:  X  We formalize and evaluate the problem of predicting search task continuation. Predicting whether a search task will be resumed in the future is new, as far as we know.  X  We extend previous work on same-task detection (notably that on research missions) by examining tasks that extend over longer time periods, studying many more searchers, and evaluating new features and classification models.  X  We describe a ne w method for automatically and semi-automatically creating labeled data sets that can be used for both problems addressed in this work.  X  We experiment with different feature sets and classifiers to identify the most informative features and the best-performing classifiers for the two pr oblems addressed here. In the following section, we formal ly define the two problems that we address and the context from which they arise. User search behavior has been modeled at different levels of granularity, ranging from eye fixations on search results [11] to information needs underlying a set of queries [16][26]. Different sources of data can be used de pending on the particular modeling task. An important data source for many high-level modeling tasks is search logs, trad itionally defined as follows: D
EFINITION 1 S EARCH LOG  X  is a temporally-ordered set of  X  is the identifier of the user,  X   X  is the time of user action,  X  for  X   X  and  X   X  is the set of clicks on results  X   X  . Queries from the search logs can be aggregated for each user to create a user search history: D particular user  X  is a temporally-ordered sequence (  X   X  of pairs of user search actions (s uch as issuing a query, clicking on a search result URL and navigating back to search results) and time stamps associated with each action. The search history provides rich sequences of observations for making inferences about search behavior, including what intent motivated the user to search and whether that intent was satisfied. In order to simplify the analysis of user search history, it is typically partitioned into units, called search sessions: D maximal subset of us er search history  X  X  X  X   X  , such that  X   X : X  X   X   X ,..., X   X  X  X   X  X   X   X  X  , where  X  is a threshold for a period of user inactivity. In query log analysis, sesson timeouts are often used as boundaries to demarcate the sessi ons. The session threshold is typically set to 30 minutes [25][26][32]. Since search sessions include user actions other than queries, the time interval between two successive queries in one session can be more than 30 minutes. A sequence of queries forms a user query stream : D temporally-ordered sequence of que ries, submitted by a particular user  X  during the course of user search history. The first two columns in Table 1 show an example of a query stream for a fictitious user. In this example, more than 20 queries (related to several intents) are issued over the course of four days. D is a subset of a query stream  X  X  X   X  , corresponding to a certain high-level intent that motivated search. Each query  X  automatically or semi-autom atically assigned a label  X  labeled task extends over multiple sessions, we designate this as a cross-session search task. Cross-session tasks typically correspond to high level information needs, which may not be directly reflected in the queries. For example, queries corresponding to the same task may not have any terms in common. Such tasks represent a level of abstraction above the stream of queries. For the purpose of modeling, all queries about the same cross-sessi on task can be assigned a label, representing such a task. The third column of Table 1 provides examples of automatic labels, assigned to the queries in the second column, according to the method presented in Section 4.2. As can be seen from this example, automatic labeling is effective when there is significant term overlap between the queries or when the inference step needed to relate one query to another query is fairly simple. However, our automatic labeling method was unable to infer the meaning of the acronym  X  X b&amp;j X  and connect it to the task labele d as  X  X eanut butter recipes. X  In order to identify previous queries on the same task or predict if a task is going to continue in the future, we must first define the task of interest. Because our search log data covered a limited period of time, we chose to focus our labeling efforts on the tasks that occurred early in the obser vation period and thus have the potential to be continued in the future. We designated those tasks as early-dominant tasks , and formally define them as follows: D  X   X  labeled with the tasks they correspond to, for a particular user  X  over  X  days, an early-dominant task is the first task that spans at least  X  distinct queries with the same label that occur within the first  X  days of the user search histor y. When multiple task labels meet the threshold criterion, the first such task is taken as the early-dominant task. The goal of identifying the early-dominant task is to create a data set for the problem of predicting whether the user will return to the task, given search log data covering a limited period of time. Parameters  X  and  X  can be set depending on the length of the available search history. The fourth column in Table 1 shows the queries that were automaticall y labeled as corresponding to the early-dominant task, based on the criteria that there should be at least two distinct queries automatic ally labeled with the same task (peanut butter recipes) in the firs t two days of the observed user search history. By requiring two distinct queries on the same task we omit most repeat navigational tasks, which are easy to identify and have been studied by others [32]. Query streams containing automati cally-identified early-dominant tasks can be post-processed by hu mans to add additional queries that were missed by the labelin g method or remove incorrectly labeled queries. As can be seen fr om the fifth column of Table 1, human annotators added queries on  X  X b&amp;j drop cookie recipe, X   X  X luffernutter, X  and  X  X oodtv X  to th e automatically-identified early-dominant task. In this work, we focus on two sp ecific practical problems arising in the context of cross-session search tasks : 1. SameTask: Given a user query, identify all pr evious queries 2. TaskContinuation: Given an early-dominant task for a user Methods that effectively solve the above two problems can be applied in a range of search scenarios. We describe some specific application scenarios for cross-se ssion tasks later in the paper. Having introduced the two problems in the context of analysis of cross-session search tasks addressed in the present work, we now discuss our approach to solving them. For both tasks we adopt a machine learning methodology by learning a classifier on a catalog of features. In the foll owing section we describe the experimental setup for both problems. The most important and challenging aspect of an experimental evaluation of models for the anal ysis of cross-session tasks is generating a training set, in which the queries are labeled with the corresponding long-term tasks. In this section, we discuss the data that we used for evaluating the m odels proposed in this work as well as the semi-supervised process for generating training labels. We used a dataset containing the anonymized logs of URLs visited by users who consented to provide interaction data through a widely-distributed browser pl ug-in. The data set contained browser-based logs with both searching and browsing episodes from which we extract search-related data. These data provide us with examples of real-world search ing behavior that are useful in understanding and modeling natural search behavior. Log entries include a timestamp for each Web page view, and the URL of the Web page visited. To remove va riability caused by geographic and linguistic variation in search behavior, we only include log entries generated in the English-speaking United States locale. The results described in this paper are based on URL visits during the last week of February 2010 representing billions of Web page visits from hundreds of thousands of unique users. From these data we extracted search sessions that started with a query to Bing using a session extraction methodology similar to [35]. After the initial query, search sessions incl ude subsequent clicked results and queries occurring in the same browser/tab instance, and ending after 30 minutes of inactivity. Labeling queries with long-term search tasks is challenging, since it requires an abstraction from the query stream to the level of information needs. The mapping from queries to information needs is perhaps best performed by human annotators, who are better able than machines to understand th e relationship between the queries and visited URLs. However, manual labeling of a large data set is cognitively demanding and time consuming. To address this, we examine both fully automatic initial labeling (for more than ten thousand users) and additional human annotation for a subset of these data (m ore than one thousand users). The automatic labeling process comprises four stages: In the first stage , a subset of users, who are likely to be involved in some long-term task, were sel ected using a simple heuristic: a user must have at least five sear ch sessions with at least 10 queries in their search history during the week covered by our log. This resulted in a set of 270,470 users. In the second stage , queries in the original query stream were expanded using two query association resources: 1. A list of queries, in which each query is associated with a set 2. A list of pairs of related qu eries, with the strength of The representation of a query wa s expanded with queries from the top-scoring query cluster, and with related queries from the query graph. Each expanded query was th en divided into terms to create a bag-of-words representation for the query. In the third stage , in order to automatically identify and label queries that are on the same task, al l pairs of queries in the user X  X  query steam were enumerated and two similarity measures were Number of users 3k 10k Human Return to dominant task 1,688 1,694 701 Number of queries 3k 10k Human Total 66,219 119,814 28,474 Query pairs 866,860 1,486,492 660,120 calculated for each query pair, using the bag-of-words query representations. We used the size of the intersection and the Jaccard coefficient between the term sets of two queries as similarity measures a nd experimented with different thresholds for both measures. If a similarity measure exceeded a threshold, the queries were labeled as belo nging to the same long-term task. The labels were assigned to the pair of queries exceeding the threshold according to the following rules: (i) if one of the queries has already been labeled before, the other query is assigned the same label; (ii) if neither query has been labeled before, the first query in the pair is used as the label for both queries. In addition to automatically assigning labels corresponding to search tasks, we determined whether there was an early-dominant task in each user X  X  search history. An early-dominant task was previously defined as the first task during the first two days of the search history that was associated with at least two unique queries. The early-dominant task la bels were used to evaluate predictions of whether a user will return to a task in a future session. Focusing labeling effort s on a single task per user (namely the early-dominant task) simplifies the human editorial task labeling process and, as we s how in the next section, leads to high inter-judge agreement. We experimented with all comb inations of query expansion strategies and similarity measures and evaluated each combination in terms of the proportion of queries with automatically assigned cross-session task labels, the to tal number of users who have early-dominant task labels, and the proportion of users who return to the early-dominant task. The best performing automatic labeling method expanded queries using both the top scoring query cluster and a one-step walk on the query graph, and used the Jaccard coefficient with threshold 0.5. In the fourth stage , from the set of 270,470 users selected in the first stage, we selected a subset of 10,852 users who had an early-dominant task and issued at least one query after the first two days. Of these people, 1,694 (15.6%) issued at least one query on the dominant task after the first two days and the remaining 9,158 (84.4%) did not. We will refer to this data set as 10k and it represents an 85-15 split between the (automatically labeled) negative and positive examples of returning to the dominant task. In order to create a training set with a balanced number of positive and negative examples, we included all users who returned to the early-dominant task and randomly subsampled 1,688 of the 9,164 users who did not return to the early-dominant task. We will refer to the resulting data set with a 50-50 split between the positive and negative examples as 3k . From the 3k data set we randomly sele cted 1,250 users and three annotators manually modified th e automatic labels. Annotators were instructed to start with the automatically-identified early-dominant task and find other queries by the same user that were on the same task. This typically involved identifying additional queries that were missed by the automatic algorithm, but sometimes also involved removing queries that were not on the same task. As a result, 1,218 users had an early-dominant task and 701 (57.6%) of these users returned to the task in a subsequent session. Table 2 summarizes the statis tics of the data sets used for experiments. The numbers presented for the 3k and 10k data sets are for the fully automatic labels, and the Human set represents the human augmentation. Automatic labeling identified 7, 038 queries corresponding to the early-dominant task (for the 1,250 users). The human labeling process identified more than 7,500 additional queries on the early-dominant task, for a total of 14 ,549 such queries. Examples of additions include: adding angelcare deluxe to a task about baby monitors , and adding princesspeach to a task about mariotoys . The labelers also removed 232 of the queries that the automatic method had identified as dominant. Examples include: removing map of south carolina from a task about visiting haiti which included the query map of haiti, and correcting some labeling errors. Overall the agreement was 96.7% for automatic postive labels and 79.2% for automatic negative labels. All annotators also labeled qu eries from 100 additional users which we used to measure inter-labeler agreement. Cohen X  X  kappa showed high inter-annotator agreem ent, ranging from 0.86 to 0.92 for the three pairs of annotators. Recall that we required two distin ct queries on the early-dominant task in an effort to omit simple re peat navigational tasks. In order to verify this, the query sessions used to assess inter-labeler agreement were also annotated as to whether the early-dominant task was navigational or informational. The majority (88%) of the tasks identified by the method de scribed above were indeed in-formational. The remaining 12% of the tasks were navigational and were included because of spelling errors, word boundary dif-ferences ( mc gilvery oil vs. mcgillvery oil ), or different query formulations to find items of interest (drudgereport com, matt drudge report, drudge report ). As intended, the vast majority of tasks identified were informational needs such as research, school work, shopping, travel planning, and general topic search [20]. We used two different classifier s to address the two problems of multi-session search tasks outlined earlier: (i) identifying all pre-vious queries on the same early-dominant task ( SameTask ), and (ii) given an early-dominant task for a user, predicting whether the task will be continued in subsequent sessions ( TaskContinuation ). The two classifiers that we used were Logistic Regression (LR) and Multiple Additive Regression Trees (MART) [13]. MART is a boosted tree algorithm that uses gradient descent for regression and classification. Logistic regr ession has previoulsy been used for similar task-modeling problems [16]. MART allows us to model conditional interactions so that we can evaluate the impor-tance of richer feature combinations. For all experiments, we used  X  -score normalization for feature values and performed 10-fold cross validation. To compare the performance of the classification methods we look at the standard performance measures of accuracy and F1 [28]. We also display precision, recall, and the contingency tables for each method. By analogy to topic classification, we look at both the macro average of F1 which weights the F1 for each query equally and micro average which weights each binary prediction (for the current query to all previ ous queries) equally 0. We look at only the micro averages for the problem of predicting whether the user will return to the task, as there is only one example per user and thus macro and micro are the same. Significance between approaches is calculated using two-tailed independent samples t -tests. In the following sections, we discuss in detail our approaches to solve the SameTask and TaskContinuation problems in the context of modeling cross-session tasks. We begin by addressing the first problem of detecting queries on the same long-term task. Specifically: for a given query, find all previous queries in the user's sear ch history that are on the same long-term task . We formulated this as a classification problem. Given a set of users, whose que ries have been manually or automatically labeled with the early-dominant task, train a classification model that will classify each pair of user queries as either on the same task or not. Formally, given a history of automatically or manually labeled queries  X  X  X  X  X  X  X   X   X ,  X  X   X ,  X   X  X ,..., X   X   X ,  X  X  X   X ,  X   X  X  X  , where  X  a label that indicates whether the query is part of the early-dominant task, we create a set of all possible pairs of queries: in which each pair is labeled as either a positive or negative example as follows:  X 
 X  X  X  X  A query pair is labeled as a positive example if both queries are related to the early-dominant task, and as a negative example if one of the queries is not related to the early-dominant task. We do not consider instances where neither query is labeled as being related to the early-dominant, si nce the queries might be on the same long-term task (but not labeled as the early-dominant task). The numbers of query pairs give n in Table 2 were calculated after dropping such pairs. Because queries are only compared with queries that occur earlier in the user X  X  history, the later in time that a query occurs, the more pairs it will be involved in. Since the SameTask problem involves pred icting the similarity relationship between pairs of que ries, we extracted pair-wise features as well as features fo r the individual queries. Individual query features are computed at different levels of granularity for historical information of a user: ranging from the session in which a query occurred, to the entire search history of a user. The 18 single-query and nine pair-wise features extracted to identify queries on the same long-term ta sk are summarized in Table 3. As a baseline (BASE), we use logi stic regression to learn a model using only Levenshtein edit distan ce between the current (given) query and all previous queries. Th is is a reasonable baseline under the assumption that an intelligently-chosen threshold applied to the dissimilarity between two quer ies could provide an accurate prediction of whether two queries are on the same task. The results of classification experiments with features specified in Table 3 on three experimental data sets are reported in Table 4. Several conclusions can be made from the findings presented in Table 4. First, both classifier s (LR and MART) consistently outperform the baseline (BASE), which is not surprising. The two classifiers show similar levels of accuracy especially for the automatically-labeled data. Second, for the automatically-labeled data, classication results improve as more data is observed, but examples changed from 50-50 to 15-85. Third, classification performance decreases on the task labels that have been assigned by human annotators. This suggest s that the human-labeled data provides a more challenging learni ng problem. This is expected since the human labels were intended to capture task structure that was not already captured automati cally, and this often involved identifying related queries that were not lexically similar. The micro precision-recall curves for the two classifiers for the task of identifying queries on the same cross-session task on different data sets are shown in Figure 1 (aggregated over all test splits). Performance at default thresholds is indicated by markers. Both classifiers show good perform ance. However, for all three dataets, LR dominates at the low recall/high precision end of the curves. This suggests that the LR model might be more applicable for high-precision ta sks such as suggesting related queries. As in Table 4, LR has notably better micro performance over the human-labeled data in the area of optimal F1 (upper performance. Understanding the relationship between micro and macro performance for this domain is an area of future work. Feature weights from the logistic regression model for the identification of queries on the sa me long-term task, trained on the manually annotated data, are summarized in Table 5. Both single and pair-wise features are importa nt in identifying queries on the same task. But, pair-wise features are more prevalent, meaning that understanding the relationshi p between the queries is more valuable than understanding either query in isolation. This is not surprising given that the goal is to understand if both queries are on the same task. Similarity features, such as whether the queries are identical and term overlap measures between the bag-of-words representations for a pair of queries (N UM T Q UERY T ERMS J AC Pai r -wise 1.44 N UM Q UERY C HARS 1 Quer y -b ase d 1.05 N UM T ERMS O VER Pai r -wise 0.93 Q UERY S UBSET Pai r -wise 0.88 N UM C LICKS H IST 2 Histor y -b ase d 0.81 N UM Q UERY C HARS 2 Query-b ase d 0.79 S AME S ESS Pai r -wise 0.52 H AVE C O C LICK D OM Pai r -wise 0.40 N UM C LICKS H IST 1 History-b ase d 0.39 N UM Q UERIES S ESS 1 Session-b ase d 0.31 S UB Q UERY S ESS 2 Session-b ase d -0.30 N UM Q UERY T ERMS 2 Query-b ase d -0.47 N UM Q UERIES H IST 1 Histor y -b ase d -0.52 N UM Q UERY T ERMS 1 Quer y -b ase d -0.68 L EVEN D IST Pai r -wise -0.84
Table 5. The top 15 (absolut e magnitude) feature weights for the logistic regression mode l to identify queries on the same cross-session task. Features related to the first query Figure 1. Micro precision/recall curves for LR and MART for Q
UERY T ERM J AC ) are among the strongest signals. The high negative weight of the L EVEN D IST feature shows that most queries on the same task are morphologically similar. Features related to the length of each individual query in characters (N
UM Q UERY C HARS ) receive high positive weights, while features related to the length in terms (N UM Q UERY T negative weights. This suggests that long, descriptive query terms are particularly indicative of cross-session tasks. This seems reasonable since longer terms may be associated with complex information needs spanning multiple search sessions. We next consider the second problem of predicting whether a user will return to a task. Specifically: given an early-dominant task for a user and the user X  X  last query on the early-dominant task, predict whether the user will return to this task in a future session . Given a stream of user queries and a target date, features are computed up to the end of the session containing the last early-dominant query on the target date. The feature vector is assigned a positive label, if there are queries on the early-dominant task after the target date, and a negative label if there are no such queries. The nature of the TaskContinuation problem suggests that the most predictive features should reflect two aspects of a cross-session task: (i) user satisfaction w ith the presented search results, and (ii) the difficulty of the task itself. The most frequently-used feature to capture user satisfactio n is click-through rate on search results (e.g., [1]). The intuition behind this is that if a user issued a series of queries and clicked on at least one search result for most or all of these queries, she is likely to have obtained useful information that allowed her to make progress on the task and, hence, is less likely to return to it. Similarly, the dwell time on results has been shown to reflect satisfcation (e.g., Fox et al. [12]). The difficulty of the task can be reflected by several patterns of user behavior including the number of queries issued, the time between successive queries, et c. [3][17]. Individual query features, reflecting these interaction patterns, are computed at different levels of granularit y (ranging from the session where a query occurred to the entire search history of a user). Features summarizing the user X  X  history w ith the early-dominant task are also included. These additional feat ures are shown in Table 6. The features in Tables 3 and 6 are used together to predict whether the user will return to the early-dominant task. As a baseline (BASE), we use logi stic regression to learn a model using the number of queries in the user X  X  history before the cutoff date (N UM Q UERIES H IST ). This is a reasonable baseline under the assumption that an intelligently-chosen threshold applied to the level of user activity in a short window of time around the task could provide an accurate prediction of task continuation. Experimental results for predicting whether the user will return to the early-dominant task with features specified in Tables 3 and 6 on three data sets are reporte d in Table 7. Several major conclusions can be drawn from Tabl e 7. First, again we see that both classifiers improve over the baseline in all datasets and perform similarly to each other overall. Additiona lly, recall and precision substantially decrease when moving from a smaller balanced dataset ( 3k ) to a larger unbalanced one with more negative examples ( 10k ). However, both LR and MART still classify a large number of negative examples correctly, as evidenced by the fact that the accuracy increases for both classifiers. Next, recall significantly improves for both classifiers on manually corrected labels, which can be attributed to the fact that this data set is less sparse . There are more positive examples, because human annotators assign the missing early-dominant task labels to some queries. Precision-recall curves for the classifiers for TaskContinuation are shown in Figure 2 (aggregated over all test splits). Performance at default thresholds is indicated by markers. Both models perform very similar for each dataset. There are some differences across datasets, with overall performance being worse on the 10k set. MART has a slight advantage over LR for the low recall/high precision region of the curv es for the human dataset. Weights of the LR model for predicting whether the user will return to the early-dominant task, trained on the editorially-corrected data set, are summarized in Table 8. From the table, it appears that the most important feature is whether the query has ever occurred in the user X  X  history (S AME Q UERY identical queries were labeled as being from the same task, both automatically and by human labe lers. The importance of this feature is consistent with prev ious research that suggests re-finding is very common [32]. Although most of the early-dominant tasks are informational (as described in 4.2.3), queries are sometimes repeated as part of such tasks. Features of the user X  X  history w ith the dominant task were also important. If many of their past queries (N UM D OM or a high proportion of them (P CT D OM Q UERIES H IST ) were related to the dominant task, they appeared particularly likely to return to the task again at a later date. This suggests that intense interest in a topic at one point in time is likely to lead to returning to the task at a later point (during the week). In the previous section we observed that past queries on the dominant task could be identified in part by complex queries. Here again we see that the complexity of the user X  X  dominant information need, as indicated by the high weight on longer queries (N UM Q UERY C HARS ) and deeper examination in result lists (N
UM T OP 10C LICKS ), suggests that it is more likely that the user will return to the task. The predictive value of N UM T OP provides evidence to support similar claims by Donato et al . [9]. Features related to the user X  X  inte nsity of search engine use (e.g., A
VG I NTER QT IME H IST , N UM S ESS H IST ) are among the most important. This is not surprising, since people who search more are more likely to search again on all tasks, including both the early-dominant task and other tasks. However, not only the absolute frequency of searches, but also the deeper engagement with past search results (N UM D WELL 30H IST ) appear to be important, suggesting that people w ho use search deeply may also use search for more extended tasks. As the importance of a deeper understanding of user search behavior continues to grow, it becomes necessary to develop models that consider complex long-term information needs and effectively incorporate them into existing search engine infrastructures. In this work, we introduced and addressed the two problems in the context of analysis of cross-session search tasks: (i) identifying queries from earlier sessions on the same task, and (ii) predicting whether a user will return to the same task during a later session, formulating both pr oblems as supervised machine learning tasks. We proposed a method for creating a semi-automatically labeled data set that can be used for both problems and developed feature sets, tailo red for each of the individual problems. Experimental results using two classifiers (logistic regression and MART) for both pr oblems indicate that we can effectively model and analyze cross-session information needs. Our research is an important first step in helping searchers more effectively manage long-term in formation needs. Knowledge of previous user queries on the same long-term task enables a search engine to provide support for task resumption. For example, if it is known that a user has previously been undertaking a vacation-planning task and has issued que ries about airline tickets, S AME Q UERY H IST Histor y -b ase d 1.11 N UM S ESS H IST Histor y -b ase d 0.60 N UM D OM Q UERIES H IST Histor y -b ased ( Table 6 ) 0.39 A VG I NTER QT IME H IST History-b ased (Table 6) 0.24 F REQ D OM Q UERIES H IST Histor y -b ased ( Table 6 ) 0.24 N UM D WELL 30H IST Histor y -b ased ( Table 6 ) 0.22 N UM Q UERY H IST History-b ase d 0.21 N UM T OP 10C LICKS Quer y -b ase d -0.16 A VG I NTER QT IME S ESS Session-b ased ( Table 6 ) -0.17 N UM C LICKS H IST History-b ase d -0.18 N UM Q UERY C HARS Quer y -b ase d -0.21 S UB Q UERY H IST Histor y -b ase d -0.23 S UP Q UERY S ESS Session-b ase d -0.40 S UP Q UERY H IST Histor y -b ase d -0.40 S UB Q UERY S ESS Session-b ase d -0.49 whenever the user comes back to this task, a search engine can show pertinent updates in the time since the last query (e.g., ticket price drops) or suggest queries to re-find useful past results. By using a model that can accurately predict continuation of a cross-session task, a search engine can determine whether it is necessary to retain the task context, start monitoring Web content (e.g., during Web crawls or others X  quer ies) for information pertaining to the task, and use the task mode l for query suggestions or search result suggestions. There are several directions for future work, including using richer prediction models and alternative feature sets, exploring new prediction and classification pr oblems in the context of cross-session information needs, and in corporating our models into commerical search engines. [1] E. Agichtein, E. Brill and S. Dumais. Improving Web search [2] A. Aula, N. Jhaveri and M. K X ki. Information search and re-[3] A. Aula, R. M. Kahn and Z. Guan. How does search [4] D. Beeferman and A. Berger. Agglomerative clustering of a [5] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis and S. [6] H. Cao, D.H. Hu, D. Shen, D. Jiang, J.-T. Sun, E. Chen and [7] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen and H. Li. [8] Y.-S. Chang, K.-Y. He, S. Yu and W.-H. Lu. Identifying user [9] D. Donato, F. Bonchi, T. Chi and Y. Maarek. Do you want to [10] D. Downey, S. Dumais, D. Liebling and E. Horvitz. [11] S. Dumais, G. Buscher and E. Cu trell. Individual differences [12] S. Fox, K. Karnawat, M. Mydl and, S. T. Dumais and T. [13] J. Friedman, T. Hastie and T. Tibshirani. Additive logistic [14] A. Hassan, R. Jones and K. Klinkner. Beyond DCG: User [15] D. He, A. G X ker, and D.J. Ha rper. Combining evidence for [16] R. Jones and K. Klinkner. Beyond the session timeout: [17] M. Kellar, C. Watters, and M. Shepherd. A field study [18] U. Lee, Z. Liu and J. Cho. Au tomatic indetification of user [19] J. Liu and N.J. Belkin. Personalizing information retrieval for [20] B. MacKay and C. Watters. Exploring multi-session Web [21] Q. Mei, K. Klinkner, R. Kumar and A. Tomkins. An analysis [22] L. Mihalkova and R. Mooney. Learning to disambiguate [23] D. Morris, M. Ringel Morris a nd G. Venolia. SearchBar: A [24] B. Piwowarski, G. Dupret an d R. Jones. Mining user Web [25] B. Piwowarski and H. Zaragoza . Predictive user click models [26] F. Radlinski and T. Joachims. Query chains: Learning to rank [27] F. Radlinski, M. Szummer and N. Craswell. Inferring query [28] C. J. van Rijsbergen. Information Retrieval . Butterworths, [29] D.E. Rose and D. Levinson. Understanding user goals in [30] X. Shen, B. Tan and C. Zhai. Context-sensitive information [31] B. Tan, X. Shen and C. Zh ai. Mining long-term search [32] J. Teevan, E. Adar, R. Jones and M.A.S. Potts. Information [33] J.-R. Wen, J.-Y. Nie and H.-J. Zh ang. Clustering user queries [34] R.W . White, P. Bailey and L. Chen. Predicting user interests [35] R.W. White and S.M. Drucker. Investigating behavioral [36] Y.Yang and Z. Liu. A re-examination of text categorization 
