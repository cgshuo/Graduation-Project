 As an important statistical network model, stochastic blockmodel (SBM) [1] enables us to reasonably decompose and then properly analyze an exploratory network with zero prior information about its intrinsic structure. That is, we believe, the most attractive merit of the model. Formally, a standard SBM is defined as a triple ( K,, X  ), where K is the number of blocks, is an K  X  K matrix in which  X  ql denotes the probability that a link from a node in block q connects to a node in block l ,and  X  is an K -dimension vector in which  X  k denotes the probability that a randomly chosen node falls in block k .
SBM is able to approximate any mixture patterns of assortative and disas-sortative structures ubiquitously demonstrated by the real-world networks, once it is properly parameterized through fitti ng observed networks. Besides, SBM is used either as a generative model to synthe size the artificial networks containing assortative communities, disassortative multi-partites and arbitrary mixtures of them, or as a prediction model to predict missing and spurious links. Therefore, SBM has attracted much attention of researchers from the domains of statistics and machine learning since it was firstly proposed by Fienberg and Wasserman in 1981[1]. So far, various extensions of SBM have been proposed to address the oriented tasks of network analysis, such as, multiple roles SBM [2], overlap-ping SBM [3], mixture SBM [4], scale-free SBM [5], hierarchical SBM [6], among others.

Although SBM has demonstrated superiority in structure analysis, the in-tractable time complexity of learning severely limits the model to those ap-plications just involving very tiny networks. For the current available learning algorithms, given the number of blocks, K , that means we take no account of model selection, the time complexity is at least O ( K 2 n 2 )where n denotes the number of the nodes. Otherwise, it will get much more, say O ( n 5 ). In practice, if one uses conventional computers, given K , the algorithms just efficiently deal with the networks with at most thousands of nodes, otherwise at most hundreds of nodes, far from the scales of most real-world networks we are interested in. For most of real-world networks, usually, we have no priori knowledge about them, in this case, only the SBM learning algorithms with model selection pro-vide us with the real powerful tools since it can automatically determine the  X  X rue X  number of blocks. However, the prohibitive time complexity hinders us from effectively analyzing large networks with thousands of nodes.
 To address this issue, on one hand, we present a fine-gained SBM named FSBM, to capture more details of networks, and on the other hand, propose a corresponding fast learning algorithm with the capability of model selection called FSL. The FSL ingeniously combines the Component-wise EM(CEM) with Minimum Message Length(MML) to achieve simultaneous rather than alterna-tive execution of model selection and para meter estimation, being able to smartly select a  X  X ood X  model from a huge problem space consisting of entire candidate models with a significantly reduced computational cost. To the best of our knowl-edge, this is the first effort in literature t o propose a parallel learning process of SBM.

The rest of this paper is organized as follows: Section 2 reviews the related works. Section 3 presents the fine-gained SBM and its learning method. Section 4 validates proposed models and algorithms as well as demonstrates their appli-cations. Finally, Section 5 concludes this work by highlighting our contributions. SBM learning has two subtasks: to l earn the parameters of model (  X  ,  X  )andto determine the number of the blocks ( K ), corresponding to parameter estimation and model selection, respectively. Mode l selection aims at selecting a model with a good tradeoff between description preci sion and model complexity. As we know, the complexity of a model is determined by the number of parameters in the model. In this sense, the model selection of SBM is actually the determination of a reasonable K , the number of blocks, since the quantity of parameters of SBM can be represented as a function of K .

Most existing algorithms adopt EM or variational EM to estimate the param-eters of SBM [2 X 4, 7, 8] in which SBM utilizes a latent variable Z to indicate the group to which a node belongs. The time complexity of such methods is at least O ( n 2 K 2 ). Currently, the available model selection methods applied to SBM fall into three categories including cross-validation(CV), bayesian-based criteria and minimum description length(MDL). The CV has been rarely used in the SBM learning algorithm since its extremely high computation cost [4]. According to the adopted approximation techniques, BIC, ICL and variation-based approxi-mate evidence are thr ee main bayesian based criteria. Airoldi et al. [4] used BIC to select optimal multiple role SBM. Daudin et al. proposed the SICL algorithm which adopted the ICL to select model [9]. Hofman et al. proposed the VBMOD algorithm which adopted variation based approximate evidence for model se-lection [10]. Latouche et al. proposed SILvb algorithm which adopted ILvb, a newly model selection criterion [11], which has the best performance among the current criteria by now. The MDL is the criterion derived from the information theory and formally coincides with the BI C. Very recently, Yang et al. proposed the GSMDL algorithm that adopted the MDL for model selection [6].

For the above criteria except CV, both of them naturally require that corre-sponding learning algorithms have to adopt a serial learning strategy. That is to say, the learning process will include the following three steps: firstly, to esti-mate the parameters of each model in mo del space, then to evaluate the learned model by a predefined certa in criterion, finally to sel ect the model with the best evaluation value as the optimal model. H ence, such a serial learning strategy requires estimating and evaluating each model in the model space even if the models in question are  X  X ad X . This leads to a extremely high time complexity since such strategy needs to estimate parameters not only for  X  X ood X  models but for  X  X ad X  models. In general, the time complexity of the learning strategy, such as SICL [9], SILvb [11] and GSMDL [6], require at least O ( n 5 ).
Recently, Hofman and Wiggins propos ed a VBMOD algorithm with time com-plexity O ( n 4 ) by reducing the quantity of parameters from K 2 + K to K +2. So far, the VBMOD still remains the lowest time complexity among all available SBM learning algorithms with the capability of model selection. However, its low time complexity is obtained at the expense of flexibility. That is, the strategy of parameter reduction will restrict the VBMOD to deal with the networks with community structures.

Figueiredo et al. [12] ever applied the MML to gaussian mixture model, and their experiments showed that the accu racy of the MML outperforms that of the MDL/BIC, LEC or ICL criteria. Although their method was proposed towards to feature vector space and is not suitable for processing networks, the rationale behind it inspires us to propose a new SBM learning algorithm by designing an parallel integration of model selection and parameter estimation.
 3.1 Fine-gained Stochastic Blockmodel As the standard SBM only uses the block connection matrix to characterize the homogeneity and heterogeneity of links between nodes, it is difficult for SBM to capture more detailed structural information. To address this issue, we relax the parameter into the two parameters  X  and , which respectively denote the connection probability from blocks to nodes and the connection probability from nodes to blocks. And based on the relaxation, we proposed the fine-gained stochastic blockmodel, FSBM.

Let N =( V,E ) be a directed and binary network where V ( N ) denotes the set of nodes and E ( N ) denotes the set of directed edges. Let A n  X  n be the adjacency matrix of N where n denotes the number of nodes and A ij = 1 if there exists an edge from the node i to the node j ,otherwise A ij = 0. In the case of undirected network, supposing there are two directed edges between nodes and A ij = A ji .
The FSBM is defined as X =( K,Z, X , X , ), where, K is the number of the blocks, Z is one n  X  K matrix in which z ik denotes the block k to which the node i belongs,  X  is one K -dimension vector in which  X  k denotes the probability that a randomly chosen node falls in block k ,  X  is one K  X  n matrix in which  X  kj denotes the probability that a link from a particular node in block k connects to vertex j , is one K  X  n matrix in which  X  kj denotes the probability that a link from node j connects to a particular vertex in block k .If N is undirected network, then  X  = . The block matrix of the FSBM can be obtained as follows: where D = diag ( n X  ). For undirected network, we have 1 = 2 = .There-fore, the standard SBM is a specific case of the FSBM.

The log-likelihood of the observed network N can be written as follows: where, f ( x,y )= x y (1  X  x ) (1  X  y ) .

The log-likelihood for complete data can be written as follows: log P ( N,Z |  X , X , )= 3.2 Fast SBM Learning Method The proposed fast SBM learning method, FSL, ingeniously combines the CEM algorithm [13] with the MML [12] together to achieve the parallel learning of parameter estimation and model selection. The FSL first obtains the estimated parameter value of one block by the CEM algorithm, then evaluates the block in terms of the MML. The bad block evaluated by the MML, that is the ex-istence probability is zero, is directly annihilated and is not estimated in the next iteration. And so on, until the conv ergence. In the process, the sequential updating approach of the CEM algorithm and the MML directly evaluating one block provide the support of the parallel learning. Finally, parameters estimation and model selection are parallelly implem ented in a time convergence process. In contrast to the serial learning algorithm, the FSL directly finds the  X  X ood X  mode in the model space and effectively reduces the computational cost by preventing the algorithm estimating the parameters of the  X  X ad X  model.
 Parameter Estimation. Although the CEM algorithm has almost the same time complexity as the EM algorithm, its faster convergence can reduce the time cost, but the major contribution is that the sequential updating approach of the CEM algorithm provides a smart framework for the parallel learning of parameter estimation and model selection. The CEM algorithm considers the decomposition of the parameter vector, and updates only one block at a time, letting the other parameters unchanged. The E-step and M-step are as follows:
E-step : Given the observed network N and h t  X  1 where h and t respectively denote the model parameters (  X , X , ) and the current iteration step, compute the conditional expectation of complete log-likelihood, i.e. Q function. where  X  ik = E [ z ik ] denotes the posteriori probability of block k to which the node i belongs according to the model h t  X  1
M-step : Update the parameters of the current block according to Eq. 5-7 which are obtained by maximizing Eq. 4: where k =( t./K max ) +1, K max is the number of the blocks and ./ denotes modulus operator. In the current step t , only the parameters of the k -th block are updated according t o Eq. 5-7, respectively.
 Model Selection. The MML is derived from information theory and the ra-tional behind MML is that the shorter code the data has, the better the data generation model is. The particular MML criterion is as follows: where N denotes the observed network, h denotes the model parameters, c is
Since FSBM contains the latent variable Z , the information matrix of FSBM can not be obtained analytically. We adopt the information matrix of the com-plete data log-likelihood, I c (  X , ), and assume the parameters of the blocks as a priori independent and also independent from the parameter  X  .Foreach factor p (  X  k , k )and p (  X  1 ,..., X  k ), we adopt the noninformative Jeffrey X  priori. Consequently, (8) becomes where (  X  ) is the cost function, K is the number of the blocks, c is the number of parameters specifying each block, h denotes the parameters (  X , X , ).
When  X  k is zero, (9) will be nonsense, howev er, from the view of data coding, the parameters of zero-probability block do not contribute to coding-length [12]. Let k nz is the number of non-zero probability blocks, the cost function becomes
Minimizing the cost function (10) with respect to h constitutes the solutions of parameters estimated. As we can see, to minimize (10) with respect to  X , X  is the same as to minimize the -Q function since the terms except  X  log p ( N | h ) is dropped, and the difference is  X  .The  X  obtained by differentiating (10) with k nz fixed is given as follows: where the  X  ik are given by the E-step, c is the dimension of parameters. Noting in (2) that any block for which 9  X  k = 0 does not contribute to the log-likelihood.
We can see that (11) provides us an approach to evaluate the blocks, that is if 9  X  k is zero in the current step t , the block k is regarded as a  X  X ad X  block and may directly annihilate it. Based on the property of the MML and the sequentially updating of the CEM algorithm can commonly achieve the parallel learning of parameter estimation and model selection.
 Algorithm Description and Complexity Analysis. A detailed pseudocode description of the FSL is listed in Table 1. After convergence of the CEM, there is no guarantee that a minimum of ( h,N ) has been found since the block an-nihilation in (11) does not consider the additional decrease in ( h,N )causedby the decrease in k nz . According to [12], we check if smaller values of ( h,N )are achieved by setting to zero blocks that were not annihilated by (11). To this end, we simply annihilate the least probable block and rerun CEM until convergence.
For the FSL, calculating the posterior of the latent variable Z and estimating parameters (  X ,, X  ) are time-consuming, which dominate the time complexity of the whole computing process. We can analyse the time complexity by the detailed algorithm steps in Table 1. Given K , the time complexity is O ( In 2 K ), where I is the iterative steps. When K is unknown, in the worst case, the time complexity is O ( In 4 ). Among the available SBM learning algorithms with model selection, the FSL has the same low time complexity as the VBMOD, far less than the other algorithms, such as GSMDL [6] O ( n 5 ), SICL [9] O ( n 5 ), SILvb [11] O ( n 5 ), MBIC[2] O ( n 7 ), Shen [7] O ( n 6 ) and so on. Noting that the FSL can analyse the networks with various structures, such as bipartite, multipartite and mixture structure, but the VBMOD just analysing the networks with the community. In this section, we validate the proposed algorithm on the synthetic networks and real-world networks, and make comparisons with the other algorithms with model selection, which are GSMDL [6], SICL [9], VBMOD [10] and SILvb [11], respectively. We also validate the generalization ability by the application of link prediction. The experiments are run on the computer with dual-core 2GH CPU and 4GB RAM, and all the programs are implemented by Matlab 2010b. 4.1 Validation on the Synthetic Networks and Real-world Networks Accuracy Validation. We use the same testing method that mentioned in [11] to validate the FSL. The SBM is used as generation model to produce three types of the synthetic networks, which respectively contain community, bipartite and multipartite structure. Each type of network is divides into 5 groups in which the number of the true blocks Q true is 3, 4, 5, 6 and 7, respectively. And each group contains 100 networks with 50 nodes which are randomly produced.
Table 2-3 respectively show the confus ion matrices of the results detected in networks with community and mixture structure. The results listed in Table 2 indicate that the FSL and SILvb are the best algorithm among the five algo-rithms, especially, when Q true is 7, they can correctly find out 68 and 83 blocks, respectively. The results listed in Table 3 indicate that the VBMOD fails to correctly find any network structure, an d the others can effectively find out the number of blocks in networks, especially, when Q true is 7, the SILvb still exhibits the best performance, the FSL following behind. Finally, we can conclude that the SILvb outperforms other existing le arning algorithms in the accuracy test. The FSL is slightly worse than SILvb, but much better than other algorithms. The VBMOD fails to analyse the networks with the other structures except the community.
 Time Complexity Validation To validate the time complexity, we use the running time metric to evaluate the time complexity of the algorithms. First, we test the running time in synthetic networks with different size. The Newman model is used to generate synthetic networks, and the parameters of model are set as follows: K =4, d =16 and z out =2. Let s in turn take value 100, 200, 300, 400, 500, 600, 700 and 800. Finally, we randomly generate eight groups of networks and each group contains 50 networks with the same size. K min and K max are respectively set 1 and 10.
 Fig. 1(a) shows the results of the average running time of five algorithms. As we can see, the running time of the FSL is obviously much less than other four algorithms. The VBMOD closely follows behind the FSL, however, the VBMOD achieves its low time complexit y at the cost of accuracy since its too few parameters, k +2, enable it to only analyse the networks with the community. Noting that when the number of nodes is 3200, on average the FSL only takes 32 s , the VBMOD, MSMDL, SICL and SILvb take 92 s , 7513 s , 153901 s and 153244 s , respectively.

Then, we demonstrate how the scale of model space affects computational cost using the football network with 115 nodes [14]. We fix K min =1 and let K max in turn take the value 10, 20, 30, 40, 50, 60, 70, 80, 90 and 100. Fig. 1(b) illustrates the relation of running time and the scale of model space. As we can see, the FSL significantly outperforms the other four algorithms.
 4.2 Generalization Ability Validation We first demonstrate the low computation cost of the FSL in the real-world net-works, and use the learned parameters to test its generalization ability according to the performance of link prediction. In the experiment, we selected seven real-world networks, which respectively are Karate network [14], Dolphins network [15], Football network [14], Polbooks network (http://www.orgnet.com), Jazz network [17], Usair network [16] and Metabolic network [17].

Let K min =1 and K max take the number of nodes. The running time of the five algorithms is listed in Table 4. We can see that the running time of the FSL is significantly much lower than other four algorithms, the following is the VBMOD, and the SICL is the worst. Noting that the FSL only consumes 412.453 s , but the VBMOD is 1567.97 s in Metabolic network with 453 nodes, the running time of other algorithms are far more than that of the FSL, even fails to deal with the networks.

Then we evaluate the generalization ability of the models and algorithms in terms of the performance of link prediction. We also make comparisons with the CN algorithm [18] which is usually used as the baseline algorithm of link prediction. The AUC metric [18] is used to evaluate the performance of the algorithms. The SBM-based approaches to link prediction predict the missing links according to the learned parameters value related to connection probability.
We construct the data set by the following way: randomly pick the 10% of edges as test set from the network and the remaining 90% of edges as training set. For each network, we randomly sample 20 times and generate 20 groups of data.

The mean and standard deviation of AUC value of the prediction results are listed in Table 5. As we see, the results indicate that the FSL has the expected best performance among the six algorithms, and the performance of the VBMOD is the worst since the model only uses two parameters to describe the network structure so many information is missing. The main reasons are that the FSBM can capture the more detailed information of network structure than other SBMs, and the FSL can reasonably learn the FSBM. In this paper, we proposed a fine-gained SBM and its fast learning algorithm with the capacity of model selection which adopts the parallel learning strategy to reduce the time complexity. To our best knowledge, this is the first time that the parallel learning strategy is proposed and applied to the SBM learning algorithm. We have validated the proposed learning algorithm on the synthetic networks and real-world networks. The results demonstrate that the proposed algorithm achieves the best tradeoff betw een effectiveness and efficiency through greatly reducing learning time while pres erving competitive learning accuracy. In contrast to existing learning algorithms with model selection just dealing with networks with hundreds of nodes, the proposed algorithm can scale to the networks with thousands of nodes. Moreover, it is noteworthy that our proposed method demonstrates the excellent gene ralization ability with respect to link prediction.
 Acknowledgments. This work was funded by the Program for New Century Excellent Talents in University under Grant NCET-11-0204, and the National Science Foundation of China under Grant Nos. 61133011, 61373053, 61300146, 61170092, and 61202308.

