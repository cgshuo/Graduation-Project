 Electrical Engineering and Computer Science Department, University of Toledo, Toledo, OH, USA 1. Introduction
Machine learning research draws upon localized theoretical insight, empirical techniques, methodolo-gies and empirical simulation studies to accomplish its goals. One highly promising machine learning algorithm is the ensemble classi fi ers, and yet the construction of classi fi cation ensembles poses serious challenges, as evident from the lack of a comprehensive and unifying theoretical explanation [22] and the many different types or designs developed to date [29]. Current machine learning body of knowledge, formed in good part through empirical simulation studies and through theoretical insight, suggests a set of principles for classi fi er design and development although much still depends on heuristics and ad hoc approaches [26]. In the absence of a commanding set of design principles, substantial effort is expended for searching and fi ne-tuning during the development phase that strives to pair a classi fi er with a dataset. Creation of design principles based on either heuristics or theoretical fi ndings, which are applicable for the development of machine learning classi fi ers for more robust and enhanced performance, is desirable. This paper proposes a heuristic-based design principle for hybrid classi fi er ensembles to enhance their performance. In more speci fi c terms, the proposed design heuristic is expected to improve a hybrid classi fi er ensemble X  X  performance for a larger or more comprehensive set of problem domains or data sets.

A general discussion on classi fi er ensembles and associated design heuristics, methods of diversity creation, and the dichotomy of global and local learners is discussed next. This is followed by a presentation on the de fi nition and incorporation of the propos ed heuristic into the design of a hybrid classi fi er ensemble. A simulation study that strive s to expose the u tility of the proposed heuristic is performed in the subsequent section. The fi nal section is the discussion of conclusions. 1.1. Classi fi er ensembles and design heuristics
Four main approaches to building ensembles are described in Kuncheva [26]. They are divided by the level of the ensemble structure, which are at the dataset, feature, classi fi er, and combination (or architecture) levels. The data level approachto building an ensembleusesa single learning algorithm with different subsets of the original dataset to train different base classi fi ers. Popular ensemble techniques built at this level are Bagging [8] and AdaBoost [17]. At the feature level, different feature subsets can be used to train the base classi fi ers. The Random Decision Forests, proposed by Ho, is perhaps the seminal work in this are a [21]. In this method, multip le decision trees are built on random subspaces of the features. Although building ensembles at the data and feature levels actually creates different base classi fi ers, those described at classi fi er level are explicit designs of base classi fi ers. The basic goal of design at the classi fi er level is to induce an accurate and diverse set of base classi fi ers, and there are two versions, namely measurement-based and ad hoc. The measurement-based classi fi er-level design approach takes an active role to selecting classi fi ers that increase diversity, while the ad hoc methodology uses a passive heuristic based on the user X  X  experience [27]. The fourth approach to ensemble design is at the combination level, some of the most common implementations include MultiScheme for selection, Voting, Stacking [34], and Grading [30] for fusion. 1.2. Diversity of base classi fi ers
In almost all cases, ensemble classi fi erdesign techniquesstrive to differentiate base classi fi ers. There is a consensus that diversity provides possibly valuable information and understanding about an ensemble X  X  performance [27,32]. There are three ways in which diversity can be created among the base classi fi ers of an ensemble [10]. These three techniques are based on data manipulation, homogeneous diversity creation, and heterogeneous diversity creation. In data manipulation, multiple classi fi ers are generated from a single learning algorithm through variations of the training data (e.g. different samples of instances or different samples of features). If the learning algorithm does not perform well for a given dataset, then the performance of the overall ensemble is adversely affected. The simplicity of this method makes it the most widely investigated diversity creation method [29].

For homogeneous diversity creation, multiple classi fi ers are generated from a single learning algorithm through variations of the parameter s (e.g. neural networks with different initial weight values). As with data manipulation ensembles, the performance of homogeneousensembles may suffer from being limited to a single learning algorithm.

Finally, for heterogeneous diversity creation, multiple classi fi ers are generated from two or more learning algorithms, which may entail, as examples, C4.5, Bayesian belief network, arti fi cial neural network, etc. Certain learning algorithms may be experts in an instance space, while others are possibly inexpert. Multiple learning algorithms help to protect the ensemble from being burdened by poor performance of any single one. However, execution of hybrid ensembles with heterogeneous diversity through two or more learning algorithms is more costly and challenging than single learning algorithm alternatives due to the fact that each learning algorithm has a unique training pro fi le, and the effect of associated tunable parameters must be understood and controlled for a given problem domain. 1.3. Global and local learners
In ensemble classi fi ers, machine learning algorithms are leveraged to instantiate the collection of base classi fi ers, and as such serve a very important role for the design. Understanding their fundamental performance attributes is critical for a successful design effort. Conceivably, one useful characteristic is the global-local property of machine learners.
 An intuitive and comparative explanation of how global and local learners function is provided in Mitchell [28]. When all training instances are considered during classi fi cation of a query instance, the learner is termed as global . When only near (in some distance metric sense such as Euclidean) training instances are considered during classi fi cation of a query instance, the learner is called local . Global learners estimate a single target function for the entire instance space, while local learners estimate target functions locally and often differently for each query instance.

Multilayer perceptrons and decision trees are considered as global learners while the instance-based methods and radial basis functions are labeled as local learners [28,33]. Function-based (e.g. logistic regression) and rule-based (e.g. PART) methods are also categorized as global [33]. Another learning algorithm, support vector machine (SVM), is known to have properties of both global and local learners, depending on the kernel parameter selected [7].

There are advantages and disad vantages for each type of learning al gorithm, and the ir ability for generalization (i.e. their performance) depends to an extent on the problem at hand. Generally, global learners do not respond well to isolated data points  X  those points in a sparsely distributed area [20]. That is, they attempt to have a model that satis fi es the majority of points while paying little attention to outliers (similar to how linear regression works). Alternatively, local learners are better for handling the isolated points since their generalization is instance-based. However, if the target function only depends on a few of the many available attributes, then the instances that are most  X  X imilar X  may actually be a large distance away [28]. One can further argue that these two types of learners could behave in a  X  X omplementary X  way: when one fails, the other may succeed since it possibly views the problem in such a different manner. Lastly, the co-existence of a global learner along with a local learner within the same ensemble framework introduces heterogeneous diversity, which is widely considered as an essential element for performance enhancement. 2. Global-local heuristic and its realization
Inspired by distinct performance and operational characteristics of global and local learners in conjunc-tion with the performance enhancing impact of heterogeneous and homogeneous diversities, we propose a design heuristic for hybrid classi fi cation ensembles. The heuristic, called  X  X lobal-local heuristic X  is stated as follows:  X  X he composition of base classi fi ers for a hybrid ensemble should entail both local and global learners, which should be leveraged to induce both heterogeneous and homogenous diversity. X  We hypothesize that this design heuristic will enhance the performance of a hybrid classi fi cation ensemble.
In a hybrid classi fi cation ensemble that is based on the proposed design heuristic, heterogeneous diversity is incorporated by the inclusion of both global and local learners, while homogeneous diversity is included through manipulation of parameter values of each type of learner. This heuristic can be transformed into a design principle: a hybrid classi fi cation ensemble can be realized by incorporating both global and local learners (possibly but not necessarily in equal numbers) in its base classi fi er composition, and both heterogeneous and homogeneous diversities among its base classi fi ers.
The proposed global-local heuristic is realized or implemented through an ensemble framework, which is entitled as Global-Local Hybrid Ensemble (GLHE). The generic architecture of the GLHE design is shown in Fig. 1. There are n base-classi fi ers from a global learner with different parameterizations. Likewise, there are m base-classi fi ers from a local learner with different parameterizations, while noting that n and m can be different. The intrinsic difference between global and local learning algorithms ensures a high level of heterogeneous diversity. Additionally, instantiation of multiple classi fi ers from each learner provides the every single learner the opportunity to explore their respective region of the hypothesis space and thus creates homogeneous diversity.

The important distinction between existing techni ques in the literature and the heuristic proposedin this paper is the explicit use of global and local learners within a hybrid ensemble framework that also boasts both heterogeneous and homogeneous diversities. The existing techniques of design at the classi fi er level is to induce an accurate and diverse set of base classi fi ers, and there are two versions, namely measurement-based and ad hoc, and neither of which explicitly promotes the use of both global and local learners in the base classi fi er composition. Deliberate combination of the two types of diversities is not common in the literature either, although some have experi mented with it either directly or indirectly [5, 11,13,35]. We validate the utility of the pr oposed ensemble design heuris tic through a simulation study in the next section. 3. Simulation study
A simulation study is conducted t o assess and evaluate t he utility of th e proposed heuristic as imple-mented within the global-local hybrid ensemble (GLHE) framework in Fig. 1. The goal of the simulation study is to pro fi le the performance enhancement for a hybrid classi fi er ensemble due to the proposed heuristic for a large number of data sets.
 An initial empirical study is used to identify a speci fi c instance of the global-local hybrid ensemble. There are two important elements in the composition of a hybrid ensemble: the type of base learners (decision tree, neural net, Bayesian net, etc.) and the ensemble architecture (voting, grading, etc.). The effect of these components on the performance of GLHE is explored fi rst. Upon conclusion of this analysis, the effectofco-presence ofglobaland locallearners alongwith heterogeneousand homogeneous diversities on the ensemble performance is explored, which is the focal point of interest in this study. 3.1. Methodology All simulations are performed with the open source Weka software, version 3.5.8 [33], using the Large Experiment and Evaluation Tool (LEET) as a front-end [4]. For data sampling and to obtain an estimate of performance, we use the 10-fold cross validation. Prediction accuracy is used as the measure of performance, since it is possibly the most general and common metric in the literature [12]. We consider 46 publically available datasets from the UCI Machine Learning Repository [2]. They are listed, along with their characteristics, in Table 1. This collection of datasets was obtained as a union of datasets that appeared in three prominent studies in the literature [3,25,30].

Arguably, comparing classi fi ers does not satisfy conditions for parametric tests, and instead it may be more appropriate to use the nonparametric alternatives [15]. Since nonparametric tests rank the classi fi ers for each dataset, the important source of variations is the (independent) datasets and not the (usually dependent) samples used to calculate the accuracy. This implies that, besides obtaining a precise estimate of performance, the sampling method is irrelevant because one does not have to worry about the Type I error generated from it. That is, the variation of each sample is not important. We use the nonparametric statistical signi fi cance tests presented in Demsar [15], and applied to other existing empirical studies in the literature [23]. The fi rst procedure is the Friedman test, which is based on the average rank of each classi fi er across the datasets [18]. Iman and Davenport showed that the Friedman test is undesirably conservative, and derived a new statistic based on its value that uses the F -distribution [19,24]. The Iman-Davenport value, F F , is then used to test the null hypothesis  X  that all classi fi ers have equivalent performance. If F F is greater than the F -distribution value, then the hypothesis is rejected. When the null hypothesis is rejected, the post-hoc Bonferroni-Dunn test is conducted, in which a control classi fi er is compared to all others in the group [16]. The post-hoc Bonferroni-Dunn test states that performances of the control and another classi fi er are signi fi cantly different if their average ranks differ by at least the critical difference ( CD ). With respect to the study reported in this paper, unless otherwise stated the control is assumed to be GLHE. 3.2. Effect of ensemble architecture on GLHE performance In this section we explore the effect of popular ensemble frameworks or architectures, namely Voting, Grading, and StackingC, on the performance of global-local hybrid ensemble (GLHE). The parameter settings for these are average of probabilities, linear regression as meta classi fi er, and IBk-3 as meta classi fi er, respectively. Decision tree and nearest neighbor classi fi ers are chosen as the global and local learners, respectively, given the relatively low computational complexity and comparatively good proven performance of these algorithms. Accordingly, the implementation of GLHE uses the global learner J48, a Java port of the C4.5 decision tree classi fi er in Weka, and the local learner IBk, a nearest neighbor classi fi er. The categorization of these learners as global or local is consistent with the literature [28].
Inclusion of these two learners, one global and one local, consequently introduces heterogeneous diversity among base classi fi ers of the ensemble. The homogeneous diversity is incorporated into the design of GLHE as follows. The GLHE implementation for the simulation study utilizes three base classi fi ers from each learner type, which allows homogeneous tendencies to be incorporated while also keeping the total number of base classi fi ers at a reasonable level with respect to the computational complexity consideration. Base classi fi ers from each type of learner are realized by varying the values of relevant parameters that affect the instantiation through training. Speci fi cally, for J48, the type of pruning is changed: values considered are standard pruning, reduced error pruning, and unpruned. For IBk, distance weighting is assigned a value of  X 1/distance X , and the number of nearest neighbors is changed: values of 1, 5, and 10 are explored. Otherwise for those parameters not speci fi ed explicitly herein, Weka defaults are used. The nonparametric statistical signi fi cance tests are applied to simulation results of the three versions of GLHE ensemble (Voting, StackingC, and Grading) on the 46 datasets from the UCI machine learning repository.

Figure 2 shows a graph of the average ranks, R , for the prediction accuracy of each GLHE ensemble, where N is the number of datasets and k represents the number of ensembles. The Iman-Davenport test concludes a statistically signi fi cant difference among the performances of these ensembles. Next, the post-hoc Bonferroni-Dunn test is applied to determine which ensembles are actually different. StackingC is used as the control for the test. High and low thresholds are placed at a distance of CD away from StackingC X  X  average rank in Fig. 2. It is noted that a rank that is at or above the  X  X igh X  threshold is statistically worse than the control and conversely a rank that is at or below the  X  X ow X  threshold is statistically better than the control. The actual values of the thresholds are also provided in the key of the same fi gure.

The graph in Fig. 2 shows that both Voting and Grading are at the CD threshold, so StackingC X  X  improvement over their performance is statistically signi fi cant. In conclusion, it is reasonable to state that based on the statistical signi fi cance tests, StackingC has the best prediction accuracy. Thus, in the remainder of this paper we employ the StackingC in the GLHE implementation whenever there is a choice to be made in that respect. 3.2.1. Effect of learning algorithm selection for base classi fi ers on GLHE performance
In this section, the effect of machine learning algorithm selection for base classi fi erson the performance of global-local hybrid ensemble (GLHE) is investigated. The GLHE implementation in the previous section uses a decision tree and a nearest neighbor classi fi er for global and local learners, respectively. As an alternate implementation of GLHE, multilayer perceptron (MLP) feedforward and radial-basis function (RBF) neural networks are used as the global and local learners, respectively. The ensemble framework is chosen as StackingC, while the parameter settings for each base classi fi er are Weka defaults unless otherwise stated. A validation set size of 34% is employed for this phase of the study due to potentially very high cost of training a neural network based classi fi er through 10-fold cross-validation, particularly in Weka environment. For MLP, momentum parameter value is set to 0.2. MLP learner utilizes three con fi gurations as MLP-1 that has a learning rate of 0.3 and one hidden layer with 5 nodes; MLP-2 that uses a learning rate of 0.3 and one hidden layer with 10 nodes; and MLP-3 that has a learning rate of 0.7 and one hidden layer with 10 nodes. For RBF, Gaussian radial basis function is used. Similarly, RBF has three con fi gurations as RBF-1 with 2 clusters, RBF-2 with 5 clusters, and RBF-3 with 10 clusters. Both neural network based learning algorithms can be time consuming to train, especially for datasets with large instance or large attribute counts. Consequently, for this part of the simulation study, we chose a subset of ten datasets from those presented in Table 1, which are denoted with a  X  +  X  symbol next to the dataset name. It is worth noting that these ten datasets are not trivial: the collection of datasets has up to 1,000 instances and 7 classes.
 Figure 3 shows the average ranks of the alternate neural network based GLHE implementation ( X  X LHE NN X ), and the original decision tree and nearest neighbor implementation of GLHE on the 10 datasets. Although testing results indicate that there is no statistically signi fi cant difference between the perfor-mances of two ensembles, it is still reasonable to assert that GLHE NN performs better than GLHE using the same results. Selection of learners notably, and perhaps not surprisingly, has a certain degree of impact on the performance of GLHE. However, it is also important to note that both ensemble designs are guided by the global-local heuristic.

Next, traditional hybrid ensembles, e.g., those that were not designed per the global-local heuristic, are included in the simulation study for a larger and informative context of comparison. The promi-nent methodology in the lite rature for selection of m achine learners for hybrid ensembles is either measurement-based or ad hoc. GLHE is proposed as an ad hoc design, so it is compared with existing ad hoc designs. In the literature, such ensembles are commonly based on a user X  X  understanding of the clas-si fi cation problem or interaction between classi fi ers. This is exempli fi ed by Seewald and Furnkranz, who support their learning algorithm selection for base-classi fi ers of hybrid ensembles as  X  X hosen to cover a variety of different biases in an attempt to maximize diversity X  (2001). Thus, we use this approach: each base classi fi er in the traditional hybrid ensemble will be conceived to be an instantiation of a different learning algorithm. What this means is that if there are three base classi fi ers in an ensemble, then each one is instantiated from a different machine learner. The learning algorithms and their parameterizations are as follows (default Weka values unless otherwise stated):  X  J48  X  A C4.5 decision tree classi fi er.  X  IBk  X  An instance-based nearest-neighbor classi fi er (distance weighting of  X 1/distance X  and 5  X  NB  X  A na  X   X  ve Bayes classi fi er (supervised discretization).  X  PART  X  A rule based classi fi er that creates rules from partial C4.5 trees.  X  KStar  X  An instance-and entropy-based classi fi er.  X  SMO  X  A support vector machine (SVM) classi fi er with sequential minimal optimization (SMO)
Consequently, traditional hybrid ensembles with two to six learning algorithms (or also base classi fi ers in this case) are formulated within the StackingC ensemble framework. This base classi fi er count range is derived from two sources. First, the literature reports that diversity has a strong effect on performance forlessthan10baseclassi fi ers [10]. Second, the reported value, in the literature, is around 4.6 for the average number of base classi fi ers for hybrid ensembles based on our own surveys. Since the simulations are costly with respect to time, exploration of an exhaustive combination of the classi fi ers was deemed to be infeasible. Accordingly, the compositions of traditional hybrid ensembles with base classi fi er (learner) counts in the range of two to six are shown in Table 2.

Figure 3 shows the average ranks of the alternate (neural network based) GLHE implementation ( X  X LHE NN X ), the originalimplementation ofGLHE (with decision tree and nearestneighborclassi fi ers), difference among the performances of all seven ensembles evaluated. In a more focused context, it makes sense to compare the performances of three ensembles in Fig. 3, namely GLHE, GLHE NN, and Trad-6, since only Trad-6 has the same number of base classi fi ers as the two versions of GLHE. It is worth noting that Trad-6 ensemble, which also implements the global-local design heuristic with the exception of inducement of homogeneous diversity, projects much more increased heterogeneous diversity as indicated by the composition of six different base learners. These base learners are three global (J48, PART, and SVM-SMO) and three local learners (IBk, NB, and Kstar). Results show that the performances of GLHE NN and Trad-6 are at par with a slight advantage (at a statistically insigni fi cant level) to the credit of Trad-6. That is, GLHE NN with only two base learners (but six base classi fi ers) is able to approximate the performance of a much more complex Trad-6 ensemble which has six different base learners.

As the trend in performance points in Fig. 3 suggests, there is an improvement in performance as the number of base learners and associated heterogeneous diversity increases although it is not readily possible to assign credit to a particular factor for this improvement. It can be claimed that the number of base learners, or classi fi ers in this case, as well as the associated increase in heterogeneous diversity do not seem to affect the performance in one way or the other, which is explained by the fl at region (formed by Trad-4, Trad-5, and Trad-6 bars) beyond a certain number of base learners. Another comparison, the performance of GLHE NN with those of Trad-4, Trad-5, and Trad-6, indicates that the lesser degree of heterogeneous diversity in GLHE NN does not seem to lead to an adverse effect on the performance: this could be explained by the fact that GLHE NN possesses and is enhanced with the homogeneous diversity. 3.3. Global-local property and performance implications
In the simulation study reported in this subsection, we focus on observing the effect of co-presence of global and local learners on the classi fi cation performance. We design global-global and local-local hybrid ensembles, which are denoted as GGHE and LLHE, respectively, to support performance evaluation. In these ensembles, both heterogeneousand homogeneous diversity exist but only one type of learning algorithm, either global or local, is used. Furthermore, in order to eliminate a potential bias that an ensemble combination may have for an ensemble architecture, all three prominent options, namely Voting, Grading and StackingC, are evaluated. The performance and diversity of these ensembles are compared to those of GLHE on the 46 UCI datasets.

The heterogeneous diversity of the GGHE and LLHE ens embles originate from t he use of two different learning algorithms for base classi fi ers, which are both global and both local, respectively. The global learning algorithms are J48 and PART in Weka terminology. The local learning algorithms are IBk and KStar. Although it was shown in an earlier section that neural network base classi fi ers boost the performance of GLHE at an appreciable level, the computational cost of training neural classi fi ers on 46 UCI datasets makes employing them impractical on the Weka platform. Accordingly, without the loss of generality, non-neural network al gorithms are used for this part of the study. The homogeneous diversity of the ensembles is achieved by var ying the paramet er settings for each learne r to create multiple base classi fi ers. As with the GLHE implementation, each learner is used to induce three base classi fi ers. The J48 and PART algorithms use variations to pruning as follows: standard pruning, reduced error pruning, and unpruned. The IBk variations have 1-, 5-, and 10-nearest neighbors. For KStar, the global blend of the entropy distance calculation is changed with the following values: 20, 50, and 80. In summary, GGHE employs three J48 base classi fi ers (versions with standard pruning, reduced error pruning, and unpruned), and three PART classi fi ers (versions with standard pruning, reduced error pruning, and unpruned), while LLHE utilizes three IBk classi fi ers (versions with 1-, 5-, and 10-nearest neighbors), and three KStar classi fi ers (versions with the global blend of entropy distance calculations with values of 20, 50, and 80).

The parameter settings for Voting, StackingC, and Grading are average of probabilities, linear regres-sion as meta classi fi er, and IBk-3 as meta classi fi er, respectively. Prediction accuracy and cpu time are monitored as performance measures as presented in Figs 4 through 7.
 The average ranks and statistical signi fi cance tests for prediction accuracies of GLHE, GGHE, and LLHE for three ensemble techniques, with GLHE as the control, are presented in Figs 4, 5, and 6 for Voting, StackingC and Grading, respectively. The average ranks of cpu execution time for the GLHE, GGHE, and LLHE StackingC ensembles are shown in Fig . 7. Since the relative r ankings and statistical signi fi cance are consistent for all three ensemble techniques, only one, which is for StackingC, is given as the representative. Simulation results in Figs 4, 5, and 6 show that, in general, GLHE has an average rank either better than or comparabl e to those of GGHE and LLHE across t he three ensemble combination techniques considered, although no statistically signi fi cant difference can be claimed. Figure 4 further shows that the performance of LLHE for the Voting ensemble is the worst, while Fig. 6 exposes that GGHE performs the worst for the Gradi ng ensemble. It is wor th reitera ting that the GLHE performance is never the worst for any of the three ensemble architectures considered. Furthermore, as Fig. 7 presents, in terms of cpu time, LLHE and GGHE are both slowe r than GLHE, the forme r being slower at a statistically signi fi cant level.

A diversity analysis for the base classi fi ers of all three ensembles is done to extend the insight into the respective performance pro fi les. Three diversity measures are considered, including the pairwise disagreement measure [31], the non-pairwise entropy measure [14], and the weighted count of errors and correct predictions [1]. Figure 8 gives the average ranks and statistical test results for the pairwise disagreement measur e, and shows that GLHE has a better div ersity than both GGHE and LLHE, where the difference is statistically signi fi cant when compared to the latter. Results for the non-pairwise entropy measure in Fig. 9 indicates that GLHE has statistically better diversity than its global-only and local-only counterparts. The weighted count of errors and correct predictions average ranks are given in Fig. 10. For this measure of diversity, G LHE is statistically better than LLHE but the same as GGHE. The statistical signi fi cance tests for all three types of diversity measures suggest that GLHE X  X  base classi fi ers are more diverse than those of both GGHE and LLHE. Correlating prediction accuracy performances with diversity results appear to support the reasonably well-accepted wisdom in the machine learning literature that diversity is an important component of overall classi fi cation performance in general. 3.4. Critique of simulation study
The simulation study presented in this paper strives to explore and validate the performance implica-tions of the proposed design principle for classi fi er ensembles, entitled as global-local heuristic. The simulation study itself is limited due to the computational cost implications of a much more compre-hensive simulation undertaking that would conceivably also consider many other design criteria, such as the effect on performance that the number of base classi fi ers or learners has. For instance, there are ensemble techniques in the literature that use hundreds or thousands of base-classi fi ers, namely random forests [9] and an overproduce-and-select method [11]. However the results of the simulation study suggest that the proposed heur istic has utility and provides a de fi nite basis to further explore its promise for performance enhancement.
 4. Conclusions
A design principle for hybrid classi fi cation ensembles which is entitled  X  X lobal-local heuristic X  was proposed. The global-local heuristic envisions combining global and local learners for the composition of base classi fi ers, and concurrent inducement of heterogeneous and homogeneous diversities within a hybrid classi fi cation ensemble for performance enhancement. The proposed heuristic was put into practice through a speci fi c realization which is called the global-local hybrid ensemble (GLHE). A simulation study of GLHE was performed on 46 UCI machine learning repository datasets to explore the performance enhancement facilitated by the pr oposed heuristic through the Weka machine learning toolset. An analysis of performance metrics, including prediction accuracy, cpu execution time, and diversity among the base classi fi ers suggest that, in the design of hybrid classi fi er ensembles, the proposed heuristic is poised to provide a performance-enhancing effect.
 References
