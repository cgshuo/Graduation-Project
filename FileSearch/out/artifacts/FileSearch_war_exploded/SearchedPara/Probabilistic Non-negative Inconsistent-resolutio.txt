 In this paper, we tackle with the problem of analyzing datasets with different resolution such as a pair of user X  X  individ-ual data and user group X  X  data, for example  X  userA visited shopA 5 times X  and  X  X sers whose attributes are men pur-chased itemA 80 times in total X . In order to establish a basic approach to this problem, we focus on the simplified scenario and propose a new probabilistic model called proba-bilistic non-negative inconsistent-resolution matrices factor-ization ( pNimf ). pNimf is rigorously derived from the data generative process using latent high-resolution data which underlie low-resolution data. We conduct experiments on real purchase log data and confirm that the proposed model provides superior performance, and that the performance improves as the number of low-resolution data increases. These results imply that our way of modeling using latent high-resolution data can become the basic approach to the problem of analyzing dataset with different resolution. Categories and Subject Descriptors: I.5.1 [Pattern Recognition]: Models Keywords: Inconsistent Resolution Dataset; Probabilistic Models; Non-negative Matrix Factorization
The success of companies that utilize the data analytical approach is driving projects on data collection and analy-sis in many business fields. Almost every week, there is a press release describing one or more projects by companies to collect customer information, to collaborate with other companies and so on. These trends increase the need to analyze datasets with different resolution such as a pair of user X  X  individual data and user group X  X  data, for example  X  userA visited shopA 5 times X  and  X  X sers whose attributes are men purchased itemA 80 times in total X . Since the user X  X  individual data represent more precise information than the user group X  X  data, we call these the high-resolution data and the low-resolution data, respectively. Thus, we call this c These results imply that our model using latent high-resolution data can become the basic approach to IRDAP.
NMF [7] is a method that factorizes a matrix into fac-tor matrices with non-negative values. The result of factor-ization can be used for missing value completion and soft clustering by considering factor matrices represent cluster assignment. Since NMF has a loss function flexibility, NMF canbeappliedtovarioustypesofdatasuchasratings,doc-uments, and purchase logs by choosing the appropriate func-tion [4]. More interestingly, Ding et al. proved that NMF us-ing generalized KL divergence is equivalent to probabilistic latent semantic indexing, which is the basis of latent dirich-let allocation [6, 2, 5]. Thus, NMF is a key machine learning algorithm and we choose it as the basis of our model.
For integrated data analysis, collective matrix factoriza-tion (CMF) methods, which factorize multiple matrices si-multaneously, have become popular tools [9]. NMF has also been extended to the CMF method [8, 10] and is called non-negative multiple matrix factorization (NMMF) in [10]. It is reported that these methods achieve better predictive per-formance than NMF. However, these methods don X  X  con-sider the differences in dataset resolution. In a different context of CMF, Aimoto and Kashima proposed a factor-ization method that uses a kind of low-resolution data (they call it aggregated data) [1]. However, the assumption they made as to the association between datasets with different resolution does not suit our problem.
In this paper, we focus on the simplified IRDAP that sat-isfies the two assumptions of (A1) common user population assumption and (A2) iid assumption . These assumptions are formally defined later, and here we provide an intuitive explanation. Let us consider the example that a supermar-ket started to issue membership cards from December and all users must use the card at the point of purchase from Dec. Then, since user-ID is not present in the November log, purchase log of Nov. is a low-resolution data while that of Dec. is a high-resolution data as shown in Fig. 1. Note that the reason why user attribute is recorded in the low-resolution data in this example is that user X  X  sex and age information, estimated by appearance, are recorded by the staff at the point of purchase. In this example, (A1) requires that the set of store users in Nov. and Dec. are the same (re-gardless of whether they purchase anything or not) and (A2) requires that the probability of item purchase by some user in Nov. is equal to that in Dec. Hereafter, we address this purchase log analysis problem, so all notations follow this example. However, our scope of study is not limited to this example and more general scenario is discussed in  X  3.4.
Notation: Let I , J and K represent the number of users, items, and attributes, respectively. We define the element of
X , x ij , as the number of purchases of item j by user i in Dec. and the element of Y , y kj , is the number of times item j was purchased by users with attribute k in Nov. Each X and Y is regarded as the high-resolution matrix and the low-resolution matrix, respectively. We also assume that user X  X  attribute information is available. This assumption is Figure 2: Graphical models. Shaded nodes indicate
We derive the proposed method based on the data genera-tive process summarized as follows: (i) define the probability distribution that generates both X and Z . (ii) use (A2) iid assumption , which we formally defined as follows: elements of
X and Z that have the same indices, x ij and z ij ,follow the identical probability distribution (in this case, Poisson dist. with parameter  X  x ij as in Eq. (1)) and these are mutu-ally independent. (A2) helps to extract factors which are in-dependent of month. (iii) use the relation between Z and Y ( y kj = i v ik z ij ) explained in the previous section. Putting all this together, the joint dist. of X , Z , Y is written as where  X  (  X  ) denotes delta function. Figure 2(a) shows graph-ical model representation. By explicitly modeling the gen-eration of latent high-resolution matrix Z , we can naturally define the probability distribution of all matrices. However, since the size of Z is I  X  J , which is considerable, it is de-sirable to work with more convenient probabilistic models.
The key is a characteristic of Poisson distributions: the sum of Poisson-distributed random variables is also Poisson-distributed random variable. In our model, z ij represents Poisson-distributed random variables and y kj is their sum-mation. Thus, we can marginalize out Z from Eq. (2) which yields the following equation: p ( X , Y | A , B , V )= p ( X , Z , Y | A , B , V ) d Z Figure 2(b) shows graphical model representation. Consid-ering that C := { c kr } K,R k,r =1 is attribute latent factor matrix, Eq. (3) can be interpreted as factorizing the high-resolution matrix and low-resolution matrix simultaneously, while re-taining the relation between factor matrices A and C using V ( C = V T A as in Eq. (4) ). Thus, we call this pro-posal probabilistic non-negative inconsistent-resolution ma-trices factorization ( pNimf ). Note that removing the linear equality relation between factor matrices, C = V T A , pNimf reduces to one of CMF method (NMMF) [10]. Thus, pNimf canbeseenasanextensionofNMMF.
Data Description: We evaluate the performance of our methods using real purchase log data: consumer panel re-search data  X  X CI X  provided by Intage Inc. We use purchase logs of daily necessities (such as milk, coffee and snacks) from 2013.1.1 to 2013.12.31 in Japan. Thus, we can expect that (A2) is satisfied since these items likely to be purchased in each month repeatedly. SCI includes user X  X  attribute infor-mation such as age, sex and job. We construct Two-month data and Four-month data as follows. Two-month data is constructed using the log in Nov. and Dec. as in Fig. 1. We use only the logs of active users who have a purchase log in each month to satisfy (A1) and items that appeared more than ten times. The size and sparseness of X Dec and Y Nov are I = 1589 ,J = 3164 ,K = 34, 99.15% and 54.4%, respec-tively. We repeat this procedure for the logs of Jan. and Feb. Size and sparseness are almost similar to that of Nov. and Dec. Four-month data is also prepared in an analogous man-ner. For Four-month data, we used the logs of Sep, Oct, Nov and Dec and made high-resolution matrix X Dec and low-resolution matrices Y Nov , Y Oct and Y Sep . The resulting size was I = 1288 ,J = 4842 ,K = 34.

Evaluation Measure: In our experiments, we use a test set log likelihood to evaluate performance. We split the elements of matrix X into a training dataset and a test dataset and compute the log likelihood of the elements in the test. Test data are treated as missing values in the training phase. Log likelihood of the test data set is defined ment indexes in the test data and | X | indicates the number of elements in the set. We prepared 10 pair of the training and the test data by randomly extracting 5% of non-zero elements as the test data.

Baseline Methods: For comparison, we consider the following methods. (1)NMF [7], traditional method which use only high-resolution matrix X . (2)NMMF [10], an NMF based state of the art CMF method that uses both X and Y . The weight parameter of NMMF is chosen from the candidates  X  =0 . 1 , 0 . 5 , 1 . 0. We report the result for  X  =1 . 0 since it shows the best result among the candidates.
Our first experiment used the Two-month data. Table 1 shows the results. It is confirmed that pNimf and NMMF outperform NMF regardless of the number of factors in all datasets. This result indicates that the use of low-resolution data improves the performance. Moreover, the performance of pNimf is superior to that of NMMF in all settings. It seems that the linear relation between factor matrices using user X  X  attribute information supports pNimf in handling the difference in resolution and thus achieving better factoriza-tion results.

Next, we evaluate the performance achieved while vary-ing the number of low-resolution data N using Four-month data. We set the number of factors to ten and compared it to pNimf using a different number of low-resolution data N . Fig. 3 shows the results. As the number of low-resolution data N increases, the performance of pNimf improves. Since pNimf can deal with multiple low-resolution data by gener-alization of data generative process, it works well even if the amount of high-resolution and low-resolution data differ.
