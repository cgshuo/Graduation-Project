 Users engaged in the Social Web increasingly rely upon con-tinuous streams of Twitter messages ( tweets ) for real-time access to information and fresh knowledge about current af-fairs. However, given the deluge of tweets, it is a challenge for individuals to find relevant and appropriately ranked in-formation. We propose to address this knowledge manage-ment problem by going beyond the general perspective of information finding in Twitter, that asks:  X  X hat is happen-ing right now? X , towards an individual user perspective, and ask:  X  X hat is interesting to me right now? X  In this paper, we consider collaborative filtering as an online ranking prob-lem and present RMFO , a method that creates, in real-time, user-specific rankings for a set of tweets based on individ-ual preferences that are inferred from the user X  X  past sys-tem interactions. Experiments on the 476 million Twitter tweets dataset show that our online approach largely out-performs recommendations based on Twitter X  X  global trend and Weighted Regularized Matrix Factorization (WRMF), a highly competitive state-of-the-art Collaborative Filtering technique, demonstrating the efficacy of our approach.
The Social Web has been successfully established and is poised for continued growth. Real-time microblogging ser-vices, such as Twitter ( twitter.com ), have experienced an explosion in global user adoption over the past years [12].
Despite the recent amount of research dedicated to Twit-ter, online collaborative filtering and online ranking in Twit-ter have not yet been extensively addressed.

Given a continuous stream of incoming tweets, we are in-terested in the task of filtering and recommending topics that meet users X  personal information needs. In particular, we use hashtags as surrogates for topics, and learn, online, a personalized ranking model based on low-rank matrix fac-torization for collaborative prediction.

Collaborative filtering (CF) is a successful approach at the core of recommender systems. CF algorithms analyze past interactions between users and items to produce personal-ized recommendations that are tailored to users X  preferences.
In the presence of a continuous stream of incoming tweets coming data in bounded space and time, and recommend a short list of interesting topics that meet users X  individual taste. Furthermore, our online CF algorithm should quickly learn the best Top-N recommendations based on real-time user interactions and prevent repeatedly suggesting highly relevant, but old information. In the absence of high qual-ity explicit feedback (e.g., ratings), we infer user preferences about items using implicit feedback . For example, in Twit-ter, if user Alice has been tagging her tweets lately with the hashtag #Olympics2012 ; and, so far, she has never used the hashtag #fashion , we exploit this information, and use it as a good indicator for her up-to-date preferences. We can infer that currently, Alice is more interested in Olympic Games than, for instance, in fashion . Thus the task can be cast as that of recommending hashtags to users.

The high rate makes it harder to: (i) capture the infor-mation transmitted; (ii) compute sophisticated models on large pieces of the input; and (iii) store the amount of input data, which we consider significantly larger than the mem-ory available to the algorithm [8]. In this paper, we present our Online Matrix Factorization approach  X  RMFO  X  for ad-dressing these research challenges.

To the best of our knowledge this work is the first empiri-cal study demonstrating the viability of online collaborative filtering for Twitter. The main contributions of this work are: 1. We introduce a novel framework for online collaborative filtering based on a pairwise ranking approach for matrix factorization, in the presence of streaming data. 2. We propose RMFO , an online learning algorithm for col-laborative filtering. We explore different variations of the algorithm and show that it achieves state-of-the-art perfor-mance when recommending a short list of interesting and rel-evant topics to users from a continuous high volume stream of tweets, under the constraints of bounded space and time. 3. Personalized and unpersonalized offline learning to rank have been previously studied in the literature. This paper proposes an innovative perspective to the problem, directed to social media streams and based on online learning and matrix factorization techniques.
In this section, we formally define the problem, introduce our approach RMFO and describe three variations of the al-gorithm, namely, Single Pass , User Buffer , and Reservoir Sampling .
 First we introduce some notation that will be useful in our setting. Let U = { u 1 ,...,u n } and I = { i 1 ,...,i m } be the sets of all users and all items, respectively. We reserve special indexing letters to distinguish users from items: for users u , v , and for items i , j . Suppose we have interactions between these two entities, and for some user u  X  U and item i  X  I , we observe a relational score x ui . Thus, each instance of the data is a tuple ( u,i,x ui ). Typical CF algorithms orga-nize these tuples into a sparse matrix X of size | U | X | I | , using ( u,i ) as index and x ui as entry value. The task of the recommender system is to estimate the score for the missing entries. We assume a total order between the pos-sible score values. We distinguish predicted scores from the known ones, by using  X  x ui . The set of all observed scores is S := { ( u,i,x ui ) | ( u,i,x ui )  X  U  X  I  X  N } . For convenience, we also define for each user the set of all items with an observed score: B + u := { i  X  I | ( u,i,x ui )  X  S } .

Low dimensional linear factor models based on matrix factorization (MF) are popular collaborative filtering ap-proaches [7]. These models consider that only a small num-ber of latent factors can influence the preferences. Their pre-diction is a real number,  X  x ui , per user item pair ( u,i ). In its basic form, matrix factorization estimates a matrix  X  X : U  X  I by the product of two low-rank matrices W : | U | X  k and H : | I | X  k as follows:  X  X := WH | , where k is a parameter corresponding to the rank of the approximation. Each row, w u in W and h i in H can be considered as a feature vector describing a user, u , and an item, i , correspondingly. Thus the final prediction is the linear combination of the factors:  X  x We focus on learning a matrix factorization model for collab-orative filtering in presence of streaming data. To this end, we will follow a pairwise approach to minimize an ordinal loss. Our formalization extends the work of Sculley [11] for unpersonalized learning to rank, to an online collaborative filtering setting.

With slight abuse of notation, we also use S to represent the input stream s 1 ,s 2 ,... that arrives sequentially, instance by instance. Let p t = (( u,i ) , ( u,j )) t denote a pair of train-ing instances sampled at time t , where ( u,i )  X  S has been observed in the stream and ( u,j ) /  X  S has not.

Formally, we define the set P as the set of tuples p = P := { (( u,i ) , ( u,j )) | i  X  B + u  X  j /  X  B + u } .
We require pairs that create a contrast in the preferences for a given user u over items i and j . Since we are dealing with implicit, positive only feedback data (i.e. the user never explicitly states a negative preference for an item) we follow the rationale from Rendle et al. [9] and assume that user u prefers item i over item j . We will restrict the study to a binary set of preferences x ui = { +1 ,  X  1 } , e.g., observed and not-observed , represented numerically with +1 and  X  1, respectively. For example, if a user u in Twitter posts a message containing hashtag i , then we consider it as a pos-itive feedback and assign a score x ui = +1. More formally, x ui = +1  X  X  X  i  X  B + u . In future work we plan to explore how repeated feedback can be exploited to establish a total order for items in B + u .

With P defined, we find  X  = ( W , H ) that minimizes the pairwise objective function: In this paper, we explore the use of the SVM loss, or hinge-loss , used by RankSVM for the learning to rank task [6]. Given the predicted scores  X  x ui and  X  x uj , the ranking task is reduced to a pairwise classification task by checking whether the model is able to correctly rank a pair p  X  P or not. Thus, L ( P, W , H ) is defined as follows: where h ( z ) = max (0 , 1  X  z ) is the hinge-loss; y uij x uj ) is the sign ( z ) function, which returns +1 if z &gt; 0,  X  w u , h i  X  h j  X  =  X  w u , h i  X   X   X  w u , h j  X  corresponds to the difference of predictor values  X  x ui  X   X  x uj . To conclude this section, we compute the gradient of the pairwise loss at in-stance p t  X  P with non-zero loss, and model parameters  X  = ( w u , h i , h j ), as follows: Our goal is to develop an algorithm to efficiently optimize the objective function (1). Based on the stochastic gradient descent concepts [1], we present the framework of our algo-rithm in Figure 1. The main components of this framework are: (i) a sampling procedure done on the streaming data, and (ii) a model update based on the sample.

The model update procedure performed by RMFO is shown in Figure 2, which includes three regularization constants:  X 
W ,  X  H + , and  X  H  X  , one for the user factors, the other two for the positive and negative item factors updates. Moreover, we include a learning rate  X  and a learning rate schedule  X  that adjusts the step size of the updates at each iteration.
In the rest of the section we explore three variations of our online algorithm based on how the sampling is performed. In this work, we explore the following three variations of our approach based on different stream sampling techniques: (1) Single Pass ( RMFO-SP ) takes a single pair from the stream and performs an update of the model at every iter-ation. This approach does not  X  X emember X  previously seen instances. That is, we sample a pair p t  X  P at iteration t , and execute procedure updateModel ( p t ,  X  W ,  X  H + ,  X   X , T  X  = 1) (Figure 2). (2) User Buffer ( RMFO-UB ) retains the most recent b in-stances per user in the system. In this way, we retain certain amount of history so that the algorithm will run in constant space. For each user, we restrict the maximum number of her items to be kept and denote it by b . More precisely, af-ter receiving the training instance ( u,i,x ui ) t at time t , the user buffer | B + u | for u , is updated as follows: if | B + u | &lt; b then B + u  X  X  i } else end if
We update the model selecting pairs, p t  X  P , from the candidate pairs implied by the collection of all user buffers B , which is defined by the function B := u  X  B + u . (3) Reservoir Sampling ( RMFO-RSV ) involves retaining a fixed size of observed instances in a reservoir . The reser-voir should capture an accurate  X  X ketch X  of history under the constraint of fixed space. The technique of random sampling with a reservoir [13] is widely used in data streaming, and re-cently has been also proposed for online AUC maximization in the context of binary classification [15]. We represent the random instances from stream S . Instances can occur more than once in the reservoir, reflecting the distribution of the observed data. We note that this approach also bounds the space available for the algorithm, but in contrast to the user buffer technique, we do not restrict the space per user, but instead randomly choose | R | samples from the stream and update the model using this history.
 RMFO Framework Input: Stream representative sample at time t : S t ; Regu-Output:  X  = ( W , H ) 1: initialize W 0 and H 0 2: initialize sample stream S 0  X  X  X  3: counter  X  0 4: for t = 1 to T S do 5: S 0  X  updateSample ( S t ) 6: counter  X  counter + 1 7: if c = counter then 8:  X   X  updateModel ( S t , X  W , X  H + , X  H  X  , X , X ,T  X  ) 9: counter  X  0 10: end if 11: end for 12: return  X  T = ( W T , H T ) RMFO Model Update based on SGD for MF Input: Stream representative sample at time t : S t ; Regu-Output:  X  = ( W , H ) 1: procedure updateModel ( S t , X  W , X  H + , X  H  X  , X  0 , X ,T 2: for t = 1 to T  X  do 3: (( u,i ) , ( u,j ))  X  randomPair( S t )  X  P 4: y uij  X  sign ( x ui  X  x uj ) 5: w u  X  w u +  X  y uij ( h i  X  h j )  X   X   X  W w u 7: h j  X  h j +  X  y uij (  X  w u )  X   X   X  H  X  h j 8:  X  =  X   X   X  9: end for 10: return  X  = ( W T  X  , H T  X  ) 11: end procedure
In this section, we demonstrate our approach by analyzing real-world data consisting of millions of tweets. The dataset corresponds to the 476 million Twitter tweets [14]. For our evaluation we computed a 5-core of the dataset, i.e., every user has used at least 5 different hashtags, and every hashtag has been used by least by 5 different users. The 5-core consists of 35,350,508 tweets (i.e., user-item interactions), 413,987 users and 37,297 hashtags.
 Evaluation of a recommender in the presence of stream data requires a time sensitive split. We split the dataset S into training S train and a testing set S test according to a times-tamp t split : the individual training examples (tweets) with timestamps less that t split are put into S train , whereas the others go into S test . Note that given the dynamics in Twit-ter, there might be users in S train not present in S test
To evaluate the recommenders we followed the leave-one-out protocol. In particular, a similar schema as the one de-scribed in [2].
 For each user u  X  X  U test | we rank her items in the test set, S test , according to their frequencies and choose one item i at random from the top-10. The goal of a recommender system is to help users to discover new items of interest, therefore we impose the additional restriction that the hidden item has to be novel for the user, and therefore we remove from the training set all occurrences of the pair ( u,i ). In total, we have | U test | = 260 , 246 hidden items. Then, for each hidden item i , we randomly select 1000 additional items from the test set S test . Notice that most of those items selected are probably not interesting to user u . We predict the scores for the hidden item i and for the additional 1000 items, forming a ranking by ordering the 1001 items according to their scores. The best expected result is that the interesting item i u to user u will precede the rest 1000 random items.
Finally, for each user, we generate a Top-N u recommen-dation list by selecting the N items with the highest score. otherwise we have a miss .
 We measure Top-N recommendation performance by look-ing at the recall metric, also known as hit rate , which is widely used for evaluating Top-N recommender systems (e.g., [2]). In our recommender systems setting, recall at top-N lists is defined as follows: where 1 [ z ] is the indicator function that returns 1 if condition z holds, and 0 otherwise. A recall value of 1.0 indicates that the system was able to always recommend the hidden item, whereas a recall of 0.0 indicates that the system was not able to recommend any of the hidden items. Since the precision is forced by taking into account only a restricted number N of recommendations, there is no need to evaluate precision or F1 measures, i.e., for this kind of scenario, precision is just the same as recall up to a multiplicative constant. We implemented the three variations of our model RMFO-SP , RMFO-UB and RMFO-SRV , and evaluated them against two other competing models: (1) Trending Topics (TT) . This model sorts all hash-tags based on their popularity, so that the top recommended hashtags are the most popular ones, which represent the trending topics overall. This naive baseline is surprisingly powerful, as crowds tend to heavily concentrate on few of the many thousands available topics in a given time frame. We evaluate the TT from the whole training set and the ones from the last four weeks before the evaluation. (2) Weighted Regularized Matrix Factorization (WRMF) . This is a state-of-the-art matrix factorization model for item prediction introduced by Hu et al. [5]. WRMF is formulated as a regularized Least-Squares problem, in which a weighting matrix is used to differentiate the contri-butions from observed interactions (i.e., positive feedback) and unobserved ones. WRMF outperforms neighborhood based (item-item) models in the task of item prediction for implicit feedback datasets, and therefore is considered as a more robust contender. Please note that this reference model is computed in batch mode , i.e., assuming that the whole stream is stored and available for training. WRMF setup is as follows:  X  WRMF = 0 . 015, C = 1, epochs = 15, which cor-responds to a regularization parameter, a confidence weight that is put on positive observations, and to the number of passes over all observed data, respectively 1 [5].

For all variations of RMFO we simulate the stream receiving one instance at the time based on the tweets X  publication dates. Tweets without hashtags were ignored.

For RMFO-UB , we want to explore the effect of the user X  X  buffer size b on the recommendation performance, we vary b  X  X  2 m | m  X  N , 1  X  m  X  9 } , i.e., from 2 to 512. million, and compute the model using 15 epochs over the reservoir only. We set regularization constants  X  W =  X  H +  X   X  = 0 . 1, learning rate  X  0 = 0 . 1, and a learning rate sched-ule  X  = 1, and find that the setting gives good performance. We are currently investigating how to efficiently perform a grid search on stream data to tune-up the hyperparameters dynamically.

We divide the seven-month Twitter activity of our dataset by choosing the first six months for training. We use the re-maining month, i.e., December, to build 10 independent test sets following the evaluation protocol described previously in this section. We compute the recall metric for Top-N rec-performance is evaluated on the test set only, and the re-ported results are the average over 10 runs.
 We found that recent topics are more valuable for recom-mendations: trending topics from the previous four weeks achieve a recall@10 of 7.8%, compared to 6.77% from the ones corresponding to the whole training period (6 months). The performance exhibited by this recommender, based on the crowd behavior in Twitter, largely outperforms a ran-dom model, whose recall@10 is under 1%. In the rest of the discussion we focus only on the recent trending topics.
Figure 4 shows the recommendation quality in terms of recall@10 for RMFO-SP , and RMFO-UB with varied user buffer sizes. We can see that recall@10 for RMFO-SP is 14.69%, 88.3% better than the overall trend.

We also observed that having a per-user buffer improves the performance. However if the buffer is small (e.g., 2 or 4), RMFO-UB achieves low recall. Although increasing the buffer size boosts the recommendation quality, we found that as the quality reaches a plateau (see Figure 4), the buffer size provides limited improvements.

Figure 3a shows that RMFO-SRV achieves the best perfor-mance over all methods evaluated when the reservoir size is greater than 4 million, which corresponds to 11.32% of the entire number of transactions in the dataset. We summarize in Figure 3b the best performance achieved by the methods evaluated for different Top-N recommendations.

With a fixed reservoir size of 8M, we also explored the impact of model dimensionality over the recommendation quality for RMFO-RSV . The results are presented in Figure 5. From the figure, we see that the 16-factor low-rank approx-imation given by RMFO-RSV exhibits a better recall@10 than WRMF computed in batch mode using 128 factors. We report in this section the CPU training times and space required for the best performing variation of our online ap-proach: RMFO-RSV , and the ones for the strongest baseline: WRMF. Please remember that running times heavily de-pend on platform and implementation, so they should be only taken as relative indicators.

All variations of RMFO were implemented in Python. RMFO ran on a Intel Xeon 1.87GHz machine. For WRMF, we used the C# implementation provided by MyMediaLite library [4]. The baseline WRMF was run on a machine with a slightly faster CPU (Intel Xeon 2.27GHz). None of the methods was parallelized and therefore used a single CPU for computa-tions. GNU/Linux 64-bit was used as OS.

In Table 1, we can observe the gains in speed of our ap-proach over the baseline for all the evaluated reservoir sizes. For reservoir sizes of 4M and 8M, RMFO-RSV is not only faster and space efficient, but also exhibits a better recommen-dation performance with respect to WRMF, for example, RMFO-RSV with a reservoir size 8M is over 36 times faster and uses 77% less space than WRMF, and yet it delivers a recommendation performance almost 25% better than the state-of-the-art baseline. As a reference, we also include the performance of RMFO-RSV INF , which uses an infinite reser-voir, e.g., one that is able to remember all observed trans-actions.

Online learning of matrix factorization methods for rating prediction have been investigated by Rendle and Schmidt-Thieme in [10]. They propose online update rules on a stochas-tic gradient descent style based on the last example ob-served. However, the best performing variant of our ap-proach, RMFO-RSV , maintains a reservoir with a represen-tative set of previously seen data points from the stream, which provides a significant boost in performance compared to the one obtained when only the last example is considered (e.g., RMFO-SP ). The technique of random sampling with a reservoir is widely used in data streaming [13], and recently has also been exploited by Zhao et al. in the context of bi-nary classification [15].
This paper provides an example of integrating large-scale collaborative filtering with the real-time nature of Twitter.
We proposed RMFO , an approach for recommending topics to users in presence of streaming data. Our online setting for collaborative filtering captures  X  X hat is interesting to me right now? X  in the social media stream.

RMFO receives instances from a microblog stream, and up-dates a matrix factorization model following a pairwise learn-ing to rank approach for dyadic data. At the core of RMFO is stochastic gradient descent which makes our algorithm easy to implement and efficiently scalable to large-scale datasets. From the RMFO  X  X  variants explored in this work, we found that the one using reservoir sampling technique performed the best.

Our empirical study used Twitter as test bed and showed that our approach worked well relative to matrix factoriza-tion models computed in batch mode, in terms of recom-mendation quality, speed and space efficiency.

Currently, we are investigating alternative sampling tech-niques, for example, based on active learning principles that select the instances based on their gradients, thus keeping the most informative ones in the reservoir. Initial promising results towards this direction can be found in [3].
