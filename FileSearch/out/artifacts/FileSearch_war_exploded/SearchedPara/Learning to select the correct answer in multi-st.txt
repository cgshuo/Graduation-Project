 1. Introduction
The great amount of available information has motivated the development of different tools for searching and browsing large document collections. The major examples of these tools are information retrieval (IR) systems, which focus on iden-tifying relevant documents for general user queries. Search engines such as Google tems which allow to retrieve information from the Web.

It is clear that IR systems have made possible the processing of large volumes of textual information; however, they desired information. This limitation, along with a growing need for improved information access mechanisms, triggered the emergence of question answering (QA) systems. These systems aim at identifying the exact answer to a question from a given document collection. In other words, given an user query in the form of natural language question (e.g., Iraq invade in 1990? ), a QA system must detect the text fragment that respond the question (e.g., returning a list of documents related to the question words.
Recent research in QA has been mainly fostered by the TREC, tions, such as factual questions (e.g., Where is the Taj Mahal? questions (e.g., Who were the members of The Beatles? ). The results from these conferences have shown some interesting (55%).

Based on these facts, some advanced approaches known as multi-stream QA systems attempt to improve the individual results by taking advantage of the complementarity from existing QA systems. Therefore, the major challenge of this kind of systems is to select the correct answer for a given question by combining the evidence from different input systems (or involves determining the cases that require a nil response.

Traditional approaches for multi-stream QA rely on measuring the confidence of streams or the redundancy of answers textual entailment recognition (RTE) 6 techniques, which decide about the correctness of answers based on information from mation between the question X  X nswer pair and the support text.

Our experimental results in a set of 190 questions in Spanish language, considering answers from 17 different QA sys-ti-stream QA approach (0.60).
 multi-stream approaches. Finally, Section 5 exposes our conclusions and outlines some future work directions. 2. Related work
A typical QA system consists of three main processes that are carried out in sequence: question analysis, document/pas-and multi-stream QA systems has been proposed.

On the one hand, meta QA systems internally combine several components or techniques at each QA process. For exam-ple, Pizzato and Molla-Aliod (2005) describe a QA architecture that uses several document retrieval methods, and Chu-car-roll, Czuba, Prager, and Ittycheriah (2003) present a QA system that applies two different components at each process.
On the other hand, multi-stream QA approaches go a step forward by achieving a superficial combination of several QA tion rather than on ranking all candidate answers. Following, we introduce the traditional approaches for multi-stream QA. correspond to answer-centered approaches, which select the final answer exclusively based on its frequency of occurrence across streams.
Ordered Skimming approach : in this case the streams (individual QA systems) are ordered by their general confidence, and the final answer is selected from the stream with the highest one. Some systems based on this approach are described by Clarke et al. (2002) and Jijkoun and de Rijke (2004) .

Dark Horse approach : it can be considered an extension of the previous approach because it also considers the confidence of streams; however, in this case, these confidences are calculated for each type of question. Using this approach each stream has different confidences associated to factual and definition questions. Jijkoun and de Rijke (2004) described a multi-stream QA system based on this approach.

Answer Chorus approach : it relies on the answer redundancies; it selects as the final respond the answer with the highest frequency across streams. Some systems based on this approach are described in de Chalendar et al. (2002), Burger et al. (2002), Jijkoun and de Rijke (2004), Roussinov et al. (2005), and Rotaru and Litman (2005).

Web Chorus approach : it uses information from the Web to evaluate the relevance of candidate answers. It selects the answer with the greatest number of Web pages containing the answer terms along with the question terms. It was proposed by Magnini, Negri, Prevete, and Tanev (2001) , and subsequently it was evaluated in Jijkoun and de Rijke (2004).

Hybrid approach : it considers the combination of criteria from the system and answer-centered approaches. The method described in Jijkoun and de Rijke (2004) is an example of this kind of approach. It uses the system X  X  confidences to dif-ferentiate between answers having the same frequency of occurrence. Its evaluation results indicated that this combina-tion could outperform the results obtained by other multi-stream QA systems based on one single strategy.
In addition to the traditional approaches adopted from IR, more recently emerged a new type of multi-stream QA ap-tion at hand. Two systems based on this new approach are the ones reported by Gl X ckner et al. (2007) and Harabagiu and Hickl (2006) , for Dutch and English respectively.

The method proposed in this paper is a new kind of hybrid approach , which is based on a supervised learning method that combines features from the answer-chorus and textual-entailment approaches. It mainly differs from previous multi-stream methods in the following concerns:
First, different from other hybrid methods, it does not consider any information about the confidence of the input QA sys-text. This difference makes our method independent from the used QA systems, and, consequently, makes it easily adaptable to work with different systems.

Second, different from other answer-chorus methods, our proposal does not consider the answer frequencies as the main performance QA systems (something common in most non-English languages), where correct answers tend to show low frequencies.

Third, different from previous textual-entailment approaches (and also varying from supervised answer-validation meth-cate the overlap between the question X  X nswer pair and the support text, but also includes some features that evaluates the non-overlapped information, allowing, in this way, to correctly analyze the situations where exists a high overlap but not
We consider this last characteristic to be very important for constructing multi-stream QA applications in different languages.

The following section describes in detail the proposed approach. 3. System description
Fig. 1 shows the general scheme of the proposed multi-stream QA approach. It consists of two main stages. In the first stage, called QA stage , several QA systems extract X  X n parallel X  X  candidate answer and its corresponding support text for incorrect, the system returns a nil response.

The following subsection describes in detail the answer selection method. It is important to comment that the current method is an extension of our previous work evaluated in the answer-validation exercise at CLEF ( T X llez-Valero, y G X mez, question X  X nswer pair and the support text, and therefore, allow to significantly improve our previous results in multi-stream QA ( T X llez-Valero, y G X mez, Pineda, &amp; Pe X as, 2008 ). 3.1. Answer selection
As we previously mentioned, this stage focuses on the selection for the final response by combining evidence from the considers three main processes: preprocessing, feature extraction and answer classification. 3.1.1. Preprocessing
The objective of this process is to extract the main content elements from the question, answer and support text, which will be subsequently used for deciding about the correctness of the answer. This process considers two basic tasks: on the the core fragment of the support text as well as the consequent elimination of the unnecessary information. by the verb invade , its actors are the syntagms Which country propositional syntagma in 1990 .

In order to detect the question constituents we firstly apply a shallow parsing to the given question. the following elements: 1. The action constituent : It corresponds to the syntagm in Q 2. The restriction constituent : It is represented by the prepositional syntagm in Q sion (e.g., in 1990 ), or including a preposition such as 3. The actor constituents : These constituents are formed by the rest of the elements in Q stituent , is formed by the rest of the syntagms, generally located at the right of the action constituent.
Finally, we also consider an answer constituent , which is simply the lemmatized candidate answer (denoted by A 0 ). 3.1.1.2. Support text X  X  core fragment detection. Commonly, the support text is a short paragraph X  X f maximum 700 bytes according to CLEF evaluations X  X hich provides the necessary context to support the correctness of a given answer. However, in many cases, it contains more information than required, damaging the performance of RTE methods that are based on utes to produce an irrelevant overlap (e.g., Kuwait was a close ally of Iraq minimum useful text fragment according to the candidate answer, we proceed as follows:
First, we apply a shallow parsing to the support text, obtaining the syntactic tree  X  S
Second, we match the content terms (nouns, verbs, adjectives and adverbs) from the question constituents against the terms from Sparsed . In order to avoid some minimal writing differences of the same concept not solved by the morpho-logical analysis (e.g., Iraq against Irak or Iraqi ). We compare the terms using the Levenshtein edition distance. Mainly, we consider that two different words are equal if their distance value is less than 0.4. Third, based on the number of matched terms, we align the question constituents with the syntagms from the support text.
Forth, we match the answer constituent against the syntactic tree  X  S in the given support text.

Fifth, we determine the minimum context of the answer in the support text that contains all matched syntagms. This min-the case that the support text includes several occurrences of the answer, we select the one with the smallest context.
Applying the procedure described above we determine that the core fragment of the support text showed at Table 1 is an Iraqi invasion of Kuwait . 3.1.2. Feature extraction This stage gathers a set of processes that allow extracting several features from the question, answer and support text. following sections describe both kinds of features and explain the way they are calculated from Q 0 , A 0 and T 0 . 3.1.2.1. Features about the question and answer.  X  Question characteristics
We consider four different features from the question: the question word (what, how, where, etc.), the question category period, event, or none).
 The question word, question category, and the expected answer type are determined using a set of simple lexical patterns.
Some of these patterns are showed below. It can be observed that each of them includes information about the question cat-because the proposed question X  X  classes are very general, we prefer using the classic approach based on hand-crafted rules ( Lee, Oh, Huang, Kim, &amp; Choi, 2000; Pasca &amp; Harabagiu, 2001 ). ( WHAT OR WHO ) is [ whatever ] ? DEFINITION  X  OTHER HOW ( many OR old )[ whatever ] ? FACTUAL  X  QUANTITY WHEN [ whatever ] ? FACTUAL  X  DATE WHERE [ whatever ] ? FACTUAL  X  NAME WHAT is the name [ whatever ] ? FACTUAL  X  NAME
On the other hand, the value of the question restriction (date, period, event or none) depends on the form of the restric-ferent kinds of question restrictions.
 Where did Freud move to live in 1939 ? ? DATE
What is the name of the collection of pictures painted by Goya
Where did Francis Cammaerts work with the Maquis during World War II
Who was Alexander Graham Bell? ? NONE  X  Question X  X nswer compatibility where the semantic class of the evaluated answer does not correspond to the expected answer type. For instance, having the answer yesterday for the question How many inhabitants are there in Longyearbyen? this correspondence does not exist.  X  Answer redundancy decided to include a feature related to the occurrence of answers across streams.

Different from other redundancy methods (like the one present by Roussinov et al. (2005) ) that directly use the frequency tween the actual evaluated answer to each one of the other candidate answers. The edition distance strategy allows dealing with the great language variability and also with the presence of some typing errors. In this way, an answer the redundancy rate of another answer Y and vice versa, even though scope and Hubble telescope ). that measure the overlap between the support text and the hypothesis (an affirmative sentence formed by combining the question and the answer); and (ii) attributes that denote the differences (non-overlap) between these two components.
It is important to explain that, different from other RTE methods, we do not use the complete support text; instead, we of question X  X nswer constituents (that is, the union of Q 0 and A 0 , which result is H 0 ).  X  Overlap characteristics
These features express the degree of overlap X  X n number of words X  X etween T 0 and H 0 . In particular, we compute an over-
For instance, we consider that the restriction constituent restriction constituent between 1990 and 1998 is overlapped by the text In order to illustrate the computation of these characteristics we consider the question justly invade in 1990? , the answer ONU , and the core fragment
In this case, the ten features corresponding to the action constituent are all set to 1 (because they have the same verb and the other type of content terms are not present) except for the one related to adverbs, which has value of 0 since the term unjustly from the action constituent of H 0 does not occur in the corresponding constituent of T 0 .  X  Non-overlap characteristics
These features indicate the number of non-overlapped terms from the core fragment of the support text, that is, they indi-counting the non-overlapped words from the text between the answer constituent and each one of the other constituents type of content term as well as for each type of named entity, generating a total of forty different non-overlap features.
Continuing with the previous example, the features extracted from the text between the answer constituent and the restriction constituent (i.e., ... against Iraq since invade Kuwait ... to place named entities, which has value of 1 because the term 3.1.3. Answer classification classified as correct; in such cases the answer with the greatest confidence is selected as the final response.
The classification of candidate answers is carried out by means of a supervised learning approach, which determined the algorithm that is a combination of decision tress with boosting, and which generates classification rules that are small and easy to interpret ( Freund &amp; Mason, 1999 ). We employed the algorithm implementation by Weka ( Witten &amp; Frank, 1999 ), and configured it to apply 10 iterations and does not consider any prune method.

It is important to comment that during the development process we also carried out some experiments considering other learning algorithms available at Weka, such as Support Vector Machines, Na X ve Bayes, C4.5 decision tree, K -Nearest Neigh-final response for questions having several  X  X  X orrect X  candidate answers. 4. Experimental results 4.1. Experimental setup 4.1.1. Training and test data
As we described in Section 3 , the core component of the proposed multi-stream QA system is the answer selection mod-ule, which relies on a supervised learning approach.

Spanish corpora used at the CLEF conferences for evaluating QA systems from 2003 to 2005. It contains 2962 training in-the training instances are negative (their answer category is of Spanish QA systems.

On the other hand, we used a set of 190 questions and the answers from 17 different QA systems considered 2369 candidate answers with their corresponding support texts. Because this corpus only contains 671 correct swers in order to allow a successfully multi-stream QA performance. 4.1.2. Evaluation measure The evaluation measure most commonly used in QA is the accuracy , i.e., the percentage of correctly answered questions. with respect to the total number of questions (refer to Formula (1) ).
In particular, in our experiments we used an evaluation measure called estimated QA performance ( Rodrigo et al., 2008a ), specially suited to compare the results from multi-stream QA systems against those from the input streams. Based on the fact that multi-stream QA systems do not have access to the target document collection, and, therefore, that their perfor-mance is always restricted by the answers extracted from the input streams, this measure (see formula (2) ) not only con-siders the traditional accuracy but also a reject accuracy , which indicates the ability of a multi-stream QA system to correctly discard all candidate answers when all of them are incorrect. 4.2. Experiments 4.2.1. First experiment: our multi-stream approach and the single streams
The objective of any multi-stream QA method is to combine the responses from different QA systems (streams) in order to increase the number of correct answers. In other words, its goal is to obtain a better performance than that from the best input stream.

In a first experiment, we attempted to evaluate the achievement of this objective. We mainly compared the estimated QA the maximum reachable estimated QA performance for any multi-stream approach in this data set. significantly outperformed the results from all streams. 14 approach could correctly answer and reject 151 questions from a total of 190 (i.e., approximately a 80%); what is more, it could correctly answered 84% of the questions having a correct response. 4.2.2. Second experiment: our multi-stream approach against other approaches As a second experiment we compared our method against the traditional multi-stream QA approaches (refer to Section 2 ). In particular, we implemented some methods from these approaches based on the following criteria:
TheOrdered-Skimming Method : It selects the answers in accordance to the global stream accuracies. In other words, it aims ing answers from stream eight.
 extract the answers for factual questions from stream twelve, and the answers for definition questions from stream eight.
The Answer-Chorus Method : It selects the answers based on their repetitions across different streams. That is, for each question it selects the most frequent answer. In the case that two or more answers have the same frequency, it randomly selects the final response.

The Web Chorus Method : It selects the answers based on the number of Web pages, retrieved by Google, that contain the terms from the question (without the question word) along with the terms of the answer. Similar to the previous method, in the case that two or more answers obtained the same qualification, the final response is selected randomly. TheAnswer-Chorus X  X ark-Horse Method . It selects the answers based on their repetitions across different streams (Answers
Chorus Method), but in the case that several answers obtain the same maximal frequency, it applies the Dark Horse cri-teria to select the final response.

Table 5 shows the results from this experiment. These results demonstrate the appropriateness of the proposed multi-stream QA method, which improved in 41% the result from the best input stream. In contrast, not any of the other ap-proaches could increase in more than 10% the performance of the best input stream. proaches could not outperform the result from the best input stream; that is the case of the Ordered-Skimming and Dark-Horse Methods.

Taking into account that the estimated QA performance not only depends on the number of correctly answered questions, sponse will be delivered. Following we describe the modifications incorporated to each one of the methods.
Ordered-Skimming Method* : It only considers answers from the best five streams (i.e., it only returns answers coming from the streams that have an estimated QA performance greater than 0.3 (see Table 4 )).

Dark-Horse Method* : It only returns answers coming from the best five streams for each question type. In this case there were selected the best five streams for answering factual questions as well as the best five for answering definition questions.

Chorus Method* : It only considers answers recommended by two or more streams; therefore, it discards all infrequent answers.

Chorus X  X ark Horse* : Firstly, it applies the Chorus Method*, but when many responses have the same maximal frequency it uses the Dark-Horse Method to select the final response.

Table 6 shows the results from this experiment. It is interesting to notice that all methods improved their estimated QA performance when they could reject some answers. The explanation of this behavior is that these modifications allowed all methods to correctly reject some questions. This experiment also helped to reveal another important characteristic of our method. It could correctly reject several answers without using any information about the confidence of streams and with-out considering any restriction on the answer frequencies. In particular, our method correctly rejects a 64% of the unan-4.2.3. Results analysis
The results from the first experiment showed us that our multi-stream QA method could satisfactorily reach its main ment of 41%. This improvement was more evident in the case of factual questions (refer to Table 7 ), where our method out-observe the low average accuracy of the input streams.
 Concerning the results from the traditional multi-stream QA methods, we may observe the following: First, the methods that rank answers based on the stream confidences, namely, the Ordered-Skimming Method and the
Dark-Horse Method, also obtained relevant results. Nevertheless, it is necessary to mention that X  X n our implementations X  these methods made use of a perfect estimation of these confidences practically impossible to obtain these perfect estimations, we consider that our proposal is more robust than these two methods.

Second, the results also give evidence that the presence of several deficient streams (which generate a lot of incorrect approaches.

It is also important to comment that we attribute the poor results achieved by the Web Chorus Method to the amount of sults, it is necessary to apply some question/answer expansions using for instance synonyms and hyperonyms. Finally, Jijkoun and de Rijke (2004) describe a multi-stream QA architecture that combines the Answers Chorus and the
Dark-Horse Methods. Its evaluation results indicated that this combination outperformed the results obtained by other sys-tems based on one single traditional multi-stream strategy. Our experimental results confirmed the conclusions of Jijkoun and de Rijke, but most important, they demonstrated the competence of our method since it could outperform the result from this hybrid approach. 5. Conclusions and future work
In this paper, we proposed a multi-stream QA method supported on a supervised learning approach. This method is founded on the idea of combining the output of different QA systems (streams) in order to obtain a better performance.
Mainly, it is a new kind of hybrid approach that combines features from the answer-chorus and textual-entailment approaches.

The proposed method differs from previous multi-stream approaches in the following concerns: first, it does not consider any information about the confidence of the input QA systems; second, it does not consider the answer frequencies as the ment relation between the question X  X nswer pair and the support text. In addition, our method is only based on a lexical X  sources. All these features together make our method very appropriate for dealing with poor performance QA systems, which test set, where current average answer accuracy is of 26% (please refer to Table 4 ).

Concerning the kind of used attributes, it is important to emphasize that, to our knowledge, our method is the first at-tempt to consider some features describing the non-overlapped information between the question X  X nswer pair and the sup-port text. Certainly, an evaluation of the proposed features during the development phase X  X sing the information gain measure X  X howed us that the non-overlap and answer-redundancy attributes were the most discriminative.
From the evaluation results achieved on a test set of 190 Spanish questions gathered from the CLEF-2006 QA collection, we could observe the following:
The proposed method significantly enhanced the estimated QA performance from the best individual stream. Its result (0.74) outperformed the best QA participating system (0.53) by a relative improvement of 41%. This relative increment was even better for factual questions, reaching a 55%.

Although our method also takes advantage of the redundancy of answers across streams, it turned out to be less sensitive to their low frequency than other approaches. In particular, it outperformed the Answer-Chorus Method by 33%.
Our method significantly outperformed the results from the Ordered-Skimming and Dark-Horse Methods, even though they used a perfect estimation of the system X  X  confidences. This fact indicates that our method does not require any knowledge about the input streams, and, therefore, that it can be more easily adapted to different application scenarios.
The proposed method allowed to significantly reduce the number of incorrect answers presented to the user. In relation to this point, our method was especially adequate to deal with questions having no correct answers. Particularly, it cor-rectly rejected 64% of these questions, outperforming other multi-stream QA approaches in more than 85%.
Finally, it is clear that any improvement in the answer validation module will directly impact the performance of the pro-posed multi-stream method. Therefore, our future work will be focused on enhancing this module. In particular we plan to consider some new features in the entailment recognition process. On the one hand, we plan to include some additional dis-criminative features that allow describing with more detail the overlap between the question X  X nswer pair with the support text. Mainly, we are considering using complex syntagms such as prepositional phrases and conjunctions/disjunctions. On the other hand, we plan to use Wordnet in order to consider synonyms and hyperonyms for computing the term and struc-ture overlaps.
 Acknowledgements This work was done under partial support of CONACYT (Project Grant 43990 and scholarship 171610). We also thank the CLEF organizers.
 References
