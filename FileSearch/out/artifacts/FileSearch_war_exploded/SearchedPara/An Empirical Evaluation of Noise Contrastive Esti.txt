 The Neural Network Joint Model of Translation, or NNJM (Devlin et al., 2014), is a strong feature for statistical machine translation. The NNJM uses both target and source tokens as context for a feed-forward neural network language model (LM). Un-fortunately, its softmax layer requires a sum over the entire output vocabulary, which slows the calcula-tion of LM probabilities and the maximum likeli-hood estimation (MLE) of model parameters.

Devlin et al. (2014) address this problem at run-time only with a self-normalized MLE objective. Others advocate the use of Noise Contrastive Esti-mation (NCE) to train NNJMs and similar mono-lingual LMs (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2015; Zhang et al., 2015). NCE avoids the sum over the output vo-cabulary at both train-and run-time by wrapping the NNJM inside a classifier that attempts to sep-arate real data from sampled noise, greatly im-proving training speed. The training efficiency of NCE is well-documented, and will not be evaluated here. However, the experimental evidence that NCE matches MLE in terms of resulting model quality is all on monolingual language modeling tasks (Mnih and Teh, 2012). Since cross-lingual contexts pro-vide substantially stronger signals than monolingual ones, there is reason to suspect these results may not carry over to NNJMs.

To our knowledge there is no published work that directly compares MLE and NCE in the context of an NNJM; this paper fills that gap as its primary con-tribution. We measure model likelihood and trans-lation quality in large-scale Arabic-to-English and Chinese-to-English translation tasks. We also test a recently-proposed translation noise distribution for NCE (Zhang et al., 2015), along with a mixture of noise distributions. Finally, we test a widely known, but apparently undocumented, technique for domain adaptation of NNJMs, demonstrating its utility, as well as its impact on the MLE-NCE comparison. The NNJM adds a bilingual context window to the machinery of feed-forward neural network language models, or NNLMs (Bengio et al., 2003). An NNLM calculates the probability p ( e i | e i  X  1 word e i given its n  X  1 preceding words, while an NNJM assumes access to a source sentence F and an aligned source index a i that points to the most influ-ential source word for the next translation choice. It calculates p ( e i | e i  X  1 2 m + 1 words of source context, centered around f the conditioning context, which we will generalize with the variable c i = e i  X  1 biguous, we drop the subscript i from e and c .
The feed-forward neural network that powers both models takes a context sequence c as input to its network, which includes an embedding layer, one or more hidden layers, and a top-level softmax layer that assigns probabilities to each word in the vocabu-lary V . Let s c ( e ) represent the unnormalized neural network score for the word e . The softmax layer first calculates Z c = to then normalize the score into a log probability log p ( e | c ) = s c ( e )  X  log Z c . Given a training set of word-context pairs, MLE training of NNJMs mini-mizes the negative log likelihood
The problem with this objective is that calculat-ing Z c requires a sum over the entire vocabulary, which is very expensive. This problem has received much recent study, but Devlin et al. (2014) pro-posed a novel solution for their NNJM, which we refer to as self-normalization . Assume we are will-ing to incur the cost of calculating Z c during train-ing, which might be mitigated by special-purpose hardware such as graphical processing units (GPUs). One can modify the MLE objective to encourage log Z c to be small, so that the term can be safely dropped at run-time: where  X  trades self-normalization against model likelihood. Devlin et al. (2014) have shown that self-normalization has minimal impact on model quality and a tremendous impact on run-time efficiency. 2.1 Noise Contrastive Estimation Introduced by Gutmann and Hyv  X  arinen (2010) and first applied to language modeling by Mnih and Teh (2012), NCE allows one to train self-normalized models without calculating Z . It does so by defin-ing a noise distribution q over words in V , which is typically a unigram noise distribution q u . It samples k noise words  X  e k 1 for each training word e , and wraps the NNJM inside a binary classifier that attempts to separate true data from noise. Let D be a binary variable that is 1 for true data and 0 for noise. We know the joint noise probability p ( D = 0 ,e | c ) = k the joint data probability using our neural network p ( D = 1 ,e | c )  X  1 that the final approximation drops Z c from the calcu-lation, improving efficiency and forcing the model to self-normalize. With these two terms in place, and a few manipulations of conditional probability, the NCE training objective can be given as:  X  which measures the probability that data is recog-nized as data, and noise is recognized as noise.
Note that q ignores the context c . Previous work on monolingual language modeling indicates that a unigram proposal distribution is sufficient for NCE training (Mnih and Teh, 2012). But for bilingual NNJMs, Zhang et al. (2015) have shown that it is beneficial to have q condition on source context. Re-call that c i = e i  X  1 a translation noise distribution q t ( X  e | f a mate q t by relative frequency from our training cor-pus, which implicitly provides us with one e i ,f a pair for each training point e i ,c i . Conditioning on f tribution, focusing training on the task of differenti-ating between likely translation candidates.
As our experiments will show, under NCE with translation noise, the NNJM no longer provides meaningful scores for the entire vocabulary. There-fore, we also experiment with a novel mixture noise distribution: q m ( X  e | f a We implement our NNJM and all candidate train-ing objectives described above in a shared codebase in Theano (Bergstra et al., 2010). To ensure a fair comparison between MLE and NCE, the various systems share code for model structures and algo-rithms, differing only in their training objectives. A GeForce GTX TITAN GPU enables efficient MLE training. Following Devlin et al. (2014), all NNJMs use 3 tokens for target context, a source context win-dow with m = 5 , and a 192-node embedding layer. We deviate from their configuration by using a sin-gle 512-node hidden layer, motivated by our inter-nal development experiments. All NCE variants use k = 100 noise samples.

NNJM training data is pre-processed to limit vo-cabularies to 16K types for source or target inputs, and 32K types for target outputs. We build 400 deterministic word clusters for each corpus using mkcls (Och, 1999). Any word not among the 16K / 32K most frequent words is replaced with its cluster.
We train our models with mini-batch stochastic gradient descent, with a batch size of 128 words, and an initial learning rate of 0.3. We check our training objective on the development set every 20K batches, and if it fails to improve for two consec-utive checks, the learning rate is halved. Training stops after 5 consecutive failed checks or after 60 checks. As NCE may take longer to converge than MLE, we occasionally let NCE models train to 90 checks, but this never resulted in improved perfor-mance. Finally, after training finishes on the com-plete training data, we use that model to initialize a second training run, on a smaller in-domain training in-domain pass uses a lower initial learning rate of 0.03.

Our translation system is a multi-stack phrase-based decoder that is quite similar to Moses (Koehn et al., 2007). Its features include standard phrase table probabilities, KN-smoothed language mod-els including a 6-gram model trained on the En-glish Gigaword and a 4-gram model trained on the target side of the parallel training data, domain-adapted phrase tables and language models (Fos-ter and Kuhn, 2007), a hierarchical lexicalized re-ordering model (Galley and Manning, 2008), and sparse features drawn from Hopkins and May (2011) and Cherry (2013). It is tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). We test two translation scenarios drawn from the recent BOLT evaluations: Arabic-to-English and Chinese-to-English. The vital statistics for our cor-pora are given in Table 1. The training set mixes Lang. Train In-dom Dev Test1 Test2 Arabic 38.6M 1.8M 72K 38K 40K Chinese 29.2M 1.9M 77K 38K 36K NIST data with BOLT-specific informal genres. The development and test sets are focused specifically on the web-forum genre, as is the in-domain sub-set of the training data ( In-dom ). The Arabic was segmented with MADA-ARZ (Habash et al., 2013), while the Chinese was segmented with a lexicon-based approach. All data was word-aligned with IBM-4 in GIZA++ (Och and Ney, 2003), with grow-diag-final-and symmetrization (Koehn et al., 2003). 4.1 Comparing Training Objectives Our main experiment is designed to answer two questions: (1) does training NNJMs with NCE im-pact translation quality? and (2) can any reduction be mitigated through alternate noise distributions? To this end, we train four NNJMs.  X  MLE : Maximum likelihood training with self- X  NCE-U : NCE with unigram noise  X  NCE-T : NCE with translation noise  X  NCE-M : NCE with mixture noise and compare their performance to that of a system with no NNJM. Each NNJM was trained as de-scribed in Section 3, varying only the learning ob-report average negative log likelihoods (NLL) and average | log Z | , both calculated on Dev. Lower NLL scores indicate better prediction accuracy, while lower | log Z | values indicate more effec-tive self-normalization. We also provide average BLEU scores and standard deviations for Test1 and Test2, each calculated over 5 random tuning repli-cations. Statistical significance is calculated with MultEval (Clark et al., 2011).
 Our results are shown in Table 2. By comparing MLE to no NNJM, we can confirm that the NNJM is a very effective translation feature, showing large BLEU improvements on all tests. By comparing MLE to NCE-U, we can see that NCE training does reduce translation quality. NCE-U outperforms hav-ing no NNJM, but lags behind MLE considerably, resulting in significantly worse performance on all tests. This is mitigated with translation noise: NCE-T and NCE-M both perform significantly better than NCE-U. Furthermore, in 3 out of 4 tests, NCE-T matches or exceeds the performance of MLE. The one Arabic-to-English case where NCE-T exceeds the performance of MLE is particularly intriguing, and warrants further study.

Though NCE-T performs very well as a trans-lation feature, it is relatively lousy as a language model, with abnormally large values for both NLL and | log Z | . This indicates that NCE-T is only good at predicting the next word from a pool of reasonable translation candidates. Scores for words drawn from the larger vocabulary are less accurate. However, the BLEU results for NCE-T show that this does not matter for translation performance. If model like-lihoods over the complete vocabulary are needed, one can repair these estimates by mixing in uni-gram noise, as shown by NCE-M, which achieves the same or better likelihoods than NCE-U, with comparable BLEU scores to those of NCE-T.
 Devlin et al. (2014) suggest that one drawback of NCE with respect to self-normalized MLE is NCE X  X  lack of an  X  hyper-parameter to control the objec-tive X  X  emphasis on self-normalization. However, the | log Z | values for NCE-U are only slightly lower than those of MLE, and are larger than those of the superior NCE-M. This suggests that we could not have improved NCE-U X  X  performance by adjusting its emphasis on self-normalization.
 4.2 Impact of the Domain Adaptation Pass We began this project with the hypothesis that NCE may harm NNJM performance. But NCE-U per-formed worse than we expected. In particular, the differences between NCE-U and NCE-T are larger than those reported by Zhang et al. (2015). This led us to investigate the domain adaptation pass, which was used in our experiments but not those of Zhang et al. This step refines the model with a second train-ing pass on an in-domain subset of the training data. We repeated our comparison for Arabic without do-main adaptation, reporting BLEU averaged over two test sets and across 5 tuning replications. We also re-port each system X  X  BLEU differential  X  with respect to MLE. The results are shown under General in Ta-ble 3, while Adapted summarizes our results from Table 2 in the same format.

The domain adaptation step magnifies the differ-ences between training objectives, perhaps because it increases performance over-all. The spread be-tween the worst and best NNJM is only 0.3 BLEU under General , while it is 0.8 BLEU under Adapted . Therefore, groups training unadapted models may not see as large drops from NCE-U as we have re-ported above. Note that we experimented with sev-eral configurations that account specifically for this domain-adaptation pass (noise distributions based on general versus in-domain corpora, alternate stop-ping criteria), so that NCE-U would be presented in the most positive possible light. Perhaps most importantly, Table 3 shows that the domain adapta-tion pass is quite effective, producing large improve-ments for all NNJMs. 4.3 Impact on Speed MLE and NCE both produce self-normalized mod-els, so they both have the same impact on decoding speed. With the optimizations described by Devlin et al. (2014), the impact of any single-hidden-layer NNJM is negligible.

For training, the main benefit of NCE is that it reduces the cost of the network X  X  output layer, re-placing a term that was linear in the vocabulary size with one that is linear in the sample size. In our experiments, this is a reduction from 32K to 100. The actual benefit from this reduction is highly implementation-and architecture-dependent. It is difficult to get a substantial speedup from NCE us-ing Theano on GPU hardware, as both reward dense matrix operations, and NCE demands sparse vector operations (Jean et al., 2015). Therefore, our deci-sion to implement all methods in a shared codebase, which ensured a fair comparison of model quality, also prevented us from providing a meaningful eval-uation of training speed, as the code and architec-ture were implicitly optimized to favour the most de-manding method (MLE). Fortunately, there is ample evidence that NCE can provide large improvements to per-batch training speeds for NNLMs, ranging from a 2  X  speed-up for 20K-word vocabularies on a GPU (Chen et al., 2015) to more than 10  X  for 70K-word vocabularies on a CPU (Vaswani et al., 2013). Meanwhile, our experiments show that 1.2M batches are sufficient for MLE, NCE-T and NCE-M to achieve very high quality; that is, none of these methods made use of early stopping during their main training pass. This indicates that per-batch speed is the most important factor when comparing the training times of these NNJMs. We have shown that NCE training with a unigram noise distribution does reduce NNJM performance with respect to MLE training, both in terms of model likelihoods and downstream translation quality. This performance drop can be avoided if NCE uses a translation-aware noise distribution. We have em-phasized the importance of a domain-specific train-ing pass, and we have shown that this pass magni-fies the differences between the various NNJM train-ing objectives. In a few cases, NCE with transla-tion noise actually outperformed MLE. This sug-gests that there is value in only considering plausi-ble translation candidates during training. It would be interesting to explore methods to improve MLE with this intuition.
 Thanks to George Foster, Eric Joanis, Roland Kuhn and the anonymous reviewers for their valuable comments on an earlier draft.

