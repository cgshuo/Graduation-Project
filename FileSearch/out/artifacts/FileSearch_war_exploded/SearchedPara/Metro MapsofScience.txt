 As th e nu mber of scien tific pub lic ations soars, even th e most enthu sia stic reader can h ave troub le staying o n t op of th e evolvin g lit erature. It is easy to focus on a narrow aspect of one X  X  field and lose track of th e big p ic tu re. Information overloa d is indeed a major challen ge for scien tists today, and is esp ecially daunt ing for new in vesti ga tors att empt in g to master a discip lin e and scientists who seek to cross disc i-plin ary borders. In th is paper, we propose metrics of influ -ence, coverag e, and conn ectivit y for scientifi c lit eratu re. We use th ese metrics to create structured summari es o f informa-tion , which we call metro maps . Most imp ortantly , metro maps explic itly show the rela tion s between p apers in a way whic h captu res develop ments in the field . Pilo t u ser stud ies demo nstrate th at our meth od can h elp resea rchers acquire new knowled ge efficien tly: map u sers achiev ed better preci-sio n and recall scores and found more seminal p apers while performing fewer sea rches.
 H.3.1 [ Info rmatio n Storage and Retrieval ]: Cont ent Analysis and Indexin g; H.3.3 [ Info rmatio n Sto rage and Retrieval ]: Informatio n S earch and R etriev al; H.5 [ Info rmatio n Interfaces and Presentation ] Metro maps, Information, Su mmariz atio n  X  X ist ringit lib rorum m ultitud o X  (th e abund ance of boo ks is a distra ction) , said Lucius Ann aeus Seneca; he liv ed in th e first centu ry.

A lot h as changed sin ce th e first centu ry, but Lucius  X  prob-lem has only become worse. The surge of th e Web brought down th e barriers of dist ribut ion , and th e scien tific c ommu-nit y find s it self overwhelmed by th e in creasin g nu mbers of pub lic ations; relev ant d ata is often bu ried in an avala nche of pub lic ations, and lo cating it is diffic ult.

Search engines have been relied up on in recent years for acc essin g the scientific lit erature, and investments have even been made to create special academic search and retrieval too ls. However, the search and b rowsin g experien ce might be best characteriz ed as providin g keyh ole views ont o th e lit eratu re: wh ile search engin es a re hig hly effective in re-trievin g scientific pub lic ation s, th e task of fitt ing th ose pub -lic ations int o a coherent p ic ture remain s diffic ult.
In contrast, we are intereste d in methods th at explici tly show th e relation ships among pub lic ations in a way th at captu res th e main d evelo pmen ts in th e disc iplin e. We be-liev e that such meth ods can allo w a user to explo re a new, complex topic and d isco ver hidden conn ection s effectively . We consid er as a sample motiv ation th e creatio n of valu able lit eratu re explora tio n t oo ls th at could help p eople entering a new field , such as new gradu ate stud ents or exp erts reaching beyond th eir tradition al disc iplin ary borders.

Several tools alrea dy exist for summa rizin g a nd visu aliz ing scien tific lit eratu re (see [Borner, 2010 ] for a compendium). However, th e output of these syst ems is often not suita ble for a starting researc her. Some syst ems X  level of granula rit y is too coa rse: Boyack et al. [2009 ] provide a g raph -summary of chemistry resea rch, where each n ode correspond s to a clus-ter of disci plin es ( X  X iolo gy-Zoolog y-Ecology X ). Bassecoula rd and Zitt [1999 ] produ ce a hierarc hic al graph , where nodes corresp ond t o clus ters of journals.

We believ e th at in order to allo w resea rchers to und er-stand h ow a field is organize d, a fin er lev el of granu la rity is needed . For th is reason, we chose pap ers as our un it of analysis. Most current tools th at work at th is lev el of granu -lari ty provid e visu aliz ations of cita tion (or co-citation) n et-works, where papers are nodes [Ch en, 2004 ; Dunn e et al., 2010 ]. Importantly, edges between papers are based on lo-cal comput ation : th e edges a re selec ted because th ey pass some thresho ld , or belon g to a spann in g tree. In such meth -ods, th ere is no notio n of coherent lines of research . We believ e th at th e notion of story lin es is essen tia l, and facili-tates users X  knowled ge acquisit ion and comprehension of th e front ier and evolutio nary hist ory of ideas in a disc ip lin e.
Several syste ms have attemp ted t o create story lin es, es-pecial ly in th e news domain [Sw an and Jensen , 2000 ; Yan et al., 2011 ; Allan et al., 2001 ]. However, th is style of sum-marization only works for simple stories, which are lin ear in natu re. In cont rast, resea rch fi elds displa y a very non-l inear behaviou r: lin es o f resea rch b ranch lik e a tangle of spag hetti with sid e stories, dead ends, and int ertwinin g narratives. In order to explo re these stories, one needs a map as a g uid e th rough un familia r territory.

The metro map meta ph or has been used befo re to dis-pla y abstract knowledg e. For exam ple, Nesbitt X  X  map shows intercon n ecting id eas runn ing th rough h is PhD th esis [Nes-bitt, 200 4]. However, th ese maps were all manu ally con-stru cted. In th is paper, we adapt th e techn iqu es o f [Shahaf et al., 2012 ] (previo usly app lied to news articles) to con-stru ct metro maps of scien tific lit eratu re automatically . Our main contribution s are as follo ws:  X  Formaliz in g metrics characteriz ing goo d metro maps, tak-in g a dvantage of th e add ition al stru cture encoded in th e scien tific domain :  X  Characterizin g th e probabilit y th at id eas in two papers  X  Quant ifyin g th e imp act of one paper on th e corpus.  X  Proposin g a notio n of conn ectivit y th at captu res how  X  Providin g efficien t methods with th eoretical guarant ees to comput e th ese metri cs and find a diverse set of hig h-imp act, coherent resea rch lin es a nd th eir interaction s.  X  Int egrating user preferenc es int o o ur fram ework by pro-vid ing a n app ropria te user-interaction model.  X  Performin g valid ation stud ies with u sers th at high light th e promise o f th e methodology. Our meth od outp erforms popu lar competitors.
We first review th e desired properties of a metro map, follo win g th e criteria o ut lin ed in [Shahaf et al., 2012 ]. We shall briefly presen t th ese criteria , motiv ate and formaliz e th em. Later, we present a prin cipled app roach t o constru ct-ing maps that optimiz es tradeoffs among these criteria . Be-fore we begin, we formally define metro maps.
 Definiti on 2.1 (Metro Map [Sh ahaf et al., 2012 ]) . A metro map M is a pair ( G,  X ), where G = ( V , E ) is a directed graph and  X  is a set of path s in G . We refer to path s as metro lines . Each e  X  E must belon g to a t leas t one lin e. The vertice s V corresp ond to scientifi c papers, and are de-noted by do cs ( M ). The lin es of  X  corresp ond to aspects of th e field . A key requiremen t is th at each lin e is coh erent : follo win g the papers along a lin e should give the user a clear und ersta nd in g o f the evolu tion of a story.

Coherence is crucial for goo d maps, but is it s ufficient as well? In order to put th is matter to a test, we comput ed maxima lly coherent lin es for th e set of papers returned in re-sponse to th e query  X  X u pp ort vector machines X  (u sin g meth-ods detailed belo w). The results were disc ouragin g. While th e lin es were indeed coherent, th ey were not important . Many of the lin es revolv ed around n arrow topic s; many fo-cused on a sin gle resea rch group , never expa nd in g beyond it.

The e xample suggest s th at maximiz in g coherence does not guarantee goo d maps. Instead, th e key challeng e is balan c-ing coherence and coverage : in add ition t o being coherent, lin es should cover topics th at are important t o the user.
Finally , a map is more th an just a set of lin es; th ere is information in its structure as well. Public ation s offer a ric h palett e of int eract io n p ossibilit ies: assumption, affirmation , contrast, methodology, rela ted work, and more. Th erefore, our last p roperty is connec tivity . The map X  X  connec tiv-ity should convey the und erlyin g structure of the field , and how differe nt lin es of resea rch interact with each oth er.
In S ection s 3-5 , we formaliz e coherence , coverage and connec tivity . In S ection 6, we e xplore tradeoffs among th em and combine th em into a sin gle objective function t o guide the construction of maps. How should we mea sure coherence for a chain of papers? We rely on th e not ion of coherence develo ped in Conn ect-th e-Dots (CTD) [Sh ahaf and Gu estrin , 2010 ]. In th e follo w-ing, we briefly review th is app roach.

In order to define coherence, a natu ral first step is to mea-sure similari ty between ea ch t wo consecu tiv e papers alon g th e chain . As a sing le bad transit ion can destroy th e coher-ence of an entire chain , we measure th e strength of the chain by th e strength of it s weakest link .
 However, th is simp le approach can p rodu ce poo r chains . Consid er, for exa mple, Chain A above. The transition s of Chain A are all reasonable when examined out of cont ext. The first t wo a rticles a re about d ebt defau lt; th e seco nd and th ird mention Repub lica ns. Despite these local conn ectio ns, th e overall effect is asso cia tive and incoherent.

Now, consid er Chain B. This chain h as the same end-points, but it is sign ifi cantly more coherent. Let us take a closer look at th ese two chain s. Figu re 1 shows word ap-pearance along both chains . For exa mple, th e word  X  X reece  X  app eared th roughout Chain B. It is easy to spot th e asso-ciat ive flow of Chain A in Figu re 1. Words app ear for short stretches; some words app ear, then d isa pp ear and reapp ear. Cont rast this with Chain B, where stretches a re lon ger and transiti ons between d ocuments are smoo ther. This observ a-tion motivates o ur defin ition of coherence. Figure 1: Word patterns in Chain A (left) a n d Chain B
We represen t d ocuments as vectors of concepts (for th e sake of presentation , assu me c oncepts C are words). Giv en a chain of papers ( p 1 , ..., p n ), we first score each t ransiti on p  X  p i +1 by th e nu mber of concepts both articles share: However, word app earance is too noisy . Articles must use th e exac t same word s; syn onyms are treated as un relat ed. Also , all words are treated equa lly: th e word  X  X reec e X  is as imp ortant as the word  X  X oday X .

Therefore, we replaced th e ind icator function 1 ( ) with a notion of influe nce of concept c in a transit ion . Intu iti vely , Influen ce ( p i , p j | c ) is high if (1) b oth d ocuments are high ly conn ected, and ( 2) c is imp ortant for th e conn ectivit y. Note th at c does not have to app ear in eith er of th e documents. After the introd uction of influ ence, th e objective becomes:
Coherence ( p 1 , ..., p n ) = min This objective guarant ees goo d t ransition s, but associat ive chains lik e Chain A can sti ll score well. However, these chains need to use many words in order to a chiev e hig h scores, as many of th eir transiti ons use a un iqu e set of words. On the other hand , coherent chain s (lik e Chain B) can often be characterized by a small set of word s, which are imp or-tant th roughout many of th e transition s.

Therefore, in stead of summing Influen ce ( c | p i , p i +1 all concepts, th e problem is transformed int o a n optimiz a-tion p roblem, where th e goa l is to choose a small set of concepts (called  X  X ctive X ), and score the chain based only on th ese concepts. Constrain ts on p ossible activations enforce a sma ll nu mber of words and smooth transit ion s, imita ting th e behavio ur of Figu re 1 (righ t) . Formally ,
Coherence ( p 1 , ..., p n ) = max Finally , th e coherence of a map is defined as th e minima l coherence across its lin es  X .
The coherence notion of [Shahaf and Gu estri n, 2010 ] (Def-inition 3.1) was develo ped for th e news domain, and relied excl usiv ely on article content . It was design ed t o use very basic features, namely words. However, th e simp lic ity of th e represen tation can sometimes resu lt in incoherent chains . To illu strate th e problem, consid er th e follo win g three papers: p 1 : Multiage nt plannin g with factored MDPs / p 2 : Timing and power issues in wireless s ensor disconnec ted delay-tolerant manet s / Daly et al /
These papers share many words, such as  X  X  etwork X ,  X  X rob-abilit y X  and  X  X ost X , and thu s can achiev e a goo d coherence score. However, they clearly do not follo w a coherent re-sear ch lin e. The problem may be alleviat ed by high er-level featu res (e.g ., disti nguish ing between d iffe rent u ses o f  X  X et-work X ); in this sec tion , we choose instead to take advant ag e of th e sid e in formation p rovided b y th e cita tion graph , and define a coherence notion more suited for scientific papers.
Upon close exam ination , our orig inal coherence notion (Definitio n 3.1) is composed of two main id eas: comput -ing th e influ ence of concepts on transit ion s, and choo sin g a small set of active concepts that captu res th e story well. While the latt er id ea seems domain-ind epen dent, comput -ing influ ence may benefi t from th e add ition al stru cture of th e citation graph .

The citation graph explic itly captu res th e way papers in -flu ence each oth er: th e cont ent of a pub lic atio n is often af-fected by cited work, th e auth ors X  prio r work and n ovel in -sig ht s. The influ ence notion p roposed in BKS [El-Arini and Guestrin, 2011 ] captu res exactly th is behaviou r. In BKS, the auth ors define a directed, acyclic graph G c for every concept c in th e corpus. Nodes represen t p apers th at cont ain c and th e edges represen t citatio ns and commo n auth orship.
To captu re th e degree of influ ence, BKS defin es a weight  X  u,v for each edge u  X  v in G c , represen ting th e probabilit y of direct influ ence from paper u to paper v with resp ect t o concept c . Some probabilit y is assig ned to  X  X  ovelt y X , th e case th at concept c in paper v was novel.

Giv en a concept-specifi c weig ht for each edge in G c defines a probabilist ic, concept-specific notion of influ ence between any two papers in th e document collec tion : Let G r c be a rand om subgraph of G c , where every edge u  X  v is in cluded in G r c with p robabilit y  X  u,v . The influ ence be-tween papers p i and p j w.r.t . c is th e probabilit y th ere exists a direct ed path in G r c between p i and p j .

The BKS notion of influ ence has many attractive proper-ties: it is simp le, and it app ears to captu re the way ideas travel alon g the citation graph . However, usin g it for co-herence sev erely limits th e chain s we can hope to id entify. Accordin g to definiti on 3.1, th e only pairs of papers that can have influ ence between th em are ancestor-desc endant p airs in G c . Therefo re, chain s with h ig h influ ence are lik ely to contain only papers th at d irectly bu ild on top of one an-oth er, especial ly papers by th e same auth ors.

Consid er papers p 2 a nd p 3 from above. Their notion of  X  X  etwork X  is si milar, but th ere is no direc t p ath from p 2 to p 3 in th e corresp ond ing g raph . To mitigate th is problem, we in trodu ce a differe nt n otion of influ ence. Rather than requir-in g th at p i influ ence p j , we are only interested in whether concept c in p i and concept c in p j refer to th e same idea. To captu re th is property, we modify th e notion of influ ence: Definit ion 3.2 ( Ancest ral Influ ence) . The influ ence between papers p i and p j with respect to concept c is th e probabilit y p and p j have a common ancesto r in G r c .
See Fig ure 2 for an illu stration of th e differen ce between direc t infl u-ence (left) and ancestra l influ ence (right) . In order for p i to have di-rect influ ence on p j , th ere has to be a path from p i to p j . In order for p i to have ancestral influ ence on p j , it is su fficien t th at th ey have a com-mon ancestor in th e graph . The an-cestor can also be p i its elf.

As for p 2 a nd p 3: with n o direc t p ath among them, th eir direc t influ ence is zero. However, as both cite Perkins  X  1999 networks paper, their ancestra l influ ence is non-zero.
In add itio n to coherence, we need to ensure th at th e map has high coverag e. Before defining coverag e of a map, we need to und ersta nd which elemen ts we wish to cover.
In [Sh ahaf et al., 2012 ], we only had th e articles X  cont ent to rely up on, and thu s th e covered elemen ts were concepts . We denoted th e amount an article p covered a concept c by cover p ( c ), and loo ked for a set of articles th at, when com-bin ed, achiev ed high coverag e for many important concepts.
However, when we app lied th e same techn iqu e to scien -tific papers, we encoun tered a problem: p apers with simila r content may app ear exc hangeable w.r.t. their coverag e, but th ey will not n ecessa rily be equiv alent in the user X  X  eyes. For exa mple, th e user may notice that th e papers aim at d iffer-ent comm un it ies, or th at one paper is more semin al th an th e oth er. Consid er th e follo wing two papers: Figure 3: Tag clo ud s for p 1 and p 2 . Th e size of a word is p 1 : SVM in Oracle database 10 g: Removing the barri ers to widesprea d adop tion of support vector VLDB  X 05 Procee dings of the 31st International Conference
SVM  X 02 Procee dings of the First International Workshop on Pattern Rec ognition with Supp ort Vector Machines
The cont ent of p 1 a nd p 2 is si milar. Figures 3(a)-(b) d is-pla y th e papers as tag cloud s: both p apers share many of th eir imp ortant words ( X  X ata X , X  X  atabase X ,  X  X vm X ,  X  X mp lemen-tation X ). Numerou s oth er words have a closely rela ted match ( X  X erformance X /  X  X ffic ienc y X ,  X  X ra cle X /  X  X ela tional database X ) .
One way to disti nguish b etween th e aforemen tion ed pa-pers is to exam in e th eir impact . Fig ures 3(c )-(d) show tag cloud s of auth ors and venues for papers citing p 1 a nd p 2. Figu re 3(c) has more words than 3(d), implyin g th at p 1 has affected mo re un iqu e authors and venu es than p 2. Int erest-ingly, despite the simila r cont ent of th e papers, th ere is al-most n o intersect io n b etween th e papers citin g them; only a sin gle paper cites both ( Mona Habib from Mic roso ft Cairo).
Based on th is intu ition , we propose to use th e papers th emselv es a s elemen ts of coverag e. A paper p should cover its elf and th e papers it has had imp act on. By this defini-tion , a high -coverag e set of papers consist s of papers th at, when combin ed, had imp act on a large portion of th e corpus.
The idea that a paper covers its descendant s (and n ot its ancesto rs) may seem co unt erintu itive at fi rst. After all, how can a paper cover future cont ribu tions? Nevertheless, we believ e th at exam inin g a paper X  X  ancesto rs merely help s und ersta nd in g the cont ext in whic h th e paper was written, while its desce nd ants truly reveal th e gist of its cont ribu tion.
We would lik e papers to cover th eir descendant s. Instead of a hard, bin ary notio n of coverag e, we prefer a softer no-tion , allo win g us to exp ress th at d escendant s are covered to vario us degrees (d epict ed as a g radient in Fig ure 4a ). Figure 4: A sim ple citation graph . Edges traverse in
Let us concentrate on th e degree to whic h p aper p covers its descendant q , cover p ( q ). In order to evalu ate the imp act th at p had on q , we e xamine th e way q is conn ected to p in th e citation graph . Intu itiv ely , if q can b e reached from p by many path s, p had a hig h imp act on q . Sin ce impact is dilu ted with each step, shorter paths are more imp ortant th an lon ger ones.

Before we devise a coverag e formulation b ased on p ath s between p and q , we consid er anoth er point: imp act is not necessa rily transit ive. Consid er, for exa mple, Figure 5. The figure out lin es a (small) fraction of th e desce nd ants of Ni-colo Cesa-Bianchi X  X  paper,  X  X  ow to Use Exp ert Advice X . As befo re, edges indica te cita tion . A snippet from the citation text app ears by each edge. Figure 5: Tw o branches in the citatio n gra ph. Th e left
The left branch of Figure 5 revolv es a rou nd Onlin e Learn-in g Theory. The papers in th is branch ( #2 and #3) bu ild on top of each other. Intu iti vely, th e root p aper had imp act on both of th em. In contrast, th e right b ranch is more diffic ult to follo w. Both d escendant s deal with ext ending th e battery life of devic es, but while paper #4 is a direct a pp lic ation of th e root p aper, paper #5 is not. In fact, when #5 cites #4, th e citation reads  X  X ote that our protocol is diffe rent from previo us work X . In oth er words, paper #5 is no lon ger relev ant t o the root node, and should n ot b e covered by it.
The difference between th e two branches can b e captu red by th e coherence notion of Section 3: The left branch is much more coherent th an th e right one. Based on th at intu ition , we only want a paper to cover th e desc endants th at can b e reached by a coh erent path . Unlik e Section 3, we are only in -terested in direct-influ ence coherent chain s (Defin ition 3.1) , as th ey model th e true impact of a paper.
In th e previou s sec tion , we provided desid erata for cover coverag e is high if th ere are many short and coherent p ath s between p and q . In order to formalize this id ea, we emplo y th e techn iqu e of rand om walks.

Let q be a paper. Conside r a walk from q to its ances-tors, takin g o nly coherent p aths in to a cc ount . At each step, th e walk er either termin ates (with p robabilit y  X  ) , or choo ses an ancestor un iformly at rand om among th e coherent p aths th at extend th e current walk. If th ere are many short, co-herent p ath s between p and q , th ere is a high p robabilit y th at th e walk reaches p befo re termin ation. We denote this probabilit y by cover p ( q ).

Let us formaliz e th is intu ition n ow. Sin ce we only consid er coherent p ath s, it is more convenient t o form ulat e coverage in terms of walks performed d irect ly on a coh erence graph G . A coherence graph is a g raph represen ting a ll coherent chains in th e domain (S ee Figu re 6 for an exa mple. In S ec-tion 7.1 we expla in how to encode the graph compactly) . Each vertex v of G corresp ond s to a sin gle paper, wh ic h we denote pap er ( v ); each p aper p may have multip le corre-spond in g vertices in G , which we denote cop ies ( p ). In Figu re 6, cop ies ( p ) are hig hlig hted.

Let G be a coherence graph . For each p aper q , we con-stru ct th e graph G q by reversin g th e direc tion of all edges in G and add in g a n add itio nal vertex, v q . v q is th e startin g vertex of our walk. We conn ect v q to each vertex of G whic h corresp ond s to paper q , cop ies ( q ). This way, a walk from v will always proceed to a copy of q , and th en to its ancestors in th e coherence graph G . Sin ce the graph is a DAG, the probabilit y that a walk reaches vertex v is easy to compu te. We first comput e a topolog ical orderin g o n G q , and compu te th e probabilit ies in th is order: cover v ( q ) = where P ( u  X  v ) is th e probabilit y th e walk er chose to go from vertex u to vertex v . We want th e walker to choose un iform ly among th e coherent p ath s th at ext end th e current walk; in other words, we want to bia s th e walk er towards ancestors that p articip ate in many coherent p aths. There-fore, we comput e for each vertex v th e nu mber of coherent path s th at end in v , #Path ( v ). For example, th e nu mber of path s th at end in th e vertex marked  X  X  X  in Fig ure 6 is two (o,s,n and p ,n). Since G q is a DAG, computing the nu mber of paths takes polyn omial time. The probabilit y th at th e walk er chooses to go from vertex v to vertex u is propor-tion al to #Path : We now have a coverag e not ion for vertices of G . However, we are int erest ed in a coverag e notion for pap ers . In order to comput e th e coverag e of paper p , we need to sum up th e scores of all vertice s in cop ies ( p ): This score corresp ond s to the probabilit y of reachin g p befo re termin ation . In p articula r, sin ce p can n ever app ear more th an once alon g a path in G , this sco re always less than 1.
Now that we have defined coverag e of a sin gle document, let us define coverag e of a map. In order to encourage di-versit y, we view set coverag e as a samplin g procedure: each paper p i in the map t ries to cover document q with p rob-abilit y cover p i ( q ). The coverage of q is th e probabilit y at lea st one of th e documents su cceeded . Thus, if the map alrea dy includ es papers which cover q well, cover M ( q ) is close to 1, and add in g a noth er paper which covers q well provid es very litt le extra coverage of q . This encourages us to pick papers which cover new area s of the graph , promoting diversity .

Figu res 4b and 4c illu strate th is id ea. Supp ose we alread y have paper A in our map, and we need to choose between papers B and C, whose content is simila r. Figu res 4b and 4c show th e effect of choosing B and C, resp ectively. Since B X  X  descendants have alrea dy been covered by A, we would prefer to choo se C. (Note th at sin ce our coverag e is soft, choo sin g B will sti ll provid e ga ins in coverage.)
We now have a way to mea sure how w ell a map covers a sin gle paper. Finally , we want t o measure how w ell a map covers th e entire corpus . Remem ber, our goa l is to ensure that th e map t ouches up on imp ortant aspects of th e corpus. Therefo re, we first assig n weight s  X  q paper q , sign ifyin g th e importance of th e paper. We model th e amount M covers th e corpus as th e weig hted sum of the amount it covers each p aper:
The weight s cause Cover to prefer maps which cover im-portant p apers. They offer a natu ral mec hanism for person-aliz ation: With n o prior knowled ge about th e user X  X  prefer-ences, we set all of th e weig hts to 1. This is equ ivalent t o askin g for a map which covers as much of the corpus as pos-sib le. In S ection 10 we disc uss lea rning weigh ts from user feedback, resu ltin g in a person aliz ed notio n of coverag e.
A map is more than just a set of lin es; there is information in its structure as well. The map X  X  connec tivity should con-vey the und erlyin g structu re of th e story, and h ow diffe rent aspects of th e story interact with each oth er.

In [Shahaf et al., 2012 ] we simply defin e conn ectivit y as th e nu mber of lin es o f  X  th at intersec t:
Unfortunately , th is simple objective does not suffice in th e scientifi c domain . Consid er th e two chains in Figu re 7: th e top chain describ es th e progress of margin clas sifi ers  X  from perceptrons, th rough lin ear SVMs, to kernel machines. The bottom chain describes th e progress of face-recog niti on Figure 7: Tw o coheren t chains (th eory of SVMs, app li-challen ge problems in visio n: from facial featu re lo cation, th rough face detection , to face reco gnition . Both chain s are clearl y relat ed; the visio n p apers use techn iqu es from th e th eory chain. However, th ere is no way to find an articl e th at would b elon g to both chains , un less we sacrifice co-herence consid erably . As a resu lt, maps that optimiz e th e aforemen tioned conn ectivit y notion are often disc onn ected.
Findin g papers th at would b elon g to both chain s may be diffic ult, but we can easily find th eory papers th at h ave had a big imp act on vision p apers. For exa mple, some of th e visio n p apers in Fig ure 7 directly cite papers from th e th eory chain. These citation s are depicted as dashed lin es.
Figu re 7 motivates us to prefer a softer notion of in ter-sectio n. Rath er th an requ estin g that th e lin es in tersect, we also a ccept lin es whic h are relat ed to each oth er: where cover (  X  i ,  X  j ) is the maxima l cover p ( q ) for p  X   X   X  , or vic e versa. We choo se to use th e maxim um (in stead of sum) in order to encourage connections between as many pairs of lin es as possible. Scoring a ll the connections between  X  i and  X  j may lead t o maps where only a few lin es a re very well-c onn ected, and th e rest are disc onn ected. The parameter  X  is chosen empiric ally .

This softer notion of intersec tion is especia lly suited to scien tific lit eratu re. Pub lic ations offer a rich p alette of inter-action p ossib ilit ies, such as affirma tion, criticism, cont rast, meth odolog y, and rela ted work. Exp osin g the relation ships between t wo lin es o f resea rch can p rove e xtremely valu able to researc hers.
Now th at we have formally defin ed our three properties, we can combine th em in to one objective function . We need to consid er tradeoffs among th ese properties: for example, maximiz in g coherence often result s in repetitive, low-coverag e chains . Maximizi ng conn ectivit y encourages choo sin g sim-ila r chains , result ing in lo w coverag e as well. Maximiz ing coverag e lea ds to low conn ectivit y, sin ce th ere is no rea son to re-use an article for more th an one lin e.

The objectiv e of [Shahaf et al., 2012 ] app lies to th e sci-entific domain as well. We in clude it here for completeness. For a full discu ssio n, plea se refer to the paper.
 Problem 6.1 . Giv en a set of cand idate docu ments D , find a map M = ( G,  X ) over D whic h maximiz es Con n ( M ) s.t . Coherence ( M )  X   X  and Cover ( M )  X  (1  X   X  )  X  , where  X  is th e maxima l coverag e across maps with coherence  X   X  and  X  is given.

There are several ways to rest rict th e siz e of M ; we chose to restrict M to K lin es o f len gth at most l . Alternatively, sin ce some stories are more complex th an others, one may prefer to a dd lin es unt il coverag e ga ins fall below a th reshold .
In th is section , we out lin e our app roach for solvin g Prob-lem 6.1 . We adapt th e algo rithm of [Shahaf et al., 2012 ] to solv e th e problem. In th e follo wing we review th e algorithm, high light in g the main diffe rences.

We start by add ressin g the coherence constraint: In S ec-tion 7.1 we represen t all coherent chain s as a g raph . In S ec-tion 7.2 we use th is graph t o find a set of K chains that maximiz e coverage; in S ection 7.3, we in crease conn ectivit y with out sacrificing coverag e.
In order to pic k goo d chains , we first wish to list all pos-sib le cand idates. However, representing all chain s whose coherence is at leas t  X  is a non-trivia l task. The nu mber of possible chains may be exponential, and th erefo re it is infea sib le to enumerate them all, let alon e evalu ate them.
The algo rith m of [Sh ahaf et al., 2012 ] emplo ys a divid e-and -conquer app roach t o th e problem, constructing lo ng chains from shorter ones. This allo ws us to compactly en-code many cand idate chains in a g raph structure whic h we call a coh erence g raph . G is a compact represen tation of the graph d ispla yed in Figu re 6. Vertices o f G corresp ond t o short coherent chain s, and th ere is a direc ted ed ge between each p air of vertices which can b e conjoin ed and maintain coherence. Importantly, th is property is transit iv e: every path in G , no matter how lon g, represen ts a coherent chain.
The only change in th e algorith m lies in th e comput a-tion of influ ence. Direc t influ ence and ancestor influ ence are in stances of th e k -terminal relia bilit y problem [B all, 198 6], whic h is # P -complete, so we cann ot h ope for a polynomial-time solution . Instead, we app ly app roxima tion s.

In BKS, th e auth ors presen ted a determin ist ic, lin ear-time dynamic programming heuristi c for calcu la ting direct influ -ence. This heurist ic is based on th e assu mption th at th e path s between two nodes are ind epen dent of each oth er. Un-fortun ately, th is assum ption is too strong for ancestor influ -ence. The path s between p 1 , p 2, and p ossib le ancesto rs are often depen dent, and treatin g them as ind epen dent resu lts in sig nificant errors. Instead, we e mplo y a simp le Mont e Carlo samplin g method with th eoretical guarant ees (BKS also proposed a simila r samplin g a pp roach) .

In order to calculat e m valu es with (  X ,  X  )-app roxima tion guarant ees (where  X  and  X  denote the upp er bound of rel-ative error and failu re probabilit y), we need O ( 1  X  2 samples. m is th e n umber of docu ment-pai rs with a common ancestor in th e graph . In th e worst case, m is quadratic in th e nu mber of papers (in practice, it is often much smaller) . Therefore, th e nu mber of samples needed is loga rithmic in th e nu mber of papers. Also note th at influ ences can b e pre-comput ed once and stored for futu re use.
After represen ting a ll coherent chains as a g raph G , we wish to find a set of chain s which maximize coverag e, sub ject to map siz e constraints.
 Problem 7.1 . Giv en a coherence graph G , find p aths  X  in G , | do cs (  X  i ) |  X  l th at maximiz e Cover ( do cs ( S
We use the coverag e-maximiz atio n algo rithm of [Shahaf et al., 2012 ] to find a high -coverag e map. The proof relies on formula ting the problem in term s of orienteering . Ori-enteerin g problems are motivated by maximiz ing a function Figure 9: A seg men t of a map com puted for the query of nodes visite d du ring a tour, sub ject t o a bud get on t our len gth. The [Shahaf et al., 2012 ] coverag e notion is sub mod-ula r, so we app lied th e Sub modu la r orien teering a lgorith ms of [Ch ekuri and Pal, 2005 ] to th e problem.

In order to adapt th e algo rith m to th e scien tific domain , we changed only th e way coverag e is comput ed (see Section 4). Note th at th e new coverag e notion is submodula r. Fig-ure 4 provides th e intu iti on for th at: add ing a paper to a sma ller set of papers helps more th an add ing it to a la rger set (d imin ish ing returns) . Therefo re, we can u se the same submodula r oriente ering a lgo rith m with th e new coverag e notion , and achiev e the same guarant ees. We now know how to find a high -coverag e, coherent map. Our fin al step is to in crease conn ectivit y with out sacrifi cing (more th an an  X  -fraction of) coverage.

In order to in crease conn ectivit y, we app ly a local-search techn iqu e. It starts from map M 0 , and t akes steps in th e search space by app lyin g lo cal moves. Each lo cal move re-plac es a sin gle lin e in  X . At ite ration i , we consid er each path p  X   X  i  X  1 . We hold th e rest of th e map fi xed, and t ry to replace p by p  X  th at in creases conn ectivit y (S ection 5) and does not d ecrease coverag e. At th e end of th e iteration , we pic k th e best move and app ly it , result ing in M i . The full details of th e alg orith m are in [Shahaf et al., 2012 ].
Figu re 8 shows a part of a map comput ed for the query  X  X ein forcement Learnin g X . As can b e seen, th e map d e-pict s multip le lin es o f resea rch: MDPs, robotics and cont rol, multi-ag ent cooperatio n, bound s and anal ysis, and exploration -explo ita tion tradeoffs. The map shows how the MDP lin e af-fects th e multi-ag ent and robotics lin es, and h ow th e e xplora tio n-explo ita tion lin e interacts with th e analysis lin e. Those rela-tion s are depicted as gray dashed p ath s. Note th at th e map does not captu re all th e in teractio ns; for exam ple, conn ec-tion s between MDPs and th e analysis lin e are not captu red.
As mention ed in Section 5, intersect io n is rare for broad queries. Figure 9 shows one such int ersect ion between two lin es in th e SVM map. One lin e is about la rge-scale SVM s, th e oth er is about multi-class SVM s. The lin es intersec t at Keerth i X  X  paper about la rge scale multi-class lin ear SVM s.
In our user stud y, we evalu ated th e e ffectiveness of metro maps in aidin g users navigate, consume, and int egrate dif-feren t aspects of a specifi c, multi-faceted informatio n n eed.
Evaluating metro maps in th e scien tific domain p oses some sig nificant challen ges. Since th e metro-ma p output is un iqu e, we cann ot condu ct a double-blind compariso n stud y, as sub-jects in evit ably differen tia te between the differen t syste ms. Therefore, we cann ot h ave a within -sub ject stud y, but are in stead forced to choose a between-sub ject d esig n. This de-sig n, in it self, causes a new problem: sinc e we need a diffe r-ent group of participant s for each cond iti on tested (metro-map or competito r), we cann ot tailo r th e query to users. Rath er, we have to fi nd a sing le domain such th at all of our participant s will (1) b e able to rea d scien tific pub lic ation s in th at d omain and ( 2) n ot know th e domain well in advance.
We recruited 30 particip ants from our un iversit y. All par-ticip ants were gradu ate stud ents with b ackground in Ma-chine Learnin g o r relat ed fields . The domain we chose was Rein forcemen t Learnin g. The machin e lea rning background of th e particip ants was enough t o make th em c omfortable with th e subject, but n one of th em had condu cted researc h in the field or stud ied it extensiv ely .

We asked p articipant s to ima gine themselv es a s first-year gradu ate stud ents embarkin g o n a resea rch p rojec t in Rein-forcement Learnin g. The participant s were asked to condu ct a quic k lit eratu re survey. In p articula r, th ey were asked to upd ate a survey paper from 1996 : identify up t o five re-search d irect io ns th at should be in cluded in th e upd ated survey, and list a few relev ant p apers for each d irec tion . We recorded particip ants X  browsin g hist ories, and t oo k a snap-shot of their progress every minut e. We limited th eir time to 40 minut es to simulat e a quick first p ass on p apers.
We used th e ACM dataset to comput e a map for th e query  X  X ein forcement lea rning X . The dataset cont ains more th an 35 ,000 papers from ACM conferences and journals. As th e nu mber of papers is relat iv ely smal l, scalab ilit y was not an issu e. We extra cted featu res as described in [El-Arini and Guestrin, 2011 ]. We had t wo cond ition s, GS and MP+GS : In GS , participants were allo wed t o use Goog le Scholar 1 search engin e th at ind exes schola rly lit eratu re. In th e seco nd cond ition ( MP+GS ), particip ants were given the pre-compu ted metro map, and asked to pretend th at th ey stumbled up on it; th ey were not ins tructed how to use the map. In add iti on to the map, the partici pant s could acc ess Goog le Scholar.
We also in clud ed two sim ula ted cond ition s in th e stud y, MP and WK : In MP , we pretend ed our map was th e user X  X  outpu t, and list ed all of it s papers. In WK , we used references from th e Wikip edia article about reinfo rcemen t learn in g. We decided t o compare aga in st Wikip edia and Goog le Scholar sin ce they represen t t wo o f th e most popu lar starting points for resea rch queries today. Other syst ems we consid -ered includ ing in th e comparative analysis were either un -availab le for downlo ad, or very restrict ed in th e span of th e scien tific domain represen ted.
 Before gradin g, we disc arded data from four particip ants. One did not und erstand th e task, and wrote a (n ice) essa y about rein forcemen t lea rning. The oth ers, desp ite visit in g many web pages, list ed less than 5 papers when time ran out .

We had an expert judge e valuate th e results of the rest of th e partici pants. We combined all of the papers th at u sers had entered into o ne list . Each entry includ es th e paper X  X  inform ation and URL. In add ition , we liste d th e lab els th at th e users supp lied for each p aper. The judge did not know th e meth od u sed to find th e papers.

Our expert judge scored th e papers on a 3-point scale: 0  X  Irrelevant, 1  X  1 : Relev ant , 2  X  Semin al. Each la bel was given a 0-1 score, based on whether it was a goo d match to th e paper. Th e resu lts are summarized b elow. http :/ /scholar.goo gle.c om
The table shows th e averag e nu mber of web p ages visit ed th roughout th e session , the average nu mber of papers listed by th e user, and th e averag e ratio of pages visit ed to papers listed . GS users visit ed mo re pages a nd listed mo re papers on average. However, when loo kin g a t th e averag e ratio , only one out of 4 . 5 pages visit ed by GS users was add ed t o th eir list, while MP+GS add ed one out of 3 . 8. In oth er words, th e map users were more focused : th ey may have visit ed less pag es, but th ey found th ese pages sat isfa ctory.
Users X  satisfa ction lev el is important , but th e real test is th e e xpert X  X  opinion . The next table shows the averag e normaliz ed scores giv en by the judge: For each u ser, w e calculat e the averag e paper score and average lab el sc ore. Then, we averag e over the users in each cond iti on:
Both th e paper and la bel sc ores of M P+GS users are hig her th an th e scores of GS users (th e median scores exh ibit simi-lar behaviou r). In add ition , th e averag e nu mber of semin al papers disco vered by GS users was 1.2 , while MP+GS users have disco vered on average 1.62 semin al papers.

The simulat ed Wikip edia u ser WK did n ot d o well: out of 15 referen ces, only fou r qualifi ed for th e stud y (p apers pub lishe d after 1996 ), and only two were deemed relev ant . In Wikip edia X  X  defense, th e other referenc es included semin al boo ks, whic h could h ave been u seful for our hypoth etical first-year stud ent.
 Finally , let us exam ine th e map ( MP ) u ser performance. Comparing the map d irec tly to user output is challeng in g a s th e map contain ed 45 papers, many more th an th e averag e user. Out of th ese papers, seven were deemed semin al, and 21 were deemed relev ant . Int erest ingly, many of th e papers th at were deemed irrelev ant were used as bridges between relev ant ( or semin al) p apers in th e map.

The find in g th at many of th e map u sers did n ot identify th e semin al papers in th e map is somewhat concernin g. A possible expla nation may be that th e users were in stru cted to focus on at most fi ve lin es o f resea rch, while the semin al papers were spread among more lin es. Note that d espit e th is fact, th e averag e normaliz ed score of MP+GS users is sti ll high er than th e score for th e map. In any case, this ph e-nomen on high light s th e need for more targeted resea rch on lo cating a nd visu alizi ng important n odes in th e map.
In add ition t o measurin g precisio n (th e fraction of re-trieved papers that are re levant) , we also teste d u ser X  X  rec all (th e fraction of relevant p apers retriev ed). It is not enough for th e users to find goo d p apers; rath er, it is also important th at th ey do not overlook important resea rch area s.
In order to mea sure recall, we have composed a list of th e top-10 subareas of reinforcemen t lea rnin g by goin g o ver con-feren ce and worksh op tracks and p ic king th e most frequ ent topics. Each u ser had t o list up t o five resea rch d irec tions; for each u ser, w e comput ed th e fraction of th ese direct io ns th at app eared in our top-10 list. GS users receiv ed an av-erag e score of 46 .4 %, while MP+GS users outp erformed th em with an averag e score of 73 .1 %.

Finally , further analysis of th e snapshots taken th rough-out th e stud y provid es a necdo tal evidence of th e utilit y of th e map. Several MP+GS users started by composin g a short list of resea rch direc tions; throughout th e session , these users have progressiv ely add ed papers to each d irection . GS users, in cont rast, did not exhib it th is  X  X ig pictu re X  behavio ur.
After th e stud y, we asked th e map u sers to tell us about th eir experien ce. Below are some of their commen ts:
Most imp ortant ly, many participant s fou nd th e map u se-ful in making sense of th e field. Some of th e participants had troub le interpreting elemen ts of th e map, or felt lik e the map was more suit ed for resea rchers with d eeper background knowledg e. We found th at many of the negative commen ts could be add ressed by imp rovemen ts in th e design of th e user interface.
When we defined our coverag e notion (S ecion 4), th e weight of each p aper was set to 1. In other words, th e objectiv e was to cover as much of th e corpus as possib le. However, so me parts of the corpus may be more imp ortant t o the user th an oth ers. In order to be useful, th e model must be capable of represen ting th e user X  X  interest s.

In th is section , we rely on u ser feedback in order to learn th eir preferenc es a nd adjust th e maps acc ordingly . We use th e interaction algo rithm of [Sh ahaf et al., 2012 ]. This algo-rithm let s the user provide feature-based feedbac k . Featu re-based feedback is a very natu ral way for specifyin g prefer-ences. We show the user a tag clo ud d escribin g th e papers of th e map. Clic kin g o n a word lets the user adjust its im-portance. For exa mple, imp ortance of 0.9 imp lies th at 90 % of the documents in whic h th e word app ears are in terestin g to the user. The relat iv e transparency of the model allo ws users to make sens e of featu re weight s.

Featu re-based feedback is esp ecially useful in th e researc h domain , as users can emplo y it t o indica te whic h auth ors and venu es th ey trust. In add iti on, sin ce our coverag e notion is bia sed aga inst newer papers (n ew papers did not h ave enough time to make a big imp act), th e users can ind icate th eir preferen ces for new, state-of-the-art papers.

When we in crease the weight of th e years 2005-2008 in th e rein forcement learn in g map, th e resu ltin g map cont ains chains about more recent t opic s, such as hierarc hic al rein-forcement lea rning. When bia sin g for AAMA S (a conferen ce on aut onomous ag ents and multiag ent syst ems), th e result-ing map includ es a new chain about robot soccer.

In th e futu re, it may be interest ing to form ulat e a notion of baselin e personaliz ation , where defau lt weig hts are set based on auth ors X  reputations : if an auth or has writ ten ma ny hig h-impact p apers, his new paper is lik ely to be important. We may also explo re oth er notion s of perso nalize d coverag e, such as [El-A rin i and Gu estrin, 2011 ; Yue and Gu estrin, 2011 ].
In th is paper, we have devised a meth od for constru cting metro maps of science. Giv en a query, our algo rithm gener-ates a metro map: a concise stru ctu red set of researc h lin es whic h maximiz es coverag e of salien t p iec es o f inform ation . Most important ly , metro maps explic itly show th e rela tions between th e researc h lin es.

We condu cted promisin g pilo t u ser stud ies, comparing our syste m to two syst ems that d ominate today X  X  resea rch-rela ted queries. The results in dicate th at our method can help u sers acquire knowled ge effic iently.

In th e futu re, we plan to experiment with richer forms of inpu t, outpu t, and interaction models. Promisin g direc-tion s in clud e edge-ann otation based on cita tion function , no-tion s of coverag e that combine stru ctu re and content, paper-based and lin e-based feedback mechanisms, and th e in tegra-tion of hig her-level sema nt ic featu res. We have also cre-ated a websit e that allo ws int eractive visu aliz ation of metro maps, which we hope to laun ch soo n. We believ e th at metro maps hold th e potentia l to become effectiv e tools to help re-searchers cope with informatio n overlo ad.

