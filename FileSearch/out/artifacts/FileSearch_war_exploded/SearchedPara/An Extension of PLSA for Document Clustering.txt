 In this paper we propose an extension of the PLSA model in which an extra latent variable allows the model to co-cluster documents and terms simultaneously. We show on three datasets that our extended model produces statisti-cally significant improvements with respect to two clustering measures over the original PLSA and the multinomial mix-ture MM models.
 H.3.3 [ Information search and Retrieval ]: Clustering; I.5.3 [ Clustering ]: Algorithms Algorithms, Experimentation Document Clustering, PLSA
With the ever-increasing volume of on-line textual infor-mation, an efficient partitioning of documents into clusters can constitute a real saving in terms of efficiency for vari-ous information retrieval or enterprise portal applications. Document clusters can for example, help users to quickly evaluate classical search engines results, or navigate through huge document collections [2]. They can also be useful for distributed search or extractive text summarization [1, 5].
Probabilistic Latent Semantic Analysis ( PLSA )isawell known algorithm to model document collections. When ap-plied to document clustering, latent topics in PLSA are iden-tified to document clusters which may be too restrictive in cases where there are more topics than document clusters in a collection.

Our paper is organized as follows, in section 2, we ex-tend the PLSA model ( Ext-PLSA ) by incorporating into its Figure 1: Graphical model representation of the Multinomial Mixture model (a), PLSA (b) and its extended version (c). The plates indicate the re-peated sampling of the enclosed variables. graphical representation an additional latent variable cor-responding to document clusters. In section 3, we present experimental results showing that our proposed approach outperforms the initial PLSA and the Multinomial Mixture model ( MM ) over three text collection. introduced by [4] is a probabilistic model which character-izes each word in a document as a sample from a mix-ture model, where mixture components are conditionally-independent multinomial distributions. This model asso-ciates an unobserved latent variable (called aspect or con-cept)  X   X  B to each observation corresponding to the oc-currence of a word w  X  X  within a document d  X  X  .The underlying generation process of this aspect model is shown in figure 1 (b).

When document clustering is performed with the PLSA model, the latent topics play the role of the document clus-ters. In this case, the probability of observing the topic  X  given the document d , p (  X  | d ) is interpreted as the probabil-ity that the document d belongs to the cluster  X  . The clus-tering is performed using : cluster( d ) = argmax  X   X  B p (  X  tent topics of a document collection and the number of de-sired document clusters are different, the PLSA model can not be used for document clustering. We propose an exten-sion of PLSA, which includes two latent variables  X ,  X  to model the topics on two different levels.
 The corrresponding generative process is as follows : -Pickadocument d with probability p ( d ), -Choose a document topic  X  with probability p (  X  | d ), -Choose a word topic  X  with probability p (  X  ) -Generate a word w with probability p ( w |  X ,  X  )
Figure 1 (c) depicts this pro cess. Words are in this case generated not only by latent topics (as it is the case with the PLSA model) but also by document clusters. This as-sumption hence enables the generative model to capture the discourse on two different semantic levels: on the general topics dealt in the collection expressed by variables  X  and on different sub-topics represented by variables  X  .Inthis case the generation of a word w within a document d can be expressed by the joint probability:
Model parameters in this case are  X  = { p ( d ) ,p (  X  | d ) ,p (  X  ) , p ( w |  X ,  X  ): d  X  X  ,w  X  X  , X   X  A,  X   X  B } and which are es-timated by maximizing the log-likelihood function using an EM -like algorithm [3]. In the E -stepweestimatetheposterior probabilities of the latent variables : In the M-step , we re-estimate the model parameters which maximize the expectation of the log-likelihood Once the model parameters are learnt each document d  X  D is assigned to the cluster which maximizes the posterior probability : cluster( d ) = argmax  X   X  A p (  X  | d )
In order to evaluate our model for the document clustering task, we used standard labeled text classification corpora using the class labels as an objective knowledge reflecting the datasets implicit structure. In the following, we describe the datasets we used as well as the measures we carried out to evaluate the clustering performance.
 experiments on the Reuters , 20Newsgroups and WebKB data sets. General preprocessing steps for these three collections consist in converting all words to lowercase, mapping digits to a single digit token, removing non alpha-numeric charac-ters and removing words occurring in less than 3 documents or appearing in a stop list. After preprocessing, the Reuters collection contains 4335 news articles divided into 7 classes. The 20Newsgroups collection contains 16010 Usenet mes-sages divided into 5 classes. The WebKB collection contains 4196 web pages divided into 4 classes. In the following, the reported performance are averaged over 10 random subset Table 1: Best average precision and the correspond-ing average NMI on the Reuters , 20Newsgroups and WebKB datasets.
 Average Prec. 20Newsgroups 0 . 62 0 . 71 0.77 splits of each initial collection while preserving the propor-tions between different classes in each subset.

In order to compare the performance of the algorithms, we used the micro-averaged precision and recall [6] as well as the Normalized Mutual Information [7].
 Results. Intable1,wepresentacomparisonof MM , PLSA and Ext-PLSA . We compared clustering performance on the 10 subsets of the three data collections. The Ext-PLSA model is significantly better than the two models according to a Wilcoxon rank sum test used at a p-value threshold of 0 . 01. In this article, we proposed an extended version of the PLSA model which separates the latent topics and the doc-ument clusters, allowing the model to simultaneously clus-ter documents and terms. Experiments conducted on the Reuters , 20Newsgroups and WebKB datasets have shown that the proposed Ext-PLSA performs significantly better than the MM model and the original PLSA model. [1] M.-R. Amini and P. Gallinari. The Use of Unlabeled [2] D.-R. Cutting, J.-O. Pedersen, D. Karger and John W. [3] A.-P. Dempster, N.-M. Laird and D.-B. Rubin.
 [4] T. Hofmann. Probabilistic latent semantic indexing. In [5] K. Kummamuru, R. Lotlikar, AS. Roy, K. Signal and [6] N. Slonim and N. Tishby. Unsupervised Document [7] A. Strehl and J. Ghosh. Cluster Ensembles, A
