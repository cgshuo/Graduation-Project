 Soft Computing Research Group, Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia, Skudai, Malaysia 1. Introduction
Classi fi cation is the epicenter of machine learning; a scienti fi c discipline to mimic the heart of intelligent life, which is learning. The repercussi on of learning is the ab ility to reason or judge. Thus the quality of judgment is highly d ependent upon the method of making inference (be it inductive or deductive) as well as the input by which the decision is made. In the context of machine learning, decision making falls into three broad categories: classi fi cation, cluster-analysis and regression. The fi rst two can be perceived as one into the other. For instance, cluster-analysis or clustering is just the process of unsupervised classi fi cation, hence signifying the importance of the subject matter. It also shows the presence of correlation therein.

As pointed by [1], every dataset contains a cohesive set of information. Much preprocessing such as normalization or scaling, data reduction, attribute selection, and attribute transformation have been introduced to ensure accurate reinterpretation by the classi fi er. Discretization combines two or more of these steps into a single pipeline. It is rooted from granular computing, one of the most recent amongst the ever growing branches of computational science [2 X 4]. Granularity de fi nes the degree of information involves in a particular process. In the case of discretization, the purpose of granularity is abstraction. Noise suppression, compact representation, and knowledge discovery are some of the advantages of data abstraction that will subsequently lead to ef fi cient post-processing.

The paper is organized as follows. Section 2 provides related work on discretization, and follows by our proposed Extended Boolean Reasoning discretization with Particle Swarm Optimization (EBRPSO) in Section 3. Section 4 provides the experimental fi ndings and analysis on the proposed method. Finally, Section 5 concludes and discusses the paper accordingly. 2. Related work on discretizations
Various discretization algorithms have been proposed and evaluated; showcasing their bene fi ts both in the context of learning and exploratory data analysis [5,6]. Regretfully, there are as many pitfalls as there are hilltops in discretization. This research area is most certainly susceptible to the prevalence of no free lunch theorem. No technique could surpass the other single handedly across all application domains. Each approach has its own sets of criteria that it tries to optimize and these goals do not necessarily conform to the dataset X  X  unique characteristics [7]. Similarly, the discretizer also needs to correspond with the objectives of the ensuing classi fi er [4]. For example, decision trees can suffer from fragmentation problem; hence they may pro fi t more from a discretizer that produces fewer intervals or partitions.
 One of the most popular discretization methods is proposed by [8], which utilizes the principle of Minimum Description Length (MDL). Based on MDL, the data is split at a particular cut-point if the entropy cost is less than before the partitioning. There is no real problem with the algorithm except for being dependent on the nature of the data itself (as most others are too), which makes it an attractive initial choice. [9] develops a fairly straightforward discretizer named the Class Attribute Interdependence Maximization (CAIM), for the obvious reason that it tries to capitalize the correlation between decision and conditional variables. The best traits of CAIM are its ef fi ciency in dealing with large datasets and execution. [10] extends the traditional Chi -square or X 2 based discretization of [11], where X 2 is a statistical term used to measure the signi fi cance between two independent attributes). The improved version eliminates the need to predetermine the X 2 parameter by borrowing the approximation concept of Rough Sets Theory (RST). This contributes towards better generalization in the case of uncertainty or inconsistent decisions. Similarly, the discretization based on Boolean Reasoning [12], BR for short, also stems from RST. The main different is BR parses the information system into a set of pair-wise objects with distinctive decisions, whereby increases the granularity or discernibility between competing classes.
Generally, the above methods represent each staple of the discretization taxonomy. Motivated by the vast applications of RST and particularly its prevalence in discretization [13 X 16], the ensuing narration will be geared towards this end. However, we opted for BR over its superset (RST) due to BR simplistic procedures, all the while exhibits the essence of RST. The conventional BR has two key drawbacks, which are the generation of spurious candidate cuts and the con fl ict among the cuts that share equivalent discernibility. Preliminary solutions include fi ltering the cuts by considering only boundary cuts [15] and the use of Genetic Algorithm (GA) to relieve the tie between cuts [17]. More advanced solutions can be found in [18 X 21].

The general view of discretization is captured in Fig. 1. Given 7 objects with attribute { a, b } , how can as interrelation between attribute and decision class dots, circles. Obviously without any optimization procedure, there can be as many as 20 unique par titions expressed by the 7 c uts (midpoints between number of partitions before and after discretization would remain the same.

BasedonFig.1,itistrivialto fi nd the best set of cuts due to the lack of a selection criterion. BR presents a modest but effective solution by introduc ing the discernibility term. Table 1 is an equivalent p } that could be (1) or could not be (0), and discern the object pairs from different decision class given in the fi rst column. Now the goal becomes to select a set of cuts that would discern all object pairs. Conventional approach is to employ MD-heuristics, which selects cuts with the highest discernibility in unidirectional search mechanism, hence could not guarantee a unique or optimal solution.

Since the primary concern in discretization is the loss of salient information that could compromise classi fi cation performance, hence, this study proposed PSO as a meta-heuristic algorithm to overcome this matter. It would have a converse effect on classi fi cation, be it in terms of greater accuracy, faster execution or even both. The BR discretization has been selected as a candidate in this study based on the aforementioned merits. Therefore improving classi fi cation would amount to fi xing the weakness of the algorithm. The most relevant research question that can drive the course of study towards fi nding the right solution is:
How can a global search meta-heuristic be implemented in such a way that it supersedes similar past attempts, all the while preserving superiority against the already ef fi cient classical approach?
Unlike the more complicated stochastic approaches, the original solution (MD-heuristics) is ef fi cient in regard to its deterministic sequential prime-cut selection method. Albeit slower, global search meta-heuristics offer a more fl exible framework that allows con fl icting selection in MD-heuristics to be resolved. Instinctively, a possible answer to the research question could very well lie under the notion of consolidating these complementary properties. This scenario has motivated us to respond to the problem by proposing Extended Boolean Reasoning discretization (EBR) with Particle Swarm Optimization (PSO) to search for optimal cuts. 3. The proposed Extended Boolean Reasoning Discretization with Particle Swarm Optimization
The standard discretization is based upon the following principles [17]. A decision table S = U, A  X  { d } ,V,f ,isde fi ned as:
The task of standard Boolearn Reasoning (BR) discretization is therefore to determine a minimal set of cuts for the transforme d decision table wit hout changing discer nibility. The widely accepted cr iteria for discretization rationality evaluation, is as follows [12]: (b) Minimum: there is no D  X  D , satisfying consistency.
For a decision system S , if a ( u then, the maximum cut set on a is as:
Conventional BR discretization adopts maximum-discern (MD) heuristics to search for the prime cuts (i.e.: maximal number of object pairs discerned by such cut set) [12]. Despite the reduction pro fi ted from the bound cut fi ltration, we believe the number is still high for some dataset (e.g.: Liver X  X  269 cuts as and in turn, compromises generalization.

Therefore, we propose to stretch the fi lter process further by means of ranking and sorting as follows: (i) Rank each candidate cut (column of the Boolean table B ) based on the number of discerned (ii) Sort (i) such that the order becomes from the most discerned to the least discerned. (iii) Add the sorted column into a new fi ltered Boolean table B , incrementally, until B contains the
The columns in both B and B are just pointers to the actual cuts. Thus, the columns X  indices in B are mapped to the ones in B in order for the corresponded cuts to be retrievable.

The typical MD-heuristics are then applied onto this fi ltered table to get the fi nal prime cuts. We label the whole scheme as Extended Boolean Reasoning discretization (EBR). From the third step (iii), we know that as far as discernib ility is concerned, it is unspoiled. In addition, a good chunk of the expedite the prime cuts extraction, but also could improve generalization as there are less spurious cuts now than it were before.

Nevertheless, the con fl ict that arises when two or more cuts sharing the same discernibility (i.e.: card ( max ?( B  X  ily at each step would have different connotations and unpredictable outcome is certainly undesirable.
Given n candidate cuts, the task of prime cut selection amounts to search through 2 n  X  1 possible subsets ( 2 n if  X  is considered a valid selection), which is NP-hard [22]. This automatically renders any heuristics approaches as suboptimal. Fortunately, most of the meta-heuristics solutions in attribute selection are also directly applicable to the so-called cut selection problem. Evolutionary-type algorithms, particle swarm optimization (PSO) in particular, is well suited for tackling optimization problem as mentioned in [4,23,24]. Cut selection is basically a set covering problem, where the objective is to fi nd the minimum number of subset from the superset whose elements cover the entire sets. Since the problem is discrete, standard real-valued PSO are un fi t for the job. Driven by the success stories of discrete-valued or binary PSO in attribute selection [25,26] as well as its relatively simple implementation but fast convergence, we adopt binary PSO in this study to overcome the shortcomings of MD-heuristics, namely the tied-discernibility con fl ict and the unidirectional search mechanics.
Essentially, PSO embodies three main phases: initia lization that involves initial segment where the particles are fi rst generated; evaluation and updating which involves the iterative execution of assessing the process terminates when the maximum number of iteration is reached. We do not rely on the fi tness value since there is no indication on what is the optimal fi tness threshold. 3.1. Particle position initialization
We keep the population size (i.e. number of particles) low so as to expedite the evaluation process. In this study, we fi xed the iteration to 100 and there are a total of 100 particles in the population. That would amount to 10000 fi tness evaluations. Thus in this study, the size is limited to 10. However, the performance of PSO is sensitive towards initial positioning of the particles [27]. The dependency becomes more pronounced when the search dimension grows larger. High dimensional search space means that the solution is substantially sparse with respect to the problem. In the worst case scenario, to a secluded section in the sear ch space. To ease this effect, we u tilize the prime c uts produced by MD-heuristics. The position for the initial particle copies the solution of MD-heuristics while the rest are superimposed with 25% random selection off the original candidate cuts in surplus to the ones by MD-heuristics. The idea concurs with the principle of PSO where the swarm is always led by the best global particle. For instance, let the prime cuts of MD-heuristics be { 1 , 7 } , selected from 12 candidate The randi (random integer) and card (cardinality) functions as given in our algorithm below help to serve this purpose.

These indices however need to be converted into binary representation such that it becomes { 1 , 0 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 1 , 1 } . Using the Heart disease dataset from the UCI Machine Learning repository, we compare the MD-heuristics seeded initialization against the conven tional random initialization. The swarm movements are tracked over a period of 100 iterations and snapshots are taken between two particle for the proposed scheme has only 14 selected cuts as opposed to 33 selections by the traditional i = number of particles, j = bit-length of every particle, k = number of object pairs
A) generate ( Swarm { Position, V elocity } )
Pbest 1 ...i = Gbest = iteration =0 , maxIteration = 100 ,w =1 . 4 ,c 1 = c 2 =0 . 2 while iteration &lt; maxIteration scheme. After 10 iterations, the cuts are reduced down to 11 and 26 for the proposed and traditional positioning converges faster. This is evidential in the lower right graph as the particles started to line up against one another. Secondly, the solution of MD-heuristics is suboptimal since it can be reduced even further by PSO. For brevity, the algorithm is termed as EBRPSO, owing to its dependency on the decision table created by EBR. 3.2. Fitness function
The common belief in RST dictates that preserving consistency equates to preserving class separa-tion [17,18]. However, by relaxing the consistency term, [28] manages to achieve better generalization. This is expected since the former ideals did not take into account, exclusive resampling of the original population. In other words, the standard train and test sets classi fi cation framework, which re fl ects on practical real world applications, where prediction is made on an independent dataset, different from that seen during development. Employing the fi tness function of [14] does not grant superiority over the traditional MD-heuristics for reasons mentioned before. The relaxation of the discernibility term is consistency, derived from RST instead of BR. Thus, here we proposed a simple statistical term, D as in Eq. (3), on top of the existing cardinality 1  X  card ( x ) the number of bit  X 1 X  in x , card ( D ) counts the number of object pairs that x can discern in the Boolean decision table, while card ( T ) totals the number of object pairs itself.
Inspired by the fi ltration process of EBR, the simple new term is composed of two sub-components: it measures the quality of each cut by averaging the ratio between the numbers of dis cerned obj ects per cut ( card ( D  X 1 X  X  of x ). The standard deviation counterpart acts as offset to extreme mean values. Each term has its to prevent one term from dominating the other. Although our algorithm (given below) serializes the evaluation procedure, a faster parallel alternative can be opted since the inter-particles dependencies at Notice that there is no gBest because it equates to the best one in pBest , hence only the index, g ,is kept. Memorizing such historical record has enabled the particle to chart its exploration or exploitation, thus avoids revisiting the same trajectories twice.

The Heart dataset is recalled to assess the discrepancy between the new fi tness function and its predecessor [18]. Figure 3 depicts the average (solid line), minimum and maximum (dotted lines) of the cardinality convergence rate over 10 runs. The original fi tness seems to be trapped in a local optimum at around the 20 th iteration whereas the proposed fi tness continues to propagate smoothly into for our proposed fi tness function, which is 0.968 (  X  0.00653), compared to 0.95867 (  X  0.00718). The  X  1 =0 . 9 and  X  2 =0 . 1 (proposed fi tness). Despite assigning high weightage to the discernibility term accuracy, albeit similar setup (  X  1 =0 . 9 ). On average, the number of cut being generated is lower for
For Particle 1 the new fi tness, which is 3.78, versus 4.48 for the original fi tness. This is unexpected since both used the same weightage for the cardinality term (  X  =0 . 1 ) but upon inspection on the smaller number of object being discerned by the new fi tness (6034.13, versus original fi tness X  X  6063.53), it becomes apparent that compromising discernibility for generalization was the cause. The purpose of adding a new term to the original fi tness has been justi fi ed and later we will discuss more on parameter selection for the coef fi cients. 3.3. Position and velocity update
An individual particle X  X  propulsion is governed chie fl y by its velocity ( V elocity dependent on the particle current position ( Position global best position ( pBest
The variables a 1 and a 2 measure the positive distance (how much farther behind the particle is to the local and global optima) between { pBest while b 1 and b 2 are the counter measures (how much farther ahead the particle is from the local and global optima). The inertia weight w , acts as a damper on V elocity w decreases, enforcing the particle to converge. The acceleration constants c 1 and c 2 , each controls the exploration ( a 1  X  b 1 ) and exploitation ( a 2  X  b 2 ) of the particle, while the random functions add a sense of stochasticity to the search paradigm. By setting them both to 2, it means that the effort for exploration and exploitation will be equal. The modulus and rounding-off of V elocity
For Particle 1 to facilitate th e updating of Position the particle ( V elocity in Position { Position i is randomly changed, different from that of pBest g . The second one promotes exploitation; same thing like previously but only few bit-fl ipping is made. Details can be retrieved from [18]. 4. Experimental fi ndings and analysis
The experiment will rely on four real-world dataset derived from the UCI Machine Learning repository. There are two from the medical domain, which are the Cleveland X  X  heart disease and the Wisconsin X  X  breast cancer datasets. Both are binary classi fi cation problems. Multiclass problems are posed by the canonical Fisher X  X  iris fl ower and the wine origin identi fi cation datasets. The number of samples is relatively small except for Breast; while the number of attributes is moderately considerable with the exception of Iris. Albeit a short list, such variety should be suf fi cient to highlight different aspects of the algorithm under scrutiny. Besides, given only four datasets, we can afford to analyze each one more extensively. Table 2 provides complete speci fi cations of these datasets.

In this study, decision trees are employed as classi fi er, speci fi cally the C4.5 of [29]. C4.5 builds decision trees based on a train-set using the concept of information entropy. At each node of the tree, with a particular class. Attributes with the highest normalized information gain would then make up the decision rules. After the rules have been extracted, it can be used to classify unknown samples or machine learning and more so in data mining. Regardless of having its own in-house discretization scheme, previous works have shown that C4.5 can still pro fi t from external discretization [30 X 33]. Also, coupled with fast training time and commendable discriminant power, makes C4.5 an attractive platform for validation. Correct classi fi cation rate is the key performance indicator for discretization quality. The classi fi cation outcome should not be jeopardized by the discretization process. This is imperative especially when the sample distributions are skewed (e.g.: Heart, Breast and Wine). By providing unbiased treatment, the accuracy for majority class will not suppress the one pertaining to minority class.
 A single template or fold in Fig. 4 embodies the core framework of the e valuation outlin e. The 10-fold CV scheme represents a single instance of the complete evaluation. Replicating this instance 10 times (10  X  10-fold CV), each with different set of ra ndom sub-par titions, will re sulted in 10 maAccs .The maAcc is based on concatenation of every class label from all 10 folds. The reason for instigating 10  X  10-fold CV is because multiple eval uations have higher resistance t oward sampling bias, i.e. it is of the test-set are strictly based on cuts and rules developed using the train-set.

In the previous section, we have deliberated the viability of the modi fi ed fi tness function. Now we accuracy. Other parameters such as number of particle, number of iteration, w , c 1 ,and c 2 are fi xed so as to allow exclusive tuning of the coef fi cient. Because EBRPSO undertakes the fi lter approach, the cut selection procedure is made to be independent from the inductor X  X  feedbacks (e.g. classi fi cation accuracy, model complexity, etc.). Notwithstanding, a classi fi er is still needed to track the consequence from parameter alteration.

Table 3 summarizes the classi fi cation performance of EBRPSO with various con fi gurations of the fourth columns represent the means of 10 maAccs (with corresponding standard deviation inside the parenthesis), one from each instance of the 10-fold CV, for the train-and test-sets respectively. The rule intuition both should be equally weighted but preliminar y trials have proved otherwise. Some datasets prefer  X  1 (Heart and Breast), while some favor  X  2 (Iris and Wine).

Owing to the stochastic nature of PSO, we reproduce the outcome for the best parameter set three times (the last three rows of each dataset) and retain the one with the highest train accuracy (bolded fi gures). Only when both mean and standard deviation of the new train accuracy exceed the current standings, does it cons titute as being bette r. The matching test accuracy will be used later to compare against other discretization methods. The crude selection method however, fails to select the best test accuracy (mean and standard deviation) in Wine, which supposed to be 0.94713 (  X  0.01769). In the case of Breast, 0.95910 (  X  0.00388) is less obvious to be better than the selected score due to the undistinguishable differences. 4.1. Comparison with existing methods In this study, we are benchmarking our proposed method against existing discretization methods. We invoke three discretizers from the KEEL (Knowledge Extraction based on Evolutionary Learning) toolbox [34]. While KEEL offers a total of 27 discretizers, we select only three that are parameterless, which are Minimum Description Length (MDL), CAIM and EC2. This is to relinquish the need for parameter selection of those discretizers. BR, which is the foundation for EBR and EBRPSO, will also be contested. Our BR implementation has been veri fi ed to yield identical output with that from the ROSETTA toolbox. C4.5, courtesy of KEEL, provides the baseline performance (denotes as Default) in phase. The primary function of C4.5 is to measure the quality of discretization of the aforesaid methods. We de fi ne discretization quality as a composition of three constituents: classi fi cation accuracy, number of decision rule (or leaf node) and number of sample attribute; in that order of importance.
The comparison emulates a speci fi c assessment suite: discretization results, visuals on the ANOVA of maAcc with corresponding leaf node distributions, and the ANOVA table itself. Discretization results round up all the discretization characteristics such as number of discernible object pair ( Object ), number of candidate cut ( Cut ), number of optimal cut ( Opt. Cut ), number of attribute ( attribute ), number of leaf node ( Leaf ) and mean maAcc ( Accuracy ) with its standard deviation ( SD ). Object and Cut are only exclusive for BR-based methods. As mentioned earlier, C4.5 does not produce of fl ine cut; hence no Opt. Cut for Default. The comparison wraps up with the ANOVA table, and it highlights the best contender that each of the proposed method (EBR and EBRPSO) managed to outdo signi fi cantly. To achieve this, the paired or one-way ANOVA test is utilized. It measures the signi fi cance of the difference between two means,  X  x (e.g.: difference between the Accuracy of EBR and Default), and if the range of the different at 95% con fi dence level (less than 0.05 probability that the difference is not signi fi cant). 4.1.1. Analysis on heart dataset with EBRPSO Table 4 evidently shows EBRPSO as the all-rounder for heart disease dataset. It has the highest Accuracy as well as the least Opt. Cut , Attribute and Leaf . Based on Table 5, although EBR only manages to have signi fi cant leads over CAIM and Default, the performance stays competent against others. Despite huge reduction in Cut , EBR fails to obtain fewer Opt. Cut . As argued previously, it is by relaxing the discernibility term (or Object ) that EBRPSO has regained the losses in EBR. What is most profound is EBRPSO X  X  reliability (indicated by the smallest SD ) in attaining those feats given the depth of the evaluation, i.e. 10  X  10-fold CV. 4.1.2. Analysis on breast dataset with EBRPSO
Analogous to Heart, EBRPSO draws a similar upshot on Breast datasets where it fi xes the short-comings of EBR except for falling slightly on SD . Notwithstanding the insigni fi cant difference in Leaf between BR and EBRPSO, the latter still excel in all other aspects (see Table 6). This runs contrariwise with EC2 and EBRPSO whereby the Accuracy lead by the latter is not substantial but the Leaf is. Table 7 tells that EBR only outperforms Default, while EBRPSO adds two more in the list (MDL and CAIM). However clear it is that both methods are not as excellent as in previous dataset, EBRPSO still stand out in the competition. 4.1.3. Analysis on IRIS dataset with EBRPSO
As with Heart and Breast, the performance trend of EBR and EBRPSO faithfully transpires onto Iris dataset. Except for the small overlap with EBR, EBRPSO is otherwise dominant in term of Accuracy . Although MDL and CAIM yield slightly fewer Leaf than EBRPSO, the Accuracy gapis quite staggering at approximately 0.04 difference (see Table 8). It is interesting to note that most methods (MDL, CAIM and EC2) fail to improve the baseline (Default) Accuracy . Hence, when EBR signi fi cantly outpaces Default as reported in Table 9, it easily beats those methods as well. Again, EBRPSO does the same thing but with a surplus of subjugating BR. 4.1.4. Analysis on wine dataset with EBRPSO Out of the four datasets, Wine dataset is the only case where mitigating the discernibility term (or Object ) would actually detriment the performance, hence why we keep it high (see the Object for EBR vs. EBRPSO in Table 10). Even so, EBRPSO s till loses to its predece ssor (EBR), albeit not a signi fi cant one. Likewise, according to Table 11, the gains EBRPSO had over EBR (i.e. Opt. Cut , Attribute and Leaf ) are also inconsequential. While EBR does not exceed CAIM substantively in term of Accuracy , the reduction in Leaf is almost two-fold. This leads to the conclusion in Table 11. In spite of the unsubstantiated improvements, either of the proposed methods manages to outclass their mutual predecessor (BR).

Based on the previous ANOVA tables, we accumulate all the responses into Table 12. It shows the win/loss/tie reco rd of the competing methods against t he proposed methods for each dataset. Summarization of the record reveals that the win /loss/tie count for EBR and EBRPSO are 11/1/12 and 18/0/6 respectively. In other words, out of the 24 comparisons, EBRPSO have a lead of 7 wins over EBR, no loss as opposed to a single loss by EBR and 6 ties shorter than EBR. Thus in term of classi fi cation performance, EBRPSO evidently bags the competition. With the undefeated record, this proves that not only the discretization of EBRPSO preserves the information system within the dataset but also enhances classi fi cation outcome in in 18/24 or 75% of the time.

Once the principal objective has been met (i.e. perfecting classi fi cation via discretization), we can we know that some methods have the a bility to pinpoint cer tain attributes that could best describe the decision hyperplane. This phenomenon is predetermined by the optimal discretization cut, which therefore infers attribute reduction as the consequence of attribute transformation, i.e. discretization. Minimal Attribute would result in optimal Leaf and the subsequent effect is of course faster execution. Figure 5 demonstrates that such privilege is sometimes unavailable and CAIM in particular, does not attempt to reduce the attribute at all. The best reducer is EBRPSO whereby in the fi rst three datasets, the deduction shoots up to 20% extra than other methods. It is only in Wine that the improvement lacks boosting. The near 90% reduction by EC2 is a fl uke due to its corresponding deprived Accuracy . 5. Discussions and conclusion
Discretization is an essential pre-processing component for some machine learning algorithms. In this study, we focus on the domain of classi fi cation and how it can bene fi t from such pre-processing. The corner stone of the proposed technique is RST, an alternate solution to fuzzy set theory for uncertainty and ambiguous decisions. As a subdivision of RST, BR provides good theoretical framework for the purpose of discretization. However, the current implementation of BR suffers from high computational cost and premature selection. It is the gist of this study to address these issues intelligently. First, we tackle the problem of high dimensional search space by introducing a fi ltration phase, denote as EBR, consisting of ranking the candidate cuts and eliminating the lower end of the list. Next, a meta-heuristic search algorithm, binary PSO, is deployed to fi nd the prime cuts. The PSO fi tness function is modi fi ed in order to take into account the qua lity of individual cuts. It is the integration of these two components (EBR + PSO) that we hypothesize could produce the best outcome. Based on rigorous empirical evaluation (10  X  10-fold CV) of four real-world classi fi cation problems (i.e.: Heart, Breast, Iris and Wine), the proposed method (EBRPSO) has proven itself as a competent discretizer. The discretization quality is superior to the ones produced by existing methods, i.e. MDL, CAIM, EC2 and BR. Not only that the discretized attributes increases classi fi cation accuracy but also decreases the number of the attributes itself, which in turn, alleviates the decision rules X  complexity. The improvement is considerably signi fi cant, given the validation from ANOVA. However, further experiments are required particularly on parameter selection of the new fi tness function as well as the native parameters of PSO. From the results we know that EBRPSO does not outperform EBR in the case of Wine. Hence, a more extensive list of datasets deems necessary. We also plan to expand the comparison by including recent methods such as Ameva [30], Uni fi cation [32], and Proportional-D [4]. It was not possible to incorporate them at this point without exhausting adequate time in tuning the parameters of each technique. Ultimately, since EBRPSO is based on RST, we would like to suggest substituting C4.5 with Rough Sets miner in order to extract better classi fi cation rules. Acknowledgment We would like to thank anonymous reviewers for the incisive comments and Soft Computing Research Group (SCRG), Universiti Teknologi Malaysia for the support in making this study a success. This work is supported by The Ministry of Higher Education (MOHE  X  LRGS/TD/2011/UTM/ICT/01  X  4L805). References
