 Query performance prediction (QPP) ha s become an important problem in the area of information retrieval (IR). These predictors aim to automatically esti-mate the performance of queries so that diff erent strategies (e. g. query expansion or reduction) can be applied based on their estimated performance. The perfor-mance of these predictors are usually compared by measuring the correlation between the output of the predictor and query performance (e.g. average preci-sion). However, many approaches to QPP are unprincipled, and it is unclear how to improve their performance, or if th eir performance can even be improved.
In this paper, we develop a principled framework based on modelling docu-ment score distributions that aims to predict query performance directly. Fig. 1 shows an example of a document score distribution returned for a query (when relevance information is known). We (1) develop formulae that directly infer av-erage precision from a document score distribution, (2) develop simple heuristics that can estimate the important parameters of the score distribution when rel-evance information is unknown, and (3) provide an analysis that informs us of the most important parameters in the distributional model. This analysis helps in narrowing the focus of future research.

The remainder of the paper is organised as follows: Section 2 reviews related work on score distributions and query performance prediction (QPP). Section 3 outlines our principled model, before the formulae for calculating the average precision from a mixture are introduced. In section 4, we outline three approaches to automatically predict the performance of a query from the score distribution when relevance information is unknown. Furthermore, we present an analysis that shows that only two parameters of the model are crucial in the estimation of average precision. Section 5 presents co mparative results of the newly devel-oped QPP approaches versus existing predi ctors. Finally, section 6 outlines our conclusions and future work. Modelling the distribution of document scores returned from IR systems has been studied from a theoretical perspect ive since the early days of IR [13]. More recently renewed interest has led to rese arch that uses score d istributions for data fusion [14]. Other researchers have modelled document score distributions for threshold filtering [1]. Others [9] have studied the generation process of the score distribution and have provided reasons for the typical shape (Fig. 1) of the distribution.

Automatically predicting qu ery performance can aid in formation retrieval sys-tems by enabling these systems to apply diff erent strategies (e.g. query expan-sion) to queries of varying difficulty. One of the earliest approaches to QPP has been that of the clarity score [4], whi ch measures the KL-divergence between the query and collection model in a language modelling framework. Some ap-proaches [15] have measured the robustness of a ranking to perturbations and have developed novel predictors from this, while others [7] have investigated the clustering ability of similarly ranked documents to develop predictors.
Recent research has shown tha t the standard deviation (  X  ) (i.e. dispersion) of scores in a ranked-list is a good predictor of query performance [10,12,5]. These approaches are more heuristic based and lack a deeper theoretical understanding. The performance of predictors are usua lly measured by calculating the correla-tion (i.e. linear and/or non-parametric) between the output of the predictor and the performance of the query (i.e. usually average precision) over a set of queries.
However, to the authors knowledge, to date there has been no research that has directed aimed to estimate the perf ormance of a query (either using score distributions or other methods). While some predictors use document scores returned from a system, and use various measures of the dispersion of such scores to develop their predictors, the methods are unprincipled and do not aim to directly predict performance, rather some surrogate of performance. In this section, we present a mixture distribution that is used in this paper to model the scores of both relevant and non-relevant documents. 3.1 Assumptions and Mixture Model Consider an IR system that retrieves a returned set of N documents, and thus N scores given a query ( Q ). We assume that a system ranks documents inde-pendently of each other, in accordance with the probability ranking principle (PRP) [11] and that the relevance judgments are binary.

The log-normal distribution has been used successfully [14] to model scores for fusion tasks in IR, and therefore, we adopt this distribution 1 . The probability density function (pdf) of the log-normal distribution is as follows: where  X  and  X  are the parameters. This distribution is supported from 0 to  X  and the cumulative density function (cdf) is again simply the integral of this function from 0 to  X  . The mean of the distribution is e  X  +  X  2 / 2 , while the variance is ( e  X  2  X  1)  X  ( e 2  X  +  X  2 ). Therefore, by rewriting the se equations the method-of-moments estimates (MME) are as follows: where m and v are the sample mean and variance res pectively. Therefore, similar to previous approaches, the document score distribution can be thought of as a mixture of relevant and non-relevant documents as follows: where P ( s | 1) is the probability density function (pdf) for the scores (s) of relevant documents, P ( s | 0) is the pdf for the scores of non-relevant documents, and where  X  = R N is the proportion of relevant documents R in the entire returned set N . 3.2 Inferring Average Precision We will now show how average precision (a standard metric for the effectiveness of a query) can be calculated directly from the mixture of continuous distribu-tions. As recall is the proportion of rel evant returned documents compared to the entire number of relevant documents, the recall at score s canbedefinedas follows: which is the cumulative density function (cdf) of the distribution of relevant documents (viewed from  X  ). Under the distributions outlined earlier for our model, we know that recall ( s ) will vary between 0 and 1, (i.e. when s =0, recall ( s ) = 1 as ensured by the cdf). Similarly, the precision at s (the proportion of relevant returned documents over the number of returned documents) can be defined as follows: Now that we can calculate the precision and recall at any score s in the range [0 :  X  ], we can create a precision-recall curv e. Furthermore, as average precision can be estimated geometrically by the area under the precision-recall curve [2], the average precision ( avg.prec ) of a query can be calculated as follows: where z ( s )= recall ( s ) which is in the range [0:1]. As these expressions are not closed-form, they can be calculated using relatively simple geometric numerical integration methods. It is worth noting that the formulae given for calculating average precision can over-estimate the act ual average precision value calculated from TREC runs. This is due to the fact that recall is calculated as the number of relevant documents in the returned set, rather than the total number of relevant documents in the collection. In this section, we develop approaches to automatically estimate (i.e. when no relevant information is known) the five parameters of the mixture model (i.e.  X  ,  X  ,  X  1 ,  X  0 ,  X  0 ) using a number of different methods. The section is comprised of three approaches to estimating the parameters of the mixture models. The first two approaches are based on heuristics and the MME of parameters. The third approach makes use of the standard EM algorithm for mixture models. We perform an analysis to find the most important parameters in one of the new parameter estimation approaches. Firstly Table 1 outlines the TREC 2 datasets used in this paper. 4.1 Estimating Moments and Mixture In this section we aim to estimate the sample moments so that the parameters of the model can, in turn, be automatica lly calculated using method of moment estimates (MME) from equations (2) (i.e. Section 3.1). Therefore, to estimate the five parameters of the log-normal model using MME, we must estimate the sample mean ( m 1 and m 0 ) and variances ( v 1 and v 0 ) for the relevant and non-relevant document scores and the mixture parameter (  X  ).

Firstly, as the number of non-relevant documents (NR) is usually much larger than the number of relevant documents (R) in the entire returned set (i.e. NR R ), we can estimate the sample mean ( m 0 )andsamplevariance( v 0 ) of the non-relevant documents by using the mean and variance of the scores in the entire returned set (i.e. N NR ), as this seems a rather sound heuristic. However, the estimation of the mean and variance ( m 1 and v 1 ) of relevant documents is more problematic.

Recent research has posited that a theo retically valid distribution should be able to approach Dirac X  X  delta function under the strong SD hypothesis [1]. Fun-damentally, as IR systems are striving to separate the set of relevant documents (R) from the set of non-relevant documents (NR), we estimate the mean ( m 1 ) and variance ( v 1 ) of the relevant set by assuming that all documents over a certain threshold score (min-max norma lised for convenience) are relevant. Fig. 2 shows the tuning 3 of this threshold on a separate tuning collection collection (i.e. the LATIMES for both title and desc queries) averaged over five different IR systems (i.e BM25, LM, Pivoted Normalisation, F2EXP [8] and ES [6]). We can see that a common stable performan ce (i.e. average Spearman correlation with average precision) for both title and desc queries, can be achieved at a min-max normalised score of around 0.5 (i.e. midway between the minimum and maximum score of a ranked list). Therefore, the sample mean ( m 1 )andvariance ( v 1 ) of the relevant document scores are estimated by calculating the mean and variance of all scores that lie in the top half of a min-max normalised score range.
At this stage, we have estimates of the mean and variance of both relevant and non-relevant document scores, and consequently, from these we can calculate four parameters of the mixture model using MME. However, the final parameter that needs to be estimated is the mixture parameter  X  . We apply a similar approach as before and assume that all documents over a certain threshold normalised score are deemed relevant 4 . Similar to the previous tuning experiment, Fig. 3 shows the performance of the mixture model at various normalised threshold scores for estimating the mixture parameter  X  . We can see that the best performance (i.e correlation with actual average precision) occurs when assuming that very few documents are relevant (i.e. only those scores that are at or above a normalised score of 0.95).

Now we have estimated, albeit heuristically, all the information needed to infer average precision without relying on relevance information. Furthermore, for all future experiments using this MMP1 (method of moments predictor) approach, the threshold for estimating m 1 and v 1 remain at normalised score of 0 . 5, and the threshold used for estimating  X  is a normalised score of 0 . 95. As we will see in the next section, the estimation of the mean and variance of non-relevant documents ( m 0 and v 0 ) is based on a rather sound heuristic. However, the approach to estimating the mean and variance of th e relevance document scores, and the mixture parameter, is where loss in predictive performance can be attributed. We shall see later in the results section (section 5.1) that the estimation from these heuristics yields very good perfor mance compared to other predictors. However, in the next section, we analyse the most important parameters (i.e. m 1 , v 1 , m 0 , v 0 ,  X  ) for the MMP1 approach outlined in this section. 4.2 Analysing Moments and Mixture In this section, we aim to identify the parameters (i.e. m 1 , v 1 , m 0 , v 0 ,  X  )that contribute most to the performance of the model outlined in the previous sec-tion (i.e. MMP1). It would be beneficial to know which parameters are more important in terms of performance (i.e. correlation with average precision). We measure the amount of information contained in each parameter by initially assuming that all parameters (i.e. moments and mixture parameters) are as ac-curate as possible (i.e. using MME when relevance labels are known). We then substitute an estimated version of each parameter (i.e. estimated without rel-evance labels) and recalculate the performance of the model. At each stage of the process, a parameter is estimated using the heuristics in the previous section (section 4.1). Theref ore, when the process is compl ete, all of the parameters of the model have been estimated witho ut use of relevance information.
Fig. 4 show the results of such a process 5 . As we view the Figure from left to right, an estimate (i.e. without using labelled data) of each parameter is sub-stituted into the model. It is clear from Fig. 4 that the estimation of the mean and variance of non-relevant documents ( m 0 and v 0 ), and the variance of rele-vant documents ( v 1 ) can be accurately estimated using the approach previously outlined (section 4.1), as the correlation coefficient does not decrease. However, when the m 1 and  X  parameters are estimated without using relevance informa-tion, the correlation coefficient decrease s a significant amount. Therefore, the two most important parameters in the model are the mean of relevant document scores ( m 1 ) and the mixture parameter (  X  ), the former being the most impor-tant. These results are averages across five different IR systems. We can report that all of the systems teste d behaved very similarly. 4.3 Motivation and Improvement While Fig. 4 (and the results later in Section 5.1) show that the first approach (MMP1) to estimating the mean and mixture of the set of relevant documents seems to be effective to some degree, th ere is little motivation as to why this may be so. The MMP1 approach estimated the mean and variance of relevant documents by using all of the scores above a normalised score of 0.5. Consider a system which returns N documents and where K =#( S ( d ) &gt; 0 . 5) is the number of documents that are above a min-max normalised score of 0.5 (i.e. they have a score in the top half of the distribution). If K is small it implies that the system has also succeeded in promoting a relatively small number of documents, compared to the returned set N . Given the view of score distributions in Fig. 1, we can see that if the relevant and non-relevant scores are separated to a higher degree, the performance of the query will also be higher. Given that the distribution of document scores from systems is positively skewed, a smaller number of documents in this set of K documents will lead to a higher mean for the relevant documents ( m 1 ). This in turn is an indicator that there is a good separation between relevant and non-rel evant documents (and subsequently an indiction of a good query).

The initial estimate of m 1 was calculated by averaging all the scores above a normalised score of 0.5. A subsequent analysis on the LATIMES tuning collection has informed us that for 80% of the queries, the mean of the set of relevant document scores lies in the top half of the score distribution. However, our initial method of estimating the mean score of relevant documents ( m 1 ) cannot return an estimate below a normalised score of 0.5. Therefore, we now propose a small modification to the initial estimate of m 1 so that a score of below 0.5 can be achieved when it is detected that the dis tribution of relevant and non-relevant document have not been seperated to a s ufficient degree. Given that a small value of K explicitly indicates good separation, the following formula give us an updated measure of the normalised mean score of relevant documents ( m 1 ) using a simple linear combination with the original normalised m 1 estimate: where K is the number of documents above a normalised score of 0 . 5, N is the returned set, and  X  is a parameter we set to 0.5 for all subsequent experiments. The left-hand side of this equation will reduce the estimate of the normalised mean score ( m 1 )when K is relatively large. Consider a query which returns N =10 , 000 documents, for which a relatively large proportion K =4 , 000 lie in the top half of the distribution. The left-hand side of the equation (1  X  log (10 , 000) =0 . 099) will return a low value which can reduce the initial normalised estimate of m 1 below 0.5. The new estimate can be unnormalised to recover a new updated mean m 1 . This updated mean m 1 canbeusedinplaceof m 1 in the initial MMP1 approach to yield a second approach (MMP2). A further discussion of the comparative results of these approaches is undertaken in the results section. 4.4 Expectation Maximisation Approach The EM algorithm is a popular unsupervised learning algorithm for estimating the parameters in mixture models [3]. We initialised the EM algorithm with the parameter estimates f rom the first MME approach (Section 4.1) that were generated using heuristics. We ran the EM algorithm for 50 iterations. Our initial experiments showed that the parameters converged prior to the 50 th iteration. In this section we present comparative results of the two QPP approaches based on heuristics that estimate the model parameters via moments (MMP1 and MMP2), and the approach based on the EM algorithm (EM). We then discuss the contributions and limitations of the research undertaken. In the subsequent results we focus on the two most popular IR systems (i.e. BM25 and LM). 5.1 Comparative Results In this section, we compare the performance of the new QPP approaches devel-oped in Section 4 (labelled MMP1, MMP2 and EM) against other state-of-the-art post-retrieval approaches. The state-of-the-art baseline approaches that we use are the clarity score [4] (a principled approach using KL-divergence), the stan-dard deviation of document scores at 100 (  X  (100)) [10], and NQC [12] also at 100 documents. We also tested the automatically tuned version of the standard deviation [10], and the maximum retrieval score of a ranked list, and found that the baselines presented in Tables 2 and 3 are stronger.

Tables 2 and 3 show the Spearman correlation 6 of the output of each predictor and average precision, for the approaches on four test collections for two promi-nent IR systems (BM25 and LM). The column labelled  X  X PT X  is the theoretically maximum correlation of the mixture model, if the parameters could be predicted using the MME from labelled relevance data. We can see that the new MMP ap-proaches outperform the clarity score on most of the collections and, in general, are comparable in performance to that of the best baselines for longer queries. In general, on short queries, the new MMP1 and MMP2 approaches outperform the baselines, with MMP2 noted as the bes t predictor. We perfo rmed statistical tests 7 on the correlation coefficients of the new MMP approaches against both baseline approaches for each collection, and found that on most of the collec-tions, the correlation coefficients were not significantly higher. We can report that when any of the baselines outperformed the MMP approaches the result was not significant, but on some collections, the MMP approaches significantly outperformed one (always the low er) of the baselines (denoted by  X  ). The MMP2 approach tends to outperform the MMP1 approach especially for longer queries. It should be noted that we have not tuned the linear combination (i..e  X  =0 . 5) parameter in this approach.
 The results of the unsupervised EM learning approach are particularly poor. We analysed the parameters returned from the approach and determined that the EM algorithm tends to grossly over-estimate the mixture parameter (  X  ), while not estimating values that are close to the actual values for  X   X  1 , X   X  1 , X   X  0 , or  X   X  0 .

It is true that the methods for estimating the parameters of the distributions are heuristic, but these can be removed when more theoretically sound methods for estimating these are discovered. There are many approaches to query per-formance prediction that have not been evaluated against the new approaches developed here, but comparative studies [10] would tend to suggest that our ap-proach is highly competitive. Furthermore, other approaches to QPP have not aimed to explicitly estimate the performance measure in question. One inter-esting practical advantage of the predi ctors developed here is that they can be easily modified to predict other performance measures. In this work, we have developed new query performance predictors that explicitly aim to predict average precision. The new predictors (MMP1 and MMP2) based on estimating the moments and mixture parameter are comparable to state-of-the-art predictors. Furthermore, an analysis of the parameters of the predictor has determined that only two parameters ( m 1 and  X  ) are of crucial importance to the performance of the predictor. This analysis aids in narrowing the focus of future work. In a broader IR sense, it follows that only these two parame-ters are of importance to any IR application using score distributions. Future work, involves researching other unsupervised learning approaches to parameter estimation in the hope that they may yi eld higher performance predictors.
