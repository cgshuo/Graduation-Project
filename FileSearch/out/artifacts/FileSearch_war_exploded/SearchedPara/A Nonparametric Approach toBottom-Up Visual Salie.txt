 The human visual system samples images through saccadic eye movements, which rapidly change strategies, such as the observer X  X  task, thoughts, or inten tions, and by bottom-up effects. The latter are usually attributed to early vision, i.e. , to a system that responds to simple, and often local image features, such as a bright spot in a dark scene. During the pas t decade, several studies have explored which image features attract eye movements. For example, Reinagel and Zador [18] found that contrast was substantially higher at gaze positions, Krieger et al. [10] reported differences in the intensity bispectra. Parkhurst , Law , and Niebur [13] showed that a saliency map [9], computed by a model similar to the widely used framework by Itti , Koch and Niebur [3, 4], is significantly correlated with human fixation patterns. Numerous other hyp otheses were tested [1, 5, 6, 10, 12, 14, 16, 17, 19, 21], including intensity, edge content, orienta tion, symmetry, and entropy. Each of the above models is built on a particular choice of ima ge features that are believed to be rel-evant to visual saliency. A common approach is to compute sev eral feature maps from linear filters that are biologically plausible, e.g. , Difference of Gaussians (DoG) or Gabor filters, and nonline arly combine the feature maps into a single saliency map [1, 3, 4, 1 3, 16, 21]. This makes it straight-forward to construct complex models from simple, biologica lly plausible components. A downside of this parametric approach, however, is that the feature ma ps are chosen manually by the designer. As a consequence, any such model is biased to certain image st ructure, and therefore discriminates features that might not seem plausible at first sight, but may well play a significant role. Figure 1: Eye movement data. (a) shows 20 (out of 200) of the na tural scenes that were presented to the 14 subjects. (b) shows the top right image from (a), toget her with the recorded fixation locations from all 14 subjects. The average viewing time per subject wa s approximately 3 seconds. Another problem comes from the large number of additional de sign parameters that are necessary in any implementation, such as the precise filter shapes, siz es, weights, nonlinearities, etc . While choices for these parameters are often only vaguely justifie d in terms of their biological plausibility, they greatly affect the behavior of the system as a whole and t hus its predictive power. The latter, however, is often used as a measure of plausibility. This is c learly an undesirable situation, since it makes a fair comparison between models very difficult. In fac t, we believe that this may explain the conflicting results in the debate about whether edges or cont rast filters are more relevant [1, 6, 13]. In this paper we present a nonparametric approach to bottom-up saliency, which does not (or to a far lesser extent) suffer from the shortcomings described above. Instead of using a predefined set of feature maps, our saliency model is learned directly from human eye movement data. The model consists of a nonlinear mapping from an image patch to a real value, trained to yield positive outputs on fixated, and negative outputs on randomly selecte d image patches. The main difference to previous models is that our saliency function is essentiall y determined by the fact that it maximizes the prediction performance on the observed data. Below, we s how that the prediction performance of our model is comparable to that of biologically motivated models. Furthermore, we analyze the system in terms of the features it has learned, and compare ou r findings to previous results. Eye movement data were taken from [8]. They consist of 200 nat ural images (1024  X  768, 8bit grayscale) and 18,065 fixation locations recorded from 14 na  X   X ve subjects. The subjects freely viewed each image for about three seconds on a 19 inch CRT at full scre en size and 60cm distance, which corresponds to 37  X   X  27  X  of visual angle. For more details about the recording setup, please refer to Below, we are going to formulate saliency learning as a class ification problem. This requires neg-ative examples, i.e. , a set of non-fixated, or background locations. As pointed ou t in [18, 21], care must be taken that no spurious differences in the local image statistics are generated by using dif-ferent spatial distributions for positive and negative exa mples. As an example, fixation locations are usually biased towards the center of the image, probably due to the reduced physical effort when looking straight. At the same time, it is known that local ima ge statistics can be correlated with image location [18, 21], e.g. , due to the photographer X  X  bias of keeping objects at the cen ter of the image. If we sampled background locations uniformly over th e image, our system might learn the difference between pixel statistics at the image center and towards the boundary, instead of the de-sired difference between fixated and non-fixated locations. Moreover, the learning algorithm might be mislead by simple boundary effects. To avoid this effect, we use the 18,065 fixation locations to generate an equal number of background locations by using th e same image coordinates, but with the corresponding image numbers shuffled. This ensures that the spatial distributions of both classes are identical.
 The proposed model computes saliency based on local image st ructure. To represent fixations and background locations accordingly, we cut out a square image patch at each location and stored the pixel values in a feature vector x background. Unfortunately, choosing an appropriate patch size and resolution is not straightforward, as there might be a wide range of reasonable values. To remedy this, we follow the approach proposed in [8], which is a simple compromise between comput ational tractability and generality: we fix the resolution to 13  X  13 pixels, but leave the patch size d unspecified, i.e. , we construct a separate data set for various values of d . Later, we determine the size d which leads to the best generalization performance estimate. For each image locat ion, 11 patches were extracted, with sizes ranging between d = 0 . 47  X  and d = 27  X  visual angle, equally spaced on a logarithmic scale. Each patch was subsampled to 13  X  13 pixels, after low-pass filtering to reduce aliasing effects . The range of sizes was chosen such that pixels in the smallest patch cor respond to image pixels at full screen resolution, and that the largest patch has full screen heigh t. Finally, for each patch we subtracted the mean intensity, and stored the normalized pixel values in a 169 -dimensional feature vector x The data were divided into a training (two thirds) and a test s et (one third). This was done such that both sets contained data from all 200 images, but never from t he same subject on the same image. For model selection (Section 4.1) and assessment (Section 4.2) , which rely on cross-validation estimates of the generalization error, further splits were required. These splits were done image-wise, i.e. , such that no validation or test fold contained any data from i mages in the corresponding training fold. This is necessary, since image patches from different locations can overlap, leading to a severe over-estimation of the generalization performance. From the eye movement described in Section 2, we learn a botto m-up saliency map f ( x ) : R 169  X  R using a support vector machine (SVM) [2]. We model saliency a s a linear combination of Gaussian radial basis functions (RBFs), centered at the training poi nts x The SVM algorithm determines non-negative coefficients  X  S ( f ) is the standard SVM regularizer 1 2 k f k 2 [2]. The tradeoff between data fit and smoothness is controlled by the parameter  X  . As described in Section 4.1, this design parameter, as well as the RBF bandwidth  X  and the patch size d is determined by maximizing the model X  X  estimated prediction performance.
 It is insightful to compare our model (1) to existing models. Similar to most existing approaches, our model is based on linear filters whose outputs are nonlinearl y combined into a real-valued saliency measure. This is a common model for the early visual system, a nd receptive-field estimation tech-niques such as reverse-correlation usually make the same as sumptions. It differs from existing approaches in terms of its nonparametric nature, i.e. , the basic linear filters are the training samples themselves. That way, the system is not restricted to the des igner X  X  choice of feature maps, but learns relevant structure from the data. For the nonlinear compone nt, we found the Gaussian RBF appro-priate for two reasons: first, it is a universal SVM kernel [20 ], allowing the model to approximate any smooth function on the data points; second, it carries no information about the spatial ordering of the pixels within an image patch x : if we consistently permuted the pixels of the training and test patches, the model output would be identical. This impl ies that the system is has no a priori preference for particular image structures. The SVM algori thm was chosen primarily since it is a Figure 2: Selection of the parameters d ,  X  and  X  . Each panel shows the estimated model performance for a fixed d , and all  X  (vertical axes, label values denote log denote log Based on these results, we fixed d = 5 . 4  X  , log powerful standard method for binary classification. In ligh t of its resemblance to regularized logistic regression, our method is therefore related to the one propo sed in [1]. Their model is parametric, however. 4.1 Selection of d,  X  , and  X  For fixing d,  X  , and  X  , we conducted an exhaustive search on a 11  X  9  X  13 grid with the grid 0 . 001 , . . . , 10 , 000 . In order to make the search computationally tractable, we d ivided the training set (Section 2) into eight parts. Within each part, and for ea ch point on the parameter grid, we computed a cross-validation estimate of the classification accuracy ( i.e. , the relative frequency of sign f ( x grid point. Figure 2 illustrates the results. Each panel sho ws the model performance for one (  X ,  X  ) -slice of the parameter space. The performance peaks at 0 . 55 (0.013 standard error of mean, SEM) at show the same, smooth pattern which is known to be characteri stic for RBF-SVM model selection [7]. This further suggests that, despite the low absolute pe rformance, our choice of parameters is well justified. Model performance (Section 4.2) and interpr etation (Section 4.3) were qualitatively stable within at least one step in any direction of the parame ter grid. Figure 3: Saliency maps. (a) shows a natural scene from our da tabase, together with the recorded eye movements from all 14 subjects. Itti  X  X  saliency map, using  X  X tandard X  normalization is shown in (b). Brighter regions denote more salient areas. The pictur e in (c) shows our learned saliency map, which was re-built for this example, with the image in (a) exc luded from the training data. Note that the differing boundary effects are of no concern for our perf ormance measurements, since hardly any fixations are that close to the boundary. 4.2 Model Performance To test the model X  X  performance with the optimal parameters ( d = 5 . 4  X  ,  X  = 1 ,  X  = 1 ) and more training examples, we divided the test set into eight folds. Again, this was done image-wise, i.e. , such that each fold comprised the data from 25 images (cf. Sec tion 2). For each fold we trained our model on all training data not coming from the respective 25 images. As expected, the use of more training data significantly improved the accuracy to 0 . 60 ( 0 . 011 SEM). For a comparison with other work, we also computed the mean ROC score of our sys tem, 0 . 64 ( 0 . 010 SEM). This performance is lower than the 0 . 67 reported in [8]. However, their model explains only about 10% of the  X  X implest X  fixations in the data. Another recent study yielded 0 . 63 [21], although on a different data set. Itti  X  X  model [4] was tested in [15], who report ROC scores around 0 . 65 (taken from a graph, no actual numbers are given). Scores of up to 0 . 70 were achieved with an extended version, that uses more elaborate long-range interactions and eccentricity-dependent processing. We also ran Itti  X  X  model on our test set, using the code from [22]. We tried both t he  X  X tandard X  [3] and  X  X terative X  [4] normalization scheme. The best performing setting was t he earlier  X  X tandard X  method, which yielded 0 . 62 ( 0 . 022 SEM). The more recent iterative scheme did not improve on thi s result, also not when only the first, or first few fixations were considered. For a qualitative comparison, Figure 3 shows our learned saliency map and Itti  X  X  model evaluated on a sample image.
 It is important to mention that the purpose of the above compa rison is not to show that our model makes better predictions than existing models  X  which would be a weak statement anyway since the data sets are different. The main insight here is that our non parametric model performs at the same level as existing, biologically motivated models, which im plement plausible, multi-scale front-end filters, carefully designed non-linearities, and even glob al effects. 4.3 Feature Analysis In the previous section we have shown that our model generali zes to unseen data, i.e. , that it has learned regularities in the data that are relevant to the hum an fixation selection mechanism. This existing models. As mentioned in Section 1, characterizing a nonlinear model solely by the feature maps at its basis is insufficient. In fact, our SVM-based mode l is an example where this would be particularly wrong. An SVM assigns the smaller (down to zero ) weights  X  training samples x misleading, since they represent unusual examples, rather than prototypes. To avoid this, we instead characterize the learned function by means of inputs x that are particularly excitatory or inhibitory to the entire system. As a first test, we collected 20 , 000 image patches from random locations in natural scenes (not in the training set) and presented the m to our system. The top and bottom 100 patches sorted by model output and a histogram over all 20 , 000 saliency values are shown in Figure 4 . Note that since our model is unbiased towards any pa rticular image structure, the different
Figure 4: Natural image patches ranked by saliency according to our model. The panels (a) and (b) show the bottom and top 100 of 20 , 000 patches, re-spectively (the dots in between denote the 18 , 800 patches which are not shown). A histogram of all 20 , 000 saliency values is given on the lower right.

The outputs in (a) range from  X  2 . 0 to  X  1 . 7 , the ones in (b) from 0 . 99 to 1 . 8 .  X 2  X 1 0 1 2 0 patterns observed in high and low output patches are solely d ue to differences between pixel statistics at fixated and background regions. The high output patches se em to have higher contrast, which is in agreement with previous results, e.g. , [8, 10, 14, 18]. In fact, the correlation coefficient of the model output (all 20 , 000 values) with r.m.s. contrast is 0 . 69 . Another result from [14, 18] is that randomly chosen locations. Figure 4 shows this trend as well : as we move away from the patch center, the pixels X  correlation with the center intensity d ecays faster for patches with high predicted salience. Moreover, a study on bispectra at fixated image loc ations [10] suggested that  X  X he saccadic selection system avoids image regions, which are dominated by a single oriented structure. Instead, it selects regions containing different orientations, lik e occlusions, corners, etc  X . A closer look at Figure 4 reveals that our model tends to attribute saliency n ot alone to contrast, but also to non-trivial image structure. Extremely prominent examples of this effe ct are the high contrast edges appearing among the bottom 100 patches, e.g. , in the patches at position (7,2) or (10,10).
 To further characterize the system, we explicitly computed the maximally excitatory and inhibitory stimuli. This amounts to solving the unconstrained optimiz ation problems arg max arg min x f ( x ) , respectively. Since f is differentiable, we can use a simple gradient method. The only problem is that f ( x ) can have multiple extrema in x . A common way to deal with local optima is to run the search several times with different initial val ues for x . Here, we repeated the search 1 , 000 times for both minima and maxima. The initial x were constructed by drawing 169 pixel values from a normal distribution with zero mean and then nor malizing the patch standard deviation to 0 . 11 (the average value over the training patches). The 1 , 000 optimal values were then clustered using k -means. The number of clusters k was found by increasing k until the clusters were stable. Interestingly, the clusters for both minima and maxima were already highly concentrated for k = 2 , i.e. , within each cluster, the average variance of a pixel was les s than 0 . 03% of the pixel variance of its center patch. This result could also be confirmed visua lly, i.e. , despite the randomized initial values both optimization problems had only two visually dis tinct outcomes. We also re-ran this experiment with natural image patches as starting values, w ith identical results. This indicates that our saliency function has essentially two minima and two max ima in x . The four optimal stimuli are shown in Figure 5 . The first two images (a) and (b) show the m aximally inhibitory stimuli. Figure 5: Maximally inhibitory and excitatory stimuli of the learned model. Note the large mag-nitude of the saliency values compared to the typ-ical model output (cf. the histogram in Figure 4). (a) and (b): the two maximally inhibitory stim-uli (lowest possible saliency). (c) and (d): the two maximally excitatory stimuli (highest possi-ble saliency), (e) and (f): the radial average of (c) and (d), respectively. These are rather difficult to interpret other than no particu lar structure is visible. On the other hand, the maximally excitatory stimuli, denoted by (c) and (d), ha ve center-surround structure. All four stimuli have zero mean, which is not surprising since during gradient search, both the initial value and the step directions X  X hich are linear combinations of the training data X  X ave zero mean. As a consequence, the surrounds of (c) and (d) are inhibitory w. r.t. their centers, which can also be seen from the different signs in their radial averages (e) an d (f). 3 The optimal stimuli thus bear a close resemblance to receptive fields in the early visual sys tem [11]. To see that the optimal stimuli have in fact prototype character, note how the histogram in F igure 4 reflects the typical distribution of natural image patches along the learned saliency functio n. It illustrates that the saliency values of unseen natural image patches usually lie between  X  2 . 0 and 1 . 8 (for the training data, they are cating that they represent the difference between fixated an d background locations in a much more articulated way than any of the noisy measurements in our dat a set. We have presented a nonparametric model for bottom-up visua l saliency, trained on human eye movement data. A major goal of this work was to complement exi sting approaches in that we keep the number of assumptions low, and instead learn as much as po ssible from the data. In order to make this tractable, the model is rather simplistic, e.g. , it implements no long-range interactions within feature maps. Nevertheless, we found that the prediction pe rformance of our system is comparable to that of parametric, biologically motivated models. Alth ough no such information was used in the design of our model, we found that the learned features ar e consistent with earlier results on bottom-up saliency. For example, the outputs of our model ar e strongly correlated with local r.m.s. contrast [18]. Also, we found that the maximally excitatory stimuli of our system have center-surround structure, similar to DoG filters commonly used in e arly vision models [3, 13, 21]. This is a nontrivial result, since our model has no preference for an y particular image features, i.e., a priori, any 13  X  13 image patch is equally likely to be an optimal stimulus. Rece ntly, several authors have explored whether oriented (Gabor) or center-surround (DoG ) features are more relevant to human eye movements. As outlined in Section 1, this is a difficult ta sk: while some results indicate that both features perform equally well [21], others suggest tha t one [1] or the other [6, 13] are more relevant. Our results shed additional light on this discuss ion in favor of center-surround features. [1] R. J. Baddeley and B. W. Tatler. High frequency edges (but not contrast) predict where we [2] C. J. C. Burges. A tutorial on support vector machines for pattern recognition. Data Mining [5] L. Itti. Quantifying the contribution of low-level sali ency to human eye movements in dynamic [6] L. Itti. Quantitative modeling of perceptual salience a t human eye position. Visual Cognition [3] L. Itti, Koch C., and E. Niebur. A model of saliency-based visual attention for rapid scene [4] L. Itti and C. Koch. A saliency-based search mechanism fo r overt and covert shifts of visual [7] S. S. Keerthi and C. J. Lin. Asymptotic behaviors of suppo rt vector machines with gaussian [8] W. Kienzle, F. A. Wichmann, B. Sch  X  olkopf, and M. O. Franz. Learning an interest opera-[9] C. Koch and S. Ullman. Shifts in selective visual attenti on: towards the underlying neural [10] G. Krieger, I. Rentschler, G. Hauske, K. Schill, and C. Z etzsche. Object and scene analy-[11] S. W. Kuffler. Discharge patterns and functional organi zation of mammalian retina. Journal of [12] S. K. Mannan, K. H. Ruddock, and D. S. Wooding. The relati onship between the locations [13] D. J. Parkhurst, K. Law, and E. Niebur. Modeling the role of salience in the allocation of overt [14] D. J. Parkhurst and E. Niebur. Scene content selected by active vision. Spatial Vision , [15] R. J. Peters, A. Iyer, C. Koch, and L. Itti. Components of bottom-up gaze allocation in natural [16] C. M. Privitera and L. W. Stark. Algorithms for defining v isual regions-of-interest: Com-[17] R. Raj, W. S. Geisler, R. A. Frazor, and A. C. Bovik. Contr ast statistics for foveated visual [18] P. Reinagel and A. M. Zador. Natural scene statistics at the center of gaze. Network: Compu-[19] L. W. Renninger, J. Coughlan, P. Verghese, and J. Malik. An information maximization model [20] I. Steinwart. On the influence of the kernel on the consis tency of support vector machines. [22] D. Walther. Interactions of visual attention and object recognition: c omputational modeling,
