 Clustering is an unsupervised learning technique used to group a set of data samples with similar attributes such that the coherence inside a cluster is higher than that be-tween clusters. Co-clustering is an approach which provides two or more simultaneous clustering, which has been widely used for years. Traditionally, clustering algorithms measure the degree of coherence by optimizing various kinds of objective functions defined to minimize the distance between samples. Co-clustering, on the other hand, has attracted great attention because it simultaneously measures the degree of coher-ence in samples and in attributes. The recently proposed Bregman co-clustering algo-rithm [2, 3] has shown significant promise, both for the quality of clusterings produced and its computational efficiency. Theoretically, the algorithm is scalable, but practi-cally, the scalability is restricted by the memory space available. For example, when the dataset does not fit entirely in main memory, existing co-clustering algorithms spend a significant fraction of their time in wasteful disk I/O. In this paper, we address the issue of I/O scalability of co-clustering algorithms. Specifically, we consider the general framework proposed in [3] and propose an ap-proach that utilizes an underlying database to improve its performance. In [5], Chen et al. emphasize the importance of implementing efficient and scalable algorithms that can work on large datasets stored in a database. A key observation in our approach is that co-clustering algorithms require the computation (and often recomputation) of a number of basic summary statistics. This cost can be significantly reduced by using an online analytical processing (OLAP) [8] engine to compute the summary statistics, used to help build the approximation matrix. We contribute to the mapping between the data access operations of the Bregman co-clustering algorithm and those of OLAP. To our knowledge, this topic has not been explor ed before. Moreover, recent studies [1, 3] have been very successful in applying co-c lustering to various applications. Thus, we believe that the techniques presented here will go a long way in making co-clustering applicable to large-scale datasets. The rest of this paper is organized as follows. Section 2 briefly introduces the Bregman co-clustering algorithm, and Section 3 presents the mapping from the algo-rithm and OLAP. Section 4 presents experimental results, while Section 5 discusses related works. Finally, conclusions and future work are given in Section 6. Co-clustering, like traditional clustering, uses attributes to group samples. However, unlike traditional clustering, co-clustering clusters rows and columns simultaneously if we use a contingency matrix where a row repr esents a sample and a column represents an attribute. By grouping similar attributes, co-clustering implicitly achieves dimen-sionality reduction. This feature reduces the running time and also leads to more in-formative clusters. 
Co-clustering gives accuracy comparable to state-of-the-art approaches in a variety of applications. [3] compares four appr oaches: SVD [15], NNMF [10], a correlation based method, and co-clustering for collaborative filtering. The mean absolute errors (MAEs) on MovieLens dataset 1 for the above four approaches are 0.7721, 0.7636, 0.8214, and 0.7608, respectively. Furthermore, [1] incorporates the statistics about users and movie content, and uses Bregman divergences to find a co-clustering that provides most accurate prediction with the adjustment of covariates. 
The Bregman co-clustering algorithm associates a co-clustering task with a matrix approximation task, and the quality of the result is evaluated by the approximation error [3]. Generally, any Bregman divergence can be used for the matrix approximation problem. [3] views a co-clustering task as the search for an optimal approximation matrix, where the approximation is based on the co-clustering so that a better co-clustering leads to a better matrix approximation. The optimality is determined by the minimum Bregman information (MBI) principle [3]. 
The algorithm builds an approximation matr ix according to a user-specified Breg-man co-clustering basis, a set of summary statistics. Table 1 summarizes six bases is a set of column and  X  , or  X  (V) , maps n columns to l column clusters. For example, C 2 column clusters, while C 5 corresponds to co-clusters, rows and columns. In [3], a set of summary statistics is defined as a set of random variables, i.e., S ={Z X  X E[Z|c]=E[Z X  X c], for all c in C} , where C is a basis and Z X  preserves the sum-mary statistics. The co-clustering problem is described as follows. Given an m-by-n matrix Z , a Bregman divergence d  X  , the number of row clusters k , the number of column clusters l , and a co-clustering basis C , the goal is to find an (  X  ,  X  ) that minimizes the expectation of the Bregman divergence between Z and the approximation matrix Z  X  which is the Z X  giving the minimum Bregman information. It is an optimization problem and the solution is obtained by solving the following equation: Note that w presents the weight and  X  * is an optimal Lagrange multiplier [3]. 
The iterative update approach [3], as in Fig. 1, is a technique to find a locally optimal solution for this NP-hard problem. Initially, for each row (column) the algorithm ran-domly assigns a row (column) cluster. Next, it calculates the summary statistics ac-cording to a user-specified co -clustering basis. When handling row (column) clusters, it treats column (row) clusters as known information and builds an approximation matrix. Subsequently it evaluates and updates row (column) clusters with the best approxima-tion matrix (w. r. t. MBI), i.e., it updates row (column) clusters with the best ap-proximation matrix. 
Computing the summary statistics is a key step, and obtaining the Lagrange multi-pliers for a specified basis is the goal of the computation. The computation requires significant effort, regardless of whether the Bregman divergence corresponds to a closed form solution. Since [3] guarantees a closed form solution for the Lagrange multipliers in terms of the summary statistics for the squared Euclidean distance and I-Divergence, computing the summary statistics is the main bottleneck. The time complexity of the Bregman co-clustering algorithm is relative to the number of non-zero elements in Z [3]. One naive implementation is to adopt a pure memory-based solution, e.g., Matlab. However, such an implementation is under the memory con-straint especially when datasets are too large to fit into main memory. For an efficient implementation, we need advanced secondary storage management techniques, which are major functions of relational databases. Furthermore, SQL is superior is providing set-oriented operations, and an OLAP engine prepares the sum-mary statistics inside the database, so we do not need to read all data into memory for the computation. 
The first design issue is how to store sparse matrices for OLAP. Three data struc-tures are generally used to store sparse matrices: coordinate storage (COO), com-pressed sparse row (CSR), and compresses sparse column (CSC). CSR requires much less storage space than COO does but the difference of processing time is not signifi-cant [6]. We employ COO to store matrices , because it corresponds to a denormalized table and others are normalized ones while querying one denormalized table is gener-ally faster than querying and joining two normalized ones. 
In addition, we use star schema to create the data cube used in our implementation. A data cube consists of all combinations of hierarchical relationships [8], and hence it can be represented in a lattice form, as Fig. 2, where a node will give a smaller data cube or a table that contains aggregates along all possible combinations of specified dimen-sions. To build the approximation matrix based on C 2 , for example, we need the weighted average of elements in each block, i.e., average of each interaction of column and row clusters, while aggregates along all combinations of row and column clusters can be obtained from Node 9 in Fig. 2. As another example, C 5 requires the summary statistics from Node 9, 12, 13, 14, 15. Table 2 summarizes the mappings between Bregman co-clustering bases and the data cube lattice. Our implementation is built in C# with Microsoft SQL Server 2005 as the backend database. We evaluate it by comparing it against the Matlab implementation for all Bregman co-clustering bases. Fig. 3 illustrates experimental results. 
Matrices used here (i.e., gre_1107, nnc1374, pores_2, dw2048, zenios, lnsp3937, and e20r5000, ordered by size) are available on http://math.nist.gov/MatrixMarket/. Both k and l are set to 10, and we allow at most 20 iterations in a run. For each dataset (except the largest one) and for each basis, we run each implementation (with the squared Euclidean distance function) 10 times. Fig. 3 presents that both implementa-tions produce similar shapes of curves but ours achieves a slower growth of the curve. As the number of non-zero elements increases, their differences also increase. For the memory consumption, the Matlab implementation requires more than 800M bytes of memory for the second largest dataset and it can not even process the largest one, but ours processes the largest dataset without difficulties. Since this is an I/O-bound problem, differences from compilers and languages are not the main issue. One example for integrating data mining algorithms into OLAP is DBMiner [9] Using OLAP to quickly collect properties of clusters for advanced analyses, DBMiner [9] supports classification, association rules mining, and cluster analysis; in contrast, we employ data cubes to implement the Bregman co-clustering algorithm. [7] extends SQL to compute the summary statistics for classification, whereas we follow standard op-erations and enjoy higher portability and flexibility. [11] discusses the vertical, hori-zontal, and hybrid table schemas for implementing EM algorithm in SQL. The vertical table schema is a flexible design even though it comes with the highest overhead. Our schema acts as the vertical one because mapping the vertical table schema to COO is straightforward. Nevertheless, our schema does not suffer from the overhead since there are less join operations used to create a cube. Furthermore, the approach proposed restricted to the length of a SELECT statement. [12] argues that integrating clustering algorithms into a database is practical, while [13] proposes a pure SQL-based approach to perform the K-means clustering over large datasets. However, it is not the nature of SQL to compute the loss function for each sample for each cluster. Thus, we compute the Bregman divergences in memory. For implementations of the K-mean algorithm, [14] compares SQL to C++ and concludes that the SQL implementation presents a slower growth. We present a similar phenomenon. [16] proposes a system, WekaDB, to help Weka 3 handle large datasets. WekaDB is slower than Weka but it can handle much larger datasets. In some cases, WekaDB is faster than a pure SQL-based approach. We agree with the conclusion: Using a database as a backend and main memory as a buffer would provide higher scalability. In this paper, we proposed a novel and efficient implementation for the Bregman co-clustering algorithm, which has been proven to generate substantially better co-clustering results than other algorithms. Our implementation utilizes an OLAP en-gine to obtain the summary statistics used for the construction of approximation ma-trices, and then it reads data from a database to memory for the computation of Bregman divergences. This paper contributes to the mapping between Bregman co-clustering bases to the data cube, and we also demonstrate that a database can act as an effective computation engine for data mining. Experimental results show that our implementation provides higher scalability by using OLAP. Future work includes the study of index structures and the extension to multi-dimensional co-clustering [4]. 
