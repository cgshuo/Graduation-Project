 The automated text categoriza tion (TC) has made prominent progress in recent years. Howe ver, seldom work is done on automatic classification with library classification systems, the largest and most sophisticated cl assification systems people ever built, such as the Dewey Decimal classification (DDC). The library classification is a very laborious and time-consuming job that requires qualificati on and good training. The large-scale classification schemes, such as the DDC, impose several obstacles to the state-o f-art TC technologies, including very deep hierarchy, data sparseness, and skewed category distribution. These problems characterize large corpora of real-world applications and it is very hard, if not impossible, to obtain satisfactory results. In this paper, we propose a novel algorithm to reconstruct classification schemes according to the document density and category distribution, and to transform the category hierarchy into a balanced virtual taxonomy by merging sparse categories, lopping dense branches and flattening the hierarchy. To make the classification performance acceptable to real-world applications, we also propose an interactive classification model that only needs two or three times of user interaction. Extensive experiments are conducted on a 10-year bibliographic data collection of the Library of Congress to verify the proposed methodology. I.5.2 [ Computing Methodologies ]: Design Methodology X  Classifier design and evaluation . H.3.m [ INFORMATION STORAGE AND RETRIEVAL ]: Miscellaneous. H.5.m [ INFORMATION INTERFACES AND PRESENTATION ] Miscellaneous. I.2.7 [ Computing Methodologies ]: Natural Language Processing-Text analysis Algorithms, Design, Experime ntation, Human Factors, Measurement, Performance. Dewey Decimal Cla ssification; Taxonomy Reconstruction; Interactive Classification; Hierarchical Classification; Bibliographic Data; Trimming Machine. Our era has witnessed an expl osive growth of digital and networked information resources. Although they have become a ubiquitous existence on the We b and can be easily accessed anytime, library collections are still one of the precious and indispensable information assets, and increase at a steady pace. For example, the Library of Congress (LC) collection has exceeded 130 million items, and is increasing at a speed of more than two million items per year [16]. One of the distinctive characteristics of library collections is that every information item has its subject matters classified with established knowledge schemes in detail, like the Library of Congress Classification (LCC) and the Dewey Decimal Classification (DDC). An astonishing fact to researcher s who are committed to automating information processing is that this job is accomplished by human force. The LC annually classi fies over 110,000 books using the DDC, and spends an average amount of nearly $100 in cataloging one book [16]. Just imagine how many libraries there are in the world, where such kind of practice is undertaken everyday. Library classification is an intelligence-demanding and time-consuming task. To be a qualified cataloger, a librarian has to go through rigid training to comprehend the huge classification systems and master the complicated classification rules. In recent years, the wide availa bility of full-text documents in digital form has intrigued a booming interest in automated Text Categorization (TC), especially the machine learning based approach, and resulted in very rich literature. But, surprisingly, few studies concern the problem of automatic library classification. This is probably because bibliographic data is somewhat  X  X ld-fashioned X , contai ning only limited content-related information, not an ideal testbed fo r the full play of TC techniques. Another reason, more important ly, is that the automatic classification with large-scale taxonomies is still a great challenge. Up to now, none of previous studi es on large-scale classification has produced satisfactory results [7, 8, 18, 27], although many aspects have been investigated, including classification algorithms, feature selection strategies, scalability, thresholding and parameter tuning. In this paper we propose two approaches to address this problem: 1) to reconstruct classification schemes: As analyzed by Liu &amp; 2) to adopt an interactive classi fication model: to bring about Library classification systems, th e largest and most well-developed classification systems people ever constructed provides an ideal testbed for such an investigation. This paper presents an thorough theoretical examination and extensive experiments on the automated assignment of DDC numbers to bibliographic data with a supervised machine learning approach. The DDC is the most widely used, studied, and discussed library cla ssification system. In the United States, it is used in 95% of all public and school libraries [3]; around the world, it has been translated into more than 30 languages and serves library users in over 200,000 libraries in 135+ countries [20]. It has been undergoing continuous revising and editing over more than a century, and has published 22 editions. And, huge amount of bibliographic data describing pub lications written in all kinds of languages has accumulated over it, which is an unparalleled corpus for machine learning, because: 1) It covers all disciplines of hu man knowledge and spans tens of 3) It has a standard format that contains both descriptive (e.g. The main contributions of the present work are: 1) It proposes an innovative algorith m to re-shape a classification 2) It is a thorough examination of the suitability of the DDC for the dataset used in this study and discuss obstacles in employing supervised learning methods in solving library classification problems. Section 4 is the focus of this paper. After a theoretical analysis, we reach the conclusion that full automatic classification is very hard, if not impossible, for ex isting TC technologies to achieve acceptable performance. We thereb y propose to reconstruct the DDC scheme and to use an interactive classification model instead. Section 5 is the extensive experiments and result analysis. The last section concludes the work. There are two groups of previous research related to the present work: automatic library classification and hierarchical classification on large-scale taxonomies. None of them have produced promising results. Only a few researchers paid a ttention to automatic library classification. Larson attempted to resolve this problem by an information retrieval approach [13]. He represented a LCC class by a virtual document that consists of title words and subject headings extracted from the bibliographic records with the same LCC code; new bibliographic record was classified by searching the virtual document closest to it. In his expe riments, thirty thousands records were used to build virtual documents, only 283 records were used as test instances, far from suffici ent. Frank &amp; Paynter followed a machine-learning approach to predict LCC codes for metadata records based on the manually assigned subject headings [8]. A linear support vector machine (SVM) was used to learn a hierarchical classifier from a la rge library catalog. They used the LCC Outline, a summary of the full LCC, in their experiments and gained an accuracy of 0.55. Othe r attempts on automatic library classification include Scorpion [25] and Pharos [6]. They are not discussed further because either the detail results are not disclosed or the reported accuracy is too low (0.14). Real-world applications often requir e a system to deal with tens of thousands of categories defined over a large taxonomy. In this case, hierarchical classification is the only feasible resolution, which decomposes a classification task into a group of smaller ones to reduce the computation cost greatly. Very few papers address the problem of classification with very large taxonomies, and promising results have not been reported yet. As far as we know, the largest taxonomy that is ever handled in TC experiments is Yahoo! Directory. It is used in Liu &amp; Yang X  X  remarkable experiments on the hierarchical classification of web pages [18]. At the time of their crawling (June, 2004), there were 246,279 categories organized into a 16-level hierarchy. After extensive experiments with SVMs, they admitted that the classification eff ectiveness  X  X s far from satisfactory, and more substantial inve stigation is needed X . The LCC Outline used in Frank &amp; Paynter X  X  study consists of 4,214 classes that are selected from the top levels of the LCC [15]. The LCC Outline, a hierarchy of about 7 levels, is only a summary of the full LCC. And a total number of 800,000 catalog records used as training instances in their experime nts. Thus the problems of deep hierarchy and data sparseness were avoided to some extent. Even on small-size taxonomies containing a few hundred categories, satisfactory results are also hard to obtain. Dumais &amp; Chen used top 2 levels of LookSmart X  X  web directory (163 categories in total) to classify web search results. The overall F1 value at the 2 around 0.5. Cai &amp; Hofmann classified patent documents that are indexed with the International Pate nt Classification (IPC) [5]. The experiments were conducted on several sections of the IPC separately, of which the largest one is 4-level deep and contains 1,172 categories. The best accuracy was 0.43. Some researchers reported high effectiveness from their hierarchical classifi cation experiments [12, 24, 26]. Looking closely, we found that the tax onomies they used were derived manually from the tested corpora by themselves. For example, Koher &amp; Sahami, and Sun &amp; Lim extracted category trees from the Reuters collections by identifying the category labels that suggest parent-child relationships [12, 24]. The largest tree is 3-level deep, and contains only 9 categories. As to the scalability and efficiency of TC algorithms over classification hierarchy, Yang proved with a thorough theoretical analysis that the hierarchical use of SVMs is efficient enough for large-scale classification. But, she left out the effectiveness problem and didn X  X  disclose the results of her experiments conducted on heart disease sub-domain of the OHSUMED corpus, which is classified with MeSH [27]. Table 1 summarizes the real-world taxonomies that have been used in TC expe riments up to now. The DDC scheme used in the present work is also listed for comparison. LCC Outline Full taxonomy 7 4,214 To the best of our knowledge, none of the TC experiments over real-world large taxonomies have produced satisfactory results. Is the low performance an intrinsic limitation imposed by the large-scale taxonomies? We believe that the research on the automatic classification with the DDC will shed light on this question. In this section, we introduce the dataset used in the present work and analyze its distribution characteristics, which will help us gain an understanding of real-world classification problems. We set up a bibliographic dataset fo r this study, referred to as the Bibliographic Data on Science and Techonology (BDS&amp;T), by extracting from the OCLC WorldCat system all the bibliographic records that are within the domains of science and technology (DDC main classes 5xx and 6xx), with a proper title in English, and created by the Library of Congress during 1994 to 2004. There are 88,440 records in total, representing LC X  X  ten-year S&amp;T collection. The BDS&amp;T is basically a single-labeled corpus, most of the bibliographic records just have one DDC numbers assigned. The DDC uses Arabic numerals to represent classes. If we take a DDC number assigned to a bibliographic record as an integral category, and let each digit represent a hierarchical level, then we have 18,462 categories in the BD S&amp;T, forming a hierarchy as deep as 23, the deepest taxonomy ever tested in TC studies. Following the TC nomenclature, we refer to a DDC class as a category and a bibliographic record as a document. The solid curve in Figure 1/2 illustrates the spindle distribution of categories/documents over the DDC hierarchy. There are more documents and categories in the middle than at the upper or lower levels of the hierarchy. Data sparseness is a severe problem in bibliographic corpora. The dotted curve in Figure 1 shows the document quantity per category at each level, which is below three after the 4 th level, barely sufficient for training an effective classifier. The dotted curve in Figure 2 shows the percentage of sparse categories per level. Sparse categories are those ones with very few documents (say, less than 3). The curve climbs up with the deepening of the hierarchy and reaches to a degree that more than 80% of the categories at each level are sparse ones after the 9 th level. The data sparseness problem stem s from the library cataloging rules that require a document to be classified as precise as possible with the provision of various DDC number synthesis devices [3]. This fits very well the purpose of book shelving, to define a unique shelve place for each book, but leaves the inductive learning process incapacitated w ith an insufficient number of training instances. For example, more than 60% of the categories in the BDS&amp;T, up to 11,552, have just 1 document. Meanwhile, the concentration of t opics induces a small number of populous categories, which only account for 1% of the total categories, but hold 25% of all th e documents. This reflects the skewed category distribution over documents, which is further illustrated in Figure 3 where the x-axis is the size of categories in terms of the document quantity and y-axis is the frequency of categories (both in log-scale). It is clear that the document quantity per category follows the power law distribution [27]. The similar problems are also observed in web pages organized in Yahoo! Directory [18]. Different from the benchmark collections used in experimental circumst ances, these problems usually characterize datasets from real-world appl ications and impose great obstacles to supervised learning methods. The inductive construction of text classifiers us ually assumes sufficient training instances and even distribution of categories. Organizing categories into a large and deep hierarchy makes these problems worse. With theoretical analysis and empirical evidence, Yang, Zhang &amp; Kisiel have shown that  X  X ow the scalability of a (classification) method depends on the topology of the hierarchy and the category distributions X  [27]. This is to say, to resolve the classification problems with larg e-scale taxonomies, the topology and structure of taxonomies needs to be transformed. This is a as yet unexplored research area. The DDC, together with the bibliographic data, provides us a good testbed for such an exploration. In this section, we first analyze the widely-used hierarchical classification model and point out that it is impossible for this model to achieve high performan ce in large-scale taxonomies. Then we introduce the interactive classification model and discuss its requirements, based on which we devise a trimming machine to reconstruct the DDC hierarchy. Basically there are two ways to utilize taxonomies as reported in TC literature: flat classification and hierarchical classification. Flat classification treats each category individually and ignores the relationship among them. It us es an exhaustive search to classify a document into the category with the highest confidence score. The computational load will increase to an unacceptable level when handling a large number of categories. Hierarchical classification decomposes a classification task into tiers of smaller ones according to the taxonomy structure, thus reducing the computation cost greatly and making feasible the large classification problems. The pachinko machine is the most widely-used hierarchical classification model in literature [7, 12, 18, 24, 28]. It builds a classifier hierarchy that is isomorphic to the category hierarchy: during the training phase, a m-ary classifier is allocated to every internal node in the category hierarchy and trained to distinguish only among its children; during the test phase, a test instance walks top-down from the root, and the classifier at a node decides on whic h sub-tree (or leaf category) to traverse next. This process is known as a pachinko-machine search because a classifier is used only if the classifier at the parent category says YES on the test instance. We applied the pachinko machine model on the BDS&amp;T. Even with a truncated DDC scheme, the SVM, one of the most high-performing classifier, obtained an accuracy of only 0.5. The accuracy is even lower when applied on corpora classified by other large-scale taxonomies, like Yahoo! web directory or MeSH [18, 27]. Now we have witnessed the consequences caused by the problems discussed in Section 3:  X  Error propagation : Error propagation is a typical problem in hierarchical classification. If an upper classifier made a wrong decision for document d , then d would choose a wrong path to traverse the taxonomy and have no chance to find the correct category any more. This problem becomes very serious in a deep hierarchy like DDC. In the BDS&amp;T the average length of the assigned DDC numbers is 6.44, wh ich means a document needs to pass an average 6 classifiers to arrive at its category in the hierarchical classification. Even if each classifier achieved an accuracy of 0.9, the overall accuracy would not exceed 0.9 0.53.  X  Ineffective learning : The problems of data sparseness and skewed distribution induce ine ffective learning. Akbani has pointed out that when working w ith imbalanced data, SVMs will produce a less effective classification boundary, and for the extreme case SVMs may totally fail since there is no sufficient number of positive instances for st atistical learning [1]. Stacking the classifiers in a hierarchy isomorphic to the taxonomy structure does no help in solving these problems.  X  Difficulty in internal node categorization : A particular difficulty for the pachinko m achine is the internal node categorization. A test document always descends to one of the leaf categories if no extra efforts are made. To stop a test document from further traversing at an internal category, either local classifiers or threshold tuning is needed [24, 27]. These methods demand extra computation cost, even dominating the time complexity of offline training in a very deep hierarchy [18]. Confined by these intrinsic problems, the performance of full automatic classification on large-s cale taxonomies is very hard, if not impossible, to meet the requirements of real-world applications. To implement a practical library classification system, we propose to take tw o measures: to introduce human intelligence in the automatic classification process and to reconstruct the structure of taxonomies. In an interactive hierarchical classification system, a user is asked to select one category from m candidates returned by the classifiers at each level of the hierarchy, so as to improve the prediction accuracy and avoid error propagation. It works as follows: each time the system returns m candidate categories (i.e. m top categories predicated by the system), and from them the user chooses one; then the cla ssification process continues from the chosen node until the user decides on the right category or the bottom of the taxonomy is reached. There are two parameters in such a system: n , the times of user interaction; m , the number of candidate categories returned in each round of interaction. Obviously, n is decided by the depth of the category hierarchy and m is dependent on the effectiveness of the classifiers. In fact, the interactive classification model is also a generalization form for both the full automatic classification and the manual classification. When n =0 and m =1, i.e. each time the system returns just one category and no interaction is allowed, this is the case of full category hierarchy U ), and m is set to the child number of the current node in each round of the classification ( m = 10 in the DDC, for example), this is the case of manual classification. For a practical interactive classification system, m can be determined according to the effectiveness of the employed classifiers. Then the key concern is how to reduce the value of n , the required interaction times, to the minimal degree without impairing the overall classification effectiveness. That means  X  to reconstruct the taxonomy, and to compact its hi erarchy as much as possible. All the taxonomies used today are bui lt for manual classification. When man comes to classify documents with a taxonomy, he comes with a prior knowledge about the knowledge structure represented by the taxonomy and he follows an analytical approach to do the classification. On the contrary, when a machine comes to utilize a taxonomy, the fi rst step is to learn inductively the knowledge structure from training instances. Is it necessary for a machine to follow the same level-by-level route as man does? The pachinko machine model tries to imitate man X  X  analytical approach in hierarchical classi fication: it views a taxonomy as a vertical tree, stacks ties of classifiers into the same hierarchy as the taxonomy during the learning phase, and steps down the hierarchy ladders level by level to search the right category during the test phase. Might we rotate the taxonomy 90 degrees and view it as a horizontal one? As shown in Figure 4-a, a taxonomy U is laid down and partitioned into three areas, which is equal to transforming U into a virtual taxonomy V as shown in Figure 4-b. Now the classification can be conducted with a zoom in/out operation: zoom out to choose the most possible area, and then zoom in to predict the exact category. And, a user only needs to interact with the system two times in the interactive classification, first to select an area and secondl y to decide a category, no matter how deep the original taxonomy is. Such a transformation brings us a big benefit: the height of V is irrelevant to the depth of U , it is decided solely by how we partition U . For example, we may first partition U into several areas, and then each areas is further divided into several sub-areas. This will produce a virtual taxonomy of 4 levels. Furthermore, the problems of data sparseness and skewed distribution can be addressed by basing the partition on the document density and category dist ribution. For example, we can merge the sparse categories into their parent nodes, and chop off a populous subtree to form an independent area, and bind several thin subtrees together to form another area. Here comes the trimming machine , a novel algorithm that re-shapes a large taxonomy into an evenly-distributed virtual hierarchy. It works in the following way: The trimming machine first merges the sparse categories into their parent nodes to form dense categor ies with a sufficient number of instances, and then chops off th ick branches and binds together thin branches to balance the distribution. Thick branches are those subtree bearing many categories, thin branches are those ones bearing a few. After the whole category tree has been chopped into pieces, the resulting branches are re-bound virtually into a new tree. The key points of th e trimming operation are: 1) the branches, once chopped off, turn in to bags of leaves, their stems are drawn out to reduce the depth of hierarchy; 2) the knots, from which the previous branches ar e chopped off and to which the resulting bags are bound, become virtual nodes that represent prediction areas rather than real categories that hold documents. The formal description of the algorithm is as follows: Definition of Parameters 1) H m (threshold for merging categories): c is a spare category 2) H c (threshold for choppin g subtrees): Subtree T is a full tree Definition of Operations 1) MERGE: merges a child node into its parent. 2) FLATTEN: makes all the nodes in a subtree to be siblings 3) CHOP: chops the subtree T off from the tree. If the root of T Procedure 1) Pick up a 2-level subtree T from the bottom of the category 2) If any leaf of T is a sparse category then MERGE it into the 3) If T is a full tree after merging then CHOP it off; otherwise 4) Goto (1) until the top of U is reached. 5) Build a virtual tree U  X  by adding a virtual root, arranging all 6) The bag level of U  X  may be subject to the above process Please note that if the root t of subtree T is a sparse category, it is not chopped off together with T , but merged into the parent of t in the next round of merging-and-choppi ng. The rationale behind this is that a broader category would be preferable in the case that the exact category cannot be determined for a test instance. Figure 5 shows the working process of the trimming machine on an example category tree when H m =3 and H c =3. (1) is the initial state of the tree, an empty node stands for a sparse category and a solid node stands for a dense category. Being a sparse category, c 10 is merged into c 6 ; now c still being a sparse category, c 6 is further merged into c following step. C 8 , The subtree rooted by c 8 , is flattened and c is re-linked to c 4 . At (2) &amp; (3), both subtree C full trees and are chopped off, forming two bags B remaining categories form another bag B 1 after c 4 is merged into c . At (4), the bags are linked to a virtual root to form a 3-level virtual tree, which can be further trimmed to generate a 4-level hierarchy if needed. In the diagram, c 2  X  = { c { c 1 + c 4 }. The trimming machine brings us several benefits: 1) It converts a taxonomy into a virtual tree, and renders a 2) In the virtual tree, the internal nodes are bags, i.e. virtual 3) The pachinko machine takes a taxonomy as a vertical one 4) An interactive classification system with a trimmed In the following experiments, we apply the trimming machine to re-shape the DDC scheme into a balanced virtual taxonomy. The trimming machine runs twice on the DDC to produce a 4-level virtual hierarchy. In the interactive classification of a document, a user interacts with the system at most 3 times, each time selecting one category from 3 candidates, and is able to assign a DDC number of any length to the test doc ument. An accuracy of nearly 90% is obtained, a great improvement comparing to the 50% accuracy of the full automatic classification. We conduct extensive experiments in the DBS&amp;T, ten-year bibliographic collection of the Li brary of Congress in the science and technology domains, to veri fy the analysis and methods presented in previous sections. Bib liographic data is quite different from the full-text benchmark collect ions, so we conduct a series of preliminary experiments to determine the best settings for bibliographic classification. In addition, the performance evaluation of the hierarchical classification and interactive classification need special trea tments. After the introduction to these issues, we report the results of the DDC reconstruction, and compare the performance of the full automatic and interactive classification. The following experimental settings are determined after thorough analyses and a series of comparative experiments. Bibliographic data is the metada ta about information items (e.g. books, magazines, newspapers, etc.). It is in the standard format of the Machine reAdable Catalo ging Record (MARC). There are the fields of abstracts and table of contents defined in MARC standard. But statistics show that just 6% of the MARC records contain these full-text fields. Then the only usable content-related fields in a MARC record are titles and subject headings, including: title and title-related fields (245$a$b$p, 240$a, 246$a, 130$a, 730$a, 740$a), and subject-access fields (630$a$x, 650$a$x). Case conversion, stopword removal are conducted on the title text. Stemming is not taken (except plural form removal) because most titles are noun phrases instead of complete sentences. A title is segmented into meaningful phrases and words, phrases are extracted by looking up compound terms in the Relative Index of the DDC and the Library of Congress Subject Authority (LCSA). Main headings and subheadings are extracted from subject-access entries, and reserved in their original forms. Thus, a bibliographic record is represented by a term vector composed of title phrases/words and headings. The binary weighting method is used since terms rarely repeat themselves in a title or assigned subject headings. A widely accepted belief is th at using individual words to represent document is better than using phrases in classification experiments [14, 22]. This is not tr ue for bibliographic corpora. In our comparative experiment, the classifiers obtain a 4% increase (micro-avg F1) when the phrase vector is used. The hierarchical classification is conducted on the original DDC scheme as a baseline. The original DDC scheme has a large number of sparse categories. So me researchers simply removed sparse categories from their experiments since they are unable to present in the training set and test set simultaneously [18, 27]. This kind of treatment is unacceptable to bibliographic corpora since more than half of the categories and their documents would lose. We devise a method to truncate a DDC number according to the document quantity. Suppose  X  is the minimum number of positive instances for a successful training, a DDC number with &lt;  X  documents is truncated by one digit a time until it has  X  X  documents. The resulting DDC numbers are checked in the DDC schedules and the existing bibliogra phic corpora to make sure it is a valid one. Bibliographic corpora are very sparse, in the BDS&amp;T 80% of the categories have  X  3 documents. By a small  X  value the number of sparse categories has been reduced considerably. the bibliographic corpus too much, and big enough to have the corpus partitioned at a reasonable ratio, say, 3:1. The features that only appear in one document are removed from the feature space to reduce the computation cost. After the above process, a bibliographic document vector has an average of 6.3 terms. Any further feature selection will result in a significant number of null-vector documents, t hus impairing the recall of the classification. Two fundamentally different cla ssification algorithms, namely Na X ve Bayes (NB) and Support Vect or Machine (SVM), are used in the experiments to make sure that the effectiveness of the proposed methods is independent of specific TC techniques. Both of them are among the most popular and high-performing classifiers. Bibliographic classification is a multi-class task, i.e. there are multiple classes in classification tasks and each test document is assigned to just one class. The NB is a m-ary classifier and fits the task very well. But the SVM is a binary classifier, and one-against-one or one-against-all strategies need to be taken to solve multi-class classification tasks [2].We adopt the LIBSVM package because it provides an integrated multi-class implementation [11], saving us from reinventing the wheel. The kernel function RBF is used, and the optimal parameters are grid-searched with 5-fold cross-validation. Standard precision and recall assume the independence between categories and fail to capture the partially correct predictions in the hierarchical cl assification. For ex ample, suppose three classifiers A , B and C assign 634, 634.22 and 534 respectively to a document labeled with 634.227. Although B is more  X  X orrect X  than A in this case, both of them are evaluated to the same degree of error as C . To capture those partial successes, we adopt Ganesan X  X  formula that is orig inally used in measuring the similarity of two nodes c i , c j in a tree [9]: path from the root of U to c i , and the Lowest Common Ancestor both c i and c j . To apply this formula to compute the similarity of two DDC numbers, we treat every DDC main class as an independent tree. According to this formula, classifier C in the above example scores 0, totally wrong; but B scores 0.91. In the following experiments, this formula is used to compute the depth-based micro-averaging precision and recall. In the interactive DDC classification, there is a user who interacts with the system, and he selects his favorite category each time the system returns m candidates. To evaluate the classification performance, we suppose that the user always makes the  X  X ost X  correct choice, i.e. the longest match of the labeled DDC number. For instan ce, the user would choose 004.582 if the classifier suggests 004.5, 004.58 and 004.582 for a test document labeled with 004.5829. Based on this assumption, the existence of a user in the experiments can be mimicked as follows: Suppose d is a test document labeled with DDC number c . During the classification d traverses the category tree top-down from the root. At each node , the classifier returns m guesses, from which d selects the longest match, and then the classification continues till the leaf category is reached. If none of the guesses is the substring of c , d goes back to the parent node and takes it as the final assignment. After all the test documents have been classified, the assignments are evaluated as usual. All the experiments were conducted on the BDS&amp;T. After the global feature reduction, the vocabulary size was 31,288; and after the DDC truncation there were 4,861 categories left, organized in a 16-level hierarchy. After the feature reduction, some training documents lost all the feature terms and were excluded from the experiments. So the actual number of training documents was 66,220, and the number of test documents was 22,110. As explained in Section 5.1.2, the DDC scheme was truncated with  X  =4, and the dataset was split at a ratio of 3:1. After the DDC truncation, each category had at least 3 training instances. This is equal to the merging operation of the trimming machine with H m = 3. To make the classification results produced on the trimmed DDC scheme comparable to that produced on the truncated DDC scheme, we set the merging parameter H m to 3 and skipped the merging operation. We ran the trimming machine twice on the BDS&amp;T. The DDC has a ten-division structure. To avoid twisting the DDC structure too much, the chopping parameter H c should be around 10. We tested 3 groups of chopping parameters: (1 st trimming H c, 2 trimming H c) = (7, 5), (9, 7), (13, 11) respectively. Among them, (9, 7) won the best performance with a small margin, about 0.005 ~ 0.01. So we only present the results produced under that pair of values. Table 2 Comparison of the Or iginal / truncated / Trimmed Original DDC 18,462 4.8 23 1.5 Truncated DDC 4,861 18.2 16 1.8 Trimmed DDC 4,861 18.2 4 12.2 Table 2 summarizes the effects of the taxonomy reconstruction. The twice trimmings gene rated a virtual tree that has 30 nodes in the 2nd level, 401 in the 3rd and 4,861 in the 4th level. A large number of sparse categories (with &lt; 3 instances) are merged into their ancestor nodes, and the average document number per category has risen from 4.8 to 18.2. The mean value of the tree fanout (i.e. the average child number of all the categories in the taxonomy) of the original DDC hierar chy is just 1.5, a surprising value in regard of the ten-division logical structure of the DDC. This reflects that the DDC is an obliquely-growing tree with a large number of deep categories. After the trimming, the fanout mean increases to 12.2, a more reasonable value resulted from flattening the DDC hierarchy. Figure 6 illustrates how the trimming machine transforms the category distribution in the BDS&amp;T. The x-axis is the category rank ordered by the number of documents, and each point in the curve tells how many documents there are in the subtree rooted by the category in question. In Figure 6 the upper curve is the distribution of the DDC hundred divisions in the BDS&amp;T (hundred divisions are the 2-digit main classes in DDC), and the lower curve is the distribution of the virtual categories resulted from the second trimming, most of them are 2-digit long. The curve becomes smoother after the trimming, illustrating a more balanced distribution produced. Additional diagrams illustrating http://KVision.pku.edu.cn/~trimmedDDC ) for more reference. We conducted three groups of e xperiments and compared their performances: the hierarchical classification on the truncated DDC scheme (referred to as HC), whic h is taken as a baseline; the hierarchical classification on the trimmed DDC scheme (MC), which is used to observe the impacts of the taxonomy reconstruction on the classification performance; and the interactive classification (IC) on the trimmed DDC scheme, the practical solution to automatic library classification. Both the NB and SVM are used to implement these classification models, all the results are listed in Table 3. To simplify the discussion, we will mainly talk about the SVM X  X  results and leave the NB X  X  results for additional reference. On the truncated DDC scheme, a 16-level taxonomy consisting of 4,861 DDC classes, the training process generated 7,291 SVM classifiers, and the test process obtained a micro-averaged score of 0.505. Although this score is not impressive compared to that obtained from experiments on the be nchmark collections classified with small category systems (usual ly above 0.8), it is not bad comparing to that obtained from other large-scale classification experiments [8, 27]. The macro-averaged score is lower because most of the DDC classes dwell on deep levels and suffer from the problems of error propagation and insufficient training instances. Some readers may wonder why the values of the micro-averaged precision and recall are same. The bibliographic dataset is a single-labeled corpus, i.e. each document is labeled with one and only one class. If every test document is assigned to a class during the classification, the micro-averaged values of the precision and recall are same. But if the system failed to assign any class to certain test documents, this would cause difference in the computation of the micro-averaged precision and recall. This is what happened in the case of the interactive classification. The depth-based micro-averaged scores are 0.754. This value is close to the ones gained from experiments on benchmark collections. The depth-based scor es can be roughly understood as a measure of the classification performance in terms of digit correctness, i.e. how many DDC digits have been correctly predicted. For example, 0.7 approximately means that the classifier can predict preceding 7 digits correctly for a 10-digit DDC number. This reminds us th ere are many partially correct numbers among those error predictions counted by the standard precision and recall. In this sense, the function of the trimming machine is to help us find the range of the partially correct digits in a predicted DDC number, and the interactive classification provides us a chance to judge how well it performs. On the trimmed DDC scheme, a 4-level taxonomy consisting of 4,861 DDC classes, the traini ng process generated 432 SVM classifiers, a great reduction in classifier quantity. Although the classification scheme had undergone dramatic rec onstruction, the SVM gained a micro-averaged F1 va lue of 0.493, almost the same as that gained on the truncated DDC scheme. The NB gained a F1 value of 0.438, even better than it did on the truncated scheme. This verifies our analys is given at Section 4.3 follow a totally different route from what a person does while traversing a taxonomy. The reconstruction of the DDC sche me brings categories lying in different levels of a hierarchy into one same level and mixes terms of different specificity into one feature space, to which the SVM is more sensitive for it depends on the support vectors to discriminate positive instances from negative ones. This is probably the reason why the effectiveness of SVM doesn X  X  improve after the taxonomy reconstruction. If the datasets were fulltext, one might take additional measures, such as local feature selection and parameter tuning, to further improve the performance of SVMs. The impact of the taxonomy recons truction on the classification performance can be observed more clearly in figure 7, which illustrates the SVM performance at each level of the trimmed DDC scheme (the 1 st level is the virtual root). Both of them are quite impressive, the high score at the initial level (0.804), and the sharp dropping of the performance with the deepening of the hierarchy (-0.15 per level). If human intelligence was introduced at this point, the prediction accuracy would be saved from falling. Here comes the interactive classification. HC MC *SVM (T): SVM results solely based on titles. performance at each level An interactive classification system on the trimmed DDC scheme requires a user to interact with the system at most 3 times to classify a document to a DDC number of any length, each time the system suggests 3 candidate cate gories from which the use select one. This is not a big deal, especially comparing to the tradition scene where a cataloger looks for an appropriate class in the printed DDC volumes packed with tens of thousands DDC numbers. The interactive classification comes to a complete success. Its performance is evaluated by the method introduced in Section 5.2.2. The NB gained a micro-averaged F1 value of 0.799, and the SVM gained 0.832, both improved by 68% over their full automatic counterparts respectively. In terms of the depth-based measurement, the perfo rmance is even amaz ing, around 0.95. This great improvement is illustrated in Figure 8 that compares the micro-averaged F1values of HC-SVM, MC-SVM and IC-SVM. Another interesting observation is that the performances of SVM and NB have been drawn very closer, just a difference of 0.03. This is to say, different classi fication algorithms do not make a big difference in the interactive classification model. In addition, we tested the performance of the interactive classification in the case that a document is solely represented by its title. Even with such a piece of insufficient and ambig uous information, the SVM can still gain an acceptable precision, up to 0.78. This proves that the methodology presented in this pape r is a practical solution to the real-world library classification problems. We have developed an online system of the Interactive Dewey Decimal Classification that classifies bibl iographic data in the domains of library, information and computer sciences. It is published on the web for open access and evaluation. (http://KVision.pku.edu.cn/auto-DDC ). To challenge the very large-scale classification problems of real-world, library classification systems, like DDC, provide us a good testbed. They are the largest and most well-developed classification systems in use, and over which have accumulated huge amount of bibliographic resources. The widely-used hierarchical classification model, i.e. pachinko-machine, imitates human classification behaviors of traversing a taxonomy top-down in an analytical manner, which is not a good choice for the supervised learning algorithms, an inductive process in nature. In this paper, the DDC is reshaped into a balanced virtual taxonomy according to th e document density and category distribution. The experiments verify that such a dramatic reconstruction doesn X  X  hurt the cl assification performance. And, with an interactive classification method, the classification accuracy can be improved to nearly 90%, reaching the level of practical application. These fact s reflect that human constructed taxonomies are more a convention for the arrangement of relevant categories rather than a hierarchy following strict logic. We will conduct more theoretical analysis and empirical studies to further investigate this issue. One limitation of this study is that bibliographic data provides only titles and subject he adings for automatic classification. But it is probably one of the best choi ces for a research on very large-scale taxonomies. Most of the categorization schemes for Web resources are  X  X acking the rigorous hierarchical structure and careful conceptual structure found in established schemes X  like the LCC and DDC [4], and other cat egorization schemes used in previous studies, like MeSH and LCSH, are subject heading lists or thesauri rather than classification schemes in a strict sense. We believe that library reso urces can contribute more to knowledge management communities. For example, the Library of Congress has initiated the Bibliographic Enrichment Project that aims at enriching online catalog records by providing more content-related information, such as tables of contents and abstracts. This will enable us to exploit this valuable information legacy for organizing non-library ma terials, especially web pages, and bring about further progress in automatic classification technologies. The work presented in this paper was supported by the OCLC/ALISE Library and Information Science Research Grant and the Nature Science Foundation of China (No. 70303002). Thank the anonymous reviewers for their detailed revision. [1] Akbani, R., Kwek, S., &amp; Ja pkowicz, N. (2004). Applying [2] Bredensteiner, E. J., &amp; Bennett, K. P. (1999). Multicategory [3] Chan, L. M., Comaromi, J. P., Mitchell, J. S., &amp; Satija, M. P. [4] Chan, L.M. (2001). Exploiting LCSH, LCC, and DDC to [5] Cai, L. &amp; Hofmann, T. (2004 ). Hierarchical Document [6] Dolin, R. A. (1998). Pharos: A scalable distributed [7] Dumais, S., &amp; Chen, H. (2000). Hi erarchical classification of [8] Frank, E., &amp; Paynter, G. W. (2004). Predicting Library of [9] Ganesan, P., Garcia-Molina, H., &amp; Widom, J. (2003). [10] Gorman, M., &amp; Winkler, P. W. (1998). Anglo-American [11] Hsu, C.-W., Chang, C.-C., &amp; Lin, C.-J. (n.d.). A Practical [12] Koller, D., &amp; Sahami, M. (1997) . Hierarchically classifying [13] Larson, R. R. (1992). Experiments in automatic Library of [14] Lewis, D. D. (1992). An evaluation of phrasal and clustered [15] Library of Congress. (1990). LC Classification Outline (6th [16] Library of Congress acquisitions and bibliographic access [17] Library of Congress (1999). MARC 21 format for [18] Liu, T.-Y., Yang, Y., Wan, H., Zeng, H.-J., Chen, Z., &amp; Ma, [19] Mitchell, J. S. (1997). Abridged Dewey Decimal [20] OCLC. Dewey is the world's most widely used library [21] Ruiz, M. E., &amp; Srinivasan, P. (2002). Hierarchical text [22] Schutze, H., Hull, D. A., &amp; Pedersen, J. O. (1995). A [23] Sebastiani, F. (2002). Machine learning in automated text [24] Sun, A., &amp; Lim, E. (2001). Hier archical Text classification [25] Thompson, R., Shafer, K., &amp; Vizine-Goetz, D. (1997). [26] Weigend, A. S., Wiener, E. D., &amp; Pedersen, J. O. (1999). [27] Yang, Y., Zhang, J., &amp; Kisiel, B. (2003). A scalability analysis 
