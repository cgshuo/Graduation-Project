 Information Retrieval (IR) research has traditionally focused on serving the best results for a single query X  so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions. This paper describes the TREC Session Track, which ran from 2010 through to 2014, which focussed on forming test collections that included various forms of implicit feedback. We describe the test collections; a brief analysis of the differences between datasets over the years; and the evaluation results that demonstrate that the use of user session data significantly improved effectiveness.
One of the commonest IR system evaluation methodolo-gies is the Cranfield approach [4] using test collections to conduct controlled, systematic, and repeatable evaluations [7]. The focus of such evaluation is on how well an IR system can locate and rank relevant documents from a single query. In practice, however, users typically reformulate queries in response to search results or as their information need alters over time [9]. Retrieval evaluation should compute system success over multiple query-response interactions [10].
The TREC Session Track 1 was an attempt to evaluate IR systems over multi-query sessions. In 2010, the track pro-duced test collections and evaluation measures for studying retrieval over sessions [11]; from 2011 on [12, 13, 1, 2], the track focused more on providing participants with user data with which to improve retrieval. The resulting collections consist of document collections, topics, and relevance as-sessments, as well as log data from user sessions. http://ir.cis.udel.edu/sessions/
The track X  X  test collections are described here and com-pared: including studying the effects of the search engines used to build the collections, user variability, and topic anal-ysis. Participant results indicate that certain types of search benefit significantly from exploiting session information.
The aim of the track was to test if the retrieval effective-ness of a query could be improved by using previous queries, ranked results, and user interactions. We constructed four test collections comprising N sessions of varying length, each the result of a user attempting to satisfy one of T pre-defined topics. Each session numbered 1 ..i..N consisted of: Ranking algorithms were evaluated on the current query un-der two conditions: A one-off ad hoc query; or a query using some or all of the prior logged data. The latter condition had several different sub-conditions that varied year to year: ( X  X L X  refers to Ranked List): The focus of the track was on the degree to which a group im-proved their retrieval system X  X  baseline effectiveness (RL1) by incorporating some or all of the additional log data. Table 1 shows statistics of the Session track collections. The ClueWeb09 collection was used in 2011 and 2012, and the ClueWeb12 collection in 2013 and 2014.

Topics: While not a part of a true log of user search activity, we felt it was important to define topic descrip-tions for overall sessions so as to make relevance assessing simpler. The challenge was to construct topics that were likely to require multiple query reformulations. In 2011, we did this by adapting multi-faceted TREC 2007 Question An-swering track topics. Because of the nature of the QA track, many topics modelled  X  X act-finding X  tasks answerable by a single document. In 2012-2013, we developed topics accord-ing to a task categorization scheme [15] with four classes: known-item ; known-subject ; interpretive ; and exploratory . In 2014, we reused topics from 2012-2013 selecting fifteen top-ics from the four categories, biasing selection to topics that had longer user sessions and more clicks.

Sessions: Assessing the impact of session data on re-trieval effectiveness required capturing user-system interac-tions, including queries, rankings, and clicks. We describe the users and search engines employed to generate the data.
Users: In 2011-2013, the primary user group were staff and students at the University of Sheffield. Using a university-wide email, we invited participants to search on as many topics as they had time for. In 2013 we solicited additional participants from the Session Track and SIG-IRList mail-ing lists. In 2014 we used a crowdsourcing platform (Me-chanical Turk) taking a similar approach to past work for crowdsourcing interactions [18].

Search process: Users were shown a topic description, a search box for entering queries, and a list of ten ranked re-sults with a pagination control to navigate to further results. Each retrieved item was represented by its title, URL, and snippet. Additionally, there was a  X  X ave X  button that users were instructed to use to collect those documents that helped them satisfy their information need. We experimented with additional components, such as a list of queries issued, but did not observe a difference in users X  behaviour.

Search engine: In 2011-2012 we used Yahoo! X  X  BOSS (Build your Own Search System) API to search the live web. We fil-tered URLs returned by BOSS against those in the ClueWeb09 collection so that users would only see pages that were present in the publicly-available corpus. A large number of pages re-turned by BOSS did not match any URL in ClueWeb09. In 2013-2014, we switched to indri search with a home-built index of ClueWeb12. The indri index included each of the twenty ClueWeb12 segments (ClueWeb12 00 through ClueWeb12 19) indexed using the Krovetz stemmer and no stopword list. The indexes searched contained only text from title fields, anchor text from incoming links ( X  X nlink X  text), and page URLs. Each query was incorporated into an indri structured query language template and a retrieval score was computed from a query-likelihood model for the full document representation and three weighted combina-tions of query-likelihood field models with unordered-window within-field models. The  X  X nlink X  model was weighted 50 times higher than the title model, and 100 times higher than the URL model. This query template is the product of man-ual search and investigation of retrieved results.

The system logged all interactions with the user, including the queries issued, which documents were ranked (including URL, title, and snippet), which documents the user viewed, and which they saved as relevant to the task (note however that the latter are not the relevance judgments). This log data was then used to create the sessions.
We used topical relevance judgments in order to com-pute measures of effectiveness like nDCG@10 for each topic. Since the Session Track examines whether session log data can be exploited, the evaluation examined the change in effectiveness from the baseline (RL1) to using some data (RL2) to using a full query log (RL3). In addition, since each topic may be the subject of more than one session, and Figure 1: Mean nDCG@10 (with error bars showing  X  2 standard error) for all 108 submitted runs X  RL1 baseline. each session may use different queries, the evaluation was over sessions rather than over topics.

Documents were selected for judging by pooling the top-10 results from all the submitted RLs along with all documents that were retrieved and viewed by the users. TREC NIST assessors ( not the original users) judged each pooled docu-ment with respect to the topic description. All original user actions were invisible to the assessors; judgments were made solely on the topical similarity to the topic description on a 6-grade scale. Over four years, 66,548 relevance judgments were made to 60,500 unique pages identified by URL: 33,686 pages from ClueWeb09 ; 26,814 from ClueWeb12. A total of 19,179 (29%) documents were judged relevant (grade 1 or higher) and 47,369 (71%) judged nonrelevant.

Since the topics for 2014 were taken from the 2012 &amp; 2013 Session Tracks and in the last two years the docu-ment collection was ClueWeb12, we have documents with multiple assessments. Table 2 shows assessor agreement. Assessors were much more likely to say a document judged non-relevant in 2013 was relevant in 2014 than vice versa.
Results: Figure 1 shows nDCG@10 for all groups X  base-line RL1 submissions, sorted by nDCG@10 and coded by year. It is evident that 2011 had the best baseline effective-ness (average nDCG@10 of 0.30), followed by 2012 (0.18), then 2014 (0.17), and finally 2013 (0.14) had the lowest base-line effectiveness. The change from 2011 to 2012 reflects a shift to more difficult topics: the 2012 known-subject and interpretive topic categories proved to be significantly more difficult than the 2011 known-item topics. The change from 2012 to 2013 reflects a change in the underlying search tech-nology from Yahoo! BOSS to the Indri-based system. Figure 2 shows the improvement over each submitted run X  X  RL1 baseline sorted by that improvement. Improvement from the RL1 baseline does not show any trend by year X  for 2011, the average improvement was 0.04, for 2012 it was 0.05, for 2013 it was 0.05, and for 2014 it dropped to 0.02. Figure 2: Largest measured improvement in nDCG@10 from RL1 to any other condition for all 108 submitted runs, with error bars showing  X  2 standard errors.

From these results, we conclude that it is possible to use session history to improve effectiveness over basic ad hoc re-trieval, and moreover that it does not take a lot of session history to do so. Further evidence is offered in [6, 5, 17]. A study of particular interest due to the fact that it was con-ducted both over a Session track collection and a commer-cial search engine proprietary collection is that by Raman et al. [16]; the session collection enabled them to demon-strate the effectiveness of their algorithm in accordance to the proprietary test collection.
In this section we perform some basic analysis of the Ses-sion Track collections and evaluation results.

Topic categories: We investigated the degree to which systems were able to improve effectiveness for each of our four topic classes. We look at the average improvement from the RL1 baseline, and find the maximum average improve-ment to any other RL condition for each run.

The overall mean improvements are 0.04, 0.07, 0.04, and 0.05 for known-item, known-subject, exploratory, and in-terpretive respectively, though only the differences between known-subject and the others were statistically significant.This suggests that known-subject topics benefit most from ac-cess to session history, but the details are more subtle. Ex-ploratory topics tend to have the largest improvements for individual systems: the five largest improvements in ex-ploratory topics are 5 X 10% larger than the five largest in known-subject topics. Exploratory topics also show the great-est benefit from the use of more log data: from RL1 to RL2, exploratory topics only increase an average of 0.03 (compare to 0.05 for known-subject topics, the largest improvement), but from RL2 to RL3 they increase by 0.05 (compare to 0.04 for known-subject topics, the second-largest improvement).
Topic variability: Most IR test collections have only one instantiation of a topic (an exception is the TREC Query track). Since we may have multiple sessions for any given topic, the Session Track gives us a chance to analyze vari-ability in effectiveness within topics.

Figure 3 shows how much effectiveness varies over the dif-ferent user sessions of a single topic. Each plot on the x-axis is a topic, the y-axis is a boxplot of the range in nDCG@10 changes from RL1 to any other RL. A taller box means more variability. A point or box plotted further up the y-axis indi-cates higher average change in nDCG@10 across the sessions Figure 3: Variability over sessions and system effectiveness for selected topics. of a topic. An extreme case is topic 24 from 2012 (the right most topic on the plot). There were five sessions recorded for this topic (numbers 48 X 52); one group improved from 0.00 in RL1 to 0.91 in RL3 on one session, but fell from 0.81 in RL1 to 0.00 in RL2 on another. Many other groups had similarly large differences across sessions on this topic.
The result indiactes that there is a substantial variability in topics, separate from the variability in system effective-ness, due to the way the users performs their search and formulates their query. Previous user study showed this as well [14]. It may be beneficial to include multiple versions of the same topic in standard test collections, so as to better capture interactions between topic and system variability.
This paper describes the four test collections produced for the TREC Session Track that have been used to assess the use of implicit feedback on retrieval performance within sessions. The key result from the track is that aggregate data from all participant submissions shows that retrieval effectiveness was improved for ad hoc retrieval using data based on session history data. It also appears that the more detailed the session data, the greater the improvement.
Through analyzing aspects of the test collections, such as topic categories and variability, we demonstrate how the re-sources can be used to investigate implicit feedback and offer reusable and publicly-accessible resources for evaluating IR systems across sessions.
This work was supported in part by the Australian Re-search Council X  X  Discovery Projects scheme (DP130104007), the National Science Foundation (NSF) under grant num-ber IIS-1350799, and the Google Faculty Research Award scheme. Any opinions, findings and conclusions or recom-mendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors. [1] B. Carterette, E. Kanoulas, A. Bah, M. Hall, and [2] B. Carterette, E. Kanoulas, A. Bah, M. Hall, and P. D. [3] B. Carterette, E. Kanoulas, P. D. Clough, and [4] C. W. Cleverdon. The significance of the cranfield [5] D. Guan and H. Yang. Is the first query the most [6] D. Guan, S. Zhang, and H. Yang. Utilizing query [7] D. Harman. Information Retrieval Evaluation .
 [8] P. Ingwersen and K. J  X  arvelin. The Turn: Integration [9] B. J. Jansen, D. L. Booth, and A. Spink. Patterns of [10] K. J  X  arvelin. Explaining user performance in [11] E. Kanoulas, B. Carterette, P. Clough, and [12] E. Kanoulas, B. Carterette, M. Hall, P. D. Clough, [13] E. Kanoulas, B. Carterette, M. Hall, P. D. Clough, [14] K. S. Kim. Information-seeking on the web: Effects of [15] Y. Li and N. J. Belkin. A faceted approach to [16] K. Raman, P. N. Bennett, and K. Collins-Thompson. [17] S. Zhang, D. Guan, and H. Yang. Query change as [18] G. Zuccon, T. Leelanupab, S. Whiting, E. Yilmaz,
