 The rapid and steady progress in corpus-based ma-chine translation (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora such as the Arabic-English and Chinese-English paral-lel corpora distributed by the Linguistic Data Con-sortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. How-ever, large parallel corpora do not exist for many language pairs. For example, there are no pub-licly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora.

Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora.

Researchers can also make the best use of exist-ing (small) parallel corpora. For example, Nie X en and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unknown source language phrases to improve coverage and translation quality.

In this paper, we focus on situations where no par-allel corpus is available (except a few hundred paral-lel sentences for tuning parameters). To tackle these extremely scarce training data situations, we pro-pose using a pivot language (English) to bridge the source and target languages in translation. We first translate source language sentences or phrases into English and then translate those English sentences or phrases into the target language, as described in Section 3. We thus assume that there is a parallel corpus consisting of the source language and En-glish as well as one consisting of English and the tar-get language. Selecting English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language.

In Section 2, we describe a phrase-based statisti-cal machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stol-cke, 2002), GIZA++ (Och and Ney, 2003), mkcls code. We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely avail-able software, as described in the introduction. The system segments the source sentence into so-called phrases (a number of sequences of consecu-tive words). Each phrase is translated into a target language phrase. Phrases may be reordered.

Let f be a source sentence (e.g, French) and e be a target sentence (e.g., English), the SMT system out-puts an  X  e that satisfies where h m ( e , f ) is a feature function and  X  m is a weight. The system uses a total of eight feature functions: a trigram language model probability of the target language, two phrase translation probabil-ities (both directions), two lexical translation prob-abilities (both directions), a word penalty, a phrase penalty, and a linear reordering penalty. For details on these feature functions, please refer to (Koehn et al., 2003; Koehn, 2004; Koehn et al., 2005). To set the weights,  X  m , we carried out minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the objective function. We use the phrase-based SMT system described in the previous section to develop pivot methods. We use English e as the pivot language. We use French f and German g as examples of the source and target languages in this section.

We describe two types of pivot strategies, namely phrase translation and sentence translation .
The phrase translation strategy means that we di-rectly construct a French-German phrase translation table (phrase-table for short) from a French-English phrase-table and an English-German phrase-table. We assume that these French-English and English-German tables are built using the phrase model train-ing code in the baseline system described in the introduction. That is, phrases are heuristically ex-tracted from word-level alignments produced by do-ing GIZA++ training on the corresponding parallel corpora (Koehn et al., 2003).

The sentence translation strategy means that we first translate a French sentence into n English sen-tences and translate these n sentences into German separately. Then, we select the highest scoring sen-tence from the German sentences. 3.1 Phrase translation strategy The phrase translation strategy is based on the fact that the phrase-based SMT system needs a phrase-table and a language model for translation. Usually, we have the language model of a target language. Consequently, we only need to construct a phrase-table to train the phrase-based SMT system.

We assume that we have a French-English phrase-table T FE and an English-German phrase-table T EG . From these tables, we construct a French-German phrase-table T FG , which requires estimat-ing four feature functions; phrase translation prob-abilities for both directions,  X  (  X  f |  X  g ) and  X  (  X  g | lexical translation probabilities for both directions, p German phrases that are parts of phrase translation
We estimate these probabilities using the proba-bilities available in T FE and T EG as follows. 3 where  X  e  X  T FE  X  T EG means that the English phrase  X  e is included in both T FE and T EG as part of phrase translation pairs.  X  (  X  f |  X  e ) and  X  (  X  e |  X  f ) translation probabilities for T FE and  X  (  X  e |  X  g )  X  (  X  g |  X  e ) are those for T EG . p w (  X  f |  X  e ) lexical translation probabilities for T FE and p w (  X  e |  X  g ) and p w (  X  g |  X  e ) are those for T EG .

The definitions of the phrase and lexical transla-tion probabilities are as follows (Koehn et al., 2003). where count (  X  f,  X  e ) gives the total number of times the phrase  X  f is aligned with the phrase  X  e in the par-allel corpus. Eq. 7 means that  X  (  X  f |  X  e ) is calculated using maximum likelihood estimation.

The definition of the lexical translation probabil-ity is
E w ( f i |  X  e, a ) = where count ( f, e ) gives the total number of times the word f is aligned with the word e in the par-allel corpus. Thus, w ( f | e ) is the maximum likeli-hood estimation of the word translation probability of f given e . Ew ( f i |  X  e, a ) is calculated from a word alignment a between a phrase pair  X  f = f 1 f 2 . . . f n and  X  e = e 1 e 2 . . . e m where f i is connected to several ( is the average (or mixture) of w ( f i | e j ) . This means that Ew ( f i |  X  e, a ) is an estimation of the probabil-the probability of  X  f given  X  e and a using the prod-uct of the probabilities Ew ( f i |  X  e, a ) . This assumes that the probability of f i is independent given  X  e and a . p w (  X  f |  X  e ) takes the highest p w (  X  f |  X  e, a ) are multiple alignments a . This discussion, which is partly based on Section 4.1.2 of (Och and Ney, 2004), means that the lexical translation probability p word translation probability w ( f | e ) .
 The justification of Eqs. 3 X 6 is straightforward. From the discussion above, we know that the prob-p w (  X  e | the ordinary sense. Thus, we can derive  X  (  X  f |  X  g )  X  (  X  g |  X  f ) , p w (  X  f |  X  g ) , and p w (  X  g | these probabilities are independent given an English phrase  X  e (e.g.,  X  (  X  f |  X  g,  X  e ) =  X  (  X  f |  X  e ) We construct a T FG that consists of all French-German phrases whose phrase and lexical transla-tion probabilities as defined in Eqs. 3 X 6 are greater than 0. We use the term PhraseTrans to denote SMT systems that use the phrase translation strategy de-scribed above. 3.2 Sentence translation strategy The sentence translation strategy uses two inde-pendently trained SMT systems. We first trans-late a French sentence f into n English sentences e , e 2 , ..., e n using a French-English SMT system. Each e i ( i = 1 . . . n ) has the eight scores calcu-lated from the eight feature functions described in Section 2. We denote these scores h e Second, we translate each e i into n German sen-tences g i 1 , g i 2 , . . . , g in using an English-German SMT system. Each g ij ( j = 1 . . . n ) has the eight scores, which are denoted as h g This situation is depicted as We define the score of g ij , S ( g ij ) , as where  X  e 2. We select the highest scoring German sentence as the translation of the French sentence f .
A drawback of this strategy is that translation speed is about O ( n ) times slower than those of the component SMT systems. This is because we have to run the English-German SMT system n times for a French sentence. Consequently, we cannot set n very high. When we used n = 15 in the experi-ments described in Section 4, it took more than two days to translate 3064 test sentences on a 3.06GHz LINUX machine.

Note that when n = 1 , the above strategy pro-duces the same translation with the simple sequen-tial method that we first translate a French sentence into an English sentence and then translate that sen-tence into a German sentence.

We use the terms SntTrans15 and SntTrans1 to de-note SMT systems that use the sentence translation strategy with n = 15 and n = 1 , respectively. We conducted controlled experiments using the Europarl corpus. For each language pair de-scribed below, the Europarl corpus provides three types of parallel corpora; the source language X  English, English X  X he target language, and the source language X  X he target language. This means that we can directly train an SMT system using the source and target language parallel corpus as well as pivot SMT systems using English as the pivot language. We use the term Direct to denote directly trained SMT systems. For each language pair, we com-pare four SMT systems; Direct , PhraseTrans , Snt-Trans15 , and SntTrans1 . 5 4.1 Training, tuning and testing SMT systems We used the training data for the shared task of the SMT workshop (Koehn and Monz, 2006) to train our SMT systems. It consists of three paral-lel corpora: French-English, Spanish-English, and German-English.

We used these three corpora to extract a set of sentences that were aligned to each other across all four languages. For that purpose, we used English as the pivot. For each distinct English sentence, we extracted the corresponding French, Spanish, and German sentences. When an English sentence oc-curred multiple times, we extracted the most fre-quent translation. For example, because  X  X esump-tion of the session X  was translated into  X  X iederauf-nahme der Sitzungsperiode X  120 times and  X  X ieder-aufnahme der Sitzung X  once, we extracted  X  X ieder-aufnahme der Sitzungsperiode X  as its translation. Consequently, we extracted 585,830 sentences for each language. From these corpora, we constructed the training parallel corpora for all language pairs.
We followed the instruction of the shared task used the trigram language models provided with the shared task. We did minimum error rate training on the first 500 sentences in the shared task develop-ment data to tune our SMT systems and used the 3064 test sentences for each language as our test set.
Our evaluation metric was %BLEU scores, as cal-culated by the script provided along with the shared test sentences. 4.2 Results Table 1 compares the BLEU scores of the four SMT systems; Direct , PhraseTrans , SntTrans15 , and Snt-Trans1 for each language pair. The columns SE and ET list the BLEU scores of the Direct SMT sys-tems trained on the source language X  X nglish and English X  X he target language parallel corpora. The numbers in the parentheses are the relative scores of the pivot SMT systems, which were obtained by dividing their BLEU scores by that of the cor-responding Direct system. For example, for the Spanish X  X rench language pair, the BLEU score of the Direct SMT system was 35.78, that of the PhraseTrans SMT system was 32.90, and the rela-tive performance was 0 . 92 = (32 . 90 / 35 . 78) . For the SntTrans15 SMT system, the BLEU score was 29.49 and the relative performance was 0 . 82 = (29 . 49 / 35 . 78) .

The BLEU scores of the Direct SMT systems were higher than those of the PhraseTrans SMT sys-tems for all six source-target language pairs. The PhraseTrans SMT systems performed better than the SntTrans15 SMT systems for all pairs. The SntTrans15 SMT systems were better than the Snt-Trans1 SMT systems for four pairs. According to the sign test, under the null hypothesis that the BLEU scores of two systems are equivalent, finding one system obtaining better BLEU scores on all six language pairs is statistically significant at the 5 % level. Obtaining four better scores is not statistically significant. Thus, Table 1 indicates
Direct &gt; PhraseTrans &gt; SntTrans15  X  SntTrans1 where  X  &gt;  X  and  X   X   X  means that the differences of the BLEU scores of the corresponding SMT systems are statistically significant and insignificant, respec-tively.
As expected, the Direct SMT systems outper-formed the other systems. We regard the BLEU scores of the Direct systems as the upperbound. The SntTrans15 SMT systems did not significantly out-perform the SntTrans1 SMT systems. We think that this is because n = 15 was not large enough to cover scoring translation from a small pool did not always lead to better performance. To improve the perfor-mance of the sentence translation strategy, we need to use a large n . However, this is not practical be-cause of the slow translation speed, as discussed in Section 3.2.

The PhraseTrans SMT systems significantly out-performed the SntTrans15 and SntTrans1 systems. That is, the phrase translation strategy is better than the sentence translation strategy. Since the phrase-tables constructed using the phrase transla-tion strategy can be integrated into the Pharaoh de-coder as well as the directly extracted phrase-tables, the PhraseTrans SMT systems can fully exploit the power of the decoder. This led to better performance even when the induced phrase-tables were noisy, as described below.
 The relative performance of the PhraseTrans SMT systems compared to the Direct SMT systems was 0.92 to 0.97. These are very promising re-sults. To show how these systems translated the test sentences, we translated some outputs of the Spanish-French Direct and PhraseTrans SMT sys-tems into English using the French-English Direct system. These are shown in Table 3 with the refer-ence English sentences.

The relative performance seems to be related to the BLEU scores for the Direct SMT systems. It was relatively high (0.95 to 0.97) for the difficult (in terms of BLEU) language pairs but relatively low (0.92) for the easy language pairs; Spanish X  X rench and French X  X panish. There is a lot of room for improvement for the relatively easy language pairs. This relationship is stronger than the relationship be-tween the BLEU scores for SE/ET and those for the PhraseTrans systems, where no clear trend exists.
Table 2 shows the number of phrases stored in the phrase-tables. The Direct SMT systems had 7.3 to 18.2 million phrases, and the PhraseTrans systems had 168.2 to 190.8 million phrases. The numbers of phrases stored in the PhraseTrans systems were very large compared to those of Direct systems. 9 How-ever, this does not cause a computational problem in decoding because those phrases that do not appear in source sentences are filtered so that only the relevant phrases are used during decoding.

The figures in the common column are the number of phrases common to the Direct and PhraseTrans systems. R (recall) and P (precision) are defined as follows.
 Recall was reasonably high. However, the upper bound of recall was 100 percent because we used a multilingual corpus whose sentences were aligned to each other across all four languages, as described in Section 4.1. Thus, there is a lot of room for im-provement with respect to recall. Precision, on the other hand, was very low. However, translation per-formance was not significantly affected by this low precision, as is shown in Table 1. This indicates that recall is more important than precision in building phrase-tables. Pivot languages have been used in rule-based ma-chine translation systems. Boitet (1988) discusses the pros and cons of the pivot approaches in multi-lingual machine translation. Schubert (1988) argues that a pivot language needs to be a natural language, due to the inherent lack of expressiveness of artifi-cial languages.

Pivot-based methods have also been used in other related areas, such as translation lexicon induc-tion (Schafer and Yarowsky, 2002), word alignment (Wang et al., 2006), and cross language information retrieval (Gollins and Sanderson, 2001). The trans-lation disambiguation techniques used in these stud-ies could be used for improving the quality of phrase translation tables.

In contrast to these, very little work has been done on pivot-based methods for SMT. Kauers et al. (2002) used an artificial interlingua for spoken language translation. Gispert and Mari  X  no (2006) created an English-Catalan parallel corpus by auto-matically translating the Spanish part of an English-Spanish parallel corpus into Catalan with a Spanish-Catalan SMT system. They then directly trained an SMT system on the English-Catalan corpus. They showed that this direct training method is superior to the sentence translation strategy ( SntTrans1 ) in translating Catalan into English but is inferior to it in the opposite translation direction (in terms of the BLEU score). In contrast, we have shown that the phrase translation strategy consistently outper-formed the sentence translation strategy in the con-trolled experiments. We have compared two types of pivot strategies, namely phrase translation and sentence translation . The phrase translation strategy directly constructs a phrase translation table from a source language and English phrase-table and a target language and En-glish phrase-table. It then uses this phrase table in a phrase-based SMT system. The sentence transla-tion strategy first translates a source language sen-tence into n English sentences and translates these n sentences into target language sentences separately. Then, it selects the highest scoring sentence from the target language sentences.
 We conducted controlled experiments using the Europarl corpus to compare the performance of these two strategies to that of directly trained SMT systems. The experiments showed that the perfor-mance of the phrase translation strategy was statis-tically significantly better than that of the sentence translation strategy and that its relative performance compared to the directly trained SMT systems was 0.92 to 0.97. These are very promising results.
Although we used the Europarl corpus for con-trolled experiments, we intend to use the pivot strate-gies in situations where very limited amount of par-allel corpora are available for a source and target lan-guage but where relatively large parallel corpora are available for the source language X  X nglish and the target language X  X nglish. In future work, we will further investigate the pivot strategies described in this paper to confirm that the phrase translation strat-egy is better than the sentence translation strategy in the intended situation as well as with the Europarl
