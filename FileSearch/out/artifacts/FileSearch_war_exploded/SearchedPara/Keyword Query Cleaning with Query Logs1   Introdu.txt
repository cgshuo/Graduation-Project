 Popularized by Web search engines, key word search has become the de-facto standard way for people to access unstr uctured information. In recent years, a great deal of research has been conducted in the database community to also support keyword search as a way for users to access structured data reposito-ries such as XML documents and relationa l databases. Enabling users to access databases using simple keywords can relieve them from the trouble of master-ing a structured query language and understanding complex and possibly fast evolving database schemas. However, for structured databases, such as relational databases, it is a very challenging task to develop high performance and robust keyword search algorithms. One such challenge is that for structured data, the expected query result is no longer a lis t of relevant documents and the corre-sponding search space is no longer a collection of documents as in unstructured information retrieval. Rather, the expected result is usually a joined network of tuples [3,4] or a database view [7]; the search space is also increased dramatically as it is necessary to assemble keyword-matching tuples from different tables into the final results. In general, the search space is exponential in the number of keywords in the query.

Besides the exponential explosion of the search space, the dirtiness of keyword queries also influence the accuracy and effectiveness of keyword search, making keyword search over the structured database more difficult. Keywords query is called dirty when it contains words which are not intended as part of query or dirty words. The so-called dirty words refer to words with spelling errors or those that do not appear in the database but are semantically equivalent to some words in the database. In addition, the keywords in a given query are not isolated and may have connection with each other, which means that nearby query words may be grouped into segments that could match some tuples in the database. For example, the query  X  X om Hanks Green Mile X  over a movie database may be considered as consisting of two seg ments,  X  X om Hanks X  and  X  X reen Mile X . Performing query segmentation before conduting keyword processing can often reduce the number of logical units to be considered during the search and thus significantly reduce the search space.

Motivated by the above observations, Pu and Yu [8] introduce a pre-processing phase to keyword query processing, called keyword query cleaning, which in-volves two main tasks: keyword rewrit ing and query segmentation. Keyword rewriting rewrites the keywords in the query to compensate for spelling errors, synonyms of text words in the database, while query segmentation is mainly responsible for grouping nearby keywords in the query into logical segments.
In this paper, we propose two approaches to enhance the original scoring function. Both approaches make use of the information embedded in the query log to extend the original scoring function. The first enhanced approach combines the segment score on the database with the segment score on the query log by normalization and weighting, while the second approach boosts the segment score using a predefined boost function. The extensive experiments we conducted prove the effectiveness of our two approaches.
 The rest of the paper is organized as follo ws. Section 2 introduces related work. Section 3 defines some related basic con cepts. Section 4 pres ents two enhanced scoring functions based on query logs. Section 5 reports the experimental results. Section 6 concludes the paper. Keyword search over the structured database has been studied extensively in recent years. Early works [1,3,4] in this area mainly focus on finding the best join networks to assemble matching tuples from different tables in the database. The optimal join network problem, or alternatively, the Steiner tree problem, has been proved to be NP-complete with respect to the number of relevant ta-bles. Therefore, some heuristic algorithms for finding the top-k candidate join networks have been proposed [4]. Some recent works mainly focus on more rea-sonable ranking functions to improve the effectiveness of keyword search [5,6]. [3] first introduce the classical information retrieval (IR) technology to the ranking function, which has been shown to be very helpful. [5] makes further improve-ment based on the previous work by introducing the size normalization factor. In addition to works on keyword search over traditional relational databases, there are many existing works on other types of structured data. [15] discusses the problem of performing keyword search on relational data streams. [7] ad-dresses the issue of keyword search on relational databases with star-schemas commonly found in OLAP applications, where the focus is finding the relevant data subspaces rather than joining network s. [16] introduces keyword search over the spatial database. Keyword search over XML data has also attracted a great deal of attention [12,14]. [10,11,13] address the issue of top-k database selection across multiple databases.

Tan and Peng [9] have addressed the issue of query cleaning in the Web con-text. However, approaches proposed there cannot be easily applied to query cleaning in the context of relational databases because of the different charac-teristics of structured data and unstructured data. Pu and Yu [8] first introduce the problem of keyword query cleaning over the databases. It is shown that the added query cleaning phase not only improves the quality of the search result but also significantly reduces the search space for the subsequent search algorithm. However, the scoring function in [8] doe s not consider user preferences. Such preferences can be mined from query logs , which contain past queries issued by the user as well as the correct results chosen. In this paper, we enhance the work by Pu and Yu by designing scoring functions used in query cleaning that could make use of query logs to adapt to user preferences. We first introduce some basic concepts involved in query cleaning. We define tokens as strings that are considered as indivisible units, and terms as sequences of tokens. Let D be a database (relational or XML). A database token is a token which appears in somewhere in the database at least once, and the set of all database tokens of D is denoted by token D . Similarly, a database term is a term which appears in the database at least once, and the set of all database terms of D is denoted by term D .

Let L be a query log. We assume that an entry in the query log L has at least the following four fields: user ID, the query posed by user, the cleaned query selected by user, and the timesta mp that queries are posed. A log token is a token which appears in the cleaned query field at least once, and the set of all log tokens of L is denoted by token L . Similarly, a log term is a term which appears in cleaned query field at lea st once, and the set of all log terms of L is denoted by term L .

Segment and Segmentation are two key concepts we often refer to in our pa-per. A segment is defined as a sequence of tokens in the query. A segment S is considered valid if this sequence appear s in the database at least once, that is, S  X  term D .A segmentation is defined as a sequence of non-overlapping segments that completely cover all tokens in the query. A segmentation S with respect to a query is valid if and only if each segment of S is valid. Note that a query may potentially have many valid segmentations. For example, the query  X  X ML docu-ment structure X  can have four valid segmen tations: (XML, document, structure), (XML document, structure), (XML, document structure), and (XML document structure). For a given qu ery, the objective is to find the optimal segmentation with respect to a certain scoring function. The main focus of our work is to identify the scoring functions that tak e into consideration user preferences. Keyword queries can have many different v alid segmentations. It is therefore necessary to rank those segmentations according to some criteria. Ideally, such criteria should take user preferences into a ccount, as even for the same query, different users may prefer different segm entations based on their information need. 4.1 The Original Approach The scoring function for a single segment S in [8] is defined as follows: where score IR ( T S )=max { tf idf ( T S ,T ): T  X  term D } .

In Equation (1),  X  Q ( S ) is the maximum distances (in the original query) between each adjacent pair of tokens in the segment.  X  exp ( S ) is the total ex-pansion distance (i.e., the sum of distances between the original tokens and the  X  X orrected X  tokens). score IR ( S ) is the information retrieval score of the segment induced terms. normalize is an anti-monotonic normalization function and boost is a monotonic function with a lower bound no less than 1.

The rationale behind this scoring function is as follows [8]: 1. We prefer that tokens adjacent to each other in the user-supplied query are 2. We prefer to minimize the changes made to the original query tokens. 3. We prefer longer segments.
 In order to adapt to user preference, this scoring function has to be enhanced. The basic idea is to make use of information contained in query logs that record past user behaviour. In what follows, two enhanced approaches are proposed to improve the cleaning quality on basis of the original scoring function by making use of the query logs.
 4.2 The First Enhanced Approach : Normalization and Weighting In the first approach we propose, two additional terms based on the log infor-mation are added to the original scoring function. The first term added to the scoring functions is as follows: Where term L is the terms in the cleaned query field of the query log. This formula is similar to Equation (1) , but it uses log information instead of the database in computing the scores. It also shares a similar rationale. That is, we favour the segment which is similar to the terms appearing in the log.
We further use the rank of a segment to measure the user preference of dif-ferent segments. This rank refers to the rank of the access frequency of its cor-responding query terms in the query log, and can be obtained as follows. The query terms are sorted based on their fre quency. Similar frequencies, where the frequency difference between the corresponding terms is below the predefined threshold, are treated as having the same rank. It is believed that segments with higher ranks are often preferred by users because they are chosen more often by users. Therefore, we consider the score of a segment to be inversely proportional to its rank. Hence, the second term added to the ranking function is as follows: Here r is the rank of a segment. Note that the logarithm of the rank is used here instead of the original rank value in o rder to dampen the effect of the rank.
Before combining the two terms mention above with the original scoring func-tion, normalization is necessary because o f their different scales. We need to nor-malize all three terms into the unit interval [0 , 1]. We use a sigmoid function to achieve this effect, which takes the form of: where the weight parameter  X  controls the linearity of the curve.

The sigmoid function can be made more linear by scaling the parameter  X  .For example, the sigmoid function is nearly same as the linear function y = kx +0 . 5 within range [  X  6 , 6] when  X  =0 . 1. The sigmoid function can also be made linear in a larger range by adjusting the  X  value. The range of the original sigmoid with the transformation 2  X  ( sigmoid ( x )  X  0 . 5)). So the normalization function used here is as follows:
The above function can be made approximately linear within a larger range by setting a reasonable  X  value. In such cases, the NRM function has the prop-erty of preserving the original order of different segmentations (as dictated by Equation (1)). The following example illustrates this property. Consider two possible segmentation of a given query Q , S 1 =( S 1 ,S 2 ,S 3 ), S 2 =( S 4 ,S 5 ). The relationship between their scores is score ( S 1 )  X  score ( S 2 ) according to Equa-tion (1). After normalizing the score of every segment, the relationship between their scores remains same, that is, NRM ( score ( S 1 ))  X  NRM ( score ( S 2 )).
We favour the segment which is similar to the segment in the query log and also have a high rank. Thus, three terms can be combined after normalization as follows: where  X  is the proportion of the score from Equation (1) in the final score. Note that Equation (3) does not require normalization because its range is in [0 , 1].
There is a relative importance between the segment similarity and segment rank,which means which factor of two is more important. Here we use the average value of the Equation (2) and (3) as the final segment score based on the query log, which means that each part contri butes half of their score to the final log score and have the same weight. Of cour se, other weighting schemes may also be feasible. Note that this enhanced score function is the same as the original scoring function when  X  =1.

Let X  X  take a simple example to illustrate the above equation. Suppose that a query Q = t 1 t 2 t 3 needs to be segmented, where t 1 t 2 and t 2 t 3 can be grouped into a valid segment, thus generating two possible segmentations S 1 =( t 1 t 2 ,t 3 ), S 2 = ( t ,t 2 t 3 ). Suppose that t 2 t 3 are used in the query log while t 1 t 2 are not. So the segmentation S 2 may be the segmentation that the user wanted. Without taking the query log into account, that is , only following the original approach, the rela-tionship between two segmentations S 1 , S 2 is that score DB ( S 1 )  X  score DB ( S 2 ). When the query log is taken into account, the relationship between two segmen-score Log ( S 1 ) &lt;score Log ( S 2 ). By combining the score based on query log, the segmentation is adapt to user preference. 4.3 Second Enhanced A pproach: Boosting the Original Score The second enhanced approach is to boost the original segment score when the segment is found in the query log. The following is the enhanced scoring function: where the boost log ( S )isaboostfunctionforsegment S .

The rationale behind Equation (7) is that the original score of a segment should be boosted if a segment appears in the query log, and its score should not change from the original score if it does not exist in the query log. By boosting the score, segments appearing more frequently in the query log are more preferred.

There may be many boosting functions to adopt. The boosting function we adopt here is as follows: where count ( S )isthetimesthatsegment S appears in the query log and maxCount is the maximum count for all segments appearing in the query log. The logarithm of count are used to dampen the effect of counts. Here we boost the score of a segment based on the coun t of the segment relative to the most popular segment. That is, the more popular a segment is, the more its score should be boosted. 5.1 Query Log Generation Due to the lack of real query log data, we g enerate synthetic query logs instead. Here zipf distribution is used to generate the query logs needed. Zipf distribution is one of the important laws in bibliometrics. It is first applied to the statistics of natural language. It shows that only few words in English are frequently used while most of the others are not used so often. The relationship between the frequency of the word and its rank in the frequency table can be expressed in the form of mathematical expression as follows: where C ,  X  are constant for specific application.

It has been shown that many other languages not just English posses this characteristic. The same relationship occurs in many other rankings, unrelated to language, such as the population ranks of cities in various countries, corporation sizes, income rankings, etc. Similarly, the only few terms are often used and most of others are not used so often. Here we assume that the relationship between the frequency of the query term and its rank in the frequency table also conforms to Zipf distribution. Consequently, it is reasonable to simulate user behaviours using Zipf distribution.

In our experiments, we sample 1000 items from the data set (described below), treating them as the items of the database frequently accessed by users, and then generate a synthetic query log wi th 10,000 query records following Zipf distribution. 5.2 Experimental Setting and Evaluation Our experiments are mainly conducted on two real datasets, the IMDB dataset and the DBLP dataset. We process the two dataset in the exactly same way as what is done in [6,8]. The IMDB data contains information on movies, actors, directors, and so on, with 9 , 839 , 026 tuples. The raw text files in the IMDB dataset were converted to relational data as described in [6], and then its text attributes are indexed.The XML file in the DBLP dataset is parsed and also converted to the relational data in the way described in [6]. The obtained DBLP data with 881 , 867 tuples contains information on publications, authors, titles, and publishers, and then its selected text attributes are extracted and indexed.
Our implementation was done purely in Java. The full-text index on the text attributes in the datasets are built usin g Apache Lucene, an open-source full-text search engine . For each data set, two kinds of indexes were built. The first one is term index, which treats each term as unit for indexing, while the second one is token index, which are obtained by tokenize the terms using whitespace as delimiters.

All experiments are conducted on a Lenovo Desktop running Ubuntu 10 . 04 operating system, with a 2 . 33 GHz Intel Dual Core processor, 2 GB of memory, and 250 GB of hard drive.

In order to systematically evaluate the performance of our approaches and eliminate the effect of the subjectivity to o ur experimental evaluation as possible as we can, test queries need to be genera ted randomly. The test queries are generated in the same way as what is done in [8,17]. Specifically, for each query to be generated, d terms are sampled from the data set, and for each sampled term, only c contiguous tokens are kept. Spelling errors are introduced to each character in the sampled token with some probability. The terms taken from the d different terms are concatenated to form the query.

The segmentation accuracy is introdu ced here to evaluate the performance on our approaches. For a given query,  X  S denote the sets of segments generated from the query cleaning algorithm, and S denote the true sets of segments. The segmentation accuracy is defined as follows: 5.3 Experiments on the F irst Enhanced Approach Figure 1(a) and Figure 1(b) demonstrate the effect of parameter  X  on the segment accuracy on IMDB and DBLP respectively. Here the experiments are carried out on three different classes of queries, which are short queries, medium queries and long queries respectively. They are gen erated in the way described in [8]. The short queries are generated with d =2and c =3,with6tokensatmost.Medium queries are generated with d =5and c = 3, which have 15 or a bit less tokens. Long queries are generated with d =10and c = 3,with about 30 tokens. In Figure 1(a), it was shown that the segmentation accuracy on arbitrary length queries on IMDB is higher than 80% ,while in Figure 1(b) the segmentation accuracy on arbitrary length queries on DBLP is above 70% in most cases.
Query ambiguity is the main reason for causing segmentation errors. When a token in the query can form the valid terms with both the preceding tokens and the following tokens, the query is ambiguous. This situation between two corresponding terms is called an ambiguous connection [17]. The number of am-biguous connections pres ent in a query is used to measure the ambiguity level of a query. We conduct extensive experiments to evaluate how our enhanced ap-proaches perform at different levels of ambiguity present in the queries, compared with the Original Approach.

In our experiment,we generate 1000 medium-sized queries with different am-biguity levels on IMDB and DBLP respectively to evaluate the performance of our approaches under different ambiguity levels. The effect of  X  on the segmen-tation accuracy under different ambiguity levels on IMDB and DBLP are shown in Figure 2(a) and Figure 2(b) respectively. Here we conduct these experiments using medium-sized queries with different ambiguity levels.

Note that the segment accuracy on  X  = 1 is equal to the segment accuracy of the Original Approach. From Figure 2(a) and Figure 2(b),we observe that the segmentation accuracy on  X  =0 . 4 , 0 . 6 , 0 . 8 is higher than that on  X  =1,which proves that the quality of query cleaning is improved by our first enhanced approach. For IMDB dataset,  X  =0 . 4 is optimal for all ambiguity levels. For DBLP dataset, choosing  X  =0 . 4 is more reasonable for applications because it is optimal for level1 and level2 and there usually not exists so many ambiguities in one query. 5.4 Experiments on the Second Enhanced Approach Similar experiments are conducted on the Original Approach and Second En-hanced Approach to compare their perfo rmance. The segment ation accuracy at different ambiguity levels on the Original Approach and Second Approach on IMDB and DBLP is shown at Figure 3(a) and Figure 3(b) respectively. The ex-perimental results show that the segmentation accuracy gets improved to some extent in the second enhanced approach. 5.5 Comparison between Two Enhanced Approaches Two approaches are proposed in this paper to enhance the scoring function for improving the cleaning quality. We conduct some experiments on First Enhanced Approach and Second Enhanced Approach to compare their performance. Figure 4(a) and Figure 4(b) show the segmentation accuracy at different ambiguity lev-els on the First Enhanced Approach and Second Enhanced Approach on IMDB and DBLP respectively, where in the Second Enhanced Approach the parameter  X  =0 . 4. It is shown that the First Enhanced Approach performs better than Second Enhanced Approach in most cases when there exists query ambiguities. Keyword cleaning can significantly reduce the search space of keyword search over relational database and improve the efficiency of subsequent keyword search. In this paper, two enhanced approaches are proposed to utilize query logs to adapt to user preferences and improve an swer quality. The effectiveness of both approaches have been verified by extensive experiments we conducted. Acknowledgement. This work was supported in part by National Natural Sci-ence Foundation of China Gra nts (No. 61070018, No. 60903108, 61003051), the Program for New Century Excellent Talents in University (NCET-10-0532), an NSERC Discovery Grant, the Independent Innovation Foundation of Shandong University (2009TB016), and the SAICT Experts Program.

