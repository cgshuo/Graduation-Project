 } @is.s.u-tokyo.ac.jp Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a ten-dency to integrate richer syntactic information into a translation rule in order to better express the trans-lation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syn-tactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase cover-age in most syntax-based systems. For example, Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based sys-tem. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between lan-guages with substantial structural differences, such as English and Japanese, which is a subject-object-verb language (Xu et al., 2009).

Forest-based translation frameworks, which make use of packed parse forests on the source and/or tar-get language side(s), are an increasingly promising approach to syntax-based SMT, being both algorith-mically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT sys-tems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and un-aligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006).

What makes word alignment so fragile? In or-der to investigate this problem, we manually ana-lyzed the alignments of the first 100 parallel sen-tences in our English-Japanese training data (to be shown in Table 2). The alignments were generated by running GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set. Of the 1,324 word alignment pairs, there were 309 error pairs, among which there were 237 target function words, which account for 76.7% of the error pairs 1 . This indicates that the alignments of the function words are more easily to be mistaken than content words. More-over, we found that most Japanese function words tend to align to a few English words such as  X  X f X  and  X  X he X , which may appear anywhere in an English sentence. Following these problematic alignments, we are forced to make use of relatively large English tree fragments to construct translation rules that tend to be ill-formed and less generalized.

This is the motivation of the present approach of re-aligning the target function words to source tree fragments, so that the influence of incorrect align-ments is reduced and the function words can be gen-erated by tree fragments on the fly. However, the current dominant research only uses 1-best trees for syntactic realignment (Galley et al., 2006; May and Knight, 2007; Wang et al., 2010), which adversely affects the rule set quality due to parsing errors. Therefore, we realign target function words to a packed forest that compactly encodes exponentially many parses. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to con-strain the exhaustive attachments of function words, we further limit the function words to bind to their surrounding chunks yielded by a dependency parser. Using the composed rules of the present study in a baseline forest-to-string translation system results in a 1.8-point improvement in the BLEU score for large-scale English-to-Japanese translation. 2.1 Japanese function words In the present paper, we limit our discussion on Japanese particles and auxiliary verbs (Martin, 1975). Particles are suffixes or tokens in Japanese grammar that immediately follow modified con-tent words or sentences. There are eight types of Japanese function words, which are classified de-pending on what function they serve: case markers, parallel markers, sentence ending particles, interjec-tory particles, adverbial particles, binding particles, conjunctive particles, and phrasal particles.
Japanese grammar also uses auxiliary verbs to give further semantic or syntactic information about the preceding main or full verb. Alike English, the extra meaning provided by a Japanese auxiliary verb alters the basic meaning of the main verb so that the main verb has one or more of the following func-tions: passive voice, progressive aspect, perfect as-pect, modality, dummy, or emphasis. 2.2 HPSG forests Following our precious work (Wu et al., 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju 2 (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign . A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content. Phrases and words represented by signs are collected into larger phrases by the appli-cations of schemata . The semantic representation of the new phrase is calculated at the same time. As such, an HPSG parse forest can be considered to be a forest of signs. Making use of these signs in-stead of part-of-speech (POS)/phrasal tags in PCFG results in a fine-grained rule set integrated with deep syntactic information.

For example, an aligned HPSG forest 3 -string pair is shown in Figure 1. For simplicity, we only draw the identifiers for the signs of the nodes in the HPSG forest. Note that the identifiers that start with  X  X  X  de-note non-terminal nodes (e.g., c0, c1), and the iden-tifiers that start with  X  X  X  denote terminal nodes (e.g., t3, t1). In a complete HPSG forest given in (Wu et al., 2010), the terminal signs include features such as the POS tag, the tense, the auxiliary, the voice of a verb, etc.. The non-terminal signs include features such as the phrasal category, the name of the schema applied in the node, etc.. In this section, we first describe an algorithm that attaches function words to a packed forest guided by target chunk information. That is, given a triple  X  F
S ,T,A ( F
S ) to target sentence ( T ) pair, we 1) tailor the alignment A by removing the alignments for tar-get function words, 2) seek attachable nodes in the source forest F S for each function word, and 3) con-struct a derivation forest by topologically travers-ing F S . Then, we identify minimal and composed rules from the derivation forest and estimate the probabilities of rules and scores of derivations us-ing the expectation-maximization (EM) (Dempster et al., 1977) algorithm. 3.1 Definitions In the proposed algorithm, we make use of the fol-lowing definitions, which are similar to those de-scribed in (Galley et al., 2004; Mi and Huang, 2008):  X  s (  X  ) : the span of a (source) node v or a (target)  X  t ( v ) : the corresponding span of v , which is an  X  c ( v ) : the complement span of v , which is the  X  P
The function closure covers the gap(s) that may appear in the interval parameter. For example, closure ( t ( c3 )) = closure ( { 0-1, 4-7 } ) = { 0-7 } . Examples of the applications of these functions can be found in Table 1. Following (Galley et al., 2006), we distinguish between minimal and com-posed rules. The composed rules are generated by combining a sequence of minimal rules. 3.2 Free attachment of target function words 3.2.1 Motivation
We explain the motivation for the present research using an example that was extracted from our train-ing data, as shown in Figure 1. In the alignment of this example, three lines (in dot lines) are used to align was and the with ga (subject particle), and was with ta (past tense auxiliary verb). Under this align-ment, we are forced to extract rules with relatively large tree fragments. For example, by applying the GHKM algorithm (Galley et al., 2004), a rule rooted at c0 will take c7, t4, c4, c19, t2, and c15 as the leaves. The final tree fragment, with a height of 7, contains 13 nodes. In order to ensure that this rule is used during decoding, we must generate subtrees with a height of 7 for c0. Suppose that the input for-est is binarized and that | E | is the average number of hyperedges of each node, then we must generate O ( | E | 2 6  X  1 ) subtrees 4 for c0 in the worst case. Thus, the existence of these rules prevents the generaliza-tion ability of the final rule set that is extracted.
In order to address this problem, we tailor the alignment by ignoring these three alignment pairs in dot lines. For example, by ignoring the ambiguous alignments on the Japanese function words, we en-large the frontier set to include from 12 to 19 of the 24 non-terminal nodes. Consequently, the number of extractable minimal rules increases from 12 (with three reordering rules rooted at c0, c1, and c2) to 19 (with five reordering rules rooted at c0, c1, c2, c5, and c17). With more nodes included in the fron-tier set, we can extract more minimal and composed monotonic/reordering rules and avoid extracting the less generalized rules with extremely large tree frag-ments. 3.2.2 Why chunking?
In the proposed algorithm, we use a target chunk set to constrain the attachment explosion problem because we use a packed parse forest instead of a 1-best tree, as in the case of (Galley et al., 2006). Mul-tiple interpretations of unaligned function words for an aligned tree-string pair result in a derivation for-est. Now, we have a packed parse forest in which each tree corresponds to a derivation forest. Thus, pruning free attachments of function words is prac-tically important in order to extract composed rules from this  X (derivation) forest of (parse) forest X .
In the English-to-Japanese translation test case of the present study, the target chunk set is yielded by a state-of-the-art Japanese dependency parser, Cabocha v0.53 5 (Kudo and Matsumoto, 2002). The output of Cabocha is a list of chunks . A chunk con-tains roughly one content word (usually the head) and affixed function words, such as case markers (e.g., ga ) and verbal morphemes (e.g., sa re ta , which indicate past tense and passive voice). For example, the Japanese sentence in Figure 1 is sepa-rated into four chunks, and the dependencies among these chunks are identified by arrows. These arrows point out the head chunk that the current chunk mod-ifies. Moreover, we also hope to gain a fine-grained alignment among these syntactic chunks and source tree fragments. Thereby, during decoding, we are binding the generation of function words with the generation of target chunks. Algorithm 1 Aligning function words to the forest 3.2.3 The algorithm
Algorithm 1 outlines the proposed approach to constructing a derivation forest to include multiple interpretations of target function words. The deriva-tion forest is a hypergraph as previously used in (Galley et al., 2006), to maintain the constraint that one unaligned target word be attached to some node v exactly once in one derivation tree. Starting from a triple  X  F S ,T,A  X  , we first tailor the alignment A to A  X  by removing the alignments for target function words. Then, we traverse the nodes v  X  P A  X  in topo-logical order. During the traversal, a function word f w will be attached to v if 1) t ( v ) overlaps with the span of the chunk to which f w belongs, and 2) f w has not been attached to the descendants of v .
We identify translation rules that take v as the root of their tree fragments. Each tree fragment is a fron-tier tree that takes a node in the frontier set P A  X  of F S as the root node and non-lexicalized frontier nodes or lexicalized non-frontier nodes as the leaves. Also, a minimal frontier tree used in a minimal rule is limited to be a frontier tree such that all nodes other than the root and leaves are non-frontier nodes. We use Algorithm 1 described in (Mi and Huang, 2008) to collect minimal frontier trees rooted at v in F
S . That is, we range over each hyperedges headed at v and continue to expand downward until the cur-rent set of hyperedges forms a minimal frontier tree.
In the derivation forest, we use  X  nodes to man-age minimal/composed rules that share the same node and the same corresponding span. Figure 2 shows some minimal rule and  X  nodes derived from the example in Figure 1.

Even though we bind function words to their nearby chunks, these function words may still be at-tached to relative large tree fragments, so that richer syntactic information can be used to predict the function words. For example, in Figure 2, the tree fragments rooted at node c 0  X  8 0 can predict ga and/or ta . The syntactic foundation behind is that, whether to use ga as a subject particle or to use wo as an ob-ject particle depends on both the left-hand-side noun phrase ( kekka ) and the right-hand-side verb ( kensyou sa re ta ). This type of node v  X  (such as c 0  X  8 0 ) should satisfy the following two heuristic conditions:  X  v  X  is included in the frontier set P A  X  of F  X  t ( v  X  ) covers the function word, or v  X  is the root
Starting from this derivation forest with minimal rules as nodes, we can further combine two or more minimal rules to form composed rules nodes and can append these nodes to the derivation forest. 3.3 Estimating rule probabilities We use the EM algorithm to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the deriva-tion forests. As reported in (May and Knight, 2007), EM, as has been used in (Galley et al., 2006) to es-timate rule probabilities in derivation forests, is an iterative procedure and prefers shorter derivations containing large rules over longer derivations con-taining small rules. In order to overcome this bias problem, we discount the fractional count of a rule by the product of the probabilities of parse hyper-edges that are included in the tree fragment of the rule. 4.1 Setup We implemented the forest-to-string decoder de-scribed in (Mi et al., 2008) that makes use of forest-based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. We analyzed the performance of the proposed translation rule sets by using the same decoder.

The JST Japanese-English paper abstract corpus 6 (Utiyama and Isahara, 2007), which consists of one million parallel sentences, was used for training, tuning, and testing. Table 2 shows the statistics of this corpus. Note that Japanese function words oc-cupy more than a quarter of the Japanese words. Making use of Enju 2.3.1, we generated 987,401 1-best trees and 984,731 parse forests for the En-glish sentences in the training set, with successful parse rates of 99.3% and 99.1%, respectively. Us-ing the pruning criteria expressed in (Mi and Huang, 2008), we continue to prune a parse forest by set-ting p e to be 8, 5, and 2, until there are no more than e 10 = 22 , 026 trees in a forest. After pruning, there are an average of 82.3 trees in a parse forest.
We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Pap-ineni et al., 2002).

Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrase-based SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K En-glish sentences in the development and test sets. We used the default configuration of Joshua, with the ex-ception of the maximum number of items/rules, and the value of k (of the k -best outputs) is set to be 200. 4.2 Results Table 3 lists the statistics of the following translation rule sets:  X  C3-T: a composed rule set extracted from the  X  M&amp;H-F: a minimal rule set extracted from  X  Min-F: a minimal rule set extracted from the  X  C3-F: a composed rule set extracted from the
We investigate the generalization ability of these rule sets through the following aspects: 1. the number of rules, the number of reordering 2. the number of rules that are applicable to the 3. the final translation accuracies.
 Table 3 and Figure 3 reflect that the generalization abilities of these four rule sets increase in the or-der of C3-T &lt; M&amp;H-F &lt; Min-F &lt; C3-F. The ad-vantage of using a packed forest for re-alignment is verified by comparing the statistics of the rules and the final BLEU scores of C3-T with Min-F and C3-F. Using the composed rule set C3-F in our forest-based decoder, we achieved an optimal BLEU score of 28.89 (%). Taking M&amp;H-F as the baseline trans-lation rule set, we achieved a significant improve-ment ( p&lt; 0 . 01 ) of 1.81 points.
 In terms of decoding time, even though we used Algorithm 3 described in (Huang and Chiang, 2005), which lazily generated the N-best translation can-didates, the decoding time tended to be increased because more rules were available during cube-pruning. Figure 4 shows a comparison of decoding time (seconds per sentence) and the number of rules used for translating the test set. Easy to observe that, decoding time increases in a nearly linear way fol-lowing the increase of the number of rules used dur-ing decoding.

Finally, compared with Joshua, which achieved a BLEU score of 24.79 (%) on the test set with a decoding speed of 8.8 seconds per sentence, our forest-based decoder achieved a significantly better ( p&lt; 0 . 01 ) BLEU score by using either of the four types of translation rules. Galley et al. (2006) first used derivation forests of aligned tree-string pairs to express multiple inter-pretations of unaligned target words. The EM al-gorithm was used to jointly estimate 1) the trans-lation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. By dealing with the ambiguous word align-ment instead of unaligned target words, syntax-based re-alignment models were proposed by (May and Knight, 2007; Wang et al., 2010) for tree-based translations.

Free attachment of the unaligned target word problem was ignored in (Mi and Huang, 2008), which was the first study on extracting tree-to-string rules from aligned forest-string pairs. This inspired the idea to re-align a packed forest and a target sen-tence. Specially, we observed that most incorrect or ambiguous word alignments are caused by function words rather than content words. Thus, we focus on the realignment of target function words to source tree fragments and use a dependency parser to limit the attachments of unaligned target words. We have proposed an effective use of target function words for extracting generalized transducer rules for forest-based translation. We extend the unaligned word approach described in (Galley et al., 2006) from the 1-best tree to the packed parse forest. A simple yet effective modification is that, during rule extraction, we account for multiple interpretations of both aligned and unaligned target function words. That is, we chose to loose the ambiguous alignments for all of the target function words. The consider-ation behind is in order to generate target function words in a robust manner. In order to avoid gener-ating too large a derivation forest for a packed for-est, we further used chunk-level information yielded by a target dependency parser. Extensive experi-ments on large-scale English-to-Japanese translation resulted in a significant improvement in BLEU score of 1.8 points ( p&lt; 0 . 01 ), as compared with our implementation of a strong forest-to-string baseline system (Mi et al., 2008; Mi and Huang, 2008).
The present work only re-aligns target function words to source tree fragments. It will be valuable to investigate the feasibility to re-align all the tar-get words to source tree fragments. Also, it is in-teresting to automatically learn a word set for re-aligning 7 . Given source parse forests and a target word set for re-aligning beforehand, we argue our approach is generic and applicable to any language pairs. Finally, we intend to extend the proposed approach to tree-to-tree translation frameworks by re-aligning subtree pairs (Liu et al., 2009; Chiang, 2010) and consistency-to-dependency frameworks by re-aligning consistency-tree-to-dependency-tree pairs (Mi and Liu, 2010) in order to tackle the rule-sparseness problem.
 The present study was supported in part by a Grant-in-Aid for Specially Promoted Research (MEXT, Japan), by the Japanese/Chinese Machine Transla-tion Project through Special Coordination Funds for Promoting Science and Technology (MEXT, Japan), and by Microsoft Research Asia Machine Transla-tion Theme.

Wu ( wu.xianchao@lab.ntt.co.jp ) has moved to NTT Communication Science Laborato-ries and Tsujii ( junichi.tsujii@live.com ) has moved to Microsoft Research Asia.

