 1. Introduction
An information retrieval (IR) system, which manages text resources, processes words to extract and assign content descriptive index terms to documents or queries. As we use naturally spoken or written language, words are formulated with many morphological variants, even if they are referred to as a common concept. Therefore stemming has to be performed in order to allow words, which are formulated with morphological variants, to group up with each other, indicating a similar meaning. Most of the stemming algorithms reduce word forms to an approximation of a common morphological root, called with their morphological variants.

Nevertheless, the effectiveness of stemming is a debated issue, and there are different results and out-comes as reported by Frakes (1992, Chap. 8). If effectiveness is measured by the traditional precision and recall measures, it seems that for a language with a relatively simple morphology, like English, stemming has little influence on the overall performance, as reported by Harman (1991). In contrast, stemming can a more complex morphology, like the romance languages, as shown by Krovetz (2000) and Popovic and
Willett (1992). Despite this debate, it is commonly accepted that the use of a stemmer is intuitive to many users who can express the query using a specific term without worrying that only a variant of this term can appear in a relevant document (Harman, 1991). Thus, stemming should be a feature of the user interface of an IR service, supplied for example by digital libraries.

To design a stemming algorithm, it is possible to follow a linguistic approach based on prior knowledge of the morphology of the specific language, or a statistical approach which employs some methods based on statistical principles to infer the word formation rules from the corpus of documents. The linguistic ap-proaches are likely to be more effective because the quality of the morphological analysis has been assured to complete the morphological analysis especially when new languages have to be added on an IR service.
Furthermore, it is a demanding task to codify all of the word formation rules for languages with a complex morphology and the resulting stemmers can be imprecise; in addition it is not always possible to have an expert for each language. On the other hand, stemming algorithms based on statistical methods ensure no additional costs to add new languages to the system X  X  X his is an advantage that becomes crucial, especially are able to manage a great amount of non-English documents as well as documents written in many dif-ferent languages. Of course, the low cost of stemmer generation provided by stemming algorithms based on statistical methods might be counterbalanced by a degradation of the quality of the morphological analysis and, consequently, the retrieval effectiveness.

The research reported in this paper has originated from the study and the experimentation of a graph-based stemming algorithm described by Bacchin, Ferro, and Melucci (2002) and briefly described here. node of a graph where each link corresponds to the word obtained by concatenating the linked prefix and suffix. We introduced the notion of mutual reinforcement between stems and derivations, in order to identify the optimal prefixes and suffixes. An optimal prefix corresponds to a stem and an optimal suffix corresponds to a derivation. The rationale to use mutual reinforcement was based on the idea that stems are which complete, at a high frequency rate, the stems X  X  X n graphical terms, derivations tend to be linked to stems, and vice versa, thus forming communities in the graph. Thus the mutual reinforcement relationship is a kind of coupled frequent usage of prefixes and suffixes that are used in order to form words. It is stems, but they are discarded if they are not followed by very frequently used suffixes.
The method used to discover the communities of stems and derivations, which are coupled together by mutual reinforcement, is based not only on a computation of the sub-string frequency, as in Hafer and
Weiss (1974) or Goldsmith (2001), but also on link analysis algorithms which proved to be effective in Web retrieval. The graph-based stemming algorithm is an instance of affix removal-based stemming and in the stem, is described in (Frakes &amp; Baeza-Yates, 1992) and adopted by most stemmers currently in use by IR, like those reported by Lovins (1968), Paice (1990), and Porter (1980).

Therefore the main thrust of this paper is to take a step forward from the graph-based stemming algorithm just described and to introduce a probabilistic framework which models the mutual reinforce-ment between stems and derivations. The idea here is to consider stemming as the inverse of a machine which generates words by concatenating prefixes and suffixes. The paper then shows how the estimation of the probabilities of the model relates to the notion of mutual reinforcement and to the discovery of the communities of stems and derivations. The paper is organized as follows: in Section 2 an intuitive view of the probabilistic framework proposed for stemmer generation is given. The model is formulated in Section 3 and implemented through an algorithm that is described in Section 4; the experiments to assess the performances of the proposed algorithm are described in Section 5; finally Section 6 draws some conclusion and presents the future work. 2. Intuitive view
Many words from some Western languages describe concepts using a root and variations connected to the root. To materialize a concept using one of these languages, a word might be compounded by linking one or more morphemes together. First, a root is probably chosen to select a meaning close to the concept the concept to the context. As result, words similar in morphology are often similar in meaning.
Stemming associates each word to a stem which is an approximation of the morphological root that generated a word; it does not aim at finding a linguistically correct root form, but rather a canonical representation for a set of words with similar meaning. According to this view, words can be seen as the outcome of a generative process performed by a hypothetical machine that takes the set of all the possible prefixes and suffixes as input and produces words as output according to some type of linguistic knowledge.
The idea which underlies the intuitive view of the model proposed in this paper is that the machine produces words according to some knowledge about the language, rather than randomly. Thus, a word suffix. This is because the machine is driven by some type of linguistic knowledge. Because of this, the probability of generating a pair is not uniform X  X  X ince the machine is supposed to correctly operate, the probability that a stem is correctly concatenated with a derivation is higher than the probability that a generic prefix is concatenated with a generic suffix, as shown in Fig. 1.
 Stemming can be seen as the inverse of the generative process that has been just described above (Fig. 2).
Given a word, a stemmer has to guess the prefix and the suffix in order to form the most probable pair that the machine has chosen to generate the word. As the machine pools together its knowledge of the language, the most probable pair is formed by the stem and the derivation of the word. 3. The probabilistic model
Given a finite collection W of words, let U be the set of N sub-strings generated after splitting each word word z , respectively, then z  X  xy and there are n 1 possible positions to which z is split, if j z j X  n .
What is known about the machine is that it composes words by concatenating stems and derivations, and that this process is governed by some basic linguistic knowledge. To the observer, the individual pairs of sub-strings composed by the machine are random events. Let us define the universe of the elementary random events as follows: The relation the same word z . This is an equivalence relation, indeed the following properties hold:  X  reflexive: 8 x 2 X , xy  X  xy () X  x ; x  X 2 E  X  z  X  ; and then xy  X  x 00 y 00  X  z , i.e.,  X  x ; x 00  X 2 E  X  z  X  .
 E  X  z  X  induces a partition of the set X because it is an equivalence relation where the sets: associates each word z to X  X  z  X  , can be then defined. The set X  X  z  X  can be written as: which is the set of possible concatenations made by the hypothetical machine forming the target word z . Since there is a bijective function between the set of words and the set of the sub-sets X  X  z  X  , generates z . Because the single x i are disjoint events and X  X  z  X  X [ n 1 where x i is one of the elementary events of X  X  z  X  .

As stemming can be seen as the inverse of the generative process, a stemmer has to infer the most probable pair of prefixes and suffixes chosen by the machine to generate the given word. Therefore, a stemmer has to compute the expression
X  X  z  X  only, Pr  X  x j X  X  z 0  X  X  X  0 for any other z 0 6  X  the most probable split. Using the Bayes X  theorem,
Because the denominator does not influence the ranking of the possible splits of z , the stemmer computes in order to find the most probable split x of z .

At this point, it is worth noting that the probability of the pair x two equivalent ways: and
The probability that the hypothetical machine has chosen x on the product of two probabilities X  X  X he probability Pr  X  x
Pr  X  y i j x i  X  that y i is the suffix to complete z . Equivalently, Pr  X  x the suffix to complete a word, and the probability Pr  X  x ditional probability Pr  X  y i j x i  X  can be seen as  X  X  X ridge X  X  to complete z , the probability Pr  X  x significantly affects the computation of Pr  X  x i ; y i  X  . Then Pr  X  x
Given the a priori probability Pr  X  y i j x i  X  , the stemmer looks for Pr  X  x highest probability that x i  X  X  x i ; y i  X  generates z . In other words, the stemmer has to compute which is an equivalent expression of x  X  arg max x 2 X  X  z  X  suffixes over the words, the estimation of the probability Pr  X  x stepping stone of word generation.

At first sight, it would seem that the maximization of Pr  X  x maximum likelihood criterion, given Pr  X  y i j x i  X  . On the contrary, the maximization of Pr  X  x with the maximization of Pr  X  y i  X  , as shown below. To estimate Pr  X  x probabilistic mutual reinforcement in stemming is introduced.

Stems are prefixes which have a high probability of being completed by derivations; derivations, in turn, are suffixes which have a high probability of completing stems.

If a collection of words is observed, a prefix is completed by diverse suffixes, and a suffix completes diverse prefixes. The mutual reinforcement relationship emphasizes that stems are more likely to be can say that the corresponding split is likely to be the right one.

Let us formalize the notion of mutual reinforcement in stemming. It is a fact that Pr  X  x and that Pr  X  y j  X  X  fact that Pr  X  x i ; y j  X  X  Pr  X  y j j x i  X  Pr  X  x i  X  and that Pr  X  x and
The mutual reinforcement relationship is given by the fact that Pr  X  x at the same time Pr  X  y j  X  is an average mean of the Pr  X  x probability that y i is chosen, the higher the probability that its potential prefixes are stems and that y completes its prefixes. Eqs. (2) and (3) highlight that a circular relationship between Pr  X  x
To resolve this circularity, an iterative algorithm is proposed in the next Section 4. 4. The algorithm
Given a finite collection of words, for each word the stemmer has to guess, without any intrinsic that can be considered as stem and derivation of the word. This aim can be reached through a two-step algorithm:  X  Global step : at this step the stemmer considers the whole collection of words and it tries to infer some basic linguistic knowledge from the collection, i.e., the stemmer strives to understand which are the best word that the stem has looked for, but it considers the relationships among prefixes and suffixes of the whole collection X  X  X his is the reason why this step is called  X  X  X lobal X  X . corresponds to a stem and a derivation, using Eq. (1). Although the stemmer uses the linguistic knowl-edge inferred in the global step, it now operates within a local scope, because it considers only the pairs which lead to the word and not the whole collection.

Thus the stemmer is organized into two parts: the first one performs the global step, while the second one
Section 4.2 explains the local step. 4.1. Global step
To illustrate the algorithm to compute the probabilities, a more compact notation is used than in Eqs. (2) and (3). Let us define and i.e., the vector of prefix scores and the vector of suffix scores, respectively. Moreover, let A  X  X  a
N N matrix such that a sr  X  Pr  X  x r j y s  X  , and let B  X  X  b
Therefore, and
After substituting, and and then p is the eigenvector of C  X  X  BA  X  0 associated to unity eigenvalue, and s is the eigenvector of D  X  X  AB  X  0 associated to unity eigenvalue.

An element of C can be expressed as which represents the probability that prefix x i is associated to prefix x
Equivalently, an element of D can be expressed as which represents the probability that suffix y i is associated to suffix y The algorithm that computes prefix and suffix scores is illustrated in Fig. 4 using pseudo-code. 4.2. Local step
The aim of this step is to choose the most probable split, which corresponds to the prefix that can be the relationship had not been employed, the estimation of Pr  X  x maximum likelihood estimators, which do not embed the mutual reinforcement relationship between stems and derivations. After the algorithm has been performed to disclose and exploit the mutual reinforcement relationship, the marginal probabilities already incorporate the evidence given by the dependencies between following have been identified:
This approach is justified by the fact that the suffix is determined once the prefix x and the word z are given; vice versa for the second equation. This approach selects the best split by selecting the most probable stem, thus disregarding the most probable derivation; vice versa for the second equation.
This approach faces the situation from a different point of view: it assumes that, during the estimation of prefix and suffix probabilities illustrated in Section 4.1, the probabilities p and s have absorbed some linguistic knowledge, each one on its own, and so x ; y can be considered as an independent event and the probability of the generation of the word z is the product of the two marginal probabilities. 3. x  X  arg max x 2 X  X  z  X  Pr  X  x  X  Pr  X  y j x  X  ,or x  X  arg max
The latter two equations view Pr  X  x ; y  X  as the combination of one marginal probability and one condi-tional probability. This approach assumes that the algorithm does not capture the whole mutual rein-forcement relationship between x , y and that it is necessary also to consider a stochastic dependence.
In Section 5 the experimental results using two approaches out of the previous three are presented. An paper. The effectiveness of arg max x 2 X  X  z  X  Pr  X  x  X  and of arg max 5. Experiments
A series of laboratory experiments were conducted according to the Cranfield evaluation model using the procedures and the test data provided by the cross-language evaluation forum (CLEF). CLEF is an ini-the stemming algorithm described in the preceding sections by comparing it with stemming algorithms of the effectiveness of retrieval, and specifically on the precision of the retrieval of CLEF documents in comparison to the queries employed during the evaluation campaigns of 2001 and 2002. The variations precision of the system using the algorithm described in Section 4 were observed. 5.1. Statistical methods to validate results
A simple comparison of the variation in percentages between the effectiveness measures of two or more methods does not provide enough information to ensure that the behaviors of the methods are a result of their structural nature and have not occurred by chance. To validate the results in an IR setting, it is compare two IR methods, A and B, the significance tests were built in a way that the null hypothesis H means that no differences exists between the two methods, and the alternative hypothesis H method is better than the other.

After the formalization of the hypothesis system, a level of significance a has to be chosen, where a rep-resents the maximum probability that one agrees to, associated with the error of rejecting the null considered as the probability of the observed difference between the performances of IR methods A and B null hypothesis.

The Wilcoxon statistical tests for paired samples were used because the two sets of measures can be test and it did not force us to formulate any assumption on the variable distribution shape.
Carrying on the statistical analysis, two alternative information retrieval methods are compared, con-method, Y i be the same effectiveness measure of the same query for the second method and D
The rank is the consecutive number assigned to a specific observation in a sample of observations sorted by the positive differences is similar to the rank sum of the negative differences, then the two methods are considered equivalent. The test statistic, whose asymptotic distribution under H where
When D i  X  0 the related statistical units are not considered in the test as reported by Sheskin (1997). 5.2. Method and design of experiments
The aim of the experiments is to evaluate the retrieval effectiveness of the algorithms being proposed, knowledge X  X  X he algorithm by M. Porter is publicly available at the Snowball Web Site (2003) for research purposes. Our algorithms were compared with the Porter findings because the latter uses a kind of a priori linguistic knowledge of the language, so the comparison with that particular  X  X  X inguistic X  X  algorithm could give some information about the possibility of estimating linguistic knowledge by statistically inferred knowledge. To test if the system performance was not been significantly hurt by the application of stem-ming, as hypothesized in Harman (1991), the impact of the stemming algorithms was assessed by com-paring their effectiveness with the one reached without any stemmer. The stemming algorithms studied were evaluated by assessing the performances of an information retrieval system in terms of the traditional documents have been retrieved, where R is the number of relevant documents for the query, as reported in
Voorhees and Harman (2000). The performances of the system were analyzed using standard test collec-tions and changing only the stemming algorithms for different runs, all other things being equal. This way, all the changes in the system performances are just imputable to the stemming process. The evaluation was divided into two stages X  X  X t the beginning a global evaluation was carried out taking into consideration the measures averaged over all queries; then, a more analytical study was performed by evaluating the measures to check the significance of the results. 5.3. Test data and experimental system
To evaluate the stemming algorithms, as explained above, the performances of an information retrieval experiments are presented in this section. 5.3.1. Test data
The retrieval experiments for the Italian language were conducted using the CLEF 2001 and CLEF 2002 test collection provided by the cross-language evaluation forum (CLEF) consortium, which is reported by Peters and Braschler (2001). In particular the Italian sub-collection of CLEF test collection was used to-referring to 1994:  X  La Stampa , which is an Italian national newspaper;  X  Italian SDA , which is the Italian portion of the news-wire articles of SDA (Swiss Press Agency). The main features of the test collection are reported in Table 1. After a simple case normalization, the
Italian sub-collection has a glossary of 333,828 unique words. Both document and query sets are for-matted using SGML, and are encoded with the ISO-Latin 1 (ISO-8859-1) character set. Finally, the
CLEF collection gives a list of relevance judgments hints which are helpful for the evaluation of the system. 5.3.2. Experimental system 5.3.2.1. IRON . For indexing and retrieval, an experimental information retrieval system, called IRON, was used. IRON was developed by the Information Management Systems (IMS) research group of the
Department of Information Engineering of the University of Padova. This software tool is part of the group research work in the area of multi-lingual information retrieval which began in 1999 together with the construction of an Italian test collection for experiments in information retrieval reported by Agosti,
Bacchin, and Melucci (1999). IRON is a software tool which has been built on top of the Lucene 1.2 RC4 started as an independent project and in September 2001 it became an official Jakarta project. It is a free software application governed by the Apache Software Licence (ASL) and it is publicly available (Lucene Web Site, 2003).

IRON consists of two main modules, Indexer and Searcher, which perform the indexing of the test collection and the retrieval of the information formulated by the queries. The system implements the vector space model, described by Salton and McGill (1983), and a (tf idf)-based weighting scheme, reported by
Salton and Buckley (1988), as provided by the Lucene library. IRON has been incorporated in our experiments to test the efficiency of the statistical stemming algorithm and it also has been used in the experiments of the IMS group in connection with the CLEF 2002 evaluation campaign, as described by
Agosti, Bacchin, Ferro, and Melucci (2003). IRON is written in Java, and hence it can be run over multi-platforms; yet it runs on a machine equipped with a UNIX operating system.

As regards the stop-words used in the experiments, i.e., the words which have little semantic meaning, words. This stop-list is recommended by the CLEF consortium for the participants of the CLEF cam-paigns. 5.3.2.2. SPLIT . The algorithm illustrated in Section 4 was implemented using the stemming program for language-independent tasks (SPLIT). From the vocabulary of the Italian CLEF sub-collection, SPLIT spawns a 2,277,297-node and 1,215,326-edge graph, which is processed in order to compute prefix and suffix scores X  X  X PLIT took 2.5 h for 100 iterations on a personal computer equipped with Linux, an 800
MHz Intel CPU and 256 MB RAM. each considered stemming algorithm, the standard evaluation software package trec _ eval 7.0. beta, by Buckley (2003) was used to obtain the set of effectiveness measures to perform the evaluation. 5.4. Experimental results
Two algorithms based on the methodology being proposed and implemented by the SPLIT software were evaluated; for this reason, the proposed algorithms are often referred to as SPLIT stemming algo-rithms. This paper presents the evaluation results of two SPLIT stemming algorithms which use different approaches to compute the best split of a word, x :  X  x  X  arg max x 2 X  X  z  X  Pr  X  x  X  ; j x j P 3,  X  x  X  arg max x 2 X  X  z  X  Pr  X  x  X  Pr  X  y j x  X  .

The former uses a probabilistic approach reported in Section 4 and a heuristic rule which forces the three characters. It was our interested in testing if a simple heuristic could help the performances of the stemming algorithm. For each run, a label and a short description are provided and Table 2 summarizes the labels and the performed runs. The labels have been used through the rest of this section.
A macro-evaluation was carried out by averaging the results of all the queries of the test collection. Out of the 50 queries for 2001 and 2002 query sets, only 47 queries for 2001 and 49 for 2002 had relevant a summary of the figures related to the macro-analysis of the stemming algorithms.

The table provides an overview of the results and suggests formulating the hypothesis that the imple-mented IR system performed well when using the SPLIT as well as the Porter stemming algorithms. To validate this hypothesis, a more detailed evaluation was conducted query-by-query, i.e., the queries were modelled as statistical units and computed the values of two random variables X  X  X recision with SPLIT and precision with the Porter algorithm; in particular the R-P and average-precision figures were computed for each query and for each run. In the following, H 0 is the null hypothesis that the two compared algorithms yield the same level of precision, and H 1 is the opposite of H is accepted, whereas by R f H 0 g is the decision of rejection of H
Table 4 reports the number of queries for which a stemming algorithm could improve, decrease or keep an equivalent average-precision with respect to the non-stemming case, while Table 5 reports the data with hypothesis has to be rejected. The number of queries remaining unaltered or which show performance enhancement after the stemming process is greater than the number of queries where precision has de-creased, however, the enhancement is not strong enough to be considered statistically significant.
An analysis with respect the Porter algorithm was also conducted. It was our interest in testing if algorithms based on SPLIT could be as effective as one based on prior linguistic knowledge, as the Porter algorithm is. The hypothesis system is as follows:
Table 6 summarizes the results taking into consideration the number of queries for which the average precision has increased, has remained equal or has decreased when a SPLIT stemmer was used instead of the Porter finding. Table 7 summarizes the results taking the R-precision as measure.

For both effectiveness measures and test collections, the null hypothesis, that Porter and S2 _ Max-
H has to be rejected for 2002 and not rejected for the 2001 query set, i.e., S2 _ Maxpref-L3 performs worse for the 2001 queries.

In order to carry out a more in depth analysis, the user-oriented point of view was given more emphasis than the system-oriented one. If stemming is applied in an interactive context, as it is applied in digital more an interesting finding that end users can obtain the relevant document after having retrieved 10 or 20 documents instead of after 50% retrieved documents. To assess stemming performance from a more user-oriented point of view, it was in our concern to evaluate how the observed improvement of effectiveness can change the ranking of retrieved documents. The precision values at 10, 20, 30 document cutoff values were observed, as suggested by Harman (1991).
 The analysis was carried out first by using the non-stemming case and then by using the Porter case. with respect to non-stemming, in Table 8. The symbol  X  X   X   X  X  is used to mean that two algorithms were ments have been statistically tested for a level of significance a  X  0 : 05.

As a further analysis, Tables 9 X 11 report the number of queries for which the proposed algorithms increase, decrease, or remain unaltered when speaking about the performances of the system with respect to the Porter baseline. A summary of the results for the analysis process performed with respect to the Porter baseline, is reported in Table 12.

Taking into consideration all of the effectiveness measures computed in our analysis, both the stemming algorithms based on SPLIT methodology, S2 _ Maxpref _ L3 and S2 _ Maxpref _ nlink , do not worsen the per-formances of the system with respect to non-stemming, therefore giving the user a method to expand the query terms with all the word variants, without loss of system performances. If compared with an algorithm forms as effectively as Porter stemming algorithm, while S2 _ Maxpref _ L3 performs worse. 6. Summary and future work
The probabilistic framework proposed in this paper describes the mutual reinforcement relationship rules used for the tested language.

In addition to Italian several other experiments were conducted within CLEF 2003 using other lan-guages, such as English, German, Dutch, Spanish, French. For all the languages tested, the proposed stemmer produced equally good results as those produced by Porter stemmer (Di Nunzio, Ferro, Melucci, &amp; Orio, 2003).

The research work presented in this paper unfolded new problems to resolve and leads to further tigated further. It is our intention to consider the problem of decompounding words, which for example is a common phenomenon for German. This issue has been studied also by Braschler and Ripplinger (2003), who demonstrate that the use of decompounding leads to an improvement in retrieval effectiveness. Acknowledgements
The authors thank Maristella Agosti, Giorgio Di Nunzio, Nicola Orio, and Luca Pretto for the invaluable discussions and suggestions. The authors owe a special thank-you to the reviewers for the useful comments which helped them improve the paper. The research has been partially supported by the En-hanced Content Delivery 1 (ECD) national project.
 References
