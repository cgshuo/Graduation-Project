 For the past few decades, ranked retrieva l (e.g. web search) has been evaluated using rank-based evaluation metrics such as Average Precision and normalised Discounted Cumulative Gain (nDCG) [7]. These metrics discount the value of each retrieved relevant document based on its rank. The situation is similar with diversified search which has gained popularity recently: diversity metrics such as  X  -nDCG [3], Intent-Aware Expected Reciprocal Rank (ERR-IA) [2] and D -nDCG [14] are also rank-based. These widely-used evaluation metrics regard the system output as a list of document IDs, and ignore all other features such as snippets and document full texts of various lengths.

The U-measure framework of Sakai and Dou [11] uses the amount of text read by the user as the foundation for discounting the value of relevant information, and can take into account the user X  X  snippet reading and full text reading be-haviours. The present study compares the diversity versions of U-measure ( D-U and U-IA ) with state-of-the-art diversity metrics using the concordance test [10]: given a pair of ranked lists, we quantify the ability of each metric to favour the more diversified and more relevant list, by counting pref erence agreements with a pure diversity metric (namely, intent recall [14]) and a pure relevance metric (namely, precision). Our results show that while D -nDCG is the overall win-ner in terms of simultaneous concordanc e with diversity and relevance, D-U and U-IA statistically significantly outperfo rm other state-of-the-art metrics. More-over, in terms of concorda nce with relevance alone, D-U and U-IA significantly outperform all rank-based diversity metrics. Thus, D-U and U-IA are not only more realistic but also more relevance-oriented than other diversity metrics. This section discusses existing studies on evaluation metrics for diversified search, which, given an ambiguous and/or underspecified query, aims to satisfy different user intents with a single search engine result page 1 . While traditional ranked retrieval only considers relevance, dive rsified search systems are expected to find the right balance between diversity and relevance. In diversified search evalua-tion, it is assumed that the following are available [14]:  X  A set of ambiguous and/or underspecified topics (i.e., queries) { q } ;  X  Asetof intents { i } for each topic;  X  The intent probability Pr ( i | q ) for each intent;  X  Per-intent (possibly graded) relevance assessments for each topic. Because diversity metrics n eed to consider the above different factors to evaluate systems, they tend to be more complex than traditional ranked retrieval met-rics. However, since the ultimate goal of IR researchers is to satisfy the user X  X  information need, we want to make sure that the metrics are measuring what we want to measure. This is the focus of the present study.

The TREC 2 Web Track ran the Diversity Task from 2009 to 2012 [6]. In the present study, we use the TREC 2011 diversity data [4]: only the 2011 and 2012 data have graded relevance assessments, and the number of participating teams was higher in 2011 (9 vs. 8). At the TREC 2009 Diversity Task, the primary metric used for ranking the runs was  X  -nDCG; ERR-IA was used primarily in the subsequent years.

NTCIR 3 ran the INTENT task [12] 4 at NTCIR-9 and -10, which also evalu-ated diversified search. The primary evaluation metric used there was D -nDCG, which is a simple linear combination of intent recall (I-rec) and D-nDCG [14]. The NTCIR-10 INTENT-2 task also used additional metrics called DIN-nDCG and P+Q to evaluate the systems X  ability to handle informational and naviga-tional intents in diversified search. However, this intent-type-sensitive evaluation is beyond the scope of this paper, as very few teams have tackled this particular problem so far.

In this study, we compare D-U and U-IA with these official diversity metrics from TREC and NTCIR, namely,  X  -nDCG, ERR-IA and D( )-nDCG, from the viewpoint of how  X  X ntuitive X  they are. We use the official  X  -nDCG and ERR-IA performance values that were computed with the ndeval software 5 ,aswellas D( )-nDCG values computed with NTCIREVAL 6 . Below, we formally define these rank-based diversity metrics from TREC and NTCIR.

First, let us define the original nDCG for traditional IR, given graded rele-vance assessments per topic, where the relevance level x varies from 0 to H .Inthe present study, H =3(SeeTable1inSection5),and x = 0 means  X  X onrelevant. X  Following previous work (e.g. [1,2]), we let the gain value of each x -relevant doc-ument be gv x =(2 x  X  1) / 2 H : hence gv 1 =1 / 8, gv 2 =3 / 8and gv 3 =7 / 8. For a given ranked list, the gain at rank r is defined as g ( r )= gv x if the document at r is x -relevant. Moreover, let g  X  ( r ) denote the gain at rank r in an ideal ranked list, obtained by sorting all relevant documents by the relevance level [7,9]. A popular version of nDCG [1] is defined as: where l is the measurement depth or document cutoff .

In diversified IR evaluation where each topic q has a set of possible intents { i } , (graded) relevance assessments are obtained for each i rather than for each q .Let I i ( r ) be one if the document at rank r is relevant to intent i and zero otherwise; let C i = r k =1 I i ( k ).  X  -nDCG is defined by replacing the gains in Eq. 1 with the following novelty-biased gain [3]: where  X  is a parameter, set to  X  =0 . 5 at TREC. Thus it discounts the value of each relevant document based on redundancy within each intent (Eq. 2) and then further discounts it based on the rank (Eq. 1). Although this definition requires the novelty-biased gains for the ideal list ( ng  X  ( r )), the problem of obtaining the ideal list for  X  -nDCG is NP-complete, and ther efore a greedy approximation is used in practice [3]. Note that  X  -nDCG cannot handle per-intent graded rele-vance: it defines the graded relevance of a document based solely on the number of intents it covers.

In contrast, ERR-IA utilises per-intent graded relevance assessments: let g i ( r ) denote the gain at rank r with respect to intent i , using the aforementioned gain the user with intent i is satisfied with this particular document at r . Then the ERR for this particular intent, ERR i , is computed as: This is an intuitive metric: the user with intent i is dissatisfied with documents between ranks 1 and r  X  1, and is finally satisfied at r ; the utility at this satisfac-tion point is measured by the reciprocal rank 1 /r . Finally, ERR-IA is computed as the expectation over the intents:
The D framework [14] used at the NTCIR INTENT task also utilises per-intent graded relevance assessments. First, for each document at rank r ,the global gain is defined as: Then, by sorting all relevant documents by the global gain, a  X  X lobally ideal list X  is defined for a given topic, so that the ideal global gain GG  X  ( r )canbe obtained. Note that unlike  X  -nDCG, there is no NP-complete problem involved here, and that, unlike ERR-IA, there is exactly one ideal list for a given topic. By replacing the gains in Eq. 1 with these global gain values, a D-measure version of nDCG, namely, D-nDCG is obtained. This is further combined with intent the system output: Here,  X  is a parameter (0  X   X   X  1), simply set to 0.5 at NTCIR. D -nDCG is a single-value summary of the I-rec/D-nDCG graph used at the NTCIR INTENT task [12], which visualises the trade-off b etween diversity and overall relevance.
As we shall demonstrate later,  X  -nDCG and ERR-IA behave very similarly, as they both possess the per-intent diminishing return property [2]: whenever a relevant document is found, the value of the next relevant document is discounted for each intent. Because redundancy within each intent is penalised, diversity across intents is rewarded. Whereas, D-nDCG does not have this property, so it is combined with I-rec, a pur e diversity metric, to compensate for this. However, none of these metrics used at TREC or N TCIR reflects the real user behaviours such as reading snippets and visiting the full text of a relevant document. The new diversity metrics that we advocate in this study, called D-U and U-IA, do just that.

The recently-proposed Time-Biased Gain (TBG) evaluation framework [17] is similar to the U-measure framework in that it can also take into account the user X  X  snippet and full text reading behaviours: while U discounts the value of relevant information based on the amount of text read so far 7 ,TBGdoesthis based on the time spent so far . The idea is basically equivalent if the user X  X  reading speed is constant. However, TBG -based diversity evaluation has not been explored in the literature. This section defines U-measure and its diversity versions D-U and U-IA as de-scribed by Sakai and Dou [11].

First, we present the general U-mea sure framework. Figure 1 introduces trail-texts , the foundation of the U-measure framework. Part (a) shows a single textual query-biased summary being shown to the user. Suppose that we have observed (by means of, say, eyetracking or mousetracking) that the user read only the first and the last sentences of this summary. In this case, we define the trailtext as a simple concatenation of these two sentences:  X  Sentence1 Sentence2 . X 
Summarisation represents an informat ion access task where the user-system interaction is minimal; at the other end of the spectrum, we may have search sessions with several query reformulations. Figure 1 Part (b) shows a session that involves one query reformulation: the user reads two snippets in the orig-inal ranked list, reformulates the query, reads one snippet in the new ranked list, and finally visits the actual document. The trailtext is then  X  Snippet1 Snippet2 Snippet3 Fulltext4 . X  Thus, the trailtext is a concatenation of all texts read by the user during her informat ion seeking activity. If evidence from eyetracking/mousetracking etc. is unavaila ble, the trailtext can alternatively be constructed systematically under a certain user model, using document relevance assessments and or click data [11].
 The general U-measure frame work comprises two steps: Step 1. Generate a trailtext, or multiple possible trailtexts, by either observing Step 2. Evaluate the trailtext(s), based on relevant information units (e.g. doc-
Formally, a trailtext tt is a concatenation of n strings: tt = s 1 s 2 ...s n .Each string s k (1  X  k  X  n ) could be a document title, snippet, full text, or even some arbitrary part of a text (e.g. nugget). We assume that the trailtext is exactly what the user actually read, in the exact order , during an information seeking process. We define the offset position of s k as pos ( s k )= k j =1 | s j | . We measure lengths in terms of the number of characters [13]. Each s k in a trailtext tt is considered either x -relevant or nonrelevant. In the present study, we assume that s k is either a web search engine snippet of 200 characters or a part of a relevant web page; we also assume that the user examines the snippets starting from the top of the list, and that she reads exactly F = 20% of every relevant web page that she sees 8 . We define the position-based gain as g ( pos ( s k )) = 0 if s k is considered nonrelevant, and g ( pos ( s k )) = gv x if it is considered x -relevant, where the gain value setting is the same as those for the rank-based metrics. In the present study where s k is either a snippet or a part of a full text, g ( pos ( s k )) = gv x if and only if s k is a part of a full text of an x -relevant document; an example will be discussed later.
 The general form of U-measure is given by: where N is a normalisation factor, which we simply set to N = 1 in this study, pos is an offset position within tt ,and D ( pos )isa position-based decay function. Following the S-measure framework [13], here we assume that the value of a relevant information unit decays linearly with the amount of text the user has read: Here, L is the amount of text at which all relevant information units become worthless, which we set to L = 132000 based on statistics from 21,802,136 ses-sions from Bing [11].

The U-measure framework can be extended to handle diversified IR evaluation in two ways. The first is to take the D-measure approach: as shown in Figure 2(a) and (b), given a ranked list, a single trailtext can be built by adding a 200-character snippet for each rank and 20% of each document full text which is relevant to at least one intent; then, the global gain at the end position of each relevant document is computed as: in to Eq. 7, to obtain D-U . The second approach is to follow the Intent-Aware approach: as shown in Figure 2(a) and (c), a trailtext is built for each intent ,and aUvalue( U i ) is computed independently for each i . Finally, the Intent-Aware U is given as:
D-U and U-IA are in fact very similar. Let { i } (  X  X  i } )bethesetofintents covered by the system output; a document in this output is strictly locally rele-vant if it is relevant to at least one intent from { i } and nonrelevant to at least one intent from { i } . It is easy to show that if there is no strictly locally relevant document in the system output, then D -U = U -IA holds . A corollary is that if the system output covers only one intent, then D -U = U -IA holds [11]. Sakai and Dou [11] compared D-U and U-IA with D( )-nDCG and a version of ERR-IA in terms of discriminative power : the ability of a metric to find statis-tically significant differences with high confidence for many system pairs. They reported that D-U, U-IA and ERR-IA underperform D( )-nDCG in terms of discriminative power, probably because D( )-nDCG does not possess the dimin-ishing return property: it does not penalise  X  X edundant X  relevant documents, so it relies on more data points and is statistically more stable. However, dis-criminative power is only a measure of stability: it does not tell us whether the metrics are measuring what they are supposed to measure. To evaluate diversity metrics from the latter point of view, we adopt Sakai X  X  concordance test [10].
Because diversity IR metr ics are complex, the concordance test tries to exam-ine how  X  X ntuitive X  they are by using some very simple  X  X old-standard X  metrics. Since we want both high diversity and high relevance in diversified search, it is possible to regard intent recall and/or precision (where a document relevant to at least one intent is counted as relevant) as the gold standard. Note that these gold-standard metrics themselves are not good enough for diversity evaluation: these merely represent the basic properti es of the more complex diversity metrics that should be satisfied.
Figure 3 shows a simple algorithm for comparing two candidate metrics M 1 and M 2 given a gold standard metric M  X  : concordance with multiple gold stan-dards can be computed in a similar way. Here, for example, M 1 ( q,X ) denotes the value of metric M 1 computed for the output of system X obtained in response to topic q . Note that this algorithm focusses on the cases where M 1 and M 2 disagree with each other, and then turn to M  X  which serves as the judge. While the condordance test relies on the assumption that the gold-standard metrics represent the real users X  preferences 9 ,itisusefultobeabletoquantifyexactly how often the metrics satisfy the basic p roperties that we expect them to satisfy, given many pairs of ranked lists. In our case, the specific questions we address are: (a) How often does a diversity metri c agree with intent recall (i.e., prefer the more diversified list)?; (b) How often does it agree with precision (i.e., prefer the more relevant list)?; and (c) How often does it agree with intent recall and precision at the same time? Table 1 shows some statistics of the TREC 2011 Diversity Task data which we used for conducting the concordance tests. Note that as we have 17  X  16 / 2 = 136 run pairs, we have 50  X  136 = 6 , 800 pairs of ranked lists for the tests. The diversity evaluation metrics, D-U, U-IA, D( )-nDCG,  X  -nDCG and ERR-IA use the measurement depth of l = 10 as diversified search mainly concerns the first search engine result page. Computation of D-U and U-IA requires the document length statistics for all the relevant documents retrieved above top l = 10: the estimated lengths are available at http://research.microsoft.com/u/ .
As the TREC diversity data lack intent probabilities Pr ( i | q ), we follow TREC and simply assume that the probability distribution across intents is uniform 10 .
Table 2 summarises our concordance test results. Part (a) shows condordance with intent recall (i.e., the ability to prefer the more diversified result); (b) shows concordance with precision (i.e., the ability to prefer the more relevant result); and (c) shows simultaneous concordance with intent recall and precision. For example, Part (a) contains the following information for the comparison between U-IA and ERR-IA in terms of con cordance with intent recall:  X  U-IA and ERR-IA disagree with each other for 1,463 out of the 6,800 ranked  X  Of the above disagreements, U-IA is concordant 84% of the time, while ERR- X  U-IA is significantly better than ERR-IA according to the sign test (  X  =
Let  X  M 1 M 2  X  denote the relationship:  X  M 1 statistically significantly out-performs M 2 in terms of concordance with a given gold-standard metric. X  Then our results can be summarised as follows 12 : (a) Concordance with I-rec (pure diversity): D -nDCG  X  -nDCG U-IA (b) Concordance with Prec ( pure relevance): U-IA, D-U D-nDCG D -(c) Simultaneous concordance with I-rec and Prec : D -nDCG U-IA D-U
Recall that D-U and U-IA are more realistic than the other metrics (including the gold-standard metrics), in that they consider the snippet and full text reading activities. Thus, we can conclude that D-U and U-IA are not only more realistic than other diversity metrics but also more relevance-oriented.

Finally, it can be observed that D-U and U-IA disagree with each other for only 54 out of the 6,800 ranked list pairs: thus, in practice, it is not necessary to use both of these metrics at the same time. Based on the above concordance test results, we recommend the use of U-IA. It can also be observed that  X  -nDCG and ERR-IA also behave similarly: they disagree with eath other for only 292 out of the 6800 ranked list pairs.
 Our results show that while D -nDCG is the overall winner in terms of simul-taneous concordance with diversity and relevance, D-U and U-IA statistically significantly outperform other state-of-the-art metrics. Moreover, in terms of concordance with relevance alone, D-U a nd U-IA significantly outperform all rank-based diversity metrics. Hence D-U and U-IA are not only more realistic but also more relevance-oriented than the rank-based metrics. Moreover, as D-U and U-IA in fact behave extremely similarly, we recommend the use of of U-IA, which outperformed D-U according to the concordance tests.

Figure 4 summarises various properties of existing diversity evaluation met-rics. Below, we provide additional comments for each row in this figure: (c),(d) These are two sides of the same coin.  X  -nDCG requires an approxima-
Our future work for diversity evaluation includes the following:  X  Exploring different (possibl y nonlinear) decay functions D ( pos ) with U-IA  X  Comparing different infor mation access styles (e.g. direct answers vs. diver- X  Exploring diversity evaluation methods without using explicit set of intents
