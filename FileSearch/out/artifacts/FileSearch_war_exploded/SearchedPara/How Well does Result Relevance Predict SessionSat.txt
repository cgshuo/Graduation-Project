 Per-query relevance measures provide standardized, repeat-able measurements of search result quality, but they ignore much of what users actually experience in a full search ses-sion. This paper examines how well we can approximate a user X  X  ultimate session-level satisfaction using a simple rele-vance metric. We find that this relationship is surprisingly strong. By incorporating additional properties of the query itself, we construct a model which predicts user satisfaction even more accurately than relevance alone.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Measurement, Experimentation search evaluation, user satisfaction, relevance metrics, pre-cision
Traditional work in search engine evaluation can be grouped into two broad categories: relevance measurements and user satisfaction studies. Relevance measurements [7, 19, 14] (such as nDCG [9], MAP [5], bpref [6], Precision at K, etc.) measure a search engine X  X  performance on a set of queries by grading the relevance of each result returned. This is convenient for comparing engines over time, because once results are graded, aggregate relevance can be re-calculated for different result lists or orderings. However, relevance measurements ignore much of what users do with search engines: long sessions with multiple searches, scanning of result titles and snippets, opening results, and using other search engine features.
 Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
Usability and user satisfaction studies [16, 2, 15, 11, 8] measure the user experience more directly, providing valu-able insights, but are much less scalable and repeatable. This raises the question: what is lost by using per-query relevance metrics instead of a more comprehensive view of the session? Or put another way, how well do relevance metrics predict user satisfaction? That is the question we address in this paper.

More specifically, assuming a user with some information need starts a search session by typing query Q : 1. How well can we predict the user X  X  satisfaction with 2. Does the relationship between first-query result rele-
For question 1, we find a surprisingly strong relationship between the relevance of the first query of the session and the user X  X  ultimate satisfaction with the session.
For question 2, we find that by incorporating additional properties of the first query, we are able to construct a model which predicts user satisfaction more accurately than rele-vance alone. In addition, we find that the remaining pre-diction error can be further reduced by incorporating the average number of events in the rated sessions.
In order to collect data representative of real searches, we drew a random sample of 200 US English-language queries submitted to the Google search engine in mid-2006. Explic-itly pornographic queries were excluded from the sample. To measure result relevance, we re-submitted these 200 queries to Google, and asked raters to assess the relevance of each of the first three search results returned. Seven raters rated each query-result pair on a graduated scale; their ratings were normalized and averaged to a score between 0 and 1. We also calculated a summary relevance score for each query, by taking a position-weighted average of the result relevance scores for the query.

Separately, we asked raters to categorize each query along several dimensions: whether it contained a misspelling, whether they felt it was navigational, transactional, or informational [4, 12], whether it was topically specific or broad, whether it re-ferred to a specific entity, and so on. Nine raters categorized each query, and category labels were assigned by majority vote. For example, a query would be labeled  X  X avigational X  if more than half the raters marked it as such.

To measure user satisfaction with search sessions starting from each query, we first asked five raters to give their inter-pretation of a typical user X  X  likely  X  X nformation need X  corre-sponding to each query. To address the possible ambiguity of some queries, we allowed raters to provide primary and secondary statements; secondary statements were supplied for 87 of the original 200 queries. These statements were then centrally reviewed, the most frequent primary and sec-ondary statements were chosen for each query, and the form of the language was normalized. A few examples of queries and their corresponding information need statements and query categories are shown in Table 1.

Finally, each query and corresponding information need were given to a separate group of human raters. Similar to  X  X pecified task X  studies, the se raters were asked to pre-tend to be a person with the information need [13]. They started a session by issuing the given query to Google, and then continued as they felt a person with this information need might, stopping when their need was met or when they wanted to give up. After the session was complete, they were asked to rate their satisfaction with their expe-rience and provide comments. Their web browser actions during the session were recorded. Seven raters were given each query+(primary or secondary) information need. For queries with only a primary information need, the seven sat-isfaction ratings were averaged and normalized to a score be-tween 0 and 1. For queries with both primary and secondary information needs, we calculated a blended satisfaction score by taking 0.7 times the average of primary satisfaction rat-ings plus 0.3 times the average of secondary satisfaction rat-ings. Similarly, we computed the average number of pages sequentially visited in the rater sessions; for queries with both primary and secondary information needs, we blended using a weighted average.

To summarize the setup: we start with random queries that were issued to Google. This has the great advantage of making our results generalizable to Google X  X  querystream. The disadvantage is that we don X  X  know the original intent behind the queries. To infer these intents, we call on a group of raters. Separately, we ask additional raters to provide query categorizations, relevance and satisfaction measures. We separated the task of inferring intent from the session satisfaction task in order to keep each task tractable for the raters, and thus produce cleaner data.
In the following sections, we compare the relevance of the first query of the session (which we will refer to sim-ply as  X  X elevance X ) to the user X  X  final satisfaction with the search session. To help make this comparison, it is conve-nient to compute an aggregate measure of relevance. For each query, we have relevance judgments between 0 and 1 for the top three search results. We define relevance of a query, Relevance[q] , as a simple position-weighted mean (position meaning whether the result was 1st, 2nd, or 3rd for the query), weighted by 1/position. Calling the relevance judgments for each position pos1 , pos2 ,and pos3 :
This is a simple cumulative discounted gain measure [9].
How strong is the correlation of first query relevance to session satisfaction? Figure 1 shows a scatter-plot of session satisfaction versus first-query relevance. The plot is scaled so that 0 and 1 correspond to the maximum and minimum observed value for each dimension. Satisfaction Correct spelling Figure 1: Session satisfaction vs. first-query rele-vance
As the plot shows, there is a reasonably strong but some-what noisy linear relationship between satisfaction and rel-evance; the Pearson correlation is 0.727. The scatter-plot shows a stronger correlation at the upper end of the rele-vance scale. In fact, there are no points in the lower right quadrant, indicating that, at the high end of the scale, first-query relevance is a very strong predictor of session satisfac-tion.

At lower relevance levels, the correlation is weaker. Ex-amining the largest outliers, we observed that several are queries for which in addition to search results, Google re-turns a spelling suggestion ( X  X id you mean... X ). We X  X e marked these queries (26 of the original 200) with an X in the plot. For these queries, session satisfaction is much higher than the corresponding first-query relevance would predict. This makes sense: users saw the spelling suggestion, clicked on it, and continued their session with the (presumably much more relevant) results of the spell-corrected query; thus the relevance of the initial search results is uncorrelated with the user X  X  satisfaction.

Spelling suggestions are only one example of  X  X xtra X  ele-ments that modern web search engines return to users; other examples are current news, images,  X  X ocal X  results, stock charts, etc. A user looking for the current price of a stock who types the ticker symbol into Google or Yahoo will of-ten be satisfied by a chart at the top of the page, without looking at any web search results. As researchers strive to evaluate how effectively search engines satisfy users X  needs, these  X  X xtra X  elements will need to be accounted for.
For misspellings, the relevance of the spell-corrected query X  X  results to the user X  X  information need could be used to help predict satisfaction. We do not have that data in the present study. Removing these 26 queries from our dataset, the
Query Primary info. need Secondary info. need Query attributes  X  X all for help X  Leo User expects to be taken to the
User is looking for information about Leo LaPorte, the host of the TV show  X  X all For Help X . leonard cohen carol User is looking for Christmas fisher price User expects to be taken to red envelope User expects to be taken to the
User is looking for the meaning of the red envelope or red packet given in Chinese society. session saver firefox 1.5 User is looking to download the
User is looking for information about the Session Saver add-on for Firefox 1.5. sleeppy hollow User is looking for information User is looking for the town of
Sleepy Hollow, NY. correlation of session satisfaction and first-query relevance increases to 0.784 for the remaining 169 queries.
Having removed misspellings, we now attempt to con-struct a more accurate model of satisfaction, based on first-query positional result relevance and other properties. We build an increasingly complex sequence of linear models and measure their performance in two different ways. The first,  X  X verall correlation X  is the co rrelation of the observed user satisfaction scores with the model predictions. Correlation is the square root of the standard R -squared statistic, which may also be calculated as the correlation between the model predictions and the true values. One potential drawback of this goodness-of-fit measure is that it must increase as the number of variables in the model increases. To address this, we also present a cross validated correlation, calculated by leaving out one data point at a time and predicting it from the others. The cross-validated correlation is the correlation between the model prediction and the leave-one-out predic-tions.

For our sequence of models, the cross-validated correlation and overall correlation are about the same, giving us some assurance that the models are not over-fitting.

Figure 2 shows a scatter-plot of satisfaction versus the relevance score for the first position result of the first query in the session (misspelled queries excluded). Our simplest model is based only this first-position relevance. Interest-ingly, the resulting model ( Pos1 ) still produces a reasonable correlation to session satisfaction. The correlation is 0.722, lower than the correlation with the 1/position-weighted rel-evance mean of the top three positions (0.784), but strong enough to underscore the importance of the first position to users. The cross-validated correlation is slightly lower, at 0.714.

Of course, we can do better by using the relevance scores of all three top positions for the first query. This model (call it Pos1+Pos2+Pos3 ) produces a correlation of 0.786. This is almost identical to the correlation produced by our 1/position-weighted mean. At least for this dataset, weight-ing by 1/position is about as accurate as deriving weights Satisfaction Figure 2: Session satisfaction vs. relevance of first-position result from the satisfaction data itself. The cross-validation corre-lation is again slightly lower at 0.774.
In this section we investigate whether other query prop-erties help explain satisfaction above and beyond the rele-vance of the results. We look first at whether the query is identified as primarily navigational, informational, or trans-actional [4, 12], based on the dominant opinion among nine human judges per query.

Figure 3 shows scatter-plots of satisfaction versus rele-vance at each of the top three positions for both naviga-tional and non-navigational queries. Least-squares lines are superimposed. The plots show that for navigational queries, the relationship between relevance and satisfaction weak-ens rapidly after the first position. For non-navigational queries, on the other hand, relevance and satisfaction are Figure 3: Session satisfaction vs. relevance, for nav-igational and non-navigational queries Table 2: Summary of (Pos1 + Pos2+ Pos3)*Nav model strongly related at all three positions. The two  X  X utliers X  on the navigational side are rare queries which are navigational in nature but for which few relevant results are returned. Therefore, the queries score low both on relevance and on session satisfaction.

To model how the relationship between relevance and sat-isfaction may vary by type of query, we built a linear model (Pos1 + Pos2+ Pos3)*Nav , which estimates two coefficients for the relevance at each position: one for navigational queries, and one for non-navigational queries. This model produces an improved correlation of 0.807, and cross-validated corre-lation of 0.791.

The model statistics are summarized in Table 2. The coefficients with no suffix correspond to non-navigational queries, while the coefficients with a  X  X av X  suffix correspond to navigational queries. Notice that for navigational queries, the implicit weighting on the first result relative the the other two is much higher than for non-navigational queries. For non-navigational queries, the relative weighting is much flatter across the three results. This comports with our in-tuition that for navigational queries, what matters most is that the target result appears in the first position, while for other kinds of queries the information accumulated in later results is more important.

We investigated several other query categories (informa-tional, transactional, specific, broad, refers-to-entity) and found that they add little information beyond relevance in explaining user satisfaction. Figure 4 shows the scatter plot of satisfaction versus relevance, labeled with  X  X  X  X  for infor-Satisfaction Figure 4: Session satisfaction vs. relevance, with informational queries indicated Table 3: Summary of (Pos1 + Pos2+ Pos3)*Info model mational queries; figure 5 shows the same for transactional queries. The X X  X  are more or less randomly mixed in with the dots in both cases, demonstrating that informational/non-informational and transactional/non-transactional status of a query is not helpful beyond relevance in explaining satis-faction.

The regression output for model (Pos1 + Pos2 + Pos3)*Info corroborates this: the coefficients show only a slight (and not statistically significant) flattening of the implicit posi-tion weights for informational queries. To the extent there is any effect, it is because informational queries are non-navigational. When navigational queries are removed from the data, even this small effect disappears, as summarized in table 4.

Similar analysis shows that the other query categories (transactional, specific, broad, refers-to-entity) mentioned above are also uninformative for this dataset. We also tried adding simple query length (number of words) to the model, on the theory that perhaps the satisfaction profile is differ-ent for long versus short queries  X  a mechanical form of the specific versus broad distinction. This was also uninforma-tive.
Our best model so far, (Pos1 + Pos2+ Pos3)*Nav has a correlation of 0.8 with observed satisfaction. What explains the remaining variance? Satisfaction Figure 5: Session satisfaction vs. relevance, with transactional queries indicated Table 4: Summary of (Pos1 + Pos2+ Pos3)*Info model on non-navigational queries
Obviously, more happened in the user X  X  sessions than just the first query. The user sometimes did multiple searches, and often looked at multiple results before declaring herself  X  X one X  with the task and giving a satisfaction rating. A full model of satisfaction based on individual events could try to combine measurements of each event or page in a use session in some form of sequence model. We do not present such a model here, but we do have one rough proxy for what happened in these sessions after the initial search: we recorded the number of  X  X vents X   X  different pages viewed (whether search result pages or other URL X  X )  X  during each session.

Adding the number of events as a main effect yields the model (Pos1+Pos2+Pos3)*Nav+NumEvents ,whichincreases correlation to session satisfaction of 0.841, and cross-validated correlation to 0.825.

Figure 6 illustrates how the models provide increasingly accurate predictions of satisfaction as more variables are added.
Relevance metrics are useful in part because they enable repeated measurement. Once results for an appropriate sample of queries are graded, relevance metrics can be eas-ily computed for multiple ranking algorithms. In a changing document collection (like the web), new results that appear over time for queries in the sample need only be graded in-crementally in order to maintain the relevance measurement.
In contrast, approaches to measuring the broader search experience from the user X  X  perspective, such as usability studies, diary studies, and assigned-task studies, are inher-ently not repeatable. Once a particular subject does a par-ticular task, she generally cannot comparably repeat it, be-cause she has learned from doing it the first time. In addi-tion, the data derived from these studies is not incremental in the way that result relevance judgments are; repeating a measurement when the underlying document collection or search algorithms have incrementally changed requires re-running the full set of tasks with new subjects.

This work may be viewed step towards a repeatable per-query metric that is more strongly correlated to user satis-faction than pure relevance metrics. By adding query-level properties such as whether the query is navigational or mis-spelled, we were able to predict eventual user satisfaction more accurately than with resu lt relevance judgments alone. We improved the model further by adding the average num-ber of events in the rated sessi ons starting from the query. While in a repeatable metric context, rated sessions pre-sumably would not be available, search engine logs might be able to provide a proxy such as the average number of query refinements and result clicks associated with a given query over time. (Interestingly, in our models the number of events was found to be a better predictive variable than the raw session time, which is sometimes thought to be a strong predictor of user satisfaction.)
The other query properties we c onsidered (transactional vs. informational, specific vs. broad, and query length) were not significant predictors of satisfaction. However, our dataset was small. While it is safe to conclude these prop-erties and others have a small role in predicting satisfaction compared to a query X  X  status as misspelled or navigational, analysis of a larger data set might uncover significant effects for these or other query characteristics.

Predicting satisfaction might also be aided by taking more of the elements of the session into account: Downstream searches and the relevance of their results, the utility of ex-tra information/elements returned (stock quotes, weather, images, and so on), the number of result pages viewed after each search, and so on. That said, we were surprised by how well user satisfaction can be predicted just from first-query relevance and some query properties. Apparently, in web search first impressions matter a lot  X  even modeling on only the first result of the first query gives a reasonable cor-relation. Adding in the second and third results, the models suggest a plausible weighting among the results  X  steeply tilted towards the first result for navigational queries, and more flat for non-navigational.

We now offer a few caveats about the methodology em-ployed here. To begin with, we asked people who were not the original issuers of each query to produce likely primary and secondary statements of user intent/information need for the query. There is some evidence that people can infer the goals of queries without additional information. Rose and Levinson citerose had people manually classify queries by type of user goal, with and without additional informa-tion about the click behavior of the session the query was drawn from, and found no substantial differences. They concluded that  X  X lthough this requires further study, it sug-gests the surprising result that goals can be inferred with al-most no information about the user X  X  behavior. X  ([12], page 17). In our case, we reviewed the user intent statements and felt they were reasonable, but we cannot be sure of the true range of user intents. We also arbitrarily used only up to two intent statements per query, and for our analysis, we combined the mean satisfaction scores for primary and secondary intents in an ad-hoc way (70%/30%). Reliable attribution of user intents to randomly selected queries is a difficult problem which merits further study.

We chose to base our analysis on relevance judgments for the first query of the users X  sessions. The reason for this is that our interest lies in measuring search engine performance X  the first query of a session is very important from this per-spective. However, for sessions containing multiple searches, it may be more predictive of user satisfaction to consider an intermediate search or the final search performed. Kahne-man X  X  peak-end rule [10] suggests that the strongest pre-dictors of satisfaction are the peak (positive or negative) and final elements of an experience, regardless of duration. The strong correlation between relevance and satisfaction observed in this study might have been even stronger had we used the final query instead of the first.

Some previous studies have attempted to measure the im-pact of per-query relevance on user performance (as opposed to user satisfaction) for various kinds of constructed tasks. For example, Turpin and Scholer [17] artificially varied mean average precision of search results returned to a group of users, and asked the users to perform a precision-based task (find a single document) and a recall-based task (find as many relevant documents as possible within five minutes). They found only a weak relationship between relevance and performance on these types of tasks. Similarly, on a set of specific question-answering tasks, Turpin and Hersh [18] found a lack of correlation between mean average precision and performance. Allan et al. [1], in contrast, found that large improvements in retrieval accuracy (bpref) did improve subjects X  speed and effectiveness in constructing answers to multi-faceted questions.

Other researchers have suggested that metrics better re-flecting actual user behavior can be produced by measuring IR systems in the context of users performing tasks. For ex-ample, Reid [11] proposed evaluating IR systems based on retrieved documents X  utility to a task rather than relevance to a query, and Borlund [3] proposed a framework for evalu-ating interactive IR systems by placing evaluators into task scenarios.

Our focus in this work is on evaluating search engines as they are used in practice. This was our motivation for start-ing with a random sample of actual user queries. These queries are widely varied, and many of them are not ade-quately described as simple precision or recall tasks. In our view, the user is the ultimate arbiter of a search engine X  X  quality. If a relevance metric correlates well with user satis-faction, that is a strong vote in its favor, even if it correlates less well with narrower measures such as user performance on constructed tasks.
Our analysis is a step towards bridging the gap between relevance metrics and user satisfaction. We have demon-strated that this gap is not nearly as large as one might think, especially given that relevance metrics ignore all as-pects of UI. Moreover, we found at least one important di-mension (navigational/non-navigational) which modulates the relationship between satisfaction and relevance. This suggests that it might be reasonable to use different rele-vance metrics or discounting functions for different types of queries.

In our view, an important attribute of any relevance met-ric is the degree to which it represents user satisfaction. As we improve our understanding of the relationship between relevance and satisfaction, we will be be better able to dis-cover how well search engines meet the needs of their users.
Thanks to Dan Russell for helpful comments on earlier drafts of this paper.
 [1] J. Allan, B. Carterette, and J. Lewis. When will [2] D. Bilal. Children X  X  use of the Yahooligans! Web [3] P. Borlund. The IIR evaluation model: a framework [4] A. Broder. A taxonomy of web search. SIGIR Forum , [5] C. Buckley and E. M. Voorhees. Evaluating evaluation [6] C. Buckley and E. M. Voorhees. Retrieval evaluation [7] C. W. Cleverdon. The significance of the Cranfield [8] B. J. Jansen and U. Pooch. A review of web searching [9] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [10] D. Kahneman, P. P. Wakker, and R. Sarin. Back to [11] J. Reid. A task-oriented non-interactive evaluation [12] D. E. Rose and D. Levinson. Understanding user goals [13] D. M. Russell and C. Grimes. Assigned and self-chosen [14] M. Sanderson and J. Zobel. Information retrieval [15] A. Spink. A user-centered approach to evaluating [16] J. Teevan, C. Alvarado, M. S. Ackerman, and D. R. [17] A. Turpin and F. Scholer. User performance versus [18] A. H. Turpin and W. Hersh. Why batch and user [19] E. M. Voorhees. Evaluation by highly relevant
