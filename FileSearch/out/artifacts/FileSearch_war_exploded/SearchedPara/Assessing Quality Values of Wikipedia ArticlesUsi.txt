 Wikipedia 1 is one of the most successful and well-known User Generated Content (UGC) websites. It has more and fresher information than existing paper-based encyclopedias, because any user can edi t any article. Many experts submit texts, and texts submitted by them should be informative for all who read it. Therefore, as well as being very large, Wikipedia is also very important. However, a dramatic increase in the number of edi tors causes an increase in the number of low-quality articles. Kittur et al. [6] showed that about 78 . 6% of 147 , 360 articles had not reached  X  X tart X  status 2 . Therefore, automatic or semi-automatic systems should be developed to identify which part of article is high-quality and which is not.
In this paper, we propose a method to identify high quality texts using edit history. Here we define the word  X  X uality X  as the degrees of excellence. The defi-nition of quality has many aspects such as credibility, expertise, and correctness. Therefore, measuring excellence is difficult task. To solve this problem, we mea-sure the number of editors who consider the article excellent, which is one of the important aspects of quality. When ma ny editors consider excellent for an article, the quality of this article is high, but when a small number of editors consider the article excellent, the quality is low. In the latter case, even if only a small number of readers read the article, and these readers consider the article excellent, we decide that the quality of the article is low. This is because, there is a small number of evidence to decide wh ether the quality of the article is high or low.
 If editors find low quality texts, the edit ors generally reject and delete them. Adler et al. [2] investigate that the recall for bad-quality short-lived text is 79% . This means that if a text survives beyond multiple edits by the other editors, the text should be high-quality. Therefore, using the survival ratio of texts, the system calculates the quality value of a text.
 Example 1: Let us consider a motivating example. One editor e a writes a part p ( e a ) of an article. Then , another editor e b edits another part of this article, but keeps p ( e a ) intact. In this case, we assume e b remains p ( e a ) as it is because s/he judged p ( e a ) to be high-quality. Next, another editor e c deletes p ( e a ) .We assume that e c judged p ( e a ) to be low-quality, hence s/he deleted the text. As a result, the paragraph p ( e a ) is confirmed by e b , but not confirmed by e c .If e c had not delete p ( e a ) , the quality in this case would have been higher than that in the former case, because the paragraph p ( e a ) is trusted by one editor in the former case whereas it was trusted by two editors in the latter case. In this case, the survival ratio of p ( e a ) is 1 when e b edits, and 0 when e c edits. Therefore, the overall survival ratio of p ( e a ) is 0 . 5 .

In this method, when a text survives beyond multiple edits, the text is judged as high quality. However, the problem is that every editor does not always read the whole articles, then if there is low-quality text on long articles, the text is treated as high-quality. In this paper, to solve this problem, we introduce to use section and paragraph as a unit instead of the whole page. This means that if an editor edits an article, the system treats that the editor gives positive ratings to the section or the paragraph which the editor edits. This is because, we believe that if editors edit articles, the editors may not read whole pages but should read whole sections or paragraphs, and delete low-quality texts. Adler et al. [1 X 3], Hu et al. [5], Wilkinson et al. [10], and Suzuki et al. [8] pro-posed a method for calculating quality values from edit histories. These methods are based on survival ratios of texts and is similar to the basic idea described at section 3.2. In these methods, edit distance is used for measuring difference between old and new versions. In this ca se, the impact for text quality by dele-tion and that by remaining are the same. However, these impacts of these two operations should be different, because the editors can delete a text only once whereas they can remain a text many times. Therefore, if we treat deletion and remaining texts equivalently, we shoul d separately calculate the impacts from these operations, and then integrate after normalizing.
 Moreover, these methods proposed in the past do not use negative ratings. Therefore, when an editor writes many te xts, and these texts are deleted imme-diately, the editor of the text do not imposed a penalty, then the quality of the editor do not decrease. We believe that the quality value of this editor should be low, then we use both positive ratings and negative ratings. Our goal is to assess quality value of articles by mutually evaluating quality values of texts and editors. The process is as follows: 1. The system extracts edi t histories of articles 2. It calculates positive and negative ratings for editors from edit histories. 3. It calculates editor quality values by combining positive and negative ratings. 4. It calculates positive and negative ratings using editor quality values. 5. It calculates editor quality values using modified positive and negative 6. Repeat 4. and 5. until editor quality value converges. 7. Calculate version quality values using editor quality values. 3.1 Modeling In this section, we define notations that are used throughout this paper as shown in Fig. 1. On Wikipedia, every article has a version list V = { v i | i =0 , 1 ,  X  X  X  ,N } where i is the version number, and v N is the latest version. We denote that if i =0 , v 0 is a version with empty contents and no editor. If editor e in all editors E creates a new article, the sy stem makes two versions, v 0 and v 1 ,and then the system stores the text of editor e in v 1 which consist of one part p ( e ) . We identify editors using editor names, but anonymous editors have no editor name. In this case, we use the IP address instead of editor name. Then, we define version v i = { p ( e ) | e  X  E } as a set of complete parts that is stored at i -th edit and that consists of a text by 1 , 2 ,  X  X  X  ,i -th editors. p ( e ) is a part of article by editor e .If e deletes all texts from i -th version, v i is an empty set.
Editor e creates a set of parts P ( e )= { p ( e ) } where p ( e ) is a part created by e for all articles. If e does not add any texts to any articles, P ( e ) is an empty set. When editors edit one article by same u ser more than twice consecutively, the system keeps the last version and deletes the other versions created by the users. That is, the editor of a version and that of next version are always different.
The aim of our proposed method is calculating version quality value T ( v i ) of version v i . To accomplish our mission, we sho uld calculate converged parts quality value  X  K ( d, e ) of parts on article d by editor e , and converted editor is an initial editor quality value. In step 6. at section 3.3, we repeat calculating k -is the converged value of  X  k ( d, e ) and u k ( e ) . 3.2 Key Idea We show how to calculate quality values of articles using an example of edit history in Fig. 3. Using this example, we explain how to calculate quality values of parts p ( e 1 ) that are added by editor e 1 in version v 1 . First, we identify the texts that are added in version v 1 .Inthisexample,theeditor e 0 writes all texts of v 0 , and the editor e 1 adds the texts  X  X eshima X  and  X  X rime Minister X  as p ( e 1 ) to version v 2 . At this time, we suppose that e 1 gives positive and negative ratings to e 0 . e 1 remains 21 letters written by e 0 ( X  X oshihiko X ,  X  X s X ,  X  X he X ,  X  X f Japan X ), then e 1 gives 21 letters of positive ratings to e 0 . e 1 deletes 13 letters written by e 0 ( X  X oda X ,  X  X resident X ), then e 1 gives 13 letters of negative ratings to e 0 .
However, the problem of this method is that if e 1 edits small edits, e 1 writes only a small number of letters and remain all texts, the system decides that e 1 permits to remain almost all texts. We believe that all editors do not always read whole articles. Therefore, when ther e are many editors who do not read whole articles, the accuracy of quality value s should decrease. To solve this problem, we use section and paragraph as a reading unit. 3.3 Quality Value In this section, we describe how to calculate positive and negative ratings. Fig. 4 shows an example to explain how to calculate positive and negative ratings. First, we define a unit of article, section, and paragraph. A unit of whole article is defined as texts in the whole article. A unit of section is defined as the texts divided by symbols which indicate separation of sections. In this example, A and B belong to the same section, C and D b elong to different section. A unit of paragraph is defined as texts divided by special, not linguistic characters, such as HTML tags and line break code. In this example, A and B belong to different paragraph because A and B is divided by line break.

We describe intuitive explanation of positive ratings. In this example, editor e edits a part of article A. When we use whole article as a unit, we assume that e 1 permits to remain parts A, B, C, and D. Therefore, e 1 gives positive ratings to editors who write When we use section as a unit, we assume that e 1 permits to remain parts A and B. In this case, we suppose that e 1 do not read C and D because e 1 do not edit. When we use paragraph as a unit, we assume that e 1 permits to remain only A. In this case, we suppose that e 1 do not read B, C, and D.
We also describe explanation of negative ratings. In this example, editor e 1 deletes two parts of A. Then, we assume that e 1 do not permit to remain these two parts. Therefore, e 1 gives negative ratings to editors who write these two parts. However, the degree of positive/negative ratings of e 1 should depends on the quality value of e 1 . This means that if e 1 is a high-quality editor, the positive and negative ratings by e 1 should be large. To solve this problem, we mutually calculate quality values of parts and editors.
 Positive Ratings. Next, we calculate the quality values of parts using the quality values of editors.
 Whole Article as a Unit. We calculate the positive ratings using whole article  X  control the effect of editor quality value. u k  X  1 ( e ) is a quality value of e which is calculated by the method using whole article as a unit.

The first part of expression of this equation means the part quality values, value of parts, we use a log scale instead of the raw number of letters, because we face a problem when the editor adds long texts. If an editor adds 10 , 000 letters to the article, and the texts survive only one edit, this text quality value is 10 , 000 , which is the same quality value as a 100 -letter texts that survives beyond 100 edits. We think that the latter text is higher quality than the former text. Therefore, we count the number of characters using a log scale.

The second part of expression means the number of deleted letters with quality values of editors who delete them. If an editor who has a low-quality value deletes apart p ( e ) , then the value of the second expression is high, then the value of  X  does not affect the part quality value. In this case, if the editor who deletes the part has a high-quality value, the second expression has a high value. Thus, the value of  X  pw k ( d, e ) decreases more than  X  pw k  X  1 ( d, e ) .
 Section as a Unit. We calculate the positive ratings of parts using section  X  is a quality value of e which is calculated by the method using section as a unit. Paragraph as a Unit. We calculate the positive ratings of part using paragraph  X  is a quality value of e which is calculated by the method using section as a unit. Negative Ratings. We calculate the negative ratings  X  n k ( d, e ) as follows: where E n is a set of editors who delete p ( e )  X  N ,and u b k  X  1 ( e ) is a quality value of e which is calculated by the method using negative ratings. We set  X  ( d, e )= Quality Values of Parts. We define the quality values  X  k ( d, e ) of part of an article d by editor e using positive and negative ratings as follows: Quality Values of Editors Using Adjusted Quality Values of Parts.
 as follows: We normalize u k ( e ) to range between 0 and 1 as follows: We repeat the processes until the values of  X  k ( d, e ) and u k ( e ) converge. Quality Values of Versions. Using the converged value of u K ( e ) , we define the version quality value T ( v ) of version v as follows: and u K ( e ) is the editor quality value of e . This function means that the version quality value is the weighted averaging ratio of part quality values, and the weight is the number of letters in the parts. To determine the accuracy of the quality values calculated by our proposed system, we did experimental evaluation. In this evaluation, we tried to confirm that when we use editor quality values to calculate text quality values, the article quality values are accurate.

In this experiment, we compared 7 systems. 3 systems used only positive rat-ings; page is the system using article based, sec is the system using section based, and par is the system using paragraph based positive ratings. 1 system, delete , using only negative ratings. 3 systems used both positive and negative ratings; del+page used both article based positive ratings and negative ratings, del+sec used both section based positive ratings and negative ratings, and del+par used both paragraph based positive ratings and negative ratings.

We compared these systems using aver age precision ratio, which is an aver-aging value of precision ratios at each recall level [4]. We compared the answer set with the list of articles in ascending order of their quality values. If articles in the answer set are ranked higher, we will be able to confirm that the system calculates accurate quality values. The key in this evaluation is the appropriate-ness of answer sets. In current information system retrieval evaluation, observers create answer sets by judging relevance of articles. However, judging the qual-ity of articles is difficult, so we cannot confirm the appropriateness of quality judgments of articles. Therefore, we put featured and good articles selected by Wikipedia users in the answer set.

We set  X  to 0 . 7 used at equations from (1) to (2 ). Before these experiments, we set  X  from 0 . 1 to 0 . 9 in 0 . 1 increments and calculate averaging precision ratio as preliminary experiment. In this result, when we set 0 . 7 , we got the highest averaging precision ratio of our proposed system.
 4.1 Data Sets In this experiment, we used the Japanese version of Wikipedia edit history dumped on Jan. 4, 2012, which can be downloaded at the Wikipedia dump download site 3 . We randomly select 1 , 000 articles which contain 192 , 227 ver-sions. The number of editors is 65 , 909 including not registered editors who are identified by IP addresses, and bots which are listed 4 . When we select articles, we referred to Wikipedia statistics 5 to decide which articles we select. We do not select the articles that do not contains at least one link to Wikipedia articles. We also do not select the articles for specific purposes, such as redirect pages, notes, and rules of Wikipedia.

In this experiment, we set the answer set of  X  X eatured X  and  X  X ood X  articles as a correct answer set. Featured and g ood articles are selected by the votes of Wikipedia users (mainly readers). These articles are evaluated by  X  X eatured article criteria X  6 and peer reviewed by many acti ve users. If vandals nominate low-quality articles for featured or good articles, the nomination is rejected by administrators. Therefore, we believe that these articles are high-quality, so we could use featured and good articles as high-quality articles for the test set. The number of featured and good articles are 72 and 611 respectively. In our selected articles, the number of featured and good articles are 2 and 5 respectively. We decide these 7 articles as answer set. 4.2 Experimental Results and Discussions Fig. 5 shows an average precision ratio p er each repeated count. The meaning of each line is described at the top of s ection 4. From this graph, we unveil that del+par , the system which use paragraph based positive ratings and negative ratings calculates article quality values more accurately than the other methods. When we compare the results of par and delete , the order of articles dramatically changes, different high-quality articles have high quality values. Therefore, when we combine these positive and negative ratings, we can calculate more accurate quality values.

However, the repeats of calculation of positive/negative ratings and editor quality values are not effective in this e xperiment. From Fig. 5, when repeated count increases, average precision ratio increases at most 0.02%, almost 0%. The reason of this is that 61% of all editors edit only one article more than once. Therefore, when we construct bip artite graph of texts and editors, the graph is very sparse. When editors edit s mall number of articles, small number of the other editors review the articles , then the editors get small number of positive/negative ratings. To be concrete, second expressions of equation (1) almost equals to 0 , then if we repeated to calculate quality values of parts, these quality values do not change. Therefore, repeats of calculating editor quality values are not effective. This is because, in this experiment, we selected only 1 , 000 articles. If we calculate all articles at the Wikipedia, averaging precision should increase by repeating calculation of editor quality values. Wikipedia is the most popular and highest quality encyclopedia to be created by many editors. The information on Wikipedia keeps expanding, but its qual-ity is not proportional to its quantity. In this paper, we propose a method to identify high-quality articles mutually evaluating editors and text to improve the accuracy of quality values of versions.

In our method, we introduce a combinat ion of a peer-review based quality value calculation method and a link analysis method, which is based on quality values of editors and texts. Not all editors are honest, and many vandals attack Wikipedia by deleting high-quality texts. Using our proposed method, quality values of editors affect that of texts. Th erefore, if vandals delete high-quality texts, they do not affect the survival ratio of the texts, so the quality values of versions which are attacked by vandals do not decrease. As a result, we can calcu-late accurate quality values of parts, editors, and versions without the activities of vandals.

Moreover, in this paper, we tackled tw o problems. One problem is that every editors do not always read whole articles, then if there is low-quality text on long articles, the text is treated as high-quality. To solve this problem, we used section and paragraph as a unit instead of whole page. This means that if an editor edits an article, the system treats that the editor gives positive ratings to the section or the paragraph which the editor edits. This is because, we believe that if editors edit articles, the editors should read whole sections or paragraphs, and delete low-quality texts. Another problem is that if there is a low-quality editor, the editor writes texts many times , and the texts are deleted frequently, the editor is treated as high-quality. This is because, survival ratio based methods do not use features of negative ratings. To solve this problem, we used features of negative ratings. From experimental evaluation, we confirm that our proposed system can calculate accurate quality values if we use paragraph as a unit for positive ratings and we also use negative ratings.

Quality of information is becoming increasingly important in information re-trieval research field. An information ret rieval system retrieves the documents that are relevant to the user X  X  query, but the system is not concerned about whether the documents are high-quality or not. However, if the retrieved doc-uments are low-quality, they should not b e retrieved even if they are relevant. Therefore, as Toms et al. [9] already mentioned, when we combine an information retrieval systems with our proposed high-quality article retrieval system, we will develop an information retrieval system more accurate than current information retrieval systems.
 Finally, we describe t hree open problems: Vagueness of Quality Value: In this paper, we calculated quality values for editors described at section 3.3. However , this editor quality value is not always distinct because the frequency of editing is different for each editor. We suppose that if an editor rarely edits articles, the editor may just happen to obtain a high quality value, but vagueness of the editor quality value should be high. Therefore, we should develop a method to calculate vagueness of editor quality values that does not depend on editor quality value.
 Use of Natural Language Processing Techniques: In our proposed method, we do not analyze linguistic structures; we only count the number of letters in texts. A strong point of our proposed system is that it can adapt to different language versions of Wikipedia articles. However, a weak point is that it cannot use important features that come from linguistic features. In our experiment, we found that high-quality articles are always written in formal language. Moreover, Sabel et al. [7] says that text analysis is useful for calculating quality values. For example, if an editor changes  X  X  is a B. X  to  X  X  is not a B. X  the number of letters changes by only three, but the meanings of these sentences are completely different. Therefore, we should analyze texts using natural language analysis techniques for calculating survival ratio of texts.
 Scalability: We implemented our system on a single PC with two CPUs and a 16 GB memory. As a result, this system took about 20 minutes to analyze 1 , 000 articles on the Japanese version of Wikipedia. Therefore, to analyze the whole Wikipedia, we will need more and more calculation costs. If we use multiple clus-ter PCs and map/reduce frameworks such as Hadoop, we will reduce calculation time of article analysis. We will therefore work on developing systems that are more scalable. Acknowledgment. This work was partly supported by Japan Society for the Promotion of Science, Grants-in-Aid for Scientific Research (23700113).
