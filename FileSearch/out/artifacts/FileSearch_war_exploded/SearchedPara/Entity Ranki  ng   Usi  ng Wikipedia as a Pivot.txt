 In this paper we investigate the task of Entity Ranking on the Web. Searchers looking for entities are arguably better served by present-ing a ranked list of entities directly, rather than a list of web pages with relevant but also potentially redundant information about these entities. Since entities are represented by their web homepages, a naive approach to entity ranking is to use standard text retrieval. Our experimental results clearly demonstrate that text retrieval is effective at finding relevant pages, but performs poorly at finding entities. Our proposal is to use Wikipedia as a pivot for finding en-tities on the Web, allowing us to reduce the hard web entity ranking problem to easier problem of Wikipedia entity ranking. Wikipedia allows us to properly identify entities and some of their character-istics, and Wikipedia X  X  elaborate category structure allows us to get a handle on the entity X  X  type.

Our main findings are the following. Our first finding is that, in principle, the problem of web entity ranking can be reduced to Wi-kipedia entity ranking. We found that the majority of entity ranking topics in our test collections can be answered using Wikipedia, and that with high precision relevant web entities corresponding to the Wikipedia entities can be found using Wikipedia X  X   X  X xternal links X . Our second finding is that we can exploit the structure of Wikipedia to improve entity ranking effectiveness. Entity types are valuable retrieval cues in Wikipedia. Automatically assigned entity types are effective, and almost as good as manually assigned types. Our third finding is that web entity retrieval can be significantly improved by using Wikipedia as a pivot. Both Wikipedia X  X  external links and the enriched Wikipedia entities with additional links to homepages are significantly better at finding primary web homepages than anchor text retrieval, which in turn significantly improved over standard text retrieval.
 Categories and Subject Descriptors : H.3.4 [Information Storage and Retrieval] : Systems and Software X  performance evaluation (efficiency and effectiveness) General Terms :Experimentation, Measurement, Performance Keywords : Web Entity Ranking, Wikipedia
Entity ranking is the task of finding documents representing enti-ties of a correct type that are relevant to a query. Searchers looking for entities are arguably better served by presenting a ranked list of entities directly, rather than a list of web pages with relevant but also potentially redundant information about these entities. Search engines have started to develop special services for entity retrieval, e.g., Google Squared 1 and the Yahoo Correlator 2 . It is difficult to quantify which part of web searches are actually entity ranking queries. It is known however that a considerable fraction of web searches contains named entities [e.g., 21].

Just like in document retrieval, in entity ranking the document should contain topically relevant information. However, it differs from document retrieval on at least three points: i) returned doc-uments have to represent an entity, ii) this entity should belong to a specified entity type, and iii) to create a diverse result list an en-tity should only be returned once. The main goal of this paper is to demonstrate how the difficult problem of web entity ranking can often be reduced to the easier task of entity ranking in Wikipedia.
To be able to do web entity ranking, we need to extract structured information, i.e. does this page represent an entity, and of what type is this entity, from the unstructured web. One approach to use structure is to add structure to unstructured web pages, for example by tagging named entities. Another approach would derive implicit structure from the link structure on the web, using links and anchor text. On the web, it is however not so easy to define, identify and represent entities. Just returning the name of an entity will not satisfy users, they need to see some kind of proof that this entity is indeed relevant, and secondly, they may want to know more of the entity than just its name. Depending on the type of entity that we are looking for these problems can be more or less significant. Entities can be represented by many webpages, e.g. an  X  X fficial X  homepage, a fan page, a page in an online encyclopedia or database like Wikipedia, Amazon or IMDB, or the entry in a social network such as Facebook, Twitter, MySpace. A complete representation or profile of a web entity would consist of many pages. The goal of entity ranking however is not to find all pages related to one result entity, but to find all relevant entities which can then be represented by one well-chosen page.

What type of page can be considered representative depends on the entity type, or even the entity itself  X  in the absence of an  X  X f-ficial X  homepage for example, alternatives might need to be con-sidered. What would for example be the homepage of a historical person, or a chemical element? The major search engines can give http://www.google.com/squared http://sandbox.yahoo.com/correlator us some clues which pages are appropriate; for movies and act ors IMDB pages are among the top results, for well-known people i t is often a Wikipedia page, and for companies their official we bsite. Following the TREC 2009 entity ranking track, we will repres ent entities by their  X  X fficial X  homepage or their Wikipedia pag e. The latter is useful for entity types where no  X  X fficial X  homepag e exists.
Instead of structuring the web ourselves, we propose to expl oit the part of the web that is manually structured: in this paper, we limit our scope to Wikipedia. Wikipedia is an excellent stru ctured resource of entities; its structure can be used as follows. E ntities are Wikipedia pages, where the name of the entity is the title of the page, the content of the page is the representation of the en-tity. Each Wikipedia page is assigned to a number of categori es. These categories can be divided into topical, type, and admi nistra-tive categories. Administrative categories such as  X  X ages needing to be revised X  are merely there for administrative purposes . Top-ical categories such as  X  X arack Obama X  indicate that the pag e is related to this topic. The categories we are interested in ar e the type categories such as  X  X eople from Westminster X  or  X  X useu ms in Michigan X . These categories give us information about th e entity type. Since Wikipedia is an encyclopedia each entity is only repre-sented once and we do not have to worry about returning duplic ate entities.

Our proposal is to exploit Wikipedia as a pivot for entity ran k-ing. For entity types with a clear representation on the web, like living persons, organisations, products, movies, we will s how that Wikipedia pages contain enough evidence to reliably find the cor-responding web page of the entity. For entity types that do no t have a clear representation on the web, returning Wikipedia page s is in itself a good alternative. So, to rank (web) entities given a query we take the following steps: 1. Associate target entity types with the query 2. Rank Wikipedia pages according to their similarity with t he 3. Find web entities corresponding to the Wikipedia entitie s
First of all, we investigate whether the Web entity ranking t ask can indeed be effectively reduced to the Wikipedia entity ra nking task. Therefore, we have to answer the following two researc h questions: We use the results of the TREC 2009 Entity Ranking Track (base d on the Web including Wikipedia) and the INEX 2009 Entity Rank -ing Track (based on Wikipedia). We extend the INEX topics to t he Web to answer these research questions.

The second step of our approach corresponds directly to the s etup of the INEX entity ranking track, so we adopt a competitive ap -proach from this track for our experiments. However, the INE X setup assumed detailed knowledge of entity target type. Alt hough users might be able and/or willing to indicate a general targ et en-tity type along with their query, e.g., choosing from people , organ-isations, products, we cannot realistically expect users t o provide accurate Wikipedia categories; thousands of these categor ies exist, and they are only very loosely organised. So, we investigate the following two issues related to the second step of our approa ch:
Finally, we evaluate our complete entity ranking approach a nd compare it to alternative approaches that do not use Wikiped ia to answer the questions:
The paper is structured as follows. The next section discuss es related work on entity ranking. Section 3 analyzes the relat ions be-tween entities in Wikipedia and entities on the web. Section 4 then examines entity ranking on Wikipedia, seeking to exploit di fferent levels of entity types. In Section 5 we focus on Web entity ran king proper with and without the use of Wikipedia as a pivot. Final ly, in Section 6 we draw our conclusions.
Entity ranking has recently become a popular new task. It sta rted out with ranking entities of a specific type, for example pers ons in expert search [2]. The more general problem is to rank all kin ds of entities, e.g. persons, locations, organizations etc.

When working with different types of entities, often some me ch-anism is needed to recognize and classify entities. A framew ork to identify persons and organizations is introduced in [7]. Besides extracting entities they also try to determine relationshi ps between them. Named entity taggers such as [14, 15] have been develop ed to extract entities of different types from documents and ar e pub-licly available.

Little work has been done on classifying entity types of quer ies automatically. Instead of finding the category of the query, the ap-proach described in [27] seeks to find the most important gene ral entity types such as locations, persons and organizations. Their ap-proach executes a query and extracts entities from the top ra nked result passages. The entity type that can be associated with most of these extracted entities is assigned to the query. The maj ority of queries can be classified correctly into three top entity typ es.
Besides ranking entities, entities can be used to support ma ny other tasks as well. Entity models of entities are built and c lustered in [23]. A semantic approach to suggesting query completion s, which leverages entity and entity type information is propo sed in [20]. A formal method for explicitly modeling the dependenc y be-tween the named entities and terms which appear in a document is proposed in [22], and applied to an expert search task.

Several search engines provide the possibility of ranking e ntities of different types. The semantic search engine NAGA for exam -ple builds on a knowledge base that consists of millions of en tities and relationships extracted from Web-based corpora [18]. A graph-based query language enables the formulation of queries wit h ad-ditional semantic information such as entity types. The sea rch en-gine ESTER combines full-text on Wikipedia with ontology se arch in YAGO [6]. The interactive search interface suggests to th e user possible semantic interpretations of his/her query, there by blending entity ranking and ad hoc retrieval.

Wikipedia is used as a resource to identify a number of candid ate entities in [30]. A statistical entity extractor identified 5,5 million entities in Wikipedia and a retrieval index was created cont aining both text and the identified entities. Different graph centr ality mea-sures are used to rank entities in an entity containment grap h. Also a web search based method is used to rank entities. Here, quer y-to-entity correlation measures are computed using page cou nts re-turned by search engines for the entity, query and their conj unction. Their approaches are evaluated on a self-constructed test c ollec-tion. Both their approaches outperform methods based on pas sage retrieval.

A lot of entity ranking research has recently been done in con -text of the INEX and TREC evaluation fora. INEX has run an entity ranking track since 2006, using Wikipedia as the test col-lection [8, 11]. The INEX entity ranking track is set up as fol -lows. The document collection is a snapshot of the English Wi ki-pedia. For the tracks from 2006 to 2008 a snapshot from Wikipe dia from early 2006 containing 659,338 articles was used [12]. S ince then Wikipedia has significantly grown, and for the 2009 trac k a new snapshot of the collection is used. It is extracted in Oct o-ber 2008 and consists of 2.7 million articles [24]. A query to pic consists of a keyword query and one or a few target categories which are the desired entity types. A description and narrat ive are added to clarify the query intent. A topic looks as follow s:
Because the Wikipedia category structure is hierarchical a nd not applied consistently, relevant result entities do not alwa ys belong to one of the specified target categories. A result entity is o nly considered relevant if it belongs to a category similar or eq ual to one of the target categories.

Several approaches have been quite successful in exploitin g cat-egory information. Wikipedia categories are used by definin g sim-ilarity functions between the categories of retrieved enti ties and the target categories. The similarity scores are estimated bas ed on the ratio of common categories between the set of categories ass ociated with the target categories and the union of the categories as sociated with the candidate entities [29] or by using lexical similar ity of cat-egory names [28]. Random walks to model multi-step relevanc e propagation from the articles describing entities to all re lated enti-ties and further are used in [26]. After relevance propagati on, the entities that do not belong to a set of allowed categories are filtered out the result list. The allowed category set leading to the b est re-sults included the target categories with their child categ ories up to the third level. A probabilistic framework to rank entities based on the language modelling approach is presented in [3]. Their m odel takes into account for example the probability of a category occur-rence and allows for category-based feedback. Finally, in a ddition to exploiting Wikipedia structure i.e. page links and categ ories, [9] applies natural language processing techniques to improve entity retrieval. Lexical expressions, key concepts, and named en tities are extracted from the query, and terms are expanded by means of synonyms or related words to entities corresponding to sp elling variants of their attributes.

TREC introduced the Entity Ranking track in 2009 [5]. It make s use of the Clueweb collection Category B, which consists of a bout 50 million English-language web pages including the comple te Wi-kipedia. The task in this track was an entity relationship se arch task: given an entity (name and document id) and a narrative, find the related relevant entities. A query topic looks as follow s:
Three entity types are used in 20 topics, 6 topics are looking for persons, 11 topics for organizations, and 3 topics for pr oducts. Although this test collection is relatively small, it is the best data available to study our research questions. We will suppleme nt the results with results on other data wherever possible, in par ticular we have extended the INEX 2009 Entity Ranking track data to th e web [10].
 TREC participants have approached the task in two main steps . First, candidate entity names are extracted, using entity r eposito-ries such as Wikipedia, or using named entity recognizers. L ink information of the given entity can be used to make a first sele c-tion of documents. In a second step, candidate entity names a re ranked, and primary homepages retrieved for the top ranked e ntity names. The University of Glasgow method builds entity profil es for a large dictionary of entity names using DBPedia and comm on proper names derived from US Census data [19]. At query time, a voting model considers the co-occurrences of query terms an d enti-ties within a document as a vote for the relationship between these entities. Purdue University expands the query with acronym s or the full name of the source entity [13]. Candidate entities are s elected from top retrieved documents, heuristic rules are applied t o refine the ranking of entities.
In this section, we investigate our first group of research qu es-tions. What is the range of entity ranking topics which can be an-swered using Wikipedia? When we find relevant Wikipedia enti -ties, can we find the relevant web entities that correspond to the Wikipedia entities?
While the advantages of using Wikipedia or any other encyclo -pedic repository for finding entities are evident, there are still two open questions: whether these repositories provide enough clues to find the corresponding entities on the Web and whether they co ntain enough entities that cover the complete range of entities ne eded to satisfy all kinds of information needs. The answer to the lat ter ques-tion is obviously  X  X o X . In spite of the fact that Wikipedia is by far the largest encyclopedia in English X  X t contains 3,147,000 articles after only 9 years of existence; the second largest, Encyclo paedia Britannica, contains only around 120,000 articles X  X ikipe dia is still growing, with about 39,000 new articles per month in 20 09 We can therefore only expect that it has not yet reached its li mit as a tool for entity ranking. One of the most important factors i mped-en.wikipedia.org/wiki/Size_of_Wikipedia ing the growth of Wikipedia and also interfering with its pot ential to answer all kinds of queries looking for entities is the cri terion of notability used by editors to decide whether a particular entity is worthy of an article. There are general and domain specific no ta-bility guidelines 4 for entities such as people, organizations, events, etc. They are based on the principle of significant coverage i n re-liable secondary sources and help to control the flow of valua ble and potentially popular topics into Wikipedia. However, th e de-sire of the Wiki community to have also repositories for the e n-tities of lesser importance led to establishing side projec ts, like Wikicompany (  X  3,200 articles about organizations), Wikispecies (  X  150,000 articles about all species of life) or CDWiki (  X  500,000 articles about audio CDs).

In order to study how far we can go with Wikipedia only when looking for entities, we analyzed the list of relevant entit ies for 20 queries used in Entity ranking track at TREC 2009, see Table 1 . We found that 160 out of 198 relevant entities have a Wikipedi a page among their primary pages, while only 108 of them have a primary web page (70 entities have both). As not all primary W i-kipedia pages are returned by participants and judged, or Wi kipe-dia pages might have not existed yet when the ClueWeb collect ion was crawled (January/February 2009), we manually searched on-line Wikipedia (accessed in December 2009) for primary Wiki pe-dia pages for the 38 entites that had only primary web pages. A s a result, we discovered primary Wikipedia pages for a furthe r 22 entities. Those 16 entities that are not represented in Wiki pedia are seemingly not notable enough. However, they include all ans wers for 3 of 20 queries (looking for audio cds, phd students and jo ur-nals). Although the numbers of topics is small, the percenta ge of pages and topics that are covered by Wikipedia is promising. Top-ics can have no primary Wikipedia entities because no partic ipant found relevant entities, or they were not judged. For some to pics however, no primary entities will exist in Wikipedia, due to its en-cyclopedic nature. For example no relevant entities for the topic  X  X tudents of Claire Cardie X  will appear in Wikipedia, unles s one of these students becomes famous in some way, and meets the re -quirements to be included in Wikipedia. To cover this gap, ot her databases can be used; e.g., it has already been shown that US Cen-sus data can be used to derive common variants of proper names to improve web entity ranking [19].
After we found that there is a strong link from entities repre -sented on the Web (so, notable to a certain extent) to Wikiped ia, it was further important to find out whether the opposite rela tion also exists. If it does, it would prove that Wikipedia has the po-tential to safely guide a user searching for entities throug h the Web and serve as a viable alternative to a purely web-based searc h, con-sidering the immense size of the Web and the amount of spam it contains. Again, thanks to the Wikipedia community, those a rticles that follow the official guidelines are supposed to have an  X  X  xter-nal links" section, where the web pages relevant to the entit y should be enlisted. Moreover, it is stated that  X  X rticles about any organi-en.wikipedia.org/wiki/Wikipedia:Notability zation, person, website, or other entity should link to the s ubject X  X  official site" and  X  X y convention are listed first" 5 . In our case, 141 primary Wikipedia pages out of 160 (  X  88%) describing relevant entities had the  X  X xternal links" section. Actually, only 4 out of 19 entities described by Wikipedia pages with no  X  X xternal l inks" section had also the corresponding primary Web pages, what c an be explained by the fact that Wikipedia pages often serve as t he only  X  X fficial" pages for many entities (e.g. historical obj ects or non-living people).

In order to be sure that it is easy to discover a primary Web pag e by looking at these external links, we also analyzed how many of these links point to primary Web pages for the same entities.
In addition to the TREC entity ranking topics, we use INEX 2009 Entity Ranking topics. The topic set consists of 55 enti ty ranking topics, and each topic has at least 7 relevant entiti es. We have mapped the relevant wikipedia pages from the INEX Wikip e-dia collection to the Clueweb collection by matching on the p age title and found matches for 1,381 out of the 1,665 relevant pa ges. Differences occur because the INEX Wikipedia collection is ex-tracted from a dump in October 2008, while the TREC Wikipedia collection is crawled in January and February 2009. All link s from relevant Wikipedia pages to pages in Clueweb (Category B) ar e judged by the authors of this paper. The difference between t he TREC topics and the INEX topics is that the TREC topics are re-stricted to the entity types person, organization and produ ct, while the INEX topics can be virtually any entity type. The TREC gui de-lines define a primary homepage as devoted to and in control of the entity. For the entity types that cannot control a homepa ge, e.g. deceased persons or concepts like chemical elements, we tak e the second best thing: an authorative homepage devoted to the en tity. For some of these entity types the Wikipedia page could in fac t be considered the best primary page.

Unfortunately, not all web-sites linked from Wikipedia are in-cluded in the TREC ClueWeb collection (Category B). For the T REC topics 98 out of 141 primary Wikipedia pages had at least one linked web-site in the collection and only 60 of them describ ed en-tities for which a primary Web page was found as well. At the sa me time, in 52 of these cases (  X  87%) at least one primary Web page was linked from the corresponding Wikipedia page. Moreover , in 4 out of the 8 unsuccessful cases another page from the primar y web page X  X  domain was linked. In the case, when we considered only the first external link in the list, 43 of 46 links pointin g to an existing page in the collection actually pointed to the prim ary Web page of the respective entity.

Looking at the INEX topics we find comparable numbers, but on a larger scale. Most relevant Wikipedia pages have extern al links (72%), but only a relatively small number of these exte rnal links point to pages in the Clueweb category B collection, i. e for 289 pages a total of 517 external links are found. Compared to the TREC topics, for INEX topics a smaller percentage of the e x-ternal links are indeed relevant primary pages, of all exter nal links 37% are relevant, of the first external links a respectable 77 % of the pages is relevant. Comparing the TREC and the INEX topics , we see that the relevance of all external links is much higher for the TREC topics than for the INEX topics, and the relevance of the first links is also lower for the INEX topics. The TREC topics conta in only 14 links below rank one that are judged, so we cannot real ly say much here about the relevance of links below rank one. The INEX topics however are more substantial, and present a clea r dif-ference between the first external link, and the lower ranked links. Out of the 361 links below rank one, only 69 are deemed relevan t. en.wikipedia.org/wiki/Wikipedia:External_links
Topic Set TREC 2009 INEX 2009 # Rel. Wiki. pages 160 1381 -with external links 141 (88%) 994 (72%) -with external Clueweb links 88 (55%) 289 (21%) # Judged ext. links 60 517 -relevant links 52 (87%) 189 (37%) # Judged first ext. links 46 156 -relevant first links 43 (93%) 120 (77%) Most of these relevant links are found for entities which hav e in-deed more than one primary homepage, for example organisati ons that link to several corporate homepages for different regi ons.
Furthermore, the TREC topics are designed to have at least so me primary homepages in the Clueweb Category B collection, oth er-wise the topic wouldn X  X  have made it into the test set. Also th e entity types restriction to products, persons and organisa tions is making these topics more likely to have easily identifiable p rimary homepages. For the less restricted INEX topics primary home pages are harder to find, moreover these pages might not be consider ed entities by the Wikipedia editors, which alleviates their n eed to link to a primary homepage.

To validate that primary web pages would not be so easily dis-covered without the Wikipedia  X  X xternal links X  section, we first measured Mean Reciprocal Rank (MRR) of the first primary web page which we find using the ranking naturally provided in the  X  X x-ternal links X  section. We also measured MRR for the ranking w hich we get by using entity names as queries to search anchor text i ndex built for ClueWeb collection (category B). We experimented with 60 entities from the TREC topics that have a Wikipedia page, a t least one primary Web page and at least one linked web-site ex -isting in the ClueWeb collection. Indeed, using  X  X xternal l inks X  is much more effective for primary web page finding ( M RR = 0 . 768 ) than using an anchor text index ( M RR = 0 . 442 ).
In this section, we investigated whether the hard problem of web entity ranking can be in principle reduced to the easier prob lem of Wikipedia entity ranking. We found that the overwelming maj ority of relevant entities of the TREC 2009 Entity ranking track ar e rep-resented in Wikipedia, and that 85% of the topics have at leas t one Wikipedia primary page.

We also found that with high precision and coverage relevant web entities corresponding to the Wikipedia entities can be found using Wikipedia X  X   X  X xternal links X , and that especially th e first ex-ternal link is a strong indicator for primary homepages.
In this section we investigate our second group of research q ues-tions. Can we exploit category information to improve entit y rank-ing queries? Can we automatically assign entity types to nat ural language queries? We conduct a range of experiments with ent ity ranking in Wikipedia, seeking to exploit entity type inform ation.
In the TREC and INEX entity ranking tracks entity types are assigned by the topic creators. In practice however, it has p roven difficult to convince users to submit more than a few keywords as a query. Common web users hardly ever use even simple structur ed queries. It is therefore an unlikely user scenario that a use r will come up with a keyword query and a specific targeted entity typ e. So, we will examine whether we can assign an entity type to a qu ery automatically.

There are many ways to automatically categorize topics, for ex-ample by building language models of each entity type and cal -culating KL-divergence between the query and/or top retrie ved re-sults. Here, we keep it simple and exploit the existing Wikip edia categorization of documents. Pseudo-relevance feedback o f the top retrieved documents is used, but instead of extracting the m ost fre-quently occurring terms from the top ranked documents as is d one in standard pseudo-relevance feedback, we extract the cate gories that are most frequently assigned. From our baseline run, we take the top 10 results, and look at the 2 most frequently occurrin g cat-egories belonging to these documents. Categories that occu r only once are excluded. These parameter settings lead to good res ults in previous similar experiments [17]. The categories are as signed as target entity types to the query topic. This entity type as sign-ment method will lead to specific entity types, since these ar e the categories that are assigned to pages. More general categor ies are more loosely connected to the pages. Due to the category stru cture of Wikipedia, which is an undirected graph, rather than a tre e, it is difficult to use the hierarchical structure to assign gene ral entity types.
To exploit entity type information we calculate for the top r anked documents in our initial ranking an entity type score, using a lan-guage modeling approach [16]. The initial ranking is based s olely on the likelihood of the query terms occurring in the documen t. This probability is calculated using a language model with J elinek-Mercer smoothing with uniformly distributed prior documen t prob-abilities: where q 1 , ..., q n are the query terms, d is the document, and D is the entire Wikipedia document collection, which is used to e stimate background probabilities.

The entity type score corresponds to the similarity between the target entity types and the document entity type. Entity typ es are represented by the names of Wikipedia categories. To calcul ate entity type scores, first of all we make a maximum likelihood e sti-mation of the probability of a term occurring in a category na me. To avoid a division by zero, we smooth the probabilities of a t erm occurring in a category name with the background collection : where t 1 , ..., t n are the terms in C , the name of the category.
To calculate the similarity between two categories we use KL -divergence as follows: where d is a document, i.e. an answer entity, C t is a target category and C d a category assigned to a document . The entity type score for a document in relation to a query topic ( S cat ( d | QT ) ) is the maximum of the scores of all target and document categories:
We use a standard method for score normalization that takes t he standard deviation of score into account, the Z-score. Scor es are normalized to the number of standard deviations that are hig her (or lower) than the mean score. The mean and standard deviati on depend on the length of the ranking. It was shown in [1] that th is is a simple, yet effective method to normalize retrieval score s.
Finally, a linear combination of the normalized scores is ma de to calculate the final score:
In this section we investigate the effects of using manually as-signed versus automatically assigned entity types. We use t he Indri search engine [25] for our experiments. We have created inde x of the Wikipedia test collection of INEX applying the Krovetz s tem-mer. Our baseline model is a language model using Jelinek-Me rcer smoothing with a collection  X  of 0.15.

We use topic sets from TREC and INEX. Entity types can be defined on many levels, from general types such as  X  X erson X  or  X  X r-ganisation X  to more specific types such as  X  X lympic medalist s X  or  X  X hoe shops X . When entity ranking is restricted to few gener al entity types, specific rankers for entity types could be designed. T o rank people for instance, people-specific attributes and models could be used [4]. We would however prefer a generic approach that is e f-fective for all types of entities. The entity types of the INE X entity ranking track are quite specific. Some examples of entity typ es are countries, national parks, baseball players, and scien ce fiction books. The TREC entity ranking track uses only three general en-tity types, i.e. people, organisations, and products. The a dvantages of these entity types are that they are clear, there are few op tions and could be easily selected by users. The disadvantage is th at they only cover a small part of all possible entity ranking querie s. To make our test set more consistent we manually assigned more s pe-cific entity types to the TREC entity ranking topics so that th ey are on the same level as the INEX entity types.

Another difference between the tracks is that the TREC entit y ranking task was entity relationship search, i.e. answer en tities should be related to a given entity. For this paper we do not us e the given website of the entity, but we add the entity name to t he narrative. Together the entity name and the narrative serve as our keyword query. By not using the given entity, we can consider this task as an entity ranking task.

Entity type information is used as described in section 4.1. 2. We rerank the top 2,500 results of the baseline run using two set s of entity type information:
For evaluation we look at P10 and NDCG for the 20 2009 TREC topics and P10 and MAP for INEX topics. We use INEX topics 2006-2008 consisting of 79 topics and INEX 2009 topics, cons ist-ing of a selection of 55 topics from the 2006-2008 topics. The main difference between the INEX runs is the version of the Wikipe dia collection. For the evaluation, we count only the pages that are rele-vant and of the correct entity type. For the TREC topics, this means we only count the so-called  X  X rimary X  pages, i.e. authorita tive or of-ficial homepages of an entity. Pages that contain relevant in forma-tion, but are not primary homepages are judged as  X  X elevant X  pages in the official qrels. We are only interested in the primary pa ges, Significance of increase or decrease over baseline accordin g to Table 4: Wikipedia retrieval results on INEX 2006-2008 topi cs Table 5: Wikipedia retrieval results on INEX 2009 topics which represent an answer entity, and not in the relevant pag es, so we only give credit to the primary pages. We remove redirecte d Wi-kipedia pages from our runs and from the assessments, and rep lace them where possible with the correct, non-redirected page.
The results of our experiments expressed in the number of re-trieved relevant pages, P10 and NDCG@R or MAP are summa-rized in Tables 3, 4 and 5. These show the results of our baseli ne run and different entity type sets over different values of  X  , where  X  is the weight of the query score and (1  X   X  ) is the weight of the KL-divergence category score.

The runs with automatically assigned entity types reach a pe rfor-mance close to the manually assigned topics. Although P 10 is low in the baseline run, the 10 top ranked documents do provide he lp-ful information on entity types. Most of the automatic assig ned categories are very specific, for example  X  X ollege athletic s confer-ences X  and  X  X merican mystery writers X . For one topic the cat egory exactly fits the query topic, the category  X  X efferson Airpla ne mem-bers X  covers exactly query topic  X  X embers of the band Jeffer son Airplane X . Unsurprisingly, using this category boosts per formance significantly. The category  X  X iving people X  is assigned to s everal of the query topics that originally also were assigned entit y type  X  X ersons X . This category is one of the most frequently occur ring categories in Wikipedia, and is assigned very consistently to pages about persons. In the collection there are more than 400,000 pages that belong to this category. This large number of occurrenc es how-ever does not make it a less useful category.

The relative improvements from using category information are similar in the two INEX runs, as well as in the TREC run, with maximum respective improvements in P10 of 46%, 58% and 45% for Tables 3, 4 and 5 respectively. Our INEX runs with the manu al assigned categories achieve performance similar to the sta te of the art INEX entity ranking approaches.

The score for the TREC topics are much lower than the scores for the INEX topics. There are several explanations for thes e low scores. First of all, the TREC runs contain a lot of unjudged p ages, since none of these runs were official submissions to the trac k and they are not in the assessment pool. Only around half the page s in the top 10 are judged, over all results around 15 to 20% of th e pages are judged. The results on the TREC topics are an under-estimation of the performance of our approach. Results clos er to the INEX runs should be achievable. More judging, or differe nt evaluation measures are needed to get more reliable estimat ions of performance. The second reason for the low scores could be th e nature of the original task, which was entity relationship s earch. By not using the given entity, we lose information that could help. For example, the outgoing links of the example entity can be u sed to generate a set of candidate documents.

Since the Wikipedia collection used in INEX 2009 is very simi lar to the Clueweb Wikipedia pages, scores comparable to the INE X 2009 topics should be achievable. The new Wikipedia collect ion is a lot larger than the old collection, and although the qualit y of the pages improves, the collection becomes less focused.

In this section we examined the value of entity type informa-tion for entity retrieval in Wikipedia. We found that entity types are valuable retrieval cues. Automatically assigned entit y types are effective, but less so than the manually assigned types. The gen-eral conclusion is that we can exploit the structure of Wikip edia to significantly improve entity ranking effectiveness. In this section we examine our third group of research questi ons. Can we improve web entity retrieval by using Wikipedia as a pi vot? Can we automatically enrich Wikipedia with additional link s to homepages of found entities? We compare our entity ranking a p-proach of using Wikipedia as a pivot to the baseline of full-t ext retrieval.
This experimental section consists of two parts: in the first part we discuss experiments with the TREC Entity Ranking topics, in the second part we discuss experiments with the INEX topics t hat we extended to the web.

Again, we use the Indri search engine [25]. We have created separate indexes for the Wikipedia part and the Web part of th e Clueweb Category B. Besides a full text index we have also cre -ated an anchor text index. On all indexes we applied the Krove tz stemmer, and we generated a length prior. All runs are create d with a language model using Jelinek-Mercer smoothing with a coll ec-tion  X  of 0.15.

Our baseline run uses standard document retrieval on a full t ext index. The result format of the TREC entity ranking runs diff ers from the general TREC style runs. One result consists of one W i-kipedia page, and can contain up to three webpages from the no n-Wikipedia part of the collection. The pages in one result are sup-posed to be pages representing the same entity.

For our baseline runs we do not know which pages are repre-senting the same entity. In these runs we put one homepage and one Wikipedia page in each result according to their ranks, t hey do not necessarily represent the same entity. The Wikipedia based runs contain up to three homepages, all on the same entity. Wh en a result contains more than one primary page, it is counted as only one primary page, or rather entity found.
 We have three approaches for finding webpages associated wit h Wikipedia pages. 1. External links: Follow the links in the External links sec tion 2. Anchor text: Take the Wikipedia page title as query, and re -3. Combined: Since not all Wikipedia pages have external lin ks,
Our second part of experiments describes our runs with the IN EX topics that we extended to the web. Instead of using the TREC entity ranking style evaluation, with results consisting o f multiple pages in one result, we use a simpler evaluation with one page per result. Therefore we can use the standard evaluation script s to cal-culate MAP and P10.
Recall from the above that the ultimate goal of web entity ran k-ing is to find the home-pages of the entities (called primary h ome-pages). There are 167 primary home-pages in total (an averag e of 8.35 per topic) with 14 out of the 20 topics having less than 10 primary homepages. In addition, the goal is to find an entity X  s Wi-kipedia page (called a primary Wikipedia page). There are in total 172 primary Wikipedia pages (an average of 8.6 per topic) wit h 13 out of the 20 topics having less than 10 primary Wikipedia ent ities.
The results for the TREC Entity Ranking track are given in Ta-ble 6. Our baseline is full text retrieval, which works well ( NDCG 0.2394) for finding relevant pages. It does however not work w ell for finding primary Wikipedia pages (NDCG 0.1184). More impo r-tantly, it fails miserably for finding the primary homepages : only 6 out of 167 are found, resulting in a NDCG of 0.0080 and a P10 of 0.0050. Full text retrieval is excellent at finding relevant informa-tion, but it is a poor strategy for finding web entities.
We now look at the effectiveness of our Wikipedia-as-a-pivo t runs. The Wikipedia runs in this table use the external links to find homepages. The second column is based on the baseline Wik i-pedia run, the third column is based on the run that uses the ma nual categories that proved effective for entity ranking on Wiki pedia in Section 4. Let us first look at the primary Wikipedia pages. We see that we find more primary Wikipedia pages, translating in to a significant improvement of retrieval effectiveness (up to a P10 of 0.1700, and a NDCG of 0.1604). Will this also translate into fi nd-ing more primary home pages? The first run is a straighforward run on the Wikipedia part of ClueWeb, using the external links to the Web (if present). Recall that, in Section 3, we already estab lished that primary pages linked from relevant Wikipedia pages hav e a high precision. This strategy finds 29 primary homepages (so 11 more than the baseline) and improves retrieval effectivene ss to an Run Link Cat+Link Rel. WP 73 73 -57  X  NDCG Rel. WP 0.2119 0.2119 -0.1959 -Primary WP 78 78 -96  X   X  P10 pr. WP 0.1200 0.1200 -0.1700  X  P10 pr. All 0.1200 0.1300 -0.1850  X   X  NDCG pr. WP 0.1184 0.1184 -0.1604  X   X  NDCG pr. HP 0.0080 0.0292 -0.0445  X  NDCG pr. All 0.1041 0.1292 -0.1610  X   X  Run Cat+Link Anchor Comb.
 Rel. HP 70 127 137 Rel. All 121 178 188 NDCG Rel. HP 0.0830 0.0890 0.1142 NDCG Rel. All 0.1542 0.1469 0.1605 Primary HP 34 29 56 Primary All 130 125 152 P10 pr. HP 0.0400 0.0450 0.0550 P10 pr. All 0.1850 0.1750 0.1850 NDCG pr. HP 0.0445 0.0293 0.0477 NDCG pr. All 0.1041 0.1472 0.1610 NDCG of 0.0292, and a P10 of 0.0300. 6 The second run using the Wikipedia category information improves significantly to 3 4 pri-mary homepages and a NDCG of 0.0445 and a P10 of 0.0400.
Recall again from Section 3 that the external links have high precision but low recall. We try to find additional links betw een retrieved Wikipedia pages and the homepages by querying the an-chor text index with the name of the found Wikipedia entity (i .e., the title of the Wikipedia page). This has no effect on the fou nd Wikipedia entities, so we only discuss the primary homepage s as presented in Table 7. Ignoring the existing external links, searching for the Wikipedia entitities in the anchor text leads to 29 pr imary homepages. The combined run supplementing the existing ext ernal links in Wikipedia with the automatically generated links, finds a total of 56 primary homepages. For homepages this improves t he P10 over the baseline to 0.0550, and NDCG to 0.0447.

Our second part of the web experiments uses the INEX topics mapped to the Clueweb collection with our additional judgme nts for the Clueweb web pages not in Wikipedia. Results can be fou nd in Table 8. Although the assessments for the Wikipedia pages are fairly complete, since they are mapped from the official INEX as-
Unfortunately, we suffer from relatively few primary pages per topic X  X ess than 10 for the majority of topics X  X nd many unjud ged pages for these runs. The baseline anchor text run has 100% of primary HPs and 66% of primary WPs judged in the top 10, but the Wikipedia Links run has only 45% and 53%, respectively, judg ed. For some of the runs discussed below this goes down to 22% of the top 10 results judged. With these fractions of judged pag es, all scores of runs not contributing to the pool are underestimat es of their performance.
 Run Link Cat+Link Primary WP 763 763 -780 -P10 pr. WP 0.2018 0.2018 -0.2673  X   X  MAP pr. WP 0.1229 0.1229 -0.1633  X   X  sessments, for the web entities we are restricted to web page s oc-curring in the Clueweb collection. The INEX topics were not s e-lected to lead to entities with homepages in the particular C lueWeb collection, so many relevant entities in Wikipedia have no k nown homepage in ClueWeb. On the negative side, this will make our scores on Wikipedia entities higher than on Web homepages. O n the positive side, the 15% of Wikipedia entities with known h ome-pages in ClueWeb substantially extend the TREC data.

Our full-text baseline run achieves poor results. While a fu ll-text run works fine on the restricted Wikipedia domain, on the Web i t does not succeed in finding primary homepages, also relative to the known homepages in ClueWeb. Again we find that exploiting the Wikipedia category information consistently improves the results for finding primary Wikipedia pages as well as primary homepa ges. Since there are more primary Wikipedia pages than homepages , the Wikipedia scores are the highest overall. In contrast to the TREC entity ranking runs previously discussed in this section, e ach result consists of only one page. Since we are better at finding prima ry Wikipedia pages, we could construct better overall runs, by simply ranking the Wikipedia pages higher than the web pages. Depen ding on your goal, you could choose to show a ranking that is less di verse and shows only or primarily Wikipedia results, but contains more relevant documents.

Summarising the section, we examined whether web entity re-trieval can be improved by using Wikipedia as a pivot. We foun d that full text retrieval fails miserably at finding primary h omepages of entities. Full text retrieval on Wikipedia, in contrast, works rea-sonable, and using Wikipedia as a pivot by mapping found Wiki pe-dia entities to the Web using the external links leads to many more primary homepages of entities being found. We also investig ated whether we could supplement the external links with homepag es found by searching an anchor text index for the retrieved Wik i-pedia entities, and found that this leads to a significant imp rove-ment over just using Wikipedia X  X  external links for finding p rimary homepages of entities.
This paper investigates the problem of entity retrieval. A n atural baseline for entity retrieval is standard full text retriev al. While this baseline does find a considerable number of relevant pages, i t is not able to locate the primary homepages, which is the main goal o f our entity ranking task. The text retrieval runs fare much bette r at find-ing Wikipedia pages of relevant entities, hence prompting t he use of Wikipedia as a pivot to find the primary web homepages of ent i-ties. Our experiments show that our wikipedia-as-a-pivot a pproach outperforms a baselines of full-text search. Both external links on Wikipedia pages, and searching an anchor text index of the we b are effective approaches to find homepages for entities represe nted by Wikipedia pages.

The approach is based on three assumptions: i) the coverage o f entities in Wikipedia is large enough; ii) we are able to find e ntities in Wikipedia, iii) we can map Wikipedia entities to the appro priate web home pages. We have shown that the coverage of topics in Wikipedia is large, and Wikipedia is constantly growing. Th e ex-ternal links on Wikipedia pages are almost always authorita tive or official homepages of the entity. We also demonstrated that a large fraction external links in Wikipedia are relevant web homep ages. Besides the external links, querying an anchor text index fo r entity names is also effective. The combination of these two approa ches leads to additional improvements. Considering entity type s, au-tomatically assigned target entity types are almost as effe ctive as manually assigned entity types. Entity type information im proves retrieval scores considerably, up to 50% improvement rates .
Our future work will examine how alternative knowledge sour ces could complement Wikipedia X  X  role as a pivot for those infor ma-tion needs involving entities not well represented there. A nalysis of search log queries is needed to study more extensively the cov-erage of Wikipedia concerning different types of entities. If we can find relevant entities in other knowledge sources, we can use these as pivots as well, and identify relevant homepages by using a n an-chor text index. Search log queries and clicks, which are cur rently unavailable, can be used in a similar way as we use the anchor text X  X eading to further improvements. Our broad conclusio n is that is it viable to exploiting the available structured inf ormation in Wikipedia and other resources, to make sense of the great amo unt of unstructured information on the Web.

Acknowledgments The created Entity Ranking topics test col-lection is available at http://staff.science.uva.nl/~ka mps/effort/data. This research was supported by the Netherlands Organizatio n for Scientific Research (NWO, under project # 612.066.513).
 [1] A. Arampatzis and J. Kamps. A signal-to-noise approach t o [2] K. Balog. People Search in the Enterprise . PhD thesis, Uni-[3] K. Balog, M. Bron, and M. de Rijke. Category-based query [4] K. Balog and M. de Rijke. Determining expert profiles (wit h [5] K. Balog, A. de Vries, P. Serdyukov, P. Thomas, and T. West -[6] H. Bast, A. Chitea, F. Suchanek, and I. Weber. ESTER: effi-[7] J. G. Conrad and M. H. Utt. A system for discovering re-[8] A. de Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, [9] G. Demartini, C. S. Firan, T. Iofciu, R. Krestel, and W. Ne jdl. [10] G. Demartini, T. Iofciu, and A. de Vries. Overview of the [11] G. Demartini, A. P. Vries, T. Iofciu, and J. Zhu. Overvie w [12] L. Denoyer and P. Gallinari. The Wikipedia XML Corpus. [13] Y. Fang, L. Si, Z. Yu, Y. Xian, and Y. Xu. Entity retrieval [14] J. R. Finkel, T. Grenager, and C. Manning. Incorporatin g [15] T. G X tz and O. Suhre. Design and implementation of the [16] D. Hiemstra. Using Language Models for Information Re-[17] R. Kaptein, M. Koolen, and J. Kamps. Using Wikipedia cat -[18] G. Kasneci, F. M. Suchanek, G. Ifrim, M. Ramanath, and [19] R. McCreadie, C. Macdonald, I. Ounis, J. Peng, and R. L. T . [20] E. Meij, P. Mika, and H. Zaragoza. An evaluation of entit y and [21] M. Pa  X sca. Weakly-supervised discovery of named entit ies [22] D. Petkova and W. B. Croft. Proximity-based document re pre-[23] H. Raghavan, J. Allan, and A. Mccallum. An exploration o f [24] R. Schenkel, F. M. Suchanek, and G. Kasneci. Yawn: A se-[25] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indr i: [26] T. Tsikrika, P. Serdyukov, H. Rode, T. Westerveld, R. Al y, [27] D. Vallet and H. Zaragoza. Inferring the most important types [28] A.-M. Vercoustre, J. Pehcevski, and J. A. Thom. Using wi ki-[29] A.-M. Vercoustre, J. A. Thom, and J. Pehcevski. Entity r ank-[30] H. Zaragoza, H. Rode, P. Mika, J. Atserias, M. Ciaramita , and
