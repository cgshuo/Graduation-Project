
Data quality is a serious concern in every data manage-ment application, and a variety of quality measures have been proposed, e.g., accuracy, freshness and completeness, to capture common sources of data quality degradation. We identify and focus attention on a novel measure, column het-erogeneity , that seeks to quantify the data quality problems that can arise when merging data from different sources. We identify desiderata that a column heterogeneity mea-sure should intuitively satis fy, and describe our technique to quantify database column heterogeneity based on using a novel combination of cluster entropy and soft clustering . Finally, we present detailed experimental results, using di-verse data sets of different types, to demonstrate that our approach provides a robust mechanism for identifying and quantifying database column heterogeneity.
Data quality is a serious concern in every data man-agement application, severely degrading common business practices  X  industry consultants often quantify the adverse impact of poor data quality in the billions of dollars an-nually. Data quality issues have been studied quite exten-sively in the literature (e.g., [5, 10, 3]). In particular, a vari-ety of quality measures have been proposed, e.g., accuracy, freshness and completeness, to capture common sources of data quality degradation [13, 17]. Data profiling tools like Bellman [6] compute concise summaries of the values in database columns, to identify various errors introduced by poor database design; these include approximate keys (the presence of null values and defaults in a column may re-sult in the approximation) and approximate functional de-pendencies in a table (possibly due to inconsistent values). This paper identifies and focuses attention on a novel mea-sure, column heterogeneity , that seeks to quantify the data quality problems that can arise when merging data from dif-ferent sources.

Textbook database design teaches that it is desirable for a database column to be homogeneous, i.e., all values in a column should be of the same  X  X emantic type X . For exam-ple, if a database contains email addresses, social security numbers, phone numbers, machine names and IP addresses, these semantically different types of values should each be represented in separate columns. For example, the column in Figure 1(a) contains only email addresses and is quite ho-mogeneous, even though there appears to be a wide diver-sity in the actual set of values present. Such homogeneity of database column values has obvious advantages, including simplicity of application-level code that accesses and mod-ifies the database.

In practice, operational databases evolve over time to contain a great deal of  X  X eter ogeneity X  in database col-umn values. Often, this is a consequence of large scale data integration efforts that seek to preserve the  X  X truc-ture X  of the original databases in the integrated database, to avoid having to make extensive changes to legacy appli-cation level code. For example, one application might use email addresses as a unique cust omer identifier, while an-other might use phone numbers for the same purpose; when their databases are integra ted into a common database, it is feasible that the CUSTOMER ID column contains both email addresses and phone numbers, both represented as strings, as illustrated in Figure 1(b). A third independently developed application that used, say, social security num-bers as a customer identifier might then add such values to the CUSTOMER ID column, when its database is in-tegrated into the common dat abase. As another example, two different inventory applications might maintain ma-chine domain names (e.g., abc.def.com) and IP addresses (e.g., 105.205.105.205) in the same MACHINE ID column for the equivalent task of identifying machines connected to the network. While these exa mples may appear  X  X atural X  since all of these different types of values have the same function, namely, to serve as a customer identifier or a ma-chine identifier, potential data quality problems can arise in databases accessed and modified by legacy applications that are unaware of the heterogeneity of values in the column. For example, an application that assumes that the CUS-TOMER ID column contains only phone numbers might choose to  X  X ormalize X  column values by removing all spe-cial characters (e.g.,  X - X ,  X . X ) from the value, and writing it back into the database. Wh ile such a transformation is appropriate for phone numbers, it would clearly man-gle the email addresses represented in the column and can severely degrade common business practices. For instance, the unanticipated transformation of email addresses in the CUSTOMER ID column (e.g.,  X  X ohn.smith@noname.org X  to  X  X ohnsmith@nonameorg X ) may mean that a large num-ber of customers are no longer reachable.

Locating poor quality data in large operational databases is a non-trivial task, especially since the problems may not be due to the data alone, but also due to the interactions between the data and the multitude of applications that ac-cess this data (as the previous example illustrates). Identi-fying heterogeneous database columns becomes important in such a scenario, permitting data quality analysts to then focus on understanding the interactions of applications with data in such columns, rather than having to simultaneously deal with the tens of thousands of columns in today X  X  com-plex operational databases. If an analyst determines that a problem exists, remedial actions can include:
We next identify desiderata that a column heterogeneity measure should intuitively satisfy, followed by a discussion of techniques to quantify column heterogeneity that meet these desiderata.
 Heterogeneit y Desiderata
Consider the example shown in Figure 1. This illustrates many of the issues that need to be considered when coming up with a suitable measure for column heterogeneity.
Number of Semantic Types: Many semantically differ-ent types of values (email addresses, phone numbers, social security numbers, circuit iden tifiers, IP addresses, machine domain names, customer names, etc.) may be represented as strings in a column, with no apriori characterization of the possible semantic types present.

Intuitively, the more semantically different types of val-ues there are in a database column, the greater should be its heterogeneity; thus, heterogeneity is better modeled as a numerical value rather than a boolean (yes/no). For exam-ple, we can be confident that a column with both email ad-dresses and phone numbers (e.g., Figure 1(b)) is more het-erogeneous than one with only email addresses (e.g., Fig-ure 1(a)) or only phone numbers.

Distribution of Semantic Types: The semantically dif-ferent types of values in a database column may occur with different frequencies.

Intuitively, the relative distribution of the semantically different types of values in a column should impact its het-erogeneity. For example, we can be confident that a column with many email addresses and many phone numbers (e.g., Figure 1(b)) is more heterogeneous than a column that has mainly email addresses with just a few outlier phone num-bers (e.g., Figure 1(c)).

Distinguishability of Semantic Types: Semantically different types of values may overlap (e.g., social secu-rity numbers and phone numbers) or be easily distinguished (e.g., email addresses and phone numbers).

Intuitively, with no a priori characterization of the set of possible semantic types present in a column, we cannot always be sure that a column is heterogeneous, and our het-erogeneity measure should conservatively reflect this possi-bility.

The more easily distinguished are the semantically dif-ferent types of values in a column, the greater should be its heterogeneity. For example, a column with roughly equal numbers of email addresses and phone numbers (e.g., Fig-ure 1(b)) can be said to be more heterogeneous than a col-umn with roughly equal numbers of phone numbers and so-cial security numbers (e.g., Figure 1(d)), due to the greater similarity between the values (and hence the possibility of being of the same unknown semantic type) in the latter case.
Given the desiderata outlined above, we now present a step-wise development of our approach to quantify database column heterogeneity.

A first approach to obtaining a heterogeneity measure is to use a hard clustering . By partitioning values in a database column into clusters, we can get a sense of the number of semantically different types of values in the data. However, merely counting the number of clusters does not suffice to quantify heterogeneity. Two additional issues, as outlined above, make the problem cha llenging: the relative sizes of the clusters and their distinguishability. A few phone numbers in a large collection of email addresses (e.g., Fig-ure 1(c)) may look like a distinct cluster, but should not impact the heterogeneity of the column as much as having a significant number of phone numbers with the same collec-tion of email addresses (e.g., Figure 1(b)). Again, a social security number (see the first few values in Figure 1(d)) may look similar to a phone number, and we would like the het-erogeneity measure to reflect this overlap of sets of values, as well as be able to capture the idea that certain data might yield clusters that are close to each other, and other data might yield clusters that are far apart.

To take into account the relative sizes of the multiple clusters, cluster entropy is a better measure for quantify-ing heterogeneity of data in a database column than merely counting the number of clusters. Cluster entropy is com-puted by assigning a  X  X robability X  to each cluster equal to the fraction of the data values it contains, and computing the entropy of the resulting distribution [4]. Consider a hard clustering T t t t k of a set of n values X ,where cluster t i has n i values, and denote p i n i n . Then the cluster entropy of the hard clustering T is the entropy of the cluster size distribution, defined as using cluster entropy, the mixture of email addresses and phone numbers in column Figure 1(b) would have a higher value of heterogeneity than the data in Figure 1(c), which consists of mainly email addresses.

The cluster entropy of a hard clustering does not effec-tively take into account distinguishability of semantic types in a column. For example, given a column with an equal number of phone numbers and social security numbers (e.g., Figure 1(d)), hard clustering could either determine the col-umn to have one cluster (in which case its cluster entropy would be 0, which is the same as that of a column with just phone numbers) or have two equal sized clusters (in which case its cluster entropy would be ln , which is the same as that of a column with equal numbers of phone numbers and email addresses). Intuitively, however, the heterogene-ity of such a column should be somewhere in between these two extremes to capture the uncertainty in assigning values to clusters due to the syntactic similarity of values. Soft clustering has the potential to address this problem; each data value in soft clustering has the flexibility of assigning a probability distribution for its cluster membership, instead of belonging to a single cluster, as in hard clustering. Het-erogeneity can now be computed as the cluster entropy of the soft clustering.

The first contribution of this paper is a measure of database column heterogeneity, namely, The discussion above justifies this definition, and explains how it captures the properties of a heterogeneity measure. However, this general formulation raises two questions:
To answer the question about the  X  X ight X  soft clustering, recall that any clustering al gorithm must balance two kinds of costs. The first, expressed by k in a hard clustering prob-lem, and some compression measure in general, encodes the complexity of the cluster descriptions -the more com-pact this is, the better. The second cost is a quality measure; how well the given clustering captures the data. All clus-tering algorithms must trade off between these two costs. Our second contribution in this paper is a principled way of choosing a tradeoff point for soft clustering, based on rate distortion theory. We find the point of diminishing re-turns , i.e., the point at which the benefits achieved by higher quality stop paying for the penalty in compression. In the language of rate distortion theory, this is the point of unit slope on the (normalized) rate distortion curve.

Our third contribution in this paper addresses the ques-tion of cluster entropy of a soft clustering, and proposes the use of mutual information of a clustering to represent clus-ter entropy; this measure, when applied to a hard clustering, yields traditional entropy, and thus is a natural relaxation of entropy to soft clusterings.

Our fourth and final contribution in this paper is a de-tailed experimental evaluation of these ideas. Using di-verse data sets of different types, we demonstrate that our approach of using cluster entropy of a soft clustering pro-vides an effective and efficient mechanism for quantifying column heterogeneity.

The rest of the paper is structured as follows. We first describe related work in Section 2. In Section 3, we present our general framework for quan tifying column heterogene-ity. This framework is instantiated in Section 4, where we also provide details of our algorithm implementation. Sec-tion 5 gives the results of the experimental evaluation of our technique.
Data quality issues have been studied quite extensively in the literature (see, e.g., [5, 10, 3]). A variety of quality metrics have been proposed, e.g., accuracy, freshness and completeness, to associate with and query along with the data [13, 17]. Mihaila et al. [13] associate quality param-eters with data, and extend SQL to control data retrieval based on the values of these parameters. Widom [17] pro-posed the Trio model for integrated management of data, accuracy, and lineage, focusing on data model (TDM) and query language (TriQL) issues.

When fields have poor quality data, record linkage tech-niques using approximate match predicates are fundamen-tal [12]. These techniques return pairs of tuples from the tables, each pair tagged with a score, signifying the degree of similarity between the tupl es in the pair according to the specific approximate match predicate. Such approximate join operations have received much research attention in recent years, due to their signi ficance and practical impor-tance (see, e.g., [9, 11]).

Data profiling tools like Bellman [6] collect concise sum-maries of the values of the database fields. These summaries (set and multiset signatures based on min hash sampling and min hash counts) allow Bellman to determine data quality problems like (i) fields that are only approximate keys (the presence of null values and defaults may result in the ap-proximation), (ii) approximate functional dependencies in a table (possibly due to inconsistent values), and (iii) approx-imate joinable keys/foreign keys. However, such profiling tools do not currently help with our heterogeneity problem. Denitions
A clustering of a set of points is a partition of the set into clusters . We can think of this as a probabilistic assign-ment of points to clusters, where the conditional probability p t j x represents the probability that the point x is assigned to cluster t . A partition results from assigning only or to these probabilities. In a soft clustering , the probabilities can be arbitrary, with the natural restriction that for any
The entropy of a (discrete) probability distribution P is the sum H P of the i th element. The entropy of a random variable X is the entropy of the distribution p X x , (which we will often write as p x , where the context is clear). Given a (hard) clustering of n points into clusters T t t t k , we say that the cluster entropy of T is the quantity H T P
The mutual information between random variables X Y is the expression I X Y this expression, p x y represents the joint distribution be-tween X Y ,and p x p y are the marginals. Given a soft clustering of points X into clusters T , we can define the joint distribution p t x p t j x p x . Typically, p x will be the uniform distribution, but this may not always be so.
There are numerous methods for performing soft cluster-ings. Perhaps among the most well known among them is the class of methods known as expectation-maximization, or EM [7]. EM returns a probabilistic assignment of points to clusters. The main problem with this method (and others like it) is that they require the user to fix k , the number of clusters, in advance. One can circumvent this problem by finding the  X  X ight X  value of k using model checking criteria like AIC [1], BIC [14] and others, but these are all based on assumptions about the distributions the data is drawn from, using maximum likelihood methods to estimate the  X  X ost likely X  value of k .

A different approach is to use the idea of rate-distortion from information theory [4]. There are two parameters that constrain any clustering method. The first is representation complexity R , or how much compression one can achieve by clustering. The second is the quality Q , or how accu-rately the clustering reflects the original data. Often, it is
Figure 2. Rate-Distortion curve for a mix-ture of email addresses and IDs. Note that the tradeoffs achieved are better than those achieved by fixing K , the number of clusters, to any specific value. more convenient to think of the error E , which is typically some constant minus Q . For any fixed level of compression (this is analogous to fixing k ), one can determine the best quality representation, and for any fixed quality level, one can determine the best compression possible.

The rate distortion theorem shows that the optimal rep-resentation cost can be measured by the mutual information between the data and the clusters, R I T X .Theer-ror E is measured by computing the average distance to the cluster center over all clusters ( E Compression and quality of the optimal solution vary along a concave curve known as the rate-distortion curve (see Fig-ure 2). A detailed explanation of rate distortion theory is beyond the scope of this paper; we merely point out that the rate distortion curve can be parameterized by a single Lagrange parameter , and the points on the curve may be found by optimizing a functional of the form F R Q .
Each choice of corresponds to a point on the rate dis-tortion curve and therefore some soft clustering. Choosing timal clustering is one in which all points are in the same cluster (this corresponds to the origin of the rate-distortion graph, if space is plotted on the x-axis and quality on the y-axis). Letting go to is equivalent to making space irrel-evant; in this case, all points are placed in separate clusters of their own (this is the top right point of the curve). Note that the slope of the curve at any point is .

It is important to note that each point on the rate distor-tion curve is a soft clustering that uses as many clusters as are needed to obtain the optimal value of the rate distor-tion functional F , for the corresponding value of .Any fixed choice of the number of clusters to use will ultimately be suboptimal, as the data separates into more and more clusters. Figure 2 also shows the compression and qual-ity values achieved for clusterings using a fixed number of clusters: for each such number K , there is a point at which the corresponding curve separates from the rate distortion curve, and proceeds along a suboptimal path (less compres-sion, lower quality).
 A choice of a clustering is thus made by choosing . Normalizing the x-axis and y-axis so that the curve goes from to , we choose the point of diminishing returns ; namely the point at which the slope of the curve is one. The reason for our choice is two-fold; the point of diminishing returns, because it has unit slope, is the point after which the benefits of increased quality do not pay for the increasing space needed for the representation. Second, this point is the closest point to the point, which is the point representing perfect quality with no space penalty.
Our proposed choice of is quite general, and is inde-pendent both of the distance function used, and any distri-butional assumption. To perform the clustering though, we must fix a representation for the data, and an appropriate distance function. As pointed out by Banerjee et al. [2], choosing any Bregman distance yields a meaningful rate-distortion framework, and thus the choice of distance de-pends on the data representation used.

Choosing as the distance function yields an EM-like method, which would be appropriate for vector data. Our data are strings, which we choose to represent as q -gram distributions. A natural distributional model for string data is a multinomial distribution (for example, the naive Bayes method), and the corresponding Bregman distance is the Kullback-Leibler distance. Us ing this measure yields a fa-miliar formulation; the information bottleneck method of Tishby, Pereira and Bialek [16]. Some algebra yields a sim-ple description of the rate distortion curve, as the set of points that minimize F I T X I T Y ,where Y is a variable representing the space of all q -grams. Note that the choice of that yields the point of unit slope on the rate distortion curve is now given by H X I X Y . This is a consequence of the fact that I T X is maximized when T X , and thus I T X I X X H X ,and I T Y is maximized when T X .Weusethe iIB algo-rithm [15] to compute a local optimum of F .

The central idea of this paper is that the entropy of a clus-tering is a good measure of data heterogeneity. However, cluster entropy is defined only for a hard clustering. Since we know that any hard clustering can be expressed in terms of probabilistic cluster assignments using only zero and one for probabilities, (and uniform priors on the elements x )we would like the measure we propose to tend towards (hard clustering) cluster entropy in this limiting case. Using Cluster Marginals The first approach that comes to mind is to determine the marginals p t P resulting distribution. Clearly, if all assignments are 0-1 and p x n , p t jf x j x t gj n , and this reduces to
H T . However, by aggregating the individual cluster membership probabilities, we have lost crucial information about the data. Consider two different soft clusterings of two points x x into two clusters t t . In the first cluster-ing, p t j x p t j x . In the second cluster-ing, p t j x p t j x . Both clusterings yield the same marginal p t and thus would have the same cluster entropy using the proposed measure. However, it is clear that in the first clustering, x is essentially in t and x essentially in t ( can be replaced by any number for this purpose), and so the cluster entropy should be close to log . In the second clustering however, t and t are in-distinguishable, which means that there is effectively only one cluster, with a cluster entropy of zero.

Two issues are illuminated by this example. First, aggre-gating cluster probabilities is not an appropriate equivalent of cluster entropy. Second, the number of clusters in a soft clustering is an irrelevant parameter . This is because two clusters having identical cluster membership probabilities will be collapsed (intuitively because they are indistinguish-able from each other). Note that the membership probabili-ties p t j x for a point x do not have to be identical for this to happen.
 Using Superpositions of Hard Clusterings The second approach we might take is to view the probabilistic assign-ments as the convex superposition of different hard cluster-ings. In this view, the assignments reflect the superposition of different  X  X orlds X , each with its own hard clustering. In this case, our strategy is clear; we assign each point to a cluster using the distribution p t j x , and compute the clus-ter entropy of the resulting hard clustering. Doing this re-peatedly and taking an aggreg ate (mean or median) gives us an estimate. Note that th is approach will yield H T ,as desired, when applied to a single hard clustering, because each point is always assigned to a specific cluster.
However, the second clustering in the above example outlines a problem with this strategy. If a set of points have identical cluster memberships in a set of clusters, then in any particular sample, the points might be distributed among many clusters, rather than all being placed together as they should. For example, in some samples, x might be placed in t ,and x might be placed in t , and in others,
Figure 3. Sampled cluster entropy as a func-tion of on a logscale. they might be placed together. T o address this problem, we have to merge clusters that have similar cluster membership probabilities, using a threshold parameter and some appro-priate merging strategy.

The result of doing this, for multiple data sets and over the range of values for , is referred to as sampled cluster entropy and is shown in Figure 3. In the figure, the x-axis represents .

Observe that for all data sets, this measure exhibits a clear minimum around . It turns out that the rel-ative ordering of the heterogeneity values at this value of correctly reflects the underlying heterogeneity in the data.
The above measure appears quite suitable as a measure of heterogeneity. However, computing it requires at least two parameters; the threshold p arameter that decides when clusters are merged, and a parameter deciding the number of samples required to get an accurate estimate. In the ab-sence of a rigorous theoretical analysis, the choice of values for these parameters will inevitably need to be determined experimentally.
 Using Mutual Information I T X A more compact so-lution exploits the relationship between H T and I T X . Rate-distortion theory tells us that I T X is a measure of space complexity; it is not hard to see that if the assign-ments p t j x are -,then I T X H T . Thus, the measure of heterogeneity (and of cluster entropy for a soft clustering) we propose is I T X .
 A brief illustration of this idea is presented in Figure 4. For each value of and for multiple data sets, we plot I T X for the soft clusterings obtained using iIB .On the x-axis, we plot relative to the chosen value H X I X Y .As increases from zero, there is a sharp increase in I T X as we approach . This increase oc-curs for all the data sets, and in roughly the same place.
The measure of heterogeneity of a data set is then the value of I T X at the point (i.e at on the graph). As we shall see experimentally in Section 5, this measure of heterogeneity correctly predicts the true under-lying heterogeneity in the data.

I T X increases monotonically; sampled cluster en-tropy as described above exhibits a series of (increasing) local minima. It is interesting, and an indication of the ro-bustness of our proposed choice of , that both measures, while approaching the problem of heterogeneity differently, display the same properties: namely, a sharp change close to , and a correct ordering of data sets with respect to their true underlying heterogeneity at this point.
We can now summarize the entire algorithm in Algo-rithm 1.
 Algorithm 1 Computing Heterogeneity. 1: Convert input strings to distribution of q -grams. Pre-2: Compute H X I X Y 3: Compute soft clustering using iIB with set to 4: Estimate heterogeneity by computing I T X .

Given our choice of I T X as the preferred measure of heterogeneity and of cluster entropy of a soft clustering, when we refer to cluster entropy in the rest of this paper we mean I T X , unless otherwise indicated.
The previous section described our general framework for quantifying column heterogeneity. We now present an instantiation of our framework based on the representation of string data using a multinomial distribution of q-grams, and the details of our algorithm implementation. The input to the algorithm consists of a column of data, viewed as a set of strings X .
 Preparing The Data Weighted q -gram Vectors We first construct q -grams for all strings in X . In particular, we construct and For each q -gram y ,let f x y be the number of occur-rences of y in x ,andlet p y be the fraction of strings containing y . We construct a matrix S whose rows are the strings of X and whose columns are q -grams, and the entry m xy f x y w y Z ,where Z is a normalizing constant so that the sum of all entries in M is ,and w y H p y w y ln p y would yield the standard IDF weighting scheme.

Note that standard IDF weighting is not appropriate for our application. IDF weighting was designed to aid in doc-ument retrieval, and captures t he idea of  X  X pecificity X , that a few key phrases could be very useful at identifying relevant documents. IDF weighting captures this by overweighting terms with low relative occurrence.

However, for heterogeneity testing, specificity means that certain rare terms occur only in a few strings. We are not concerned with such outliers, and IDF weighting will only give such strings artificially high importance. Rather, robust heterogeneity testing requires us to to identify terms that distinguish large sets o f data from each other; the en-tropy weighting scheme captu resthisasitismaximized when p , and decays symmetrically in either direc-tion.
 Adding Background Context Any clustering makes sense within a context; a high concentration of points in a small range is significant only i f viewed against a relatively sparse, larger background. For example, the collection of strings in Figure 1(a) form a cluster only with respect to the set of all strings. If the background for this data were only the set of email addresses, then this set has no apparent un-usual properties.

For heterogeneity testing, an appropriate background is the space of all strings. This needs to be introduced into each data set in order to define the  X  X ounding volume X  of the space. As we represent data a s distributions, the back-ground consists of random distributions, chosen from the space of all distributions. These are added to the data be-fore soft clustering is performed, and are then removed 1
It is well known that the uniform distribution over the d -dimensional simplex is a Dirichlet distribution, and thus a uniform sample from this space is obtained by the fol-lowing simple procedure. Sample d points x x d from an exponential distribution with parameter , and normal-ize the values by dividing each by d -vector is a uniform sample from the simplex [8]. A uni-form sample from an exponential distribution is computed by sampling r uniformly in and returning the value ln r .

To generate the background, we use a set of q -grams dis-joint from the q -grams in Y , of the same cardinality as Using the above procedure, we generate j X j points, yield-ing a matrix N that is then normalized so all entries sum to .Both S and N have dimension j X jj Y j .

We fix a parameter (the mixing ratio )that controls the mixture of data and background context. The final joint density matrix M is of dimension j X j j Y j , containing S as its first j X j rows and j Y j columns and M is a valid joint distribution since its entries sum to .We will abuse notation and refer to the rows of M as X and the columns as Y in what follows.

In Section 3 we derived an expression for the value of corresponding to the point on the rate-distortion curve that balanced error and compression. This value of is given by H X I X Y . We compute this from the ma-trix M , using only the data rows and columns. We use the standard empirical estimator for entropy (which treats the normalized counts as fractions).

Given M and ,wenowrunthe iIB algorithm[15] for computing the information bottleneck. This algorithm is a generalization of the standa rd expectation-maximization method. Although the algorithm generates a soft cluster-ing, it requires as input a target set of clusters (not all of which may be used in the output). We specify a very large number of clusters ( j X j ). Empirically, we see that this is sufficient to find a point on the rate distortion curve. We emphasize here that we do not need to fix a number of clus-ters in advance; the number of clusters that we supply to iIB is merely an artifact of the implementation and need only be a very loose upper bound. It only affects the run-ning time of the algorithm, and not the final heterogeneity measure computed.

The output of this algorithm is a soft clustering T , spec-ified as the conditional probabilities p t j x , from which the cluster masses p t and the cluster centers p y j t can be de-rived using Bayes X  Theorem and the conditional indepen-dence of T and Y given X .
We depict the algorithm in Figure 5, paralleling the steps of algorithm 1.
 Sampling Of Data Columns in a database have many thousands of entries. We sample a small fraction of the en-tries and compute our heterogeneity estimate on this sam-ple. Since our goal is to detect gross structure in the data, sampling allows us to discard small outlier sets without sig-nificantly affecting the results.
 Number of Iterations of iIB The iIB algorithm can of-ten require many iterations to converge. However, experi-ments show that it almost always converges to within 1% of the final answer within 20 iterations. Hence, we terminate the algorithm after 20 iterations. We present these experi-mentsinSection5.
We now present a detailed experimental evaluation of our heterogeneity detection scheme. We will do this using di-verse data sets of different types, mixed together in various forms to provide different levels of heterogeneity. We start with a description of the platform and the data sets. Platform and Data Sets The machine we run our experiments on has a Intel(R) Xeon(TM) 3.2 Ghz CPU and 8GB of RAM. It runs Red Hat Linux with the 2.4 kernel and gcc v3.2.3.

We consider mixtures of four different data sets. email is a set of 509 email addresses collected from attendees at the 2001 SIGMOD/PODS conference. ID is a set of 609 employee identifiers, phone is a diverse collection of 3064 telephone numbers, and circuit is a set of 1778 network circuit identifiers. Strings in ID and phone are numeric ( phone data contains the period as well). Strings in email and circuit are alphanumeric, and may contain special characters like  X  X  X  and  X - X .

Each input for our experiments consists of a fixed num-ber of sampled data elements, mixed with the same number of background elements (constructed using the procedure described in Section 4). Elements were sampled from the data sets uniformly, and data mixtures were constructed us-ing equal amounts from each source. A two-set mixture contained equal numbers of elements of each type, and so on. We tried all k -way mixtures for k .
Figure 6. Cluster entropy as a measure of het-erogeneity. The x-axis plots .
 We start by demonstrating the value of cluster entropy I T X as a measure of heterogeneity. Figure 6 plots the estimated cluster entropy as a function of ,forvalues of around H X I X Y . Note that might be different for different data sets. Observe that all the individ-ual data sets have very small cl uster entropies, and are well separated from the mixtures. Further, mixtures of two data sets in general have lower cluster entropy than mixtures of three and four. We observe that as the number of elements in the mixture increases, the heterogeneity gap decreases, and that the separations are not strict for the more heteroge-neous sets; this is natural, as individual data sets may have characteristics that are similar (e.g., ID and phone ).
A closer look at the plot illustrates the idea of well-separatedness and how it is captured in our measure. Con-sider Figure 7, which plots cluster entropy I T X only for 2-mixtures, and a mixture of all the data sets.

We see a clear separation between the cluster entropy of pairs and the cluster entropy of the 4-way mixture. How-ever, we also notice one 2-mixture that has much lower cluster entropy, much closer to what we would expect for a single data source. The two data sets are ID and phone , both data sets that are predominantly numeric strings, and which often have the exact same number of digits. It is diffi-cult to tell these kinds of strings apart, and as a consequence
Figure 7. Cluster entropy for mixtures of two sets, and a mixture of all the data sets. their mixture is not viewed as particularly heterogeneous.
The various mixtures whose cluster entropies are de-picted in Figures 6 and 7 use equal amounts of data from each source. In Figure 8, we plot cluster entropy for data obtained from a login column of a real data set, contain-ing an unknown mixture of values. The heterogeneity of this column was determined t o be between a 1-mixture and a 2-mixture. A subsequent inspection of the data revealed a large number of email values mixed with a smaller num-ber of ID values.

Cluster entropy appears to capture our intuitive notion of heterogeneity. However, it is derived from a soft clustering returned by the iIB algorithm. Does that soft clustering actually reflect natural groupings in the data? It turns out that this is indeed the case. In Figure 9 we display bitmaps that visualize the soft clusterings obtained for different mix-tures. In this representation, columns are clusters, rows are data, and darker probabilities are larger. For clarity, we have reordered the rows so that all data elements coming from the same source are together, and we reordered the columns that have similar p t j x distributions.

To interpret the figures, recall that each row of a fig-ure represents the cluster membership distribution of a data point. A collection of data points having the same cluster
Figure 8. Cluster entropy for an unknown mixture, compared with known mixtures. membership distributions represent the same cluster. Thus, notice how the clusters separ ate out quite cleanly, clearly displaying the different data mixtures. Also observe how, without having to specify k , the number of clusters, iIB is able to separate out the groups. Further, if we look at Fig-ure 9, we notice how the clusters corresponding to ID and phone overlap and have similar cluster membership distri-butions, reinforcing our observation that they form two very close (not well-separated) clusters.

In addition to performing soft clustering and computing cluster entropy, our algorithm adds in background context to aid in the clustering process. In the next experiment, we establish the need for this step.

An important aspect of our algorithm is the addition of a background context, as discussed in Section 4.1. We argued that intuitively, the effect of this addition is to  X  X xpand X  the space being clustered, so a set of points that is highly clus-tered shows up clearly in contrast to the background. Obvi-ously, if the background level is too high, any data will get swamped, and if too low, will be useless.

Using email and ID , we illustrate the effect of adding background in Figure 10. Setting the parameter to one removes all background from the data. As predicted, when a background context is not added, the data spreads over all clusters in unpredictable ways. Adding it in immediately and predictably collapses the data into a few clusters.
The default sample size used in our experiments was 200. Increasing the sample size increases the running time of the procedure, but it has the potential for revealing more
Figure 9. Soft Clustering of email / ID , email / circuit , circuit / phone , email / ID / circuit / phone mixtures. accurate clusterings. 200 samples suffice to capture hetero-geneity accurately on our data; i n Figure 11, we plot cluster entropy against for 400-sample mixtures. As always, we use the same number of background elements as data points. Notice that the behavior of the curves is very similar to the behavior with 200 samples (Figure 6).

Conversely, could we have reduced our sample size? The answer, on our data sets, appears to be negative. Figure 12 plots cluster entropy against for 100-sample data sets. Here we can see that the heterogeneity estimates are starting to deteriorate, most prominently for some 3-way mixtures.
We now look at the performance of our algorithm. Ta-ble 1 breaks down the time taken by various parts of the algorithm. For this experiment, we used an input set of size n n , consisting of n data rows and n back-ground rows, using 1-grams and a maximum of initial clusters to  X  X eed X  iIB . Recall that we do not fix the number of clusters finally produced by the algorithm.

We also compare the performance impact of using 2-grams rather than 1-grams when converting the input data to a collection of distributions, in Table 2. The most signif-icant time degradation happens in each iteration of iIB ,as
Figure 10. ID clusters with and without back-ground. the intrinsic dimensionality of the data increases.
As we can see from the above tables, the main bottle-neck in the algorithm is an iIB update step. The best way of reducing this time is being able to pick a single value of to start the iIB iteration, and that is an important feature of our algorithm. Further, each update step of iIB is ex-pensive, and so if we can reduce the number of update steps needed, we can improve the a lgorithm performance. The number of update steps is controlled by the rate of conver-gence of iIB . It turns out that in practice, iIB converges rather rapidly, in general requiring 20 iterations or less to converge to within a few percent of its final value. The con-vergence numbers are shown in Figure 13. Even for far larger instances (not shown in the figure), convergence typ-ically occurs in 30 iterations or less.
In this paper, we proposed a novel measure of database column heterogeneity as the cluster entropy of a soft cluster-ing of the data, useful to understand data quality issues that can arise when merging data from multiple sources. We addressed the problem of picking the  X  X ight X  soft cluster-ing by formulating a principled way of choosing a tradeoff point between compression and quality for soft clustering, based on rate distortion theory, as H X I X Y .
 We identified mutual information of a clustering, I T X , as a robust measure of cluster entropy of a soft clustering, and demonstrated that it quan tifies the column heterogene-
Figure 11. Cluster Entropy with sample size of 400.
Figure 12. Cluster Entropy with sample size of 100. ity of the underlying data via a detailed experimental study.
There are many further optimization strategies that we might adopt in order to improve the performance and qual-ity of our algorithm. Some that we have experimented with include reducing the alphabet size, trying different term weighting schemes, and using different models for back-ground context. We plan on exploring this space further, so as to be able to employ our algorithm on databases contain-ing hundreds of thousands of columns.

A central question that comes out of our work is the role of and cluster entropy I T X in determining  X  X ood X  clusters. Our choice of was based on intuitive arguments about the tradeoff between quality and compression. This was validated by the behavior of I T X for different mix-tures of data sets at, and in the vicinity of, . A better understanding of this phenomenon might lead to deeper in-sights into the way soft clusterings evolve, as well as what structural properties are captured by cluster entropy.
Table 1. Breakup of running time (in sec-onds). Generation of background and en-tropy estimation take negligible time (less than 0.02s) and were omitted.

Table 2. Time (in seconds) per iIB iteration when using 1-grams and 2-grams.
