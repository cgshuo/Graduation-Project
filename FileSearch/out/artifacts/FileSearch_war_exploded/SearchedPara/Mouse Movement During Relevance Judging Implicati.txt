 Several researchers have found that a user X  X  mouse position gives an indication of the user X  X  gaze during web search and other tasks. As part of a user study that involved rele-vance judging of document summaries and full documents, we recorded users X  mouse movements. We found that in a large number of cases, the users did nothing more with their mouse than move it to the buttons used for recording the relevance decision. In addition, we found that different search topics can result in large differences in the amount of mouse movement that is indicative of user attention. For simple reading tasks, such as short document summaries, mouse-tracking does not appear to be an effective means of discerning user attention. While more complex tasks may allow mouse movements to provide information regarding user attention, on average, indications of user attention ex-isted in only 59% of the relevance judgments made for full documents.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval] Keywords: mouse tracking; relevance judging
Encouraged by research that showed a correlation between the mouse and a user X  X  gaze [8], in late 2009 and early 2010, we collected mouse positions as part of a user study [9] that required users to judge the relevance of documents and doc-ument summaries. Our hope was that by collecting the posi-tion of the mouse, we could gain insight into user attention. In effect, we hoped the mouse position could be a  X  X oor man X  X  eye-tracker X  [2]. On inspection of the mouse move-ments, we discovered that for a large number of users, the mouse told us nothing more than that the user moved the mouse to click the relevant and not relevant buttons in the user interface. We knew the users had read an appropriate amount of the material to make reasonably good relevance judgments, but for many of the judgments, we had no in-search engine results page (SERP), we are considering mouse movement on pages consisting of text to read and buttons to enter a relevance judgment. Our results give support to the work of Cox and Silva [3] who showed that as web menu search tasks included more distractors, users increasingly used the mouse to  X  X ag X  potentially good selections. For the mouse to be a good poor man X  X  eye-tracker, we apparently need to present the user with consistently complex tasks. We used mouse movement data from a previous study [9]. In this study, the mouse movements of 48 participants were recorded as they judged the relevance of documents and doc-ument summaries. Each participant judged 4 search topics selected from the 8 topics shown in Table 1. The docu-ments came from the AQUAINT news collection, and the document summaries were in the style of search result snip-pets with the article X  X  title displayed as a link and up to two sentences displayed as a query-biased summary. The participants judged alternating summaries and documents, which were displayed in orders with a uniform precision of 0.3 and 0.6, i.e. 3 or 6 of every 10 documents were relevant as per the NIST relevance judgments. Topics and result list precision were balanced across blocks of participants such that each topic was judged by 24 participants, of which half experienced a precision of 0.3, and the other half, 0.6. Par-ticipants worked on each topic for 10 minutes.

Mouse movements were captured using javascript in the browser and sent in batched updates to the web server for logging. At 0.1 second intervals, we recorded the position of the mouse. We had one issue with our mouse-tracking code whereby rather than poll the mouse position, we updated the known position on every mouse move event. As a result, if the user did not move the mouse, the variables holding the mouse position had undefined as their values. Thus, the position of the mouse was only known to us once the user began to move the mouse. In many cases, the user never moved the mouse to judge a document, for the user left the mouse over the button used to judge the previous document and the user X  X  judgment for this document was the same.
The participants used LCD monitors with a resolution of 1680 pixels wide by 1050 pixels high. At all times, the user interface was maximized. The effective viewport for the web page was 1680 by 915 pixels. We captured screenshots of the full web page, which for long documents extended beyond the bottom of the screen. The recorded mouse movements were plotted on top of the screenshot with each recorded location drawn as a small circle and subsequent locations connected with a straight line. The starting position was drawn with a green or yellow circle and the end position with a red circle. A yellow circle was used to represent that some positions had been recorded as undefined at the start of this web page as described above. If the mouse stayed in one position for an extended time, the corresponding circle was increased in size and the time spent in that location was output as well. For example, Figure 1 shows the mouse moving from the  X  X elevant X  button to a location below the document summary and resting there for 14.59 seconds be-fore the mouse was moved to the  X  X ot Relevant X  button, where it stayed for 5.5 seconds before the page exited.
The second and third authors examined these images of mouse movements for many different users and each sep-arately developed a classification scheme for the types of mouse movements they saw. The independent schemes were discussed and then a final classification was jointly devel-oped, which is shown in Table 2.

In addition the categories in Table 2, we define the mouse as giving no indication of user attention when no mouse movement was seen or when the category is decision-only and none of the following categories is true: horizontal, ver-tical, highlighting, scrolling, and random.

The user study produced 6722 images of mouse move-ments. Each image represented a single study participant X  X  interaction with the relevance judging interface for a sin-gle document summary or full document. The second and third authors acted as annotators. We randomly selected 500 images to be judged by both the annotators and then randomly divided the remaining images between the anno-tators. The jointly judged images allowed the annotators to improve their consistency as well as allow us to measure the degree to which they agreed in their judgments. The annotators first judged 100 of their 500 shared images. Af-ter judging the first 100 images, the annotators discussed differences in their judgments and slightly refined the classi-fication scheme. The remaining 400 shared images occurred at random amongst the remaining randomly ordered images.
For each of the categories in Table 2, we measured agree-ment between the annotators on the 500 images as overlap . We did not use Cohen X  X   X  , for  X  requires mutually exclusive categories. Let A be the set of documents judged to be of a given category by annotator A , and B the set of documents judged to be of the same category by annotator B . The over-lap for the two annotators is then equal to: | A  X  B | / | A  X  B | . Table 3 shows the overlap for each of the categories in Ta-ble 2 as well as agreement for when the mouse gave no in-dication of user attention. Good agreement was obtained for the decision-only category and likewise for no-indication of user attention, which incorporates the decision-only cate-gory. Reasonable agreement existed for horizontal, vertical, and scrolling behaviors, but identification of highlighting, random, and re-scoping had low agreement.

For our final analysis, we randomly selected between the two annotators X  judgments on their 500 shared images.
Given the annotated images of mouse movements, we com-puted the average percent of images in which each of the movement categories were seen. To compute these averages, we first compute a study participant X  X  average for each topic, and then average these averages. This method of averaging is important because participants worked at their own rate and judged different numbers of documents and summaries. Figure 4: An example of random mouse movement (see Table 2).
 Table 4: Average percent of relevance judgments made with different mouse behaviors. The category  X  X o-indication X  represents when the mouse gives no indication of the user X  X  attention.

Table 4 shows the average percent of relevance judgments made with the exhibited category of mouse movement. The average study participant judges 76% of the document sum-maries without making any indication of what their atten-tion is, and 41% of the full documents are also judged with no indication of user attention. Evidence of horizontal move-ment is nearly the same for both summaries and documents, and is likely because a subset of users frequently read using their mouse as one would use a finger to trace under the words as read, and this behavior occurs whether the user is reading a short summary or a longer document. The other behaviors increase for documents. The increase is likely a result of the added length of documents and complexity of finding relevant information in documents.

Table 5 shows that different topics have different amounts of mouse movement that give some indication of user at-tention. In general, the topics with more indication of user attention, e.g. 383, contain longer documents and/or require careful searching of the document for relevant information.
While the percentages in Table 4 reflect the average user on an average topic, they do not mean that for a given user we should expect a certain percentage of judgments to show a given mouse movement behavior. For example, horizontal mouse movement is largely restricted to a subset of study participants. Eleven of the 48 participants produced 90%
