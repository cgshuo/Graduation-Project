 Magnetic resonance imaging (MRI) [7, 2] is a key diagnostic technique in healthcare nowadays, and of central importance for experimental research of the brain. Without applying any harmful ioniz-ing radiation, this technique stands out by its amazing versatility: by combining different types of radiofrequency irradiation and rapidly switched spatially varying magnetic fields (called gradients ) superimposing the homogeneous main field, a large variety of different parameters can be recorded, ranging from basic anatomy to imaging blood flow, brain function or metabolite distribution. For this large spectrum of applications, a huge number of sequences has been developed that describe the temporal flow of the measurement, ranging from a relatively low number of multi-purpose tech-niques like FLASH [5], RARE [6], or EPI [9], to specialized methods for visualizing bones or perfusion. To select the optimum sequence for a given problem, and to tune its parameters, is a dif-ficult task even for experts, and even more challenging is the design of new, customized sequences to address a particular question, making sequence development an entire field of research [1]. The main drawbacks of MRI are high initial and running costs, since a very strong homogeneous mag-netic field has to be maintained, moreover long scanning times due to weak signals and limits to gradient amplitude. With this in mind, by far the majority of scientific work on improving MRI is motivated by obtaining diagnostically useful images in less time. Beyond reduced costs, faster imaging also leads to higher temporal resolution in dynamic sequences for functional MRI (fMRI), less annoyance to patients, and fewer artifacts due to patient motion.
 In this paper, we employ Bayesian experimental design to optimize MRI sequences. Image recon-struction from MRI raw data is viewed as a problem of inference from incomplete observations. In contrast, current reconstruction techniques are non-iterative. For most sequences used in hospitals today, reconstruction is done by a single fast Fourier transform (FFT). However, natural and MR images show stable low-level statistical properties, 1 which allows them to be reconstructed from fewer observations. In our work, a non-Gaussian prior distribution represents low-level spectral and local natural image statistics. A similar idea is known as compressed sensing (CS), which has been applied to MRI [8].
 A different and more difficult problem is to improve the sequence itself. In our Bayesian method, a posterior distribution over images is maintained, which is essential for judging the quality of the sequence: the latter can be modified so as to decrease uncertainty in regions or along directions of interest, where uncertainty is quantified by the posterior. Importantly, this is done without the need to run many MRI experiments in random a priori data collections. It has been proposed to design sequences by blindly randomizing aspects thereof [8], based on CS theoretical results. Beyond being hard to achieve on a scanner, our results indicate that random measurements do not work well for real MR images. Similar negative findings for a variety of natural images are given in [12]. Our proposal requires efficient Bayesian inference for MR images of realistic resolution. We present a novel scalable variational approximate inference algorithm inspired by [16]. The problem is re-duced to numerical mathematics primitives, and further to matrix-vector multiplications (MVM) with large, structured matrices, which are computed by efficient signal processing code. Most pre-vious algorithms [3, 14, 11] iterate over single non-Gaussian potentials, which renders them of no use for our problem here. 2 Our solutions for primitives required here should be useful for other ma-chine learning applications as well. Finally, we are not aware of Bayesian or classical experimental design methods for dense non-Gaussian models, scaling comparably to ours. The framework of [11] is similar, but could not be applied to the scale of interest here. Our model and experimental design framework are described in Section 2, a novel scalable approximate inference algorithm is developed in Section 3, and our framework is evaluated on a large-scale realistic setup with scanner raw data in Section 4. Denote the desired MR image by u  X  R n , where n is the number of pixels. Under ideal conditions, the raw data y  X  R m from the scanner is a linear map 3 of u , motivating the likelihood Here, each row of X is a single Fourier filter, determined by the sequence. In the context of this paper, the problem of experimental design is how to choose X within a space of technically feasible sequences, so that u can be best recovered given y . As motivated in Section 1, we need to specify butions  X  a Gaussian prior would not be a sensible choice. We use the one proposed in [12]. The posterior has the form the prior being a product of Laplacians on linear projections s j of u , among them the image gradient in [12]. MVMs with B cost O ( q ) with q  X  3 n . MAP estimation for the same model was used in [8].
 Bayesian inference for (1) is analytically not tractable, and an efficient deterministic approximation is discussed in Section 3. In the variant of Bayesian sequential experimental design used here, an extension of X by X  X   X  R d,n is scored by the entropy difference is combined from parts, each extension being chosen by maximizing the entropy difference over a candidate set { X  X  } . After each extension, a new scanner measurement is obtained for the single extended sequence only. Our Bayesian predictive approach allows us to score many candidates ( X  X  , y  X  ) without performing costly MR measurements for them. The sequential restriction makes sense for several reasons. First, MR sequences naturally decompose in a sequential fashion: they describe a discontinuous path of several smooth trajectories (see Section 4). Also, a non-sequential approach would never make use of any real measurements, relying much more on the correctness of the model. Finally, the computational complexity of optimizing over complete sequences is staggering. Our sequential approach seems also better suited for dynamic MRI applications. In this section, we propose a novel scalable algorithm for the variational inference approx-imation proposed in [3]. We make use of ideas presented in [16]. First, e  X   X   X  j | s j | = [3]. Let  X  = (  X  j ) and  X  = diag  X  . To simplify the derivation, assume that B T  X  B is invertible, 4 Gaussian, and where the maximum is attained at u = h . Therefore, P ( y )  X  C 1 (  X  2 ) e  X   X  (  X  ) / 2 with and the bound is tightened by minimizing  X  (  X  ) . Now, g (  X  ) := log | A | is concave, so we can z  X  R q + ; the inner loop consists of minimizing the upper bound w.r.t.  X  for fixed z . Introducing because z T (  X   X  1 ) is convex (all z j  X  0 ). Minimizing over  X  gives the convex problem which is of standard form and can be solved very efficiently by the iteratively reweighted least squares (IRLS) algorithm, a special case of Newton-Raphson. In every iteration, we have to solve gradients (LCG) algorithm [4], requiring a MVM with X , X T , B , and B T per iteration. The line search along the Newton direction d can be done in O ( q ) , no further MVMs are required. 0 =  X   X   X  T z 0  X  g (  X  ) = z 0  X  X  X   X  g (  X  ) , and z cannot be computed by a few LCG runs. Since A has no sparse graphical structure, we cannot use belief propagation either. However, the Lanczos algorithm can be used to estimate z 0 [10]. This algorithm is also essential for scoring many candidates in each design step of our method (see Section 3.1).
 Our algorithm iterates between updates of z (outer loop steps) and inner loop convex optimization of ( u ,  X  ) . We show in [13] that min  X   X  (  X  ) is a convex problem , whenever all model sites are log-concave (as is the case for Laplacians), a finding which is novel to the best of our knowledge. Once converged to the global optimum of  X  (  X  ) , the posterior is approximated by Q (  X | y ) of (3), whose mean is given by u . The main idea is to decouple  X  (  X  ) by upper bounding the critical term log | A | . If the z updates are done exactly, the algorithm is globally convergent [16]. Our algorithm is inspired by [16], where a different problem is addressed. Their method produces very sparse solutions of Xu  X  y , while our focus is on close approximate inference, especially w.r.t. the posterior covariance matrix. It was found in [12] that aggressive sparsification, notwithstanding being computationally convenient, hurts experimental design (and even reconstruction) for natural images . Their update of z requires (5) as well, but can be done more cheaply, since most  X  j = +  X  , and A can be replaced by a much smaller matrix. Finally, note that MAP estimation [8] is solving (4) once for z = 0 , so can be seen as special case of our method. 3.1 Lanczos Algorithm. Efficient Design The Lanczos algorithm [4] is typically used to find extremal eigenvectors of large, positive definite matrices A . Requiring an MVM with A in each iteration, it produces Q T AQ = T  X  R k,k after k iterations, where Q T Q = I , T tridiagonal. Lanczos estimates of expressions linear in  X  = A  X  1 are obtained by plugging in the low-rank approximation QT  X  1 Q T  X   X  [10]. In our Q need not be done then, and For nc candidates of d rows, computing scores would need d  X  nc LCG runs, which is not feasible. Using the Lanczos approximation of  X  , we need k MVMs with X  X  for each candidate, then nc Cholesky decompositions of min { k,d } X  min { k,d } matrices. Both computations can readily be parallelized, as is done in our implementation. Note that we can compute  X   X ( X  X  ) / X  X  for X  X  = X  X  (  X  ) , if  X  X  X  / X  X  is known, so that gradient-based score optimization can be used. The basic recurrence of the Lanczos method is treacherously simple. The loss of orthogonality in Q has to be countered, thus typical Lanczos codes are intricate. Q has to be maintained in memory. The matrices A we encounter here, have an almost linearly decaying spectrum, so standard Lanczos codes, designed for geometrically decaying spectra, have to be modified. Our A have no close low rank approximations, and eigenvalues from both ends of the spectrum converge rapidly in z z , underestimations from the Lanczos algorithm entail more sparsity (although still z k,j &gt; 0 ). In practice, a smaller k often leads to somewhat better results, besides running much faster. While the global convergence proof for our algorithm hinges on exact updates of z , which cannot be done to the best of our knowledge, the empirical success of Section 4 may be due to this observation, noting that natural image statistics are typically more super-Gaussian than the Laplacian. In conclusion, approximate inference requires the computation of marginal variances, which for general models cannot be approximated closely with generic techniques. In the context of sparse linear models, it seems to be sufficient to estimate the dominating covariance eigendirections, for which the Lanczos algorithm with a moderate number of steps can be used. More generally, the Lanczos method is a powerful tool for approximate inference in Gaussian models, an insight which does not seem to be widely known in machine learning. We start with some MRI terminology. An MR scanner acquires Fourier coefficients Y ( k ) at spatial long it takes to obtain a complete image, depending on the number of trajectories and their shapes. Gradient amplitude and slew rate constraints enforce smooth trajectories. In Cartesian sampling , trajectories are parallel equispaced lines in k -space, so the FFT can be used for image reconstruc-tion. Spiral sampling offers a better coverage of k -space for given gradient power, leading to faster acquisition. It is often used for dynamic studies, such as cardiac imaging and fMRI. A trajectory Kaiser-Bessel kernel [1, ch. 13.2] to approximate the multiplication with X k , which would be too expensive otherwise. As for other reconstruction methods, most of our running time is spent in the gridding (MVMs with X , X T , and X  X  ). For our experiments, we acquired data on an equispaced grid. 7 In theory, the image u is real-valued; in reality, due to resonance fre-quency offsets, magnetic field inho-mogeneities, and eddy currents [1, ch. 13.4], the reconstruction con-tains a phase  X  ( r ) . It is com-mon practice to discard  X  after re-construction. Short of modelling a complex-valued u , we correct for low-frequency phase contributions by a cheap pre-measurement. 8 Note that | u true | , against which recon-structions are judged below, is not al-tered by this correction. From the corrected raw data, we simulate all further measurements under different sequences using grid-ding interpolation. While no noise is added to these measurements, there remain significant high-frequency erroneous phase contributions in u true .
 where the gradient g ( t )  X  d k /dt grows to maximum strength at the slew rate, then stays there [1, ch. 17.6]. Sampling along an interleave respects the Nyquist limit. The number of revolutions N r and interleaves N shot determine the radial spacing. The scan time is proportional to N shot . In our setup, N r = 8 , resulting in 3216 complex samples per interleave. For equispaced offset angles  X  0 , the Nyquist spiral (respecting the limit radially) has N shot = 16 . Our goal is to design spiral se-quences with smaller N shot , reducing scan time by a factor 16 /N shot . We use the sequential method resolution is 256  X  256 , so n = 65536 . Since u true is approximately real-valued, measurements at score candidates (  X / 256)[0 : 255] in each round, comparing to equispaced placements j X /N shot , and to drawing  X  0 uniformly at random. For the former, favoured by MRI practitioners right now, the maximum k -space distance between samples is minimized, while the latter is aligned with com-pressed sensing recommendations [8].
 For a given sequence, we consider different image reconstructions: the posterior mode (convex MAP estimation) [8], linear least squares (LS; linear conjugate gradients), and zero filling with density compensation (ZFDC; based on Voronoi diagram) [1, ch. 13.2.4]. The latter requires a single MVM with X T only, and is most commonly used in practice. We selected the  X  scale parameters (there are two of them, as in [12]) optimally for the Nyquist spiral X nyq , and set  X  2 to the variance of X nyq ( u true  X  X  u true | ) . We worked on two slices (8,12) and used 750 Lanczos iterations in our in Table 3, and some reconstructions (slice 8) are shown in Figure 2. The standard reconstruction method ZFDC is improved upon strongly by LS (both are linear, but LS is iterative), which in turn is improved upon significantly by MAP. This is true even for the Nyquist spiral ( N shot = 16 ). While the strongest errors of ZFDC lie outside the  X  X ffective field of view X  (roughly circular for spiral), panel f of Figure 2 shows that ZFDC errors contain important structures all over the image. Modern implementations of LS and MAP are more expensive than ZFDC by moderate constant factors. Results such as ours, together with the availability of affordable high-performance digital computation, strongly motivate the transition away from direct signal processing reconstruction algorithms to modern iterative statistical estimators. Note that ZFDC (and, to a lesser extent, LS) copes best with equispaced designs, while MAP works best with optimized angles. This is because the optimized designs leave larger gaps in k -space (see Figure 4). Nonlinear estimators can interpolate across such gaps to some extent, using image sparsity priors. Methods like ZFDC merely interpolate locally in k -space, uninformed about image statistics, so that violations of the Nyquist limit anywhere necessarily translate into errors.
 It is clearly evident that drawing the spiral offset angles at random does not work well, even if MAP reconstruction is used as in [8]. The ratio MAP rd / MAP op in L 2 error is 1.23, 1.45, 2.99, 2.33 in Table 3, upper left. While both MAP op and MAP eq essentially attain Nyquist performance with N shot = 8 , MAP rd does not decrease to that level even with N shot = 16 (not shown). Our results strongly suggest that randomizing MR sequences is not a useful design principle. 11 Similar shortcomings of randomly drawn designs were reported in [12], in a more idealized setup. Reasons why CS theory as yet fails to guide measurement design for real images, are reviewed there, see also [15]. Beyond the rather bad average performance of random designs, the large variance across trials in Table 3 means that in practice, a randomized sequence scan is much like a gamble. The outcome of our Bayesian optimized design is stable, in that sequences found in several repetitions gave almost identical reconstruction performance. The closest competitors in Table 3 are MAP op and MAP eq . Since u true is close to real, both attain close to Nyquist performance up from N shot = 8 . In the true undersampling regime N shot  X  { 5 , 6 , 7 } , MAP op improves signifi-cantly 12 upon MAP eq . Comparing panels b,c of Figure 2, the artifact across the lower right leads to distortions in the mouth area. Under-sampling artifacts are generally amplified by regular sampling, which is avoided in the op-timized designs. Breaking up such regular de-signs seems to be the major role of random-ization in CS theory, but our results show that much is lost in the process. We see that approx-imate Bayesian experimental design is useful to optimize measurement architectures for subsequent MAP reconstruction. To our knowledge, no similar design optimization method based purely on MAP estimation has been proposed (ours needs approximate inference), rendering the beneficial interplay between our framework and subsequent MAP estimation all the more interesting. The computational primitives required for MAP estima-tion and our method are the same. Our implementation requires about 5 hours on a single standard desktop machine to optimize 11 angles sequentially, 256 candidates per extension, with n and d as above. The score computations dominate the running time, but can readily be parallelized. It is neither feasible nor desirable on most current MR scanners to optimize the sequence during the measurement, so an important question is whether sequences optimized on some slices work better in general as well (for the same contrast and similar objects). We tested transferability by measuring five other slices not seen by the optimization method. The results (Table 3, upper right) indicate that the main improvements are not specific to the object the sequence was optimized for. 13 Two spirals found by our method are shown in Figure 4 (2 of 8 interleaves, N shot = 8 ). The spacing is not equidistant, and as noted above, only nonlinear MAP estimation can successfully interpolate achieved by random sampling. We have presented the first scalable Bayesian experimental design framework for automatically optimizing MRI sequences, a problem of high impact on clinical diagnostics and brain research. The high demands on image resolution and processing time which come with this application are met in principle by our novel variational inference algorithm, reducing computations to signal processing primitives such as FFT and gridding. We demonstrated the power of our approach in a study with spiral sequences, using raw data from a 3T MR scanner. The sequences found by our method lead to reconstructions of high quality, even though they are faster than traditionally used Nyquist setups by a factor up to two. They improve strongly on sequences obtained by blind randomization. Moreover, across all designs, nonlinear Bayesian MAP estimation was found to be essential for reconstructions from undersamplings, and our design optimization framework is especially useful for subsequent MAP reconstruction.
 Our results strongly suggest that modifications to standard sequences can be found which produce similar images at lower cost. Namely, with so many handles to turn in sequence design nowadays, this is a high-dimensional optimization problem dealing with signals (images) of high complexity, and human experts can greatly benefit from goal-directed machine exploration. Randomizing param-eters of a sequence, as suggested by compressed sensing theory, helps to break wasteful symmetries in regular standard sequences. As our results show, many of the advantages of regular sequences are lost by randomization though. The optimization of Bayesian information leads to irregular se-quences as well, improving on regular, and especially on randomized designs. Our insights should be especially valuable in MR applications where a high temporal resolution is essential (such as fMRI studies), so that dense spatial sampling is not even an option. An extension to 3d volume reconstruction, making use of non-Gaussian hidden Markov models, is work in progress. Finally, our framework seems also promising for real-time imaging [1, ch. 11.4], where the scanner allows for on-line adaptations of the sequence depending on measurement feedback. It could be used to help an operator homing in on regions of interest, or could even run without human intervention. We intend to test our proposal directly on an MR scanner, using the sequential setup described in Section 2. This will come with new problems not addressed in Section 4, such as phase or image errors that depend on the sequence employed 14 (which could be accounted for by a more elaborate noise model). In our experiments in Section 4, the choice of different offset angles is cost-neutral, but when a larger set of candidates is used, respective costs have to be quantified in terms of real scan time, error-proneness, heating due to rapid gradient switching, and other factors. Acknowledgments We thank Stefan Kunis for help and support with NFFT.
 References
