 Online content recommendation aims to identify trendy arti-cles in a continuously changing dynamic content pool. Most of existing works rely on online user feedback, notably clicks, as the objective and maximize it by showing articles with highest click-through rates. Recently, click shaping [4] was introduced to incorporate multiple objectives in a constrained optimization framework. The work showed that significant tradeoff among the competing objectives can be observed and thus it is important to consider multiple objectives. However, the proposed click shaping approach is segment-based and can only work with a few non-overlapping user segments. It remains a challenge of how to enable deep per-sonalization in click shaping. In this paper, we tackle the challenge by proposing personalized click shaping . The main idea is to work with the Lagrangian duality formulation and explore strong convexity to connect dual and primal solu-tions. We show that our formulation not only allows effi-cient conversion from dual to primal for online personalized serving, but also enables us to solve the optimization faster by approximation. We conduct extensive experiments on a large real data set and our experimental results show that the personalized click shaping can significantly outperform the segmented one, while achieving the same ability to bal-ance competing objectives.
 Categories and Subject Descriptors: H.3.5 [Informa-tion Storage and Retrieval]: Online Information Services General Terms: Algorithms Keywords: Personalization, click shaping, dual plan
A significant portion of content on the web is ephemeral, new items are continuously published through various chan-nels and obsolete ones are removed quickly. For example,  X 
This work was conducted when all authors were affiliated with Yahoo! content on Web portals (e.g., Yahoo!, AOL, and MSN) are refreshed frequently to recommend relevant and timely con-tent to users. With overwhelming amount of fast changing information and unique tastes of individual users, it becomes critical to recommend interesting content to users in a per-sonalized and timely manner.

Traditional information filtering provides personalized rec-ommendation based on historical user ratings on items and includes both content-based filtering and collaborative fil-tering (e.g., [27, 13]). These methods usually work well on a relatively stable item pool, such as movies or products, but they are inferior for online content recommendation which deals with fast changing item pool such as news articles [2]. The main challenge in online content recommendation prob-lems is to quickly identify the most interesting items (e.g., breaking news). In such a setting, it is also hard to collect a data set with explicit user ratings on items. Instead, most of online recommendation works [2, 17] rely on implicit user feedback, notably clicks, and optimize click-rates by show-ing articles with the highest click-through rates(CTR) that are estimated through explore/exploit techniques [16, 5].
However, using CTR as the only objective is not satis-factory because it cannot fully reflect other important en-gagement metrics such as number of page views or the total time-spent on a Website, which are indicative of user satis-faction [8] and also reported by third-party metric companies (e.g., comScore). Recently, click shaping [4] was introduced to tackle the problem of recommending items to jointly op-timize for multiple objectives: clicks and post-click down-stream utilities. The work showed that significant tradeoffs among these competing objectives can be observed and a constrained optimization formulation is an effective imple-mentation. However, the proposed approach in [4] assumes that users are partitioned into a few coarse non-overlapping segments. In many applications a user is characterized by a high-dimensional feature vector (thousands of dimensions with large number of all possible combinations), segmented click-shaping fails to provide recommendations at such gran-ular resolutions (user or fine grained user segments) because decisions are made at coarse user-segment resolutions in epochs  X  in the current epoch (e.g., 10-minute interval), a linear program (LP) is solved to decide a serving plan { x ij } : for every (user segment i , item j ) pair, x ij represents the probability that the system will show item j to user segment i in the next epoch.

While it looks we can apply the LP formulation at granu-lar user resolution by having one variable x uj for each (user u , item j ) pair, such a naive extension is infeasible due the following reasons: (1) This makes the LP ill-defined in the online setting because there are unseen users (user segments) that are observed for the first time in the next epoch. The modified LP has to predict the set of unseen users(unseen user segments) and include corresponding variables in order to serve them in the next epoch; (2) Even if we can accu-rately predict which users(user segments) will be seen in the next epoch, such an extension increases the size of the LP dramatically since there are usually hundreds of thousands users(user segments) per epoch. The segmented models can bypass these issues by assigning a new user to one of the pre-defined segments and use the segment serving plan for the new user. Thus, there are inherent limitations to directly utilize segmented click shaping for personalization. On the other hand, when optimizing clicks is the only objective, per-sonalized models [22, 3] usually achieve much better results than segmented models. How to enable personalization in click shaping is a challenging, but important research prob-lem that we address in this paper.

In this paper, we propose personalized click shaping which combines deep personalization with click shaping for multi-objective online content recommendation. Instead of work-ing with the primal serving plan x ij  X  X , our key idea is to convert the primal problem through Lagrangian duality and obtain the dual serving plan. By exploiting strong convexity of our objective function, we show that the dual plan can be efficiently converted to the primal plan for online person-alized serving. Furthermore, our dual formulation can also enable us to solve the optimization faster through approxi-mation. Specifically, we make the following contributions in this paper:  X  In Section 3, we propose a novel technique to solve  X  In Section 4, we develop a variety of approximation  X  In Section 5, we empirically show that personalized click
In this section, we describe the problem setting and review segmented click shaping [4].
 Application setting: Consider a content recommendation module on the front page of a portal such as Yahoo!. In each bucketized time epoch t , there is a set of candidate items available for recommendation, denoted by A t . Each item j  X  A t belongs to one of K different properties of the portal (e.g., Yahoo! News, Finance, Sports, etc). Let P = { P 1 , ..., P K } denote the set of properties and j  X  P means the landing page of item j belongs to property P k . Let U t denote the set of all users in epoch t . A user u  X  U visits the front page and is recommended an item from A t We only consider single item recommendation in this paper; multi-item problem is more involved and left as future work. When a user clicks the recommended item, it routes her to a page in the property that the clicked item belongs to. User visits to different properties are usually significantly influenced by the clicks on the front page. For the sake of illustration, consider that the portal wants to optimize for two different objectives  X  (a) total number of clicks on the front page and (b) total time-spent on landing properties by users who click recommended items on the front page. Note that other objectives can be incorporated in our framework. Explore/exploit: Content optimization is an explore/exploit problem. To optimize for any metric, we need to estimate the performance of each candidate item in terms of that met-ric. Without displaying an item to any user, it is difficult to know the performance of that item. The explore/exploit problem for a single objective is well studied (e.g., [3, 18]). Developing explore/exploit methods for multiple objectives that ensure certain notion of optimality is nontrivial. In this paper we assume some explore/exploit scheme is run-ning in the system. In particular, we use a simple scheme  X   X  -greedy, which has been empirically shown to achieve good performance [26]. This scheme works as follows: We serve a small fraction of randomly selected user visits (called explo-ration population) with items selected uniformly at random from the current content pool in order to collect data for ev-ery item. For the remaining visits (called exploitation pop-ulation), we serve the item with the highest estimated click-through rate (CTR) if the goal is to maximize clicks. With multiple objectives, the serving scheme for the exploitation population is different from that of displaying the highest CTR item as we shall see later in this paper.
 Segmented models: An important assumption made in the segmented click shaping work [4] is that users are clus-tered into m segments. Let S denote the set of segments and i  X  S denote a segment in the rest of the paper. To measure the utility of an item j , statistical models are used to estimate: (a) the probability p ijt that a user in segment i would click item j when it is displayed in epoch t , and (b) the time d ijt that a user in segment i would spend post-click in the landing property of item j . We call p ijt CTR and d time-spent. Note that if the goal is to maximize clicks (or time-spent, but not both), the optimal solution is to recom-mend the item j with the highest p ijt (or p ijt  X  d ijt ) to users in segment i .
 Segmented serving scheme: We call an algorithm that recommends items to users a serving scheme. For each epoch t , a segmented serving scheme uses information obtained before the epoch to produce a segmented serving plan x t = { x ijt : i  X  S , j  X  A t } , where x ijt is the probability that the serving scheme will recommend item j to users in segment i in epoch t . When a user visits the front page in epoch t , she is first assigned to an appropriate segment and then an item is served through a multinomial draw according to { x ijt j  X  A t } . It should be clear that x ijt  X  0 and P j x ijt 1. Different optimization methods generate different serving plans according to different criteria. For example, the click maximization method would set x ij  X  t to 1, if j  X  has the highest CTR item, and 0 for the remaining items. Note that the serving plan for epoch t is made before epoch t , i.e., in epoch t  X  1 in our application setting.
 Objectives: Recall that we consider two objectives in this paper, number of clicks and time-spent. Let N t denote the total number of visits during epoch t , and let  X  t = (  X  1 t ,  X   X   X  ,  X  mt ) denote the fraction of visits in different user segments. Obviously, P i  X  X   X  it = 1 and N t  X  it is the total number of visits of segment i . Usually,  X  t can be estimated based on past user visits [4]. For succinctness, we drop sub-script t from the notations since we always consider the cur-rent epoch t . For example, x = { x ij } is the serving plan for the epoch t . Given serving plan x , the two objectives are:  X  The expected total number of clicks on the front page:  X  The expected total time-spent on property P k : Click maximization scheme: The status quo algorithm is to optimize the total number of clicks. We can obtain the serving plan as z = { z ij } in the following, We use TotalClicks  X  = TotalClicks ( z ) and TotalTime  X  ( P TotalTime ( z , P k ) to denote the values of the two objectives for the click maximization scheme; they are constants since z is a set of constants defined above.
 Linear program: Although a variety of multi-objective programs (MOP) were introduced in [4], we only discuss the most flexible formulation localized multi-objective program (  X  -MOP) in this paper. Formally, the optimization problem is given below. where I is a subset of P , for which we want to ensure certain level of time-spent. This linear program seeks to maximize the total time-spent on all properties, such that the potential loss in the total number of clicks is bounded by  X  (0  X   X   X  1) compared to the status quo click maximization scheme. For each of the key properties P k  X  I , the total time-spent on P k is guaranteed to be at least  X  (0  X   X   X  1) times that of the click maximization scheme.
 Summary: It is important to note that any linear con-straint can be added to the linear program. The above choice is useful in our application setting and serves as a running example for illustration purposes. In every epoch, we use statistical models to predict p ij , d ij and  X  i , and then solve the linear program to obtain the serving plan x for the next epoch. Thus, user visits in the next epoch will be served according to the plan made before we actually see the users.
As shown above, linear programs can be effectively used to perform segmented click shaping. However, such a seg-mented serving scheme treats all the users in a segment in the same way  X  it lacks the ability of serving each individual user differently to satisfy his/her unique personal informa-tion need. In this section, we extend click shaping beyond user segments to provide personalized serving plan for indi-vidual users.
In this section, we define the primal formulation of person-alized click shaping by extending the segment-based linear program in Equation 4 through redefining the form of serv-ing plan x .
 Personalized serving plan: For personalization, we have a user-specific serving plan for each user u , instead of segment-specific ones. Thus the personalized serving plan is defined as x = { x uj : u  X  U , j  X  A} , where x uj is the proba-bility that user u will be served with item j in the next epoch. Since the x uj  X  X  are probabilities, we have x uj  X  0 and P j x uj = 1. A personalized serving scheme serves items to each individual user according to these probabilities. Objectives: Because the form of serving plan x changed, the definitions of TotalClicks ( x ) and TotalTime ( x , P need to be updated.
 where p uj is the probability that user u would click item j and d uj is the length of time that user u would spend on pages in the landing property of item j after he/she clicks the item. Both the estimation of p uj and d uj are orthogo-nal to our problem formulation and any personalized statis-tical model such as online regression proposed in previous works [3, 2] can be plugged in. Based on CTR estimation p uj , we can compute the click optimization serving scheme similarly to Equation 3 and define the status quo constants TotalClicks  X  and TotalTime  X  for personalized serving ac-cordingly. Please note that if we instantiate p uj and d using a segment-based model (i.e., p uj = p ij and d uj = d for all u in segment i ), Equation 5 will reduce to Equation 1 and Equation 2.
 Challenges: With these new definitions of x , TotalClicks and TotalTime , on the surface, Equation 4 can be trivially applied to personalized click shaping: We can solve this lin-ear program in each epoch and use the solution to serve users in the next epoch. However, such a formulation is challenging for the following reasons:  X  Unseen users: In the linear program, the variables  X  Scalability: This linear program requires a set of vari-Key idea: To tackle these challenges, we exploit Lagrangian duality formulation of our constrained optimization prob-lem. In the primal program of Equation 4, x uj  X  X  are the primal variables. Although we have a large number of user-specific primal variables, there are only a small number of nontrivial constraints in the primal formulation. Our key idea is to explore Lagrangian duality to capture the pri-mal variables by using a small number of user-independent dual variables, one per constraint in the primal program. Now suppose that we can efficiently compute the optimal dual solution for the next epoch (which will be discussed in Section 4). Also suppose that in the next epoch, we can efficiently convert the dual solution on the fly to the pri-mal solution (i.e., serving plan) for each individual user at serving time. Then the challenges are solved.

Unfortunately, linear programs do not allow easy conver-sion from dual solutions to primal solutions and vice versa because the derivatives of the Lagrangian vanish [7]. To en-sure convertibility, we slightly modify the originally linear objective function so that it becomes strongly convex. Intu-itively, let q = { q uj : u  X  U , j  X  A} be some baseline serving plan. We add terms to the objective function to penalize serving plans x that are far away from q . There are a few choices for the baseline q . One is the click maximization plan z . Another is the uniform serving plan q uj = 1 / |A| , which will encourage some notion of  X  X airness X  among items. These penalty terms can be the L2 norm or KL divergence. We focus on the L2 norm penalty term and the uniform serving plan q in this paper. Revised primal program: After adding the penalty term, we obtain the revised primal program. where  X  specifies the importance of the penalty. Such a modification is generic since it can be applied to any linear program based click shaping. Notice that we also change from maximization to its equivalent minimization to put the primal program in the standard form of quadratic program-ming problem. In some sense, the added penalty term serves as regularization and can potentially reduce the variance of the solution.
We now introduce the dual variables and an algorithm to efficiently convert a dual solution to its corresponding primal solution. For ease of exposition, let The Lagrangian function of the primal program is  X ( x ,  X  ,  X  ,  X  ) = 1 2  X  P u P j ( x uj  X  q uj ) 2  X  P u where  X  0  X  0,  X  k  X  0, for all k  X  I and  X  uj  X  0, for all u and j . This is obtained by expanding TotalTime and TotalClick according to Equation 5 and applying the Lagrange multi-pliers  X  0 to ensure the total click constraint,  X  k to ensure the per-property total time constraint,  X  u to ensure P j x uj and  X  uj to ensure x uj  X  0. Note that  X  0 ,  X  k ,  X  u and  X  also called the dual variables .
 Note that 1 { True } = 1 and 1 { False } = 0. Thus, if we have the dual solution for  X  0 ,  X  k ,  X  u and  X  uj , we can reconstruct the primal solution x uj . However, this does not help to solve the challenges since  X  u and  X  uj still depend on the users u in the next epoch. In the following, we provide an efficient al-gorithm that reconstructs x uj from  X  = {  X  0 ,  X  k }  X  k  X  X  without the need for  X  u and  X  uj .
 Dual serving plan: We call  X  the dual serving plan , which does not include any user-specific variables. The algorithm that converts a dual plan to its corresponding primal serving plan is based on the following proposition.
 Proposition 1 In the optimal solution, given user u and two items j 1 and j 2 , if c uj 1  X  c uj 2 and x uj 2 &gt; 0 , then x Proof sketch: According to the KKT conditions, at the op-timum point,  X  uj 2 = 0 since x uj 2 &gt; 0. Then, because c Without loss of generality, for each user u , we reindex items such that c u 1  X  c u 2  X   X   X   X  c un , where n is the number of items. Notice that this ordering is user-specific. Based on Proposition 1, there exists a number 1  X  t  X  n such that, in the optimal solution, x uj &gt; 0 for j  X  t and x uj = 0 for j &gt; t . To find the value of t , we check from t = 1 to n whether the following linear system has a feasible solution: Notice that  X  uj = 0 if x uj &gt; 0. The largest t value that still gives a feasible solution is what we are looking for. By some algebra, given t , we have  X  u = (  X   X  P t j =1 c uj ) /t. The above system is feasible if the smallest x ut &gt; 0; i.e., Algorithm 1 Conversion algorithm Input : dual plan  X  and an incoming user u Output : primal plan { x uj } 1: Predict p uj and d uj for each item j 2: Compute c uj based on  X  , p uj and d uj 3: Order items by c uj so that c u 1  X  c u 2  X  . . . 4: Set a =  X  and t = 1 5: repeat 6: if c ut + ( a  X  c ut ) /t  X  0 then 7: t = t  X  1 and break 8: else 9: a = a  X  c ut and continue 10: end if 11: t = t + 1 12: until t  X  |A| 13:  X  u = a/t 14: for j = 1 to t do 15: x uj = ( c uj +  X  u ) / X  16: end for 17: return { x uj } Conversion algorithm: Given dual plan  X  and an incom-ing user u , the primal serving plan { x uj } for u is obtained by the conversion algorithm which is summarized in Algo-rithm 1.
 Proposition 2 If the input dual plan  X  is optimal for a set of users, then the output serving plan from the conversion algorithm is also optimal for the same set of users. Proof sketch: Based on dual variable  X  , the conversion al-gorithm gives us  X  and x . We can further compute  X  uj for all x uj = 0 based on Equation 7. It can be verified that all these values satisfy all the KKT conditions.  X 
The complexity of the conversion algorithm is dominated by the prediction of p uj and d uj and the sorting of c uj Since the number of items is usually small (from hundreds to thousands), the conversion algorithm is very efficient. Fur-thermore, the computation can be easily paralleled by users because each user can be computed independently from oth-ers.
 Discussion: By comparing the above formulation with a simple scalarization that linearly combines different objec-tives, their similarities lie in the weights  X  k  X  X  which can be thought as the importance for different objectives/constraints. However, scalarization is limited in its ability to obtain all the meaningful Pareto optimal points because it does not allow for fractional serving [4, 7]. The existence of  X  u our formulation can achieve fractional serving and thus can move the Pareto optimal solutions in a more controlled way than scalarization.
Since we do not observe all the users who will visit in the next epoch, we can only solve the QP approximately. The main idea is to exploit the observation that the distribution of users in the next epoch is similar to that in the current epoch. This is a mild requirement because our epoch is usu-ally of short duration (10 minutes in our paper) and user population usually does not change significantly. It is im-portant to note that, if we use the primal solution directly for serving, we can only serve the set of seen users because we do not have primal variables for unseen users. However, if we use the dual plan for serving, we can convert it to the per-user serving plan on the fly for all users. Our dual plan only requires that the sets of user features in the two adjacent epochs are statistically similar(we do not require users themselves to be similar). For this reason, our dual formulation can even work with a sample of users. In this section, due to the potentially large number of users in an epoch, we explore two techniques to reduce computational cost: clustering and sampling.
Our goal is to obtain the dual solution for the QP defined in Equation 6. To reduce the QP problem size, an intuitive choice is to cluster users and then we have a small number of primal variables. Specifically, we define a set of primal vari-ables { x ij }  X  j  X  X  for each cluster i , instead of each user u . An off-the-shelf QP solver is then used to find the dual solu-tion  X  of the small QP problem. Finally, at serving time,  X  is used to compute the personalized serving plan { x uj } for each individual user u using Algorithm 1. Two clustering methods are considered:  X  k -Means: The standard k -means algorithm is applied  X  Top1Item: This method puts all the users having the After we get the user clusters, we approximate the QP as fol-lows: for each cluster i , we estimate p ij and d ij by averaging the per-user p uj and d uj over users u in the cluster. Also, to ensure feasibility, the baseline performance TotalClicks and TotalTime  X  are defined based on the click maximization scheme in Equation 3. We then solve the QP to get the dual optimal values  X  .
Another way to reduce computational cost is to down-sample users. We consider two types of sampling meth-ods:  X  Random sampling: Given a sample rate r , we ran- X  Stratified sampling: Given a set of user clusters, After sampling, we obtain a subset of users in the current epoch. When we solve the QP, we define a set of primal variables { x uj }  X  j  X  X  only for sampled users u . Again, we up-date the baseline performance TotalClicks  X  and TotalTime by only considering the sampled users. Then, the small QP is solved to obtain the dual solution  X  which is used by the conversion algorithm for online personalized serving.
In this section, we compare different methods for click shaping as summarized in Table 1. Note that Segement-LP relies on the primal optima directly for segmented serv-ing, while kMeans-QP-Dual and Top1Item-QP-Dual rely on dual optima for personalized serving. We report our exten-sive empirical results using an unbiased evaluation method-ology [18] based on Yahoo! Front Page log data. We will mainly compare the personalized click shaping with the most effective segmented ones in prior work [4]. We also compare the proposed methods in terms of their abilities to enforce constraints and achieve desired tradeoff between clicks and time-spent. Data: Our data set is derived from Yahoo! Web server logs, which record users X  clicks and views of items displayed in the Today module on the Yahoo! Front Page. The data is collected from a  X  X andom bucket X  of users in August 2010 for the purpose of evaluating different serving schemes. A random sample of users was assigned to the random bucket, in which each user visit was served with an item uniformly randomly picked from an editor-selected item pool. It is important to note that, based on the replay methodology described in [18], this random bucket data allows us to per-form provably unbiased comparisons among different serv-ing schemes. Around 2 million click and view events were collected per day. To compute downstream time-spent, we also collected post-click information on all the pages that a user visited within Yahoo! after clicking on a Today mod-ule item. Each user is identified by an anonymized browser-cookie and has a profile consisting of demographics (age and gender) and his/her affinities to different categories (such as sports, finance and entertainment) based on his/her activ-ities throughout the Yahoo! network. No personally iden-tifiable information is used in our experiments. To create user segments or clusters, we use the best performed method proposed in [4]. Specifically, we collect a history data of 10 days in April 2010 and create an activity vector for each user. Each entry in the vector corresponds to an item in the history data and the value is the predicted CTR of the item based on the user profile. Users are then clustered using the k-means algorithm based on the activity vectors and new users are assigned to a cluster based on cosine similarity. This applies to all k-means related methods.
 Metrics: We define the time-spent d uj of a user u after
Table 2: Comparison of time-spent prediction. clicking article j  X  P k as the length (in second) of the session of the user X  X  events, that starts from the click and ends at the last page view inside property P k , before the user either leaves the property or has no activity for more than 30 min-utes. For confidentiality reasons, we cannot reveal the total number of clicks or total time-spent. Thus, we only report the relative CTR and relative time-spent as defined below. After running a replay experiment using serving scheme A , we compute the average number of clicks per view p A (i.e., CTR) and the average time-spent per view q A . Fixing one baseline algorithm B , we report the performance of algo-rithm A by two ratios: CTR ratio  X  CTR = p A p  X  Estimation of p ujt and d ujt : Predicted CTR p ujt and time-spent d ujt are necessary input to our quadratic pro-gram. The choice of statistical models for such prediction is orthogonal to the methods presented in this paper and any model can be used here. In our experiments, we use the online logistic regression (OLR) model proposed in [3] to predict CTR p ujt based on the feature vector (i.e., the profile consisting of demographics and category affinities) of user u . One OLR model is trained for each item and updated per epoch, in order to quickly capture the unique behavior of each item that does not generalize well through item fea-tures. Time-spent d ujt is predicted by an age-gender model. We discretize ages into 10 groups and have 3 gender groups (male, female, and unknown); this gives 30 groups in total. For each group, we use a dynamic Gamma-Poisson model to track the mean of time-spent d ujt on item j by a ran-dom user u in the group. Note that because time-spent is very noisy, this simple age-gender model cuts down the vari-ance and provides good prediction performance. More fine-grained models such as building an linear regression model on user features for each item were tested, but did not pro-vide improvement: Table 2 compares the Mean Absolute Er-ror (MAE) and Root Mean Square Error (RMSE) of the two methods using 2-fold cross validation. Age-gender model performs just slightly better. A better time-spent model needs further research and is not the focus of this paper. Advantage of the personalized click shaping: We first show the that personalized click shaping significantly out-performs segmented click shaping in Figure 1. We start with a simplified special case that only considers the tradeoff be-tween total clicks and total time-spent and does not have any per-property constraint. We show the results of Random-Sampling method with sampling rate r = 20% and  X  = 0. The tradeoff curve is generated by varying  X  from 1 to 0 (where each point is produced by averaging over 5 sampling runs). In this simplified setting without per-property con-straints, scalarization is also applicable: serving each user u with the item j that has the highest weighted sum of the two objectives:  X   X  p uj +(1  X   X  )  X  p uj d uj . By varying  X  from 1 to 0, we obtain a tradeoff curve for scalarization in Figure 1. Each specific value of  X  or  X  gives us a serving scheme that, Figure 1: Personalized vs the segmented methods.
 after running through the data set, generates a pair of CTR ratio and TS ratio. To compare with segmented click shap-ing in this setting, we also plot the tradeoff curve using the scalarization method with different number of clusters. All the implementation details are the same as [4]. These meth-ods are labeled as k clusters in the figure. From this figure, we can see that both versions of personalized click shaping outperform all the segmented click shaping on the tradeoff curves. For example, when  X  = 1 and  X  = 1, the person-alized models achieve 2% CTR lift (statistically significant at level 0.1) and 4% TS lift (statistically significant at level 0.05). Thus working with personalized models is beneficial. Segmented click shaping achieves the best performance when the number of cluster is 30. When the number of clusters be-come large, the performance drops mainly because we have few users in each cluster and the component estimation of p ijt and d ijt (based on Gamma-Poisson) has high variance due to smaller sample size.

Figure 1 shows that both Random-Sampling and scalar-ization achieves similar tradeoff. This means that our dual formulation combined with sampling-based approximation is very effective to trade off competing objectives. In Table 3, we show the temporal variances of both methods on the TS ratio and CTR ratio over all the epochs. As can be seen, the variances of scalarization are much larger than those of Random-Sampling, meaning that Random-Sampling is a more stable method. This observation is similar to [4] where primal plans are used. Thus our dual formulation enjoys the same desirable property of constrained optimiza-tion, which makes sure no significant deviation from status quo performance in most epochs. It is important to note that scalarization cannot provide detailed control over per-property performance constraints (as shown in [4]). Thus, we do not further discuss scalarization in the following. Constraint satisfaction: We have proposed a variety of approximation methods for personalized click shaping. A main question is whether they can satisfy the per-property constraints. An approximation would not be useful if it can-not satisfy the constraints well. In this set of experiments, we set  X  = 1. Given an  X  value and a property P k , we look at the relative difference in time-spent on that property be-tween an approximation method and the baseline click max-imization scheme  X  ( P k ) &lt; 0 means that the constraint on property P k is violated and |  X   X  ( P k ) | represents the degree of violation. We then define a satisfaction measure over all the per-property constraints as where V = { k |  X   X  ( P k ) &lt; 0 , k  X  I} is the set of properties which constraints are violated.

In Figure 2, we set  X  = 0 . 95 and plot the satisfaction measure for each method. We set the sampling rate r = 20% for all sampling-based methods. As can be seen, all the methods except for clustering-based dual methods have  X   X   X  1 and thus satisfy the per-property constraints very well. In particular, all sampling-based dual methods sat-isfy the constraints well. However, clustering-based dual methods (kMeans-QP-Dual and Top1Item-QP-Dual) signif-icantly violate the constraints. To further understand this behavior, we plot kMeans-QP-Dual and Top1Item-QP-Dual in Figure 3 for each individual property using  X   X  ( P k ) with  X  = 0 . 95. It can be seen that both do not respect the constraints and gives some properties more traffic than suf-ficient, while gives others less than necessary. This means that the clustering we tried is not a good approximation for personalized click shaping.
 Tradeoff comparison: In Figure 4, we compare the trade-off curves of different methods with per-property constraints by setting  X  = 1. Each curve is generated by varying  X  from 1 to 0. We will show that stratified sampling methods have very similar performance to Random-Sampling. Thus, for plot clarity, we only show the tradeoff curve for Random-Sampling (for which, we ran the experiments 5 times and show the average curve).

From this figure, we can see that the Random-Sampling method performs much better than the Segmented-LP method, under the per-property constraints. We can also see that Random-Sampling have the same ability to trade off the two competing objectives as Segmented-LP. Our Random-Sampling is personalized click shaping based on a dual plan,
Figure 3: Per-property satisfaction for clustering.
Figure 4: Tradeoff curves of different methods. while Segmented-LP is based on a primal plan. This result confirms that our Lagrangian duality formulation is not only mathematically sound, but practically effective to incorpo-rate personalization into click shaping.

In this figure, we also show the results of kMeans-QP-Dual for contrast. It can be seen that clustering-based approxima-tion is not able to trade off the two objectives appropriately. The sharp jump in the tradeoff curve is a by-product of violating the per-property constraints as shown in the pre-vious section. To confirm the correctness of our results, we also show the primal variation of k-means based approxima-tion: kMeans-QP-Primal. This method shows a reasonable tradeoff curve. The difference between kMeans-QP-Dual and kMeans-QP-Primal is that the former uses dual plan but the latter uses the primal plan. The latter method is segmented because the primal serving plan is only for seg-ments and each user needs to be assigned to a segment before served. This result again confirms that our clustering-based approximation is not an effective approach to approximate the Lagrangian duality.
 The impact of  X  : In Figure 5, we set  X  = 0 . 95 and use the Random-Sampling method to show the impact of the parameter  X  . It can be seen that when  X  is less than 1, the results are not sensitive to the choice of  X  . When  X  is too large, the penalty term has high weight and thus the TS lift gets smaller. In all the experiments we set  X  = 0 . 001. Relaxation of the per-property constraints: In Fig-ure 6, we show the tradeoff curves for  X  = 1 and  X  = 0 . 95. Figure 6: The tradeoff curve of different  X  values. For each parameter setting, we have 5 runs of experiments and we show the mean of these 5 runs. We also plot the error bars of TS ratio. It can be see that we can achieve a better tradeoff when  X  is smaller because the constraints are less restrictive. We also show the  X   X  =0 . 95 for both meth-ods in Figure 7. It can be seen that the Random-Sampling method can roughly guarantee the per-property constraints. For  X  = 1,  X   X  are almost all non-negative. For  X  = 0 . 95,  X  are almost all  X   X  0 . 05.
 Comparing different sampling methods: In Figure 8, we study the sampling ratios for Random-Sampling. As ex-pected, when we have a larger number of samples, our ap-proximation becomes better, thus a better tradeoff curve. In Figure 9, we show the (relative) running time needed for one run of our replay and we compare different sampling ratios. It can be seen that the running time is super-linear and this shows that sampling is necessary for our method. As shown in Figure 8, 20% sampling rate can achieve similar results as 50% sampling rate and thus sampling is an effec-tive way of reducing the time complexity while retaining the effectiveness of our methods.

In Figure 10 and Table 4, we compare different sampling methods based on 5 sampling runs with 20% sample rate. Figure 10 shows the tradeoff curves averaged over the 5 runs and we can see that different sampling methods perform comparably with each other. In Table 4, we compute the click and TS variance as follows: For each  X  , we compute the standard deviation over the 5 runs. The reported re-Figure 7: The impact of  X  on time-spent difference. sults are the average values of the standard deviation over all the  X   X  X . From this table, we can see that stratified meth-ods have lower variance than random sampling. Among the two stratified methods, Stratified-kMeans has lower variance than Stratified-Top1Item. This means that k-means creates more homogeneous clusters than the Top1Item method. Top N Properties: In Figure 11, we relax the number of per-property constraints in the Random-Sampling method and show the results when we only have the per-property constraints for the top 3, 5, and 7 properties. It can be seen that when the number of per-property constraints is reduced, we can have a better tradeoff curve. This again shows the effectiveness of dual formulation for personalized click shaping.
Our work is related to the broad category of Web content recommendation, which is to recommend users with person-alized and timely content from a dynamic candidate pool such as news stories. For example, traditional collaborative filtering techniques [13, 15] have been extended to predict user clicks for news personalization [10, 20, 19]. To quickly identify interesting content, existing work such as [3, 2, 17] leverages explore/exploit methods [6, 16, 5] to actively col-lect user feedback (e.g., clicks) on new content in order to quickly estimate the performance of new content and max-imize the number of clicks in the long run. The focus of
Table 4: Average variance of sampling methods. our paper is not on accurate prediction of user clicks, but on how to jointly optimize multiple objectives. Other work such as [1, 22] uses latent factor model to combine both fea-tures and interaction activities to handle both warm-start and cold-start scenarios. Although all these methods pro-vide personalized recommendation, they only consider a sin-gle objective in their problem formulations (e.g., clicks). To the best of our knowledge, there is little prior work that attempts to recommend personalized content to simultane-ously optimize multiple objectives. The previous work [4] introduces the problem of click shaping. However, the pro-posed methods only work with user segments, instead of individual users. A main contribution of our paper is to close the gap between personalization and multi-objective optimization in Web content recommendation.

Our constrained optimization problem is similar to the guaranteed delivery problem [25, 9] in display advertising, where incoming users are allocated to see different ads in order to optimize ad-related utilities. For example, Vee et al. [25] consider multi-objective programming and provide techniques to simultaneously optimize for revenue of rem-nant inventory and overall ad quality delivered to advertis-ers. Our use of duality is similar to that of [25], but our content recommendation setting is very different from their advertising setting. Also, the focus of [25] is on theoreti-cal properties, while the focus of this paper is to provide extensive experiments on real data.

Multi-objective optimization is also discussed in a number of other problem settings. For instance, auctions in spon-sored search incorporate both revenue (measured by bids) and ad quality (measured by CTR) for ads ranking [11]. A common approach is simply to rank ads by the product of bid value and CTR. Sculley et al. [21] also study ad CTR and quality in sponsored search and define a new measure called bounce rate to capture ad quality by inferring aban-donment rate on ad landing page. However, this study is exploratory and focuses on bounce rate prediction, instead of multi-objective optimization. Besides online advertising, Jambor and Wang [14] consider constraints like limited sup-ply of the items in a collaborative filtering setting; Svore et Figure 10: Comparison of different sampling meth-o ds (sampling ratio=0.2). a l. [24] consider several objectives in learning to rank. These two studies are in a static setting, different from our online recommendation setting.

Finally, we note that there is a rich literature in multi-objective programming [23], convex optimization [7], and stochastic optimization [12]. Our contribution is in the ap-plication of optimization techniques to effective online per-sonalized content recommendation.
In this paper, we study the problem of personalized click shaping which combines deep personalization with click shap-ing. We have shown that Lagrangian duality can be ef-fectively used to solve the two challenges in applying con-strained optimization to personalized serving  X  unseen users and scalability. By slightly modifying the objective func-tion to achieve strong convexity, we are able to efficiently convert a dual plan , which consists of a small number of user-independent dual variables, to its corresponding primal serving plan at the time when a new user comes. Based on extensive experiments on a large real dataset, we show the set of Pareto optimal points for personalized click shaping significantly outperforms and uniformly dominates the ones for segmented click shaping.

Interesting problems for future work include how to ex-tend our method to the scenario where multiple items can be simultaneously recommended at multiple positions on a portal page, and how to extend the current per-epoch con-straints (e.g., requiring click loss to be bounded for every epoch) to long-term constraints (e.g., only requiring the to-tal click loss in any n -epoch period to be bounded).
