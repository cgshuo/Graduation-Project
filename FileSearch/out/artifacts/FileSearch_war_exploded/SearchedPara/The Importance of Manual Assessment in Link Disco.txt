 Using a ground truth extracted from the Wikipedia, and a ground truth created through manual assessm ent, we show that the appar-ent performance advantage seen in machine learning approaches to link discovery are an artifact of trivial links that are actively rejected by manual assessors. H.3.4 [ Information Storage and Retrieval ]: System and Soft-ware  X  Information networks, Perfor mance evaluation (efficiency and effectiveness). Documentation, Performance, Ex perimentation, Human Factors Wikipedia, Link Disc overy, Assessment, Evaluation, INEX. Maintenance of hypertext links between documents in a centra-lized document repository is probl ematic for several reasons: when new documents are added old documents must be updated to point to the new; when old documents are deleted all links to the deleted document must be remove d; if the topical content of a document changes over time then li nks must be added, deleted, and updated. In a growing collectio n, such as the Wikipedia, the maintenance can quickly become more time-consuming than add-ing new content. This maintena nce requirement was motivation for the INEX Link-the-Wiki track, a standard evaluation forum for automated link discovery in a closed document repository. The track methodology proceeds as follows: Take a snapshot of the Wikipedia. From that snap shot, extract one document and eradicate all links to and from that document from and to the col-lection (orphan the document). Usi ng the orphan as an IR topic, identify a ranked list of links to (and from) that document into (and out of) the collection. Repeat the process a large number of times. Finally, measure the pe rformance of the link discovery system against the ground-truth as is in the pre-orphaned docu-ments. Different from the work of Milne &amp; Witten [5] and of Mihalcea &amp; Csomai [4], a ranked list of links is required because INEX considers link detection systems to be recommender sys-tems and as such assumes a human will read a list of results. After two years of the track it appeared as though identifying high quality outgoing links was solved. Jenkinson et al. [3] submitted a run based on the work of Itakura &amp; Clarke [2] and of Geva [1] which scored a mean average precision (MAP) score of 0.73. The run maintained high precision even at moderately late points of recall (for example, a precision of 0.85 at a recall of 0.5). High against the two sets to result in higher performance against the MANUAL set  X  assuming all AUTOMATIC links are relevant. In total 30 runs were submitted, pools contained between 405 and 1722 links. Figure 1 shows the soft ware especially designed for assessment; on the right is the pool, left the orphan, and middle is the link target document. Assessors selected links and then marked anchors, targets, or both as relevant or not. We estimate that between 4 and 6 hours was spend assessing each topic. On average 7.4% of a pool was judged relevant. Two fundamentally different appr oaches to link discovery are seen in the INEX runs, our analysis is on one of each approach: Anchor link analysis is due to Itakura &amp; Clarke [2]. First, all anc-hor-texts and target documents used in the collection are identi-fied. Next, the document frequencies of the anchor-text in the whole collection are identified. Fi nally, the anchor-texts from the collection are identified in the orphan and the most probable target document chosen. Links are ranked on ratio of the target docu-ment frequency to anchor text document-frequency. We use the The uncorrected run ranked 1 st at INEX 2008. Page name analysis is due to Geva [1]. If a document title is ever seen in the orphan then a link to the document is added. Prefe-rence is given to longer over shor ter titles. We us e the corrected Geva run, QUT . The uncorrected run ranked 13 th at INEX 2008. A third run, Wikipedia , was artificially generated directly from the orphan documents by using just the first 50 links seen in the pre-orphaned documents. This is the best possible run. The precision recall graph in Fi gure 2 shows the performance of the three runs against the AUTOMATIC assessments. As ex-pected, run Wikipedia scores perfectly, Otago performs well, and QUT performs adequately. In Figure 3 the same three runs are assessed using the MANUAL set. It can be seen there that the runs are tightly clustered wh ereas using AUTOMATIC assess-ment they are not. It can also be seen that the run derived from the Wikipedia performs little-better than the other two. A two-tailed t -test shows all runs significantly differ (at 1%) from each other using AUTOMATIC assessment but no run is significantly differ-ent from any other (even at 5%) using MANUAL assessment. Run Otago performs very well agains t the AUTOMATIC assess-ment set, but Trotman 2 recently showed the performance of that run is limited by the 50-targets track restriction. Near perfect scores can be achieved using anchor link analysis . Shown here, however, is that the same run does not perform as well when as-sessed against the MANUAL se t  X  and neither does run Wikipe-dia ! There are several reasons wh y this might be observed: Some of the hypertext links present in the Wikipedia documents are trivial (such as dates). When the human assessors in the expe-Corrected after INEX but before the published proceedings. Unpublished, but in his pr esentation at INEX 2008. 
