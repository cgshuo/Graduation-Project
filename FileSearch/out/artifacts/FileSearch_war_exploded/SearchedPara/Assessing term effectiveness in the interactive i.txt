 1. Overview
This paper addresses the question of whether there is a relationship between the method by which query terms are produced and their effectiveness in the information access process. Query terms are strings of text that (1) represent an information need, and (2) serve as input to any kind of information access system. Query terms, as discussed in this paper, are units of what have been called  X  X  X ndex languages X  X  in information retrieval (IR) research (e.g., Cleverdon, 1967; van Rijsbergen, 1979 ).

In contradistinction to earlier studies of index languages, which treat index languages as lists of terms, we adopt the linguistically-motivated view that query terms are products of the method used to generate them ( Chomsky, 1965; Chomsky, 1957 ). We refer to the systems that produce query terms as query languages ; these systems may be cognitive, in which case the processing is mental and not amenable to direct observation, or they may take the form of a computer algorithm.

Researchers such as Salton (1986); Croft (1989) and Sparck Jones (2005) have contended that experimental evidence suggests that different types of index languages do not significantly affect the outcome of the search process; their claims are based on evaluations of system effectiveness that use the Cranfield model, in which real users do not participate (Cleverdon 1967). We study instead the question of whether the method by which query terms have been identified has an impact on their effectiveness in an interactive information access sys-tem. We used two user-centered measures: correctness and time. Correctness is a measure of whether terms lead participants to correct answers; query languages that lead information seekers to correct answers are more effective than those that do not. Time is a measure of how long it takes participants to find correct answers; query languages that allow information seekers to find correct answers more quickly are more effective.

Our results demonstrate that two sets of query terms (HS and HUM) were significantly more effective than the third (TEC). HS and HUM were more effective than TEC with regard to correctness at a significance level of .05; HS helped participants find correct answers more quickly than TEC terms did at a significance level of .05. The query languages that produced the three sets of terms and the measures of effectiveness are described in greater detail below. We conclude that query languages do have an impact on user effectiveness in the inter-active access process.

At a practical level, this research provides a method for assessing query language effectiveness for real users performing real tasks. The assessments obtained by this method may be useful to information professionals who develop automatic indexing software, build indexes manually, and/or purchase information access sys-tems ( Wacholder and Song, 2003 ). At a theoretical level, this research advances our understanding of the impact of query languages and query terms on the outcome of the interactive information access process. 2. Query term effectiveness
In general, researchers who have assessed the effectiveness of systems that identify query terms have con-cluded that the effort required to build more complex languages is not worth the effort. For example, Keen (1973) compared five index languages and found (p.33) that there were no  X  X  X eally large differences in retrieval effectiveness X  X . He concluded (p.34) that  X  X  X he index language is a sub-system of minor import X  X . Basing his comments on the work of Salton (1986) and others, ( Croft (1989) , p.91), said that experimental results show that  X  X  X imple automatic indexing of free text apparently works at about the same level of performance as man-ual or human indexing with controlled vocabulary X  X . More recently, ( Sparck Jones (2005) , p.427), citing the work of Mitra, Buckley, Singhal, and Cardie (1997) on statistical and syntactic phrases and Perez-Carballo and Strzalkowski (2000) on natural language processing techniques, noted that  X  X  X he TREC (Text Information
Retrieval Conference) experiments have not shown that more refined indexing with complex terms is especially advantageous X  X . However, these conclusions are based on system evaluations performed using the Cranfield method, in which query terms are tested in a laboratory environment; human information seekers are delib-erately excluded because of their confounding impact on the outcome (Cleverdon 1991).

The goal of the present research is to understand what characteristics of query terms make them better in the context of the information seeking process (e.g., Belkin, Oddy, &amp; Brooks (1982); Marchionini (1995);
Kuhlthau (1993) ). In Wacholder and Liu (2006) , we demonstrated that participants strongly preferred terms identified by one query language (HUM) over two others (HS and TEC) and we argued that this user pref-erence reflected the fact that these terms required less mental effort to process. In this paper, we focus on another important measure of query term quality  X  effectiveness  X  a measure of whether terms lead users of information systems to correct results. Specifically, we consider the effectiveness of terms selected by partici-pants, relative to the query language used to create the selected terms. We adopted the following hypothesis:
H 0 : Query languages do not have an impact on the effectiveness of selected query terms in the interactive information access process.

H 1 : Query languages have an impact on effectiveness of selected query terms in the interactive information access process.

To evaluate the hypothesis, we used a standard experimental design intended to ensure that we measure the impact of the query language, rather than any other factor, and to eliminate as many rival explanations as possible ( Krathwohl, 1998 ). In what follows, we briefly describe the procedure and we explain how we mea-sured effectiveness of outcome. Readers interested in more detail about the interface, the questions, and the merging of the lists of query terms should consult Wacholder and Liu (2006) . 2.1. Procedure
To maintain maximal control over the relationship between query terms and text content, we used a 350 page college-level text ( Rice, McCreadie, &amp; Chang, 2001 ). Participants were presented with questions and asked to find answers using ESBI (Electronic Searching and Browsing Interface), a system designed specifically for this experiment. Twenty-four subjects were presented with a list of merged terms identified by all three methods; selecting a term led to a list of contexts in which the term appeared. The task was to find answers to questions that had been selected because the answers were known to be in the book. 2.2. Terms used in the experiment
In this section, we describe the three sets of query terms used in the experiment. For automatic query lan-guages, the computer algorithm that identifies the query terms can readily be inspected. However, the cogni-tive processing that generates human query terms is not visible either to the individual engaged in the processing or to an outside observer; this makes it difficult to study human query languages. Here again, there exists a parallel between natural languages and human query languages; just as a grammar that generates sen-tences of a natural language exists in the minds of a speaker of the language, so the grammar of a person who produces a query language exists only in the mind of that individual. A human grammar can be analyzed only by inspection of the set of terms produced by the grammar and by study of human cognitive processing. For the human query language, we therefore report on characteristics of the query terms. 2.3. Automatically identified terms The two techniques for automatic identification were the Technical Terms (TEC) algorithm ( Justeson &amp;
Katz, 1995 ) and the Head Sorting method (HS) ( Wacholder, 1998; Dagan &amp; Church, 1994 ). Each of these methods identifies noun phrases (NPs) in full text documents and applies frequency counts to determine which
NPs are actually terms. A noun phrase is a grammatically coherent unit whose head (most important word) is the noun. Examples of NPs are mud , red clay , and barn door . In the case of a one-word NP, the noun is also the head. In multi-word NPs, adjectives (e.g., red ) and nominals (e.g., barn ) that precede the head are called modifiers.

The HS and TEC algorithms, when run over the same text, identify different sets of NPs. Consider the fol-lowing text chunk.

They sold an antidote for spider bites . Then they sold an antidote for snake bites . They made more money from the one for spider bites .

After the removal of determiners (e.g., the ), pronouns and all other words that are not part of NPs, six NPs snake bites . Snake bites is the only multi-word NP that is repeated twice, so the TEC method identifies just one term.

The HS method is more sophisticated than the TEC approach in that it uses an additional linguistically-motivated criterion to identify terms. In the HS method, terms are identified by grouping NPs with a common head (e.g., laptop computer and personal computer ); those NPs whose heads appear in two or more phrases are selected as HS terms. One-word NPs meet this criterion because the only word of a single word NP is the head.
The HS method therefore identifies three query terms in the example text chunk: antidote , snake bites and spi-der bites. As this example illustrates, the HS method identifies many more terms than the TEC method and the
HS terms are more representative of the content. 2.4. Human-identified terms
The third set of terms was identified by a human indexer (HUM). We note that this set of terms is only one of many that can be identified by mental processing (see, e.g., Furnas et al., 1987; Gomez et al., 1987 ). An important reason for choosing the Rice et al., 2001 book was the availability of a back-of-the-book index that had been pre-constructed for the printed book. We judged this index to be a very good one  X  it used especially long, syntactically complex terms designed to lead people to answers to the type of questions people might reasonably expect to find in this book.

The book index used a standard back-of-the-book display format: main entries were aligned at the left mar-gin; each main heading had no more than one level of subheadings. A few terms excerpted from the index are shown in Table 1 .

So that we could organize all of the terms into a single list, we automatically merged headings and subhead-ings, so that the terms in Table 1 were displayed as shown in Table 2 .

In the rest of this paper, we refer to this set of terms as having been identified by the HUM method; strictly speaking, this set of terms is only one of many that could have been identified by the HUM method.
Table 3 shows the exhaustivity of the sets of query terms identified by the three different methods. Some terms were identified by more than one method so the percentages of the individual methods add up to more than 100%.

The HS method identified more than 10 times as many terms as the HUM method does, and more than 4 times as many as the TEC method.

Table 4 shows some possibly significant ways in which the sets of terms differed. The three sets of terms varied in length, as measured in words, and in two measures of linguistic complexity, number of prepositions per term and number of content-bearing words per term. All three measures of the properties in Table 4 are generally correlated with semantic specificity; for example, the single word NP antidote is shorter, has fewer prepositions and fewer content-bearing words than the NP antidote to rattlesnake bites .
 The difference between HUM and the other two sets was significant for all three measures. The data in Tables 3 and 4 encouraged us to believe that the three sets of terms were interestingly different. 3. Experimental design
Major concerns in designing the experiment and the solutions we adopted include the following: (1) Ensuring that participants really used the query terms during the information access process. (2) Compensating for different properties of the query terms.
 (3) Display of query terms.
 (4) Different characteristics of subjects and questions.
 3.1. Measuring correctness
Because of the known difficulty of assessing the output of question X  X nswering and IR systems, we devoted considerable effort to identifying correct answers and to developing procedures to reliably compare term confident that we could find nearly all of the correct answers to our pre-identified questions. We conceived of a correct answer as one that a professor would consider satisfactory (i.e., a grade of A or B) in a college-level class. As a first step, one of the experimenters used ESBI to answer all 26 questions used in the study. The process of finding these answers provided the basis for the experimenter-rated judgment of the question dif-ficulty described in Wacholder and Liu (2006) . But although these experimenter-identified answers represent the judgment of one knowledgeable individual, they do not represent the complete set of correct answers.
Three other members of the experimental team therefore reviewed any answer that our participants gave that was not on the initial list of answers; if the team agreed that the answer was correct, we added it to the list of correct answers. This approach is similar to that used in the NIST (National Institute of Standards) TREC question X  X nswering track ( Voorhees &amp; Tice, 2000 ), except that our corpus is much smaller.
Another problem we faced was the selection of a unit of text for measuring correctness. Initially we chose paragraphs, based on our observation that most participants cut and pasted one or more full paragraphs into the text box. But this approach proved unsuccessful  X  it became apparent that the extensiveness of the answers, as represented by number of paragraphs, was based on the style of the individual participants rather than on the effectiveness of the terms. In making the evaluation of effectiveness described below, we therefore counted as evidence of a term X  X  effectiveness any case where the selection of the term led to a correct answer, regardless of the length of the answer. 4. Results
The results of this experiment support the hypothesis that query languages do have an impact on the rate at which participants obtained correct answers and the length of time it took them to reach the correct answers.
We assume that a language that produces a higher percentage of terms that lead to correct answers is more effective than one that produces a lower percentage and that a term that helps users find answers more quickly is more effective than a slower language.
 Row 3 of Table 5 shows that 31% (55/180) of HUM terms, 36% (90/247) of HS terms, and 17% (11/66) of TEC terms led to correct answers.

The Proportion Test demonstrates that we can reject the null hypothesis with more than 99.9% confidence ( p = 0.008). We then performed a pair-wise comparison of the sets of terms to determine whether any set led to significantly more correct answers than any other set. HS and HUM performed significantly better than TEC ( p = 0.004 and 0.043, respectively), but the difference between HS and HUM was not significant ( p = 0.24).
Another way to measure the effectiveness of different query languages is to see which one made it possible for participants to find correct answers more quickly. Measuring the time was somewhat complicated because all of the terms were merged into a single list, so that participants used query terms identified by different query languages for a single topic. We therefore calculated time spent on a per term basis by extracting from the system log the time when the subject selected each search term. Time spent per term began when the par-ticipant selected one term and ended when the participant selected the next. There were three processes that participants could engage in during this interval: 1. Hyperlink to context, if term appeared plausible. 2. Browse text to search for answers. 3. Submit answer(s).

If a participant found no answer she/he thought was correct, s/he immediately returned to the displayed list of terms and selected another. This required the least time, as shown in Column A. If the participants found an answer they thought was correct, they had to spend time recording it, either by cutting and pasting or by typ-ing. Column B records the time spent on terms that led to any answer, whether the answers were judged cor-rect or not. A subset of the submitted answers were judged correct by the process described above. Column C records the time spent only on terms that led to correct answers ( Table 6 ).

Under all conditions, participants spent about 60 more seconds using TEC terms than using HUM terms and about 90 more seconds using TEC terms than HS terms. The time spent to reach answers can be expressed as HS &lt; HUM &lt; TEC. To determine whether these differences were likely to have occurred by chance, we used the Tukey X  X ramer method to perform a pair-wise assessment of confidence intervals ( Neter, Wasserman, &amp; Kutner, 1990 ). Table 7 summarizes the results.

Based on the time spent obtaining correct answers, we conclude that HS terms are most effective, followed by HUM and finally by TEC.

Table 8 summarizes the results for correctness and for time. We conclude that HS is better than TEC on both measures and that HUM is significantly better than TEC on one measure. On the assumption that each measure is equally important, HS is the best method, followed closely by HUM. Since it only takes a single case to prove that a generalization like H 0 is false, these results provide evidence that H guages do have an impact on the effectiveness of selected query terms in the interactive information access process. 5. Discussion
Our experiment shows that one method, TEC, is significantly less effective than the other two. In this sec-tion, we analyze the properties that make two of these query languages significantly more effective than the third and we consider factors in experimental design that singly, or in combination, might have led to this result.
 5.1. Correctness
TEC identifies significantly fewer correct terms than does either HUM or HS. Since HS and HUM differ with regard to length of terms, number of prepositions, number of terms identified, and whether terms were extracted or synthesized, none of these factors is linked in any simple way to correctness. It is of particular interest that the HS terms are just slightly more effective than the HUM terms in this regard. This shows that query languages that extract terms from text and query languages that involve synthesis and generalization can both be very effective. It is reassuring, though not surprising, that a good human indexer can identify help-ful terms. But the excellent performance of the HS terms suggests that the HS method does indeed do a very good job of identifying terms that represent document content and can be effectively used as displayed terms in interactive information access systems. 5.2. Time
With regard to time spent reaching correct answers, TEC once again was the least effective and HS terms were most effective. Since HS and TEC are similar on exhaustivity, semantic specificity and string-similarity, we again conclude that none of these factors are responsible for the difference. We note that HUM terms per-formed almost as well as the HS terms in face of an unavoidable disadvantage; namely, the HUM terms were largely not identical to strings in the text. When a participant selected a HS or TEC term, the interface (ESBI) led him/her directly to the highlighted string. But when a HUM term was selected, ESBI simply led the par-ticipant to the middle of the page listed in the back-of-the-book index. This feature of the HUM terms may have forced subjects to look at more text to find the answer and may therefore have affected the rate of correct answers identified by the HUM method. It may also have added to the amount of time required to find answers using HUM. In any case, this problem of where to hyperlink to in the text is an intrinsic problem associated with query terms that do not occur in the text; it is one reason that automatically extracted terms may be preferable in a hypertext environment.

These results show that the number of query terms identified per method and the measures of complexity are not by themselves responsible for the observed differences in effectiveness. We speculate that the two more effective query languages provide better coverage of text content. The HUM method is based on human gen-eralization and synthesis; there are relatively few HUM terms, which suggests that the indexer displayed good judgment in selecting them. The HS method is linguistically more sophisticated than TEC in that it (1) counts single word NPs as phrases and (2) counts occurrences of the heads of NPs, which are a syntactically and semantically linguistically distinguished unit. Wacholder (1998) hypothesizes that HS is better suited for doc-ument representation than TEC because the head of a common NP makes a greater contribution to the doc-ument as a whole than do modifiers and suggests that automatic methods for identifying significant terms in documents should reflect the prominence of heads of NPs. The greater effectiveness of HS terms over TEC for correctness and time is consistent with this hypothesis.

However, since we assessed only three query languages on a single book, we cannot safely generalize based on these results. Extending these results to more query languages raises the daunting possibility that we might need to evaluate each of the theoretically infinite sets of query languages. Fortunately there exists a clear cri-terion for identification of query language quality: we will know that we have succeeded when we can predict, based on the properties of the query language, how effective a set of query terms will be. This prediction may be stated with respect to an algorithm, properties of terms such as exhaustivity and specificity, kinds of infor-mation needs, classes of users or document types.

Finally, we advance a number of explanations for why the results of our experiment indicate that query languages make a significant difference in search effectiveness, even though previous studies have failed to identify such an impact. 5.3. Putting humans in the loop
Most studies of query term effectiveness have used the Cranfield paradigm, an approach to evaluation conducted in  X  X  X  laboratory environment ... freed as far as possible from the contamination of operational variables X  X  ( Cleverdon, 1991 p.7). Such is the case, for example, with the above-mentioned work of ( Perez-methods of IR indexing are not effective. Both Perez-Carballo and Strzalkowski and Mitra et al., report on results of the TREC (Text Retrieval Competition) in which the computer system uses some method of IR indexing to represent document content and to automatically create query terms. Many researchers (e.g., Wacholder et al., 2007; Sparck Jones, 2001; Hersh, Pentecost, &amp; Hickam, 1996; Robertson &amp; Hancock-
Beaulieu, 1992; Ellis, 1992 ) have pointed out that while the Cranfield method is well suited to system evalu-ation, the results do not necessarily carry over to experiences of real people using real systems. 5.4. Choice of query languages
We deliberately selected query languages that identified sets of query terms with different characteristics in the hope of increasing the chances that we would obtain statistically significant results. Other possible query languages, automatic and human, may prove to be more or less effective. 5.5. Experimental materials
Standard IR systems operate over large numbers of documents on a variety of topics by a variety of authors and use complex term weighting schemes to identify relevant documents. Our study used only a single book edited for consistency of style of vocabulary, and ESBI returned results based strictly on term frequency.
Further research is needed to replicate these results and to identify properties that predict what kinds of characteristics increase query language effectiveness. 6. Conclusion
Our results show that one of the three query languages performed significantly less well than the other two with regard to correctness and time. We conclude that query languages do have an impact on the outcome of of query languages for information seekers establishes the potential value of further research to better under-stand what characteristics of query languages promote or inhibit effectiveness under what conditions. Finally, this research introduces an empirical approach to measuring query language effectiveness that has practical significance for the design of real-world information access systems.
 Acknowledgements
This research was funded by the Rutgers University Information Science and Technology Committee and by NSF grant #0414557, Michael Lesk and Nina Wacholder, PIs. We thank Mark Sharp, Peng Song and Xiaojun Yuan for their work on the experiment and the initial data analysis.
 References
