 Traditional batch evaluation metrics assume that user interaction with search results is limited to scanning down a ranked list. How-ever, modern search interfaces come with additional elements sup-porting result list refinement (RLR) through facets and filters, mak-ing user search behavior increasingly dynamic. We develop an evaluation framework that takes a step beyond the interaction as-sumption of traditional evaluation metrics and allows for batch eval-uation of systems with and without RLR elements. In our frame-work we model user interaction as switching between different sub-lists. This provides a measure of user effort based on the joint effect of user interaction with RLR elements and result quality.
We validate our framework by conducting a user study and com-paring model predictions with real user performance. Our model predictions show significant positive correlation with real user ef-fort. Further, in contrast to traditional evaluation metrics, the pre-dictions using our framework, of when users stand to benefit from RLR elements, reflect findings from our user study.

Finally, we use the framework to investigate under what condi-tions systems with and without RLR elements are likely to be effec-tive. We simulate varying conditions concerning ranking quality, users, task and interface properties demonstrating a cost-effective way to study whole system performance.
 H.5.2 [ User Interfaces ]: Evaluation/methodology Simulation; Search behavior; Faceted search; Evaluation
Many of today X  X  enterprises require a dedicated search system, i.e., a particular optimal configuration of both a ranking algorithm and interface , to effectively support their specific type of user, task, and collection. To select this optimal configuration an evaluation metric X  X apturing a particular user behavior and interface combina-tion X  X ogether with a fixed set of queries, documents, and relevance judgements, is required to determine the quality of various rank-ing algorithms [33]. Traditional batch evaluation metrics, how-ever, typically assume that after launching a query, user interac-tion remains limited to scanning down a ranked result list and stops at some rank k [4]. As interfaces of modern search systems are equipped with additional elements, such as result filters, and users become more actively involved in the search process, the whole system effectiveness no longer solely depends on the quality of the ranking, but also on how the interface elements function, as well as on how users operate these elements [3, 14, 16, 26]. Conse-quently, batch evaluation metrics for traditional search systems no longer accurately reflect system performance when it comes to the combination of a ranking algorithm and interface.

A key problem, then, is how to choose between systems with varying combinations of interface elements and ranking algorithms. As a first step, in this paper, we present a framework for comparing search systems equipped with a particular class of interface ele-ments, i.e., elements supporting result list refinement (RLR). We define RLR search systems as those that provide: (i) a fixed set of filter values that remain visible to the user at all times; and (ii) the filter values operate on a fixed initial result set for a particular query. For example, a system with minimal RLR elements has a single filter value, where elements of increasing complexity are a list of multiple filter values (keywords/entities), and filter values grouped in categories (facets). A relation exists between facets and RLR elements, however, we do not require filter values to be mutually exclusive, exhaustive [34], or orthogonal [13].

We limit the initial framework to search systems with RLR el-ements for two reasons. The first is pragmatic: without loss of generality, user interactions with these elements can be modeled as switching between a limited number of different subsets of a result list. It allows this work to go beyond the standard user interaction model in batch evaluation, cf. [4], while remaining tractable. The second is methodological: we wish to focus on user interactions with a class of elements that require users only to recognize a suit-able filter value to refine a result list with. This in contrast to, for example, query (re)formulations that require additional mental ef-fort on the part of the user [29], thereby allowing greater variability in user interactions depending on individual user characteristics.
Our evaluation framework for systems with RLR elements has two parts: (i) an evaluation measure specified by a model that char-acterizes how users interact with systems supporting RLR and a specification of how these interactions are associated with user ef-fort and gain (Section 3); and (ii) a simulation strategy, i.e., an in-stantiation of the interaction model parameters (Sections 4 and 5).
The framework has two immediate applications. (i) Prediction : by obtaining estimates of the parameters of the interaction models from usage data, e.g., from online systems or user studies, system performance can be predicted and evaluated off-line. This allows optimization of systems by varying the ranking algorithm and inter-face elements, for a particular application and user group. (ii) Sim-ulation : our framework allows us to perform  X  what-if  X  analyses, i.e., to investigate system performance under varying conditions by simulating different combinations of ranking quality, interface ele-ments, and type of users. Such simulation results can inform deci-sions about the type of interface, ranking algorithm, and queries to be used when comparing systems in subsequent user studies.
To demonstrate the efficacy of our evaluation framework we ap-ply it to the comparison of two standard snippet-based search sys-tems, one with RLR elements and one without. Here, our first goal is to examine the accuracy of the framework in predicting user effort when interacting with an RLR system. We instantiate the model parameters of the two systems with user data and then com-pare the model predictions with real usage data. Specifically, we seek answers to the following questions: (i) does the effort pre-dicted by our framework correlate with user effort on a task with the two systems; and (ii) does comparing simulated system per-formance allow us to accurately predict when RLR elements are beneficial and when they are not?
Having validated its accuracy, our second goal is to investigate system performance under varying conditions, including different ranking quality, filter properties, as well as user behaviors X  X nd how these factors interplay. In short, we study the question:  X  X hen does an RLR-enabled system help to improve user performance? X 
User interaction models are closely related to effectiveness met-rics developed for batch-evaluation: users invest effort to operate a system in order to gain relevant information. With some abstrac-tion, a batch evaluation metric can be viewed as a function that makes predictions about user effort and gain based on its user inter-action model, i.e., assumptions about how users operate the system, cf. [4]. For example, widely used metrics P@ K and NDCG@ K employ user interaction models of two different types of user search behavior: P@ K assumes users examine all top K results in no par-ticular order, and NDCG@ K assumes users examine top K results from top-to-bottom and find lower-ranked results of less value. In both cases, we can interpret the number of documents examined as effort, and the number of relevant documents found as gain. User interaction with a traditional search interface. Modeling user interaction with retrieved results has become a central topic in recent discussions on evaluation methodology [1]. A wide range of stochastic models have been learned from Web search engine click logs [5, 8, 11, 12]. Common to these models are a few assumptions that have been identified as typical user interaction patterns in Web search. For example, the examination assumption states that users are less likely to view lower ranked results [21]; and the cascade model assumes users browse a ranked result list from top to bottom, and stop once a relevant result is found [9].

Meanwhile, several effectiveness metrics have been proposed in an effort to integrate more realistic user interaction models. For ex-ample, the Rank Biased Precision (RBP) [28] models the  X  X ersis-tence X  of a user, i.e., how likely a user examines a next result when going down a ranked list; the Expected Reciprocal Rank (ERR) [6] is derived from the cascade model; and Chuklin et al. [7] proposed to turn click models into evaluation measures. Time-biased gain takes into account user variability in terms of the time needed to process information, e.g., reading summaries [31].

An alternative use of user interaction models is to simulate user search activities, in order to evaluate system performance under various conditions such as  X  X hat if users do X? X  which are not likely to be investigated by a user study. For example, Smucker and Clarke [31]presentedamethodtosimulatetime-biasedgain. A shared assumption in the studies listed so far is the traditional ranked list search interface.

Studies that go beyond the traditional single ranked list based user interaction is session-based evaluation [20, 22]. Here, in addi-tion to the examination assumption, an extra user decision X  X hether to reformulate their query X  X s modeled (cf. Section 3.1). Beyond traditional search interfaces. Moving away from the traditional ranked list search interface, Fuhr [14] proposed the in-teractive probabilistic ranking principle (iPRP) aimed at providing a formal description of user interactions and the corresponding op-timization strategy for a system. However, instantiation of this gen-eral model for practical use remains an open problem.

More concretely, faceted search is a typical example where user interaction models need to go beyond the assumptions of a ranked list interaction model. Often, simulation based evaluation is em-ployed [23 X 25, 27, 32]. A key notion shared by these simulation models is  X  utility  X  X  23, 25, 32] X  X he trade-off between the effort users spend and the benefit gained, e.g., finding a target document.
Various heuristic user interaction models have been proposed for simulation. These models assume different user goals and how users interact with retrieved results and facets. In terms of user goals, Kashyap et al. [ 23] assumed users examine all filtered results in the context of database queries. Alternatively, in [25, 30, 32] users were assumed to find only one relevant result. In terms of user operation with facets, Koren et al. [25] assumed that users can always recognize the facet that contains relevant document(s), and select facets in one of the following ways: (i) randomly; (ii) facets with least document coverage; (iii) the first facet that contains the target document; or (iv) the optimal facets. Kong and Allan [24] assumed that users sequentially scan facets and skip an entire facet when they find it irrelevant.
 Our work. Our goal is to devise a new evaluation method for sys-tems with a search interface enabling RLR elements. What we need is a user interaction model that is able to characterize not only the traditional  X  X xamine a result list X  interactions, but also interactions with RLR elements.

We evaluate an RLR system under the same notion of utility (user effort and gain) as in the above studies in faceted search. Our work differs in two important ways. First, we model user interactions with RLR elements (including facets) in a more nat-ural way X  X sers scan filtered results without a particular order; and they may and may not recognize a  X  X ood X  filter value. Second, we do not make explicit assumptions to create categories of users (like in [25]). The variability of users is captured by a probabilistic framework: by varying two model parameters, we are able to sim-ulate a wide range of users. This second property of our model al-lows us to encompass empirical user interaction models developed for traditional search interfaces, i.e., to fit the model with real usage data and make predictions of system performance with respect to a particular group of users/search tasks.
Our evaluation framework has the following components: (a) a user interaction model that characterizes how users interact with a system that enables result refinement (Section 3.1); (b) associating effort and gain to user interactions for evaluation (Section 3.2); and (c) integration of the above two components (Section 3.3).
We assume that users perform actions to make progress on a search task (e.g., inspect results); every action costs effort ; and the user may gain from that action by finding relevant information. With an interaction model, we simulate and predict action paths of users during a search task, which vary across users and search tasks, and are influenced by the quality of the result lists. By asso-ciating effort and gain with different paths of search actions, we are able to predict user effort given different types of users, tasks and the quality of result lists.

Overall, the effectiveness of a system can be measured by an-swering questions such as: How much effort is required to achieve x amounts of gain with system A, as compared to system B? User interaction with a basic interface. With a basic search in-terface, the common assumptions are: users browse a result list from top to bottom; and after examining each result, they make a decision X  X hether to continue to examine another result, or to give up this result list [12, 15].
 User interaction with an RLR-enabled interface. With an RLR-enabled interface, apart from examining results in the retrieved ranked list, users may choose to refine the result list by filtering on a particular value. A typical consequence of these RLR interac-tions is that users switch between different filtered versions of the original result list. We refer to these different versions of the result list, including the original ranked list, as sublists .

Without making assumptions about the specific implementation of these elements, at a functional level, we can model the user in-teractions with RLR elements as selecting a sublist .

This leads to at least two additional decisions a user needs to make: (1) continue with the current sublist, switch to a different sublist, or quit searching? and (2) if switching, which sublist to select next? Parameterization of user interactions. Each of the decision points introduces uncertainty in computing user effort and gain dur-ing a search task: it is at these points users diverge from each other X  X  action paths. Taking these decision points as variables of our inter-action model allows us to capture variability in user behavior.
Specifically, to quantify the uncertain nature of user decisions, we model the outcome of the aforementioned decisions as random variables following specific distributions:  X  Continuation decision: we model the decision of user u at  X  Switching decision: similarly, we model the decision of user  X  Sublist selection decision: we model user decisions on sublist These three probabilities can be set to empirical values estimated from usage logs (Section 4), or based on hypotheses about their values (Section 5). Of course, users may decide to quit search-ing. However, as quitting is complementary to the continuation and switching decisions, there is no need to explicitly define it.
With the conceptual user interaction model in place, we now specify how we can associate effort and gain to user interactions. User actions. We consider 3 types of action:  X  Examine result: Users examine a result to determine its rele- X  Pagination: While not explicitly modeled as a decision, a user  X  Select candidate result list: This activity involves a series of Effort of actions. Each action is associated with an amount of effort. Let A be the possible actions users can perform, and a ,...,a t be the action path of a user that starts browsing results of q until he/she stops, a i  X  A . The user X  X  effort along P a where w i is the effort needed for action a i .
 Effort can be implemented in different ways. For example, with NDCG or P@10, the only actions considered are  X  X xamine X  a doc-ument, and each costs a unit effort. With time-biased gain [31], effort is implemented in a more elaborate way, e.g., the effort re-quired for an  X  X xamine X  may depend on the document length. Gain of actions. We assume that user gain is determined by the relevant documents they encounter. Let D P user encounters along action path P a ;itstotalgainis where rel ( d, q ) is the relevance judgement of d w.r.t. the query This approach to  X  X easuring X  effort and gain is of course closely related to the cumulative gain type of evaluation measures [2, 19].
The final ingredient that ties together the above components for measuring user effort/gain when interacting with an RLR system is the user action path P a . Assuming users examine documents in a ranked list from top to bottom with a basic interface, the order in which users examine documents is deterministic. The only un-certainty is that users may quit, which can easily be handled by computing the gain at a cut-off point, or at an expected search depth [28]. However, with an RLR interface, the possible paths users can take for a query is combinatory given that users can switch between sublists without a particular order. Thus we resort to a Monte Carlo method. The interaction model specified in Section 3.1 allows us to simulate possible user action paths for a given set of parameters, creating a sample of user effort and gain for a particular task and user behavior. System performance can then be compared with these samples using standard statistical tools.
 Action path constraints. To further reduce the complexity of the simulation process, we constrain the possible user action paths with the following assumptions.
 A1 Users examine results in a ranked list from top to bottom. A2 When switching between sublists, users skip and only skip the A3 Instead of assuming a user quits searching with a certain prob-Steps to simulate a user action path. 1. Specify model parameters (with empirical or hypothesized val-ues), i.e., p r,u for continuation decisions, and c u for sublist se-lection decisions. 2. At each step, draw s r,u  X  Ber ( p r,u ) . If s r,u =1 ine to P a ; else draw f  X  Cat ( K, c u ) , add select sublist and examine to P a . When encountered, add pagination and handle situations specified in A2. 3. Stop when: (1) the total effort/gain meets a predefined cut-off value; or (2) all results are exhausted.
The core of our framework consists of a specific mapping  X  y = h (  X  x ) , where  X  y is the estimated user effort, aimed at approximating the actual user effort y , given the input variables  X  x . In a typical machine learning scenario h (  X  ) would be selected from a pool of possible hypotheses by fitting example pairs of y and  X  x trast, we have specified in advance a single hypothesis h interaction model motivated in Section 3, and the values of termined by specific types of user behavior. Here, we validate our hypothesis h  X  by examining how well its output,  X  y , approximates actual user effort y .

In this section, we validate our model using usage data gath-ered for an RLR interface. Specifically, we calibrate our interac-tion model parameters ( p u,r and c u ) with empirical values derived from the usage log of a particular group of users (i.e., participants of our experiment), and examine whether the model prediction cor-responds to the actual user effort as recorded in the log. We aim to answer the following questions: Q1 Does the predicted effort (  X  y ) correlate to user effort ( puted with usage data? Q2 Can we accurately predict when an RLR interface is beneficial, compared to a basic interface?
We use the TREC Federated Search data [10], for it has two im-portant properties: (1) all document-query pairs in this collection have been assessed. This provides us with a more accurate estima-tion of system performance; and (2) this collection has been con-structed by querying a large number of web search engines all of which are categorized. These categories can be converted to fil-ter values in an RLR system. The complete list contains 24 cate-gories [10, Table 3], including academic, travel, etc.

The collection contains 50 (judged) test topics, their associated web pages, and the summaries (snippets) to represent these pages. We create rankings for each topic based on a standard query likeli-hood model as implemented in Indri.

We consider two types of system: one with a basic interface, and the other with an RLR interface, where categories are used to construct sublists for each topic. We treat every document as being annotated with the category of its source search engine. Since an engine can be in multiple categories, and documents may have been retrieved by multiple engines, every document is associated via its source(s) to one or more categories.

Relevance judgements for the Federated Search track are graded, 4 levels from highly relevant to non-relevant. We only distinguish between relevant (level 1 X 3) and non-relevant to ensure more than 10 relevant documents per topic are available (see Section 4.1.3). RLR interface. Fig. 1 shows a screenshot of the RLR enabled in-terface, where numbers 1 X 6 indicate components of the system. On the left are the filter values (1) as provided by the federated search track [10]. On the right a dashboard (2) is available indicating the number of clicks left for a task and the number of relevant docu-ments found. After 25 clicks a  X  X ive up X  button (3) would appear providing the option to skip the remainder of the task. The topic description (4) is available at the top of the screen. An additional button allows users to expand the description and review the exam-ples as provided before starting a task. The middle of the screen is devoted to a scrollable result list (5) with 10 snippets. At the bottom of the page a pagination button (6) is available. See [17]. Basic interface. The basic interface is similar to this design except that the filter panel (1) is unavailable. One of the filter values in the RLR interface is  X  X ll, X  which is the unfiltered list as it would be in the basic interface. It is possible for users to ignore the filter values and use the RLR interface exactly as the basic interface.
We investigate the effort it takes users to locate relevant docu-ments. While our interaction model can be applied to both effort-based and gain-based measures, in this study design we focus on measuring effort.
 User task. Users were asked to find 10 relevant documents for a topic. It is not as trivial as, e.g., finding 1 relevant document, allowing variability between user action paths. Meanwhile, it limits the effort required to a manageable amount.

Specifically, we asked users to locate by clicking on 10 result summaries of relevant documents within 50 clicks, where a click is counted if it is on a result summary, a pagination button, or a filter value. We use the 50-click limit to prevent users from clicking ev-ery result and to force users to make conscious choices instead. We require users to only click on summaries to abstract away from ac-tions such as opening and reading documents as well as to keep the Figure 1: The RLR interface: (1) filter values; (2) dashboard; (3) give up button; (4) topic description; (5) result list; (6) pag-ination. The basic interface excludes (1). time necessary to complete a task manageable. To reduce user vari-ability in judgements of relevance, we provide feedback to users whether a clicked result is relevant or not.
 Experiment design. We recruited participants via university mail-ing list and social media. We used a standard between-subject de-sign common to A/B testing, where each new user is randomly as-signed to one of the two interfaces and directed to the same inter-face on subsequent visits. To reduce learning effects, new tasks are randomly assigned to users.
In total 145 task instances were completed by 49 users for the system with the basic interface, and 255 by 48 users for the RLR interface. The median number of completed task-instances per task is 2 for the basic interface and 3 for the system with an RLR in-terface. As some tasks have been completed by more participants than others, we consider median values in our analysis.
To answer Q1 and Q2, we need to compute the following quanti-ties: (i) user effort y (with basic or RLR interface) as derived from the usage data, and (ii) the predicted effort  X  y , where the model pa-rameters are calibrated with the actual usage data.
For simplicity we assume equal effort for user actions. We mea-sure user effort ( y ) as the number of result summaries users visited and the clicks they spent on choosing filter values and pagination.
To determine which summaries a user has visited we consider mouse hovers over results, which has been shown to correlate with eye-gaze [ 18]. Fig. 2(a) shows the percentage of total hovers over the 10 ranks of each result page. We observe that the distribution of hovers over the ranks is relatively uniform, i.e., compared to the distribution of hovers over ranks on Web search engine result pages where differences of 38% between the highest and lowest rank are observed [18]. We observe a difference of 3% between the highest and lowest rank for the basic interface (back bar), and 6% for the RLR interface. That the skew is slightly stronger for the RLR inter-face is expected as not all filter values return 10 results. The hover data suggests that participants tend to visit all result summaries on a page. This is by design as in our systems we reduced the effort needed to judge a result page (i.e., read summary, open/read the page) to judging the result summary. We are therefore able to focus on user effort as introduced by interaction with RLR elements and not by pages and result summaries. (a) Distribution of hovers over the 10 ranks for basic (black) and RLR (white hatched).
Given these observations, we approximate the number of result summaries visited as follows. We assume that if a user does pagina-tion, they have seen all the results in the previous page and count all results in SERPs before the last visited page as visited. On the last page, we count the number of results up to the last clicked result.
So far we have obtained user effort y by directly counting the number of actions recorded in the usage log. To compute  X  y approximation of y of our user group X  X e calibrate our model pa-rameters with empirical values derived from the same log data. Continuation probability. We compute p r,u using the empirical distributions of the search depths aggregated from all users. The probability that a user will examine the result at rank r as the number of times r has been visited, normalized by the max-imum number of times a rank has been visited. In terms of the number of results a user has visited, we take the same approach described above (Section 4.2.1). Fig. 2(b) shows the probability of visiting each rank for the basic and RLR interface. Under the assumption that participants visit all result summaries on a page, ranks 1 X 10 are visited an equal number of times.
 Sublist selection probability. To model user preferences of sub-lists (filter values), we collect the counts of filter value clicks for each query, and set these as the parameter c u . That is, the expected value of the probability a filter value will be chosen is proportional to how often it is chosen by the users. Since the original result list, i.e., the filter value  X  X ll, X  is always shown to the users as a starting point, we always add 1 count to it.
We simulated a sample of 1000  X  y r for an RLR interface. To answer Q1, we perform a correlation analysis between the median of predicted effort (  X  y r ) and the median of user effort ( 50 topics. We see a significant linear correlation between the two (Fig. 3): Pearson  X  =0 . 79 ( p&lt; 0 . 001 ). This suggests that our proposed model can be used to reliably predict user effort needed in accomplishing a search task in terms of the number of results visited and the number of filter values they need to explore.
Next, we investigate how the predicted effort can be used to com-pare system effectiveness, e.g., between a system with a basic in-terface and with an RLR interface (Q2). To proceed, we investigate whether and on which topics an RLR interface reduces the effort needed to complete the task, as compared to a basic interface.
We compute two quantities for each topic (see Fig. 4): 1. User difference:  X  effort 2. Predicted difference:  X  effort Figure 3: Correlation between estimated effort and user effort. Note that y b (effort spent with the basic interface) is fixed as the action path of a user with a basic interface is deterministic.
In terms of user difference, we observe that on 31 out of 50 topics less user effort is needed with the basic interface (  X  effort on 16 topics effort is less with the RLR interface and on 3 there is no difference. Of the 31 topics where  X  effort able to save more than 10 actions (points of effort), e.g., paginating to a next page and scanning 10 results. Of the 16 topics where  X  effort
On 34 out of 50 topics,  X  effort which interface would reduce user effort in completing a topic. All cases of disagreement are on topics where users of the RLR inter-face spent more (or the same) amount of effort than users of the basic interface. In these cases participants may have struggled to effectively use filter values or did not use filters at all. If partic-ipants did effectively use filters, our interaction model predicted that using filters would save effort.

We observe that our model is able to identify 100% of the cases when an RLR interface is better (i.e., costs less effort) than a basic interface. However, when it predicts that an RLR interface is better, it is only correct in 52% of the cases (0.68 F1-measure). In contrast, it is able to predict when a basic interface is better with 85% pre-cision and 55% recall (0.66 F1-measure). This suggests that if a user would benefit from using the RLR interface, then the model will predict so, and if our model indicates that the basic interface is more beneficial, use of the RLR interface should be avoided.
Further, we find that  X  y r has significant negative correlation with  X  effort A negative correlation means that the higher the effort needed with an RLR interface predicted by our model, the more likely that a basic interface is better. The stronger this correlation is, the better the model is at predicting which interface is beneficial. Figure 4: The black bars show  X  effort (hatched) bars show  X  effort effort is spent with the RLR interface. Difference values ex-ceeding 100 or less than -100 have been cut-off for legibility.
In Section 4 we have demonstrated that our user interaction model is simple, tractable, and able to accurately predict user effort with empirical parameter settings. We observed that an RLR interface can be useful for some queries while the basic interface is good for others. Many factors may contribute to this observation, ranging from system properties (backend as well as UI) to user properties. However, it is difficult to investigate the exact impact of these dif-ferent factors with user studies: a large number of experiments are needed given the number of factors considered and their combina-tion; and it is difficult to control user behavior.

In this section, we discuss how our evaluation framework can be used for studying whole system performance, under strictly con-trolled and varying conditions that may not be attained in real life studies. We use the same test collection as before, and focus on one question: Q3. When does an RLR interface help?
By instantiating our interaction model with parameter values de-signed to reflect user behavior with desired properties, we generate simulated usage data of large quantity and under strict control. We then study whether and how various factors (and their interactions) affect the advantage of an RLR interface versus a basic one.
To start with, we describe how we instantiate the interaction model with parameters that characterize different user behaviors. Examination depth on a ranked list. Users do not visit all doc-uments in a ranked list. The common examination assumption [9] states that the deeper a user investigates a ranked list, the less likely they are to continue examining the list X  X  next document; consistent with the probability ranking principle, deep down the ranked list we expect IR systems to return fewer relevant documents.
In our interaction model, at rank r , users decide either to con-tinue examining another document, or to switch to a different sub-list (cf. A3). The assumption that users are more likely to switch when they move deeper down the ranked list can be captured by controlling the parameter p r,u of the Bernoulli distribution with an exponential decay function: where  X  controls the decay rate. That is, a user with a larger would decide to switch list at an earlier rank. The resulting expo-nential decay (Fig. 5(a)) is a good fit for Fig. 2(a).

We simulate the continuation decision of a user with  X  u in the following steps: 1. Compute p u,r with Eq. 3 given  X  u ; 2. Draw a decision s u,r  X  Ber ( p u,r ) .
 Accurate and inaccurate users. When switching between sub-lists, some users make better choices than others. A good decision leads to a sublist with many (unseen) relevant documents ranked near the top, potentially reducing the total effort the user needs for (a) Probability of reaching rank r given different  X  . his/her search task. We simulate users with different levels of accu-racy by varying the sublist selection probability (cf. Section 3.1).
Specifically, we sample c u from its conjugate prior distribution, i.e., a Dirichlet distribution Dir (  X  u ) ,withhyperparameters By setting  X  u to different values we can simulate users with dif-ferent types of prior knowledge. E.g., users who do not have a clue which sublist to choose can be simulated by setting  X  uniform distribution; an  X  X ccurate X  user can be simulated by set-ting  X  u proportional to the quality of the list, e.g., as measured by NDCG. The properties of the Dirichlet distribution ensure that the expected value of c u,k is  X  k / ! performance of the sublists.

In summary, for a given  X  u , we simulate a user X  X  choice of sub-list with the following procedure: 1. Draw c u  X  Dir ( K,  X  u ) ; 2. Draw the decision vector ( f 1 ,...,f K )  X  Cat ( K, c Influence of user behavior on search performance. Before sim-ulating system performance under varying conditions, we conduct a sanity check to examine how the above parameter settings influ-ence estimated user effort given the following setup.
 User task We consider an information gathering task where users aim to collect 1 , 10 , or all relevant documents; Examination depth We set  X   X  { 1 , . 5 , . 1 , . 05 , . 01 to reflect different user examination depths on a ranked list. User accuracy We consider two cases: a uniform prior  X  u,k 1 /K , and a prior biased on list quality, i.e.,  X  k is set to the NDCG value of the corresponding result list.
 User effort and gain As before, we assume equal effort for all ac-tions and binary relevance to compute gain.
 Given the possible settings above, we run the simulation over all combinations of these parameters, each for 1000 times.

Fig. 6 shows the simulation results. We plot the median of sim-ulated user effort with two different user accuracy priors: NDCG vs. Random. With each prior, the continuation probability is set to different values with varying  X  . From the figure we observe: (i) in all cases, irrespective of the value of  X  and the task type, good prior knowledge about which sublist to choose is beneficial. A uni-form prior corresponding to random selection always leads to more effort. (ii) The continuation probability has a limited effect when fewer relevant documents need to be found. When more relevant documents need to be found, e.g., in the Find-all task, it is better to go deeper down a ranked list (i.e., small  X  ). These observations are intuitive and provide a sanity check on the simulation results using our user interaction model and the proposed simulation strategy.
Next, we describe the method we employ to analyze conditions when an RLR interface is likely to be beneficial and when it is not.
We identified the following factors that, presumably, determine whether an RLR interface is preferable over a basic one. Query difficulty for the basic interface ( D q ). A priori, if the ranked list in a basic interface is good, i.e., with all relevant doc-uments on top, then users do not need to switch to other sublists for their tasks. We use the effort users need to accomplish a task with the basic interface ( effort b ) as the indicator of effort b , the more difficult the query is for a basic interface. Sublist relevance ( R q ). The effectiveness of an RLR element should depend on the quality of the sublists created for result refinement. If the sublists would filter the relevant documents that were buried deep down in the original ranked list, then it is likely to help users to accomplish their tasks faster. We compute R q as the averaged NDCG scores over the sublists of a query.
 Sublist entropy ( H q ). A priori, if few sublists cover most of the relevant documents, these could help to effectively filter out irrel-evant documents. Meanwhile, if many sublists contain many rel-evant documents, then it may be easy for users to find them. In short, we believe the effectiveness of an RLR system is related to how the relevant documents are distributed among sublists, but the exact relation is yet to be explored.

We compute H q =  X  ! query q , where p i is the probability that the sublist i tains relevant documents, derived from the empirical distribution of the relevant documents among the sublists of q .
 User accuracy ( U level ). As shown in Fig. 6, how users choose the sublists makes a big difference on the effectiveness of using RLR. Following the distribution of NDCG scores of the sublists is a good strategy, while choosing randomly leads to inferior performance.
Here, we aim to investigate the impact of user accuracy at a more refined level, i.e., how accurate should the user be in order to make the RLR work? Recall that user sublist selection behav-ior is controlled by the parameter  X  . Assuming user choices fol-lowing NDCG scores are  X  X racles, X  by gradually smoothing out with respect to this oracle distribution, we can create user accura-cies of different levels between the oracle and the complete ran-dom choices (i.e., uniform  X  ). Fig. 5(b) shows the relation between amounts of smoothing added and the median of the KL divergence of the  X  X ew X  user from the  X  X racle X  user over the 50 test topics. We create 4 user levels, with the amount of smoothing set to 0, 0.1, 0.5, and 1.0, corresponding to the oracle user (level 1), and approximately 15% (level 2), 50% (level 3), and 67% (level 4) less accurate users.
 User task. Intuitively, the impact of these factors would be differ-ent with respect to different user tasks. For example, when the task is to find all relevant documents, it is not important how good the top of the ranked list is, but rather, where the last relevant document is located. We consider finding 1 , 10 , and all relevant documents.
As a final note, in Fig. 6 we have observed the influence of user search depth on the RLR performance. Overall its impact is not as obvious as user accuracy, in terms of the magnitude of changes in efforts it leads to. In this analysis, we focus on the sublist selection aspect of the users and fix  X  to 0.01, which seems to be optimal for finding all relevant documents. For the other two tasks it does not seem to make a major difference when set to a different value.
Let  X  effort = effort b  X  effort r be the difference between the ef-fort needed to complete a search task with a basic interface and that with an RLR interface. We then make  X  effort &gt; 0 a depen-dent variable (DV), which takes binary values (1 as yes, 0 as no), and the above four factors ( D q , R q , H q , and U level variables (IVs). Our goal is to investigate how each of the IVs, and their interactions, influence the outcome of whether or not an RLR interface improves over a basic interface.

We apply a generalized linear model (GLM) for this purpose, which allows us to analyze how the IVs contribute to explain the variance observed in the DV. Specifically, given that  X  effort a binary variable, we take the form of a logistic regression model. We fit the models with the data simulated with parameters set to different user tasks and different U levels . The conditions w.r.t. the rest of the variables D q , R q , and H q are determined at a per-query level, which comes with the test collection. results, and the shades indicate their upper and lower quartiles.
To determine which IVs and interaction terms should be included in the model, we conducted model selection based on the Bayesian information criterion (BIC) using both forward and backward se-lection. Further, we expect that for different types of tasks (e.g., Find-1 vs. Find-all), the importance of these factors would differ dramatically, and different models would be appropriate. There-fore, we fit a model for each of the user tasks individually.
Table 1 shows the parameters for models best able to predict whether the RLR interface will be effective based on combinations of the four factors for each task.
We observe that for Find-1 none of the main factors in the model have a significant effect on the dependent variable (RLR effective-ness). However, for Find-10 and Find-all tasks we see that the has a significant effect. The negative coefficients for the ables indicate that, as users deviate from the  X  X racle X  sublist se-lection behavior, the log-odds of the RLR interface being effective decrease. Those user level effects for Find-10 and Find-all but not Find-1 suggest that if a user X  X  task is to locate 1 document, then just Table 1: Estimated coefficients of the selected models and their effects on the odds that an RLR interface helps. The overall ef-fect of user accuracy (ulevel) is tested by Wald test. The model goodness-of-fit (GOF) is tested by Hosmer-Lemeshow test. Sig-nificance codes:  X  0.001 ( ! );  X  0.01(  X  );  X  0.05(  X  ).
Coefficients Find-1 Find-10 Find-all (Intercept) -7.3401 -10.4365 -0.5337
D q 0.1058 -0.0686 0.0017
U
U
U
H q -1.0443 3.6347 -1.6492
R q  X  -49.7916 114.9398
D
D
D
D q : H q 1.3103
D q : R q  X  3.2363
H q : R q  X  13.9680 -57.2773
D q : H q : R q  X  -0.8422  X   X  Overall effect of  X  2 =1.6  X  2 =16 . 0  X   X  2 =25 . 6 !
U level ( df=3 )( df=3 )( df=3 )
Model GOF (p-value) 0.9339 0.9928 0.9213 the accuracy with which users select sublists is not enough to pre-dict whether an RLR interface will be beneficial. One explanation is that, since most sublists will have at least one relevant document ranked highly, users do not need to be accurate in their choice of sublist to achieve the task. When collecting more relevant docu-ments however, knowing which sublist to pick is important. Regarding R q we find that it has a significant effect only for Find-all. As the average relevance of sublists increases the log-odds of the RLR interface being effective increase as well. That is, having sublists with relevant documents ranked high is essen-tial for the RLR interface to be effective for the Find-all task. For the Find-1 and Find-10 tasks sublist relevance alone is not enough to predict RLR effectiveness and the effect depends on the interac-tion between two or more of the main factors. We look into these interaction terms in more detail next.
To investigate the effect of the interaction terms on the probabil-ity of RLR effectiveness, we express the relation visually. Due to space limitation, we focus here on the model for the Find-10 task.
To visualize interaction terms between continuous variables, we plot the predicted value of the DV against one varying IV, and re-center the other IVs to a fixed level. We take the 25% and quantile of the values of a variable as its low and high level, respec-tively. A model fitting the re-centered data then shows the effect of the varying IV on the DV with respect to the different levels of the re-centered IVs. For Find-10 there are two interactions terms that have significant effect: D q : R q and D q : R q : H q . For center D q to its low and high levels, and fix H q to its median. For D : R q : H q we re-center both D q and R q to a low and high level. Fig. 7(a) shows the effect of increasing R q on the probability P (  X  effort &gt; 0) when D q is high for varying levels of user accuracy. At relatively low levels (0.15 to 0.25) of R q there is a steep increase in the probability of RLR being more effective for all user levels. This suggests that when a query is difficult (i.e., the quality of the original ranked list is low), the sublists and the users do not need to be very accurate for an RLR interface to be more effective than a basic interface. In Fig. 7(b) we see that when query difficulty is low, high quality sublists (relevant documents ranked high) and higher user accuracy are necessary for an RLR interface to be helpful.
The relation between D q , R q , and H q in the final interaction term for task Find-10 is shown in Fig. 8. It shows the effect of increasing H q on P (  X  effort &gt; 0) under conditions in terms of com-binations of different levels of D q (high/low), R q (high/low), and varying levels of user accuracy. We see that a high level of combined with low/medium levels of H q result in a relative high P (  X  effort &gt; 0) for both high (Fig. 8(c)) and low (Fig. 8(a)) levels of
R q . This interaction aligns with our intuition of when sublists (a) D q : high; H q : median Figure 7: Effect of interaction terms query difficulty : sublist relevance for task Find-10. Figure 8: Effect of interaction terms query difficulty : sublist relevance : sublists entropy for task Find-10. are beneficial, i.e., having a single high quality sublist when the original ranked list is of low quality. When D q is low, we observe that P (  X  effort &gt; 0) decreases at low levels of H q when (Fig. 8(d)) and at medium levels of H q when R q is high (Fig. 8(b)). This suggests that at lower levels of query difficulty very specific conditions need to be met for an RLR interface to be beneficial.
H q plays a role in different interaction terms depending on the task as well (cf. Table 1). For Find-1 we find that as both H q increase the log-odds of the RLR interface being beneficial in-creases. Since the task requires a single relevant document, the ranking within the vertical is less important. Having more sublists with a relevant document allows users to complete the task effec-tively with the RLR interface even when users select sublists ran-domly. For Find-all, an increase in both H q and R q result in a de-crease in the log-odds of the RLR interface being beneficial. As we saw in Fig. 8, for Find-10 high sublist entropy results in low prob-ability of an RLR interface being helpful. Sublist relevance only plays a role when query difficulty is low; however, as the number of relevant documents to be found increases, it is less likely that enough documents are available at the top of the ranking.
Query difficulty alone is not a good predictor for the probability of an RLR interface being helpful. Depending on the task, differ-ent factors determine whether users will be more effective with the basic or RLR interface. In the case of Find-1, sublists do not have to be of high quality for an RLR interface to be helpful; it becomes more likely to be beneficial when the query is difficult and the en-tropy of the sublists is high. For Find-10, high query difficulty and low entropy are conditions for the RLR interface to be benefi-cial. The importance of sublist relevance depends on user accuracy; when they are accurate, lower levels of sublist relevance are neces-sary. For Find-all, the conditions necessary for an effective RLR interface are high query difficulty, high user accuracy, high sublist relevance and low entropy.
We have illustrated how our evaluation framework can be used for simulating and predicting RLR system performance in two ways, and its efficacy has been validated with usage data. Next, we dis-cuss how it relates to metrics for evaluation of traditional search systems, i.e., normalized Discounted Cumulative Gain [19, nDCG], Expected Reciprocal Rank [6, ERR], normalized Rank-biased pre-cision [28], average precision (AP), and precision@10 (P@10).
From a modeling perspective, Carterette [4] has proposed a con-ceptual framework for analyzing and comparing different effective-ness measures (for traditional systems). Our framework is close to the category of Model 3 under his classification, i.e., computing the effort a user needs to achieve a particular amount of utility. Fur-ther, all metrics discussed in [4] compute an expected value (util-ity, effort or gain). Computing the expected performance directly is rather intractable in our setting, as the order in which sublists are se-lected and the number of results viewed in each of these lists is non-deterministic. To compute the expected performance, one needs to obtain the distribution of all possible orders in which sublists are selected. Here, simulation provides samples of possible sequences from which performance can be approximated (Section 3.3).
We now move on to an empirical investigation. By examining to what extent metrics for traditional systems are able to predict the performance of an RLR system, we investigate whether our frame-work offers new insights. Table 2 shows the correlation between traditional measures and actual user effort (column 1, 2; obtained in Section 4), predicted user effort (column 3, 4), and the difference between user effort with the basic and RLR interface (  X  effort column 5, 6). We observe that nDCG measured at low cut-offs (10) has no significant correlation with user or predicted effort. When all relevant documents are taken into account (nDCG@all), the corre-lation is significant at  X  =  X  . 42 (negative, for gain vs. effort). We observe a similar pattern for binary nDCG (BnDCG); however, in this case the correlation is stronger than for nDCG. When collecting usage data we did not distinguish between highly relevant and rel-evant documents. Other measures have a negative correlation with user effort and our model in the range between nDCG and BnDCG. We focus on BnDCG here, as it is most strongly correlated.
The correlation between user effort and BnDCG@all is  X  =  X  . 72 , which indicates that for topics with high BnDCG@all scores, i.e., with many relevant documents at the top of the ranking, user effort is low. The magnitude of the correlation of both BnDCG and our model with actual user effort is high. The negative correlation between BnDCG@all and our model is lower than that of either measure with user effort (  X  =  X  0 . 59 , p&lt; 0 . 001 ), indicating that these measures disagree on the effort needed for some topics.
We apply BnDCG@all to the task of predicting whether a topic would cost a user more effort with the basic or the RLR interface (  X  effort  X  effort is no significant correlation (  X  =0 . 04 , p =0 . 776 ): BnDCG@all cannot differentiate between topics suitable for a basic or RLR in-terface. In comparison, the correlation of  X  effort Table 2: Correlation of traditional metrics with user effort, pre-dicted effort, and the  X  effort measure  X  p-value  X  p-value  X  p-value nDCG@10 -0.21 0.142 -0.19 0.185 0.02 0.896 nDCG@all -0.42 0.002 -0.34 0.016 0.00 0.994 NRBP -0.41 0.003 -0.33 0.018 0.08 0.568 ERR@10 -0.45 0.001 -0.36 0.010 0.08 0.567 P@10 -0.56 &lt; 0.001 -0.46 &lt; 0.001 0.00 0.980 AP -0.63 &lt; 0.001 -0.54 &lt; 0.001 0.02 0.875 BnDCG@10 -0.54 &lt; 0.001 -0.44 0.001 0.02 0.875 BnDCG@all -0.72 &lt; 0.001 -0.59 &lt; 0.001 0.04 0.776 our model 0.79 &lt; 0.001  X  X  -0.49 &lt; 0.001 dicted effort (by our model) is significant (  X  =  X  0 . 49 That is, simulated effort tells us which interface is the best.
As a final remark, the lack of correlation between traditional met-rics and  X  effort i.e., the quality of the original ranked list alone, is not sufficient to predict whether an RLR interface is preferable over a basic inter-face (cf. Section 5.3).
We have developed a simulation-based evaluation framework that measures the effectiveness of systems enabling result refinement, e.g., facets or filters. Its key component is an interaction model that characterizes the user X  X  search behavior in the presence of re-sult list refinement features. Using this framework, we investigate whole system performance, under various conditions. Instantiat-ing the parameters of the user interaction model, corresponding to properties of search task and user type, allows us to predict system performance for specific groups of users. We validated the predic-tions made using data collected with two search systems, re-using the TREC Federated Search test collection.

We found that user effort estimated by our model is correlated significantly with actual user effort measured in the user data. We applied our interaction model to the task of predicting when a user should or should not use an RLR interface, and found a significant correlation between the predictions we made and observations in the user data. We did not find such correlations when applying tra-ditional retrieval metrics to this task, demonstrating the value of the proposed user interaction model for search with result refinement.
Our study extends user interaction models beyond the classic  X 10 blue links. X  It provides a means to evaluate retrieval systems while considering the interaction effects between non-standard search UI features and search system effectiveness and the variability in how different people use the search UI.

