 Many clustering methods partition the data groups based on the input data similarity matrix. Thus, the clustering results highly depend on the data similarity learning. Be-cause the similarity measurement and data clustering are often conducted in two separated steps, the learned data similarity may not be the optimal one for data clustering and lead to the suboptimal results. In this paper, we pro-pose a novel clustering model to learn the data similarity matrix and clustering structure simultaneously. Our new model learns the data similarity matrix by assigning the adaptive and optimal neighbors for each data point based on the local distances. Meanwhile, the new rank constraint is imposed to the Laplacian matrix of the data similarity matrix, such that the connected components in the resulted similarity matrix are exactly equal to the cluster number. We derive an efficient algorithm to optimize the proposed challenging problem, and show the theoretical analysis on the connections between our method and the K -means clus-tering, and spectral clustering. We also further extend the new clustering model for the projected clustering to han-dle the high-dimensional data. Extensive empirical results on both synthetic data and real-world benchmark data sets show that our new clustering methods consistently outper-forms the related clustering approaches.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms Clustering; Block diagonal similarity matrix; Adaptive neigh-bors; Clustering with dimensionality reduction
Clustering, which partitions the data points into different groups such that the objects in the same group have high similarity to each other, is one of the most fundamental topic in data mining. The clustering technique has been playing an outstanding role in data mining applications. In the past decades, many clustering algorithms have been proposed, such as hierarchical clustering [10], K -means clustering [13], spectral clustering [15], spectral embedded clustering [19], support vector clustering [1], maximum margin clustering [22], initialization independent clustering [18], multi-view clustering [21, 3, 4], etc .

Due to the efficiency and simpleness, the most popularly used clustering method is the K -means clustering algorithm, which aims to learn c cluster centroids that minimize the within cluster data distances. The spectral clustering method [15] does a low-dimension embedding of the affinity matrix between samples, followed by a K -means clustering in the low dimensional space. Because the data graph and manifold information are utilized in the clustering model, the graph based clustering methods (such as normalized cut [20], ra-tio cut [7]) usually show better clustering performance than K -means method. Such methods especially work well for a small number of clusters. More recently, the Nonnega-tive Matrix Factorization (NMF) has been used as the re-laxation technique for clustering with excellent performance [12, 16]. Although the graph-based clustering methods have good performance, they partition the data based on the fixed data graph such that the clustering results are sensitive to the input affinity matrix.

In this paper, we propose to solve the clustering prob-lem from a new point of view and learn the data similar-ity matrix by assigning the adaptive and optimal neighbors for each data point based on the local connectivity. Our main assumption is that the data points with a smaller dis-tance should have a larger probability to be neighbors. More important, we impose the rank constraint on the Lapla-cian matrix of the learned similarity matrix to achieve the ideal neighbors assignment, such that the connected com-ponents in the data are exact the cluster number and each connected component corresponds to one cluster. Our new model learns the data similarity matrix and cluster struc-ture simultaneously to achieve the optimal clustering results. We derive a novel and efficient algorithm to solve this chal-lenging problem, and show the theoretical analysis on the connections between our method and K -means clustering, and spectral clustering. Moreover, we extend the proposed clustering model for the projected clustering to handle the high-dimensional data. Extensive experiments have been conducted on both synthetic data and real-world benchmark data sets. All empirical results show that our new clustering models consistently outperform the related clustering meth-ods.

Notations: Throughout the paper, all the matrices are written as uppercase. For matrix M ,the i -th row (with transpose) and the ( i, j )-th element of M are denoted by m and m ij , respectively. The trace of matrix M is denoted by Tr ( M ). The L2-norm of vector v is denoted by v 2 , the Frobenius norm of matrix M is denoted by M F .An identity matrix is denoted by I ,and 1 denotes a column vector with all the elements as one. For vector v and matrix M , v  X  0and M  X  0 mean all the elements of M and v are equal to or larger than zero.
Exploring the local connectivity of data is a successful strategy for clustering task. Given a data set { x 1 ,x 2 we denote X  X  R n  X  d as the data matrix. The neighbors of x  X  R d  X  1 can be defined as the k -nearest data points in the data set to x i . In this paper, we consider the probabilistic neighbors, and use the Euclidean distance as the distance measure for simplicity.

For the i -th data point x i , all the data points { x 1 ,x can be connected to x i as a neighbor with probability s ij Usually, a smaller distance x i  X  x j 2 2 should be assigned a larger probability s ij , so a natural method to determine the probabilities s ij | n j =1 is solving the following problem: where s i  X  R n  X  1 is a vector with the j -th element as s
However, the problem (1) has a trivial solution, only the nearest data point can be the neighbor of x i with probability 1 and all the other data points can not be the neighbors of x . On the other hand, if we solve the following problem without involving any distance information in the data: the optimal solution is that all the data points can be the neighbors of x i with the same probability 1 n ,whichcanbe seen as a prior in the neighbors assignment.

Combining Eq.(1) and (2), we can solve the following problem: The second term in Eq.(3) is a regularization and  X  is the regularization parameter. Denote d x ij = x i  X  x j 2 2 , and de-note d x i  X  R n  X  1 as a vector with the j -th element as d then the problem (3) can be written in vector form as We will see in Subsection 2.4 that this problem can be solved with a closed form solution.

For each data point x i , we can use Eq.(3) to assign its neighbors. Therefore, we can solve the following problem to assign the neighbors for all the data points: In the clustering task to partition the data into c clusters, an ideal neighbors assignment is that the connected com-ponents in the data are exact c . Usually the neighbors as-signment with Eq.(5) can not reach the ideal case for any value of  X  . In most cases, all the data points are connected as just one connected component. In order to achieve the ideal neighbors assignment, the probabilities s ij | n i,j problem (5) should be constrained such that the neighbors assignment becomes an adapt ive process to make the con-nected components are exact c . It seems look like an im-possible goal since this kind of structured constraint on the similarities is fundamental but also very difficult to han-dle. In this paper, we will propose a novel but very simple method to achieve this goal.

The matrix S  X  R n  X  n obtained in the neighbors assign-ment can be seen as a similarity matrix of the graph with the n data points as the nodes. Suppose each node i is as-signed a function value as f i  X  R c  X  1 , then it can be verified that where F  X  R n  X  c with the i -th row formed by f i , L S = D degree matrix D S  X  R n  X  n is defined as a diagonal matrix where the i -th diagonal element is j ( s ij + s ji ) / 2.
If the similarity matrix S is nonnegative, then the Lapla-cian matrix has an important property as follows [14, 5]. Theorem 1. The multiplicity c of the eigenvalue 0 of the Laplacian matrix L S is equal to the number of connected components in the graph with the similarity matrix S . Theorem 1 indicates that if rank ( L S )= n  X  c , then the neighbors assignment is an ideal assignment and we already partition the data points into c clusters based on S , without having to perform K -means or other discretization proce-dures. Motivated by Theorem 1, we add an additional con-straint rank ( L S )= n  X  c into the problem (5) to achieve the ideal neighbors assignment with clear clustering structure. Thus, our new clustering model is to solve: It is difficult to solve the problem (7). Because L S = D 2 and D S also depends on S , the constraint rank ( L S )= n  X  k is not easy to tackle. In the next subsection, we will propose a novel and efficient algorithm to solve this chal-lenging problem.
Suppose  X  i ( L S )isthe i -th smallest eigenvalue of L S know  X  i ( L S )  X  0since L S is positive semi-definite. It can be seen that the problem (7) is equivalent to the following problem for a large enough value of  X  : When  X  is large enough, note that  X  i ( L S )  X  0 for every i , then the optimal solution S to the problem (8) will make the second term c i =1  X  i ( L S ) to be zero, and thus the constraint rank ( L S )= n  X  c in the problem (7) could be satisfied.
According to the Ky Fan X  X  Theorem [6], we have Therefore, the problem (8) is further equivalent to the fol-lowing problem Compared with the original problem (7), the problem (10) is much easier to solve. We can apply the alternative opti-mization approach to solve it.

When S is fixed, the problem (10) becomes The optimal solution F to the problem (11) is formed by the c eigenvectors of L S corresponding to the c smallest eigen-values.

When F is fixed, the problem (10) becomes According to Eq.(6), the problem (12) can be rewritten as Note that the problem (13) is independent between different i , so we can solve the following problem individually for each i : d  X  R n  X  1 as a vector with the j -th element as d ij = d x  X d ij , then the problem (14) can be written in vector form as We will see in Subsection 2.4 that this problem can be solved with a closed form solution.

The detailed algorithm to solve the problem (7) is sum-marized in Algorithm 1.
In practice, to accelerate the procedure, the  X  can be de-termined during the iteration. We can initialize  X  =  X  ,then increase  X  if the connected components of S is smaller than c and decrease  X  if it is greater than c in each iteration. Algorithm 1 Algorithm to solve problem (7). input Data matrix X  X  R n  X  d , cluster number c , parameter  X  , a large enough  X  1 . output S  X  R n  X  n with exact c connected components.
Initialize S by the optimal solution to the problem (5). while not converge do end while Denote the centering matrix by and denote D x  X  R n  X  n as a distance matrix where the ( i, j )-th element is d x ij = x i  X  x j 2 2 . To analyze the connection of Algorithm 1 to K -means, we first need the following lemma: Lemma 1. HD x H =  X  2 HXX T H Proof. Since d x have D x = Diag ( XX T ) 11 T + 11 T Diag ( XX T )  X  2 XX where Diag ( XX T ) is a diagonal matrix with the diagonal elements of XX T .Notethat H 1 = 1 T H = 0 according to the definition of H ,wehave HD x H =  X  2 HXX T H . Theorem 2 reveals that Algorithm 1 is exactly solving the K -means problem when  X   X  X  X  .

Theorem 2. When  X   X  X  X  , the problem (7) is equivalent to the problem of K -means.
 Proof. The problem (7) can be written in matrix form as Due to the constraint rank ( L S )= n  X  c ,thesolution S has exact c components (that is, S is block diagonal with proper permutation). Suppose the i -th component of S is S component, then solving problem (17) is to solve the follow-ing problem for each i : When  X   X  X  X  , then the problem (18) becomes The optimal solution to the problem (19) is that all the elements of S i are equal to 1 n i .

Therefore, the optimal solution S to the problem (17) should be the following form when  X   X  X  X  : We denote the solution set that satisfies the form in Eq.(20) by V . Note that for any possible partition of the c com-ponents such that S has the form as in Eq.(20), S 2 F has the same value, i.e., S 2 F = c . Therefore, the problem (17) becomes According to the constraint of S in Eq.(21), S is symmet-ric and S 1 = 1 T S = 1 .So Tr ( HD x HS )= Tr ( D x S )  X  1 T D x 1 and thus the problem (21) is equivalent to the fol-lowing problem: Define the label matrix Y  X  R n  X  c ,wherethe( i, j )-th ele-ment is Then according to Eq.(22) and Lemma 1, we have which is exactly the problem of K -means. The matrix S w is called within-class scatter matrix in classical Linear Discrim-inant Analysis (LDA) when the labels Y of data are given. K -means is to find the optimal labels Y such that the trace of the within-class scatter matrix Tr ( S w ) is minimized.
We will see in the next subsection that the proposed method in Algorithm 1 is closely related to spectral clustering. There-fore, although the new algorithm is to solve the K -means problem (which can only partition data with spherical shape) when  X   X  X  X  , it can partition data with arbitrary shape when  X  is not very large. We will also see in the experiments that this new algorithm can find much better solution to the K -means problem even when  X  is not very large.
Givenagraphwiththesimilarity S ,spectralclusteringis to solve the following problem: The optimal solution is the spectral decomposition of the Laplacian matrix L S , i.e., the optimal solution F is formed by the c eigenvectors of L S corresponding to the c smallest eigenvalues, as in Eq.(11).

Usually, given a similarity S ,theobtained F can not be directly used for clustering since the graph with S does not has exact c connected components. K -means or other dis-cretization procedures should be performed on F to obtain the final clustering results [9].

In the convergence of Algorithm 1, we also obtain an opti-mal solution F to the problem (25), the difference is that the similarity S is also learned by the algorithm. Note that in the convergence, the last term 2  X Tr ( F T L S F ) in the problem (10) will approximate zero, the learned S is mainly achieved by solving problem (5).

Thanks to the constraint rank ( L S )= n  X  c , the graph with the learned S will has exact c connected components. Therefore, the optimal solution F ,whichisformedbythe c eigenvectors of L S corresponding to the c smallest eigenval-ues, can be written as where Y  X  R n  X  c is the label matrix defined in Eq.(23) and Q  X  R c  X  c is an arbitrary orthogonal matrix. That is to say, we can use the obtained F directly to get the final clus-tering result, without the K -means or other discretization procedures as traditional spectral clustering has to do.
In another viewpoint, it can be seen that the proposed algorithm learns the similarity matrix S and the label matrix F simultaneously, while traditional spectral clustering only learns the F given the similarity matrix S . Therefore, the new algorithm could achieve better performance in practice since it also learns an adaptive graph for clustering.
In practice, regularization parameter is difficult to tune since its value could be from zero to infinite. In this sub-section, we present an effective method to determine the regularization parameter  X  in the problem (7).

For each i , the objective function in the problem (7) is equal to the one in the problem (4). The Lagrangian func-tion of problem (4) is where  X  and  X  i  X  0 are the Lagrangian multipliers.
According to the KKT condition [2], it can be verified that the optimal solution s i should be In practice, usually we could achieve better performance if we focus on the locality of data. Therefore, it is preferred to learn a sparse s i , i.e., only the k nearest neighbors of x have chance to connect to x i . Another benefit of learning a sparse similarity matrix S is that the computation burden can be alleviated largely for subsequent processing.
Without loss of generality, suppose d x i 1 ,d x i 2 , ..., d dered from small to large. If the optimal s i has only k nonzeroelements,thenaccordingtoEq.(28),weknow s ik &gt; 0and s i,k +1 = 0. Therefore, we have According to Eq.(28) and the constraint s T i 1 =1,wehave So we have the following inequality for  X  i according to Eq.(29) and Eq.(30): Therefore, in order to obtain an optimal solution s i to the problem (4) that has exact k nonzero values, we could set  X  to be The overall  X  could be set to the mean of  X  1 , X  2 , ...,  X  is, we could set the  X  to be The number of neighbors k is much easier to tune than the regularization parameter  X  since k is an integer and has ex-plicit meaning.
Clustering high-dimensional data is an important and chal-lenging problem in practice. In this section, we propose a projected clustering approach with the adaptive neighbors to solve this problem. The goal is to find an optimal sub-space on which the adaptive neighboring is performed such that there are exact c connected components in the data.
Denote the total scatter matrix by S t = X T HX ,where H is the centering matrix defined in Eq.(16). Suppose we are to learn a projection matrix W  X  R d  X  m . First, we constrain the subspace with W T S t W = I such that the data on the subspace are statistically uncorrelated. As in Eq.(5), we assign the neighbors for each data by solving the following problem: Similarly, to make the neighbors assignment be adaptive such that the connected components in the data are exact c , we impose the constraint on S with rank ( L S )= n  X  c . Therefore, we propose the following problem for learning the projection W and the clustering simultaneously:
Using the same trick as in Subsection 2.1, we know that solving problem (35) is equivalent to solving the following problem We can also apply the alternative optimization approach to solve this problem.

When S is fixed, the problem (36) becomes the problem (11), and the optimal solution F is formed by the c eigen-vectors of L S corresponding to the c smallest eigenvalues.
When F is fixed, the problem (36) becomes In problem (37), if S is fixed, then the problem becomes which can be rewritten as the following problem according to Eq.(6): The optimal solution W to the problem (39) is formed by the m eigenvectors of S  X  1 t X T L S X corresponding to the m smallest eigenvalues (we assume the null space of the data X is removed, i.e., S t is invertible).

In problem (37), if W is fixed, then according to Eq.(6) again, the problem (37) can be rewritten as Note that the problem (40) is independent between different i , so we can solve the following problem individually for each i : denote d w i  X  R n  X  1 as a vector with the j -th element as d ij = d wx ij +  X d f ij , then the problem (41) can be written in vector form as which is the same problem as in Eq.(15) and can be solved with a closed form solution.

The detailed algorithm to solve the problem (35) is sum-marized in Algorithm 2. We can also use Eq.(33) to deter-mine the regularization parameter  X  . Algorithm 2 Algorithm to solve problem (35). input Data matrix X  X  R n  X  d , cluster number c , reduced dimension m , parameter  X  , a large enough  X  . output S  X  R n  X  n with exact c connected components, pro-jection W  X  R d  X  m .

Initialize S by the optimal solution to the problem (3). while not converge do end while
In this subsection, we show that when  X   X  X  X  , the pro-posed method in Algorithm 2 is to solve the LDA problem with the labels also being optimized at the same time. The result is summarized in the following theorem:
Theorem 3. When  X   X  X  X  , the problem (35) is equiv-alent to the problem of LDA, in which the labels are also variables to be optimized.
 Proof. The problem (35) can be written in matrix form as where D wx  X  R n  X  n is a distance matrix with the ( i, j )-th element as d wx ij = W T x i  X  W T x j 2 2 . Due to the constraint rank ( L S )= n  X  c ,thesolution S has exact c components. Suppose the i -th component of S is S i  X  R n i  X  n i ,where n is the number of data points in the component, then solving problem (43) is to solve the following problem for each i : When  X   X  X  X  , then the problem (44) becomes The optimal solution to the problem (45) is that all the elements of S i are equal to 1 n i .

With similar derivation as in Subsection 2.2, we know the problem (43) is equivalent to the following problem when  X   X  X  X  : where V is the solution set of S that satisfies the form in Eq.(20).

Then according to Eq.(46) and Lemma 1, we have where Y is the label matrix defined as in Eq.(23). If the label matrix Y is given, then the optimal solution to the problem (47) is equal to the solution of LDA. Therefore, when  X   X  X  X  , the proposed method in Algorithm 2 is to solve the LDA problem, but the labels are also unknown and are to be optimized at the same time.

When  X  is not very large, the matrix X T L S X in Eq.(39) can be viewed as local within-class scatter matrix. In this case, the Algorithm 2 can be viewed as a unsupervised ver-sion of local scatter matrices based LDA methods, which are designed for handling multimodal non-Gaussian data [17].
In this part, we will show the performance of the proposed clustering method (Algorithm 1) and the projected cluster-ing method (Algorithm 2) on both synthetic data and real data sets. For simplicity, we denote our clustering method as CAN (Clustering with Adaptive Neighbors), and our pro-jected clustering method as PCAN (Projected Clustering with Adaptive Neighbors) in the following context.
We use two synthetic data sets to measure the clustering efficiency of CAN. Also, we find that CAN is able to obtain an excellent initialization index vector for K -Means, which decreases its objective value and promotes its clustering ac-curacy to a large extent.

The first toy data set we used is a randomly generated two-moon data. In this data set, there are two clusters of data distributed in the two-moon shape. Our goal is to construct a new similarity matrix to divide the data points into exact two clusters. From Fig. 1 we can easily find the clustering efficiency of the proposed CAN method. In this figure, we set the color of the two clusters to be cyan and blue separately and let the width of the connecting line denote the learned similarity of two corresponding points. In the initial similarity matrix learned by Eq.(5), several pairs of points from different clusters are connected. Whereas, after the learning by Eq.(7), there is not even a single line between these two clusters, which means that the proposed clustering method CAN can successfully partition the original data into two classes.

The second toy data set is a randomly generated multi-cluster data. In this data set, there are 196 clusters dis-tributed in a spherical way. Firstly we run K -Means for 10000 times and report the result with the minimal K -means objective value in the independently 10000 times run. Then we run CAN once to generate an clustering result and use it as initialization for K -means. The comparison results are shown in Tab. 1 and Fig. 2. It is apparent that even after 10000 times run, the minimal K -means objective value and clustering accuracy obtained by K -means are far behind the result obtained by PCAN with only one run.
 Table 1: Clustering accuracy and minimal K -Means objective value on multi-cluster synthetic data sets.
For PCAN method, not only did we measure its clustering efficiency but we also tested its projection ability. The first toy data set for PCAN is a randomly generated three-ring data. In this data set, there are five dimensions, among which the data in the first two dimensions are distributed in a three-circle shape while the data in the other three dimensions are noises.

Since only the first two dimensions contain useful infor-mation, it would be important if the method could find a subspace which contains only important dimensions. We compare the PCAN method with two popular dimension re-duction methods, Principal Component Analysis (PCA) [11] and Locality Preserving Projections(LPP) [8]. From Fig. 3 we can see the projection results of these three methods. Apparently, PCA and LPP are not powerful enough to find a good subspace for this projection task. In contrast, the proposed method PCAN successfully finds a subspace which is almost the same as the subspace formed by the first two significant dimensions, and also accomplishes the clustering task.

The second toy data set for PCAN is a randomly gen-erated two-Gaussian data. In this data set, there are two clusters of data which obeys the Gaussian distribution. Our goal is to find a good projection direction which helps to partition the two clusters apart. Still, we compare PCAN with PCA and LPP. The comparison results are displayed in Fig. 4. Seen from Fig. 4, we can find out that when these two clusters are far from each other, all these three methods could easily find a good projection direction. However, as the distance between these two clusters lower down, PCA becomes inefficient. As the two clusters become more close, LPP also loses its way to achieve the projection goal. How-ever, the PCAN method always behave well. The reason for this phenomenon is that PCA focus on the global structure thus fail immediately after the two clusters become close. While LPP pays more attention to the local structure, thus could achieve good performance when the two clusters are relatively close. But when the distance of these two clus-ters are fairly small, LPP is not capable any more. But our method, PCAN, lays more emphasis on the discriminative structure and thus keeps its projection quality all the time.
We evaluated the proposed clustering methods on 15 bench-mark datasets: Stock, Pathbased, Movements, Wine, Com-pound, Spiral, Yeast, Glass, Ecoli, UmistData, COIL25, JAFFE, USPS, Palm and MSRA, among which three are shape data sets, six are biological data sets from UCI Ma-chine Learning Repository and the other six are image data sets. The descriptions of these 15 datasets are summarized in Tab. 2.

We compared our clustering methods CAN and PCAN with K -means, Ratio Cut, Normalized Cut and NMF meth-ods.

In the clustering experiment, we set the number of clusters to be the ground truth in each data set. For those methods calling for an input of an affinity matrix, like Ratio Cut, Normalized Cut and NMF, the graph is constructed with the self-tune Gaussian method [23]. For the methods involving K -means, including K -means, Ratio Cut and Normalized Cut, we use the same initialization and repeat 100 times independently to perform K -means for discretization. For these methods, we record the average performance, standard deviation and the performance corresponding to the best K -Means objective value. As for NMF, CAN and PCAN, we run only once and record the results.

The evaluation of different methods is based on two widely used clustering metrics: accuracy and NMI (normalized mu-tual information). From Tab. 3 and Tab. 4, we can come into the conclusion that our proposed methods CAN and PCAN outperform other methods on most of the benchmark data sets. We always get a better accuracy and NMI under differ-ent circumstances. In addition, the results of other methods are dependent of the initialization, while ours are always stable with a certain setting. (a) The First Two Dimensions (c) Learnt Subspace by LPP
In this experiments, we evaluated the projection ability of the proposed PCAN method on 6 benchmark data sets with high dimension: Movements, UmistData, Coil20, Jaffe50, Palm and MSRA50. Similar to that in the synthetic data experiments, we compared PCAN with PCA and LPP meth-ods.

The affinity matrix for LPP is constructed with the self-tune Gaussian method [23]. In order to measure the quality of dimension reduction, we let the three methods learn a projection matrix first and perform K -Means on the pro-jected data for projection ability measurement. For each method, the K -Means method is repeated for 100 times so as to compute the average clustering accuracy and standard deviation.

The results are reported in Fig. 5 and Tab. 5, from which we can see the superiority of the PCAN method for projec-tion task. Moreover, the PCAN method is able to project the original data onto a subspace with quite small dimensions( c -1), where c denotes the number of clusters in each data set. Such a subspace with low dimensions projected by our method would even outperform that obtained by PCA and LPP with higher dimensions. It is worthy to noting that, if we apply PCAN method for only projection task, the c in Data sets Num of Instances Dimensions Classes Pathbased 300 2 3 Movements 360 90 15 Compound 399 2 6 Table 2: Descriptions of 15 Benchmark Datasets PCAN can be viewed as a parameter and thus we can reduce data into arbitrary dimensions. In this experiment, we set the c in PCAN to the class number of data for simplicity. Movements 44.46  X  2.48 47.47  X  2.55 55.99  X  3.06 Table 5: The best results with optimal dimensions.
In this paper, we proposed a novel clustering model to si-multaneously learn the data similarity matrix and the clus-tering structure. In our new clustering method, the data similarity matrix is learned by assigning the adaptive neigh-bors for each data point based on the local connectivity. The new rank constraint is imposed on the Laplacian matrix of the data similarity matrix to create the clustering structure in the similarity matrix as several disconnected components. We derived an efficient algorithm to optimize the proposed challenging problem, and proved the theoretical connections between our method and other clustering approaches. We further extended the proposed clustering model for the pro-jected clustering on high-dimensional data. Extensive exper-iments have been conducted on both synthetic data and 15 real-world benchmark data sets to demonstrate the superior performance of our models. This research was partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628.
