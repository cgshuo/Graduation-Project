 The field of network analysis is a well established field which has existed as an independent research discipline since the late seventies [ Zachary , 1977 ]and early eighties [ Burt and Minor , 1983 ]. In recent years, analysis of heterogeneous information networks [ Sun and Han , 2012 ] has gained popularity. In contrast to standard (homogeneous) information networks, heterogeneous networks describe heterogeneous types of entities and different types of relations. To encode even more information into the network, analysis of enriched heterogeneous infor-mation networks , where nodes of one type carry additional information in the form of experimental results or text documents, has arisen in recent years [ Dutkowski and Ideker , 2011 ; Hofree et al. , 2013 ].
 This paper addresses the task of mining text enriched heterogeneous informa-tion networks [ Gr X  caretal. , 2013 ]. Compared to the original methodology, our implementation allows for the analysis of much larger heterogeneous networks given significantly decreased computation time. This was achieved by a modified PageRank computation, which takes into account only the parts of the network reachable from the given node, as explained in the methodology section. We showcase the utility of the improved approach on a large citation network in the field of psychology, where nodes X  X epresenting publications X  X re enriched with the publication abstracts. We analyze how the size of the network and the amount of structural information affect the accuracy of paper categorization. Section 3 presents the upgraded methodology used to analyze text enriched heterogeneous information networks. Section 4 presents the application of the methodology on a large data set of publications from the field of psychology. Section 5 presents the evaluation and analysis of information different compo-nents contribute to the quality of classifiers. Section 6 concludes the paper and presents the plans for further work. Network mining algorithms perform data analysis in a network setting, where each data instance is connected to other instances in a network of connections. PageRank [ Page et al. , 1999 ], SimRank [ Jeh and Widom , 2002 ] and diffusion kernels [ Kondor and Lafferty , 2002 ], authority is propagated via network edges to discover high ranking nodes in the network. Sun and Han [ 2012 ] introduced the concept of authority ranking for heterogeneous networks with two node types (bipartite networks) to simultaneously rank nodes of both types. Sun et al. [ 2009 ] address authority ranking of all nodes types in heterogeneous networks with a star network schema, while Gr X  car et al. [ 2013 ] apply the PageRank algorithm to only find PageRank values of nodes of a particular node type.
 is to find class labels for some of the nodes in the network using known class labels for a part of the network. A typical approach to solving this problem involves propagating the labels in the network, a concept used in [ Zhou et al. , 2004 ] and [ Vanunu et al. , 2010 ]. The concept of label propagation was expanded to heterogeneous networks by Hwang and Kuang [ 2010 ], performing label prop-agation to different node types with different diffusion parameters, similarly to the GNETMINE algorithm proposed by Ji et al. [ 2010 ]. Classification in hetero-geneous networks can also be assisted by ranking, as shown by the ranking based classification approach described by Sun and Han [ 2012 ].
 ing enriched information networks. While Dutkowski and Ideker [ 2011 ]and Hofree et al. [ 2013 ] explore biological experimental data using heterogeneous biological networks, Gr X  car et al. [ 2013 ] perform videolectures categorization in a heterogeneous information network of nodes enriched with text information. mining. The task addressed is text categorization in which one has to predict the category of a given document, based on a set of prelabeled documents. Most text mining approaches use the bag-of-words vector representation for each pro-cessed document. The resulting high dimensional vectors can be used by any machine learning algorithm capable of handling such vectors, such as a SVM classifier [ D X  X razio et al. , 2014 ; Kwok , 1998 ; Manevitz and Yousef , 2002 ], kNN classifier [ Tan , 2006 ], Naive Bayes classifier [ Wong , 2014 ], or a centroid classifier [ Han and Karypis , 2000 ]. This section presents the basics of the methodology of mining text enriched information networks, first introduced by Gr X  car et al. [ 2013 ]. The methodology combines text mining and network analysis on a text enriched heterogeneous information network (such as the citation network of scientific papers) to con-struct feature vectors which describe both the node content and its position in the network.
 The information network is represented as a graph, a structure composed of a set of vertices V and a set of edges E . The edges may be either directed or undi-rected. Each edge may also have a weight assigned to it. The vertices (or nodes) of the graph in the information network are data instances. A heterogeneous information network , as introduced by Sun and Han [ 2012 ], is an information network with an additional structure which assigns a type to each node and edge of the network. The requirement is that all starting (or ending) points of edges of a certain type belong to the same type.
 The data in a text enriched heterogeneous information network represents a fusion of two different data types: heterogeneous information networks and texts. Our data thus comprises of a heterogeneous information network with different node and edge types, where nodes of one designated type are text documents. Network Decomposition. In the first step of the methodology, for the desig-nated node type (i.e., text documents), the original heterogeneous information network is decomposed into a set of homogeneous networks. In each homogeneous network, two nodes are connected if they share a particular direct or indirect link in the original heterogeneous network. Take an example of a network containing two types of nodes, P apers and A uthors, and two edge types, Cites (linking papers to papers) and Written by (linking papers to authors). From it, we can construct two homogeneous networks of papers: the first in which two papers are connected if one paper cites another, and the second in which they are con-nected if they share a common author 1 . The choice of links to be used in the network decomposition step is the only manual step of the methodology: taking into account the real-world meaning of links, the domain expert will select only the decompositions relevant for the given task. Feature Vector Construction. In the second step of the methodology, a set of feature vectors is calculated for each text in the original heterogeneous network: one bag-of-words vector constructed from the text document itself, and one feature vector constructed from every individual homogeneous network. natural language processing techniques. Typically the following steps are per-formed: preprocessing using a tokenizer, stop-word removal, stemming, construc-tion of N-grams of a certain length, and removal of infrequent words from the vocabulary.
 the personalized PageRank (P-PR) algorithm [ Page et al. , 1999 ]isusedtocon-struct feature vectors for each text in the network.
 the stationary distribution of the position of a random walker which starts its walk in node v and at either selects one of the outgoing connections or travels to his starting location. The probability (denoted p ) of continuing the walk is a parameter of the personalized PageRank algorithm and is usually set to 0 . 85. The PageRank vector is calculated iteratively. In the first step, the rank of node v is set to 1 and the other ranks are set to 0. Then, at each step, the rank is spread along the connections of the network using the formula where r ( k ) is the estimation of the PageRank vector after k iterations, and A is the coincidence matrix of the network, normalized so that the elements in each of its rows sum to 1.
 Equation 1 , converges to the PageRank vector at a rate of p . In our experi-ments, the number of steps required ranged from 50 to 100, and since each step requires one matrix-vector multiplication, the calculation of a single P-PR vector may take several seconds for a large network, making the calculation of tens of thousands of P-PR vectors computationally very demanding.
 by considerably decreasing the amount of computation for cases, where the size of the network taken into account during computation can be decreased. For each network node v , we can consider only the network G v , composed of all the nodes and edges of the original homogeneous network that lie on paths leading from v . The P-PR v values, calculated on G v , are equal to the P-PR values, calculated on the entire homogeneous network. If the network is strongly connected, G be equal to the original network, yielding no change in the performance of the P-PR algorithm, However, if the network G v is smaller, the calculation of the P-PR v algorithm will be faster as it is calculated on G v network. In our implementation we first estimate if the network G less than 50% of the original nodes. This is achieved by expanding all possible paths from node v and checking the number of visited nodes in each step. If the number of visited nodes stops increasing after a maximum of 15 steps, we know we have found the network G v and can count its nodes. If the number of nodes is still increasing, we abort the calculation of G v . We limit the maximum number of steps because each step of G v is computationally comparable to one step in the PageRank iterative algorithm which converges in about 50 steps. Therefore we can considerably reduce the computational burden if we do not perform too many steps in the search for G v .
 Once calculated, the resulting PageRank and BOW vectors are normalized according to the Euclidean norm.
 Data Fusion. The result of running both the text mining procedure and the personalized PageRank is a set of vectors { v 0 ,v 1 ,...,v v is the BOW vector, and where for each i (1  X  i  X  n , where n is the number of network decompositions), v i is the personalized PageRank vector of node v in the i -th homogeneous network. In the final step of the methodology, these vectors are combined to create one large feature vector. Using positive weights  X  which sum to 1, a unified vector is constructed which fully describes the publi-cation from which it was calculated. The vector is constructed as where the symbol  X  represents the concatenation of two vectors. The values of the weights  X  i can either be set manually using a trial-and-error approach or can be determined automatically.
 A simple way to automatically set weights is to use an optimization algorithm such as the multiple kernel learning (MKL), presented in [ Rakotomamonjy et al. , 2008 ] in which the feature vectors are viewed as linear kernels. For each i ,the vector v i corresponds to the linear mapping v i : x  X  x  X  ity is to determine the optimal weights using a general purpose optimization algorithm, e.g., differential evolution [ Storn and Price , 1997 ]. In previous work, Gr X  car et al. [ 2013 ] used the described methodology to assist in the categorization of video lectures, hosted by the VideoLectures.net repos-itory. The methodology turned out useful because of the rapid growth of the number of hosted lectures and the fact that there is a relatively large number of possible categories into which the lectures can be categorized. In this paper, the methodology is applied to a much larger network which allowed us to see 1) how the methodology scales up to big data and 2) if the information contained in the network structure is necessary at all when the textual data is abundant. We collected data for almost one million scientific publications from the field of psychology. Like the video lectures, the publications belong to one or more categories from a large set of possible categories. The motivation is to construct a classifier which is capable to find appropriate categories for new publications with more probable categories listed first. Such a classifier can be used to assist in the classification of new psychology articles. The same methodology and data set could be exploited to form reading recommendations based on selected paper and to assist authors in submitting their papers to the most appropriate journal. we describe creation of heterogeneous network of publications and authors and experiments performed on the data set.
 Data Collection. The first step in the construction of a network is data collec-tion. To the best of our knowledge, there is no freely available central database containing publications in the field of psychology. Because of this, we decided to crawl the pages connected with psychology on Wikipedia.
 visited the hierarchical tree of Wikipedia X  X  subcategories of the category Psy-chology. We examined all categories up to level 5 in the hierarchy. The decision was based on the difference between the number of visited categories and the number of articles at depths 4, 5 and 6. We crawled through all Wikipedia pages, belonging to the visited categories, and extracted the DOIs (digital object iden-tifiers) of all publications, referenced in the pages.
 If a publication was found on MAS, we collected the information about the title, authors, year of publication, the journal, ID of the publication, IDs of the authors, etc. Whenever possible, we also extracted the publication X  X  abstract. Additionally, we collected the same information for all the publications that cite the queried publications.
 Dataset. The result of our data collection process is a network consisting of 953 , 428 publications of which 63 , 862  X  X ore publications X  were obtained directly from Wikipedia pages. Other publications were citing the core publications. Each of the core publications was labelled with one or more Wikipedia categories from which it was collected. The categories at levels 3, 4 and 5 were transformed into higher level categories by climbing up the category hierarchy to level 2. This was done to decrease the total number of classes. We collected 93 , 977 abstracts of the publications, of which 4 , 551 belong to the core publications.
 works: the paper-author-paper (PAP) network, the paper-cites-paper (PP) net-work and a symmetric copy of the PP network in which directed edges are replaced by undirected edges (PPS).
 Experiment Description. In all the experiments we used the same settings to obtain the feature vectors. As in [ Gr X  car et al. , 2013 ], n -grams of size up to 2 and a minimum term frequency of 0 was used to calculate the BOW vectors. For the calculation of personalized PageRank vectors the damping factor was set to 0 . 85 (the standard setting also used by Page et al. [ 1999 ]). In the experiments with more than one feature vector, the vectors were concatenated using weights determined by the differential evolution optimization [ Storn and Price , 1997 ]. In all the experiments we used the centroid classifier using the cosine similarity distance. This classifier first calculates the centroid vector for each class (or category) by summing and normalizing all vectors belonging to instances of that class. For a new instance with feature vector w , it then calculates the cosine similarity distance which represents the proximity of the instance to class i . The class (category) with the minimal distance is selected as the prediction outcome. We also use the  X  X op n  X  classifier, where the classifier returns n classes with the minimal dis-tances. As in [ Gr X  car et al. , 2013 ], we consider a classifier successful if it correctly predicts at least one label of an instance.
 We use the centroid classifier for two reasons. First, Gr X  car et al. [ 2013 ] show that it performs just as well as the SVM and the k -nearest neighbor classifier. Second, for large networks calculating all the personalized PageRank vectors is computationally very expensive. As shown in [ Gr X  car et al. , 2013 ], the centroids of each class can be calculated in a single iteration of the PageRank algorithm. We performed three sets of experiments using different number of papers and different homogenization of the heterogeneous network.
 In the first set, we use the publications for which abstracts are available. Because most of the 93 , 977 qualifying papers are not core publications, we con-struct only two feature vectors for each publication: a bag-of-words (BOW) vec-tor and a personalized PageRank vector obtained from the PAP network. We examine how the predictive power of the classifier increases as the number of and 93 , 977 publications.
 In the second round of experiments all the collected papers are used (953 , 428 papers). Because the papers are labelled using citations the PP and PPS net-works are not used because the links in this network were used to label the papers. Since the abstracts are not available for most of these papers, only the personalized PageRank vectors obtained from the PAP network are used in the classification.
 In the third round of experiments, we use only the core publications for which an abstract is available (4 , 551 papers). While this is the smallest data set, it allows us to use all of the feature vectors the methodology provides: the BOW vectors and the personalized PageRank obtained from all three networks (PP, PPS and PAP). In each of the experiments, described in Sect. 4 , we predicted the labels of the analyzed publications. The classification accuracy was measured for the top 1, 3, 5 and 10 labels, returned by the classifiers. For each experiment the data set was split into a training set, a validation set and a test set. In the first and third round of experiments, the sizes of the testing and validation set were fixed to 2500 instances, all the remaining instances were used for training. In the second round of experiments, the size of the validation and testing set was set to 1500 instances. The centroids of all classes were calculated using the training set and concatenated according to the weights optimized using the validation set. The performance of the algorithm (the percentage of papers for which the label is correctly predicted) was calculated using the test set.
 mance of the classifier using BOW vectors does not increase with more instances, while the classifier using PAP vectors is steadily improving as we increase the number of publications. The classifier using both BOW and PAP vectors con-sistently outperforms the individual classifiers. This shows that combining the network structural information and the content of the publication is useful. As the performance of the PAP classifier increases, the gap between the BOW clas-sifier and the classifier using both vectors also increases. The results obtained with all the 93 , 977 publications are shown also in Table 1 .
 The classifier using the full PAP network (calculated in the experiment 2), also shown in Table 1 , outperforms the classifiers using all other networks, show-ing that increasing the network size does help the classification. However, its performance is still lower than that of the BOW classifier for smaller networks. It appears that authors in the field of psychology are not strictly limited to one field of research, making prediction using co-authorship information difficult. Table 2 shows the accuracies, obtained in the third round of experiments. Because more information was extracted from the network, these results are the most comprehensive overview of the methodology. The results show that using a symmetric citation network (PPS), i.e. allowing the PageRank to use both directions of a citation yields better results than using the unidirectional cita-tion network (PP). Combining both the PP and PPS vectors does not improve the performance of the classifier, which means that vectors, obtained from the PP network, carry no information that is not already contained in the PPS net-work. However, this is an exception and training classifiers with other vectors combinations increases the prediction accuracy over single vectors: using both BOW and PAP is better than using only BOW, and adding also PP increases the performance even further.
 We also analyze the performance of classifiers for different class values. We analyze each class c in the following way. First, we obtain the ordered list of labels that the classifier returns for each test instance from class c . In this list, class values are ordered according to the distance between the instance and the (already computed) class centroids. The first element in the list is the class value whose centroid is closest to the given instance. We then find the rank of class c on this list. For each instance, we compute the minimum value of n for which the top-n classifier predicts class c for this instance. For each class value, we average the obtained ranks n over test instances with this class. This gives us an estimate of the ranking error.
 We plot these average ranks versus number of instances with each class value. The results are shown in Fig. 2 . The graphs are similar for classifiers using BOW, PPS and PAP vectors. We can see that classes containing a small num-ber of instances have considerably higher average ranks than classes with many instances, meaning that prediction is much less successful for underrepresented class values. The classifier using PP vectors is the only classifier for which this trend does not appear. For the PP classifier, the results for small classes show much more noise than for larger classifiers, but average ranks (i.e., error) does not decrease with increasing number of instances. While network analysis in general is an established field of research, analysis of heterogeneous networks is a much newer field. Methods taking the heterogeneous nature of the networks into account show an improved performance, as shown in Davis et al. [ 2011 ]. Some methods like RankClus and others presented in [ Sun and Han , 2012 ] are capable of solving tasks that cannot even be defined on homogeneous information networks (like clustering two disjoint sets of entities). Another important novelty is joining network analysis with the analysis of data, either in the form of text documents or results obtained from various experiments [ Dutkowski and Ideker , 2011 ], [ Hofree et al. , 2013 ] and [ Gr X  car et al. , 2013 ]. [ Gr X  caretal. , 2013 ], which combines the information from heterogeneous net-works with textual data. By improving the computational efficiency of the app-roach we were able to address a novel application, i.e. the analysis of a large citation network of psychology papers. Our contribution is also the analysis of performance with different number of instances and different types of network structures included. The results show that relational information hidden in the network structure is beneficial for classification, while the errors are shown to be mostly due to low number of instances for some categories.
 In the work presented, we only use a part of the information we collected about the publications. In future, we will to examine how to incorporate the temporal information into our methodology; we have already collected the year of publication, which allows us to observe the dynamics of categories, aiming to improve the classification accuracy. In addition, we plan to use a combination of network analysis and data mining on PubMed and DBLP articles. We will also address biological networks enriched with experimental data and texts.
