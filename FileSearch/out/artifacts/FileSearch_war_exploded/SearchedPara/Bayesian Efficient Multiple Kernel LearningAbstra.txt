 Mehmet G  X onen mehmet.gonen@aalto.fi Helsinki Institute for Information Technology HIIT The main idea of kernel-based algorithms is to learn a linear decision function in the feature space where data points are implicitly mapped to using a kernel function ( Vapnik , 1998 ). Given a sample of N independent and identically distributed training instances { x i  X  X} N i =1 the decision function that is used to predict the target output of an unseen test instance x  X  can be written as where the vector of weights assigned to each train-ing data point and the bias are denoted by a and b , respectively, and k  X  = k ( x 1 , x  X  ) . . . k ( x N , x where k : X  X  X  X  R is the kernel function that cal-culates a similarity measure between two data points. Using the theory of structural risk minimization, the model parameters can be found by solving a quadratic programming problem, known as support vector ma-chine (SVM) ( Vapnik , 1998 ). The model parameters can also be interpreted as random variables to obtain a Bayesian interpretation of the model, known as rel-evance vector machine (RVM) ( Tipping , 2001 ). Kernel selection (i.e., choosing a functional form and its parameters) is the most important issue that affects the empirical performance of kernel-based algorithms and is usually done using a cross-validation procedure. Multiple kernel learning (MKL) methods have been proposed to make use of multiple kernels simultane-ously instead of selecting a single kernel (see a recent survey by G  X onen &amp; Alpayd X n ( 2011 )). Such methods also provide a principled way of integrating feature representations coming from different data sources or modalities. Most of the previous research is focused on developing efficient MKL algorithms. Nevertheless, existing Bayesian MKL methods are problematic in terms of computation time when combining hundreds or thousands of kernels. In this paper, we formulate a very efficient Bayesian MKL method that solves this issue by formulating the combination in a novel way. In Section 2 , we give an overview of the related work by considering existing discriminative and Bayesian MKL algorithms. Section 3 gives the details of the proposed fully conjugate Bayesian formulation, called Bayesian efficient multiple kernel learning (BEMKL). In Section 4 , we explain detailed derivations of our de-terministic variational approximation for binary clas-sification. Extensions towards multiclass learning and semi-supervised learning are summarized in Section 5 . Section 6 evaluates BEMKL with large numbers of ker-nels on standard benchmark data sets in terms of time complexity, and reports the classification results on one bioinformatics and three image recognition tasks, which are frequently used to compare MKL methods. MKL algorithms basically replace the kernel in ( 1 ) with a combined kernel calculated as a function of the input kernels. The most common combination is to use a weighted sum of P kernels { k m : X  X  X  X  R } P m =1 : where the vector of kernel weights is denoted by e and k m, X  = k m ( x 1 , x  X  ) . . . k m ( x N , x  X  )  X  ing MKL algorithms with a weighted sum differ in the way that they formulate restrictions on the kernel weights: arbitrary weights (i.e., linear sum), nonnega-tive weights (i.e., conic sum), or weights on a simplex (i.e., convex sum).
 Bach et al. ( 2004 ) formulate the problem as a second-order cone programming (SCOP) problem, which is formulated as a semidefinite programming problem previously by Lanckriet et al. ( 2004 ). However, SCOP can only be solved for medium-scale problems effi-ciently. Sonnenburg et al. ( 2006 ) reinterpret the prob-lem as a semi-infinite linear programming (SILP) prob-lem, which can be applied to large-scale data sets. Rakotomamonjy et al. ( 2008 ) develop a simple MKL algorithm using a sub-gradient descent (SD) approach, which is faster than SILP method. Later, Xu et al. ( 2009 ) extend the level method , which is originally de-signed for optimizing non-smooth objective functions, to obtain a very efficient MKL algorithm that carries flavors from both SILP and SD approaches but out-performs them in terms of computation time.
 The aforementioned methods tend to produce sparse kernel combinations, which corresponds to using the  X  -norm on the kernel weights. Sparsity at the ker-nel level may harm the generalization performance of the learner and using non-sparse kernel combi-nations (e.g., the  X  2 -norm) may be a better choice ( Cortes et al. , 2009 ). Varma &amp; Babu ( 2009 ) propose a generalized MKL algorithm that can use any differen-tiable and continuous regularization term on the kernel weights. This also allows us to integrate prior knowl-edge about the kernels to the model. Xu et al. ( 2010 ) and Kloft et al. ( 2011 ) independently and in parallel develop an MKL algorithm with the  X  p -norm ( p  X  1) on the kernel weights. This method has a closed-form update rule for the kernel weights and requires only an SVM solver for optimization. Sequential minimal optimization (SMO) algorithm is the most commonly used method for solving SVM problems and efficiently scales to large problems. Vishwanathan et al. ( 2010 ) propose a very efficient method, called SMO-MKL, to train  X  p -norm ( p &gt; 1) MKL models with squared norm as the regularization term using SMO algorithm for solving MKL problem directly instead of solving inter-mediate SVMs at each iteration.
 Most of the discriminative MKL algorithms are devel-oped for binary classification. One-versus-all or one-versus-other strategies can be employed to get mul-ticlass learners. However, there are also some di-rect formulations for multiclass learning. Zien &amp; Ong ( 2007 ) give a multiclass MKL algorithm by formu-lating the problem as an SILP and show that their method is equivalent to multiclass generalizations of Bach et al. ( 2004 ) and Sonnenburg et al. ( 2006 ). Gehler &amp; Nowozin ( 2009 ) propose a boosting-type MKL algorithm that combines outputs calculated from each kernel separately and obtain better results than MKL algorithms with SILP and SD approaches on im-age recognition problems.
 Girolami &amp; Rogers ( 2005 ) present Bayesian MKL al-gorithms for regression and binary classification us-ing hierarchical models. Damoulas &amp; Girolami ( 2008 ) give a multiclass MKL formulation using a very simi-lar hierarchical model. The combined kernel in these two studies is defined as a convex sum of the input ker-nels using a Dirichlet prior on the kernel weights. As a consequence of the nonconjugacy between Dirichlet and normal distributions, they choose to use an impor-tance sampling scheme to update the kernel weights when deriving variational approximations. Recently, Zhang et al. ( 2011 ) propose a fully Bayesian inference methodology for extending generalized linear models to kernelized models using a Markov chain Monte Carlo (MCMC) approach. The main issue with these approaches is that they depend on some sampling strategy and may not be trained in a reasonable time when the number of kernels is large.
 Girolami &amp; Zhong ( 2007 ) formulate a Gaussian pro-cess (GP) variant that uses multiple covariances (i.e., kernels) for multiclass classification using a variational approximation or expectation propagation scheme, which requires an MCMC sub-sampler for the covari-ance weights. Titsias &amp; L  X azaro-Gredilla ( 2011 ) pro-pose a multitask GP model that combines a common set of GP functions (i.e., information sharing between the tasks) defined over multiple covariances with task-dependent weights whose sparsity is tuned using the spike and slab prior. A variational approximation ap-proach is derived for an efficient inference scheme. Our main motivation for this work is to formulate an efficient Bayesian inference approach without resorting to expensive sampling procedures.
 In order to obtain an efficient Bayesian MKL algo-rithm, we formulate a fully conjugate probabilistic model and develop a deterministic variational approx-imation mechanism for inference. We give the details for binary classification, but the same model can easily be extended to regression.
 Figure 1 illustrates the proposed probabilistic model for binary classification with a graphical model. The main idea is to calculate intermediate outputs from each kernel using the same set of weight parameters and to combine these outputs using the kernel weights and the bias to estimate the classification score. Differ-ent from earlier probabilistic MKL approaches such as Girolami &amp; Rogers ( 2005 ) and Damoulas &amp; Girolami ( 2008 ), our method has two key properties that en-able us to perform efficient inference: (a) Intermediate outputs calculated using the input kernels are intro-duced. (b) Kernel weights are assumed to be normally distributed without enforcing any constraints on them. The notation we use throughout the rest of the manuscript is as follows: N and P represent the num-bers of training instances and input kernels, respec-tively. The N  X  N kernel matrices are denoted by K m , where the columns of K m by k m,i and the rows of K m by k i m . The N  X  1 vectors of weight param-eters a i and their priors  X  i are denoted by a and  X  , respectively. The P  X  N matrix of intermediate out-puts g m i is represented as G , where the columns of G as g i and the rows of G as g m . The bias parame-ter and its prior are denoted by b and  X  , respectively. The P  X  1 vectors of kernel weights e m and their pri-ors  X  m are denoted by e and  X  , respectively. The N  X  1 vector of auxiliary variables f i is represented as f . The N  X  1 vector of associated class labels is represented as y , where each element y i  X  { X  1 , +1 } . As short-hand notations, all priors in the model are denoted by  X  = {  X ,  X  ,  X  } , where the remaining vari-ables by  X  = { a , b, e , f , G } and the hyper-parameters omitted for clarity throughout the manuscript. The distributional assumptions of our proposed model are defined as where the auxiliary variables between the intermediate outputs and the class labels are introduced to make the inference procedures efficient ( Albert &amp; Chib , 1993 ), and the margin parameter  X  is introduced to resolve the scaling ambiguity issue and to place a low-density region between two classes, similar to the margin idea in SVMs, which is generally used for semi-supervised learning ( Lawrence &amp; Jordan , 2005 ). N (  X  ;  X  ,  X  ) rep-resents the normal distribution with the mean vector  X  and the covariance matrix  X  . G (  X  ;  X ,  X  ) denotes the gamma distribution with the shape parameter  X  and the scale parameter  X  .  X  (  X  ) represents the Kronecker delta function that returns 1 if its argument is true and 0 otherwise.
 When we consider the random variables in our model as deterministic values, the auxiliary variable that cor-responds to the decision function value in discrimina-tive methods can be decomposed as f  X  = e where we see that the combined kernel in our model can be expressed as a linear sum of the input kernels. Note that sample-level sparsity can be tuned by assign-ing suitable values to the hyper-parameters (  X   X  ,  X   X  ) as in RVMs ( Tipping , 2001 ). Kernel-level sparsity can also be tuned by changing the hyper-parameters (  X   X  ,  X   X  ). Sparsity-inducing gamma priors can simu-late the  X  1 -norm on the kernel weights, whereas unin-formative priors simulate the  X  2 -norm.
 Exact inference for our probabilistic model is in-tractable and using a Gibbs sampling approach is com-putationally expensive ( Gelfand &amp; Smith , 1990 ). We instead formulate a deterministic variational approxi-mation, which is more efficient in terms of computa-tion time. The variational methods use a lower bound on the marginal likelihood using an ensemble of fac-tored posteriors to find the joint parameter distribu-tion ( Beal , 2003 ). We can write the factorable ensem-ble approximation of the required posterior as p (  X  ,  X  |{ K m } P m =1 , y )  X  q (  X  ,  X  ) = and define each factor in the ensemble just like its full conditional distribution: where  X  (  X  ),  X  (  X  ),  X  (  X  ), and  X (  X  ) denote the shape pa-rameter, the scale parameter, the mean vector, and the covariance matrix for their arguments, respec-tively. T N (  X  ;  X  ,  X  ,  X  (  X  )) denotes the truncated nor-mal distribution with the mean vector  X  , the co-variance matrix  X  , and the truncation rule  X  (  X  ) such T N (  X  ;  X  ,  X  ,  X  (  X  )) = 0 otherwise.
 We can bound the marginal likelihood using Jensen X  X  inequality: log p ( y |{ K m } P m =1 )  X  and optimize this bound by maximizing with respect to each factor separately until convergence. The ap-proximate posterior distribution of a specific factor  X  can be found as For our model, thanks to the conjugacy, the resulting approximate posterior distribution of each factor fol-lows the same distribution as the corresponding factor. 4.1. Inference Details The approximate posterior distributions of the preci-sion priors can be found as where the tilde notation denotes the posterior expec-tations as usual, i.e., g h (  X  ) = E q ( The approximate posterior distribution of the weight parameters is a multivariate normal distribution: q ( a ) = N a ;  X ( a ) The approximate posterior distribution of the interme-diate outputs can be found as a product of multivariate normal distributions: q ( G ) = The approximate posterior distribution of the bias and the kernel weights can be formulated as a multivariate normal distribution: q ( b, e ) = N where we allow kernel weights to take negative values. If the kernel weights are restricted to be nonnegative, the approximation becomes a truncated multivariate distribution restricted to the nonnegative orthant ex-cept for the first dimension, which is used for the bias. The approximate posterior distribution of the auxil-iary variables is a product of truncated normal distri-butions given as where we need to find their posterior expectations to update the approximate posterior distributions of the intermediate outputs, the bias, and the kernel weights. Fortunately, the truncated normal distribution has a closed-form formula for its expectation.
 P inference to reduce the computational complexity. ( 3 ) requires inverting an N  X  N matrix for the covariance calculation, whereas ( 4 ) requires inverting a ( P + 1)  X  ( P + 1) matrix. One of these two updates dominates the running time depending on whether N &gt; P . 4.2. Convergence The inference mechanism sequentially updates the ap-proximate posterior distributions of the model param-eters and the latent variables until convergence, which can be checked by monitoring the lower bound in ( 2 ). The first term of the lower bound corresponds to the sum of exponential form expectations of the distribu-tions in the joint likelihood. The second term is the sum of negative entropies of the approximate poste-riors in the ensemble. The only nonstandard distri-bution in these terms is the truncated normal distri-bution used for the auxiliary variables; nevertheless, the truncated normal distribution has a closed-form formula also for its entropy. Exact form of the vari-ational lower bound can be found in the supplemen-tary material available at http://users.ics.aalto. fi/gonen/bemkl/ . 4.3. Prediction We can replace p ( a |{ K m } P m =1 , y ) with its approximate posterior distribution q ( a ) and obtain the predictive distribution of the intermediate outputs g  X  for a new data point as The predictive distribution of the auxiliary variable f  X  can also be found by replacing p ( b, e |{ K m } P m =1 with its approximate posterior distribution q ( b, e ): p ( f  X  | g  X  , { K m } P m =1 , y ) = and the predictive distribution of the class label y  X  can be formulated using the auxiliary variable distribution: where Z  X  is the normalization coefficient calculated for the test data point and  X (  X  ) is the standardized normal cumulative distribution function. The proposed MKL method can be extended in sev-eral directions. We shortly explain two variants for multiclass learning and semi-supervised learning. 5.1. Multiclass Learning In multiclass learning, we consider classification prob-lems with more than two classes. The easiest way is to train a distinct classifier for each class that sep-arates this particular class from the remaining (i.e., one-versus-all classification). In this setup, for each classifier, we learn a different combined kernel func-tion, which corresponds to using a different similarity measure. Instead, we can have different classifiers but use the same set of kernel weights for each of them ( Rakotomamonjy et al. , 2008 ). The inference details of our model with such an approach can be found in the supplementary material. Another possibility is to use multinomial probit in our model for multiclass clas-sification as done by Damoulas &amp; Girolami ( 2008 ). 5.2. Semi-Supervised Learning In kernel-based discriminative learning framework, semi-supervised learning can be formulated as an in-teger programming problem, known as transductive SVMs ( Vapnik , 1998 ). Even though there are some approximation methods, the computational complex-ity of such models are significantly higher than that of fully supervised models. Considering this complex-ity issue, it may not be feasible to use discriminative MKL algorithms for semi-supervised learning. How-ever, our proposed model can be modified to make use of unlabeled data with a slight increase in compu-tational complexity using the low-density assumption ( Lawrence &amp; Jordan , 2005 ).
 We first test our new algorithm BEMKL on benchmark data sets to show its computational efficiency. We then illustrate its generalization performance comparing it with previously reported MKL results on one bioin-formatics and three image recognition data sets. We implement the proposed variational approximation for BEMKL in Matlab and our implementation is avail-able at http://users.ics.aalto.fi/gonen/bemkl/ . 6.1. Experiments on Benchmark Data Sets Our first set of experiments is designed to evaluate the running times of BEMKL. The experiments are performed on eight data sets from the UCI repository: breast , bupaliver , heart , ionosphere , pima , sonar , wdbc , and wpbc . For each data set, we take 20 replica-tions, where we randomly select 70 per cent of the data set as the training set and use the remaining as the test set. The training set is normalized to have zero mean and unit standard deviation, and the test set is then normalized using the mean and the standard deviation of the training set.
 We construct Gaussian kernels with 10 different widths different degrees ( { 1, 2, 3 } ) on all features and on each single feature, following the experimental set-tings of Rakotomamonjy et al. ( 2008 ), Xu et al. ( 2009 ; 2010 ), and Vishwanathan et al. ( 2010 ). All kernel ma-trices are normalized to have unit diagonal entries (i.e., spherical normalization) and precomputed before run-ning the inference algorithm. The experiments are per-formed on a PC with 3.0GHz CPU and 4GB memory. We run BEMKL for two different sparsity levels on the kernel weights: sparse and non-sparse. The hyper-parameter values for these scenarios are selected as (  X   X  ,  X   X  ,  X   X  ,  X   X  ,  X   X  ,  X   X  ) = (1 , 1 , 1 , 1 , 10  X  10 (  X  We take 200 iterations for variational inference scheme. Table 1 lists the results obtained by BEMKL with two scenarios on benchmark data sets in terms of three dif-ferent measures: the training time in seconds, the test accuracy, and the number of selected kernels. We see that our deterministic variational approximation for BEMKL takes less than a minute with large numbers of kernels, ranging from 91 to 793. The classification accuracy difference between sparse and non-sparse ker-nel combination is quite obvious for some data sets such as sonar in accordance with the previous stud-ies. Using sparsity inducing priors on kernel weights clearly simulates the  X  1 -norm by eliminating most of the input kernels, whereas using uninformative priors picks much more kernels by simulating the  X  2 -norm. 6.2. Comparison on MKL Data Sets We use four data sets that are previously used to com-pare MKL algorithms and have kernel matrices avail-able for direct evaluation. Note that the results used for comparison may not be the best results reported on these data sets but we use the exact same kernel ma-trices that produce these results for our algorithm to have comparable performance measures. These data sets have small numbers of kernels available and we do not force any sparsity on the kernel weights using an uninformative Gamma prior in accordance with the previous studies. All data sets consider multiclass clas-sification problems and we report both one-versus-all and multiclass results for BEMKL. 6.2.1. Protein Fold Recognition Data Set Protein data set considers 27 different fold types of 694 proteins (311 for training and 383 for testing) and it is available at http://mkl.ucsd.edu/dataset/ protein-fold-prediction/ . There are 12 distinct feature representations summarizing protein charac-teristics. We construct 12 kernels on these representa-tions and take 20 replications using the experimental procedure of Damoulas &amp; Girolami ( 2008 ). Table 2 gives the classification results on Protein data set. We see that BEMKL is significantly better than the proba-bilistic MKL method of Damoulas &amp; Girolami ( 2008 ). 6.2.2. Oxford Flowers17 Data Set Flowers17 data set contains flower images from 17 different types with 80 images per class and it is avail-able at http://www.robots.ox.ac.uk/ ~ vgg/data/ flowers/17/ . It also provides three predefined splits with 60 images for training and 20 images for test-ing from each class. There are seven precomputed distance matrices over different feature representa-tions. These matrices are converted into kernels as distance between training point pairs. The classifica-tion results on Flowers17 data set are shown in Ta-ble 3 . BEMKL achieves higher average test accuracy with smaller standard deviation across splits than the boosting-type MKL algorithm of Gehler &amp; Nowozin ( 2009 ). We also see that using the same set of kernel weights for each class as in our multiclass formulation Method AUC EER Accuracy
Titsias &amp; L  X azaro-Gredilla ( 2011 ) BEMKL (one-versus-all) 0.969 0.068 67.0 BEMKL (multiclass) 0.969 0.069 68.9 is better than learning separate sets of kernel weights as also observed by Gehler &amp; Nowozin ( 2009 ). 6.2.3. Oxford Flowers102 Data Set Flowers102 data set contains flower images from 102 different types with more than 40 images per class and it is available at http://www.robots.ox.ac.uk/ ~ vgg/data/flowers/102/ . There is a predefined split consisting of 2040 training and 6149 testing images. There are four precomputed distance matrices over different feature representations. These matrices are converted into kernels using the same procedure on Flowers17 data set. Table 4 shows the classification results on Flowers102 data set. We report averages of area under ROC curve (AUC) and equal error rate (EER) values calculated for each class in addition to multiclass accuracy. We see that BEMKL outperforms the GP-based method of Titsias &amp; L  X azaro-Gredilla ( 2011 ) in all metrics on this challenging task. 6.2.4. Caltech101 Data Set Caltech101 data set consists of object images from 102 different categories and it is available at http:// www.robots.ox.ac.uk/ ~ vgg/software/MKL/ . There are three predefined splits with 15 images for training and 15 images for testing from each class along with 10 precomputed kernel matrices. The classification re-sults on Caltech101 data set are given in Table 5 . BEMKL achieves higher average test accuracy with smaller standard deviation across splits than the dis-criminative MKL algorithm of Varma &amp; Babu ( 2009 ). Similar to the results on Flowers17 and Flowers102 , we see that using the same set of kernel weights for each class instead of separate sets is better in terms of generalization performance. This approach allows the classifiers trained for each class to share information with others similar to multitask learning.
 In this paper, we introduce a Bayesian multiple ker-nel learning framework in order to have computation-ally feasible algorithms by formulating the combina-tion in a novel way. This formulation allows us to develop fully conjugate probabilistic models and to de-rive very efficient deterministic variational approxima-tions for inference. We give the inference details for binary classification only due to space limitation and explain briefly how the method can be extended to multiclass learning and semi-supervised learning. An-other interesting direction is to formulate a multitask learning method by enforcing similarity between the kernel weights of different tasks.
 Experimental results on benchmark binary classifica-tion data sets show that the proposed method can combine hundreds of kernels in less than a minute. We also report very good results on one bioinformatics and three image recognition data sets, which contain mul-ticlass classification problems, compared to previously reported results.
 Acknowledgments. This work was financially sup-ported by the Academy of Finland (Finnish Centre of Excellence in Computational Inference Research COIN, grant no 251170).
 References
