 Natural Language Processing (NLP) applications benet from the availability of lar ge amounts of an-notated data. Ho we ver, such data is often scarce, particularly for non-mainstream languages. Semi-supervised learning addresses this problem by com-bining lar ge amounts of unlabeled data with a small set of labeled data in order to learn a classica-tion function. One class of semi-supervised learn-ing algorithms that has recently attracted increased interest is graph-based learning. Graph-based tech-niques represent labeled and unlabeled data points as nodes in a graph with weighted edges encoding the similarity of pairs of samples. Various tech-niques are then available for transferring class la-bels from the labeled to the unlabeled data points. These approaches have sho wn good performance in cases where the data is characterized by an underly-ing manifold structure and samples are judged to be similar by local similarity measures. Ho we ver, the question of how to best construct the graph forming the basis of the learning procedure is still an under -investigated research problem. NLP learning tasks present additional problems since the y often rely on discrete or heterogeneous feature spaces for which standard similarity measures (such as Euclidean or cosine distance) are suboptimal.

We propose a two-pass data-dri ven technique for graph construction in the frame work of label propa-gation (Zhu, 2005). First, we use a supervised clas-sier trained on the labeled subset to transform the initial feature space (consisting of e.g. lexical, con-textual, or syntactic features) into a continuous rep-resentation in the form of soft label predictions. This representation is then used as a basis for measur -ing similarity among samples that determines the structure of the graph used for the second, semi-supervised learning step. It is important to note that, rather than simply cascading the supervised and the semi-supervised learner , we optimize the combina-tion with respect to the properties required of the graph. We present several techniques for such op-timization, including regularization of the rst-pass classier , biasing by class priors, and linear combi-nation of classier predictions with kno wn features.
The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEV AL-3 word sense dis-ambiguation task. In both cases our technique sig-nicantly outperforms our baseline systems (label propagation using standard graph construction and discriminati vely trained supervised classiers). Several graph-based learning techniques have re-cently been developed and applied to NLP prob-lems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs:  X  a labeled set { ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x  X  an unlabeled set { x n +1 , . . . , x N } ;  X  a distance measure d ( i, j ) i, j  X  X  1 , . . . N } de-The goal is to infer the labels { y the unlabeled set. The algorithm represents all N data points as vertices in an undirected graph with weighted edges. Initially , only the kno wn data ver-tices are labeled. The edge linking vertices i and j has weight: where  X  is a hyperparameter that needs to be empir -ically chosen or learned separately . w label afnity of vertices: the lar ger w lik ely it is that i and j have the same label. The LP algorithm constructs a row-normalized N  X  N tran-sition probability matrix P as follo ws:
The algorithm probabilistically pushes labels from the labeled nodes to the unlabeled nodes. To do so, it denes the n  X  C hard labels matrix Y and the N  X  C soft labels matrix f , whose rst n rows are identical to
Y . The hard labels matrix Y is invariant through the algorithm and is initialized with probability 1 for the kno wn label and 0 for all other labels: where  X  is Kroneck er' s delta function. The algo-rithm iterates as follo ws: 1. f 0  X  P  X  f 2. f 0 3. If f 0  X  = f , stop 4. f  X  f 0 5. Repeat from step 1 In each iteration, step 2 x es the kno wn labels, which might otherwise be overriden by propagated labels. The resulting labels for each feature x where i  X  X  n + 1 , . . . , N } , are: It is important that the distance measure is locally accurate, i.e. nodes connected by an edge with a high weight should have the same label. The global distance is less rele vant since label information will be propagated from labeled points through the entire space. This is why LP works well with a local dis-tance measure that might be unsuitable as a global distance measure.

Applications of LP include handwriting recogni-tion (Zhu and Ghahramani, 2002), image classi-cation (Balcan et al., 2005) and retrie val (Qin et al., 2005), and protein classication (W eston et al., 2003). In NLP , label propagation has been used for word sense disambiguation (Niu et al., 2005), doc-ument classication (Zhu, 2005), sentiment analy-sis (Goldber g and Zhu, 2006), and relation extrac-tion (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP , as well as other graph-based learning techniques, is how to best con-struct the graph. Currently , graph construction  X is more of an art than science X  (Zhu, 2005). Typically , edge weights are deri ved from a simple Euclidean or cosine distance measure, regardless of the nature of the underlying features. Edges are then estab-lished either by connecting all nodes, by applying a single global threshold to the edge weights, or by connecting each node to its k nearest neighbors ac-cording to the edge weights. This procedure is often suboptimal: Euclidean distance relies on a model of normally distrib uted i.i.d. random variables; cosine distance lik ewise assumes that the dif ferent feature vector dimensions are uncorrelated. Ho we ver, man y applications, particularly in NLP , rely on feature spaces with correlated dimensions. Moreo ver, fea-tures may have dif ferent ranges and dif ferent types (e.g. continuous, binary , multi-v alued), which en-tails the need for normalization, binning, or scaling. Finally , common distance measures do not tak e ad-vantage of domain kno wledge that might be avail-able.

Some attempts have been made at impro ving the standard method of graph construction. For in-stance, in a face identication task (Balcan et al., 2005), domain kno wledge was used to identify three dif ferent edge sets based on time, color and face features, associating a dif ferent hyperparameter with each. The resulting graph was then created by super -posing edge sets. Zhu (Zhu, 2005, Ch. 7) describes graph construction using separate  X  hyperparame-ters for each feature dimension, and presents a data-dri ven way (evidence maximization) for learning the values of the parameters. Unlik e pre vious work, we propose to optimize the feature representation used for graph construction by learning it with a rst-pass supervised classi-er . Under this approach, similarity of samples is dened as similarity of the output values produced by a classier applied to the original feature repre-sentation of the samples. This idea bears similar -ity to classier cascading (Alpaydin and Kaynak, 1998), where classiers are trained around a rule-exceptions paradigm; howe ver, in our case, the clas-siers work together , the rst acting as a jointly op-timized feature mapping function for the second. 1. Train a rst-pass supervised classier that out-2. Apply postprocessing to Z 3. Use vectors Z 4. Perform label propagation over the constructed The adv antages of this procedure are:  X 
Uniform rang e and type of featur es : The out-put from a rst-pass classier can produce well-dened features, e.g. posterior probability distrib u-tions. This eliminates the problem of input features of dif ferent ranges and types (e.g. binary vs. multi-valued, continuous vs. cate gorical attrib utes) which are often used in combination.  X 
Featur e postpr ocessing : The transformation of features into a dif ferent space also opens up pos-sibilities for postprocessing (e.g. probability distri-bution warping) depending on the requirements of the second-pass learner . In addition, dif ferent dis-tance functions (e.g. those dened on probability spaces) can be used, which avoids violating assump-tions made by metrics such as Euclidean and cosine distance.  X 
Optimizing class separ ation: The learned repre-sentation of labeled training samples might reveal better clusters in the data than the original represen-tation: a discriminati vely-traine d rst pass classier will attempt to maximize the separation of samples belonging to dif ferent classes. Moreo ver, the rst-pass classier may learn a feature transformation that suppresses noise in the original input space. Dif culties with the proposed approach might arise when the rst-pass classier yields condent but wrong predictions, especially for outlier samples in the original space. For this reason, the rst-pass classier and the graph-based learner should not simply be concatenated without modication, but the rst classier should be optimized with respect to the requirements of the second. In our case, the choice of rst-pass classier and joint optimization techniques are determined by the particular learning task and are detailed belo w. 4.1 Lexicon acquisition task Our rst task is a part-of-speech (POS) lexicon ac-quisition task, i.e. the labels to be predicted are the sets of POS tags associated with each word in a lex-icon. Note that this is not a tagging task: we are not attempting to identify the correct POS of each word in running text. Rather , for each word in the vocab-ulary , we attempt to infer the set of possible POS tags. Our choice of this task is moti vated by our long-term goal of applying this technique to lexicon acquisition for resource-poor languages: POS lexi-cons are one of the most basic language resources, which enable subsequent training of taggers, chun-kers, etc. We assume that a small set of words can be reliably annotated, and that POS-sets for the remain-ing words can be inferred by semi-supervised learn-ing. Rather than choosing a genuinely resource-poor language for this task, we use the English Wall Street Journal (WSJ) corpus and articially limit the size of the labeled set. This is because the WSJ corpus is widely obtainable and allo ws easy replication of our experiments.

We use sections 0-18 of the Wall Street Journal corpus ( N = 44 , 492 ). Words have between 1 and 4 POS tags, with an average of 1.1 per word. The number of POS tags is 36, and we treat every POS combination as a unique class, resulting in C = 158 distinct labels. We use three dif ferent randomly se-lected training sets of various sizes: 5000, 10000, and 15000 words, representing about 11%, 22%, and 34% of the entire data set respecti vely; the rest of the data was used for testing. In order to avoid experi-mental bias, we run all experiments on ve dif fer -ent randomly chosen labeled subsets and report av-erages and standard deviations. Due to the random sampling of the data it is possible that some labels never occur in the training set or only occur once. We train our classiers only on those labels that oc-cur at least twice, which results in 60-63 classes. La-bels not present in the training set will therefore not be hypothesized and are guaranteed to be errors. We delete samples with unkno wn labels from our unla-beled set since their percentage is less than 0.5% on average.

We use the follo wing features to represent sam-ples:  X  Inte ger: the three-letter suf x of the word;  X  Inte ger: The four -letter suf x of the word;  X  Inte ger  X  4: The indices of the four most fre- X  Boolean: word contains capital letters;  X  Boolean: word consists only of capital letters;  X  Boolean: word contains digits;  X  Boolean: word contains a hyphen;  X  Boolean: word contains other special charac-We have also experimented with shorter suf x es and with prex es but those features tended to degrade performance. 4.2 SENSEV AL-3 word sense disambiguation The second task is word sense disambiguation using the SENSEV AL-3 corpus (Mihalcea et al., 2004), to enable a comparison of our method with pre viously published results. The goal is to disambiguate the dif ferent senses of each of 57 words given the sen-tences within which the y occur . There are 7860 sam-ples for training and 3944 for testing. In line with existing work (Lee and Ng, 2002; Niu et al., 2005), we use the follo wing features:  X  Inte ger  X  7: seven features consisting of the  X  Inte ger  X  X  variabl e leng th  X  : a bag of all words  X  Inte ger  X  15: Local collocations C ij ( i , j are Note that syntactic features, which have been used in some pre vious studies on this dataset (Mohammad and Pedersen, 2004), were not included. We apply a simple feature selection method: a feature X is se-lected if the conditional entrop y H ( Y | X ) is abo ve a x ed threshold (1 bit) in the training set, and if X also occurs in the test set (note that no label infor -mation from the test data is used for this purpose). For both tasks we compare the performance of a su-pervised classier , label propagation using the stan-dard input features and either Euclidean or cosine distance, and LP using the output from a rst-pass supervised classier . 5.1 Lexicon acquisition task 5.1.1 First-pass classier
For this task, the rst-pass classier is a multi-layer perceptron (MLP) with the topology sho wn in Fig. 1. The input features are mapped to con-tinuous values by a discrete-to-continu ous mapping layer M , which is itself learned during the MLP training process. This layer connects to the hidden layer h , which in turn is connected to the output layer o . The entire netw ork is trained via backprop-agation. The training criterion maximizes the regu-larized log-lik elihood of the training data: The use of an additional continuous mapping layer is similar to the use of hidden continuous word rep-resentations in neural language modeling (Bengio et al., 2000) and yields better results than a standard 3-layer MLP topology .

Problems caused by data scarcity arise when some of the input features of the unlabeled words have never been seen in the training set, resulting in un-trained, randomly-initialized values for those fea-ture vector components. We address this problem by creating an approximation layer A that nds the kno wn input feature vector x 0 that is most similar to x (by measuring the cosine similarity between the vectors). Then x in vector  X  x =  X  x 1 , . . . , x has no unseen features and is closest to the original vector . 5.1.2 LP Setup
We use a dense graph approach. The WSJ set has a total of 44,492 words, therefore the P ma-trix that the algorithm requires would have 44 , 492  X  44 , 492  X  = 2  X  10 9 elements. Due to the matrix size, we avoid the analytical solution of the LP problem, which requires inverting the P matrix, and choose the iterati ve approach described abo ve (Sec. 2.1) in-stead. Con vergence is stopped when the maximum relati ve dif ference between each cell of f and the corresponding cell of f 0 is less than 1%.
 Also for data size reasons, we apply LP in chunks. While the training set stays in memory , the test data is loaded in x ed-size chunks, labeled, and dis-carded. This approach has yielded similar results for various chunk sizes, suggesting that chunking is a good approximation of whole-set label propaga-tion. 1 LP in chunks is also amenable to paralleliza-tion: Our system labels dif ferent chunks in parallel.
We trained the  X  hyperparameter by three-fold cross-v alidation on the training data, using a geo-metric progression with limits 0 . 1 and 10 and ratio 2 . We set x ed upper limits of edges between an unlabeled node and its labeled neighbors to 15 , and between an unlabeled node and its unlabeled neigh-bors to 5 . The approach of setting dif ferent limits among dif ferent kinds of nodes is also used in re-lated work (Goldber g and Zhu, 2006).

For graph construction we tested: (a) the original discrete input representation with cosine distance; (b) the classier output features (probability distri-butions) with the Jef fries-Matusita distance. 5.2 Combination optimization The static parameters of the MLP (learning rate, reg-ularization rate, and number of hidden units) were optimized for the LP step by 5-fold cross-v alidation on the training data. This process is important be-cause overspecialization is detrimental to the com-bined system: an overspecialized rst-pass classi-er may output very condent but wrong predic-tions for unseen patterns, thus placing such samples at lar ge distances from all correctly labeled sam-ples. A strongly regularized neural netw ork, by con-trast, will output smoother probability distrib utions for unseen patterns. Such outputs also result in a smoother graph, which in turn helps the LP process. Thus, we found that a netw ork with only 12 hidden units and relati vely high R (  X  ) in Eq. 5 (10% of the weight value) performed best in combination with LP (at an insignicant cost in accurac y when used as an isolated classier). 5.2.1 Results
We rst conducted an experiment to measure the smoothness of the underlying graph, S ( G ) , in the two LP experiments according to the follo wing for -mula: where y better as the y reect less afnity between nodes of dif ferent labels.) The value of S ( G ) was in all cases signicantly better on graphs constructed with our proposed technique than on graphs constructed in the standard way (see Table 1). Table 1 also sho ws the performance comparison between LP over the discrete representation and cosine distance ( X LP X ), the neural netw ork itself ( X NN X ), and LP over the continuous representation ( X NN+LP X ), on all dif-ferent subsets and for dif ferent training sizes. For scarce labeled data (5000 samples) the neural net-work, which uses a strictly supervised training pro-cedure, is at a clear disadv antage. Ho we ver, for a lar ger training set the netw ork is able to perform more accurately than the LP learner that uses the discrete features directly . The third, combined tech-nique outperforms the rst two signicantly . 2 The dif ferences are more pronounced for smaller train-ing set sizes. Interestingly , the LP is able to extract information from lar gely erroneous (noisy) distrib u-tions learned by the neural netw ork. 5.3 Word Sense Disambiguation We compare the performance of an SVM classier , an LP learner using the same input features as the SVM, and an LP learner using the SVM outputs as input features. To analyze the inuence of train-ing set size on accurac y, we randomly sample sub-sets of the training data (25%, 50%, and 75%) and use the remaining training data plus the test data as unlabeled data, similarly to the procedure fol-lowed in related work (Niu et al., 2005). The re-sults are averaged over ve dif ferent random sam-plings. The samplings were chosen such that there was at least one sample for each label in the training set. SENSEV AL-3 sports multi-labeled samples and samples with the  X unkno wn X  label. We eliminate all samples labeled as unkno wn and retain only the rst label for the multi-labeled instances. 5.3.1 SVM setup
The use of SVM vs. MLP in this case was justi-ed by the very small training data set. An MLP has man y parameters and needs a considerable amount of data for effecti ve training, so for this task with only on the order of 10 2 training samples per classi-er , an SVM was deemed more appropriate. We use the SVM light package to build a set of binary clas-siers in a one-v ersus-all formulation of the multi-class classication problem. The features input to each SVM consist of the discrete features described abo ve (Sec. 4.2) after feature selection. After train-ing SVMs for each tar get label against the union of all others, we evaluate the SVM approach against the test set by using the winner -tak es-all strate gy: the predicted label corresponds to the SVM that outputs the lar gest value. 5.3.2 LP setup
Again we set up two LP systems: one using the original feature space (after feature selection, which beneted all of the tested systems) and one using the SVM outputs. Both use a cosine distance measure. The  X  parameter (see Eq. 1) is optimized through 3-fold cross-v alidation on the training set. 5.4 Combination optimization Unlik e MLPs, SVMs do not compute a smooth out-put distrib ution but base the classication decision on the sign of the output values. In order to smooth output values with a vie w towards graph construc-tion we applied the follo wing techniques: 1. Combining SVM predictions and perfect fea-2. Biasing uninformative distrib utions: For some 3. Weighting by class prior s: For each training 5.4.1 Results
As before, we measured the smoothness of the graphs in the two label propagation setups and found that in all cases the smoothness of the graph pro-duced with our method was better when compared to the graphs produced using the standard approach, as sho wn in Table 3, which also sho ws accurac y re-sults for the SVM ( X SVM X  label), LP over the stan-dard graph ( X LP X ), and label propagation over SVM outputs ( X SVM+LP X ). The latter system consistently performs best in all cases, although the most mark ed gains occur in the upper range of labeled samples percentage. The gain of the best data-dri ven LP over the kno wledge-based LP is signicant in the 100% and 75% cases.
For comparison purposes, Table 2 sho ws results of other published systems against the SENSEV AL corpus. The  X htsa3 X ,  X IRST -kernels X , and  X nusels X  systems were the winners of the SENSEV AL-3 con-test and used extra input features (syntactic rela-tions). The Niu et al. work (Niu et al., 2005) is most comparable to ours. We attrib ute the slightly higher performance of our SVM due to our feature selection process. The LP/cosine system is a system similar to our LP system using the discrete features, and the LP/Jensen-Shannon system is also similar but uses a distance measure deri ved from Jensen-Shannon divergence. We have presented a data-dri ven graph construction technique for label propagation that utilizes a rst-pass supervised classier . The outputs from this classier (especially when optimized for the second-pass learner) were sho wn to serv e as a better repre-sentation for graph-based semi-supervised learning. Classication results on two learning tasks sho wed signicantly better performance compared to LP us-ing standard graph construction and the supervised classier alone.
 Ackno wledgments This work was funded by NSF under grant no. IIS-0326276. An y opinions, ndings and conclusions, or recommendations ex-pressed herein are those of the authors and do not necessarily reect the vie ws of this agenc y.
