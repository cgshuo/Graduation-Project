 Sentiment analysis of unstructured tex t has recently received a lot of attention in the information retrieval community. One of the most popular tasks in document-level sentiment analysis is to determine whether the sentiment expressed in a document is positive or negative. Nowadays, sentiment analysis often maintains a low accuracy. One reason is that the rapid growth of the internet makes it easy to collect data on a large scale, often beyond the capacity of individual disks, and too large for processing. Traditional centralized algorithms are often too costly or impractical for large scale data. On the other hand, most algorithms assume that the features used for sentiment analysis are pairwise independent. The goal of this paper is to present a parallel BN learning technique for learning predominant sentiments of on-line texts, available in unstructured format, that: 1. is able to capture dependencies among words and find a minimal vocabulary 2. is able to handle the large scale unstructured web data.
 In this paper, we describe our experiences with developing and deploying a BN learning algorithm over large datasets using the MapReduce model of distributed computation. Then, we apply this parallel BN learning algorithm to capture the dependencies among words, and, at the same time, finds a vocabulary that is efficient for the purpose of extracting se ntiments. Here, we learn the BN struc-ture from massive real-world online blog data. We show how the parallel BN learning algorithm can be scaled effectively to large datasets. Moreover, BN can allow us to capture semantic relations and dependent patterns among the words, thus approximating the meaning of sentences. Further, performing the sentiment classification task using BN for the sentiment variables has excellent performance.

The rest of the paper is organized as follows. In section 2 we review the related work about sentiment analysis, MapReduce used in machine learning and BN structure learning algorithm TPDA (Three-Phase Dependency Analysis). In section 3 we provide the framework of the distributed BN learning algorithm over large datasets. We demonstrate the performance of our algorithm with experiments on online movie comments i n section 4 and conclude in section 5. 2.1 Sentiment Analysis The problem of sentiment e xtraction is also referre d to as opinion extraction or semantic classification in the literature. Eguchi and Lavrenko[7] proposed a generative model that jointly models sentiment words, topic words and senti-ment polarity in a sentence as a triple. McDonald et al.[10] investigated a global structured model that learns to predict se ntiment of different levels of granularity in text. Turney and Littman[16] applied an unsupervised learning algorithm to classify the semantic orientation in the word/phrase level, based on mutual infor-mation between document phrases and a small set of positive/negative paradigm words like good and bad. Blitzer et al.[1] focused on domain adaption for senti-ment classifiers with respect to different types of products X  online reviews. Choi et al.[3] dealt with opinion analysis by combining conditional random fields (CRFs). In the sentence level, a semi-supervised machine learning algorithm was proposed by Pang and Lee[15], which employs a subjectivity detector and minimum cuts in graphs. Another system by Kim and Hovy[8] judges the sentiment of a given sentence by combining the individual word-level sentiment. 2.2 MapReduce Used in Machine Learning MapReduce is a simple model for distributed computing that abstracts away many of the difficulties in parallelizing data management operations across a cluster of commodity machines. A MapR educe[6] operation takes place in two main stages. In the first stage, the map function is called once for each input record. At each call, it may produce any number of output records. In the second stage, this intermediate output is sorted and grouped by key, and the reduce function is called once for each key. The reduce function is given all associated values for the key and outputs a new list of values (see Fig. 1 ).

With the wide and growing availability of MapReduce-capable compute infra-structures[6], it is natural to ask whet her such infrastructures may be of use in parallelizing common data mining tasks such as BN structure learning. For many data mining operations, MapReduce may offer better scalability with vastly sim-plified deployment in a production setting. Panda et al.[13] described PLANET: a scalable distributed framework for learning tree models over large datasets using the MapReduce model of distributed computation. MRPSO[11] utilizes the Hadoop implementation of MapReduce to parallelize a compute-intensive application, Particle Swarm Optimization. Cohen[5] investigated the feasibility of decomposing useful graph operations into a series of MapReduce processes. Companies such as Facebook and Rackspace[9] use MapReduce to examine log files on a daily basis and generate statistics and on-demand analysis. Chu et al.[4] give an excellent overview of how different popular learning algorithms (e.g., K-means, neural networks, PCA, EM, SVM)can be effectively solved in the MapReduce framework. 2.3 Bayesian Networks A BN can be formulized as a pair ( G, P ), where G =( V, E ) is a directed acyclic graph (DAG). Here, V is the node set which represents variables in the problem domain V = { x 1 ,x 2 , ..., x n } and E is the arc (directed edge) set which denotes each arc &lt;x i ,x j &gt; means that x i implicates x j .Foranode x  X  V ,aparent of x is a node from which there exists an arc to x . Pa ( x i ) is the parent set of x i . The parameter set P describes probability distributions associated with Pa ( x i ) condition.

Three-Phase Dependency Analysis (TPDA)[2] is one of the widely used BN structure learning algorithm. The BN st ructure is learned by identifying the conditional independence relationships among the nodes of G .Itusessome CI test (mutual information) to find the conditional independence relationships among the nodes of G and use these relationships as constraints to construct aBN.

In [2], a BN is viewed as a network system of information channels, where each node is either active or inactive and the nodes are connected by noisy information channels. The information flow can pass through an active node but not an inactive one. When all the nodes on one undirected path between two nodes are active, we say this path is open. If any one node in the path is inactive, we say the path is closed. When all path s between two nodes are closed given the status of a set of nodes, we say the two nodes are d-separated by the set of nodes. The status of nodes can be changed through the instantiation of a set of nodes. The amount of information flow b etween two nodes can be measured by using mutual information, when no nodes are instantiated, or conditional mutual information, when some other nodes are instantiated.

In general, the volume of information flow between two variables A and B is measured by mutual information: And the conditional mutual information, with A and B respect to the condition-set C ,isformulatedas where A , B are two variables and C is a set of variables. Here, we use conditional mutual information as CI tests to measure the average information between two nodes. When I ( A, B | C ) is smaller than a certain threshold value ,wesaythat A , B are d  X  separated by the condition-set C , and they are conditionally inde-pendent. A is independent of B whenever I ( A, B ) &lt; , for some suitably small threshold &gt; 0. We will similarly declare conditional independence whenever I ( A, B | C ) &lt; .

The three phases of the TPDA algorithm are drafting, thickening and thin-ning. In  X  X rafting X  phase, mutual information of each pair of nodes is computed to produce an initial set of edges based on a simpler test C basically just having sufficient pair-wise mutual information. The draft is a singly-connected graph (a graph without loops). The second  X  X hickening X  phase adds edges to the current graph when the pairs of nodes cannot b e separated using a set of relevant CI tests. The graph produced by this phase will contain all the edges of the under-lying dependency model when the underlying model is DAG-faithful. The third  X  X hinning X  phase examines each edge and it will be removed if the two nodes of the edge are found to be conditionally independent. In this section, we discuss the technical details of the major components of BN learning algorithm with MapReduce -the controller that manages the entire BN structure learning process, and the two critical MapReduce that compute the mutual information and conditional mutual information.

Given a set of attributes V and the training dataset D . Here, we assume that the node and arc sets of the BN is small enough to fit in the memory. Thus, the controller can maintain a vector V and a list E stored in a centralized file -BNFile. The controller constructs a BN structure using a set of MapReduce jobs, which compute the mutual information of two nodes and the conditional mutual information of two nodes with respect to their minimum cut set. At any point, the BNFile contains the entire BN structure constructed so far. 3.1 Map Phase In dataset D , the training data is organized as Table 1. Every line stores all attributes and their values in the form of value-attribute pairs. For example, here v1, v2, v3 and v4 represent four attributes. They take the values High, normal, false, low respectively.

The training dataset D is partitioned across a set of mappers. Pseudocodes describing the algorithms that are executed by each mapper appear in Algo-rithm 1 and 2. Algorithm 1 prepare the statistic data for calculating mutual information. Given a line of training dataset, a mapper first emits every (value-attribute, one) as (key, value) output (line 2, Alg. 1). And for every attribute pairs, the mapper emits corresponding (value-attribute pair, one) as (key, value) output (line 5, Alg. 1). For example, a mapper loads the first line of Table 1. It emits following (key, value) lists (see Fig. 2):
Algorithm 2 prepare the statistic data for calculating conditional mutual in-formation. Each mapper requires training dataset, two attributes and their con-dition set as input. Given a line of training dataset, a mapper first extracts the corresponding values of the two input attributes and condition set (Line 1, 2, 3, Alg.2). Then the mapper emits (value-attribute and condition-set pair, one) as (key, value) output (line 4, 5, 6, Alg.2). For example, a mapper loads the first line of Table 1, two attribute v1, v2 and condition set (v3, v4) as input. It emits following (key, value) lists (see Fig. 3): 3.2 Reduce Phase The reduce phase, which works on the outputs from the mappers, performs aggregations and computes the sum of the statistic data for calculating mutual information and conditional mutual information.

The pseudocode executed on each redu cer is outlined in Algorithm 3. The shuffling model groups values by keys. Each reducer receives (key, a list of all associated values) pairs as inputs. There are three types of keys: value-attribute, value-attribute pair, value-attribute and condition-set pair. The value list con-tains a list of  X  X ne X , such as:
Reducers process keys in sorted order, aggregate the value lists and emit the sums as outputs (see Fig. 4). The following outputs are illustrated as examples:
The controller takes the aggregated values produced by all the reducers and calculate the mutual information for attribute pair and the conditional mutual information for attributes-condition-set pair.
 3.3 Controller Design At the heart of the distributed BN learning algorithm is the  X  X ontroller X , a single machine that initiates, schedules and controls the entire structure learning process. The controller has access to a c omputer cluster on which it schedules MapReduce jobs.

The main controller thread (Algorithm 4)schedules the TPDA algorithm[2] until the final BN structure is constructed. While the overall architecture of the controller is fairly straightforward, we would like to highlight a few important design decisions. MR MI job returns all statistic information for calculating the mutual information values of all pairs &lt;X,Y &gt; in V . When the MR MI job returns, the controller calculate the mutual information with equation (1) using function Calculate MI(X,Y). MR CMI job returns all statistic information for calculating the conditional mutual information values of &lt;X,Y &gt;,D ( &lt;X,Y &gt; ) pair. When the MR CMI job returns, the controller calculate the conditional mutual information with equation (2) using function Calculate CMI(X,Y). 4.1 Unstructured Web Data WetestedourmethodonthedatasetusedinPangetal[14].Thisdataset[12] contains approximately 29,000 posts to the rec.arts.movies.reviews newsgroup archived at the Internet Movie Database (IMDb). 10,000 positive posts and 10,000 negative posts are selected. The original posts are available in the form of HTML pages. Some pre-processing was performed to produce the version of the data we used.

In our study, we used words as features. Intuitively the task of sentiment extraction is a hybrid task between authorship attribution and topic categoriza-tion; we look for frequent words, possibly not related to the context, that help express lexical patterns, as well as low frequency words which may be specific to few review styles, but very indicative of an opinion. We considered all the words that appeared in more than 8 documents as our input features. A total num-ber of 7,321 words as input features and class feature Y = { positive, negative } were selected. In our experiments, each document is represented by a vector, W = { w 1 , ..., w 7321 ,Y } ,whereeach w i is a binary random variable that takes the value of 1 if the i th word in the vocabulary is present in the document and the value of 0 otherwise. 4.2 Results Analysis of Parallel BN Learning Algorithm All of our experiments were performed on a MapReduce equipped cluster with 20 machines, where each machine was configured to use 512MB of RAM and 1GB of hard drive space. Running time was measured as the total time between the start of BN learning algorithm and it exiting with the learned structure as output. All the running times have been averaged over multiple runs. Fig. 5 shows the running time with centralized and distributed algorithms. When the dataset is small, the centralized algorithm needs less running time than distributed algorithm with MapReduce. It is because that MapReduce start up and tear down costs are primary performance bottlenecks. Therefore, MapRe-duce does not suit the computation on small dataset. When the dataset grows, the excellent performance of MapReduce becomes explicit. 4.3 Results Analysis of Sentiment Classification We compare the performance of our parallel BN classifier with na  X  i ve Bayes(NB) classifier and support vector machine (SVM) classifier along with a TF-IDF re-weighting of the vectors of word counts. Two BN classifiers are used. One is the original BN(OBN) classifier using 7321 features and Y. The other is the thin BN(TBN) classifier using the features selected from the OBN, including the set of parents of Y , the set of children of Y , the set of parents of parents of Y ,the set of children of children of Y and the set of parents of children of Y . Here, we choose 413 features for TBN.

In order to compute unbiased estimat es for accuracy and recall we used a ten-fold cross-validation scheme. Ta ble 2 compares the OBN and TBN with the performances of the other classifiers using the whole feature set as input. As we expected, more features did not nece ssarily lead to better results, as the classifiers were not able to distinguish discriminating words from noise.
Table 3 compares the performance of the OBN and TBN with others classifiers using the same number of features selected by information gain(IG) and princi-pal component analysis(PCA). We notice that feature selection using information gain and PCA criterion does not tell us how many features have to be selected, but rather allows us to rank the features from most to least discriminating instead. In this paper, we present a parallel BN learning technique for learning predom-inant sentiments of on-line texts, available in unstructured format, that: 1. is able to capture dependencies among words and find a minimal vocabulary 2. is able to handle the large scale unstructured web data.
 We describe our experiences with developing and deploying a BN learning al-gorithm over large datasets using the MapReduce model of distributed com-putation. Then, we apply this parallel BN learning algorithm to capture the dependencies among words, and, at the same time, finds a vocabulary that is efficient for the purpose of extracting se ntiments. Experiment results suggest that our parallel Bayesian network learning algorithm is capable of handling large real-world data sets. Moreover, for problems where the independence as-sumption is not appropriate, the BN is a better choice and leads to more robust predictions.
 This work was supported by the National Science &amp; Technology Pillar Program, No. 2009BAK63B08 and China Postdoctoral Science Foundation funded project. It is our great pleasure to welcome you to the 2nd International Workshop on XML Data Management (XMLDM 2011).

XML has gained lot of attention from database and Web researchers who are actively working in one or more of the emerging XML areas. XML data are self-describing, and provide a platform independent means to describe data and therefore, can transport data from one platform to another. XML docu-ments can be mapped to one more of the existing data models such as relational and object-relational, and XML views c an be produced dynamically from the pre-existing data models. XML queries can be mapped to the queries of the un-derlying models and can use their optimization features. XML data integration is useful for E-commerce applications such as comparison-shopping, which requires further study in the domain of data, schema and query based integration. XML change management is another important area that has attracted attention in the context of web warehouse. XML has been in use in upcoming areas such web services, sensor and biological data management. The second International Workshop on XML Data Management focuses on the convergence of database technology with XML technology, and brings together academics, practitioners, users and vendors to discuss the use and synergy between these technologies. XMLDM attracted 8 submissions from Asia, Europe, and North America. The program committee accepted 4 full pa pers. These papers cover a variety of topics, including XML views and data mappings, XML query languages and optimization, XML applications in semantic web and so on. We hope that they will serve as a valuable start point for much brilliant thinking in XML data management.

Paper  X  X SD-DB: A Military Logistics Mobile Database X  discusses how to transform and display XML data as HTML tables with multilevel headers, to preserve the original easy-to-read format while having a well defined schema, and describe how it can be used in concrete applications, focusing on a military logistics database. In addition, Rinfret et al. also show how to integrate the XML layer into a more complete application.

Paper  X  X uerying and Reasoning with RDF(S)/OWL in Xquery X  investigates how to use the XQuery language for querying and reasoning with RDF(S)/OWL-style ontologies. XQuery library for the Semantic Web and XQuery functions have been proposed by Almendros-Jimenez to handle RDF(S)/OWL triples and encode RDF(S)/OWL.

In paper  X  X  Survey on XML Keyword Search X , Tian et al. survey several representative papers with the purpose of extending the general knowledge of the research orientation of the XML keyword search.

In paper  X  X chema mapping with quality assurance for data integration1 X , the concept of quality assurance mechanisms was proposed by Bian et al. They discuss that a new model with qualityassurance, and provide a suitable method for this model, and then they propose the strategy of weak branch X  X  convergence on the basis of Schema.

In this workshop, we are very glad to invite Dr. Bogdan Cautis to give a keynote talk. He reports in this talk on r ecent results on richer classes of XPath rewritings using views. He presents both theoretical and practical results on view-based rewriting using multiple XML views.

Making XMLDM 2011 possible has been a team effort. First of all, we would like to thank the authors and panelists for providing the content of the program. We would like to express our gratitude to the program committee and external reviewers, who worked very hard in reviewing papers and providing suggestions for their improvements. We would thank Steering Chairs Professor Xiaoyong Du and Professor Jianmin Wang for their helps and important instructions. In particular we extend our special thanks to Linlin Zhang for maintaining the XMLDM2011 web site and for his effort in organizing the workshop.

We hope that you will find this program interesting and thought-provoking and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world. We report in this talk on recent results on richer classes of XPath rewritings using views.

The problem of equivalently rewriting queries using views is fundamental to several classical data management tasks. Examples include query optimization using a cache of materialized results of previous queries and database security, where a query is answered only if it has a rewriting using the pre-defined security views.

While the rewriting problem has been well studied for the relational data model, its XML counterpart is not yet equally well understood, even for basic XML query languages such as XPath, due to the novel challenges raised by the features of the XML data model.

We have recently witnessed an industrial trend towards enhancing XPath queries with the ability to expose node identifiers and exploit them using inter-section of node sets (via identity-based e quality). This development enables for the first time multiple-view rewritings ob tained by intersecting several material-ized view results. We present both theoret ical and practical results on view-based rewriting using multiple views. First, we characterize the complexity of the in-tersection-aware rewriting problem. We then identify tight restrictions (which remain practically interesting) under which sound and complete rewriting can be performed efficiently, i.e. in polynomial time, and beyond which the problem becomes intractable. As an additional contribution, we analyze the complexity of the related problem of deciding if an XPat h with intersection can be equivalently rewritten as one without intersection or union.

Then, going beyond the classic setting of answering queries using explicitly enumerated view definitions, we report on results on the problem of querying XML data sources that accept only a limit ed set of queries. This is motivated by Web data sources which, for reasons such as performance requirements, business model considerations and a ccess restrictions, do not allo w clients to ask arbitrary queries. They instead publish as Web Serv ices a set of queries (views) they are willing to answer.

Querying such sources involves finding one or several legal views that can be used to answer the client query. Services can implement very large (potentially infinite) families of XPath queries, and in order to compactly specify such families of queries we adopt a formalism close to context-free grammars (in the same spirit in which a potentially infinite language is finitely specified by a grammar).
We say that query Q is expressed by a specification (program) P if it is equivalent to some expansion of it. Q is supported by P if it has an equivalent rewriting using some finite set of P  X  X  expansions. We present our results on the complexity of expressibility and support and identify large classes of XPath queries for which there are efficient (tractable) algorithms.
 This survey mainly summarizes two recent papers, which are joint work Alin
