 The goal of sequential pattern mining is to detect patterns in a database com-prised of sequences of itemsets . For example, retail stor es often collect customer purchase records in sequence databases in which a sequential pattern indicates a customer X  X  buying habit. In such a database, each purchase is represented as a set of items, itemsets , purchased together, and a customer sequence would be a sequence of such itemsets.

Sequential pattern mining is commonly defined as finding the complete set of frequent subsequences [1]. Much research has been devoted to efficient discovery of such frequent sequential patterns [1] [7] [8]. However, such problem formu-lation of sequential patterns has some inherent limitations. First, the result set is huge and difficult to use without more post processing. Even the number of maximal or closed sequential patterns are huge, and many of the patterns are redundant and not useful. Second, the exact match based paradigm is vulnerable to noise and variations in the data. Many customers may share similar buying habits, but few follow exactly the same buying patterns. Finally, frequency alone cannot detect statistically significant patterns [4].

To overcome these limitatio ns, recently there is an increasing interest in new in-telligent mining methods to find more meaningful and compact results. The new methods abandon the traditional paradigm and take a fundamentally different approach. One such approach to intelligent sequential pattern mining is the con-sensus sequential pattern mining based on sequence alignment. Consensus sequen-tial patterns can detect general trends in a group of similar sequences, and may be more useful in finding non-trivial and interesting long patterns. It can be used to detect general trends in the sequence database for natural customer groups, which is more useful than finding all frequent subsequences.

Formally, consensus sequential patterns are patterns shared by many sequences in the database but not necessarily exactly contained in any one of them. Table 1 shows a group of sequences and a pattern that is approximately similar to them. In each sequence, the bold items are those that are shared with the consensus pat-tern. seq 1 has all items in consensus pat , in the same order and grouping, except it is missing item A and has an additional item E . Similarly, seq 4 is missing C and has an extra E . In comparison, seq 2 and seq 3 have all items but each has a couple of extra items. These eviden ces strongly indicate that consensus pat is the hidden underlying pattern behind the sequences. Such pattern mining of consensus pat-terns can effectively summarize the datab ase into common customer groups and identify their buying patterns.

An effective algorithm for consensus sequential pattern mining has been pro-posed in [5]. The alignment based method, ApproxMAP (APPROX imate M ultiple A lignment P attern mining), has been applied to many areas such as multi-database mining [3], temporal streaming data mining [6], and policy analysis. Moreover, a detailed comparison study of the alignment based and support base methods has shown the effectiveness of ApproxMAP [4].

However, ApproxMAP has quadratic time complexity with respect to the size of the database limiting its application to very large databases. The time com-plexity is dominated by the clustering step which has to calculate the proximity matrix and build the clusters. In this paper, we introduce two effective optimiza-tion techniques. First, ApproxMAP can be optimized by calculating the proximity matrix to only the needed precision. In this paper, we introduce and prove the theoretical bound of the required precision reducing the running time consider-ably. Second, the clustering step can be improved by adapting the well known k-means method to ApproxMAP . Here, we introduce modifi cations to the typical algorithm that address the issues with calculating the mean, cluster initializa-tion, and determining the number of clusters required. We further investigate the tradeoff between time and space empirically to determine the appropriate sample size and the utility of the optimization technique.

The remainder of the paper is organized as follows. Section 2 illustrates the basic ApproxMAP algorithm. The details and theoretical basis for the optimiza-tion are given in Section 3. Finally, Sectio n 4 presents the exper imental results. We presented sequential pattern mining based on sequence alignment in [5]. Ex-tending research on string analysis, we generalized string multiple alignment to find consensus sequential patterns in ordered lists of sets. The power of multi-ple alignment hinges on the following insight: the probability that any two long data sequences are the same purely by chance is very low. Thus, if several long sequences can be aligned with respect to pa rticular frequent items, we will have implicitly found sequential patterns that are statistically significant.
ApproxMAP has three steps. First, k nearest neighbor clustering is used to partition the database. Second, for each partition, the optimal multiple align-ment is approximated by the following greedy approach: in each partition, two sequences are aligned first, and then a se quence is added incrementally to the current alignment until all sequences have been aligned. At each step, the goal is to find the best alignment of the added sequence, p , to the existing alignment of p  X  1 sequences. A novel structure, weighted sequence , is used to summarize the alignment information in each cluster. In short, a weighted sequence is a sequence of itemsets with a weight asso ciated with each item. The item weight represents the strength of the item where strength is defined as the percentage of sequences in the alignment that have the item present in the aligned position. Third, a consensus pattern is generated for each partition.
Tables2to4isanexample. GivenTable2, ApproxMAP (1) calculates the proximity matrix and partitions the data into two clusters ( k = 2), (2) aligns the sequences in each cluster  X  the alignment c ompresses all the sequences into one weighted sequence per cluster, and (3) s ummarizes the weighted sequences into consensus patterns using the cutoff point min strength . Note that the consensus patterns (A)(BC)(DE) and (IJ)(K)(LM) do not match any sequence exactly. ApproxMAP has total time complexity of O ( N 2 seq  X  L 2 seq  X  I seq + k  X  N seq )where N seq is the total number of sequences, L seq is the length of the longest sequence, I seq is the maximum number of items in an itemset, and k is the number of nearest neighbors considered. The time complexity is dominated by the clustering step which requires the computation of the proximity matrix ( O ( N 2 seq  X  L 2 seq  X  I seq )) and builds clusters ( O ( k  X  N seq )). The quadratic run time with respect to the database size may limit its applications to very large databases. There are two components constituting the running time for calculating the proximity matrix: (1) the per for the proximity matrix. We discuss how to optimize both in this section. 3.1 k -Nearest Neighbor ( k -NN) Clustering ApproxMAP uses uniform kernel density based k -nearest neighbor ( k -NN) clus-tering. We have found that such a density based method worked well due to its ability to build clusters of arbitrary size and shape around similar sequences. In this agglomerative method, each point links to its closest neighbor, but (1) only with neighbors that have greater density, and (2) only up to k nearest neighbors. Thus, the algorithm essentially builds a forest of single linkage trees (each tree representing a natural cluster), with the proximity matrix defined as follows, 1 . D ( seq i ,seq j ) is the commonly used hierarchical edit distance defined via a recurrence relation. dist k ( seq i )isthe k -NN region defined as the maximum dis-tance over all k -NN and Density ( seq i ,k )= n k ( seq i ) dist number of sequences in the k -NN region. An effective implementation has three steps : (1) build the proximity matrix, (2) build the k -NN list using the matrix, and (3) merge the k -NN sequences when applicable. The details are in [3]. 3.2 Optimizing the Proximity Matrix Calculation Each cell in the proximity matrix is calculated using Equation 1. Thus, the time complexity is O ( L 2 seq  X  I seq ) for solving the recurrence relation for D ( seq i ,seq j ) through dynamic programming such as shown in Table 5. Often a straight for-ward dynamic programming algorithm can be improved by only calculating up to the needed precision. Here we discuss how to reduce the per cell calculation time by stopping the calculation of su ch a table midway whenever possible.
In Table 5, we show the intermediate calculation along with the final cell value. Each cell RR ( p, q ) has four values in a 2x2 matrix. Let us assume we are converting seq 7 to seq 8 . Then, the top left value shows the result of moving diagonally by replacing itemset p with itemset q . The top right value is the result of moving down by inserting q . The bottom left cell is the result of moving right by deleting p . The final value in the cell, shown in the bottom right position, is the minimum of the three. The arrows indicate the direction. The minimum
For example, when calculating the value for RR (3 , 2) = 1 5 6 ,youcaneither replace (KQ) with (K) (upper left: RR (2 , 1)+ REP L (( KQ ) , ( K )) = 1 1 2 + 1 3 =1 5 6 ), insert (KQ) (upper right: RR (3 , 1) + INDEL =2 1 2 +1 = 3 1 2 ), or delete (K) (lower left: RR (2 , 2) + INDEL =1 1 2 +1=2 1 2 ). Since 1 5 6 is the minimum, the replace operation is chosen (diagonal). The final distance 2 1 6 can be found by following the minimum path: diagonal (REPLACE), right(DELETE), diagonal, and diagonal. This path gives the pairwise alignment shown in Table 4.
In ApproxMAP ,wenotethatwedononeedtoknow dist ( seq i ,seq j ) for all i, j to full precision. In fact, the modified proximity matrix based on dist ( seq i ,seq j ) has mostly values of  X  because k N . Thus, if a cell is clearly  X  at any point, we can stop the calculation and return  X  . This will reduce the per cell calcu-lation time significantly. dist ( seq i ,seq j ) is clearly  X  if seq i is not a k -nearest neighbor of seq j ,and seq j is not a k -nearest neighbor of seq i . Remember that the modified proximity matrix is not symmetric. The following theorems prove that seq i and seq j are not k -nearest neighbor of each other when min row ( p ) max { seq the k -nearest neighbor region for sequence seq i ,and min row ( p ) is the smallest value of row p in the recurrence table. In the following theorems, we denote a cell in the recurrence table as RR ( p, q ) with the initial cell as RR (0 , 0) = 0 and the final cell as RR ( seq i , seq j )= D ( seq i , seq j ).
 Theorem 1. There is a connected path from RR (0 , 0) to any cell RR ( p, q ) such that (1) cells along the path are monotonically increasing, (2) the two indices never decrease (i.e. the path always moves downward or to the right), and (3) theremustbeatleastonecellfromeachrow 0 to p  X  1 , in the connected path. Proof. The theorem comes directly from the definitions. First, the value of any cell RR ( p, q ) is constructed from one of the three neighboring cells (up, left, or upper left) plus a non-negative number. Consequently, the values have to be monotonically incr easing. Second, every cell mus t be constructed from three neighboring cells -namely up, left, or upper left. Hence, the path must move downward or to the right. Finally, since there has to be a connect path from RR (0 , 0) to RR ( p, q ), there must be at least one cell from each row 0 to p  X  1. Theorem 2. RR ( seq i , seq j ) is greater than or equal to the minimum row value in any row. (i.e. RR ( seq i , seq j )  X  min row ( p ) for all 0  X  p  X  seq i ) Proof. Let us assume that there is a row, p , such that RR ( seq i , seq j ) &lt; min row ( p ). Let min row ( p )= RR ( p, q ). There are two possible cases. First, RR ( p, q ) is in the connected path from RR (0 , 0) to RR ( seq i , seq j ). Since the connected path is mo notonically increa sing by Theorem 1, RR ( seq i , seq j ) must be greater then equal to RR ( p, q ). Thus, RR ( seq i , seq j )  X  RR ( p, q )= min row ( p ). This is a contradiction. Second, RR ( p, q ) is not in the connected path from RR (0 , 0) to RR ( seq i , seq j ). Now, let RR ( p, a ) be a cell in the connected path. Then, min row ( p )= RR ( p, q )and RR ( p, a )  X  RR ( p, q ). Thus, RR ( seq i , seq j )  X  RR ( p, a )  X  RR ( p, q )= min row ( p ). This is also a contra-diction. Thus, by contradiction RR ( seq i , seq j ) &lt;min row ( p )doesnothold for any rows p .Inotherwords, RR ( seq i , seq j )  X  min row ( p ) for all rows p . p ,then seq i is not a k -NN of seq j ,and seq j is not a k -NN of seq i . Proof. By Theorem 2, RR ( seq i , seq j )= D ( seq i ,seq j )  X  min row ( p ) for any &gt;max { dist k ( seq i ) ,dist k ( seq j ) } , seq i and seq j are not k -NN of each other. In summary by Theorem 3, as soon as the algorithm detects a row p in the is clear that dist ( seq i ,seq j )= dist ( seq j ,seq i )=  X  . At this point, the recur-rence table calculation can stop and simply return  X  . Checking for the condition negligible time and space when k N and k L . 3.3 Optimizing the Clustering Method Now, we investigate how to reduce the N 2 seq cell calculations by using an itera-tive clustering method similar to the well known k-mediods clustering methods. k-mediods clustering is exactly the same as the more popular k-means algorithm, except it works with the representative points in clusters rather than the means Algorithm 1 (Sample Based Iterative Clustering) of clusters. There are two major difficulties in using the k-mediods clustering methods directly in ApproxMAP . First, without proper initialization, it is impos-sible to find the proper clusters. Thus, finding a good starting condition is crucial for k-mediods methods to give good resu lts in terms of accuracy and speed [2]. Second, the general k-mediods method requires that the user input the number of clusters. However, the proper number of partitions is unknown in advance.
To overcome these pro blems, we introduce a sample based iterative clustering method. It involves two main steps. The first step finds the clusters and its representative sequences based on a small random sample of the data, D ,using the density based k -NN method. Then in the second step, the number of clusters and the representative sequences are used a s the starting condition to iteratively cluster and recenter the fu ll database until the algor ithm converges. The full algorithm is given above. When D D , the time complexity for clustering is obviously O ( t  X  N seq )where t is the number of iterations needed to converge. The experimental results show that the al gorithm converges ver y quickly. Figure 1(a) shows that in most experiments it takes from 3 to 6 iterations.

When using a small sample of the data, k (for k -NN algorithm) has to be smaller than what is used on the full database to achieve the clustering at the same resolution because the k -NN in the sampled data is most likely ( k +  X  )-NN in the full database. In ApproxMAP , the default value for k is 5. Hence, the default for k in the sample based iterative clustering method is 3. In our previous work, we have developed a benchmark that can quantitatively as-sess how well different sequential pattern mining methods can find the embedded patterns in the data [4]. In this section, we apply the benchmark to conduct an extensive performance study on the two optimizations.

The benchmark uses the well known IB M data generator [1] which allows us to study the performance systematically. In addition, the IBM data generator embeds base patterns that represent the underlying trend in the data. By match-ing the results back to these embedded patterns, the benchmark can be used to measure the loss in accuracy due to the optimization. In particular, recoverabil-ity provides a good estimation of how well the items in the base patterns were detected. Recoverability is comparable to the commonly used recall except that it weights the results by the strength of the patterns in the database.
Most other criteria were not influenced by the optimizations. As expected in any alignment model, in all experiments there were no spurious patterns and negligible number of extraneous items r esulting in excellent precision close to 100%. The amount of redundant patterns i n the results remained similar to that of the basic ApproxMAP algorithm. The only criteria that was affected was the total number of patterns returned. Not surprisingly, recoverability is a good indicator for the number of total patterns returned increasing or decreasing accordingly. Thus, for simplicity we only report recoverability in our results. 4.1 Proximity Matrix Calculations ApproxMAP can be optimized with respect to O ( L 2 seq ) by calculating the prox-imity matrix used for clustering to only the needed precision. Here we study the speed up gained empirically. We only need to study the reduction in running time because this first optimization maintains the results of ApproxMAP .Fig-ure 1(b) shows the speed up gained by the optimization with respect to L seq in comparison to the basic algorithm. The figure indicates that such optimization can reduce the running time to almost linear with respect to L seq .

To investigate the performance further, we examined the actual number of cell calculations reduced by the optimization. That is, with the optimization, the modified proximity matrix has mostly values of  X  because k N . For those dist ( seq i ,seq j )=  X  , we investigated the dynamic programming calculation for dist ( seq i ,seq j ) to see how many cells in the recurrence table were being skipped. To understand the savings in time, we report the following in Figure 1(c). When 10  X  L seq  X  30, as L seq increases more and more proportion of the recurrence table calculation can be skipped. Then at L seq = 30, the proportion of savings levels off at around 35%-40%. Thi s is directly reflected in the savings in running time in Figure 1(c). Figure 1(c) reports the reduction in calculations and running time due to the optimization as a proportion of the original algorithm. Clearly, the proportion of savings increase until L seq = 30. At L seq =30the running time levels off at around 40%. Thus, we expect that when L seq  X  30, the optimization will give a factor of 2.5 speed up in running time. This is a substantial improvement in speed withou t any loss in accuracy of the results. 4.2 Sample Based Iterative Clustering The sample based iterative clustering method can optimize the time complexity with respect to O ( N 2 seq ) at the cost of some reduction in accuracy and larger memory requirement. The larger the sa mple size the better the accuracy with slightly longer running time. We investigate the tradeoff empirically. Figure 2(a) presents recoverability with respect to sample size ( k =3).When seq  X  40 , 000 , recoverability levels off at 10% sample size with good recoverabil-ity at over 90%. When N seq &lt; 40 , 000 , ApproxMAP requires a larger sample size of 20%-40%. In summary, the experiment suggests that the optimization should be used for databases when N seq  X  40 , 000 with sample size 10%. For databases with N seq &lt; 40 , 000 a larger sample size is required as 10% will result in signifi-cant loss in accuracy (Figure 2(b)). Essentially, the experiments indicate that the sample size be at least 4000 seqs to get comparable results when k =3.
In the iterative clustering method more memory is required in order to fully realize the reduction in running time because the N 2 seq proximity matrix needs to be stored in memory across iterations. In the basic method, although the full proximity matrix has to be calculated, the information can be processed one row at a time and there is no need to return to any values. That is, we only need to maintain the k -NN list without keeping the proximity matrix in memory. However in the iterative clustering method, it is faster to store the proximity matrix in memory over different iterations so as not to repeat the distance calculations. When N seq is large, the proximity matrix is huge. Hence, there is a large memory requirement fo r the fastest optimized algorithm.
Nonetheless, the proximity matrix b ecomes very sparse when the number of clusters is much smaller than N seq . Thus, much space can be saved by using a hash table instead of a matrix. Furthermore, a slightly more complicated scheme of storing only up to the possible number of values and recalculating the other distances when needed (much like a cache) will still reduce the running time compared to the basic method. Efficient hash tables are a research topic on its own and will be studied in the future. For now, the initial implementation of a simple hash table demonstrates the huge potential for reduction in time well. In order to fully understand the potential, we measured the running time assuming (1) memory was limitless and (2) no memory was available to store the proximity matrix. That is, distance values were ne ver recalculated in the first experiment and always recalculated in the second experiment.

Figure 2(b) and (c) show the loss in recoverability and the gain in running time with respect to N seq with the optimization (sample size=10%, k =3). Figure 2(d) depicts the relative running time with respect to N seq . optimized (all) is a simple hash table implementation with all proximity values stored and optimized (none) is the implementation with none of the values stored. The implementation of a simple hash table was able to run up to N seq =70 , 000 with 2GB of memory (Figures 2(c) and 2(d) -optimized (all) ). A more efficient hash table could easily improve the memory requirement.

A good implementation would give running times in between the optimized (all) and the optimized (none) line in Figure 2(c). The results clearly show that the optimization can speed up time significantly at the cost of negligible reduction in accuracy. Figure 2(d) show that the optimization can reduce running time to roughly 10%-40% depending on the size of available memory. Even in the worst case the running time is significantly faster by a factor of 2.5 to 4. In the best case, the running time is an order of magnitude faster. Optimizing data mining methods is important in real applications which often have very large databases. In this paper, we proposed two optimization tech-niques for ApproxMAP that can reduce the running time significantly for consen-sus sequential pattern mining based on sequence alignment.

