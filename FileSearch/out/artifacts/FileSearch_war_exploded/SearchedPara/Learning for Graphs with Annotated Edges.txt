 Automatic classification with graphs containing annotated edges is an interesting problem and has many potential ap-plications. We present a risk minimization formulation that exploits the annotated edges for classification tasks. One major advantage of our approach compared to other meth-ods is that the weight of each edge in the graph structures in our model , including both positive and negative weights, can be learned automatically from training data based on edge features. The empirical results show that our approach can lead to significantly improved classification performance compared to several baseline approaches.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms graph regularization, webpage categorization
Automatic classification with graphs containing annotated edges is an interesting problem and has many potential ap-plications. For example, in webpage categorization, the hy-perlink structures among web-pages are very useful resources. The hyper-links can be thought as edges annotated by their associated anchor text and can be used to improve web-page classification performance. Many researcher have studied the problem of classification with graph structures. For ex-ample, [1] has exploited the web links using pagerank scores and combines these scores with local text features for web-page categorization tasks. [2] and [3] developed semisuper-vised learning methods to take advantage of graph structures in classification tasks.

While the above works successfully improved the classifi-cation performance using graph structures, they did not ex-ploit the annotation information associated with the edges in the graph. For example, in [2], it is assumed that all the edges in hyperlink structures have the same positive weight. However, this assumption may not always yield the best result from a classification standpoint. The importance of these edges could be very different in terms of classification, depending on their associated anchor text. Furthermore, sometimes the edges in hyperlink structures may have nega-tive weights. For example, in the catalog and product web-page classification task, the catalog pages often connect to product pages but seldom connect to other catalog pages.
One simple way to use annotation information is to pass edge annotations to node features. That is to treat the an-notations of an edge as additional features of the two node connected by this edge. However, by doing this, we have merged the edge features into node features and lost part of the useful information. In this paper, we propose a novel objective function in the graph regularization framework to exploit the annotations on the edges. The general idea used in the paper is to create regularization for the graph with the assumption that the likelihood of two nodes to be in the same class can be estimated using annotations of the edge linking the two nodes.

There are other discriminative models that could learn edge weights in the graph automatically from the training data. For example, conditional random field (CRF) has been widely used for classification tasks on chain graphs. However, when the graph topology becomes more complex (which is always the case in hyper-link structures), the ac-curate solutions of such models will be very difficult to get. Our approach does not have such a problem.
Consider the problem of predicting unknown labels for the testing examples based on their input vectors. In this paper, we are interested in the setting where we observe a training set ( x i ,y i )for i =1 , ..., n and a test set ( x j = n +1 , ..., m .Here x i is the input vector of the ith example and scalar y i  X  X  X  1 , 1 } is the class label of the ith example. The true labels y j for the test set ( x j ) are unknown and to be predicted. We are interested in the case that in addition to the input vectors, a graph is also observed on the whole dataset (including both the training and the testing set). Each node in the graph corresponds to an example and each edge in the graph corresponds to some relationship between the two connected examples.
The objective function we proposed is listed as formula 1,
Here Loss () represents the loss function we defined on the training data. In our work, we will use the the least square loss function thus loss ( f,y )=( f  X  y ) 2 . K is the m  X  kernel matrix that is constructed only from the local text features.  X  and  X  are regularization parameters that are treated as constants here. Notice that formula 1 will reduce to a standard ridge regression classifier when  X  is set to be zero. c ij are the edge weights to be learned from the anchor text features. Vector a ij =( a ij 1 ,...,a ijp ) represents the values of the p anchor text features associated with the hyperlink connecting web-page i and web-page j .Vector  X  =(  X  1 ,..., X  p ) T consists of the parameters that represent the weights of anchor text features. Notice that we are using a least square loss function as Loss ( f i ,y i ) so that the values of f i f j are encouraged to be similar to  X  1.

The intuition of formula 1 is clear. If we consider regu-larization of the form ( f i f j  X  c ij ) 2 with unknown annota-tion dependent parameter c ij , then the proposed method seems the best way to use annotation information to opti-mize parameter c ij because c ij = E ( f i f j | a ij ). This has the desirable effect that if given the annotations, f i and f likely to have the same sign, then our objective function will strengthen this trend. When f i and f j are likely to have different signs given the annotations, formula 1 works in a similar pattern. In formula 1, each edge weight c ij =  X a is assumed to be the sum of anchor text weights appeared in this hyperlink. Note that the f i values for the training examples are encouraged to be close to y i (either 1 or  X  by the first term of the above formula. [2] has presented an efficient algorithm to solve the ob-jective function with graph regularizers. In this paper, a similar algorithm with slight modifications is used to solve the objective function in formula 1.

Notice that formula 1 can be re-written as
Here u plays the role as a stabilizing parameter and we set it to be a small constant 0 . 1. It can be regarded as adding a feature the objective function as formula 2, the construction of the dense kernel matrix K is avoided. We can then solve the problem iteratively using the gradient descent algorithm. In each iteration, we first fix  X  and solve v and w .Thenwe fix v and w and solve  X  . This process is iterated until the algorithm converges. The initial values of these parameters are all set to be zero.
 Table 1: The micor-F1 performance (mean  X  std-dev % ) Table 2: The macor-F1 performance (mean  X  std-dev % The two real hyperlinked collections used in our paper are WebKB data (http://www.cs.cmu.edu/webkb/) and Yahoo! Directory data (http://www.yahoo.com/). We constructed co-citation graphs on these two collections, in which two pages are connected by an undirected edge if both pages are linked to by a third page.

The WebKB dataset consists of 8275 web-pages crawled from university web sites. The vocabulary consists of 20000 most frequent words. Its co-citation graph has 1143716 edges. The Yahoo! Directory dataset consists of 22969 web pages. Each page of this collection belongs to one of the 13 top level topical directory categories (for example, arts, business and education). The vocabulary consists of 50000 most frequent words. The number of edges in its co-citation graph is 1170029. We randomly split the labeled data into two parts: 50 percent for training and another 50 percent for testing. We draw five runs and report test set averages and standard deviations.

The micro-F1 and macro-F1 performance of different ap-proaches on multiple datasets with co-citation graphs are listed in table 1 and table 2. We use the graph regular-ization approach in [2] as our baseline approach and call it  X  X aseline X  X n the two tables. In order to be fair, we have also tried an extension of the baseline approach by putting the anchor text features associated with each edge into the body text of the two pages connected by this edge. We call this extension  X  X aseline with anchor text X  in the following ta-bles. The performance of our approach is shown in the last raw of the two tables. We can see that our approach has achieved significant improvement compared with the other two approaches.
This paper presents a novel risk minimization formula-tion for classification tasks with graphs containing anno-tated edges. The empirical results show that our approach can lead to improved classification performance, compared to several baseline approaches. [1] Z. Gyongyi, H. Garcia-Molina, and J. Pedersen. Web [2] T. Zhang, A. Popescul, and B. Dom. Linear prediction [3] X. Zhu, Z. Ghahramani, and J.Lafferty.

