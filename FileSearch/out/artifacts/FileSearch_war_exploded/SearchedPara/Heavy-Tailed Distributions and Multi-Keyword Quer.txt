 H.3.1 [ Content Analysis and Indexing ]: Indexing Methods Algorithms, Performance, Experimentation information retrieval, partial match, power law, heavy-tailed, index
At the core of Information Retrieval performance lies the ability to intersect long lists of postings quickly. Much research has cen-tered on reordering these lists to reduce the fraction of them that is processed (e.g. [11, 3]), and on improving the processing of the-se intersections. Still, some queries require costly deep traversal into long lists. Consider e-commerce web sites such as Amazon and eBay with large catalogs of products. It is important to these businesses that customers can find what they want quickly. Long latencies increase abandonments and decrease sales and adverti-sing revenue. A few long latencies can be serious, even when the average is not that bad.

The challenge is to reduce the worst-case overhead required to process arbitrary keyword queries. The database literature has stu-died high-dimensional indexing and partial-match queries, and found Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. this problem to be hard in the general case for unrestricted datasets. Fortunately, datasets of interest to the SIGIR community may be easier than the general case. SIGIR datasets tend to obey power laws, which are common in natural language [4].
Consider an e-commerce web-site where products are searchable by name, description and category (e.g.,  X  X oman X  X  shoes X ,  X  X old jewelry X  ... ). Some terms are more frequent than others. The more frequent terms have relatively long inverted lists. Intersection time typically depends on frequency; intersecting long lists can lead to unacceptably long latencies.

Figure 1 shows that it can take much longer to intersect longer lists than shorter lists. The figure is based on a corpus of more than 20 million products from a large e-commerce portal [1]. We used keywords from three frequency ranges: F for frequent (over 900K postings), M for middle (about 50K postings), and L for low fre-quency (less than 1K postings). The intersections were performed using the full-text component of a commercial database system.
Intersections of long inverted indexes are very slow relative to other queries. Unfortunately, they are not uncommon. We analyzed a search trace of more than 3 million customer searches from the portal and found that many searches ( 29% ) contained a frequent keyword (in the top 0.1% of keywords) 1 , 8% of searches contained at least one of the 0 . 01% most frequent keywords and 1 . ches contained at least one of the 0 . 001% most frequent keywords. The higher the frequency of a keyword, the longer the intersection cost; this means that there is an opportunity for improving the inter-section performance, as frequent keywords are commonly queried and even very frequent keywords are not rare. In the remainder of the paper, we address the following problem:
Given a document collection, propose a set of indexes to mate-rialize, so that the time for intersecting keywords does not exceed a given threshold  X  . The key challenge here is the space requirement. If space were not an issue, we could trivially archive the time requi-rement by materializing all possible combinations of keywords, but this would take exponential space. The additional indexes should not be larger than k times the size of the original inverted index, for a small factor of k . We will show how to materialize such a set of indexes for reasonable values of  X  (e.g., the time required to scan 20% of the largest inverted index), at least for a collection of short documents distributed by a power law.
To address search performance, the IR community has developed numerous techniques aimed at reducing the amount of data that needs to be processed, by either ordering the postings within each index in a suitable manner, or by proposing approximations of the used scoring methods which may be computed more efficiently. We take a different approach in that our techniques are orthogonal to the specific ranking function used and address the issue of reducing the time needed to compute the intersection exactly .

The problem studied in this paper is related to the issue of inde-xing phrases in Information Retrieval. Similar to our problem sce-nario, full materialization of indexes of all common phrases entails prohibitive storage costs. The approach adopted in [5, 12] is to use different types of indices  X  inverted indices for rare words, a variant of nextword indices for the commonest words and a phrase index for the commonest phrases; similarly, we use different combinati-ons of access paths depending on keyword frequencies.

The underlying indexing problem can also be phrased as an in-stance of the partial-match problem: lower bounds on the perfor-mance of partial-match queries have been studied theoretically in [8] using a cell-probe framework.

Also in the database context, various multidimensional search structures [7] have been proposed. To apply them, each keyword query could either be formulated as a high-dimensional range que-ry over point data or as a high-dimensional point query over heavily overlapping spatial data. Either problem formulation results in an indexing problem over very sparse data in very high dimensionality ( &gt; 10K dimensions). It is well known [6] that non-redundant space-partitioning techniques suffer from the  X  X urse of dimensionality X , meaning that they result in access times exceeding the cost of scan-ning the full data set for as little as 10 dimensions, rendering them useless for our purposes.
In this paper we will present index structures and processing stra-tegies addressing the above problems. Our work makes the followi-ng salient contributions: (1) We introduce index structures and a query processing frame-work that enforce a given threshold  X  on the overhead of compu-ting conjunctive keyword queries. (2) We leverage the fact that the frequency distribution of natural-language text follows a power law to model the resulting index size. In particular, we show that while the number of possible l -keyword combinations relevant for indexing grows exponentially with incre-asing l , the underlying data distribution implies that only a small fraction of these combinations needs to be indexed, when the docu-ment sizes are small. (3) Determining for which keyword-combination to materialize in-dexes may require significant I/O and main memory. To overcome this limitation, we will describe a probabilistic scheme that signifi-cantly reduces this overhead. (4) We provide an extensive experimental evaluation of the perfor-mance on large real-life datasets.

The remainder of this paper is organized as follows: Next, we will describe the query processing strategies we consider and the details of the index structure we propose (Section 2). Then we will describe how to instantiate this structure to ensure a given thres-hold  X  on the execution cost of conjunctive keyword queries. Sub-sequently, we describe the relevant properties of the distributions of keywords and keyword-combinations in natural-language text and show how to leverage these to model on the size of the resulting in-dex structure (Section 3). We will describe the index construction in Section 4 and provide an experimental evaluation in Section 5.
For each query Q , we denote the keywords it contains by words = { w 1 ,...,w l } . In the following, we consider only queries con-taining up to a threshold k max of keywords (e.g., 7). Each of these keywords comes from a global vocabulary V . Note that the maxi-mum number of keywords in a single query we have to consider for searches in the e-commerce scenario is small; it is well-known that most search queries are short (e.g., according to [14] over 50% of all internet searches contain at most two words, and 75% at most three). We have found similar trends in traces of search queries against the e-commerce site [1].

For all keywords we maintain a global ordering  X  . We use this or-dering for indexing; i.e. when materializing a keyword-combination C containing the words { w 1 ,...,w l } we let i 1 ,...,i be a set of indices such that  X  j  X  1 ,...l  X  1: w i j &lt; &lt;  X  denoting the ordering induced by  X  and write C as ( w i 1 instead. Basically, this ensures that we never distinguish between permutations of the same keyword-combination.

In the following, we sometimes use a query Q and the set of key-words words ( Q ) interchangeably, with the correct meaning being clear from context. We denote the number of items (=documents) whose text contains all keywords of a query Q by size ( Q larly, for a single keyword w we denote the number of documents containing w by size ( w ) . Finally, we use the notation the number of keywords a query Q contains.
To build structures that reduce the maximum latency of keyword queries, we introduce a simple cost model to quantify these laten-cies. Costs will be expressed as a linear combination of two costs: (1) disk seeks to the beginning of posting lists, plus (2) scanning postings. It is useful to consider these two costs separately becau-se the combination rule is a bit of a moving target. Computatio-nal costs have decreased dramatically over time and will continue to do so going forward. However, some costs have decreased mo-re than others. Scanning costs are dropping faster than seek costs. This trend is likely to continue going forward.

For ease of exposition, we normalize the costs s.t. scanning a single posting in an inverted index has unit cost. This normalization allows us to think of  X  as specifying both a cost bound as well as a maximum number of postings that can be scanned.

The cost model assumes only the simplest possible IR-engine, which computes intersections by fully scanning the inverted in-dex of every keyword. However, our approach is equally appli-cable to more sophisticated engines and hardware configurations (which in turn would lead to different cost models), in particular the case in which the all inverted indexes are read and intersected in parallel (allowing the intersection of the indexes for keywords ( w 1 ) ,..., ( w k ) in O (max i =1 ,...,k size ( w i )) ) or for engines allo-wing random access within the indexes (allowing the intersection of two indexes of size n, m ,with n&lt;m in O ( n  X  log 2 m ) In both cases, fewer keyword-combinations are indexed, which in turn reduces the size of materialized structures significantly. The cost of a query Q depends on the execution strategy chosen. Initially, there are two access strategies available to us: ID-intersection: This strategy retrieves all inverted indexes of the queried keywords and intersects them. We model the execution cost as | Q | seek accesses to disk (the cost of one of which we model as a constant Cost seek ) to retrieve the inverted indices and the cost of reading their contents entirely: Post-filtering: If one of the keywords w i in Q is very rare, we can process Q by only processing the inverted index of w i andthenre-trieving the text of all matching items and verifying the remaining keyword constraints using text itself. The advantage of this strat-egy is that its processing costs become independent of the number of additional keywords and the lengths of their inverted indices; however, matching the remaining keywords against the text is si-gnificantly more expensive than index-intersections for the same number of postings. We model its cost as the cost to retrieve the text associated with size ( w i ) items (which is dominated by the seek times) and applying | Q | X  1 keyword-filters to the text, which is a function of the text-length for each column. For simplicity, we model the text length of the items as a constant length , which we multiply by the cost of applying a single like-predicate: Cost If necessary, we can ensure that this function over-estimates latency in cases with varying text-lengths by choosing this constant large enough; however, for the scenarios we consider, the text lengths tend to be small and not vary very much; hence, the costs for this strategy tend to be dominated by the seek times:
Given a cost model we now describe additional indexes to com-plement single-keyword inverted indexes which enforce an executi-on cost of less than a threshold  X  by limiting the maximum number of postings we need to retrieve for an arbitrary query. The structure that we utilize are additional inverted indices that materialize the postings for documents containing combinations of keywords; i.e., each such index can be thought of as the materialized result to that particular keyword query. The salient features of these structures are that (a) We only materialize indexes for a k -keyword combination if the corresponding query result can not be obtained quickly (i.e., with less then  X  / 2 overhead) using intersection of inverted indexes for keyword combinations of size k &lt;k . (b) Part of the query-processing time of a query is allotted to pro-bing the  X  X atalogue X  of the materialized structures to discover which relevant keyword-combinations are indexed. We also obtain infor-mation on the size of the inverted indexes as part of this probing, allowing us to subsequently chose the best possible execution stra-tegy (as predicted by the cost-model) before the actual processing of the query. (c) For a small number of keyword combinations simply retrieving the fully pre-computed answer to a search query requires more than the target latency. However  X  again due to data skew  X  there will be few such instances; moreover, since these are search results, the user interface needs to initially display only the top-ranked results (ordered by whatever ranking scheme we use) and can use the ti-me required for the user to browse them to retrieve the remainder. Therefore, for the few such keywords or keyword-combinations, we materialize the top-ranked results separately.

The main structure we use to complement the inverted indexes adds one layer of of indirection to the standard inverted index (Fi-gure 2): instead of pointers from each vocabulary item to the cor-responding inverted index, we maintain  X  for each vocabulary item w  X  a list of all keyword combinations containing w for which we have materialized the corresponding inverted index, the so-called match list . We denote the set of all keyword combinations reali-zed as match list entries by Match . Each entry in the match list in turn points to an inverted index containing postings of all items matching all keywords in the entry. In addition, each entry in the match list also stores the number of postings in the corresponding inverted index; we also maintain the number of postings in each single-keyword inverted index together with the vocabulary. The resulting structure is in many ways similar to the nextword indexes of [5, 12] and can be implemented in a similar manner.

The physical layout of this structure is as follows: since (as we will describe later) we only materialize combinations of frequent keywords and only a small fraction of them, it is possible to main-tain an index with the first two keywords of each combination in main memory 2 . In the following, we will assume for purposes of cost modelling that this layout is in place. Once we have this index structure in place, we process a query Q over keywords w 1 ,...,w k as follows: if Q contains a keyword w sufficiently rare so that the post-filtering strategy becomes suffi-ciently inexpensive, we use this strategy. Otherwise, we retrieve all match-list entries containing two keywords from Q as their prefix (we assume that the single-keyword vocabulary and sizes are alrea-dy memory-resident). Using the size-information contained in the match-list entries we can now determine if size ( Q ) is sufficiently large that we cannot process Q entirely without violating the cost-threshold  X  ; if this is the case, we retrieve the top-ranked tuples from the corresponding index. For queries with smaller result si-zes, we now determine which combination of inverted indexes co-vers all keywords in Q (possibly more than once) while minimizing the cost (using our cost model) of intersecting these indexes  X  note that this covers both multi-and single-keyword inverted indexes. This formulation results in an optimization problem Figure 4: There aren X  X  very many frequent words, even in large docu-keywords, at most one of which may appear in a match list entry and (c) a small number of high-frequency keywords for which we materialize a number of indexes. We will describe this in detail in Section 3.3. For our method to be scalable, we need to ensure that the number of keywords in the latter two classes does not grow quickly with corpus size.
 Fortunately, this appears to be the case, as illustrated in Figure 4. Each line, labeled N (  X  ) , shows the growth rate in the AQUAINT corpus for words with document frequency of  X  or more. Thus, for example, the line for N (10000) shows that there aren X  X  very many words that appear in 10000 documents or more. The number of such words increases slowly with corpus size, much slower than N(1000). We are particularly concerned with large  X  s. Note that N (10000) &lt;&lt; N (1000) &lt;&lt; N (100) &lt;&lt; N (10) very many frequent words, even in large document collections.
In this section we will describe the structures materialized to en-sure that the cost for processing an query of up to k max does not exceed  X  . To populate the match-lists, we first consider keyword-combinations of size 2 for materialization, and then incre-ase the size until we reach k max . Now, for any size k we materialize all combinations C for which  X  w  X  words ( C ): Cost P robe ( w ) &gt;  X  (2) and Cost Opt ( C )  X  and size ( C )  X   X   X  Cost Seek .(4) The resulting structures ensure that any query Q for which it holds that size ( Q )  X   X   X  Cost Seek can be computed using less than cost: If Cost Opt ( Q )  X   X  2  X  Cost Seek using indexes over com-binations of less than | Q | X  1 keywords (condition 3) and post-filtering is not an option (condition 2), then we materialize an addi-tional inverted index, as condition 4 must hold.
To model the index sizes based on these observations, we will use a relatively simple analytical model of the word-frequency dis-tributions for ease of exposition. The main contribution of the theo-retical model will be to show that the potentially exponential grow-th of possible keyword-combinations is balanced by the power-law behavior of the word-distribution in natural language corpora.
We use the following notation: let N be the total number of words in the text distribution, and V = |V| be the number of di-stinct words. Due to the power-law, the frequency of a word of rank z can be expressed as where  X  is a normalizing constant smaller than 1 ensuring that of the distribution. For ease of exposition we set  X  equal to uni-ty, resulting in the standard harmonic probability distribution over words. Under this distribution, the number of words that occur m times, V ( m ) , can be modelled as
We will first show how the power-law distribution and the con-struction of the previous section lead to the  X  X riage X  of keywords we have described: since the cost of the post-filtering strategy on-ly depends on the length of the text associated with items and the number of occurrences of the rarest keyword in a query, equation 5 means that the majority of keywords will not occur in any keyword-combination in the match list. Any keyword w for which size ( w )  X   X  tail = cannot lead to execution costs in excess of  X  , and hence no addi-tional indexing is required, eliminating V  X   X   X  N  X  consideration.

Similarly, not more than one keyword w with size ( w )  X   X  ( X  / 2  X  Cost Seek ) can occur in an k -keyword entry in the match list. We will prove this by contradiction: consider the case of such a combination being materialized: assume a keyword-entry C con-sisting of k keywords words ( C )= { w 1 ,...,w k } ;let w the least frequent keywords with size ( w 1 )  X  size ( w 2 The (because of the requirements described in Section 2.4) the al-gorithm must consider an execution strategy that intersects the in-dexes used when processing two subsets C 1 ,C 2 of words ( ring no keywords, one of which contains w 1 and the other w refore, either C 1 is not materialized, implying that Cost  X  / 2  X  Cost Seek , or it is, meaning we can retrieve it using cost  X  / 2 . Using a similar argument for C 2 , Cost Opt ( C ) can be at most  X  , meaning there is no need to materialize an entry C , lea-ding to a contradiction.
 Figure 5: Only words of frequency greater than  X  match Modelling the number of frequent keyword-combinations: Now we will use this model to model the number of l -keyword combina-tions that occur in more documents than a threshold  X  . We denote this value as occurrences ( l,  X  ) . Subsequently, we will show that the number of l -keyword entries into the match list can be modelled as a function of occurrences ( ... ) . Note that in the target scenario the individual items are associated with relatively small text entries (e.g., a product, a review, or a seller), which we will show to result in a small rate of growth for occurrences ( l,  X  ) with increasing values of l .

First, we define avg w as the average numbers of words contai-ned in the text associated with an item. For ease of exposition, we will assume that all items are associated with exactly avg w words (as opposed to modelling the distribution of this value explicitly). There are necessarily some duplicate words in an item, so we mo-del the number of distinct words V e ( n ) in a document of n words as a function of the document size: for a constant 3 R . Using this model any item will contain distinct l -keyword combinations. Under the simplifying assumpti-on that the power-law distribution governing the l -keyword distri-bution follows the same skew-parameter as the original keyword-distribution, the number of l -keyword combinations occurring mo-re often than  X  can be constrained as
This means that while the number of possible keyword-combinations grows exponentially in the number of keywords, the number of l -keyword combinations larger than a threshold  X  grows by a factor of this factor is a function of the square root of the individual text si-zes (which are small for the target scenarios) and independent of the corpus size or the vocabulary size (both of which can become very large in this context), and (b) the factor decreases as l grows, resulting in tractable numbers of combinations to materialize.
This immediately allows us to model the number of keyword-combinations for which we have to explicitly materialize the top results, since their result-sets are too large to be read within Example: To demonstrate the size of the resulting values, con-sider a data distribution modelled on the product database des-cribed in Section 1.1, containing N/avg w =60  X  10 6 entities; each entity contains roughly w = 100 words, meaning  X  beco-mes  X  1 / 15 and there is a total of N  X  avg w = 6000 Million postings. We choose R =2 . 5 . Assuming we index for queries con-taining up to k =5 keywords, and set  X  at 50 K ID-values, we ob-tain: occurrences (3 , X  )=18 . 4 K , occurrences (4 , X  ) = 101 and occurrences (5 , X  ) = 425 K . Even when multiplied with the number of top-ranked postings we materialize for these keyword-combinations, these numbers still are small fraction of the 6 Billion postings in the original index.
 Modelling the number of match list entries: Moreover, we can use the above to model a loose constraint the number of l -keyword entries in the match list, of the form f  X  occurrences (( To show this, consider an arbitrary entry C = { w 1 ,...w match-list; let w min be the keyword in C for which size ( in minimal, C = words ( C )  X  w min . Now one of two conditions must hold: (a) size ( C ) &gt; X  match : in this case, the only statement we can (b) or size ( C )  X   X  match : In this case, let S 1 , S 2 This means that  X  with growing number of keywords  X  we expect the number of entries in the match list to grow more slowly than the number of keyword-combinations occurring more often than a threshold (as  X  match grows linearly with l ). However, depending on the value of  X  , the factors  X   X  N  X  very large. In these cases, we may have to apply our techniques to a subset of the most frequent keywords only (e.g., only keywords occurring in search logs).
 Size-distribution of the resulting inverted indices: While the abo-ve calculations allow us to model the number of keyword combina-tions for which we create additional inverted indexes, it does not tell us anything non-trivial about the distribution of posting-sizes of the corresponding inverted indexes. We studied these size-distributions resulting from experiments and found them to be highly skewed as well. Not only does the vast majority of keyword-combinations sa-tisfying conditions 2-4 result in empty intersections 4 , but most of the remaining indexes have less than 10 postings. To demonstrate this, Figure 6 shows the size distribution of the 32K largest inver-ted indexes in an experiment similar to the one described in detail in Section 5.1 (with a different value of  X  ). Again, we find a power law governing the distribution of the index sizes. Figure 6: The distribution of inverted index sizes follows a power law
Construction of the proposed structures requires two elementary operations: (a) deciding which additional inverted indices to ma-terialize and (b) building and maintaining the indexes themselves. Part (b) has been studied in great depth already, part (a) however is challenging, as it requires knowledge of the intersection sizes for very large inverted indexes, which are unlikely to fit into main me-mory at the same time. This may make this part of the computation prohibitively expensive.

As a consequence, we use a probabilistic scheme to estimate the required intersection sizes, allowing us to maintain compact repre-sentations of the relevant inverted indexes, which fit into main me-mory. This is made possible by the fact that the cost-thresholds themselves are necessarily large enough so allow the retrieval of tens of thousands of postings without exceeding  X  (a full-text re-trieval system that cannot handle these numbers is likely a no-starter in the first place), providing some leeway regarding the ac-curacy of the probabilistic techniques.
Computing the size of intersections between lists of postings cor-responds to the problem of computing L 1 -distances between co-lumns in the indicator matrix A formed using the keywords as one dimension and the item/document ID values as the other. Popular techniques for such distance computations in limited memory are based on random projections , which multiply A by a appropriately chosen random matrix R to generate a much smaller data matrix B = A  X  R . However, these estimation methods are typically not applicable to multi-way intersections, which we require. As a con-sequence, we use a different technique, based on a combination of sketches and sampling introduced in [9, 10]: Let ID denote the set of identifiers all documents in the corpus. This method then uses a random permutation  X  ID : ID  X  X  1 ,..., |ID|} and  X  for every inverted index  X  constructs a sample the first (according to  X  postings in the index. Now we can estimate intersection sizes bet-ween a list of inverted indexes I 1 ,...,I l , based on these samples as follows: let D s be the smallest among the maximum (according to  X 
ID ) postings in the respective samples. Now we trim the samples from all postings i for which  X  ID ( i ) &gt;D s . The resulting samples are equivalent to a random sample of D s rows from across the re-spective l columns in he indicator matrix A ; this sample can now be used to compute a maximum-likelihood estimate of the intersection size. We evaluate the accuracy of this approach in Section 5.3.
Note that the sampling ultimately only affects one condition among the three governing which keyword-combinations to materialize (Section 3.2): Condition (4). Condition (2) depends only on the sizes of single-keyword indexes, which we store together with the vocabulary. Moreover, since we construct the match-list entries and the corresponding indexes in order of the number of keywords they contain (this way, we can use existing indexes, significantly redu-cing construction-costs), the exact sizes of all materialized multi-keyword indexes over ( k  X  1) -keyword combinations are known when determining which indexes over k -keyword combinations to construct; this is turn means that condition (3) can also be evaluated exactly and only the size of the new index has to estimated. Note that this means that bad estimates can never cause us to fail to meet the threshold  X  ; we just might construct too many indexes.
In order to further compress the resulting structures, we augmen-ted each posting (which in our experimental setup corresponds to a 32-bit document ID before compression) with an additional 32-bit field, which indicates the presence of certain high-frequency key-words in the document the posting refers to. For example, we can use this field to indicate the presence or absence of one of the 32 most frequent non-stopwords in the corpus. In this case, we of-ten can avoid having to materialize a multi-keyword index over a combination of these high-frequency words and less frequent words { w 1 ,...,w h } , as we can use the index on { w 1 ,...,w h } however, may be larger) to obtain the same information.

In the experiments on the e-commerce dataset, most frequent keywords correspond to distinct product categories (e.g.,  X  X ook X ) and a few frequent product attributes ( X  X ed X ,  X  X lack X ,  X  X ages X ), mea-ning that relatively few combinations of them actually co-occur in product descriptions in the corpus. This allows us to encode all occurring combinations of significantly more than 32 frequent key-words in the 32 bit field. While the additional field doubles the size of each posting before compression (the encoded values are highly skewed and thus should compress well), it can significantly decre-ase the number of keyword-combinations we materialize.
In this section, we describe the experimental evaluation of our techniques, including the resulting query costs for real-life data (Section 5.1), the total size of the structure on disk (Section 5.2) and the accuracy the probabilistic techniques to estimate the inter-section sizes (Section 5.3).
 Prototype Implementation: We implemented the match-list da-ta structures using a commercial database system, which we also use to process queries against the match-lists. Due to size restric-tions we omit the implementation details. While our experimental results can be used as a proof-of-concept, the absolute performan-ce is not representative of the performance improvement possible when using a full-fledged IR system. To measure the performance of the unmodified IR system, we used a commercial  X  X ull-text X  ex-tension shipped as part of the database system. In all experiments we pruned the indexed corpora of stop-words and formatting tags.
To evaluate the effect of our approach on measured query cost, we used the product data set described in Section 1.1, containing 20 million items. For this experiment, we materialized the index struc-ture for a subset of frequent keywords with more than 10K postings that occurred at least once in the query log; we used the parameters k max =4 , Cost Seek = 1000 (i.e. a disk seek is as expensive as scanning 1000 postings) and set  X  to the cost of scanning 20% of the number of postings of the largest single-keyword inverted in-dex. This means that  X  once our structure is in place  X  no query of 4 or fewer keywords accesses more than a fifth of the number of postings in total that would have been read when scanning the inverted index of the most frequent keyword alone.

To measure performance, we generated random 2-word and 4-word queries from the keyword set and compared the running ti-mes of the commercial IR engine to our approach, with all inverted indexes residing on disk. We flushed the database caches before every measurement. For individual queries, our approach resulted in speed-ups of up to a factor of 18x (2 keywords) and 14x (4 key-words); averaged over the query set, our approach improved query times by a factor of 6.7x (2 keywords), and 3.1x (4 keywords).
Because we selected the workload as a random combination of keywords, especially the latter results significantly overstate the im-pact our techniques would have on average query performance. Ho-wever, they do indicate that our approach can significantly benefit the sub-class of queries that cause user-perceptible latency.
The most important aspect of our work is the size of the resulting indexes; if they are overly large, any gains in processing times be-come immaterial. To evaluate index sizes, we used the e-commerce corpus which contains a total of 899M postings. We use the para-meters k max =4 , Cost Seek = 1000 ,set  X  to the cost of scanning 20% of the number of postings in the largest inverted index and used  X  tail =50 (i.e. no additional indexes for keywords occurring in less than 50 documents), leaving 141 K keywords for indexing. For queries with results of more than  X   X  Cost Seek postings, we materialize the top 20 postings. The resulting multi-keyword index structures contained 734M postings, i.e. an 81.6% increase, indica-ting that our approach scales up to large corpora and vocabularies. Corpora of larger Documents: The significant limitation of our approach is that it does not scale to corpora with larger document lengths. To demonstrate this, we evaluated the index sizes on the 314K document subset of the AQUAINT corpus; the average length of these documents is almost an order of magnitude larger than it is for the e-commerce corpus. In addition, this corpus is challen-ging for our techniques as a number of keywords occur in 50% or more of the corpus (unlike the e-commerce data where the most frequent keyword occurs in 23% of the documents). For this col-lection, we used the same parameters as above, but set  X  to the cost of reading 1/3 of the number of postings in the largest inver-ted index. The resulting multi-keyword index structures contained more than 10x the number of postings of the original index, ma-king straight-forward application of our technique impractical. For such corpora with larger document lengths, depending on the app-lication details, our techniques may still be applicable, if either the number of relevant keyword combinations is reduced in a suitable manner (e.g., by only taking keyword-combinations that appear in query logs into consideration) or the documents are broken down into smaller chunks (e.g., paragraphs). Also note that our cost mo-del is based on the assumption of an overly simplistic processing engine, as we assume that all indexes are processed entirely when computing intersections. If an IR engine can compute the intersec-tion between a small and a very large index by scanning the small index and doing random lookups/skips into the large index (e.g. see [13], Chapter 4.3), then match-list entries for such combinati-ons need not be materialized, potentially reducing the size of the materialized structures dramatically.
Finally, we studied the effect of the probabilistic techniques de-scribed in Section 4.1. Here, we used sampling rates of 1% 0 . 1% , i.e. for each inverted index we maintained the 1% (or 0.1%) smallest elements (according to the  X  ID ) and used these samples to determine which nodes to materialize in the match list. For this ex-periments, we used the inverted indexes for a subset of 2000 words from the e-commerce corpus, which were evenly distributed across the spectrum of frequency-ranks.

The experimental results underline the robustness of the estima-tion: for the 1% sampling rate the match list constructed via proba-bilistic techniques contained 99 . 3% of the entries in the exact one  X  here, nearly all missing entries were due to underestimation of  X  X ar-ge result X  combinations for which we would otherwise materialize only the top-ranked results (a case that we can detect and correct at build-time, but did not for this experiment); these in turn then made materialization of some larger keyword-combinations unnecessary. Compared to the exact list, the probabilistic technique materialized indexes for 0 . 08% additional keyword combinations. Interestingly, for 0 . 1% sampling, we achieved almost the same numbers, con-structing 99 . 2% of the entries in the correct match list.
This paper proposed a multi-dimensional index structure that improves latencies for intersecting postings. In the general case, multi-dimensional indexes consume exponential space, which is prohibitive. However, there are some important special cases that include many of the collections of interest to the Information Re-trieval community, where multi-dimensional indexes are more pro-mising, especially when appropriate care is taken in deciding which indexes to materialize.

We introduced a cost model to decide what to materialize. The cost model, in junction with various power-law assumptions, led to a triage process, where keywords were assigned to three tiers based on document frequency. The most frequent words require extensive indexing, but fortunately, there are not too many high frequency words. There are more words in the middle tier, at most one of which can occur in a query for which we materialize the result. The vast majority of keywords are assigned to the low frequency tier. No additional indexes beyond standard inverted indexes are required for these low frequency keywords.

The cost model was primarily motivated by latency considerati-ons. We do not have any guarantees on space, though space is not prohibitive for the collections of short documents we have looked at thus far. The experiments with the AQUAINT corpus suggest that space requirements also depend on a few other factors such as document length 5 .
