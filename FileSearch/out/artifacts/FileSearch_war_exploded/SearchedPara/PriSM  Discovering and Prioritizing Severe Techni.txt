 Online forums provide a channel for users to report and discuss problems related to products and troubleshooting, for faster resolution. These could garner negative publicity if left unattended by the companies. Manually monitoring these massive amounts of discussions is laborious. This pa-per makes the first attempt at collecting issues that require immediate action by the product supplier by analyzing the immense information on forums. Features that are specific to forum discussions, in conjunction with linguistic cues help in capturing and better prioritizing issues. Any attempt to collect training data for learning a classifier for this task will require enormous labeling effort. Hence, this paper adopts a co-training approach, which uses minimal manual label-ing, coupled with linguistic features extracted using a set-expansion algorithm to discover severe problems. Further, most distinct and recent issues are obtained by incorporat-ing a measure of  X  X entrality X ,  X  X iversity X  and temporal aspect of the forum threads. We show that this helps in better pri-oritizing longstanding issues and identify issues that need to be addressed immediately.
 H.3.5 [ On-line Information Services ]: Web-based ser-vices Discovering and Prioritizing Severe Technical Issues
Online social media sites and discussion forums have rapidly become the primary channel for users to share views on is-sues ranging from troubleshooting hardware to deciding on what car to buy. Forums encourage user participation and simultaneously reduce demand on the companies X  service centers. Users today prefer to post product-related issues directly on forums as it often facilitates quicker responses.
In addition to providing benefits to users, these sites have become an excellent source of information that can be mined to estimate consumer sentiment. However, as a result of the rapid growth in the popularity and access to these so-cial media sites, they are often unstructured and difficult to process automatically, requiring effective use of both natural language processing and data mining techniques.

One alternative to online forums are, product manuals which typically contain frequently asked questions (FAQs). Unfortunately, these often do not contain solutions to all problems that may arise. An example of this is the reception problems that were faced by consumers on the release of Apple X  X  R  X  iPhone4 1 . A workaround for this was posted on forums much before official FAQs or solutions were released.
Discussion threads on various issues related to a product grow rapidly when a product is launched or an update is released (Apple R  X  discussions has  X  300k threads on iPhone related issues 2 ), making it a formidable task to monitor them manually. Hence, a technique to automatically obtain some kind of  X  X riority X  assignment is required to help developers prioritize these fixes. To the best of the authors X  knowledge, our work is the first attempt at both discovering and Priori-tizing Severe issues by Mining (PriSM) discussion forums in order to extract actionable business intelligence.

Specifically, the contributions of this paper are as follows: 9 1 http://www.pcworld.com/article/199807/iphone_4_ antenna_problems_mar_apples_big_day.html 9 2 https://discussions.apple.com/community/iphone Table 1: Apple R  X  Discussions: Threads clustered and ranked based on density of the clusters.
Typically companies manually assess the severity of an issue from bug reports. This can be tedious, expensive and time-consuming. A few attempts have been made in the past towards mining and prioritizing bug fixes from bug reports [5]. However, no attempts have been made at extracting and prioritizing problems faced by users on forums.

An obvious approach to prioritizing issues is to use a mea-sure of  X  X requency X  [3]. We argue that frequency alone is not a good indicator for this task; in fact, many frequently asked problems have solutions in product manuals or in the offi-cial FAQ lists and these problems are clearly not a high priority. To demonstrate this, we clustered the threads in our crawl of iPhone forum data to group similar threads to-gether. Sample threads from the biggest clusters, indicating highest frequency, are shown in Table 1. Clearly, many of these problems (shown in bold) are already in the FAQ list provided by the iPhone manufacturer. [5] used just lexical features to train supervised classifiers. We show that the performance of classifiers trained on lexical features can be boosted by incorporating forum-specific fea-tures (Section 4.1) (which are not present in bug reporting tools). These simple experiments highlight the fact that, in order to extract severe issues that need the manufacturer X  X  attention, other cues from the discussion threads are crucial.
We propose using forum-specific features and other lin-guistic cues to automatically classify issues from forum dis-cussions. However, the amount of labeled data available to learn these classifiers was small. We use Co-training [1], which is a learning method that has been shown to work well when the amount of labeled data is insufficient.
We then rank these severe threads to find distinct issues, using a measure of  X  X entrality X  and  X  X iversity X  to prevent re-dundant (or very similar) issues from receiving a high rank. At the same time, the issues that are ranked the highest are the ones that form a good representation of all related is-sues. We use GRASSHOPPER (Graph Random-walk with Absorbing StateS that HOPs among PEaks for Ranking) [8] that ranked items in a way that highly ranked items formed good representatives of their local groups (items similar to the highly ranked item) and the top ranked items were di-verse items. In this paper, we extend this approach by incor-porating the  X  X emporal aspect X  of the issues by discounting older threads to better prioritize longstanding issues as well as identify issues that need immediate attention by appro-priately modifying the GRASSHOPPER algorithm.
This paper proposes a bottom-up approach to discover problems that require immediate attention, from social me-dia discussions. The first step is to decide at a discussion (thread) level, if that thread is discussing an issue that could be severe. Once such potentially severe issues have been identified, they are aggregated, to discover and rank issues globally, according to their severity. In the rest of the pa-per,  X  X everity of an issue X  refers to the degree of impact the issue has on the usability of a product. For e.g., a bug that crashes an iPhone frequently can be said to have high severity. Severity level of different issues vary with product, functionality and the domain.

Several cues to determine the severity of an issue exist in the posts. To decide the severity at a thread level, the key is to identify these  X  X everity indicators X . The indica-tors are grouped into two distinct groups: Lexical (LEX), which takes cues from words or phrases used in the text of the discussion, and Structural (STRUCT), which uses the structure and forum-specific features of the discussion. Consider the thread (predicted as severe) in Figure 1: LEX : The following features are obtained using cues from the phrases mentioned in the discussions:
STRUCT : The following features are obtained using struc-tural and forum-specific features of the discussion thread:
The fact that severity indicators can be grouped into inde-pendent sets of features, paves way for using the co-training methodology [1] with one classifier C lex trained on LEX fea-Figure 1: LEX and STRUCT cues in a sample Dis-cussion Thread indicating a high severity level.
 tures and another C struct , trained on STRUCT features. Each of L 1 thru L 4 of LEX are employed as dictionaries, the building of which is discussed in Section 3.2. The fea-tures to C lex are the frequency counts for terms/phrases that matched in these dictionaries. Implementation of C lex and C struct classifiers used LIBSVM 3 .
Given a small set of seed entities, set expansion discov-ers other entities that may belong to the same concept set. The dynamic thresholding algorithm, SEISA [4], was im-plemented for this. SEISA has been shown to outperform Random walk-based strategies [4] such as, SEAL, especially, when used on inherently noisy general-purpose web data and when the size of the seed set is small.
 Terms from discussion threads obtained from a crawl of Apple X  X  R  X  iPhone forums (more details in Section 4) were used for this purpose. For example, with a seed set contain-ing,  X  X rashing X  and  X  X reezing X , the algorithm retrieves other entities such as,  X  X seless X ,  X  X ess X , etc. Terms supporting fea-tures, L 1 , L 2 , L 3 and L 4 , were obtained with small seed sets. The candidate terms for L 1 and L 2 were restricted to verbs, adjectives and adverbs 4 . Candidate terms for L 3 and L were limited to adverbs and adjectives. Additionally, when-ever a candidate term was preceded by the adverb,  X  X ot X  (as in the phrase:  X  not useful X ), the  X  X ot X  was also consid-ered as part of the candidate term. Each stemmed term was modeled as a graph node on the left and the context (prefix and suffix of the term, up to 3 tokens) on the right. Sample output of set expansion for LEX is in Table 2, along with the sizes of the seed and the expanded sets. 9 3 www.csie.ntu.edu.tw/  X  cjlin/libsvm/ 9 4 http://nlp.stanford.edu/software/tagger.shtml
The co-training methodology of [1] allows using inexpen-sive unlabeled data to augment a much smaller labeled set for achieving better accuracies. Here, each example has two distinct views and the assumption is that either of the two views is sufficient for learning if enough data were available. Both these views are used together for achieving better per-formance when a much smaller labeled set is available. Two learning algorithms are trained separately on the two dif-ferent views of the examples, and each algorithm predicts possible labels for the examples in the unlabeled set. The prediction of one algorithm is used to augment the train-ing data of the other learning algorithm and vice versa. A PAC-style analysis of this setting is given in [1] and has been shown to provide significant improvements by adopting this approach on the unlabeled examples.

In this paper, C lex and C struct are the conditionally in-dependent views of the data. Classifiers trained in the final iteration are used to retrieve the final list of  X  X igh X  severity threads. To obtain a global ranking for issues from these threads, all threads on which the classifiers disagreed on the label are filtered out. Then the confidence of prediction on the rest of the threads is calculated as P ( label | C P ( label | C struct ) using the assumption of conditional inde-pendence, and those with confidence lower than the thresh-old, t conf , are filtered out. The remaining threads are then clustered on their title and ranked based on the cluster sizes.
The previous step classified examples as having  X  X igh X  or  X  X ow X  severity. To find the most severe issues related to the product, one could sort the issues based on the classifier X  X  score ( P ( label | C lex )  X  P ( label | C struct )). Table 3 shows the top ranked issues sorted based on the scores obtained from the classifier. The first column indicates the entity to which the issue is related to, the second column corresponds to the Topic (mentioned in the forums) and the third column sum-marizes the issue posted by the thread initiator. We see that many of the issues are related to each other. For example, the first and the second issue are both related to the  X  X at-tery draining X  issue. Since it is uninformative to view such similar items, there is a need to rerank these issues such that the list only contains  X  X iverse X  issues covering many distinct problems. At the same time, a highly ranked issue should be a representative of a group of related issues ( X  X entrality X ).
For this, an adjacency matrix A of size n  X  n (where n is the number of issues obtained from the previous section) is built with each entry A i,j representing the cosine similarity between, issue i and issue j . W is the score obtained from the classifiers from the previous section ( P ( label | C P ( label | C struct )). A and W are discounted by  X  Y 2 , where, y represents the difference in the year in which the thread i was posted and the present year. For example, if the present year is 2012 and the thread was posted in 2007, the differ-ence y i = 5. When calculating the similarity between node i and node j , each of their time differences, i.e., y i and y added to obtain y . This gives a higher score to issues posted more recently, as issues posted much earlier may already have been resolved by the producers of the product.
One could use only the most recent issues to perform diversity-based ranking under the assumption that issues posted recently are more severe than older issues as they may have already been solved. However, this fails to iden-Figure 2: F-scores obtained with the two classifiers after each iteration of co-training. tify longstanding issues. Considering old as well as new is-sues boosts the scores of recurring issues due to their large number of occurrences. Ideally, we would want to have a balance between the two. The parameter,  X  2 can be set by the product producers. In this paper,  X  2 was set to 0.4.  X  P is normalized to obtain the probability transition matrix P .  X  .  X   X  in Equation 1 represents element-wise multiplication. P can also be viewed as a graph with n nodes and each entry in P , P i,j corresponds to the edge between node i and node We perform a random walk on this graph. Ranking issues just based on the stationary distribution obtained at the end of the walk, will have the same problem of lack of diversity. We use the algorithm proposed in [8] to obtain distinct is-sues. The topmost node ( g 1 ) obtained from the stationary distribution is converted into an absorbing state.[2] suggests a way to calculate the number of visits to each node before absorption. The node with the highest expected number of visits is ranked as the next prominent issue g 2 . The process is repeated until the top K nodes are selected.
Since there were no publicly available data sets with dis-cussions, the dataset was crawled from Apple R  X  Discussions with threads created during the period, 2007-2011. Out of about 147,000 discussion threads crawled, 270 threads were randomly chosen and manually labeled as  X  X igh X  or  X  X ow X  according to their severity level by annotators who were iPhone users. Of these, 20 were used as the training set (initial labeled data for co-training) and the remaining 250 were used as the test set. The remaining threads were used as the unlabeled data for our co-training experiments.
The threads were processed to remove common stopwords and stemmed using Porter Stemmer [6]. In each iteration of Co-training, 5 most confident predictions from each of C lex and C struct were added to the set of training examples. This process was repeated for 40 iterations. t conf was set to 0 . 7 which resulted in 512 threads which were then ranked based on centrality and diversity (as explained in Section 3.4).
To demonstrate the benefits of co-training, we show the accuracy of each of the classifiers for a few iterations on the 250 threads test set in Figure 2. We use the F-Score [7] as our accuracy measure. The results clearly show that each of the classifiers learn to improve their performance and correct Table 4: First row: F-score obtained on the LEX, STRUCT and Combined classifiers with supervised learning, using only the labeled examples. Second row: F-score obtained with these classifiers with the co-training approach. their mistakes using the predictions of the other classifier. For example, the STRUCT based classifier initially had an F-Score of 0, but using the additional (originally unlabeled) data labeled by the LEX classifier, it X  X  accuracy improves to 43 . 3. Similarly, the LEX classifier benefits substantially from the STRUCT classifier X  X  predictions.

We also analyzed the improvements that could be ob-tained over standard separate training of supervised clas-sifiers. The labeled set of 20 examples was used to train two classifiers-one trained only on LEX features, and the other trained on the STRUCT features. A combined classifier was also used by multiplying the probabilities of the predictions of the two classifiers ( P ( label | C combined ) = P ( label | C P ( label | C struct )) and the class ( X  X igh X  or  X  X ow X ) with the highest score was used. The resulting accuracies are given in the first row of Table 4. Accuracies of the individual classifiers obtained in the final iteration of co-training with the same initial labeled set are given in the second row of Table 4. As seen, a substantial gain in performance is ob-served with the use of the unlabeled data highlighting the effectiveness of co-training using our features.
Next, we study the value of time discounting and diversity ranking. Table 5 shows the top ranked results without the time discounting in Equation 1, and Table 6 shows the re-sults with time discounting. If very recent discussions need to be given higher importance, then  X  2 needs to be smaller.  X  2 in Equation 1 was set to 0.4.  X  1 was set to 0.6 to give a higher weight to the classifiers X  predictions. Variation of  X  ( &gt; 0 . 5) only resulted in variations of the ranks of the top few severe issues. However, the top issues shown in Table 6 re-mained within the Top 15 ranks. In general, this parameter may need to be controlled by the product producers.
To understand the usefulness of time discounting, we fur-ther analyzed the results in Table 5 and Table 6. A few of the issues in Table 5 correspond to issues with older versions of updates, also implied by the year in which the threads were created. Issues ranked high in Table 6 correspond to more recent issues. Considering other examples-the first version of iPhone did not support AVRCP 5 , a feature that allows remote control of playback functions on the iOS device from Bluetooth devices. However, since the newer versions sup-ported this capability, issues regarding the AVRCP disap-peared in Table 6 where time discounting is involved, giving higher preference to recently posted issues. Other issues, such as  X  X attery draining X ,  X  X Phone freezing X  have been re-curring issues, and they remain even with time discounting.
After the release of iPhone and subsequent long discus-sions on different social media about the issues that surfaced, 9 5 http://support.apple.com/kb/HT3647 several bloggers and market researchers have attempted to collate the most-talked about issues; for example, these web-pages 6 7 lists the most annoying problems and it can be noted that, Table 6 closely matches many of them.
This paper proposed an approach to automatically dis-cover severe issues from discussions with very minimal su-pervision from large amounts of semi-structured information available in forums. Linguistic cues and other forum specific features with minimal amounts of training data were used to train severity predictors. To prevent similar issues from all obtaining a high rank, a measure of  X  X iversity X  and  X  X en-trality X  was adopted. Temporal aspect of forum threads was used to further give higher preference to recently created threads which helped in prioritizing longstanding issues as well as identifying issues that need immediate attention. [1] A. Blum and T. Mitchell. Combining labeled and 9 6 http://iphoneappcafe.com/10-most-annoying-iphone-problems-and-how-to-solve-them/ (accessed Feb 3, 2012) 9 7 http://www.macnn.com/articles/10/07/28/ ios.said.to.be.unusable.on.the.iphone.3g/ [2] P. G. Doyle and J. L. Snell. Random walks and electric [3] A. Fourney, R. Mann, and M. Terry. Characterizing the [4] Y. He and D. Xin. Seisa: set expansion by iterative [5] A. Lamkanfi, S. Demeyer, Q. D. Soetens, and [6] M. F. Porter. Readings in information retrieval. chapter [7] C. J. van Rijsbergen. Information Retrieval (2nd ed.) . [8] X. Zhu, A. B. Goldberg, J. Van, and G. D.

