 Information retrieval (IR) is the task of retrieving, given a query, the documents relevant to the user from a large quantity of documents (Salton and McGill, 1983). IR has become very important in recent years, with the proliferation of large quanti-ties of documents on the world wide web. Many IR systems are based on some relevance score function R ( j,q ) which returns the relevance of document j to query q . Examples of such relevance score functions include term frequency-inverse document frequency (tf-idf) and Okapi BM25 (Robertson et al., 1992).
Besides the effect that documents containing more query terms should be more relevant (term fre-quency), the main effect that many relevance scores try to capture is that of inverse document frequency: the importance of a term is inversely related to the number of documents that it appears in, i.e. the popularity of the term. This is because popular terms, e.g. common and stop words, are often un-informative, while rare terms are often very infor-mative. Another important effect is that related or co-occurring terms are often useful in determining the relevance of documents. Because most relevance scores do not capture this effect, IR systems resort to techniques like query expansion which includes syn-onyms and other morphological forms of the origi-nal query terms in order to improve retrieval results; e.g. (Riezler et al., 2007; Metzler and Croft, 2007). In this paper we explore a probabilistic model for IR that simultaneously handles both effects in a prin-cipled manner. It builds upon the work of (Cow-ans, 2004) who proposed a hierarchical Dirichlet document model. In this model, each document is modeled using a multinomial distribution (making the bag-of-words assumption) whose parameters are given Dirichlet priors. The common mean of the Dirichlet priors is itself assumed random and given a Dirichlet hyperprior. (Cowans, 2004) showed that the shared mean parameter induces sharing of infor-mation across documents in the corpus, and leads to an inverse document frequency effect.

We generalize the model of (Cowans, 2004) by re-placing the Dirichlet distributions with Dirichlet tree distributions (Minka, 2003), thus we call our model the hierarchical Dirichlet tree . Related terms are placed close by in the vocabulary tree, allowing the model to take this knowledge into account when de-termining document relevance. This makes it unnec-essary to use ad-hoc query expansion methods, as re-lated words such as synonyms will be taken into ac-count by the retrieval rule. The structure of the tree is learned from data in an unsupervised fashion, us-ing a variety of agglomerative clustering techniques.
We review the hierarchical Dirichlet document (HDD) model in section 2, and present our proposed hierarchical Dirichlet tree (HDT) document model in section 3. We describe three algorithms for con-structing the vocabulary tree in section 4, and give encouraging experimental evidence of the superi-ority of the hierarchical Dirichlet tree compared to standard baselines in section 5. We conclude the pa-per in section 6. The probabilistic approach to IR assumes that each document in a collection can be modeled probabilis-tically. Given a query q , it is further assumed that relevant documents j are those with highest gener-ative probability p ( q | j ) for the query. Thus given q the relevance score is R ( j,q ) = p ( q | j ) and the doc-uments with highest relevance are returned.

Assume that each document is a bag of words, with document j modeled as a multinomial distri-bution over the words in j . Let V be the terms in the vocabulary, n of term w  X  V in document j , and  X  flat bility of w occurring in document j (the superscript  X  X lat X  denotes a flat Dirichlet as opposed to our pro-posed Dirichlet tree). (Cowans, 2004) assumes the following hierarchical Bayesian model for the docu-ment collection: In the above, bold face a = ( a is a vector with | V | entries indexed by w  X  V , and u is a uniform distribution over V . The generative process is as follows (Figure 1(a)). First a vector  X  with concentration parameter  X  . Then we draw the Dirichlet distribution with mean  X  flat tion parameter  X  . Finally, the term frequencies of the document are drawn from a multinomial distri-bution with parameters  X  flat
The insight of (Cowans, 2004) is that because the common mean parameter  X  flat duces dependencies across the document models in the collection, and this in turn is the mechanism for information sharing among documents. (Cowans, 2004) proposed a good estimate of  X  flat where n taining term w , i.e. the document frequency. Inte-grating out the document parameters  X  flat the probability of query q being generated from doc-ument j is: Where Const are terms not depending on j . We see that n n  X  + The inverse document frequency factor is directly related to the shared mean parameter, in that popular terms x will have high  X  flat ments to assign higher probability to x , and down weighting the term frequency. This effect will be inherited by our model in the next section. Apart from the constraint that the parameters should sum to one, the Dirichlet priors in the HDD model do not impose any dependency among the param-eters of the resulting multinomial. In other words, the document models cannot capture the notion that related terms tend to co-occur together. For exam-ple, this model cannot incorporate the knowledge that if the word  X  X omputer X  is seen in a document, it is likely to observe the word  X  X oftware X  in the same document. We relax the independence assump-tion of the Dirichlet distribution by using Dirichlet tree distributions (Minka, 2003), which can capture some dependencies among the resulting parameters. This allows relationships among terms to be mod-eled, and we will see that it improves retrieval per-formance. 3.1 Model Let us assume that we have a tree over the vocab-ulary whose leaves correspond to vocabulary terms. Each internal node k of the tree has a multinomial distribution over its children C ( k ) . Words are drawn by starting at the root of the tree, recursively picking a child l  X  C ( k ) whenever we are in an internal node k , until we reach a leaf of the tree which corresponds to a vocabulary term (see Figure 2(b)). The Dirich-let tree distribution is the product of Dirichlet dis-tributions placed over the child probabilities of each internal node, and serves as a (dependent) prior over the parameters of multinomial distributions over the vocabulary (the leaves).

Our model generalizes the HDD model by replac-ing the Dirichlet distributions in (1) by Dirichlet tree distributions. At each internal node k , define a hier-archical Dirichlet prior over the choice of the chil-dren: where u of node k , and each internal node has its own hy-perparameters  X  choosing child l if we are at internal node k . If the tree is degenerate with just one internal node (the root) and all leaves are direct children of the root we recover the  X  X lat X  HDD model in the previous sec-tion. We call our model the hierarchical Dirichlet tree (HDT). 3.2 Inference and Learning Given a term, the path from the root to the corre-sponding leaf is unique. Thus given the term fre-quencies n number of times n node k is known and fixed. The probability of all words in document j , given the parameters, is then a product of multinomials probabilities over internal nodes k : The probability of the documents, integrating out the  X  p ( { n j }|{  X  0 k } ) = (6)
Y The probability of a query q under document j , i.e. the relevance score, follows from (3): where the second product is over pairs ( kl ) where k is a parent of l on the path from the root to x .
The hierarchical Dirichlet tree model we pro-posed has a large number of parameters and hy-perparameters (even after integrating out the  X  since the vocabulary trees we will consider later typ-ically have large numbers of internal nodes. This over flexibility might lead to overfitting or to param-eter regimes that do not aid in the actual task of IR. To avoid both issues, we constrain the hierarchical Dirichlet tree to be centered over the flat hierarchi-cal Dirichlet document model, and allow it to learn only the  X  parameters.

We set {  X  tree, so that it induces the same distribution over vo-cabulary terms as  X  flat The hyperparameters of the local trees  X  timated using maximum a posteriori learning with likelihood given by (6), and a gamma prior with informative parameters. The density function of a Gamma( a,b ) distribution is where the mode happens at x = a  X  1 mode of the prior such that the hierarchical Dirichlet tree reduces to the hierarchical Dirichlet document model at these values: and b&gt; 0 is an inverse scale hyperparameter to be tuned, with large values giving a sharp peak around  X  the results we report in the next section are not sen-sitive to b . This prior is constructed such that if there is insufficient information in (6) the MAP value will simply default back to the hierarchical Dirichlet doc-ument model.
 mization method to find the MAP values, where the gradient of the likelihood part of the objective func-tion (6) is:  X  log p ( { n where  X ( x ) :=  X  log  X ( x ) / X  X  is the digamma func-tion. Because each  X  the optimization is very fast (approximately 15-30 minutes in the experiments to follow on a Linux ma-chine with 1.8 GH CPU speed). The structure of the vocabulary tree plays an impor-tant role in the quality of the HDT document model, Algorithm 1 Greedy Agglomerative Clustering since it encapsulates the similarities among words captured by the model. In this paper we explored using trees learned in an unsupervised fashion from the training corpus.

The three methods are all agglomerative cluster-ing algorithms (Duda et al., 2000) with different similarity functions. Initially each vocabulary word is placed in its own cluster; each iteration of the al-gorithm finds the pair of clusters with highest sim-ilarity and merges them, continuing until only one cluster is left. The sequence of merges determines a binary tree with vocabulary words as its leaves.
Using a heap data structure, this basic agglom-erative clustering algorithm requires O ( n 2 log( n ) + sn 2 ) computations where n is the size of the vocab-ulary and s is the amount of computation needed to compute the similarity between two clusters. Typi-cally the vocabulary size n is large; to speed up the algorithm, we use a greedy version described in Al-gorithm 1 which restricts the number of cluster can-didates to at most m  X  n . This greedy version is faster with complexity O ( nm (log m + s )) . In the experiments we used m = 500 .

Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. Each word is represented by the frequencies of various words in a window around each occurrence of the word. The similarity between two words is com-puted to be a symmetrized KL divergence between the distributions over neighboring words associated with the two words. For a cluster of words the neigh-boring words are the union of those associated with each word in the cluster. Dcluster has been used extensively in text classification (Baker and McCal-lum, 1998).

Probabilistic hierarchical clustering (Pcluster) (Friedman, 2003). Dcluster associates each word with its local context, as a result it captures both semantic and syntactic relationships among words. Pcluster captures more relevant semantic relation-ships by instead associating each word with the doc-uments in which it appears. Specifically, each word is associated with a binary vector indexed by doc-uments in the corpus, where a 1 means the word appears in the corresponding document. Pcluster models a cluster of words probabilistically, with the binary vectors being iid draws from a product of Bernoulli distributions. The similarity of two clus-ters c i.e. two clusters of words are similar if their union can be effectively modeled using one cluster, rela-tive to modeling each separately. Conjugate beta pri-ors are placed over the parameters of the Bernoulli distributions and integrated out so that the similarity scores are comparable.

Brown X  X  algorithm (Bcluster) (Brown et al., 1990) was originally proposed to build class-based language models. In the 2-gram case, words are clustered such that the class of the previous word is most predictive of the class of the current word. Thus the similarity between two clusters of words is defined to be the resulting mutual information be-tween adjacent classes corrresponding to a sequence of words. 4.1 Operations to Simplify Trees Trees constructed using the agglomerative hierarchi-cal clustering algorithms described in this section suffer from a few drawbacks. Firstly, because they are binary trees they have large numbers of internal nodes. Secondly, many internal nodes are simply not informative in that the two clusters of words below a node are indistinguishable. Thirdly, Pcluster and Dcluster tend to produce long chain-like branches which significantly slows down the computation of the relevance score.

To address these issues, we considered operations to simplify trees by contracting internal edges of the tree while preserving as much of the word relation-ship information as possible. Let L be the set of tree leaves and  X  ( a ) be the distance from node or edge a to the leaves: In the experiments we considered either contracting edges 3 close to the leaves  X  ( a ) = 1 (thus remov-ing many of the long branches described above), or edges further up the tree  X  ( a )  X  2 (preserving the informative subtrees closer to the leaves while re-moving many internal nodes). See Figure 2. (Miller et al., 2004) cut the BCluster tree at a cer-tain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. They use the tree to get extra features for a discrimina-tive model to tackle the problem of sparsity X  X he features obtained from the new tree do not suffer from sparsity since each node has several words as its leaves. This technique did not work well for our application so we will not report results using it in our experiments. In this section we present experimental results on field dataset consists of 1,400 documents and 225 queries; its vocabulary size after stemming and re-moving stop words is 4,227. The Medline dataset contains 1,033 documents and 30 queries with the vocabulary size of 8,800 after stemming and remov-ing stop words. We compare HDT with the flat HDD model and Okapi BM25 (Robertson et al., 1992). Since one of our motivations has been to get away from query expansion, we also compare against Okapi BM25 with query expansion. The new terms to expand each query are chosen based on Robertson-Sparck Jones weights (Robertson and Sparck Jones, 1976) from the pseudo relevant docu-ments. The comparison criteria are (i) top-10 preci-sion, and (ii) average precision. 5.1 HDT vs Baselines All the hierarchical clustering algorithms mentioned in section 4 are used to generate trees, each of which is further post-processed by tree simplification op-erators described in section 4.1. We consider (i) contracting nodes at higher levels of the hierarchy (  X   X  2 ), and (ii) contracting nodes right above the leaves (  X  = 1 ).

The statistics of the trees before and after post-processing are shown in Table 1. Roughly, the Dcluster and BCluster trees do not have long chains with leaves hanging directly off them, which is why their average depths are reduced significantly by the  X   X  2 simplification, but not by the  X  = 1 sim-plification. The converse is true for Pcluster: the trees have many chains with leaves hanging directly off them, which is why average depth is not reduced as much as the previous trees based on the  X   X  2 simplification. However the average depth is still re-duced significantly compared to the original trees.
Table 1 presents the performance of HDT with different trees against the baselines in terms of the top-10 and average precision (we have bold faced the performance values which are the maximum of each column). HDT with every tree outper-forms significantly the flat model in both datasets. More specifically, HDT with (original) BCluster and PCluster trees significantly outperforms the three baselines in terms of both performance measure for the Cranfield. Similar trends are observed on the Medline except here the baseline Okapi BM25 with performed by HDT with BCluster tree.

To further highlight the differences among the methods, we have shown the precision at particular recall points on Medline dataset in Figure 4 for HDT with PCluster tree vs the baselines. As the recall increases, the precision of the PCluster tree signifi-cantly outperforms the flat model and BM25. We at-tribute this to the ability of PCluster tree to give high scores to documents which have words relevant to a query word (an effect similar to query expansion). 5.2 Analysis It is interesting to contrast the learned  X  of the clustering methods. These  X  relations on the probabilities of the children under k in an interesting fashion. In particular, if we com-pare  X  implies that the probabilities of picking one of the children of k (from among all nodes) are positively correlated, while a smaller value of  X  ative correlation. Roughly speaking, this is because drawn values of  X  to be closer to uniform (relative to the flat Dirichlet) thus if we had picked one child of k we will likely pick another child of k .

Figure 3 shows scatter plots of  X  sus  X  Firstly, smaller values for both tend to be associ-ated with lower levels of the trees, while large val-ues are with higher levels of the trees. Thus we see that PCluster tend to have subtrees of vocabu-lary terms that are positively correlated with each other X  X .e. they tend to co-occur in the same docu-ments. The converse is true of DCluster and BClus-ter because they tend to put words with the same meaning together, thus to express a particular con-cept it is enough to select one of the words and not to choose the rest. Figure 5 show some fragments of the actual trees including the words they placed together and  X  for their internal nodes. Moreover, visual inspection of the trees shows that DCluster can easily misplace words in the tree, which explains its lower perfor-mance compared to the other tree construction meth-ods.

Secondly, we observed that for higher nodes of the tree (corresponding generally to larger values of  X  higher levels of the tree exhibit negative correlation. This is reasonable, since if the subtrees capture pos-itively correlated words, then higher up the tree the different subtrees correspond to clusters of words that do not co-occur together, i.e. negatively corre-lated. We presented a hierarchical Dirichlet tree model for information retrieval which can inject (semantical or syntactical) word relationships as the domain knowl-edge into a probabilistic model for information re-trieval. Using trees to capture word relationships, the model is highly efficient while making use of both prior information about words and their occur-rence statistics in the corpus. Furthermore, we inves-tigated the effect of different tree construction algo-rithms on the model performance.

On the Cranfield dataset, HDT achieves 26.85% for average-precision and 32.40% for top-10 preci-sion, and outperforms all baselines including BM25 which gets 25.66% and 31.24% for these two mea-sures. On the Medline dataset, HDT is competi-tive with BM25 with Query Expansion and outper-forms all other baselines. These encouraging results show the benefits of HDT as a principled probabilis-tic model for information retrieval.

An interesting avenue of research is to construct the vocabulary tree based on WordNet, as a way to inject independent prior knowledge into the model. However WordNet has a low coverage problem, i.e. there are some words in the data which do not ex-ist in it. One solution to this low coverage problem is to combine trees generated by the clustering algo-rithms mentioned in this paper and WordNet, which we leave as a future work.

