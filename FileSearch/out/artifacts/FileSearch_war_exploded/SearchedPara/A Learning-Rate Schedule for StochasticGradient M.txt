
Wei-Sheng Chin, Yong Zhuang, Yu-Chin Juan, and Chih-Jen Lin Given an incomplete matrix R  X  R m  X  n , matrix factorization (MF) finds two denotes the indices of the existing elements in R , r u,v row and the v th column in R , p u  X  R k is the u th column of P , is achieved by solving the following non-convex problem where  X  is a regularization parameter. Note that the process to solve P and Q them as the test set. Once P and Q are found, root-mean-square error (RMSE) on the test set is often used as an evaluation criterion. It is defined as where  X  test represents the indices of the elements belonging to test set. single element r u,v is sampled to obtain the following sub-problem. The gradient of ( 3 )is is updated along the negative direction of the sampled gradient, that all r u,v have been handled once. Algorithm 1 summarizes the SG method for matrix factorization. In SG, the learning rate can be fixed as a constant while some schedules dynamically adjust  X  in the training process for faster training process for MF.
 as follows. Section 2 investigates the existing schedules for matrix factor-ization and a per-coordinate sched-ule for online convex problems. Note that a per-coordinate schedule assigns each variable a distinct learning rate.
 We improve upon the per-coordinate schedule and propose a new schedule in Section 3 . In Section 4 , experimen-tal comparisons among schedules and state-of-the-art packages are exhibited. In summary, our contributions include: 1. We propose a new schedule that outperforms existing schedules. introduced in Section 2.2 . 2.1 Existing Schedules for Matrix Factorization Fixed Schedule (FS). The learning rate is fixed throughout the training pro-for example, [ 8 ].
 Monotonically Decreasing Schedule (MDS). This schedule decreases the learning rate over time. At the z th outer iteration, the learning rate is general optimization problems, two related schedules [ 6 , 10 , 12 ]are for faster convergence.
 Bold-Driver Schedule (BDS). Some early studies on neural networks found  X  2.2 Per-Coordinate Schedule (PCS) Some recent developments discuss the possibility to assign the learning rate mization. For matrix factorization, if r u,v is sampled, ADAGRAD adjusts two matrices G u and H v using and then updates the current model via inversion in ( 8 ) is expensive. That is, G u and H v are maintained by rate is related to the squared sum of past gradient elements. proposed. However, we focus on ADAGRAD in this study because the compu-tational complexity per iteration is the lowest among them. Inspired by PCS, a new schedule, reduced per-coordinate schedule (RPCS), is proposed in Section 3.1 . RPCS can reduce the memory usage and computa-tional complexity in comparison with PCS. Then, in Section 3.2 we introduce a our argument. See Section 4 for the experimental settings such as parameter selection and the data sets used. 3.1 Reduced Per-Coordinate Schedule (RPCS) The cost of implementing FS, MDS, or BDS schedules is almost zero. How-nate of p u and q v has its own learning rate. Maintaining G O are needed for calculating and using diagonal elements of G These overheads can be dramat-ically reduced if we apply the same learning rate for all elements in p u (or q ). Specifically, at each iteration, G u and H v are reduced from matrices to scalars. Instead of ( 9 ), G u and H v are now updated by G  X  G u + In other words, the learning rate of p u or q v is the average over its k coor-dinates. Because each p u or q v has one learning rate, only ( m + n ) addi-( m + n ) k of PCS. Furthermore, the learning rates, G and H v by ( 10 ). Note that the O ( k )costof( 10 ) is comparable to that of implementations. 1. Store g u and h v .  X  X  for loop to calculate g u , h v and G u ,H v . Then g u stored.  X  X  for loop to update p u , q v by ( 8 ). 2. Calculate g u and h v twice.  X  X  for loop to calculate g u , h v and then G u ,H v .  X  X  for loop to calculate g u , h v and update p u , q v by ( 8 ). Clearly, the first approach requires extra storage and memory access. For the G single for loop to calculate g u and h v , update p u and and calculate g T u g u and h T v h v to obtain new G u and H Thus, the cost of Algorithm 2 is comparable to that of a standard stochastic gradient iteration.
 relationship between RMSE and the number of outer iterations. The convergence speeds of PCS and RPCS are almost identical. Therefore, using the same rate each iteration becomes cheaper, a comparison on the running time in Figure 2 shows that RPCS is faster than PCS.
 is reasonable for RPCS. Assume p u  X  X  elements are the same, h their expected values are the same. Consequently, p u  X  X  (or are identical in statistics and hence our explanation can be applied. 3.2 Twin Learners (TL) Conceptually, in PCS and RPCS, the decrease of a learning rate should be con- X  split the elements of p u (or q v )totwogroups { 1 ,...,k The two groups respectively maintain their own factors, G convergence. We follow the setting in Section 3.1 to use G H v of the previous iteration. Therefore, at each iteration, we have 1. One for loop going through the first k s elements to calculate ( update ( p u ) 1: k s ,( q v ) 1: k s , and obtain the next G ( g G Figure 3 shows the average learning rates of RPCS (TL is not applied), and slow and fast learners (TL is applied) at each outer iteration. For RPCS, the ensure fast learning. A comparison between RPCS with and without TL is in Figure 4 . Clearly, TL is very effective. In this paper, we fix k choice of k s . We conduct experiments to exhibit the effectiveness of our proposed schedule. factorization and non-negative matrix factorization (NMF) in Sections 4.4 and 4.5 , respectively. 4.1 Implementation data storage, and Streaming SIMD Extensions (SSE) are applied to accelerate the computation.
 implemented in a naive way by writing 1/sqrt(  X  ) in C++. Fortunately, SSE inverse square roots for single-precision floating-point numbers. 4.2 Settings Data Sets. Six data sets listed in Table 1 are used. We use the same train-official training/test sets for Webscope-R1 and Webscope-R2 original data. Within this sub-sampled data set, we randomly sample 1% as the test set, and using the remaining for training.
 Platform and Parameters. We run the experiment on a machine with 12 cores on two Intel Xeon E5-2620 2.0GHz processors and 64 GB memory. We ensure that no other heavy tasks are running on the same computer. A higher number of latent features often leads to a lower RMSE, but needs able balance between speed and RMSE, so we use it for all data sets. For the regularization parameter, we select the one that leads to the best test RMSE as well. A similar normalization procedure has been used in [ 18 ]. schedule are listed in Table 2 . They are the fastest setting to reach 1.005 times the best RMSE obtained by all methods under all parame-ters. We consider such a  X  X ear-best X  RMSE to avoid selecting a parame-ter that needs unnecessarily long run-ning time. Without this mechanism, our comparison on running time can become misleading. Note that PCS and RPCS shares the same  X  0 . For BDS, we follow [ 4 ]tofix  X  =1 . 05 and three parameters  X  0 ,  X  ,and  X  together. 4.3 Comparison Among Schedules BDS, PCS, and RPCS. RPCS outperforms other schedules including the PCS schedule that it is based upon. 4.4 Comparison with State-of-the-art Packages on Matrix We compare the proposed schedule (implemented based on LIBMF as LIBMF++ ) with the following packages.  X  The standard LIBMF that implements the FS strategy.  X  An SG-based package NOMAD [ 19 ] that has claimed to outperform  X 
LIBPMF : 5 it implements a coordinate descent method CCD++ [ 17 ]. For all packages, we use single-precision storage 6 and 12 threads. The com-to MDS for MovieLens and Webscope-R1 .Wedonotrun NOMAD on and
Hugewiki because of the memory limitation. Taking the advantage of the proposed schedule RPCS, LIBMF++ is significantly faster than NOMAD outperforms LIBMF and CCD++ , but our experiments show an opposite result. We think the reason may be that in [ 19 ], 30 cores are used and may have comparatively better performance if using more cores. 4.5 Comparison with State-of-the-art Methods for Non-negative Matrix Factorization (NMF) non-negative. The optimization problem is subject to P du  X  0 ,Q dv  X  0 ,  X  d  X  X  1 ,...,k } ,u  X  X  1 ,...,m in
LIBPMF [ 5 ] solves NMF by projecting the negative value back to zero at each can be applied to NMF. We compare them in Figure 7 .
 verge slower for NMF. This result seems to be reasonable because NMF is a more complicated optimization problem. Interestingly, we see the convergence degradation is more severe for CCD++ ( LIBPMF )thanSG( LIBMF to the rich experiments conducted. By using the proposed method, an extension codes are publicly available at loss and squared hinge loss.

