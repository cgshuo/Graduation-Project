 An important problem in data mining is detecting changes in large data sets. Although there are a variety of change detection algo-rithms that have been developed, in practice it can be a problem to scale these algorithms to large data sets due to the heterogeneity of the data. In this paper, we describe a case study involving payment card data in which we built and monitored a separate change de-tection model for each cell in a multi-dimensional data cube. We describe a system that has been in operation for the past two years that builds and monitors over 15,000 separate baseline models and the process that is used for generating and investigating alerts using these baselines.
 Categories and Subject Descriptors: G.3 [Probability and Statis-tics]: Statistical computing, statistical software; I.5.1 [Models]: Statistical Models General Terms: Algorithms Keywords: baselines, data quality, change detection, cubes of models It is an open and fundamental research problem to detect interest-ing and actionable changes in large, complex data sets. In this pa-per, we describe our experiences and the lessons learned over the past three years developing and operating a change detection sys-tem designed to identify data quality and interoperability problems for Visa International Service Association ( X  X isa X ). The change Illi noisatChicago .Pleas esendcorrespondenc eabou tthispape rto him.
 detection system produces alerts that are further investigated by an-alysts.
 The problem is difficult because of the following challenges: 1. Visa X  X  data is high volume, heterogeneous and time varying. 2. Alerts arising from the change detection system generally re-In this paper, we describe our experiences using a methodology for addressing these challenges. The methodology we use is to build a very large number of very fine grained baselines, one for each cell in a multi-dimensional data cube. We call this approach Change Detection using Cubes of Models or CDCM.
 For example, we built separate baseline models for each different type of merchant, for each different member bank, for each different field value, etc. In total, over 15,000 different baseline statistical models are monitored each month and used to generate alerts that are then investigated by analysts.
 We believe that this paper makes the following research contribu-tions: 1. First, we have highlighted an important category of data min-2. Second, we have described our experiences implementing and Section 2 contains some background on payment card transactions. Section 3 describes the Change Detection using Cubes of Models Figure 1. This is an example of one of the measures monitored in this project. This graph shows how the ratio of declined transactions varies for one of the Merchant Category Codes (MCC) monitored. Note the daily, weekly and monthly varia-tion in the data. This variation, which is typical of the measures tracked, is one of the reasons detecting changes in this data is challenging. The vertical axis scale has been omitted due to con-fidentiality reasons. (CDCM) algorithm that we intr oduce. Section 4 describes some typical alerts detected by the system we developed and deployed. Section 5 describes some of the lessons learned during the first few years of deployment. Section 6 describes some related work. Sec-tion 7 is the summary and conclusion. We begin by providing some background on payment card transac-tions that will make this paper more self-contained. In this section, we define cardholder, merchant, acquiring bank, and issuing bank and describe the major steps involved when using a payment card. 1. A transaction begins when cardholder purchases an item at a 2. The merchant has a relationship with a bank called the acquir-3. The acquiring bank has a relation with a financial payment 4. The payment system processes the transaction and passes the 5. The issuing bank processes the transaction and determines if 6. For each of these cases, the path is then reversed and the trans-Our problem was to use baselines and change detection algorithms to help detect data and interoperability problems at Visa [5]. Pay-ment data arrives at Visa from millions of merchant locations worldwide. For the year ending September 30, 2006, total annual global card sales volume was over USD $4.45 trillion. 1 Payment data is processed through risk management rules set by over 20,000 individual member banks (issuing and acquiring banks). These rules determine if a payment authorization request from a merchant either is approved or rejected by the paying bank.
 For this problem, we built separate baselines for a variety of data fields, for each member bank, and for thousands of merchants. Overall, over 15,000 separate baselines are currently used each month to monitor payment card transactions at Visa. Note: The examples in this section are hypothetical and only used for the purposes of illustrating how to define baselines. A payment card transaction typically includes a number of fields, such as information about the point of services (POS) environment, the merchant X  X  type of business and location, the cardholder X  X  iden-tity, the transaction currency, the t ransaction amount, and bank rout-ing information.
 We begin with an informal description of baselines based upon a simplified example. In this simplified example, assume that one of the fields of interest describes characteristics of the point of service (POS). Specifically, we assume that this field can take the following (hypothetical) values: 00, 01, 02, 03, and 04.
 For an observation period of a week, assume that the frequency of these values for a certain acquirer is given by the first table in Figure 2. Later, during the monitoring, assume that distribution is instead given by the right hand table in this figure. The observed distribution in Figure 2 is similar, except the value 04 is six times more likely in the observed distribution compared to the baseline distribution, a lthough in both cases the values 02, 03 and 04 still as a whole contribute less than 3% of the distribution.
 The challenge for detecting significant changes is that the distribu-tions depend upon many factors, including the region, the season, the specific merchant, and the specific issuer and acquirer. In this section, we describe a methodology called Change Detection using Cubes of Models or CDCM that is designed to detect changes in large, complex data sets. commercial funds transfers in China As reported by member finan-cial institutions globally and therefore may be subject to change. Figure 2. The distribution on the left is the baseline distribution. The distribution on the right is the observed distribution. In this example, the value 04 is over 6x more likely in the observed distribution, although the two dominant values 00 and 01 still account for over 97% of the distribution. Change detection models are a standard approach for detecting de-viations from baselines [3].
 We first describe the cumulative sum or CUSUM change detection algorithm [3]. We assume that we know the mean and variance of a distribution representing normal behavior, as well as the mean and variance of another distribution representing behavior that is not normal.
 More explicitly, assume we have two Gaussian distributions with mean  X  i and variance  X  2 i , i = 0 , 1. The log odds ratio is then given by and can now define a CUSUM algorithm as follows [3]: An alert is issued where the Z n exceeds a threshold.
 Quite often the statistical distribution of the anomalous distribution is not known. In this case, if the change is reflected in the mean of the observations and the standard deviation is the same pre-and post-change, the generalized likelihood ratio or GLR algorithm can be used [3]: where  X  0 is the mean of the normal distribution and  X  is the stan-dard deviation of the both the normal and abnormal distributions, which are assumed to be Gaussian. Here k is fixed and determines the size of the window used to compute the score. Again, the detec-tion procedure is to announce a change at the first up-crossing of a threshold by the GLR score. The basic idea of the CDCM algorithm is that for each cell in a multi-dimensional data cube, we estimate a separate change detec-tion model.
 For the purposes here, we can define a data cube as usual, namely a multi-dimensional representation of data in which the cells contain measures (or facts) and the edges represent data dimensions which are used for reporting the data.
 We defi ne a cube of models as a data cube in which each cell is associated with a baseline model. In our model, we assume that there are a stream of events, which in our case are transactions, and each event can be assigned to one or more cells in cube of models. For example, for each project, each transaction is assigned to the appropriate cell(s), as determined by one of six regions, by one of over 800 Merchant Category Codes (MCCs), by one of 8 terminal types, etc. We also assume that vari-ous derived data attributes are computed from the event data to form feature vectors, which are sometimes called profiles in this context. The profiles contain state information and derived data and are used as inputs to the models as usual.
 Estimating baseline models. To learn the baseline models, we take a collection of event data and process it as follows. 1. First, we assign each event to one or more cells in the data 2. Second, we transform and aggregate the events and compute 3. Third, for each cell, we use the resulting profiles to estimate Scoring baseline models. To score a stream of event data, we pro-ceed as follows. 1. First, we retrieve the appropriate XML file describing the seg-2. Next, we assign each event to one or more cells in the data 3. We then access the profile associated with each cell, and up-4. We then use the profile as the input to the appropriate baseline 5. Next, we process the resulting score using the appropriate 6. Next, we apply XSLT transformations to the score to produce 7. Finally, if an alert is produced, we pass the alert to the required Figure 3. The basic idea with change detection using cubes of models or CDCM is that there is a separate change detec-tion model for each cell in a multi-dimensional data cube. In the work described here we estimated and maintained over 15,000 different baseline statistical models and monitored them monthly. In this section, we describe some typical alerts that have been gen-erated by the CDCM system we developed. Currently, we compute alerts each month and re-estimate baselines several times a year. The system has been in operation for approximately two years. It is important to remember when reading the case studies in this section that the issues identified by these alerts represent both a very small fraction of the transactions and a very small fraction of the total purchase dollars. For the various alerts that we describe below, we used the following dimensions to define a data cube:  X  The geographical region, specifically the US, Canada, Europe,  X  The field value or combination of values being monitored.  X  The time period, for example monthly, weekly, daily, hourly,  X  The type of baseline report, for example a report focused on Today (January, 2007), for each of 324 field values times 7 regions times 1 time period times 3 report types, we estimate a separate baseline, which gives 324 x 7 x 1 x 3 = 6816. In addition, for 623 field values times 7 regions times 1 time period times 2 report types, we estimate a separate baseline, which gives an additional 623 x 7 x 1x2= 8726 separate baseline models. So in total, we are currently estimating 15,542 (=6816+876).
 Actually, the description above is a simplified version of what actu-ally takes place. For example, the 6816 baselines mentioned arise from 324 x 7 = 2272 different field values, but the 2272 different field values are not spread uniformly across the 7 regions as indi-cated, although the total is correct.
 We are in the process of doubling the number of field values and increasing the number of time periods, so shortly we will be esti-mating and monitoring over 40,000 separate baselines. In this example, an airline was coding some of its transactions us-ing a Merchant Category Code (MCC) B instead of the preferred MCC A, which lowered the approval rate for the transactions. This resulted in a baseline alert that in turn resulted in a manual inves-tigation by an analyst. Following this, a conference call was ar-ranged with the individual responsible for the relationship with the bank who was the acquiring bank for the airline. As a result of this call, a fix was installed by the airline. This fix resulted in an annual recoverable costs of over $300,000. In this example, the decline rate for a large bank was essentially the same month to month but the baseline model identified a par-ticular category of transactions (specified by a combination of five fields) for which the decline rate sharply peaked in September 2006 compared to an earlier baseline period. One way of thinking about this, is that for this bank, most of the 50,000+ or so baselines were normal for September, but one was not. When investigated, this particular baseline was elevated due to suspected testing of stolen/counterfeit cards to determ ine whether they were still valid. After several discussions and further investigation, changes were made so that this type of testing was not possible. In this example, a European merchant X  X  transactions were coded in-correctly so that the merchant city name field contained incorrect information. This resulted in a lower acceptance rate for the trans-actions from this merchant. This lower acceptance rate was picked up by a baseline alert that for each acquirer monitored decline lev-els for each MCC. After an investigation, the merchant corrected the problem. Lesson 1. The most important lesson we learned was that thus far it has been more fruitful to examine many individual baseline and change detection models, one for each different segment of the event stream, even if the these models are very simple, than to build a single, relatively complex model and apply it to the entire event stream.
 Lesson 2. The time and effort required to get the alert format right is substantial. Although it was certainly expected, the business re-turn on the project was dependent to a large degree on the ability to deliver to the analysts information in a format that they could read-ily use. After quite a bit of experimentation, a report format was developed that reported:  X  What is the issue? This part of the report identifies the relevant  X  Who has the issue? This part of the report identifies the rele- X  What is the business opportunity? This part of the report iden- X  What is the business impact? This part of the report describes  X  The final part of the report contains additional information, One way to summarize the report is that the items in alerts gradually changed from those items related to the statistical models and how the alerts were generated to items directly related to how the alerts were investigated and how the business impact was estimated. The surprise for us was not that this transition had to be made, but rather the time and effort required to get it right.
 Lesson 3. It turned out that some of the most important alerts we found were alerts that had low statistical significance. For each report, we include an estimate of the statistical significance of the alert (low, medium, high and very high) as well as an estimate of the business significance of the alert (in dollars). It turned out that after investigation, the alerts that generated by most dollars saved, were often the alerts with low statistical significance. For this reason, it was usually not a good idea to investigate alerts in the order of most statistically significant to least statistically significant. Rather, the analysts used a more complex prioritization that thus far we have not tried to formalize.
 Lesson 4. As a result of analysis of an alert, it was sometimes possible to create specialized baselines and reports that would look for similar problems in the future. We quickly learned that even a few specialized reports like this could easily occupy most of our available time. The lesson we learned was that it was important to devote some of our time to looking for new opportunities (think of this as a survey activity), since some of these turned out to be even more important than what we were currently doing. There is a large amount of research on change detection algorithms per se. The monograph by Basseville and Nikiforov [3] is a good summary of this research field. In particular, the change detection algorithms that we use here, including CUSUMs, Generalized Like-lihood Ratios, and related algorithms are covered in this reference. The work described in this paper differs from classical change de-tection and contingency tables in that it uses a separate change de-tection model for each cell in a cube of models.
 More recently, Ben-David, Gehrke and Kifer [4] introduced a non-parametric change detection algorithm that is designed for streams. The methods used here are parametric. In contrast to their approach which uses a single change detection model, we build a large num-ber of models in order to handle complex, heterogeneous data, one for each cell in a multi-dimensional data cube.
 The paper by Fawcett and Provost [6] has a similar goal  X  detect-ing unusual activity in large data sets  X  but uses a much different approach. Their approach is to introduce operating characteristic style measures in order to identify unusual behavior.
 Guralnik and Srivastava [8] study event detection in time series data by using a new change detection algorithm they introduce, which involves iteratively deciding whether to split a time series interval to look for further changes.
 In contrast to all these methods, our approach is to use relatively simple classical change detection algorithms, such as CUSUM and GLR, but to build thousands of them, one for each cell in a multi-dimensional data cube. As far as we are aware of, our paper is also one of the few papers in the data mining literature that presents a case study of change detection involving a system as large and heterogeneous as VisaNet. In this paper, we have shared our experiences and some of the lessons learned over the past two years developing and operating a baseline and change detection system for Visa. Because of the com-plex and highly heterogeneous nature of Visa X  X  transactional data, we did not build a single change detection model, but rather over 15,000 individual change detection models. Indeed we built a sep-arate change detection model for each cell in a multi-dimensional data cube. This is an example of what we have been calling Change Detection using Cubes of Models or CDCM.
 Overall, the approach seems to work quite well. Indeed, substantial business value is being generated using this methodology, and thus far we have not been able to achieve the same performance using a single baseline or change detection model.
 To summarize, we have demonstrated through this case study that change detection using data cubes of models (CDCM) is an effec-tive framework for computing changes on large, complex data sets. [1] Alan Agresti, An Introduction to Categorical Data Analysis, [2] The Augustus open source data mining system can be down-[3] M. Basseville and I. V. Nikiforov. Detection of Abrupt [4] Shai Ben-David, Johannes Gehrke, Daniel Kifer, Detecting [5] Joseph Bugajski, Robert Grossman, Eric Sumner, Tao Zhang, [6] Tom Fawcett and Foster Provost, Activity monitoring: notic-[7] Robert L. Grossman, PMML Models for Detecting Changes, [8] Valery Guralnik and Jaideep Srivastava, Event detection from
