 1. Introduction
This paper describes a virtual document method to incorporate prior knowledge by applying a simple trans-formation to relevant documents to deal with a shortage of training data in text categorization. By incorpo-rating virtual documents into a training set, a categorization system can use the enlarged document space for learning.

Text categorization is a task to assign a new document to the pre-defined categories and a supervised learn-ing task, in which it is required to learn from a training set to make a proper decision about whether a new document belongs to a topic or not. When a training set consists of only a small number of documents, a text categorization system has difficulty in estimating a proper decision boundary.

To deal with the limitation of a training set, there has been interest in statistical re-sampling approaches such as bagging and boosting ( Tan, Steinbach, &amp; Kumar, 2005 ) by manipulating a training set and a co-train-In the image recognition field, there have recently been researches on the incorporation of prior knowledge. problem which is available in addition to the training examples. Either the knowledge is directly incorporated in a learning algorithm, or it is used to generate artificial training examples by transforming training examples ( DeCoste &amp; Scho  X  lkopf, 2002; Scho  X  lkopf, Burges, &amp; Vapnik, 1996 ).

This paper explores prior knowledge by artificially generating relevant documents to a topic by a simple transformation of documents, i.e., making virtual documents by combining relevant document pairs for a topic in a training set. To generate virtual documents, for instance, new document units are possible on con-dition that the document unit preserves relevance to the topic. Intuitively, a virtual document can be gen-erated by concatenating two or more documents which describe the same topic. The virtual document method simply extends a document unit to the combination of two documents. For weighting terms in the combined document, we give significant weights to terms that occur in both of the combined documents by multiplying two weights, based on the observation that terms that co-occur in pair-wise documents have stronger relevance to the topic to which the documents belong. Our hypothesis on the virtual documents thus created not only is expected to preserve the topic, but even improve the topical representation by exploiting core terms to the topic that are not given high importance in individual real documents. To test our method, we use support vector machines. Since support vector machines largely depends on the choice of kernels, we compare performances on kernel functions such as linear, polynomial and radial-basis func-tion kernels in experiments. To show the effectiveness of virtual documents, we analyze what change occurs in the support vector set. Our experiments focus on the small size of categories in dealing with a shortage of training data.

The rest of the paper is organized as follows: Section 2 presents related works, Section 3 describes the pro-posed method by incorporating virtual relevant documents into a training set and Section 4 shows our experi-mental results and analyses on Reuters-21578 test set. We will conclude our paper in Section 5 . 2. Related works
To deal with the problem of learning with a small set of training examples, there have been researches on sampling, statistical re-sampling, co-training, and incorporating prior knowledge approaches.
In such tasks as text categorization, routing and information retrieval, some researches tried to improve the performance by a sampling method . This is for balancing between the positive and negative evidences for learn-ing. Allan, Ballesteros, Callan, Croft, and Lu (1996) discarded non-relevant documents to make the size of relevant and non-relevant documents equal in the learning phase of a routing query. Singhal, Mitra, and
Buckley (1997) selectively used the non-relevant documents that do have some relationship to a user query (query zone) in learning routing queries. Kwok and Grunfeld (1997) selected the most effective training subset of relevant documents to create a feedback query based on a genetic algorithm. As the sampling techniques basically aim at the reduction of the size of a training set, they suffer from degradation when the number of available training data is small.

To alleviate the variance of a classifier X  X  performance by the distribution of training data, recently boot-strapping and boosting method have become popular combining techniques for improving weak classifiers ( Skurichina &amp; Duin, 2002 ) by manipulating a training set. Bootstrapping is a re-sampling method by repeat-edly random sampling with replacement from a training set ( Tan et al., 2005 ). Bootstrapping improves gen-evident when a training set is small. Boosting is an iterative method used to adaptively change the distribution of the training data. The base classifiers focus on data that are hard to classify ( Tan et al., 2005 ). The
AdaBoost algorithm ( Freund &amp; Schapire, 1997 ) has been shown to perform well on machine learning tasks ( Freund &amp; Schapire, 1996, 1997; Schapire &amp; Singer, 2000; Schwenk &amp; Bengio, 1997 ).
There has recently been significant interest in a co-training method , a weakly supervised learning method that incorporates both labeled and unlabeled data due to the lack of labeled data in text categorization vised learning algorithm for bootstrapping a model from a small set of labeled examples, given a larger set of unlabeled training examples. The co-training algorithm has a close connection to bootstrapping from incom-plete data in the Expectation-Maximization setting ( Blum &amp; Mitchell, 1998 ). Balcan and Blum (2005) pro-vided a PAC-style model that incorporates both labeled and unlabeled data, and have given a number of sample-complexity bounds.
 Many machine learning applications in image recognition incorporated prior knowledge ( DeCoste &amp;
Scho  X  lkopf, 2002 ) about the desired behavior of a system into training data. For instance, image recognition systems use new examples by small distortions of the input image such as translations, rotations, scaling; speech recognition systems produce them by time distortions or pitch shifts. In 3D object recognition, Poggio and Vetter (1992) exploited appropriate transformations to generate new views from a single 2D view. In handwritten digit recognition, DeCoste and Scho  X  lkopf (2002) added virtual examples produced by shifting the images by one pixel to the training examples. The open issue about the incorporation of prior knowledge is to what extent transformations can be safely applied to the training example, as some distortions can pro-technique showed significant improvements on the large test collection of the TREC filtering task ( Lee, Kage-
In text categorization, Sassano (2003) generated virtual documents by randomly adding or deleting a small number of words. The experiment was performed on the most frequent categories.

This paper focuses on prior knowledge about transformation invariance in text categorization by artificially generating virtual documents as a means of compensating for a shortage of training data. Experiments will focus on the effectiveness for the topics with a small number of relevant documents and common kernel func-tions such as linear, polynomial and radial-basis function kernels in support vector machines. 3. Our approach: Incorporating virtual relevant documents into a training set cation problem. However as we are just given a limited training set, the decision boundary made for the limited training set by a learning method is different from the true boundary. It would lead to wrong classification for the new document indicated by the question mark ( Fig. 1 (b)). If virtual documents can represent the topic prop-erly, incorporating virtual examples into a training set would yield a more accurate decision boundary ( Fig. 1 (c)).
 digit recognition problem with support vector machines. To get a ten-class classifier for digit recognition, they incorporating only translational invariance improves performance significantly, from 4.0% to 3.2% error rate. 3.1. Generating virtual relevant documents
The prior knowledge about transformation invariance is generated by a virtual relevant document (VRD) method in text categorization. The method manipulates a training set more freely, using the information in each document but not restricted to the existing unit of document . The VRD method applies a simple trans-formation to documents by extending the single document unit by combining relevant document pairs for a topic in a training set.

The weight of a term in an artificially generated document is calculated by multiplying two weights of the term from each real two documents on the observation that terms that co-occur in pair-wise documents have stronger relevance to the topic. Our hypothesis on generating a VRD is that including all terms and differen-tiating co-occurring terms in pair-wise documents in weighting by a multiplication operator would provide new information for learning the decision boundary.

Incorporating virtual documents thus into the training set can give effects on the change of the distribution in the training data, though without randomization, as in bootstrapping.
 The method of generating VRDs is straightforward as follows:
Document representation : Each document is represented as a weighted term vector, d The weight is calculated by tfidf and normalized by cosine normalization ( Salton, Wong, &amp; Yang, 1975 ).
VRDs generation : For the pair-wise document for a positive training set, one virtual relevant document is created. The time complexity for the generation procedure is O( n instance, 1,3,6, ... ,and n ( n 1)/2 virtual relevant documents can be generated for 2,3,4, ... , and n real documents, respectively.
Term weighting for a VRD : The virtual document is produced by giving weights for all terms appeared in two real documents. The weight of a term is calculated as follows: where w i , k and w j , k are weights of the term k in real documents d term k in the virtual document v ij generated from d i and d to 0.005 in experiments). Finally, the weight vector is normalized by cosine normalization.
Note that for the terms of a virtual document, non-shared terms in real documents are not discarded since diverse terms can be used to describe a certain topic and two documents can describe the topic using different terminologies.

For instance, in 2-dimensional document space, let a real document vector be d term t 1 . Let another real vector be d 2 :  X  0,1  X  with a term t ing zero with the default value, and normalization. As shown in Fig. 2 (b), the real vectors d d
The resulting virtual document space is enriched by this method, with higher weights for the terms that co-occur in two real documents. 3.2. Virtual documents and support vectors
Support vector machine (SVM) is a learning approach for solving pattern recognition problems. It is based on the Structural Risk Minimization principle from computational learning theory ( Cristianini &amp; Shawe-Taylor, 2000; Joachims, 1998; Scho  X  lkopf, Burges, &amp; Smola, 1999; Vapnick, 1995, 1998 ). The method is defined over a vector space where the problem is to find a decision boundary that best separates the data points in two classes.

Support vector machines use a particular type of function class: classifiers with large  X  X  X argins X  X  in a feature linear threshold function. Nevertheless, by a simple plug-in of an appropriate kernel function, they can be used to learn polynomial and radial basis function classifiers. The examples of common kernels are following: Linear kernel: k ( x , y )= x T y .
 Polynomial kernel: k ( x , y ) = (s( x T y )+c) d .

Radial basis function( rbf ) kernel: k ( x , y ) = exp( c i x y i In experiments we compare performances on three kernel functions in SVM.

A support vector (SV) set is an essential subset of positive and negative examples, which is learned by a support vector machine for a training set. The SV set is used for determining categories in the test phase.
Scho  X  lkopf, Burges, and Vapnik (1995) and Vapnick (1995) observed that the SV set contains all the infor-recognition. Following these observations, we generate the VRDs from the positive SV set, not from all the relevant documents in the training set.

The procedure to train a SVM with VRDs is as follows: (a) Training an SVM for the training set for a topic; thus producing the first SV set which consists of posi-(b) Generating VRDs using the positive SVs extracted from the first SV set. (c) Training another SVM by incorporating VRDs generated at (b) into the first SV set at (a) to the topic;
Note that two SVM trainings are performed, i.e. (a) and (c), one for obtaining the initial SV set for the training set, and then the other for finding the second SV set for the enlarged training set which consists of the original SV set and VRDs. The first SV set is used as the decision boundary of the base system (baseTR) and the second SV set is used for the proposed method (VRDsv) in our experiments. In contrast of Poggio X  X  work with neural nets, SVMs only require augmentation via the support vectors, which reduces the compu-tational complexity of the process dramatically. 4. Experiments 4.1. Experiments setup
We evaluated our method on Reuters-21578 test set. This test set has been used for evaluation in text cat-egorization ( Kawatani, 2002; Yang &amp; Liu, 1999 ). We tested on the ModApte split of Reuters-21578 compiled by Lewis (1999) . This split leads to a test set of 7770 training documents, and 3019 test documents. The num-ber of topics (or categories) is 90. To see the effectiveness of the proposed method for the small size of cate-gories, we compared performances according to the size of positive documents in the training set: 25 topics, 67 topics, 76 topics and 81 topics with less than 10, 100, 300, and 500 relevant documents, respectively. Fig. 3 shows the distribution of the number of relevant (or positive) documents for the 90 topics.
The extracted terms are 16,422 by stemming and removing stop words. The SVM 2002 ) is used for learning classifiers via linear , polynomial and radial-basis function ( rbf ) kernel for more detailed experiments on SVM. All SVM light options that affect learning are left as default.
We have compared the effectiveness of the proposed virtual documents technique with the base SVM for the training set: baseTR: the performance of SVM for the original training set.
 VRDsv: the performance of SVM by incorporating VRDs generated from the positive SV set into the initial SV set.

Performances are evaluated via scaled linear utility (T11SU), F-beta measure (T11F) as defined in a TREC-11 filtering task ( Robertson &amp; Soboroff, 2002 ), macro-averaged F ratio. The measures are defined as follows: (a) Scaled linear utility (T11SU) : The linear utility measure assigns a credit of 2 for a relevant document (b) F-beta (T11F) : The F-beta is a function of recall and precision, together with a free parameter beta (c) F 1 measure : The F 1 -value is a harmonic average of precision and recall. Micro-averaged F (d) MissRatio and FalseRatio : Let A , B , C and D be, respectively, the numbers of true positives, false alarms, 4.2. Experimental results
Since the performance of a SVM depends on the choice of kernels, the performances are compared on three kernel functions in SVMs. Table 1 shows that the proposed method achieved significant improvements for all of three kernel functions. The linear kernel performs better than polynomial and rbf kernel function for overall measures. By the results on kernel functions, the following experiments are performed with the SVM linear kernel function.

The performances according to the number of positive documents used in the learning phase are evaluated to see the effectiveness of the proposed method for the small size of categories. In the training set, the number of topics with less than 10, 30 and 50 positive documents is 25, 46 and 58 topics, respectively. The number of negative documents (non-relevant documents) is: 7770 #rd. Hence the size of a negative training set is rela-tively big, compared to the positive one for overall categories. As shown in Table 2 , the proposed method achieved about 186%, 131%, 34% and 12% improvements in micro-averaged F and 27% in macro-averaged F 1 for 18, 25, 46 and 58 topics, respectively. 4.3. Result analyses
The effects of virtual documents are analyzed for each category for 58 topics with less than 50 positive doc-uments in the training set. Table 3 shows that the performances of 20 topics are improved by incorporating virtual documents; that of one topic,  X  X eat X , is deteriorated.

The change of the positive and negative SV set is examined how to virtual documents give effect to the deci-sion boundary in SVM learning. Table 4 shows the details of the support vector set learned for SVM classi-fiers: baseTR and VRDsv. For training of the base system (baseTR), the training set consists of 62.37 positive real documents and 7707.63 negative real documents. As the result of SVM learning, the SV set consists of 39.56 positive documents and 165 negative documents. The number of virtual relevant documents is 2244.61, artificially generated from the positive SV set of baseTR. For training of the proposed system (VRDsv), the training set consists of 39.56 positive SVs and 165 negative SVs of the base system, and 2244.61 virtual documents. As the result of SVM learning for the proposed method, the positive SV set con-sists of 19.77 virtual documents and 36.65 positive documents.
 By the analysis of the SV set, we can see that a lot of positive SVs in the new support vectors are taken from
VRDs. These results indicate that the proposed method provided new information in learning decision bound-aries for support vector machines.

Fig. 4 illustrates the performance comparisons for baseTR and VRDsv in the error rate by the MissRatio measure for 36 topics. For 81 topics, the performances of 35 topics are improved, while that of only 1 topic,  X  X ugar X , is deteriorated, and those of 45 topics are unchanged. The result shows that VRDs can reduce the error rate on positive documents on each category.
 Fig. 5 shows performance changes for each category between performance of baseTR and that of VRDsv.
The performances of the proposed method were improved for most topics. For example, a topic  X  X nstal-debt X  shows a big change since the performance is 0 by baseTR and 1.0 by VRDsv in F
We have analyzed the performance changes gauged by the F 1 describe the performances of baseTR by training SVMs with all negative documents and real positive docu-resent the performances of VRDsv by training SVMs with all negative SVs, all positive SVs, and VRDs generated from 2, 3, and N real documents in the positive SV set, respectively. For example, a topic  X  X inc X  has 21 positive documents (#rd = 21) and 7749 negative documents by (7770 21) in the training set. The performance of  X  X 19 X  is 0.27 by baseTR trained with 19 real positive documents and all negative documents.
The performance of  X  X r17 X  is 0.47 by VRDsv learned with the initial SV set and virtual documents generated from 17 real documents in the positive SV set.

These results show the general pattern in baseTR and VRDsv that using more training documents in learn-ing produces the better performance. The relative margin of the contribution of VRDs is highly notable, espe-cially for those cases in which the training set consists of a small number of positive documents (e.g., nickel , steady improvement of the performance.

Artificially generated documents thus result in the change of the distribution in the training data without the randomization. 5. Conclusions
Incorporating virtual relevant documents into a training set in the learning phase is effective for text cate-gorization. We suggested the virtual document technique to enlarge positive training documents. The perfor-mance is improved for categories with a small size of positive documents for learning. The proposed method achieved 131%, 34%, and 12% improvements in micro-averaged F aged F 1 for 25, 46 and 58 topics with less than 10, 30, and 50 positive documents in the learning phase, respec-tively. These improvements are important, especially given the fact that the virtual documents are produced simply and the incorporation of the virtual relevant documents contributed to the steady improvement of the performance. Virtual documents result in the change of the distribution in a training set without the random-ization. Our results show that the suitable term space could be manipulated for learning within the limitation of a training set, not restricted to the existing unit of the document.
 References
