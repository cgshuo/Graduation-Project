 Social features are increasingly integrated within the search results page of the main commercial search engines. There is, however, little understanding of the utility of social fea-tures in traditional search. In this paper, we study utility in the context of social annotations, which are markings in-dicating that a person in the social network of the user has liked or shared a result document. We introduce a taxon-omy of social relevance aspects that influence the utility of social annotations in search, spanning query classes, the so-cial network, and content relevance. We present the results of a user study quantifying the utility of social annotations and the interplay between social relevance aspects. Through the user study we gain insights on conditions under which social annotations are most useful to a user. Finally, we present machine learned models for predicting the utility of a social annotation using the user study judgments as an optimization criterion. We model the learning task with features drawn from web usage logs, and show empirical evi-dence over real-world head and tail queries that the problem is learnable and that in many cases we can predict the utility of a social annotation.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Design, Experimentation, Measurement Social relevance, social aspects, web search
Social offerings are becoming table stakes for the major search engines. Querying for your local pizza restaurant on Figure 1: Example social annotation for the query  X  maui hotels  X .
 Bing or Google may yield reviews from your friends about the restaurant, or about nearby restaurants liked by your friends. A query for  X  maui hotels  X  may yield a result indi-cating that your colleagues like the Royal Lahaina Resort on Facebook and a query for  X  luau history  X  may yield a social annotation on the result  X  X awaii Luau History X  stating that your friend has liked or shared the document.

The user can benefit from such social experiences in vari-ous ways, including: (a) discovery of socially vetted recom-mendations; (b) personalized search results; (c) connecting to the lives of their friends; (d) result diversity; and (e) emo-tionally connecting with an otherwise static and impersonal search engine. There is, however, very little understanding whether these social features are useful or detrimental to the whole-page user experience, or, for that matter, how to even measure the utility of social features.

Consider a social annotation feature, such as the one depicted in Figure 1, where some results on the search results page (SERP) are enriched with markings indicating some of your friends that have previously liked or shared that result 1 . Are such endorsements from dearest friends more relevant to the user than from acquaintances or coworkers? Are expert opinions or those from friends who live in the vicinity of the restaurant more valuable? Do annotations on irrelevant results amplify their negative perception?
Studying such aspects and their effect on social relevance form the basis of this paper. We begin by enumerating a taxonomy of social relevance aspects, i.e. cues or criteria that influence the perceived utility of social annotations. We consider aspects related to the user query, the social connection, and the relevance of the related content (e.g., a document returned by the search engine). We then define measures of social annotation utility and present the results of a large controlled user study of the interplay between each of these aspects on head and tail queries drawn from several months of real-world queries issued to a commercial search
Social networks have different terms to indicate explicit positive user interest of a document, such as like , +1 ,and tag .Hereinweusetheterm like generically to indicate any of these, and dislike to indicate negative interest. engine. We show evidence that social annotations add per-ceived utility to users in varying degrees according to the social relevance aspects. Finally, we turn our attention to the task of automatically predicting the utility of a social annotation. We model the prediction task using a multiple additive regression tree model over features available to a standard search engine. We show empirical evidence that the task is learnable and that we can automatically predict the utility of a social annotation. The major contributions of our research are:
Most relevant to our work is that of Muralidharan et al. [18] who show through user studies and eyetracking anal-ysis that the presentation of the social annotation, such as the size of the profile thumbnail, greatly impacts user engagement of the annotation. Through anecdotal feed-back, participants from their studies conjectured that an-notations would be useful in certain social and subjective topics (e.g., restaurant and shopping queries) or when pre-sented by friends believed to be topical authorities (e.g., a fitness trainer annotating a fitness web document) or when in a close relationship with the searcher; but for other situa-tions the participants believed annotations were not helpful, such as when there is no label explaining why an annotation was present. In this paper, we build upon their work by exploring a taxonomy of influential social relevance aspects, including query class, content relevance, and social network aspects; and quantifying their utility and interplay.
Evans and Chi [9] performed a detailed user study about the role that social interactions play in collaborative search tasks. Outside of collaborative search, social signals have primarily been explored as implicit ranking features hid-den deep inside the ranking functions [20, 26], for exam-ple by examining how users would benefit from personal-ized search results considering implicit behavior similarity attributes such as from click-based measures [22, 23, 1]. Bao et al. [2] further argue that the quality of a web page can be improved by the amount of del.icio.us annotations and Carmel et al. [6] show that personalized search improves the quality of intranet search results. For traditional web search, Heymann et al. [13] predicted that while social book-marks can provide ancillary data not present in the web page, the majority of tags are also present in the document, or inlinks/outlinks, and therefore would have limited use as ranking features.

Recently, social features are appearing as explicit user-facing features such as in: (1) annotations , where interest by the searcher X  X  social network is visibly marked on an ex-isting search result (v.s. Figure 1); (2) injected results , where social data, such as tweets or status posts, are pre-sented in a manner similar to and within the existing search results, but sourced outside of the web corpus; and (3) in-dependent results , where the social data is presented in a manner not to be mistaken for one of the web search results, such as in a web answer or direct display. This paper specif-ically focuses on measuring utility in the context of social annotations and leaves the analysis of other visualizations to future work.
Relevance is a multidimensional dynamic concept and there is a wide range of factors that influence a user X  X  perception of relevance. Via an extensive user study, Barry and Scham-ber proposed a set of categories capturing users X  relevance criteria or cues (e.g., accuracy, specificity, expertise, pre-sentation, etc.) in the context of an information seeking task [3]. More recently, Borlund provided a framework for examining relevance in the context of information retrieval evaluation [4]. The relevance and usefulness of results re-turned by web search engines are typically evaluated using variants of nDCG [16], expected reciprocal rank [7], mean average precision, relative information gain, and a variety of click/feedback metrics [25, 15], such as (1) clicks on lower-ranked documents indicate that higher-ranked documents are less relevant for the query; and (2) clicks to documents which are quickly abandoned by the user for other search results are deemed less relevant for the query. However, use of these traditional metrics can present challenges when personalizing and annotating web search results, as higher-ranked search results may be passed up for lower-ranked search results with social annotations. As shown in [8] and [12], annotations and other modifications to captions can alter the success rate for users independent of where the document was ranked in the result set. Fidel and Cran-dall [10] show factors beyond the document that affect the perception of relevance, including recency, detail, and genre; but they do not discuss social factors. This paper extends their work and also proposes metrics by which the utility of a social annotation feature can be measured.
At the core of a search engine is the ability to learn to rank candidate documents according to their relevance to a user query. As such there is a plethora of work on model-ing, feature extraction, selection, and model adaptation, see Liu [17] for a comprehensive survey and Burges et al. [5] for a description of the system that won the recent Yahoo! Learn-ing to Rank Challenge. In our work, we introduce a com-plementary task, that of learning to predict the relevance of a social annotation. As such, we make use of a state-of-the-art learning algorithm [27] and predict social relevance based on runtime features from web usage logs and query classifiers commonly used in web search ranking, as well as social relevance cues defined in this paper.
Consider a search results page consisting of ranked con-tent (e.g., documents, videos, images) in response to a user query. Formally, we define a social annotation as a tu-ple, { q,u,c,v } , consisting of a query q ,content u ,asocial network connection c and the connection X  X  interest valence v in the content (e.g., like , dislike or share ). For exam-ple, Figure 1 illustrates such a social annotation impression where q is  X  maui hotels  X , u is a relevant Expedia hotel page, c is  X  Tim Harrington  X  X nd v is like .

In this section we propose a taxonomy of aspects that in-fluence the utility of a social annotation, spanning the query, the social connection, and the content.
We divide queries into two sets based on whether they are navigational ( nav ) or non-navigational ( nnv ) .We expect that social annotations will be less valuable when the user is simply looking for the url of a web page she wants to reach. Other possible intents that are not explicitly studied in this paper include informational and transactional intents, which we grouped within our nnv intent, and which partially overlap with our QA-CLS aspects described next.
The utility of a social annotation may be influenced by the class of the issued query. For example, although one might find value in knowing the interest of a social con-nection when querying for a movie, song, or book, we may expect only certain connections such as experts to be of in-terest for a health query. Similarly for local queries, interest from connections in the vicinity of the target location are likely more valuable than distant connections.

We focus our analysis on the following query classes cover-ing the majority of social annotations found on a commercial search engine:
The social network connection is at the heart of the so-cial annotation and it clearly influences its utility. Consider the query  X  korean restaurant  X  and a set of resulting links to nearby korean restaurants. We expect that a connection who is an expert on korean cuisine may increase the utility of an annotation, and that a connection living near the lo-calized neighborhood is more important than one far away. A dear friend X  X  interest in the restaurant may also hold more weight than a more distant work colleague. And finally, the interest valence, whether positive, negative or neutral might affect relevance. Below we summarize each of these aspects and their value ranges.
The circle of a connection refers to the relation of the connection to the searcher. Intuitively, a co-worker X  X  interest in an article related to the workplace might hold more value than a family member or friend X  X  interest. We consider the following circles: work colleague ( wkc ) , family member ( fam ) ,and friend ( frn ) . Other interesting circles out of scope for this paper include school friends, college friends, church friends, and sports club friends.
The affinity between a searcher and a connection refers to their degree of closeness. Although affinity has a continuous range, in this paper we consider two affinities: close ( cls ) and distant ( dst ) . As conjectured in [18] we generally ex-pect the closeness of a connection to greatly influence the perceived utility of a social annotation.
Whether a connection is an expert ( exp ) or non-expert ( nex ) on the search topic may influence the value of a social annotation especially when issuing informational or transac-tional queries. Other possible values not considered in this paper include hobbyist and enthusiast.
For local queries, we expect that a connection living near ( nea ) the target location will add more value than if living far ( far ) . For non-local queries, the value of this aspect is not applicable ( n/a ) . One might also consider the geo-graphic distance between the searcher and the connection, but we leave this aspect for consideration in future work.
Social annotations require both a social network connec-tion and the interest valence of that connection with respect to the annotated search result. Most experiences today clas-sify the valence as either a like ( lik ) or a share ( shr ) ,i.e. that the user has shared the document with someone. In this paper, we also consider a third valence, dislike ( dis ) .
Other aspects may influence the value of a social anno-tation, such as the connection X  X  gender, age, and general interests. We leave these for further study in future work.
Finally, the social annotation is influenced by the rele-vance of the document to the intent of the user query. We consider graded relevance according to the following scale, similar to that proposed by J  X  arvelin and Kek  X  al  X  ainen [16]:
This section presents the results of a user study quan-tifying the utility of a social annotation. We analyze the interplay between the social relevance aspects presented in Section 3 and identify situations where a social annotation is more relevant than others. Our approach is to sample so-cial annotation impressions on a commercial search engine and to have human annotators judge the utility of each im-pression. In the following sections, we describe our process for sampling social annotation impressions (i.e., { q,u,c,v } tuples) and the guidelines for judging their relevance. Then, in Sections 4.4 and 4.5, we present our analysis.
We define U as the universe of all social annotation impres-sions observed on a commercial search engine during three weeks spanning three months of US English web usage logs: 10/7/2011-10/14/2011, 11/4/2011-11/11/2011, and 12/2/ 2011-12/9/2011. We admitted only queries that were clas-sified by the search engine to be in our domains of interest, namely commerce, health, movies, music, or restaurants (see Section 3.1.2). We applied a low classification threshold to admit a larger number of queries since we later manually annotate the query class of all queries in our test set. We further rejected any query suspected of being bot-generated.
We define the head of U as all tuples where q is in the top-20% of the most frequent queries, and the tail as all tuples where q is in the bottom 30%. A query-frequency weighted sample of queries from both sets yield our test queries, referred to as HEAD and TAIL consisting of 2388 and 1375 queries respectively.

For each query in HEAD and TAIL , we randomly selected one url from U using an impression-weighted sampling (i.e., a url with many social annotation impressions in the usage logs for the query is more likely to be chosen than a url with fewer social annotation impressions for the query).
We used a crowdsourcing tool to have each query in HEAD and TAIL manually classified according to the query classes defined in Section 3.1.2 as well as an other class. We ob-tained three judgments per query (7526 judgments) from a total of 32 independent annotators and kept the majority vote. The inter-annotator agreement as measured by Fleiss X   X  was 0.495 (moderate agreement).

We further manually judged the relevance of each url to the associated query in HEAD and TAIL . The relevance cat-egories and guidelines are those listed in Section 3.3. As this task is known to be more difficult than query classifi-cation, we employed seven professional independent annota-tors with experience in search engine relevance testing. The observed inter-annotator agreement as measured by Fleiss X   X  was 0.176 (slight agreement).
In order not to bias the social annotations to the specific social networks of our human annotators, we simulated a social network for our judges. We used this network to create the connections c and interest valences v for our random query-url pairs in HEAD and TAIL .

Figure 2 illustrates the virtual social network. It consists of twelve connections spanning the social circles and affini-ties defined in Section 3.3. We assume in this network that family members always have a close affinity whereas work colleagues have a distant affinity. Friends can have affin-ity either close or distant .

For each query-url pair in HEAD and TAIL we assigned a social annotation as follows. First, we randomly sampled a circle (either work colleague , family ,or friend ). Then, we randomly selected an individual from that circle, which also determines the affinity of the connection (either close or distant ). Next, we randomly picked whether the con-nection was an expert or non-expert with respect to the document and we randomly chose the interest valence (ei-ther like , dislike or share ). Finally, if the query was annotated as a local query, we randomly determined if the connection lived near or far from the intended target loca-tion. Otherwise, we set the geo-distance aspect to n/a .
Deploying our user study over the actual social networks of the participants is preferable, however several problems arise that make this infeasible, most notably privacy concerns. Also, since our study requires significant training and exper-tise, we found it necessary to hire professional annotators, thus limiting the total number of independent annotators. Using their personal networks would not only cause privacy concerns, but it would also bias towards a non-representative set of search users. Simulating a social network as we do car-ries its own risks. Firstly, we expect people X  X  personal social networks to vary in terms of attribute value distributions (for example, some people have only work colleagues in their network while others have mostly friends and family) as well as diversity distribution. Although in our setup we assumed uniform priors for all attributes, if given the true priors it is trivial to reweight the findings. Secondly, the degree of an individual in their social network (i.e., average number of network connections) is significantly larger than the twelve in our virtual network, and also varies significantly 2 .To balance the cognitive load on our judges, the reliability of judgments, and reducing the risks of simulating a social net-work, we chose to keep the connection degree small enough whilst ensuring diversity in age, gender, and ethnicity. Fi-nally, by explicitly drawing the judges X  attention to social aspects, there is a risk of overemphasizing their importance and thus influencing the judgments, though we expect this influence to be minimal.
The judges were presented with the following scenario:
Hill and Dunbar [14] estimate the average connection de-gree at 153 and Ugander et al. [24] measured the median degree of Facebook users in May 2011 at 99. Table 1: Summary of the test datasets and the hu-manlabelsbrokendownbyjudgmentcategory.

The annotators were also presented with their virtual so-cial network from Figure 2 along with a textual descrip-tion of each of the twelve individuals in the network. For each test case, the social annotation was graphically pre-sented as though returned from a search engine, similar to what is illustrated in Figure 1. A textual description of the annotation is also presented to the annotator, stating the connection X  X  circle, affinity, expertise, interest valence and geo-distance (if the query was a local query, such as for a restaurant).

For each { q,u,c,v } tuple in HEAD and TAIL the annotation task is to assess the utility of the social annotation according to the following guidelines:
Annotators were encouraged to enter a comment justifying their judgments and were required to do so if their judgment was dont-know or error .

Each test tuple in HEAD and TAIL wasannotatedbytwo judges randomly drawn from a pool of seven paid profes-sional independent annotators, for a total of 7532 judgments. We trained the judges by iterating on the guidelines over an independent dataset 3 . In cases where the judges disagreed, we adjudicated as follows. If one judge added a comment that we determined clearly justified the judgment, we ad-judicated the test tuple to that judge X  X  decision. If both judges added a comment deemed clearly justifying their de-cision or if both judges omitted a comment, then we retained the disagreeing judgments.
The training phase was necessary to achieve a fair inter-annotator agreement. As such, although it would have been desirable to crowdsource the judgments we deemed the task too difficult and that paid professional independent judges were necessary.

Table 1 summarizes the breakdown of the resulting judg-ments. Inter-annotator agreement as measured by Fleiss X   X  on this annotation task was 0.395 (0.487 on HEAD and 0.236 on TAIL ), considered moderate to fair agreement. For our final datasets HEAD and TAIL we omit the 2.7% of the judg-ments that were judged as dont-know and error .
Let T be a set of test tuples, such as those from HEAD and TAIL , where t i = { q i ,u i ,c i ,v i } ,let A be a set of aspect values and T A be the subset of tuples in T matching an aspect value in A .Forexample,if A = { QA-CLS-hea , SA-CIR-fam , SA-INT-lik } , then T A is the set of all test tuples in HEAD and TAIL where q is a health query, and c is a family connection that has liked u . Finally, let J ( t ) be the set of judges that annotated a test tuple t .

We define R ( T ), the expected utility of social annotations in T , as the average utility of each tuple in T : where  X  : J  X  R is a real-valued utility function mapping judgments J to real numbers in the range [0,1], where J is the set { sig-util , some-util , no-util } defined in Section 4.3.
Table 2 lists the two variants of R ( T )thatwereporton in this paper. R Rel ( T ), our relevance utility metric, assigns a graded utility score to each judgment similar to that done for query-url relevance judgments [16]. R Pre ( T ) expresses a binary utility where a social annotation has positive utility if it is judged as either significantly or somewhat relevant, otherwise negative utility. R Pre ( T ) can be thought of as a measure of social annotation precision.

Table 3 lists the overall utility and per aspect utility break-down, over the HEAD and TAIL data sets 4 . The expected over-all relevance, at 0.543 indicates that social annotations are generally somewhat relevant, useful or of interest to users. Each individual aspect, however, influences the utility in very different ways. Those aspects with significantly more utility are bolded with a  X  symbol and those with less utility are bolded with a  X  symbol.

Overall, we observe no statistically significant difference between the expected utility of head vs. tail queries (note that health queries and restaurant queries seem to have higher utility on tail, but this is not statistically significant). It is surprising that the query class aspects ( QA-CLS )gener-ally do not show different utility with respect to the average, however further analysis in Section 4.5 reveals significantly differentiated influence in combination with social aspects. Also counter to our expectations, judges found equal util-ity between navigational and non-navigational queries. For the content aspects ( CA ), although we observe no statistical
CA-det had too few judgments to report results. significance within the class, the descending utility trend fol-lows the graded relevance judgments, with higher utility on both HEAD and TAIL for perfect and excellent content versus lowerutilityforfairandbadcontent.

The social aspects SA generally have significant differenti-ating influence. The social affinity ( SA-AFF ) shows the most influence on utility followed by expertise ( SA-EXP ) and con-nection circle ( SA-CIR ), where colleagues and friends have equal utility but family members have much higher expected utility. For interest valence ( SA-INT ), knowing that a con-nectionhasliked( lik ) a link shows more utility than av-erage, but a share ( shr ) shows a negative utility influence. Interestingly, disliking a link ( dis ) has little effect on util-ity, which is further confirmed in our analysis in Section 4.5. Since only 7% of the queries in HEAD and 4% in TAIL were local queries, our sampling resulted in too few geo-distance ( SA-GEO ) instances that were near or far . The result is large confidence bounds and the only statistically significant util-ity difference is shown in the TAIL where knowing near -ness is more valuable than not. Further investigation on this as-pect is warranted because we expect geographical distance to have influence on social annotations of local queries.
We performed feedback analysis by inspecting a random sample of the comment boxes filled by our judges. For the no-util judgment, the feedback was mainly split between poor content matches or vague queries, and connections of very poor perceived utility. For the latter, expertise was the main discussed social cue followed by affinity and in-terest valence . Example feedback includes:  X  X ecause he is a distant friend, neutral and a non-expert, his opinion is not going to be useful to me. X  and  X  X ven though the connection is a family member, being a non-expert and neu-tral on the page would make me think that they do not know a lot on the result. X  For the some-util judgment, feedback mainly revolved around the circle and dislike aspects as the driving cues for utility. Example comments include:  X  X hris X  dislike might get me to click another link, even though it X  X  what I X  X  looking for, it could be a bad quality link. X ;  X  X ven though Bob is neutral, since he X  X  an expert there is added value in his annotation. X ; and  X  X ny-time one of my friends  X  X islikes X  a website, it might make me dig further to see why. X  Finally, for the sig-util judgment, expertise and affinity drove the utility cues. Example feedback includes:  X  X here is a lot of added value to this an-notation because Bob is my close friend and an expert. X  and  X  X he is only a colleague, but she is an expert on the result and her opinion matters to me because she knows what she is talking about. X 
Finally, we further analyzed the utility of all pairwise com-binations of aspects. We list the top-20 and bottom-20 in terms of R Rel utility in Table 4, computed over T HEAD  X  The main conclusion from this analysis is that social anno-tations have very large variations in utility depending on the aspects defined in Section 4, with relevance utility ranging from 0.336-0.754 in pairwise combinations. These should be leveraged in deciding when to impress a social annotation to auser.
We turn now to measuring the interplay between social relevance aspects. In a production system, some aspects are more expensive than others to obtain. For example, most search engines will already have query classifiers in place whereas it may be harder to obtain some of the social aspect values such as a connection X  X  affinity or expertise. In this section, we are interested in answering the question: what is the value of a set of aspects if we know another set of aspects? Table 4: Top-20 and Bottom-20 social relevance as-pect combinations in terms of R Rel utility.
More formally, given a corpus of tuples T and a set of aspect values A 1 , we are interested in the expected relative gain or loss in social annotation utility, RG T ( A 2 A 1 we learn another set of aspect values A 2 : Consider for example A 1 = { health } (query class) and A { family } (circle). Using Eq. 1, we can compute the expected utility of a social annotation in T { health } as R ( T { can similarly compute the expected utility if we also knew the family aspect, R ( T { health }  X  T { family } ). Eq. 2 measures the ratio between these two utilities, which captures the ex-pectedgainorlossofknowing family given that we knew health . RG T is non symmetric, that is RG ( T A 1 T A 2 ) = RG ( T A 2 T A 1 ). Positive gain is indicated by the sign of RG T , and no gain is observed if RG T =0.

The interplay between social relevance aspects can also be expressed in terms of information gain. Although we report only values of RG T in this paper and prefer its in-terpretability (it can be directly interpreted as the expected ratio increase in utility), for completeness we derive the in-formation gain criteria. First, let P T ( v ) be the probability that a tuple t  X  T is judged as v  X  J : where  X  j ( t, v ) indicates if tuple t is judged as v by annotator j . Then the information gain of A 2 given A 1 is defined as:
We focus our analysis on the relative gain of social aspects with respect to each other, the query class aspects ( QA-CLS ) and content aspects ( CA ). We omit the geo-distance aspect for lack of space and since very few gains were significant. Table 5 and Table 6 list the relative utilty gains of SA vs. QA-CLS and CA , respectively. Bolded entries indicate statis-tically significant gains. The value of any cell can be inter-preted as the utility gain or loss (in terms of R Rel and over both HEAD and TAIL of knowing the row aspect given the column aspect, i.e. RG HEAD  X  TAIL ( row column ). For exam-ple, given the query class movie , knowing that the connec-tion is in the family circle increases the relevance utility by a factor of 0.27, whereas if it is in the work colleague circle reduces the relevance utility by a factor of 0.132. Dashed en-tries represent either &lt; 0.001 utility gain/loss or impossible combinations (e.g., since family members are always consid-ered to have close affinity in our setting then T family  X  is an empty set).

Generally, knowing all social aspects with the exception of friend and dislike yields significant utility gain (or loss) over the query classes and content relevance aspects. The top-3 social aspects resulting in the most overall gains are the family circle and the affinity aspects. We computed RG between all combinations of affinity and circle as-pects (table omitted for lack of space). We found that if we knew the circle then further knowing affinity leads to a utility difference of a factor of 0.319. In contrast, if we knew first the affinity , then further knowing the circle leads to a utility difference of a factor of only 0.243. Clearly cir-cle and affinity are not independent. Hence, if one has to choose, investing in obtaining the affinity of a connection is more valuable than obtaining circle . If the affinity is known to be close then there is no value in also knowing that the circle is a friend (i.e., a social annotation from a close friend or family member has equal utility in our data). If the affinity is distant , however, then there is significant value in determining if the circle is work colleague (0.116 gain) or friend (0.243 loss).

Expertise, as expected, affects utility relatively more than other social aspects for health queries. With respect to music queries, expertise along with affinity , interest and the family circle equally most influence utility. Content that was shared has more influence on utility than other social aspects for music queries.
In the previous sections we have identified and examined the influence of social relevance aspects on the utility of so-cial annotations, and our user study confirmed that this in-fluence is of a differentiated and complex nature. In this sec-tion we aim to build on these results by asking the question: can we predict automatically whether a social annotation adds utility to a search result? To address this question, we develop discriminative models by learning from signals ob-tained in two different ways: (1) offline features, obtained from the social relevance aspects used in our user study in Section 4; and (2) online features, obtained from signals available at runtime in a commercial search engine, to ex-amine how well we can perform in our prediction task in a real-world scenario without access to features such as a con-nection X  X  circle and affinity or gold judgments on query class and content relevance .
Our 16 offline features are derived from the social rele-vance aspects presented in Section 3. The query class as-pects ( QA-CLS ) are mapped to five binary features, namely commerce , health , movie , music ,and restaurant . The so-cial aspects each become a categorical feature as follows: circle = { wkc , fam , frn } ; affinity = { cls , dst } ; exper-tise = { exp , nex } ; geo-distance = { nea , far , n/a }  X  interest-valence = { lik , shr , dis } . Finally, the content relevance aspects ( CA ) are mapped to six binary features, namely perfect , excellent , good , fair , bad , detrimental .
We turn now to features that are available to a search en-gine at runtime, which we call online features. Although no human annotator can provide relevance or query classi-fication judgments at runtime, most search engines today have proxies from automatic query classifiers [21] and con-tent rankers [19]. Social aspects are also generally unavail-able at runtime. However, there are a multitude of other measurements computed by search engines, and usage logs are routinely collected. The total number of features we collect in our online models runs at over 150. Below we describe the major classes of features and list examples. For all our prediction experiments we use the Multiple Additive Regression Trees (MART) [27] algorithm, which is based on the Stochastic Gradient Boosting paradigm [11]. We used log-likelihood as the loss function, steepest-descent as the optimization technique and binary decision trees as the fitting function. MART offers a range of crucial advan-tages: it has been proven to yield high accuracy, it does not require any feature normalization and can handle any mix of real-valued, multi-valued and binary features, and finally, through its use of decision trees it can handle non-linear de-pendencies between the features. The latter advantage is of particular importance in our case since we have already found in Section 4 that combinations of social relevance as-pects are predictive for social relevance. We cast our task as a supervised learning problem for predicting the utility of a social annotation t = { q,u,c,v } (see Section 3). For reasons of simplicity, we cast the prediction task as a binary task (i.e., a social annotation is either relevant or not). We use the data produced by our user study in Section 4. For each judged social annotation tuple t in the data, we extract a feature vector according to Sections 5.1 and 5.2. The online aggregate features are extracted from US English Web search usage logs from the same 3-month period as the samples drawn for our user study (see Section 4.1). The on-line features are directly obtained from the annotated tuples from our user study. We retain a total of 2380 HEAD tuples and 1371 TAIL tuples 5 .

For each tuple, we mapped the two annotator judgments from the space J to a binary value indicating whether the so-cial annotation in the tuple was relevant (1) or not (0). To this end, we use a conservative minimum-based approach, where we label any tuple as relevant if the minimum anno-12 of the 3763 tuples were discarded where online feature extraction failed. Figure 3: Prediction performance on HEAD using fea-ture sets: offline, online, online + social aspects. tated utility is some-util , i.e. where none of the judges has labeled the case as no-util .In HEAD this results in 909 class 0 (non-relevant) examples and 1471 class 1 (relevant) ex-amples. In TAIL we have 552 non-relevant and 819 relevant examples.

For the MART learner, we train for 100 iterations (result-ing in 100 trees) and restrict the decision tree stumps to 10 leaf nodes that cover a minimum of 25 samples. All reported results are based on 10-fold cross-validation.
We present the results of our prediction experiments sep-arately for HEAD and TAIL tuples since we observe systemat-ically different behavior between the two in our prediction task. Figures 3 and 4 illustrate the precision-recall char-acteristics of the relevance prediction with different feature sets. Overall, the task is learnable and as could be expected, the prediction model that uses offline features outperforms all other models on both HEAD and TAIL .

It is common in practice to impress a social annotation anytime one is available for a query-url context. Looking at the split between positive and negative examples in the test data, one would achieve 61.8% relevance precision on HEAD and 59.7% relevance precision on TAIL by impressing every available social annotation to a user. Using our offline model on HEAD , one could increase the rate of relevance by a factor of 13% while maintaining 88% recall, or by a factor of 25% at 50% recall. On TAIL , the same model would increase therateofrelevancebyafactorof7%at87%recall,orby a factor of 29% at 50% recall.

We analyzed the importance of the offline features in our model by observing the weights assigned by MART in its training log files. For HEAD , the social aspects were consis-tently ranked highest: circle, affinity and expertise are the three most important features, followed by content relevance judgments. For TAIL , the picture is different. Here, the content relevance features are dominant. The top-ranked feature is whether the content relevance is marked as bad , which is not surprising since instances of non-relevant urls are higher in TAIL and social annotations on irrelevant doc-uments have less utility. This relevance feature is followed in the importance ranking by a number of social aspects (affin-ity, circle and expertise), followed by whether the content relevance is excellent , followed by query class features.
We also observe that there is enough signal in the online features for predictions up into the 70-75% precision range, albeit at much lower recall rate than for offline features. Figure 4: Prediction performance on TAIL using fea-ture sets: offline, online, online + social aspects. Figure 5: Ablation of two most predictive online feature familes on HEAD : Query Classes and Results Metrics.
 Prediction ability is highest on TAIL nearly achieving the performance of offline features. Somewhat surprisingly, in both HEAD and TAIL , we observed that the Query Class features were by far the most predictive, followed by the Results Metrics features -see ablation results in Figure 5 for HEAD ( TAIL omitted for lack of space). In fact for HEAD queries, only using query class features produces results close to those using all online features. In TAIL , the picture is more differentiated, where adding other online features to the query classifier features improves results.

Based on this analysis of feature importance and the find-ing that online features are predictive but not as predictive as offline features, we also experimented with adding the most predictive offline features (aspect family) to the set of online features. For HEAD queries we added the social aspect features ( circle , affinity , expertise ,etc.),and for TAIL queries we added the content relevance features ( perfect , excellent , fair , etc.) Results are also shown in Figures 3 and 4. We observe that we can increase the performance of online features by adding social aspects and content relevance features. We take this as an encouraging result since proxies of both these feature families may be made available at runtime. Content relevance features such as PageRank scores can be assessed, which -while not as accurate as human judgments -may provide additional sig-nal. Similarly, social relevance features could be amenable to statistical modeling from observable data such as social network characteristics or profile modeling at runtime, an area that we plan to investigate in future research.
We presented a taxonomy of aspects that influence the perceived utility of social annotations in a Web search sce-nario, drawn from the query, social connection, and content relevance. Via a user study, we took a first step at quantify-ing the utility of social annotations and gained insights on the complex interplay between the social relevance aspects. We concluded that there are large variations in utility de-pending on the aspects in play and that these should be leveraged when deciding to impress a social annotation to a user. We learned that social aspects are most influential in perceived utility, in particular affinity, expertise and inter-est valence. We further established that close social connec-tions and experts in the search topic provide the most utility, whereas distant friends and friends that show no positive or negative interest valence provide the least utility, by a factor of over 50%.

We also showed that we can automatically predict whether a social annotation is relevant for a given query/url pair. We cast the task as a binary supervised learning problem over a stochastic gradient boosting model. In an offline ex-periment, we drew features from the manually labeled user study and established that we can accurately predict so-cial utility. In a configuration simulating an online scenario, we drew session-, query-, document-, and user-level features from query classifiers and web usage logs, which can be com-puted at runtime by commercial web search engines. In this online setting, we established the prediction task as learn-able and approaching the performance of the offline model. Finally, by adding the social aspects and content relevance aspects from the offline features to the online features, we gained predictive performance over just the online features.
A promising avenue of future work is to develop social aspects classifiers in order to increase our ability to predict the utility of a social annotation. Other directions include investigating the influence of other social aspects such as age, gender, user location, and school; and applying our framework to other social features such as interleaved re-sults. Perhaps most valuable, however, is to broaden our concept of utility, which we have limited to a search result, to the whole-page user experience, as outlined in Section 1, and further our understanding of how social features affect the overall information seeking, discovery, and sensemaking processes. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] S. Bao, G. Xue, and X. e. a. Wu. Optimizing web [3] C. L. Barry and L. Schamber. Users X  criteria for [4] P. Borlund. The concept of relevance in ir. JASIST , [5] C.J.C.Burges,K.M.Svore,P.N.Bennett, [6] D. Carmel, N. Zwerdling, I. Guy, S. Ofek-koifman, [7] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [8] C. Clarke, E. Agichtein, S. Dumais, and R. White. [9] B.M.EvansandE.H.Chi.Anelaboratedmodelof [10] R. Fidel and M. Crandall. Users X  perception of the [11] J. Friedman. Greedy function approximation: a [12] K. Haas, P. Mika, P. Tarjan, and R. Blanco. Enhanced [13] P. Heymann, G. Koutrika, and H. Garcia-Molina. Can [14] R. A. Hill and R. I. M. Dunbar. Social network size in [15] S. Huffman and M. Hochster. How well does result [16] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [17] T.-Y. Liu. Learning to rank for information retrieval. [18] A. Muralidharan, Z. Gyongyi, and E. H. Chi. Social [19] L. Page, S. Brin, R. Motwani, and T. Winograd. The [20] J. Pitkow and H. e. a. Sch  X  A  X  lutze. Personalized search. [21] D. Shen, J. Sun, Q. Yang, and Z. Chen. Building [22] J. Teevan, S. Dumais, and D. Liebling. To personalize [23] J. Teevan, S. T. Dumais, and E. Horvitz. Personalizing [24] J. Ugander, B. Karrer, L. Backstrom, and C. Marlow. [25] K. Wang, T. Walker, and Z. Zheng. Pskip: estimating [26] S. Wedig and O. Madnani. A large-scale analysis of [27] Q.Wu,C.J.Burges,K.M.Svore,andJ.Gao.

