
Background: A data warehouse defined in Chaudhuri and Dayal (1997) is a col-lection of data from multiple sources, integrated into a common repository and ex-tended by summary information (such as aggregate views), that is primarily used in organisational decision making. A class of queries that typically involves group-by and aggregation operators is called an online analytical processing or OLAP. OLAP software enables analysts and managers to gain insight into the performance of an enterprise and help decision making.

OLAP applications are dominated by ad hoc, complex queries. There are two approaches used in its implementation. Th e first approach uses the relational fea-ture of conventional databases, which is called the relational-OLAP (ROLAP). The other approach uses a data cube (Gray et al. 1996), known as MOLAP (Agrawal et al. 1997). Data-cube systems support a query style in which the data is best thought of as a multidimensional array, whic h is influenced by end-user tools such as spreadsheets, in addition to database query language. In the data-cube model, of interest. The remaining attributes are referred to as functional attributes or dimen-are combined (i.e. summed up) into aggregate values. Thus, a data cube could be viewed as a d -dimensional array, indexed by the values of the d dimension attributes, the cells of which contain the values of the measure attributes for the corresponding combination of dimension attributes.
 selection is specified as contiguous ranges in the domains of some of the attributes.
Two types of queries, range sum and range max , have been extensively studied in recent years (Ho et al. 1997; Lee et al. 2000).
 is sales data . The measure attribute is sales and the dimensions are item , day ,and branch . An example of a range-sum query is as follows: Find the total sales of stationery items that have an item code ranging from 1,201 to 1,300, between day 130 and 159 in the western outlets, with a branch number ranging from 45 to 89. This is a range-sum query, which can be realised using the following SQL-like statement:
SELECT SUM(amount) FROM sales WHERE ( 1,201  X  item  X  1,300 day  X  159 ) AND ( 45  X  branch  X  89 ) .
 avg , which is equal to range sum divided by the total count. For example, the average sales per stationery item, the average stationery sales per branch, etc., are all derived from the range sum of the above SQL query.

Scenario: There are three distinctive features that make our problem different from other applications in this field. 2. The data needs to be accessed quickly, as its response time is important. There-3. The only attributes that need to be protected are the measure attributes, whereas
Objective: Data privacy refers to the issue of how to preserve the confidential infor-mation in individual data cells while still be ing able to provide an accurate estimation data privacy: 1. Security X  X ny sensitive data must be protected from being revealed. 2. Accuracy X  X esults of any analyses are valuable in a business X  X  decision making. 3. Accessibility X  X  data warehouse is primarily built as an open system. In par-ticular, exploratory OLAP analysis requires this open nature and so there should be no unnecessary restriction o r control on the access to data.
However, data warehouses by their very nature create a security conflict as de-rity controls may hinder the analytical discovery process (refer to Priebe and Pernul (2000)). Another conflict is between security and accuracy of the query results. With the increase in the number of data warehouses and OLAP users, the misuse of data warehouse is steadily growing, which has led to the need for proper techniques to support all three goals of data privacy.

Justifications: Typically, people think of the issue of privacy as the need to protect individual data. Most effort has been in this direction. In a data-cube model, this pri-vacy problem can be scaled up to aggregated or collective information. That is, the release of a block (subcube) of information should not enable identification of cell information. Such collective information i s generally needed in the decision-making data being revealed when collective infor mation is released. Data privacy for data warehouses/data cubes is important for companies. An investment company can col-lect and hold the following data and information:  X 
Over a million customer accounts.  X 
In each account, the stocks a customer bought and the quantity, purchase_date and price of each purchase.  X  The stocks each custom er sold and the quantity, sale_date and price.
Much interesting and useful knowledge is contained in such an Investment-data revealed. The company may be willing to participate in a collaboration project but only if the protection of its own data can be ensured.

Contributions: The main contributions of this paper are summarised as follows: 1. We propose a simple but effective solution, called the zero-sum method, for pro-viding an accurate estimation of the orig inal values for range queries in a data cube while preserving the confidential information in individual data cells. Our method is based on random-data-distortion techniques and relative random-data distortion is applied. 2. We derive theoretical formulas to analyse the performance of our method. Also, we demonstrate that why a random matrix-based spectral filtering technique pro-posed by Kargupta et al. (2003) cannot be effectively applied to compromise our method, even if this filtering technique was designed to challenge the privacy-preserving approaches based on random data perturbation. 3. We conduct extensive experiments using a APB benchmark dataset from the
OLAP Council (1998). Various parameters, such as the privacy factor and the accuracy factor , have been considered and tested in the experiments. Our experi-mental results show that th ere is a trade-off between privacy preservation and range-query accuracy and our method has f ulfilled three design goals: security, accuracy and accessibility.

Overview: The remainder of this paper is organised as follows. Section 2 reviews related works. In Sect. 3, we introduce our distortion-based data-privacy-preserving approach. Theoretical formulas and some r elated matters are also discussed in this and future work are presented.
Compared with literature on accelerating response time in OLAP, OLAP privacy receives little attention. In contrast, there has been extensive research in the statistical databases community on the privacy problem (an excellent survey is in Adam et al. (1989)). For instance, there has been work on preserving privacy by intentionally distorting original data. The data-distortion method for security control has been extensively studied in the statistical database literature.
Data privacy is a concern common to both OLAP and statistical database (SDB) com-munities. This concern has b een recently extended to the data-mining area (Agrawal and Srikant 2000). The similarities and differences between OLAP and statistical databases were given in detail in Shoshani (1997). The similarities are that both
OLAP and SDB deal with multidimensional datasets and both are concerned with statistical summarisations over the dimensions of the data set.
 nity: 1. Query restriction  X  X he query-restriction category includes restricting the size of 2. Data perturbation  X  X he data-perturbation category includes swapping values be-tion methods are undesirable. Furthermore, data security may still be in danger from a set of smartly designed probes accessing the data cube.
 data-perturbation methods that are based on the use of the original data cube or queried results are also not feasible.
 of their attribute values needs to be preserved, swapping techniques are undesirable and unnecessary.
 in SDB may be useful for OLAP privacy preservation. This method was developed by Traub et al. (1984). The idea is to return a perturbed value by adding a random noise drawn from some distribution to the true value. Suppose, for example, that the true value of a given attribute (e.g. sales) of an entity k is x query, under this method, will be = n k = 1 y k ,where y perturbation variable with E ( z k ) = 0and Var ( z k ) =  X  different k  X  X .
Privacy-preserving data mining means ge tting valid data-mining results without learn-ing the underlying data values. In most applications, the privacy issue is somehow related to an individual or groups of individuals sharing some common character-may be used in a counterproductive manner that violates the privacy of an individual or group of individuals. Therefore, it is important to protect the privacy of the data and its context while mining.

In recent developments of privacy prese rving in data mining (refer to Agrawal and Srikant (2000)), the idea has been dealt for scrambling a customer X  X  personal data by randomisation and simultaneously reconstructing the original distributions of the values of the confidential attributes. Note that the goal is to reconstruct distribu-tions, not values in individual records. Thus, both the objectives of privacy protection and statistical-based rule accuracy can be achieved at the same time. In Agrawal and sociation rules. The work in Kantarcioglu and Clifton (2002) addresses the problem of association rule mining where transac tions are distributed across data sources.
Privacy preservation in data mining concentrates on the reconstruction of data distribution , whereas our concern is the reconstruction of aggregates . The underlying database used by data mining is not necessarily a data cube. Sparsity and response
Therefore, the methods developed for da ta mining are not adaptable because they are different application domains with different concerns and working environments.
Access control is an important security t echnique and is commonly used in opera-tional data sources to control the access to d ata sources (data warehouse and source databases). However, the relational model predominates in operational systems while
OLAP systems make use of the nontraditional multidimensional model. In OLAP it is not easy to map the traditional access control into O LAP systems.
In Priebe and Pernul (2000), different OL AP access-control requirements are pro-posed. These requirements are related to O LAP operations mentioned in the previous section. The main objective o f access control is to hide information in the data cubes.
The advanced requirements sa tisfy more security requirements, but also create more difficulties in implementation becau se of the increasing complexity.
With complex security requirements, information hiding will cause several prob-lems. For example, if certain slices are hidden, how should the higher level summary data be decided? Should the hidden slices be included? If including the hidden slices to reflect the real totals, the report will be left in an inconsistent state (the displayed total is not the summary of the displayed data). If the hidden slices are not included, only the total of the displayed values being shown, then the total will have to change from one query to another even though the queried domain remains the same. In fact, both approaches can be found in today X  X  commercial systems.
 outsiders to directly access the inner datab ase may result in the l eakage of sensitive information even with perfect access contro l. For example, some companies allow their customers or partners to access their data cubes for viewing aggregated data, such as monthly or yearly data. However, the highly sensitive individual data should be strictly protected.
 aggregations is investigated in Barbara and Sullivan (1997). The idea is to divide an additional estimation procedure being introduced to estimate the missing entries cube). The quasi-cube is designed to save s torage, but it does hide some individual entries. Queries of the quasi-cube could provide approximate answers by estimat-
However, processing the estimation procedure must be faster than computing the data from the underlying relations and a certain portion of the storage has to be used for the description.

SDB seems more attractive to us. The advantages of a distortion-based algorithm are as follows: 1. The method is simple. 3. The distorted data can be open to many users without introducing any access 4. Unlike the quasi-cube, which needs an additional estimation procedure, all ex-
However, negative results show that these techniques cannot provide High-quality statistics and prevent low disclosure of individual information at the same time (refer to Adam et al. (1989)). This is mainly b ecause the goal to provide high-quality estimates is at a point level, which greatly conflicts with the objective of preventing information disclosure, which is also set at the same point level.
In this section, we introduce our privacy-preserving method. The method we pro-posed is also based on random data distortion, but it is specifically aimed at solving the data-cube summation problem . Several terminologies will be defined before the privacy-preserving method is described.
Definition 1. A data cube  X  of d dimension is a d -dimensional array. For each dimension i ,thesizeis n i , which represents the number of distinct values for that dimension. Thus, the data cube consists of n 1  X  n 2  X  ...  X  be represented as  X  [ X 1 , X 2 , ..., X d ] where 0  X  X
Definition 2. Given a data cube  X  of d dimensions and the si ze of each dimension being n i ( 1  X  i  X  d ) , with d partition factor b 1 , partitioned into d i = 1 ( n i / b i ) disjoint subregions known as blocks or partitions .
Definition 3. The input to a range-sum query can be expressed as h ) ,where l i and h i ( 1  X  i  X  d ) denote the lowest and highest bound of the range occupied. Some cells are empty, i.e. the y contain NULL value because they actually correspond to no record. The density of a data cube is defined as Generally speaking, density ranges from 10 to 40% in the real dataset.
Example 1. Figure 1 shows a simple 2-dimensional data cube. The size of its di-mension is 9  X  16 . X  [ 3 , 11 ]= 129,  X  [ 6 , 4 ]= 71. Density partition factors are 3 and 4 for the dimension X 1 and X area indicates the query range .

There are four typical OLAP operations: 1. Drill up  X  X ata is summarised by climbing up the hierarchy or by dimension reduction. For example, drill up from the current view of a weekly report to a monthly report. 2. Drill down  X  X his is the reverse operation of drill up. It enables the user to nav-igate the data cube from a higher level summary to a lower level summary or detailed data. Through drill up/down, users can navigate among levels of data ranging from the most summarised to the most detailed. 3. Slice and dice  X  X  slice is a subset of a multidimensional array that corresponds to a single value for one or more members of the dimensions not in the subset.
A dice is a selection of some ranges over the dimensions. The operation of slice and dice helps the user to select one or more dimensions. 4. Rotate  X  X llows users to change the dimensional orientation of a report or page display. An example of the rotate operation is swapping the rows and columns.
From statistics literature, two popular scrambling methods, discretisation and value distortion , can be used: 1. Discretization  X  X he method used most often for hiding individual values. In this 2. Value distortion  X  X he basis of this method is to use a value, x tures. However, a distortion-based method cannot be effectively applied to OLAP directly. Because the OLAP cube is usually sparse, to simply apply the value dis-tortion, fixed-data perturbation will lead the distorted cube X  X  density to increase dra-matically, to almost 100%. With the multidim ensional characteristic of data cubes, changing one cell will affect several summation values. The distortion method ap-plied to OLAP must guarantee a certain degree of accuracy of range-sum queries.
This is not considered in SDB. If the query size is very large, the perturbed database tends to give a large error. Thus, our solution to this problem is to adjust the initially distorted data so that the accuracy of range -sum queries is close to 100%, especially when dealing with large queries. then adjusted so that all the marginal sums of each block are zeroes. These adjusted method. For illustration purpose, consider a case of a 2-dimensional data cube. The process of zero sum is to make adjustments on each row such that its (new) distor-also summed to zero.
 the OLAP operations. From this angle, the slice and dice and the drill up/down operations can be viewed as range queries.
 mation in each individual data cell while still being able to provide an accurate estimation of the original summation values for range queries. The data set is a 2-dimensional data cube with 9 blocks. The gray rectangle in Fig. 2 is a range query. This range query covers 9 blocks: A 1 , A 2 ,..., A
According to the zero-sum method, the ce lls in each block have been adjusted, so the marginal sums of a distorted block is equal to that of the corresponding original block. Let S be the sum of the range query over the original data and S be the sum of the distorted data. Let s i be the sum of the gray area in the block A distortion and s i be the sum of the gray area in the block A
It can be seen that S  X  S is a small fraction of S . That X  X  why the zero-sum method can help to provide an accurate es timation for range-sum queries.
There are several ways to enforce marginal sums to be zeroes. One of the ways each cell in the row such that the new sum becomes zero. This redistribution can be done uniformly so that each cell receives the same amount of adjustment. After the row adjustments are done, we repeat the same process for column adjustments.
This process converts the original distortions to zero-sum form, and a proof is given in Theorem 1.

Theorem 1. A block (partition) of k -dimension can be converted to zero-sum form with k iterations.

Proof. Let d i 1 , i 2 ,... , i k = initial distortion value for cell i 1  X  j  X  k .Let S ( i , i Similarly, is defined as the (marginal) sum of distortions at marginal point the same way, we can define the (marginal) sum of distortions for all other marginal points.

In the first iteration, we have d i 1 , i 2 ,... , i
Now, the (new) value S i 1 , i 2 ,... , i k  X  1 ,  X  becomes zero. Similarly, S becomes zero, and so on, for the sum of dis tortions at all other marginal points.
Repeat the process, and after the second iteration, the value S comes zero.

However, the value S i 1 , i 2 ,... , i k  X  1 ,  X  still remains zero. This is because
Similarly, after the third iteration, the values of S i 1
S And the same is true for all subsequent iterations.
 sums of dimension i ( 1  X  i  X  k ) can be adjusted to zeroes while all marginal sums of the k -dimensional block are zeroes.

Example 2. An example of zero-sum iterations on a 2-dimensional data cube is given in Fig. 3. Suppose that there is an original block with 7 rows and 5 columns. a randomly generated number within [  X  9 , 9]. The first iteration is to adjust the sum of each row to be zero. Each row has 5 cells and the sum of the first row is 11. The sum of the first row becomes zero by adding  X  ( 11 first row. Then we adjust all other rows in the same manner. After the first iteration, as shown in Fig. 3(b), the sum of each row is zero.
 adding  X  ( 1 . 4 / 7 ) = X  0 . 2 to each of the seven cells in this column. that the sum of each column is zero, and t he sum of each row also remains as zero. distortions to the corresponding cells in the original block, the value of all the cells
Therefore, after distortion, a certain de gree of the accuracy of the range-sum query can be guaranteed to some extent.

Alternatively, using formula (1) below, a block can be converted to zero-sum form by one iteration.
Example 3. The same example in Fig. 3 can be converted to zero-sum form by the d = 6, is converted to d then distortions can be reinstalled by rounding the numbers. The boundary values can be used to make the marginal sums remain as zeroes. This is illustrated by Fig. 4(c).

The time complexity for k iterations is k  X  n 1  X  n 2  X  ...  X  for 1-iteration is 2 k  X  n 1  X  n 2  X  ...  X  n k .

A special case is that all cells are initially distorted to a constant value. For are averaged and the average value is used for every cell in the block. In this special in the 2-dimensional case, the new value y ij at cell (i,j) becomes
In this section, we present theoretical analysis of the zero-sum method. We derive a bound on distortions that is useful when explained in probabilistic terms. A gener-alisation of the commonly known Chernoff  X  X  bounds (Motwani and Raghavan 1995) is used to derive the bound.
 Theorem 2. Let X i ,1 &lt; i &lt; n , be mutually independent random variables with all
E [ X i ]= 0 and all | X i | &lt; X  i .Let S n = X 1 + ... + x = X   X  paring the Taylor series of the two functions e  X  X  i and e and we get Because  X  = a /  X  2 i , the inequality becomes Hence, the claim holds.
 From Theorem 2, we can derive the following two corollaries.

Corollary 1. Under the same conditions as Theorem 2, we have Pr 2 e
Corollary 2. Under the same conditions as Theorem 2, if all
Pr [|
When we describe the accuracy measur ement in Sect. 4.1.2, Theorem 2 will be further discussed.
Security: Because the (original) cubes are not allowed for probing, it X  X  not pos-sible for snoopers to improve their estimation of the value of a field in a record by repeatedly placing queries (Adam et al. 1989).

It is shown in Faloutsos et al. (1997) that it is possible to fully recover original distribution from nonoverlapping, contiguous partial sums. However, it requires the ference between successive elements of the vector.) This smooth assumption is not true in general for data cubes.

Research work on estimating attribute distributions from partial information can be found in Barbara et al. (1997). Work on approximating queries on subcubes from higher level aggregations can be found in Barbara and Sullivan (1997). However, these works did not deal with information that has been deliberately distorted. Fur-thermore, their estimations require some additional statistical information of actual data.

Another security attack can come from the known marginal information. For ex-ample, in the case of 2-dimensional data cube s, the correct values of the sums of rows and the sums of columns are known. However, this known data is relatively much smaller than the unknown information. For example, in the case of 2-dimensional data cubes, it is impossible to solve a system of ( m + n variables, where m is the number of rows and n is the number of columns.
Recently, Kargupta et al. (2003) proposed a random matrix-based spectral fil-tering technique to challenge the privacy-preserving approaches based on random tering technique cannot be effectively applied to compromise our approach for the on the assumption that the eigenvectors of t he covariance matrix of the perturbed data are orthogonal to the eigenvectors of the covariance matrix of the original data. generate the perturbed data such that the eigenvectors of the covariance matrix of the perturbed data are not orthogonal to the eigenvectors of the covariance matrix of the original data. In other words, one of the major assumptions in this filtering approach is invalid in our paper. Second, if there is no prior knowledge of perturbed-data dis-the variance of the perturbed data. Even if this variance can be correctly estimated, the process for estimating the variance adds complexity and incr eases computation cost significantly. Finally, the filtering approach requires computing the eigenvalues a data cube, can be intractable.

Precomputing: The paper presented by Ho et al. (1997) introduces the idea of pre-computing multidimensional prefix sums of a data cube for speeding up the range-sum query. Prefix sum means, for any cell at a position d cube, summing up all the values in the range starting from the position 0 of d 1 , d 2 ,... , d n of the auxiliary data structure. Because this summation looks like a prefix of the summation of the whole data cube, it is given the name prefix sums .
By precomputing as many prefix sums as the number of elements in the original data cube, any range-sum query can be an swered by accessing and combining 2 appropriate prefix sums, where d is the number of dimensions for which ranges have been specified in the query. Precomputing h as no effect on the data security but only affects the tradeoff between response speed and storage space.

Size of block: Each dimension should have at least two values. The boundary of a block, if matched with a dimensional hierarchy, will guarantee that the drill-up operation is completely correct.

Sparsity: Sparse cubes are cubes in which many cells are empty. An empty cell zeroes or as missing values that are not counted. If they are treated as zeroes, after scrambling, a zero cell may become nonzero. This increases the d ensity of a block.
Also, counting empty cells as zeroes will affect the calculation on average. However, if empty cells are treated as uncounted, formula (1) cannot be guaranteed to obey zero-sum forms. This is because the number of counted cells in each row or column can be (very) different. So Theorem 1 is no longer valid. However, the experimental results show that such a nonstrict zero-sum form does not affect the accuracy much. Our zero-sum method simply leaves this row unadjusted.

Absolute data distortion vs. relative data distortion: Absolute data distortion is distorting original data by an absolute value, whereas relative data distortion is dis-of scale. For example, perturbing a salary of $15,000 by 3,000 would preserve the would be considered a compromise. Perturbing the data in a relative-distortion range is an alternative way to overcome the scale problem.
In this section, we present an experimental evaluation of the zero-sum method. After giving a brief description of our experimental setup and performance-evaluation mea-sures, we show the performance of the zero-sum method with respect to parameters, including initial distortion, sparsity and block size. Also, we illustrate the interactive effect between privacy and range-query accu racy. This section is then concluded with a summary of experimental results and discussions.
 Experimental data sets. The experimental data sets are generated using the APB
Benchmark program from OLAP Council (1998). There are four dimensions: cus-tomer, product, channel and time. The size of each dimension is 900 (customer), 9,000 (product), 9 (channel) and 17 (time). The measure attribute is dollar, with ing or uncounted. The overall density of the data cube is 20%. Hence, the file size of the data cube is 900  X  9,000  X  9  X  17  X  0 . 2 = 247,860,000 bytes
The original size of data file generated by APB Benchmark is approximately 15 G the data cube.
 Distortion range. Relative distortion range is applied to generate initial distortions.
For example, given a relative distortion range [0 , 50%] and a cell value distortion of this cell is randomly generated within range [ Experimental platform. Our experiments were performed on a PC with a Pentium
III 733 MHz, 40 G hard disk and 256 MBytes of memory.
To evaluate the performance of the zero-sum methods, two measures, privacy and accuracy , are used. The measure of privacy indicates how close ly the original value can be estimated.
Original distortions are usually generated by using a density function, f interval of [ 0 , X  ] and then assigning positive or negative values with equal probability to it. Therefore, the expectation of distortion variable, z ,is E of variable | z | is
The work in Agrawal and Srikant (2000) used a simple measure in terms of the amount of privacy, which is defined as follows: assume the original distortion is uniformly distributed in an interval of [ 0 , X  ] considered as the amount of privacy.

We generalise the privacy measure in Agrawal and Srikant (2000) to all types distribution has density function f ( z ) in an interval [ is defined as 2  X   X  0 zf ( z ) dz = 2 E ( | z | ) .
 f ( z ) in an interval [ 0 , X  ] , then the amount of privacy is 2 privacy of the adjusted distortions is . Because we do not know the density function of the adjusted distortions, we can only estimate the amount of privacy at cell i by using the difference between x i (the true original value) and y value). Based on the spirit of expectation and large-number theory, we will give the adjusted distortions.
 Privacy factor, F p , a block. The value 2  X  F p is considered as the amount of privacy of the adjusted distortions. It is interesting to compare the value 2  X  F into account the value of the original data. To quantify privacy accurately, we need a method that takes such information into account. A modified version of formula (3) is given as a block. The value 2  X  F c is considered as the amount of conditional privacy of the adjusted distortions. Accordingly, the conditional privacy of the original distor-will generate distortions that are (uniformly) distributed over an interval of
E ( | X | ) , of the distortion distribution.
The difference between the sum of the distorted values and the original values over a query Q is referred to as the accuracy loss of Q .Let true_sum be the sum of all original values of cells in query Q and answer be the sum of all distorted values.
We h av e t h e relative accuracy loss of Q equal to | answer  X  true _ sum of accuracy is defined as follows: Accuracy factor, F a , Q ,
Note that the accuracy factor lies betw een 0 and 1 (inclusive). Also, because | answer  X  tru e _ sum |=| z 1 + ... + z n | and | answer  X  and E [ z i ]= 0, we can derive the following expression by applying Corollary 1 of Theorem 2:
Formula (6) shows the relationshi p between the relative accuracy loss (denoted as a ) and privacy (denoted as  X  ). Note that it is also related to the size of the query (denoted as n ) and the sum of the query (expressed in terms of
If the relative distortion is taken into consideration, in which all
E [ z ]= 0, then formula (6) becomes the following:
Here, we tested the effect of query sizes using a fixed block size as 5
More specifically, we examined the performan ce of adjusted distortion as well as ori-ginal distortion. We applied uniform di stributions with the range [50%, 100%] for the initial distortion and generated 200 range-sum queries with the change of size from (50, 100) to (1,000, 2,000). The experimental results are shown in Table 1. In the table, the obtained average values fo r the privacy factor and the accuracy factor of 200 queries show that the zero-sum method yields better accuracy on data sets with adjusted distortion, as well as satisfactory privacy.
 60% and the distortion range to be [50%, 100%]. In this experiment, 200 range-sum queries with query size in [200, 1,000] were generated. Figure 5(a) shows the obtained privacy values for adjusted distortion and original distortion. Although the privacy values of adjusted distortion are smaller than that of original distortion when of original distortion. Figure 5(b) shows th at the accuracy values of adjusted distor-distortion regardless of the block size.

To test the effect of cube sparsity, we arranged the block size to be 10 and the distortion range to be [50%, 100%]. In this experiment, we applied 200 range-sum queries with a query size of [200, 1,000]. Figure 6(a) shows the obtained privacy values of adjusted distortion and original distortion. As shown in the figure, when sparsity is as high as 80%, the privacy of original distortion is better than that observed that the privacy of adjusted distortion is increased with the decrease of sparsity.
 significantly better than that of original distortion in all ranges of the cube sparsity.
In addition, the accuracy of original dist ortion is slightly incr eased with the decrease of sparsity.
 the cube sparsity does not affect greatly the accuracy of adjusted distortion.
In the experiment of examining the effect of distortion range, we arranged the block size to be 10  X  10  X  3  X  4 and the cube sparsity to be 60%. We applied 200 range-sum queries with query sizes between 200 and 1,000. We then examined the average privacy and accuracy of these 200 queries with the change of distortion ranges. Fig-ure 7(a) shows the privacy performance of the zero-sum method on both adjusted and original distortions when changing the lower bound of distortion range. In the figure, it is not surprising to see a trend that the privacy values of both adjusted dis-tortions and original distortions are inc reased with the increase of the lower bound of relative distortion ranges, because higher rel ative distortion can bring better privacy preservation. Also, we observed that the obtained privacy of adjusted distortions is systematically better than that of original distortion.

Figure 7(b) shows the accuracy performance of the zero-sum method on both adjusted and original distortions when changing the lower bound of distortion range. creased with the increase of the lower bound of distortion range. In contrast, chang-ing distortion ranges does not affect signi ficantly the accuracy of adjusted distortion.
The same effect on privacy and accuracy is observed when changing only the summary, no matter how we change the dis tortion range, the privacy and accuracy achieved by adjusted distortion are bette r than the privacy and accuracy obtained by original distortion.
This experiment evaluated the interactive effect between privacy and accuracy with
Figure 9(a) shows the privacy and accura cy values with the change of distortion ranges. The privacy of adjusted distortion is increased as the lower bound of distor-lower bound of distortion ranges. In other words, there is a trade-off between privacy and accuracy. The same trade-off effect can also be observed in Figs. 9(b) and 9(c).
These two figures show the privacy and accuracy with the changes of cube sparsity and block sizes, respectively.
Summary: Several major experimental results are summarised as follows.  X 
The accuracy of adjusted distortions is si gnificantly and systematically better than the accuracy of original distortions. T his is due to the fact that the zero-sum method constrains the marginal sums of distortions to be zero. In Sect. 3.2.1, we have shown in Fig. 2 that the partially c overed blocks may yield the inaccuracy of range-sum query. Data sets with adjusted distortions may guarantee yielding higher accuracy of th e range-sum query.  X 
Based on our experimental results, using relative data distortion, our zero-sum method obtains better privacy in many cases than the privacy of original dis-tortions. This is because each marginal su m of the original distortion is redis-tributed back to each cell in the block. If the same amount of adjustment is applied to two different-valued cells, the cell with smaller value has more sig-nificant change in comparison with the change in the cell with the larger value.
Therefore, when computing relative pri vacy, the gains by smaller-valued cells will exceed the losses by larger-valued cells. Thus, in general, our method can achieve better results.  X 
When the zero-sum method is applied, th ere is a trade-off between privacy and accuracy: higher privacy preserves bette r the confidentiality of the data by scar-cifying the accuracy of range-sum query.

Discussions: We now address some limitations of the zero-sum method and several issues related to its experimental evaluation.  X 
The cost of data preparation for the zero-sum method is relatively high. For instance, it took about 20 hours to create the original distortion and adjusted distortion from the raw data cube generated by APB Benchmark. However, this is only one time expense for a data cube, and the subsequent query response time is not affected. For OLAP applications, the query response time is rather more critical. In contrast, many other privacy-preserving methods, such as query restriction or access control, require extra processing time each time when an-swering a query. This additional processing time will significantly slow down the response time.  X 
A common concern among people is to figure out what level of privacy would be reasonable for practical purposes. Generally speaking, a 100% privacy with 5% to 10% loss of accuracy is considered to b e a satisfactory lev el. Another indicator as a good level of privacy is 25%. This is because many people consider 50% of distortion as the maximum tolerance for bias; beyond 50% is considered as too biased (Adam et al. 1989). Therefore, if we take half of the maximum tolerance for bias, the level of privacy a t 25% is considerably acceptable.
In this paper, we proposed an effective approach, called the zero-sum method, for preserving data privacy in data cubes. Th is method provides an accurate estimation of the summation for range queries while preserving the confidential information in random data distortion techniques rather tha n simple unadjusted original random data distortion techniques. We provided a theoretical analysis of the performance of the proposed method. Our experimental results showed that our method can achieve both better privacy preservation and range-que ry accuracy than the unadjusted original random data distortion method.
 ity of the zero-sum method is k  X  n 1  X  ...  X  n k ,where k is the number of dimensions for higher dimensional data cubes. There may be a way to improve the computation performance of the proposed method. Second, we analysed privacy preservation for range-sum queries in a data cube. The privac y preservation for other query types should also be investigated for the sake of practical applications.
