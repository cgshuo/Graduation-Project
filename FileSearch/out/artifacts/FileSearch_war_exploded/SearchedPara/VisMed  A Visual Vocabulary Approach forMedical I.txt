 Medical images are an integral part in medical diagnosis, research, and teaching. Medical image analysis research has focused on image registration, measurement, and visualization. Although large amounts of medical images are produced in hospitals every day, there is relatively less research in medical content-based im-age retrieval (CBIR) [1]. Besides being valuable for medical research and train-ing, medical CBIR systems also have a role to play in clinical diagnosis [2]. For instance, for less experienced radiologists, a common practice is to use a refer-ence text to find images that are similar to the query image [3]. Hence, medical CBIR systems can assist doctors in diagnosis by retrieving images with known pathologies that are similar to a patient X  X  image(s).
 tering driven feature selection and weighting has received much attention as general visual cues often fail to be discriminative enough to deal with more sub-tle, domain-specific differences and more objective ground truth in the form of disease categories is usually available [3,4].
 local features such as those extracted from segmented dominant image regions approximated by best fitting ellipses have been proposed [5]. A hierarchical graph-based representation and matching scheme has been suggested to deal with multi-scale image decomposition and their spatial relationships [5]. How-ever, it has been recognized that pathology bearing regions cannot be segmented out automatically for many medical domains [1]. As an alternative, a compre-hensive set of 15 perceptual categories related to pathology bearing regions and their discriminative features are carefully designed and tuned for high-resolution CT lung images to achieve superior precision rates over a brute-force feature selection approach [1].
 in terms of semantic local features, that can be learned from examples (rather than handcrafted with a lot of expert input) and do not rely on robust region segmentation. In this paper, we propose a structured learning framework to build meaningful medical terms associated with visual appearance from image sam-ples. These VisMed terms span a new feature space to represent medical image contents. After a segmentation-free multi-scale detection process, a medical im-age is indexed as compact spatial distributions of VisMed terms. A flexible tiling (FlexiTile) matching scheme is also proposed to compare the similarity between two medical images of arbitrary aspect ratios.
 known as the Lung Image Database Consortium (LIDC) to develop an image database that will serve as an international research resource for the develop-ment, training, and evaluation of computer-aided diagnostic (CAD) methods in the detection of lung nodules on CT scans. One of the key efforts is to create a visual nodule library with images of lesions that span the focal abnormality spectrum and the subset nodule spectrum [6]. All lesions have been characterized by a panel of experienced thoracic radiol ogists based on attributes that include shape, margin, internal structure, and subtlety. The library is intended to serve as a standard for the development of a practical radiologic definition of nodule as radiologists believe that  X  X he expertise of the interpreter lies in a vast experi-ence of seeing many thousands of radiologi c patterns and sythesizing them into a coherent, organized, and searchable mental matrix of diagnostic meaning and pathologic features X  [7].
 retrieval task. Based on 2% of the 8725 CasImage data, we cropped 1170 image regions to train and validate 40 VisMed terms using support vector machines [8]. The Mean Average Precision over 26 query topics is 0 . 4156, an increase over all the automatic runs in ImageCLEF 2004. We detail the VisMed framework and evaluation in the next two sections respectively. In this paper, we aim to bridge the semantic gap between low-level visual features (e.g. texture, color) and high-level semantic terms (e.g. brain, lung, heart) in medical images for content-based indexing and retrieval. At the moment, we focus on visual semantics that can be directly extracted from image content (without the use of associated text) with computer vision techniques. diversity) in the medical domain, we propose a structured learning framework to facilitate modular design and extraction of medical visual semantics, VisMed terms, in building content-based medical image retrieval systems.
 meanings to medical practitioners and that can be learned statistically to span a new indexing space. They are detected in image content, reconciled across multi-ple resolutions, and aggregated spatially to form local semantic histograms. The resulting compact and abstract representation can support both similarity-based query and compositional visual query efficiently. In this paper, we only report evaluation results for similarity-based retrieval (i.e. query by image examples). For the unique compositional visual qu ery method and its evaluation based on consumer images, please refer to another regular paper in the same proceeding [9]. We have also performed the semantic-based query based on the composi-tional visual query method for ImageCLEF 2005 queries and dataset. We will report this work elsewhere in the near future. 2.1 Learning of VisMed Terms VisMed terms are typical semantic tokens with visual appearance in medical im-ages (e.g. X-ray-lung, CT-head-brain, MRI-abdomen-liver, mouth-teeth). They are defined using image region instances cropped from sample images and mod-eled based on statistical learning.
 port vector machines (SVMs) [8] for VisMed term representation and learning respectively though the framework is not dependent on a particular feature and classifier. The notion of using a visual vocabulary to represent and index image contents for more effective (i.e. semantic) query and retrieval has been proposed and applied to consumer images [10,11].
 and texture features for an image region and denote this feature vector as z .A SVM S k is a detector for VisMed term k on z . The classification vector T for region z is computed via the softmax function [12] as That is, T k ( z ) corresponds to a VisMed entry in the 40-dimensional vector T adopted in this paper.
 RGB, HSV, LUV) as it performed better in our experiments. For the texture feature, we adopted the Gabor coefficients which have been shown to provide excellent pattern retrieval results [13].
 texture feature vector z t . We compute the mean and standard deviation of each YIQ color channel and the Gabor coefficients (5 scales, 6 orientations) respec-tively [11]. Hence the color feature vector z c has 6 dimensions and the texture feature vector z t has 60 dimensions. Zero-mean normalization [14] was applied to both the color and texture features. In our evaluation described below, we adopted RBF kernels with modified city-block distance between feature vectors y and z , where N c and N t are the numbers of dimensions of the color and texture fea-ture vectors (i.e. 6 and 60) respectively. This just-in-time feature fusion within the kernel combines the contribution of color and texture features equally. It is simpler and more effective than other feature fusion methods that we have attempted. 2.2 Image Indexing Based on VisMed Terms After learning, the VisMed terms are detected during image indexing from multi-scale block-based image patches without region segmentation to form semantic local histograms as described below.
 processing architecture. The bottom layer denotes the pixel-feature maps com-puted for feature extraction. In our experiments, there are 3 color maps (i.e. YIQ channels) and 30 texture maps (i.e. Gabor coefficients of 5 scales and 6 orientations). From these maps, feature vectors z c and z t compatible with those adopted for VisMed term learning (Equation (2)) are extracted.
 be indexed, the image is scanned with windows of different scales, similar to the strategy in view-based object detection [15,16]. More precisely, given an image I with resolution M  X  N , the middle layer, Reconciled Detection Map (RDM), has a lower resolution of P  X  Q, P  X  M, Q  X  N .Eachpixel( p, q )inRDM corresponds to a two-dimensional region of size r x  X  r y in I . We further allow tessellation displacements d x ,d y &gt; 0in X, Y directions respectively such that adjacent pixels in RDM along X direction (along Y direction) have receptive fields in I which are displaced by d x pixels along X direction ( d y pixels along Y direction) in I . At the end of scanning an image, each pixel ( p, q )thatcoversa region z in the pixel-feature layer will consolidate the classification vector T k ( z ) (Equation (1)).
 20  X  20 to 60  X  60 at a displacement ( d normalized image. That is, after the detection step, we have 5 maps of detection of dimensions 23  X  35 to 19  X  31, which are reconciled into a common RDM as explained below.
 basis, we adopt the following principle: If the most confident classification of a region at resolution r is less than that of a larger region (at resolution r +1) that subsumes the region, then the classification output of the region should be replaced by those of the larger region at resolution r + 1. For instance, if the detection of a face is more confident than that of a building at the nose region (assuming that both face and building (but not nose) are in the visual vocabulary designed for a particular application), then the entire region covered by the face, which subsumes the nose region, should be labeled as face. regions at resolution r +1 as shown in Figure 1. Let  X  = max k max i T i ( z r +1 k ) where k refers to one of the 4 larger regions in the case of the example shown in Figure 1. Then the principle of reconciliation says that if max i T i ( z r ) &lt; X  , the classification vector T i ( z r )  X  i should be replaced by the classification vector T tions at a time, in descending window sizes (i.e. from windows of 60  X  60 and 50  X  50 to windows of 30  X  30 and 20  X  20). After 4 cycles of reconciliation, the detection map that is based on the smallest scan window (20  X  20) would have consolidated the detection decisions obt ained at other resolutions for further spatial aggregation.
 outcome in a larger spatial region. Suppose a region Z comprises of n small equal regions with feature vectors z 1 ,z 2 ,  X  X  X  ,z n respectively. To account for the size of detected VisMed terms in the spatial area Z , the classification vectors of the reconciled detection map are aggregated as tecture where a Spatial Aggregation Map (SAM) further tessellates over RDM with A  X  B, A  X  P, B  X  Q pixels. This form of spatial aggregation does not encode spatial relation explicity. But the design flexibility of s x ,s y in SAM on RDM (the equivalent of r x ,r y in RDM on I ) allows us to specify the location and extent in the content to be focused and indexed. We can choose to ignore unimportant areas (e.g. margins) and emphasize certain areas with overlapping tessellation. We can even have different weights attached to the areas during similarity matching.
 ratios  X  , we design 5 tiling templates for Eq. (3), namely 3  X  1, 3  X  2, 3  X  3, 2  X  3, and 1  X  3gridsresultingin3,6,9,6,and3 T k ( Z ) vectors per image respectively. Since the tiling templates have aspect ratios of 3, 1 . 5, and 1, the decision thresh-olds to assign a template for an image are set to their mid-points (2 . 25 and 1 . 25) as  X &gt; 2 . 25, 1 . 25 &lt; X   X  2 . 25, and  X   X  1 . 25 respectively based on  X  = L S where L and S refer to the longer and shorter sides of an image respectively. For more details on detection-based indexing, readers are referred to [11]. 2.3 FlexiTile Matching Given two images represented as different grid patterns, we propose a flexible tiling (FlexiTile) matching scheme to cover all possible matches. For instance, given a query image Q of 3  X  1 grid and an image Z of 3  X  3 grid, intuitively Q should be compared to each of the 3 columns in Z and the highest similarity will be treated as the final matching score. As another example, consider matching a 3  X  2gridwith2  X  3 grid. The 4 possible tiling and matching choices are shown in Fig. 2.
 image Q and a database image Z are represented as M 1  X  N 1 and M 2  X  N 2 grids respectively. The overlaping grid M  X  N where M =min( M 1 ,M 2 )and N =min( N 1 ,N 2 ) is the maximal matching area. The similarity  X  between Q and Z is the maximum matching among all possible M  X  N tilings, where u 1 = M 1  X  M +1 ,v 1 = N 1  X  N +1 ,u 2 = M 2  X  M +1 ,v 2 = N 2  X  N +1 and the similarity for each tiling  X  ( Q m 1 ,n 1 ,Z m 2 ,n 2 ) is defined as the average similarity over M  X  N blocks as puted based on L 1 distance measure (city block distance) as, where p 1 = m 1 + i, q 1 = n 1 + j, p 2 = m 2 + i, q 2 = n 2 + j and it is equivalent to color histogram intersection except that the bins have semantic interpretation as VisMed terms.
 want images of similar semantics with different spatial arrangement (e.g. mirror images) to be treated as similar, we can have larger tessellated block in SAM (i.e. the extreme case is a global histogram). However in applications such as medical images where there is usually very small variance in views and spatial locations are considered differentiating across images, local histograms will provide good sensitivity to spatial specificity. Furthermore, we can attach different weights to the blocks to emphasize the focus of attention (e.g. center) if necessary. In this paper, we report experimental results based on even weights as grid tessellation is used. As part of the Cross Language Evaluation Forum (CLEF), the ImageCLEF 2004 track [17] that promotes cross language image retrieval has initiated a new medical retrieval task in 2004. The goal of the medical task is to find images that are similar with respect to modality (e.g. Computed Tomography (CT), Magnetic Resonance Imaging (MRI), X-ray etc), the shown anatomic region (e.g. lung, liver, head etc) and sometimes with respect to the radiologic protocol (e.g. T1/T2 for MRI (contrast agents alter selectively the image intensity of a particular anatomical or functional region)).
 anonymized medical images, e.g. scans, and X-rays from the University Hos-pitals of Geneva (visit www.casimage.com for example images). Most images are associated with case notes, a written English or French description of a pre-vious diagnosis for an illness the image identifies. The case notes reflect real clinical data in that it is incomplete and erroneous. The medical task requires that the first query step has to be visual (i.e. query by image example). Although identifying images referring to similar medical conditions is non-trivial and may require the use of visual content and additional semantic information in the case notes, we evaluate the VisMed approach on the medical task without using the case notes in this paper.
 the help of a radiologist which represented the database well. Each topic is denoted by a query image (Fig. 3). An image pool was created for each topic by computing the union overlap of submissions and judged by three assessors to create several assessment sets. The task description of the 26 topics given to the assessors is listed in Table 1. The relevance set of images judged as either relevant or partially relevant by at least two assessors is used to evaluate retrieval performance in terms of uninterpolated mean average precision (MAP) computed across all topics using trec eval . The sizes of the relevance sets for each topic are listed in the rightmost column in Table 1.
 ageCLEF 2004 benchmark. We designed 40 VisMed terms that correspond to typical semantic regions in the CasImage database (Table 2). While the first 30 VisMed terms are defined on grey-level images, the last 10 VisMed terms are for the minority of color images. Note that  X  X rint-sketch X  and  X  X rint-slide X  refer to drawing and text in presentation slides respectively. With a uniform VisMed framework, dark background in the scan images (e.g. CT, MRI) and empty ar-eas in drawing etc are simply modeled as dummy terms instead of using image preprocessing to detect them separately.
 image regions with 20 to 40 positive samples for each VisMed term (Table 2). For a given VisMed term, the negative samples are the union of the positive samples of all the other 39 VisMed terms. The 172 images are selected to cover different appearances of the 40 VisMed terms. We ensure that they do not contain any of the 26 query images though Q06, Q15, Q16, and Q22 have a total of 7 duplications in the database.
 validation sets respectively (i.e. 585 each) to optimize the RBF kernel parameter of support vector machines. The best gener alization performance with mean error 1 . 44% on the validation set was obtained with C = 100 , X  =0 . 5 [18]. Both the training and validation sets are then combined to form a larger training set to train a new set of 40 VisMed SVM detectors.
 work as described in the previous section (Eq. (1) to (6)). However, to avoid spurious matching between very different grids (e.g. 3  X  1and1  X  3), we set the similarity to zero if the difference in a grid dimension between two image indexes is more than one. That is, two images are considered dissimilar if they exhibit very different aspect ratios. The MAP over 26 query topics is computed using the same trec eval program used in ImageCLEF 2004.
 the top 5 automatic runs as reported in ImageCLEF 2004 [17] where the per-centages of improvement are shown in brackets,  X  X F X  stands for the use of pseudo relevance feedback and  X  X ext X  means the case notes were also utilized. The group  X  X uffalo X ,  X  X mperial X , and  X  X achen-inf X  refer to State Univ. of New York (USA), Imperial College (UK), and Dept. Medical Informatics, RWTH, Aachen (Germany) respectively. All these systems used low-level visual features for image indexing and matching. Details are given in their working notes at http://clef.isti.cnr.it/ . The best run in ImageCLEF 2004 was a manual run with a MAP value of 0 . 4214 by the University Hospitals of Geneva. attained a MAP of 0 . 4156, clearly an improvement over all the automatic runs in ImageCLEF 2004. the average precisions at top 10, 20, 30 and 100 retrieved images of the VisMed approach are 0 . 70, 0 . 65, 0 . 60, and 0 . 41 respectively (similar results for the runs of ImageCLEF 2004 are not available), which we consider reasonable for practical applications.
 grids of 40 dimensional vectors) of semantically meaningful terms, the VisMed approach also has the following advantages: enables efficient matching, provides explanation based on VisMed terms that are detected and matched, and sup-ports new compositional queries expressed as spatial arrangement of VisMed terms [11].
 Medical CBIR is an emerging and challenging research area. We have proposed a structured framework for designing image semantics from statistical learning. Using the ImageCLEF 2004 CasImage medical database and retrieval task, we have demonstrated the effectiveness of our framework that is very promising when compared to the current automatic runs [17]. Indeed our adaptive frame-work is scalable to different image domains [11,19] and embraces other design choices such as better visual features, learning algorithms, object detectors, spa-tial aggregation and matching schemes when they become available.
 to design the VisMed terms manually with labeled image patches as training samples. We have begun some work in a semi-supervised approach to discover meaningful visual vocabularies from minimally labeled image samples [20]. In the near future, we would also explore the integration with inter-class semantics [19] and other source of information such as text. Last but not least, we would also work with medical experts to desi gn a more comprehensive set of VisMed terms to cover all the essential semantics in medical images.
 We would like to thank Mun-Kew Leong and Changsheng Xu for their feedback on the paper. We also thank T. Joachims for making his SV M light software available.

