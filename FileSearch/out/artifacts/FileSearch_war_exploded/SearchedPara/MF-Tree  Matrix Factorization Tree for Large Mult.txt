 Many big data applications require accurate classification of ob-jects into one of possibly thousands or millions of categories. Such classification tasks are challenging due to issues such as class im-balance, high testing cost, and model interpretability problems. To overcome these challenges, we propose a novel hierarchical learn-ing method known as MF-Tree to efficiently classify data set-s with large number of classes while simultaneously inducing a taxonomy structure that captures relationships among the class-es. Unlike many other existing hierarchical learning methods, our approach is designed to optimize a global objective function. We demonstrate the equivalence between our proposed regularized loss function and the Hilbert-Schmidt Independence Criterion (HSIC). The latter has a nice additive property, which allows us to decom-pose the multi-class learning problem into hierarchical binary clas-sification tasks. To improve its training efficiency, an approximate algorithm for inducing MF-Tree is also proposed. We performed extensive experiments to compare MF-Tree against several state-of-the-art algorithms and showed both its effectiveness and effi-ciency when applied to real-world data sets.
Multi-class learning is a pervasive problem in the current age of big data, cutting across a broad range of applications, includ-ing text categorization [26, 19], image recognition [11, 24, 14], video classification [18, 23], and malware detection [9, 8, 25, 7]. In addition to the massive volume of data that must be handled, the classification tasks in these applications involve thousands or possibly millions of classes. Traditional multi-class learning meth-ods, based on one-versus-one or one-versus-all strategies, require building O ( c ) or O ( c 2 ) binary classifiers, where c is the number of classes. Such methods are computationally expensive during test-ing since one must apply a large number of classifiers to predict the class of a test instance.

Hierarchical learning methods [1, 2, 10, 28], which organize the binary classifiers into a tree-like structure, have been developed to overcome this limitation. The advantages to these methods are three-fold. First, they can reduce model testing to sublinear time in c  X  c since the test complexity depends on the maximum depth of the tree. Second, they could help alleviate the class imbalanced prob-lem by learning to discriminate groups of related classes instead of individual classes (except at the leaf nodes of the tree). Finally, the hierarchical structure induced by such methods may offer interest-ing insights into the semantic relationships among the classes.
Numerous hierarchical learning methods have been developed in recent years, from greedy top-down decision tree classifiers that use simple attribute test conditions as internal nodes of the tree, to more advanced methods that employ binary classifiers such as SVM as their internal nodes. Each of these methods has its own pros and cons. While decision tree classifiers are easy to construct, they are susceptible to the class imbalanced and data fragmentation prob-lems. In addition, a class can be assigned to multiple leaf nodes of the tree, resulting in an unnecessarily large and complex tree. This affects both its runtime for testing as well as interpretability of it-s hierarchical structure as a class taxonomy. Alternative methods, such as filter tree [2] and its variants, impose a random tree struc-ture on the classes and learn a set of binary classifiers to route the classes to their appropriate leaf nodes in the tree. Because of the arbitrariness in which the classes are assigned to the leaf nodes, the resulting tree might be suboptimal and loses descriptive informa-tion about the class relationships. More importantly, both types of methods were not designed to optimize a global objective function. To address these limitations, we propose a novel method known as matrix factorization tree (MF-Tree, in short) to organize the classes into a binary tree by solving a global objective function based on a regularized loss function.

We provide a theoretical proof to demonstrate the equivalence between the proposed loss function and the Hilbert-Schmidt Inde-pendence Criterion (HSIC) [13][3][30]. HSIC is a recently pro-posed criterion for taxonomy learning in an unsupervised learning setting. In this paper, we demonstrate how HSIC can be applied to a supervised multi-class learning problem. We also showed the additive property of the objective function, which allows us to de-compose the problem into a hierarchical matrix factorization task that can be represented as a binary tree. Since the tree is construct-ed by partitioning the classes into disjoint subclasses, the resulting tree provides a good representation of a class taxonomy as each class resides in exactly one leaf node of the tree.

For massive classification problems involving thousands of class-es or more, factorizing the matrices is computationally expensive especially at the top levels of the tree. To address this problem, we present an approximate algorithm for inducing MF-Tree, where the partitioning of classes at the top level of the hierarchy is performed using a fast method instead of solving a matrix factorization prob-lem involving large number of classes. The more expensive matrix factorization can be applied at the lower levels of the tree when there are fewer number of classes and training examples to be dealt with. We showed that the accuracy of the approximate MF-Tree is quite comparable to that of MF-Tree, but with a significant im-provement in its training time. In short, the main contributions of this paper are as follows:
Classification with large number of classes is an important but challenging problem, and has received increasing attention in re-cent years [10][18][12]. Existing methods for multi-class learning employ a collection of binary classifiers, organized either in a non-hierarchical (flat) structure or a hierarchical tree architecture.
Flat (Non-hierarchical) classification methods, including one-versus-all [29], one-versus-one [15], and error-correcting output coding (ECOC) [21], require invocation of all the binary classifiers dur-ing testing time in order to determine the final prediction. Both one-versus-one and one-versus-all methods are inefficient when the number of classes is large due to the linear and quadratic number of binary classifiers that must be invoked during testing. The ECOC method assigns a unique m bit codeword to each class label (where m &gt; log 2 c ). Given a test instance, we must generate an m -bit vec-tor by applying m binary classifiers and then compute its distance to the codewords of all c classes, which is an expensive operation.
Hierarchical methods [28, 2, 1, 10] help to reduce the number of classifiers that need to be invoked during the testing phase by organizing the classifiers in a hierarchical structure. Their runtime for testing depends on the depth of the structure. For example, the Decision Directed Acyclic Graph (DDAG) [28] approach arranges the one-versus-one classifiers in a rooted binary DAG in order to reduce the runtime complexity from O ( c ( c  X  1) / 2) to O ( c ) . In fact, it is possible to achieve sublinear complexity (e.g., O (log c ) ) by assigning each class to a single leaf node of the hierarchy. Such a strategy was adopted by label tree learning methods such as [11, 1, 10], which constructs a linear classifier at each node to assign training instances to their corresponding child nodes. With this ap-proach, only the binary classifiers located along the path from the root node to one of the leaf nodes are invoked when determining the class label of a test instance.

Hierarchical methods vary in terms of how their structure and the associated classes of each leaf node are learned from training da-ta. For example, Bengio et al. [1] developed a label embedding tree approach by training c one-vs-all classifiers to obtain an initial con-fusion matrix, which is then used as the affinity matrix for applying spectral clustering to partition the classes into smaller subgroups. Deng et al. [10] presented an alternative strategy that simultaneous-ly learns the class partitioning and the weights of the associated lin-ear classifiers by optimizing their joint objective function. Howev-er, the approach requires solving an integer programming problem, which is NP-hard. Instead they relaxed the optimization problem into a linear program in order to find a polynomial time solution. Shi et al. [11] also introduced a relaxed hierarchical learning ap-proach for large-scale visual recognition. At each node, the classes are colored into positive and negative labels by a binary classifier while a subset of confusing classes are ignored. The coloring of classes and training of binary classifier is done simultaneously us-ing a max-margin optimization approach. Lei et al. [22] proposed a label tree learning approach that allows for soft partitioning of the classes by minimizing an information theoretic loss function. Un-like other previous works, the leaf node of the tree may contain one or more one-class SVM models to distinguish the instances from different classes. Despite the extensive work, the main drawbacks of existing hierarchical methods is that they either assume the hier-archical structure is given [2] or they allow a class to be assigned to multiple leaf nodes of the tree. As noted in the previous section, the latter produces trees that are larger than necessary and affects its interpretability.

Hierarchical methods have also been widely used for unsuper-vised learning, starting with classical methods such as agglomera-tive hierarchical clustering. Kuang et al. [20] developed a hierar-chical clustering method based on rank-2 nonnegative matrix fac-torization. The authors proposed a fast algorithm based on the ac-tive set method to solve their optimization problem. Their method also spent considerable amount of time detecting outliers to avoid generating too many leaf nodes with few instances. However, sim-ilar to classical hierarchical clustering, there is no global objective function solved by their algorithm. Taxonomy learning is another related area of our work. A family of unsupervised learning algo-rithms for taxonomy learning [13][3][30][4] have been proposed to cluster data into a hierarchical structure. Our work is inspired by the Hilbert-Schmidt Independence Criterion (HSIC) metric em-ployed by these works. We showed how HSIC is related to our global objective function for matrix factorization, which enables us to decompose the problem into a hierarchical tree learning task.
This section presents a formal definition of the multi-class learn-ing problem. We also introduce the concepts of Hilbert-Schmidt Independence Criterion and the tree-structured covariance matrix. The connections between these concepts and the proposed MF-tree method will be shown in Section 4.
Let D = { ( x 1 ,y 1 ) , ( x 2 ,y 2 ) ,..., ( x n ,y n ) } ing instances sampled from an unknown distribution P over Y , where X  X  R d is the d -dimensional feature space and Y { 1 , 2 ,  X   X   X  ,c } is the set of permissible categories. For large multi-class learning problems, we assume c &gt;&gt; 2 . The goal of multi-class learning is to infer a target function f that minimizes the expected risk of misclassifying any instance drawn from P , i.e., tion.

A standard approach for multi-class learning is to reduce the problem into multiple binary classification tasks. However, when the number of classes c is too large, applying all the binary classi-fiers to predict the class of a test instance is computationally infeasi-ble. To overcome this problem, hierarchical labeled tree algorithms have been developed for large multi-class learning problems. A hi-erarchical labeled tree is a 4-tuple T = &lt; V,E, , &gt; , where V is the set of nodes, E is the set of edges connecting parent n-odes to their corresponding child nodes, = { f v ( x ) | the set of classifiers associated with the internal nodes of T , and leaf nodes. In this paper, we restrict our framework to binary rooted trees only, even though, in principle, it is applicable to rooted trees with a branching factor greater than 2. Our objective in this work is to learn an optimal tree T for large, multi-class learning problems.
This section introduces the Hilbert-Schmidt Independence Crite-rion. Our presentation follows closely the definitions given in [13] [30][3]. Let ( X,Y ) be a pair of random variables drawn from a joint distribution P X;Y . The Hilbert-Schmidt Independence Cri-terion (HSIC)[13] measures the dependence between X and Y by calculating the norm of its cross-covariance operator C xy space. It has been shown that this norm vanishes if and only if X and Y are independent variables. A larger value of HSIC also indicates a stronger dependence between the two variables, with respect to the choice of kernels [30].

Let F be a Reproducing Kernel Hilbert Space (RKHS) of func-tions from X to R with a feature map  X  , such that  X   X  ( x ) , X  ( x k ( x,x  X  ) , where k X : X  X  X  X  R is a positive definite kernel. Likewise, let G be the RKHS on Y with a feature map  X  , such that  X   X  ( y ) , X  ( y  X  )  X  G = k Y ( y,y  X  ) , where k Y : Y  X  Y  X  R responding kernel for the metric space Y . The covariance operator C xy is defined as where  X  x = E [  X  ( x )] ,  X  y = E [  X  ( y )] and  X  denote a tensor product. The Hilbert-Schmidt Independence Criterion is defined as the squared Hilbert-Schmidt norm of the cross-covariance operator C xy , which can be written as follows: Let { ( x 1 ,y 1 ) , ( x 2 ,y 2 ) ,  X   X   X  , ( x n ,y n ) } stances drawn from Pr XY . HSIC can be empirically estimated from the training data as follows [13, 3]: where tr [  X  ] denote the trace of a matrix, H n = I  X  1 n centering matrix, I is the identity matrix, 1 n is a column vector of all ones, ^ K is an n  X  n uncentered kernel matrix for x , and L is an n  X  n kernel matrix for the class labels y . The matrix product H n ^ KH n corresponds to a centered kernel matrix. If K denote the centered kernel matrix, then the computation of HSIC simplifies to ( n  X  1) 2 tr [ KL ] .
Given a rooted binary tree T on the labeled set Y , we can repre-sent it as a tree-structured covariance matrix [3] as follows. D EFINITION 1. (Tree-Structured Covariance Matrix) A matrix C  X  R c c is a covariance matrix associated with the label tree structure T , where C ij is a measure of similarity between the i-th and j-th classes computed based on the path length from the root node to the closest common ancestor between the leaf nodes for i and j in T . The larger the value of C ij , the closer is the relationship between the two classes.
 Figure 1 shows an example of a labeled tree structure for 4 class-es, c 1 ,c 2 ,c 3 , and c 4 . Let a,b,c,d,e,f denote the edge weights between the nodes. Based on the structure, the tree-structured co-variance matrix is a 4  X  4 matrix shown on the right hand side of the diagram. For example, C 1 ; 2 = a since the closest common ancestor between the classes has a path length equals to a from the root node. Similarly, C 3 ; 3 = b + e , which is the path length from the root to the leaf node for c 3 . Note that the edge weight a,b,c,d,e,f determines the similarity between the different cate-gories. For brevity, the edge weights are assumed to be 1 in the examples presented in the next section.
This section presents an overview of the proposed method. We first introduce the global objective function to be optimized by MF-Tree. We then show its connection to the HSIC measure described in the previous section. We also illustrate the additive property of HSIC, which allows us to develop a hierarchical tree-based induc-tion algorithm.
Let X denote an n  X  d centered design matrix, whose rows corre-spond to the data instances and columns correspond to the features. An uncentered design matrix ~ X can always be centered by applying the following operation, H n ~ X , where H n is the centering matrix defined in Section 3.2.

MF-Tree is designed to minimize the following regularized squared error loss function.
 J =  X  X  X  ZW T  X  2 where Z is an n  X  c matrix, which represents the class membership of each data instance, W is a d  X  c matrix, which represents the feature vector of each class, A is a symmetric, positive-definite, c  X  c class covariance matrix, and  X  is the regularization parameter.  X   X   X  F denote the Frobenius norm of a matrix and the superscript T denote the transpose operator. The intuition behind the objective function given in Equation (3) is to factorize the centered design matrix X into a product of latent factors Z and W .
To demonstrate the relationship between the objective function and HSIC, let us take the partial derivative of J given in Equation (3) with respect to W and set it to zero: Replacing this back into Equation (3) yields the following: J = tr Thus, finding a solution for Z that minimizes J is equivalent to finding one that maximizes the following objective function: Let ^ K = XX T be the centered kernel matrix associated with the training instances X . During training, we enforce the following constraint: where Y is an n  X  c matrix of class labels and C is the tree-structured covariance matrix. This can be achieved by setting Z Y and C =
T HEOREM 1. If A be a symmetric, positive-definite matrix, and  X  &gt; 0 , then Z T train Z train +  X  A is also positive-definite.
P ROOF . Given any real vector v , v T v is a positive definite matrix.
 quality constraint in Equation (4) ensures that an appropriate  X  and A can be chosen to generate a viable tree, in which the entries of the matrix C must have a specific structure. Furthermore, let L = YCY T denote the kernel matrix associated with the class labels. The objective function can now be simplified to: which is equivalent to a constrained optimization of HSIC.
In the previous subsection, we have shown that the global objec-tive function for MF-Tree is equivalent to HSIC by setting appro-priate constraints on A and  X  . This enables us to define a kernel matrix for the class labels as L = YCY T , where C is the tree-structured covariance matrix. In this section, we demonstrate the additive property of HSIC, which enables us to decompose the op-timization problem into a hierarchical binary classification task.
First, we show that the tree-structured covariance matrix can be written as a sum of block diagonal matrices. We illustrate this with a simple example. Consider the following covariance matrix for a labeled tree that contains 8 leaf nodes (assuming the edge weights are equal to 1): We can rewrite the matrix as follows: where  X  denote a Kronecker product and
The first term on the right hand side of Equation (7) simply par-titions the first 4 rows and columns of the matrix into 1 cluster and the last 4 rows and columns into a second cluster. The second term on the right hand side partitions the first cluster into 2 smaller sub-clusters, while the third term puts each subcluster as a cluster itself (this is at the leaf node of the tree). In general, we can write the binary tree. We can further simplify the decomposition to the following: C where each diagonal block has the following form and the dimension for each of the submatrices of 0 s and 11 B 2 i ;j is 2 m i  X  2 m i . Furthermore, given a block matrix B have where As each B i;j corresponds to a block-diagonal matrix given by E-quation (9), we can write This decomposition enables us to solve the optimization problem with a hierarchical tree-like decomposition. The outer sum corre-sponds to decomposition at each depth of the tree, while the inner sum corresponds to expanding the internal nodes associated with the particular depth. Instead of learning C directly by maximizing the trace of HSIC (see Equation (5)), we can learn the set of B iteratively, each of which is associated with an internal node in the tree structure.
To construct the tree, we need to solve the objective function given in (5). Based on the discussion in the previous subsection, the objective function can be simplified as follows: This allows us to solve the optimization problem one node at a time. Let D j = Y T j K j;j Y j , which is simply a c  X  c similarity matrix of the classes, computed based on their attributes X and the known ground truth labels associated with the block B 2 i ;j . At each node of the tree construction process, we need to find a matrix B maximizes its alignment with D j , i.e.: For notational convenience, we drop the subscripts i and j in the remainder of the discussion. Note that B is simply a co-association matrix, whose element To create binary partitions, let G be a c  X  2 matrix 1 , where It is easy to show that B = GG T . Thus, This objective function is equivalent to finding a clustering of the classes in a way that maximizes the within-cluster similarity.
Note that the following optimization problem at each node is not feasible unless we enforce a non-negative constraint G [6]. Furthermore, if we relax the condition that elements of G must be either 0 or 1 and replace it with a constraint that G must be an orthogonal matrix we can solve the problem easily by finding the first two eigenvec-tors corresponding to the largest eigenvalues of D [27, 17, 5]. Let GG T be the eigendecomposition of matrix D .

By definition, D = Y T KY . Following the eigendecomposition of D , we have Since G T G = I : Let P = YG , which is an n  X  2 matrix that determines the as-signment of each data instance to the corresponding child nodes. Thus,
During the testing phase, let ^ k be an n -dimensional column vec-tor that contains similarity between the test instance to all the train-ing instances. We compute the 2-dimensional partition vector ^ p  X  2 as follows: After computing ^ p , we assign the test instance to the partition with the larger value, i.e., arg max i ( P T ^ k ) i .
The pseudocode for constructing MF-Tree is summarized in Al-gorithm 1. Our algorithm recursively partitions each node by call-ing the TreeGrowth function. Each invocation of the function returns a root node for a subtree that partitions the classes and their associated training instances into 2 groups. Each node is assumed to have a complex data structure, with the following fields: (1) v.index contains the indices of the training instances assigned to the node, (2) v.class represents the class label if v is a leaf node, (3) v. G is the discriminant function if v is an internal node, and (4) v.left and v.right are pointers to its left and right child.
Here, we assume the number of classes associated with the given node is c . As the tree grows deeper, the number of classes also decreases, which in turn, reduces the number of rows in G . Algorithm 1 MF-Tree Construction Output : Tree Structure, T 1. root = TreeGrowth ( X train ,Y train ) 2. Insert root into T 3. return T function TreeGrowth ( X , Y ) 1. v = createNode () 2. v.index = getIndex( X ) 3. if number of classes in Y &lt; 2 then 4. v.class = unique ( Y ) 5. else 7. v.left = TreeGrowth ( X ( l ) ,Y ( l ) ) 8. v.right = TreeGrowth ( X ( r ) ,Y ( r ) ) 9. end if 10. return v function Partition ( X , Y )
The Partition function decides how to partition each class and their training examples into different branches of the tree. The partitioning is done based on the magnitude of matrix G as de-scribed in the previous section. If a class k is assigned to the left child of a node, all the training instances that belong to class k will also be propagated to the left child. The algorithm continues to ex-pand a node until all the training instances associated with the node belong to the same class.

To determine its time complexity, note that at each node v , we need to compute the kernel matrix K , an operation that requires O ( n 2 d ) time. Computing D from the kernel matrix K and class label Y takes O ( n 2 c + c 2 n ) time while finding its first two eigen-vectors requires, in the worst-case, O ( c 3 ) time. Thus, the compu-tational cost at each node is O ( n 2 c + c 2 n + c 3 ) . The cost reduces significantly as the tree grows deeper since the size of matrices K and D decreases substantially. Once the tree structure is induced from the training set, we can predict the label for any test instance using the method presented in Algorithm 2.
As shown in the previous section, the time complexity for con-structing MF-Tree can be quite expensive especially at the top lev-els of the hierarchy. This is due to the large size of matrices that must be constructed and factorized according to their largest two eigenvalues. However, as we traverse down the tree, the number of classes and training instances to be dealt with decreases consid-erably, which in turn, helps to speed up the computations. Thus, the bottleneck of our computation lies in the first few iterations of the MF-Tree algorithm. In this section, we present an approach to speed up the tree construction process by learning an approxi-mation for the G matrix during the first few iterations. Once the matrices become small enough, we can proceed with applying the original TreeGrowth function shown in Algorithm 1.
 Algorithm 2 Testing with MF-Tree Output : Y test 1. for each x  X  X test do 2. v = T.root 3. while v.class =  X  do 4. ^ k = Similarity ( x ,X train ,v.index ) 5. ^ Y = Subset ( Y train ,v.index ) 6. ^ G = v. G 7. ^ p = ^ G T ^ Y T ^ k 8. v = getChild ( ^ p,v ) 9. end while 9. Insert y = v.class into Y test 10. end for 11. return Y test
Our proposed method attempts to balance the tradeoff between accuracy and efficiency of the labeled tree construction process. While efficiency is a concern at the top levels of the tree, accura-cy is an issue at lower levels of the tree. Even if the class parti-tioning at the top levels is suboptimal, one might still be able to recover good partitions at subsequent iterations when refining the tree. Based on this rationale, we develop the following two-step approach to improve training efficiency. First, we apply a simple method to calculate G at the top levels of the tree. When the depth of the tree exceeds some threshold  X  , we revert back to the original TreeGrowth function. We termed this approach as Approximate MF-Tree , which is summarized in Algorithm 3.

Consider the root node of the labeled tree. Let n be the number of training instances and c be the number of classes. We first sort the classes in decreasing order of their class size and then assign the largest class ( c 1 ) to the left node of the root and the second largest class ( c 2 ) to the right child. The sorting operations requires only O ( c log c ) computations. For the remaining classes, we com-pute their corresponding mean vectors, an operation that requires O ( nd ) time. We then assign each class to the left or right child based on their distance to the mean vectors of the first two largest classes. For example, if the mean vector for c 3 is closer to the mean Otherwise, we assign it to the right child. This process is repeated until depth  X  . This reduces the time complexity of each node from O ( c 3 ) to O ( c log c + nd ) . As will be shown in our experimen-tal results section, substantial improvement in training time can be achieved by setting the depth threshold  X  to be 2 or 3 without losing significant accuracy. In this section, we compared the performance of our proposed MF-Tree algorithm against several state-of-the-art hierarchical learn-ing algorithms 2 , including DAGSVM [28], Discriminative Relaxed Hierarchy Learning (DRHL) [11], confusion matrix based label embedding tree learning approach (CMTL) [1] and Recursive Non-negative Matrix Factorization (RNMF) [22]. In addition to these methods, we have also implemented a recursive version of HSIC clustering by dependence maximization (RDM) algorithm [30]. In this approach, the data instances are partitioned into p groups at each level of the tree until a stopping criterion is met (when no other classes have more than 5 instances at the leaf node). We con-
Ev en though the accuracy of non-hierarchical methods such as one-versus-one was comparable to hierarchical methods, they are extremely slow when the number of classes is too large. Algorithm 3 Approximate MF-Tree Output : Tree structure T 1. Set depth, d = 0 2. root = TreeGrowthApprox ( X train ,Y train ,d, X  ) 3. Insert root into T 4. return T function TreeGrowthApprox ( X , Y , d ,  X  ) 1. v = createNode () 2. v.index = getIndex( X ) 3. if d &lt;  X  and number of classes in Y &lt; 2 5. v.left = TreeGrowthApprox ( X ( l ) ,Y ( l ) ,d + 1 ) 6. v.right = TreeGrowthApprox ( X ( r ) ,Y ( r ) ,d + 1 ) 7. else 8. v = TreeGrowth ( X,Y ) 9. end if 10. return v function Partition2 ( X , Y ) 1. classes = Sort ( Y ) 2. ( X ( l ) ,Y ( l ) ) = { ( x i ,y i ) y i = c 1 (largest class) 3. ( X ( r ) ,Y ( r ) ) = { ( x i ,y i ) y i = c 2 (second largest class) 4. meanvectors = Mean ( X , Y ) 5. for each remaining class c  X  classes do 6. m 1 = distance (meanvectors, c , c 1 ) 7. m 2 = distance (meanvectors, c , c 2 ) 8. if m 1  X  m 2 then 10. else 12. end if 13. end for struct a binary classifier at each internal node of the tree as well as 1-class SVM classifiers at the leaf nodes to determine the class labels. We performed our experiments on two real-world data sets: (1) Caltech-256 3 , which is a benchmark data set for multi-class learning. The data set contains 30,607 images from 256 classes, with at least 80 images in each class, and (2) Wiki , a collection of Wikipedia articles. We generated four variants of the data sets for our experiments and denote them as Wiki 1 , Wiki 2 , Caltech and Caltech 2 , respectively. A summary of the characteristics for each data set is shown in Table 1.
 Caltech 1 and Caltech 2 are subsets of image data from the Caltech-256 collection. Caltech 1 is generated using the same approach as described in [11], in which 80 images are randomly sampled from each class to create the data set. We use a repeat-ed holdout method to create the training and test sets. Specifically, half of the sampled images were reserved for training while the re-maining half for testing. We repeat the sampling process five times to create five versions of the data set. We then applied all the al-gorithms on the data sets and compute their average F1-score. The F1-score is defined as the harmonic mean of the precision (positive predictive value) and recall (sensitivity) for each class. Results are http://authors.library .caltech.edu/7694/ reported based on the average F1-score for all the classes over all five versions of the data sets. In addition, we created a larger data set called Caltech 2 by randomly choosing 40 images from each class for training and use all the remaining images for testing. The sampling process is again repeated five times to generate five ver-sions of Caltech 2 for our experiments. In both Caltech 1 Caltech 2 data sets, we extracted SIFT (scale-invariant feature transform) features to represent the images.

Wiki 1 is a data set generated using the same approach as de-scribed in [22]. Here, we choose articles from the 214 largest cate-gories to obtain a data set that contains 24,378 articles. In addition, we also created a larger sample data set from the Wikipedia dump, which covers the largest 1618 categories with 65,156 articles. Sim-ilar to Caltech 1 , half of the sampled data were used for training and the remaining half for testing. The sampling process is also repeated five times to generate five versions of the data sets.
This section presents the results of our experiments. We use av-erage F1-score as our measure of accuracy for each algorithm. In addition, we compared the testing time for each algorithm along with the size of the induced hierarchy.
We first compare the performance of MF-Tree against the five baseline algorithms. The results shown in Table 2 suggest that the proposed MF-Tree algorithm consistently outperforms other base-line algorithms both in terms of its F1-score and testing efficiency. The size of the hierarchy generated by MF-Tree is also comparable to some of the best algorithms. Even though RNMF generates a shorter tree than MF-Tree, its test time is higher because the leaf n-odes of RNMF contains multiple 1-class SVM classifiers that must be invoked in order to predict the class. In contrast, the leaf n-odes of MF-Tree has a single class label, which allows us to predict the class efficiently. Among all the competing methods, DAGSVM creates the largest tree. This is because it eliminates one class at a time at each level of the hierarchy. So the depth of the tree is equal to the number of classes. In the meantime, DRHL and RDM also create larger trees because they both allow a class to be as-signed to more than one leaf nodes. This affects the test time of the algorithms.
Unlike MF-Tree, which is parameter-free, the performance of several baseline algorithms depends on the values of their parame-ters. For example, the branching factor p in RNMF method deter-mines the structure and depth of the tree. If p = 1 , this produces a decision stump consisting of c 1-class SVM models. This is equiv-alent to implementing a one-versus-all approach. If p = 2 , then it produces a binary tree. Clearly, the choice of p affects the depth of the tree. As depth increases, the test efficiency reduces significantly at the expense of decreasing F1 values. Similarly, tree construction parameters are also needed in other baseline methods. To provide a fair comparison, we vary the parameters for each method based on the suggestion of their original papers. Specifically, for RNMF [22] Caltech 1 F1 T est Time (s) Depth of Tree D AGSVM 0.369  X  0.0019 436.8  X  10.2 256 MF-T ree 0.3816  X  0.03 77.5  X  4.5 10 Caltech 2 F1 T est Time (s) Depth of Tree D AGSVM 0.356  X  0.0017 751.68  X  11.8 256 MF-T ree 0.376  X  0.029 135.6  X  5.6 10 D AGSVM 0.5309  X  0.002 942.9  X  17.6 214 MF-T ree 0.5406  X  0.0216 152.7  X  4.2 9 D AGSVM 0.3106  X  0.0021 1618.6  X  17.6 1618
MF-T ree 0.3318  X  0.016 267.7  X  6.9 11 and R DM [30], we vary their parameters p  X  { 2 , 3 , 4 , 5 for DRHL [11], we set  X   X  { 0 . 5 , 0 . 6 , 0 . 7 , 0 . 8
We plotted their classification performance in Figure 2, where the horizontal axis corresponds to test time while the vertical axis corresponds to average F1-score. Ideally, we seek for a classifier with lowest test time and highest F1-score (i.e., closest to the upper left corner). As can be seen from these plots, MF-Tree is better than the baseline hierarchical learning methods on all four data sets.
In addition to their F1-scores, it is useful to compare the tree gen-erated by the different methods against their ground truth structure. To do this, we need an evaluation measure to compare the similarity between two trees. In this paper, we apply the edit distance mea-sure, which was originally used for string comparison, to compare the ordered labeled trees (see [31] for a review). Ordered labeled trees are trees in which the left-to-right order among siblings is significant. The distance between two trees is computed by consid-ering the optimal mapping between the two trees. Specifically, the distance is given by the minimum cost of elementary operations to convert one tree into the other. An alternative way to map and edit the trees is by using tree alignment [16].

To quantify the difference between two trees based on their pair-wise class ordering, we first construct an adjacency matrix A for a tree structure using the formula: A ij = 1 num A ij is the similarity between classes c i and c j and num hops c ) is the minimum number of hops required to traverse the tree from class c i to c j . To measure the difference between two trees represented by the matrices A and ^ A , we compute its mean squared error ( MSE ), which is given by: T o illustrate how the measure works, consider the hypothetical trees shown in Figure 3. Let A be the adjacency matrix for the actual tree while ^ A result 1 and ^ A result 2 are the corresponding adjacen-cy matrices for two competing trees, result 1 and result 2 . The MSE for result 2 is 0.01383, which is smaller than the MSE for result 1 , which is 0.03125. This is consistent with our observa-tion since result 2 resembles the actual tree except for the mis-placed class c 2 . In contrast, the classes c 2 and c 3 are misplaced in result 1 . For example, A ( c 3 ,c 4 ) = 0 . 5 in the actual tree as the number of hops from category c 3 to c 4 is equal to two. However, ^ A result 1 ( c 3 ,c 4 ) = 0 . 25 for result 1 since class c distance according to the actual tree.

We compared the MSE score of MF-Tree against the confusion matrix based label embedding tree learning (CMTL) method. We choose this baseline method for two reasons. First, the size of the tree is comparable to MF-Tree. Second, similar to MF-Tree, this approach produces a tree in which each class can only reside in a single leaf node of the tree (whereas other methods allow a class to be assigned to multiple leaf nodes). Trees that restrict each class to a single leaf node are more interpretable as they may be used to define a concept hierarchy for the application domain. The experi-mental results shown in Figure 4 suggest that MF-Tree produces a tree structure that is closer to the ground truth compared to CMTL. Note that Caltech 1 and Caltech 2 use the same set of classes for training to construct the taxonomy, so their trees are identical. We report their results together as Caltech in Figure 4. Figure 5 shows a subset of the tree structure generated by MF-Tree on the Caltech 1 data set. Observe that the tree was able to capture the relationships among many of the classes quite well. For example, some of the classes of animals (dog, horse, chimp, bear) was assigned to the same branch. Though there were exceptions and misclassifications (e.g., bats and crabs) but the related classes are often quite close together at the lower levels of the tree.
Despite its lower test time compared to other approaches, train-ing an MF-Tree is still expensive when the number of classes is large. To overcome this problem, we propose an approximate MF-Tree algorithm, which uses an inexpensive partitioning method to assign classes to their child nodes at the top levels of the hierar-chy. The algorithm has a parameter  X  that determines the maximum depth at which to apply the inexpensive partitioning method. Figur e 4: MSE comparison of the tree produced by MF-Tree against the one produced by CMTL
To justify the effectiveness of the approximate MF-Tree approach, we perform an experiment on the Wiki 2 data set. We choose this data set because it has the largest number of classes. For this exper-iment, we vary  X  from 3 to 6 and compare the average F1-score as well as training time of the algorithms. The results are shown in Ta-ble 3. Recall that the average F1-score for MF-Tree on the Wiki data set is 0.3318. When  X  = 3 , the F1-score reduces slightly to 0.3289. However, there was a 22.6% training time improvement. As  X  increases, the F1-score gradually decreases while its training time continues to improve. Note that the size of the tree generated by the original MF-Tree algorithm has a depth equals to 11. When  X  = 6 , the improvement in training time appears to taper off while its F1-score decreases to 0.2868. This clearly shows a trade-off be-tween accuracy and training time efficiency. In practice, we could set the  X  threshold based on the specific needs of the problem. If our priority is lower training speed, we could set a higher  X  value.  X  can also be set based on the computational resources available. If there is limited memory,  X  can be set to a threshold in such a way that the resulting data matrices can fit into the memory available.
In this paper, we proposed a novel hierarchical method for large-scale multi-class learning called MF-Tree. Our proposed algorithm is driven by a global objective function. We demonstrate the equiv-alence between the global objective function and the HSIC metric and show it has an additive property that can be exploited to design a hierarchical tree learning algorithm. Experimental results com-paring the method against five baseline methods demonstrated both the effectiveness and efficiency of our method. [1] S. Bengio, J. Weston, and D. Grangier. Label embedding [2] A. Beygelzimer, J. Langford, and P. Ravikumar. Multiclass [3] M. Blaschko and A. Gretton. Learning taxonomies by [4] M. Blaschko, W. Zaremba, and A. Gretton. Taxonomic [5] D. Boley. Principal direction divisive partitioning. Data [6] D. Chen and R. J. Plemmons. Nonnegativity constraints in [7] P. M. Comar, L. Liu, A. Nucci, S. Saha, and P.-N. Tan. [8] P. M. Comar, L. Liu, S. Saha, A. Nucci, and P.-N. Tan. [9] P. M. Comar, L. Liu, S. Saha, P.-N. Tan, and A. Nucci. [10] J. Deng, S. Satheesh, A. C. Berg, and F. F. F. Li. Fast and [11] T. Gao and D. Koller. Discriminative learning of relaxed [12] R. Ghani. Using error-correcting codes for efficient text [13] A. Gretton, O. Bousquet, A. Smola, and B. Sch X lkopf. [14] J. M. Hailpern, R. Vernica, M. Bullock, U. Chatow, J. Fan, [15] T. Hastie and R. Tibshirani. Classification by pairwise [16] T. Jiang, L. Wang, and K. Zhang. Alignment of trees -an [17] W. Jiang and F.-l. Chung. A trace ratio maximization [18] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, [19] G. Koutrika, L. Liu, and S. Simske. Generating reading [20] D. Kuang and H. Park. Fast rank-2 nonnegative matrix [21] J. Langford and A. Beygelzimer. Sensitive error correcting [22] L. Liu, P. M. Comar, S. Saha, P.-N. Tan, and A. Nucci. [23] L. Liu, G. Koutrika, and S. Wu. Learningassistant: A novel [24] L. Liu, J. Liu, and S. Wu. Image discovery and insertion for [25] L. Liu, S. Saha, R. Torres, J. Xu, P.-N. Tan, A. Nucci, and [26] L. Liu and P.-N. Tan. A framework for co-classification of [27] T. Ngo, M. Bellalij, and Y. Saad. The trace ratio optimization [28] J. C. Platt, N. Cristianini, and J. Shawe-taylor. Large margin [29] R. Ryan and K. Aldebaro. In defense of one-vs-all [30] L. Song, A. J. Smola, A. Gretton, and K. M. Borgwardt. A [31] E. Tanaka and K. Tanaka. The tree-to-tree editing problem.
