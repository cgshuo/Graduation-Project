 Chinese Word segmentation is a necessary step in Chinese-English statistical machine translation (SMT) because Chinese sentences do not delimit words by spaces. The key characteristic of a Chi-nese word segmenter is the segmentation specifi-cation 1 . As depicted in Figure 1(a), the dominant practice of SMT uses the same word segmentation for both word alignment and translation rule induc-tion. For brevity, we will refer to the word seg-mentation of the bilingual corpus as word segmen-tation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), be-cause it determines the basic tokens of translation rules 2 , which also determines how the translation rules would be matched by the source sentences. 
It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for align-ment is not necessarily better for translation. 
To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for align-ment, viz. character alignment ; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to conventional word alignment for translation rule induction. In the experiment, our approach consistently outper-formed two baselines with three different word segmenters: fully word-based system (using word for both alignment and translation) and fully char-acter-based system, in terms of alignment quality and translation performance. 
The remainder of this paper is structured as fol-lows: Section 2 analyzes the influences of WSA and WSR on SMT respectively; Section 3 discusses how to convert character alignment to word align-ment; Section 4 presents experimental results, fol-lowed by conclusions and future work in section 5. We propose a solution to tackle the suboptimal problem: using Chinese character for alignment while using Chinese word for translation. Character alignment differs from conventional word align-ment in the basic tokens of the Chinese side of the training corpus 3 . Table 1 compares the token dis-tributions of character-based corpus ( CCorpus ) and word-based corpus ( WCorpus ). We see that the WCorpus has a longer-tailed distribution than the CCorpus . More than 70% of the unique tokens ap-pear less than 5 times in WCorpus . However, over half of the tokens appear more than or equal to 5 times in the CCorpus . This indicates that modeling word alignment could suffer more from data sparsity than modeling character alignment. 
Table 2 shows the numbers of the unique tokens (# UT ) and unique bilingual token pairs (# UTP ) of the two corpora. Consider two extensively features, fertility and translation features, which are exten-sively used by many state-of-the-art word aligners. The number of parameters w.r.t. fertility features grows linearly with # UT while the number of pa-rameters w.r.t. translation features grows linearly with # UTP . We compare # UT and # UTP of both corpora in Table 2. As can be seen, CCorpus has less UT and UTP than WCorpus , i.e. character alignment model has a compact parameterization than word alignment model, where the compactness of parameterization is shown very important in sta-tistical modeling (Collins, 1999). 
Another advantage of character alignment is the reduction in alignment errors caused by word seg-mentation errors. For example,  X   X  X  X  (Cheney) X  and  X   X  (will) X  are wrongly merged into one word  X  X  X  X  by the word segmenter, and  X  X  X  X  wrongly aligns to a comma in English sentence in the word alignment; However, both  X  and  X  align to  X  Cheney  X  correctly in the character alignment. However, this kind of errors cannot be fixed by methods which learn new words by packing already segmented words, such as word packing (Ma et al., 2007) and Pseudo-word (Duan et al., 2010). 
As character could preserve more meanings than word in Chinese, it seems that a character can be wrongly aligned to many English words by the aligner. However, we found this can be avoided to a great extent by the basic features (co-occurrence and distortion) used by many alignment models. For example, we observed that the four characters of the non-compositional word  X   X  X  X  X  X  X  (Arafat) X  align to Arafat correctly, although these characters pre-serve different meanings from that of Arafat . This can be attributed to the frequent co-occurrence (192 times) of these characters and Arafat in CCorpus . Moreover,  X  usually means France in Chinese, thus it may co-occur very often with France in CCorpus . If both France and Arafat appear in the English sentence,  X  may wrongly align to France . However, if  X  aligns to Arafat ,  X  will probably align to Arafat , because aligning  X  to Arafat could result in a lower distortion cost than aligning it to France . 
Different from alignment, translation is a pattern matching procedure (Lopez, 2008). WSR deter-mines how the translation rules would be matched by the source sentences. For example, if we use translation rules with character as WSR to translate name entities such as the non-compositional word  X  X  X  X  X  X  , i.e. translating literally, we may get a wrong translation. That X  X  because the linguistic knowledge that the four characters convey a spe-cific meaning different from the characters has been lost, which cannot always be totally recovered even by using phrase in phrase-based SMT systems (see Chang et al. (2008) for detail). Duan et al. (2010) and Paul et al., (2010) further pointed out that coarser-grained segmentation of the source sen-tence do help capture more contexts in translation. Therefore, rather than using character, using coarser-grained, at least as coarser as the conven-tional word, as WSR is quite necessary. In order to use word as WSR, we employ the same method as Elming and Habash (2007) 4 to convert the character alignment ( CA ) to its word-based version ( CA X  ) for translation rule induction. The conversion is very intuitive: for every Eng-lish-Chinese word pair  X  X , X  X  in the sentence pair, we align  X  to  X  as a link in CA X  , if and only if there is at least one Chinese character of  X  aligns to  X  in CA . 
Given two different segmentations A and B of the same sentence, it is easy to prove that if every word in A is finer-grained than the word of B at the cor-responding position, the conversion is unambiguity (we omit the proof due to space limitation). As character is a finer-grained than its original word, character alignment can always be converted to alignment based on any word segmentation. Therefore, our approach can be naturally scaled to syntax-based system by converting character alignment to word alignment where the word seg-mentation is consistent with the parsers. 
We compare CA with the conventional word alignment ( WA ) as follows: We hand-align some sentence pairs as the evaluation set based on char-acters ( ESChar ), and converted it to the evaluation set based on word ( ESWord ) using the above con-version method. It is worth noting that comparing CA and WA by evaluating CA on ESChar and evaluating WA on ESWord is meaningless, because the basic tokens in CA and WA are different. However, based on the conversion method, com-paring CA with WA can be accomplished by evalu-ating both CA X  and WA on ESWord . FBIS corpus (LDC2003E14) (210K sentence pairs) was used for small-scale task. A large bilingual corpus of our lab (1.9M sentence pairs) was used for large-scale task. The NIST X 06 and NIST X 08 test sets were used as the development set and test set re-spectively. The Chinese portions of all these data were preprocessed by character segmenter (CHAR), ICTCLAS word segmenter 5 (ICT) and Stanford word segmenters with CTB and PKU specifica-tions 6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as ESChar , which is converted to three ESWord s based on three segmentations re-spectively. These ESWord s were appended to training corpus with the corresponding word seg-mentation for evaluation purpose. 
Both character and word alignment were per-formed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional align-ments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). We first evaluate the alignment quality. The method discussed in section 3 was used to compare char-acter and word alignment. As can be seen from Table 3, the systems using character as WSA out-performed the ones using word as WSA in both small-scale (row 3-5) and la rge-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the vocabulary size and trans-lation performance without explicitly distinguish-ing WSA and WSR. We then evaluated the translation performance. The baselines are fully word-based MT systems ( WordSys ), i.e. using word as both WSA and WSR, and fully character-based systems ( CharSys ). Table 
Table 4 Translation evaluation of WordSys and pro-4 compares WordSys to our proposed system. Sig-nificant testing was carried out using bootstrap re-sampling method proposed by Koehn (2004) with a 95% confidence level. We see that our pro-posed systems outperformed WordSys in all seg-mentation specifications settings. Table 5 lists the results of CharSys in small-scale task. In this setting, we gradually set the phrase length and the distortion limits of the phrase-based decoder (context size) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as WSR for fair comparison with WordSys as suggested by Duan et al. (2010). Comparing Table 4 and 5, we see that all CharSys underperformed WordSys . This observation is consistent with Chang et al. (2008) which claimed that using characters, even with large phrase length (up to 13 in our experiment) cannot always capture everything a Chinese word segmenter can do, and using word for translation is quite necessary. We also see that CharSys under-performed our proposed systems, that X  X  because the harm of using character as WSR outweighed the benefit of using character as WSA, which indicated that word segmentation better for alignment is not necessarily better for translation, and vice versa. 
We finally compared our approaches to Ma et al. (2007) and Ma and Way (2009), which proposed  X  X acked word (PW) X  and  X  X ilingual motivated word (BS) X  respectively. Both methods iteratively learn word segmentation and alignment alterna-tively, with the former starting from word-based corpus and the latter starting from characters-based corpus. Therefore, PW can be experimented on all segmentations. Table 6 lists their results in small-scale task, we see that both PW and BS underper-formed our approach. This may be attributed to the low recall of the learned BS or PW in their ap-proaches. BS underperformed both two baselines, one reason is that Ma and Way (2009) also em-ployed word lattice decoding techniques (Dyer et al., 2008) to tackle the low recall of BS, which was removed from our experiments for fair comparison. Interestingly, we found that using character as WSA and BS as WSR (Char+BS), a moderate gain (+0.43 point) was achieved compared with fully BS-based system; and using character as WSA and PW as WSR (Char+PW), significant gains were achieved compared with fully PW-based system, the result of CTB segmentation in this setting even outperformed our proposed approach (+0.42 point). This observation indicated that in our framework, better combinations of WSA and WSR can be found to achieve better translation performance. We proposed a SMT framework that uses character for alignment and word for translation, which im-proved both alignment quality and translation per-formance. We believe that in this framework, using other finer-grained segmentation, with fewer am-biguities than character, would better parameterize the alignment models, while using other coars-er-grained segmentation as WSR can help capture more linguistic knowledge than word to get better translation. We also believe that our approach, if integrated with combination techniques (Dyer et al., 2008; Xi et al., 2011), can yield better results. We thank ACL reviewers. This work is supported by the National Natural Science Foundation of China (No. 61003112), the National Fundamental Research Program of China (2010CB327903). 
