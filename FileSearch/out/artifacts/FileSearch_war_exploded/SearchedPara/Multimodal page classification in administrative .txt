 ORIGINAL PAPER Mar X al Rusi X ol  X  Volkmar Frinken  X  Dimosthenis Karatzas  X  Andrew D. Bagdanov  X  Josep Llad X s Abstract In this paper, we present a page classification application in a banking workflow. The proposed architec-ture represents administrative document images by merg-ing visual and textual descriptions. The visual description is based on a hierarchical representation of the pixel inten-sity distribution. The textual description uses latent semantic analysis to represent document content as a mixture of top-ics. Several off-the-shelf classifiers and different strategies for combining visual and textual cues have been evaluated. Afinalstepusesan n -grammodelofthepagestreamallowing a finer-grained classification of pages. The proposed method has been tested in a real large-scale environment and we report results on a dataset of 70,000 pages.
 Keywords Digital mail room  X  Multimodal page classifi-cation  X  Visual and textual document description 1 Introduction Big corporations and public institutions such as banks, insur-ance companies, city halls, or national health services along with their citizens and clients create massive amounts of documents X  X axes, letters, forms, invoices, etc. X  X hat more oftenthannothavetobedealtwithinaclosetoreal-timeman-ner. These are vital communications with clients, providers, and other stakeholders that flow into, through, and out of the organization. Processing paper-based correspondence is a labor-intensive task. Letters are opened, read, sorted, routed, and delivered. Depending on the contents, the contained doc-uments are then forwarded to the appropriate recipient for the required action. The needs of the market have been the lead-ing force behind a huge amount of research and development across the document life cycle from digitization to image analysis and from indexing and classification to knowledge management,re-purposing,androuting.Thecollectiveappli-cation of the above processes for the management of docu-ment flows at large scales is known as the digital mail room.
Document Analysis research provides solutions fora automating the screening process and determining the doc-ument type (whether invoice, contract, letter, etc.), and for extracting the relevant information from each document with minimal human intervention. This information is stored in appropriate databases for future querying and feeding out-bound communications.

In this paper, we present a page classification application tested in a banking workflow. When asking for a mortgage or a line of credit, the bank asks clients to deliver paperwork in bulk in order to study the viability of the financial transaction. Such paperwork contains tax forms, invoices, contracts, etc. Before analyzing the risk of this operation, the bank digitizes all this material and categorizes each page for forwarding to a specific analyst. This paper presents our proposed page classification system aimed at mitigating the load of manual effort devoted to page categorization. 1.1 Related work Document image classification is a mature research topic and many different approaches have been proposed in the litera-ture. The interested reader is referred to the survey papers on document image representation, retrieval, and classification from Doermann [ 12 ] and Chen and Blostein [ 8 ]. Besides the supervised machine learning techniques used for clas-sifying the incoming images ( k -NN, decision trees, SVM, neural networks, etc.), the different methods can be catego-rized according to the used document representation.
First there are methods that define document classes in terms of visual similarity. The proposed descriptors normally use statistics computed over low-level features in order to encode how documents  X  X ook. X  For instance, H X roux et al. proposed in [ 24 ] a document descriptor that encodes in a hierarchical fashion pixel densities within a grid partition. Sidiropoulos et al. [ 38 ] proposed a similar descriptor that encodes the average of gray intensity over an adaptive grid. In [ 19 ], Gordo et al. proposed a document description based on multiscale run-length histograms. Such simple descrip-tors are helpful when dealing with problems where docu-ments from the same class are visually similar although their contents might change (e.g., forms).

More elaborate methods encode document similarity in terms of their structure. Structural features are obtained from eitheralogicalorphysicallayoutanalysisofdocumentpages. Physical layout analysis decomposes document images into blocks and document similarity can be expressed in terms of the spatial relationships among these blocks. For exam-ple, Bagdanov and Worring construct an attributed relational graph [ 3 ] in order to model the layout structure within a doc-ument genre. Cesarini et al. use X  X  Y trees in [ 6 ] to both physically segment and describe the document types. In [ 16 ], Gaceb et al. use a hierarchical graph coloring strategy to simultaneously perform segmentation and physical descrip-tion. As an example of the family of methods that describe documents in terms of the structure of logical elements, we cite the work by Dengel and Dubiel presented in [ 10 ]. In this case, the document description encodes how logical ele-ments are located and which are the spatial relationships among them. Structural descriptors are much more robust for assessing visual similarity among document classes than image-based methods. However, they have the drawback that computing a mapping between two layout structures is com-putationally expensive.

Finally, document classes can be defined in terms of content similarity [ 1 ]. After a complete transcription of the documents, document similarity can be expressed by means of textual content. In general, text documents are represented as a set of words together with their associ-ated frequencies in each document, which is known as the bag-of-words model. Because of its simplicity for classi-fication purposes, most text classification methods use the bag-of-words representation, combined with a wide variety of different classifiers [ 36 , 39 ]. Refinements of the bag-of-words model for both feature selection (e.g., [ 40 ]) and fea-ture transformation (e.g., latent semantic analysis [ 9 ], prob-abilistic latent semantic analysis [ 25 ], and latent Dirich-let allocation [ 5 ]) have been proposed in order to model document contents in terms of a mixture of topics. Tex-tual content descriptors are of course suitable when deal-ing with document classes that  X  X alk X  about the same topic although the visual appearance of documents within a class may differ. In the specific case of administra-tive documents such as invoices, some ad hoc document representations in terms of discriminative keywords have been proposed in the literature [ 22 , 27 ]. Such domain-specific representations offer high classification perfor-mancesbutareusuallynotgeneralizabletobroaderdocument collections.

In our application, we deal with document classes where visual similarity is strong evidence (such as forms and invoices from the same provider) and classes that exhibit strong textual content similarity and no predefined standard look-and-feel (such as audit reports or contracts). This fact motivates the use of an architecture that combines multi-ple modalities. However, very few attempts on fusing differ-ent information modalities for document image classifica-tion can be found in the literature. For instance, the work by Erol and Hull presented in [ 14 ] achieved a semantic classification of administrative documents by merging fea-tures from different domains, namely textual, color, hand-writing, or layout features. More recently, the paper pre-sented by Augereau et al. in [ 2 ], inspired byu our pre-vious work [ 32 ], proved the success of the combination of visual and textual features for administrative document classification.

On the other hand, there are very few works in the liter-ature dealing with multipage documents [ 18 , 32 ] or with the treatment of image streams [ 21 ]. We strongly believe that in digital mailroom scenarios in which documents are often digitized in bulk, the use of the context of which pages come before or after the others is strong evidence to exploit when categorizing individual pages. 1.2 Contributions In this paper, we present a page classification application in a banking workflow. The proposed strategy represents admin-istrative document images by merging visual and textual descriptions. Several off-the-shelf classifiers and combina-tion strategies have been tested.
The main contributions of the paper are twofold. First, we describe a multimodal representation of administrative docu-ment images. Very few attempts at combining different views of document images have been studied in the literature. We show how the combination of both modalities clearly outper-formstheexclusiveuseofeithervisualortextualinformation. In addition, we present an exhaustive analysis of the perfor-mance of different state-of-the-art classifiers and combina-tion strategies. The second contribution is an n -gram model of the sequential distribution of different pages as they appear in the processed stream, yielding an extra improvement on the final page classification. The proposed method has been tested in a real large-scale environment and we report results of experiments on 70,000 pages.
 The remainder of this paper is organized as follows. In Sect. 2 , we present an overview of the proposed architec-ture. Section 3 is devoted to the multimodal document image description strategy. In Sect. 4 , we briefly describe the clas-sifiers and the combination schemes we used. The addition of sequential information by means of an n -gram model is detailed in Sect. 5 . Experimental results are presented in Sect. 6 . Finally, conclusions and further research lines are discussed in Sect. 7 . 2 System overview Our proposed system architecture is illustrated in Fig. 1 . Given a flow of incoming documents, both visual and textual descriptors and specific classifiers are computed in parallel. The visual modality encodes the appearance of the document image in terms of pixel densities and the classifier outputs the probabilities of belonging to each document class. Regarding the textual modality, a commercial OCR engine transcribes document images. After some preprocessing steps, a bag-of-words representation of the document is projected onto a topic space by means of latent semantic analysis (LSA). Finally, the textual classifier outputs class probabilities as well.Later,acombinationstepweightstheinfluenceofvisual and textual cues in order to output the individual page classi-fication. An n -gram model of the page stream is finally used. A rejection threshold is set on the confidence value of the classification in order to refuse categorizations with weak evidence. 3 Multimodal description of document images Our proposed description method combines textual informa-tion with a global visual document image description. We first detail the visual description of document images and 3.1 Visual description Within the document analysis and retrieval literature, many descriptors encoding the visual appearance of document images have been proposed. In this paper, we use a sim-ple description of documents that encodes pixel densities at different scales. In order to remove small details and noise from the incoming images, a Gaussian smoothing opera-tor is used to blur the images before computing the visual descriptor.

WeusethemultiscaledescriptorpresentedbyH X rouxetal. in [ 24 ] which encodes pixel densities at different locations and at different scales. This descriptor, although extremely simple and efficient to compute, has proven to yield very competitive results [ 17 , 24 ] when compared to more elab-orate structural descriptors. Besides being discriminative, it also tolerates slight skew deformations. Each document image is recursively split into rectangular regions to form a pyramid. In each region, the pixel density is computed and stored in the corresponding position of the feature vec-tor. We can see an example of the first levels of the pyra-midinFig. 2 . In our experimental setup, we use four scale levels, yielding an 85-dimensional visual descriptor f v .The visual feature vectors are finally normalized by their L 2 norm. 3.2 Textual description In order to use textual information as another source to per-form classification, we have used latent semantic analysis. The document images are OCRed with the commercial OCR fromABBYY. 1 GiventheASCIIrepresentationsofeachdoc-ument image, a text preprocessing step is applied. Then, the use of LSA overcomes some of the problems of directly using a bag-of-words model since it adds semantic coherence to the obtained results. We begin by detailing the preprocessing steps. 3.2.1 Text preprocessing Before extracting the textual descriptor for each document image, we apply several off-the-shelf preprocessing meth-ods that help increase the robustness of the obtained textual description.

The first preprocessing step is to reduce inflected and derived words to their root in order to treat them equally. This process is known as stemming. We have used the Span-ish version of the Porter stemming algorithm implemented in the Snowball [ 29 ] system. Then, stopword filtering is applied to eliminate very common words that do not convey any semantic information. In our experimental setup, we end with a dictionary containing nearly 600,000 terms.

Finally, we represent each document image by its bag-of-words vector f t . Each f t is then weighted by applying the tf-idf model [ 33 ]. This normalization emphasizes the terms that are frequent in a particular document and infrequent in the complete document corpus. The tf-idf weighting scheme assigns to each term t a weight in the document d given by tf  X  idf where tf t , d is the term frequency, i.e., the number of occur-rences of term t in document d , and idf t is the inverse docu-ment frequency computed as idf where N is the total number of documents and the document frequency df t corresponds to the number of documents in the collection that contain the term t . 3.2.2 Latent semantic analysis The classic bag-of-words model has some shortcomings. Often, the words appearing in the document to be classified are not the same as those from the documents in the cor-pus. Users in different contexts often use different words to describe the same information. This phenomenon is known as synonymy . The problem of synonymy often hinders the accuracyof classifiers. Inaddition, thedimensionalityof bag-of-words representations might explode when dealing with large datasets and it is often advised to somehow reduce the size of the representation. In order to overcome this problem, Deerwester et al. introduced in [ 9 ] the latent semantic analy-sis technique. The motivation of using LSA is that, given a text classification framework, it is able to classify documents that are conceptually similar in meaning in a given class, even if they do not share a significant set of words among them, while also drastically reducing the dimensionality of the fea-ture vector.

The LSA model assumes that there exists some underly-ing semantic structure in the descriptor space. This seman-tic structure is defined by assigning to each document a set of topics, which can be estimated in an unsupervised way using standard statistical techniques. The goal is to obtain a transformed space where documents having similar topics but with different terms will lie close.

From our corpus, we select a subset of documents that will be used as the training set to build the LSA space. We represent the training set with a term-by-document matrix A  X  R M  X  Q , where M is the number of different terms and Q is the number of documents in the training set. The trans-formed space is obtained by decomposing the training term-by-document matrix into three matrices by a truncated sin-gular value decomposition (SVD). In order to compute the truncated SVD aiming to reduce the descriptor space to T topics, we proceed as follows: A  X  A = U T S T ( V T ) , (3) where U T  X  R M  X  T , S T  X  R T  X  T and V T  X  R Q  X  T . In our experimental setup, we use a value of T = 300 topics, which offers a good trade-off between the descriptor X  X  dimension-ality and the achieved discriminative power.

When encoding the whole corpus, each feature vector f t is projected to the topic space in order to obtain the topic descriptor  X  f t by  X  f = f
Finally, each topic descriptor  X  f t from the corpus is nor-malized using the L 2 -norm. In this work, we used the LSA implementation technique proposed by [ 31 ]. This technique introduces a streamed distributed algorithm for incremental SVD updates which has the advantage that it does not need a single-pass matrix decomposition algorithm that operates in constant memory with regard to the collection size. It presents and important advantage when dealing with large data collections. 4 Multimodal classification For the sake of completeness, in our experiments we have tested a number of different off-the-shelf classifiers and dif-ferent strategies for combining visual and textual cues. We first briefly enumerate the classifiers used in our study and then the combination approaches. 4.1 Supervised classifiers In order to provide a thorough analysis of classifier perfor-mance, we have chosen to test classifiers from three different families. We evaluate a distance-based classifier, a number of Bayesian classifiers, and finally some kernel classifiers.
To represent distance-based classifiers, we used a simple k -nearest neighbor ( k -NN) classifier. The cosine distance is used between the input and the training samples.

The second family of tested classifiers are probabilis-tic ones [ 13 ] based on Bayes X  theorem. Within this family, we have used the na X ve Bayes classifier (NB) and both lin-ear and quadratic Bayes normal classifiers (LDC and QDC, respectively).

Finally, we have tested kernel classifiers. We have chosen the support vector machines classifier with three different kernels [ 28 ]: the radial-basis function kernel, the  X  2 and the histogram intersection kernel (SVM-RBF, SVM- X  2 and SVM-HI, respectively).

For the k -NN and the Bayes classifiers, we have used the implementations in the PRTools [ 23 ] package. For the SVM classifiers, we have used the LibSVM [ 7 ] library. 4.2 Combination of visual and textual classifications For each visual and textual page description, we obtain fea-ture vectors f v and  X  f t describing the pages. Separate visual and textual classifiers are trained to test which is the most pertinent information. In addition, in order to combine both information cues, in our experiments, we have tested an early fusion and four different late fusion strategies:  X  Early fusion encodes the documents in a single his- X  Late fusion strategies perform individual page classifica- X  X UM combines the probability vectors by adding them with a previous power normalization step.
  X  X ROD multiplies the probability vectors after also apply-ing a power normalization factor to each of them.  X  X AX computes the fusion by taking the maximum of the probability vectors after a power normalization step.  X  X OG useslogisticregressiontoachieveasinglecombined probability vector.
 5 Modeling the page stream with n -grams In a document stream, individual documents are not indepen-dent of each other and contextual information can be used to increase classification accuracy. We have used an n -gram model similar to the ones proposed in [ 26 , 35 ]. In order to include contextual information, the conditional occurrence probabilities are estimated on the training set. Assume we have a document stream  X  d = d 1 ,..., d D , where each docu-ment of a certain type c ( d i )  X  X  1 ,..., C } is to be classified. Then, the n -gram probability of a document is defined as the probability of the document d i being of a certain type a conditioned on the types of the n  X  1 previous documents p ( c ( d
With this is mind, the goal is to find the sequence of docu-ment classes  X  c = c 1 ,..., c D that maximizes at the same time the class probabilities according to the individual classifiers as well as the probability according to the n -grams
This can be solved with a token passing algorithm, similar to those used for speech or handwriting recognition with hid-den Markov models [ 41 ]. In such a token passing algorithm, each token represents a certain classification hypothesis of the documents from the beginning up to a certain point. A token  X  is defined by three values:  X . p is the classification probability of the followed hypothesis,  X . h is the history, which is a link to a token at a previous time step, and  X . the class of the current document.

The algorithm is initialized by different classification hypotheses for the first document which are stored in sep-arate tokens in a list L 1 . Then, the algorithm iterates over all documents in the sequence. For a time step t ,atoken  X  in list L t  X  1 is used to generate C new tokens in list L t anewtoken  X  stands for the hypothesis that the documents d ... d document d t is classified as c ( d t ) = j . The values of the new token are therefore  X  . c = j  X  . p =  X . p  X  p  X  . h =  X  To keep the runtime from becoming exponential, only the best N tokens are kept in each list before the next list is created.Thefinalclasssequencecanberetrievedinbackward order, starting from the token with the highest probability in the last list L D and following the links back to the beginning. An illustration of the algorithm is given in Fig. 3 . 6 Experimental results We first validate the visual and textual descriptors using pub-lic document image datasets, then we introduce our in-house dataset, the evaluation measures and the experimental frame-work used to assess the performance of the proposed system and then analyze the obtained results. 6.1 Descriptor evaluation using public datasets In order to validate the proposed visual and textual fea-tures, we have run a simple classification experiment over the NIST Tax Forms Dataset (SPDB2) [ 11 ] and the MARG [ 15 ] medical papers dataset. The NIST dataset contains 5590 tax form images spread over 20 different categories, whereas the MARG dataset consists of 1,553 first pages of medical papers categorized into 9 different layout classes. Here, visual and textual features were used alone and we used a 1-NN classi-fier in a one-versus-all setup. The obtained results are given in Table 1 .

We observe that the proposed features are really perfor-mant on the NIST dataset; both visual and textual represen-tations perfectly classified the documents in a one-versus-all setup. On the MARG dataset, which is groundtruthed at the layout level (i.e., two documents are considered from the same class if they share the same layout structure), obvi-ously a textual content representation fails to achieve good classification accuracies whereas the visual representation is more suitable in those scenarios (although it does not reach the classification accuracies of pure layout descriptors). 6.2 In-house dataset Our dataset consists of nearly 70,000 real document images sampled from a banking workflow. This corresponds to a portion of certain types of documents received during two months. This dataset contains 13 different document classes which have been manually labeled.

Note that the class labels are defined in terms of doc-uments . Since the incoming documents are multipage, this requires that different pages from the same document share the same label although they might look different and con-tain different textual content. Examples of such documents are given in Fig. 4 .

In order to train the classifiers and validate the different parametersinvolvedinourarchitecture,wehavesplitthiscol-lection into two different sets, each corresponding to a month of documents. The training set consists of 38,313 images and the test set has 31,424 images.

Over the training set, we have used a tenfold cross-validation strategy to train the classifiers and validate the different parameter values such as the k in the k -NN classi-fier, c and  X  for the SVMs, the power normalization factors i and j for the late fusion strategies, and so on.
In order to measure the performance of the system, we report the classification accuracy rates for all the classifiers, information cues, and combination strategies. In addition, in order to evaluate the rejection ability of the system, we report the accuracy-coverage plot which is an indicator of the accuracy evolution as we keep on rejecting classifications. 6.3 Results In Table 2 , we present the classification accuracies obtained by the different page representations, the different classifiers, and combination strategies. If we first look at the independent visual and textual classification, we see that in our use case the textual features clearly outperform the visual representa-tion no matter which classifier we use. The best results are obtained with the SVM classifier using a radial-basis kernel for both the visual and textual representations. These results indicatethat,inourscenario,textualcontentismorediscrimi-native than visual page appearance. Regarding the classifiers, the Bayesian ones perform much worse than the kernel-based ones, although the k -NN classifier performs respectably.
Concerning the early fusion combination, we observe that the best classifier is the SVM using the histogram intersec-tion kernel. Again, Bayesian classifiers do not compare to the performance achieved with the kernel-based ones. How-ever, it is worth noting that the early fusion strategy is not a good option in our scenario. No matter the classifier, better performance is obtained when using the textual description alone than when trying to combine the visual and textual page descriptors with early fusion.

Late fusion experiments for the k -NN classifier were not carried out since it does not output an a posteriori proba-bility vector. For the rest of the classifiers, we see that no matter which late fusion strategy we use, we obtain bet-ter performance than the use of the single modalities alone. This means that, even if the textual representation outper-formed the visual one when used alone, the addition of visual information helps to disambiguate some cases where the textual information is not pertinent. We have conducted a paired-sample t -test at the 0.01 significance level and the gain in performance when combining modalities against the performance of the textual description alone was statisti-cally significant for all the classifiers. Except for the QDC classifier, the best performance is obtained when using the PROD late fusion combination strategy. Here again the best classifier is the SVM with the radial-basis function kernel, althoughthedifferencewiththeother kernel-basedclassifiers and fusion strategies is not significant. There is however an important improvement when compared with the Bayesian classifiers.

In Fig. 5 , we report the confusion matrices for the 13 classes when using the visual, the textual modalities alone and the late fusion PROD combination with the SVM-RBF classifier. Observe how the visual classifier misclas-sifies many more elements than the textual or the combined approaches. Wealsoseethat themainconfusions arebetween just a few document classes (e.g., classes 7, 8, 11, 12 or 13), and that the use of textual information attenuates this. Such failure cases correspond to classes that are visually heteroge-neous, such as invoices, receipts, or audit reports. The slight improvement with respect to the textual representation when combining textual and visual information is inappreciable in the confusion matrices, but is present in eight of the thirteen classes. In fact, these eight classes are the ones in which the visual descriptor provides acceptable results. We conclude that for classes that can be either visually or textually rep-resented, the use of combined features provides better accu-racies. Whereas for classes in which one of the modalities is not really suitable (visually representing an invoice class or textually representing a class sharing a certain layout), the combination of textual and visual modalities hinders the final performance against the sole use of the most suitable modality.

We report in Fig. 6 the coverage versus accuracy plots for different classifiers when using a rejection threshold over the final probability issued by the combination of classifiers. Depending on the application scenario, the threshold can be set to a low value in order to provide low rejection rates at the risk of accepting falsely classified pages. Or it can be increased in a conservative fashion, and in such a sce-nario just highly confident classifications are accepted and thus rejecting a significant number of positive samples. The coverage-accuracy plot shows the accuracy drop when the system is asked to provide an answer for more and more ele-ments from the collection (i.e., as the rejection threshold is set from conservative to more tolerant values). Again, the SVM classifier yields the best performance. For instance, in order to reach a 98 % classification accuracy, the system with the SVM classifier rejects approximately 10 % of the incoming pages whereas the na X ve Bayes classifier rejects more than 50 %. We also appreciate the significant drop in accuracy for the same document coverage. Finally, for the most rigid and critical scenarios, we see that the proposed method is able to reach a 100 % recognition rate by just rejecting 35 % of the pages and forwarding them to manual inspection.

In Table 3 , the classification rates when using document n -grams after classification with the SVM-RBF classifier are given. The results show that the inclusion of n -gram proba-bilities increases recognition accuracy both when using the textual and visual features alone and in the combined sce-nario. In particular, the best results are obtained when using the late fusion strategy in combination with 5-grams, where the accuracy is boosted from 95 . 49 to 96 . 84 %. 7 Conclusions In this paper, we have presented a page classification application tested in a banking workflow. The proposed architecture represents administrative document images by merging two different modalities, namely visual and textual ( a ) (b) (c) descriptions. The visual description relies on the pixel inten-sity distribution whereas the textual description uses latent semantic analysis to represent document content as a mix-ture of topics. The performance of Bayesian, distance-based, and kernel-based classifiers has been analyzed. Early and late fusion strategies aimed at combining the visual and tex-tual cues have also been compared. A final step modeling the page stream by means of an n -gram model has been used to refine the individual page classification. The pro-posed method has been tested in a real large-scale envi-ronment, and we report results on experiments with 70,000 pages.
 References
