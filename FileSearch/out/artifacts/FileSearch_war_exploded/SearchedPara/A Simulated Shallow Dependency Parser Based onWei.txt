 Recently, dependency grammar has gained re newed attention and becomes more promi-nent. Currently it is dominant that using data-driven approaches to learn parsers auto-matically from experience, such as probabilistic generative models [3], generative prob-abilistic parsing models [2] and deterministic discriminative model [7] and so on. Gen-erally speaking, data-driven approaches fall into two categories, i.e. generative models and discriminative models. The latest state-of-the-art dependency parsers are discrimi-native which are based on classifiers trained t o score trees, given a sentence, either via factored whole structure scores [5] or local parsing decision scores [6]. However, sel-dom work about shallow dependency parsing like shallow phrase-structure parsing has been done. In the paper, a discriminative dependency parser based on weighted hierar-chical structure learning is proposed to simulate shallow dependency parsing, aiming at improving dependency parsing for nodes closer to the root node.

The remainder of this paper is organized as follows. Section 2 first makes a brief introduction to dependency grammar, and then describe dependency parsing algorithm in detail. Section 3 gives the details of adopted learning algorithm and some discussion. To demonstrate the usefulness of our algorithm, Section 4 contains the results produced by several dependency parsers. Last section contains some conclusions plus some ideas for future work.
 2.1 Overview of Dependency Grammar In Dependency Grammar, individual words in a sentence are considered to be linked together in dependency relations instead of being combined just mechanically. When-ever two words are linked by a dependency relation, we say that one of them is the head and the other is the dependent, and that th ere is an edge connecting them. In general, the dependent is the modifier or complement; the head plays the larger role in deter-mining the behavior of the pair. The dependent presupposes the presence of the head; the head may require the presence of the dependent. The figure 1 depicts the skeleton of dependency structure of a sentence. The dashed line means the head  X  X oot X  and the arc pointing from head to dependent. The dependency structure is a tree with the main verb as its root (head).

Similar to shallow phrase-structure parsing, shallow dependency parsing breaks up sentence into  X  X pans X , and then link them with directed arcs. The edges connecting dif-ferent spans are named  X  X pan-link X , and the two nodes linked by  X  X pan-link X  are defined as  X  X pan-head X  with respect to corresponding span. Different from full parsing, shallow dependency parsing only focuses on  X  X pan-head X  and  X  X pan-link X , instead of nodes and edges inside spans. Currently there is no s tandard about what is shallow dependency parsing like shallow phrase-structure parsing, and the following gives a rough guide-line: the dummy node  X  X oot X  is the root of a dependency tree; each subtree is treated as a span in shallow parsing. Based on above analysis, it is reasonable to think that  X  X pan-head X  and  X  X pan-link X  closer to  X  X oot X  are more important than the others in shallow parsing, such as that inside span. Thus it is feasible to improve accuracy of dependency relations closer to  X  X oot X  to simulate shallow dependency parsing, with regular full parsing. 2.2 Parsing Algorithm The CKY algorithm is a well-known O ( n 3 ) algorithm for PCFG parsing [4]. When applied to dependency parsing, however, the CKY has the time complexity of O ( n 5 ) . Eisner proposed an parsing algorithm similar to CKY that has a time complexity of O ( n 3 ) [3]. The idea is to parse the left and right dependents of a word independently and combine them at a later stage. During dependency parsing, there are many spans produced. Among them adjacent spans are po ssible to be combine d into a longer span iteratively. At last one span including all words can be generated as output. This pars-ing algorithm removes the need for the additional head indices and requires only two additional binary variables that specify the direction of the item and whether an item is complete. For space limitation the parsing algorithm is described here briefly, and for details please refer to [3,5]. As indicated earlier, dependency tree is built bottom-up via combining small spans iteratively. The number of generated spans, however, grows exponentially with the size of sentence length, so the learning task is to, given a sentence, find the best one from numerous candidates. In this paper we adopt a strategy similar to McDonald et al [5], that is to say, every candidate is scored and chooses the one with highest score as final dependency parsing output.

An extension of original binary perceptr on for multiclass problem (MPA) is proposed by Collins [1] as follows: where z i is a prediction for instance x i and  X  is a constant positive factor for promo-tion or demotion, and the definition of  X  is the same as the previous representations of feature vector. Note that the parameter  X  is a constant, that means weights of all relations in dependency structure are updated (add or minus) with equal scalar. How-ever, it is not always reasonable. For instance, in figure 2 there are three dependency tree candidates -a correct dependency tree ( a), both incorrect dependency trees (b) and (c) (node enclosed by dashed circle has incorrect head) -for sentence  X  X conomic news had little effect on financial markets X . The shallow parsing result,  X  X ews had effect on markets X , can be easily drawn from the right candidate (a) or the wrong candidate (b), both having one wrong relation. As discussed in subsection 2.1, we only concentrate on  X  X pan-head X  and  X  X pan-link X  in shallow dep endency parsing: the nodes and edges closer to  X  X oot X  transmit more semantic information than others. Based on analysis above, we proposed a simulated shallow dependency parser derived from a full parsing.

To differentiate nodes and edges in dependency tree we replace the scalar factor  X  with a diagonal matrix A =(  X  1 ,..., X  n ) . Assuming that feature vector  X  ( x, y ) (  X  1 or 0. In what follows we make some assumptions for simplicity. Given a sentence x i and corresponding dependency tree y i ,let T be the set of candidates. The inner product,  X  ( x scores than correct dependency tree y i : Comparing to original perceptron algorithm and others, the innovation of our algorithm derived from the refinement of update factor, i.e. diagonal matrix A . Given a candidate z drawn from error set E , A z is defined as follows: where hl ( f i ,z ) is a function named  X  X ierarchical length function X  and defined as: (1). The height of a relation &lt;i,j&gt; is the number of passed edges from  X  X oot X  to node j
Then we can rewrite formula 1 by substituting  X  X  X  for  X   X   X  and obtain formula 4: where E is the error set of size k and z i is some candidate belonging to error set E .Itis clear that diagonal element  X  i is zero when corresponding feature f i is zero, therefore, we can ignore  X  X ero X  features in implementation for simplifying the computation. The definition of A in equation 3 implies that the closer to  X  X oot X  nodes and relations are, the more aggressive the update to them are during learning. Consequently, we make a trade-off between relations closer to  X  X oot X  and relations far from  X  X oot X : improve the learning of the former at the cost of decrease of the latter. We have drawn a conclusion in subsection 2.1 that nodes and edges close to  X  X oot X  are more semantic and important than those far from  X  X oot X . Likewise, shallow dependency parsing concentrates on nodes and edges which are close to  X  X oot X . So it is feasible to simulate shallow dependency parser via the proposed approach. 4.1 Data and Task Definition The original data consisting of 10,000 Chinese instances are provided by Information Retrieval Lab of Harbin Institute of Tec hnology. These instances have been labeled manually with POS, relation and relation type in advance. The total data in the exper-iments were randomly divided into two groups: one for training with size of 9000 and one for test with size of 1000.

The task in the experiment is to assign la beled dependency edges to Chinese sen-tences. Each sentence was represented as a sequence of tokens plus POS. For each to-ken, the parser must output its head and corresponding dependency relation. The metrics applying to evaluate parsers are defined as follows (Specially, excluding punctuation from scoring):  X  LAS (labeled attachment score). It is the proportion of  X  X coring X  tokens that are  X  UAS (unlabeled attachment score). It is the proportion of  X  X coring X  tokens that are  X  LS (label attachment score). It is the proportion of  X  X coring X  tokens that are as- X  LAS-n (labeled attachment score with height n ). The height of a token is the num-4.2 Experimental Results and Analysis In this paper we proposed a variation of multic lass perceptron algorithm (VMPA), using update rule in formula 4, for simulated shallow dependency parsing. To verify the effec-tiveness of the proposed approach, we carried out the experiment in which two parsers were built from the training dataset of size 9000 using VMPA and original multiclass perceptron algorithm (MPA) described in formula 1, and then applied them to the test dataset of size 1000 respectively. To make a comparison, we evaluated additional two state-of-the-art dependency parsers: MSTParser 1 and MaltParser 2 , which were devel-oped by McDonald [5] and Nivre [6] respectively. Note that MaltParser used embedded SVM learner and predefined feature model for Chinese. MSTParser used its default setting in the experiment. All results were reported in table 1. The results of VMPA showed that the accuracies of tokens with lower height, comparing to that of MPA, had some improvement. LAS-1 and LAS-2 increased 5.86% and 2.02% respectively. Of course, the improvement was at the cost of decrease of accuracies of nodes far from  X  X oot X . At the same time, the results of both VMPA and MPA were a little worst than two state-of-the-art parsers, i.e. MSTParser and MaltParser. We believed that may be due to the difference of learning algorithms. In this paper we focus on shallow dependenc y parsing and propose a discriminative de-pendency parsing algorithm based on weighted hierarchical structure learning to sim-ulate it. The results demonstrated that accu racies of nodes closer to  X  X oot X  increased at the cost of some decrease in nodes far from  X  X oot X . This improvement, however, is somewhat limited because the learning is ba sed on instance one by one thus could not make overall trade-off over all training instances. Some improvements to the proposed approach may be brought through additional r esearch. First, the definition for shallow dependency parsing in this paper is still r ough and simple. Secondly, the trade-off is based on single example one by one instead of the whole examples due to the online framework of the learning algorithm. In future work we may consider applying batch learning algorithm, such as SVM, with trade-off strategy for shallow parsing.
