 The retrieval effectiveness of the underlying document search component of an expert search engine can have an important impact on the effectiveness of the generated expert search results. In this large-scale study, we perform novel exper-iments in the context of the document search and expert search tasks of the TREC Enterprise track, to measure the influence that the performance of the document ranking has on the ranking of candidate experts. In particular, we show, using real and simulated document rankings, that while the expert search system performance is related to the relevance of the retrieved documents, surprisingly, it is not always the case that increasing document search effectiveness causes an increase in expert search performance.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Performance, Experimentation Keywords: Expert Search, Document Search
Many models for expert search are based on the premise that the more a document is related to the topic of the query, the more likely that candidates associated to that document will have relevant expertise to the query [2, 3]. However, the manner in which the strength of topicality is weighted  X  how much each document is related to the query  X  has seen less analytical research. Typically, experiments have shown that by applying a known information retrieval (IR) technique which usually improves the retrieval performance of a document search engine, performance is also improved for the expert search engine (for example [2, 3]).
What remains unclear from these analyses is which docu-ments in a document ranking are actually useful for produc-ing an accurate ranking of experts. Should the document search component be trained to give as many relevant doc-uments as possible, or only to highly rank a few key pages for the topic (i.e. focusing on recall or precision)? The aim of this paper is to revisit the document search component, by analysing various aspects of the quality of the document ranking to ascertain how these affect the retrieval perfor-mance of the expert search system.

The central contribution of this work is a novel large-scale study  X  using distinct sources of document rankings  X  of which factors of the quality of the document ranking af-fect the retrieval performance of a range of expert search approaches. For instance, our methodology allows us to de-termine if a particular expert search approach  X  X refers X  a document ranking of high precision more so than another approach. In our study, we use two diverse sources of docu-ment rankings, namely the document rankings submitted by participants to the TREC 2007 document search task, and fictitious rankings with various simulated retrieval perfor-mances. Surprisingly, we find that it is not always the case that increasing document search effectiveness causes an in-crease in expert search performance.
Various approaches have been proposed for expert search which use documentary evidence of expertise for each can-didate (called candidate profiles) to rank candidates in re-sponse to a query. In general, the most effective approaches are based on mapping a ranking of documents into a rank-ing of candidates. This is the approach taken by the Voting Model [3], which sees the expert search task as a voting process. In the Voting Model, the ranking of documents (denoted R ( Q )) defines votes for candidates to be retrieved: each time a document associated with a candidate is ranked in R ( Q ), then this is an implicit vote for that candidate to have relevant expertise to the query. The so-called Model 2 approach works in a similar manner [2], but, unlike the Vot-ing Model, is limited to use in language modelling settings.
The Voting Model defines many voting techniques, each corresponding to a different way of aggregating the votes from a ranking of documents into a ranking of candidates. Using the Voting Model, we have the advantage of experi-menting with various voting techniques, each of which en-capsulates different intuitions about how evidence from the document ranking is used to rank experts. In this work, we study six voting techniques (summarised in Table 1), each of which uses either the score of a document with respect to the query, or the rank of a document in the underlying ranking. prof ile ( C ) defines the set of documents associated to each candidate as evidence of their expertise.
It seems intuitive that a more refined, higher quality doc-ument search component will allow an expert search sys-tem to attain improved retrieval performance. For example, training the document search component [2, Ch. 4][3, Ch. 5], or applying field-based weighting models or query term Table 1: Summary of voting techniques used in this paper. D ( C, Q ) is the set of documents R ( Q )  X  prof ile ( C ) . k X k is the size of the described set. proximity [3, Ch. 6] have been shown to improve an expert search engine. Different formulations of query expansion on the document ranking have both been shown to help or hin-der expert search performance [3, Ch. 7]. Moreover, the application of Web IR features on the document ranking (e.g number of inlinks, URL length) was found not to be as useful as other expert search-specific evidence, such as the proximity of query terms to occurrences of candidates X  names [3, Ch. 7].

In these previous works, the application of different tech-niques has improved the document ranking in some way that has often resulted in an improved candidate ranking. However, the aspects of the document ranking which had an impact on the expert search performance are unknown. Moreover, there were no relevance assessments with which to directly evaluate the document ranking in context. To tackle this, in [4], we studied approximating a document ranking evaluation. In contrast, in this study, we use many document rankings with known retrieval performances as in-put to various voting techniques, and compare and contrast the document ranking performance with the corresponding expert search performance. In other words, we are testing whether topically relevant documents are necessary and suf-ficient expertise evidence.
In this work, we aim to address the following question: when used as input to an expert search approach, which as-pects of a document ranking have an impact on the retrieval performance of the generated candidate ranking? The par-ticular document ranking aspects which produce accurate candidate retrieval performance may depend on the partic-ular voting technique applied. For example, for effective per-formance, ApprovalVotes requires many documents that are related to the topic and associated to relevant candidates to be retrieved, while minimising the number of retrieved documents associated to irrelevant candidates. In contrast, for other voting techniques such as RecipRank or Borda-Fuse, the document ranking should highly rank documents that are related to the topic and associated to relevant candi-dates. Documents that are not about the topic or associated to irrelevant candidates should not be retrieved, or should be ranked as lowly as possible; RecipRank focuses more on the top of the document ranking than BordaFuse.

The difficulty in measuring the quality of the document ranking is that there are no measures which easily encap-sulate these preferences of the various voting techniques on the document ranking. Instead, we examine both the effec-tiveness of the document ranking when used for a document retrieval task, and the retrieval performance of the rank-ing of candidates generated by use of a voting technique on Figure 1: Scatter plot showing the correlation be-tween D-MAP &amp; E-MAP for the BordaFuse voting technique. such a document ranking. By comparing the performance of the document ranking to the accuracy of the generated ranking of candidates, we aim to draw conclusions about the features of the document ranking which matter most for a given voting technique.

In particular, we use the document search task and the expert search task of the TREC 2007 Enterprise track. Im-portantly, for both tasks, participants used a common set of queries, and a common document collection called CERC. In the document search task, systems should identify rele-vant documents for each query, while for the expert search tasks, relevant experts should be suggested.

In the following sections, we aim to determine how the re-trieval performance of an IR system on the document search task has an impact on the accuracy of the generated rank-ing of candidates, when that IR system is used as input to a given voting technique. We perform this experiment using two sources of rankings, namely the TREC 2007 submitted runs (Section 4.1) and simulated IR systems (Section 4.2).
We are interested in determining how document rank-ings, of various but known retrieval performances, affect the performance of various voting techniques. To achieve this, we measure the performance of 63 real document rankings which were actual submitted runs to the TREC 2007 docu-ment search task, and then compare with the performance of each when used as the input to a voting technique. The relevance assessments of the TREC 2007 document search task (DS07) are used to assess the quality of the document rankings, while the relevance assessments of the TREC 2007 expert search task (EX07) are used to measure the accuracy of the generated candidate rankings. For clarity, the evalu-ation of a document ranking using MAP with DS07 judge-ments is denoted D-MAP, while evaluation of a candidate ranking is denoted E-MAP.

The associations between candidates and documents (the candidate profiles) form the most important experimental parameter. To identify possible candidates in the collection, we search the documents for email addresses of the form firstname.lastname@csiro.au. To generate the document-candidate associations for each candidate, documents are identified by the presence of the candidate X  X  exact name or email address [2, Ch. 7][3, Ch. 6], which has been shown to be effective for this task in the past.

Figure 1 illustrates D-MAP vs. E-MAP over all submitted document search runs, when applied using the BordaFuse voting technique. We observe that while there are some out-liers, we can see that there is a rough correlation between D-MAP and E-MAP. A higher D-MAP makes BordaFuse more likely to have a higher E-MAP. However, around the range of D-MAP 0.28 X 0.45, there is less correlation, and we have a less clear picture. We note that of the runs with D-MAP in this range, when applied to BordaFuse, some perform stronger than others. This means that the exact character-istics of the document ranking desired by BordaFuse are not being well measured by D-MAP  X  of the outliers, there are some runs with low D-MAP but with strong E-MAP. On further inspection, we found that these runs have returned far less documents than the other runs. This degrades their D-MAP performance, however (E-)MAP on the EX07 task is improved by considering less documents in the document ranking [3, Ch. 6].

We can quantify the extent to which the system rankings by D-MAP and E-MAP in Figure 1 are correlated, using the Spearman X  X  rank correlation co-efficient  X  . Moreover, because it has been previously noted that the voting tech-niques performed best for the EX07 task using only the top 50-ranked documents, we perform our correlation experi-ments where the R ( Q ) for every query has been cutoff after 50 retrieved documents.

The top part of Table 2 presents the correlations between various document search task measures and the accuracy of various voting techniques. We assess the D-MAP, D-MRR, D-nDCG 1 , D-P@10 and D-Recall measures, to determine which are correlated with the official measures of the expert search task, namely E-MAP and E-MRR. The best correla-tions for each candidate ranking measure and voting tech-nique (row) are emphasised, while correlations which are statistically different (using a Fisher Z-transform and the two-tailed significance test) from the best correlation in each row are denoted * ( p &lt; 0 . 05) and ** ( p &lt; 0 . 01). Finally, the best E-MAP and E-MRR performances for each voting technique for any input document ranking are also reported.
From the results in Table 2, we observe overall strong pos-itive correlations, suggesting that the performance of various voting techniques can be predicted by various measures cal-culated on the document ranking. However, from the overall trends it is not the case that for each E-measure, the corre-sponding D-measure is the most correlated. Instead, various voting techniques focus on different parts of the document ranking in different ways, and the document ranking quality affects their overall accuracy in different ways. In the fol-lowing, we detail how document ranking quality affects each voting technique in turn.
 ApprovalVotes : Highest correlations are with D-Recall. This is expected, as this technique only considers the number of votes, which we postulate will be highly correlated with D-Recall. Other measures which examine the entire ranking, e.g. D-MAP, D-nDCG, and D-P@50 are also strongly corre-lated with E-MRR and, in particular, E-MAP. Conversely, less strong correlations are observed with measures that ex-amine only the higher ranked documents (e.g. D-MRR or D-P@10), which is expected, as ApprovalVotes treats all re-trieved documents equally, regardless of rank.
 BordaFuse : High correlations with D-MAP, D-nDCG &amp; D-Recall, showing that while BordaFuse uses all the re-trieved documents, it focuses on the more highly ranked ones. The higher correlation for D-nDCG than D-MAP indicates candidate ranking performance is enhanced by a document ranking which ranks highly relevant documents before relevant ones.
 RecipRank : The trends exhibited by RecipRank are simi-lar to BordaFuse, however with slightly less high correlations
DS07 task has ternary judgments [1]. overall. Surprisingly, there is no bias toward top-heavy D-measures such as P@10.
 CombMAX : Intuitively, CombMAX is most influenced by the top of the document ranking, hence it is expected that a retrieval system which has good success at early ranks will likely enable CombMAX to perform well, explaining why CombMAX only shows high correlations with D-MRR. expCombSUM : Similarly to BordaFuse, we find that exp-CombSUM has a high correlation with D-MAP and D-nDCG, showing a focus towards the top of the document ranking (particularly highly relevant documents). The correlations for D-Recall is only slightly higher than D-nDCG, and not significantly so. expCombMNZ : expCombMNZ also exhibits high correla-tions with D-MAP, D-nDCG, &amp; D-Recall. Compared with expCombSUM, D-Recall is relatively more important than D-MAP, which is explained by the number of votes compo-nent in expCombMNZ.

Overall, the strength of the correlations exhibited are prom-ising, indicating that there is a strong likelihood of a rela-tionship between the topical retrieval performance of R ( Q ), and the performance of a voting technique. In particular, our intuitions about the  X  X references X  of the voting tech-niques are confirmed -e.g. CombMAX prefers a high preci-sion ranking. When choosing a voting technique, a system designer should choose one which has a high correlation to a document ranking measure on which the existing document IR system is particularly effective. For example, a document IR system which has good MRR should use CombMAX, while another with high Recall/MAP may use expComb-SUM or expCombMNZ.

However, we do not find any 100% correlations, showing that not every improvement in document search effective-ness can have a positive impact on an expert search engine. The correlations found here do not show that topic relevance document retrieval performance is perfectly related to can-didate retrieval performance. This infers that there are some characteristics of the document ranking which are important to the voting techniques that are not being captured by the topical relevance document evaluation measures.

Recall that the majority of the real document rankings had a D-MAP between 0.28 and 0.45. Given these corre-lations, another natural question that arises is whether the observed correlations hold for a larger range of possible D-MAP values. In the next section, we use simulation to gen-erate document rankings of various document retrieval per-formances, and determine how effective these are for expert search using the considered voting techniques.
So far, we have been investigating how real document rankings of various retrieval effectiveness affect the expertise retrieval performance when applied to various voting tech-niques. We now extend our experiments to use simulated document rankings, which cover an extended range of pos-sible D-MAP values. We use the AP simulation algorithm proposed by Turpin &amp; Scholer in [6], which makes improving or degrading random swaps of relevant and irrelevant doc-uments until the target AP performance is achieved (or no more swaps are possible).

Firstly, for each query, the D-MAP range is split into 20 equal-sized bins (size 0.05). Then, we generate 20 rankings in each bin, using a random target D-MAP value within the range of the bin, to give a total of 400 simulated  X  X uns X . Each run, which has 50 queries of very similar effectiveness, is then used as input to a voting technique. As the simula-tion does not produce document relevance scores, we focus only on rank-based voting techniques in these experiments. Moreover, each document ranking is unique, using a differ-ent ordering of the relevant and irrelevant documents, which may have an impact on the effectiveness of the used voting techniques that consider the ordering of documents.
The second part of Table 2 presents the correlations be-tween various document search task measures for the simu-lated retrieval systems and the accuracy of three voting tech-niques using them. On comparing these results with those from the top part of Table 2, we note considerably stronger correlations. This reinforces, that across a full range of pos-sible document search performance, there appears to be a link between the overall topic relevance quality of a docu-ment IR system, and its likelihood to be useful as a compo-nent of an expert search engine.

However, in contrast to our earlier correlation results, we note that all voting techniques are mostly correlated with D-MRR, and that there are no significant differences between the correlation measures. On further inspection of the simu-lated document rankings, we found that all D-measures were very similar (e.g. D-MAP vs. D-MRR has  X  = 0 . 9042 for the 400 simulated document rankings, compared to  X  = 0 . 4134 for the 63 real document rankings). Future work will exam-ine how to create more realistic simulated rankings which have different performances on different queries, perhaps starting from the real document rankings.

Table 2 also presents the maximum E-MAP and E-MRR values achieved for each voting technique by any document ranking. From these, we note that the maximum achieved E-MAP and E-MRR values using the simulated rankings are not as high as those from the real TREC runs, even though the simulation experiments contain systems with al-most perfect D-MAP document rankings. For instance, the highest E-MAP (0.3147, BordaFuse) was produced by a doc-ument ranking with a D-MAP of only 0.6590. These results strengthen those reported in [5], which postulates that not all relevant on-topic documents may be good indicators of expertise evidence, and their exact ordering has an impact on the retrieval performance achievable by a voting tech-nique. It is also possible that there exist some irrelevant documents which can be of benefit to an expert search vot-ing technique [5], and for a ranking with very good D-MAP, these documents have been suppressed, to the detriment of expert search effectiveness.
This work is the first large-scale empirical study into the influence of the document ranking in an expert search sys-tem. In particular, we studied this influence on several vot-ing techniques from the Voting Model. However, the results here should generalise to other expert search approaches such as [2]. We experimented with both real and simulated document rankings, and showed that there is a correlation between the ability of the document ranking system to re-trieve topically relevant documents with the ability of vot-ing techniques to retrieve an accurate ranking of candidate experts. In particular, using the real document rankings, D-MAP, D-nDCG and D-Recall were all shown to be important predictors of expert search performance. However, from the low maximal performances using the simulated rankings, it is clear that increasing the quality of the input document ranking does not always result in an increase in the retrieval performance of the resulting ranking of candidates. Future work will investigate more advanced simulations, possibly using real document rankings.
 1. Introduction
An expert search system is designed to rank candidate experts in response to a query, usually using documentary evi-dence that represent each candidate X  X  expertise areas. With the advent of the Text REtrieval Conference (TREC) Enterprise track in 2005 ( Craswell, de Vries, &amp; Soboroff, 2006 ), expert search systems have been the object of some research in recent years. In the literature, much research has been devoted to the development of expert search approaches, in addition to the sources of, and identification of expertise evidence. Evaluation is performed by using an expert search test collection, which has candidate relevance assessments that denote the candidates with relevant expertise.

Many expert search models are based on the premise that the more a document is related to the topic of the query, the more likely that candidates associated to that document will have relevant expertise to the query. However, the manner in which the strength of topicality is weighted  X  how much each document is related to the query  X  has seen less analytical research. Typically, experiments have shown that by applying techniques that usually improve the retrieval performance of a document search engine, performance is also improved for the expert search engine (for example ( Balog, 2008; Macdonald &amp; Ounis, 2009b )).

What remains unclear from these analyses is which documents in a document ranking are actually useful for producing an accurate ranking of experts. Should the document search component be trained to give as many relevant documents as possible, or only to highly rank a few key pages for the topic (i.e. focusing on recall or precision)? The aim of this paper is to revisit the document search component, by analysing various aspects of the quality of the document ranking, with a view to ascertaining how these affect the retrieval performance of the expert search system. In particular, we use the Voting Model ( Macdonald &amp; Ounis, 2009b ) for generating the ranking of candidates from a ranking of documents. The Voting Model has  X  many effective voting techniques, each of which defines a different way of aggregating the votes from a ranking of docu-ments into a ranking of candidates, hence allowing us to experiment with various expert search approaches. Moreover, the Voting Model is agnostic to the used document ranking, meaning that it can be applied on rankings created by various information retrieval (IR) systems.

The central contribution of this work is a novel large-scale study  X  using distinct sources of document rankings  X  of which factors of the quality of the document ranking affect the retrieval performance of a range of voting techniques (i.e. different expert search approaches). For instance, our methodology allows us to determine if a particular voting technique  X  X  X refers X  a document ranking of high-precision over another technique. In our study, we use three diverse sources of document rank-ings, namely the document rankings submitted by participants to the TREC 2007 document search task, fictitious simulated rankings where document relevance assessments are used to generate runs with various retrieval performances, and finally, simulated document rankings derived from the candidate relevance assessments. From these experiments, we draw conclu-sions about how various voting techniques are impacted by different features of the document ranking. Surprisingly, we find that it is not always the case that increasing document search effectiveness causes an increase in expert search performance.
Another central finding of this work is the impossibility of creating a perfect ranking of candidates. Through a failure anal-ysis, we show the reasons behind such difficulty.
 The remainder of this article is structured as follows: Section 2 defines the particular voting techniques from the Voting
Model that we apply in this work, and reviews work that demonstrate the effect of changes in the document search com-ponent. We describe our methodology for comparing document search and expert search effectiveness in Section 3 , and empirically correlate document search effectiveness with expert search effectiveness, using both real and simulated IR systems as input to the expert search approaches. In Section 4 , we simulate an ideal document ranking using the candidate relevance assessments, and then analyse why this does not result in a perfect ranking of candidates. Concluding remarks and directions for future work are highlighted in Section 5 . 2. Expert search approaches
Over the past few years, various approaches have been proposed for expert search, where documentary evidence of expertise for each candidate (called candidate profiles ) are used to rank candidates in response to a query. In general, the most effective approaches are based on mapping a ranking of documents into a ranking of candidates. This is the approach taken by the Voting Model for expert search ( Macdonald &amp; Ounis, 2009b ), which sees the expert search task as a voting pro-cess. In the Voting Model, the ranking of documents (denoted R ( Q )) defines votes for candidates to be retrieved: each time a document associated with a candidate is ranked in R ( Q ), then this is an implicit vote for that candidate to have relevant expertise to the query.

The Voting Model defines many voting techniques, each corresponding to a different way of aggregating the votes from a ranking of documents into a ranking of candidates. Hence, the Voting Model is a framework, which can generate several ex-pert search approaches (i.e. voting techniques). In this work, we study eight voting techniques, each of which uses either the Some of the voting techniques of the Voting Model have direct correspondence with the Model 2 approach of Balog,
Azzopardi, and de Rijke (2006) . Indeed, various language modelling approaches for expert search have been proposed, such as those of Balog and De Rijke (2008), Fang and Zhai (2007) and Fang et al. (2010) . In common, these approaches consider the association between candidate and documents to be non-binary. Macdonald, Hannah, and Ounis (2008) later showed how the Voting Model could be extended to non-binary associations  X  for instance, using the proximity of the occurrences of the candidate X  X  name and the query terms. In this work, we use only the basic Voting Model, which has the advantage of a broad range of voting techniques, each of which encapsulates different intuitions about how evidence from the document ranking is used to rank experts. Moreover, by focusing on binary associations, we can remove possible complications from our exper-iments, such as the training of weights for the associations. Finally, as will be seen later, we are experimenting with many document rankings with unknown ranking models, hence the fact that the Voting Model can handles document scores that are not probabilities is also a central advantage. 2.1. Voting techniques for expert search
In the first voting technique considered in this work, namely ApprovalVotes, the score for a candidate ( score ( C , Q )) is ob-tained by counting the number of voting documents in the ranking of documents R ( Q ): retrieved in ranking R ( Q ) that are also associated to the profile of candidate C .

Next, the BordaFuse voting technique differs from ApprovalVotes by weighting the votes to each candidate by the rank ( rank ( d , Q )) at which the voting document occurred in the document ranking R ( Q ), as follows:
The last rank-based voting technique is RecipRank (RR), where a vote is defined as the reciprocal of the rank (this contrasts with BordaFuse, which uses directly uses the rank as the vote):
Among the score-based voting techniques, CombSUM is similar to BordaFuse, except that the votes are weighted by the score ( score ( d , Q )) of the voting documents, instead of the rank:
CombMNZ is similar to CombSUM, except that the number of votes is explicitly integrated:
CombMAX takes into account only one document from the document ranking for each candidate, i.e. the strongest vote:
The last two considered voting techniques are slight variants of CombSUM and CombMNZ. In these variants, the score of each document is transformed by applying the exponential function (exp()). Applying the exponential function has two ef-fects: it removes the logarithm present in many document weighting models (such as language modelling, which is com-monly implemented as logarithms of probabilities), and, in doing so, it places more emphasis on the highly scored documents: It is of note that many of the voting techniques used here have the same names as data fusion techniques introduced by
Fox and Shaw (1994) or Aslam and Montague (2001) . However, they differ from data fusion techniques in that they are not designed to combine multiple rankings of documents, but instead they convert a single ranking of documents into a ranking of candidates ( Macdonald &amp; Ounis, 2009b ).

The CombSUM and expCombSUM voting techniques are related to the Model 2 language modelling approach for expert search ( Balog et al., 2006 ). Indeed, if the underlying document weighting model returns document scores in the probability space, then CombSUM is similar to Model 2. If the document weighting model returns the logarithm of probabilities, then expCombSUM is similar to Model 2 ( Macdonald &amp; Ounis, 2009b ). 2.2. Improving expert search performance
The focus of this work is to investigate the role of the document ranking, and its effect on the quality of the generated ranking of candidates. At first hand, it seems intuitive that a more refined, higher quality document search component will allow an expert search system to attain improved retrieval performance. Indeed, various studies have examined how the application of existing or adapted information retrieval (IR) techniques  X  each known to enhance retrieval effectiveness on document retrieval tasks  X  to the document search component of the expert search system has resulted in increased re-trieval performance. In particular, Balog et al. (2006) and Macdonald and Ounis (2008b) showed that training of the docu-ment search component could improve the retrieval effectiveness of the expert search engine. Moreover, when field-based were applied to the document ranking, the retrieval performance of the candidate ranking could again be enhanced. Apply-ing a Divergence From Randomness-based query expansion, either directly on the document ranking, or using a pseudo-rel-evant set consisting of related documents from the top-ranked candidates, was also shown to bring an enhanced document ranking, from which a more effective candidate ranking can be generated ( Macdonald &amp; Ounis, 2007 ).

However, overall, much of this evidence is circumstantial to the extent that applying techniques generally known to im-prove document rankings often improve expert search rankings. In other cases, some techniques were not beneficial: Balog (2008) found that while blind-relevance feedback could enhance a document search engine, its direct application was not beneficial to the expert search engine. While this observation contrasts with ( Macdonald &amp; Ounis, 2007 ), we note that a different query expansion technique was applied. Macdonald et al. (2008) found that the application of Web IR query-dependent features that are normally useful in Web or Enterprise settings (e.g number of inlinks, URL length) were not as useful as other expert search-specific evidence. Likewise, Zhu (2008) examined how Web IR evidence, namely anchor text and inlink evidence impacted on both document and expert search retrieval performance  X  indeed, anchor text was found to be significantly beneficial to expert search but not to document search. However, similarly to Macdonald et al. (2008) , Zhu (2008) found that inlinks did not bring significant benefit to expert search retrieval performance.

In these previous works, the application of different techniques has improved the document ranking in some way that has often resulted in an improved candidate ranking. However, the aspects of the document ranking which had an impact on the expert search performance are unknown, as are the reasons that cause techniques which usually enhance the document ranking to be of less usefulness on the resulting ranking of candidates. Moreover, there have been no relevance assessments with which to directly evaluate the document ranking in context. To tackle this, in Macdonald and Ounis (2008a) , we pre-viously studied how to approximate a document ranking evaluation. In contrast, in this study, we use many directly evalu-ated document rankings with various retrieval performances as input to various voting techniques, and compare and contrast with their corresponding expert search performance. 3. Comparing document search and expert search performance
In Section 2.2 above, we reviewed known retrieval techniques that improve the quality of the document ranking, and when they lead to an improvement in the effectiveness of the ranking of candidates. However, thus far, we have not been able to measure the preferred characteristics that each expert search approach has of the document ranking. In this work, we aim to address the following question: when used as input to an expert search approach, which aspects of a document ranking have an impact on the retrieval performance of the generated candidate ranking?
Intuitively, the features of the generated document ranking which produce accurate candidate retrieval performance are dependent on the particular voting technique applied. For the voting techniques that we apply in this work, we suggest that the document ranking preferences that produce an accurate ranking of candidates are as follows:
ApprovalVotes: For an accurate ranking of candidates, ApprovalVotes requires many documents that are related to the topic and associated to relevant candidates to be retrieved, while minimising the number of retrieved documents asso-ciated to irrelevant candidates.

RecipRank, BordaFuse, CombSUM, CombMNZ, expCombSUM, expCombMNZ: For these voting techniques, the document ranking should highly rank documents that are related to the topic and associated to relevant candidates. Documents that are not about the topic or associated to irrelevant candidates should not be retrieved, or should be ranked as lowly as possible; RecipRank and expComb * will focus more on the top of the document ranking.

CombMAX: There should be an on-topic document associated to each relevant candidate. Documents associated to irrel-evant candidates must be ranked lower than those associated to relevant ones.

The difficulty in measuring the quality of the document ranking is that there are no measures which easily encapsulate the demands of the various voting techniques on the document ranking. For instance, a methodology to precisely determine whether the document ranking has accurately ranked documents related to relevant candidates would firstly have to know all documents that should be associated to each candidate  X  i.e. a complete profile set ground truth. However, the generation of such a ground truth would be complex, and would require N M judgements made on document X  X andidate pairs (for N documents, M candidates).

Instead, we examine both the effectiveness of the document ranking when used for a document retrieval task, and the retrieval performance of the ranking of candidates generated by the use of a voting technique on such a document ranking.
This permits us to contrast document search performance with that of the expert search engine. In particular, we use the tasks of the TREC 2007 Enterprise track. For this track, two evaluation tasks were devised, namely the document search task, and the expert search task. Importantly, for both tasks, participants used a common set of queries, and a common document collection called CERC, which is a crawl of the website of an Australian government research organisation, namely the Com-monwealth Scientific and Industrial Research Organisation (CSIRO). In TREC 2007, the aim of the document search task was to identify relevant documents for each query, particularly those that were key for a user to achieve a good understanding of gest relevant experts for each query. Table 1 details the salient statistics of the document search and expert search tasks. It is of note that while the TREC 2007 document search task used pooling during relevance assessing, for the expert search task, no pooling was conducted, as staff from the CSIRO organisation provided a list of relevant candidates for each query ( Bailey et al., 2008 ). Finally, note that in our experiments, we do not consider the TREC 2008 document and expert search tasks, as the document task judging was carried out in a stratified manner ( Balog et al., 2009 ), meaning that considerably less relevant documents are available for each query than for TREC 2007.

In the following sections, we aim to determine how the retrieval performance of a document ranking, as measured using the document search task, has an impact on the effectiveness of the generated ranking of candidates, when that IR system is used as input to a given voting technique. We perform this experiment using two sources of rankings. Firstly, in Section 3.1 , we take each of the submitted TREC 2007 runs to the document search task, and use these as an input to various voting tech-niques. Secondly, in Section 3.2 , we use the document search task relevance assessments to simulate IR systems with various document retrieval performances. In each experiment, by comparing the performance of the document ranking to the accuracy of the generated ranking of candidates, we aim to draw conclusions about the features of the document ranking that matter most for a given voting technique. 3.1. Real document rankings
Here, we are interested in determining how document rankings, of various but known retrieval performances, affect the performance of the different voting techniques. In this scenario, we measure the performance of many real document rank-ings, and then compare this with how each performs when used as the input to a voting technique. In particular, we use the relevance assessments of the TREC 2007 document search task (DS-07) to assess the quality of the document rankings, while the relevance assessments of the TREC 2007 expert search task (EX07) are used to measure the accuracy of the generated candidate rankings. For document rankings, we use the actual submitted runs to the TREC 2007 document search task. Note that these are the output of real IR systems, and are suitable because the voting techniques are agnostic to the techniques used to generate the document rankings. We compare the ranking of systems based on a document search task evaluation measure such as MAP, which we denote D-MAP, to the ranking of systems after applying a voting technique and measuring using an expert search task evaluation measure, which, for clarity, is denoted E-MAP.

The document search task of the TREC 2007 Enterprise track consists of 50 queries (the same queries as for the expert search task), and associated relevance assessments generated by the participating groups X  judging pools of documents from submitted runs ( Bailey et al., 2008 ). In the document search task, there were 63 submitted runs, by 16 different participating groups. Fig. 1 a shows the distribution of D-MAP of the runs submitted to the document search task. From the figure, it is clear that the distribution of D-MAP across the runs is somewhat uneven. Essentially, there are a few runs of poor quality (D-MAP &lt; 0.25), and four runs of excellent quality (D-MAP &gt; 0.45). The middle is more mixed: 20 runs have 0.1 &lt; D-MAP 0.35, while a total of 34 runs have 0.35 &lt; D-MAP 6 0.45. This clustering of runs around the high quality end of the scale means that, for our experiments, we do not have a selection of runs of varying quality equally distributed across the possible range of D-MAP values. This may have an impact on the obtained results, as discussed in the next section. Fig. 1 b shows the precision-recall curves of the average submitted retrieval system, the best submitted system, and the worst submitted sys-tem (by D-MAP). From this figure, we note that the average system is much closer to the best submitted system than to the worst, emphasising the point that there is not an even distribution of document ranking systems across the range of the eval-uation measure. This observation is mirrored in Fig. 1 c, which shows that the mean and best of the submitted runs both have very good precision at early ranks. However, precision tails off after rank 100, when many of the relevant documents have been retrieved (average 147.2 documents per query).

We now use these rankings as input to the various voting techniques described in Section 2.1 . In terms of experimental setup, it is important to note that no index of the documents in the CERC collection is required for ranking the documents, as the document rankings are as submitted to TREC. Instead, the associations between candidates and documents (the candi-date profiles) form the most important experimental parameter. To identify possible candidates in the collection, we search the documents for email addresses of the form firstname.lastname@csiro.au associations for each candidate, documents are identified by the presence of the candidate X  X  exact name. This has previously been shown to lead to more effective candidate profiles than by using the candidates X  last names or email addresses alone
Fig. 2 a and b compare D-MAP and E-MAP over all submitted document search runs, when used with the BordaFuse (Eq. (2) ) and expCombMNZ (Eq. (7) ) voting techniques, respectively. From the figures, several observations can be made.
While there are some outliers, we can see that there is a rough correlation between D-MAP and E-MAP. A higher D-MAP makes the voting techniques more likely to have a higher E-MAP. However, around the range of D-MAP 0.28 X 0.45, there is less correlation, and we have a less clear picture. We note that of the runs with D-MAP in this range, when used with the voting techniques, some perform stronger than others. This means that the exact characteristics of the document ranking desired by the voting techniques are not being well measured by D-MAP  X  of the outliers, there are some runs with low D-MAP but with strong E-MAP. On further inspection, we found that these runs have returned far less documents than the other runs. This degrades their D-MAP performance, however on the EX07 task, (E-)MAP is improved by considering less documents in the document ranking ( Macdonald, 2009 ). Lastly, in Fig. 2 b, note that many runs with various D-MAP values have obtained E-MAP of 0. This is caused by the runs not providing reasonable relevance scores, thus making the score-based voting techniques useless. 1 However, as can be seen from Fig. 2 a, the BordaFuse voting technique performs well for all of these runs, as it does not rely on the document relevance scores. This demonstrates the benefit of having rank-based voting techniques, such as BordaFuse and ApprovalVotes, which can be successfully applied to search engines where scores are not provided.
 We can quantify the extent to which the system rankings by D-MAP and E-MAP in Fig. 2 a and b are correlated, using the
Spearman X  X  rank correlation coefficient q . Moreover, because it has been previously noted that the voting techniques per-formed best for the EX07 task using only the top 50-ranked documents ( Macdonald, 2009 ), we perform our correlation experiments both when the various R ( Q )s have unlimited size (up to 1000 retrieved documents for every query, as is com-mon TREC practice), and when they have been cut-off after 50 retrieved documents.

Tables 2 and 3 present the correlations between various document search task measures and the effectiveness of various voting techniques, when the R ( Q ) has size 1000 or 50, respectively. In particular, we assess the D-MAP, D-MRR, D-nDCG,
D-P@10 and D-Recall measures, to determine which are correlated with the official measures of the expert search task, namely E-MAP and E-MRR (E-P@10 is not used, as there are on average less than 10 relevant candidates; E-nDCG was also not suitable as only binary judgements were available for candidates  X  see Table 1 ). The best correlations for each candidate ranking measure and voting technique (row) are emphasised, while correlations that are significantly worse (using a Fisher
Z-transform and the two-tailed significance test) than the best correlation in each row are denoted
Finally, the best E-MAP and E-MRR performance for each voting technique for any input document ranking is also reported (in the columns denoted Expert Measure Max)  X  these allow the best performance of the voting techniques for any of the input document rankings to be determined.

Comparing the two tables, we note higher correlations and higher maximum E-MAP and E-MRR performances in Table 3 (the one exception, CombMAX, is explained in our analysis below). This is expected, as from ( Macdonald, 2009 ), it has al-ready been noted that some voting techniques have a high preference for only examining the top 50 retrieved results on the EX07 task. This is somewhat supported by the distribution of the Precision@rank curve shown in Fig. 1 c, from which we can see that the early precision of most of the document retrieval systems was very good. For both of these reasons, in the remainder of this section, we concentrate on the results reported in Table 3 .

From the results in Table 3 , we can make several observations. Overall, the performance of various voting techniques, as measured by several candidate ranking measures, can be accurately predicted by various measures calculated on the docu-ment ranking. However, examining the overall trends, we note that it is not the case that for each E-measure, the corre-sponding D-measure is the most correlated. Instead, various voting techniques focus on different parts of the document ranking in different ways, and the document ranking quality affects their overall effectiveness in different ways.
In the following, we take each voting technique in turn, and discuss the document ranking aspects that affect the voting technique X  X  retrieval performance, referring back to the predictions made in Section 3 above.

ApprovalVotes: For this voting technique, we note that the highest correlations are observed with D-Recall. This is expected, as this technique only considers the number of votes, which we postulate will be highly correlated with D-
Recall. Other measures which examine the entire ranking, e.g. D-MAP, D-nDCG, D-P@50 and D-rPrec are also strongly cor-related with E-MRR and, in particular, E-MAP. Conversely, less strong correlations are observed with measures that exam-ine only the higher ranked documents (e.g. D-MRR or D-P@10), which is expected, as ApprovalVotes treats all retrieved documents equally, regardless of rank.

BordaFuse: This voting technique exhibits high correlations with D-nDCG, D-MAP, D-rPrec and D-Recall, showing that while it uses all the retrieved documents, it appears to have some focus on the more highly ranked ones. The fact that there is a higher correlation for nDCG than MAP indicates that the highly relevant documents are more important as expertise evidence than the ones judged relevant, and that there are gains to be made for candidate ranking accuracy in placing these highly relevant documents higher in the document ranking.

RecipRank: The trends exhibited by RecipRank are similar to BordaFuse, however with slightly less high correlations over-all. Surprisingly, there is no particular bias towards top-heavy D-measures such as P@10.

CombMAX: It is easy to see that CombMAX focuses on the top of the document ranking for the retrieval of most of its candidate votes, hence it is of no surprise that a retrieval system which has good success at early ranks will likely enable
CombMAX to perform well. This explains why CombMAX only shows high correlations with D-MRR. Moreover, this cor-relation is emphasised when the document ranking is extended to length 1000 ( Table 2 ), inferring that the cut-off of the document ranking at rank 50 is hindering the recall of CombMAX for some relevant candidates that are only associated with low-ranked documents.

CombSUM, CombMNZ: These voting techniques are interesting, in that they are supposed to use information from all of the document ranking  X  more so than expComb * . However, their performance is more correlated with D-MRR than D-MAP or D-nDCG. Recall that CombSUM and BordaFuse are related: if CombSUM is correlated to D-MRR more so than
BordaFuse, then this suggests that the distribution of document scores for most document rankings over-emphasises some highly ranked documents. This is strengthened by the high correlations with D-P@10, D-P@30. expCombSUM: Again, similarly to BordaFuse, we find that expCombSUM has a high correlation with D-MAP and D-nDCG, showing that they have an increased focus on the top of the document ranking (particularly highly relevant documents).
The correlations with D-Recall and D-rPrec are only slightly higher than D-nDCG, and not significantly so. D-rPrec is known to be highly correlated to D-MAP ( Buckley &amp; Voorhees, 2004 ). expCombMNZ: Similarly to expCombSUM, expCombMNZ exhibits high correlations with D-MAP, D-nDCG, D-Recall and D-rPrec. We note that D-Recall is relatively more important than D-MAP to expCombMNZ when compared with exp-CombSUM. This is explained by the number of votes component in expCombMNZ.

Overall, the strength of these correlations exhibited is promising, and indicate that there is a strong likelihood of a rela-tionship between the retrieval performance of the document ranking R ( Q ), as measured here, and the retrieval performance of an expert search approach (i.e. voting technique). When choosing a voting technique, a system designer should choose one which has a high correlation to a document ranking measure on which the existing document IR system is particularly effective. In this way, the expert search engine should also exhibit good retrieval performance. For example, a document
IR system which has good MRR should use CombMAX, while another with high Recall/MAP may choose expCombSUM or expCombMNZ.

However, we do not find any 100% correlations, showing that not every improvement in document search effectiveness can have a positive impact on an expert search engine. As the correlations found here do not show that topic relevance doc-ument retrieval performance is perfectly related to candidate retrieval performance, this infers that there are some charac-teristics of the document ranking that are important to the voting techniques but are not being captured by the topical relevance document evaluation measures.

Given these correlations, another natural question that arises is whether the accuracy of the candidate ranking continues to improveasthedocumentrankingisimprovedbeyondD-MAP0.45(themaximumperformanceamongtheusedDS-07runs).In the next section, we use simulation to generate document rankings of various document retrieval performance, including  X  X er-fect X  document rankings, and determine how effective these are for expert search using the considered voting techniques. 3.2. Simulated document rankings
Given knowledge of the relevant and irrelevant documents, it is possible to simulate document rankings of a particular performance. For instance, in a  X  X erfect X  ranking of documents, the IR system would retrieve relevant documents before retrieving any irrelevant documents. Various other permutations of relevant and irrelevant documents result in different re-trieval performances. In ( Macdonald &amp; Ounis, 2009a ), we investigated the use of (relevant-only) perfect document rankings in the context of expert search. Here, instead, we employ a more general strategy for generating rankings with various re-trieval performances. In particular, Turpin and Scholer (2006) used IR systems with simulated accuracy in an interactive evaluation. They proposed an algorithm that generates a ranking of documents for a query with a pre-determined Average
Precision (AP) performance, by making random swaps of relevant and irrelevant documents until the target AP performance is achieved (or no more swaps between relevant and irrelevant documents are possible).

So far, we have been investigating how real document rankings of various retrieval effectiveness affect the expertise re-trieval performance when applied to various voting techniques. We now extend this work to include simulated document rankings. The use of simulated document rankings allow the experiments in Section 3.1 to be repeated using document rank-ings with an extended range of possible D-MAP values, including a perfect document ranking, where all relevant documents are ranked ahead of irrelevant ones.

It is of note that in simulating document rankings, an ordering of documents is produced, but the actual relevance scores (as normally produced by a document weighting model, such as BM25 ( Robertson, Walker, Hancock-Beaulieu, Gull, &amp; Lau, 1992 )) are not. While there has been some work on simulating realistic score distributions ( He, Peng, &amp; Ounis, 2009 ), such simulation would add another parameter to our experiments, and hence is considered outwith the scope of this paper. In-stead, as many of the voting techniques require score distributions to work, we choose to only use rank-based voting tech-niques in the following experiments instead.

In the following, we simulate rankings with various target D-MAP values. To ensure a good spread of possible document ranking performances, we experiment with 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 and 1.0 target D-MAP values for the sim-ulation algorithm. The TREC 2007 Enterprise track document search task and corresponding relevance assessments are used.
For each query, all documents from the relevance assessments pool for that query are considered during simulation. In par-ticular, as the simulation algorithm makes random swaps resulting in non-deterministic document rankings, we generate 10 document rankings for each query and D-MAP target values. Each document ranking is unique, as a different ordering of the relevant and irrelevant documents may have an impact on the effectiveness of the voting techniques that consider the order-ing of documents. As in Section 3.1 , we experiment with document ranking size 50 and document ranking size 1000. For each query and each ranking size, this makes a total of 300 document rankings for each query, with varying D-MAP retrieval performance.
 Fig. 3 compares D-MAP and E-MAP for simulated document rankings of size 1000, for the BordaFuse voting technique. Firstly, from the figure, it is noticeable that all of the document ranking performances are clustered around the target
D-AP 3 values of 0.2, 0.3, etc. However, as explained above, the simulation algorithm is not deterministic, and hence there are variances in the D-AP performances. Moreover, not all target D-AP values were achievable for all queries  X  in particular, some D-AP values are impossible to achieve, and hence we accept a ranking which has j target (D-AP) actual (D-AP) j &lt; 0.005, or if no more swaps are possible (e.g. there are not enough irrelevant documents to push the relevant documents far enough down).

Tables 4 and 5 present the correlations between various document search task measures for the simulated retrieval sys-tems and the accuracy of three rank-based voting techniques using them, when the R ( Q ) has size 1000 or 50, respectively.
Moreover, it also presents the maximum E-MAP and E-MRR values achieved for each voting technique by any document ranking. Firstly, on examining these tables, we note that the maximum achieved E-MAP and E-MRR values are not as high as those from the real TREC runs used in Tables 2 and 3 , even though the new experiments in Tables 4 and 5 contain systems with almost perfect D-MAP document rankings (this observation can also been made from Fig. 3 ).

These rather surprising results allow us to postulate that not all relevant on-topic documents may be good indicators of expertise evidence, and their exact ordering has an impact on the retrieval performance achievable by a voting technique.
Moreover, it is also possible that there exist some irrelevant documents which can be of benefit to an expert search voting technique, and for a ranking with perfect D-MAP, these documents have been suppressed.

Next, we examine the correlations in Tables 4 and 5 , and contrast with those in Tables 2 and 3 . We note that the corre-lations using the simulated document rankings are noticeably higher than those using the real document rankings. This sug-gests that by having a more even distribution of D-MAP across the full range, a higher correlation can be observed, strengthening the link between the overall topic relevance quality of a document IR system, and its likeliness to be useful as a component of an expert search engine.

Contrasting Tables 4 and 5 themselves, we note that for the BordaFuse and RR voting techniques, higher correlations are observed for the document ranking size of 1000. For ApprovalVotes, higher correlations are always observed for document ranking size 50. Overall, this suggests that, with simulated document rankings, using more evidence (longer document rank-ings) does not add too much noise for the BordaFuse and RR voting techniques, as these voting techniques do not place much emphasis on low-ranked documents. However, it is clear that these documents can bring some noise, as ApprovalVotes has higher correlations for ranking size 50.

The overall strength of the correlations exhibited suggest that the performance of the simulated document rankings clo-sely match the performance of the voting techniques. Recall that the irrelevant documents in the simulated rankings are ta-ken from the TREC pool, and hence are more likely to be vaguely related to the query than the irrelevant documents the IR systems may have retrieved deeper in the real rankings. Using random irrelevant documents would likely produce lower retrieval performances, and hence lower correlations.
 Examining the voting techniques, we note that BordaFuse and RecipRank exhibit higher correlations for D-MAP and
D-nDCG in both Tables 4 and 5 than they did using real document rankings ( Tables 2 and 3 ). However D-P@10 is still an important predictor  X  indeed, D-P@10 is more important for RR than BordaFuse, which is natural again given that RR focuses more on the top of the document ranking than BordaFuse. In both Tables 4 and 5 , the highest correlations are exhibited by ApprovalVotes with D-Recall, which is expected.

Overall, the experiments using the simulated document rankings strengthens the conclusion from Section 3.1 that expert search performance is closely related, but not absolutely correlated to document search performance. Moreover, in no case did a document ranking with perfect performance did not result in a perfect ranking of candidates  X  for instance, the max-imum E-MAP achieved by any document ranking for any voting technique was 0.2441 for document ranking size 1000, and 0.3164 for document ranking size 50 (see the Expert Measure Max columns of Tables 4 and 5 , respectively). This interesting observation (similar to that in Macdonald &amp; Ounis (2009a) ) infers that the ordering of relevant documents may be important for candidate retrieval performance, but also suggests that documents that are irrelevant, but which are somehow related to the topic area may also have a positive impact on the retrieval performance of the voting techniques, if highly ranked and associated to relevant candidates.

So how could such a perfect ranking of candidates be achieved? We hypothesise that the ordering of documents that would give the most effective candidate ranking should only comprise documents associated to relevant candidates. In the next section, we investigate a different way of measuring the performance of a document ranking. This time, instead of relating it to  X  X  X opical X  document relevance assessments, we induce document relevance assessments according to the rel-evant candidates. 4. From candidates to documents
In Section 3 , we used the relevance assessments of the document search task to compare and contrast the performance of document ranking to the performance of a corresponding ranking of candidates. In particular, we showed that various per-mutations of relevant and irrelevant documents in the rankings could not achieve a high-performing ranking of candidates.
In this section, we tackle the problem from the other direction: given knowledge of the relevant candidates, is it possible to generate a document ranking that will in turn give a perfect ranking of candidates?
Consider a perfect ranking of candidates: all relevant candidates are ranked ahead of irrelevant candidates. To produce such a ranking, we use the candidate profiles to infer the documents associated to relevant candidates that should be ranked above documents associated to irrelevant candidates. Hence, we can generate pseudo-document relevance assessments for a query, by considering any document associated to a relevant candidate and which contains at least one query term relevant. Any other documents that are associated to irrelevant candidates and not associated to any relevant candidates are considered irrelevant. These constraints permit a document ranking to be evaluated with a particular set of relevant candidates in mind. When evaluating a document ranking using this set of pseudo-relevance assessments, we use the notation ED-measure (ED is a mnemonic for  X  X  X xpert to document X ).
 Next, using the pseudo-document relevance assessments, we again apply the simulated AP algorithm of Turpin and Scholer, described in Section 3.2 , to simulate document rankings using the pseudo-document relevance assessments. The accuracy of the candidates identified by applying voting techniques to the simulated rankings is measured using the E-measures, as per Section 3 .

It is of note that there is not a one-to-one relationship between candidates and documents, because a document may mention more than one person X  X  name. However, in doing so, it will identify all associated candidates as having potential expertise evidence, whether they have relevant expertise or not. While the accumulation of evidence by the voting technique (all voting techniques except CombMAX can take more than one document into account as expertise evidence) may mitigate this problem to some extent, it is possible that a document associated to a relevant candidate may also lead to irrelevant candidates being retrieved. Hence, it is likely that the document rankings induced from the candidate relevance assessments may not produce a perfect of ranking candidates.

In the following, we experiment to determine with document rankings of varying ED-performance. We then evaluate the resulting candidate rankings, to see how connected ED-performance is with the ability of the voting techniques to correctly rank the relevant candidates. It is of note that this experiment requires an IR system to identify documents only containing at least one query term. In particular, we use Terrier ( Ounis et al., 2006 ) to index the CERC corpus with anchor text, applying
Porter X  X  stemmer and removing standard stopwords. 4.1. Experimental results
Tables 6 and 7 detail, for document ranking sizes 1000 and 50, the correlations between various ED-measures calculated on the document rankings, with E-MAP and E-MRR calculated on the expert search rankings. Again, the three rank-based voting techniques are used, and the highest E-MAP and E-MRR values are reported among all input document rankings.
Firstly, we note maximum E-MAP and E-MRR values in Table 7 that are higher than those found in any of the tables in Section 3 . This suggests that, for ranking size 50, some of the simulated document rankings have characteristics which lead to good expert search retrieval performance. Comparing Tables 6 and 7 , the best performance is again found at ranking size 50. We next examine the correlations in Tables 6 and 7 , and note that these are markedly higher than those from the tables in
Section 3 above. This suggests that the document relevance assessments induced from the candidate relevance assessments are more useful in predicting the expert search performance of the voting techniques than the topical document relevance assessments for the same topics.

For ranking size 1000 ( Table 6 ), all voting techniques are most highly correlated with ED-Recall, showing that the more evidence retrieved for each candidate, the more expert search performance is benefited. ED-MAP exhibits very similar cor-relations to ED-Recall, suggesting that the overall presence of relevant candidate related documents is more important than their exact ordering.

For ranking size 50 ( Table 7 ), ED-MAP is a good indicator of overall performance for E-MAP and E-MRR, suggesting that the document relevance assessments induced from the candidate relevance assessments do indeed model expert search per-formance. However, for the ApprovalVotes and RecipRank voting techniques, ED-MRR shows highest correlation to E-MRR.
This is somewhat surprising in the case of ApprovalVotes, where one would expect the overall quality of the ranking to be more important than the top-ranked documents. However this can be explained in that when a relevant candidate has asso-ciated documents at high ranks (as measured by ED-MRR), then the candidate is more likely to have sufficient votes overall to be correctly ranked.

The document rankings created in this experiment are both related to the query in that they contain at least one query term, and each relevant document is related to at least one relevant candidate. But how does the performance of these doc-ument rankings correlate with the performance measured using the topical document relevance assessments from the doc-ument search task? By correlating the D-MAP and ED-MAP measures of the 150 document rankings created with ranking size 50, and the 150 with ranking size 1000, we find q = 0.1261 and q = 0.0583, respectively. These correlations suggest that there does not need to be a connection between actual document ranking performance and the ability of the document ranking to rank candidate experts accurately. Note that this does not contradict the results in Section 3 , as having an accurate document ranking is shown to be beneficial in achieving a quality ranking of candidates. In contrast, the results here show that a document ranking that produces good expert search performance may actually have poorer performance when mea-sured using topical document relevance assessments. Indeed, when a retrieved document is topically irrelevant to the query but associated with a relevant candidate, performance will be enhanced. 4.2. Failure analysis
In the previous experiment, only documents which are associated to relevant candidates are retrieved in the document rankings. This infers that it should have been possible to create a perfect ranking of candidates. However, we note from Table 7 that the maximum E-MAP and E-MRR measures obtained are less than 1.0. In particular, the best E-MAP performance achieved is for the BordaFuse voting technique, which had document ranking size 50, and ED-MAP 1.0. The resulting
E-MAP was 0.7797. Given this less than perfect expert search performance, what factors are preventing a perfect ranking of candidates from being attained? In this section, we analyse this best performing candidate ranking, to determine which factors affected the final retrieval performance. In doing so, we hope to understand more about the expert search process and how failures occur across the two layers of an expert search system.

For the candidate ranking in question, 21 of the 50 queries have perfect expert search performance, i.e. E-AP 1.0. For the remaining queries with an E-AP retrieval performance of less than 1.0, we identified the following causal factors (some query failures were caused by more than one factor, or by the same factor multiple times  X  see Table 8 for the list of topics affected by each factor 5 ): 1. Documents mentioning relevant and irrelevant candidates (22 queries): A document may be an overview of many areas 2. Relevant candidate not found in corpus (13 queries): When building the candidate profiles, a relevant candidate did not 3. Missing judgement (8 queries): On manual examination, we found that an unjudged candidate might have been con-4. Higher management (6 queries): A candidate is a higher manager (e.g. division lead or CEO), and appears on pages with 5. Not a person (3 queries): A retrieved candidate, associated to a document which also was associated to a relevant can-
Overall, this failure analysis gives an insight into failures that an expert search engine can exhibit. This suggests ways that an expert search engine can be improved. For example, the voting techniques used in this work rely on the co-occurrence of query terms and candidate names. Hence, irrelevant candidates, such as science communicators in the case of the CERC cor-pus, can be retrieved in response to an expert search query. However, by the use of candidate-query term proximity infor-mation (such as Macdonald et al. (2008) ), and awareness of science communicators X  limited knowledge within the organisation hierarchy ( Karimzadehgan, White, &amp; Richardson, 2009 ), expert search retrieval performance would be improved.
 5. Conclusions
This work is the first large-scale empirical study into the influence of the document ranking in an expert search system. In our experiments, we showed that there is a correlation between the ability of the document ranking system to retrieve rel-evant documents with the ability of voting techniques to retrieve an accurate ranking of candidate experts. In particular, the exact document ranking preferences of the voting techniques were found to be varied, with some preferring high-precision, while most others accumulate evidence of expertise from the entire document ranking. However, no perfect correlations were exhibited, suggesting that some retrieval-enhancing techniques applied by the document search runs may not be suit-able for use by a particular expert search approach. Moreover, often an expert search evaluation measure did not correlate most with the corresponding document search measure (see Tables 2 and 3 ).

The document rankings employed in the first experiment are real TREC runs, and hence did not evenly cover the feasible values of each document ranking evaluation measure. For this reason, we also experimented with using simulated document rankings, targeting a range of MAP values (see Tables 4 and 5 ). Once again, while correlations were higher, the performance of the simulated document rankings did not show perfect correlation with expert search performance. This strengthens our observation that increasing the quality of the input document ranking of an expert search engine does not always result in an increase in the retrieval performance of the resulting ranking of candidates.

In the last experiment, we induced document relevance assessments from the candidate relevance assessments, and used those to again generate simulated document rankings (see Tables 6 and 7 ). Our experiments found these new document rankings to be more amenable to high expert search performance, and evaluation using the induced document relevance assessments were better than the topical document relevance assessments at predicting the resulting expert search perfor-mance. One of the underlying reasons behind this observation is that topically irrelevant documents can often bring useful expertise evidence, and hence should not be suppressed in the document ranking. However, despite a perfect ranking of doc-uments, built with the relevant candidates in mind, surprisingly, a perfect ranking of candidates could still not be generated.
We used a failure analysis to identify five factors for why perfect E-MAP could not be achieved, such as the candidate did not have sufficiently specific knowledge, or the expert relevance assessments were incomplete.

In the future, we wish to investigate the effect of the underlying score distributions of the document rankings on the inte-gration of the voting techniques  X  for instance, we have previously shown how score distributions should be fitted for opin-ion finding techniques ( He et al., 2009 ). Moreover, we wish to investigate if the training of the underlying IR system, or applying a learning to rank approach ( Liu, 2009 ) using appropriate features can be more successfully accomplished using the expert-induced pseudo-document relevance assessments than with conventional topical document relevance assess-ments, or if the relevance information sources at different levels can be combined in a dual-based learning approach, in con-trast to the discriminative modelling approach to dual learning at the document and expert levels taken by Fang, Si, and Mathur (2010) .
 References
