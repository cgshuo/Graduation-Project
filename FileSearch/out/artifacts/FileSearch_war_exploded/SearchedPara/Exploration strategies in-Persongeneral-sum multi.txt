 Department of Electrical Engineering, Amir Kabir University of Technology, Tehran, Iran 1. Introduction policies outperform the independent learning [7].
 point (NEP) [9]. Along with the assumption on unique NEPs, complexity of computing NEPs in games can be reviewed in survey papers [4].

In some real-life applications, simultaneous decision making need a number of cumbersome con-decision making. MinMax-Q was later partly modi fi ed in Asymmetric-Q [12]. In asymmetric-Q, the with perfect information have pure strategy Nash equilibrium points [14]. Most of the papers were mainly involved with ensuring that MRL algorithms eventually converge to and exploitation. Simulation results present the effectiveness of the proposed methods. reinforcement learning and game theory are reviewed in accordance with RL terminologies. MRL in given in Section 6. 2. Preliminary concepts the terminologies adopted from the established frameworks in RL.
 2.1. Single agent Q-learning Process (MDP) in RL. MDP is a mathematical framework for modeling action selection in situations where outcomes are partly random and partly under the control of an agent. De fi nition 1. A Markov Decision Process is a tuple ( S , A , R , P ), where:  X  S is the set of states,  X  A is the set of admissible actions,  X  R = { R | R : S  X  A  X  X  is the reward function, s : where for all s, s  X  S, a  X  A , The optimal state-value function gives the value of each state under the optimal policy: future reward starting in s ,taking a , and henceforth following  X  : The optimal action-value function is, through the following recursive equation [20], 2.2. Game theory
Game theory initially was used for reasoning in economics, which later has been widely utilized  X  X = { x 1 , x 2 , ... , x N } is the set of agents,  X  Q = { Q i | Q i : X   X  X  assigns the values of joint actions for each agent, referred to as game them volunteer to select another action.
 pro fi table for that agent [21], that is where  X   X  i is a strategy pro fi le of all agents except for agent i .
Computing NEP in normal form game is a complex problem. It is not known whether Nash equilibria is not pro fi table more than  X  for that agent [21], that is Obviously,  X  -NEP with  X  = 0isanNEP.
 games, subgame must be de fi ned. A subgame is any part of a game that can be analyzed as a game itself. 2  X   X   X  1 , the subgame  X   X  i  X  1  X  g i is the following extensive form game [21].  X  Agents, the agents in g . of g [21].

Markov games [23] are the generalization of both static games and (fully observable) MDPs. We introduced extensive Markov game (EMG), which can be regarded as an extension to Markov games in which each game state is in extensive form with perfect information [13]. De fi nition 7. Extensive Markov game is a tuple  X = G, X,  X  ,P,R ,where:  X  G is the set of extensive form games with perfect information,  X  X = { x 1 ,x 2 ,...,x N } is the set of agents with fi xed priorities in actions selections,  X  P : G  X   X   X   X ( G ) is the game transition function, where  X ( G ) is the set of probability  X  R = { R i | R i : G  X   X   X  X  is the reward function agent is not pro fi table for that agent in any game g  X  G ,thatis pro fi le of all agents except for agent i .
 to the set of Q-values of agents as Extended Q-values , 3. Multiagent reinforcement learning Agent i lead the game to the subgame  X  g k expected discounted reward over the set of games, where for all g, g  X  G,  X   X   X  , NEPs, is the expected discounted future reward starting in g ,taking  X  , and henceforth following  X  : Similarly, optimal action-value function is, Accordingly, Q-Learning is extended to EMGs by the following recursive rule [13]: where SPE V immediate rewards of the other agents, and updates its beliefs about them similar to its own,
The following algorithm presents the procedure in updating Q-values. 4. Action selection strategy not to act fully rational, whereas rational behavior is a central tenet in NEPs. compactly as follows, 1) Find the set of optimal actions A  X  2) Find the set of optimal actions A  X  3) Continue the same procedure until agent i is reached. 5) Select an action that yields the maximum value. x 1 induces that:  X  ...
  X  If x 1 selects a 11 , x 2 and x 3 will select a 22 a 32 ,and x 1 will gain 4,  X  If x 1 selects a 12 , x 2 and x 3 will select a 21 a 31 ,and x 1 will gain 5. is must be considered.
 respect to the possible set of agents X  actions according to their Q-values. better (Exploration).
 learning agent i after a subsequence of actions  X   X  i  X  1 =( a 1 ,...,a i  X  1 ) is given bellow, where a  X  (( action a i when exploring from the game state g is: strategy approaches greedy action selection.
 In Fig. 2, the set of associative Q-values for x 1 is: Depending on what action x 1 selects, a 11 or a 12 , the set of associative Q-values for x 2 is: or, x distribution in their subgames.
 of actions  X   X  i  X  1 , associative Q-values for action a i  X  A i can be calculated as follows, The procedure is brie fl y explained for the learning agent i, i =1 ,...,N  X  1 as follows, 1) Update extended Q-values based on Boltzmann distribution  X  Q B 2) Calculate the proba bilities according to  X  Q B 3) Calculate the associative Q-values, 4) Probabilistically select action with respect to associative Q-values. The last agent x N probabilistically selects its actions at terminal nodes based on Q B presented in Table 1.
 If x 1 selects a 11 and x 2 selects a 21 , the probability that agent x 3 selects a 31 is: Equivalently, for x 3  X  X  second action, x  X  X  fi rst action, associative Q-value is calculated as follows, bellow, where values of temperature parameter  X  .
 Consider the following table as the game preferences of game g in Fig. 2.
Based on traditional BI, J 2 is SPE. In early stages of learning,  X  is high and APBI tends more a about the Q-values, it will not cause a serious problem.
 x APBI tends for exploitation, the more it will change into traditional BI. 5. Simulation results We examined our proposed methods in 5  X  5 grid world games. They have been frequently used in LEFT, RIGHT, STAY } . 4Blocks of the environment are indexed from 1 to 25. The whole game-action the goal state as presented in Fig. 5.

On the other hand,computing NEP is still an open problem in normal form games [26]. Lemke-Howson n using the value of  X  . NEP can be considered as a minimization problem, where a ij  X  A i is j th action of agent i . The problem is, fi nd the answer. The main problem is the long running time.
 f to zero.
 the Q-values are zeros and agents are indifferent to movements. We randomly selects among them number of iterations to reach the goal.
 convenient to use extensive-Q in multiagent systems.
 such as political economy.

As in the previous simulation, the simulation is performed in a 5  X  5 grid world game. There are movement. Among the possible formations, straight line formation rewarded 3, and the other four 6. Conclusions and further discussions learning in this class of MAS.
 other agents.
 games as a minimization problem, which was solved by GA.

Numericalexamples are given to clarify the methods. Moreover, we performtwo computersimulations, normal form games.
 References
