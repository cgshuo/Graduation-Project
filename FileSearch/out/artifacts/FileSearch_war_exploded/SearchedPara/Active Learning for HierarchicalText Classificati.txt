 Hierarchical text classification plays an important role in many real-world ap-plications, such as webpage topic classification, product categorization and user feedback classification. Due to the rapid increase of published documents (e.g., articles, patents and product descripti ons) online, most of the websites (from Wikipedia and Yahoo! to the small enterprise websites) classify their documents into a predefined hierarchy (or taxonomy) for easy browsing. As more documents are published, more human efforts are needed to give the hierarchical labels of the new documents. It dramatically increases the maintenance cost for those organization or companies. To tackle this problem, machine learning techniques such as hierarchical text classification can be utilized to automatically categorize new documents into the predefined hierarchy.

Many approaches have been proposed to improve the performance of hierar-chical text classification. Different approaches have been proposed in terms of how to build the classifiers [2,19], how to construct the training sets [1,6] and how to choose the decision thresholds [15,1] and so on. As a hierarchy may con-tain hundreds or even tens of thousands of categories, those approaches often require a large number of labeled examples for training. However, in real-world applications, such as webpage topic classification, the labeled documents are very limited compared to the total number of unlabeled documents. Obtaining a large size of labeled documents for training requires great amount of human efforts. How can we build a reliable hierarchical classifier from a relatively small number of examples? Can we reduce the number of labeled examples significantly?
To tackle the lack of labeled examples , active learning can be a good choice [16,12,18]. The idea of active learning is that, instead of passively receiving the training examples, the learner actively selects the most  X  X nformative X  exam-ples for the current classifier and gets their labels from the oracle (i.e., human expert). Usually, those most informative examples can benefit the classification performance most. Several works have successfully applied active learning in text classification [16,5,20]. However, to our best knowledge, no previous works have been done in hierarchical text classification with active learning due to several technical challenges. For example, as a large taxonomy can contain thousands of categories, it is impossible to have one oracle to provide all labels. Thus, similar to DMOZ 1 , multiple oracles are needed. Wha t would be a realistic setting for multiple oracles for active learning in hierarchical text classification? How can we leverage the hierarchical relation to further improve active learning?
In this paper, we study how active learning can be effectively applied to hi-erarchical text classification so that the number of labeled examples (or oracle queries) needed can be reduced significantly. We propose a new setting of mul-tiple oracles, which is currently in use in many real-world applications (e.g., DMOZ). Based on this setting, we propose an effective framework for active learning in hierarchical text classification. Moreover, we explore how to uti-lize the hierarchical relation to further improve active learning. Accordingly, several leveraging strategies and heuri stics are devised. According to our ex-periments, active learning under our framework significantly outperforms the baseline learner, and the additional strategies further enhance the performance of active learning for hierarchical text classification. Compared to the best per-formance of the baseline hierarchical l earner, our best strategy can reduce the number of oracle queries by 74% to 90%. When active learning is applied to text classification, as far as we know, all previous works (e.g., [5,20]) explicitly or implicitly assume that given a document that might be associated with multiple labels, there always exist oracles who can perfectly answer all labels. In hierarchical text classification, it is very common that the target hierarchy has a large number of categories (e.g., DMOZ has over one million categories) across various domains, and thus it is unrealistic for one oracle (expert) to be  X  X mniscient X  in everything. For example, an expert in  X  X usiness X  may have less confidence about  X  X omputer X , and even less about  X  X rogramming X . If the expert in  X  X usiness X  has to label  X  X rogramming X , errors can occur. Such error introduces noise to the learner.

Therefore, it is more reasonable to assume that there are multiple oracles who are experts in different domains. Each oracle only gives the label(s) related to his or her own domains. Thus, the labels provided by multiple oracles will be more accurate and reliable than the labels given by only one oracle. Although previous works have studied active learning with multiple oracles [4,10], as far as we know, their settings are quite different from ours as their oracles provide labels for all examples for only one category, while in our case, different oracles provide labels for examples in different categories in the hierarchy.
Our setting of multiple oracles is actually implemented in DMOZ. As far as we know, DMOZ holds a large number of categories. Each category is gener-ally maintained by at least one human editor whose responsibility is to decide whether or not a submitted website belongs to that category. 2 We adopt the similar setting of DMOZ. In our setting, each category in the hierarchy has one oracle, who decides solely if the se lected document belongs to the current category or not (by answer  X  X es X  or  X  X o X ). In this paper, we mainly discuss pool-based active learning where a large pool of unlabeled examples is available for querying oracles. Figure 1 shows the basic idea of our hierarchical active learning framework. Simply speaking, at each iteration of active learning, classifiers on different categories independently and simultaneously select the most informative examples from the unlabeled pool for themselves, and ask the oracles on the corresponding categories for the labels. The major steps of our hierarchical active learning algorithm are as follows: 1. We first train a binary classifier ( C ) on each category to distinguish it from its 2. Then, we construct the local unlabeled pool ( D U ) for each classifier (see 3. For each query, the oracle returns  X  X es X  or  X  X o X  to indicate whether the 4. This process is executed simultaneously on all categories at each iteration There are two key steps (step two and th ree) in the algorithm. In step two, we introduce the local unlabeled pool to avoid selecting out-of-scope (we will define it later) examples. In step three, we tack le how to leverage the oracle answers in the hierarchy. We will discuss them in the following subsections. 3.1 Unlabeled Pool Building Policy From step one of our algorithm, we know that the training examples for a deep category (say c ) must belong to its ancestor categories. However, it is likely that many unlabeled examples do not belong to the ancestor categories of c .We define those examples as out-of-scope examples. If those out-of-scope examples are selected by c , we may waste a lot of queries. Thus, instead of using one shared unlabeled pool [5] for all categories, we construct a local unlabeled pool on each of the categories. To filter out these out-of-scope examples, we use the predictions of the ancestor classifiers to build the local unlabeled pool. Specifically, given an unlabeled example x and a category c , only if all the ancestor classifiers of c predict x as positive, then we will place x into the local unlabeled pool of c . 3.2 Leveraging Oracle Answers For the two answers ( X  X es X  or  X  X o X ) from oracles, there are several possible ways to handle them. We give a brief overview here and discuss the detailed strategies in Section 5.

If the answer is  X  X es X , we can simply update the training set by directly including the queried example as a positive example. To better leverage the hi-erarchical relation, we can even add the positive example to all the ancestor categories. Furthermore, since the positive example is possibly a negative exam-ple on some of the sibling categories, we may consider including it as a negative example to the sibling categories.

If the answer is  X  X o X , we can not simply add the example as a negative ex-ample, since we don X  X  know whether the queried example actually belongs to the ancestor categories. Thus, we could simply discard the example. Alternatively, we can also query the oracle on the parent category to see if the example belongs to the parent category, but the extra query may be wasted if the answer is  X  X o X .
In the following parts, we will first present our experimental configuration, and then empirically explore whether our framework can be effectively applied to hierarchical classification and whethe r different strategies described above can indeed improve active learning. 4.1 Datasets We utilize four real-world hierarchical text datasets (20 Newsgroups, OHSUMED, RCV1 and DMOZ) in our experiments. T hey are common benchmark datasets for evaluation of text classification methods. We give a brief introduction of the datasets. The statistic information of the four datasets is shown in Table 1.
The first dataset is 20 Newsgroups 4 . It is a collection of newsgroup docu-ments partitioned evenly across 20 different newsgroups. We group these cate-gories based on subject matter into a three-level topic hierarchy which has 27 categories. The second dataset is OHSUMED 5 . It is a clinically-oriented MED-LINE dataset with a hierarchy of twelve l evels. In our experiments, we only use the sub-hierarchy under subcategory  X  X eart diseases X  which is well-studied and usually taken as a benchmark dataset for text classification [8,13]. The third dataset is RCV1 [9]. It includes three classification tasks: topic, industrial and regional classification. In our experiments, we focus on the topic classification task. 6 The last dataset is DMOZ . It is a human-edited web directory with web-pages manually organized into a complex hierarchy. DMOZ is extracted from a sub collection rooted at  X  X cience X  and it has three-level category hierarchy. 7 4.2 Performance Measure To evaluate the performance in hierarchical classification, we adopt the hierar-chical F-measure, which has been widely used in hierarchical classification for evaluation [17,3,14]. The definition the hierarchical F-measure is as follows, where hP and hR are the hierarchical precision and the hierarchical recall,  X  P i is the set consisting of the most specific categories predicted for test example i and all its (their) ancestor categories and  X  T i is the set consisting of the true most specific categories of test example i and all its (their) ancestor categories.[14] 4.3 Active Learning Setup In our experiment, linear Support Vector Machine (SVM) is used as the base classifier on each category in the hierarchy, since the high dimensionality of text data usually results in the dataset being linearly separable [16]. Specifically, LIBLINEAR [7] package is used as the implementation of linear SVM. For LI-BLINEAR, there are primarily two parameters C and W that will affect the performance. C is the penalty coefficient f or training errors and W balances the penalty on the two classes. In our experiment, we set C = 1000 and W as the negative class proportion. For example, if the class ratio of positive and neg-ative class in the training set is 1:9, then W =0 . 9. The purpose is to give more penalty to the error on the minority class.

For active learning, due to the simplicity and effectiveness of Uncertainty Sam-pling 8 , we adopt uncertainty sampling as the strategy to select the informative examples from the unlabeled pool. It should be noted that our hierarchical active learning framework is independent of the specific active learning strategy. Other strategies, such as expected error reduction [12] and representative sampling [18] can also be used. We will study them in the future.

We split all the four datasets into labeled (1%), unlabeled (89%) and testing (10%) parts. As we already know the lab els of unlabeled examples, we will use the simulating oracles instead of the real human oracles (experts). We set a query limit (see Section 5.1). The training process is decomposed into a sequence of iterations. In each iteration, each category simultaneously selects a fixed number of examples 9 from its local unlabeled pool and q ueries the oracles (one query will be consumed when we ask one oracle for o ne label). After each category updates its training set, we recompute the parameter W and update the classification model. The entire training process terminates when the number of queries con-sumed exceeds the query limit. To reduc e the randomness impact of the dataset split, we repeat this active learning proces s for 10 times. All the results (curves) in the following experiments are averaged over the 10 independent runs and accompanied by error bars indicating the 95% confidence interval. In this section, we will first experimentally study the standard version of our active learning framework for hierarchical text classification, then propose several improved versions and compare them with the previous version. 5.1 Standard Hierarchical Active Learner In order to validate our active learning framework, we will first compare its stan-dard version (we call it standard hierarchical active learner) with the baseline learner. The standard hierarchical active learner uses intuitive strategies to han-dle oracle answers (see Section 3.2) in d eep categories. If the oracle answer is  X  X es X , the standard hierarchical active learner directly includes the example as a positive example; if  X  X o X , it simply discards the example. On the other hand, the baseline learner is actually the non-active version of the standard hierarchi-cal active learner. Instead of selecting the most informative examples, it selects unlabeled examples randomly on each category.
 Empirical Comparison: We set the query limit as 50  X | C | where | C | is the total umber of categories in the hierarchy. Thus, in our experiments the query limits for the four datasets are 1,350, 4,300, 4,800 and 4,850 respectively. We denote the standard hierarchical active learner as AC and the baseline learner as RD . Figure 2 plots the average learning curves for AC and RD on the four datasets. As we can see, on all the datasets AC performs significantly better than RD . This result is reasonable since th e unlabeled examples selected by AC are more informative than RD on all the categories in the hierarchy. From the curves, it is apparent that to achieve the best performance of RD , AC needs significantly fewer queries (approximately 43% to 82% queries can be saved) 10 .
Although the standard hierarchical active learner ( AC ) significantly reduces the number of oracle queries comp ared to the baseline learner ( RD ), we should note that there is no interaction between categories in the hierarchy (e.g., each category independently selects examples a nd queries oracle). Our question is: can we further improve the performance of the standard active learner by taking into account the hierarchical relation of different categories? We will explore several leveraging strategies in the following subsections. 5.2 Leveraging Positive Examples in Hierarchy As mentioned in Section 3.2, when the oracle on a category answers  X  X es X  for an example, we can directly include the example into the training set on that category as a positive example. Furtherm ore, according to the category relation in a hierarchy, if an example belongs to a category, it will definitely belong to all the ancestor categories. Thus, we can propagate the example (as a positive example) to all its ancestor categories. In such cases, the ancestor classifiers can obtain free positive examples for training without any query. It coincides with the goal of active learning: reducing the human labeling cost!
Based on the intuition, we propose a new strategy Propagate to propagate the examples to the ancestor classifiers wh en the answer from oracle is  X  X es X . The basic idea is as follows. In each iteration of the active learning process, after we query an oracle for each selected exampl e, if the answer from the oracle is  X  X es X , we propagate this example to the training sets of all the ancestor categories as positive. At the end of the iteration, each category combines all the propagated positive examples and the examples sel ected by itself to update its classifier. Empirical Comparison: We integrate Propagate to the standard hierarchical active learner (we name the integrated version as AC+ ) and then compare it with the original AC . The first row of Figure 3 shows the learning curves of AC+ and AC on the four datasets in terms of the hierarchical F-measure. Overall, the performance of AC+ is slightly better than that of AC . By propagating positive examples, the top-level classifiers of AC+ can receive a large number of positive examples and thus the (hierarchical) recall of AC+ increases faster than AC as shown in the second row. This is the reason why AC+ can defeat AC on the first three datasets. However, from the third row, we can see the hierarchical precision of AC+ actually degrades very sharply since the class distribution of the training set has been altered by the propagated positive examples. It thus weakens the boosting effect in the hierarchical recall and hinders the improvement of overall performance in the hierarchical F-measure.

Since positive examples can benefit the hierarchical recall, can we leverage negative examples to help maintain the hierarchical precision so as to further improve AC+ ? We will propose two possible solutions in the following. 5.3 Leveraging Negative Examples in Hierarchy We introduce two strategies to leverage negative examples. One is to query parent oracles when the oracle answers  X  X o X ; the other is to predict the negative labels for sibling categories when the oracle answers  X  X es X .
 Querying Negative Examples: For deep categories, when the oracle answers  X  X o X , we actually discard the selected example in AC+ (aswellasin AC ,see Section 5.1). However, in this case, the training set may miss a negative example and also possibly an informative example. Furthermore, if we keep throwing away those examples whenever oracle says  X  X o X , the classifiers may not have chance to learn negative examples. On the other hand, if we include this example, we may introduce noise to the training set, since the example may not belong to the parent category, thus an out-of-scope example (see Section 3.1).

How can we deal with the two cases? We introduce a complementary strategy called Query . In fact, the parent oracle can help us decide between the two cases. We only need to issue another quer y to the parent oracle on whether this example belongs to it. If the answer from the parent oracle is  X  X es X , we can safely include this example as a negative example to the current category. If the answer is  X  X o X , we can directly discard it. Here, we do not need to further query all the ancestor oracles, since the example is already out of scope of the current category and thus can not be included into its training set. There is a trade-off. As one more query is asked, we may obtain an informative negative example, but we may also waste a query. Therefore, it is non-trivial if this strategy works or not.
 Predicting Negative Labels: When the oracle on a category (say  X  X stron-omy X ) answers  X  X es X  for an example, it is very likely that this example may not belong to its sibling categories such as  X  X hemistry X  and  X  X ocial Science X . In this case, can we add this example as a negative example to its sibling categories? In those datasets where each example only belongs to one single category path, we can safely do so. It is because for the categories under the same parent, the example can only belong to at most one category. However, in most of the hi-erarchical datasets, the example belongs to multiple paths. In this case, it may be positive on some sibling categories. If we include this example as negative to the sibling categories, we may introduce noise.

To decide which sibling categories an example can be included as negative, we adopt a conservative heuristic strategy called Predict . Basically, when a positive example is included into a category, we add this example as negative to those sibling categories that the example is l east likely to belong to. Specifically, if we know a queried example x is positive on a category c ,wechoose m sibling categories with the minimum probabilities (estimated by Platts Calibration [11]). We set where D L is the labeled set,  X  c is the parent category of c , n is the number of children categories of  X  c ,  X   X  c ( x ) is the number of categories under  X  c that the example x belongs to.
 Empirical Comparison: We integrate the two strategies Query and Predict discussed above into AC+ and then compare the two integrated versions ( AC+Q and AC+P ) with the original AC+ . Since in AC+ positive examples are propa-gated, we can use this feature to further boost AC+Q and AC+P .For AC+Q , when the parent oracle answers  X  X es X , besides obtaining a negative example, we can also propagate this example as a positive example to all the ancestor cate-gories. For AC+P , as a positive example is propagated, we can actually apply Predict to all the ancestor categories.

We plot their learning curves for the hierarchical F-measure and the hierarchi-cal precision on the four datasets in Figure 4. As we can see in the figure, both AC+Q and AC+P achieve better performance of the hierarchical F-measure than AC+ . By introducing more negative examples, both methods maintain or even increase the hierarchical precisi on (see the bottom row of Figure 4). As we mentioned before, AC+Q may waste queries when the parent oracle answers  X  X o X . However, we discover that the average number of informative examples obtained per query for AC+Q is much larger than AC+ (at least 0.2 higher per query). It means that it is actually worthwhile to issue another query in AC+Q . Another question is whether AC+P introduces noise to the training sets. Ac-cording to our calculation, the noise rate is at most 5% on all the four datasets. Hence, it is reasonable that AC+Q and AC+P can further improve AC+ .

However, between AC+Q and AC+P , there is no consistent winner on all the four datasets. On 20 Newsgroup and DMOZ, AC+P achieves higher per-formance, while on OHSUMED and RCV1, AC+Q is more promising. We also try to make a simple combination of Query and Predict with AC+ (we call it AC+QP ), but the performance is not significantly better than AC+Q and AC+P . We will explore a smarter way to combine them in our future work.
Finally, we compare the improved versions AC+Q and AC+P with the non-active version RD . We find that AC+Q and AC+P can save approximately 74% to 90% of the total queries. The savings for the four datasets are 74.1%, 88.4%, 83.3% and 90% respectively (these numbers are derived from Figures 2 and 4). To summarize, we propose sev eral improved versions ( AC+ , AC+Q and AC+P ) in addition to the standard version ( AC ) of our hierarchical active learn-ing framework. According to our empiri cal studies, we discover that in terms of the hierarchical F-measure, AC+Q and AC+P are significantly better than AC+ , which in turn is slightly better than AC , which in turn outperforms RD significantly. In terms of query savings, our best versions AC+Q and AC+P need significantly fewer queri es than the baseline learner RD . We propose a new multi-oracle setting fo r active learning in hierarchical text classification as well as an effective activ e learning framework for this setting. We explore different solutions which attempt to utilize the hierarchical relation between categories to improve active learning. We also discover that propagating positive examples to the ancestor categor ies can improve the overall performance of hierarchical active learning. However , it also decreases the precision. To handle this problem, we propose two additional strategies to leverage negative examples in the hierarchy. Our empirical study shows both of them can further boost the performance. Our best strategy proposed can save a considerable number of queries (74% to 90%) compared to the baseline learner. In our future work, we will extend our hierarchical active learning algorithms with more advanced strategies to reduce queries further.

