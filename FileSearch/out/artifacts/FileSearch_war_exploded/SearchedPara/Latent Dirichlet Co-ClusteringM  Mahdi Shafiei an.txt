
We present a generative model for simultaneously clus-tering documents and terms. Our model is a four-level hierarchical Bayesian model, in which each document is modeled as a random mixture of document topics , where each topic is a distribution over some segments of the text. Each of these segments in the document can be modeled as a mixture of word topics where each topic is a distri-bution over words. We present efficient approximate in-ference techniques based on Markov Chain Monte Carlo method and a Moment-Matching algorithm for empirical Bayes parameter estimation. We report results in docu-ment modeling, document and term clustering, comparing to other topic models, Clustering and Co-Clustering algo-rithms including Latent Dirichlet Allocation (LDA), Model -based Overlapping Clustering (MOC), Model-based Over-lapping Co-Clustering (MOCC) and Information-Theoretic Co-Clustering (ITCC).
Finding the appropriate representation model for text data has been one of the main issues for the data min-ing community since it started to look at the problem of processing text automatically. The  X  X ag-of-words X  repre-sentation is the basic and most widely used representation method for textual data [19]. In this approach, the order of words at which they appear in documents are ignored and only the word frequencies are taken into account. But this approach has been criticized for several reasons. Among those, it provides a relatively high dimensional represen-tation of data (equal to the dictionary size) which causes curse of dimensionality problem [19]. Furthermore, it does not consider synonymy and polysemy relations of words in natural language. It has been also criticized of losing info r-mation due to its ignorance of word order. Various prepro-cessing steps such as removing stop-words and stemming have been used to reduce dimensionality, create and select better features.

To overcome the high dimensionality issue of the bag-of-words representation, several dimension reduction method s have been proposed. Feature selection methods select a sub-set of words to reduce the dimensionality. Feature transfor -mation methods try to tackle not only the high dimensional-ity problem of  X  X ag-of-words X  representation, but indirec tly consider synonymy and polysemy as well. Latent Semantic Indexing (LSI) [6] is one of these approaches which uses singular value decomposition to identify a linear subspace in the original space of features. It is believed that the res ult-ing new features also capture the two mentioned properties of natural language -polysemy and synonymy.

But the problem with most cartesian space representation approaches for text like LSI is their inability to provide in -terpretable components. Despite some work on interpreting the dimensions generated by these methods [5], these ap-proaches are still far from providing a natural interpretat ion in the case of text. Topic models, on the other hand, are a class of statistical models in which the semantic propertie s of words and documents are expressed in terms of proba-bilistic topics. Probabilistic topic modeling as a way of re p-resenting the content of words and documents has the dis-tinct advantage that each topic is individually interpreta ble, providing a probability distribution over words that picks out a coherent cluster of correlated terms. The major dif-ference between cartesian space methods like LSI and sta-tistical topic models is that LSI family methods claim that words and documents can be represented as points in the Euclidean space whereas for the topic models, this is not the case.

One common assumption among most statistical models for language is still the bag-of-words assumption . In these models, no assumption is made about the order of words. In other words, while this family of methods tries to deal with the two first issues of bag-of-words representation, high di -mensionality and ignoring polysemy and synonymy prop-erties, it still keeps the  X  X ag-of-words X  assumption intac t. Recently, there has been increased research interest in mod -els sensitive to this kind of information [11].

The basic idea behind all proposed topic models [10, 3] is that a document is a mixture of several topics where each topic is some distribution over words. Each topic model is a generative model which specifies a simple probabilistic pro -cess by which the words in a document are being generated on the basis of a small number of latent variables.
Using standard statistical techniques, one can invert the process and infer the set of latent variables responsible fo r generating a given set of documents [21]. Assuming a model for generating the data, the goal of fitting this gen-erative model is to find the best set of latent variables that can explain the observed data (i.e., observed words in doc-uments).

Probabilistic Latent Semantic Analysis (also known as the aspect model) [12] was one of the first attempts toward using probabilistic models for document and text modeling. In this model, each word is assumed to be a sample from a mixture model. Mixture components are multinomial ran-dom variables that can be viewed as representations of  X  X op-ics X . Each word is generated from a single topic and a doc-ument is a collection of words generated potentially from different topics.

Though a useful step after LSI, the PLSI model does not provide a generative model for a document, instead it is a model for word/document co-occurrences [13]. This assumption makes it difficult to assign probabilities to doc -uments outside of the learning corpus. Latent Dirichlet Al-location [3], on the contrary, is a true generative model for documents and therefore provides the means for generating both the observed and unseen documents. In LDA, the doc-uments are assumed to be sampled from a random mixture over latent topics, where each topic is characterized by a distribution over words. Furthermore, the mixture coeffi-cients are also assumed to be random and by considering a prior probability on them, LDA provides a complete gener-ative model for the documents [10].

In Latent Dirichlet Allocation, a document is gener-ated by first picking a distribution over latent topics from a Dirichlet distribution, which determines the multinomia l distribution over topics for words in that document. The words in the document are then generated by picking a topic for each word from this distribution and then picking a word from that topic according to another multinomial distribu-tion. Fig. 1.b shows the graphical model corresponding to the generative model of LDA.

The major and direct output of these models is a set of overlapping clusters of words. Clustering documents can be viewed only as a byproduct and not as a direct output of topic models. On the other hand, co-clustering [8, 20, 15] is a data mining technique with various applications such as text clustering and microarray analysis. Co-clustering algorithms try to simultaneously cluster rows and columns of a two-dimensional data matrix. One of the benefits of co-clustering algorithms is taking advantage of the dual-ity between documents and words and in general the du-ality between the rows and columns of an adjacency ma-trix. Co-clustering algorithms, using the clustering resu lts on words as a low dimensional representation of documents can achieve a more accurate clustering for documents [8]. In this work, we try to combine these two ideas, proba-bilistic topic models and co-clustering, using topic mod-els to construct a low-dimensional representation of docu-ments. Therefore, we extend the original idea of topic mod-els to consider the resulting low-dimensional representat ion of documents in another nested topic model for clustering documents.

The topics discovered by most probabilistic topic models capture the correlation between words, but the correlation s between topics are not modeled. Several models have been recently proposed to capture the correlation between topic s, such as Hierarchical Dirichlet Processes Model (HDP) [22], Correlated Topic Models (CTM) [2] and Pachinko Alloca-tion Model (PAM) [14]. In natural text data, it is common to have correlations among topics. As pointed out in [2],  X  X  document about sports is more likely to also be about health than international finance X . In the LDA model, the topic proportions are derived from a Dirichlet distributio n and hence are nearly independent. CTM tries to capture topic correlations by introducing logistic normal distrib u-tion instead of Dirichlet distribution for drawing topic mi x-ture proportions. The logistic normal distribution is yet again a distribution on the simplex where the correlation between pairs of components is described through a covari-ance matrix. In CTM, only the pairwise correlations are modeled and the number of parameters grows quadratically with the number of topics [14]. In the PAM model, similar to our proposed model, the concept of topic is extended to include not only distributions over words, but also distrib u-tion over topics. The model structure is an arbitrary DAG where each leaf is associated with a word and each non-leaf node is a distribution over its children. The direct parents of the leaf nodes are distributions over words and correspond to topics in LDA. All other interior nodes are distributions over topics and called  X  X uper-topics X . Allowing an arbitra ry DAG structure, the PAM model is able to capture arbitrary correlations between word topics.

In this paper, we propose a generative model for text doc-uments based on LDA model which is able to cluster both words and documents simultaneously. The model is also more sensitive to to the locations of words in documents by focusing on meaningful segments of text. This enables the model to detect multiple topics covered by a document.
The rest of this paper is organized as follows. In sec-tion 2, we present the Latent Dirichlet Co-Clustering Model (LDCC). We propose inference and parameter estimation algorithms for LDCC in Section 3. Finally, we conclude the paper with a brief review of the paper and some discussion on future works in section 5.
We use the same notation and definitions as in [3]. We define the following terms:  X  A word is the basic building block of our data and it  X  A document is a sequence of words w =  X  A corpus is a collection of M documents denoted by
The basic idea of the LDA model is to assume each doc-ument as a random mixture over latent topics, where each topic is specified by a distribution over words. We extend this idea by assuming each document is a random mixture of topics , where each topic is a distribution over some seg-ments of the document. Now, each of these segments in the document can be modeled by LDA.

The intuition behind this work is that documents are composed of meaningful single-topic segments put to-gether. Each of these segments is assumed to convey a sin-gle concept or topic. This topic is among a handful of top-ics which specifies the theme of the document. If one looks at each of these segments separately, the order of words in the segment is assumed to have little impact on the concept which the segment is trying to convey. Thus, the  X  X ag-of-words X  assumption for these segments is fairly realistic, u n-like for the whole document. In this work, we assume seg-ments are paragraphs of the text. The proposed model tries to model each segment based on its word content similar to most probabilistic topic models. Then these learned topics on the words are being used to represent document topics. In other words, each document topic is considered a mix-ture of word topics where the mixture coefficients uniquely specifies the document topic.

The generative probabilistic model we propose is shown as a graphical model in Fig. 1. Plate notation [4] is a stan-dard and convenient way of illustrating probabilistic gene r-ative models with repeated sampling steps. In this graphica l notation, shaded and unshaded variables indicate observed and latent (i.e., unobserved) variables respectively. Figure 1. LDA Model and Co-Clustering Model inspired by LDA
The LDCC model assumes the following generative pro-cess for each document d in a corpus D (intuitive explana-tions of model parameters are given in the text following the overview of the generative process.): 1. Choose S  X  P oisson (  X  ) : number of segments (para-2. Choose  X   X  Dir (  X  ) 3. For each of the S segments s
We have assumed that the number of word and document topics (and hence the dimensionality of topic variables and y ) are known and fixed. We also model the word proba-bilities conditioned on the topics by a L  X  V matrix  X  where  X  ij = p ( w j = 1 | z i = 1) will be estimated though the learning process. Finally, as in the LDA model, we can use any other document length distribution instead of the Poisson distribution as it is no t important for the rest of the model. Furthermore, note that the N erating variables (  X  , z ,  X  and y ) and we therefore ignore its randomness in the subsequent development.

Note that  X  represents the mixing proportion of document-topics in a document. It specifies the parameters of the K -dimensional multinomial distribution from which the model draws samples for document topics.  X  ple from the Dirichelt distribution and specifies the mixing proportion of word-topics in the text segment s . Note that this mixing proportion depends on the document-topic that the current text segment is generated from. The model as-sume that each document-topic is a mixture of several word-topics and this fact is modeled through the matrix of hyper-parameters  X  .

The Dirichlet distribution is a conjugate prior for the multinomial distribution. Choosing a conjugate prior make s the problem of statistical inference easier. Basically, th e posterior distribution would have the same functional form as prior distribution and the process of statistical infere nce, instead of being caught up in impractical integrations, be reduced to simply adjusting the parameters of the posterior distribution given the new evidence.

A k -dimensional Dirichlet random variable  X  can take values in the ( k  X  1) -simplex. The probability density of a k -dimensional distribution on this simplex is defined by:
The parameters of this distribution are represented by a k -vector  X  with components  X   X  a prior observation count of the number of times that the corresponding topic has been sampled.  X ( x ) is the Gamma function.

Given the parameters  X  ,  X  and  X  , the joint distribution of a word-topic mixture  X  , document-topic mixture  X  , a set of N word-topics z , a set of S document-topics y , and a set of N  X  S words w is given by: p (  X , y ,  X , z , w |  X ,  X ,  X  ) = p (  X  |  X  ) where p ( z and p ( y Note that variables z a single component equal to one and all other components zero. The component equal to one simply represents the word-topic or document-topic that the corresponding word or segment belongs to. Integrating over  X  and  X  and sum-ming over z and y , we obtain the marginal distribution of a document w : p ( w |  X ,  X ,  X  ) = Z p (  X  |  X  ) Taking the product of marginal probabilities of documents in a corpus gives us the probability of the corpus.
The inference problem is to compute the posterior distri-bution of hidden variables given the input variables  X ,  X ,  X  and observations w : which is intractable to compute in general. Given a docu-ment collection, we also need to estimate the model param-eters  X ,  X ,  X  so that the model likelihood for the collection gets maximized.

Exact inference on models in the LDA family cannot be performed practically. Three standard approximation methods have been used to carry out the inference and ob-tain practical results: variational methods [3], Gibbs sam -pling [10], and expectation propagation [16]. The EM based algorithms tend to face local maxima problems in this mod-els [3]. Therefore, we use algorithms in which some of the hidden parameters -in our case  X ,  X  and  X  -can be inte-grated out instead of explicitly being estimated. Note that we use conjugate priors in our model, and thus we can easily integrate out these parameters. This simplifies the samplin g since we do not need to sample  X ,  X  and  X  at all. Besides doing inference for document and word topic assignment variables y and z , we also need to learn the parameters of the Dirichlet distribution  X  = {  X  tion, we describe procedures for inference and parameter estimation and present a Gibbs sampling procedure for do-ing inference in the proposed model.

MCMC algorithms are a family of approximate itera-tive algorithms used to draw samples from a complex and usually high-dimensional distribution. Gibbs sampling is a member of this family and is applicable where the whole joint distribution is unknown or impractical to sample from , but the conditional distributions are known and sampling from them is not difficult.

In each turn of the algorithm, a subset of variables are sampled from their conditional distribution conditioned o n the current values of all other variables. This process is pe r-formed sequentially and continues until the sampled values approximate the target distribution. In our problem, the di s-tribution that we want to sample from is the posterior dis-tribution of word-topics and document-topics given the col -lection of documents. Since this distribution is intractab le and difficult to sample from, in each iteration of Gibbs sam-pling, we sample from the conditional distribution of a sin-gle word in a document given that the topic assignment for all other words and paragraphs in all documents except the current word are known. We also sample from the condi-tional distribution of a single paragraph given that the top ic assignments of all other words not in the current paragraph and topic assignments of all other paragraphs are known. For our proposed model, Gibbs sampling algorithm is easy to implement, requires little memory, and is competitive in speed and performance with existing algorithms.

We order the documents in the corpus and represent the collection of documents by three list of indices: word in-dices wl , paragraph indices pl and document indices dl wl words (if we assume the whole corpus as a sequence of words fed to the algorithm) and dl ment index and paragraph index of the corresponding word respectively. These lists will then be fed to the Gibbs Sam-pling algorithm. For each word token, the Gibbs sampling algorithm estimates the probability of assigning the curre nt word to word-topics given assignment of all other words to word-topics from the corresponding conditional distribut ion that we will derive shortly. Then the current word would be assigned to a word-topic and this assignment will be stored for being used when the Gibbs sampling algorithm works on other words.

While scanning the list of words, we keep track of the paragraphs. For each new paragraph, the Gibbs sampling algorithm estimates the probability of assigning this para -graph to document-topics given assignments of all other paragraphs to document-topics. These probabilities are computed from the corresponding conditional distribution for a paragraph given all other topic assignment to every other paragraph and all words not in this paragraph. Then the new paragraph would be assigned to a document-topic.
In our case we need to compute the conditional distribu-tion p ( z represents the word-topic assignment for word w n in document d and paragraph s ) and z word-topic assignments for all other words except the cur-rent word w ment for paragraph p the document-topic assignments for all paragraphs except the current paragraph p bility of a dataset, and using the chain rule, we can obtain the conditional probabilities conveniently. The derivati ons are provided in detail in Appendix A. For the LDCC Model, we obtain: Algorithm 1 : LDCC Gibbs Sampling Algorithm graph s of document d has been assigned to topic z  X  the word w p ( y ds | z, y  X  ds , w ) , we have p ( y ds | z, y  X  ds , w ) = (  X  y where y paragraph s in document d and P a paragraph in document d has been assigned to document-topic y document d .  X  for document-topic y has been assigned to.

The Gibbs sampling algorithm is initialized by assigning each word token to a random word-topic in [1 ..L ] and each paragraph to random document-topic [1 ..K ] . A number of initial samples have to be discarded (also known as burn-in samples) because they are poor estimates of the posterior. After this burn-in period, the next Gibbs samples start to approximate the target distribution (i.e., the posterior d istri-bution over word-topic and document-topic assignments). Now, we pick a number of Gibbs samples and save them as a representative set of samples from this distribution. Thi s should be done at regularly spaced intervals to prevent cor-relations between samples [9].
 In the LDA model as adopted by previous works, the Dirichlet parameters  X  are assumed to be given and fixed. This would give us reasonable results when we choose a uniform Dirichlet. But for our proposed model, the pa-rameters  X  capture relationships between document and word topics and must be learned from the data. In a sense, they somehow summarize the corresponding term-document matrix of the corpus. For estimating parameters of a Dirichlet distribution, one can use different approach es proposed in the literature [17]. These methods are based on maximum likelihood or maximum a posteriori estimation of parameters. There is no closed-form solution for these methods and one should use iterative methods to learn the parameters. In order to avoid these often computationally expensive methods, we use moment matching [17] to ap-proximate the parameters of the Dirichlet prior  X  . In each iteration of Gibbs sampling, we update where S document-topic k and N signed to document-topic k . n ( s ) times a word in paragraph s has been assigned to word-that for mean assigned to document-topic k . For each document-topic k we first compute sample mean mean var kl . They are computed over all paragraphs assigned to document-topic k . Algorithm 1 shows the pseudocode for the Gibbs sampling process for the proposed model. A sum-mary of symbols and their description is given in Table 1.
We use two real-world datasets in our experiments. We built our first dataset using NIPS conference papers avail-responds to a XML file contains nested tags for pages, columns, paragraphs, lines, and words. We removed all the words occurred in less than 5 documents from the list of final word tokens. We also used a list of standard  X  X top-words X  and deleted all numbers, words with length less than 3 and having non-ascii characters. For NIPS dataset, we keep certain two characters length words like  X  X M X  and  X  X L X . We do not consider paragraphs with less than 5 words and do not also include documents with less than 3 paragraphs. As a result, the NIPS dataset contains 1803 documents with the total of 1858577 word tokens. There are 11891 paragraphs in this dataset and word tokens are taken from 20485 unique words.

Our second dataset is a subset of Wikipedia XML cor-pus 2 [7]. This subset contains 1236 articles categorized in 9 overlapping classes. Each documents belongs to 1 . 47 classes on average. The biggest class corresponds to  X  X rt/Categories X  whit 510 documents. The smallest class,  X  X nited Kingdom/Categories X  has 74 documents. There are 774958 word tokens, 21453 paragraphs and 17406 unique words after preprocessing. The preprocessing phase is sim-ilar to the one for NIPS dataset. For the Wikipedia dataset, we do not have the tags for separating words, therefore we used all delimiting characters to separate words.
In this section, we describe the details of our experiments that demonstrate the improved performance of LDCC on NIPS dataset, compared to the LDA in terms of generaliza-tion of the topics found measured by perplexity. We also show the improved clustering performance of LDCC com-pared to the MOC and MOCC models.

In Gibbs sampling for both LDCC and LDA, we run 5 markov chains, discarding the first 500 iterations as burn-in iterations, and then draw 5 samples from each chain at a lag of 50 iterations, a total of 25 samples for each experiment. For the NIPS dataset, the total training time for LDCC is approximately 23 hours on a machine with a dual core In-tel Pentium IV 64 -bit ( EM 64 T ) processor ( 2  X  3 . 0 GHz processor) with 2 GB of RAM. In this section, we show 9 word-topics derived from NIPS dataset and 10 word-topics derived from Wikipedia dataset, each represented by their first 10 most probable words, presented in Fig.2 and Fig.3 respectively. As it can be seen, the model seems to be able to capture some of the underlying word-topics in both datasets.

We have also constructed a graph of latent word-topics and document-topics which explains the correlations found by the model amongst word-topics appearing in a cluster of documents represented by a document-topic. A part of this graph is shown in Figure 4. For each word-topic in the graph, we have a box where the word-topic is represented by its 6 most probable words. For each document-topic k , we rank the word-topics { l } according to Dirichlet parame-ters  X  topics, we have picked some of them and it is depicted in Figure 4. The idea is that we try to get word-topics as tight as possible representing a very specific word-topic. We can illustrate these word-topics by a set of their most probable words. Document-topics are distributions over these word-topics and thus theoretically can be represented by their most probable words. But each document-topic can be a mixture of seemingly unrelated word-topics and this makes representation of a document-topic with words less descrip -tive than with word-topics. Representing document-topics with a set of most probable word-topics would allow the user himself to figure out the associated concept. Addition-ally, this representation makes the visualization of word-topic correlations more intuitive.
We trained LDA and LDCC using the training set and we want to compare the generalization performance of these two models in terms of the likelihood achieved on the test set. Perplexity is a widely used and standard measure for comparing the performance of statistical models for natu-ral language [3]. Perplexity can be though of as the uncer-to several word-topics and capture their correlation. tainty in predicting a single word according to the model and lower values are better. Formally, for a test set of documents, it is defined as: In order to compute perplexity, we need to compute the like-lihood p ( w ) and this requires summing over all possible as-signments of words to word topics z and text segments (in our datasets, the text segments corresponds to paragraphs) to document topics y . This problem has no closed-form so-lution. Previous work on LDA [10] has used harmonic mean estimator introduced in [18]. We estimate p ( w ) by taking the harmonic mean of a set of values p ( w | z ) . By using the chain rule and integrating the parameter out, we get: p ( w | z ) = z is sampled from the posterior P ( z | w ) using the Gibbs sampling procedure described in section 3. The derivation of this is similar to the one described in Appendix A.
In these experiments, we use the NIPS dataset and split it into 80% for training and 20% for calculating the likeli-hood. We use 20 document topics and change the number of word topics from 20 to 100 . We split the dataset randomly so that the training and test subsets have relatively 80% 20% of the documents respectively and each topic in both subsets has at least 5 documents.

We present perplexity results for different number of word topics in Fig. 5. For all these experiments, the num-ber of document topics are assumed fixed and equal to 20 . As it can be observed, for different number of word-topics,
Figure 5. Perplexity results for NIPS dataset with different numbers of topics LDCC always produce lower perplexity compared to LDA. As it can be seen in Fig. 5, the perplexity for the LDCC method is increasing in the range of values that we have examined, as opposed to LDA model. This shows that the proposed method can not keep up its generalization perfor-mance as the number of word-topics increases, in contrast to the LDA model.

We also show the results comparing LDCC and LDA when we use different amount of training data for learning the model parameters. In these set of experiments, the num-ber of word-topics and document-topics are assumed fixed and equal to 20 and 100 respectively. We present these re-sults in Fig. 6. As it can be seen, the LDCC model has lower perplexity compared to LDA model as the amount of training data increase. The perplexity for LDCC model de-creases while we increase the amount of training data while for the LDA model, the perplexity has a peak when we use
Figure 6. Perplexity results for NIPS dataset with different amounts of training data 60% of the data for training.
We compare LDCC algorithm in terms of cluster-ing accuracy with another algorithm for overlapping clustering, namely Model-based Overlapping Clustering (MOC) [1] and two other co-clustering algorithms, namely Model-based Overlapping Co-Clustering (MOCC) [20] and Information-Theoretic Co-Clustering (ITCC) [8]. We con-ducted this experiment on our subset of Wikipedia XML corpus [7].

In order to compare clustering results, we use precision, recall, and F-measure calculated over pairs of points, as de -fined in [1]. For each pair of points that share at least one cluster in the overlapping clustering results, these measu res try to estimate whether the prediction of this pair as being in the same cluster was correct with respect to the under-lying true categories in the data. Precision is calculated a s the fraction of pairs correctly put in the same cluster, re-call is the fraction of actual pairs that were identified, and F-measure is the harmonic mean of precision and recall.
Table 2 presents the results of LDCC versus ITCC al-gorithm in terms of precision, recall and F-Measure for the Wikipedia Corpus. Each reported result is an average over ten trials. We have chosen the number of word-topics to be fixed and equal to 50 . Table 2 contains the results for two different values for the number of document-topics, 15 and 20 . Table 2 shows that the precision of LDCC is very close to the other three methods investigated but it can also be seen that our proposed algorithm shows a major improve-ment in terms of recall and F-Measure.
This paper has introduced a generative model for simul-taneously clustering documents and terms. Latent Dirichle t Table 2. Comparison of results of LDCC, MOC, MOCC and ITCC algorithms on Wikipedia Corpus in terms of Precision, Recall and F-Measure.
 Co-Clustering (LDCC) models each document as a random mixture of document topics , where each topic is a distribu-tion over some segments of the text. Each of these segments in the document can be modeled as a mixture of word top-ics where each topic is a distribution over words. Efficient approximate inference techniques based on Markov Chain Monte Carlo method and a Moment-Matching algorithm for empirical Bayes parameter estimation has been proposed. We have reported promising results on two text datasets, a subset of Wikipedia articles and NIPS conference papers. We compare the proposed model with the Latent Dirichlet Allocation (LDA) model in terms of its ability in document modeling and show improved performance in terms of per-plexity. We also compare our algorithm for document clus-tering with several other clustering and co-clustering alg o-rithms and demonstrate improved performance in terms of clustering quality.

We are grateful to the following institutions for their fi-nancial support: the Natural Sciences and Engineering Re-search Council of Canada, IT Interactive Services Inc., and GINIus Inc.

Beginning with the joint distribution p ( w , z , y ) , we can take advantage of conjugate priors to simplify the formulae . All symbols are defined in Section 3 and Table 1.
Using the chain rule, we have
Using chain rule, again we have
