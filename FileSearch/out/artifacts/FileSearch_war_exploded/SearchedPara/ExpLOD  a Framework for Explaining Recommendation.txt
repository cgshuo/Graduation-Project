 In this paper we present ExpLOD , a framework which ex-ploits the information available in the Linked Open Data (LOD) cloud to generate a natural language explanation of the suggestions produced by a recommendation algorithm. The methodology is based on building a graph in which the items liked by a user are connected to the items recom-mended through the properties available in the LOD cloud. Next, given this graph, we implemented some techniques to rank those properties and we used the most relevant ones to feed a module for generating explanations in natural lan-guage.

In the experimental evaluation we performed a user study with 308 subjects aiming to investigate to what extent our explanation framework can lead to more transparent, trust-ful and engaging recommendations. The preliminary results provided us with encouraging findings, since our algorithm performed better than both a non-personalized explanation baseline and a popularity-based one.
The performance of a recommender system is generally evaluated on the ground of its capability of predicting items the user would probably like. Accordingly, evaluation met-rics usually reward systems that maximize the predictive accuracy. However, especially when unknown items are pro-posed, the system should help the user to make an informed choice instead of just proposing a list of items based on an obscure reasoning mechanism [4].

Tintarev and Masthoff [8] point out that explaining a rec-ommendation is generally intended as justifying the sug-gestion, but it might be also intended as providing a de-tailed description that allows the user to understand the quality of the recommended item. Accordingly, they define seven possible aims for explanation, namely: transparency, scrutabilty, trust, effectiveness, persuasiveness, efficiency, satisfaction . The authors also demonstrate that a person-alized explanation strategy generally works better than a generic one. Typically, the explanations are grouped in three classes according to the information they exploit: prefer-ences of similar users (e.g. customers who bought this item also bought... ), similar items (e.g., this item is similar to other items you liked ), attributes of interest (e.g., this item has attributes you prefer ). However, there is not a general assessment on which approach works best, since a strategy can maximize an aspect rather than another.

In this work we investigated five of the aforementioned aims by implementing a personalized explanation strategy exploiting both similar items and attributes of interest through a graph-based recommendation model. Our explanation framework is based on the Linked Open Data cloud, and more specifically on the properties encoded in DBpedia 1 to link the items the user liked with the recommended ones. These properties are thus used for generating textual expla-nations. A similar attempt is due to [5], who demonstrated that a graph-based explanation results in a better user expe-rience, However, they focused on the usage of graphs for vi-sualization purposes, while our contribution exploits graphs for modeling the properties describing the items and identi-fying the most relevant ones to be used in a natural language explanation.

In the experimental session we evaluated the effectiveness of our framework against several baselines through a user study. Moreover, we also investigated the impact of the sin-gle features on the overall explanation. This research line has been already investigated in the past, and several au-thors [2, 4] showed a clear relationship between some fea-tures (as the favorite actor in a movie recommendation sce-nario) and the effectiveness of the explanation.

The rest of the article is organized as follows: in Section 2 describes the framework designed to generate personalized explanations. The description of the experiments carried out to evaluate the effectiveness of our strategy is given in Section 3. Finally, Section 4 concludes this work by dis-cussing the main findings and defining the future research directions.
The workflow carried out by ExpLOD is depicted in Fig-ure 1. The framework is organized in four main modules: Data Mapper , Graph Builder , a property ranker and a generator .

First, the Data mapper implements mechanisms to map each item with an element in the LOD cloud. As an example, http://wiki.dbpedia.org/ if a user is interested in the movies The Matrix and Cloud Atlas , the module will find the DBpedia nodes they refer to. In our case, the mapping is carried out by implementing a shallow matching of the name of the item with the title of the Wikipedia page the DBpedia node is linked to, but more complex mapping methods may be easily implemented. The output of this step is a list in which all the items in the user profile and all the items in her recommendation list are mapped with an URI (if any) in the LOD cloud. This step is mandatory since the explanations generated by the system will rely on a subset of the properties available in the LOD cloud.

Next, those LOD-aware item representations are used to feed the graph builder , which exploits the information avail-able in the LOD cloud to build a graph-based data model. Formally, let I p be the set of items in the user profile, let I be the set of recommendations and let exists ( s,o ) be a pred-icate returning true if an RDF triple having s as subject and o as object exists in the LOD cloud. Next, we define P = { p | i  X  I r  X  exists ( i,p ) = true } as the set of the values of the properties available in the LOD cloud describing the items the user received as recommendations. Given such a repre-sentation, the module builds a bipartite graph G = ( N,E ), where N = I p  X  I r  X  P and E = { ( i,p ) | i  X  I p  X  I r Basically, this graph connects the items the user liked and those in her recommendation list through the values of the properties describing those items in the LOD cloud. A vi-sual explanation of the data model is provided in Figure 2. Elements in I p , I r and P are reported in blue, red and green, respectively. In this case, the user liked Saving Pri-vate Ryan , The Matrix and The Da Vinci Code , and re-ceived as recommendation the movie Cloud Atlas . Accord-ingly, four properties connecting the movies in the profile to the recommendation are added as nodes in the graph 2 . We label them as candidate properties , since they represent the preliminary set of properties which could explain the recommendation received by the user.

Next, the property ranker analyzes such a data model to identify the subset of candidate properties which are likely to explain the recommendation. To this end, the module assigns a relevance score to each property, based on the in-sight that a good explanation should emphasize those prop-erties which can describe the current recommendation on the ground of the items the user liked. Given a candidate
The LOD cloud contains many more overlapping proper-ties. Due to space reasons we just reported a small subset of them.
 property c , the property ranker calculates its score as: where n c,I p is the number of edges connecting c with the items in the user profile, n c,I r is the number of edges con-necting c with the items in the recommendation set,  X  and  X  are two weighting factors and IDF c is an adaptation over DBpedia of the classical Inverse Document Frequency [7], which calculates how many items over all the dataset are described by that property. Formula (1) gives a higher score to those properties which are highly connected to the items in I p and I r and which are not so common as well. Through this calculation, all the candidate properties are provided with a relevance score and the top-K are used to generate the explanation. By following the example in Figure 2, Tom Hanks is the most relevant property explaining the recom-mendation since it occurs in two out of three movies in the user profile. Next, it is likely that the IDF component of the formula would help to rank as second property Dystopian Movies which is surely a less common one than The Wack-owski and American Epic Films . It is worth to note that our ranking formula is independent of the size of the recommen-dation list, so it can produce a single explanation also for a larger set of recommendations. When | I r | &gt; 1, the algo-rithm will rank first those properties which are common to many items in the profile and can also describe many items in the recommendation list.

Finally, once the top-K properties have been returned, the Generator module builds a natural-language explana-tion supporting the recommendation. The basic idea is to use the properties returned by the ranker to fill a template-based structure which generates the final explanation. As an example, the explanation generated for the data model pro-vided in Figure 2 is:  X  X  recommend you Cloud Atlas since you often like movies starred by Tom Hanks as Da Vinci Code and Saving Private Ryan . Moreover, I recommend it because you sometimes like Dystopian Movies as The Matrix and American Epic Films as Saving Private Ryan  X .
 Text in small caps refers to the elements coming from the LOD cloud. Adverbs as often or sometimes are dynami-cally defined by computing the normalized occurrences of that property in the data model and by mapping each ad-verb to a different range of the score. Moreover, expressions as starred by are obtained by mapping each property in the LOD cloud (in this case, dbpedia:owl:starring ) with a natural language expression. This is done for all the proper-ties available in the LOD cloud, as dbpedia:owl:director , dbpedia:owl:musicComposer and so on. Due to space reasons it is not possible to provide further details about this module. However, even this description showed how the combination of techniques for generating natural-language expressions with the richness of the information available in the LOD cloud let the algorithm produce very meaningful explanations supporting the recommendation set.
The goal of the experimental evaluation was twofold: to understand to what extent our explanation framework may lead to more transparent, trustful or engaging recommen-dations (Experiment 1) and to investigate whether a corre-lation exists between the choice of the properties used to generate the explanation and the effectiveness of the ex-planation itself (Experiment 2). To this end, we designed a user study in the movie domain by involving 308 sub-jects (male=70%, degree or PhD=62%, medium interest in movies). We evaluated the following explanation aims: transparency , persuasiveness , engagement , trust and effec-tiveness , as in [8].

Experimental Protocol. To run the experiment, we deployed a web application 3 implementing the previously described framework. The platform was designed to run a between-subject experiment, i.e. we tested four different explanation styles and each user was randomly assigned to one of them. As explanations styles we defined two versions of the previously presented natural language-based expla-nations (by ranking the properties with and without IDF) and two baselines, a popularity-based explanation and a non-personalized explanation style based on the presentation of the movie properties extracted from DBpedia . Those base-lines were already adopted in literature [8] for similar exper-imental protocols. In a nutshell, each user involved in the experiment carried out the following steps: (1) Collection of Demographic Data and Prefer-ence Elicitation . We asked the users to provide some basic demographic data. Then, each user provided her preferences in the movie domain: a small portion of the users (around 5%) authorized our application to extract preferences from their Facebook profiles, while the other explicitly rated a randomly generated subset of 20 movies extracted from the top-100 popular movies in IMDB. Once the profiles were built, recommendations were generated by running Person-alized PageRank [3] as recommendation algorithm. (2) Generation of the Explanations . We used the preferences of the users and the Top-1 recommendation gen-erated by the algorithm to feed our framework. When the popularity-based baseline was picked as explanation style, each user was provided with a simple explanation as  X  X e suggest this item since it is very popular among people who like the same movies as you. X  . On the other side, as non-personalized explanation style we exploited the properties encoded in DBpedia to build a structured description of the movie used as explanation (e.g. starring, director, mu-sic composer, subject, and so on) without any filtering or ranking of the properties. Finally, when ExpLOD was ran-domly picked, we run our framework as previously shown. In this setting, the Data Mapper was able to map to DB-pedia 90,318 movies gathered from IMDB. The mapping is available online 4 . Next, Graph Builder queried DBpedia to extract the properties describing the movies. It is worth to http://193.204.187.192:8080/WebLodrecsys/Benvenuto.jsp http://www.di.uniba.it/ swap/explod/mapping.txt note that all the available properties were extracted, with-out performing any filtering. Finally, the ranking formula was set by choosing 0 . 5 as weighting factor for  X  and  X  and the top-3 properties were returned. All these values were roughly set through some simple heuristics. In the future we will also investigate the impact of a proper tuning of the parameters on the overall results. (3) Evaluation through Questionnaires . Finally we asked the users to fill a questionnaire to evaluate the quality of the different explanation styles. Each user was asked to evaluate the previously presented explanation aims through a five-point rating scale and to evaluate how much she liked that suggestion. The questions the users had to answer are presented in Table 1. Finally, in the last part of the experi-ment, each user had to enjoy a trailer of the movie and had to evaluate again the movie after watching the trailer. The whole experiment took less than 3 minutes. A screen record-ing showing a demo of the experiment is available online 5
Evaluation Metrics. We evaluated transparency , per-suasiveness , engagement and trust of the recommendation as the average score collected through the user question-naires, while the effectiveness was calculated as the normal-ized difference between the pre-and post-trailer ratings the user provided for the recommendation. This is an evalua-tion protocol which is very common in literature [1], whose insight is that an effective explanation may help the user to evaluate to what extent she would like the recommenda-tion, even before enjoying it. For each explanation style 77 observations were collected. Given that in [6] the minimum acceptable sample size for each experimental condition was set as 73, we can state that our experiment guaranteed the significance of the results.
Results of Experiment 1 are reported in Table 2. Re-sults show that our framework is able to significantly out-perform both the baselines (statistical significance has been assessed through a Mann-Whitney U Test ) for most of the explanation aims we evaluated ( p &lt; 0 . 001). Specifically, configurations based on ExpLOD are able to provide users with more transparent , persuasive and trustful recommen-dations. Our framework also obtained the best results in terms of engagement , but a statistically significant improve-ment was noted only over the simple popularity-based base-line. The only metrics that did not benefit of the adoption of the explanation generated by our framework is the effec-tiveness , since in this case the best results are obtained by http://bit.ly/1WEPhc4 the non-personalized explanation based on the structured presentation of LOD properties. This result may be due to the fact that also a simple summary reporting the most relevant properties of the movies may be helpful to under-stand whether the user will like it or not. However, we did not notice any statistical significance, so we can state that our framework can be as effective as the non-personalized baseline on this task. On the other side, when more com-plex aspects as the trust or the persuasion are taken into account, our strategy is the only one which can significantly outperform the baseline. Finally, as regards the use of the IDF, experiments did not show any significant difference so it is possible to state that its use does not influence the overall performance of the explanation.
 Table 2: Results of Experiment 1. The best-performing con-figuration is in bold. Significant improvements over the base-lines are in italics and bold. popularity 3.01 2.59 2.31 2.67 0.93 LOD properties 3.04 2.84 3.28 2.81 0.66 ExpLOD -noIDF 4.00 3.39 3.48 3.39 0.72 ExpLOD 4.18 3.41 3.31 3.36 0.75
In Experiment 2, we investigated whether some relation-ship between the choice of particular properties and the over-all performance of our explanation framework exists. In-deed, it is likely that in some scenarios it would be useful to generate explanations able to maximize a specific explana-tion aim, thus it is worth to know whether to use (or to ig-nore) a specific property may optimize a specific aspect. As an example, in a movie recommendation scenario it would be good to persuade the user watching that movie. To this end, we split the data we collected during the first experiment on the ground of the properties selected by ExpLOD . Next, we calculated again the average transparency, persuasion, en-gagement, trust and effectiveness by only considering those explanations which contained a specific property, as the di-rector of the movie, the producer , the music composer , and so on.
 Table 3: Results of Experiment 2: average improvement (or average decrease) over the results presented in Table 1 are reported in parenthesis. persuasion won (+15%) location (-8%) director (+12%) producer (-9%) engagement writer (+25%) producer (-13%) director (+20%) distributor (-3%) trust won (+21%) producer (-4%) composer (+5%) topic (-9%)
Due to space reasons, we reported in Table 3 only the most interesting findings emerging from this experiment. The re-sults provided several insights, since they showed that some interesting correlations which can drive the choice of the most promising properties actually exist. Specifically, data showed that the usage of particular properties as the direc-tor of the movie or its writer positively affects most of the explanations aims. On the other side, properties as the pro-ducer or the distributor do not improve the engagement nor the persuasion of the explanation. Moreover, it is very inter-esting that both the persuasion and the engagement of the explanation are improved when the information about which prizes the movie has won was reported in the explanation. Finally, it is also worth to note that including information about the general topic of the movie (encoded in the DBpedia properties dcterms:subject ) may lead to more transpar-ent explanations, but it negatively affects the trust. This is a very interesting insight, since it shows that a different choice of the properties may lead to the maximization of a different explanation aim , thus our framework may be prop-erly tuned in order to use (or to avoid) a specific property according to the goals of the explanation.

To sum up, in this paper we presented a framework for generating natural language explanations based on the in-formation available in the LOD cloud. Preliminary results showed the goodness of the framework, since our strategy significantly outperformed all the baselines we took into ac-count. Moreover, we also showed that the choice of spe-cific properties may influence the behavior of the framework, leading to explanations able to maximize some specific as-pects. As future work, we will evaluate the effectiveness of the framework in different domains, as music and books, but we will also investigate the impact of different data points gathered from the LOD cloud. Indeed, we plan to extract also properties not directly connected to the items to be rec-ommended, in order to build explanations containing more interesting and unexpected patterns connecting the items the user liked to those she got as recommendation, hopefully leading towards more engaging and effective explanations. [1] M. Bilgic and R. Mooney. Explaining [2] G. Carenini and J. Moore. An empirical study of the [3] T. H. Haveliwala. Topic-Sensitive PageRank: A [4] J. L. Herlocker, J. A. Konstan, and J. Riedl. Explaining [5] B. Knijnenburg, S. Bostandjiev, J. O X  X onovan, and [6] B. Knijnenburg and M. Willemsen. Evaluating [7] C. Manning, P. Raghavan, and H. Sch  X  utze. Scoring, [8] N. Tintarev and J. Masthoff. Evaluating the
