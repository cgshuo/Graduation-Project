
Many real-world applications, such as image segmenta-tion, social network analysis and data clustering can be abstracted into a graph partition problem: finding the nor-malized min-cut (Ncut) of a given graph. Although the Ncut problem is generally intractable, it is well-known that its relaxed form can be solved by spectral clustering. The seminal work by Shi and Malik [1] represents the first incarnation of spectral clustering which was passive and unsupervised. However, in many application domains, considerable domain expertise exists and encoding domain knowledge into clustering algorithms is important if the results are to be novel and actionable. To address this issue, user supervision in the form of pairwise relations between two nodes of the graph have been proposed [2]: Must-Link (they belong to the same side of the cut) and Cannot-Link (they belong to different sides of the cut) constraints. Constrained clustering, including constrained spectral clustering, has been extensively studied [2] and much previous work [3] X  X 5] has shown that ML and CL constraints, when used properly , can greatly improve the quality of the resultant clustering. Those work represents the second incarnation of spectral clustering: passive and semi-supervised.

In this work we make the natural and important progres-sion to the third incarnation of spectral clustering: active and semi-supervised. In this formulation the constraints are provided incrementally after querying an oracle rather than a priori in a batch before clustering begins. Active and semi-supervised spectral clustering has many natural problem settings that will see its wide-scale use:
Therefore, it is a natural and important question to ask: instead of passively taking a given set of constraints, which may consist of both helpful and harmful constraints, is it possible for the algorithm to actively query/fetch only the constraints that are expected to be helpful? With the availability of an oracle, we can put constrained spectral clustering into the context of active learning [6]. Our goal is to maximally improve the quality of the resultant clustering (maximizing performance gain) while making as few queries as possible (minimizing cost). We propose an active learning framework for constrained spectral clustering, or active spectral clustering for short. Our framework consists of two key components: We apply these two components alternatively and the resul-tant clustering will be refined over iterations and eventually converge to the groundtruth result.

Our contributions are: 1) This is the first principled framework for active spec-2) We propose a ready-to-use active spectral clustering 3) We address some important implementation issues for 4) We empirically show that our method significantly
The rest of the paper is organized as follows: related work is discussed in Section II; we propose our active spectral clustering framework in Section III; it is evaluated empiri-cally in Section IV; implementation issues are discussed in Section V and future directions are discussed in Section VI; we conclude our work in Section VII.

Active clustering is a special sub-category of active learn-ing algorithms [6]. The difference is that active clustering algorithms query pairwise relations between two nodes instead of the labels of individual nodes. Most existing active clustering methods [3] X  X 5], [9] X  X 11] were built upon hard clustering schemes such as K -means clustering and hierarchical clustering. Little attention has been paid to the active learning framework for spectral clustering , which is the most popular soft clustering scheme and a solution to many real-world applications.

Xu et al. [12] proposed an active spectral clustering method that examines the eigenvectors of the graph Lapla-cian to identify boundary points and sparse points, and then queries the oracle for constraints among these ambiguous points. Their work is limited mainly because they explicitly assume that the underlying clusters in the data set are nearly separated and it is the boundary points that cause the inaccuracy in the cluster assignment; it is unclear if the propose method would still work otherwise. Also, since the proposed method is built upon the KKM constrained spectral clustering method [13], it can only incorporate hard constraints, not soft ones.

Active spectral clustering is also related to the area of matrix perturbation analysis for spectral clustering [14], [15], which studies how much the resultant clustering will change when a perturbation is applied to the original graph Laplacian. The results from perturbation analysis can give us an idea of how stable the clustering is and how many constraints are needed for the clustering to change significantly. However, the bounds in matrix perturbation theory are typically of the form involving the norm of the matrix and hence do not give directly suggestion on what constraints we should query .

In this section we present our active spectral cluster-ing framework. We provide the background knowledge in Section III-A and an overview of our framework in Sec-tion III-B. We begin in Section III-C by introducing the first important component of our framework, a constrained spectral clustering algorithm that can handle both hard and soft constraints. Then in Section III-D we describe the second important component of our framework, an active query strategy based on maximum expected error reduction. Important notations used throughout the rest of the paper are listed in Table I. Note that the superscript  X * X  when attached to a symbol refers to the groundtruth answer typically only available to the oracle.
 A. Background and Preliminaries
We have a graph G with N nodes. A is the associated affinity matrix. A is symmetric and nonnegative. D is the degree matrix of G where L = D  X  A is the graph Laplacian of G .

It is well-known result [1] that spectral clustering finds the normalized minimum cut of G in its relaxed form: where vol ( G ) = cator vector. The optimal solution to Eq.(1) is the eigenvector associated with the second smallest eigenvalue of L . The actual 2 -way cut is given by assigning nodes corresponding to the positive entries in u to one side and negative nodes to the other.

In practice, the graph Laplacian L is often generated with noise and/or from a biased sample of the underlying data dis-tribution. As a result, the clustering found by (unconstrained) spectral clustering, u , will differ from the groundtruth cluster assignment, u . In order to find a u that better approximates the groundtruth result, a popular approach is to incorporate constraint information into spectral clustering. Formally, let A be a constrained spectral clustering algorithm: Now the resultant clustering u is decided by both the graph Laplacian L and a constraint matrix Q . In its generalized form, Q  X  R N N is a symmetric matrix that encodes pairwise relations between the nodes of G as follows: For hard constraints, we have Q ij = 1 for Must-Link s and Q ij =  X  1 for Cannot-Link s. For soft constraints, the magnitude of Q ij indicates how confident we are about that constraint. Previous work on constrained spectral clustering assumes the constraint matrix Q is provided a priori , i.e. the selection of known entries in Q is independent from the algorithm A and we have no control over the selection process.
 B. An Overview of Our Framework
In this work, we make the assumption that there is an oracle who has access to the groundtruth constraint matrix Q = u u T , where u  X  R N is the groundtruth cluster assignment. We assume that we can actively query an oracle about the value of any entry in Q , one entry at a time. Q is the matrix that contains all the constraints we have queried so far ( 0 for unknown entries). Note that the nonzero entries in Q is always a proper subset of the nonzero entries in Q . Our goal is to minimize the difference between u = A ( L; Q ) and u using as few queries as possible.
We propose an iterative process to incrementally query the oracle about the constraint that can maximally reduce the expected error in our current result. We start with an empty constraint matrix Q (0) with all 0 entries, then we compute the current clustering using a constrained spectral clustering algorithm A : Note that u (0) should be the same as the clustering found by the unconstrained spectral clustering algorithm as we have no constraint so far. Then assume at iteration t we already have A query strategy Q will evaluate the current clustering u and the current constraints Q ( t ) to decide what is the next entry in Q we should query from the oracle. Let the value of Q ij (since the constraint matrix is symmetric). Then we update the clustering by We repeat this iteration until certain stopping criteria is met.
Our framework has two key components: the constrained clustering algorithm A and the query strategy Q . Next we will discuss their realization in detail, respectively. C. The Constrained Spectral Clustering Algorithm
The first key component of our framework is a constrained spectral clustering algorithm A . A takes the graph Lapla-cian L and a constraint matrix Q as input and outputs a (relaxed) cluster indicator vector u . In general, our frame-work has no restriction on the realization of A as long as it satisfies the following property:
Property 1 (Convergence): As the constraint matrix Q approaches Q , the output of the constrained clustering algo-rithm, u , will converge to the groundtruth cluster assignment u : This property is to ensure that our active learning framework will converge to the groundtruth cluster assignment as more constraints are revealed by the oracle. As trivial as this property may seem like, it is not automatically guaranteed by all constrained clustering algorithms. For example, some constrained K -means clustering algorithms are sensitive to the order in which the constraints are enforced and thus may not converge to the groundtruth clustering after all.
There are a number of possible candidates for A [13], [16], [17] in the literature. In this work we implement the framework using the constrained spectral clustering algorithm we proposed in a recent work [18]. The main advantage of this approach is that it not only satisfies the convergence property but also is flexible enough to incorporate both hard and soft constraints. Its objective function is as follows: By comparing Eq.(2) to Eq.(1), we can see that a new term u
T Q u  X  is added to the original formulation of spectral clustering. Recall Q is the constraint matrix and u is the cluster assignment vector, then can be considered as a measure (in relaxed form) of how well the pairwise relations as implied by cluster assignment u conform to those as demanded by the constraint matrix Q : the larger the value is, the more consistent they are. Hence the term u T Q u  X  essentially lower bounds how well the constraints in Q are satisfied (in its relaxed form) by the cluster assignment u .
 Eq.(2) is intractable in general. However, we can use the Karush-Kuhn-Tucker Theorem [19] to find a sub-optimal solution (please see the original paper [18] for detailed derivation). Intuitively speaking, recall that the solution to the unconstrained spectral clustering problem (Eq.(1)) is provided by the eigenvalue problem: Similarly, the solution to our constrained spectral clustering problem (Eq.(2)) is provided by the following generalized eigenvalue problem where I is an N  X  N identity matrix and &lt; max , max to be the largest eigenvalue of Q . As we have shown in [18], all generalized eigenvectors of Eq.(3) associated with positive eigenvalues 1 will satisfy the constraint u T Q u and among those the one that minimizes u T L u would be a sub-optimal solution to Eq.(2). Note that is a lower-bound on how well the constraints in Q are satisfied: larger implies the resultant clustering conforms more strictly to the given constraints. When is sufficiently large, there will only be one generalized eigenvector associated with nonnegative eigenvalue and the eigenvector will be used as the solution to Eq.(2).
 D. The Query Strategy
The second key component of our framework is the query strategy Q . Our strategy evaluates the current cluster assignment u and the constraints in Q and decides what is the best entry of Q to query next. The principle it uses is maximum expected error reduction, which means that for all unknown pairwise relations between two nodes, we compute the expected error between our current estimation of that value and its groundtruth value, and we pick the pair of nodes with largest expected error and query the oracle for their relation.

Formally, let P ( t ) ij be our estimation of the pairwise relation between node i and j at time t . A straightforward way to compute P ( t ) ij from the cluster assignment vector u is: Let d  X  R  X  R 7 X  R be a distance function that measures the error between our current estimation and the groundtruth value: Since Q ij remains unknown until after we actually query it, we cannot compute the error d ( P ( t ) ij ; Q ij ) directly. Instead, we compute the mathematical expectation of the error over the two possible answers from the oracle: Now the question becomes how we can estimate Pr ( Q ij = 1) and Pr ( Q ij =  X  1) based on the information we already have. Recall that we assumed Q = u u T , thus Q is a rank-one matrix. If we treat the current constraint matrix Q ( t ) as an approximation to Q with missing values, then it is a standard approach to use the rank-one approxi-mation of Q ( t ) to recover the unknown entries in Q [20]. Let u ( t ) be the largest singular vector of Q ( t ) , then is the optimal rank-one approximation to Q ( t ) in terms of Frobenius norm [21]. Then we can compute Pr ( Q ij = 1) and Pr ( Q ij =  X  1) as follows:
Finally, we query the entry that has the maximum ex-pected error:
To show the effectiveness of our approach, we evaluated its performance on several UCI benchmark data sets [22]. Our goal is to show that our framework can achieve better performance with a smaller number of actively selected con-straints, as compared to a randomly selected constraint set. This effectively tests our active selection approach against the batch approach using the same number of constraints.
We compared our method ( active ) to a baseline method ( random ). Both methods used the exact same implementa-tion of the constrained spectral clustering algorithm. The only difference was that the constraints used by active were actively selected using our query strategy, whereas the constraints used by random were randomly selected. We used both hard and soft constraints in our experiments. For hard constraints, we chose five different data sets with groundtruth labels, namely Hepatitis, Iris, Wine, Glass, and Ionosphere. We performed 2 -way partition on all data sets. We removed the SETOSA class from the Iris data set, which is the class that is known to be well-separately from the other two. For the same reason we removed Class 1 from the Wine data set. We also removed data instances with missing values. The statistics of the data sets after preprocessing are listed in Table II. For each data set, we computed the affinity matrix A using the RBF kernel (the edge weight between two nodes is the similarity between those two data instances).

For soft constraints, we chose a subset of the 20 News-group data, as shown in Table III. We randomly sampled about 350 documents from 6 groups. At the highest level, those groups can be divided into two topics: computer (comp) and recreation (rec). To generate soft constraints, if two articles are from different topics, we set the correspond-ing entry in Q to  X  1 ; if two articles are from the same topic but different groups, we set the corresponding entry to 0 : 5 ; if they are from the same group, we set the entry to 1 . The affinity matrix A was generated from the similarity matrix based on inner-product (the edge weight between two nodes is the number of words those two articles have in common).
On all data sets, we started with no constraint and queried one constraint at a time from the oracle. Note that we did not use the transitive or entailment properties [5] to deduce more constraints based on the existing ones, which is impossible to do when the constraints are soft. To evaluate the accuracy of the resultant clustering at each time step, we used Rand index [23]. For each data set, we made up to 2 N queries, where N was the size of the data set. There is only one parameter in our method, which is for the constrained spectral clustering algorithm (see Eq.(3)). Throughout all experiments, we simply set it to max = 2 , where max is the largest eigenvalue of Q . In this way, we guarantee the existence of at least one feasible solution, while requiring that a reasonable amount constraints must be satisfied. The results are shown in Fig. 1 and we can observe that:
The above observations can be explained by noting that our actively set of constraints complement each other whilst the randomly selected constraints may very well be contradictory. These results for the baseline approach are consistent with earlier work [5] which showed the randomly chosen constraint sets often hurt performance of the underlying algorithm when measured by the Rand index.
In our experiments, we also noticed that there were cases where our query strategy found more than one constraints with the same largest expected error. In this case, we used a randomized tie-breaking step to pick one of them to query. As a result, although our query strategy is designed to be deterministic, its output on certain data sets could vary over many trials. However, for reasonably large data sets, the variation between different trials appeared to be insignificant.

The data sets and Matlab codes used in our experiments are publicly available. Please contact the authors for infor-mation.
 A. Outliers
Our query strategy Q is an instance-based strategy. As a result, the existence of outliers may cause a large number of additional queries. For example, imagine we have a graph with one outlying node. Without constraints, the spectral clustering algorithm may identify the outlying node as one cluster and the rest of the graph as another (especially when the majority of the graph is a relatively well-connected com-ponent). According to our query strategy, the outlying node will have the largest expected error, because the resultant cluster assignment will be entirely different depending on whether or not this node is a true outlier, or it is actually a cluster by itself. Our query strategy will keep querying the pairwise relations between this outlying node and all the other nodes in the rest of the graph until it reaches a conclusion.

On the one hand, we need to point out that this kind of intensive queries on a key node is necessary without prior knowledge on the existence of outliers or the underlying distribution of the data. On the hand other, if we do have prior information, e.g. the minimum size of the potential clusters, we could remove obvious outliers during the pre-processing step. This can help avoid initiating our method with a completely wrong clustering, which inevitably would take much more queries to converge to the groundtruth result.

We also found out that normalizing our estimation of the relation between node i and j , which is P ( t ) ij as shown in Eq.(4), can help reduce the influence of potential outliers in the graph. This can avoid a relatively large entry in u from dominating the query process. Specifically, we have: B. Time Complexity
Our active learning method is an iterative process. The time complexity for each iteration is constant. Within the iteration, there are two main steps, the constrained spectral clustering process, and the query process. The runtime of the constrained spectral clustering algorithm we introduce is dominated by that of solving a generalized eigenvalue problem on an N  X  N matrix; the runtime of the query pro-cess is dominated by computing the rank-one approximation to an N  X  N matrix, which takes no longer than solving the eigenvalue problem. Therefore, the overall time complexity of our method is equal to the number of iterations times the time complexity of solving an eigenvalue problem on an N  X  N matrix, depending on which solver you choose to use, but O ( N 2 ) at least. In other words, the runtime of our method is mainly decided by 1) the size of the data set; 2) the number of iterations/queries.

Note that the time complexity of our method does not increase with the number of constraints we have queried or the number of constraints we query at each time. Thus in practice we can choose to query more than one constraint during each iteration. However, under the assumption that each query comes with a cost, this is essentially a tradeoff between the runtime and the cost of querying the oracle. C. Stopping Criterion
A common consideration when implementing active learning algorithms is when to stop querying, because either 1) the result has converged and will no longer change with more constraints, or 2) the result is  X  X ood enough X  thus further queries are no longer worth the cost. It is possible to find such a criterion when the learning task itself is supervised/semi-supervised and there is some kind of auxiliary information to measure the quality and/or stability of the result. However, it is less likely to find such a measure that works for unsupervised learning (clustering) in general, due to the absolute absence of groundtruth information. Moreover, as Burr Settles stated in his survey on active learning [6]:
Therefore, from the practical perspective, we suggest to make as many queries as possible/affordable, and our method will consistently improve the result unless it has already converged to the groundtruth.

Our query strategy as described in Section III-D assumes that Q is, or can be well approximated by, a rank-one matrix. Only with this assumption, we can estimate the expected error using the rank-one approximation technique. However, we notice that there are real-world application scenarios where Q may have a higher rank. For example, the Q may be derived from a K -way partition of the data set ( K &gt; 2 ), then the rank of Q would be K  X  1 . Or there might be one-sided oracles who only provide Must-Link or Cannot-Link constraints, but not both. To deal with these scenarios, we need to adopt more sophisticated method to estimate the expected error between our current cluster assignment and the groundtruth result.

Another natural extension for our method is K -way par-tition where K &gt; 2 . The spectral formulation of clustering is for cutting a graph and many applications of the approach only require a K = 2 clustering. Although it is possible to extend the formulation to K -way partition, some important theoretical properties will be lost after the extension, e.g. the result is no longer deterministic and it is difficult to interpret the result as a normalized min-cut. To modify our current algorithm for K -way partition, we need to modify the constrained spectral clustering algorithm A to support K -way partition. Common practice is to, instead of only looking at one eigenvector, look at the top-K eigenvectors all together and perform K -means clustering on the rows of the N  X  K matrix [25]. Note that now the output of A ( L; Q ) would become the N  X  N matrix P that directly encodes the pairwise relations between nodes, since a single indicator vector u cannot encode K -way partition for K &gt; 2 . Then we need to modify the query strategy to deal with a Q whose rank is now K  X  1 , as we mentioned above.
In this work, we proposed an active learning framework for spectral clustering. Its goal is to maximally improve the performance of a given constrained spectral clustering algorithm by using as few constraints as possible. We designed a query strategy that incrementally and iteratively picks the constraint with the largest expected error among all unknown constraints and then retrieves the groundtruth value for that constraint from an oracle. Our framework is not only principled, but also high flexible to work with both hard and soft constraints that may occur in real-world applications. We used several UCI benchmark data sets to validate the advantage of our approach, by comparing to the baseline method with randomly selected constraint set. Empirical results showed that our method can find the groundtruth cluster assignment by only using a small constraint set, and it outperformed the baseline method of specifying the same number of constraints as a batch by a large margin.
The authors gratefully acknowledge the support of this research from the NSF (IIS-0801528) and ONR (N000140910712 P00001).

