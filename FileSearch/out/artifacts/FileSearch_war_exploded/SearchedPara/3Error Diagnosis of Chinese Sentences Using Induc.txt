 RU-YNG CHANG, CHUNG-HSIEN WU, PHILIPS KOKOH PRASETYO, World Internet statistics reveal that Chinese is becoming the world X  X  secondmost fre-quent Internet user language [Group 2010]. While Chinese has approximately 885 mil-lion native speakers, the total number of speakers is far larger at 1.3 billion [Wikipedia 2010b]. Also, as the Mandarin Chinese language contains many Hanzi characters, five tones, and a large number of homophones, Chinese is regarded as one of the most difficult languages to learn for English native speakers [Wikipedia 2010a]. Computer-assisted language learning (CALL) systems can assist non-native learners in prac-ticing what they have learned in the classroom, help reduce communication time, and save tutorial/editorial costs [Fotos and Browne 2004]. Microsoft ESL Assistant 1 [Leacock et al. 2009] is one such famous CALL system. However, Microsoft ESL As-sistant focuses on English sentence detection and correction and does not provide feedback which increases a learner X  X  awareness of errors and subsequently facilitates learning. Therefore, this study attempts to construct a CSL error diagnosis system which can process sentential, multiple grammatical, and local semantic errors, and additionally identify error types, word positions, and possible error causes for future correction. The first Chinese CALL system was a character teaching program developed in the 1970s and executed in a mainframe environment [Cheng 1973] and the first Chinese CALL system for personal computers was also a character teaching system [Yao 1996]. In the subsequent years, most Chinese CALL systems focused on language learning resource collection and integration [Huang et al. 2003, 2004a], and how to further integrate these with different multimedia applications [Chen and Liu 2008; Chen et al. 2009; Lo and Chang 2000], pedagogies [Chang and Lee 2006; Chen and Liu 2008], and platforms [Al-Mekhlafi et al. 2009; Zi et al. 2009] to increase interaction.
In addition, some CALL systems may involve error detection or correction. Error detection indicates whether a user X  X  answer is correct or incorrect and error correction tries to make the sentence more accurate. For Chinese, detection and correction of erroneous characters in Chinese words was a key research topic in the past [Lin et al. 2002; Liu et al. 2009a, 2009b; Qiu 2001]. For instance, the words  X   X  X  X  X   X   X  ( qu3 gao1 he4 gua3 ) X  and  X   X   X  X  X  X  ( tou1 ji1 qu3 qiao3 ), X  should be corrected as  X   X  X  X  X  X  X  ( qu3 gao1 he4 gua3 ) X  and  X  ( X ) X  X  X  X  ( tou2 ji1 qu3 qiao3 ), X  respectively. However, while CSL researchers focused on character handling, CSL learners often failed in semantic, structural, and the pragmatic features. For CSL learners, a number of character-istics of Chinese are particularly difficult to control, such as reduplicative formulas, complements, sentence with subject-predicate and ( X ) -sentence [Cheng 1997]. For in-stance,  X   X  X  X   X   X   X  X  ( I am very busy ) X  should be  X   X  X  X  X  X   X  X nd X   X  X  X   X   X   X   X  X  ( He is an American .) X  should be  X   X  X  X  X  X  X  X  X   X . In the past few years, CALL systems tended to focus on English sentence error detection [Brockett et al. 2006; Chodorow et al. 2007; Felice and Pulman 2007, 2008; Han et al. 2006; Lee and Seneff 2008] and they tried to detect/correct the problems in prepositions [Chodorow et al. 2007; Felice and Pulman 2007, 2008], determiners [Felice and Pulman 2008], articles [Han et al. 2006], mass nouns [Brockett et al. 2006], and verb forms [Lee and Seneff 2008]. Most of the previ-ous work adopted binary classifiers to verify whether the input and modified sentences are correct or incorrect. In these related works, the Maximum entropy classifier was the most popular classifier for the English error detection.

The limit of the maximum entropy and binary classifier-based approaches is that they can only provide a correct/incorrect decision or error type for a test sentence, but learners still do not understand the error for future correction. In addition, most previous detection work considered one or two error types based on n -gram and POS (Part-of-Speech) information. Sometimes n -gram and POS information is not enough to determine whether the sentence is correct or not. For instance, the error sentence  X   X  X  X  X  X  X  X  X  X  X   X   X  ( He is twenty one years old .) X  should be corrected as  X   X  X  X  X  X  X  X  X  X   X  X   X . Furthermore, a sentence may contain multiple errors. For in-stance, the sentence  X   X  X  X  X   X  X  X  X  X  X   X   X   X  X  X  X  X  X  X  X  X  ( Yesterday, I wrote a letter and made a new friend .) X  contains two errors. Due to these limitations, a novel approach which can capture rich and various attributes of sentences, is desirable to deal with multiple error diagnosis and provide human-interpretable rules for further correction.
Inductive logic programming (ILP) is a research area at the intersection of machine learning and logic programming [Muggleton and Raedt 1994]. ILP can combine rich sources and utilize expressive language to characterize multiple and logical relations. Therefore, ILP can obtain higher accuracy than other machine learning approaches in learning the English past tense [Muggleton 1999] and for biological multi-relational data reasoning and mining [Paschke and Schroder 2007]. For this reason, ILP could be a suitable solution for error diagnosis. One of the earliest and best-known ILP induction algorithms is the First-Order Inductive Learning (FOIL) algorithm [Quinlan 1990].

In addition, t -score has been widely used for collocation discovery and can test how probable it is that a certain constellation will occur [Manning and Schutze 1999]. Therefore, t -score could be a suitable solution to test how probable it is that two at-tributes will co-occur in a sentence. The objective of this study is to design a system to automatically diagnose errors in a Chinese sentence. Error diagnosis not only identifies a sentence as correct or incorrect, but also provides error type information including error description and analysis. The rationale for this study is that relevant, precise, and understandable feedback is of cen-tral importance, especially when human tutors are not always available [Heaneey and Daly 2004]. Therefore, it is hoped that feedback can give CSL learners the error types, word positions, and causes of an error sentence rather than only a correct/incorrect decision. What X  X  more this study focuses on multiple grammatical and semantic errors at the sentence level and first attempts to diagnose errors in CSL sentences. In this study, sentence-level error means that error can be determined by information within the sentence. Compared to single sentence level errors, multiple errors in a sentence have more variation; therefore, diagnosing multiple error types is more challenging than focusing on one error type. The target error types of this study are lexical choice, omission, redundancy, and the handled error sentences may contain multiple errors. Table I lists the target error type of this study where the illustrative sentences are selected from CSL learners [Cheng 1997]. The main contributions of this study are summarized as follows.  X  A penalized probabilistic First-Order Inductive Learning (pFOIL) algorithm is pre-sented to diagnose the errors in Chinese sentences. The pFOIL algorithm integrates inductive logic programming (ILP), First-Order Inductive Learning (FOIL) and a penalized log-likelihood function for error diagnosis. The pFOIL algorithm can con-sider uncertain, imperfect, and even conflicting background knowledge for different errors to infer errors resulting from overlapping and incomplete error characteris-tics and produce human-interpretable rules for further error correction. Comparing to the traditional FOIL algorithm [Quinlan 1990], the pFOIL algorithm can deal with multi-class problems and use penalty to maximize the boundaries between classes.  X  In pFOIL algorithm, the expressive information in a sentence is represented by re-lation pattern background knowledge and quantized t -score background knowledge.
Concept information and grammatical relations were considered in error diagnosis, which differs from previous related work using only word forms and POS informa-tion. The relation pattern background knowledge captures morphological, syntactic, and semantic relations among words in a sentence and then one or two extracted re-lations are integrated into a pattern. The quantized t -score background knowledge uses quantized t -scores to retain the various relations in a sentence, thus can reduce the computation time for error diagnosis model construction and improve the recall rate for error diagnosis. The relation pattern background knowledge and quantized t -score background knowledge are generated and used for likelihood estimation in the pFOIL algorithm.  X  In order to predict multiple errors, a decomposition-based testing mechanism that diagnoses a sentence class by class using the decomposed background knowledge needed for each error type is proposed. Chinese grammatical units are words, phrases and sentences and a sentence is com-posed of words or phrases. Grammatical roles or functions performed by words or phrases in a sentence are called sentence elements. In Chinese, the sentence elements are subject, object, adverbial adjunct, and so on. Sentence elements in syntax are closely related to POS and morphology of Chinese words. A simple Chinese sentence can be categorized as either a subject-predicate sentence, such as  X   X  X  X  X  X  ( Ilove you .) X  or a non-subject-predicate sentence, such as  X   X  X  X  X  X  X   X  [Cheng 1997]. This implies that a subject is not always necessary in a Chinese sentence. In addition, Chi-nese word segmentation is one of the most prominent tasks in computer processing of Chinese language [Chen and Bai 1998], while word segmentation is not a problem in English.

When learning Chinese, CSL learners find that functional words, such as ent semantic interpretations. For instance,  X   X  X  X  X  X  X  X  X  X   X   X  (That dictionary is me .) X  should be  X   X  X  X  X  X  X  X  X  X  X  X  (That dictionary is mine.)  X . Furthermore, CSL learners often make a mistake with measure words owing to missing or wrong collocations for a noun. For instance, the sentence  X   X  X  X  X  X  X  X  X   X   X  X  X  X   X  loses a measure word and the sentence  X   X  X  X   X   X  X  X  X  X  X  X  X   X  contains a wrong measure word [Cheng 1997]. This is because second language learners apply knowledge from their native language to a sec-ond language [Odlin 1989]. In Cheng [1997], millions of erroneous sentences from CSL learners were collected and analyzed, and then some sentences were selected as typi-cal CSL error sentences. Furthermore, the corresponding possible corrected sentences were provided and then used to explain the error cause. For instance, the CSL er-ror sentence  X   X  X  X  X  X  X  X  X  X   X   X  X  X  X  X  X   X  (Last week, we went to several parks.)  X  should be  X  ror sentences in Cheng [1997], three major findings related to error sentences can be inferred.  X  Chinese sentences are governed to a large extent context by considering the seman-tic or pragmatic factors, which differs from English, which is mainly governed by grammatical functions [Li and Thompson 1981]. Therefore,  X   X  X  X  X  X  X   X  X  X  X  X   X  X nd  X   X  X  X  X  X  X  X  X  X  X   X  are both correct Chinese sentences and means  X  That dictionary is mine  X , even though the measure word  X   X   X  does not exist in the second sentence.
But the wrong measure word  X   X   X  for the word  X   X  X  X   X  leads to an error in the sentence  X   X  X  X   X   X  X  X  X  X  X  X   X . After considering these uncertain, imperfect and even conflicting characteristics for Chinese sentences, the pFOIL algorithm which inte-grates ILP, FOIL and a penalized log-likelihood function is employed for Chinese error diagnosis.  X  As we have already found, the causes of errors are often morphological, syntactical, or semantic; however, errors are sometimes caused by a combination of these fac-tors. Morphology of Chinese means how Chinese words are constructed and used [Packard 2000]. For instance, morphological information could be used to diagnose the error sentence  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  X  X  X  X  X  (Students actively attend a sport meeting.)  X ( X   X   X  should be  X   X   X ). From the semantic information, a concept pat-tern could be a solution to a sentence with a reduplicate meaning. For instance, the error sentence  X   X  X  X  X   X   X  X  X  X  X  X  X  (I send you away.)  X  should be  X   X   X  X  X  X  X  X   X  X   X .
In addition, syntactical information can be used to identify whether a sentence is correct or not. For instance, the sentence  X   X  X  X  X  X  X  X   X   X  X  X  X  X  X  X  (I went to a store and bought a sweater.)  X  should be  X  ical change types also can assist in fixing this error. Because the word  X   X   X  ex-ists after a verb to represent accomplishment [Chang 2004] and the error sentence  X   X  X  X  X  X  X  X   X   X  X  X  X  X  X  X   X  loses a verb before  X   X   X . However, the above illustrative sentence  X  formation for error diagnosis. Thus, all the morphological, syntactical, or seman-tic relations in the sentences are required for error diagnosis. This is why ILP is adopted as it can capture and integrate various, multiple relations and sources to characterize a Chinese sentence.  X  A sentence may contain multiple errors and the causes of the errors can be used to explain multiple error types. For instance, the above sentence  X 
 X  and lexical choice ( X  words (namely  X   X  X nd X   X  X  X   X  X nd X   X  X  X   X  X nd X   X  X  X   X ) can be used to determine whether the sentence has a lexical choice or a redundancy error type. Therefore, this study proposes a decomposition-based testing method that iteratively diagnoses er-rors with the background knowledge of one error type to diagnose all possible error types. Figure 1 shows the proposed system framework for Chinese error diagnosis in this study.

During the construction of an error diagnosis model, both learners X  error sentences and native speakers X  sentences are used to generate relation pattern background knowledge and quantized t -score background knowledge. Learners X  error sentences and native speakers X  sentences also provide examples indicating which error/correct type occurs. For each sentence, the morphological, syntactic, and semantic relations among the words are extracted, combined and represented by relation pattern back-ground knowledge. Following this, the continuous values of t -score of collocation, bi-gram, bi-POS, or bi-relation are estimated based on the native speakers X  sentences. Then it is quantized into discrete values to generate the quantized t -score background knowledge that is used to represent each sentence. The error diagnosis model con-struction process applies a pFOIL algorithm to construct a hypothesis consisting of induced rules inferred from the relation pattern background knowledge, quantized t -score background knowledge and examples. Finally, this inferred hypothesis is then employed as the error diagnosis model and the clauses in the hypothesis can be used to represent the error type and cause related to the error.

During the diagnosis phase, a new sentence is first preprocessed to extract the re-lation pattern background knowledge, quantized t -score background knowledge and all possible examples. The sentences from native speakers used for training are also used to assist in quantized t -score background knowledge generation. Finally, a decomposition-based testing mechanism is then used to produce the error diagnosis result.

In the following illustration, the error sentence  X   X  X  X  X  X  X  X   X   X  X  ( I know that man ) X  (identifier: s1) is selected as a diagnosed sentence to illustrate the background knowledge, examples, and hypothesis. The s Rel W W ( s1 ,[nsubj],[  X  X  X  X  ],[  X  ]) and s MW NW Tscore( s1 , [  X  ],[  X  ] ,[low]) are two possible types of background knowledge for s1 . The s Rel W W ( s1 , [nsubj],[  X  X  X   X  ],[  X  ]), one type of relation pattern back-ground knowledge, means that s1 contains a grammatical relation  X  nominal subject  X  between  X   X   X  X nd X   X  X  X   X   X  X ndthe s MW NW Tscore( s1 , [  X  ],[  X  ] , [low]) , one type of quantized t -score background knowledge, indicates that s1 has a measure-noun collocation  X   X   X   X  with a low t -score. All error and correct types are all possible examples for s1 . Therefore, possible examples for s1 are LexicalChoice( s1 ), Redun-dancy( s1 ), Omission( s1 )and Correct( s1 ). The s1 in the two background knowledge and all possible examples, and [ [  X  ] ground knowledge s MW NW Tscore( s1 , [  X  ],[  X  ] , [low]) are variable arguments that can perform  X  Substitution  X  operations. If the clause LexicalChoice ( A )  X  s MW NW Tscore( A,[B],[C] ,[low]) obtains the highest score in the hypothesis, then the error diagnosis result shows that the diagnosed sentence has a lexical choice prob-lem and provides the cause of the error from the corresponding interpreted meaning of s MW NW Tscore( A,[B],[C] ,[low]) . The s MW NW Tscore( A,[B],[C] ,[low]) indicates that the diagnosed sentence contains a measure-noun collocation with a low t -score. Figure 2 illustrates technological terminologies: example , background knowledge and hypothesis in the ILP. Figure 2 also illustrates the steps of the error diagnosis model inference using an ILP-based approach via pFOIL algorithm to a lexical choice er-ror sentence:  X   X  X  X   X   X  X  X  X  X  X  X  X   X  (identifier: s2) and a correct sentence:  X   X  X  X  X  X   X  (identifier: s3 ) in this study.

In ILP, a predicate is a relation identifier of its argument (arg), namely s MW NW Tscore( arg , arg,arg ,arg), s W W Tscore ( arg,arg , arg ,arg), LexicalChoice ( arg ) , Correct( arg ),andsooninFigure2. 2 Predicates are used to define the ar-gument relations among the background knowledge, example and hypothesis. First, the predicate s MW NW Tscore( arg , arg,arg , arg) has four arguments and is used to represent background knowledge which means the sentence has a measure-noun col-location with a t -score. Next, the predicate s W W Tscore ( arg,arg , arg , arg) has four arguments and is used to represent background knowledge which means the sentence has a bi-gram with a t -score. Then, the predicate LexicalChoice( arg ) has one argu-ment and is used to represent an example which means the target type of the sentence is lexical choice. Finally, the predicate Correct ( arg ) has one argument and is used to represent an example which means the target type of the sentence is correct. The argument in the predicate can be represented using constants or variables. Constant argument is often used to identify the objects, such as  X  low X ,  X  X ero X ,  X  X igh X ,  X  X Subj  X ,  X  det X ,  X   X   X ,  X   X   X   X  and so on. On the other hand, the variable argument is used to generalize the constant, and can be substituted with a constant of different values. variables V 1 , ..., V n ,suchas { A /s2, B /  X  , C/  X  X  X  } . Notation t  X  is used to denote the instantiated logical term t based on substitution  X  . Based on the substitution opera-tion, applying  X  = { A /s2, B /  X  , C /  X  X  X  } to s MW NW Tscore( A,[B],[C] ,[low])  X  obtains s MW NW Tscore(s2, [  X  ],[  X  X  X  ] ,[low]) . In this article, the boldfaced letter argument or argument value with boldfaced form in the predicate represents a variable argu-ment. Variable arguments can operate substitution operations to check predicate with various constant values. Predicate and its argument number and type are designed based on characteristics of data and goal of the task.

For error diagnosis model construction, the first step is a pre-process to generate the background information for all the sentences. The background information can be obtained from heterogeneous assisted resources and represents the morphological, semantic and syntactical configurations of the sentences which could affect error diag-nosis and error generation. The top part of Figure 2 is the background information for the two illustrative sentences where word segmentation, POS, and grammatical rela-tion information are obtained from the assisted resources and the quantized t -score of collocation, bi-gram, bi-POS, and bi-relation are estimated from the native speakers X  sentences.

The following step describes the process of generating background knowl-edge. In the middle part of Figure 2, clausal logic represents the relations in the above background information as the background knowledge . For instance, s Rel W W( s3 ,[nSub], [  X  ],[  X  ] ) and s MW NW Tscore( s2 , [  X  ],[  X   X  ] , [low]) are two types of background knowledge. The background knowledge s Rel W W( s3 ,[nSub], [  X  ],[  X  ] ) indicates that s3 contains a grammatical relation  X  nominal subject  X  be-tween  X   X   X  X nd X   X   X  X ndin s2 the s MW NW Tscore( s2 , [  X  ],[  X   X  ] , [low]) indicates a measure-noun collocation  X   X   X  X  X   X  with a low t -score. The outputs of the assisted resources can be used to assist background knowledge generation.

In this step, the native speakers X  sentences and learners X  error sentences also provide examples. Examples representing the background facts are also the target relations for the hypothesis. Examples supporting true facts are termed positive examples. Assuming that the target relation learned from the sentence of Figure 2 is LexicalChoice and Correct relations, positive examples could be LexicalChoice ( s2 ) and Correct( s3 ) where  X  s2  X  X nd X  s3  X  are variable arguments and can perform sub-stitution operations. Positive examples contain class information from both correct and error sentences. Thus, this task only employs positive examples for CSL error diagnosis in pFOIL algorithm and examples in this article only represent the positive examples.

The final step is to infer a hypothesis from examples and background knowledge from both native speakers X  sentences and learners X  error sentences. Figure 3 shows the format and the corresponding meaning of a hypothesis in ILP-based approach. Hypothesis inferred from the above examples and background knowledge using ILP contains a set of first-order clauses, which are usually definite clauses. A definite clause is composed of a head and a body where the head is a predicate that is implied to be true if the conjunction of predicates in the body is true. For the relations in Figure 2, the following is a possible inferred definite clause for lexical choice in a hypothesis. Based on data observation and the error causes for each error type, background knowl-edge and its argument number and types on the error diagnosis task are designed to characterize error sentences for each error type. Learners X  error sentences are used to extract the background knowledge regarding each tagged error type, while the native speakers X  sentences are used to extract the background knowledge regarding all er-ror types. In this study, relation pattern background knowledge and quantized t -score background knowledge are proposed to represent sentences from native speakers X  sen-tences and learners X  error sentences.

The relation pattern background knowledge captures the morphological, syntac-tic, and semantic relations and the various relations among the words in a sentence are combined. In Figure 2, the relation patterns s Rel W W( s2 ,[det], [  X  X  X  ],[  X  ] ) and s Rel W W( s3 ,[nSub], [  X  ],[  X  ] ) are two types of relation pattern background knowl-edge respectively, indicating s2 contains a grammatical relation  X  determiner  X  between  X   X   X  X nd X   X   X   X  X ndthe X  nominal subject  X  X f X   X   X  X s X   X   X  X n s3 . The relation pattern background knowledge can be composed of a mixture of word form (W), part-of-speech (POS), grammatical relation (Rel), and word concept (Cept).

The quantized t -score background knowledge uses the t -scores of collocations, bi-grams, bi-POSs, and bi-relations which are estimated based on native speaker sen-tences and quantized into discrete values to represent the various relations in each sentence. That is to say, the quantized t -scores estimated from the native speakers X  sentences are regarded as the statistic confidence for collocations, bi-grams, bi-POSs, and bi-relations that are generated from the native speakers X  sentences and learn-ers X  error sentences. The t -score is computed and then quantized into four values based on the confidence interval of the t -distribution: high ( t &gt; = 2.576), medium (1.282 0.25). In Figure 2, s W W Tscore( s2 , [high]) and s MW NW Tscore( s2 , score background knowledge. In s2 the s W W Tscore( s2 , [high]) shows the word  X   X   X  followed by the POS  X  Vt  X  X ithahigh t -score. In s2 , the s MW NW Tscore( s2 , alow t -score. During pFOIL algorithm inference, the quantized t -score background knowledge s MW NW Tscore( s2 , [  X  ],[  X   X  ] ,[low]) is compiled as s MW NW Tscore ( A,B,C ,[low]) , indicating a measure-noun collocation with a low t -score. Therefore, this background knowledge can cover numerous measure-noun collocations with low t -scores.

For background knowledge generation, the CKIP Chinese word segmentation system 3 , Stanford Chinese Dependency parser 4 , and SinicaBow 5 are considered as as-sisted resources and produce word boundary, POS, grammatical relations, and concep-tual information of the words for background information generation respectively. The CKIP Chinese word segmentation system was ranked as the number one system for traditional Chinese word segmentation at the First International Chinese Word Seg-mentation Bakeoff held by ACL SIGHAN [CKIP 2004; Ma and Chen 2003]. The CKIP Chinese word segmentation system is the first word segmentation system with out-of-vocabulary word identification and syntactic category prediction capabilities [Chen and Ma 2002; Ma and Chen 2005; Tsai and Chen 2003]. POS information is a word-level syntactic category. The CKIP Chinese word segmentation system is thus adopted to generate both Chinese word boundary and POS information. Furthermore, the Stanford Chinese Dependency Parser can produce a richer set of grammatical rela-tions that express the more abstract information between Chinese words [Chang et al. 2009]. SinicaBOW can provide conceptual information for Chinese/English words with different POSs [Huang et al. 2004b]. The outputs from these resources are also shown at the top of Figure 2. The goal of ILP is to find a hypothesis capable of maximizing the score function from the training data and then the inferred hypothesis is the induced rules that can be used for error sentence diagnosis and further correction.

The original FOIL algorithm [Quinlan 1990] starts from the most general clause and successively checks clauses with more predicates that score highly with respect to the examples, background knowledge, and hypothesis. For each iteration, FOIL searches and checks the definite clause with the highest score to be added to the hypothesis. The FOIL algorithm does not repeat checking an example namely the examples covered by the clause are removed from the example set. In the original FOIL algorithm, informa-tion gain is employed to calculate covers and score functions based on positive and neg-ative examples, and minimum description length method is adopted as the stopping criterion. Traditional ILP formulation can be considered a set of hard constraints on background knowledge. By including a probabilistic concept, this study attempts to re-lax these constraints to obtain a scored result by penalized probability rather than sim-ply a true or false result. The probabilistic concept in nFOIL [Landwehr et al. 2007], which integrates Na  X   X ve Bayes and FOIL, extends ILP inference and softens the ILP Algorithm 1. Penalized Probabilistic First-Order Inductive Learning Algorithm entailment to deal with uncertain, imperfect, and even conflicting background knowl-edge. Moreover, the Na  X   X ve Bayes can empower FOIL to deal with multi-class problems so that overlapping and incomplete error causes can be employed to infer multiple errors. For error diagnosis, sentences of an error/correct type should have similar at-tributes, but error sentences violate some rules of correct sentences. Furthermore, some error sentences and correct sentences sometimes share the same attributes. Ac-cordingly, a penalized probabilistic First-Order Inductive Learning (pFOIL) algorithm, integrating ILP, FOIL, and penalized log-likelihood function, is used for Chinese sen-tence error diagnosis. The pFOIL algorithm is shown in Algorithm 1 and illustrated in Figure 4.

In the pFOIL algorithm, represents the empty set, c denotes the clauses and p ( X 1 ,...,X n ) represents the predicates with arguments X . The pFOIL algorithm starts with an empty hypothesis ( H H H := ). For each iteration in the outer loop, the algorithm initializes an empty clause for the candidate clause ( c := p ( X 1 , ..., X nalized probabilistic score is then used to evaluate each possible candidate clause and calculated based on native speakers X  sentences and learners X  error sentences. The penalized probabilistic score function scores the body of the clause given all possible examples (classes). The body of the clause with the best score is then kept as a candidate clause. In the specialization process, the pFOIL algo-rithm attempts to obtain more specific clauses. Thus, the bodies of the clauses are obtained by adding the remaining predicates to the body of the current can-didate clause. For instance, if the clause body s MW NW Tscore ( A,[B],[C] ,[low]) has the best score, s MW NW Tscore ( A,[B],[C] ,[low]) X s Rel W W( A , [det], [  X  X  X  ],[  X  ] ) and s MW NW Tscore ( A,[B],[C] ,[low]) X s W W Tscore( A,[D],[E] ,[zero]) are two possi-ble bodies of candidate clauses in the specialization process. In other words, the pFOIL algorithm tries to combine different predicates to form the candidate clause body to cover information of the sentence. Then the score function is once again applied to measure the candidate clause quality and if this specialization process improves the score, the predicate giving the best improvement of score will be added to the cur-rent candidate clause body. Otherwise, no predicate is added to the current candidate clause body. In the end of the specialization process, the candidate clause is added to the hypothesis.

Two thresholds are adopted as the stopping criteria in the pFOIL algorithm. For the inner loop, clause specialization is stopped if it causes the size of the clause body (number of predicates in the body of the clause) to exceed the maximum m or no more predicate can be added. For the outer loop, hypothesis development is stopped if the change in the score function falls below the threshold value  X  .

Moreover, a beam search is adopted in the pFOIL algorithm. If the background knowledge s Rel W W( A ,[det], [  X  X  X  ],[  X  ] ) is not the highest score (beam size = 1) in each inner iteration of the pFOIL algorithm, it will not be selected in the hypothesis, even though s Rel W W( A ,[det], [  X  X  X  ],[  X  ] )is one type of background knowledge for the sentences with lexical choice error.

Comparing to the FOIL algorithm, the main modification of the pFOIL algorithm is that all examples must be considered in calculating the likelihoods of all examples from native speakers X  sentences and learners X  error sentences. Consequently, the pFOIL algorithm does not remove the examples already covered by this hypothesis.
In the pFOIL algorithm, the final goal is to find hypothesis H which maximizes the score function considering background knowledge B and a set of examples E. Con-sequently, hypothesis H incorporates penalized probabilistic parameter H  X  which as-signs a penalized probability to each clause in hypothesis H . Therefore, hypothesis H becomes the augmented hypothesis H =( H Q , H  X  ), where H  X  denotes the associated parameters of clause set and H Q = { q 1 ,...,q j } denotes a clause body set where j denotes an index of a clause body and q denotes the body of clause c.
 where E denotes the set of examples, B represents background knowledge and H de-notes the hypothesis. In the pFOIL algorithm, the score function is defined in Equa-tion (2) using the penalized probabilistic covers function in Equation (6). The score function assesses the likelihood sco re ( E | H , B ) of the data given the hypoth-esis and background knowledge. The score function is computed by maximizing the likelihood of the examples to assess a set of clauses. By incorporating the probabilistic concept, the covers function becomes the probabilistic covers function, and coverage is then measured using probabilistic concepts. The covers function is then defined as the likelihood of example e given hypothesis H and background knowledge B from both native speakers X  sentences and learners X  error sentences.
In the pFOIL algorithm, substitution  X  can be considered as a query from an ex-ample for a clause which returns a value of true or false. For a definite clause c , cate of a class label with its argument and thus the substitution of  X  with example e , ground knowledge, and thus a query of a clause c, q c =  X  B 1 ,...,B n , can be regarded as the background knowledge or attribute for the examples. Substitution  X  of this query, q  X  =  X  B hypothesis H :
Looking at s2 in Figure 2, p = LexicalChoice( A )and { A /s2 } assumes p  X  is true since s2 has a LexicalChoice error. q 1 (namely s MW NW Tscore( A,[B],[C] ,[low]) )is the background knowledge query for s2 in the body of the first clause, and q 2 (namely s Rel W W( A ,[nSub], [  X  ],[  X  ] ) ) is the background knowledge query for s2 in the body of the second clause. Since the background knowledge for s2 matches the body of the first clause, substitution q 1  X  is true. On the other hand, if the background knowledge for s2 does not match the body of the second clause, substitution q 2  X  is false. The likelihood of example e is then defined as Equation (4).
 P  X  q 1  X , ..., q j  X  | p  X  is equal to sumption. Using the above independence assumption, Equation (4) is expressed as Equation (5) P  X  ( p  X  ) specifies the prior class distribution and P  X  q 1  X , ..., q j  X  can be calculated by summing out.

The error diagnosis task supposes that a clause in the hypothesis can character-ize a specific class. In addition, the background knowledge for the correct sentences is not expected to appear in the error classes. Some error sentences and correct sen-tences sometimes share the same attributes. Thus, the penalized probabilistic covers function is adopted rather than the pure probabilistic covers function. The penalized probabilistic covers function is defined as Equation (6.1) when the current example is an error class, and is defined as Equation (6.2) when the current example is a correct class. In Equation (6.1) and Equation (6.2), e denotes an example, E denotes the set of examples, e correct denotes the example e belonging to the correct class, e error denotes the example e belonging to the error class, # Errors denotes the number of error classes and  X  denotes the weighting coefficient of the second term. The value of  X  is between zero to one. The second term in Equation (6.1) or Equation (6.2) represents the ratio of the likelihood of the correct or error sentences to the likelihood of all examples. Thus, the denominator of the second term in Equation (6.1) is the sum of the likelihood of all examples, and the numerator of the second term in Equation (6.1) is the sum of likelihood of examples belonging to the correct class. The denominator of the second term in Equation (6.2) is the product of the number of error classes and the sum of the likelihood of all examples and the numerator of the second term in Equation (6.1) is the sum of the likelihood of examples belonging to error classes.
For diagnosing new diagnosed sentence, the penalized probabilistic covers func-tion calculates the penalized likelihood of possible error types as example and cor-responding background knowledge. The example (class) with the maximum score is the predicted class of the new diagnosed data. Moreover, the body of the clause in the hypothesis which yields the largest covers function score is regarded as the error cause. The pFOIL algorithm diagnosis indicates that LexicalChoice( A ), the head of clause, has maximum score and s MW NW Tscore( A,[B],[C] ,[low]) , the body of the clause, yields the largest cover function score for the diagnosed sentence  X   X  X  X  X  X  X  X   X  X   X  (identifier: s1). This condition is like the clause LexicalChoice( A )  X  s MW NW Tscore( A,[B],[C] ,[low]) which obtains the highest score in the hypothesis. Then, the error diagnosis result shows that the sentence has a lexical choice problem owing to the low t -score of measure-noun collocation. Compared to traditional ILP, the pFOIL algorithm using probabilistic and penalized concepts selects more robust clauses for each class. In addition, all clauses in the hypothesis are considered when the penalized probabilistic FOIL algorithm identifies the best class. Since the occurrence frequency for multiple error types and overlapping background knowledge is high, a decomposition-based testing mechanism is desirable. The decomposition-based test mechanism decomposes the diagnosed sentence into differ-ent sets of background knowledge needed for each error type and this diagnosis is performed class by class. In each diagnosis process, a new diagnosed sentence is diag-nosed using the background knowledge for one error type iteratively. The diagnosed sentence is labeled if the pFOIL algorithm gives the corresponding error type. This procedure goes through each error type until all the background knowledge for each error type has been diagnosed and can be seen in Figure 5. After generating all back-ground knowledge of the diagnosed sentence s5 , the background knowledge of one error type omission is diagnosed and then the illustrative sentence, s5 , is diagnosed as a correct sentence because the hypothesis returns correct type of the diagnosed back-ground knowledge. The background knowledge for the other error type is generated and diagnosed in the following step. When background knowledge for redundancy error is diagnosed, the background knowledge s W W Tscore( s5 , [ ],[  X   X  ] ,[low]) matches the clause body s W W Tscore( A,[B],[C] ,[low]) and the clause redun-dancy( A )  X  s W W Tscore( A,[B],[C] ,[low]) obtains the highest score in the hypothesis. Then, the pFOIL algorithm returns that the diagnosed sentence contains a redun-dancy error. Because the hypothesis returns redundancy and lexical choice error types when background knowledge for redundancy and lexical choice errors is diagnosed, the final result of the diagnosed sentence is the redundancy and lexical choice error. Therefore, the decomposition-based testing mechanism can capture multiple errors in a sentence, even though there is overlapping background knowledge for different error types. The produced error causes were inferred from the background knowledge which was designed based on the definition and causes for each error type. If the error sentence was correctly classified, the inferred error cause of the original input sentence must be reasonable in order to interpret the error. Therefore, the experiment only assesses classification performance via comparisons with other classifiers.

In the following experiments, different background knowledge was used for likelihood estimation in the pFOIL algorithm for different evaluations. Maximum entropy, decision tree based C4.5, and Na  X   X ve Bayes classifiers in the Mallet toolkit 6 were used for comparison. For a fair comparison, background knowledge used in the pFOIL algorithm was transformed into binary attributes in these attribute-value baseline systems. For instance, the quantized t -score background knowledge s MW NW Tscore( s2 , alow t -score for both measure and noun words. For the grammatical relation  X  de-terminer  X  between  X   X   X  X nd X   X   X   X , the relation pattern background knowledge s Rel W W ( s2 ,[det], [  X  X  X  ],[  X  ] ) was transformed into true.

For performance assessment, the training and test sentences were composed of two sources: Cheng X  X  book [1997] and Dr. Eye corpus 7 [Wu et al. 2010]. Error sentences from the Dr. Eye corpus and the sample error sentences listed in Cheng [1997] were employed as the learners X  error sentence corpus, and correct sentences from the Dr. Eye corpus and corresponding corrected sentences listed in Cheng [1997] were re-garded as the native speakers X  sentences. Altogether, there are 3,501 learners X  error sentences and 3,501 native speaker sentences used in this study. The Dr. Eye cor-pus used in this study is composed of 2,707 error sentences and 2,707 corresponding corrected sentences. To generate the error sentences from the Dr. Eye corpus, English sentences from the Dr. Eye dictionary 8 are extracted and then translated by a machine translation model, the baseline system of EACL 2009 fourth workshop on statistical machine translation 9 . Then, the translated sentences are identified by Chinese native speakers. If the translated sentences are reasonable error sentences as by a learner of Chinese as a second language, then the sentences were regarded as error sentences in the Dr. Eye corpus. The native speakers X  sentences from the Dr. Eye corpus are man-ually corrected from these error sentences. This study only considered the error sen-tences containing lexical choice, omission, and redundancy errors. Punctuation error sentences from the Cheng book [1997] were not used in the training and test dataset. In Cheng [1997], there are 30 error sentences with multiple corresponding corrected sentences. For instance, the error sentence  X   X  X  X  X  X  X  X  X  X  X  X  X  X   X   X   X   X  X  X  X  X  X  X  ( After I got advice from a doctor, I was determined to take exercise ) X  has two corresponding corrected sentences, namely,  X   X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X   X . This condition was regarded as two error sen-tences and two correct sentences in the training and test dataset. The error types of the learners X  error sentences were judged by native Chinese speakers and the fol-lowing instances demonstrate this process. The error sentence  X   X  X  X  X  X  X  X  X   X   X  X  X  X   X  has an omission problem, since the word  X   X   X  must be added to match the corre-sponding correct sentence  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X   X , The word  X   X  must be deleted and the word  X   X   X   X  must also be changed to match the corresponding correct sentence  X   X   X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X   X , and then the error sentence is annotated with re-dundancy and lexical choice errors. The average number of errors per sentence on the training and test sentences is 1.26, meanwhile the average number of errors per sen-tence in Cheng [1997] is 1.07 and the average number of errors per sentence in the Dr. Eye corpus is 1.33. This statistics implies that learners X  error sentences often contain more than one error and are similar to machine-translated sentences.

Recall rate, precision rate and F1-Score were employed as evaluation measures and were computed based on how many error types contained in the sentence. Figure 6 illustrates the recall and precision computation.

Approximately 80% of the dataset was employed as the training data and 20% of the dataset as the test data. In the pFOIL algorithm, there are four parameters including the threshold  X  for the stopping condition, the weighting parameter  X  , clause body size m , and the beam size in the beam search for candidate clause generation. The best parameter setting was evaluated based on a five-fold cross-validation method with various parameter settings, and the average F1-score was adopted for assessment. In this study, the following experiments were evaluated using the following parameter settings: threshold  X  = 0.01 or 0.00001, weighting parameter  X  = 0, 0.2, or 1 and beam size = 1 or 2. The experimental experiences show that the maximum clause size of a hypothesis is less than four, thus the clause body size is set to five ( m =5).
The effect of quantized t -score and relation pattern background knowledge, and the performance of the pFOIL algorithm on error diagnosis were evaluated in the following experiments. Table II summarizes the results for the evaluation based on quantized t -score back-ground knowledge. This experiment evaluated whether the quantized t -score level of the quantized t -score background knowledge effectively cover traditional word form-based background knowledge/collocations or not. The diagnosis model was trained with background knowledge regarding lexical choice error type and examples from lex-ical choice error sentences and native speakers X  correct sentences. This lexical choice error type was considered for the experiment because this problem relies on traditional word form-based background knowledge. The quantized t -score background knowl-edge was then compared with traditional word form-based background knowledge (TW), which uses a combination of word and POS in collocation, such as noun-noun, noun-verb, verb-noun, verb-verb, adjective-noun, verb-adverb, adverb-verb, adverb-adjective, determiner-measure, measure-noun, and conjunction-conjunction. If the traditional word form-based background knowledge was represented by a quantized t -score, it was equal to the quantized t -score background knowledge regarding error types of this lexical choice problem.

Experimental results demonstrate that the model trained using quantized t -score background knowledge significantly outperformed that trained using traditional word form-based background knowledge (TW) in average precision, recall and F1-score with p &lt; 0.01. The experimental result shows that using a t -score level for two word colloca-tions is better than using the actual word and POS information. The outcome indicates that quantized t -score background knowledge can effectively capture the lexical choice error patterns, whereas traditional word form-based background knowledge lacks ex-pressiveness, generates a large numbers of features, and does not generalize knowl-edge representation. For instance, these two clause bodies are compared in terms of quantized t -score background knowledge and traditional word form-based expression: (1) s DetW MW Tscore( A,B,C ,[low]) and (2) s DetW MW ( A , [  X  ],[  X  ] )
The first clause body, which is interpreted as a low t -score of a determiner-measure collocation, provides a better generalization than the second one, which simply in-dicates that measure word  X   X   X  exists after the determiner  X   X   X . The first clause body includes the condition in the second clause body. s DetW MW ( A , [  X  ],[  X  ] ) only can diagnose the sentence that contains determiner word  X   X   X  and the measure word  X   X   X , while s DetW MW Tscore( A,B,C ,[low]) can diagnose sentences that contain determiner-measure collocation with a low t -score. Substitution operations of the ILP-based approach check various values in the background knowledge and thus raise the precision rate for the error diagnosis. The quantized level of t -score used in background knowledge can cover all the same collocation patterns with the same quantized level and thus can improve the recall rate. Furthermore, induction using traditional word form-based background knowledge is time-consuming. In the proposed experiment, it took an average of 24 seconds to finish the inference using the quantized t -score background knowledge, compared to average 87 minutes and 3 seconds using the TW background knowledge on an Intel Core 2 Duo machine at 2.00GHz with 2Gb RAM.
For Table II, the parameter settings for the experiment based on the quantized t -score background knowledge were:  X  =1,  X  = 0.00001, m = 5, and beam size = 2 and the parameter settings for the experiment based on the traditional word form-based background knowledge were:  X  =1,  X  = 0.01, m = 5, and beam size = 1.

Table III further shows the results of pFOIL algorithm using quantized t -score back-ground knowledge on various values of  X  ,  X  , and beam size for the lexical error prob-lem. In this evaluation, penalty (  X  : weighting for penalty) improved the performance. And different values for the stopping threshold  X  affected the performance, especially for the recall. Different beam sizes made a slight difference on the performance be-tween different folds, but better performance was not guaranteed when the pFOIL algorithm used the same  X  and  X  setting with larger beam sizes.

The next experiment is to compare the pFOIL algorithm with other baseline sys-tems and evaluate the impact of the quantized t -score and relation pattern background knowledge for all error/correct types. Table IV lists the experimental results of this ex-periment in the case where QR denotes the proposed quantized t -score and relation pattern background knowledge/features, and TW contains the bi-gram, bi-POS, POS, and word form pair as the background knowledge/features.

Relation pattern and quantized t -score background knowledge was more effective than traditional word form based expression for the pFOIL algorithm in recall, precision, and F1-score. The experimental result indicates that relation pattern and quantized t -score background knowledge significantly improves the recall, precision, and F1-score, even for other baseline approaches. Furthermore, the maximum entropy model trained using relation pattern and quantized t -score background knowledge outperformed these baseline systems. The Naive Bayes classifier was a simple conditional model and is more suitable when the dimensionality of the inputs was high. The Maximum Entropy classifier estimated the conditional distribution using a maximum entropy principle and integrated a Gaussian prior to overcome the data sparseness problem that could happen in a Na  X   X ve Bayes classifier. The C4.5 decision tree employed normalized information gain as the splitting criterion and could tolerate missing feature values. But the C4.5 repeatedly sorted nodes in the decision tree resulting in lower efficiency of memory usage. The pFOIL algorithm retained various relations among words in sentences, utilized substitution checking predicates with various values of arguments to improve precision rate, applied probabilistic concept to search and check overlapping and incomplete background knowledge, and adopted a penalty to maximize the boundaries between classes. Finally, the evaluation results indicate that the pFOIL-based approach outperformed the other baseline systems.
In Table IV, the pFOIL algorithm parameter settings for the experimental result based on the two types of background knowledge were:  X  =1,  X  = 0.00001, m = 5, and beam size = 1. When the pFOIL algorithm used the relation pattern and quantized t -score background knowledge in this evaluation, different beam sizes caused a very slight change in the performance. Furthermore, a better performance and lower stan-dard deviation was reached with a smaller threshold  X  for the stopping condition and larger weighting parameter  X  .

Table V further shows the evaluation of the pFOIL algorithm and baseline systems for different error types. It can clearly be seen that the pFOIL algorithm that used the relation pattern and quantized t -score background knowledge achieved the best F1 score for all error types. The average experimental result for maximum entropy was comparable to that of C4.5. It was found that the simple Na  X   X ve Bayes obtained the worst results and the largest standard deviations between different fold valida-tions in an F -score for all error types. Additionally, lexical choice error diagnosis was the most difficult task and had the largest standard deviations between different fold validations in an F -score and precision rate for all systems.

This study used a paired, two-tailed t -test to determine whether performance dif-ferences among these methods were statistically significant. In Tables IV and V, a * indicates a statistically significant difference ( p &lt; 0.05) and a + indicates a statisti-cally significant difference ( p &lt; 0.01) from the pFOIL algorithm using the same type background knowledge. For F1-scores in different error types, and for average recall, precision, and F1-score in all error types, the pFOIL algorithm that used the relation pattern and quantized t -score background knowledge was statistically significant dif-ferent from the maximum entropy classifier that used the same type of background knowledge ( p &lt; 0.05). It can be seen that the relation pattern and quantized t -score background knowledge can improve performance confidence in all systems. For lexi-cal choice errors, the pFOIL algorithm that used the relation pattern and quantized t -score background knowledge was significantly different from all the other baseline systems that used the same type of background knowledge. This study also compared the pFOIL with different types of background knowledge. For average recall, preci-sion, and F1-score, the pFOIL algorithm that used the relation pattern and quantized t -score background knowledge was significantly different from the pFOIL algorithm that used traditional word form based background knowledge ( p &lt; 0.01).
An interesting finding is that quantized t -score background knowledge of quantized t -score of the word followed by the POS (namely s W POS Tscore ) and bi-POS back-ground knowledge (namely s Bi POS POS , one type of relation pattern background knowledge) is very powerful for error Chinese sentence diagnosis. The CKIP Chinese word segmentation system is based on the segmentation and POS standard described in [Chen and Liu 1992; CKIP 1993, 1998; Huang et al. 1997] to produce Chinese word boundary and POS category. In CKIP Chinese word segmentation system, POS  X  T  X  contains De ( for  X  ) , particles and interjections. A particle almost appears at the end of a sentence and an interjection normally appears at the start of a sentence. The POS  X  ASP  X  indicates tense marker and usually immediately appears after a verb. For in-stance, the POS of  X   X   X ,  X   X   X ,  X   X   X , and  X   X   X  X s X  T  X  X ndthePOSof X   X   X  X nd X   X   X  X s  X  ASP  X . Such distinctive POS function words enhanced the power of the two types of background knowledge. For instance,  X   X   X   X  X  X  X   X   X  X  X  X  X  X  X   X  the error sentence lacks a verb word before the POS  X  X SP X  (the POS of the word  X   X   X ). Take another instance to illustrate. The sentence  X   X  X  X  X  X  X  X   X  X  X   X   X   X   X   X  (She has a very beautiful cloth shoe)  X  lacks a POS  X  T  X  X fter X   X  X   X  and should be  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X . The back-ground knowledge of two continuous word concepts (namely s Bi Cept Cept , one type of relation pattern background knowledge) is useful for diagnosing redundancy errors. There are some fixed conjunction pairs in a Chinese sentence, such as  X   X  X  X  ....  X  X  X  (Because . . . so)  X  X nd X   X   X  ...  X  X  X  (nowthat...since...)  X . Therefore a quantized t -score background knowledge of conjunction-conjunction collocation could be a solution for di-agnosing the error sentences  X   X  X  X  X  X  X  X  X  X  X   X   X   X  X  X  X  X  X  X  (I will be definitely coming, now that you have invited me)  X  X nd X  the newspaper contained important news, he saw all news in the newspaper.)  X . For En-glish, whether the noun starts with an uppercase or lowercase letter and whether the word begins with a vowel or consonant can affect previous article word choice. There-fore, the error in the two sentences  X  This is a error X  and  X  Mary is from the China.  X  could be diagnosed but similar relations could not be found for Chinese sentence diag-nosis in this study.

One more interesting observation was that the pFOIL-based model inferred from traditional word form, relation pattern and quantized t -score background knowledge achieved the same performance as the pFOIL based model inferred from relation pat-tern and quantized t -score background knowledge. But for the pFOIL based model in-ferred from traditional word form, relation pattern and quantized t -score background knowledge took more than decuple times to finish the inference on an Intel Core 2 Duo machine at 2.00GHz with 2Gb RAM.

In addition, this study also explored the other background knowledge. For instance, a score for the bi-gram language model in a sentence is computed, normalized by the number of words, and then quantized with discrete values. It means that all bi-grams in a sentence are represented by background knowledge with a quantized score. How-ever, the background knowledge does not seem to be very useful for Chinese error diagnosis.

Grammatical relation parser and word concept extraction are based on the Chinese word segmentation results which are highly related to the diagnosis results. Since the number of words and the boundary of words in an error sentence may be different from that of a correct sentence, the standard of assisted resources will affect background design and human error type judgment. For instance,  X   X  X  X  X   X  are two words based on the word segmentation standard used in the CKIP Chinese word segmentation system, therefore,  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X   X  word in a sentence then  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X   X  an omission error. Then, the two types of background knowledge s W POS Tscore and s Bi POS POS would not be a good solution for error Chinese sentence diagnosis. CSL learners also have word order problems when learning Chinese [Jiang 2009]. For instance, the error sentence  X   X  X  X  X  X  X  X  X  X  X  X   X   X   X   X   X  (Every day I have classes until two o X  X lock in the afternoon.)  X  X hould be  X  ture, this study can be extended to Chinese word order diagnosis. The experimental results showed that the pFOIL approach was suitable to describe, infer, classify and diagnose expressive, complex, heterogeneous, and multiple logic data. Therefore, the proposed approach can also be applied to other applications such as error diagnosis for different genres, domains and intelligent tutoring. One major advantage of the proposed approach is that it is language-independent and thus can be applied to other languages. This study has presented an error diagnosis approach to assisting non-native speak-ers in Chinese language learning. The presented approach first diagnoses the errors in a sentence based on the pFOIL algorithm which estimates the occurrence penal-ized probability of the matched error or correct sentences using the first-order logic expression and a penalized log-likelihood function. The decomposition-based testing mechanism introduces numerous possible error diagnoses and helps improve the di-agnosis performance. Furthermore, experimental results reveal that the pFOIL algo-rithm is more reliable than other approaches in classifying errors in a sentence. An investigation on the knowledge representation of sentences explores the quantized t -score and relation pattern background knowledge in the first-order logic of sentences. The quantized t -score and relation pattern background knowledge is extracted to ef-fectively represent a sentence for CSL error sentence diagnosis. Overall, it can be said that the experimental results show that the presented approach significantly improves the performance for error cause identification.

