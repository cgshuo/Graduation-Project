 Today, reporting is an essential part of everyday business life. But the preparation of complex Business Intelligence data by formulating relevant queries and presenting them in meaningful visualizations, so-called reports , is a challeng-ing task for non-expert database users. To support these users with report creation, we leverage existing queries and present a system for query recommendation in a reporting environment, which is based on query matching. Target-ing at large-scale, real-world reporting scenarios, we propose a scalable, index-based query matching approach. More-over, schema matching is applied for a more fine-grained, structural comparison of the queries. In addition to inter-actively providing content-based query recommendations of good quality, the system works independent of particular data sources or query languages.

We evaluate our system with an empirical data set and show that it achieves an F 1 -Measure of 0.56 and outperforms the approaches applied by state-of-the-art reporting tools (e.g., keyword search) by up to 30%.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  information filtering, selection process Query Matching; Query Recommendation; Business Intelli-gence
Today, the collection and processing of large amounts of data has become important in various fields. In line with this is the need for extracting and preparing the often com-plex data for analysis and presentation; and, reporting has become an essential part of everyday business life, recently. For that reason, however, reporting is more and more done by business experts themselves (i.e., instead of database ex-perts) [13]. For them, reports represent a means to visualize various facts and complex calculations on their specific data for presenting it to others in everyday business.

Reporting thus makes out an integral part of business. Es-pecially for non-expert database users, however, the creation of reports represents a major challenge. In particular, the report query must cover the relevant Business Intelligence (BI) data and therefore include more complex constructs such as filters and groupings in SQL. These specifics of the data are then to be reflected in appropriate visualizations, which convey the knowledge as intended.

To ease the complicated task of report creation, the reuse of reports (i.e., applying the same visualization or query on different data) has become common practice. People look for and try to adapt existing queries or copy visual-izations of colleagues. However, doing so, they encounter several problems. The first challenge are the large amounts of existing reports, where the finding of relevant samples to adapt is hard. It is also not clear how the comparison of reports and queries in different formats and languages X  X o find good samples X  X an be done independently of specific tools. In addition, semantic differences and inconsistencies between queries need to be considered. There are, for ex-ample, different design possibilities to describe semantically equivalent concepts. Moreover, the data a query describes may be domain-specific, which is reflected in the language used, too. It is not possible to cope with these syntactic and semantic differences without a fine-grained and comprehen-sive consideration of the queries.

Today X  X  reporting tools provide only few means to alle-viate the difficulties of the users; for instance, with visual query builders [2]. Additionally, they comprise add-ons such as keyword search [2] or auto completion mechanisms [3, 2], and recommender systems are investigated [9, 16]. Never-theless, predefined example queries are used in practice [6, 4].

In this paper, we take up this practice of the users and propose a system for automatic recommendation of proven existing queries, which is integrated in the reporting envi-ronment Remix [22]. In this way, we encourage collabo-ration and sharing of reports, motivate reuse, and provide substantial support for the everyday reporting tasks in busi-ness. When a user is preparing a data source, Remix com-pares the query of the user to queries from existing reports and recommends similar queries and corresponding visual-izations to the user. The key contribution of this paper is the automated comparison of queries, the so-called query match-ing . Instead of a simple syntactic comparison (e.g., keyword or text search) as it is applied by today X  X  reporting envi-ronments, Remix provides a combined, content-based (i.e., focusing on the information contained within the queries) approach for matching queries. Working on an abstract rep-resentation of the queries, our system is independent of spe-cific report formats that use particular query languages. To handle the large numbers of reports to be considered in real-world reporting environments, our system first uses a scal-able filtering technique that applies a coarse-grained syn-tactic query comparison. The specific, nested structure of report queries is subsequently addressed with a fine-grained matching step based on schema matching.
 We implemented this query matching approach in the Remix prototype and evaluated it on two empirical datasets to demonstrate that Remix interactively provides query rec-ommendations of good quality (i.e., it correctly identifies queries to be recommendations and ranks them accordingly). In particular, it achieves an F 1 -Measure of 0.56 and with up to 30% outperforms the approaches applied by state-of-the-art reporting tools.

The paper is structured as follows: Section 2 describes a motivating example of reporting in Remix, and Section 3 gives an overview of related work. We present the query matching part of the Remix system in Section 4 and subse-quently describe the evaluation in Section 5. Finally, Sec-tion 6 concludes the paper. In this section, we describe a motivating example of using Remix to support query formulation. Consider the follow-ing SQL query, from now on called original query , which was created in a real reporting environment and looks for the revenue per industry (selected with column BRAN1 ) of company org.ai in Chicago. It might be issued by a new employee who wants to create a corresponding report. Note that the names used are rather cryptic and one has to be familiar with the database (which can contain several tables with hundreds of columns) or browse through it to formulate such a query. Also note that we restrict the ex-amples in this paper for reasons of space and clarity X  X ueries of real reports are usually much larger (e.g., a word count of 550 is not unusual).

Assume now, though being entirely valid, the above query does return an incomplete result obviously missing tuples. If similar queries were created in the company before (e.g., in other departments) and stored within the system, Remix X  upon request X  X ould provide them as recommendations. Two such recommendations are given below.
 Although these recommendations consider different sections of the company X  X  database, they are intuitively similar to the original query. For instance, the first query contains a hint of how the filter condition should be adapted in order to include all the relevant data. Obviously, the spelling of city names varies within the database of org.ai. The second recommendation shows that there is a column GROSSREV-ENUE in the database, which our employee decides to in-clude in his report, too. The result of adapting the original query is the following.

Finally, Remix would recommend a suitable visualization to our employee and integrate the different components into a complete report.

Note that the tasks for remix are complex. First, it has to cope with heterogeneous queries of arbitrary reports (e.g., in different formats) and to process the possibly large numbers of queries efficiently. Also, the matching should be fine-grained and concentrate on different kinds of information contained within the queries. For instance, a simple keyword search for  X  SYS BIC X . X  X rg.ai/CA AI RELATIONSHIP X  would not retrieve the second recommendation, because the latter does not contain exactly this relation name. However, the recommendation is definitely similar to the original query and, if included, would improve the quality of the result.
In summary, the challenges for a query recommendation system for Remix are to process the (1) heterogeneous and (2) large numbers of report queries in the repository effi-ciently in order to (3) provide content-based query recom-mendations of good quality. Thus, our requirements extend those of existing systems, which are described in the next section.
In this section, we describe related work in the fields of content-based query recommendation, auto completion, and matching. We further review a matching technique that can be applied for matching queries.
The automated recommendation of similar queries is a very recent area of research. Database management systems (DBMS) generally do not offer such functionality. Neverthe-less, they often maintain query repositories [4, 6] with well-defined query examples or query logs [2] where the user can manually search for relevant queries X  X ithout being pointed to selected queries.

In [15], Khoussainova et al. describe a query log with en-hanced browsing functionality, an extended keyword search; but the identification of relevant queries in the log, which is based on rather low-level matching techniques (i.e., the functionality provided by the underlying DBMS), is only re-ferred to marginally.

The goal of the QueRIE recommender system [9] is to de-termine queries in the log that fit into the session of the cur-rent user. Thereby, the query similarity is computed by com-paring the parts of the database addressed by the queries. However, though considering the queries on a more fine-grained level by parsing and decomposing them, QueRIE basically performs a string comparison.

Similar methods are applied by content-based recommen-der systems for multidimensional databases, which consider properties of the query as a whole (e.g., result measures, the selection, etc.) and compute the similarity value for two queries by using string-based similarity measures [16]. Note that, in this context, Marcel et al. [16] raise an important issue by stating that the quality of the recommendations needs to be assessed based on real users and cases, which has not been done, to date.

As with query recommendation, query logs are consid-ered as basis for recommendations by auto completion ap-proaches, which are another kind of service provided on top of DBMS.
In contrast to recommender systems, which recommend entire queries, auto completion systems focus on recom-mending query parts. Several commercial systems provide auto completion regarding tables or columns [3, 2], but more elaborate completion approaches are still subject of research.
Yang et al. [24] automatically complete a query with join operations based on the requested attributes. The paths of joins are found by mining the query log and applying a predefined quality measure (e.g., the share of queries in the log that contain the selected path).

SnipSuggest [14] is an interactive query completion sys-tem, which takes various parts of the query (e.g., tables and selection conditions) into consideration. By keeping the queries from the query log within a single graph structure capturing structural relations between the queries, valid and probable completions can be recommended to the user.
FIMIOQR [13] is an auto completion system for multi-dimensional queries. Similar to the approach of SnipSug-gest, its recommendation of query parts for a partial query is based on mining the query log for the parts that most frequently complement those of the partial query.
Query matching has been studied early in the area of query optimization [12]. However, these approaches are usu-ally exhaustive X  X nd thus complex X  X omparisons regarding only a single DBMS and rather simple queries (e.g., Select-Project-Join queries). Recent works (e.g., [21, 25]) also ap-ply more efficient methods, but usually focus on physical properties (e.g., the cost of execution) instead of the intent of the queries.

Apart from the fields already mentioned, different kinds of queries (e.g., keywords or natural language sentences) are completed, compared, and translated in several areas such as keyword search [18] and natural language interfaces to databases [17]. However, such queries are of a nature very different from complex report queries.

The approaches presented above usually address the spe-cific syntax and semantics of queries by parsing them, and often also normalizing them, before processing. Instead of considering the query parts individually, the auto comple-tion approaches described explicitly take into account struc-tural similarities between queries by using graphs for query representation. However, these graphs would be consider-ably greater in size and more complex if more heterogeneous queries of arbitrary complexity and data sources were con-sidered. Also, the comparison of queries is simplified in the context of a single DBMS, because term equality can be used for a straightforward comparison of query parts that never-theless leads to results of good quality. Hence, we need to consider further methods for Remix.
To obtain a more general and at the same time structural comparison of the queries, we consider schema matching. Schema matching is a research area in the database commu-nity, which contains a large body of work on various match-ing approaches (e.g., based on linguistic methods, structural analysis, or domain knowledge). Further, there are several matching systems [8], which could be applied for matching queries, too. However, for query matching the existing sys-tems need to be adapted: the missing full automation of the matchers, which is caused by a lack in preciseness and completeness, and their ignorance of the semantics of the query language would otherwise lead to unsatisfactory re-sults. W.r.t. the scalability required for Remix, the usually quadratic (or larger) complexity of schema matching needs to be addressed, too.

Section 2 described the challenges for a query matching system to interactively provide content-based recommenda-tions of good quality within Remix. However, this section showed that the existing approaches usually are either ef-ficient (for interactivity) or precise (for quality). For that reason, we combine the methods applied by state-of-the-art approaches, extend, and augment them, for creating a query recommendation system for Remix. This is described in the next section.
In this section, we present the query recommendation part of Remix, which is based on a combined query matching ap-proach. Remix itself is a reporting environment with spe-cial focus on report recommendation and integration, as de-scribed in Section 2. It maintains a report repository, where it stores reports created in the past and also separates the different components of the reports: links to data sources, queries regarding such data sources, and visualizations of these queries. Upon request, it matches a given query of a user to the queries in the repository and provides the most similar queries as recommendations to the user.

This query matching procedure is shown in Figure 1. Its input consists of the user-query, and the system itself con-tains the set of possible query recommendations. First, the queries are pre-processed to abstract from specific query lan-
Query REL NAMES PRO ATTRS SEL ATTRS q r r
Figure 1: The query matching approach of Remix guages. This pre-processing has the goal to prepare and fa-cilitate the matching phase. In order to enable a comparison, it parses queries in different source formats and languages and translates them into a common representation based on relational algebra. Note that the current implementation of our system supports only SQL queries, though.

The main processing is then performed by a combina-tion of two matching algorithms, also called matchers . In particular, we apply the Feature Matcher and the Schema Matcher . The former only applies a coarse-grained com-parison of the queries and serves as scalable filter for the possibly large amount of queries. The latter, in contrast, performs a fine-grained and, in particular, structural com-parison to capture subtle differences and similarities between the queries.

The output of the system is an ordered subset of the best recommendations. In the following, we give details about the two matchers.
The Feature Matcher is applied in the beginning of match-ing in order to filter out queries that are not relevant for the final recommendation, because they are not similar to the original query. It is based on similarity search , a matching technique that, given a set of objects, looks for the most sim-ilar objects (i.e., queries, in our case). In this context, we introduce features , the characteristics of the queries that are compared by the matcher. Then, we give a detailed overview of the comparison and subsequent similarity calculation for the queries, the actual query matching.
For a coarse-grained comparison of the queries, the Fea-ture Matcher concentrates on general characteristics of que-ries, which can be extracted for every query (e.g., the data sources it addresses, the functions it applies, etc.). We ex-tract these properties from the queries and record them tex-tually as so-called features , as it is demonstrated in Exam-ple 1. A complete overview of the features considered in Remix is given in Appendix A.

Example 1. Consider the first version of the query given in Section 2. Table 4 shows several features (i.e., the names of the relations addressed and those of the attributes in the projection and selection) we extract from it ( q 0 the two recommendations ( r 1 and r 2 ) proposed for q 0 . Since we record the features of the queries textually, espe-cially those of the queries in the repository, and because we use equality as comparison criterion, we can store them eas-ily after extraction and apply index search for comparing them.
We apply similarity search as a textual comparison of fea-tures such that it can be realized with an inverted index [7], a data structure that generally supports fast full text search on large databases (e.g., a set of documents). With this application of index search, which achieves logarithmic complexity, the Feature Matcher efficiently realizes the com-parison of the extracted features and can be used as scalable filter for queries not relevant for the final recommendation. For that, the features of the queries to be compared are extracted and stored separately, with references to the re-spective queries, in an inverted index. For the comparison of two queries, the index then is queried by transforming one of the two queries in a search query for the index. This is demonstrated in Example 2.

Example 2. Below, q 0 has been transformed in a query for index search. The index considered uses the three features REL NAMES, PRO ATTRS, and SEL ATTRS for index-ing. This indexing of the repository queries is illustrated for r , which is shown how it is represented in the index. q : REL NAMES : SYS BIC.org.ai/CA AI RELATIONSHIP OR PRO ATTRS : REVENUE OR PRO ATTRS : BRAN1.description OR SEL ATTRS : LOC01 r : REL NAMES : SYS BIC.org.ai/CA AI RELATIONSHIP PRO ATTRS : PROFITMARGIN,BRSCH.description SEL ATTRS : LOC01,LOC01,LOC01
Since the index-based comparison is based on the text recorded in the features, the function determining the simi-larity between two queries has to concentrate on this infor-mation. We consider an extended version of term frequency-inverse document frequency (TF-IDF), the approach com-mon in text search, and, as a basis, use the formula described in detail in [5], which is applied in Apache Lucene [1], a li-brary for index search. Further, we adapt and extend the approach of Apache Lucene in three ways, which has been shown to be of advantage [23]: Example 3 demonstrates the similarity calculation.

Example 3. Consider again the queries q 0 , r 1 , and r In this example, a similarity value is computed for the queries r and r 2 regarding q 0 (i.e., we assume, our index contains only these three queries). For the corresponding formula f score , refer to [5]. First the normalizing factor, common for all values, is calculated as qN orm ( q 0 ) = 1 p Then, the individual values are calculated as for the two examples below: f score ( q 0 , r 1 ) = 2 f score ( q 0 , r 2 ) = 1
Similarly, such values can be computed for all queries stored within Remix (i.e., q 0 , r 1 , and r 2 , in our case) leading to the matrix shown below. Note that the values calculated above correspond to the entries in the first row of the matrix. After adapting the matrix regarding the interval [0..1] (i.e., all values of a row are divided by the maximum of that row) and taking the average of all matrix entries that should be symmetric, the matrix looks as follows.
Note that the normalization of the values described above would not be necessary since we use the Feature Matcher in Remix only as filter. Hence, the ranking (obtained by function f score ) would be sufficient for our purpose, which is to forward the best ranked queries as recommendations to the Schema Matcher . Also, the normalization requires the original query to be contained within the index. Thus, the index has to be recreated if this is not the case. Nevertheless, the normalization has been shown to be clearly beneficial for the quality of our system.

In particular, a query r is eliminated and not consid-ered further as recommendation for a query q if the sim-ilarity value calculated by the Feature Matcher for a pair of queries ( q , r ) is 0. In addition, the Feature Matcher can be configured with one of two filter-parameters, maxN or threshold. Configured with a maxN n , it ranks the queries after calculating the preliminary similarity values as shown in Example 3 and forwards the best n queries to the Sche-ma Matcher . Configured with a threshold t (e.g., obtained using training data), it forwards only those queries to the Schema Matcher for which the computed value is greater than or equals t . Note that for the latter, the normalization has to be carried out.

The Feature Matcher serves well to reduce the large set of possible recommendations in the system repository. How-ever, the features only partially capture the syntactic infor-mation contained in the queries. Especially, structural re-lations regarding individual parts of the query (e.g., nested selection conditions or properties of subqueries, which count to those of the surrounding query) are not reflected and thus not considered during similarity calculation, because every feature is recorded only once and in a rather simple textual form. In order to take also more subtle differences and sim-ilarities between queries into account, we additionally ap-ply the Schema Matcher , which explicitly regards the query parts in the context of the structure.
The Schema Matcher is applied for a more fine-grained comparison and also determines the order of the final recom-mendations by calculating similarity values. It is based on different methods from schema-matching, which compares tree structures. Therefore, we consider the parse trees of the queries as the information to be matched. For compar-ing these often deeply nested structures of report queries, schema matching algorithms, also schema matchers are par-ticularly suitable.
The schema structure taken in this work, in essence, is the parse tree of the queries. It is based, however, on the abstract representation. The schema we create is further augmented by making additional information (e.g., the name of the query and data types) explicit. For instance, the schema for q 0 is displayed below.
 Schema q 0 [ComplexRelation] -Projection[Projection] -REVENUE[Attribute] -BRAN1.description[Attribute] -Group[Group] -BRAN1.description[Attribute] -aggregationAttributes[List &lt; ComputedAttribute &gt; ] -Sort[Sort] -REVENUE[Attribute] -Selection[Selection] -condition[BinaryCondition] -SYS BIC.org.ai/CA AI RELATIONSHIP[Relation]
Since the schemas we create for the queries contain dif-ferent sorts of information, the Schema Matcher applies not only one but several distinct schema matchers for comparing the queries.
The application of schema matching especially targets the nested structure of report queries. Schema matching algo-rithms, so-called schema matchers , map the elements of two schemas that correspond semantically to each other. For that, the cartesian product of element-to-element similarity values v , with v  X  [0 .. 1], is computed between the schemas. It constitutes a matrix of pair-wise similarity values. The values in this matrix determine the degree of correspondence between the pairs of elements. Elements that correspond to each other are called matches for each other.

For such a structural comparison of two queries, the Sche-ma Matcher makes use of several different schema matchers by applying an entire schema matching system, the Auto Mapping Core (AMC) [19]. The latter executes various schema matchers in parallel and aggregates their results based on different strategies. In preliminary experiments [23], a specific combination of three matchers, which are de-scribed below, turned out to best suit our query matching approach. All three of them are based on the Name Matcher, which compares the names associated with the schema ele-ments and  X  X atches schema elements with equal or similar names X  [20] by applying, for example, established string sim-ilarity algorithms [10] or considering synonyms.
 Weighted Name Matcher The Weighted Name Matcher Children Matcher The Children Matcher compares two Leaf Matcher The Leaf Matcher matches two elements by
Note that the matchers described above represent a sub-stantial improvement compared to the Feature Matcher , which only considers term equality. They would, for example, detect a correspondence between the two relation names  X  SYS BIC X . X  X rg.ai/CA AI RELATION MONITOR X  and  X  SYS
BIC X . X  X rg.ai/CA AI RELATIONSHIP X  considered in the ex-ample of Section 2. Moreover, the Children and Leaf Matcher regard relations between different query parts. Also note that we take into account the issues to be considered when applying schema matching to queries described in Section 3. By applying a combination of different matchers, the defi-ciencies of individual matchers, which only focus on a single characteristic of the schemas, can be diminished, and even an automated application leads to convincing results [23]. Further, the pre-processing helps to overcome the missing awareness of query semantics. And since we apply a scal-able filter in advance, the number of match-tasks for the complex Schema Matcher can be kept constant.
 S chema q 0 [ComplexRelation] -Projection[Projection] -Selection[Selection] -S YS. . .
 Figure 2: An extract of the mapping of the Weigh-ted Name Matcher for queries q 0 and r 1 Figure 2 shows (an extract of) the result of applying the Weighted Name Matcher to compare q 0 and r 1 . We compute such a mapping using the three matchers described above and then aggregate the mappings of the individual matchers by taking the average value for each pair of elements between the two query schemas (e.g., for mapping q 0 and r 1 , we take the average for each pair where the first element is in the schema of q 0 and the second in the schema of r 1 ), as it is proposed in [11]. In order to match only relevant element pairs, we further consider only those pairs with a similarity greater than a so-called selection threshold [11] and set the remaining values to 0. This is demonstrated in Example 4.
Example 4. The below matrices show an extract of the mapping between q 0 and r 1 obtained by applying the Weigh-ted Name Matcher before (top) and after filtering the values with a selection threshold of 0.7, which was shown to be of advantage in [23].
Nevertheless, the resulting mapping of the query schemas still consists of several values instead of one value expressing the over-all similarity of the two queries. For that reason, the individual values are combined into one single value using a (slightly) adapted version 1 of the function for a combined schema similarity described in [11]. The resulting function basically takes the average of all similarity values greater than 0 and then takes the share corresponding to the el-ements that are matched w.r.t. all elements of the two schemas. Example 5 demonstrates its application.

Example 5. Consider again Example 4, for the result of the Weighted Name Matcher after the application of the selection threshold. Within that matrix only 4 entries are matches. In addition, there are 14 other matches, which are not displayed due to space restrictions. Apart from one match with a value of 0.75, all matches have a value of 1.00. Further, both the LOC01 and the EQ element in the schema of q 0 are matched three times, and the SUM element twice. All other elements appear in only one match. Thus, 13 el-ements of the source and 18 elements of the target schema, take part in the mapping. Let E q 0 and E r 1 denote the sets of elements in q 0 and r 1 , respectively, with | E q | E r 1 | = 32 . Then, the corresponding similarity value for the queries is computed as: ssim ( E q 0 , E r 1 , M q 0 ,r 1 ) = 17  X  1 . 00 + 1  X  0 . 75
After limiting the set of possible recommendations with the Feature Matcher and applying the three schema match-ers within the Schema Matcher the query recommendation system of Remix would suggest the recommendations or-dered as follows.

In this section, we present an extensive, empirical evalua-tion of the query matching and recommendation in Remix. To this end, we implemented the query recommendation sys-tem prototypically in Java using amongst others the Apache Lucene Search Engine Library [1] and the schema matching system AMC [19] for the matching functionality. Moreover, we compared our approach, Combined Matcher in the fol-lowing, to the methods applied by state-of-the-art tools by considering queries from real applications rated (w.r.t. their qualification as recommendations), amongst others, by real people.
In particular, our experiments aim to show the following:
Specifically, we consider the matches as being undirected (i.e., the computed matrix of similarity values is supposed to be symmetric) and instantiate the function with a selection function that takes the average of the similarity values of all match candidates.
The effectiveness of the individual approaches is shown by comparing the results they delivered to the results of two baseline approaches, the String Matcher and the Text Matcher . In particular the String Matcher , which computes the similarity value by relating the size of the largest com-mon substring of two queries to the one of the longer of them, represents the keyword search applied in state-of-the-art DBMS. The Text Matcher maintains an index and per-forms general text search thereby calculating the similarity values based on TF-IDF.
 To measure the quality of matching, the recommendations R determined by a system are to be compared to those classi-fied as recommendations in a reference set R 0 . According to Do [11], there are three subsets describing the recommenda-tions that were identified correctly, I , those that were falsely classified as recommendations, F , and the recommendations that were missed, M . Based on this classification, we con-sider the following three quality measurements, all ranging between 0 and 1, with 1 representing the best value: Precision describes the accuracy of the result and is de-Recall describes the completeness of the result (i.e., the F -Measure is the harmonic mean of precision and recall
For finding the best recommendations, however, it suffices to regard the (sub)list of actual recommendations of a sys-tem. Hence, we consider also the k topmost results using the so-called precision@k [7].
 Precision@k describes the accuracy of the result regarding
The evaluation results presented are usually the average values for all recommendation tasks in both datasets (i.e., naturally, the number of recommendation tasks corresponds to the number of queries in a dataset). In addition, we considered scalability. Therefore, we measured the runtime during the evaluation executing all matchers on a usual office PC using an Intel Core 2 Duo, four gigabyte RAM and a Java 6 (32-bit) environment.
We evaluate Remix over two datasets with rather different characteristics, shown in Table 2. To measure the quality of query matching, our empirical datasets do not only have to contain an appropriate number of real-world queries, but must also contain details about the similarity between these queries. According to Marcel et al. [16], such datasets did not exist at the time of our experiments. Hence, we con-ducted an empirical study [23] where database users rated the similarity of query pairs on a scale of 1 to 4.
To get an overview of the characteristics of the individual matchers, we first considered queries extracted from an SQL tutorial as dataset SQLT, which we tagged by ourselves. These queries contain rather different features of SQL and have very different intends. Most of these queries are about the business domain.

For the SDSS dataset, we chose a random sample of que-ries extracted from the query log of the Sloan Digital Sky Survey (SDSS) [6]. To obtain the corresponding similar-ity values, we conducted an empirical study. The SDSS maintains a large open database containing scientific data about the universe. Its query log contains millions of SQL queries issued by different users and also by bots. As prepa-ration, we removed query duplicates, queries too long to be grasped by the survey participants in an acceptable amount of time (i.e., queries consisting of more than 350 charac-ters), and erroneous queries, and took 150 queries as basis for the SDSS dataset. We further determined query pairs that neither query the same table or a corresponding view nor use common attributes as not to be similar. This re-duced the original 22,500 similarity values to be found to about 9,000. In total, 121 people participated in the survey. About 50% of them reported to have basic SQL knowledge, 40% had deeper, and 10% only rudimentary knowledge of SQL. On average, the participants rated 82 queries. Con-versely, the retrieved similarity values were averagely rated by 1.2 people, who in about 70% of the 1,307 cases with mul-tiple votes agreed in their rating. Altogether, the study lead to a dataset of 150 queries with similarity values between all of them.

Although the datasets described above consider distinct domains and contain queries with rather different charac-teristics, they have to be regarded critically; certainly, they cannot reflect the diversity of the real world. And also the similarity ratings, though retrieved empirically, are subjec-tive by nature. Nevertheless, the data is suitable to show the quality and performance of our combined query match-ing approach compared to the methods applied by state-of-the-art tools. The results of this comparison are given in the next section.
In this section, we first present the evaluation results re-garding the quality of the recommendations and then give an overview of the results concerning the scalability of the individual approaches.
First, we configured our system using one third of the sample data. This preliminary evaluation turned out that it achieves best results with a filter-parameter of n = 20. Re-call, this means that the Feature Matcher ranks the queries after calculating preliminary similarity values and forwards the best 20 queries to the Schema Matcher . The latter re-calculates the similarity values for the 20 queries, and the similarity values of the remaining queries, which are not con-sidered by the Schema Matcher , are set to 0.

It turned out that all approaches considered have a very similar performance w.r.t. the top k recommendations. An overview of pr@5, pr@10, and pr@15 is given in Figure 3. Though the similarity of the results, only the Sche-ma Matcher and the Combined Matcher achieve a pr@k greater than 0.6. The Feature Matcher shows a better qual-ity than the baseline approaches, but the differences seem to be marginal; all values range between 0.5 and 0.6. This induces that with all matching approaches, about 50% of the top five recommendations are correct recommendations. 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 F igure 3: The evaluation of the query matchers w.r.t. recommendation quality 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 Figure 4: The evaluation of the query matchers w.r.t. matching quality
However, Figure 4 shows that precision, recall and F Measure clearly show differences in the matching perfor-mance of the different matchers. Although all of the lat-ter find the correct recommendations, which is indicated by the high recall, their over-all matching performance differs strongly. With F 1 -Measure values of 0.38, 0.47, and 0.56 for the Feature , Schema , and Combined Matcher , respectively, the increase in the value makes up about 10%. With a pre-cision of 0.47, only the Combined Matcher is near 0.5. This indicates that it is the only matcher where at least half of all recommendations determined are useful.
Figure 5 gives an overview of the time the systems need for processing in dependence of the number of queries in the systems. Note that the data confirms the specifications of the complexity of the approaches, stated previously.
Ti me (ms) Figure 5: The evaluation of the query matchers w.r.t. scalability
The curves of the two baseline approaches both develop rather steadily with only a slight increase with growing in-put. Although it has to maintain the terms, the Text Matcher requires less time than the String Matcher (not shown here)  X  X y using an index, it achieves logarithmic complexity. Similarly based on an index, the Feature Matcher shows the same development. However, since it is based on the ab-stract representation of the queries, the pre-processing re-quired by the approach leads to an increase of processing time. Also the quadratic complexity of the schema match-ers is reflected by the time they need for processing, which even increases along the quadratic parable. Finally, the Combined Matcher shows a logarithmic curve, too. Since the filter-parameter n sets a bound for schema-matching, its complexity is based on the indexing of the filtering.
In this paper, we presented a query recommendation sys-tem for the reporting environment Remix, which is based on query matching. Our system is motivated by the increas-ing complexity of reporting in Business Intelligence and the fact that this is often done by non-expert database users, today. To interactively provide content-based query recom-mendations of good quality to them, we proposed a hybrid approach of query matching combining an efficient and, in particular, scalable feature-based matcher as preliminary fil-ter for the possibly large set of queries and a fine-grained structural matcher based on schema matching. Moreover, we evaluated our system on empirical datasets containing real queries and similarity ratings obtained from real users. Thus, we showed that it outperforms the methods applied by state-of-the-art applications by up to 30%.

Future work on the system includes to focus on the partic-ularities of report queries (e.g., to consider also multidimen-sional queries and additional information about the reports the queries come from) and to refine the existing matchers. In particular, the comparison of the Feature Matcher cur-rently based on term equality should be less restrictive (e.g., through clustering features with similar values). Further, it would be interesting to consider query semantics also for matching (e.g., to refine the ranking of syntactically very similar queries). [1] Apache lucene. http://lucene.apache.org/. [2] Aqua data studio. [3] Databasespy -multi-database tool and sql editor. [4] Gene ontology. http://www.geneontology.org/. [5] Lucene 3.5.0 api. -class similarity. [6] Sloan digital sky survey. http://www.sdss.org/dr8/. [7] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [8] P. A. Bernstein, J. Madhavan, and E. Rahm. Generic [9] G. Chatzopoulou, M. Eirinaki, S. Koshy, S. Mittal, [10] W. W. Cohen, P. D. Ravikumar, and S. E. Fienberg. [11] H.-H. Do. Schema Matching and Mapping-based Data [12] A. Halevy. Answering queries using views: A survey. [13] R. Khemiri and F. Bentayeb. Interactive query [14] N. Khoussainova, Y. Kwon, M. Balazinska, and [15] N. Khoussainova, Y. Kwon, W.-T. Liao, [16] P. Marcel and E. Negre. A survey of query [17] N. Nihalani, S. Silakari, and M. Motwani. Natural [18] J. Park and S. goo Lee. Keyword search in relational [19] E. Peukert, J. Eberius, and E. Rahm. Amc -a [20] E. Rahm and P. A. Bernstein. A survey of approaches [21] P. Roy, S. Seshadri, S. Sudarshan, and S. Bhobe. [22] M. Seguran, A. Senart, and D. Trastour. remix : A [23] V. Thost. Calculating similarity of arbitrary reports. [24] X. Yang, C. M. Procopiuc, and D. Srivastava.
 [25] J. Zhou, P.- X  A. Larson, J. C. Freytag, and W. Lehner.
In Table 3, this section gives an overview of all features considered in Remix during query matching with the Fea-ture Matcher . With the large selection of features we aim to capture as many characteristics as possible. For match-ing, however, we only consider the features appearing more often within the current set of queries, as it is described in Section 4.
 Feature Description
REL NAMES The names of the relations that PRO ATTRS The attributes in the projection
SEL ATTRS The attributes that occur in
SEL COMP OPS Comparison operations that oc-
SEL SQL OPS SQL-specific condition opera-
SEL PRED OPS Predicates (i.e.,  X  X nd X  or  X  X r X ) that
SEL ARITHM OPS Arithmetic operations that occur
SEL AGG OPS Aggregation operations that oc-
REN REL NAMES The names of the relations that REN ATTRS The attributes that are renamed Feature Description
EXT NEW ATTRS The names of computed at-
EXT COMP OPS Comparison operations used for
EXT SQL OPS SQL-specific condition opera-
EXT PRED OPS Predicates that occur in scope
EXT ARITHM OPS Arithmetic operations that oc-
SORT ATTRS The attributes that are used for
GRO GRO ATTRS The attributes for which a
GRO ATTRS The attributes that occur in
GRO NEW ATTRS The names of the new attributes
GRO ARITHM OPS Arithmetic operations that oc-
GRO AGG OPS Aggregation operations that oc-
JOIN ATTRS The attributes that are used in JOIN OPS The join operations in the query
JOIN COMP OPS Comparison operations that oc-
JOIN SQL OPS SQL-specific condition opera-
JOIN PRED OPS Predicates that occur in scope
JOIN ARITHM OPS Arithmetic operations that oc-
SET OPS The set operations that occur in REN COUNT Count of rename operations JOIN COUNT Count of join operations SET COUNT Count of set operations
FUN The functions that are used in
FUN ATTRS The attributes that are used as
CASE COUNT Count of condition statements SUB COUNT Count of subqueries
VALUES The values that occur in the
Table 3: The query-features considered in Remix
