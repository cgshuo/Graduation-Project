 Using relevance feedback can significantly improve the ef-fectiveness of ad hoc (query-based) retrieval. However, re-trieval performance can significantly vary with respect to the given set of relevant documents. Our goal is to estab-lish a quantitative analysis of what makes a relevant doc-ument a good representative of the relevant-documents set regard less of the retrieval approach employed. That is, we would like to estimate the extent to which a relevant doc-ument can effectively help in finding (other) relevant docu-ments using some relevance-feedback method employed over the corpus. We present various representativeness estimates; some of which treat documents independently and some uti-lize inter-document similarities. Empirical evaluation shows that relevant documents that are centrally located within the similarity space of the relevant-documents set tend to be good representatives. In addition, we show that there exist highly representative clusters of similar relevant documents, and devise methods for ranking clusters based on their pre-sumed representativeness. Finally, we study the connection between representativeness and TREC X  X  gradual relevance judgments.

The ad hoc retrieval task is finding documents in a cor-pus that pertain to information need expressed by a query. The task becomes easier if relevance feedback is available (e.g., examples of relevant documents). Indeed, there is a large body of work on devising retrieval methods that ex-ploit information induced from relevant (and non-relevant) documents [27, 11, 20, 39, 28, 3, 25, 36].

However, some previous work showed that the effective-ness of relevance-feedback-based retrieval can significantly vary with respect to the given set of relevant documents [9, 37, 33]. Specifically, there are many cases wherein using some relevant documents degra des retrieval performance re-gardless of the retrieval method used [37, 33]. Yet, the ques-tion of which relevant documents are more effective than others for relevance-feedback-based retrieval has remained an open challenge.

Thus, we aim to establish a quantifiable characterization of the degree to which relevant documents can be effectively used in relevance-feedback-based retrieval regard less of the retrieval approach that is employed. To that end, we set the following task as a goal. Given a query, and a set of documents relevant to the information need it expresses, we want to automatically identify the best  X  representatives  X  X f the set. We consider a relevant document as a good represen-tative to the extent it can effectively help to find documents from the relevant-documents set via a search performed over the corpus. More specifically, we want to identify a subset of k relevant documents that if fed with the query to some relevance feedback algorithm [27, 28, 3] used to rank the en-tire corpus, then the resultant performance is optimal with respect to subsets of k relevant documents.

We present two paradigms of identifying the most repre-sentative relevant documents. The first is ranking relevant documents using estimates of their representativeness, and selecting the top-ranked ones. Some of the estimates we pro-pose consider documents independently while others utilize inter-document similarities. We also explore a learning-to-rank [22] approach that integrates the estimates.
The second paradigm is based on the premise, which could be viewed as an extension of the cluster hypothesis [35], that representative relevant docume nts are similar to each other. Accordingly, we devise methods for ranking clusters of rel-evant documents based on their presumed representative-ness  X  i.e., the resultant performance of a relevance feed-back method when fed with the cluster X  X  constituent doc-uments. The documents in the highest ranked cluster are then considered as the best representatives.

Experiments performed over TREC corpora using two state-of-the-art relevance-feedback methods show that rele-vant documents that are centrally located within the similar-ity space of the relevant-documents set are often good repre-sentatives. Furthermore, we show that there are highly rep-resentative clusters of similar relevant documents, but auto-matically identifying these is a hard challenge. A learning-to-rank method for clusters is the most effective among those we consider for identifying representative clusters.
Further exploration shows that documents considered as  X  X ad X  representatives by our methods tend to be  X  X oison pills X  [33]; that is, if used alone (with the query) in relevance-feedback-based retrieval, then the resultant performance is worse than that of not utilizing relevance feedback at all.
In addition, we show that levels of representativeness in-duced by our methods are connected with TREC X  X  levels of relevance (relevant vs. highly relevant). Hence, the meth-ods could potentially be viewed as a means for creating (one type) of gradual relevance judgments. However, it turns out that while document-query surface level similarity is among the worst representativeness estimates that we study, it is a highly effective indicator of relevant documents that may be regarded as  X  X ighly relevant X  by TREC X  X  judges.

Finally, it is important to differentiate our goal here from that of devising (i) a relevance feedback method, and (ii) a method for selecting which documents should be presented to the user to provide feedback on [30]. Furthermore, some of the estimates that we present utilize the entire set of relevant documents, which is obviously not available during retrieval time. As stated above, our focus is on providing a better understanding  X  specifically, a quantitative analysis  X  of what makes relevant documents (in)effective for relevance-feedback-based retrieval.
Throughout this section we assume that the following have been fixed. A query q , a corpus of documents D ,andtheset D rel (henceforth corpus that are relevant to the information need expressed by q . 1
Our goal is to identify the k ( &lt;n )documentsin D rel that are its best  X  representatives  X . We consider a relevant document as a good representative of D rel to the extent it can  X  X ffectively help X  to find relevant documents from D rel via a search performed over the corpus. More specifically, the task is identifying the k documents in D rel that if used as input, along with the query, to some relevance feedback algorithm that is used to rank the entire corpus, then the resultant retrieval performance will be optimal with respect to all subsets of k documents in D rel .

In what follows, we propose two paradigms for addressing our goal. The first, presented in Section 2.1, is ranking by the presumed representativeness of its documents, and se-lecting the top-k ranked documents. The second paradigm, presented in Section 2.2, is based on the premise that repre-sentatives are similar to each other. Specifically, we cluster D rel into clusters of k similar documents, rank the clusters based on their presumed representativeness  X  i.e., the re-sultant performance of a relevance feedback algorithm when fed with the cluster X  X  constituent documents  X  and then select the documents in the highest ranked cluster. The methods we present utilize language models [26, 6]. We use p ( w | x ) to denote the probability assigned by a lan-guage model induced from the text (or text collection) x to term w ; by definition, p (  X | x ) is a probability distribution over the vocabulary. Unless otherwise stated, we assume that the language model is smoothed. Some of our methods also utilize sim ( x, y )  X  a language-model-based measure of
In the evaluation presented in Section 4 this set contains all documents marked as relevant by TREC X  X  annotators. the similarity between texts x and y . Details regarding our language-model induction technique and similarity measure are provided in Section 4.1.1.
The following methods assign document d (  X  X  rel )with ascore, Score ( d ), that presumably reflects the extent to which d is a good representative of D rel . The top-k scoring documents in D rel are then used, along with the query, as input to a relevance-feedback method 2 .
The first family of methods that we consider estimates d  X  X  representativeness independently of other documents in D
As a reference comparison to all proposed approaches, we use the Random method that scores d by then, k documents from D rel are randomly selected. As D rel is a set of relevant documents, we consider the QuerySim method that regards document d that exhibits high (surface-level) query similarity as a good representa-tive:
The Length method is based on the assumption that short relevant documents are better representatives than long relevant documents: | d | is the number of terms in d . A case in point: the ratio between the volume of relevant and non-relevant informa-tion in a short document is potentially larger than that in a longer document.

Along the same lines, we hypothesize that documents with homogeneous content (i.e., that focus on a single/few top-ics/themes) are potentially more representative than those containing heterogeneous content. To quantify this notion, we use the entropy of d  X  X  unsmoothed language model, where H ( p (  X | d )) def =  X  entropy imply that the term-frequency distribution is con-centrated around a relatively small number of terms, which in turn potentially implies to content homogeneity [18].
Another potential indicator of the content-homogeneity of a document is its clarity with respect to the corpus. That is, we hypothesize that the more different a document (unsmoothed) language model is from that of the corpus  X  which could be viewed as a general non-relevant doc-ument  X  the more  X  X ocused/clear X  the document; conse-quently, the better representative of D rel the document is assumed to be. We use the KL-divergence ( D )tomea-sure this language-model difference: D
We assume that the documents X  scores are not provided to the relevance-feedback method. P w p ( w sentativeness criterion: (High KL-divergence means large difference between the lan-guage models.) Originally, the notion of clarity was pre-sented for queries so as to predict query performance [7].
We now turn to study methods that estimate representa-tiveness by utilizing (similarity) relationships between doc-uments in D rel . In contrast to the approaches from above that use only information in the document (and some corpus statistics), these methods require a knowledge of the entire set of relevant documents.
 The centroid method. A natural approach to estimating document d  X  X  representativeness is measuring its similar-ity to some representation of the entire set D rel  X  e.g., D rel  X  X  centroid, Cent ( D rel ). In language model terms, the centroid is a probability distribution over the vocabulary: p ( w | Cent ( D rel )) def = 1 n troid method estimates d  X  X  representativeness using the KL-divergence of its induced language model from the centroid: the lower the KL-divergence, the closer the document lan-guage model is to the centroid.
 Graph-based methods. Some work on re-ranking search results [8, 18] has demonstrated the merits of utilizing in-formation induced from inter-document similarities. Specif-ically, given an initial list of documents retrieved in response to a query, documents that are highly similar to many other documents in the list were shown to have high probability of relevance [18]. The idea is that these documents repre-sent the entire list, and by the virtue of the way the list was created  X  i.e., in response to the query  X  they might be rel-evant to the underlying information need. However, the list is composed of both relevant and non-relevant documents.
Here, we opt to adapt the idea just described to the case of the set D rel , which contains only relevant documents. Specifically, we hypothesize that a relevant document in D is representative to the extent it is similar to other (repre-sentative) documents in D rel . To quantify these represen-tativeness notions, we employ previously-proposed graph-based approaches [18].

Let G =( D rel , D rel  X D rel ) be the complete directed 3 graph defined over D rel .Theweight w t ( d 1  X  d 2 )ofthe edge between d 1 and d 2 is defined as w t ( d 1  X  d 2 ) def = where N bhd ( d 1 ;  X  )isthesetof  X  documents d  X  X  rel  X  { d 1 } that yield the highest sim ( d 1 ,d )  X  i.e., d 1  X  X  nearest-neighbors in the similarity space.
 The weighted in-degree criterion for representativeness, WInDeg , is based on the premise that a document is rep-
There is some work on the importance of directionality of edges in such graphs [16]. resentative to the extent it gets similarity-based  X  X upport X  from other documents:
To further reward documents that get similarity-based support from documents that are, themselves, representa-tives to a good extent, which leads to a recursive definition of representativeness, we use PageRank X  X  ( WPR ) approach [2]. (The  X  X  X  stands for using weighted edges.) That is, we smooth the edge-weight function: w t  X  is a free parameter. Thus, G with the edge-weight func-tion w t [  X  ] constitutes an ergodic Markov chain, for which a stationary distribution, P (  X  ), exists. We set
For completeness, we also consider variants of the WInDeg and WPR representativeness measures, denoted UInDeg and UPR , respectively, that use uniform edge weights [18]: Thus, UInDeg estimates d  X  X  representativeness by the num-ber of documents in D rel that d is among the  X  nearest-neighbors of. UPR is the analog of the original PageRank formula, wherein edges with a uniform, non-zero, weight are drawn between documents and their  X  nearest neighbors. Diversity. Recall that our goal is to select a set of k rep-resentative relevant documents that will help in finding rele-vant documents when using relevance-feedback-based search. An hypothesis that we turn to explore now is that the se-lected documents should be different (content wise) from each other, so as to potentially reflect diversified aspects of relevance. To that end, we use the Maximal Marginal Relevance criterion [5], MMR . Specifically, a document is scored by its similarity to the query, and dissimilarity with documents already selected ( S ):
Score MMR ( d ) def =  X sim ( q, d )  X  (1  X   X  )max d i  X  S  X  is a free parameter; the first selected document is that exhibiting the highest query similarity, sim ( q, d ).
To integrate the different estimates for representativeness presented insofar, we take a learning-to-rank , LTR , approach [22]. Specifically, we use the standard SVM rank method [15] that given examples of queries and rankings of documents for these queries, learns a model that can be used to rank documents for new queries. Here, for each query we pro-vide the learner with a ranking of its relevant documents. The ranking is determined based on the retrieval effective-ness (specifically, as measured by mean average precision (MAP)) of the relevance-feedback method when fed with the query and a (single) document from D rel .

Documents in SVM rank are represented by vectors of fea-tures. In our case, these features should attest to the poten-tial representativeness of the document. Specifically, we use the QuerySim, Length, Entropy, Clarity, Centroid, WInDeg, WPR, UInDeg, and UPR values for a document. 4
The methods presented above assign a representativeness score to each document; then, the top-k scoring documents are selected. While, in general, no constraint has been posted on the relationships between the selected documents, the graph-based methods are based on the implicit premise that these documents should be similar to each other. We further explore this idea by explicitly posting an inter-document-similarity constraint.

The principle of searching for k similar representative doc-uments could be regarded as an operational extension of the cluster hypothesis [35]. The hypothesis states that  X  X losely-associated documents tend to be relevant to the same re-quests X , and is often translated to statements about the similarity between relevant documents being stronger than that between non-relevant documents, and than that be-tween relevant and non-relevant documents [13]. Here, we postulate that inter-document-similarities between represen-tative relevant documents are stronger than those between non-representative relevant documents, and those between representative and non-representative relevant documents.
To devise a concrete method based on the hypothesis just stated, we cluster D rel into clusters of k documents. Specif-ically, we use a simple nearest-neighbor clustering approach wherein we define for each d (  X  X  rel )acluster c d that con-tains d and the k  X  1documents d ( d = d )in D rel that yield the highest sim ( d, d ). Such overlapping nearest-neighbor clusters are often used in work on cluster-based retrieval [10, 19, 23, 24].

Given the clusters, we opt to find the one that is the most representative  X  i.e., that using its constituent documents, along with the query, as input to a relevance feedback al-gorithm will yield the best performance with respect to all clusters. Our goal is then to rank the clusters based on their presumed representativeness.

We first consider a simple approach, clust -M ,thatas-signs cluster c with the mean representativeness score of its constituent documents: M is one of the representativeness models from Sections 2.1.1 and 2.1.2, except for Random and MMR 5 ; clust-Random will denote random selection of a cluster. Hence, the k doc-uments selected for relevance feedback  X  i.e., those that belong to the highest ranked cluster  X  exhibit two prop-erties. Their average representativeness score is high, and they are similar to each other by the virtue of belonging to the same cluster.

Another cluster ranking approach that we examine is based on the hypothesis that the more the cluster is dense the bet-ter representative it is. We study two measures of density:
For each query we use a min-max normalization for the values assigned by an estimate to documents in D rel .Ex-periments  X  results of which are omitted as they convey no additional insight  X  show that this normalization yields performance superior to that of using the raw values.
Note that the MMR criterion stands in contrast to the goal of finding clusters of similar representative documents. the average similarity between the constituent documents and the basis-document used to create the cluster ( clust-BaseDense ): and the average similarity between all documents in the clus-ter ( clust-AvgDense ): 6
To integrate the cluster ranking methods from above, we employ a learning-to-rank approach, clust-LTR ,using SVM rank [15]. In the training phase, we order the clus-ters for each query with respect to the MAP performance obtained by the relevance-feedback algorithm when fed with the query and the cluster X  X  constituent documents. To repre-sent cluster c , we use for features the values Score clust where M X  X  QuerySim, Length, Entropy, Clarity, Centroid, WInDeg,WPR,UInDeg,UPR } , and the two cluster-density values assigned by the measures from above. Normalization of feature values is performed as for documents in Section 2.1.3.
There is much work on devising effective relevance feed-back methods (e.g., [27, 11, 20, 39, 28, 4, 3, 25, 36]). In contrast, we use some relevance feedback method(s) to eval-uate the representativeness of relevant documents.
Some previous work showed that there exist relevant docu-ments that are ineffective for use in relevance-feedback-based retrieval [37, 33]. However, a quantifiable characterization of such documents has remained an open question. We show in Section 4.2.3 that documents considered as  X  X ad X  represen-tatives by our methods tend to be ineffective for relevance-feedback-based retrieval.

One of our goals is to rank clusters of relevant documents by their presumed representativeness. This is reminiscent of the optimal cluster detection task: finding clusters of top-retrieved documents that contain a high percentage of rele-vant documents [12, 34, 19, 23, 24, 17]. It was shown that graph-based centrality and query similarity of the cluster X  X  constituent documents are among the most effective indica-tors of the percentage of relevant documents in the cluster [17, 24]. As noted in Section 2, we use these information sources, and others, to detect representative documents and clusters. However, we show in Section 4.2 that query simi-larity is a relatively weak indicator for representativeness.
It is important to differentiate the representatives-selection task that we pursue here from those of active relevance feed-back [30] and aspect coverage [38]. The active relevance feed-back task [30] is selecting a subset of documents from those most highly ranked by an initial search (both relevant and non relevant) to have the user provide feedback on. The aspect coverage problem [38] is finding a subset of relevant documents that covers many query-related aspects that were defined by human annotators. An effective strategy for both tasks is to select a document set that exhibits high content
We assume that documents are assigned with unique sortable IDs. diversity. However, this strategy, as manifested in our MMR method, is often inferior to other methods that we consider.
In what follows we present an evaluation of the different methods for identifying representative relevant documents. We then study the connection between representativeness and TREC X  X  gradual relevance judgments.
We use unigram language models that assume term inde-pendence. Let p MLE ( w | x ) be the maximum-likelihood esti-mate of term w with respect to text (collection) x ; p MLE is referred to as x  X  X  unsmoothed language model. Unless otherwise specified, we use a Dirichlet smoothed language model, p Dir (  X | x ), with parameter  X  [40]. To construct rel-evance models (see Section 4.1.2), we also use the Jelinek-Mercer smoothed language model, p JM (  X | x ), with smoothing parameter  X  [40].

To measure similarity between texts x and y , we utilize a previously proposed estimate [18, 19] that is based on the KL-divergence between their induced language models: The merits of this estimate, specifically, with respect to as-signing language-model probabilities to long sequences of text (documents in our case), were demonstrated in some previous work [18, 19].
To evaluate the effectiveness of the representatives-selection methods, we use two highly effective, yet quite different, relevance-feedback methods that operate in the language modeling framework; namely, the model-based feedback ap-proach [39], and relevance models [20].

Both methods take as input a query q and a set of relevant documents S , and construct a  X  X opic X  language model p (  X | that is assumed to generate the terms in documents rele-vant to q . The methods differ by the estimation approach of the topic model as described below. As is common [1], the estimated topic model is then clipped by setting p ( w to zero for all but the  X  terms w with the highest p ( w | normalization is performed to yield a probability distribu-tion  X  p (  X | T ). We finally derive the feedback language model p (  X | S ) by further anchoring the constructed topic model to the query [39, 1] using interpolation with a free parameter  X  : p ( w | S ) def =  X p MLE ( w | q )+(1  X   X  )  X  p ( w | ranked with respect to the feedback model based on the KL divergence D proach [39] assumes that each document d in S is generated by a mixture, with free parameter  X  , of a topic language model p (  X | T ) with the corpus model p MLE (  X |D ). To esti-mate p (  X | T ), the EM algorithm is used [39].

To estimate a relevance model (RM1) given a set of rel-evant documents S [21], a centroid of the (Jelinek-Mercer smoothed) language models of documents in S is used: model, p (  X | S ) (a.k.a. RM3 [1]), constructed as described above, is an analog of Rocchio X  X  model [27] in the language-modeling framework [21]. Note that while with RM3 the (query-anchored) centroid of the selected set of presumed representatives is used to rank the corpus, in Section 2.1.2 the centroid of the entire set of relevant documents ( D rel was used for selecting representatives from D rel  X  the Cen-troid method. We come back to this point in Section 4.2.1.
We note that the model-based feedback approach and the relevance model treat relevant documents as equi-important.
We conducted experiments with the following TREC data:
Titles of TREC topics serve for queries. We applied to-kenization, Porter stemming, and stopword removal (using the INQUERY list) using the Lemur toolkit 8 ,whichwas used for experiments. The set D rel of the documents rel-evant to a query q is that determined by TREC X  X  human annotators ( X  X rels X  files).

The effectiveness of the document-based representatives-selection methods from Section 2.1 is studied when selecting either k =1or k = 5 relevant documents. Each case consti-tutes a separate experimental setting with respect to tuning of free parameter values. (See the below.) Note that in the single representative case ( k = 1), we essentially evaluate representativeness by the relative extent to which (language) models of relevant documents in the corpus are similar to that induced from the selected document (and the query), with respect to models of non-relevant documents. For the cluster-based methods (Section 2.2), we set the cluster size ( k ) to 5, and select the 5 relevant documents in the highest ranked cluster.

The representatives-selection methods are evaluated by the resultant effectiveness of the relevance-feedback meth-ods when fed with the selected documents and the query. An important issue with relevance-feedback evaluation is whether to include the documents used as input in eval-uation, or exclude them and use only the residual corpus [3]. Since the representatives-selection methods select dif-ferent sets of relevant documents to be used for relevance feedback, the residual-corpus approach suffers from signif-icant problems as previously noted [3]. Thus, we evaluate relevance-feedback performance based on the MAP(@1000) and precision of the top-10 documents (p@10) attained with respect to all known relevant documents. Hence, represen-tativeness of selected documents is evaluated based on their effectiveness in helping to find relevant documents  X  them-selves and others  X  via relevance-feedback-based search per-formed over the corpus. Statistically significant performance differences are determined using the two-tailed paired t-test at a 95% confidence level [29, 31].
When no relevance feedback is available, relevance model construction is based on a pseudo feedback approach [20]. www.lemurproject.org QuerySim methods are marked with  X  X  X  and  X  X  X , respectively. Parameter values. Our goal is to focu s on the underlying principles of the representatives-selection methods, rather than engage in excessive parameter tuning. Therefore, we have taken the following experimental-design decisions.
The document language model Dirichlet-smoothing pa-rameter (  X  ) is set to 1000 in all methods following pre-vious recommendations [40]. The free parameters of the graph-based methods,  X  and  X  ,aresetto5and0.8,respec-tively, following limited experimentation with  X   X  X  5 , 10 and  X   X  X  0 . 2 , 0 . 8 } and previous recommendations [18]. The free parameter of the MMR method,  X  ,issetto0 . 1 following SVM rank package [15], used by the learning-to-rank meth-ods, is employed with default parameter values. For learn-ing/testing we use ten fold cross validation performed over the queries for a corpus; each fold contains a consecutive set of queries, wherein queries are ordered by their IDs.
The free parameters of the relevance feedback methods are set for all representatives-selection approaches to values that yield optimized MAP performance for the QuerySim ap-proach, which serves as a reference comparison to all meth-ods. The search ranges for parameter values are:  X  { 0 , 0 . 1 , 0 . 3 ,..., 0 . 9 , 1 } ,  X   X  X  5 , 10 , 25 , 50 , 75 , 100 0 . 3 ,..., 0 . 9 , 1 } ,and  X   X  X  0 . 1 , 0 . 3 ,..., 0 . 9
In Table 1 we present the performance of the document-based representatives-selection methods from Section 2.1. As can be seen, selecting documents based on their query-similarity (QuerySim) can often be less effective than ran-dom (Random) selection. Henceforth, we use these two methods as reference comparisons to all other approaches.
We can see in Table 1 that for selecting k =5repre-sentatives, short documents (Length), documents with term distribution concentrated around a relatively small num-ber of terms (Entropy), and documents with models distant from that of the corpus (Clarity), are all good choices, with Length being the most effective approach among the three 9 Indeed, note that several of the boldface marks in the table, which indicate the best performance per corpus, evaluation measure, and k , appear in lines corresponding to these three methods when k = 5. Furthermore, Length, Entropy and Clarity post performance for k = 5 that is in most relevant comparisons better than that of Random and QuerySim; these improvements are often also statistically significant.
However, Length, Entropy an dClarityaremuchlessef-fective for selecting a single representative ( k =1),specifi-cally, with respect to methods that utilize inter-document relationships, and which we discuss below. This finding is not surprising. A case in point, when using 5 relevant documents, which is considered a  X  X ecent X  sample [11], the relevance-feedback methods essentially  X  X mphasize X  the com-monalities between these documents; taking care, in addi-
Experimental results  X  actual numbers are omitted due to space considerations  X  show that (i) selecting short docu-ments is much preferable to selecting long documents, (ii) selecting documents with low entropy is much preferable to selecting documents with high entropy, and (iii) selecting documents with high clarity is preferable to selecting docu-ments with low clarity. These results support the hypotheses stated in Section 2.1, based on which Length, Entropy and Clarity were devised. tion, that the documents are somewhat focused (as esti-mated by Length, Entropy, and Clarity), improves the rep-resentativeness of the constructed feedback model. On the other hand, selecting a single document, as focused as it might be, independently of other documents, is less likely to result in a representative feedback model due to non query-related aspects potentially manifested in the document.
We now turn to explore methods utilizing inter-document relationships. It is evident in Table 1 that similarity to the centroid of the relevant-documents set (Centroid) is a highly effective representatives-selection approach, which in many cases is superior to the other methods. Specifically, Centroid almostalwaysoutperforms X  X ftentoaasubstantialand statistically-significant degree  X  Random and QuerySim.
At first glance, the performance of Centroid with the rel-evance model could be viewed as somewhat biased. That is, the Centroid method selects representatives based on sim-ilarity to the centroid of the relevant-documents set, while the relevance model uses a (query-anchored) centroid of the selected representatives to rank the corpus. If these two centroids are quite close to each other, then the Centroid method could potentially be regarded as having  X  X dvantage X  over other methods. However, a closer look at Table 1 re-veals that there are many cases wherein the Centroid is out-performed by other methods when using the relevance model  X  e.g., by Length for many comparisons with k =5,andby Random (when MAP is considered) over WT10g.

Table 1 shows that the graph-based methods, which uti-lize inter-document similarities, WInDeg, WPR, UInDeg, and UPR, are also quite effective. Specifically, all four ap-proaches outperform Random and QuerySim in almost all relevant comparisons; often, to a statistically significant de-gree. Among the four, UInDeg, which counts for a document the number of documents it is among the nearest-neighbors of, and WPR, which is a weighted version of PageRank em-ployed over the similarity-based graph, are the most effec-tive. Thus, we see that documents centrally located within the similarity-based graphs tend to be good representatives. Moreover, as noted above, for selecting a single representa-tive, utilizing inter-document relationships, as in the graph-based and Centroid methods, yields performance substan-tially better than that of considering documents indepen-dently (QuerySim, Length, Entropy, and Clarity).

The effectiveness of the graph-based and Centroid meth-ods in finding a single representative provides an interesting perspective on the cluster hypothesis [35]. Recall that for the single representative case, the relevance feedback meth-ods essentially search the corpus for documents with mod-els similar to that of the selected document (anchored to the query). Thus, the effectiveness just mentioned implies that relevant documents centrally located in the similarity space of relevant documents are closer to relevant documents than to non-relevant documents; non centrally-located rel-evant documents might manifest these relative similarities to a lesser extent. Thus, while the cluster hypothesis im-plies that relevant documents are similar to each other, the observation just stated could be viewed as its refinement.
We next turn to examine the MMR approach, which looks for a diversified set of representatives. Evidently, MMR is effective with the relevance model, but much less effective with the model-based feedback method. In the latter case it is often outperformed by Random and QuerySim. (Recall that for k = 1 MMR amounts to QuerySim.)
Finally, we turn to the learning-to-rank approach, LTR, which integrates all methods from above (except for MMR and Random). Clearly, LTR is highly effective. It almost always outperforms Random and QuerySim; often to a sub-stantial and statistically significant degree. We note that the Centroid is assigned with the highest weight by the learner, and that this weight is much higher than that assigned to other methods. Yet, the overall superiority of LTR to Cen-troid when using the model-based feedback method attests that other approaches can yield further improvements on top of that posted by Centroid. Among those, as indicated by the weights the learner assigns, are QuerySim and WInDeg. Summary. All in all, we see that methods that look for doc-uments centrally located in the similarity space are highly effective representatives-selection approaches, especially for selecting a single representative; and, that integrating those with other methods can further improve performance.
In Table 2 we present the performance of the cluster-based methods for selecting k = 5 representative documents  X  i.e., those in the highest ranked cluster. The first four rows present performance numbers for reference compar-isons. The first, Opt. Clust. , is manually selecting a cluster for each query so that the MAP performance of the relevance-feedback method when fed with the cluster X  X  con-stituent documents and the query is the highest with re-spect to all other clusters. Hence, the performance of Opt. Clust. serves as an upper bound for that of all cluster-based methods in the table. The second reference comparison, clust-Random , selects a cluster randomly. The third and fourth, doc-Random and doc-QuerySim ,aretheRan-dom and QuerySim methods from Table 1, which select rel-evant documents (rather than clusters) randomly, and based on document-query similarities, respectively.

Our first observation based on Table 2 is that there are highly representative clusters, as the performance numbers for Opt. Clust. attest. Finding these clusters results in per-formance that is by far better than that of all other methods considered in this paper.

We can also see in Table 2 that the cluster-based methods (clust-QuerySim, ... , clust-LTR) yield performance that is almost always better  X  often to a substantial extent  X  than that of selecting a random cluster (clust-Random). (Many of these performance improvements are also statistically-significant; these were not marked in the table to avoid clut-tering.) However, the methods that are based on the mean document score in the cluster (clust-QuerySim, ... ,clust-UPR), are (i) consistently less effective than their counter-parts in Table 1 that use the scores to directly rank docu-ments, and (ii) often less effective than the document-based baselines, doc-Random and doc-QuerySim; in many cases, to a statistically-significant degree. The latter also holds for the cluster-density-based methods, BaseDense and Avg-Dense. Thus, we see that identifying representative clusters is a hard challenge.

Another observation that we make based on Table 2 is that Centroid is the most effective document-based scoring method in terms of the resultant cluster-based performance (clust-Centroid). Furthermore, clust-Centroid is the method that is assigned with the highest weight by the learner in Table 2: Evaluation of cluster-based methods (clust-X) for selecting 5 representative relevant docu-ments. (Refer to Section 2.2.) The document-based (doc-)Random and (doc-)QuerySim methods from Table 1 are presented for reference;  X  X  X , and  X  X  X  mark statistically-significant differences with these refer-ence comparisons, respectively. Boldface marks the best cluster-based performance in a column. the learning-to-rank approach of clusters (clust-LTR). These findings echo those from Table 1.

Clearly, the most effective cluster-based selection method in Table 2 is the learning-to-rank (clust-LTR) approach. Specifically, clust-LTR posts performance that is often bet-ter  X  sometimes to a statistically-significant degree  X  than that of the document-based reference comparisons (doc-Random and doc-QuerySim). Although clust-Centroid is the method assigned with the highest weight by the learner, as noted above, clust-LTR consistently outperforms clust-Centroid. This finding attests to the (somewhat) complementary na-ture of the methods integrated by clust-LTR. Specifically, clust-QuerySim and clust-AvgDense are assigned with rela-tively high weights by the learner, albeit much lower than that of clust-Centroid.
Previous work showed that there are queries for which some relevant documents are quite ineffective for relevance-feedback-based retrieval regardless of the retrieval method Table 3: The relevance-feedback-based MAP per-formance of using a single document considered as the worst (best) representative by a representatives-selection method. The performance of using the rel-evant document that actually yields the worst (best) performance  X  i.e., the  X  X rue-worst(best) X  repre-sentative per relevance feedback method, and that of a standard query-likelihood (QL) retrieval model that does not utilize relevance feedback, is presented for reference. used [37, 33]. Some of these documents, termed poison pills , when used alone (with the query), yield performance that is even worse than that of not utilizing relevance feedback at all [33]. However, characterizing and automatically identifying such documents is an open question [33].

We therefore turn to explore whether relevant documents considered as  X  X ad X  representatives by our methods  X  i.e., that are assigned with low representativeness values  X  are indeed ineffective for relevance-feedback-based retrieval. More generally, we further study the effectiveness of our document-based methods (refer to Sections 2.1 and 4.2.1) in differentiating  X  X ood X  from  X  X ad X  representatives.
Table 3 presents the MAP performance of relevance-feed-back-based retrieval when using the worst and best repre-sentatives identified by some of our methods  X  i.e., the documents that are assigned with the lowest and highest representativeness values, respectively. We also present the performance of the  X  X rue-wors t X  and  X  X rue-best X  represen-tatives per relevance feedback method; that is, the perfor-mance of using the documents that actually yield the worst and best performance, respectively. Thus, while  X  X rue-worst X  and  X  X rue-best X  are relevance-feedback-method specific, our methods define representativeness regard less of the relevance-feedback method used. In addition, the performance of the query likelihood ( QL ) retrieval model [32], which scores doc-ument d in the corpus by sim ( q, d ), and which does not utilize relevance feedback, is presented for reference.
We can see in Table 3 that using  X  X rue-worst X  yields per-formance that is substantially worse than that of not using relevance feedback at all (the QL method); and, using  X  X rue-best X  yields much better performance than that of QL. These findings further attest to the significant performance impact of the relevant documents used for feedback. Table 4: The percentage of queries for which the document considered as the worst representative by a method is a  X  X oison pill X  [33] in terms of MAP performance.  X  X round truth X  is the percentage of queries for which a poison pill exists.

We can also see in Table 3 that random selection of a relevant document yields performance that is much better than that of QL, but much worse than that of  X  X rue-best X .
Another observation that we make based on Table 3 is that the QuerySim method, which utilizes document-query similarities, does not effectively differentiate good from bad representatives. That is, the difference in performance be-tween using the worst and best identified representative is quite small (except for the AP case) when compared, for example, with that posted by the Centroid method. In fact, for the ROBUST corpus, the  X  X est X  identified representative of QuerySim yields performance that is worse than that of the  X  X orst X  identified representative.

Table 3 also shows that the Centroid method is quite ef-fective in differentiating good from bad representatives. In-deed, the performance difference between using the worst and best representative is qui te large. Furthermore, the performance of using the best Centroid-based representa-tive is much better than that of random document selection; and, using the worst Centroid-based representative yields performance that is much worse than that of random selec-tion. Yet, the worst and best representatives identified by Centroid still yield performance quite different than that of  X  X rue-worst X  and  X  X rue-best X , respectively.

In further exploration, we pre sent in Table 4 the percent-age of queries for which our methods find a  X  X oison pill X ; that is, the percentage of queries for which using the docu-ment considered as the worst representative (along with the query) yields MAP performance that is worse than that of QL. Table 4 also presents the  X  X round truth X  numbers  X  the percentage of queries for which a poison pill exists.
As can be seen in Table 4, the worst representatives iden-tified by our methods are much more often poison pills than randomly selected relevant documents. Furthermore, in al-most all cases, for more than half of the queries for which a poison pill exists, the least representative document as de-termined by the Centroid method tends to be a poison pill. These findings further show that relevant documents consid-ered as bad representatives by our methods tend to be quite ineffective for relevance-feedback-based retrieval. Table 5: The connection between representative-ness and gradual relevance judgments. We mea-sure NDCG@k for the WT10g corpus when rank-ing relevant documents based on their representa-tiveness and using level-1 ( X  X elevant X ) and level-2 ( X  X ighly relevant X ) relevance judgments. MBF-true and RelModel-true denote a ranking induced by the  X  X rue X  representativeness level of a document with respect to the model-based feedback and rel-evance model approaches, respectively. The best re-sult in a column is boldfaced;  X  X  X  marks statistically-significant difference with Random ranking.
The next question we turn to explore is whether repre-sentative relevant documents would be considered by hu-man judges as  X  X ore relevant X  than non-representative rel-evant documents. To that end, we use the WT10g corpus for which gradual relevance judgments are available; specif-ically, a relevant document is marked with relevance-level  X 1 X  ( X  X elevant X ) or  X 2 X  ( X  X ighly relevant X ). We rank the rele-vant documents in D rel by the representativeness values as-signed by the different document-based methods, and com-pute NDCG [14] at various cutoffs using the two relevance levels. We present for reference the NDCG of a ranking in-duced by the  X  X rue X  representativeness level of a document as determined with respect to the model-based feedback ap-proach ( X  X BF-true X ) and with respect to the relevance model approach ( X  X elModel-true X )  X  i.e., with respect to the resul-tant relevance-feedback-based performance when using the document with the query.

We can see in Table 5 that most of our representatives-selection methods yield ranking that is superior to random ranking; many of the improvements posted over random ranking by methods that utilize inter-document similarities (e.g., the Centroid and the graph-based methods) are also statistically significant. Furthermore, some of our methods post performance that is superior to that of a ranking based on the  X  X rue X  representativeness as determined with respect to a relevance feedback method. These findings attest to the connection between the notion of representativeness em-ployed by our methods, which is relevance-feedback-method agnostic, and TREC X  X  gradual relevance judgments.
However, we also see in Table 5 that the QuerySim method, which was among the worst considered above for finding representative documents, is the most effective for differen-tiating between  X  X elevant X  and  X  X ighly relevant X  documents. Thus, while relevant documents that exhibit high surface-level query similarity tend to be deemed  X  X ighly relevant X  by TREC X  X  annotators, these documents, as shown above, are not highly effective in helping to find other relevant doc-uments using relevance-feedback-based retrieval.
We have addressed the question of what makes some rele-vant documents more effective than others for use in relevance-feedback-based retrieval. Specifically, we presented a suite of methods for estimating the representativeness of rele-vant documents; the level of representativeness is measured by the resultant performance of some relevance feedback method when fed with the documents and the query, and employed over the corpus. We showed that documents cen-trally located within the similarity space of the relevant-documents set are good representatives. Furthermore, we showed that there are highly representative clusters of simi-lar relevant documents, and proposed methods for ranking clusters based on their presumed representativeness. In ad-dition, we showed that documents considered as  X  X ad X  repre-sentatives by our methods often yield ineffective relevance-feedback-based retrieval performance. Finally, we demon-strated the connection between representativeness and TREC X  X  gradual relevance judgments.
 Acknowledgments We thank the reviewers for their com-ments. This paper is based upon work supported in part by Israel X  X  Science Founda tion under gra nt no. 890015, by G. S. Elkin research fund at the Technion, by Google X  X  and IBM X  X  faculty research awards, and by IBM X  X  SUR award. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not nec-essarily reflect those of the sponsoring institutions.
