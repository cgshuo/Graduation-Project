 Spelling error correction is a longstanding Natural Language Processing (NLP) problem, and it has recently become especially relevant because of the many potential applications to the large amount of informal and unedited text generated online, including web forums, tweets, blogs, and email. Misspellings in such text can lead to increased sparsity and errors, posing a challenge for many NLP applications such as text summarization, sen-timent analysis and machine translation.

In this work, we present GSEC, a Generalized character-level Spelling Error Correction model, which uses supervised learning to map input char-acters into output characters in context. The ap-proach has the following characteristics:
Character-level Corrections are learned at the beling approach.

Generalized The input space consists of all characters, and a single classifier is used to learn common error patterns over all the training data, without guidance of specific rules.

Context-sensitive The model looks beyond the context of the current word, when making a deci-sion at the character-level.

Discriminative The model provides the free-dom of adding a number of different features, which may or may not be language-specific.

Language-Independent In this work, we in-tegrate only language-independent features, and therefore do not consider morphological or lin-guistic features. However, we apply the model to correct errors in Egyptian Arabic dialect text, following a conventional orthography standard, CODA (Habash et al., 2012).

Using the described approach, we demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and we improve over a state-of-the-art system (Eskander et al., 2013) which relies heavily on language-specific and manually-selected constraints. We present a de-tailed analysis of mistakes and demonstrate that the proposed model indeed learns to correct a wider variety of errors. Most earlier work on automatic error correction addressed spelling errors in English and built mod-els of correct usage on native English data (Ku-kich, 1992; Golding and Roth, 1999; Carlson and Fette, 2007; Banko and Brill, 2001). Ara-bic spelling correction has also received consider-able interest (Ben Othmane Zribi and Ben Ahmed, 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012; Eskander et al., 2013; Zaghouani et al., 2014).
Supervised spelling correction approaches trained on paired examples of errors and their cor-rections have recently been applied for non-native English correction (van Delden et al., 2004; Li et al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012; Rozovskaya and Roth, 2011). Discriminative models have been proposed at the word-level for error correction (Duan et al., 2012) and for error detection (Habash and Roth, 2011).

In addition, there has been growing work on lex-ical normalization of social media data, a some-what related problem to that considered in this pa-per (Han and Baldwin, 2011; Han et al., 2013; Subramaniam et al., 2009; Ling et al., 2013).
The work of Eskander et al. (2013) is the most relevant to the present study: it presents a character-edit classification model (CEC) using der et al. (2013) analyzed the data to identify the seven most common types of errors. They devel-oped seven classifiers and applied them to the data in succession. This makes the approach tailored to the specific data set in use and limited to a specific set of errors. In this work, a single model is con-sidered for all types of errors. The model consid-ers every character in the input text for a possible spelling error, as opposed to looking only at cer-tain input characters and contexts in which they appear. Moreover, in contrast to Eskander et al. (2013), it looks beyond the boundary of the cur-rent word. 3.1 Modeling Spelling Correction at the We recast the problem of spelling correction into a sequence labeling problem, where for each input character, we predict an action label describing how to transform it to obtain the correct charac-ter. The proposed model therefore transforms a given input sentence e = e 1 , . . . , e n of n char-acters that possibly include errors, to a corrected sentence c of m characters, where corrected char-acters are produced by one of the following four actions applied to each input character e i :  X  ok : e i is passed without transformation.  X  substitute  X  with ( c ) : e i is substituted with  X  delete : e i is deleted.  X  insert ( c ) : A character c is inserted bef ore We use a multi-class SVM classifier to predict the action labels for each input character e i  X  e . A decoding process is then applied to transform the input characters accordingly to produce the cor-rected sentence. Note that we consider the space character as a character like any other, which gives us the ability to correct word merge errors with space character insertion actions and word split er-rors with space character deletion actions. Table 1 shows an example of the spelling correction pro-cess.

In this paper, we only model single-edit actions and ignore cases where a character requires mul-tiple edits (henceforth, complex actions), such as multiple insertions or a combination of insertions and substitutions. This choice was motivated by the need to reduce the number of output labels, as many infrequent labels are generated by complex actions. An error analysis of the training data, de-scribed in detail in section 3.2, showed that com-plex errors are relatively infrequent (4% of data). We plan to address these errors in future work.
Finally, in order to generate the training data in the described form, we require a parallel cor-pus of erroneous and corrected reference text (de-scribed below), which we align at the character level. We use the alignment tool Sclite (Fiscus, 1998), which is part of the SCTK Toolkit. 3.2 Description of Data We apply our model to correcting Egyptian Ara-bic dialect text. Since there is no standard dialect orthography adopted by native speakers of Ara-bic dialects, it is common to encounter multiple spellings of the same word. The CODA orthogra-phy was proposed by Habash et al. (2012) in an attempt to standardize dialectal writing, and we use it as a reference of correct text for spelling correction following the previous work by Eskan-der et al. (2013). We use the same corpus (la-beled "ARZ") and experimental setup splits used by them. The ARZ corpus was developed by the Linguistic Data Consortium (Maamouri et al., 2012a-e). See Table 2 for corpus statistics. Error Distribution Table 3 presents the distri-bution of correction action labels that correspond to spelling errors in the training data together with tions into: Substitute , Insert , Delete , and Complex , and also list common transformations within each group. We further distinguish between the phe-nomena modeled by our system and by Eskander et al. (2013). At least 10% of all generated action labels are not handled by Eskander et al. (2013). 3.3 Features Each input character is represented by a feature vector. We include a set of basic features inspired by Eskander et al. (2013) in their CEC system and additional features for further improvement. Basic features We use a set of nine basic fea-tures: the given character, the preceding and fol-lowing two characters, and the first two and last two characters in the word. These are the same features used by CEC, except that CEC does not include characters beyond the word boundary, while we consider space characters as well as char-acters from the previous and next words.
 Ngram features We extract sequences of char-acters corresponding to the current character and the following and previous two, three, or four characters. We refer to these sequences as bi-grams, trigrams, or 4-grams, respectively. These are an extension of the basic features and allow the model to look beyond the context of the cur-rent word. 3.4 Maximum Likelihood Estimate (MLE) We implemented another approach for error cor-rection based on a word-level maximum likeli-hood model. The MLE method uses a unigram model which replaces each input word with its most likely correct word based on counts from the training data. The intuition behind MLE is that it can easily correct frequent errors; however, it is quite dependent on the training data. 4.1 Model Evaluation Setup The training data was extracted to gener-ate the form described in Section 3.1, using the Sclite tool (Fiscus, 1998) to align the input and reference sentences. A speech effect handling step was applied as a preprocessing step to all models. This step removes redundant repetitions of charac-ters in sequence, e.g., The same speech effect handling was applied by Eskander et al. (2013).

For classification, we used the SVM implemen-tation in YamCha (Kudo and Matsumoto, 2001), and trained with different variations of the fea-tures described above. Default parameters were selected for training ( c =1, quadratic kernel, and context window of +/-2).

In all results listed below, the baseline corre-sponds to the do-nothing baseline of the input text. Metrics Three evaluation metrics are used. The word-error-rate WER metric is computed by sum-ming the total number of word-level substitution errors, insertion errors, and deletion errors in the output, and dividing by the number of words in the reference . The correct-rate Corr metric is com-puted by dividing the number of correct output words by the total number of words in the refer-ence. These two metrics are produced by Sclite (Fiscus, 1998), using automatic alignment. Fi-nally, the accuracy Acc metric, used by Eskander et al. (2013), is a simple string matching metric which enforces a word alignment that pairs words in the reference to those of the output. It is cal-culated by dividing the number of correct output words by the number of words in the input . This metric assumes no split errors in the data (a word incorrectly split into two words), which is the case in the data we are working with.
 Character-level Model Evaluation The per-formance of the generalized spelling correction model (GSEC) on the dev data is presented in the first half of Table 4. The results of the Eskan-der et al. (2013) CEC system are also presented for the purpose of comparison. We can see that using a single classifier, the generalized model is able to outperform CEC, which relies on a cascade of classifiers ( p = 0 . 03 for the basic model and p &lt; 0 . 0001 for the best model, GSEC+4grams). 4 Model Combination Evaluation Here we present results on combining GSEC with the MLE component (GSEC+MLE). We combine the two models in cascade: the MLE component is applied to the output of GSEC. To train the MLE model, we use the word pairs obtained from the original training data, rather than from the output of GSEC. We found that this configuration allows us to include a larger sample of word pair errors for learning, because our model corrects many errors, leaving fewer example pairs to train an MLE post-processor. The results are shown in the second half of Table 4.

We first observe that MLE improves the per-formance of both CEC and GSEC. In fact, CEC+MLE and GSEC+MLE perform similarly ( p = 0 . 36 , not statistically significant). When adding features that go beyond the word bound-ary, we achieve an improvement over MLE, GSEC+MLE, and CEC+MLE, all of which are mostly restricted within the boundary of the word. The best GSEC model outperforms CEC+MLE ( p &lt; 0 . 0001 ), achieving a WER of 8.3%, corre-sponding to 65% reduction compared to the base-line. It is worth noting that adding the MLE com-ponent allows Eskander X  X  CEC to recover various types of errors that were not modeled previously. However, the contribution of MLE is limited to words that are in the training data. On the other hand, because GSEC is trained on character trans-formations, it is likely to generalize better to words unseen in the training data.
 Results on Test Data Table 5 presents the re-sults of our best model (GSEC+4grams), and best model+MLE. The latter achieves a 92.1% Acc score. The Acc score reported by Eskander et al. (2013) for CEC+MLE is 91.3% . The two results are statistically significant ( p &lt; 0 . 0001 ) with re-spect to CEC and CEC+MLE respectively.
 4.2 Error Analysis To gain a better understanding of the performance of the models on different types of errors and their interaction with the MLE component, we separate the words in the dev data into: (1) words seen in the training data, or in-vocabulary words (IV) , and (2) out-of-vocabulary (OOV) words not seen in the training data. Because the MLE model maps every input word to its most likely gold word seen in the training data, we expect the MLE compo-nent to recover a large portion of errors in the IV category (but not all, since an input word can have multiple correct readings depending on the con-text). On the other hand, the recovery of errors in OOV words indicates how well the character-level model is doing independently of the MLE compo-nent. Table 6 presents the performance, using the Acc metric, on each of these types of words. Here our best model (GSEC+4grams) is considered.
When considering words seen in the training data, CEC and GSEC have the same performance. However, when considering OOV words, GSEC performs significantly better ( p &lt; 0 . 0001 ), veri-fying our hypothesis that a generalized model re-duces dependency on training data. The data is heavily skewed towards IV words (83%), which explains the generally high performance of MLE.
We performed a manual error analysis on a sam-ple of 50 word errors from the IV set and found that all of the errors came from gold annotation er-rors and inconsistencies, either in the dev or train. We then divided the character transformations in the OOV words into four groups: (1) characters that were unchanged by the gold ( X-X transforma-tions), (2) character transformations modeled by CEC ( X-Y CEC ), (3) character transformations not modeled by CEC, and which include all phenom-ena that were only partially modeled by CEC ( X-Y not CEC ), and (4) complex errors. The character-level accuracy on each of these groups is shown in Table 7.

Both CEC and GSEC do much better on the second group of character transformations (that is, X-Y CEC ) than on the third group ( X-Y not CEC ). This is not surprising because the former transformations correspond to phenomena that are most common in the training data. For GSEC, they are learned automatically, while for CEC they are selected and modeled explicitly. Despite this fact, GSEC generalizes better to OOV words. As for the third group, both CEC and GSEC per-form more poorly, but GSEC corrects more errors (43.48% vs. 31.68% accuracy). Finally, CEC is better at recognizing complex errors, which, al-though are not modeled explicitly by CEC, can sometimes be corrected as a result of applying multiple classifiers in cascade. Dealing with com-plex errors, though there are few of them in this dataset, is an important direction for future work, and for generalizing to other datasets, e.g., (Za-ghouani et al., 2014). We showed that a generalized character-level spelling error correction model can improve spelling error correction on Egyptian Arabic data. This model learns common spelling error patterns automatically, without guidance of manually se-lected or language-specific constraints. We also demonstrate that the model outperforms existing methods, especially on out-of-vocabulary words.
In the future, we plan to extend the model to use word-level language models to select between top character predictions in the output. We also plan to apply the model to different datasets and differ-ent languages. Finally, we plan to experiment with more features that can also be tailored to specific languages by using morphological and linguistic information, which was not explored in this paper. This publication was made possible by grant NPRP-4-1058-1-168 from the Qatar National Re-search Fund (a member of the Qatar Foundation). The statements made herein are solely the respon-sibility of the authors.
