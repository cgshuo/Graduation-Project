 Document similarity needs to be measured in a variety of applications for clustering, filtering, sorting and retrieving, etc. For example, in a personalized digital library, the computing of document similarity is the foundation of collaborative filtering [1]. But along with astonishing amount of information being placed online, the computation of document similarity encounters great challenges in two aspects: (1) the complexity of internal and external information of a document; (2) the large scale of document amount. For this reason, the ability to measure similarity between documents in an accurate and efficient way is a key determinant for many applications. 
A variety of approaches have been proposed to model document similarity based on different foundations. Some traditional approaches calculate similarity according to document contents (especially document-term relationship), such as Vector Space exploiting link structure of objects, some methods focusing on link-based object ranking are proposed by researchers [5, 6, 7]. If viewing documents as nodes and relationships among documents as edges, document similarity can be measured by these link-based object ranking methods with the contents of documents ignored. 
In this paper, we propose a new approach by using link-based content analysis to measure document similarity effectively. This approach takes advantage of document-term relationship and builds a link graph among documents. Then link analysis is imported to assess the similarity between documents. There are plenty of link analysis methods introduced in [12]. Our approach propagates the similarity between docu-random walk theory [9]. Moreover, internal and external information of documents can be combined effectively using our method, which is not applicable for most simi-larity measuring methods mentioned above. 
As Figure 1(a) shows, the content of a document can be modeled as a collection of &lt; Term , TF &gt; pairs, where TF is term frequency in the corresponding document. Using some simple statistics, we obtain a matrix describing relationship between documents and terms as seen in Figure 1(b). Transition probability based on contents between documents is derived from relationship matrix, via normalization step and matrices product step (explained in Section 3.2 and shown in Figure 1(c)). External informa-tion is also considered (discussed in Section 4). From the citation relationship shown in Figure 1(a), we can get transition probability shown in Figure 1(d). Given a ratio of importance between contents and citation relationship in measuring the similarity, we can combine these two kinds of transition probabilities as a whole, which is the input of similarity computation. Based on random walk theory, we define similarity as the determined by similarities of all its direct-connected document-pairs and transition probabilities from ( a, b ) to these pairs (details can be found in Section 3.3). 
The internal information (that is content) and external information (namely, outside links) of documents can be combined effectively using the same computational model. Experiments on real datasets are conducted to test the accuracy and perform-ance of our link-based content analysis. Observation on convergence indicates that similarity result of the first iteration is acceptable for most cases. Section 3, we introduce link-based content analysis. Afterwards, Section 4 describes how to combine internal and external information. The results of experiments are shown in Section 5 and this study is concluded in Section 6. Measuring pair-wise document similarity has been extensively studied for decades, with lots of methods proposed. These methods can be roughly divided into several types according to the document information they focus on. Generally, related works of document similarity measuring can be considered from three views: content analy-sis, link analysis and their combination. (1) Content analysis 
Analysis of content (or internal information) is a traditional information retrieval task. Early methods treat a document as a bag of words and calculate cosine similarity according to the tf-idf weight [13], such as Vector Space Model and its variations [2, 14]. Considering the sequence of terms, n -grams [3] are introduced to gauge the simi-larity. A more complicated approach is Latent Semantic Analysis [4], which maps each document and term vector into a lower dimensional space associated with con-cepts. Aslam et al. [15] propose an information-theoretic measure for document simi-larity in an axiomatic manner, which is a different research route from others. (2) Mining links of documents 
Some documents (especially web pages) have rich outside links (or called external information). Viewing documents as nodes and external relationships as edges, the corpus of documents can be modeled as a graph, and exploiting this link structure may be one of the best ways to measure document similarity. There are multiple link-based object ranking methods. SimRank [5] provides a wonderful definition for similarity on a SimRank is  X  X andom surfer-pairs X , a concept derived from random walk theory. Xi et al. [6] use a Unified Relationship Matrix (URM) to represent a collection of heterogeneous similarities between objects in a compact way. These link-based object ranking methods usually involve iterative computation and ignore the inside attributes of an object. (3) Combining content and outside links 
Researchers have introduced techniques for combining link-based and content-based methods to improve the accuracy of web document classification [16]. Multiple content matching and link information in a single unified framework to improve re-trieval. Zhu et al. [18] design an algorithm that carries out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and represents web pages in a low-dimensional space. 
The intuitive model of our method is based on random walk on graphs, which is a special Markov Chain [10]. We apply link-based idea to content analysis, and inte-grate it with outside links naturally, thus making it different from other methods. In this section we introduce link-based content analysis method. 3.1 Modeling the Content of a Document Before modeling the content of a document, a cleaning step is performed to remove stop-words, and stem leftover words using the Porter Stemmer algorithm [19]. The stop-words list is taken from SMART Retrieval System [20], and a complete manifest can be found in [21]. 
We model the content of a document as a set of &lt; Term , TF &gt; pairs, where TF is the frequency of this term occurring in this document. Formally, let D denote the content of a document. Supposing that D contains n different terms, we describe D by 
For a corpus of documents, we give simple definitions for some terminologies which will be used in the rest of this paper. Definition 1 . ( Document Vector ) The document vector of a corpus is an ordered array of all document objects in this corpus. Let DV denote it. different terms in this corpus. We signify it by TV . Obviously, the relationship between document vector and term vector can be modeled terms. A relationship matrix can be deduced from this bipartite graph, with | DV | rows and | TV | columns, where | DV | denotes the number of documents in DV and analo-gously for | TV |. Let R denote relationship matrix. 
We take a corpus example of documents shown in Figure 2(a) for the convenience bipartite graph corresponding to corpus example in Figure 2(b). The relationship matrix between documents and terms is easy to obtain in Figure 2(c). 3.2 Transition Probability between Documents From the viewpoint of Random Walk Theory, given a directed unweighted graph G ( V, E ), supposing there is a random surfer standing on node a , he has identical pos-sibility to visit each node at which he can arrive on next step, and zero possibility to edge weight. This possibility is entitled  X  X ransition probability X  by researchers. 
Treating every document in DV as a node and transition probability of each pair of documents as an edge, the corpus of documen ts can be viewed as a graph. However, the transition probability between two documents can X  X  be ascertained directly. The reason is, from the view of a random surfer in document A , he do not know any inter-nal information (the content) of another document B . He only knows the terms in A . So our solution is using terms as the bridge between two documents. A normalization step is performed before computing of transition probability. From the probability theory, transition probabilities of a node to all other nodes terminology  X  transition matrix  X  is defined by researchers to represent the normalized element in the i -th row and the j -th column of T ( O 1 , O 2 ). 
Base on the above discussions, the transition matrix from documents to terms can transpose of R . 
We model the computation of transition probability from document A to B as two some term T n with transition probability P ( A , T n ), and the second step is another ran-steps mean a multiplication. Considering all terms including T n , we take to represent the transition probability from document A to B , where | TV | is the number of different terms in term vector TV . 
Considering the transition matrix of documents, supposing A and B are the i -th and T ( DV , TV ) and P ( T n , B ) = T nj ( TV , DV ). Thus we have the following theorem. 
Theorem 1 . The transition matrix of documents, T ( DV , DV ), can be computed by Proof . From Equation (3) we can get ) , ( ) , ( ) , ( which can be also obtained from Equation (2). and T ( TV , DV ) using relationship matrix R shown in Figure 2(c). The transition matrix of documents is computed by Equation (3) and shown in Figure 3(b) with another not equal to zero , which means the random surfer has possibility to stay in his current position on next step. 3.3 Assessing Similarity two random surfers starting from A and B respectively (similar definition is found in [5]). It is easy to get Sim ( A , B ) = Sim ( B , A ) according to this definition. In our method, the meeting probability (or similarity) is reinforced step by step. Each step of random surfers means a re-distribution of meeting probabilities, and and B has m outgoing edges, there are n X m docum ent-pairs needing to be considered. Let d be a decay factor (usually d = 0.8) and we represent Sim k+ 1 ( A , B ) by B ments in DV respectively ( p  X  q ), we get Based on Equation 5, we give a theorem as follows. Theorem 2 . The ( k +1)th iteration of similarity matrix S k +1 ( DV ) can be computed by rection matrix making every element on diagonal of S k +1 ( DV ) to be 1. Proof . In fact, Equation (6) is the matrix form of Equation (5). A notice here is the initialization of S k ( DV ) when k = 0. Since we can X  X  foreknow the similarity between two objects before iteration, it is reasonable to simply define ( S and is symmetrical, which ensures Sim ( A , B ) = Sim ( B , A ) on all iterations. 
Our similarity computation method can be viewed as an extension of SimRank [5] on directed weighted graph. The naive method of SimRank is only applicable for undirected unweighted graph. Besides, the starting points of our method and SimRank our method evaluates the meeting probability of two random surfers. SimRank gets an iterative formula similar to Equation (4), and the mathematic proof of convergence given in the Appendix of [5] is also adaptable for our method with a few changes. 
Let X  X  consider the example shown in Figure 2(a). Using the transition matrix and the convergent result S ( DV ) (via 10 iterations) is presented in Figure 4(b). 
We summarize the major steps of link-based content analysis method as follows. 1. Preprocessing. Remove stop-words from document contents and stem the left words (Section 3.1). 2. Obtain relationship matrix R between documents and terms using statistical tech-nologies (Section 3.1). 3. Compute transition matrix of documents T ( DV , DV ) by Equation (3) (Section 3.2). 4. Initialization. Set k = 0 and S 0 ( DV ) to be a | DV |-by-| DV | identity matrix. 5. For the ( k +1)th iteration, compute S k +1 ( DV ) using Equation (6) (Section 3.3). jump to step 5; else return S k +1 ( DV ). 3.4 Complexity Analysis and the average outgoing edge number of a node is d . The time cost on our link-based content analysis can be roughly divided into two parts: (1) Computation of transition matrix. According to Equation (2), the time consumed by transition matrix computa-tion is O ( mn 2 ). (2) Iterative computation of similarity matrix. According to Equation (4), supposing the number of iterations is k , time complexity for iterative computation There are some methods for improving performance of SimRank , such as pruning point of this paper. The aim of our work is to introduce a link-based method into con-tent analysis, and combine internal and external information (introduced in Section 4) for more accurate similarity measuring. In addition, the convergence feature studied in Section 5.3 indicates that the result is acceptable when k = 1. Relationships between documents can be explored from different aspects, for instance information of documents is using a link graph. Extensive studies have been per-formed on exploiting the link structure such as web graph. In this section, we focus on how to integrate external information into link-based content analysis. 4.1 Integration of Transition Matrices Usually, there is more than one kind of relationships between documents, and we only consider the ones independent with each other. For example, references and authors are independent, while references and  X  X ited-by X  are not. A relationship matrix R can be obtained according to each relationship. Simi lar with link-based content analysis in Section 3.2, the transition matrix between documents can be described as T ( DV , DV ) = R N . Taking documents shown in Figure 1(a) as an example, transition probabilities of citation relationship are shown in Figure 1(d). 
For a corpus of documents, supposing there are K different relationships between T ( DV , DV ). Considering the transition matrix computed by link-based content analy-ces and compute a weighted mean matrix. That means the integrated transition matrix T ( DV , DV ) can be calculated by the following formula. where w i is the weight of T i ( DV , DV ) and 1 computed iteratively using Equation (6). 4.2 Estimating Weights by machine learning methods such as decision tree and neural networks, we design an approach to learn weights from training dataset. Usually we take a portion of classified documents (e.g. 10%) as training dataset. For a transition matrix T i ( DV , DV ), we get corresponding similarity matrix S ( DV ) by | DV |. The weight of T i ( DV , DV ) can be estimated by We have described a method for link-based content analysis and a solution to com-bine internal and external information. In this section, the accuracy and iterative proc-ess of similarity propagation will be tested, compared with (1) VSM [2], a traditional content-based method. We implement it strictly following this equation term in a document. (2) SimRank [5], a link-based approach on graph. 
The similarity score between two objects is hard to ascertain without performing extensively user studies. ACM Computing Classification System [22] (CCS) is a credible subject classification system for Co mputer Science, which provides an identi-fication of similar papers by organizing these papers in the same category. Note that CCS is only a rough evaluation of similarity. Our dataset is crawled from three cate-gories in ACM CCS, and contains 5469 documents with the ineffectual papers re-moved. There are three kinds of informatio n in this dataset: (1) Document-term in-formation. Terms are extracted from ABSTRACT of each paper. This relationship is used for content analysis. (2) Reference information. If document A refers to docu-ment B , there is an edge from A to B , so we can construct a citation relationship which thor, an edge between A and B exists. Thus author relationship can be described as an undirected weighted graph. More details are listed in Table 1. 
All experiments are performed on a PC with a 1.86G Intel Core 2 processor, 2 GB memory, and Windows XP Professional. All algorithms are implemented using Java. 5.1 Similarity Evaluating In this section we report experiments to examine the accuracy of our method, com-pared with others. For the given ACM dataset, we compute the similarity matrix of documents in five different ways. To be specific, based on document-term relation-ship, we obtain two similarity matrices using VSM and our link-based content analysis respectively. Another two are obtained by using SimRank on reference relationship obtain a holistic similarity measuring of this dataset. 
We use PAM [8] to cluster papers into groups based on similarity matrix. Compar-ing these groups with CCS categories, we define accuracy as the maximal ratio be-tween the number of correct classified papers and the total number of papers. 
Table 2 shows the accuracy of different approaches. The accuracy of VSM is not very good compared with other methods, mainly because VSM behaves weak in large amount of short documents (e.g. web pages). Anyway, our link-based content analysis gets comparable accuracy with SimRank . Then, we compute weights via accuracy according to Equation 8, and obtain a combin ation of internal and external informa-tion by Equation 7. The accuracy of this combination is 56.81% and higher than oth-ers, which means a more accurate similarity measuring. 
To observe the reinforcement of similarity in iterative process of our link-based of each document-pair on the k -th iteration. In Figure 5, the similarity score increases iteration by iteration, and the rising amplitude of each iteration indicates the effect of this iteration to final convergent similarity. 5.2 Performances We have discussed issues about time complexity in Section 3.4. Supposing there are n documents and a document has m terms on average, VSM takes O ( mn 2 ) time and for analysis takes O ( kd 2 n 2 ) time, where d is the average number of outgoing edges. 
Table 3 lists performances of different a pproaches. We can see link-based content viewing terms as bridges between documents, outgoing edge number of a document is usually big. As we have said before, ther e are some technologies to improve the per-formance of our method. In next subsection, convergence feature indicates that we only need to perform the first iteration when using link-based content analysis. More-over, our link-based content analysis can be combined with external information such results in a performance similar to link-based content analysis. 5.3 Convergence Feature Iterative process is common for link-based methods based on random walk on graphs. maximum difference and accuracy. If let Sim k ( A, B ) denote the similarity score on the tolerance factor of convergence (e.g. 0.001), iterative process will stop. 
We get maximum difference and the accuracy of each link-based method on each iteration shown in Figure 6(a) and Figure 6(b) respectively. Comparing with other reason is that dense links between docum ents can accelerate similarity propagation and result in faster convergence rate (but also consume more time on each iteration). 
Based on this feature, we can draw a conclusion that in most cases, similarity com-puted by the first iteration of link-based co ntent analysis is acceptable for most appli-Sim 1 ( A,B ) to avoid expensive computing cost of iterative process. In this paper, we introduce a link-based method to content analysis, and by exploiting document-term relationship, we propose a link-based content analysis to measure document similarity iteratively. Moreover, our link-based content analysis can be combined with external information to obtain a more accurate similarity measuring. 
The contributions of our work are summarized as follows.  X 
We introduce a link-based method into content analysis research. Traditionally, link analysis and content analysis are mutual noninterference, and they differ in re-search routes and theory models. Our meth od is a link-based content analysis based on random walk on graphs.  X 
The internal and external information of documents can be combined effectively using the same computational model. That means our method not only suits for content analysis (utilizing internal information), but also is applicable for utilizing external information such as references, authors and publication date, etc. This work was supported in part by the National Natural Science Foundation of China under Grant No. 70871068, 70621061, 70890083, 60873017, 60573092 and 60496325. 
