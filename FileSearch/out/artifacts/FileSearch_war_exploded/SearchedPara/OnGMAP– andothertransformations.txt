 As an alternati ve to the usual Mean Average Precision, some use is curren tly being made of the Geometri c Mean Average Precision (GMA P) as a measure of average searc h e ecti ve-ness across topics. GMAP is speci cally used to emphas ise the lower end of the average precision scale, in order to shed light on poor perform ance of search engines. This paper discuss es the status of this measure and how it should be unders tood.
 H.3.3 [ Informat ion Systems ]: Inform ation Storage and Retrieval| infor matio n search and retrieval Meas urem ent evaluati on, e ecti veness measures
In the now fairly long history of evaluating informa tion re-trieval systems, we have accum ulated a signi can t number of di eren t measures of e ectiveness. There are favourites (such as Mean Average Precision, MAP) which are very widely used, and there is some sense it is good to use a measure with which other researc hers are familiar; but there is nothing close to agree ment on a common measure which everyone will use.

Some of the di erences between authors have to do with constrai nts external to the measur e itself: for example, if there are multi-level relev ance judgemen ts, and the author wishes to make use of these , s/he is constrained in the choice of measure (e.g. MAP does not take accoun t of multiple lev-els). Other di erences have to do with trans parency (e.g. it Cop yright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. is very obvious what Precision at rank 5, P@5, means; MAP is less clear), with stabi lity (e.g. MAP is known to be a more stable measure than P@5), with what the measure seems to be measur ing or with a task or user model (e.g. Mean Recip-rocal Rank, MRR , is only concerned with the rst relevant documen t, which may be appropriat e for factual questions; MAP is concerned with all relev ant documen ts, which may be appropriat e for a literat ure review). There is a whol e raft of di eren t criteria which may be applied.

One particular recen t concern has been with `dicul t' top-ics: those queries or topics on which systems seem to do badly. The TREC Robust track [11] is devoted to this ques-tion. The measure ment issue here is not so much about the prim ary measure, but about the way it is averaged or oth-erwise summari sed over a set of topics. The track has used di erent measures , but the main measure used recen tly, fol-lowing a suggesti on from the present author, is Geomet ric Mean Average Precisio n, GMAP . The geom etric mean is now being used on other measures too e.g. [4]; and other similar tranform ations have been prop osed [2].

This paper discusses the rationale for this choice. It does not make any original prop osals for measures or measure-ment (except for trivial variations), nor does it present any data. Its contribution is to lay out the argumen ts systema t-ically and reasonably comprehens ively, and to help readers think about the choice and its implications. The central ar-gument, however, is a polemical one: that in principl e, the geomet ric mean (or mean of logs) is just as valid and useful a form of average as the (arithmeti c) mean, and should be accorded a similar status.
This paper is not primari ly about the Average Precision measure as de ned on an individual query or topic , but what happ ens after that. However, it is appropriate to revisit the basic ideas behind this measure, as a reminder.

We consider just one query at a time. We assume that the output of a search system in response to a query is a ranked list of items, and also that user judgemen ts cate-gorise documents in binary fashion as relev ant or not to the topic or inform ation need that gave rise to the query. The e ect iveness of the system is general ly measured in terms of how well the system succeeds in ranking relevant documen ts (according to the user's judge ment) at or near the top of the list. If we think in terms of the tradi tional measures of re-call (prop ortion of the relevant documents that have been retrieved) and preci sion (prop ortion of the retrieved docu-ments that are relev ant), stepping down the ranked list from the top involves stepping up the recal l scale, each time we encoun ter a relev ant documen t in the list. Typical ly, as we increase recall in this way, precisio n tends to drop. Plot-ting preci sion at di eren t recal l levels gives us the familiar recal l-preci sion curve. When comparing two systems, if sys-tem A has a higher curve than system B, we regard it as more e ecti ve.

E ecti veness measures may be broadl y categorised into those that relat e to a singl e rank position or point on the recal l-preci sion curve, and those that attempt to describe the entire curve or ranking. P @10, precisio n at rank ten, describes a single point only. Average preci sion is essen-tially a measure of the area under the recall -preci sion curve, therefore re ecti ng the entire ranking. This `area-unde r-the-curve' interpretati on provides some sort of intuitive interpre-tation of what average preci sion is trying to measure . Formal ly, we follow the usual `non-interpolated' de nition: where R is the set of relev ant documen ts, r is a singl e rel-evant documen t, and P @ r is the precision at the rank po-sition of r . In practical term s, Average Precision is usually calculated from a ranking which is truncat ed at some rank position (typically 1000 in TREC, maybe much lower in a live system). In these condi tions, for any r which ranks be-low this truncation point or not at all, P @ r is treated as zero. This is usual ly a reasonabl e appro ximati on.
In any experimental evaluation of systems over many queries, there is considerable variation in the Average Pre-cision of di eren t queries. The usual approac h is to take the (arithmeti c) mean of Average Precision values over the queries. This is the measure known as Mean Average Preci-sion. Because the per-query Average Precision is norm alised to the range [0,1], irrespective of (say) the number of rele-vant documen ts in the query, the mean can be said to treat all queries equally.

Average Precision is someti mes criticised as an opaque measure which is dicul t to unde rstand, as compared to (say) P @10, whose meani ng would be immediately apparen t to a user { although as indicated above, the `area-unde r-the-curve' interpretat ion provides some intuitive validati on. As against this supp osed opaq uenes s, it is known to be a much more stable measure than (say) P @10 [1], is more sensitive to di erences between rankings, and tends to predict other measures well. It is also, despite being a measure on the whole curve, much more sensitive to di erences at the top of the ranking than lower down { a characteristi c which is absolut ely necessary to ranking evaluation measure s used in inform ation retriev al. These reasons make it attract ive to researchers, despite its lack of transparency .
The de nition of the geom etric mean of n values is the n th root of the product of the n values: However, an alternativ e way to think about it which is ac-tually better for our presen t purp ose is that we take logs of the observ ations and average them. We may convert this back to the original scale by exponen tiating, but this might be regarded as optional . Thus we might de ne the Average Log (AL):
For this purp ose the base of the logarit hms is e . Note also that it is assumed here that the original values x i positive. General ly this will be applied only to variabl es that are constrai ned to be non-negati ve, but the possibility that one or more of the observ ed values is zero will cause probl ems; this is discus sed further in section 4.1.
The log is a strictly monot onic transform ation (log x i log x j () x i &gt; x j . Thus whatev er measur e the x val-ues represe nt, the log preserves all its ordinal or prefe rence quali ties.

In the main application of geomet ric mean considered in this paper, the x i values are the average preci sion values for individual queries, and the geomet ric mean is what is referre d to in this paper as GMA P. Note that if the values x are always less than one, as in the average precision exampl e, then the log is always negative, thus AP itself is negati ve. This is not a problem in a formal sense, however, it may be though t confusing. This might be taken as a reason for exponen tiating back to the original scale.
 Because of its monot onici ty, for a single query, log(Average Precision) will displa y precisely the same prefer ence order of systems as Average Precision itself. However, because of the non-l ineari ty of the transformat ion, it will average dif-feren tly over a set of queries. The same statemen ts apply to any other measure of e ecti veness.
There is a traditional way of classifying measuremen t scales which is useful in this context, due to Stevens [5]. Scales may be (in order of increas ing constrai nt) Nominal, Or-dinal, Interval or Ratio. Nom inal scales have (necessaril y discrete) unordered values: a set of exclusive categories is a nominal scale for the categori sed objects. Ordinal scales have the addit ional requirement that there is a natural order on the values; an exampl e might be a relevance scale (e.g. highly relevant, parti ally relevant, not relevant). Interval scales have the addit ional prop erty that di erences can be compared; for example, temp erature is usually regarded this way (a temperature change from 10 to 15 is in some sense the same as a change from 30 to 35 ). Ratio scales ad-ditional ly require ratio s to be comparabl e; many physical measuremen ts such as lengt h are ratio scales. Note however that ratio scales must have a meaningful zero; thus temper-ature (in most of the scales usuall y used) has an arbit rary zero, and does not have the ratio prop erty. For exampl e, it is meaningl ess to say that a temperature of 20 is `twice as hot as' 10 .

This traditional classi cation is fairly simple, missing out a number of subtleties, and has been extens ively criticised (e.g. [7]). Nevertheless , it does provide some useful in-sights. In parti cular, certai n standard statistical operati ons make assumptions about the scales, which are sometimes questionable. The prime example is averagi ng (arithmet ic mean) . Taking a mean of a set of observations of a vari-able implicitly assumes that the variabl e is on an interval scale: for exampl e, if one observation is raised by 5 units, and another is lowered by 5 units, the mean operation can-cels these changes out. Nevertheless , means are used for many variables whose adherence to the interval prop erty is at best questi onable. Similarly, the most obvious interpre-tation of GM would probably involve an assumpti on of the ratio prop erty { which is equivalent to the interval prop erty on the log values.

In the case of a variable which cannot be assumed to be interval, or for which the assumpt ion is dubious, we might take di eren t views . A measure of central tendency which is strictly suitable for ordinal-only scales is the median; this can be useful, but is somewhat limited. Alternati vely, we might take means anyway, but look for ways to investigate the e ects of making the interval assumption, and the sen-sitivity or robustnes s of the result s to this assumption.
If we have any reason to doubt that a di erence of 5 points at one end of the scale is comparable to a di erence of 5 points somewhere else, then we might investigate sensitiv-ity/robustnes s by putting our values through a nonlinear but monoto nic trans formati on before averagi ng them . Note that if the interval assumpt ion is not valid for the original measure nor for any speci c transformat ion of it, then any monotoni c transforma tion of the measure is just as good a measur e as the untransformed version. If we believe that the interval assumpti on is good for the original measure, that would give the arithmeti c mean some validity over and above the means of trans formed versions. If, however, we believe that the interval assumption might be good for one of the trans form ed versions, we should perhaps favour the transformed version over the original . But if there is no parti cular reason to believe the interval assumpti on for any version, then all versions are equally valid. If they di er, it is because they measure di erent things.

We might there fore go through this process repeatedl y, with any number of di erent transformat ions. Good robust-ness would be indicated if the concl usions looked the same whatev er transform ation we used; if we found it easy to nd transform ations which would substan tially change the concl usions, then we might infer that our concl usions are sensitive to the interval assumpt ion, and that the di erent transforma tions measure di eren t things in ways that may be important to us.

It will be clear now that the IR evaluat ion eld has indee d been taking a step along this path. The transformat ion con-cerned is the log; in the Robus t Track, we have disco vered that some of our conclusions are indee d sensitive to the in-terval assumpti on. Speci cally, for example, on the original MAP , we can get signi can t gains through blind feedbac k; on the transform ed measure (or equiv alently on GMAP ), blind feedbac k is often found to be detrimental. Depend-ing on our view of the relative importance of di erences in di erent parts of the scale, a GMAP -based conclusion that blind feedbac k is detrimental is just as valid as a MAP-based concl usion that blind feedbac k is bene cial. (See the next section for a further discussion on this point.)
The conclusion must be that MAP and GMAP measure somewhat di eren t things. Each reveals di erent aspects of system e ecti veness, in much the same way that MAP and P@5 reveal di eren t aspects. The di erence between MAP and P@5 has to do mainly with emphasis on the di erent parts of the recal l-preci sion curve; the di erence between MAP and GMAP has to do solely with emphas is on di erent parts of the e ectiveness distribut ion over topics.
In section 4.2 below, we discuss a di eren t transformat ion that may be useful for some measures .
To illustrat e the di erences between the arithm etic and the geometri c mean, we generat e some random data: 50 data points between zero and one, with deliberately high varianc e but biased towards the low end. They may be though t of as average preci sion observations for 50 topics. A histogra m of the distri butio n of data points used is shown in Figure 1. Note also that there are three zeros in the set, and also three observations of 0.01. Figure 1: Distribut ion of values in the example.

The arithm etic mean of these values (MA P) is 0.309; the geomet ric mean (GMAP) is 0.117 (see section 4.1 below for how we deal with the zeros). Note that we expect GMA P to be less than MAP .

We indicate the way in which GMAP is sensitive to the smal ler observ ations as follows: we boost each value by 0.05. For obvious reasons , MAP increase s by the same amount, to 0.359; GMAP however increas es much more, to 0.263. The reason for this is that the boost of 0.05 has prop ortionatel y more e ect on the very smal l observations (zero increas es to 0.05 and 0.01 to 0.06) . If we split the observ ations into the lowest and highest 25, and apply the boost to one set only, the e ect on MAP is the same for both sets: an increase of 0.025. On the other hand, GMAP increases to 0.249 when we boost the lowest 25, but only to 0.123 when we boost the highest 25.
The emphas is of the Robus t Track at TREC can be de-scrib ed in two slightly di erent ways: either it is prim arily about a class of topics which might be term ed `dicult', or it is about the distributi on of e ecti veness over topics, and in parti cular the lower end of the e ectiveness scale. In part it has revolved around identifying dicul t topics and biasing measuremen t towards those topics; in other words, regard-ing (for this purp ose) some topics as more importan t than others. Initially, the methods of measuremen t were designed explicitly with this in mind [9].

The shift to GMA P actually changes the emphasis very slightly. If we think of GMAP as the average of logs (whether or not we transform it back to the AP scale after averaging) , then it becomes clear that there is no emphasis on partic ular topics. The average of logs (of AP) is a straight arithmet ic mean over topics; each topic has exactly the same in u-ence, there is no weighting based on topic s. The empha sis is instead based on the scale of the e ect iveness measure. That is, GMAP treats a chang e in AP from 0.05 to 0.1 as having the same value as a change from 0.25 to 0.5. MAP would equate the form er with a change from 0.25 to 0.3, regarding a change from 0.25 to 0.5 as ve tim es larger. These two statemen ts e ectively exempl ify the interval as-sumpti ons applied to GMAP and MAP respectively. In the opinion of the present author, there is no reason to believe that one of these is correct and the other incorrect, or even that one is better than the other.

The distinction between emphas ising di eren t topics and emphasisi ng di erent parts of the scale may seem slight, but is importan t. It reinforces the point that a concl usion based on GMAP has exactly the same status as a conclusion based on MAP . In both cases we are giving equal weight to all topics in the test set. We might for external reasons prefer one to the other (one possibl e reason is discusse d in section 4.4 below), but there is little intrinstic di erence.
There is one argum ent that might be used about the va-lidity of the interval assumpti on, by reversing the example above. If we consider a change in AP for one topic of 0.5 to 0.25, what would be a corresp onding chang e for a topic with a starting AP of 0.1? Using untransformed AP the change would have to be from 0.1 to -0.15 { but this of course is impossibl e. In this sense, we might argue that the interval assumption appli ed to a measure which is constrai ned to be positive does not make sense. On the other hand, this example works perfectly sensibly in the log AP scale.
However, the log AP scale does not deal with a similar probl em at the other end: AP is also constrai ned to a maxi-mum of 1. In order to deal with the probl ems at both ends, we could use another transforma tion, the logit, as discuss ed in section 4.2 below. This argum ent seems to sugges t not that all trans formati ons have equivalent validity, but rather that the logit transformat ion actual ly produces a measure in which the arithmetic mean operatio n has mo re validity than it does in the original scale.
The log transform ation maps from the strictly positive reals to the entire real line. It is not de ned for zero or negati ve numbers. As implied above, it is usual ly appli ed to quan tities which are constrained in some way to be positive. If the variable is a prop ortion, or perhaps a probabil ity es-timated by a prop ortion, then it can in princ iple and may in practice somet imes take the value zero. In the case of a probabi lity, we might reasonabl y assume that the true value is never zero; nevertheless , it may be that in the sampl e from which we are estimating, zero can arise.

Similarly Average Precision as usually calculat ed may be estimated as zero for a few individual topics. In princi ple Average Precision de ned over a ranking of the entire col-lection cannot be zero. That is, the measure is unde ned if there are no relevant documents; if there are relev ant docu-ments, and we rank the entire collection, then at the rank position of each relev ant document there must be a non-zero precisi on. Nevertheless , as indicat ed above, the usual way of calcul ating Average Precision is to truncate the ranking at some point. A relev ant documen t which is not retrieved before the trunc ation point is assumed to contribute zero precisi on to the calculati on. This means that a query which fails to retrieve any relev ant document at all before the trun-cation point will be assigned zero average preci sion. Thus we need to deal with this situati on in practice.

A simple pragm atic soluti on to this problem is to add a smal l quantity to the estim ate before taking the log, and remo ving it again afterw ards, thus: wher e is an arbitrary small number.

This is a little untidy, because it requires the speci cat ion of and allows compari sons only in the case where the same value has been used. One could perhaps discover a ratio-nale or princi ple for the choice of { possibly on the basis of estimati on argumen ts. One possibility for the Average Precision case is to change the rule about assigning zero to unretri eved relevant documents, and instead pretend that they are all ranked at the very end of the entire collection. Thus each would have a smal l but non-zero precisio n value of (coun ting backwards from the last) j R j N , j R j 1 where N is the size of the collection. This gives a deter-ministic lower bound Average Precision for a full-collection ranking. A variant would be to treat all documents below the truncat ion point (the entire remaining collection) as ran-domly ranked. This would give a slightly higher, stochastic lower bound.
 However, these are probably unneces sary complications. If we were to observ e that many queries had zero Average Precision under the usual method, we would probably want to do some very di erent failure analysis. If we are simply deali ng with a few instances, the method is probably quite adeq uate. In the Robus t track at TREC 2005, a varian t on the method, still involving an arbitrary smal l number, was used [11]. The individual topic AP values were not modi ed unless zero; every zero value was reset to 0.00001.
In the example above, we used the simple addition method above with = 10 5 ; as indicated, this gives GMAP= 0 : 117 If we reduce to 10 6 , GMA P falls to 0.102; if we increase to 10 4 , GMAP rises to 0.134. Thus GMAP is somewhat sensitive to the chosen ; it will also a ect a little the relative sensitivity of GMAP to di eren t small values.
Clearl y the log transformat ion is not the only one that may be useful. The log is intended to deal with the sit-uation where , for the e ectiveness measure concerned, the region close to zero is critical. For probabil ity or prop ortion measures , it is sometimes the case that the region close to 1 is also critical, and always the case that the meas ure is constrai ned to a maximum of 1. Thus for example in spam ltering, a success rate of 99.99% is really very di erent from a success rate of 99.9% . Most measure s comm only used in IR do not norma lly approac h so close to 1; however, the spam example indicat es that there are exceptions.
A transform ation which deals with both sensitivities si-multaneous ly is the logistic or logit transform ation (in the case of a probabi lity, also known as log-odds): This transformat ion is, like the log, strictly monotoni c. It has been used in the Spam Track at TREC [2]; the reason for its use here is related to the argum ents of this paper. It is not to do with averagi ng across queries, but with combin-ing two di erent measures of performanc e, each of which is an error probabi lity, into a singl e composite measure. Re-cently, Cormac k [3] has shown that under some sampl ing assumptions, estimation errors in logit(Average Precision) appro ximately t a Gaussian distribut ion; the t is better than estimation errors for Average Precision itself.
As indicated above, it is at least arguabl e that the appro-priate transformat ion of a prop ortion or probabil ity (whic h is constrai ned to lie between zero and one) is the logit or log-odds, becaus e this gives it a ( 1 ; + 1 ) range, and therefore allows the application of the interval assumption (as well as the use of linear models) without built-in contradictions at the ends of the scale. Cormac k's result s might be taken to supp ort the same concl usion. All the qualitative arguments in this paper apply just as well to the logit as to the log.
For this transformat ion, the zeros probl em also potentially occurs at both ends: the logit is unde ned if either p = 0 or p = 1. If the estimation process could yield such result s, then some similar solution to the above is required to deal with both ends, for example by adding to both numerator and denom inator in the de nition of logit.
As in the previous case, transform ations such as the log or logit might potentially be usefull y applied to other measures than average preci sion. An obvious example is NDCG, nor-malised discoun ted cumulative gain, for multi-level relevance judgemen ts. This has similar distributi onal charact eristic s to MAP: that is, for a signi can t number of queries, it might take a value very close to zero. If we wish to examine that end of the distribution of e ect iveness over queries, then it makes sense to consi der geometri c mean NDCG, as opposed to the usual arithmeti c mean.
As indicated at the beginni ng, one considerati on in the choice of measure may be its stability or robus tness under various condit ions { for example, its statistical reliability when measured on limited numbers of topics [8]. There is evidence that MAP is rather more stable in this respect than many other measure s. Such stability would not neces-sarily transfe r directly to trans formed versions of AP such as GMAP .
 Anal ysis done for the TREC Robust Track indicates that GMA P, while not as stable as MAP, is still reasonably stabl e, more so than the measures previously used in the track [10].
To some extent, IR researchers are used to dealing with multiple meas ures. In the opinion of the prese nt autho r, a signi can t contribution to the success of the TREC pro-gram me is the variety of measures in use. Many of the tracks report a whole range of di eren t e ectiveness mea-sures, which may or may not be highly correl ated; there may be a singl e speci ed main measure , but others may re ect di eren t features . All this is good, both as a san-ity check on the results and because di eren t measures may show di eren t things.

On the other hand, researc hers would often prefer the sim-plicity of a singl e measure; those who are not interes ted in issues of measuremen t per se may nd the variety confusing and hard to deal with. More particularly, in recent years, the probl em of optimisation (in relati on to a set of free parame-ters of a ranking funct ion, say) has encouraged the choice of a single measure; it is hard to optimise more than one mea-sure at once, parti cularl y in the context of an autom atic, iterativ e procedure.

Even in this case, however, variety may be good. There are many open issues associated with choice of measure s for optimisation [6]. For one, it is not clear that the measure that you really want to optimise in the test set should be the one that you optimise in the traini ng set. In this sense, having a variety of measures and unders tanding the charac-teristics of each and how they relat e may be helpful.
MAP and GMAP may be seen as similar measures of av-erage ranking e ecti veness of a system across a test set of topics. They di er not in how much attention they pay to di erent topics in the set, but in how much atten tion they pay to di eren t parts of the Average Precision scale. Given that a strong assumpti on about the interval nature of the AP scale is not justi ed, the two average measures might rea-sonably be regarded as having equal validity. Experimental results suggest that they measure somewhat di erent things.
They are, in e ect , di erent members of the menageri e of measures developed over the last half-century for retrieval system e ectiveness . The size of this menageri e may be probl ematic for researchers, but has some distinct advan-tages.

More generall y, given any measure of search or ranking e ect iveness, there are good reasons to consider also mono-tonic transform ations of such a measure, which might place di erent emphasis on than the original on di erent parts of the scale. The two transformat ions considered in this paper, the log and the logit, have been shown to reveal interes ting prop erties not apparen t when we consider only the original measures to which they were applied. Thus they may be said to contribut e to our unde rstandi ng of ranki ng e ecti veness. [1] C. Buckley and E. Voorhee s. Evaluating evaluati on [2] G. Corm ack and T. Lynam. TREC 2005 spam track [3] G. V. Cormack and T. R. Lynam. Statistical precision [4] T. Sakai. Evaluating evaluation measures based on the [5] S. S. Stevens. On the theory of scales of evidence. [6] M. Taylor, H. Zaragoz a, N. Craswell, and [7] P. F. Velleman and L. Wilkinson. Nominal, ordinal, [8] E. Voorhees and C. Buckley. The e ect of topic set [9] E. M. Voorhees . Overview of the TREC 2003 robust [10] E. M. Voorhees . Overview of the TREC 2004 robust [11] E. M. Voorhees . Overview of the TREC 2005 robust
