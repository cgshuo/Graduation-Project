
Department of Electrical and Computer Engineering, National University of Singapore, Singapore Data clustering is a fundamental problem in many fields, such as machine learning, data mining and computer vision [1]. Unfortunately, there is no universally accepted definition of a cluster, probably belonging to a cluster satisfy certain internal coherence condition, while the objects not belonging to a cluster usually do not satisfy this condition.
 Most of existing clustering methods are partition-based, such as k-means [2], spectral clustering [3, 4, 5] and affinity propagation [6]. These methods implicitly share an assumption: every data point must belong to a cluster. This assumption greatly simplifies the problem, since we do not need to judge whether a data point is an outlier or not, which is very challenging. However, this assumption also results in bad performance of these methods when there exists a large number of outliers, as frequently met in many real-world applications.
 The criteria to judge whether several objects belong to the same cluster or not are typically expressed by pairwise relations, which is encoded as the weights of an affinity graph. However, in many applications, high order relations are more appropriate, and may even be the only choice, which naturally results in hyperedges in hypergraphs. For example, when clustering a given set of points into lines, pairwise relations are not meaningful, since every pair of data points trivially defines a line. However, for every three data points, whether they are near collinear or not conveys very important information.
 As graph-based clustering problem has been well studied, many researchers tried to deal with hypergraph-based clustering by using existing graph-based clustering methods. One direction is to transform a hypergraph into a graph, whose edge-weights are mapped from the weights of the original hypergraph. Zien et. al. [7] proposed two approaches called  X  X lique expansion X  and  X  X tar expansion X , respectively, for such a purpose. Rodriguez [8] showed the relationship between the spectral properties of the Laplacian matrix of the resulting graph and the minimum cut of the orig-inal hypergraph. Agarwal et al. [9] proposed the  X  X lique averaging X  method and reported better results than  X  X lique expansion X  method. Another direction is to generalize graph-based clustering method to hypergraphs. Zhou et al. [10] generalized the well-known  X  X ormalized cut X  method [5] and defined a hypergraph normalized cut criterion for a k -partition of the vertices. Shashua et al. [11] cast the clustering problem with high order relations into a nonnegative factorization problem of the closest hyper-stochastic version of the input affinity tensor.
 Based on game theory, Bulo and Pelillo [12] proposed to consider the hypergraph-based clustering problem as a multi-player non-cooperative  X  X lustering game X  and solve it by replicator equation, which is in fact a generalization of their previous work [13]. This new formulation has a solid theoretical foundation, possesses several appealing properties, and achieved state-of-art results. This method is in fact a specific case of our proposed method, and we will discuss this point in Section 2 . In this paper, we propose a unified method for clustering from k -ary affinity relations, which is applicable to both graph-based and hypergraph-based clustering problems. Our method is motivated relations, and most of these (sometimes even all) k -ary affinity relations should agree with each other on the same criterion. For example, in the line clustering problem, for m points on the same a line. The ensemble of such large number of affinity relations is hardly produced by outliers and is also very robust to noises, thus yielding a robust mechanism for clustering. Clustering from k -ary affinity relations can be intuitively described as clustering on a special kind of edge-weighted hypergraph, k -graph. Formally, a k -graph is a triplet G = ( V, E, w ) , where V = { 1 ,  X  X  X  , n } is a finite set of vertices, with each vertex representing an object, E  X  V k is the set of hyperedges, with each hyperedge representing a k -ary affinity relation, and w : E  X  R is a weighting function which associates a real value (can be negative) with each hyperedge, with larger weights representing stronger affinity relations. We only consider the k -ary affinity relations with no duplicate objects, that is, the hyperedges among k different vertices. For hyperedges with duplicated vertices, we simply set their weights to zeros.
 Each hyperedge e  X  E involves k vertices, thus can be represented as k -tuple { v 1 ,  X  X  X  , v k } . The weighted adjacency array of graph G is an and defined as Note that each edge { v 1 ,  X  X  X  , v k } X  E has k ! duplicate entries in the array M . For a subset U  X  V with m vertices, its edge set is denoted as E U . If U is really a cluster, then most of hyperedges in E U should have large weights. The simplest measure to reflect such ensemble phenomenon is the sum of all entries in M whose corresponding hyperedges contain only vertices in U , which can be expressed as: Suppose y is an n  X  1 indicator vector of the subset U , such that y v i = 1 if v i  X  U and zero otherwise, then S ( U ) can be expressed as: Obviously, S ( U ) usually increases as the number of vertices in U increases. Since there are m k summands in S ( U ) , the average of these entries can be expressed as: where x = y/m . As corresponds to the problem of maximizing S av ( U ) . In essence, this is a combinatorial optimization problem, since we know neither m nor which m objects to select. As this problem is NP-hard, to reduce its complexity, we relax x to be within a continuous range [0 ,  X  ] , where  X   X  1 is a constant, while keeping the constraint where  X  n = { x  X  R n : x  X  0 and abbreviated by f ( x ) to simplify the formula.
 The adoption of  X  1 -norm in (5) not only let x i have an intuitive probabilistic meaning, that is, x i represents the probability for the cluster contain the i -th object, but also makes the solution sparse, which means to automatically select some objects to form a cluster, while ignoring other objects. Relation to Clustering Game. In [12], Bulo and Pelillo proposed to cast the hypergraph-based clustering problem into a clustering game , which leads to a similar formulation as (5). In fact, their formulation is a special case of (5) when  X  = 1 . Setting  X  &lt; 1 means that the probability of choosing each strategy (from game theory perspective) or choosing each object (from our perspective) has an known upper bound, which is in fact a prior, while  X  = 1 represents a noninformative prior. This point is very essential in many applications, it avoids the phenomenon where some components of x dominate. For example, if the weight of a hyperedge is extremely large, then the cluster may only select the vertices associated with this hyperedge, which is usually not desirable. In fact,  X  offers us a tool to control the least number of objects in cluster. Since each component does not exceed  X  , the z . Because of the constraint x i  X  [0 ,  X  ] , the solution is also totally different from [12]. Formulation (5) usually has many local maxima. Large maxima correspond to true clusters and small maxima usually form meaningless subsets. In this section, we first analyze the properties of the maximizer x  X  , which are critical in algorithm design, and then introduce our algorithm to calculate x  X  .
 Since the formulation (5) is a constrained optimization problem, by adding Lagrangian multipliers function: The reward at vertex i , denoted by r i ( x ) , is defined as follows: Since M is a super-symmetry array, then @f ( x ) @x of f ( x ) at x . Any local maximizer x  X  must satisfy the Karush-Kuhn-Tucker (KKT) condition [14], i.e., the first-order necessary conditions for local optimality. That is, Since x  X  i ,  X  i and  X  i are all nonnegative for all i  X  X , x i &gt; 0 , then  X  i = 0 , and Hence, the KKT conditions can be rewritten as: V the solution of (5), which are further summarized in the following theorem.
 Theorem 1 . If x  X  is the solution of ( 5 ), then there exists a constant  X  ( =  X /k ) such that 1) the rewards at all vertices belonging to V 1 ( x  X  ) are not larger than  X  ; 2) the rewards at all vertices smaller than  X  .
 Proof : Since KKT condition is a necessary condition, according to (9), the solution x  X  must satisfy 1), 2) and 3).
 values of some components belonging to V d ( x ) must decrease and the values of some components r That is, let and define Then when  X  = min( x j ,  X   X  x i ) , the increase of f ( x ) reaches maximum; if r ij &gt; 0 , then when  X  = a prior (initialization) x (0) , the algorithm to compute the local maximizer of (5) is summarized in Algorithm 1 , which successively chooses the  X  X est X  vertex and the  X  X orst X  vertex and then update their corresponding components of x .
 Since significant maxima of formulation (5) usually correspond to true clusters, we need multiple initializations (priors) to obtain them, with at least one initialization at the basin of attraction of every significant maximum. Such informative priors in fact can be easily and efficiently constructed from the neighborhood of every vertex (vertices with hyperedges connecting to this vertex), because the neighbors of a vertex generally have much higher probabilities to belong to the same cluster. Algorithm 1 Compute a local maximizer x  X  from a prior x (0) 1: Input: Weighted adjacency array M , prior x (0) ; 2: repeat 3: Compute the reward r i ( x ) for each vertex i ; 4: Compute V 1 ( x ( t )) , V 2 ( x ( t )) , V 3 ( x ( t )) , V d ( x ( t )) , and V u ( x ( t )) ; 6: Compute  X  and update x ( t ) by formula (10) to obtain x ( t + 1) ; 7: until x is a local maximizer 8: Output: The local maximizer x  X  .
 Algorithm 2 Construct a prior x (0) containing vertex v 1: Input: Hyperedge set E ( v ) and  X  ; 2: Sort the hyperedges in E ( v ) in descending order according to their weights; 3: for i = 1 ,  X  X  X  , | E ( v ) | do 4: Add all vertices associated with the i -th hyperedge to L . If | L | X  [ 1 " ] , then break; 5: end for 6: For each vertex v j  X  L , set the corresponding component x v j (0) = 1 | L | ; 7: Output: a prior x (0) .
 For a vertex v , the set of hyperedges connected to v is denoted by E ( v ) . We can construct a prior containing v from E ( v ) , which is described in Algorithm 2 .
 Because of the constraint x i  X   X  , the initializations need to contain at least [ 1 " ] nonzero compo-nents. To cover basin of attractions of more maxima, we expect these initializations to locate more uniformly in the space { x | x  X   X  n , x i  X   X  } .
 Since from every vertex, we can construct such a prior, thus, we can construct n priors in total. From these n priors, according to Algorithm 1 , we can obtain n maxima. The significant maxima of (5) are usually among these n maxima, and a significant maximum may appear multiple times. In this way, we can robustly obtain multiple clusters simultaneously, and these clusters may overlap, both of which are desirable properties in many applications. Note that the clustering game approach [12] utilizes a noninformative prior, that is, all vertices have equal probability. Thus, it cannot obtain which means that it can only drop points and if a point is initially not included, then it cannot be selected. However, our method can automatically add or drop points, which is another key difference to the clustering game approach.
 In each iteration of Algorithm 1 , we only need to consider two components of x , which makes V pose the maximal number of hyperedges containing a certain vertex is h , then the time complexity of Algorithm 1 is O ( thk ) , where t is the number of iterations. The total time complexity of our method is then O ( nthk ) , since we need to ran Algorithm 1 from n initializations. We evaluate our method on three types of experiments. The first one addresses the problem of line clustering, the second addresses the problem of illumination-invariant face clustering, and the third addresses the problem of affine-invariant point set matching. We compare our method with clique averaging [9] algorithm and matching game approach [12]. In all experiments, the clique averaging approach needs to know the number of clusters in advance; however, both clustering game approach and our method can automatically reveal the number of clusters, which yields the advantages of the latter two in many applications. 4.1 Line Clustering In this experiment, we consider the problem of clustering lines in 2 D point sets. Pairwise similarity measures are useless in this case, and at least three points are needed for characterizing such a property. The dissimilarity measure on triplets of points is given by their mean distance to the best sensitivity of the similarity measure to deformation.
 these points have been perturbed by Gaussian noise N (0 ,  X  ) . We also randomly add outliers into the point set. Fig. 1 (a) illustrates such a point set with three lines shown in red, blue and green colors, respectively, and the outliers are shown in magenta color. To evaluate the performance, we ran all algorithms on the same data set over 30 trials with varying parameter values, and the performance is measured by F-measure.
 We first fix the number of outliers to be 60 , vary the scaling parameter  X  d from 0 . 01 to 0 . 14 , and the result is shown in Fig. 1 (b). For our method, we set  X  = 1 / 30 . Obviously, our method is nearly not affected by the scaling parameter  X  d , while the clustering game approach is very sensitive to  X  d . Note that  X  d in fact controls the weights of the hyperedge graph and many graph-based algorithms are notoriously sensitive to the weights of the graph. Instead, by setting a proper  X  , our method overcomes this problem. From Fig. 1 (b), we observe that when  X  d = 4  X  , the clustering game approach will get the best performance. Thus, we fix  X  d = 4  X  , and change the noise parameter  X  from 0 . 01 to 0 . 1 , the results of clustering game approach, clique averaging algorithm and our method are shown in blue, green and red colors in Fig. 1 (c), respectively. As the figure shows, when the noise is small, matching game approach outperforms clique averaging algorithm, and when the noise becomes large, the clique averaging algorithm outperforms matching game approach. This is because matching game approach is more robust to outliers, while the clique averaging algorithm seems more robust to noises. Our method always gets the best result, since it can not only select coherent clusters as matching game approach, but also control the size of clusters, thus avoiding the problem of too few points selected into clusters.
 In Fig. 1 (d) and Fig. 1(e), we vary the number of outliers from 10 to 100 , the results clearly demon-strate that our method and clustering game approach are robust to outliers, while clique averaging algorithm is very sensitive to outliers, since it is a partition-based method and every point must be assigned to a cluster. To illustrate the influence of  X  , we fix  X  d =  X  = 0 . 02 , and test the perfor-stressed in Section 2 , clustering game approach is in fact a special case of our method when  X  = 1 , thus, the result at  X  = 1 is nearly the same as the result of clustering game approach in Fig. 1 (b) under the same conditions. Obviously, as 1 / X  approaches the real number of points in the cluster, the result become much better. Note that the best result appears when 1 / X  &gt; 30 , which is due to the fact that some outliers fall into the line clusters, as can be seen in Fig. 1 (a). 4.2 Illumination-invariant face clustering It has been shown that the variability of images of a Labmertian surface in fixed pose, but under variable lighting conditions where no surface point is shadowed, constitutes a three dimensional linear subspace [15]. This leads to a natural measure of dissimilarity over four images, which can be used for clustering. In fact, this is a generalization of the k -lines problem into the k -subspaces problem. If we assume that the four images under consideration form the columns of a matrix, and normalize each column by  X  2 norm, then d = s 2 4 s 2 where s i is the i th singular value of this matrix.
 In our experiments we use the Yale Face Database B and its extended version [16], which contains 38 individuals, each under 64 different illumination conditions. Since in some lighting conditions, the images are severely shadowed, we delete these images and do the experiments on a subset (about 35 images for each individual). We considered cases where we have faces from 4 and 5 random individuals (randomly choose 10 faces for each individual), with and without outliers. The case with outliers consists 10 additional faces each from a different individual. For each of those combinations, we ran 10 trials to obtain the average F-measures (mean and standard deviation), and the result is reported in Table 1 . Note that for each algorithm, we individually tune the parameters to obtain the best results. The results clearly show that partition-based clustering method (clique averaging) is very sensitive to outliers, but performs better when there are no outliers. The clustering game approach and our method both perform well, especially when there are outliers, and our method performs a little better. Figure 1: Results on clustering three lines with noises and outliers. The performance of clique averaging algorithm [9], matching game approach [12] and our method is shown as green dashed, blue dotted and read solid curves, respectively. This figure is best viewed in color. 4.3 Affine-invariant Point Set Matching An important problem in the object recognition is the fact that an object can be seen from different viewpoints, resulting in differently deformed images. Consequently, the invariance to viewpoints is a desirable property for many vision tasks. It is well-known that a near-planar object seen from different viewpoint can be modeled by affine transformations. In this subsection, we will show that matching planar point sets under different viewpoints can be formulated into a hypergraph clustering problem and our algorithm is very suitable for such tasks.
 Suppose the two point sets are P and Q , with n P and n Q points, respectively. For each point in P , it may match to any point in Q , thus there are n P n Q candidate matches. Under the affine transformation A , for three correct matches, m ii  X  , m jj  X  and m kk  X  , S ijk S as a point, then s = exp(  X  ( S ijk configuration then naturally form a cluster. Note that in this problem, most of the candidate matches are incorrect matches, and can be considered to be outliers.
 We did the experiments on 8 shapes from MPEG-7 shape database [17]. For each shape, we uni-formly sample its contour into 20 points. Both the shapes and sampled point sets are demonstrated in Fig. 2 . We regard original contour point sets as P s, then randomly add Gaussian noise N (0 ,  X  ) , and transform them by randomly generated affine matrices A s to form corresponding Q s. Fig. 3 (a) shows such a pair of P and Q in red and blue, respectively. Since most of points (candidate matches) should not belong to any cluster, partition-based clustering method, such as clique aver-aging method, cannot be used. Thus, we only compare our method with matching game approach and measure the performance of these two methods by counting how many matches agree with the ground truths. Since | det ( A ) | is unknown, we estimate its range and sample several possible values in this range, and conduct the experiment for each possible | det ( A ) | . In Fig. 3 (b), we fix noise parameter  X  = 0 . 05 , and test the robustness of both methods under varying scaling parameter  X  d . Obviously, our method is very robust to  X  d , while the matching game approach is very sensitive to it. In Fig. 3(c), we increase  X  from 0 . 04 to 0 . 16 , and for each  X  , we adjust  X  d to reach the best performances for both methods. As expected, our method is more robust to noise by benefiting from and  X  d = 0 . 15 , and test the performance of our method under different  X  . The result again verifies the importance of the parameter  X  .
 Figure 3: Performance curves on affine-invariant point set matching problem. The red solid curves demonstrate the performance of our method, while the blue dotted curve illustrates the performance of matching game approach. In this paper, we characterized clustering as an ensemble of all associated affinity relations and relax the clustering problem into optimizing a constrained homogenous function. We showed that the clustering game approach turns out to be a special case of our method. We also proposed an efficient algorithm to automatically reveal the clusters in a data set, even under severe noises and a large num-ber of outliers. The experimental results demonstrated the superiority of our approach with respect to the state-of-the-art counterparts. Especially, our method is not sensitive to the scaling parameter which affects the weights of the graph, and this is a very desirable property in many applications. A key issue with hypergraph-based clustering is the high computational cost of the construction of a hypergraph, and we are currently studying how to efficiently construct an approximate hypergraph and then perform clustering on the incomplete hypergraph. This research is done for CSIDM Project No. CSIDM-200803 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority (MDA) of Singapore, and this work has also been partially supported by the NSF Grants IIS-0812118, BCS-0924164 and the AFOSR Grant FA9550-09-1-0207. [1] A. Jain, M. Murty, and P. Flynn,  X  X ata clustering: a review, X  ACM Computing Surveys , vol. 31, [2] T. Kanungo, D. Mount, N. Netanyahu, C. Piatko, R. Silverman, and A. Wu,  X  X n efficient [3] A. Ng, M. Jordan, and Y. Weiss,  X  X n spectral clustering: Analysis and an algorithm, X  in Ad-[4] I. Dhillon, Y. Guan, and B. Kulis,  X  X ernel k-means: spectral clustering and normalized cuts, X  [5] J. Shi and J. Malik,  X  X ormalized cuts and image segmentation, X  IEEE Transactions on Pattern [6] B. Frey and D. Dueck,  X  X lustering by passing messages between data points, X  Science , vol. [7] J. Zien, M. Schlag, and P. Chan,  X  X ultilevel spectral hypergraph partitioning with arbitrary [8] J. Rodriguez,  X  X n the Laplacian spectrum and walk-regular hypergraphs, X  Linear and Multi-[9] S. Agarwal, J. Lim, L. Zelnik-Manor, P. Perona, D. Kriegman, and S. Belongie,  X  X eyond [10] D. Zhou, J. Huang, and B. Scholkopf,  X  X earning with hypergraphs: Clustering, classification, [11] A. Shashua, R. Zass, and T. Hazan,  X  X ulti-way clustering using super-symmetric non-negative [12] S. Bulo and M. Pelillo,  X  X  game-theoretic approach to hypergraph clustering, X  in Advances in [13] M. Pavan and M. Pelillo,  X  X ominant sets and pairwise clustering, X  IEEE Transactions on Pat-[14] H. Kuhn and A. Tucker,  X  X onlinear programming, X  ACM SIGMAP Bulletin , pp. 6 X 18, 1982. [15] P. Belhumeur and D. Kriegman,  X  X hat is the set of images of an object under all possible [16] K. Lee, J. Ho, and D. Kriegman,  X  X cquiring linear subspaces for face recognition under vari-[17] L. Latecki, R. Lakamper, and T. Eckhardt,  X  X hape descriptors for non-rigid shapes with a single
