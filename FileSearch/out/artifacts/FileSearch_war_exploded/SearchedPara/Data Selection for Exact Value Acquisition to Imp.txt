 How to handle the uncertainty of data r ecently becomes an important issue because the quantity of existing uncertain data con tinuously increases. In the literature, many research results have been reported on the management or mining of uncertain data [1]. The uncertainty might be caused by many factors, including limitation of under-lying equipment such as sensors in a senso r network, data modification according to privacy preserving policies, approximation of missing data, and just the nature of data like trajectories, which are the spatiotemporal information of moving objects, to name a few. No matter what the cause of uncertainty is, uncertain data could be stored in a probabilistic database for further usage.
 In many applications, clustering is required for better understanding of the database. For most kinds of clustering tasks, traditional algorithms could not be directly applied to uncertain data since their similarity computation is designed only for determinis-tic values. By modifying traditional ones, clustering algorithms that can be applied to uncertain data are proposed. However, the clustering quality largely depends on how uncertain the database is. For example, if distributed regions of data are quite large, the whole system is with high uncertainty and t he clustering result might not be acceptable. In this case, if we can lower the uncertainty of the system, a better clustering result is expected to be obtained. Uncertainty of a system is lowered if we get more detailed information of some data. This could be achieved by using better equipments or by increasing sampling frequency.
 Motivational Example 1: Climate Information. To forecast weather and to moni-tor the climate change, lots of equipments are deployed around the world to carry out measurements like temperature and rainfall. Different kinds of equipments have differ-ent capabilities and result in different degrees of uncertainty. It is known that simple ombrometers can get rough values of rainfall while a meteorological station can get quite precise values. Not only the nature of equipments but also the location where equipments are installed would affect the degree of uncertainty. For example, the value measured by an ombrometer in an arid area might be different from that measured by an ombrometer in a moist area although the rainfalls of the two areas are actually the same. In this case, if we have a limited set of precise equipments, we should use them to replace the original equipments at the places where the uncertainty is relatively large and with large application-dependent influence.
 Motivational Example 2: Summarization of Stream Data. Another example is the sensor network. Sensors read values periodically and upload summarized information following given policies. If a sensor is with more varying readings or is more important than other sensors due to certain reasons (e.g., its location), it should be sampled and be summarized more frequently to represent recent situation.

In this paper, we study the problem of selecting some uncertain data to acquire their exact values to help improve the quality of clustering result as much as possible. The problem is important because no matter how we ll the uncertain clustering algorithms are designed, the quality of clustering result of uncertain data is usually much worse than that of deterministic data.

It is reasonable to increase the amount of information to decrease database uncer-tainty. However, in most cases, we can only select a part of data due to the limitation on resource. The problem is quite challenging becau se it is difficult to leverage all possible factors that might affect the clustering result as a whole.

For the above reasons, we propose a metric to estimate the importance of each un-certain object in an uncertain database. T he metric is designed according to how an uncertain object could influence the global clustering result. Sampling process is used to simplify the representation of an object. The metric would be used for selection such that data with the largest metric values are s elected. However, computational load of the metric mentioned above has to be taken into consideration, and it could be prohibitively large. To make the selection process more practical, we propose another metric which considers only neighboring objects but not all objects. The utility of localized metric is not much worse than the global one but the computation time is significantly shortened.
Our contributions in this paper are many folds.  X  We introduce the concept of exact value acquisition for uncertain data clustering.  X  We propose an algorithm-independent metric to help decide which uncertain data  X  In view of the efficiency and practicability, a localized metric is proposed.  X  We conduct experimental studies to evaluate the proposed ideas. It is shown that The remainder of this paper is organized as follows. Related work is reviewed in Section 2. The problem description is given in Section 3, which contains the proba-bilistic model used and the detailed problem definition. The uncertain data selection is discussed in Section 4, in view of lots of factors that might be related to the clustering result and also in view of global and local computation. In Section 5, experimental re-sults are shown after the proposed selection algorithm is applied with a density-based uncertain clustering algorithm. Finally, the paper is concluded in Section 6. To address the uncertainty of data, many corresponding data mining algorithms are pro-posed. Explicitly, there are algorithms cons idering the uncertainty of data for support vector machine classification, frequent pattern mining [14], outlier detection, and clus-tering [9] [10] [12] [2]. Since the uncertainty of data would affect the results of mining applications, the proposed methods focus on how to handle the uncertainty properly. For example, an attribute would be more important for classification if the uncertainty of it is lower than others.

Since clustering is quite important among mining applications, many clustering al-gorithms for uncertain data are proposed. In [9], the DBSCAN algorithm [8] is trans-formed into an uncertain version which can handle the uncertainty well and can find out density-based clusters. Another clustering algorithm focusing on data with uncertainty is [10], which modifies the hierarchical density-based OPTICS algorithm [4]. The well-known K-means algorithm is also modified to an uncertain version [12]. In addition to the case of static data, the problem of clustering uncertain data is extended to the case of streaming data [2]. It follows the concept of microclustering proposed in [3] and modifies the structure of microclusters for additional error information.

In a distributed environment, data collection process is usually designed in view of the characteristic and distribution of data. For example, collection process could be improved by altering sampling strategy [5] [13], doing partial cleaning [6], or modifying data acquisition plan and scheduling [7] . Cheng et al. proposed a method to choose a set of uncertain objects for cleaning to improve the quality of query answers [6]. The PWS-quality metric is proposed to measure the ambiguity of query answers under the possible world semantics, and the improvement of PWS-quality could be viewed as quality improvement of query answers. Deshpande et al. shown that interactive sensor querying is enriched with statistical modeling techniques [7]. Its data acquisition policy is to balance the increase in the confidence of answering queries based on the statistical model and the cost of observation resulting from wireless communication. Different from the works mentioned above, we study how to select data for exact value acquisition to improve the quality of clustering results. In this section, we introduce the model of uncertainty used in this paper. Then, we describe the problem of point selection for exact value acquisition. 3.1 Probabilistic Model In a probabilistic database, an uncertain object can be represented by all possible in-stances with their probabilities, where the summation of probabilities is equal to one. However, while dealing with uncertainty in real applications, probabilistic models are usually used for simplified description. In this work, without losing generality, it is as-sumed that the distribution of each object is known in advance and it could be described using a continuous or discrete proba bility density function [9] [10].
 Definition 1 (Uncertain Object): An uncertain object o i is a d dimensional tuple where Computation of similarity between two objects is basic and important in most clus-tering algorithms. With similarity information, relationship of a pair of objects could be decided. The Euclidean distance is usually used to present the similarity because it is easy to compute and usually leads to good performance. However, in an uncertain environment, the value of similarity between two objects would no longer be an exact numerical value but a random variable follo wing a probability density function [9]. Definition 2 (Uncertain Distance) : Uncertain distance along an axis between two ob-3.2 Problem Definition In this paragraph, definition of the problem is introduced. It is assumed that probabilistic density functions of all objects are available as background knowledge [8] [4] [12]. By selecting an object and doing some further p rocessing, the distribution can be replaced by its exact value. This can be accomplished b y replacing a sensor by another one with a higher ability. The problem is to decide which objects to be selected to improve the clustering result most. In reality, the number of selected objects could be constrained in view of the budget issue.
 Definition 3 (Object Selection for Exact Value Acquisition) : Given nd -dimensional In the above description, C exact and C approx are the results after applying some un-certain clustering algorithm C to deterministic data and to uncertain data, respectively; C acqui ( o i ) represents the result after applying C to uncertain data while knowing the ex-act value of object o i . The dissimilarity of clustering results between two sets of clusters C i and C j is represented as dis ( C i ,C j ) . It is just a measurement to decide if two sets are similar, and its definition is not restricted. For example, it could be represented by the number of objects that are partitioned into different clusters in the two cluster sets. As mentioned in the preceding section, ad equate objects should be selected for exact value acquisition to improve the quality of clustering result. To decide which object is the most adequate one, characteristics of the distribution of every single object, ten-dency of all objects, and the result of initial clustering should all be taken into consider-ation at the same time. This is not easy because it is difficult to consider uncertainty of all objects and it is also difficult to evaluate the quality of clustering result. Moreover, the result of selection might be quite different when considering differenct clustering algorithms due to their different definitions and concepts. 4.1 Influence of Uncertainty Since the uncertainty is described by probabilistic density functions of objects, the se-lection should be applied on objects. While selecting objects, several factors should be considered simultaneously. For example, if an object could appear in a large region, uncertainty of the distance between it and another object would be large and it should be selected. However, if the region is far from all other objects, it could be ignored as an outlier. All we have to do is to leverage possible factors, which are listed below, to do a better decision.
 Factor: Characterist ics of Distribution. Considering the uncertainty of data, a simple idea is to rank the span of all objects and then select the top ones with the highest values. However, the span is not enough to represent the distribution well. In place of the span, variance of an object could be computed an d then recorded as a label of the object to show the statistical dispersion. Also, the shape of probabilistic density function could be partitioned into pre-defined classes.
 Factor: Relative Position in Feature Space. Not only is the distribution of an object itself considered, the position of the object is also an important factor. If an object is near the border between two clusters, it mi ght be quite important. It is because coverage of the two clusters may change after the exact value of the object is acquired. Also, a centroid object in a k-medians-like algorithm could be important since its value may change or it may become a non-centroid obj ect after the exact value acquisition. Factor: Position of Initial Clustering Results. For a density-based algorithm, if an object is near the boundary betw een two clusters, which is the result of an initial clus-tering, the object is more important than an object surrounded by other objects in the same cluster. Factor: Algorithm-Dependent Errors. The influence of an object in the clustering process is also important information for selection. Therefore, the expected error in-curred in the initial clustering process shoul d be taken into consideration. Since the connectivity of clusters is an important issue when talking about density-based cluster-ing algorithms, the expected error of an object should be computed based not only on the distribution itself but also on distributions of its neighboring objects. 4.2 Metric for Selection If we want to get better quality of selection, more factors should be taken into con-sideration. However, it is quite difficult to leverage the influence of different kinds of factors since they are usually described in different ways. How to find a metric that can show the importance of every object and refl ect the influence of all factors remains an open problem. Other than using a part of factors to estimate the influence of selecting an object, we try to evaluate the importanc e of a object according to possible cluster-ing results. The idea is to observe possible c lustering results of selecting each object. If the result of selecting an object is quite different from that of selecting no point, the influence of selecting it is large and it should be recognized as an important object.
The flow of data selection is described as follows. First, sampling technique is used to replace the original object representation with a set of samples to remove lots of inte-gration computations. Then, any chosen uncertain clustering algorithm is applied on all samples of each object. Note that there is no re striction on which kind of uncertain clus-tering algorithm is used. Clustering results of all samples are compared to the original clustering result and the values of dissimilarity are summed up, showing the expected influence of selecting the object. The expect ed influence of selecting an object is used to represent its importance. It is noted that the dissimilarity is just a measurement between two sets to decide if they are similar, while the way of defining it is not restricted. After all computation is finished, objects could be ranked accord ing to their influence. We can simply do the selection by choosing the object with the top influence. Moreover, if we want to select m objects instead of only one object, we can choose the top-m objects according to the influence ranking.

Ta ke o i as an example. Assume s i 1 is selected and o i is viewed as s i 1 with a hundred percent probability. Then, the uncertain clustering algorithm is applied and generate C i 1 , which is the new clustering results. Rep eat the process for all samples and we can clustering results is generated, the influe nce of selecting an object could be obtained. The influence of selecting object o i is then defined as I i , which is the summation of all dissimilarities.
 4.3 Localized Metric for Selection Errors in clustering results induced by uncertainty are usually due to misunderstanding of relationship between neighboring objects. Taking density-based clustering as an ex-ample, an object would induce severe error only if it is near the boundaries of more than one cluster. The severe error might be the case that two groups of objects are consid-ered as connected while they are not in reality, or the case that two groups of objects are considered as unconnected while they are act ually connected. If an object is near the boundary of a single cluster, it would only affect the objects neighboring to it and the number of those objects is restricted. On the other hand, if an object is near the common boundary of at least two clusters, it might affect objects in all neighboring clusters and the number of affected objects could be the total of all those clusters.

Because of the characteristics mentioned a bove, it is reasonable to find out the influ-ence of an object by observing if the clustering results of neighboring objects change largely. That is, influence is evalu ated according to only partial objects.
The clustering results of selecting the j -th sample of object o i is marked as C L ij , where there are only r items in C L ij representing the clustering results of the neighboring r objects. If the dissimilarity of two clustering results is shown as dis ( C L ix ,C L iy ) ,the local influence I L i of object o i is computed as follows.
 Although the strategy of selection considering local information is similar to that of general case, the localized metric could save a large amount of computation effort. How much computational time to be saved is an issue depending on what kind of the uncertain clustering algorithm is used. In this section, experimental results are presented to show the performance of the pro-posed data selection method for exact value acquisition and the effects of using different input parameters. The FDBSCAN algorithm [9] is chosen to be the uncertain clustering algorithm in our selection algorithm, where all kinds of uncertain clustering algorithm could be used in practical. Although there are lots of uncertain clustering algorithms, the FDBSCAN algorithm is chosen in our work since it can discover arbitrary shape of clusters and it is designed with sampling technique so that it is suitable to be integrated with our algorithm. 5.1 The FDBSCAN Algorithm The FDBSCAN algorithm is based on the DBSCAN algorithm [8], which clusters deter-ministic data into groups according to dens ity information. In FDBSCAN, uncertainty of data is taken into consideration by being described by probability density functions, and the distance between two uncertain objects is computed by integration.

To introduce the parameters used in experiments, the FDBSCAN is roughly de-scribed as follows. For every two objects, the probability that the distance between them is smaller than , which is a given threshold, is defined as the probability that two objects are connected. Based on this, the probability that object o j is density-reachable from o i could be computed by multiplying the probability that o i and o j are connected by the probability that there are more than (  X   X  1) other objects connected to o i ,where  X  is a given value describing density. The latter probability could be computed while assuming the probability of connection for every pair of object s is independent of one another. When FDBSCAN is applied, an object is added to the current cluster only if the probability of being density-reachable from the query object exceeds 0.5. 5.2 Quality Evaluation To evaluate the quality of clustering results, a measurement is designed to compare it with the ground truth, which is obtained when all objects are deterministic. It is to com-pare the dissimilarity of two sets of clustering result which is computed by summing up the number of object pairs that are not put into the same cluster in two sets of results.
As mentioned in Section 3, the definition of dissimilarity is not restricted. In our experiments, the following formula is used to compute the dissimilarity between two sets of clustering result. where Match ( C i ,C j ) means the number of objects that are partitioned into the same cluster in two sets of clustering results.

To evaluate the effect of our selection strategy, both the qualities of the clustering result after data selection for exact value acquisition and of the clustering result after only the FDBSCAN algorithm are applied. For all of our experiments, the quality of our method and the quality of the FDBSCAN algorithm are shown at the same time for comparison.

Normalized error is used to reflect the quality of a set of clustering results. Compu-tation of the normalized error is defined as follows.
 It could be observed that the smaller the normalized error, the better the quality of clustering results.

To decide if an object is in the same cluster in two sets of clustering results, the mapping of clusters is needed. The concept of mapping process is to conduct mini-mum weight perfect matching [11]. A perfect matching M is regarded as a minimum weight perfect matching if and only if for any other perfect matching M , the following inequality holds. Note that the distance between ( x, y ) is defined according to the union and intersec-tion of them, and if the numbers of clusters in two sets are different, null clusters are added in. 5.3 Experimental Setup All experiments are based on a synthetic data set which is generated as follows. There are 1000 objects in the uncertain database. After a set of cluster heads is randomly generated, randomly assign each object to one of the cluster, which means its location is near to the cluster head. The location of a n object X  X  location is determined based on uniform distribution. Then, one of the three distributions (uniform, normal, and inverse normal distribution) is assigned to each object . Samples of an object are generated based on the above distribution.

The proposed selection algorithm is imp lemented according to the FDBSCAN algo-rithm for uncertain clustering. All algorithms are implemented in C++. The experiments were run on a Windows Server with a 3.00 GHz processor and 8.00 GB main memory. The setting of parameters used in the experiments is listed in Table 1. To show the average case of each experiment, each result shown below is the average of 40 runs. 5.4 Experiment Results Results of experiments are shown below. The results are discussed in view of effective-ness and efficiency.
 Effectiveness. Fig. 1 and Fig. 2 are experimental results reflecting the effectiveness. Since the selection is to move a step further from the original uncertain clustering al-gorithm, the results are compared to the original uncertain clustering algorithm to show the improvement.

In the left part of Fig. 1, the impact of th e number of locally selective objects is shown. It is clear to see that the normalized error is quite small when the number is 100. It means that most errors are corrected while selecting 100 objects to calculate the localized metric. However, to balan ce accuracy and computation overhead, 70 is chosen for the following experiments. The impact of the number of samples for an object is shown in the right part of Fig. 1. The description of an object gets closer to the underlying probability density func tion when the number of samples becomes larger. However, the number of samples seems not to affect the quality of the selection method. In Fig. 2, the impact of thresholds of both connection and density is shown.
It is observed that the impact of the number of locally selective objects is prominent while that of the other parameters is not. The reason is that the number of locally selec-tive objects is directly related to the selection while other parameters are related to the FDBSCAN algorithm.
 Efficiency. Time consumption of our methods and the FDBSCAN algorithm according to the number of locally selective objects is shown in Table 2. S-Local refers to the pro-posed selection method using the localized metric and S-Global refers to the proposed selection method using the original metric. Computation of the S-Global method takes quite long time and it is hence considered impractical. For S-Local , time consumption grows when r grows, where r means the localized metric is computed according to in-formation of only r neighboring objects. If the number of r is set to be a small number, computational load would be acceptable.

In Fig. 3, time consumption related to parameters other than the number of locally selective objects is shown. It is observed that the definition of density would not change the computing overhead while the threshold of connection and the number of samples would. The relationship between time consumption and the threshold of connection is almost linear. The reason might be that if the threshold is high, it is much harder to decide if an object is dense so that most computation tasks could not be pruned. Moreover, the relationship between time consumption and the number of samples is between linear and square linear. This is reas onable since the computation overhead should be square of the number of samples while there might be some tasks pruned according to too many number of samples. In this paper, the concept of exact value acquisition for uncertain clustering is intro-duced. According to the assumption that t here are limited resources to acquire more precise values, uncertainty of the system could be lowered and quality of mining results could be improved. In addition to the con cept, we propose a method to decide which data to be selected for exact value acquisition. It is based on a proposed algorithm-independent metric representing expected influence of selecting an object. Considering the efficiency and practicability, our method can also be simplified to a localized ver-sion by only observing influence on neighboring objects. The selection result could be employed when we do distributed resource planning. Experimental studies are con-ducted and it is shown that clustering results indeed improve after selective exact value acquisition is applied.
