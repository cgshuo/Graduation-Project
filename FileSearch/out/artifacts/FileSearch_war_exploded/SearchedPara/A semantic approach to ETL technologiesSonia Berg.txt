 1. Introduction data marts [1] .
 language in the form of comments and documentations).
 intervention involve data source schemata that are generally easily exploited for building the data warehouse.

Nowadays, the actual business needs require EIS to have a great required. automatically perform data transformations are needed. intelligence processes. provide useful information about the context where the enterprise work to be exploited in the data analysis. semantically related thus de fi ning a transformation function for populating with process is parametric: the result can range from  X  loose  X  in understanding the approach. Finally, in Section 5 , some conclusion and future work are sketched out. 2. Related work they are based on a well-accepted, standard modeling language. Other approaches are based on bene fi t of representing ETL processes without any restriction imposed by a previously de processing the data by multiple subsequent operators) and other operators de that introduces and populates a new surrogate key column is a re classi fi ed on the basis of their characteristics.
 well as the data warehouse schema.
 sources. All the schema terms are annotated, i.e. manual relationships between ontology elements are de relationships and hierarchies among them are derived.
 major database vendors provide ETL solutions bundling them with their DBMS [20
In the following, the MOMIS and RELEVANT systems that are exploited and extended in this paper are introduced. 2.1. The MOMIS system at a glance
The M ediator Envir O nment for M ultiple I nformation S ources (MOMIS)
Virtual View (GVV). The GVV generation process (see Fig. 1 ) can be outlined as follows:
ODL I 3 (an extension of the ODL standard language  X  http://www.odmg.org );
WordNet meaning(s) for each conceptual schema element provided by the wrapper; describes intra and inter-schema knowledge in the form of synonyms (SYN); broader terms/narrower terms (BT/NT); meronymy/holonymy (RT). attributes of the local sources; annotated according to WordNet. 2.2. The RELEVANT data analysis system
RELEVANT [4] is based on the idea that analyzing an attribute domain, we may rv At = b rvn At , values At N . rvn At is the name of the relevant value set, while values
RELEVANT is based on the following functional tasks (see Fig. 2 ): 1. Data pre-processing : like most cluster tasks with non-numeric attributes, the problem is to measures: 1) syntactic , mapping all the words of the attribute values in an abstract space, and de semantically related values expressed with a different terminology.
 provides satisfactory results.
 attribute values a 1 and a 2 , we say that a 1 dominates a specialized) with multiple words in many ways.
 (synsets), each one expressing a distinct concept. Synsets are described with a de one or more synsets are potentially similar. We can thus compute similarity on the basis of the shared synsets. information retrieval (Simple Matching, Russel and Rao measure, Tanamoto coef different similarity measures by setting speci fi c parameters. the output of the hierarchical clustering algorithm or the set of root elements. 4. Name selection : The simplest way to detect a list of rvn function. The integration designer may select the most appropriate name among them. simple mode uses some default parameters and provides four sliders: the the advanced mode, the designer may set among about hundred different con 6. Feedback : The system provides a feedback on the results of a run that may be exploited to re that is the number of elements which are in more than one relevant value. 3. Supporting the extraction process with MOMIS and RELEVANT
The goal of the extraction tool is to fi nd, for each data warehouse table, a set of one-to-many mappings m warehouse schema element corresponding to the data source schema elements. schema and the data source schemata.
 with this issue by proposing other two similarity measures to be combined to the syntactic one: the algorithm.

Nevertheless, since the coef fi cient has a direct and signi choices and if necessary adjust the results. 3.1. Functional architecture of the extraction tool clusters (see Section 3.5 ). 3.2. Running example
DataFlow company. DataFlow is a software company, leading the new application, called  X  Bollicine Community Business Intelligence (BCI) general key performance indicators obtained by aggregating data of all the members. 3.3. Semantic enrichment sources, the annotation process and the creation of the thesaurus.
 fl
Wrappers for several kinds of data sources have been developed (spreadsheets, XML documents, text documents, ... ) and, for each data source, the designer has only to con address and the account values.
 most suitable concept of a reference ontology/database (we exploit the WordNet expanding acronyms and abbreviations in schema labels.
 terms (SYN), broader terms/narrower terms (BT/NT) and related terms from MOMIS and is based on structural analysis (where relationships are de foreign key, relationships between attributes in the same tables, techniques.
 SALES(CODE, ARTICLE_CODE, BRANCH_CODE, DATE_MONTH, AMOUNT, QUANTITY, HECTOLITRES) ARTICLE(ARTICLE_CODE,DESCRIPTION,CATEGORY_DESCRIPTION, SUBCATEGORY_DESCRIPTION) BRANCH(BRANCH_CODE, DESCRIPTION) TIME(DAY_CODE,WEEK_CODE,MONTH_CODE,BIMESTRAL_CODE,TRIMESTRAL_CODE, CATEGORY_DESCRIPTION(DW) is mapped with the corresponding attributes in S1 and S2. Concerning the thesaurus, since the attributes FAMILY_DESCRIPTION(S1), CATEGORY_DESCRIPTION(S2), CATEGORY_DE-
SCRIPTION(DW) are annotated with the same  X   X  description ARTICLE.CATEGORY_DESCRIPTION(DW) SYN MERCHANDISE.FAMILY_DESCRIPTION(S1) ARTICLE.CATEGORY_DESCRIPTION(DW) SYN ARTICLE.CATEGORY_DESCRIPTION(S2) ARTICLE.CATEGORY_DESCRIPTION(DW) BT PRODUCT.CLASS_LABEL(S3) ARTICLE.CATEGORY_DESCRIPTION(S2) SYN MERCHANDISE.FAMILY_DESCRIPTION(S1) ARTICLE.CATEGORY_DESCRIPTION(S2) BT PRODUCT.CLASS_LABEL(S3)
MERCHANDISE.FAMILY_DESCRIPTION(S1) BT PRODUCT.CLASS_LABEL(S3) 3.4. Generation of clusters adding new similarity measures. In particular, clusters are created by exploiting three similarity measures: 1. syntactic similarity , which compares the alphabets used for describing the attribute values; 2. memberships , which measure the closeness of attributes belonging to the same table;
Each similarity measure is internally represented with an af application of a clustering algorithm (with a user-select threshold) generates clusters of similar elements. cluster with all the attributes is computed.

This means that mappings between the FAMILY_DESCRIPTION(DW) and the corresponding attributes of the new sources will be generated. The process is performed for each target table, the de
Furthermore, for the datetime attributes the formats are formally speci
Euro or Dollar is indicated. 3.5. Cluster validations and mapping generations the data warehouse.
 tables), nr s the number of clusters generated considering only the schema of the new source s and nr warehouse after the insertion of a new source will have two effects:  X  it produces a number of clusters near to that of the data warehouse before the insertion,  X  the clusters generated in the new source alone and in the data warehouse will be similar.  X  the before/after insertion cluster ratio ddsr = nr d / nr  X  the source/populated warehouse cluster ratio sdsr = nr s / nr  X  the source/populated warehouse Jaccard index; let us de fi the relatedness of a data source to the data warehouse by means of the Jaccard coef cases:  X  without any concern; this case is characterized by ddsr near 1, sdsr is less than 1 and JC  X  different domain in this case the new source represents a domain signi characterized by ddsr and JC s signi fi cantly less than 1 ( sdsr is not meaningful);  X  characterized by ddsr and sdsr less than 1, and JC s near 1.
 grouped in the clusters. The following cases are possible: mapped into all the data source elements in the cluster. the choice of a too selective clustering threshold. 3. A cluster contains only attributes of the data warehouse schema: no mappings are generated from this con of cluster may show that the data warehouse schema could be re the cluster) semantically related that may, in case, be fused into a unique element. warehouse, and the number of clusters after the insertion will not change. 4. The transformation function model population of the data warehouse with attributes enabling data analysis processes. con model composed of the following functions:  X 
RETRIEVE : retrieves all the data values from a speci fi c data source given as parameter.  X   X  Column Split : splits the content of a single column into multiple target columns (or a speci  X  Column Merge : merges two or more local columns into a single target column;  X 
UNION : performs the union of data records from two or more data sources.  X 
JOIN : permits the join among different local data sources obtaining object identi real world object are identi fi ed [33] .  X 
CONVERT(c 1 ,c 2 ) : converts all the values of a speci fi prede fi ned in the system:  X  Currency : converts the currency values retrieving from the web the current rate.  X 
FILTER ( c ): fi lters data records coming from a local data source on the basis of a speci  X 
AGGREGATE ( c ): proposes an aggregation function (SUM, AVG, COUNT, stemming the values and for modifying the values according to some regular expression rules). providing a semantic reconciliation of the attribute values. The transformation function is de following syntax: RELEVANT_VALUE( b TARGET_ATTRIBUTE N , b DATA_SOURCE_LIST where b TARGET_ATTRIBUTE N is the attribute of the target schema whose values are grouped and analysis, if needed. The transformation function works accordingly to the following steps:  X   X  will consist of homogeneous values thus allowing OLAP analysis (i.e. drill down) on them.  X  cardinality of values in the data warehouse and consequently a more synthetic representation of the object. 4.1. Automatic selection of transformation functions the data mappings.

The process is described by the Transformation Function Identi mappings between the data source and the data warehouse attributes (we call M the union of the mappings m semantic and domain compatible level.

Algorithm 1. Transformation function identi fi cation (TFI)
Input: all mappings m ij ( t a
Output: the set of transformation functions proposed for m foreach table  X  target source  X  mapping level : for each target class i in the data warehouse, the attribute values to be merged and addition, the t a attributes or attributes annotated with terms like  X  code  X  attribute is a hyponym of the target level, an AGGREGATE function is proposed.  X  compatible values of the local and the data warehouse attributes.
 (name and parameters) and can be directly executed by our tool. 4.2. Running example mappings (QUANTITY and HECTOLITRES), the FILTER function is proposed. At the semantic level, since  X  hectoliter  X  , a CONVERT function is proposed.

ARTICLE(CODE,DESCR,CATEGORY_DESCRIPTION,SUBCATEGORY_DESCRIPTION) where CATEGORY_DESCRIPTION is a dimension for BI analysis.
 The system proposes the application of the following function: RELEVANT_VALUE(ARTICLE.CATEGORY_DESCRIPTION, S1.FAMILY_DESCRIPTION, S3.CLASS_LABEL, data warehouse attribute instead of the 22 original values: UPDATE CLUSTER_ARTICLE_CATEGORY_DESCRIPTION
SET TARGET = COALESCE ((SELECT MIN(CL1.CATEGORY_DESCRIPTION) FROM CLUSTER_ARTICLE_CATEGORY_DESCRIPTION AS CL1 WHERE CHARINDEX(RTRIM(LTRIM(CL1.CATEGORY_DESCRIPTION)), CLUSTER_ARTICLE_CATEGORY_DESCRIPTION.CLUSTER) N 0AND
CL1.SOURCE = S1), (SELECT MIN(CL3.CATEGORY_DESCRIPTION) FROM CLUSTER_ARTICLE_CATEGORY_DESCRIPTION AS CL3 WHERE CHARINDEX(RTRIM(LTRIM(CL3.CATEGORY_DESCRIPTION)), CLUSTER_ARTICLE_CATEGORY_DESCRIPTION.CLUSTER) N 0AND
CL1.SOURCE = S3), (SELECT MIN(CL2.CATEGORY_DESCRIPTION) FROM CLUSTER_ARTICLE_CATEGORY_DESCRIPTION AS CL2 WHERE CHARINDEX(RTRIM(LTRIM(CL2.CATEGORY_DESCRIPTION)), CLUSTER_ARTICLE_CATEGORY_DESCRIPTION.CLUSTER) N 0AND CL2.SOURCE = S2)) exploited in detailed analysis concerning the values of the data warehouse attributes. 5. Conclusion and future work particular, we focused our work on the extraction phase, by implementing a technique that semi-automatically de Acknowledgements  X  Romagna Region  X  PRRIITT, measure 3.4).

References
