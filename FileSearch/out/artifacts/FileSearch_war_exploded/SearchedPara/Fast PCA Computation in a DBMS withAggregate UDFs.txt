 Ecient and scalable execution of numerical methods in-side a DBMS is dicult as its architecture is not suited for intense numerical computations. We study computing Principal Component Analysis (PCA) on large data sets via Singular Value Decomposition (SVD). Given the diculty to program and optimize numerical methods on an existing DBMS, we explore an alternative reusability approach: call-ing the well-known numerical library LAPACK. Thus we study several alternatives to summarize the data set with aggregate User-De ned Functions (UDFs) and how to e-ciently call SVD numerical methods available in LAPACK via Stored Procedures (SPs). We propose algorithmic and system optimizations to enhance scalability and to push pro-cessing into RAM. We show it is feasible to eciently solve PCA by rst summarizing the data set with arrays incre-mentally updated with aggregate UDFs and then pushing heavy matrix processing in SVD to RAM calling LAPACK via SPs. We benchmark our solution on a modern DBMS. Our solution requires only one pass on the data set and it exhibits linear scalability.
 H.2.4 [ Database Management ]: Systems| Relational databases Big data, LAPACK, linear algebra, numerical methods, SQL
Most data mining, statistics and mathematical models are based on linear algebra computations [3]. Programming these numerical methods inside a DBMS is dicult and their performance is generally bad. At a fundamental level, rela-tional algebra and matrices [3] are quite di erent mathemat-ical abstractions, requiring di erent programming languages for their computation. There are good reasons to study the integration of SQL and LAPACK [3]. Array support remains limited in SQL, but it is now available within User-De ned Functions (UDFs) and Stored Procedures (SPs) source code, with the added bene t that it is unnecessary to modify inter-nal DBMS source code. Moreover, such arrays are directly allocated and manipulated in RAM, using a traditional pro-gramming language. We believe DBMSs have the potential to solve analytic tasks beyond data mining. In general, it is bene cial to call numerical libraries (e.g. LAPACK, BLAS) to avoid redeveloping and re-optimizing existing methods. From a hardware perspective, it is critical to leverage com-puting power in multi-core architectures, especially now that CPU speeds have reached a clock speed threshold. Finally, it is dicult, time-consuming and error-prone to change a DBMS internal architecture and subsystems to support nu-merical computations. LAPACK routines use BLAS to per-form dense matrix operations such as LU decomposition (to solve linear equations), QR decomposition (to solve least square problems), and also singular value and eigen prob-lems. These libraries exploit cache memory, RAM and mul-ticore CPUs. Even though MapReduce is now becoming a hype for so-called big data analytics, the need to analyze data sets inside a DBMS remains a pressing challenge. Based on such motivation, we study how to integrate LAPACK with the DBMS, leveraging its SQL programming and ex-tensibility mechanisms, especially UDFs and SPs. Thus our contributions are memory-ecient scalable algorithms and database-oriented optimizations to solve PCA on large data sets. Our solution is based on data set summarization with aggregate UDFs and solving SVD on its correlation matrix calling the LAPACK library via SPs. Our optimized algo-rithms can solve PCA problems inside a DBMS an order of magnitude larger and two orders of magnitude faster than existing state-of-the-art algorithms. Let X = f x 1 ; :::; x n g be the input data set with n points, where each point has d dimensions. That is, X is a d n matrix, where the point x i is represented by a column vec-tor (equivalent to a d 1 matrix) and i and j are used as matrix subscripts. Solving PCA on X is equivalent to solving SVD on its variance-covariance matrix (commonly called covariance matrix) or its correlation matrix. The cor-relation matrix is a d d matrix, where the correlation coecients of indicate the strength and direction of the lin-ear relationship between two variables in the range [ 1 ; 1]: trix of X , a positive semi-de nite matrix. There is a direct relationship between PCA and SVD when principal compo-nents are calculated from the correlation matrix . We use the correlation matrix by default since = XX T when X is normalized with a z-score (zero mean, standard devia-tion equals 1). We would like to emphasize our algorithms generalize to the covariance matrix as well.
 The standard singular value decomposition [3] to solve PCA is: XX T = U E 2 V T ; where U and V T are eigenvectors of d d matrix and E 2 is a d d -matrix. Using the correlation matrix for XX T , the problem is to nd a set of eigenvectors Since SQL is rigid and slow to process non-relational data, such as matrices, most DBMS provide capabilities to embed code into SQL. These features include user-de ned aggregate functions (UDFs, also called user-de ned aggregates) [4, 5], stored procedures (SPs) and table-valued functions (TVFs) [1] programmed in C-like languages (i.e., not SQL). UDFs are called in the SELECT statement. In our case, UDFs enable arrays and loops. Aggregate UDFs and SPs are pro-cessed at runtime in a di erent manner. Aggregate UDFs are processed in three phases and enable multi-threaded pro-cessing, leaving thread management to the DBMS, while en-joying the capability to interleave I/O with CPU processing. On the other hand, SPs enable sequential processing on the input table with a cursor interface (reader/writer) producing several output tables, avoiding exporting data. In our case, SPs help exible matrix manipulation. Finally, User-de ned types (UDTs) simulate vectors.
 The Linear Algebra PACKage (LAPACK) [3] is a well-known open-source numerical library written in FORTRAN that uses lower level routines from the BLAS (Basic Linear Al-gebra System) library, which in turn performs basic matrix operations. An alternative library called DotNumerics, is the LAPACK source code rewritten in C# for the Microsoft .NET environment. Therefore, we will study how to call LAPACK and the DotNumerics C# source code libraries via Stored Procedures (SPs). We emphasize that LAPACK and BLAS are the standard libraries used underneath math-ematical tools like MATLAB or the R package.
Based on the state of the art [5], we know the best way to solve SVD on statistical matrices from large data set is to compute sucient statistics in one pass on X in time O ( d 2 n ) and then perform the rest of computations in time O ( d 3 ) over the d d correlation matrix. We take a fur-ther step by splitting the manipulation of the correlation matrix into two steps in order to reorganize matrix entries in RAM. Therefore, we will defend the idea that the best way to compute SVD, calling LAPACK in the DBMS, is to perform these three steps: 1. Compute sucient statistics: n; L; Q ; 2. Compute correlation matrix from n; L; Q and pass 3. Call SVD method from LAPACK library.
The correlation matrix can be calculated in one pass over X using summary matrices: n , L and Q (also called sucient statistics [5]). Let where n is data set size, L is d 1 vector with the linear sum of points and Q is the Quadratic sum of points in a d d matrix. In other words, Q is the sum of the cross-products of each point dimensions (as a vector outer product with itself). These summary matrices allow us to compute several statistics eciently, due to their small size compared to X when d n . The correlation ab between dimensions a and
Summarization matrices L and Q can be computed with three main methods in a DBMS: (1) with SQL queries only (no UDFs, or SPs); (2) with Stored Procedures (SPs; similar to reading a at le); (3) with aggregate UDFs (similar to MapReduce). X is assumed to be stored in a table with a horizontal layout by default ( d dimension columns per row), to enable fast block-based I/O scans. Alternatively, X can be stored with a vertical layout, with one dimension per row for high d and a sparse matrix X . Refer to Section 2 for X de nition and an overview on UDFs and SPs.
 Method (1) is the most portable and easiest to program. Even though Method (1) is theoretically interesting and portable, we will show it is quite slow. Also, method (1) faces DBMS limitations on a maximum number of columns when X has a horizontal layout.
 Method (2), based on a SP, uses a reader() function to scan X table rows, one at a time and calculates n , L , Q . The SP runs sequentially on a single thread and it can create lists, arrays, or any enumerable object, casting results as relational tables. The SP approach is less portable compared to an aggregate UDF, yet simpler in terms of programming.
Method (3) based on aggregate UDF, requires passing x i as a user-de ned type (UDT) where each dimension becomes one attribute in the UDT (i.e., simulating a vector). We emphasize this approach is cleaner and faster than previous work passing x i as a string [5] (which requires parsing di-mension values at runtime). The UDF allocates and updates L; Q in arrays in RAM with multi-threaded processing. The correlation matrix is returned with a second UDT to enable an ecient call to LAPACK, explained in detail below. We are not computing the outer product for Q with LA-PACK routines. There is a good reason: we aim to exploit the parallel multithreaded I/O capability of the DBMS to compute the aggregate UDF and do a table scan on X .
The correlation matrix is calculated with arrays in both the SP and the aggregate UDF. The fundamental di erence between an aggregate UDF and the SP is the aggregate UDF works in parallel with multiple threads and thus each thread has its own local partition of n; L; Q . In the aggregate UDF, once the values of n , L , Q are merged in the merge phase, the correlation matrix is sequentially computed in the ter-minate phase with the computed n , L , Q . Finally, the correlation matrix is returned as a UDT in the SVD call.
In order to call LAPACK the rows of the correlation ma-trix must be stored in contiguous memory (e. g. a single block of RAM address space), obeying FORTRAN internal array storage (column major order).

To guarantee block-based RAM storage, a one-dimensional array of size d d is created and for any given entry a ij A matrix of size d d , the address is i d + j . Since is a sym-metric matrix, meaning ij = ji , the values stored in mem-ory will be the same in column major order and row major order. For instance, the \column major order" block for a 3 3 A matrix is: [ A 11 ; A 21 ; A 31 ; A 12 ; A 22 ; A 32 ; A
In FORTRAN's column major order the second subscript changes more slowly than the rst one. Since both arrays are in RAM and conversion from a bidimensional array into a unidimensional array (i.e., a \ attened" matrix) takes place in RAM, this reorganization takes negligible time.
We basically exploit a SP to receive and produce the out-put matrices with eigenvalues and eigenvectors. We present two alternatives to call the LAPACK library: (1) FOR-TRAN LAPACK: single threaded precompiled FORTRAN code; (2) C# LAPACK: single threaded LAPACK source code, compatible with Microsoft .NET environment.
The best system programming approach is to allow man-aged code to run by itself (LAPACK Source code) and un-managed code (Precompiled FORTRAN LAPACK library) via a wrapper class as a way to interface the dynamic linked library (DLL) calls. We explain unmanaged and managed code management at runtime in the next subsection.
Alternative 1: LAPACK provides a version of the BLAS which is not optimized for any CPU architecture. This refer-ence BLAS implementation may be much slower than opti-mized implementations, especially for matrix factorizations like SVD and other computationally intensive matrix oper-ations. The important steps required to call the LAPACK SVD routine within DBMS are creating the wrapper class that calls the LAPACK routine, adding the compiled li-braries to the module, linking and invoking the SVD call in the linked module being imported.
 Alternative 2: This version uses C# source code (Dot-Numerics) which is a faithful translation of the LAPACK library originally written and tuned in Fortran. It simply rewrites all the driver routines and numerical calls in the LAPACK library for the .NET development platform. This version is the easiest to integrate between the two versions as there is no need to use a multiple language runtime in-terface (.NET CLR in our case) to link the external library. Thus this version is the simplest.
A LAPACK call can be managed by the running environ-ment of the DBMS (managed/protected code) or directly by the operating system (unmanaged/unprotected code) [1]. Therefore, the LAPACK C# code implementation is \man-aged" and the LAPACK (Fortran object code) libraries are \unmanaged". The decision on which runtime environment manages this call is related to the approach taken for the in-tegration. The pros of managing a routine inside the DBMS is that the thread memory is protected by the system and metric n; L; Q corr matrix LAPACK Time O ( nd 2 ) O ( d 2 ) O ( d 3 ) I/O cost n 0 d + d 2 Table 2: Hardware and Software Speci cations.
 the data structures passed along the modules do not need any marshaling. On the other hand, if a routine is called as a dynamic library (e.g. DLL in Windows), the operating sys-tem (OS) is in charge of managing the call. An immediate disadvantage of such scenario, aside from the need to mar-shal the data structures across modules, is that the DBMS is unable to control the stability of the system, especially with memory leaks (common with new code). Despite this risk, an unmanaged function call is able to take full advantage of a variety of optimizations speci c to a particular hardware architecture. Regardless of the running environment, man-aged code and unmanaged code support multithreaded pro-cessing. However, the threads of managed code (which have to be explicitly de ned) have to reside within the DBMS space. Instead, thread support for unmanaged code is pro-vided entirely by the operating system.

The number of oating point operations (FLOPs), big O () (based on d and n ) and I/O cost for the three steps are shown in Table 1. The rst column has the number of FLOPS re-sulting from the n; L; Q computation. The second column shows the time to needed to convert a bidimensional array to a single block with a unidimensional array. The last col-umn has the number of FLOPS for SVD solved on and it includes an intermediate step from the dgesvd() function that reduces the general matrix into a bidiagonal matrix.
We present benchmarking experiments on the three main steps: summarization matrices, getting the correlation ma-trix and solving SVD. The time to write SVD result matrices at the end is not considered. We compare processing times on three hardware con gurations: true Dual core, true Quad core and \virtual" Dual core, as explained below. We record times from seven runs, eliminating the maximum and mini-mum, getting the average of the remaining ve. The DBMS server memory (bu ers) is cleared before each run. Accuracy (numerical precision) is not measured because LAPACK is a reference library. Times are given in seconds. We conducted experiments on two rack servers having Dual and Quad core CPUs, respectively, shown in Table 2. We conducted experiments deactivating two cores out of four cores in the true Quad core machine by making use of SQL Server 2008 DBA management interface (I/O and CPU
Table 3: Comparison to compute PCA ( d = 100 ). anity) and make it run as a simulated Dual core server with every other setting of hardware and software con gurations being the same. This virtual Dual core was our default base-line con guration. Our Dual-core and Quad-core machines di er with respect to CPU clock speeds, cache memory and bus speeds. Hence, to overcome these con guration di er-ences, we normalized measured execution times in the true Dual core machine and the true Quad core machine consid-ering CPU clock speed.
 We used the ISOLET data set from the UCI Machine Learning repository. The original data set had d =617 at-tributes and n = 7797 data points. Data sets varying d and varying n (keeping the other size xed) were created out of the ISOLET data set using replication to reach the desired d and n sizes. The data set X is stored on a DBMS table with d columns with double precision in order to get highest accuracy and work with the same precision as LAPACK.
We ran these experiments on the Quad core server. We set d = 100, which represents high dimensionality, produc-ing large matrices. We emphasize again that RAM (DBMS bu ers) is cleared before each run to make sure X is read from disk. We compare the R statistical package, SQL queries, SPs and UDFs.

Table 3 compares several alternatives to compute PCA varying n keeping d xed. We can observe the time to ex-port the data set with the fastest available mechanism out-side the DBMS (bulk export) is a bottleneck. In fact, that time is greater than the time to process the data set with SPs or UDFs. We can see the R package is quite slow. No-tice that R works in RAM and since X does not t in RAM it must be processed in blocks of 100k rows to derive the correlation matrix. The second slowest alternative are SQL queries. In this case the algorithm becomes faster than the R package with the largest n , but overall they exhibit similar speed. However, considering export times SQL queries beat the R package. Then we can see SPs become an order of magnitude faster than SQL queries. Finally, our proposed solution combining UDFs and LAPACK is an order of mag-Figure 1: Computing matrices L; Q with n =1M (Dual core vs. Quad core). nitude faster than SPs, which we consider an achievement. The gap between UDF/LAPACK and SP becomes signi -cant when n =10M. Moreover, this alternative shows clean linear scalability. In short, our solution is much faster than the rest and it takes a small fraction of the time to export the data set (20%).

Table 4 provides a breakdown of PCA into two main steps: getting sucient statistics n; L; Q and solving SVD on the correlation matrix. Trends are consistent with the overall time to compute PCA. Most of the time is spent summariz-ing X , which is both I/O intensive (a full table scan) and CPU intensive O ( d 2 ) ops. The UDF is much faster than the SP to compute n; L; Q , which is explained by parallel I/O and aggregation. Solving SVD is more CPU intensive. Given such high d the time to compute SVD with queries is signi cant: queries are inecient for matrix computations. On the other hand, SPs are fast to solve SVD, but not the fastest. Since LAPACK works at peak performance, the time to solve SVD in LAPACK is negligible compared to the time to compute n; L; Q .
 Since we proved aggregate UDFs are much faster than the R package and SQL queries it in unnecessary to analyze their scalability on d . Table 5 shows time as d varies while keeping n constant for the fastest mechanisms to solve PCA: UDFs to summarize X and SPs to call LAPACK. Due to DBMS limitations, d = 1600 could not be handled by the UDF, but d = 800 produces very large matrices. Time grows O ( n 2 ) for n; L; Q matrices and SVD. Times for LAPACK now become noticeable for high d (barely above one second), but they are still orders of magnitude smaller than the time to summarize X with n; L; Q . The aggregate UDF is also much faster than the SP, but the di erence compared to time in n is less signi cant, highlighting O ( n 2 ) ops. Clearly, UDFs and LAPACK are a winning combination. The summarization matrices and correlation matrix com-puted using the reader function and arrays in a SP took more time on both Dual and Quad core machines compared to the other approach using the aggregate UDF as we see in Figure 1. The functions in CLR SP works sequentially in a single thread and thus no parallelization is exploited here. A table scan takes place sequentially on a large intermediate le; thus there are no performance bene ts with more cores.
In case of aggregate UDFs, the Quad core runs much faster Table 6: Computing SVD on correlation matrix (Dual vs. Quad core; times in secs). Table 7: Pro ling steps with n = 1M (times in secs). than the Dual core by exploiting parallel processing of mul-tiple cores as shown in Figure 1. The table data is read in parallel scans with multiple threads on the accumulate phase, computing the corresponding n; L; Q on each thread. We can observe that Quad core performed almost twice as fast as the Dual core server to process the aggregate UDF (linear speedup). Therefore, an aggregate UDF to compute summarization matrices is the best mechanism when using multiple cores.
 Since the correlation matrix computation is the same in-side the aggregate UDF and the SP, the execution time is the same on both and it is negligible compared to the time taken for computing n; L; Q and SVD. The time to convert the correlation matrix in two dimensions into a block is neg-ligible.
 The LAPACK (precompiled Fortran) version runs faster than the LAPACK C# code version, as shown in Table 6. We must mention d = 1024 cannot t in RAM allocated by the UDF. The di erence is one order of magnitude at highest d (ten times slower). The reason is, the standard LAPACK library is single threaded and hence no matter how many cores are available for the DBMS, they are not exploited.
Table 7 shows relative importance of each step. Evi-dently I/O is the bottleneck because the entire table must be scanned. Since this matrix summarization step is I/O bound and already using parallel multithreaded processing, it seems dicult to accelerate it further. In short, these ex-periments prove our solution e ectively exploits a multicore CPU, especially for data set summarization.
To the best of our knowledge, there is not an ecient one-pass algorithm to solve PCA that can work in parallel and that can interact with the LAPACK library. More speci -cally, we are not aware of previous attempts to call LAPACK via SPs manipulating matrices in RAM, which is more di-cult and useful than calling LAPACK in the internal DBMS source code. On the other hand, there has been interest on incorporating arrays into SQL [1]. Most of prior research has been in the direction of calling LAPACK outside the DBMS running on multi-core CPU architectures [2].
We showed the best way to compute PCA is to do it in three phases: data set summarization with an aggre-gate UDF, building and reorganizing correlation matrix as a block, and calling the SVD numerical method in the LA-PACK library, passing the matrix in block form in RAM. We showed it is feasible and ecient to call the numeri-cal library LAPACK within a UDF, avoiding matrices being exported to external les. The key mechanism that enables calling LAPACK functions is to reorganize the input matrix into column major order, departing from row-based stor-age, into one block. Processing the matrix and calling SVD are quite ecient because they work in RAM. Pre-compiled FORTRAN code is the most ecient code for matrix compu-tations. For a large correlation matrix (one thousand dimen-sions) SVD is solved in seconds; SQL queries are orders of magnitude slower. The DBMS internal parallel threaded ar-chitecture, memory manager and the LAPACK library are treated as black boxes, providing an abstract component-based architecture. Even though internally integrating LA-PACK with a DBMS could potentially achieve greater per-formance, we believe it is not a plausible idea due to the programming e ort and marginal performance gain. Our optimized UDF-based algorithms can help solve PCA an or-der of magnitude faster than SQL queries and the R package. In summary, we presented a solution to call the LAPACK linear algebra library in a modern DBMS, based on UDFs, achieving linear scalability on data set size, crunching large matrices and working at peak CPU speed.

There are several important research issues. We need to study parallel processing in depth. We want to study if the same UDF/LAPACK combined approach is applicable to other linear models. We will develop DBMS mechanisms to handle large matrices in LAPACK, especially when they can-not t in RAM. We will study tradeo s between UDTs and recent array extensions in SQL. Finally, we intend to create mechanisms to call LAPACK during a table scan through scalar UDFs that can further accelerate aggregate UDFs. This work was supported by NSF grant IIS 0914861. [1] J.A. Blakeley, V. Rao, I. Kunen, A. Prout, M. Henaire, [2] A. Buttari, J. Langou, J. Kurzak, and J. Dongarra. [3] J.W. Demmel. Applied Numerical Linear Algebra . [4] C. Luo, H. Thakkar, H. Wang, and C. Zaniolo. A native [5] C. Ordonez. Statistical model computation with UDFs.
