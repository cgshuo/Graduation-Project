 Google Research, 111 8th Avenue, New York, NY 10011 Google Research, 111 8th Avenue, New York, NY 10011 Ensemble methods are general techniques in machine learning for combining several predictors or experts to create a more accurate one. In the batch learning set-ting, techniques such as bagging, boosting, stacking, error-correction techniques, Bayesian averaging, or other av-eraging schemes are prominent instances of these meth-ods ( Breiman , 1996 ; Freund &amp; Schapire , 1997 ; Smyth &amp; Wolpert , 1999 ; MacKay , 1991 ; Freund et al. , 2004 ). En-semble methods often significantly improve performance in practice ( Quinlan , 1996 ; Bauer &amp; Kohavi , 1999 ; Caru-efit from favorable learning guarantees. In particular, Ad-aBoost and its variants are based on a rich theoretical anal-ysis, with performance guarantees in terms of the margins of the training samples ( Schapire et al. , 1997 ; Koltchinskii &amp; Panchenko , 2002 ).
 Standard ensemble algorithms such as AdaBoost combine functions selected from a base classifier hypothesis set H In many successful applications of AdaBoost, H is reduced to the so-called boosting stumps , that is decision trees of depth one. For some difficult tasks in speech or image processing, simple boosting stumps are not sufficient to achieve a high level of accuracy. It is tempting then to use a more complex hypothesis set, for example the set of all decision trees with depth bounded by some relatively large number. But, existing learning guarantees for AdaBoost depend not only on the margin and the number of the training examples, but also on the complexity of H mea-sured in terms of its VC-dimension or its Rademacher com-plexity ( Schapire et al. , 1997 ; Koltchinskii &amp; Panchenko , 2002 ). These learning bounds become looser when us-ing too complex base classifier sets H . They suggest a risk of overfitting which indeed can be observed in some experiments with AdaBoost ( Grove &amp; Schuurmans , 1998 ; This paper explores the design of alternative ensemble al-gorithms using as base classifiers a hypothesis set H that may contain very deep decision trees, or members of some other very rich or complex families, and that can yet suc-ceed in achieving a higher performance level. Assume that the set of base classifiers H can be decomposed as the union of p disjoint families H 1 ,...,H p ordered by increas-ing complexity, where H k , k 2 [1 ,p ] , could be for example the set of decision trees of depth k , or a set of functions based on monomials of degree k . Figure 1 shows a pictorial illustration. Of course, if we strictly confine ourselves to using hypotheses belonging only to families H k with small k , then we are effectively using a smaller base classifier set H with favorable guarantees. But, to succeed in some chal-lenging tasks, the use of a few more complex hypotheses could be needed. The main idea behind the design of our algorithms is that an ensemble based on hypotheses drawn from H 1 ,...,H p can achieve a higher accuracy by making use of hypotheses drawn from H k s with large k if it allo-cates more weights to hypotheses drawn from H k s with a small k . But, can we determine quantitatively the amounts of mixture weights apportioned to different families? Can we provide learning guarantees for such algorithms? Note that our objective is somewhat reminiscent of that of model selection, in particular Structural Risk Minimization (SRM) ( Vapnik , 1998 ), but it differs from that in that we do not wish to limit our base classifier set to some optimal H base hypotheses even relatively deep trees from rich H k s, with the promise of doing so infrequently, or that of re-serving them a somewhat small weight contribution. This provides the flexibility of learning with deep hypotheses. We present a new algorithm, DeepBoost , whose design is precisely guided by the ideas just discussed. Our algorithm is grounded in a solid theoretical analysis that we present in Section 2 . We give new data-dependent learning bounds for convex ensembles. These guarantees are expressed in terms of the Rademacher complexities of the sub-families H k and the mixture weight assigned to each H k , in ad-dition to the familiar margin terms and sample size. Our capacity-conscious algorithm is derived via the application of a coordinate descent technique seeking to minimize such learning bounds. We give a full description of our algo-rithm, including the details of its derivation and its pseu-docode (Section 3 ) and discuss its connection with previ-ous boosting-style algorithms. We also report the results of several experiments (Section 4 ) demonstrating that its per-formance compares favorably to that of AdaBoost, which is known to be one of the most competitive binary classifi-cation algorithms. Non-negative linear combination ensembles such as boost-ing or bagging typically assume that base functions are se-lected from the same hypothesis set H . Margin-based gen-eralization bounds were given for ensembles of base func-tions taking values in { 1 , +1 } by Schapire et al. ( 1997 ) in terms of the VC-dimension of H . Tighter margin bounds with simpler proofs were later given by Koltchinskii &amp; Panchenko ( 2002 ), see also ( Bartlett &amp; Mendelson , 2002 ), for the more general case of a family H taking arbitrary real values, in terms of the Rademacher complexity of H . Here, we also consider base hypotheses taking arbitrary real values but assume that they can be selected from sev-eral distinct hypothesis sets H 1 ,...,H p with p 1 and present margin-based learning in terms of the Rademacher complexity of these sets. Remarkably, the complexity term of these bounds admits an explicit dependency in terms of the mixture coefficients defining the ensembles. Thus, the ensemble family we consider is F = conv( S p is the family of functions f of the form f = P T where  X  =(  X  1 ,...,  X  T ) is in the simplex and where, for each t 2 [1 ,T ] , h t is in H k Let X denote the input space. H 1 ,...,H p are thus fam-ilies of functions mapping from X to R . We consider the familiar supervised learning scenario and assume that training and test points are drawn i.i.d. according to some distribution D over X  X  { 1 , +1 } and denote by S = (( x 1 ,y 1 ) ,..., ( x m ,y m )) a training sample of size m according to D m .
 Let  X  &gt; 0 . For a function f taking values in R , we de-note by R ( f ) its binary classification error, by R  X  (  X  -margin error, and by b R S,  X  ( f ) its empirical margin error: R ( f )= E b R where the notation ( x, y )  X  S indicates that ( x, y ) is drawn according to the empirical distribution defined by S . The following theorem gives a margin-based Rademacher complexity bound for learning with such functions in the binary classification case. As with other Rademacher com-plexity learning guarantees, our bound is data-dependent, which is an important and favorable characteristic of our results. For p =1 , that is for the special case of a single hypothesis set, the analysis coincides with that of the stan-dard ensemble margin bounds ( Koltchinskii &amp; Panchenko , 2002 ).
 Theorem 1. Assume p&gt; 1 . Fix  X  &gt; 0 . Then, for any &gt; 0 , with probability at least 1 over the choice of a sample S of size m drawn i.i.d. according to D m , the following inequality holds for all f = P T
R ( f )  X  b R S,  X  ( f )+ Thus, R ( f )  X  b R S,  X  ( f )+ 4 with C ( m, p )= O This result is remarkable since the complexity term in the right-hand side of the bound admits an explicit depen-dency on the mixture coefficients  X  t . It is a weighted aver-age of Rademacher complexities with mixture weights  X  t , t 2 [1 ,T ] . Thus, the second term of the bound suggests that, while some hypothesis sets H k used for learning could have a large Rademacher complexity, this may not be detri-mental to generalization if the corresponding total mixture weight (sum of  X  t s corresponding to that hypothesis set) is relatively small. Such complex families offer the potential of achieving a better margin on the training sample. The theorem cannot be proven via a standard Rademacher complexity analysis such as that of Koltchinskii &amp; Panchenko ( 2002 ) since the complexity term of the bound would then be the Rademacher complexity of the family of hypotheses F = conv( S p pend on the specific weights  X  t defining a given func-tion f . Furthermore, the complexity term of a standard Rademacher complexity analysis is always lower bounded by the complexity term appearing in our bound. Indeed, since R m (conv( [ p ing lower bound holds for any choice of the non-negative mixtures weights  X  t summing to one: Thus, Theorem 1 provides a finer learning bound than the one obtained via a standard Rademacher complexity anal-ysis. The full proof of the theorem is given in Appendix A . Our proof technique exploits standard tools used to de-rive Rademacher complexity learning bounds ( Koltchin-skii &amp; Panchenko , 2002 ) as well as a technique used by Schapire, Freund, Bartlett, and Lee ( 1997 ) to derive early VC-dimension margin bounds. Using other standard tech-2012 ), Theorem 1 can be straightforwardly generalized to hold uniformly for all  X  &gt; 0 at the price of an additional term that is in O In this section, we will use the learning guarantees of Sec-tion 2 to derive a capacity-conscious ensemble algorithm for binary classification. 3.1. Optimization problem Let H 1 ,...,H p be p disjoint families of functions taking values in [ 1 , +1] with increasing Rademacher complex-ities R m ( H k ) , k 2 [1 ,p ] . We will assume that the hy-pothesis sets H k are symmetric, that is, for any h 2 H k we also have ( h ) 2 H k , which holds for most hypothe-sis sets typically considered in practice. This assumption is not necessary but it helps simplifying the presentation of our algorithm. For any hypothesis h 2[ p by d ( h ) the index of the hypothesis set it belongs to, that is h 2 H d ( h ) . The bound of Theorem 1 holds uniformly for all  X  &gt; 0 and functions f 2 conv( S p last term of the bound does not depend on  X  , it suggests selecting  X  to minimize admit the same generalization error, we can instead search for  X  0 with P T min The first term of the objective is not a convex function of  X  and its minimization is known to be computation-ally hard. Thus, we will consider instead a convex upper bound. Let u 7! ( u ) be a non-increasing convex func-tion upper bounding u 7! 1 u  X  0 with differentiable over R and 0 ( u ) 6 =0 for all u . may be selected to be for example the exponential function as in AdaBoost ( Freund &amp; Schapire , 1997 ) or the logistic function. Using such an upper bound, we obtain the following convex optimization problem: min where we introduced a parameter 0 controlling the bal-ance between the magnitude of the values taken by function and the second term. Introducing a Lagrange variable be equivalently written as min Here, is a parameter that can be freely selected by the algorithm since any choice of its value is equivalent to a choice of  X  in ( 2 ). Let { h 1 ,...,h N } be the set of distinct base functions, and let G be the objective function based on that collection:
G (  X  )= with  X  =(  X  1 ,...,  X  N ) 2 R N . Note that we can drop the requirement  X  0 since the hypothesis sets are symmetric and  X  t h t =(  X  t )( h t ) . For each hypothesis h , we keep either h or h in { h 1 ,...,h N } . Using the notation for all j 2 [1 ,N ] , our optimization problem can then be rewritten as min  X  F (  X  ) with
F (  X  )= with no non-negativity constraint on  X  . The function F is convex as a sum of convex functions and admits a sub-differential at all  X  2 R . We can design a boosting-style algorithm by applying coordinate descent to F (  X  ) . ter t 1 iterations and let  X  0 = 0 . Let e k denote the k th unit vector in R N , k 2 [1 ,N ] . The direction e k and the step  X  selected at the t th round are those minimizing F (  X  t 1 +  X  e k ) , that is F denote by D t the distribution defined by where S t is a normalization factor, S t = P m y f t 1 ( x i )) . For any s 2 [1 ,T ] and j 2 [1 ,N ] , we denote by  X  s,j the weighted error of hypothesis h j for the distribu-tion D s , for s 2 [1 ,T ] : 3.2. DeepBoost Figure 2 shows the pseudocode of the algorithm DeepBoost derived by applying coordinate descent to the objective function ( 4 ). The details of the derivation of the expres-sion are given in Appendix B . In the special cases of the D 1 for i 1 to m do 2 D 1 ( i ) 1 3 for t 1 to T do 4 for j 1 to N do 5 if (  X  t 1 ,j 6 = 0) then 7 elseif  X  t,j 1 8 d j 0 10 k argmax 15  X  t log 16 else  X  t log 19 for i 1 to m do 21 f P N 22 return f exponential loss ( ( u ) = exp( u ) ) or the logistic loss ( ( u ) = log is given for the step size (lines 12-16), which is the same in both cases (see Sections B.4 and B.5 ). In the generic case, the step size  X  t can be found using a line search or other numerical methods. Note that when the condition of line 12 is satisfied, the step taken by the algorithm cancels out the coordinate along the direction k , thereby leading to a sparser result. This is consistent with the fact that the ob-jective function contains a second term based on (weighted) L -norm, which is favoring sparsity.
 Our algorithm is related to several other boosting-type al-gorithms devised in the past. For =0 and =0 and using the exponential surrogate loss, it coincides with Ada-Boost ( Freund &amp; Schapire , 1997 ) with precisely the same direction and same step log as the hypothesis set for base learners. This corresponds to ignoring the complexity term of our bound as well as the control of the sum of the mixture weights via . For =0 and =0 and using the logistic surrogate loss, our algo-rithm also coincides with additive logistic loss ( Friedman et al. , 1998 ).
 In the special case where =0 and 6 =0 and for the exponential surrogate loss, our algorithm matches the L -norm regularized AdaBoost (e.g., see ( R  X  atsch et al. , 2001a )). For the same choice of the parameters and for the logistic surrogate loss, our algorithm matches the L 1 norm regularized additive Logistic Regression studied by Duchi &amp; Singer ( 2009 ) using the base learner hypothesis set H = S p foundation of our algorithm and analysis is instead to take into account the relative complexity of the sub-families H Also, note that L 1 -norm regularized AdaBoost and Logis-tic Regression can be viewed as algorithms minimizing the learning bound obtained via the standard Rademacher com-plexity analysis ( Koltchinskii &amp; Panchenko , 2002 ), using the exponential or logistic surrogate losses. Instead, the objective function minimized by our algorithm is based on the generalization bound of Theorem 1 , which as discussed earlier is a finer bound (see ( 1 )). For =0 but 6 =0 , our algorithm is also close to the so-called unnormalized Arc-ing ( Breiman , 1999 ) or AdaBoost  X  ( R  X  atsch &amp; Warmuth , 2002 ) using H as a hypothesis set. AdaBoost  X  coincides with AdaBoost modulo the step size, which is more con-servative than that of AdaBoost and depends on  X  . R  X  atsch &amp; Warmuth ( 2005 ) give another variant of the algorithm that does not require knowing the best  X  , see also the re-lated work of Kivinen &amp; Warmuth ( 1999 ); Warmuth et al. ( 2006 ).
 Our algorithm directly benefits from the learning guaran-tees given in Section 2 since it seeks to minimize the bound of Theorem 1 . In the next section, we report the results of our experiments with DeepBoost. Let us mention that we have also designed an alternative deep boosting algorithm that we briefly describe and discuss in Appendix C . An additional benefit of the learning bounds presented in Section 2 is that they are data-dependent. They are based on the Rademacher complexity of the base hypothesis sets H k , which in some cases can be well estimated from the training sample. The algorithm DeepBoost directly inher-its this advantage. For example, if the hypothesis set H k is based on a positive definite kernel with sample matrix K k , it is known that its empirical Rademacher complexity can be upper bounded by tions taking binary values, we can use an upper bound on the Rademacher complexity in terms of the growth func-tion of H k ,  X  H for the family H stumps  X  functions for each dimension with m points. Thus, the fol-lowing inequality holds: Similarly, we consider the family of decision trees H stumps of depth 2 with the same question at the internal nodes of depth 1 . We have  X  are d ( d 1) / 2 distinct trees of this type and since each induces at most (2 m ) 2 labelings. Thus, we can write More generally, we also consider the family of all binary decision trees H trees that VC-dim( H trees 1997 ). More generally, the VC-dimension of T n , the fam-ily of decision trees with n nodes in dimension d can be bounded by (2 n + 1) log et al. , 2012 )). Since R m ( H )  X  for any hypothesis class H we have The experiments with DeepBoost described below use ei-for some K&gt; 0 , as the base hypothesis sets. For any hy-pothesis in these sets, DeepBoost will use the upper bounds given above as a proxy for the Rademacher complexity of the set to which it belongs. We leave it to the future to experiment with finer data-dependent estimates or up-per bounds on the Rademacher complexity, which could further improve the performance of our algorithm. Re-call that each iteration of DeepBoost searches for the base hypothesis that is optimal with respect to a certain crite-rion (see lines 5-10 of Figure 2 ). While an exhaustive search is feasible for H stumps sive to visit all trees in H trees fore, when using H trees potheses we use the following heuristic search procedure in each iteration t : First, the optimal tree h  X  found via exhaustive search. Next, for all 1 &lt;k  X  K , a locally optimal tree h  X  ing only trees that can be obtained by adding a single layer of leaves to h  X  in the set { h  X  are the hypotheses selected in previous iterations. Breiman ( 1999 ) and Reyzin &amp; Schapire ( 2006 ) extensively investigated the relationship between the complexity of de-cision trees in an ensemble learned by AdaBoost and the generalization error of the ensemble. We tested DeepBoost on the same UCI datasets used by these authors, http:// archive.ics.uci.edu/ml/datasets.html , specifi-cally breastcancer , ionosphere , german(numeric) and diabetes . We also experimented with two optical character recognition datasets used by Reyzin &amp; Schapire ( 2006 ), ocr17 and ocr49 , which contain the handwritten digits 1 and 7, and 4 and 9 (respectively). Finally, because these OCR datasets are fairly small, we also constructed the analogous datasets from all of MNIST, http://yann. lecun.com/exdb/mnist/ , which we call ocr17-mnist and ocr49-mnist . More details on all the datasets are given in Table 4 , Appendix D.1 .
 As we discussed in Section 3.2 , by fixing the parameters and to certain values, we recover some known algorithms as special cases of DeepBoost. Our experiments compared DeepBoost to AdaBoost ( = =0 with exponential loss), to Logistic Regression ( = =0 with logistic loss), which we abbreviate as LogReg, to L 1 -norm regular-as AdaBoost-L1, and also to the L 1 -norm regularized ad-ditive Logistic Regression algorithm studied by ( Duchi &amp; Singer , 2009 )( &gt; 0 , =0 ) abbreviated as LogReg-L1. In the first set of experiments reported in Table 1 , we com-pared AdaBoost, AdaBoost-L1, and DeepBoost with the exponential loss ( ( u ) = exp( u ) ) and base hypothe-timized over 2 { 2 i : i =6 ,..., 0 } and for Deep-Boost, we optimized over in the same range and 2 { 0 optimization procedure is described below.
 In the second set of experiments reported in Table 2 , we used base hypotheses H trees maximum tree depth K was an additional parameter to be optimized. Specifically, for AdaBoost we optimized over K 2 { 1 ,..., 6 } , for AdaBoost-L1 we optimized over those same values for K and 2 { 10 i : i =3 ,..., 7 } , and for DeepBoost we optimized over those same values for K, and 2 { 10 i : i =3 ,..., 7 } .
 The last set of experiments, reported in Table 3 , are identi-cal to the experiments reported in Table 2 , except we used the logistic loss ( u ) = log We used the following parameter optimization procedure in all experiments: Each dataset was randomly partitioned into 10 folds, and each algorithm was run 10 times, with a different assignment of folds to the training set, validation set and test set for each run. Specifically, for each run { 0 ,..., 9 } , fold i was used for testing, fold i +1( mod 10) was used for validation, and the remaining folds were used for training. For each run, we selected the parameters that had the lowest error on the validation set and then measured the error of those parameters on the test set. The average error and the standard deviation of the error over all 10 runs is reported in Tables 1 , 2 and 3 , as is the average number of trees and the average size of the trees in the ensembles. In all of our experiments, the number of iterations was set to 100 . We also experimented with running each algorithm for up to 1 , 000 iterations, but observed that the test errors did not change significantly, and more importantly the or-dering of the algorithms by their test errors was unchanged from 100 iterations to 1 , 000 iterations.
 Observe that with the exponential loss, DeepBoost has a smaller test error than AdaBoost and AdaBoost-L1 on ev-ery dataset and for every set of base hypotheses, except for the ocr49-mnist dataset with decision trees where its per-formance matches that of AdaBoost-L1. Similarly, with the logistic loss, DeepBoost performs always at least as well as LogReg or LogReg-L1. For the small-sized UCI datasets it the larger ocrXX-mnist datasets, our results with Deep-Boost are statistically significantly better at the 2% level using one-sided paired t-tests in all three sets of experi-ments (three tables), except for ocr49-mnist in Table 3 , where this holds only for the comparison with LogReg. This across-the-board improvement is the result of Deep-Boost X  X  complexity-conscious ability to dynamically tune the sizes of the decision trees selected in each boosting round, trading off between training error and hypothesis class complexity. The selected tree sizes should depend on properties of the training set, and this is borne out by our experiments: For some datasets, such as breastcancer , DeepBoost selects trees that are smaller on average than the trees selected by AdaBoost-L1 or LogReg-L1, while, for other datasets, such as german , the average tree size is larger. Note that AdaBoost and AdaBoost-L1 produce ensembles of trees that have a constant depth since neither algorithm penalizes tree size except for imposing a maxi-mum tree depth K , while for DeepBoost the trees in one ensemble typically vary in size. Figure 3 plots the distri-bution of tree sizes for one run of DeepBoost. It should be noted that the columns for AdaBoost in Table 1 simply list the number of stumps to be the same as the number of boosting rounds; a careful examination of the ensembles for 100 rounds of boosting typically reveals a 5% duplica-tion of stumps in the ensembles.
 Theorem 1 is a margin-based generalization guarantee, and is also the basis for the derivation of DeepBoost, so we should expect DeepBoost to induce large margins on the training set. Figure 4 shows the margin distributions for AdaBoost, AdaBoost-L1 and DeepBoost on the same sub-set of the ionosphere dataset. We presented a theoretical analysis of learning with a base hypothesis set composed of increasingly complex sub-families, including very deep or complex ones, and de-rived an algorithm, DeepBoost, which is precisely based on those guarantees. We also reported the results of exper-iments with this algorithm and compared its performance with that of AdaBoost and additive Logistic Regression, and their L 1 -norm regularized counterparts in several tasks. We have derived similar theoretical guarantees in the multi-class setting and used them to derive a family of new multi-class deep boosting algorithms that we will present and dis-cuss elsewhere. Our theoretical analysis and algorithmic design could also be extended to ranking and to a broad class of loss functions. This should also lead to the gener-alization of several existing algorithms and their use with a richer hypothesis set structured as a union of families with different Rademacher complexity. In particular, the broad family of maximum entropy models and conditional max-imum entropy models and their many variants, which in-cludes the already discussed logistic regression, could all be extended in a similar way. The resulting DeepMaxent models (or their conditional versions) may admit an al-ternative theoretical justification that we will discuss else-where. Our algorithm can also be extended by consider-ing non-differentiable convex surrogate losses such as the hinge loss. When used with kernel base classifiers, this leads to an algorithm we have named DeepSVM . The the-ory we developed could perhaps be further generalized to encompass the analysis of other learning techniques such as multi-layer neural networks.
 Our analysis and algorithm also shed some new light on some remaining questions left about the theory underly-ing AdaBoost. The primary theoretical justification for AdaBoost is a margin guarantee ( Schapire et al. , 1997 ; Koltchinskii &amp; Panchenko , 2002 ). However, AdaBoost does not precisely maximize the minimum margin, while other algorithms such as arc-gv ( Breiman , 1996 ) that are designed to do so tend not to outperform AdaBoost ( Reyzin &amp; Schapire , 2006 ). Two main reasons are suspected for this observation: (1) in order to achieve a better margin, algo-rithms such as arc-gv may tend to select deeper decision trees or in general more complex hypotheses, which may then affect their generalization; (2) while those algorithms achieve a better margin, they do not achieve a better mar-gin distribution. Our theory may help better understand and evaluate the effect of factor (1) since our learning bounds explicitly depend on the mixture weights and the contri-bution of each hypothesis set H k to the definition of the ensemble function. However, our guarantees also suggest a better algorithm, DeepBoost.
 We thank Vitaly Kuznetsov for his comments on an ear-lier draft of this paper. The work of M. Mohri was partly funded by the NSF award IIS-1117591.
 Bartlett, Peter L. and Mendelson, Shahar. Rademacher and
Gaussian complexities: Risk bounds and structural re-sults. JMLR , 3, 2002.
 Bauer, Eric and Kohavi, Ron. An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. Machine Learning , 36(1-2):105 X 139, 1999. Breiman, Leo. Bagging predictors. Machine Learning , 24 (2):123 X 140, 1996.
 Breiman, Leo. Prediction games and arcing algorithms. Neural Computation , 11(7):1493 X 1517, 1999.
 Caruana, Rich, Niculescu-Mizil, Alexandru, Crew, Geoff, and Ksikes, Alex. Ensemble selection from libraries of models. In ICML , 2004.
 Dietterich, Thomas G. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Machine Learning , 40(2):139 X 157, 2000.
 Duchi, John C. and Singer, Yoram. Boosting with structural sparsity. In ICML , pp. 38, 2009.
 Freund, Yoav and Schapire, Robert E. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer System Sciences , 55(1): 119 X 139, 1997.
 Freund, Yoav, Mansour, Yishay, and Schapire, Robert E.
Generalization bounds for averaged classifiers. The An-nals of Statistics , 32:1698 X 1722, 2004.
 Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
Additive logistic regression: a statistical view of boost-ing. Annals of Statistics , 28:2000, 1998.
 Grove, Adam J and Schuurmans, Dale. Boosting in the limit: Maximizing the margin of learned ensembles. In AAAI/IAAI , pp. 692 X 699, 1998.
 Kivinen, Jyrki and Warmuth, Manfred K. Boosting as en-tropy projection. In COLT , pp. 134 X 144, 1999.
 Koltchinskii, Vladmir and Panchenko, Dmitry. Empiri-cal margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics , 30, 2002.
 MacKay, David J. C. Bayesian methods for adaptive mod-els . PhD thesis, California Institute of Technology, 1991. Mansour, Yishay. Pessimistic decision tree pruning based on tree size. In Proceedings of ICML , pp. 195 X 201, 1997.
 Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar, Ameet. Foundations of Machine Learning . The MIT Press, 2012.
 Quinlan, J. Ross. Bagging, boosting, and C4.5. In AAAI/IAAI, Vol. 1 , pp. 725 X 730, 1996.
 R  X  atsch, Gunnar and Warmuth, Manfred K. Maximizing the margin with boosting. In COLT , pp. 334 X 350, 2002. R  X  atsch, Gunnar and Warmuth, Manfred K. Efficient margin maximizing with boosting. Journal of Machine Learning Research , 6:2131 X 2152, 2005.
 R  X  atsch, Gunnar, Mika, Sebastian, and Warmuth, Man-fred K. On the convergence of leveraging. In NIPS , pp. 487 X 494, 2001a.
 R  X  atsch, Gunnar, Onoda, Takashi, and M  X  uller, Klaus-
Robert. Soft margins for AdaBoost. Machine Learning , 42(3):287 X 320, 2001b.
 Reyzin, Lev and Schapire, Robert E. How boosting the margin can also boost classifier complexity. In ICML , pp. 753 X 760, 2006.
 Schapire, Robert E. Theoretical views of boosting and ap-plications. In Proceedings of ALT 1999 , volume 1720 of
Lecture Notes in Computer Science , pp. 13 X 25. Springer, 1999.
 Schapire, Robert E. The boosting approach to machine learning: An overview. In Nonlinear Estimation and Classification , pp. 149 X 172. Springer, 2003.
 Schapire, Robert E., Freund, Yoav, Bartlett, Peter, and Lee,
Wee Sun. Boosting the margin: A new explanation for the effectiveness of voting methods. In ICML , pp. 322 X  330, 1997.
 Smyth, Padhraic and Wolpert, David. Linearly combining density estimators via stacking. Machine Learning , 36: 59 X 83, July 1999.
 Vapnik, Vladimir N. Statistical Learning Theory . Wiley-Interscience, 1998.
 Warmuth, Manfred K., Liao, Jun, and R  X  atsch, Gunnar. To-tally corrective boosting algorithms that maximize the
