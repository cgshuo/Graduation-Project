 Lately there exist increasing d emands for online abnormality mon-itoring over trajectory streams, which are obtained from moving object tracking devices. This problem is challenging due to the re-quirement of high speed data processing within limited space cost. In this paper, we present a novel framework for monitoring anoma-lies over continuous traj ectory streams. First, we illustrate the im-portance of distance-based anomaly monitoring over moving object trajectories. Then, we utilize the local continuity characteristics of trajectories to build local clusters upon trajectory streams and monitor anomalies via efficient pruning strategies. To further re-duce the time cost, we propose a piecewise metric index structure to reschedule the joining order of local clusters. Finally, our ex-tensive experiments demonstrate the effectiveness and efficiency of our methods.
 H.2.4 [ Database Management ]: Systems X  Multimedia Databases ; H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithms, Design, Experimentation Outlier Detection, Similarity Search, Temporal Data
Recently, trajectory mining has attracted much attention due to its wide applications, especially in context-aware computing en-vironment. Many researchers have worked on trajectory cluster-ing [23] and classification [13]. Sometimes abnormal trajectories tend to carry critical information of potential problems which re-quire immediate attention and need be resolved at an early stage. There are many applications that require real-time monitoring of abnormal sequential patterns over streaming trajectories. Exam-ples include elder care, child custody, accurate mobile localization, automatic driving and so on. Below is a motivating example.
E XAMPLE 1. Many senior citizens require constant monitoring and care but such care is expensive without an automatic process. Thus to continuously monitor anomalies of their trajectory streams generated from mobile tracking devices will be very useful, espe-cially when they go outdoor. Such anomalies are very rare pat-terns that may indicate events such as taking a wrong bus, having a bad fall, encountering a sudden slow-down and getting lost. If their families could be notified in time, immediate and possible life-saving actions can be taken. Figure 1 describes a typical scenario: one day Bob X  X  father takes a strange detour (the red trajectory b ) compared to his usual route (the blue trajectory a ); then Bob is notified immediately about this abnormal case.

There are some nice solutions for problems like online event [15, 24], burst [32, 29], and novelty detection [25] over stream time se-ries (or trajectories), where the respective definitions of anomalies are: 1) distribution/model variations from assumed distribution, 2) bursts with statistical aggregations exceeding a threshold, and 3) abnormal classes by classification based on labeled data. Unfortu-nately, none of the solutions for these anomalies could be extended to find more general anomalies for a great number of applications, because in a trajectory stream, 1) distribution is always changing, 2) burst is only one kind of anomalies, and 3) labeling data has a huge cost. In the example described above, the anomalies are rare patterns which would have big spatial deviations (distances) from the normal trajectories in a certain temporal interval , like the red trajectory (trajectory a ) in Figure 1.

Thus, in this paper, we would like to focus on continuous mon-itoring distance-based anomalies from trajectory streams, where distance is used as a measure of (dis-)similarity between trajectory subsequences. In fact, the usefulness of distance-based anoma-lies for general databases has been thoroughly justified in previous work [20] and [21].

Though distance-based anomalies are more general and effec-tive, the performance of anomaly monitoring suffers from intensive distance computations, thus efficient pruning strategies become es-sential for streaming cases. Furthermore, targeting mobile appli-cations, the proposed algorithm s hould require little extra memory. Na X ve solutions like sequential scan will result in excessive time cost, while R -tree styled structures will incur huge memory con-sumptions for indexing all subsequences extracted from a trajec-tory, concomitant with high update cost. Even with dimensionality reduction, the reduced representation for every subsequence will occupy k r times more space than the original stream data, where k is the dimensionality after reduction. Therefore it is a challenging problem to tame the computation and memory costs of distance-based anomaly monitoring on trajectory streams.

In this paper, we make the following contributions:  X 
Efficient Anomaly Monitoring by Local Clusters : we build local clusters for trajectories in a streaming manner, then anomalies are detected by an efficient cluster join mechanism using pruning strategies without introducing false dismissals.  X 
Piecewise Index for Rescheduling Cluster Join : to further im-prove the performance, we design a piecewise VP-tree (vantage point tree [10]) based index structure and reschedule the order of cluster joins.  X 
Experimental Study : we test the proposed techniques on real world as well as synthetic data. The results show that our anomaly definition is much more useful than other known definitions and our techniques can achieve orders of magnitude improvement in performance compared to a simple pruning approach.

The rest of the paper is organized as follows. Related work is discussed in Section 2. Section 3 states the problem. Section 4 in-troduces a preliminary method and Section 5 introduces our prun-ing methods by local clustering. Section 6 is about the piecewise VP-tree based indexing and rescheduling technique. We present extensive experimental evaluations in Section 7. We conclude in Section 8.
In this section, we briefly review the related work in two relevant areas: time series data management and anomaly detection.
Since trajectory is a special type of time series, we review some work in time series databases. Agrawal et al. X  X  pioneering work [1] uses DFT (discrete fourier transform) to reduce the dimensional-ity of time series and then conducts the search in the reduced di-mensional space. Later on, Faloutsos et al. [11] propose a general frame work for similarity search over time series data, called GEM-INI framework, to support subsequence matching. The main idea is conducting the search in a filter and refine manner via lower bounds derived from dimension reduction techniques. Subsequent work includes various methods of dimension reduction for time series, such as SVD [22], DWT [7], APCA [18], and Chebyshev Polyno-mials [6], all of which guarantee no false dismissals. Meanwhile, many novel and effective distance measures for (dis-)similarity be-tween time series are proposed along with corresponding lower bounds, such as DTW (dynamic time warping) [30], LCSS (longest common subsequence) [28], ERP (edit distance with real penalty) [8], EDR (edit distance on real sequence) [9] and probabilistic mea-sure [31]. However, these techniques are designed for similarity search. For anomaly detections in archive time series, Keogh et al. [19] use SAX (symbol approximate aggregation) and heuristic reorder on candidates to find discords. In their work, discords are defined as sequences that have the furthest distances to their nearest neighbors. A Trie structure is used to reduce the search time. How-ever, in our new problem setting, the trie technique is no longer applicable due to the changing feature of data and the high update cost.

With emerging requirements on continuous stream time series management, Zhu and Shasha [32] find statistics over a single stream and correlations among multiple data streams, where incremental DFT is used to prune uncorrelated stream pairs. Vlachos et al. [29] identify bursts based on the computation of the moving average (MA) and propose a novel burst similarity measure by MA and in-tersections. Gao et al. [12] reduce I/O cost for continuous similarity queries by pre-loading the predicted index pages and archived time series into the allocated cache memory. Bulut et al. [5] provide a well designed multi-resolution hierarchical index for monitoring aggregate and similarity queries over a stream time series. How-ever, none of the above work address the problem of monitoring general anomalies along a stream time series or trajectory streams where no fixed pattern exists.

A lot of research work has been conducted for mining distance-based anomaly (outlier) in traditional databases [14]. The com-mon solution for distance-based anomaly detection applies a nested loop to count range neighbors for every anomaly candidate. Knorr and Ng [20] develop a CELL-based method which can efficiently locate anomalies in very large datasets, yet it is known that the CELL-based method is not scalable to high-dimensional datasets like time series. Ramaswamy et al. [26] rank top n anomalies by distances( D k (  X  ) )totheir k -th nearest neighbors, where a R tree is used for the k -nearest neighbors search of every point p and a partition-based algorith m is proposed. However, the R method cannot be adapted to trajectory streams because it will suf-fer from continuous high speed update and huge memory cost. Bay et al. [3] search anomalies by a range neighbor search with a ran-dom order, together with a simple pruning step, however, it is still quite slow for trajectory streams since too many subsequences need to be checked. Tao et al. [27] prove an upper bound for the mem-ory consumption, which permits the discovery of all anomalies by scanning the dataset three times, but it is not applicable to streams where available data keep changing. Breunig et al. [4] propose the concept of  X  X ocal outliers X , according to which an object is an out-lier if it is significantly different from its spatially nearby objects. Jin et al. [16] propose efficient pruning strategies to find  X  X op-k lo-cal outliers X  quickly. Different from  X  X ocal outliers X , the anomalies we monitor are those trajectory subsequences significantly differ-ent from their spatial nearby ones in a certain temporal window, which fit the feature of stream data.

To summarize, traditional anomaly definitions require the access to a global and static database, which could not exist in stream scenarios. In stream scenarios, data keep updating, but the lim-ited memory can only store a limited  X  X ecent X  sliding window. Of course the solutions for static database anomalies could be applied here, but from the experimental results, we could find that our solu-tion considering the specific features of trajectory streams has got orders of magnitude performance improvement to previous ones.
Before we give the formal definition of the problem, we intro-duce several terms used in this paper. For simplicity of illustration, trajectories used in the following definitions are 1 -dimensional, yet we will see how they can be trivially extended for multi-dimensional trajectories at the end of this section. We define a trajectory stream as an totally ordered infinite sequences S = { s 1 , s 2 ,..., s where s i is a real value arriving at a specific time tick i , and time tick i is after time tick i  X  1 . Without loss of generality, in our diagrams, the stream flow always comes from the right side.
Given a trajectory stream S , any subsequence B = { s i , s ..., s i + w b  X  1 }of S , is called a base window ,where w defined base window length. Given a base window B = { s i ..., s i + w b  X  1 } of trajectory stream S = { s 1 , s 2 sequence L = { s i  X  w l , s i  X  w l +1 , ..., s i  X  1 } of length w as
B  X  X  left sliding window , the subsequence R = { s i + w b where w l and w r are pre-defined sliding window lengths. Figure 2 gives a visualized example about the basic constructs of trajectory stream, base window, left sliding window and right sliding window.
D EFINITION 1(D ISTANCE ). Distance(q, c) is a function that has base window q and c as inputs and returns a nonnegative value d , which is said to be the distance from q to c .

Without loss of generality, we take Euclidean distance as the dis-tance measure in our implementations: for base window X and Y , distance( X , Y )= w b i =1 ( X i  X  Y i ) 2 .

D EFINITION 2(N EIGHBOR ). Given a base window B = { s j , s it is an anomaly, a trajectory stream S = { s 1 , s 2 h  X  w b , and distance threshold d , for any subsequence S i s +1 ,..., s i + w b  X  1 }of S ,if distance ( B , S i ) &lt;d , we say that a neighbor of B in S .
 window B in S , we can get the number ( n 1 )of B s neighbors in its left sliding window L , and the number ( n 2 )of B s neighbors in its right sliding window R ,then B is a trajectory stream anomaly if n + n 2 &lt;k .

The basic problem is to determine if a base window B has less than k neighbors in its left and right sliding windows, and if so, B is an anomaly. The rationale behind Definition 3 is the observa-tion that in real world applications, anomalies in a trajectory stream usually not only have salient spatial deviations from both preced-ing and succeeding normal subsequences, but also last for a short period. In fact, other than the sliding window, the anomaly defini-tion is the same as the distance-based outliers defined in [20]. In other words, we adopt traditional d istance-based anomaly in stream scenarios. However, the solution proposed for detecting distance-based anomalies in traditional static databases is not applicable to trajectory streams due to high-speed data update of stream data. Now we give an introductive definition of the anomaly monitoring problem.

D EFINITION 4(P ROBLEM ). On trajectory stream S , at every time tick t , upon the arrival of s t , we check whether the base win-dow B ending at s t  X  w r is a trajectory stream anomaly.
In practice, we recommend applications to take a vary large left sliding window length w l and a short right slide window length w to ensure quick responses to anomalies, because for anomaly base windows, part of the monitoring procedure for the base windows is deferred to the time when its whole right sliding window comes up. To extend our problem definition for a m -D trajectory stream, we only need to extend the distance function: for base window X and Y , distance( X , Y )= w b i =1 m j =1 ( X i [ j ]  X  Y
Obviously, directly computing the anomalies tick by tick is com-putationally expensive. For each base window B in the trajectory stream, we need n 1 and n 2 , the numbers of neighbors in the left and right sliding windows of B , respectively. If n 1 + n B is an anomaly. One can adopt a more efficient method by ran-dom sampling suggested by [3] for ou tlier detection in traditional databases. To gather the count of neighbors one can repeatedly pick some base window B in the left or right sliding window of B in a random manner, and check whether B is B  X  X  neighbor. If B  X  X  neighbor count reaches k , B is certified to be non-anomaly and remaining search for B can be simply pruned. We call this ran-dom sampling method simple pruning . Simple pruning can be seen as a set of independent Bernoulli trials where we keep drawing samples until k successes or the whole dataset is exhausted. The following theorem [3] gives an analytic result of time cost by sim-ple pruning.
 probability t hat a randomly drawn sampl e lies within distance d to base window x , P a be the anomaly rate, N = w l + w r be a random variable representing the number of trials (distance computations) until we have k successes, and let P ( Y = the probab ility of obtaining the k -th success on trial y . Then the expectation of Y follows:
Since P a is tiny, E ( Y ) is dominated by k F x ( d ) , which is inde-pendent of the sliding window size. In most trajectory stream ap-plications, data arrive at a very high speed, thus it is still of great challenges for us to design exact and more efficient algorithms be-yond simple pruning, in order to avoid data buffer overflow.
Our goal is to do better than the simple pruning technique. Our idea is based on the observation that most trajectory streams are locally continuous, so that two highly overlapping base windows tend to have a short distance. We propose a local clustering-based approach to monitor anomalies, which utilizes the property of lo-cal continuity. In fact, many current time series/trajectory index-ing methods have implicitly assume d the underline time series data have the local continuity property. It is known that neither DFT [1], DWT [7], APCA [18] for Euclidean distance, nor LB_Keogh [17], LB_Zhu [33] for DTW distance could get a satisfactory pruning power on a highly fluctuant time series or trajectories. In Section 5.1, we introduce an online local clustering algorithm. Then, batch monitoring is established with pruning strategies in Section 5.2. Finally we give a detailed cost analysis in Section 5.3.
In the following we define local cluster C as a trajectory subse-quence of length at most m b in which there exists a base window that all other base windows in C are within a certain distance  X  from B . One such base window B is chosen as the pivot of C . Actually local clusters are a special kind of temporal partitions on a trajec-tory stream, and each cluster contains a number of consecutive data points within a stream. We shall show that local clusters are easy to build and can be incrementally updated, which fits the requirement Algorithm Online Local Clustering 1: if pivotf ound =FALSE then 2: (Comments: accumulate points in the left bin) 4: the current left bin minus time tick t  X  1 is the final left bin 6: pivotf ound  X  TRUE 7: else 8: time tick t is added to the current left bin L 9: update r to be the greatest dist i 10: end if 11: end if 12: if pivotf ound then 13: (Comments: accumulate points in the right bin) 15: if dist &gt;  X  or current cluster size is m b then 17: set current left bin L to { t } , r  X  0 , 19: else 20: if dist &gt; r then 21: r  X  dist 22: end if 23: end if 24: end if of continuous trajectory streams perfectly. How to determine m and  X  will be discussed later.

D EFINITION 5(L OCAL C LUSTER ). With a distance thresh-old  X  and a temporal constraint m b , given a sequence C = { , ..., s j } , j  X  i +1  X  m b , which is a piece of a trajectory stream S ,if  X  s k  X  X  ,  X  s x  X  X  , B x and B k being base windows ending at s and s k respectively, distance ( B k , B x )  X   X  , C is called a local cluster .
 For a local cluster C of distance threshold  X  , a special point s is selected as C  X  X  pivot , s x satisfies the condition that B x and B i being base windows ending at s x and s i respectively, distance( B x , B i )  X   X  . The value of maximum s  X  X  distance is called C  X  X  radius . Points whose time ticks are less (greater) than x in C constitute pivot s x  X  X  left bin ( right bin ).

The concepts of pivot, radius, left bin and right bin are used in the algorithm of online local cluster construction. The parameter m specifies that at most how many based windows could be included in a local cluster, while the parameter  X  specifies the upper bound of distances from the pivot to all base windows in the local cluster. Both parameters need to be specified for local clustering.
In the data structure of a local cluster, 4 variables are stored: its pivot X  X  position on the sliding window, the start and end positions that it covers, and its radius. In the following part, we use pivot to denote either the base window ending at its position or the cor-responding local cluster data structure. Figure 3 shows the online local clustering algorithm, where a greedy approach is used to form local cluster partitions. In this algorithm, for a resulting cluster each time tick t i in the left bin requires at most l i distance compu-tations if there are l i elements aligned on t i  X  X  left but within time ticks in the right bin, each requires a single distance computa-tion. Thus if the average cluster size is m then the average number of distance computations per time tick is at most m/ 2 . With local clusters, we search for neighbors in a batch manner. Neighbor search for base window B is firstly triggered upon the for-mation of B  X  X  local cluster, and secondly called upon the arrival of the rightmost point in B  X  X  right sliding window if B  X  X  accumulated neighbor count has not reached k . In order to achieve the neigh-bor search we must keep all needed data points, which we call the current sliding window W .Let t o 1 be the oldest time tick in the local clusters that overlaps with the left sliding windows of the new data points that have not been clustered, and t o 2 be the oldest time tick of a local cluster whose entire right sliding window has not yet arrived. W is made up of all time ticks from min { t o 1 ,t current time tick.

There are two concurrent steps in the batch monitoring:  X 
When a new local cluster C new is constructed, we join it with the preceding clusters in a random order (by a clusterjoin function) in W until every base window in C new has more than k neigh-bors. If we exhaust all possible candidate base windows in the left sliding window and cannot find k neighbors for base window
B in C new , we keep the neighbor count for B .  X 
When the entire right sliding window of all base windows in local cluster C old are arriving, in C old , if any base window neighbor count is kept in step (1), B  X  X  neighbor count in its right sliding windows are determined in a similar way. The neighbor count n 1 collected in step (1) for a base window B is to be added to the count n 2 in this step to give the total neighbor count.
In the two batch monitoring steps, the clusterjoin function is to accumulate the neighbor count in some cluster C for each base win-dow in C new , which is a critical and expensive operation. Note that if the average cluster size is m , the brute force join cost is O However, based on properties of the local clusters, we could utilize several rules to enhance efficiency as follows.

L EMMA 1. Given two local clusters C 1 and C 2 ,if distance C .pivot , C 2 .pivot )+ C 1 .radius + C 2 .radius &lt; d ,then window pair B i and B j , B i  X  C 1 and B j  X  X  2 , distance d .

L EMMA 2. Given two local clusters C 1 and C 2 ,if distance C .pivot , C 2 .pivot)-C 1 .radius  X  X  2 .radius &gt; d ,then dow pair B i and B j , B i  X  X  1 and B j  X  X  2 , distance ( B
L EMMA 3. Given a base window B and a local cluster C ,if distance ( B , C .pivot )+ C .radius &lt; d ,then  X  base window C , distance ( B , B i ) &lt;d .

L EMMA 4. Given two base windows B i  X  X  1 and B j  X  X  2 , consider base windows as points and local clusters as hyper-balls in high-dimensional space with their pivots as centroids, the per-pendicular bisector hyper-plane P of C 1 and C 2 ,if distance C .pivot , C 2 .pivot )  X  X  1 .radius  X  X  2 .radius &lt; d and distance P )+ distance ( B j , P ) &gt;d ,then distance ( B i , B j
The 4 lemmas allow us to devise pruning strategies that guar-antee the correctness in anomaly monitoring. Totally, there are cases for the spatial relationship between two local clusters, as de-picted in Figure 4(a) to Figure 4(e). In Case 1 all elements in cluster C are neighbors of all elements in the query cluster Q , and in this case, joining the two clusters requires only one distance computa-tion (between their pivots). In Case 2 no element in cluster the neighbor of any element in query cluster Q , and in this case, to join the two clusters requires only one distance computation, too. 3 (d) Case 4 (e) Case 5 In Case 3 where all elements in cluster C are neighbors of some elements in query cluster Q , and for those elements in Q distance computation is needed. In Case 4 some elements in clus-ter
C are neighbors of all elements in query cluster Q , and for those elements in C , only one distance computation is needed. In Case 5 some elements in cluster C are neighbors of some elements in query cluster Q , in this undesirable case, still some distance computations could be pruned by Lemma 4, which is illustrated in Figure 5.
With the above analysis, we can join clusters by checking which case the relationship belongs to, and then prune the distance com-putations. Note that situations of Cases 3, 4 and 5 are overlapping, so that some situation may belong to all the 3 cases. Therefore, we apply pruning in the order of Case 3, Case 4, then Case 5.
By our proposed batch monitoring scheme with local clustering and pruning rules, we can guarantee the correctness (in terms of Definition 3) of retrieved result set (no false dismissal). However by other clustering algorithms such as k -means, it will be difficult to guarantee this point. The difference is that local clustering re-quires not only the distances of base windows are close, but also the base windows in a cluster are consecutive. No existing solution considers this point. Good clustering is not our objective, but good pruning power in processing continuous anomaly monitoring is.
Now we analyze the cost of local clustering based batch moni-toring, where both the benefit and overhead of local clustering are considered. In our analysis, it is assumed that the sliding window is of sufficient length (this models very large databases ), so we could increase the neighbor counts only when Case 1 in Figure 4 is encountered, and ignore the other 4 cases. Although this may not be optimal, it is enough to theoretically show our superiority over simple pruning.

Let m be the average cluster size. We can think of the process of joining a query cluster with candidate local clusters as independent Bernoulli trials where we keep drawing candidates until k/ m successes appear or all candidates are exhausted. Hence, the num-ber of trials follows the Pas cal distribution. Assuming the average radius of clusters is r , we have the following analytic results: The-orem 2 gives the time cost per time tick, while Corollary 2 proposes the optimized setting for cluster size.
 be the probab ility that a r andomly drawn sample pivot lies within distance d to query cluster X  X  pivot x , and P a is the rate of anoma-bin or right bin of x , and N= w l + w r . Then the expected number of distance computations Y at the time tick t is upper bounded by:
P ROOF . The total expected cost of batch monitoring consists of 3 parts: the cost expectation on neighbor search for normal cases, the expected cost on search for clusters with anomalies, and the cost of building local clusters. We first analyze the expected dis-tance computations Z during cluster joins for false candidate clus-ter which does not contain any anomaly. The process will defi-nitely stop at some join when the candidate X  X  neighbor count ex-ceeds threshold k . Then, the following formula could be derived: Due to pruning by case 2, the cost for clusters with anomalies is
The cost of local clustering incurring on every data point should be included, which is: E( Y )=E( Z + C anomaly + C ) = E( Z )+E( C anomaly )+E( C ), thus In-equality 2 holds.
 for anomaly cases (which is a small constant for any algorithm), the cost of batch monitoring on every time tick could be optimized when: then, the upper bound of minimized cost min ( E ( Y obtained:
P ROOF . The cost in right side of Inequality 2 could be mini-mized when its partial derivative on m is 0 , thus, we let: Therefore, we could get: Applying Equation 3 to Inequality 2, Inequality 4 holds. Following Corollary 1 , we periodically reset m b = 3 4 k  X  = F  X  1 ( m b F ( r )) ,where F (distance distribution) and r are aggre-gated from the past stream. At initialization, we set m b  X  =d/8. Note that the initialization has little impact on the long term performance of continuous monitoring.

Once a cluster is formed, it will never be changed; therefore there is no computational maintenance cost, but only some memory cost to store the cluster data structure (defined in Section 5.1), which is given as follows, measured by memory units (4 bytes a unit). Since each local cluster structure stores 4 variables, the additional memory cost is Cost BM = 4 | W | m , which is very small, compared to the basic necessary memory | W | .

For trajectory streams with local continuity, attested by our ex-periments, r usually is tiny compared to d . Hence, F x ( F ( d ) . Thus the search cost of batch processing is nearly the cost of simple pruning divided by m 2 ,where m is the average size of local clusters. With ideal cluster size, the cost could be reduced to nearly 3
Through local cluster-based pruning, the distance computations will be largely reduced. However, most distance computations are spent on the normal cases since most base windows extracted from the trajectory streams are not anomalies. Hence, looking at the batch monitoring, if candidate clusters on W are joined in a perfect order that the query cluster first joins with candidate clusters which satisfy the condition of case 1 in Figure 4(a), the query could be proven to contain no anomalies quickly by very few distance com-putations (only one distance computation for candidate cluster C , while the query X  X  neighbor count increases by | C | ). If we examine Figure 4 again, case 3 or case 4 is more likely to give rise to more neighbors for the query cluster with less distance computations than case 5 . Hence we use to measure the goodness of a candidate cluster. Obviously the greater the utility is, the better the candidate is; and vice versa. Comparing case 3, 4, and 5 in the above, the distance between the pivot of a candidate cluster and pivot of a query cluster usually can indicate the utility: smaller dista nce corresponds to higher utility.
Therefore, our basic idea is to build index for pivots along current sliding window and then reschedule the cluster join order so as to approach the best search order. We propose a novel way to index trajectory stream by piecewise VP-trees (VP-tree: Vantage Point Tree [10]). Here VP-tree occupies much less memory than R -tree, because R -tree has to store bounding boxes in its nodes, but VP-tree X  X  nodes only need to store pivots. Actually, any tree index structures that do not store high dimensional vectors in their nodes could be employed here.

We build piecewise VP-trees, in particular a different VP-tree is built for an interval of the trajectory stream. In other words, each VP-tree is indexing a fixed number of consecutive pivots. The piecewise indexing method allows us to remove an entire VP-tree on data expiry, which is much more efficient than the use of a single index tree where the expired data need to be deleted one by one.
Figure 6 displays an overview of the structure of piecewise VP-trees over a trajectory stream: each VP-tree is to index pivots of a continuous period, and then pivots the whole sliding window are indexed by v VP-trees, from VP-tree 1 to Vp-tree v , except some boundary ones near the start or end of the sliding window. Next we introduce how the piecewise index structure works and consider the insert , delete ,and query operations on the piecewise index. Tree Insertion. When a new data point appends to the trajectory stream, the online local clustering procedure is triggered. When a new local cluster with its pivot P new is formed, it is immediately put into a list L new (as shown in Figure 6) for new coming pivots. The list L new is continuously updated until its size reaches the pre-defined number ( T N ) of pivots indexed by one VP-tree, at which point a new VP-tree is built from the pivots in L new and inserted into the VP-tree list L VP ,and L new is reset to empty. Note that once a VP-tree is built, the only function of it is to answer range queries targeting at the pivots which are indexed by it, and will never be updated all its life time.
 Tre e D e l e t i on . When some data point in the scope of a VP-tree phases out, the VP-tree is removed from L VP , all its pivots will be inserted into a list L old (as shown in Figure 6). Once a data point s phases out, the  X  X tart position X  of its pivot P old (also current oldest pivot) will be changed to t +1 .When P old  X  X  end position equals to its start position, P old will be removed from L old . Query Processing. When a new local cluster C new is formed, we can accumulate its neighbor counts in the left sliding windows with the assistance of VP-trees. Also when the entire right sliding win-dow of local cluster C old have arrived, we can count the total neigh-bors of base windows in C old to uncover anomalies. In either case, C new or C old becomes a query cluster Q . Given a query cluster Q with radius r Q ,let r max be the maximum radius of the current set of local clusters, for anomaly detection, a special case of range query with ( d + r max + r Q ) (see Definition 2 for d ) as the range is issued. When the number of neighbors of every base windows in the query cluster reaches k (see Definition 2 for k ), the query processing is stopped. This is called the stopping condition for Q (  X  q  X  Q , q  X  X  neighbor count is larger than k ).
When a batch search (range query) is issued, we first look for the query cluster X  X  spatially nearby clusters through the VP-tree list. However, the trick here is that we do not join the query cluster with selected candidate clusters immediately when they are found, for the reason that in worst case, the join cost for two local cluster pair of size m will require m 2 distance computations. Instead, we first enter candidate local clusters X  pivots into a candidate list, which is to be used by the upcoming query optimization steps.
When a new local cluster C new is constructed (or the whole right sliding window of C new is arriving), we search its spatially nearby local clusters in the VP-trees one by one until the stopping condi-tion is reached or all trees are searched. Once the candidate local cluster list is obtained after executing the query on one VP-tree, candidate clusters in the list are put into a minimum heap, where the ranking of clusters is based on the distances between candidate local clusters X  pivots and the query cluster X  X  pivot. The smaller is the distance, the higher layer the candidate is placed in the mini-mum heap. The original cluster join procedure is now reordered to join spatially nearest candidate first. Figure 7 demonstrates an ex-ample about how the rescheduling works on one VP-tree. For the query Q ,wefirstget 4 candidate pivots through the range search in VP-tree i , then the 4 candidates are put into the minimum heap H , the ranking of which is based on distance between candidates X  piv-ots and Q  X  X  pivot. Finally, Q keeps joining with the top element of H until the stopping condition is met or all candidates are searched.
The above strategy improves efficiency from both VP-trees X  prun-ing power and join rescheduling. Even without rescheduling, as-suming the average pruning power of the VP-trees is p ( 0  X  the search cost could be reduced to (1  X  p )  X  E ( Y ) (see E(Y) in Theorem 2 ). With rescheduling, the data processing speed will be even faster. Property 2 and 3 analyze the extra cost during index construction and extra requirements for memory.
 is the total number of pivots along current sliding window, n is the number of pivots indexed by one VP-tree, n l is the number of pivots indexed in one leaf node, m is average cluster size, the number of distance computations during tree construction incurred for every time tick is:
Obviously, in most cases, compared to the cost E ( Y ) (in Theorem 2 ), Cost build is small.
 Given m and n l as in Property 2 , | W | (the current sliding window length), we derive the extra memory cost for VP-trees:
Cost VP  X  1 4 Cost BM ( Cost BM is in Property 1 ), since normally n is much larger than 4 .

The tree size n vp and leaf size n l are dynamically tuned for dif-ferent workload streams.
In this section, we evaluate both the effectiveness and efficiency of our anomaly monitoring framework by extensive experimental studies. Both real world datasets (movement, spaceshuttle, ge, alti-tude, latitude, longitude) and synthetic dataset (random-walk) used. A Pentium IV 2.2GHz PC with 2GB RAM is used to con-duct all our experiments. In our implementation, we simulate the streaming manner by using a sliding window in the main memory. First we load a trajectory dataset into the main memory, and ini-tialize the sliding window X  X  start and end position. Then we keep sliding the sliding window. When the sliding window slides one point forward, the anomaly detection procedure is triggered, and when the procedure is completed, the sliding window moves one point forward again. For temporally overlapping anomalies, we only report the first discovered one which can represent the whole problematic region.
We test the usefulness of trajectory stream anomaly (as Defini-tion 3) on 2 annotated datasets  X  X ovement X  and  X  X paceshuttle X .  X  X paceshuttle X  contains a series of space shuttle marotta values.  X  X ovement" (MV) contains 12 days X  3D trajectories of one person on his way between home and office, and is annotated to indicate where are the really meaningful anomaly subsequences, including special events and localization errors. We measure the quality of reported anomaly base windows by F-measure [2] as follows: where D o is the set of annotated anomalies in a dataset, R set of  X  X nomaly" results reported by an algorithm (perhaps includ-ing false alarms). Here X Y={ w i | w i  X  X,  X  w j  X  Y, w laps with w j }. Note that we will explain detailed results based on  X  X ovement X  in Figure 8, while similar results on  X  X paceshuttle" could be derived from Figure 9.
We compare our definition of trajectory stream anomaly with 2 counterparts: burst [32] and discord [19], both of which could capture a portion of meaningful anomalies. Burst is a period on trajectory streams with aggregated sum (for m -D base window X, aggr(X)= m j =1 | w b i =1 X i | ) exceeding a threshold, while top-k dis-cords are the subsequences with the top-k largest distances to its nearest neighbor in archived time series. In the experiment for bursts, we vary the threshold (the metric is its proportion over the largest aggregated sum) and the base window length w b to investi-gate how F-measure is impacted, and the results are shown in Fig-ure 8(c). Similarly, by treating the dataset as an archived one, we search top-k discords under different base window length w Figure 8(d) shows F-measure of reported top-k discords for differ-ent ( k , w b ). From the results, we find that F-measure is always less than 0.15 for burst, and less than 0.82 for discord no matter how their parameters are tuned. From Figure 8(a) once the parame-ters for our definition fall in a rather loose range, the F-measures is nearly 100%. Figure 8(b) shows th at the precision of our definition is also good and robust. Besides results from  X  X ovement X  dataset, similar results have been observed on  X  X paceshuttle X  dataset, which
All the datasets and descriptions are available at: http://www.cse.ust.hk/  X  leichen/sigkdd09-repository/index.htm confirm that our definition is superior. The rationale is that burst misses spatial deviations and discord misses similar anomalies, but our definition covers them.
We investigate the impact of parameter setting on  X  X ovement X  dataset.
 Effect of k and d .Wevary k and d while let w b =64 and sliding win-dow be the whole dataset to investigate the effectiveness of our def-inition trajectory stream anomaly, the results of which are demon-strated in Figure 8(a). Here the metric of d is its proportion over the diameter of the dataset (the farthest distance between a base win-dow pair). It could be found that when k and d lie in a rather loose range ( 0 . 015  X  d  X  0 . 03 , 4  X  k  X  10 ), the F-measure is 100%. Moreover, the precision is 100% when k and d vary in a even larger range, as in Figure 8(b).
 Effect of Base Window Length w b . Figure 10(a) shows how F-measure changes with w b under different ( k , d ) configurations. Of course w b depends on the applications and the length of periods that people are interested in. Yet here we find that our anomaly detection results usually are not very sensitive to w b . Effect of Sliding Window Length | W | . In this experiment, we change the current sliding window length | W | , while let w b =64, to see how the F-measure is affected by | W | . From the results in Figure 10(b), we can conclude that no matter how ( k , d ) is configured, the larger |
W | is, the better the monitoring quality will be. Therefore, appli-cations should set the sliding window as long as possible. Suitable Range for All Parameters . From more experiments, we find that when k is 4  X  10 , d is 0 . 015 to 0 . 03 of diameter, and w is 32  X  256 , the F-measures are always high. For | W | , it should better be as large as the memory could hold.
Since our solutions aim at high speed and large scale moving object trajectory streams, which differ from other work where usu-ally small or non-streaming datasets are used, there are 6 very large trajectory datasets involved in the efficiency experiments: altitude (Al), latitude (La), longitude (Lo), random-walk (RW), ge (GE), and movement (MV).

We compare the costs of the 4 algorithms: SP (simple prun-ing) [3], DWT (discrete wavelet transform) [7], BM (our batch monitoring) and VR (our VP-tree based rescheduling), by the num-ber of distance computations they called and the total running time. Similarity search techniques like SVD [22], DWT [7], APCA [18], and Chebyshev Polynomials [6] could naturally be employed for our monitoring procedure. Since DWT has similar performance with those peers and is widely used, we choose DWT [7] as a repre-sentative to compare with our algorithms, where truncated wavelet coefficients for each candidate subsequence are maintained in order to obtain distance lower bounds. The number of distance computa-tions of DWT consists of the number of both non-pruned distance computations on subsequences and conversions of computations for lower bound distances on coefficients.

In these experiments, we let w b =128 and 256, vary k from 8 to 15, and change d from 0.015 to 0.025 diameter, and then record an average performance for the 4 competitive algorithms. Sliding window length | W | for each dataset is as follows: 244159 (Al), 73410 (La), 124442 (Lo), 111139 (RW), 55832 (GE), and 165122 (MV), and we slide forward the sliding window 650000 ticks for each dataset (loop sliding if the end of dataset is touched). The reduced dimensionality of DWT coefficients is 8 (for MV, it is 2 because larger dimensionality will lead to memory overflow!). We use very large sliding windows in order to simulate real applica-tions X  workloads.

From the results in Figure 11(a), (b), (c) and (d), we could see clearly that VR gets orders of magnitude improvement to SP (on average 179.87 times speedup) and beats DWT significantly (on average 31.64 times speedup). Thus, even if DWT could only use the first coefficient to achieve the same pruning power, VR is still much more efficient. With SP, the average processing time per data point arrival is 4 . 5  X  33 . 7 milliseconds, while with VR it is only 0 . 05  X  0 . 5 milliseconds! Sin ce all the 4 algorithms guarantee the correctness and get the same results, the efficiency improvement by our techniques has no quality regression, and consumes little extra memory (see Property 1 and 3). The weakness of VR is that it can-not get significant performance improvement on highly fluctuant time series, which is also attested by experiments.
In this paper, targeting typical scenarios of context-aware appli-cations, we extract a useful and general research problem: monitor-ing distance-based anomalies over moving object trajectory streams, and propose data structures and algorithms employing local clus-tering and piecewise VP-tree based rescheduling to efficiently con-duct such a task. The experimental results validate our solutions by showing both the usefulness of distance-based anomalies and or-ders of magnitude improvement in performance compared to sim-ple pruning approach. To the best of our knowledge, this is the first work that provides efficient support for continuous monitoring of distance-based anomalies over trajectory streams.
 ACKNOWLEDGEMENTS: The research of Y. Bu, A. Fu was supported in part by the RGC Earmarked Research Grant of HK-SAR CUHK 4120/05E, 4118/06E, and RGC Direct Allocation 07-08. The research of L. Chen is supported by the National Ba-sic Research Program of China (973 Program) under grant No. 2006CB303000, NSFC Project No. 60763001, and NSFC/RGC Joint Research Scheme N_HKUST602 /08. All opinions, findings, conclusions and recommendations in this paper are those of the au-thors and do not necessarily reflect the views of the funding agen-cies. Thanks for Sean X. Wang X  X  insightful comments.
