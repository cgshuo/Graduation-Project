 standard, they produce many competitive approaches for various prediction tasks. We focus here on the task of visual recognition in video - X  X oes this spatiotemporal window contain an object X ? In this domain, scanning-window templates trained with linear classification yield state of the art performance on many benchmark datasets [6, 10, 7].
 Bilinear models, introduced into the vision community by [23], provide an interesting generalization of linear models. Here, data points are modelled as the confluence of a pair of factors. Typical ex-amples include digits affected by style and content factors or faces affected by pose and illumination factors. Conditioned on one factor, the model is linear in the other. More generally, one can define multilinear models [25] that are linear in one factor conditioned on the others.
 Inspired by the success of bilinear models in data modeling, we introduce discriminative bilinear models for classification. We describe a method for training bilinear (multilinear) SVMs with bi-convex (multiconvex) programs. A function f : X  X  Y  X  R is called biconvex if f ( x,y ) is convex in y for fixed x  X  X and is convex in x for fixed y  X  Y . Such functions are well-studied in the optimization literature [1, 14]. While not convex, they admit efficient coordinate descent algo-rithms that solve a convex program at each step. We show bilinear SVM classifiers can be optimized with an off-the-shelf linear SVM solver. This is advantageous because we can leverage large-scale, highly-tuned solvers (we use [13]) to learn bilinear classifiers with tens of thousands of features with hundreds of millions of examples.
 While bilinear models are often motivated from the perspective of increasing the flexibility of a linear model, our motivation is reversed -we use them to reduce the number of parameters of a Figure 1: Many approaches for visual recognition employ linear classifiers on scanned windows. Here we illustrate windows processed into gradient-based features [6, 12]. We show an image window ( left ) and a visualization of the extracted HOG descriptor ( middle ), which itself is better represented as gradient features extracted from different orientation channels ( right ). Most learning formulations ignore this natural representation of visual data as matrices or tensors. Wolf et al. [26] show that one can produce more meaningful schemes for regularization and parameter reduction through low-rank approximations of a tensor model. Our contribution involves casting the resulting learning problem as a biconvex optimization. Such formulations can leverage off-the-shelf solvers in an efficient two-stage optimization. We also demonstrate that bilinear models have additional advantages for transfer learning and run-time efficiency. factorizing W into a product of low-rank factors. This parameter reduction can reduce over-fitting and improve run-time efficiency because fewer operations are needed to score an example. These are important considerations when training large-scale spatial or spatiotemporal template-classifiers. In our case, the state-of-the-art features we use to detect pedestrians are based on histograms of gradient (HOG) features [6] or spatio-temporal generalizations [7] as shown in Fig.1. The extracted feature set of both gradient and optical flow histogram is quite large, motivating the need for dimensionality reduction.
 Finally, by sharing factors across different classification problems, we introduce a novel formulation of transfer learning . We believe that transfer through shared factors is an important benefit of multilinear classifiers which can help ameliorate overfitting.
 We begin with a discussion of related work in Sec.2. We then explicitly define our bilinear classifier in Sec. 3. We illustrate several applications and motivations for the bilinear framework in Sec. 4. In Sec. 5, We describe extensions to our model for the multilinear and multiclass case. We provide several experiments on visual recognition in the video domain in Sec. 6, significantly improving on the state-of-the-art system for finding people in video sequences [7] both in performance and speed. We also illustrate our approach on the task of action recognition, showing that transfer learning can ameliorate the small-sample problem that plagues current benchmark datasets [18, 19]. Tenenbaum and Freeman [23] introduced bilinear models into the vision community to model data generated from multiple linear factors. Such methods have been extended to the multilinear set-ting, e.g. by [25], but such models were generally used as a factor analysis or density estimation technique. Recent work has explored extensions of tensor models to discriminant analysis [22, 27], while our work focuses on an efficient max-margin formulation of multilinear models.
 There is also a body of related work on learning low-rank matrices from the collaborative filter-ing literature [21, 17, 16]. Such approaches typically define a convex objective by replacing the Tr( W T W ) regularization term in our objective (6) with the trace norm Tr( seen as an alternate  X  X oft X  rank restriction on W that retains convexity. This is because the trace norm of a matrix is equivalent to the sum of its singular values rather than the number of nonzero eigenvalues (the rank) [3]. Such a formulation would be interesting to pursue in our scenario, but as [17, 16] note, the resulting SDP is difficult to solve. Our approach, though non-convex, leverages existing SVM solvers in the inner loop of a coordinate descent optimization that enforces a hard low-rank condition. Our bilinear-SVM formulation is closely related to the low-rank SVM formulation of [26]. Wolf et. al. convincingly argue that many forms of visual data are better modeled as matrices rather than vectors -an important motivation for our work (see Fig.1). They analyze the VC dimension of rank constrained linear classifiers and demonstrate an iterative weighting algorithm for approximately solving an SVM problem in which the rank of W acts as a regularizer. They also outline an algo-rithm similar to the one we propose here which has a hard constraint on the rank, but they include an additional orthogonality constraint on the columns of the factors that compose W . This requires cy-cling through each column separately during the optimization which is presumably slower and may introduce additional local minima. This in turn may explain why they did not present experimental results for their hard-rank formulation.
 Our work also stands apart from Wolf et. al. in our focus on the multi-task learning, which dates back at least to the work of Caruna [4]. Our formulation is most similar to that of Ando and Zhang [2]. They describe a procedure for learning linear prediction models for multiple tasks with the assumption that all models share a component living in a common low-dimensional subspace. While this formulation allows for sharing, it does not reduce the number of model parameters as does our approach of sharing factors. Linear predictors are of the form Existing formulations of linear classification typically treat x as a vector. We argue for many prob-lems, particularly in visual recognition, x is more naturally represented as a matrix or tensor. For example, many state-of-the-art window scanning approaches train a classifier defined over local feature vectors extracted over a spatial neighborhood. The Dalal and Triggs detector [6] is a partic-ularly popular pedestrian detector where x is naturally represented as a concatenation of histogram of gradient (HOG) feature vectors extracted from a spatial grid of n y  X  n x , where each local HOG descriptor is itself composed of n f features. In this case, it is natural to represent an example x matrix representation, fixing n f = 1 . This holds, for example, when learning templates defined on grayscale pixel values.
 We generalize (1) for a matrix X with where both X and W are n y  X  n x matrices. One advantage of the matrix representation is that it is more natural to regularize W and restrict the number of parameters. For example, one natural mechanism for reducing the degrees of freedom in a matrix is to reduce its rank. We show that one can obtain a biconvex objective function by enforcing a hard restriction on the rank. Specifically, we enforce the rank of W to be at most d  X  min( n y ,n x ) . This restriction can be implemented by writing This allows us to write the final predictor explicitly as the following bilinear function: 3.1 Learning Assume we are given a set of training data and label pairs { x n ,y n } . We would like to learn a model with low error on the training data. One successful approach is a support vector machine (SVM). We can rewrite the linear SVM formulation for w and x n with matrices W and X n using the trace operator.
 The above formulations are identical when w and x n are the vectorized elements of matrices W and X n . Note that (6) is convex. We wish to restrict the rank of W to be d . Plugging in W we obtain our biconvex objective function: In the next section, we show that optimizing (7) over one matrix holding the other fixed is a convex program -specifically, a QP equivalent to a standard SVM. This makes (7) biconvex. 3.2 Coordinate descent We can optimize (7) with a coordinate descent algorithm that solves for one set of parameters holding the other fixed. Each step in this descent is a convex optimization that can be solved with a standard SVM solver. Specifically, consider The above optimization is convex in W y but does not directly translate into the trace-based SVM formulation from (6). To do so, let us reparametrize W y as  X  W y : One can see that (9) is structurally equivalent to (6) and hence (5). Hence it can be solved with a standard off-the-shelf SVM solver. Given a solution, we can recover the original parameters by W small d . Using a similar derivation, one can show that min W x L ( W y ,W x ) is also equivalent to a standard convex SVM formulation. We outline here a number of motivations for the biconvex objective function defined above. 4.1 Regularization Bilinear models allow a natural way of restricting the number of parameters in a linear model. From this perspective, they are similar to approaches that apply PCA for dimensionality reduction prior factor of 4 without a loss in performance. Image windows are naturally represented as a 3D tensor X  X  R n y  X  n x  X  n f , where n f is the dimensionality of a HOG feature. Let us  X  X eshape X  X into a 2D matrix X  X  R n xy  X  n f where n xy = n x n y . We can restrict the rank of the corresponding model to d by defining W = W xy W T f . W xy  X  R n xy  X  d is equivalent to a vectorized spatial template defined over d features at each spatial location, while W f  X  R n f  X  d defines a set of d basis vectors biconvex formulation, the basis vectors are not constrained to be orthogonal, but they are learned discriminatively and jointly with the template W xy . We show in Sec. 6 this often significantly outperforms PCA-based dimensionality reduction. 4.2 Efficiency Scanning window classifiers are often implemented using convolutions [6, 12]. For example, the product Tr( W T X ) can be computed for all image windows X with n f convolutions. By restricting W to be W xy W T f , we project features into a d dimensional subspace spanned by W f , and com-pute the final score with d convolutions. One can further improve efficiency by using the same d -dimensional feature space for a large number of different object templates -this is precisely the basis of our transfer approach in Sec.4.3. This can result in significant savings in computation. For example, spatio-temporal templates for finding objects in video tend to have large n f since multiple features are extracted from each time-slice.
 Consider a rank-1 restriction of W x and W y . This corresponds to a separable filter W xy . Hence, our formulation can be used to learn separable scanning-window classifiers. Separable filters can be evaluated efficiently with two one-dimensional convolutions. This can result in significant savings because computing the score at the window is now O ( n x + n y ) rather than O ( n x n y ) . 4.3 Transfer problem 1  X  m  X  M . One can write all M learning problems with a single optimization:
L ( W 1 ,...,W M ) = As written, the problem above can be optimized over each W m independently. We can introduce a rank constraint on W m that induces a low-dimensional subspace projection of X m n . To transfer knowledge between the classification tasks, we require all tasks to use the same low-dimensional subspace projection by sharing the same feature matrix: Note that the leading dimension of W m xy can depend on m . This fact allows for X m n from different tasks to be of varying sizes. In our motivating application, we can learn a family of HOG templates of varying spatial dimension that share a common HOG feature subspace. The coordinate descent algorithm from Sec.3.2 naturally applies to the multi-task setting. Given a fixed W f , it is straightfor-ward to independently optimize W m xy by defining A = W T f W f . Given a fixed set of W m xy , a single matrix W f is learned for all classes by computing: If all problems share the same slack penalty ( C m = C ), the above can be optimized with an off-the-shelf SVM solver. In the general case, a minor modification is needed to allow for slack-rescaling [24].
 In practice, n f can be large for spatio-temporal features extracted from multiple temporal windows. The above formulation is convenient in that we can use data examples from many classification tasks to learn a good subspace for spatiotemporal features. 5.1 Multilinear In many cases, a data point x is more natural represented as a multidimensional matrix or a high-order tensor. For example, spatio-temporal templates are naturally represented as a 4 th -order tensor capturing the width, height, temporal extent, and the feature dimension of a spatio-temporal window. For ease of exposition let us assume the feature dimension is 1 and so we write a feature vector x as X  X  R n x  X  n y  X  n t . We denote the element of a tensor X as x ijk . Following [15], we define a scalar product of two tensors W and X as the sum of their element-wise products: With the above definition, we can generalize our trace-based objective function (6) to higher-order tensors: We wish to impose a rank restriction on the tensor W . The notion of rank for tensors of order greater than two is subtle -for example, there are alternate approaches for defining a high-order SVD [25, 15]. For our purposes, we follow [20] and define W as a rank d tensor by writing it as Combining (11) -(13), it is straightforward to show that L ( W y ,W x ,W t ) is convex in one matrix given the others. This means our coordinate descent algorithm from Sec.3.2 still applies. As an example, consider the case when d = 1 . This rank restriction forces the spatio-temporal template W to be separable in along the x , y , and t axes, allowing for window-scan scoring by three one-dimensional convolutions. This greatly increases run-time efficiency for spatio-temporal templates. 5.2 Bilinear structural SVMs We outline here an extension of our formalism to structural SVMs [24]. Structural SVMs learn models that predict a structured label y n given a data point x n . Given training data of the form { x n ,y n } , the learning problem is: above optimization problem is convex in w . As a concrete example, consider the task of learning a multiclass SVM for n c classes using the formalism of Crammer and Singer [5]. Here, where each w i  X  R n x can be interpreted as a classifier for class i . The corresponding  X  ( x,y ) will be a sparse vector with n x nonzero values at those indices associated with the y th class. It is natural to model the relevant vectors as matrices W,X n ,  X  X  that lie in R n c  X  n x . We can enforce W to be of rank d &lt; min( n c ,n x ) by defining W = W c W T x where W c  X  R n c  X  d and W x  X  R n x  X  d . For example, one may expect template classifiers that classify n c different human actions to reside in a d dimensional subspace. The resulting biconvex objective function is
L ( W c ,W x ) = Using our previous arguments, it is straightforward to show that the above objective is biconvex and that each step of the coordinate descent algorithm reduces to a standard structural SVM problem. We focus our experiments on the task of visual recognition using spatio-temporal templates. This problem domain has large feature sets obtained by histograms of gradients and histograms of optical flow computing from a frame pair. We illustrate our method on two challenging tasks using two benchmark datasets -detecting pedestrians in video sequences from the INRIA-Motion database [7] and classifying human actions in UCF-Sports dataset [18].
 We model features computed from frame pairs x as matrices X  X  R n xy  X  n f , where n xy = n x n y is the vectorized spatial template and n f is the dimensionality of our combined gradient and flow feature space. We use the histogram of gradient and flow feature set from [7]. Our bilinear model learns a classifier of the form W xy W T f where W xy  X  R n xy  X  d and W f  X  R n f  X  d . Typical values include n y = 14 , n x = 6 , n f = 84 , and d = 5 or 10 . 6.1 Spatiotemporal pedestrian detection Scoring a detector: Template classifiers are often scored using missed detections versus false-positives-per-window statistics. However, recent analysis suggests such measurements can be mis-leading [9]. We opt for the scoring criteria outlined by the widely-acknowledged PASCAL com-petition [10], which looks at average precision (AP) results obtained after running the detector on cluttered video sequences and suppressing overlapping detections.
 Baseline: We compare with the linear spatiotemporal-template classifier from [7]. The static-image detector counterpart is a well-known state-of-the-art system for finding pedestrians [6]. Surprisingly, when scoring AP for person detection in the INRIA-motion dataset, we find the spatiotemporal model performed worse than the static-image model. This is corroborated by personal communi-cation with the authors as well as Dalal X  X  thesis [8]. We found that aggressive SVM cutting-plane optimization algorithms [13] were needed for the spatiotemporal model to outperform the spatial model. This suggests our linear baseline is the true state-of-the-art system for finding people in video sequences. We also compare results with an additional rank-reduced baseline obtained by set-ting w f to the basis returned by a PCA projection of the feature space from n f to d dimensions. We use this PCA basis to initialize our coordinate descent algorithm when training our bilinear models. We show precision-recall curves in Fig.2. We refer the reader to the caption for a detailed analysis, but our bilinear optimization seems to produce the state-of-the-art system for finding people in video sequences, while being an order-of-magnitude faster than previous approaches. 6.2 Human action classification Action classification requires labeling a video sequence with one of n c action labels. We do this by training n c 1-vs-all action templates. Template detections from a video sequence are pooled together to output a final action label. We experimented with different voting schemes and found that a second-layer SVM classifier defined over the maximum score (over the entire video) for each template performed well. Our future plan is to integrate the video class directly into the training procedure using our bilinear structural SVM formulation.
 Action recognition datasets tend to be quite small and limited. For example, up until recently, the norm consisted of scripted activities on controlled, simplistic backgrounds. We focus our results on the relatively new UCF Sports Action dataset, consisting of non-scripted sequences of cluttered sports videos. Unfortunately, there has been few published results on this dataset, and the initial work [18] uses a slightly different set of classes than those which are available online. The published average class confusion is 69.2%, obtained with leave-one-out cross validation. Using 2-fold cross 64.8% (Fig. 3). Again, we see a large improvement over linear and PCA-based approaches. While not directly comparable, these results suggest our model is competitive with the state of the art. Transfer: We use the UCF dataset to evaluate transfer-learning in Fig.4. We consider a small-sample scenario when one has only two example video sequences of each action class. Under this scenario, we train one bilinear model in which the feature basis W f is optimized independently for each action class, and another where the basis is shared across all classes. The independently-trained model tends to overfit to the training data for multiple values of C , the slack penalty from (6). The joint model clearly outperforms the independently-trained models. We have introduced a generic framework for multilinear classifiers that are efficient to train with existing linear solvers. Multilinear classifiers exploit the natural matrix and/or tensor representation of spatiotemporal data. For example, this allows one to learn separable spatio-temporal templates for finding objects in video. Multilinear classifiers also allow for factors to be shared across clas-sification tasks, providing a novel form of transfer learning. In our future experiments, we wish to demonstrate transfer between domains such as pedestrian detection and action classification.
