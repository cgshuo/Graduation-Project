 Question answering communities such as Naver and Yahoo! An-swers have emerged as popular, and often effective, means of infor-mation seeking on the web. By posting questions for other partic-ipants to answer, information seekers can obtain specific answers to their questions. Users of popular portals such as Yahoo! An-swers already have submitted millions of questions and received hundreds of millions of answers from other participants. However, it may also take hours  X  X nd sometime days X  until a satisfactory an-swer is posted. In this paper we introduce the problem of predicting information seeker satisfaction in collaborative question answering communities, where we attempt to predict whether a question au-thor will be satisfied with the answers submitted by the commu-nity participants. We present a general prediction model, and de-velop a variety of content, structure, and community-focused fea-tures for this task. Our experimental results, obtained from a large-scale evaluation over thousands of real questions and user ratings, demonstrate the feasibility of modeling and predicting asker satis-faction. We complement our results with a thorough investigation of the interactions and information seeking patterns in question an-swering communities that correlate with information seeker satis-faction. Our models and predictions could be useful for a variety of applications such as user intent inference, answer ranking, interface design, and query suggestion and routing.
 H.3.3 [ I ]: NFORMATION STORAGE AND RETRIEVAL H.3.3: Information Search and Retrieval : Search process ; H.3.5: On-line Information Services : Web-based services Algorithms; Design; Experimentation; Evaluation Community question answering; information seeker satisfaction.
Community Question Answering (CQA) has recently become a viable method for seeking information online. In addition to using general-purpose web search engines, information seekers now have an option to post their questions (often complex and specific) on Community QA sites such as Naver or Yahoo! Answers, and have their questions answered by other users. These sites are growing rapidly. Hundreds of millions of answers have already been posted for millions of questions in just two years since Yahoo! Answers appeared. The site continues to grow rapidly. Understanding the reason for the growth, the characteristics of the information needs that are met by such communities, and the benefits and drawbacks of community QA over other means of finding information, are all crucial questions for understanding this phenomenon.

We pose one such fundamental question: can we predict if an asker in CQA will be satisfied with the answers proposed to her by the community? Our goal is to begin to unravel the many fac-tors that go into success of a CQA portal, and ultimately to apply our insights to better design of social media applications. In par-ticular, community question answering allows us to directly study search satisfaction from the information seeker perspective. This is in contrast to the more traditional relevance-based assessment that is often done by judges different from the original information seeker, which may result in ratings that do not agree with the tar-get user. While the idea of relevance being inherently subjective has been pointed out in the past (e.g., see references [29] and more recently [21]), nowhere does the problem of subjective relevance arise more prominently than within Community QA, where many of the questions are inherently subjective, complex, ill-formed, or often all of the above. The problem of complex and subjective QA has only recently started to be addressed in the question an-swering community, most recently as the first opinion QA track in TREC [7]. We review related work in more detail in Section 6, but in short, as far as we know, ours is the first large-scale study of real user satisfaction in complex and subjective information seeking.
In addition to studying asker satisfaction as a vehicle to expand our understanding of information seeking, there are significant prac-tical benefits to predict satisfaction in CQA. Potential applications include user intent inference, answer ranking, and query suggestion and routing. For example, we could notify the information seeker when an appropriate answer has been posted (which we call the  X  X ffline X  setting), or predicting at the time of posting whether the asker is likely to get a satisfactory answer to this question (the  X  X n-line X  setting). As we will show, human assessors have a difficult time predicting asker satisfaction, thereby requiring novel predic-tion techniques and evaluation methodology that we begin to de-velop in this paper. More specifically, our contributions include: We now introduce the asker satisfaction problem in more detail.
First we review the life of a question in a QA community. Then we frame the general problem and provide conceptual and empiri-cal motivation for studying asker satisfaction (Section 2.2). Finally, we formally state the asker satisfaction problem (Section 2.3). The process of posting and obtaining answers to a question in CQA is outlined in Figure 1. A user posts a question by selecting a category, and then enters the question subject (title) and, optionally, detail (description). For conciseness, we will refer to this user as the asker for the context of the question, even though the same user is likely to also answer other questions or participate in other roles for other questions. Note that to prevent abuse, the community rules typically forbid the asker from answering own questions or vote on answers. After a short delay (which may include checking for abuse, and other processing) the question appears in the respec-tive category list of open questions, normally listed from the most recent down. At the point, other users can answer the question, vote on other users X  answers, or comment on the question (e.g., to ask for clarification or provide other, non-answer feedback), or pro-vide various meta-data for the question (e.g., give questions stars for quality). Depending on the site, many more interactions may be available.

Over the lifetime of an open question the asker may be notified of the answers as they are submitted, or may check the collected answers periodically. If the asker is satisfied with any of the an-swers, she can choose it as best , and provide feedback ranging from assigning stars or rating for the best answer, and possibly textual feedback. At that point, the question is considered as closed by the asker , and no new answers are accepted. We believe that in such cases, the asker is likely satisfied with at least one of the responses, usually the one she chooses as the best answer.

But in many cases the asker never closes the answer personally, and instead, after some fixed period of time, the question is closed automatically . In this case, the  X  X est X  answer may be chosen by the voters, or by some other means (e.g., by automatically predicting answer quality following references [12] or [2]). So, while it is pos-sible that the best answer chosen automatically is of high quality, it is unknown if the asker X  X  information need was satisfied. There may be many reasons why the asker never closed a question by choosing a best answer. Based on our exploration we believe that the main reasons are either a) user loses interest in the information and b) none of the answers are satisfactory. In both cases, the QA community has  X  X ailed X  to provide satisfactory answers in a timely manner and  X  X ost X  the asker X  X  interest. While the true reasons are not known, for simplicity, to contrast with the  X  X atisfied X  outcome above, we consider this outcome to be  X  X nsatisfied X .
The problem of whether an asker in QA community is satisfied is a special instance of the general problem of predicting if an in-formation need of a searcher is satisfied. Question Answering com-munities are both an important application by itself, and also pro-vide unprecedented opportunity to study feedback from the asker herself. Furthermore, asker satisfaction plays crucial role in the growth or decay of a question answering community. If many of the askers in CQA are not satisfied with their experience, they will not post new questions and will rely on other means of finding in-formation. Furthermore, by modeling asker satisfaction, we could provide better ranking of questions, or notify an asker if they are likely to be satisfied with the answers to their questions. Hence, predicting, understanding and monitoring asker satisfaction is at the core of maintaining an active and healthy QA community.
It is important to note the differences of our task from traditional question answering and ad-hoc information retrieval: we want to predict what is essentially a subjective notion of satisfaction, which requires to model the intent of the asker, the expectation of what comprises a satisfying answer, and to some extent providing a  X  X ec-ommendation X  to the asker on the expected satisfaction with the answers. Furthermore, the information needs of askers in CQA are typically more complex and subjective compared to the traditional TREC benchmarks. Often, the intent of the asker is not obvious to either annotator or community participants, as we explore in Sec-tion 5. In summary, we believe that asker satisfaction, as studied in the context of CQA, can provide both new insights into informa-tion seeking behavior and spur the development of new techniques for user modeling and information finding. We now state more formally what we mean by asker satisfaction:
D EFINITION 1. An asker in a QA community is considered sat-isfied iff: the asker personally has closed the question, selected the best answer, and provided a rating of at least 3  X  X tars X  for the best answer quality. Otherwise, we define the asker to be unsatisfied .
We believe that this definition captures key aspects of asker satis-faction, namely that we can reliably identify when asker is satisfied but not the converse. Similarly, we do not attempt yet to analyze the distinction between possibly satisfied and completely unsatis-fied, or otherwise dissect the case where the asker is not satisfied. We can now state our problem more formally: The Asker Satisfaction Problem: Given a question submitted by an asker in CQA, predict whether the user will be satisfied with the answers contributed by the community.

There are two important special cases of this problem: the of-fline setting, where the posted question has already obtained some answers; and the online setting, where we attempt to predict imme-diately whether a user will be satisfied with the answers at some intermediate point in the process (e.g., while answers are still arriv-ing), or even before any answers arrive. We will attempt to solve the general version of this problem by adapting machine learning techniques, and, as our results in Section 5 show, our techniques are feasible for both the offline and online variants of the problem.
We now introduce our Asker Satisfaction Prediction system (ASP) that learns to classify whether the question asker is satisfied with the obtained answers. We use a standard classification framework for this task. Given a question thread posted by an asker, we de-rive features to represent the associated information (e.g., question text, text of the answers, user feedback) to predict whether the asker would be satisfied. Naturally, the features used are crucial (which we describe next). We then briefly describe the specific classifica-tion algorithms used for the experiments of Sections 5.
Our features are organized around the basic entities in a question answering community: questions, answers, question-answer pairs, users, and categories. We now review the features we used to rep-resent our problem. The complete list is reported in Table 1. Question: This group includes traditional question answering fea-tures such as the words and 2-word phrases in the question, the wh-type (e.g.,  X  X hat X  or  X  X here X ), and the length of the subject (title) and detail (description) of the question. As a more specific feature to communities we also include posting time, as well as any user feedback received for the question (e.g.,  X  X tars X  in Yahoo! An-swers community).
 Question-Answer Relationship: This group describes the rela-tionship between the question and the answer. We include stan-dard features such as overlap between question and answer, answer length, and number of candidate answers. We also use special-ized features such as the number of positive votes ( X  X humbs up X  in Yahoo! Answers), negative votes ( X  X humbs down X ), and various vote-related statistics such as the maximum of positive or nega-tive votes received for any one answer (e.g., to detect cases of bril-liant/popular answers or, conversely, blatant abuse).
 Asker User History: This group is unique to question answering communities, and particularly important for our task. Since user satisfaction is, to a large extent, subjective, we posit that it relies largely on past user activity history  X  in particular, how the asker was satisfied with responses to previous questions. Care was taken not to  X  X heat X   X  only information available about the asker prior to posting the question was used.
 Answerer User History: Similarly to the Asker User History , we develop features to describe the history of the users providing the answers, such as the number of questions resolved, number of an-swers provided, and number of answers rated as best . Since a ques-tion may draw multiple answers, we include three  X  X urrogate X  an-swerer features: the average of the answerer history, the features for the answerer with the highest CQA reputation score, and the answerer that attracted the most positive votes for this question. Category Features: We hypothesized that user behavior (and asker satisfaction) varies by topical question category, as recently shown in reference [2]. Therefore we model the prior of asker satisfaction for the category, such as the average asker rating (satisfaction) with answers contributed to all previous questions in the category. Textual Features: Additionally, we derive word n-gram (unigram and bigram) features from the text of the question, and the text of the answers (separate features spaces are used to represent the question and answer terms). As a simple feature selection method, only the most frequent 1000 features are included.
We explored three families of classification algorithms: Sup-port Vector Machines (SVM), Decision trees, Boosting and Naive Bayes, all using the implementations in the Weka [28] framework. Decision Trees : We use two implementations of the decision tree [19]: C4.5 and RandomForest. A benefit of decision tree is interpretabil-ity of the models and results. By using a decision tree classifier, we expect to get high precision on the target class, with the potential drawback of overfitting. To account for this, we use random forests as well as feature selection.
 SVM : Support vector machines are considered the classifier of choice for many tasks, due to robustness in the presence of noise, and high reported accuracy. Specifically, we use the Weka implementation of SMO [18] Boosting : Additionally, we use meta-learning as an alternative to SVM for the noisy features (and labels) in our domain. AdaBoost [10] has been shown quite effective for many text-classification applica-tions, and we apply the Weka implementation of AdaBoost. Naive Bayes : Last, we use Naive Bayes classifier, which is a very simple and fast, yet often surprisingly effective method to quickly investigate the success of our approach.
 The methods above are representative of the state-of-the-art in clas-sification, so we expect the experimental results described in Sec-tion 5 to be generalizable to other variants of classification algo-rithms.
We now describe the metrics used for the evaluation, the datasets, and methods compared in the experimental results of Section 5.
Even though ours is formally a two-class classification problem, we primarily focus on the satisfied or positive class. The reason for this is that we have higher certainty about the true positive like-lihood of our satisfied labels compared to the unsatisfied  X  more properly to be stated as unknown cases. Specifically, we measure tory(AH), and Category (CA). Textual features are not listed. the Precision , Recall , and F1 for the satisfied class, and, where ap-propriate, the overall Accuracy for both classes.
In the experiments that follow we will primarily focus on pre-dicting the satisfied class, hence we will rely more on the Precision, Recall, and F1 rather than the overall Accuracy.
Our problem is inherently subjective. Hence, as the gold stan-dard we use the asker rating for the best answer (if chosen) as a measure of satisfaction. Note that in many cases askers do not even
Table 2: Ratings for 130 questions (54 satisfied/76 unsatisfied) bother to choose the best answer, indicating a degree of dissatisfac-tion that we plan to quantify in future work. For this study, how-ever, we simply consider the asker ratings as the  X  X ruth X , interpreted as defined in Section 2.3.

To complement the asker ratings we also obtained human judge-ments from Amazon X  X  paid rater service, the Mechanical Turk The raters are provided a  X  X IT X  (Human Intelligence Task), and for a small fee the workers submit their responses. For our task we obtained five independent ratings for each question, and used a ma-jority to identify and resolve ambiguous cases. In total, 130 ques-tions were manually rated by Mechanical Turk workers. Finally, we obtained a number of  X  X xpert X  ratings  X  provided by researchers to calibrate the asker satisfaction and the Mechanical Turk (hence-forth, MTurk) ratings. Interestingly, as we will show in Section 5, MTurk ratings have higher correlation with the asker satisfaction than the (more strict) expert ratings. The rated dataset is summa-rized in Table 2. http://www.mturk.com/mturk/welcome
Our data is based on a snapshot of Yahoo! Answers ( http: //answers.yahoo.com ), a popular CQA site, crawled in the early 2008. The initial broad categories to start the crawl were  X  X ealth X ,  X  X ducation &amp; Reference X ,  X  X ports X ,  X  X cience and Math-ematics X  and the  X  X rts X . The resulting snapshot is our universe of 216,170 questions, listed in Table 3.
 Table 3: Statistics of the complete data crawled from the Ya-hoo! Answers site.

In order to focus on a realistic asker satisfaction prediction task (that is, reflective of the current state of Yahoo! Answers), we se-lected a random subset of 5,000 questions from the most recent 10,000 questions in the snapshot above. We will use this sample of 5,000 questions for all of the experiments. To allow other re-searchers to replicate our results, all the datasets used in this paper are available online 2 .

The details of our dataset are reported in Table 4. The total of 90 categories are represented, and we report detailed statistics for the top 10 most frequent categories. As we can see, questions in these categories comprise almost 51% of all questions in the dataset (this skewed distribution is representative of our complete crawl snap-shot). In particular, the Mathematics category is the most popular, containing 13% of the questions and drawing on 3.6 answers for each question on average. Interestingly, Chemistry , while also a popular category, draws only about 2 answers per question, while Football (American) attracts more than 11 answers for each ques-tion. The asker satisfaction varies widely with the category. While more than 70% of askers are satisfied with the answers provided in the Mental Health category, only 34% are satisfied with the answers contributed for Biology questions, and similar low satis-faction holds for other sciences. Not surprisingly, questions that are closed by the asker are usually closed within a day (and often within 1 hour). Also, when the asker closes the question person-ally, the asker rating is usually high, averaging 4.3  X  X tars X  out of 5 possible, with low variance across categories. However, when a question is closed by community voters, the average number of votes awarded to the best answer varies widely by category. For ex-ample, Voters in the Chemistry category on average award only 1.2 votes to the best answer (despite the high popularity of the Chem-istry category). In contrast, voters in the Diet &amp; Fitness category on average award about 4.5 votes to the best answer, which indicates higher overall satisfaction of the community with the contributed answers. In summary, asker satisfaction and other statistics of the questions vary widely by the topical category, and the correspond-ing user community, supporting our decision to develop a number of category-normalized features (Section 3).
We now describe the baselines and our specific methods for pre-dicting asker satisfaction. Note that the  X  X ruth X  ratings are provided by the asker and hence are difficult to predict even for human raters. The predictions we compare include: http://ir.mathcs.emory.edu/shared/sigir2008 Rater group Precision Recall F1 Accuracy Expert (strict) 0.36 0.68 0.47 0.45 Casual (majority=3/5) 0.43 1.0 0.60 0.47 Casual (majority=4/5) 0.44 1.0 0.61 0.48 Casual (majority=5/5) 0.41 0.75 0.53 0.46 Table 5: Comparing casual human raters (Mechanical Turk Workers) with expert raters (130 randomly sampled questions)
We now turn to the experimental evaluation of the asker satisfac-tion prediction methods.
First, we present some intuitions into the problem itself. In Sec-tion 5.1 we report the main classification results of the paper, which we subsequently will study in depth in the remainder of the section. In particular, we show that our ASP system is able to take advantage of the context (i.e., asker user history) to make better predictions than human raters. We conclude this section with feature analysis and analysis of the results (Section 5.2).

Before we present our experiments, it is important to understand the difficulty of the problem of predicting asker satisfaction (Ta-ble 5). For example, the ratings of expert judges at best had weak correlation with asker satisfaction, and with the most favorable thresholding only achieved the precision of 0.36 and recall of 0.68 when trying to predict satisfaction. Similarly, Mechanical Turk workers (whom we call  X  X asual labelers X ), had better success with precision of 0.44 and recall of 1 (i.e., they were overly optimistic about satisfaction). Interestingly, the best precision and recall were achieved not where all the raters agreed, but rather when at least 4 out of 5 raters predicted asker satisfaction. Based on these results, we will use the Mechanical Turk ratings as the strongest manual baseline, using the majority threshold of 4 for all subsequent ex-periments.
Table 6 reports prediction accuracy for the different implementa-tions of ASP, in particular comparing the choice in classifier algo-rithm and feature sets (namely, whether to use the textual features, and whether to use feature selection). Surprisingly, ASP_C4.5 re-sults in the best performance of all the classification variants, with F1 on the satisfied class of 0.77 when selecting only the top 15 fea-tures, chosen by Information Gain. In contrast, the human raters only achiever the F1 of 0.61, which is in fact lower than the naive baseline that always guesses the  X  X atisfied X  class, and lower than the heuristic baseline that achieves the best F1 of 0.64. Feature Selection: The top 15 features selected are reported in Ta-ble 7. Note that all the four asker history features are included. Interestingly, the most salient feature is the previous rating by the asker (when available). We can view it as the prior on the asker which may relate to the self-selecting nature of CQA (i.e., askers who recently were successful return to submit new questions). Sim-ilarly, the amount of experience with CQA (the  X  X ember since X  features) is an important factor. Another interesting result is the presence of several category features, which confirms our intuition about the importance of the category as the prior on question sat-isfaction independent of the asker. Also note that the reputation of the answerers submitting the responses is not as important as many other features, suggesting that authority or expertise of answer con-tributors is only important for some, but not all, information needs. Table 6: Accuracy of ASP_SVM, ASP_C4.5, ASP_RandomForest, ASP_Boosting, and ASP_NB for varying parameters (5-fold cross validation). Table 7: Top 15 features with Highest Information Gain (IG)
We next report the precision, recall, and F1 for varying training set sizes in Figure 2. We report the average of three experiments, each with a randomly chosen test set of 1,000 questions, held fixed for varying amounts of training data. Our ASP system outperforms all other predictors, including human raters. In particular, 2,000 questions in training is sufficient to achieve F1 of 0.75, and addi-tional training data is not as helpful, nevertheless improving perfor-mance of ASP to achieve F1 of 0.77, substantially outperforming all other methods. In fact, as few as 500 training questions are suf-ficient to achieve F1 of 0.7, which may be practical enough even for the less popular question categories. Online vs. Offline Prediction: Previously, we discussed results of predicting satisfaction in the off-line setting  X  that is, after some an-swers have been contributed, allowing us to exploit features such as the number of answers, answer content length, and feedback from other users (votes). We now consider a more difficult task of pre-dicting asker satisfaction in the online setting  X  that is, before any answers to the question are contributed. Table 8 reports the com-parison between off-line and on-line settings. As we can see, there is a noticeable degradation in accuracy (0.74 F1 online vs. 0.77 F1 off-line), nevertheless that performance is significantly higher than the various baselines  X  suggesting that ASP is practical even for on-line prediction.
 Feature Ablation: To gain a better understanding of the impor-tant features for this domain, we perform ablation study on our feature set. For this, remove each of the feature categories listed in Section 3.1. Table 9 reports the accuracy of ASP with each of the feature categories removed. Without question features or asker user history, the prediction F1 score drops drastically. In contrast, Question-Answer relationship, and answerer user history, appear to have less of an effect  X  or perhaps are redundant given the presence of the other feature categories. Nevertheless, it should be noted that, surprisingly, answerer reputation does not appear to be im-portant for asker satisfaction. We conjecture that this is due to in-creasingly subjective nature of many questions in CQA, where the accuracy of the provided answer is less important than other, more subjective characteristics of the answer, e.g., whether the answer appears as caring or supportive for Health-related questions. Textual Features: We also explore which textual features are most helpful for this task, using the Information Gain metric. From Table 10, it appears that most of the textual features suggest the
Table 10: Textual features with high Information Gain (IG) predominance of subjective questions, which may in fact correlate with asker satisfaction (and requires further investigation). Asker Satisfaction Varying with Past Experience: The impor-tance of previous asker history features suggests that prediction ac-curacy should vary significantly with the amount of history avail-able for the asker. To explore this hypothesis we test our model on groups of askers with varying number of previous questions posted. For this experiment, we train our ASP_C4.5 system as described before, but instead of averaging accuracy over all the questions in the test set, we compute Precision, Recall, and F1 separately for each group of askers. In particular, we group together questions from askers with just 1 question (that is, no prior questions posted), 2 questions (i.e., only 1 previous question posted), etc. The results are reported in Figure 3. Not surprisingly, the accuracy of pre-diction increases dramatically for askers with at least one previous question, reaching F1 of 0.9 for askers with at least three previous questions resolved in the past. This suggests the benefits of person-alizing prediction models, as we are exploring in our current work.
Community Question Answering sites, such as Yahoo! Answers and Naver, have been gaining increasing popularity among many online users. Unlike in automatic question answering, the goal is not to develop a better algorithm for retrieving and extracting an-swers, but instead to enable the exchange of high-quality, relevant information between community participants. Finding such qual-ity information, where in QA communities quality varies signifi-Figure 3: Satisfaction prediction accuracy for groups of askers with varying number of posted questions, and the correspond-ing number of questions posted by askers in each group. cantly [23], provides a unique challenge, which recently has been addressed in references [2], [13], and [12].

Community question answering builds on the rich history in au-tomatic question answering [25] and web question answering [5]. However, a significant difference includes the large amount of meta-data available to find relevant and high-quality content [2]. Ad-ditionally, while previous work focused on how to retrieve high quality answers from the CQA content, the question of information seeker satisfaction was not explored. In contrast, we present a gen-eral prediction model to investigate the ability of a QA community to provide satisfactory answers from the asker X  X  perspective.
Our work is related to, but distinct from interactive Question An-swering [7]. In particular, we can directly study the satisfaction from information seeker perspective. Nowhere does the problem of subjective relevance arise more prominently than in community QA, where many of the questions are inherently subjective, com-plex, ill-formed, or all of the above. While automatic complex QA has been an active area of research, ranging from simple modifica-tion to factoid QA technique (e.g., [22]) to knowledge intensive ap-proaches for specific domains (e.g., [8]), the technology does not yet exist to automatically answer open domain complex and sub-jective question. A corresponding problem is complex QA eval-uation. Recent efforts at automatic evaluation show that even for well-defined, objective, complex questions, evaluation is extremely labor-intensive and has many challenges [16, 17]. The problem of subjective QA has only recently started to be addressed in the ques-tion answering community, most recently as the first opinion QA track in TREC [7]. We believe that this work can contribute to both the understanding of complex QA satisfaction, and explores impor-tant evaluation issues in a new setting. To our knowledge, this paper is the first large-scale study of real user satisfaction with obtaining information for complex and/or subjective information needs.
There is a rich tradition of relevance-based assessment of IR and QA (see [24]) for an overview). While the idea of relevance be-ing inherently subjective has been pointed out by many researchers (e.g., see references [29] and more recently [21]), we note that in community QA a large fraction of the questions are subjective, compounding the problem of both relevance assessment (which is no longer meaningful). Information seeker satisfaction has been studied in ad-hoc IR context in [11] (refer to [15] for an overview), but studies have been limited by lack of realistic user feedback on whole-result satisfaction and instead worked primarily within the Cranfield evaluation model.

Our work is also related to user modeling for web search, where the goal is to predict which results will be relevant (e.g., [1, 27, 26, 9]); other uses include classifying user intent into a particu-lar category (e.g., [20])). This work builds on the influential user model introduced by Belkin et al. [3, 4]. Recently, eye tracking has started to emerge as a useful technology for understanding some of the mechanisms behind user behavior (e.g., [14, 6], which may provide additional insight into user satisfaction with web search results. In contrast, we deal with complex information needs and community-provided answers (with explicit, noisy,  X  X elevance X  rat-ings from other users). Furthermore, we deal with subjective rat-ings provided by users themselves, instead of other assessors.
In order to predict asker satisfaction, we exploit standard classi-fication techniques. Many models and techniques have been pro-posed for classification problem, including support vector machines, decision tree based techniques [19] and boosting-based techniques [10]. We use these techniques to build our prediction models by using Weka [28], a popular library of machine learning methods. In particular, we use the Weka X  X  implementation of SMO [18], Ad-aBoost [10], and an implementation of the C4.5 decision tree [19].
In this paper we presented, to our knowledge, the first attempt to quantify and predict asker satisfaction in question answering communities. We introduced and formalized the new problem of asker satisfaction prediction, and explored state-of-the-art classifi-cation techniques to implement our models. We have shown the importance of asker history to this highly personal, difficult, and subjective task, and demonstrated that our system can outperform human assessors who do not benefit from knowing the prior asker history. Our work opens a promising direction towards modeling user intent, expectations, and satisfaction, and can potentially re-sult in practical improvements to the quality of question answering communities.
 ACKNOWLEDGEMENTS : We thank the Yahoo! Answers team for allowing us extended use of the API, the Emory College Seed Fund for partially supporting this research, and the anonymous re-viewers for valuable suggestions. [1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning [2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and [3] N. Belkin, R. N. Oddy, and H. M. Brooks. Information [4] N. J. Belkin. User modeling in information retrieval. Tutorial [5] E. Brill, S. Dumais, and M. Banko. An analysis of the [6] E. Cutrell and Z. Guan. Eye tracking in MSN Search: [7] H. T. Dang, D. Kelly, and J. Lin. Overview of the TREC [8] D. Demner-Fushman and J. Lin. Answering clinical [9] D. Downey, S. T. Dumais, and E. Horvitz. Models of [10] Y. Freund and R. Schapire. Experiments with a new boosting [11] S. P. Harter and C. A. Hert. Evaluation of information [12] J. Jeon, W. Croft, and J. Lee. Finding similar questions in [13] J. Jeon, W. Croft, J. Lee, and S. Park. A framework to predict [14] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, [15] M. Kobayashi and K. Takeda. Information retrieval on the [16] J. Lin and D. Demner-Fushman. Methods for automatically [17] J. Lin and P. Zhang. Deconstructing nuggets: the stability [18] J. C. Platt. Fast training of support vector machines using [19] J. Quinlan. Improved use of continuous attributes in c4.5. In [20] D. E. Rose and D. Levinson. Understanding user goals in [21] I. Ruthven, L. A. Glasgow, M. Baillie, R. Bierig, E. Nicol, [22] R. Soricut and E. Brill. Automatic question answering: [23] Q. Su, D. Pavlov, J. Chow, and W. Baker. Internet-scale [24] E. M. Voorhees. The philosophy of information retrieval [25] E. M. Voorhees. Overview of the TREC 2003 question [26] R. White, M. Bilenko, and S. Cucerzan. Studying the use of [27] R. W. White and S. M. Drucker. Investigating behavioral [28] I. Witten and E. Frank. Data Mining: Practical machine [29] J. Zobel. How reliable are the results of large-scale
