 A relational data among m objects and n objects is a bipartite network on a set of m vertices and another set of n vertices, which describes the relationships among objects in social, physical, and other phenomena. Equivalently, a relational data is represented by a matrix with m rows and n columns. For example, POS data is a relational data between customer s and items, and a friend list of a social network service (SNS) such as the Facebook is a relational data among users.
With the emergence of such large amounts of relational data, there has been an increase in the interest in methods that can efficiently discover hidden interac-tion patterns among objects from given relational data. For example, enterprises involved in e-commerce and SNS might want to know about the following rela-tionships:  X  Which type of items does a customer purchase using e-commerce?  X  Which other users are in a relationship with a SNS user?  X  To which user does another user re-tweet when communicating on Twitter? Clustering methods are among the most effective approaches to obtain answers to such questions, and several methods have been proposed so far [5,3,4,16]. The Infi-nite Relational Model (IRM) [11] is a well-known and important generative model that represents processes for generating relational data. Co-clustering based on the model can produce a proper set of clusters that summarizes the relationships among objects. Moreover, the number of cl usters is automatically estimated from the input data, even when the cluster structure and its size are unknown.
However, the IRM might fail to detect unknown structures when the data has a large amount of noise or the model can describe only a part of the data. Owing to the use of infinite clustering based on the Dirichlet Process (DP) [6], the IRM works to some extent, but it finds many small clusters to adapt itself to contradicting data. In fact, the prob lem of the co-clustering of real-world datasets is often difficult, because the data are noisy or sparse. For example, a spam blog that leaves comments randomly on other blogs has too many links. Such a noisy blog makes it difficult to analyze the relationship among blogs. Moreover, an inactive blog, which the author is not eager to write, has very few links. Such an insignificant blog also becomes an obstacle in finding important clusters. As we show later in Section 5, co-clustering with the IRM on such ill-formed data finds ineffective clusters.

To handle these ill-formed data, we incorporated a subset selection mechanism into the IRM and proposed a new relational model. In our model, the relevance of each object is parameterized by an indivi dual Bernoulli parameter. The relevance indicates the degree of confidence with which an object forms informative relations coming from the latent cluster structure. For example, for POS data, an active customer tends to generate relevant relations with many items, as done by a well-known item as well. Their relevance becomes comparatively high in our model. Then, either a relevant relation or an irrelevant relation is generated stochastically for pair-wise objects according to the inte raction of their relevance parameters.
Our contributions in this paper are summarized as follows:  X  We proposed a new generative model, which is an extension of the IRM  X  We derived posterior probabilities for running the Collapsed Gibbs Sampling  X  We performed experiments on synthetic and real-world datasets. The experi-Therefore, the proposed method is effect ive in analyzing noisy relational data. 1.1 Related Works Hoff et al. [8] discussed an ill-formed problem with clustering vector data. They introduced a background distribution tha t describes irrelevant elements within the vector data, so that their model can find cluster robustly against noise based on a relevant subset of the data.

Ishiguro et al. [10] extended the IRM with a similar idea. They introduced switch variables to indicate whether an object is relevant for cluster analysis, or is an irrelevant troublesome one. In their model, only relationships among rele-vant objects are analyzed. That is, thei r model is an object-wise subset model. However, in some cases, it would not be pr eferable to select subset of objects for clustering target. For example, when we utilize co-clustering results for rec-ommendation, we want to suggest the nearest cluster for any object. In our new model, the clustering target is s elected in a relation-wise manner. In this section, we first define the relational data discussed in this paper. Then, we discuss the IRM, a generative model for co-clustering relational data. relational data between T 1 and T 2 as R : T 1  X  T 2  X  X  0 , 1 } .If R ( i,j ) = 1(0), we say that there is a link ( non-link ) between O 1 i and O 2 j 1 . For a purchase dataset, T 1 and T 2 are the sets of customers and item s, respectively. We can represent customer i  X  X  purchase of item j by R ( i,j ) = 1, while R ( i,j ) = 0 indicates that customer i have not bought item j . The co-clustering problem on relational data basedongivendata R ,where C 1 = { 1 , 2 ,  X  X  X  ,K } and C 2 = { 1 , 2 ,  X  X  X  ,L } are the sets of cluster indices for T 1 and T 2 , respectively.

The IRM proposed by Kemp et al. [11] is a generative model for relational data that can co-cluster objects based on the similarities of the relationships among the objects. In the IRM, the Dirichlet Process (DP) [6] is used as a prior distribution for the number of clusters. The DP is a nonparametric stochastic process that can be viewed as an infinite-dimensional Dirichlet distribution, and can generate any-dimensional multinomial distributions. Therefore, the IRM can adaptively estimate the number of clusters for the observed data. The generative model of the IRM is described as follows: where CRP(  X  ) is the Chinese Restaurant Process (CRP) [2], which is one of the well-known constructive algorithms of DP; Beta(  X  ,  X  ) is the beta distribution; and Bernoulli(  X  ) is the Bernoulli distribution, respectively. Figure 1a shows the IRM graphically.

We will briefly review the above process. First, the cluster assignments z 1 i and z 2 j are given by CRPs (Eq. (1)), where  X  1 and  X  2 are the concentration parameters of the DP that controls the number of clusters to be generated. We denote the cluster assignments for all objects other than object i as z 1  X  i .When z i is given, the conditional probability P ( z to the cluster k  X  by CRP is given as follows: where m 1  X  i,k  X  is the number of objects other than object i that are assigned to the cluster k  X  . As Eq. (4) shows, the assignment z 1 i basically depends on the probability proportional to the number m 1  X  i,k  X  of objects that belong to each cluster. However, new clusters are generated at the rate  X  1 . Assume that K  X  L clusters ( C 1 = { 1 , 2 ,  X  X  X  ,K } and C 2 = { 1 , 2 ,  X  X  X  ,L } ) have been generated for T 1  X  T 2 . Then, from Eq. (2), a Bernoulli parameter  X  ( k,l ) is given according to the beta prior for each pair of clusters C 1  X  C 2 . The parameter  X  ( k,l ) indicates the intensity of the relationshi p between an object in the cluster k andanobject in the cluster l . Finally, the relation R ( i,j ) is generated from the corresponding Bernoulli trial (Eq. (3)). In this section, we presen t our new model, called the Relevance-Dependent Infi-nite Relational Model (rdIRM).

In real-world relationships, whether each relation is intentionally generated depends on the objects related to the relation. In the case of a purchase, a customer who knows about a large number of items will have a certain opinion about whether he needs these items. As a result, this customer will generate very important relations that are relevant to decide his cluster assignment. In contrast, a customer who knows only about a few items will have vague opinions. Thus, relations that are generated by th is customer would be irrelevant. That is, such an irrelevant relation should not affect the co-clustering. For the items, it is reasonable to consider that similar properties exist in terms of popularity.
To model the above situation, for each object O 1 i and O 2 j , we introduce rele-vance parameters  X  1 i , X  2 j  X  [0 , 1] that indicate the degree of confidence to generate the relevant relations. Then, we consid er a generative mechanism in which each relation R ( i,j ) between objects is generated fr om a mixture of the distribution inherent in a cluster  X  ( k,l ) (foreground distribution) and the distribution com-mon to the entire data  X  0 (background distribution). We can construct such a mechanism as follows: where f (  X  ,  X  ) is an arbitrary Boolean function that returns 1 or 0. The above mechanism enables us to embed a relevance-dependent subset selection into the relational model: only the informative (relevant) relations are generated from the foreground distribution  X  ( k,l ), and the background distribution  X  0 describes the non-informative (irrelevant) part of the relational data. For example, when f is a When f is a logical product, the mixture rate becomes  X  1 i  X   X  2 j . The other logical functions work similarly.

To summarize, the generative process for the rdIRM is defined as follows: Figure. 1b graphically represents this model.

Now, we will briefly explain the rdIRM process. First, the cluster assignments z 1 and z 2 are given as in the original IRM, (Eq. (5)). Second, the foreground distribution  X  ( k,l ) and the background distribution  X  0 are independently given from a beta prior (Eq. (6)). Third, the relevances  X  1 i and  X  2 j for O 1 i and O 2 j , respectively, are given from beta priors (Eq. (7)). Fourth, the two switches r 1 i  X  j and r 2 j  X  i are given by a Bernoulli trial with corresponding relevances (Eq. (8)). Fifth, either the foreground  X  ( k,l ) or the background  X  0 is selected by the in-R ( i,j ) is generated from the selected probability (Eq. (10)).
The difference between our rdIRM and the original IRM is that we modeled a generative process of noisy relationship s by introducing objects X  relevances and their interaction mechanism. That is, our rdIRM can co-cluster relational data based on a subset of relations that are relevant to underlying cluster structures.
When f is a logical sum, a relevant relati on can be generated when at least one of the related objects O 1 i or O 2 j has high relevance. This models situations in which the relevant relationship between objects can be generated by a one-sided request, such as sending an e-mail or following a hyperlink on the Internet. When f is a logical product, the relevant relat ion is generated only when the objects cooperate with each other. This models situations in which an object that wants to have a relevant relation with anoth er can be constrained from doing so. Of course, we can employ other logical functions for other interaction models. We use the Collapsed Gibbs Sampler [12] to infer the parameters of the rdIRM 2 . Given r i,j , the relational data R are separated into a foreground part and a can be integrated out. Therefore, the inference of the rdIRM is performed by sampling the assignments z 1 , z 2 and the switches r 1 , r 2 one after the other. In this section, we only show the derived posteriors for running the Gibbs sampling below, because of the space limitation. 4.1 Sampling Cluster Assignments z 1 ,z 2 Because z 2 j can be sampled in the same way as z 1 i , we concentrate on z 1 i .Wecan assume that the switch variables r ( r 1 and r 2 ) have already been given before takingasampleof z 1 i , so that the cluster assignments are influenced only by the foreground part of the observations. Therefore, the conditional posterior for z i = k Here, we use B (  X  ,  X  ) to denote the beta function. Symbols m r ( m r )denotethe numbers of links (non-links) in the foreground part of the observation, and are computed as follows:
Note that if r i,j = 1 for all ( i,j ), Eq. (11) is equivalent to the original IRM X  X  sampler. 4.2 Sampling Switch Variables r 1 i  X  j ,r 2 j  X  i As the sampling of r 2 j  X  i is done in the same way as the sampling of r 1 i  X  j ,we concentrate on r 1 i  X  j .Given z 1 and z 2 , we have a finite number K  X  L of clusters. Thus, the conditional posterior for r 1 i  X  j is derived as follows: that are related to object i without r 1 i  X  j . The terms on the right-hand side of Eq. (12) are computed as follows: where m  X  ( i,j ) r and m  X  ( i,j ) r denote the numbers of links and non-links, respec-denote the numbers of links and non-links, respectively, such that z 1 s = k , z 2 t = l and r s,t = 1 for all pairs ( s,t ) =( i,j ); and n  X  ( i,j ) r 1 Specifically, these counts are computed as follows: In this section, we present our experiment al results. To clarify the effectiveness of our subset selection mechanism, the performance of our rdIRM is compared with that of the original IRM. Through all the experiments, we assumed that the priors of all the binary variables in the generative models were uniform (Beta(1 . 0 , 1 . 0)). In addition, we estimated the concentration parameters  X  1 , X  2 for the DPs assuming Gamma priors by sampling method presented in [8]. 5.1 Experiments on Synthetic Datasets We prepared 12 synthetic datasets. Firs t, in accordance with the generative model of our rdIRM, we created five synthetic datasets, Data1(0 . 0), Data1(0 . 2), Data1(0 . 5), Data1(0 . 8), and Data1(1 . 0), where the numbers in parentheses in-dicate the background link probabilities  X  0 for the datasets. We set the logical function f for the rdIRM to be a logical sum. The cluster assignments z 1 and z 2 were independently generated from fixed-dimensional multinomial distributions. The parameter values used for g enerating the datasets were N 1 = N 2 = 200,  X  =(0 . 5 , 0 . 5), and  X  1 =  X  2 =(4 . 0 , 3 . 0); the number of clusters were set as K =4 also created five synthetic datasets in a similar manner (from Data2(0 . 0) to Data2(1 . 0)), except that we set the logical function f to be a logical product and we set both  X  1 and  X  2 to be (4 . 0 , 2 . 0). Finally, we created two datasets without background influences, (Data1(NULL) and Data2(NULL)). We applied the logical sum version of the rdIRM to Data1 and the logical product version to Data2.
 We used three measures to evaluate c lustering perform ance. One was the Adjusted Rand Index (ARI) [9], which is widely used for computing the similarity between true and estimated clustering results. The ARI takes a value in the range 0.0  X  1.0, and takes a value of 1.0 when a clustering result is completely equivalent to the ground truth. Another was the number of erroneous estimated clusters (EC). We computed the average of these measures for the two sets T 1 and T 2 . The rest was the test data log likelihood (TDLL), which indicates the predictive robustness of a generative model; we hid 1.0% of the observation during inference (keeping it small so that the latent cluster structure did not change), and measured the averaged log likelihood such that a hidden entry would take the actual value. A larger value is better, and a smaller one means that the model overfits the data. Finally , we repeated the experiment 10 times for each dataset using different random seeds to find an overall average. Table 1 lists the computed measures. I n the case of every dataset, except Data1(NULL) and Data2(NULL), we confirmed that the rdIRM outperformed the IRM. In particular, the rdIRM maintained good performance for sparse (  X  0  X  0 . 0) or dense (  X  0  X  1 . 0) data. We also list in Table 2 the maximum a posteriori (MAP) estimations of the background probability  X   X  0 and the estimated ratios of the foreground for synthetic datasets, except Data1(NULL) and Data2(NULL). The ground truths of the foreground ratios (FRs) are 0 . 8197 for Data1 and 0 . 4622 for Data2. As the table shows, the rdIRM performs well in estimating the ground truths. 5.2 Experiments with Real-World Datasets We applied the rdIRM to two real-world datasets. One was the  X  X ovieLens X  dataset 3 , which contains a large number of user ratings of movies on a five-point scale. In our experiment, we created a bina ry relational dataset with a threshold that yields R ( i,j ) = 1 for ratings higher than 3 points and R ( i,j ) = 0 for all other ratings. That is, a relational value R ( i,j ) = 1 indicates that user i likes movie j . There are a total of 943 users and 1,682 movies in the dataset, and 3.5% of the relations are links. The other dataset was the  X  X nimal-feature X  dataset [14], which includes relations between 50 animals and 85 features. Each feature is rated on a scale of 0 X 100 for each animal. We prepared the binary data with a threshold that yields R ( i,j ) = 1 for all ratings higher than the average of the entire set of ratings (20.79). That is, we used the relational value R ( i,j )=1 ( R ( i,j ) = 0) to indicate that animal i has (does not have) feature j .Inthis dataset, 36.8% of the relations are links.

We used a logical sum version of the rdIRM for the MovieLens dataset and a logical product version for the animal-feature dataset. Our reason to use the former was that a user can watch any mov ie according to his or her preference, and similarly, movies are usually promoted independent of the users. Therefore, it seemed natural that the foreground (relevant relations) for the MovieLens dataset should be generated as per either the user X  X  relevance  X  1 i or the movie X  X  relevance  X  2 j . On the other hand, animal features are acquired through evolution based on the specific type of animal. For example, aquatic features such as  X  X wims X  or  X  X ater X  cannot be acquired by t errestrial animals. Therefore, the type of animal limits the features that it can acquire, and conversely, the type of feature limits the types of animals that are related to that feature. Therefore, we used the logical product version of the rdIRM for the animal-feature dataset.
Figure 2 shows the clustering results and the computed TDLL for these real-world datasets. Figure 3 shows color maps for the estimated foreground proba-bilities  X   X  ( k,l ). The background probabilities  X  0 that the rdIRM estimated were 0 . 0000 for the MovieLens dataset and 0 . 0036 for the animal-feature dataset. It can be seen that the original IRM organized many non-informative cluster-blocks, because the IRM considered that all the relations were relevant for cluster analysis. In contrast, the rdIRM found more vivid cluster structures owing to the use of our subset selection mechanism, which selects an informative subset of relations via the interaction of the o bjects X  relevances. The computed TDLLs show that the rdIRM predicts hidden entries more robustly than does the orig-inal IRM for both datasets.

The left side of Table 3 lists the examples of the movie clusters produced by the rdIRM for the MovieLens dataset. In the columns for the number of links means that we can regard the relevances as an indication of the popularity of the movies within the cluster. On the other hand, the original IRM treats all the links and non-links as relevant, so that the differences of the popularity of movies popularity affect the cluster assignment. The right side of Table 3 lists the examples of the feature clusters obtained by the rdIRM for the animal-feature dataset. As with the results for the MovieLens dataset, we can see that the estimated  X  2 j tends to increase with the number of links. One interesting result produced by the rdIRM is that representative features such as  X  X wims, X   X  X ater, X   X  X aws, X   X  X estspot X  and  X  X eet X  were found to have high relevance in their clusters. From these results, we c an say that the relevances estimated by the rdIRM indicate the popularities or representativeness of the objects. Con-sequently, the rdIRM finds clusters in terms of major categories by introducing the relevance-dependent subset selection mechanism. In this paper, we proposed a new probabilistic relational model called the Relevance-Dependent Infinite Relational Model (rdIRM), which is suitable for noisy relational data analysis. The rdIRM parameterizes objects X  relevances and incorporates a relevance-dependent subset selection mechanism, so that the rdIRM can estimate objects X  relevances, a nd can co-cluster noisy relational data selecting only relevant relations that are informative for co-cluster analysis.
Our experiments with synthetic datasets confirmed that the rdIRM can find proper clusters in a noisy relational data, especially, in sparse or dense data. Moreover, our experiments on real-world datasets confirmed that the clusters obtained by the rdIRM represent major categories and that the estimated rele-vances can be viewed as the popularity or representativeness of the objects.
Our future research plans include extending the rdIRM so that it can also estimate the logical function f , which was given statically in this paper. We are also interested in applying our relevan ce-based subset selection mechanism to more advanced relational models, such as the mixed (or multiple) membership models [1,13], the hierarchical structure models [15], and the time-varying models [7].

