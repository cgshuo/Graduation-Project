 This paper presents Latent Association Analysis (LAA), a generative model that analyzes the topics within two doc-ument sets simultaneously, as well as the correlations be-tween the two topic structures, by considering the semantic associations among document pairs. LAA defines a correla-tion factor that represents the connection between two doc-uments, and considers the topic proportion of paired doc-uments based on this factor. Words in the documents are assumed to be randomly generated by particular topic as-signments and topic-to-word probability distributions. The paper also presents a new ranking algorithm, based on LAA, that can be used to retrieve target documents that are poten-tially associated with a given source document. The ranking algorithm uses the latent factor in LAA to rank target docu-ments by the strength of their semantic associations with the source document. We evaluate the LAA algorithm with real datasets, specifically, the IT-Change and the IT-Solution document sets from the IBM IT service environment and the Symptom-Treatment document sets from Google Health. Experimental results demonstrate that the LAA algorithm significantly outperforms existing algorithms.
 Source Code http://www.uweb.ucsb.edu/~miao/resources.html I.7.0 [ Computing Methodologies ]: Document and Text Processing Topic Model, Variational Inference, Ranking Algorithm
The numerous and diverse documents generated in busi-ness and society present both challenges and opportunities for data mining research. Among the common, yet relatively unexplored, types of documents are the documents that oc-cur in pairs. Examples of such document pairs include ques-tions and answers, changes to IT systems and consequent problems, disease symptoms and diagnoses, etc . Such doc-ument pairs can be used to build valuable knowledge bases that help improve decisions in business and society. Table 1: Example Change Problem Document Pairs.
 Table 1 shows example document pairs from the IBM IT Change document sets that contain changes to IT systems (source documents) and the resulting problems (target doc-uments). Given such document pairs, we seek to address two fundamental problems: 1. What is the underlying principle that makes the con-2. Given a source document, how do we use this principle
The solutions to the Modeling and Ranking problems can help us understand the semantic connection ( i.e. , latent as-sociation ) between paired documents and can provide tremen-dous value in real-world applications. For instance, in the IT service industry, changes are frequently made to an op-erational IT environment, and the service consultants need to evaluate the potential problems caused by a proposed change, so that they can make plans accordingly.
Both the modeling and the ranking problems present great challenges that cannot be readily addressed using existing approaches. For instance, topic models, such as CTM [4], LDA [5] and PLSI [12], are designed to model only single document sets. However, we need not only to model individ-ual documents correctly, but also to capture the connection between the documents accurately. Furthermore, the exis-tence of one-to-many or many-to-one mappings in a bipartite graph suggests possibly different interpretations of the top-ics of a document. For example, a question might refer to different topics if the answers emphasize different aspects of the question. What we need is a model that puts a docu-ment in the context of a document pair and allows its topic proportion to be interpreted differently in different contexts. Existing topic models do not support flexible topic propor-tions in the same document. The ranking problem is also non-trivial. Given a source document, the number of po-tentially related target documents can be huge. The model needs to be able to identify an appropriate target document from a large number of candidates accurately.
 In this paper, we present our novel Latent Association Analysis (LAA), which models the topic structures and their correlations together. In the LAA model, each document pair is considered as a randomly drawn correlation factor that initiates the connection between the two documents. The topic proportions of the two documents are drawn con-ditionally depending on the correlation factor. Each word in the documents is assumed to be generated based on a topic assignment and the topic-to-word probability distribution.
For LAA, we adopt concepts from two well-known mod-els, namely, Correlated Topic Model (CTM) [4] and Canon-ical Correlation Analysis (CCA) [2]. We then develop a novel ranking algorithm to retrieve target documents based on their latent associations with the given source document. We evaluate the ranking algorithm using the IT-Change and the IT-Solution document sets from the IBM IT service en-vironment and the Symptom-Treatment document sets from Google Health. Experimental results show that the LAA al-gorithm significantly outperforms existing algorithms, which confirms that LAA successfully captures the semantic-level connections among document pairs.

To the best of our knowledge, this work is the first such work to model paired document sets within a unified frame-work. Our contributions are two-fold. First, we show that exploring document-level correlations is more effective at capturing semantic topic associations in document pairs, compared to using word-level or topic-level correlations. LAA considers a document pair as a whole and, thus, can deliver better association semantics than existing approaches. Sec-ond, the ranking algorithm based on LAA performs excep-tionally well for target document retrieval. Through diverse experiments, we show that this ranking algorithm has broad applications in document analysis and retrieval. The problem we address involves a source document set D s and a target document set D t . Each source document d  X  X  s is paired with at least one target document d t  X  X  t and vice versa. The pairing between source and target doc-ument sets can be represented by a bipartite graph G ,with its two sets of vertices being the source document set and the target document set, and its set of edges corresponding to the source and target document pairs. Specifically,
In the example in Table 1, there is a one-to-one mapping between the source and target documents. However, one-to-many or many-to-one mappings are not uncommon in other paired document sets. In this study, we consider the other mappings as special cases of one-to-one mappings and convert them to multiple one-to-one document pairs.
Given the above formulation, we aim to solve two prob-lems: (1) Modeling : Model the associations between the source documents in D s and the target documents in D t ;(2) Ranking : For a new source document d s ,rankandretrieve the target document d t , that is most likely to be associated with d s , from a repository of target documents.
The objective of our modeling problem is different from that of existing work [4, 5, 12]. Our focus is to model the association between a pair of documents. The task is also different from traditional information retrieval tasks: (1) Our query involves a document, which is much noisier than a keyword query in traditional information retrieval tasks; (2) The source (query) document and the target documents to be retrieved arise from two separate document sets, for which we do not assume any vocabulary overlap. Therefore, similarity-based relevance scores do not apply to this prob-lem. Conceptually, the association between the source and target documents can be considered at three different levels of granularity, yielding three possible solutions: Figure 1: Analyzing the Associations at Different Levels of Granularity.

Word-level correlation (Fig. 1(a)): Given individ-ual words in the source documents, we can directly model whether and how they are correlated with the words in the target documents using a training dataset. Unfortunately, synonyms and polysemy in free text make the correlation at the word level noisy.

Topic-level correlation (Fig. 1(b)): Topics are more stable than words. Topic-level correlation can be analyzed in two ways: 1) learn two topic structures from the two docu-ment sets separately and then discover the mapping between the two topic spaces; 2) assume that the two document sets share the same topic space and analyze the common topics. A problem with the first approach is that topics learned sep-arately might not reflect the associations in document pairs. A problem with the second approach is that the common topic space won X  X  be able to capture the different semantic associations between pairs of documents.

Document-level correlation (Fig. 1(c)): Instead of generating topics separately, we can learn the topics for the source and target documents simultaneously. We de-fine a correlation factor for a document pair. The topic proportions of the two documents are drawn based on this correlation factor. In this approach, the topic distribution of each (source or target) document is studied in the con-text of a document pair. This approach allows flexible topic assignment if the same source document is paired with dif-ferent target documents, and vice versa. Thus, the same source document can have different topic assignments in different contexts. Hence, this approach handles the one-to-many or many-to-one mappings between two document sets smoothly, because mappings involving many documents are broken down into multiple document pairs that can be considered separately.
The Latent Association Analysis (LAA) framework takes the document-level correlation approach. As shown in Fig. 2, the LAA framework consists of two components, the corre-lation factor y between two latent variables x s and x t ,and the document-generation processes for d s and d t .Wecan instantiate LAA with different correlation models and topic models. Moreover, the models for generating source and tar-get documents can differ. After learning based on training document pairs, LAA can be directly applied to solve our ranking problem: for a given query d s , we can rank pairs ( d s ,d t ) based on not only the topics of d s and d t , but also the correlation factor between them.
In this section, we consider an instantiation of the LAA framework with the Canonical Correlation Analysis (CCA) [2] and the Correlated Topic Model (CTM) [4], and derive a variational method [3] to estimate the parameters for the model.
Canonical Correlation Analysis [15] works on two sets of random variables and their covariance matrix. Two linear transformations are found for the two sets of random vari-ables such that the two sets of projected variables have max-imum correlation with each other. Bach et al. [2] gave a probabilistic interpretation of CCA and considered CCA as a model-based method that could be integrated with other probabilistic methods.

In CCA, the observed random variables x 1  X  R m 1 and x 2  X  R m 2 depend on a latent correlation factor y  X  R d .The generative process can be described as follows.
In LAA, we can use CCA to capture the semantic associa-tion between the source document and the target document. The two random variables x s and x t are lower-dimensional representations of the source and target documents, respec-tively. The correlation factor y represents why these two documents are associated on a semantic level.
Whereas CCA can capture the semantic association in a document pair, other existing topic models can capture the topics of the two documents. These existing topic models in-clude CTM [4], LDA [5], PLSI [12], etc .IfPLSIisused,the random variables x s and x t are the topic proportions of the documents d s and d t . If LDA is used, the random variables x s and x t are the Dirichlet priors of the topic proportions in d s and d t . In both cases, the topics are assumed to be independent of each other; and the correlation between dif-ferent topics cannot be properly modeled. If CTM is used, the topic proportion documents are assumed to have a mul-tivariate Gaussian prior distribution, which is a natural fit with the Gaussian variables x s and x t in CCA. The correla-tion between different topics can be explicitly captured by the covariance matrix of the Gaussian distribution. In this paper, we choose CTM to instantiate LAA.
 The instantiated LAA model is depicted in Fig. 3. The LAA model comprises the model parameters in the set M = {
 X  s ,T s , X  s ,  X  t ,T t , X  t , X  s , X  t } . For source and target docu-ments d s and d t of lengths l s and l t ,thewords w s, 1: w 1: l t in the source and target documents are the observable variables. The latent variables ( i.e. , variables that are nei-ther directly observable nor explicitly specified in the learned model) form the parameter set V l = { y,x s ,x t ,z s, 1:
The generative process can be described as follows: 1. For each edge in the bipartite graph G ( i.e. ,adocu-2. For each document pair connected by an edge, draw 3. For each word in the source document, choose:
Although the topic modeling portion of LAA stems from the idea of CTM, LAA is more complicated than the ex-isting topic models: It is built on a set of document pairs, instead of a single document set as in existing topic models. As a result, the latent topic structures in the source and the target document sets, as well as their correlation, need to be analyzed simultaneously. LAA considers each edge in the bipartite graph as a correlation factor that initiates the connection between two documents. The generation process of the topic proportions depends on the correlation factor. That is, first, LAA decides what makes the connection be-tween the source documents and the target documents at the document level. Then, LAA models the pair consisting of the source document and the target document as a co-occurrence interpreted by the correlation factor, instead of assuming a causal relationship between the two documents, which is difficult to validate.

It is worth noting that the topic proportion of a document is context-dependent. The same piece of text, in the eyes of different observers with different emphases, can belong to different topics. In LAA, each source or target document is put in the context of a pair, allowing the topic proportion of each document to be mutually enhanced and to be context-dependent. Doing so provides the flexibility of not deciding the topic of a document until we learn what is emphasized in the other document paired with it.
Given the LAA model described above, we solve the fol-lowing problems: (1) Model fitting: Given a set of document pairs, how do we find model parameters that best fit the data? (2) Inference: For a new document pair, how do we decide the correlation factor y , the topic proportions x and the topic assignments z for each word? The true poste-rior distributions are computationally intractable, when the hidden variables are not independent of each other, given an observed document pair. Similar to CTM, our LAA model employs a variational method to solve these problems.
Consider a pair ( d s ,d t ) of documents, represented as sets { w sn } and { w tn } of words, where w sn is the n th word in y  X  d s and w tn is the n th word in d t , Eq. (1) gives the prob-ability that the document pair arises from an LAA model represented by a parameter set M .

P ( d s ,d t | M )=
Ideally, the latent variables in the set V l should be cho-sen to maximize the probability P ( d s ,d t | M )tobestfitthe pair of documents. Unfortunately, it is computationally in-tractable to determine the true posterior distribution over V , because the latent variables are coupled together. Thus, we introduce a variational distribution Q ( V l ), in which the latent variables are independent of each other, to approxi-mate the true posterior distribution P ( V l | d s ,d t ). The graph-ical representation of Q is shown in Fig. 4. According to the variational distribution, Q ( y )  X  X  (  X  y,  X ), Q ( x si ) Q ( x ti )  X  X  (  X  x ti , X  2 ti ), Q ( z sn )  X  Mult(  X  sn Mult(  X  tn ). Note that each component in the topic propor-tions x s and x t is drawn independently. The variational parameters introduced in the variational distribution are fit such that the KL-divergence between Q ( V l )and P ( V l | is minimized.

Using the variational distribution and Jensen X  X  inequal-ity, we take the logarithm of the probability in Eq. (1) and rewrite the objective function in Eq. (2). Instead of max-imizing the log likelihood directly, which is intractable, we maximize the lower bound of the log likelihood to obtain an approximation of the optimal value of the latent variables. log( P ( d s ,d t | M ))  X  E Q log( P ( d s ,d t | M ))+ H ( Q )=
The above maximization problem is a convex optimization problem and, thus, the optimal values of the variational pa-rameters occur when the derivatives are zero. According to the decomposition of the marginal probability in Eq. (1), we expand the lower bound of the log likelihood as follows: L =
Each term on the right-hand side is a function over the variational parameters as shown in Eqs. (4) -(8):
Here, a represents the source document s or the target document t in a pair. Because a document pair is symmetric, we use the same set of equations with different subscripts.
According to LAA, the topic assignment z is drawn based on the Gaussian prior x , P ( z n = k | x )= exp ( x k )  X  = j exp( x j ). If we take the first-order Taylor expansion with respect to  X  at point  X  to approximate log P ( z n = k we have log P ( z n = k | x )= x k  X  log(  X  )  X  1  X  ( j exp( x  X  )+ O ((  X   X   X  ) 2 ). Thus, where  X  is an additional variational parameter.

H ( Q )=  X 
We substitute Eqs. (4)-(8) into Eq. (3), and then maximize the lower bound of the log likelihood by taking the partial derivatives with respect to each of the variational parameters and setting them to zero.

For the variational parameters  X  ,  X  , X and y ,theoptimal values that maximize the objective function are achieved by:
For the variational parameters  X  x and  X  , there are no an-alytical solutions. The optimal values of these variables are the solutions to Eqs. (13) and (14), which are solved itera-tively using Newton X  X  method.
For each edge in the bipartite graph, we calculate the vari-ational parameters using Eqs. (9) -(14) iteratively until the log likelihood lower bound in Eq. (3) no longer increases. The resulting variational parameter values are an approxi-mation of the optimal values of the latent variables. Specif-ically, y  X  =  X  y , x  X  ak =  X  x ak , z  X  an =arg k max(  X  a  X  X  s, t } , k  X  X  1 , 2 , ..., K a } , n  X  X  1 , 2 , ...l a
We estimate the model parameters using the variational expectation-maximization algorithm. In the E-Step, we up-date the variational parameters for each edge in the bipartite graph. In the M-Step, we update the model parameters, so that the sum of the log likelihood lower bound on each edge is maximized.

The process used in the M-Step is similar to that of vari-ational inference. The goal here is to maximize the aggre-gated log likelihood of all the edges in the bipartite graph, rather than maximizing the log likelihood of a single edge. We sum the lower bounds of the log likelihood in Eq. (2) for each edge and take the partial derivative over the set M of model parameters. We then calculate the optimal values of the model parameters by setting the derivatives to zero.
The E-Step and the M-Step are performed iteratively until the model parameters converge, indicating that the model parameters fit the training dataset.
GivenanLAAmodel M learned from a training dataset, for a new source document d s , we aim to rank the target documents in a test dataset according to their potential as-sociations with the source document. In this section, we introduce three different approaches to this problem. In Sec-tion 6, we evaluate these three approaches, together with the PTM method proposed by Zhang et al. [25].
First, we discuss the Two-Step approach that mines the topics in the target and source document sets independently and then determines the correlation between their topic struc-tures. This method is used as the baseline for comparison with LAA.

The training process consists of two steps: (1) Find the topics in the source and target document sets; (2) Find the correlation between the source and target topic structures. In the first step, CTM is independently applied to the two document sets D s and D t . The topic proportion priors x and x t are obtained for D s and D t , respectively, using the variational inference method presented in [4]. For each docu-ment pair ( d s ,d t ), their corresponding topic proportion pri-ors ( x s ,x t ) form a pair. In the second step, these topic proportion priors (which follow Gaussian distributions) are fed into CCA. The CCA parameters T 1 , T 2 ,  X  1 ,  X  2 , X  are fit to the topic proportion pairs ( x s ,x t ).
In the document retrieval task, given a new source docu-ment d s , our goal is to rank the target documents in a test set. The candidates d t are ranked based on the probability P ( d t | d s ) that a target document d t canbeobservedina document pair containing the source document d s .
We assume that the topic proportion priors x are a lower dimensional representation of the document d .Thus, P ( d  X 
P ( x t | x s ). In CCA, given x 1 , the latent correlation fac-tor y follows a normal distribution: y | x 1  X  X  ( M T 1 U  X  ) ,I  X  M 1 M T 1 ) [2], and given y , x 2 follows a normal dis-tribution: x 2 | y  X  X  ( T 2 y +  X  2 ,  X  2 ). Thus, given the topic proportion prior x s of a source document d s , its correspond-ing document d t has a topic proportion prior x t that follows a normal distribution: x t | x s  X  X  ( T 1 M T ( x s  X   X  1 )+  X  2 ,  X  2 + T 1 ( I  X  where M =( P l ) 1 / 2 and P l is the diagonal matrix of the top l canonical correlations.

Given a source document d s and a candidate target doc-ument d t , their topic proportion priors x s and x t can be inferred using CTM. Thus, the target documents can be ranked using P ( x t | x s ), calculated from Eq. (19).
The LAA model presented in Section 4 allows us to pre-dict, for a new source document d s , which target document d is more likely to be associated with d s .Adirectwayof ranking target documents is to evaluate how likely a hypo-thetical document pair ( d s ,d t ) arises from the underlying LAA model. The lower bound log( P ( d s ,d t | M )) can be es-timated by Eq. (3) using the variational inference method discussed in Section 4.3.1. Thus, we can use the function R ( d s ,d t )= log( P ( d s ,d t | M ) to rank the target documents. Because both the source and the target documents are con-sidered to be a bag of interchangeable words in the LAA model, the generation probability of a long document is less than the generation probability of a short document. Note that, in this prediction method, the rank of a document pair is inversely proportional to the document length. To avoid unfairly penalizing long documents, we normalize all of the documents to unit length.
Although the above LAA direct approach is intuitive, it has some drawbacks. In ranking document pairs, the most important factor should be the semantic association between the source and target documents, whereas the exact wording of a document in expressing its semantic meaning should not be overemphasized. However, when evaluating a document pair using the probability that the document pair arises from the LAA model, the LAA direct approach considers all the words in the source and target documents to be equally im-portant. As a consequence, if a target document contains rare words, it will have a low rank. The reason is that, even if the rare words in the target document might associate perfectly with the source document semantically, the prob-ability of generating such words is still very small, which brings down the rank of the target document. Moreover, in our ranking method, the value of the correlation factor should not matter, as long as it interprets the semantic as-sociation in a document pair. The LAA direct approach cannot accommodate this feature either.

To address the aforementioned problems, we developed the LAA latent approach based on the semantic association between source and target documents. In this approach, only the topic association information is used to rank the document pairs. For any given source document d s and can-didate target document d t , first we use variational inference to calculate the most probable correlation factor y  X  =  X  y , variational distribution. Then, we evaluate how likely there exists an association between the two documents based on the topic proportion, and use the following ranking function to rank the target documents.
 In Eq. (20), P ( x s | y  X  )  X  X  ( T s y  X  +  X  s ,  X  s ), and P ( x N ( T t y  X  +  X  t ,  X  t ).
We trained the LAA model based on real-world datasets and evaluated its performance for the document retrieval task. Two IT service datasets from IBM, the IT-Change and IT-Solution datasets, and a relatively smaller publicly available dataset from Google Health [1], the Symptom-Treatment dataset, were used to evaluate the effectiveness of the LAA ranking algorithm.
The IT-Change dataset was obtained in the context of IT change management at IBM. In this dataset, each docu-ment pair consists of a change document, which describes the planned change, and a problem document, which de-scribes the problem resulting from this change. Both the change and problem documents are written in free text, and the associations between them are established by a human expert. Given a historical IT-Change dataset, we built the LAA model and used it to retrieve the potential problem documents (from a set of problems reported) caused by a change request. The original dataset contains 24,317 pairs of documents. We randomly sampled 20,000 document pairs for training and used the rest to evaluate the performance of our ranking algorithm.
 The IT-Solution dataset was obtained in the context of IT problem management at IBM. In this dataset, each doc-ument pair consists of a problem document and its corre-sponding solution document identified by a human expert. LAA is used to predict possible solutions for new problems. This dataset contains 19,696 pairs of documents. We ran-domly selected 15,000 document pairs for training and the rest for testing.

The Google Health Symptom-Treatment dataset contains pairs of disease symptoms and treatments. This dataset is relatively small and contains 1,287 document pairs. We used 1,187 document pairs for training purposes and tested the model on the remaining 100 document pairs. We compare our two LAA-based approaches, i.e. ,LAA Direct (LAA-D) and LAA Latent (LAA-L), against the Two-Step method and the PTM method [25], as well as the Polylingual topic model (PLTM) [17], in terms of their accu-racy in retrieving target documents for a given source doc-ument. For a given source document, PTM predicts a word distribution of its potential target document and compares it with the word distributions of the candidate documents. The word distribution of PTM has two components: one from the model, and the other from the similarity between the source and target documents. In LAA, we do not as-sume any overlap between the vocabularies of the source and target documents, which is a key advantage compared to PTM. For comparison purposes, we used only the model component in PTM as shown in Eq. (21) and adopted the KL-divergence distance [14] to evaluate the candidate target documents, as presented in [25].

Besides these methods, we also considered a two-way clas-sification approach. We used the real source and target document pairs as positive samples, and randomly gener-ated source and target document pairs as negative samples. Based on these labeled samples, we trained a classifier using SVM to predict future source and target document pairs. However, this approach does not work due to lack of good quality negative samples.

From each of the two datasets, we randomly selected a batch of 100 document pairs with one-to-one mappings be-tween the source and target documents. Given a source doc-ument randomly selected within these 100 document pairs, we then ranked the 100 target documents based on the four different approaches. We used the average rank of the cor-rect target document (the one actually paired with the se-lected source document) to measure the accuracy. We re-peated this process for five batches ( i.e. , 500 queries in total) for each datasets.

To train the LAA model, we varied the number of top-ics from 10 to 50, and chose the best performing models for the three datasets. For the IT-Change and IT-Solution datasets, we chose 20 topics for both the source and target document sets to train the three models. For the Google Health dataset, we chose 30 topics, where the model per-formed best. The dimensionality of the correlation factor was set to 10 for both the LAA models and the Two-Step method.

Fig. 5 compares the performance of these five approaches on the three datasets. The y -axis shows the average rank of the correct target document from the 100 target document candidates. For the IT-Change and IT-Solution datasets, each bar in the figure shows the performance range of one method over the five batches of test cases. The average over the five batches is marked in red on each bar. Because the Google Health dataset does not contain as many document pairs as the other two datasets, we investigated the accuracy of the model on only one batch of 100 test document pairs.
For all three datasets, LAA-L outperforms all other ap-proaches, and the Two-Step method and PLTM perform closer to LAA-L. The key difference between LAA-L and the Two-Step method is that the topic structures of the source and target documents in the Two-Step method are learned independently without considering correlations be-tween them. As a result, the performance of the Two-Step methodisnotasgoodasthatofLAA-L.

PLTM, on the other hand, assumes a pair of linked docu-ments that are identical in topic. A topic model is learned for all of the document pairs. For testing how likely a new source document and a new target document are associated, PLTM predicts their topic proportions independently and compares their similarities. PLTM is most effective when modeling the same set of articles written in different languages. In LAA, we don X  X  assume that the topic proportions of a source doc-ument and the linked target document are the same. Hence, LAA is more general and can be applied to document sets that have arbitrary semantic associations, such as questions and answers, symptoms and diagnoses, etc .

LAA-D suffers from the problems discussed in Section 5.2 and does not perform well in our document retrieval task. Due to the noisy nature of word-level correlation (as noted in Section 3), the PTM method does not show good perfor-mance either. We also experimented with a modified version of PTM that compares the topic distributions, rather than the word distributions, between the source and target docu-ments. The performance of this modified method is similar to that of the Two-Step method, but significantly worse than that of LAA-L.
To show the robustness of the LAA-L model in capturing the semantic associations in document pairs, we trained the model with different numbers of topics and compared the results of the document retrieval task in an experimental setting similar to that in Section 6.2. Figure 6: A Comparison of Ranks for the Different Methods Using Different Numbers of Topics.

Fig. 6 shows the experimental results for the IT-Change dataset. Limited by the space, we do not show the results for the IT-Solution and Google Health datasets, but the re-sults are quite similar. We chose the same number of topics for the source and the target document sets, and the di-mension of the correlation factor L = 1 2 K s = 1 2 K t .With different numbers of topics, the performance of the LAA-L approach remains stable, and is better than that of the other approaches.
The LAA framework assumes a correlation factor. The topic portion priors of a pair of documents are drawn cen-tered around a point in their corresponding topic simplex. Because each point in the topic simplex implies a mixture of the topics and each topic is represented by a probabil-ity distribution of words, the point in the topic simplex can also be mapped to a distribution over words. Here, we show examples of correlation factors and the corresponding top-ranked words in the source and target documents. First, we choose a sample correlation factor, which can be mapped to the topic distributions in both the source and target docu-ment sets. The topic distribution can be further mapped to a word distribution using a linear combination of the topics. Then, we show the top-ranked word list. In these examples, the dimension of the correlation factor is set to 10. The number of topics in both the source and target document sets are set to 20. Note that the topic numbers in the source and target documents do not have to be the same.
As demonstrated in Fig. 7, the LAA model successfully captures the semantic-level connections between the source and target documents. Cases 1 and 2 were extracted from the IT-Change dataset, whereas cases 3 and 4 were from the IT-Solution dataset. For Cases 1 through 4, the top-ranked words indicate that the correlations between the source and target documents are around Database, Network, Business, and Scheduling , respectively 1 .
The labels for the correlation factors were added by the authors. Figure 7: Sample Top Ranked Words Linked to the Same Correlation Factor.
Topic models have been extensively studied and have be-come a powerful tool for exploring the semantic content of large-scale document corpora. Most topic models deal with a single document corpus. LSI [8] uses SVD to approximate a high-dimensional document-to-word co-occurrence matrix using lower-dimensional document-to-topic and topic-to-word co-occurrence matrices. PLSI [12] introduces a probabilistic explanation of LSI. Neither LSI nor PLSI is naturally gen-eralizable to new documents. To overcome this limitation, Blei et al. [5] proposed LDA, in which the topic proportions of documents are randomly drawn from a Dirichlet distribu-tion. The Dirichlet prior is used to guide the generation of topic proportions for new documents. The CTM method [4] introduces a covariance matrix over the topic proportions and allows topics to be correlated with each other. IFTM [19] combines CTM with PCA [20] to allow exploration of a very large number of topics.

Besides the textual information in a document corpus, a number of topic models consider structural information. Steyvers et al. [21] use the authorship graph between au-thors and articles to explore author-to-topic relationships. Nallapati et al. [18] consider the citation graph for a doc-ument set to perform link predictions. Zhou et al. [28] study Web pages and tag graphs to explore user interests. Mei et al. [16] propose topic models with network regular-ization. Unlike those models, LAA focuses on document-to-document associations, and explores topics of two document sets simultaneously. Thus, LAA is better suited for ranking document pairs.

Researchers have studied topic structures of cross-lingual corpora. Zhao et al. [26, 27] explore probabilistic word alignments across languages using aligned bilingual docu-ment pairs, i.e. , the same set of articles written in two different languages. Mimno et al. [17] study the shared topic structure of an aligned document corpora over possi-bly many languages. Jagaralamudi et al. [13], assuming that a dictionary exists between words in two languages, analyze a single topic structure over bilingual unaligned document sets. MuTo [6] also utilizes word matchings in a dictionary to analyze the topics as distributions over the word pairs. Haghighi et al. [11] also applied the CCA model to learn bilingual translation lexicons.
This paper has presented one of the first attempts to tackle the problem of analyzing the topic structures of two document sets linked by a bipartite graph. The Latent As-sociation Analysis (LAA) model draws the topic proportion priors of a pair of documents based on a correlation factor. Unlike other topic models, the goal of LAA is not only to provide a semantic-level explanation of the topics of the doc-ument pairs, but also to retrieve the associated target doc-ument, when a new source document is given. Using LAA, we introduced a document-level ranking method that can help to retrieve target documents associated with a source document. Experiments on real datasets confirm the effec-tiveness of our method for extracting semantic concepts of associated document pairs, and establish that LAA outper-forms state-of-the-art algorithms in ranking document pairs. LAA can be extended to more complex association struc-tures over multiple document sets. For other applications, the symmetric structure of the source and target documents can be replaced by an asymmetric structure, if that is more appropriate. The first author, Gengxin Miao, was supported by an IBM Ph.D. Fellowship. This research was also supported by the U.S. National Science Fo undation under grant IIS-0954125 and by the Army Research Laboratory under cooperative agreement W911NF-09-2-0053 (NS-CTA). The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official poli-cies, either expressed or implied, of the Army Research Lab-oratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Govern-ment purposes notwithstanding any copyright notice herein. [1] Google health: https://health.google.com. [2] F. R. Bach and M. I. Jordan. A probabilistic [3] C.M.Bishop. Pattern Recognition and Machine [4] D. M. Blei and J. D. Lafferty. Correlated topic [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [6] J. Boyd-Graber and D. M. Blei. Multilingual topic [7] H. T. Dang, D. Kelly, and J. J. Lin. Overview of the [8] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. [9] P.Forner,A.Penas,E.Agirre,I.Alegria,C.Forascu, [10] J. Gao, K. Toutanova, and W. tau Yih.
 [11] A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and [12] T. Hofmann. Probabilistic latent semantic indexing. In [13] J. Jagaralamudi and H. Daum  X  e. Extracting [14] J. Lafferty and C. Zhai. Document language models, [15] K.V.Mardia,J.T.Kent,andJ.M.Bibby.
 [16] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic [17] D. Mimno, H. M. Wallach, J. Naradowsky, D. A. [18] R. M. Nallapati, A. Ahmed, E. P. Xing, and W. W. [19] D. P. Putthividhya, H. T. Attias, and S. Nagarajan. [20] J. Shlens. A tutorial on principal component analysis. [21] M.Steyvers,P.Smyth,M.Rosen-Zvi,andT.Griffiths. [22] T. Strohman, W. B. Croft, and D. Jensen.
 [23] B. Taskar, M. F. Wong, P. Abbeel, and D. Koller. [24] X. Xue, J. Jeon, and W. B. Croft. Retrieval models [25] D. Zhang, J. Sun, C. Zhai, A. Bose, and N. Anerousis. [26] B. Zhao and E. P. Xing. Bitam: Bilingual topic [27] B. Zhao and E. P. Xing. HM-BiTAM: Bilingual topic [28] D. Zhou, J. Bian, S. Zheng, H. Zha, and C. L. Giles.
