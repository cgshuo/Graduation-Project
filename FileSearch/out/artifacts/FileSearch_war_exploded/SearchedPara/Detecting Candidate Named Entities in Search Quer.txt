 The information extraction task of Named Entities Recog-nition (NER) has been recently applied to search engine queries, in order to better understand their semantics. Here we concentrate on the task prior to the classification of the named entities (NEs) into a set of categories, which is the problem of detecting candidate NEs via the subtask of query segmentation.We present a novel method for detecting can-didate NEs using grammar annotation and query segmen-tation with the aid of top-n snippets from search engine results and a web n-gram model, to accurately identify NE boundaries. The proposed method addresses the problem of accurately setting boundaries of NEs and the detection of multiple NEs in queries.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval  X  Query formulation Keywords: named entity recognition, query logs.
The challenges posed by the structure of search engine queries, such as their short length, lack of grammar and orthographic features, imply that classical NER techniques cannot be used without modification. In response, researchers have recently proposed several approaches to address these challenges. A relatively simple yet effective method is to start with a set of seed instances for each NE category, and then scan the query log in order to extract the corresponding contexts, which can then be used to identify new instances for each category. Both Pa  X sca [5] and Guo et al. [2] applied this method, with the context including the prefix and/or suffix surrounding instances that were detected. For exam-ple, the word  X  X yrics X  could be the context for song names; the same context appears in queries such as  X  X ong lyrics X ,  X  X ames Brown songs lyrics X , and  X  X usic and Lyrics X . The in-stances extracted could be: (i)  X  X ong X , which is not a NE, (ii)  X  X ames Brown songs X , where the NE boundaries are incor-
Figure 1: Query Log Candidate NE Recognition rect, or (iii)  X  X usic and X , where the context is part of the NE which is a movie name. Thus a problem with this approach is that the queries X  structure was not considered. Jain and Pennacchiotti [4] presented an unsupervised approach that utilises the syntactic representation of words such as Capi-talisation. Although confidence scores were assigned to each extracted NE based on its presence in a Web corpus, relying on Captilisation will often miss many potential NEs in the query log that were not typed with capital letters.
Here, we present a processing pipeline which overcomes the above drawbacks by: (1) Tagging each query token with its part of speech (POS) and the corresponding orthographic features (ORTH) based on the context in which tokens ap-pear, using top-n query snippets; (2) Finding the most prob-able query segmentation to accurately set the boundaries of candidate NEs; and finally (3) Using these annotations for NER. The contribution of our work lies in the following: (i) Presenting a NER approach for queries by considering English language structural features; (ii) Using query seg-mentation to better set the boundaries of candidate NEs, especially in multiple NE queries; and (iii) Fine-grained eval-uation of NER for search engine queries.
Figure 1 presents the processing pipeline we use to iden-tify candidate NEs in a query log. It involves four main stages: (1) Pre-processing the query log; (2) Tagging each query with the corresponding grammatical annotation; (3) Segmenting the query to identify boundaries; and (4) Recog-nising the candidate NEs with respect to annotations and segmentation.

Pre-Processing: It is important that the query log is processed to exclude noise , which is defined as any query that is a sequence of symbols containing no words. URLs are also detected in advance before any further natural language processing of the query log. Finally, the spelling of each query is checked and corrected.

G rammar Annotation: For each query in the log, the top-n snippets are retrieved. Each snippet consists of few sentences from a document retrieved for the query. The advantage of using snippets is that they contain tokens sur-rounding the query that usually provide enough context to annotate the query, avoiding the cost of parsing complete web documents as in [4]. In addition, the approach we are proposing operates in an offline scenario, therefore snippets are retrieved and stored in advance. The set of grammat-ical annotations include, parts-of-speech (POS) and ortho-graphic (ORTH) annotations. We assume there are no de-pendencies between query tokens, adopting the bag-of-words approach as in [1]. It is important to note that we differ-entiate between common nouns and proper nouns to help identify NEs, and therefore the POS tagging is more specific than that used in [1]. The orthographic features capture the string representation of each query token, such as capital initial, all capital, or mixed letter cases.

Query Segmentation: Many approaches of query seg-mentations have been proposed in literature. For instance, Hagen et al. [3] used raw n-gram counts and Wikipedia to segment queries. Although their approach achieved high accuracy, in our case we need to segment queries accord-ing to their appearance in top-n related snippets. We de-fine query segmentation as follows. For a query Q consist-ing of tokens t 1 , t 2 , ..., t n , the set of all possible segmen-tations is S(Q) = { S 1 , ..., S m } , where m  X  2 n  X  1 . Each segmentation S i  X  S(Q) consists of one or more segments, and each segment, say s ij , is a sequence of query tokens that obeys the original order of the tokens. We define the best segmentation S  X  as the most probable one over all S i  X  S(Q) . The probability of each S i is calculated as Pr ( S i ) = ing a local n-gram model ( M snippet ) created from the set of retrieved snippets for the query Q . This probability is smoothed by the probability of the segments given a web n-gram model ( M web ) using an empirically set parameter  X  between 0 and 1, to obtain
Recognition of Candidate NEs: A small set of rules was defined by examining a random sample of grammati-cally annotated and segmented queries as described above. In the sample, three main cases were observed: (1) A se-quence of proper nouns contained in a segment (approxi-mately 93%), (2) A conjunction, e.g. X &amp; X , or preposition, e.g.  X  X f X  followed and preceded by proper nouns such as  X  X niver-sity of Wisconsin X  (approximately 2%), and (3) A sequence of proper nouns that include numbers, such as  X  X icrosoft Office Professional 2003 X  (approximately 5%). Hence, rules were created to reflect these cases. Applying these rules with respect to segmentation boundaries will result in, for example, the detection of two NEs from a segmented query  X  X marriot] [new jersey] X . Experiment: The approach was applied using a 2006 MSN query log consisting of approximately 15M queries. Removing duplicates and after a further pre-processing stage, approximately 5.5M queries were left. Thereafter, the spelling of each query was checked and corrected using Yahoo API provided through the Spelling Suggestion YQL table. In ad dition, for each query, the top eight query snippets high-lighting the query tokens along with their immediate con-text were retrieved and stored using Google Custom Search API. Then each snippet was Grammatically annotated using GATE and accordingly each query token was then annotated with the most probable POS and orthographic feature. We used the Bing web n-gram model to find the probability of each distinct segment, and smoothed the probability of the same segment given the local n-gram model with  X  = 0 . 6 be-ing empirically set. Finally, candidate NEs were extracted using the set of defined NER rules. Table 1 presents exam-ples of segmented queries and corresponding extracted NEs. Evaluation and Results Analysis: In previous work, NER was evaluated by manually checking a random sample [4], or the top-n [5][2] of instances extracted for each NE class without considering the NEs they missed or the con-text of the extracted ones. For example,  X  X afari X  is a NE in the query  X  X ownload safari X , while it is not in the query  X  X a-fari trips X . Since we use hand-crafted rules to extract NEs, the quality of these rules should be reflected in the evalu-ation. Therefore, we manually checked a uniform random sample of 1000 queries.The performance of our method was assessed at the query-level, which is stricter than the pre-vious evaluation methods at the NE-level. True and False Positives (TP, FP), and True and False Negatives (TN, FN) were counted, and these are defined as follows: TP : Query has one or more NEs and all were correctly detected. TN : Query has no NE and none were detected. FP : One or more query tokens were incorrectly tagged by the rules as NE. FN : One or more query NEs were incorrectly missed by the rules. The accuracy was measured using two evaluation methods (see table 2): Evaluation I , where NEs detection is tagged as correct regardless of its boundary (with accuracy 80.6%) (e.g. extracting  X  X ony PS3 news X ), and a stricter ver-sion, Evaluation II , where NEs boundary must be accurate (with accuracy 67.8%) (e.g. extracting  X  X ony PS3 X ).
