 1. Introduction
Term weighting for document retrieval and ranking has been a key research issue in information retrieval for decades weighting methods are based on document oriented statistics such as term frequency (TF) in a document and inverse doc-ument frequency ( IDF ) from a collection. For term weighting, TF IDF based schemes and their variations have been proven to be robust and difficult to be beaten, even by much more carefully designed models and theories ( Amati, 2003; Gerard Salton, son &amp; Zaragoza, 2009 ). Term frequency is a document-specific local measure and typically computed as follows: where rtf is the raw term frequency and the max_freq is the frequency of the most common term in the document.
The inverse document frequency or IDF ( Robertson, 2004 ) is based on counting the number of documents containing the term in a query in the collection being searched. The most commonly cited form of IDF is as follows:  X  where N and df t are the total number of documents in the collection and the frequency of documents containing a term t , where W l ( t ) and W g ( t ) represent local and global weights, respectively.
 accumulated retrieval experiences and introduce a term X  X  evidential weight in addition to local and global information. It is obtained from a term X  X  history of separating relevant from non-relevant documents, which can be found in past retrieval results. A term X  X  history can be obtained directly from conventional test collections constructed through TREC, CLEF, and
NTCIR or indirectly from click-through data (see Xue et al. (2004) and Joachims (2003) for example) that have been made available with commercial search engines.
 relevant documents. When a high-IDF term in a query has two senses with roughly equal numbers of occurrences in a col-lection, for example, about a half of the retrieved documents would be irrelevant. When observed over a large number of queries, two terms with similar IDF values may play different roles in separating relevant from non-relevant documents. ination power ( DP ).
 ten to incorporate the new term weight based on DP as follows: which relevance judgments are available. DP values are computed using the ranks or the similarity values of the relevant and non-relevant documents retrieved by a query. 2. Related work porate users X  information needs indirectly with local relevance judgments. In general, the contents of relevant and non-rel-evant documents retrieved are used to modify the original query. Given that the user has a difficulty in recalling proper terms for the information need, the probabilistic relevance feedback ( Manning, Raghavan, &amp; Sch X tze, 2008 ) suggests a new set of query terms from the retrieved results, rather than preserving and modifying the original query, based on the
Harper (1979) also used the probabilistic model for initial search as is used for relevance feedback search. But their method differs from the approach taken in this paper in that we use accumulated DP values as a mediator for the original weight function as in formula (2) .
 the query at hand, whereas the proposed work in this paper attempts to gather evidence for the overall importance, like IDF , of a term in retrospect from an accumulation of relevance judgments. Given that the evidential weight of a term is used to measure its overall importance across queries, it is proposed as an auxiliary factor working together with the well-known term weighting method as in formula (2) , where it complements the strictly local information (TF) and generic document discrimination information (DF). Instead of requiring relevance feedback information for the current query, which may not be available, it utilizes relevance information that can be collected over many past queries.
 example) also uses accumulated relevance information. Given a set of queries and their relevant documents, its goal is to automatically learn an optimal ranking function of a retrieval engine through various kinds of machine learning approaches.
From the machine learning perspective, the proposed approach is to learn the empirical importance of individual terms based on their past roles in separating relevant from non-relevant documents. That is, while the Learning to Rank approach finds one best ranking function from the entire training data, ours determines goodness of individual term separately. 3. Discrimination power queries in which it appeared. A query term in a retreived document is supposed to play an important role on the relevance of the document. But its general impact on relevance of any document, or importance, must depend not only on its degree of ambiguity but also the actual user information need, the collection being searched, time-sensitive trends, etc. While there are many factors that help a term discriminate relevant from irrelevant documents, we conjecture that a term X  X  overall dis-crimination power can be obtained from its role in the past queries. In other words, we attempt to compute a term X  X  discrim-inating power by accumulating its roles in separting relevant from non-relevant documents over many queries in the past.
Given a set of queries and top-ranked for example, hundred), retrieved documents per query, which can be divided into two subsets of relevant and non-relevant documents, we can obtain two distributions for a term with its frequency (or weight) in relevant and non-relevant documents, respectively. Fig. 1 shows actual histograms of two subsets of relevant and non-relevant documents with respect to a term  X  nation  X . The weights shown in Fig. 1 are normalized in the range of [0,1] across queries.

Given a pair of curves for a term in a query, which represent distributions of term frequencies for relevant documents and non-relevant documents, respectively, we can take a sum of all the statistics for a term in the past queries containing the term to result in a graph like the one in Fig. 2 . The difference between the two averages representing the sets of term fre-quencies obtained from relevant and non-relevant documents can be regarded as a discrimination power ( DP ) of the term. The idea of comparing two distributions has been shown to be useful in past research. For example, Robertson X  X  work utilized the difference of two distributions of matching function values of both relevant and irrelevant documents in order to enhance query expansion performance ( Robertson, 1990 ). The distributions represent probability density of matching function val-ues, not individual term weights or frequencies as in our case, and the method proposed there is to estimate the effect of adding a new term on the mean difference of the two distributions for a specific query. Our approach, on the other hand, attempts to measure the overall quality of a term by comparing two distributions of a term X  X  weight in both relevant and irrelevant documents accumulated over past queries.

The evidential weight w E of a term can be obtained by averaging the DP values from all the past queries. In estimating DP of a term, we consider the ratio, instead of difference, of the means of the term frequency distributions of relevant over non-for a term t k contained in a given query q i can be computed as follows: numbers of relevant (Rel) and non-relevant (Nonrel) documents in the search results (|Rel| + |Nonrel| = # of search results) the value, we apply the following based on sigmoid function to the DP init, resulting in the range of (0,2). DP init for a term not occurring in either relevant or non-relevant documents is not calculated but assigned 1.0 as a default Since DP is an accumulated statistical value obtained from a set of past retrieval results, no change should be made to the affect the term X  X  weight one way or the other. By the same token, the default value is assigned to a term when there is no difference in the accumulated evidence between relevant and irrelevant sets. Such terms are construed to be  X  X eutral X  rather than  X  X seless X  in DP calculated from the past retrieval results so that other indicators for their importance should be re-spected as they are. While such terms may be seen as useless in discriminating relevant from irrelevant documents when uments than those with DP values greater than 1.0, and hence its final weight should be lowered.
 according to the given query q i . An ideal query should retrieve all the relevant documents before any non-relevant docu-ments are retrieved. Furthermore, such a query should ensure that the difference between the similarity values of relevant documents and those of non-relevant ones be as large as possible. Instead of trying to devise a better ranking function ( Cao term so that we can judge the quality of the term indirectly by analyzing many queries.
 as follows: ments should be ranked (positioned) first followed by the v non-relevant ones. Given that there is no information about the over non-relevant documents, for a query q i containing a particular term t k : of terms occurring in the query q i , freq  X  t k  X  j q tically changes and the ratio with sqrt performed well empirically.
 query containing a particular term t k : 0 when all the u relevant documents are ranked (positioned) first followed by the v non-relevant ones based on the decreasing order of the similarity values.
 From Eq. (4) , now a DP value for a term can be computed as: Alternatively, it can be computed using OPT sim as follows together with the local and global weights. 4. Experiments
To demonstrate the value of the proposed term-weighting method utilizing evidential weights based on term X  X  DP values, we ran experiments using two systems, Terrier 1 and Indri 2 with and without applying the evidential weight component in computing term weights. We chose them because they are well-known open source search engines that have been validated as in Eq. (1) into the proposed one in Eq. (2) . 4.1. Experimental setup and data sets of documents. It provides several state-of-the-art document ranking models including DFR_BM25 (Divergence From Ran-domness) ( Amati, 2003 ), TF IDF , language modeling approach, etc. In addition, it supports a number of parameter-free DFR term weighting models for automatic query expansion plus Rocchio X  X  query expansion.

We modified Terrier X  X  term weighting module so as to apply the proposed DP based method and show its impact. In par-ticular, TF IDF and DFR_BM25 for probabilistic models, and Hiemstra language modeling method ( Hiemstra, 2001 ) were modified in the experiment. The DP values were applied according to the Eq. (2) in this model.

We also used Indri, one of the most well-known language modeling-based search engines, which combines the language TREC conference, we decided to use it as a variation of a language modeling-based search engine, in addition to the Hiemstra language model. The DP -based scheme is employed in such a way that it estimates the term X  X  (called node in Inference Net-work) belief score. It is calculated as follows: where d is a default belief that ensures term belief values to be above a certain value.

DP is computed as in the following equation if t k has been found in the collection of past queries. Otherwise, a default DP value 1.0 is given.
 In order for the experimental results to be comparable with previous performance values, we used three TREC collections: TREC-3, TREC-4, and TREC-5. Table 1 shows the basic statistics about them. 4.2. Training/testing procedure  X  Factoring a query into a set of terms (or phrases).  X  Calculating individual term X  X  weight in a document.  X  Merging term weights based on the query structure.  X  Ranking documents based on the merged weight.
 to be used for both computing DP scores and conducting retrieval experiments, the leave-one-out method was used. That is, after computing DP scores for terms using q 1 out of q queries, the remaining query was used for testing. This process was repeated q times.
 results including rank and similarity scores at step 6, and relevance judgments at step 7. We calculated DP values for indi-no previous history, is 1.0 .
 given that the document is relevant. In the same way, the weight on a non-relevant document can be treated as p ( t k | r ).
As a result, we can estimate the basic ratio p ( t k |r )/ p ( t k | r ) to calculate the DP -based weight. 4.3. Coping with limited training set that the number of terms for which a DP value (henceforth DP terms ) is available depends on the number of unique terms in the query sets. In reality, however, the number of query terms in a collection may be quite small compared to that of the terms in the collection-wide term dictionary. As a result, a query for which none of the terms appear in the past queries experimental environment because only three collections are used.

In order to cope with this limitation, especially for the experiments, we employed pseudo relevance feedback, an auto-matic query expansion method. The DP values were computed for not only the original query terms but also expanded terms. This way, the DP -based weighting scheme was applied more widely. The two search engines, Terrier and Indri, provide well-known pseudo relevance feedback mechanisms. The former supports a number of parameter-free DFR term weighting mod-els for automatic query expansion, in addition to Rocchio X  X  query expansion. The latter, on the other hand, provides auto-matic pseudo relevance feedback mechanism, an adaptation of Lavrenko X  X  relevance models ( Lavrenko &amp; Croft, 2001 ).
Table 2 shows the numbers of DP terms for different weighting models and test collections. In TF IDF search, for example, the number of DP terms is at most 712 (TREC-3) and increased to 1249 with query expansion. The last row shows the min-imum and maximum percentages of DP terms for the collections, which are not high. 5. Results 5.1. Rank vs. similarity based DP Since we designed two optimality calculation methods for DP in Eqs. (5) and (7) , the first task was to compare them with TF IDF weighting in the three collections. The result was that the similarity optimality based DP method is better without exception as shown in Table 3 although the paired t -test of the difference between Rank Optimality and Similarity Optimal-alone showed unsatisfactory performance. It is because DP init and similarity (or rank) optimality do not work independently. Based on the result in Table 3 , the subsequent experiments were conducted with the similarity optimality based DP method. 5.2. DP for language modeling approach
To show the effect of applying the DP -based term weighting scheme to the language modeling method, we compared two language modeling-based search engines: the Hiemstra language model implemented in the Terrier engine and the Indri lan-guage model in the Indri engine. Table 4 shows the results of the experiments performed on each TREC collection. There are two baselines:  X  X aseline X  refers to the basic search while  X  X aseline w / QE  X  means that the search was performed with query expansion using pseudo relevance feedback (PRF). The experiment with  X  X aseline w / QE  X  shows the contribution made by DP scores over and above the query-specific expansion (PRF in this case). We use the default values for the system param-eters. The performance values are all in Mean Average Precision (MAP), and numbers in parentheses are percentage increase over the baseline results.

While the magnitudes of the improvements are not significantly high except for one case (TREC-3/HLM), the results are consistent without any performance decrease with or without query expansion across all the collections. Given that the number of queries in the collections and hence the training set are relatively small, we consider this is very promising. The biggest improvement (20.3%) for TREC-5/HLM is due to the lowered performance of the baseline when query expansion was used, not entirely due to the DP -based scheme.

The effect of using query expansion is interesting by itself in that we still obtained improvement over the performances of query set would increase the positive effect of the DP -based weighting scheme. 5.3. DP for probabilistic models
To show the effect of DP -based weighting scheme on the probabilistic models, we made comparisons with two probabi-listic search methods: TFIDF and DFR_BM25 in the Terrier engine. Table 5 shows the results of the experiments performed with the two search engines on each TREC collection.
 esting observation across the two results is that the effect of DP -based scheme is collection-dependent. The effect of query expansion is smaller for TREC-3 across all different retrieval models whereas it is increased for TREC-4 after expansion. It for future research. 5.4. Effect of the number of DP terms slowly increase as the number of DP terms (dotted line) increases. It shows one of the three experimental results we have mance if we gather more DP terms from the previous search results. It should be noted that the maximum number of DP terms is only 1249 in Table 2 . 5.5. DP for web blogs were used. Noting that the TREC collections we used are different in their nature with the web search environment, we ran experiments with ClueWeb-09-T09B, 3 the TREC 2009 Blog Track collection, which consists of 50 million documents (size of documents: 250 GB compressed). The experiment used 592 queries (# of average terms: 2.58) containing relevant documents. eral, user click-through data can be extracted from a large amount of search logs accumulated by web search engines. These logs typically contain user-submitted search queries and the URL for user-clicked Web pages available in the search result pages. Although these click-through data do not necessarily reflect the exact relevance information, they provide indications were heavily used for a certain time period and which were found frequency in clicked pages. While this calls for an inter-esting experiment, such data is not easily available outside the small community of Web portal companies. ClueWeb-09 was the best possible resource available to us to get close to the Web search situation.

In order to make our experiment possible, we had to use only 10% of 50 million documents in the collection; dealing with the entire collection in our experimental setting was prohibitively expensive. The subset consists of randomly selected 10% of non-relevant documents and entire relevant documents. The experiment is applied to 592 queries that have relevant doc-uments. The ranking algorithms used were TFIDF, DFR_BM25, and Hiemstra Language Model in Terrier. Since 90% of the non-relevant documents were removed, the retrieval task became easier than the original one. As such, the effect of the DP values was expected to be smaller.

Table 6 shows the results of the experiments. The Base MAP indicates the MAP produced by the original ranking algo-rithms, while DP Applied MAP indicates the performance of the proposed DP applied search. The performance increase in the parentheses is promising because the number of applied DP terms per query is 1.31 on an average, which is smaller than that of the previous results on TREC 3 X 5. The total number of DP terms in each experiment was about 1000, which is a very small portion of the vocabulary size 17 M in the collection. 5.6. Parameter selection
The DP Eq. (11) has an important parameter A, which is an amplifier to control the range of the values when it is used together with the local and global weights. The parameter value, 0.8, was used for the experiments based on a preliminary experiment whose purpose was to investigate its effect on MAP variations for all the collections and retrieval models. Since the results show similar trends, only the result of the Hiemstra language model search is depicted in Fig. 5 as a representa-weights. Consequently, the value 0.8 is actually narrowing the DP range. 5.7. DP vs. IDF
DP and IDF share a common characteristic: they are term properties that are computed globally from a corpus or rele-vance judgments and intended to help separating relevant from non-relevant documents. However, they are distinct from each other in their statistical characteristics. In addition to the retrieval performance improvements, which show that DP seems to play a unique role beyond IDF , their values are distinct from each other as Table 7 shows examples of query terms and their IDF and DP value pairs extracted from TREC-3.

Besides the anecdotal evidence, the Pearson correlation measure ( Rodgers &amp; Nicewander, 1988 ) between them, 0.03, indi-cates their values are not correlated. For an intuitive understanding, a scatter plot between DP and IDF is shown in Fig. 6 where the straight line is a result of linear regression.
 5.8. Storage and efficiency requires a history of users X  relevance judgments be kept. In a contemporary search engine, it amounts to storing click-through data that have been shown very important for various schemes for query understanding ( Joachims, 2003, 2005; to multiply a constant value when a term X  X  weight is computed as in Eq. (2) . Since DP ( t ) is computed once for a term like to be computed periodically offline, like IDF ( t ), as the user relevance judgments data is updated. 6. Conclusion
Term weighting schemes are crucial for document retrieval effectiveness in that they are the core of document weighting and ranking. We propose a novel term weighting method that utilizes availability of past retrieval results containing rele-vance judgments. A term X  X  evidential weight depends on the degree to which the mean frequency values for the relevant uments. In a sense, it has the effect of accumulating local relevance feedback information across many queries to determine uments that contain the term at hand.

The experiments using standard test collections show that the proposed weighting scheme indeed improves retrieval effectiveness. It is interesting to note that we obtained the performance increase with only a small number of terms found collection but based on the relevance-judged documents, is clearly distinct from IDF .

The new weighting scheme opens up new research directions. First of all, we will have to investigate on different statis-tical properties of relevance-judged documents to refine the evidential weight calculation method. Second, we need to ex-plore further the effect of the size of the relevance-judged document collection on retrieval effectiveness. Third, an interesting and promising area is to apply this scheme to Web search, especially attempting to compute evidential weights in a time window so that term weights vary with time. Finally, we will explore how best evidential weights can be mixed with other local and global weights by understanding the relationships among them.
 Acknowledgements
This research was supported by WCU (World Class University) program under the National Research Foundation of Korea and funded by the Ministry of Education, Science and Technology of Korea (Project No.: R31-30007).
 References
