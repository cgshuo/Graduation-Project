 Jerome A. Feldman (University of California, Berkeley) Cambridge, MA: The MIT Press (A Bradford book), 2006, xx+357 pp; hardbound, ISBN 0-262-06253-4, $36.00 Reviewed by Stefan Frank Radboud University Nijmegen Over the last decade or so, it has become increasingly clear to many cognitive scientists that research into human language (and cognition in general, for that matter) has largely neglected how language and thought are embedded in the body and the world. As argued by, for instance, Clark (1997), cognition is fundamentally embodied ,thatis,it can only be studied in relation to human action, perception, thought, and experience. As Feldman puts it:  X  X uman language and thought are crucially shaped by the properties of our bodies and the structure of our physical and social environment. Language and thought are not best studied as formal mathematics and logic, but as adaptations that enable creatures like us to thrive in a wide range of situations X  (p. 7). Although it may seem paradoxical to try formalizing this view in a computational theory of language comprehension, this is exactly what From Molecule to Metaphor does. Starting from the assumption that human thought is neural computation, Feldman develops a computational theory that takes the embodied nature of language into account: the neural theory of language .
 the basic ideas behind embodied language and cognition and explains how the embod-iment of language is apparent in the brain: The neural circuits involved in a particular experience or action are, for a large part, the same circuits involved in processing language about this experience or action.
 information processing by neurons. This detailed exposition is followed by a description of neuronal networks in the human body, in particular in the brain.
 explained how localist neural networks, often used as psycholinguistic models, can represent the meaning of concepts. This is done by introducing triangle nodes into the network. Each triangle node connects the nodes representing a concept, a role, and a filler X  X or example,  X  X ea, X   X  X as-color, X  and  X  X reen. X  Such networks are trained by a process called recruitment learning , which is described only very informally. This is certainly an interesting idea for combining propositional and connectionist models, but it does leave the reader with a number of questions. For instance, how is the concept distinguished from the filler when they can be interchanged, as in  X  X ats, feed-on, mice X  versus  X  X ice, feed-on, cats. X  And on a more philosophical note: Where does this leave embodiment? The idea that there exists a node representing the concept  X  X ea, X  neurally distinct from its properties and from experiences with peas, seems to introduce abstract and arbitrary symbols. These are quite alien to embodied theories of cognition, which generally assume modal and analogical perceptual symbols (Barsalou 1999) or even no symbols at all (Brooks 1991).
 straction. A description at the computational level involves role-filler (or feature-value) structures and rules for manipulating these. The reader is regularly reminded that this is just a higher-level description of a connectionist, neural structure. Nevertheless, it remains unclear how exactly such a computational system can be implemented in a neural network, especially as the models X  complexity increases in the later chapters of the book.
 structures that emerge from the organization of our bodies and of the world, and are therefore universal across cultures. Examples are basic concepts such as  X  X rasp X  and  X  X upport. X  Although the conceptual schemas themselves are universal, the mapping between words and schemas differs across languages. For instance, there is no one-to-one translation between the spatial prepositions of different languages. The acquisition of words denoting spatial relations in different languages is simulated by Regier X  X  (1996) connectionist model, which is the first specific model discussed in the book. actions is encoded as execution schemas for controlling motor behavior. People are not aware of complete actions, but only of particular parameters that can be set in the execution schema X  X or instance, the action X  X  speed, direction, and duration. Learning the meaning of a verb comes down to associating the word with the corresponding parameters of the execution schema. These ideas are implemented in Bailey X  X  (1997) model of verb learning, presented at the end of this part of the book.
 a metaphor is used, words in the abstract domain are understood by using words and knowledge from more-concrete domains. Primary metaphors are grounded directly in perception and action. For instance, the expression a warm greeting makes use of primary metaphor by talking about affection in terms of temperature. Complex metaphors are conceptual combinations of primary metaphors. One of the most important complex metaphors is used for describing the structure of events, and combines mappings such as  X  X auses are physical forces, X   X  X tates are locations, X  and  X  X hanges are movements. X  but comes down to mentally simulating the event or situation described by the sentence (after metaphorically mapping to a more-concrete domain, if necessary). In support of this view, the book gives evidence from brain-imaging experiments but, oddly, ignores compelling behavioral findings by, for instance, Glenberg and Kaschak (2003), Stanfield and Zwaan (2001), and Zwaan, Stanfield, and Yaxley (2002).
 senting Narayanan X  X  (1997) model of story comprehension. In this model, background knowledge is implemented in temporal belief (Bayes) networks. These perform proba-bilistic inference by means of a process of parallel multiple constraint satisfaction, which is quite plausible from a neural perspective. Again, metaphorical or abstract language is understood by mapping to a more-concrete domain. As an example, it is shown how the model would process an excerpt from a newspaper article about economics.
 theory of language relies less on the syntactic structure of sentences than do more traditional theories of language comprehension. According to the proposed theory of grammar (based on Bergen and Chang X  X  [2005] Embodied Construction Grammar), the basic element of linguistic knowledge is the construction: a pairing of linguistic form and meaning. By applying linguistic and conceptual knowledge, an utterance (in the context of a situation) is transformed into a network of conceptual schemas that 260 specifies the meaning of the sentence.  X  X he job of grammar is to specify which semantic schemas are being evoked, how they are parameterized, and how they are linked together in the semantic specification. The complete embodied meaning comes from enacting or imagining the content ...and always d epends on the context in addition to the utterance X  (p. 288). This is clarified by a complete and detailed analysis of the simple sentence Harry strolled into Berkeley .
 tions are learned by associating an utterance (a linguistic form) to its meaning as inferred from the situational context.
 of language and computational modeling in cognitive linguistics. It discusses a wide range of issues, many of which were not included in this review. The view that language should not be described as a mathematical or logical system but is foremost a part of human behavior and a function of the brain could well be new and thought-provoking to readers of this journal. However, the same readers are also likely to desire a high level of computational detail which the book does not provide, as it does not present any formal specification of the discussed models.
 References
