
Automatic term extraction from texts of a spe-cific domain is one of the well-studied applica-tions in natural language processing and document analysis. During many years of research a lot of useful features of domain term extraction were proposed, including frequency-based and context-based features, word association measures, etc. ((Daille, 1995), (Zhang, 2008)).

Since these features characterize various prop-erties of terms, machine-learning models based on multiple features are now increasingly used for term extraction. It was shown that such models can work considerably better than those based on single features ((Aze et al., 2005), (Loukachevitch, 2012)). Nevertheless, the signifi-cance of particular features for term extraction by machine learning depends on several important as-pects concerning the domain in the target text col-lection, structure of extracted terms, and type of a terminological resource to be developed.

Firstly, specific domains vary in their scope (e.g., the broad social-political domain vs. the relatively narrow banking domain). Besides, domain-specific languages vary in their closeness to the general language (e.g. banking vs. im-munology domain). This enhances or diminishes the role of a reference text collection required to calculate some term features (usually, a news col-lection or a national corpus is used).

Secondly, terms may be single-word and multi-word. To extract single-word terms, word associ-ation measures (mutual information, t-score, etc.) are not applicable; extraction of three-word and longer terms requires special forms of associa-tion measures. It means that extraction models for terms of different lengths can differ.

At last, terms are extracted for various types of terminological resources: terminological dictio-naries, information-retrieval thesauri, ontologies for NLP. Dictionaries are mainly intended to sup-ply terms with definitions, whereas information-retrieval thesauri are to provide concepts (descrip-tors) for domain-specific applications (Z39.19, 2005).

For example, such terms from EuroVoc information-retrieval thesaurus as agricultural product, milk product, European party, eco-nomic consequence denote important concepts in the contemporary socio-political life of European Union, however, it is difficult to imagine these terms as entries in terminological dictionaries. Therefore, a particular type of a terminological re-source needs specialized term extraction models (Loukachevitch, 2012).

In this paper we consider the term extraction task specially for thesauri intended to be used in the information retrieval context (search, cat-egorization, clustering and other applications), 69 because we suppose that such terminological resources have specific properties partially ex-plained in specialized standards (Z39.19, 2005).
For this task we experimentally study machine-learning models based on various features for term extraction. Our study is based on two manu-ally created thesauri and two languages: the En-glish version of Eurovoc thesaurus and the Rus-sian Banking thesaurus. We restrict our study to single-word and two-word terms to compare the extraction models for the most frequent types of terms.
Machine-learning or combined approaches to automatic term extraction were studied in a num-ber of works: (Vivaldi et al., 2001), (Aze et al., 2005), (Foo and Merkel, 2010), (Zhang, 2008), (Loukachevitch, 2012).

In most works automatically extracted terms are evaluated on the basis of available terminological resources or expert annotations of domain terms ((Daille, 1995), (Church and Hanks, 1990), (Dun-ning, 1993), (Church and Gale, 1995)). If to con-sider evaluation of machine-learning models for term extraction, in (Aze et al., 2005) experiments were fulfilled for texts in biological and human resources domains with expert annotation of do-main terms. In the work (Foo and Merkel, 2010) two patent collections with term pre-annotation were studied. (Zhang, 2008) extracted terms from the Genia corpus, for which Genia ontology was created, and also utilized an artificial corpus of Wikipedia articles with expert annotation of terms.
In contrast to the above-mentioned works, in our study of term extraction we focus on the specific type of terminological resources  X  the-sauri intended for information-retrieval applica-tions. We take the well-known terminological re-source EuroVoc and Banking thesaurus created for the Central Bank of the Russian Federation. Both resources are used in indexing and retrieval of doc-uments in real information-retrieval systems. 3.1 EuroVoc Thesaurus and Europarl Text
For the English part of our study we took Eu-roVoc thesaurus and Europarl parallel corpus. Eu-roVoc is an official thesaurus of the European Union and is intended for manual indexing of EU parliamentary documents. It is a multidisciplinary thesaurus covering the EU activities and contain-ing terms in 22 languages of the EU. The En-glish version of EuroVoc comprises 15161 terms ( http://eurovoc.europa.eu/drupal ).

The Europarl parallel corpus was extracted from the proceedings of the European Parliament ( http://www.statmt.org/europarl/ ).
 The English part includes almost 54 mln. words.
In fact, EuroVoc thesaurus is intended just for the description of Europarl documents. There-fore, we can model how EuroVoc thesaurus could be developed from the Europarl corpus. EuroVoc represents a broad socio-political domain, and its language is close to general English. 3.2 Banking Thesaurus for the Central Bank For the Russian part of our study we took the Banking thesaurus created for the Central Bank of the Russian Federation. It is used in an information-retrieval system for indexing, search and vizualization of information and as a basis for text categorization. The thesaurus includes about 15 thousand terms and comprises the terminology of banking activity, banking regulation, monetary politics and macroeconomics.
 As an appropriate text collection we took 10422 Russian articles from various on-line magazines: Auditor, RBC, Banking Magazine, etc. These documents contain almost 15.5 mln. words.

Since the banking thesaurus is used in real in-formation retrieval tasks, we can model how it could be developed from the banking text corpus. In contrast to the broad socio-political domain of EuroVoc, this thesaurus represents relatively nar-row banking domain, and its language is not so close to general language.
In our study we investigated single-word and two-word term extraction separately in order to have possibility to compare corresponding extrac-tion models. As single-word term candidates we consider only Nouns and Adjectives (for Russian language) and Nouns (for English language); as two-word candidates we consider only Adjective + Noun and Noun + Noun (for Russian language) and Adjective + Noun , Noun + Noun , and Noun + 70 of + Noun (for English language) since they cover the majority of terms.

We use several types of enough known features for term extraction proposed in previous works and relatively new topic-based features proposed in (Bolshakova et al., 2013). 4.1 Traditional Features
The first type of traditional features is frequency-based features . The main assumption is that terms differ in their frequency and the distri-bution from other words in the target corpus. We consider the following 8 features: Term Frequency in the collection (tf), Document Frequency (df), TF-IDF, TF-RIDF (Church and Gale, 1995), Do-main Consensus (Sclano and Velardi, 2007), Term Contribution, Term Variance Quality, Term Vari-ance (Liu et al., 2005).

The second type of traditional features is based on the target and reference corpora and sup-poses that term frequencies in the target and ref-erence corpora should be significantly different. We consider 9 such features, namely: Weird-ness (Ahmad et al., 1999), corpus-based TF-IDF (where TF is taken from the target corpus, and IDF is taken from the reference corpus), Rele-vance (Pe  X  nas et al., 2001), Contrastive (Basili et al., 2001) and Discriminative (Wong et al., 2007) Weights , Lexical Cohesion (Park et al., 2002), Reference Weight, KF-IDF (Kurz and Xu, 2002), Loglikelihood (Gelbukh et al., 2010). In our study n-gramm statistics from British Na-tional Corpus ( http://www.natcorp.ox. ac.uk/ ) and Russian National Corpus ( http: //www.ruscorpora.ru ) were used as statis-tical data of a reference corpus for English and Russian collections correspondingly.

The third type of traditional features comprises word-association measures estimating mutual correlation of term candidate usage. They are primarily intended for two-word collocation ex-traction and are not applicable for single-word term extraction. We consider 19 word association measures: Mutual Information (MI) (Church and Hanks, 1990), Augmented MI (Zhang, 2008), Cu-bic MI (Daille, 1995), Normalized Pointwise MI (Bouma, 2009), True MI, Dice Coefficient (DC) (Smadja et al., 1996), Modified DC, Generalized DC (Park et al., 2002), T-Score, Z-Score, Sym-metric Conditional Probability (Lopes and Silva, 1999), Simple Matching Coefficient, Kulczinksy Coefficient, Ochiai Coefficient, Yule Coefficient, Jaccard Coefficient (Daille, 1995), Chi Square, Loglikelihood Ratio (Dunning, 1993), Gravity Count (Daudarvi  X  cius and Marcinkevi  X  cien  X  e, 2005).
The last type of traditional features is context-based features that account for phrases encom-passing term candidates and their left and/or right context. We define a context of a term candidate as the bounds of encompassing noun phrases. In our study 11 known context-based features were con-sidered: C-Value, NC-Value (Frantzi and Anani-adou, 1994), MNC-Value, Token-LR, Token-FLR, Type-LR, Type-FLR (Nakagawa and Mori, 2003), Sum3, Sum10, Sum50, Insideness (Loukachevitch, 2012).

Besides, we propose a novel context-based fea-ture: Modified Gravity Count (MGCount) . It is based on Gravity Count association measure described in (Daudarvi  X  cius and Marcinkevi  X  cien  X  e, 2005). MGCount for xy phrase is calculated as follows:
M GCount = log where f ( x ) is the frequency of x , f ( y ) is the fre-quency of y , f ( xy ) is the frequency of xy phrase, l ( x ) is the number of different words to the left of x , and r ( y ) is the number of different words to the right of y ; l ( x ) and r ( y ) are considered only within the bounds of encompassing noun phrases. second component of the sum), thus the measure was transformed from the association measure to the context one. 4.2 Topic-Based Features
The next type comprises features based on so-called topic models (Blei and Lafferty, 2009). Topic models are intended to describe texts in terms of their topics, they determine, which top-ics are related to each document, and which words (or phrases) form each topic. In fact, each topic is represented as a list of frequently co-occurring words (or bigrams) ordered by descending degree of belonging to it. As an example, the first five words and bigrams from the top of four randomly selected topics of the English corpus along with 71 their probabilities of belonging are presented in the Table 1.

Typically, there are two types of topic models: non-probabilistic ones that are based on hard clus-tering methods (K-Means, hierarchical agglom-erative clustering, etc.) and probabilistic ones (PLSI, LDA, etc.) that represent each document as a mixture of topics and each topic is considered as a probabilistic distribution over words (Blei and Lafferty, 2009), (Bolshakova et al., 2013).
The topic-based features are relatively new and are obtained by revealing topics in the target text corpus. These features account for the idea that domain terms should usually correspond to some subtopics of the domain. As it was shown that NMF (Non-Negative Matrix Factorization) algo-rithm with KL-divergence minimization is the best topic model in terms of terminology extraction (Bolshakova et al., 2013), we applied it to reveal subtopics, as well as probabilities in them. Ba-sically, given a non-negative term-document ma-trix V , this algorithm tries to find non-negative term-topic matrix W and topic-document matrix H , such that V = W H . We consider the ver-sion of NMF that minimizes Kullback-Leibler di-vergence D ( V || W H ) (Lee and Seung, 2000).
We consider the following 7 topic-based fea-tures: Term Frequency, TF-IDF, Domain Consen-sus, Maximum Term Frequency (Bolshakova et al., 2013), Term Score (TS) (Blei and Lafferty, 2009), TS-IDF, Maximum Term Score . Most of these features are extensions of the standard frequency-based features applied to the revealed subtopics, considering probabilities of the term candidates in topics as frequencies (cf. Table 2; P i ( w ) denotes a probability of the term candidate w in the topic i , and K is the number of topics).

We also used 6 single-topic document features (documents are regarded as separate topics). In fact, we used all above-mentioned topic-based features except Domain Consensus, since this fea-ture is already considered in the section of tradi-tional frequency-based features (cf. section 4.1). 4.3 Other Features
Other features considered in our study include:  X  5 Linguistic features: Ambiguity (determines  X  Features for term candidates that play subject  X  2 features for term candidates that are in  X  Average position of the first occurrence in
Thus, 27 features belong to this group. To sum up, the full list of features comprises 69 fea-tures for single-word candidates and 88 features for two-word term candidates.
We studied models for single-word and two-word term extraction from two above-described corpora: Russian banking electronic magazines, and English part of parallel corpus Europarl.
To extract single-word and two-word term candidates from these corpora, documents were processed by morphological analyzers. Thus, for English corpus we used Stanford POS tagger ( http://nlp.stanford.edu/ software/corenlp.shtml ), while for Russian corpus we used our own morphological analyzer. Besides, from the set of extracted English term candidates we excluded words from the stop list created for the experiments ( other, another, that, this, those, mrs, sir, etc. ), and word pairs including stop-words were excluded as well.
Having extracted term candidates, we trained combined models comprising the above-described types of term features. The features were com-bined by Gradient Boosting machine learning al-gorithm, which proved to be the best one in our study. Namely, we used an open-source realization of this algorithm from http:// scikit-learn.org . It is well-known that Gradient Boosting has a lot of parameters that need to be tuned. So, in all experiments we fixed all parameters, except the number of trees and maximum allowed depth of trees, that were tuned in each experiment individually. Besides, for training and evaluation four-fold cross valida-tion was applied, which means that every time the training set was three-quarters of the whole list while the testing set was the remaining part.
A term extraction model has to find the best or-der, where real terms should be located at the be-ginning of the ordered list of term candidates. As an evaluation measure, we used Average Precision (AvP) often applied as a measure for term extrac-tion (Zhang, 2008), (Bolshakova et al., 2013). It is defined for a set D of all term candidates with a subset of approved ones D q  X  D as follows:
AvP ( D ) = where r i = 1 if the i -th term  X  D q and r i = 0 otherwise.

At the first step of experiments we separately studied term extraction models for single-word and two-word terms. As baselines we considered several well-known features: Weirdness, TF-IDF, C-Value for single-word models and TF-IDF, C-Value, Mutual Information for two-word ones. In the Figures 1, 2, 3, 4 plots of AvP on various num-bers of most frequent candidates are presented for these baselines, the best single feature and the re-sulted model combined by Gradient Boosting.
As we can see, the best single feature for single-word terms turned out to be a topic-based feature (either Maximum Term Score or Maximum Term Frequency ), the performance of these features is considerably better than well-known baselines. So it seems that for single-word terms their relation to a domain subtopic is important.

The best single feature for two-word terms was found to be the novel context-based feature Mod-ified Gravity Count . Besides, in all cases we can see the huge improvement of the combined model performance compared to well-known baselines and best single features.

At the second step of experiments we tried to determine the contribution of each above-described group of features to the whole com-bined model. We fixed the number of most fre-quent term candidates to 5000, excluded each of the following groups separately from the whole list: frequency-based features; features, based on the reference corpus; word association measures; context-based features, and topic-based features. The results of combining the remaining features by Gradient Boosting are presented in the Table 3.
As we can see, features, which are based on the reference corpus, give the most significant contri-bution to the two-word term extraction models re-gardless of the subject domain and language.
Besides, the use of word association measures does not improve the quality of extraction of two-word terms. The latter conclusion contradicts the assumption of numerous studies that association measures should be useful for multi-word term extraction (Zhang, 2008), (Daille, 1995), (Kurz and Xu, 2002). From the other side, this con-clusion can be quite evident because, for exam-ple, EuroVoc includes a lot of terms looking as compositional phrases with free separate usage of components (as European party, European idea, economic consequence etc.). Introduction of such terms into an information-retrieval thesaurus is possible due to multiple principles of term in-clusion in information-retrieval thesauri (Z39.19, 2005).

At the last step of experiments we investigated both models for single-word and two-word term candidates together. We created a unified model for both types of term candidates, taking into ac-count all features except association measures and obtaining as a result the unified list of candidates.
Then we created specific models separately for single-word and two-word term candidates. As the models are specialized, they can be potentially more efficient. We summed up resulted lists of extracted terms according to their probability val-ues generated by Gradient Boosting, and in such a way obtained the summed-up list of term can-74 didates. We should notice that in the case of the unified model there is more data to train it, so this model can be potentially very efficient too.
The comparison of AvP for these two models (for both corpora) shows that summed-up model slightly outperforms the unified one  X  cf. Figure 5.
In addition, as an example of the extracted term candidates, we present in the Table 4 the first 10 elements from the top of the term candidates lists created by unified models for Russian and English corpora (the elements in italics are real terms).
The resulting unified models may be too com-plex in the number of applied features. Some of them may be redundant for Gradient Boosting and have no use in the models, make their training harder. In order to exclude them we applied a step-wise greedy algorithm Add for selecting the most significant features.

The algorithm starts with the empty set of fea-tures, and then at each step it adds the feature that maximizes the overall Average Precision, un-til there is any improvement between successive iterations. As a result, the combinations of only 13 features (out of total 69 features) were found for both corpora (see Table 5). We grouped simi-lar features in the same rows of the table.
Since there are representatives of all above-described groups in both found subsets of features, we conclude that each such group is significant for unified models of term extraction regardless of the scope and language. Besides, we can see that short models for both thesauri are quite similar.
In this paper we modelled single-word and two-word term extraction for the specific type of ter-minological resources  X  information-retrieval the-sauri. Our experiments revealed features signifi-cant for extraction of single-word and two-word terms in the broad EuroVoc and relatively narrow banking domains. We showed that the best fea-tures for single term extraction in both cases are relatively new topic-based features, based on pre-liminary clustering of words in the target text col-lection. The context-based features are the most important for two-word term extraction.

The interesting result of our study is that the use of association measures does not improve the quality of term extraction models intended for information-retrieval thesaurus construction. It was also proved that the unified model can be ap-plied to both single-word and two-word term ex-traction.
 The work is partially supported by Dmitrii Zimin Dynastia Foundation with financial support of Yandex founders. 75 and Management of Monolingual Thesauri NISO. 76
