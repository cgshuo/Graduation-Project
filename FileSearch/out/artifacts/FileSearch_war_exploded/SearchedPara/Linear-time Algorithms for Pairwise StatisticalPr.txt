 previous work has sought to reduce the number of distance com putations needed. efficiently.
 distances involved are large enough, with appropriate accu racy guarantees for some methods. kd  X  in order to prune references that have little effect on the query. Though conjectured to have O ( N ) growth, they lack rigorous, general runtime bounds. Q neighbors problem in O ( N ) .
 proofs of worst-case O ( N ) runtimes for the following all-query problems: the same section, we apply the kernel summation result to the N -body simulation problem from computational physics, and we draw some conclusions in Sect ion 5. denote the set of nodes at scale i . For all scales i , the following invariants hold: consists of infinitely many levels C and the level C or has a child other than a self-child.
 Structural properties. The intrinsic dimensionality measure considered here is th e expansion dimension from Karger &amp; Ruhl, 2002 [6] defined as follows: Definition 2.1. Let B around a p  X   X  . Then, the expansion constant of  X  is defined as the smallest c  X  2 such  X  B
 X  ( p, 2 )  X   X  c  X  B  X  ( p, )  X   X  p  X   X  dimension) of  X  is given by d We make use of the following lemmas from Beygelzimer et.al., 2006 [1] in our runtime proofs. Lemma 2.1. (Width bound) The number of children of any node p is bounded by c 4 . Lemma 2.2. (Growth bound) For all p  X   X  and &gt; 0 , if there exists a point r  X   X  such that 2 &lt; d ( p, r )  X  3 , then  X  B ( p, 4 )  X  X  X  1 + 1 Lemma 2.3. (Depth bound) The maximum depth of any point p in the explicit representation is O ( c 2 log N ) .
 from q . The following theorem provides a runtime bound for the sing le point search. of q can be found in time O ( c 12 log N ) .
 Batch Query: The dual tree algorithm for all-nearest-neighbor ( FindAllNN subroutine in Algo-of the query-reference pair ( Q ,  X  ) is the maximum number of descends in S between any two descends in T .
 this definition, we can prove the main result of this section.
 Theorem 3.1. Given a reference set  X  of size N and expansion constant c FindAllNN subroutine of Algorithm 1 computes the nearest neighbor in  X  of each point in Q in O ( c 12  X  c 4 Q N ) time.
 Proof. The computation at Line 3 is done for each of the query nodes at most once, hence takes O (max i  X  R i  X  X  X  N ) computations.
 Algorithm 1 Single tree and batch query algorithm for Nearest Neighbor s earch and Approximate Kernel summation FindNN (  X  -Tree T , query q ) 3: R = { C X  X ldren ( r ): r  X  R i } 6: return arg min FindAllNN ( Q -subtree q 3: else if j &lt; i then 6: FindAllNN ( q j , R i  X  1 ) 9: end if KernelSum (  X  -tree T , query q ) 3: R = { C X  X ldren ( r ): r  X  R i } 6: end for descends between any two reference descends is upper bounde d by and the number of explicit algorithm is at most O ( c 4 Since at any level of recursion, the size of R is bounded by c 4 maximum depth of any point in the explicit tree is O ( c 2 encountered in Line 6 is O ( c 4+2 duplication, and the duplication of any reference node is up per bounded by c 4 O ( c 4 Q c 6  X  max i  X  R i  X  log N ) in the whole algorithm.
 O ( N ) time.
 Consider any R and d &gt; 2 i +2 , d ( q If in ball B ( r, 2 i  X  2 ) can contain at most one point in C Thus, the algorithm takes O ( c 6 c , the FindAllNN subroutine of Algorithm 1 has a runtime bound of O ( c 16 N ) . Proof. In the monochromatic case,  X Q X  =  X  X  X  X  = N , c follows. approximating schemes listed below: Definition 4.1. An algorithm guarantees absolute error bound , if for each exact value f ( q q  X  X  , it computes  X  f ( q i ) such that  X  f ( q i )  X  f ( q i )  X  N . Definition 4.2. An algorithm guarantees relative error bound , if for each exact value f ( q q  X  X  , it computes  X  f ( q i )  X   X  such that  X  f ( q i )  X  f ( q i )  X   X  f ( q i )  X  . Approximate kernel summation is more computationally inte nsive than nearest neighbors because 4.1 Single Tree Approximate Kernel Summations Under Absolu te Error KernelSum subroutine of Algorithm 1. The following theorem proves tha t KernelSum produces an approximation satisfying the absolute error.
 Proof. A subtree rooted at r  X  C for each contribution computed exactly for r  X  R approximate sum  X  f ( q ) will be within N of the true kernel sum f ( q ) . The following theorem proves the runtime of the single-quer y kernel summation with smooth and monotonically decreasing kernels using a cover tree.
 the kernel summation at a query q approximately up to absolute error with a runtime bound of = log 2 K (  X  1) ( ) , =  X  log 2  X   X  , i 1 = j log 2  X  such that R because of the concavity of the kernel function K (  X  ) . Now, interval [ a, b ] for the given argument x . For r  X  R m which implies that Similarly, for r  X  R u Note that 0  X  K  X  ( d ( q, r ))  X  K  X  (  X  ) for d ( q, r ) &gt;  X  + 2 i , which implies that  X  thus i  X  j log i Case 1: i &gt; i Trivially, for r  X  R nel function to obtain: d max &lt; K (  X  1) Case 2: i = i Let =  X  log et.al., 2006, the running time is bounded by: 4.2 Dual Tree Approximate Kernel Summations Under Absolute Error query subtree. Additionally, every query node q mulates the postponed kernel contribution for all query points under the subtree q theorem proves the correctness of the AllKernelSum subroutine of Algorithm 1. Theorem 4.3. For all q in the in the query set Q , the AllKernelSum subroutine of Algorithm 1 computes approximations  X  f ( q ) such that  X   X  f ( q )  X  f ( q )  X  X  X  N . Proof. Line 9 of the algorithm guarantees that  X  r  X  R  X  R for all q  X  L ( q value and the result follows by the triangle inequality. given by the following theorem: Theorem 4.4. Let  X  be a reference set of size N and expansion constant c query set of size O ( N ) and expansion constant c the absolute error bound in time O ( N ) .
 Proof. We first bound max  X  R such that R niques shown for the single-tree case to show that max  X  R approximate kernel summation. 4.3 Approximations Under Relative Error error criterion given in Definition 4.2.
 except that the definition of R level i ) needs to be changed to satisfy the relative error constrain t as follows: where f ( q ) is the unknown query sum. Hence, let d max = max Note that d max can be trivially upper bounded by: d max  X  d ( q, r the scale of the root of the reference cover tree in the explic it representation. error in O (log N ) time.
 Proof. A node r  X  C for each contribution computed exactly for r  X  R relative error is an instance of the absolute error, the algo rithm also runs in O (log N ) . tree to its descendants. Hence R where d ( q algorithm follows naturally from Theorems 4.4 and 4.5. Sum subroutine of Algorithm 1 with Line 11 redefined as Eq. 2 compu tes an approximate kernel rules that generate R accelerating this using an alternative method [9, 10, 11] th at is preferable in practice. 4.4 N -body Simulation N -body potential summation is an instance of the kernel summa tion problem that arises in com-f ( q ) = P r 1 Corollary 4.2. Given a reference set  X  of size N and expansion constant c , an error value and summation at a query q with error in O (log N ) time.
 Proof. Let d min = min The effective kernel K effective kernel is K  X  X  and convex otherwise, so the second derivative agrees at d = d min . Note that K applying the same theorem on the KernelSum subroutine of Algorithm 1 with the aforementioned kernel, we prove the O (log N ) runtime bound.
 The runtime analysis for the batch case of the algorithm foll ows naturally. Corollary 4.3. Given a reference set  X  of size N and expansion constant c size O ( N ) and expansion constant c approximates the potential summation  X  q  X  X  up to error with a runtime bound of O ( N ) . Proof. The same effective kernel as Corollary 4.2 is used, except th at d min = min with K ( d ( q, r )) = 1 /d ( q, r ) is equivalent to running the algorithm with K general bichromatic case of the all-query problems.
 [3] K. Deng and A. W. Moore. Multiresolution Instance-Based Learning. pages 1233 X 1242. [4] D. Lee and A. G. Gray. Faster Gaussian Summation: Theory a nd Experiment. In Proceedings [9] A. G. Gray and A. W. Moore. Nonparametric Density Estimat ion: Toward Computational [11] D. Lee and A. G. Gray. Fast High-dimensional Kernel Summ ations Using the Monte Carlo
