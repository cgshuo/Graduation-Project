 Being different from English, there are no delimiters in Chinese text to indicate words. The words are potentially existent, in a certain sense, and important for further NLP tasks or information retrieval (IR). The fundamental task to segment characters in Chinese text in to words is named as Chinese word seg-mentation (CWS).

Xue [1] presented a maximum entropy (ME) based model for CWS as char-acter tagging using local context as evidence. To avoid the weakness of these local models, conditional random field ( CRF) models [2], perceptrons [3] and other global discriminative models [4] were introduced. Global models, namely sentence models, deal with each sentenc e or paragraph as a whole. Indirectly, long-distance relations can be taken in to account. These models get good per-formance in the literature. However, th e learning process of these models are time-consuming.

Language models, which estimate the generating probability of each sentence, are used for CWS. As generative models, language models can learn from cor-pora much faster than discriminative models, and can be easily adapted for unsupervised learning [5], [6].

In this paper, we present a local generat ive model, which has not been explored before. The relationship between our model and existent models is shown in Table 1.

Following the notation of [7] and [8], the interval between two successive characters in Chinese can be classified into two types, namely separated and combined . A separated interval indicates that those two characters belong to two separated words, while a combined interval indicates that those two characters belong to the same words. The separated intervals are therefore recognized as word boundaries.

Our local generative model estimates the probability of four successive char-acters, called the context ,giventhetypeoftheintervalinthemiddleofthese characters. And the model predicts the i nterval type separately and only based on the corresponding context.
 We also provide a method to generate word candidates based on our model. The word candidates are the words that an input sentence could contain at some segmentation granularity. Comparing to using words, using word candidates can improve the performance of IR.

Notice that the traditional output of CWS may not fit the purpose of Chi-nese IR. Due to the lack of explicit word boundaries in Chinese text, there is no commonly accepted definition of word s among Chinese speakers and even linguists. The difference is mainly about the segmentation granularity in some situations. That causes the granularity of the words in queries may differ from the ones in the CWS. Indexing only segmented Chinese words can not get a good performance for Chinese IR.

A binary tree based segmentation representation for Chinese IR was intro-duced by [9]. The result shows that indexing all word candidates at different granularity improves the performance of IR. However, the method proposed by [9] needs a specific annotated corpus and a ranking-based model.

Using the output of our model, we can get all word candidates without any specific annotation on the training corpus. All the word candidates of a sentence correspond to all the nodes in the binary tree of the corresponding sentence. So indexing word candidates benefits Chinese IR as well as the binary tree based method does.
The experiments are on SIGHAN bakeoff 05 [10] corpora in both supervised and unsupervised CWS.

The contributions of this paper are two-fold. (1) We introduce a local genera-tive model. This simple model has a competitive performance in both supervised and unsupervised CWS, while it enjoys a faster learning process and has the abil-ity to make use of larger resources. (2) We find a general way to generate all the possible segmented word at any segmentat ion granularity, which may benefit IR and other applications. 2.1 The Model We denote the type of interval I by y , which can be either a separated interval s or a combined interval c , and denote the corresponding context consisting of four treated as a sample ( x, y ). Examples can be found in Table 2.

The aim of our model is to estimate the generative probability of the samples we use is of the generative form, we call our model local and generative.
The priori p ( y ) is easy to be estimated appropriately by the maximum like-lihood estimation. And there could be various way to estimate the likelihood p ( x | y ), which is the more complicated part. The choice needs to show the rela-tionship between the interval type and the context, and to be dependent on the size of the corpus.

In supervised learning, the estimations we used for both separated interval and combined interval are in the same form The probabilistic graphical model representation of this model is shown in Figure 1. These two characters, l 1 and r 1 , in the middle are first generated together according to the interval type, for they are strongly dependent on the interval type. Then two distinct trigram models are used to generate the outer two characters l 2 and r 2 respectively.
 Unlike the n-gram language models, these probabilities in the right side of Equation 1 are distinct and estimated individually. 2.2 Predicting estimated. In the predicting phrase, which can be roughly regarded as a binary classification task, for each new context x  X  in the training data, a predicted interval y  X  can be obtained by the following approach based on Bayesian decision theory. This approach is for both supervised and unsupervised learning.
We define a discriminate function g ( x  X  )as: The interval y  X  to be predicted can be got by: 2.3 Unsupervised Learning and the Rules Following Mochihashi et al. ([6]), we use Gibbs sampling method in the unsu-pervised learning to learn the model from the raw text.
 With slight changes, the local generative model is suitable for the unsupervised Chinese word segmentation, which is usually not feasible for most discriminative models. We also slightly modify the Gibbs sampling method. Details are in the next section.

Three additional rules used in the sampling are introduced below. Wherever these rules are applied, the type of int ervals can be determined without any other processes.

The first rule is that any interval is the separated interval, if one of the two immediate characters of this interval i s a Chinese character, and the other is a punctuation mark. This idea is from [11], where punctuation marks in raw Chinese text were used as the implicit annotations for unsupervised CWS. So we can say that the punctuation marks make unsupervised CWS semi-supervised. For example, the interval in  X ,  X   X  X saseparatedinterval.
Besides, we create the second rule based o n Arabic numerals, that any interval is the combined interval, if two immediate characters of this interval are both Arabic numerals. For example, the interval in  X 1 0 X  of  X   X  X  X  X  X  X  X  X  X  X  X  X  X   X   X  is a combined interval.

Finally, The third rule is that any intervals at the beginning or at the end of the sequences are all separated intervals.
 Table3givesusanoverviewofhowtherulesworkontherawChinesetext.
 We found that nearly 1/5 intervals in the raw Chinese text can be determined by the three simple rules in the first three corpora.
 3.1 Interpolated Kneser-Ney Smoothing Unlike the n-gram language model, where we estimate different probabilities for the characters in differen t positions in the context x , similar smoothing technique is needed in the probability estimation process since the size of corpus is limited. The final performance of this model is sensitive with the smoothing technique.
Here we only discuss the smoothing method for supervised learning. The smoothing method for unsupervised learning is similar. There are two kinds of smoothing needed for the probabilities for supervised learning. One is for the language model smoothing method called interpolated Kneser-Ney smoothing to smoothen the probabilities. The notation is mainly based on the technical report by [12].

The probability of x with the interpolated Kneser-Ney smoothing is This probability is for both separated and combined intervals, only with different contexts to be estimated.
 First, we introduce the smoothed probabilities of the unigram given a bigram, P The probability of character unigram c given the context n-gram u is where C u is the count of certain sequence u appearing in the training data, t  X  = |{ c | c u c &gt; 0 }| is the number of different chara cters following the context u , d i is the discount,  X  ( u ) is the postfix of the context u which has length of | u | X  1, and P IKN
For the back-off probabilities, modified counts are used and defined as follows: The back-off probabilities can be defined recursively until the context u is  X  . When the context u is  X  , we use a maximum entropy estimation as the final back-off probability: The maximum entropy probability is where N is the size of the set of all possible characters.
 Then we introduce the smoothed probability of the bigram, P IKN bi ( l 1 ,r 1 ).
When apply the interpolated Kneser-N ey smoothing method to estimate the probability of bigram ( l 1 ,r 1 ) in the middle, an adapted version is used as: assumes that l 1 and r 1 are generated individually. This is quite different from the ordinary n-gram language model back-off method.

The discount d i can be optimized by holding out a development set or be assigned empirically. The whole learnin g process for the generative model is to process the samples of contexts and inte rvals once, without any iteration to optimize any coefficients of the model as the maximum entropy models, CRFs or perceptron methods do. This makes the learning process of our model much faster than those of the discriminative models. 3.2 Adapted Gibbs Sampling In unsupervised learning, we modify our model and the Gibbs sampling method to learn a model from the raw corpus.

First of all, we slightly change the generative model when we apply this model to unsupervised learning for CWS, for the convergence will be slow and difficult if we use the original model.
The probabilities of the model we use for unsupervised CWS are where the probability of the combined intervals is the same as the one for the supervised learning, but the probability of the separated intervals ignores the relationship between these two characters from different words, for this relation-ship is hard to learn from the raw corpus.

The adapted algorithm of Gibbs sampling [13] is described in Figure 2. There are two changes from the original Gibbs sampling algorithm. The function add() remove() and sample() are the operations for the estimation of the p ( x, y )in Gibbs sampling.

First, three rules introduced in Section 2 are used in this algorithm. The interval types of the samples that those three rules can be applied is not sampled but assigned directly a ccording to those rules.

Another change is for the priori p ( y ) when we apply the sampling. The Gibbs sampling may cause the priori probability we learn from the data set to be quite different from the real value that we can get from the gold standard segmented corpus, even if we set a priori to the p ( y ). Thus, we adapt the Gibbs sampling in order to fix the p ( y ) that the Gibbs sampling could converge to. The total number of intervals n and the number of sampled separated intervals n s are The strength is controlled by k .
 3.3 Word Candidates at Different Granularity Using the output of our model, we can get all word candidates, at different granularity without any specific annotation on the corpus. The aim of using word candidates is to improve the performance of IR.

For an input sentence c 1 c 2 ...c n , we can calculate the degree of confidence q ( c i ) that there is a word boundary after c i : A substring c a ...c b of the sentence is a word candidate if and only if: That is, if two intervals right before and after a substring are more likely to be the word boundary than any inner intervals in the substring, this substring can be identified as a word at some granularity.

We can define the word as a substring c a ...c b such that The only difference is that there is a zer o in the middle of the inequations. This means that words are only generated at a certain granularity which the training corpus has. Interestingly, we can say that the definition of word candidates is the generalization of the definition of words.
 a lot of knowledge) , we have q (  X  )=  X  2 . 517, q (  X  )=  X  2 . 194, q (  X  )=2 . 027, q (  X  )=1 . 791 and q (  X  )=  X  1 . 644. For this sequence, all the word candidates are  X   X   X ,  X   X   X ,  X   X   X ,  X   X   X ,  X   X   X ,  X   X   X ,  X   X  X   X ,  X   X  X  X   X ,  X   X  X  X   X ,  X   X  X  X  X   X  X nd X   X  X  X   X  X  X  X   X .

Generally, all the word candidates of a sentence correspond to all the nodes in the binary tree [9] of the same sentence. So indexing word candidates benefits Chinese IR as well as the binary tree based method does. 4.1 Experiment Setup All our experiments of supervised and unsupervised learning are on four corpora in SIGHAN bakeoff 05. Two corpora provided by Academia Sinica (AS) and City University of Hong Kong (CTU) are in traditional Chinese, while the other two corpora provided by Peking University (PKU) and Microsoft Research (MSR) are in simplified Chinese. The overview of these corpora are in Table 4. The OOV (out of vocabulary) rate is the rate of the words in the test set that do not appear in the training set.
 The discounts in the interpolated Kneser-Ney smoothing need to be assigned. Empirically, the discounts d 0 , d 1 and d 2 are assigned to 0.25 0.85 and 0.95 respectively for the smoothing of the probabilities for unigram given bigram, and d 0 and d 1 are assigned to 0.4 and 0.75 respectively for the smoothing of the probabilities for bigram. Those values are for all corpora and both supervised and unsupervised learning. Notice that the discounts could also be optimized by holding out a development set.

The f1 measure based on the precision and recall [10] is the commonly used measurement for the evaluation for CWS. We use the f1 measure for our evalu-ation and comparison. 4.2 Supervised Learning The learning process for generative models is much faster than the one for dis-criminative models. The whole learning and predicting process for each corpus terminated in minutes.

Results are shown in Table 5. Here we on ly concern about the f1 measure for each corpus. It shows that the performances of our model are competitive with the best performances reported by SI GHAN [10], all except one of which are based on discriminative models. Especially on CTU corpus, our performance is better than the best one reported by SIGHAN. And the performances of our model are better than those of the language model [6], which is also a generative model.

There is an interesting observation o n the types of segmentation errors. We can adjust the output by replacing some words with corresponding word candi-dates at finer or coarser granularity according to the gold standard result. The performance could be significantly improved, which is shown in the parentheses of Table 5.

Fig. 3 shows the distributions of g ( x ) for the separated and combined intervals on the MSR corpus in supervised learning. This figure indicates that the g ( x ) of misclassified samples centers near 0. For the predicting approach is based on the Bayesian decision theory, the value g ( x ) can be recognized as the degree of confidence. A majority of the types of the intervals could be predicted with high confidence. 4.3 Unsupervised Learning For unsupervised learning, we directly assign  X  in the adapted Gibbs sampling to the value calculated based on the training corpus. In fact, empirical experiments show that this value is not the best estimation for  X  .

Table 6 shows the results of experiments on unsupervised learning. The models are learned in two ways, one is to normally use training data, the other is to use test data only for the Gibbs sampling. Figure 4 shows two learning curves of both ways on MSR corpus. The performances of our model are comparable with the state-of-the-art unsupervised method reported by [6].

Similar phenomena occur in the unsupervised learning if we adjust the output using word candidates. The f1 can significantly increase if the result is at a good granularity.
 The experiments on SIGHAN bakeoff 05 data show that our model are bet-ter than the language model in the supervised learning, and with much faster learning process comparing to discriminative models like CRFs and perceptron. The f1 measure in one of four corpora is better than the best one reported in SIGHAN bakeoff 05. The performance of unsupervised learning of this model is comparable with the state-of-the-art.

In our model, comparing to existent linear discriminative models, information of character trigrams are used, when we calculate the probabilities of l 2 and r 2 . And the smoothing technique makes our formulas nonlinear, although there X  X  no coefficients optimization. This may be the reason for why our model can get better performance than the common exp ectation for the gen erative models. In addition, four successive characters as the context are proved to be enough for determine a interval type in most of the cases.

For the sake of IR, we define the word candidates for CWS. A previous study already showed that it benefits the performance of IR. The word candidates can be got by our model naturally and directly. Notice that they can also be got by global models such as CRFs, if we redefine the degree of confidence q ( c i )as some marginal probability.
 Another interesting result indicated by our experiment is that the errors of CWS are mainly caused by the segmentat ion granularity difference, without which the performance will increase significantly.

We believe that larger training data or even larger raw data will be helpful to improve the performance. Our model with the advantage shown in this paper is suitable for learning from larger res ources. We will work on these issues. Acknowledgments. This research is supported by the Boeing-Tsinghua Joint Research Project  X  X obust Chinese Word Segmentation and High Performance English-Chinese Bilingual Text Alignment X .

