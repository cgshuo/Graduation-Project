 This paper addresses the problem of making text mining results more comprehensible to hum anities scholars, journalists, intelligence analysts, and other rese archers, in order to support the analysis of text collecti ons. Our system, FeatureLens visualizes a text collection at several levels of granularity and enables users to explore interes ting text patterns. The current implementation focuses on frequent itemsets of n-grams, as they capture the repetition of exact or similar expressions in the collection. Users can find meani ngful co-occurrences of text patterns by visualizing them within and across documents in the collection. This also permits users to identify the temporal evolution of usage such as in creasing, decreasing or sudden appearance of text patterns. The interface could be used to explore other text features as well. Initial studies suggest that FeatureLens helped a literary sc holar and 8 users generate new hypotheses and interesti ng insights using 2 text collections. H.5.2 Graphical user interfaces (GUI), H.2.8 Data mining. General Terms: Algorithms, design, experimentation, human factors, measurement. Keywords: Text mining, user interface, frequent closed itemsets, n-grams, digital humanities. development of digital libraries, researchers can easily search and retrieve large bodies of texts, images and multimedia materials online for their research. Those archives provide the raw material memories to find  X  X nteresting X  facts that will support or contradict existing hypotheses. In the fields of the Humanities, computers are essentially used to access to text documents but rarely to support their interpretation and the devel opment of new hypotheses. Some recent works [4, 11] addressed this problem. One approach, techniques together with a meaningful visualization of the text annotations. For example Compus [4] supports the process of finding patterns and exceptions in a corpus of historical document by visualizing the XML tag a nnotations. The system supports filtering with dynamic queries on th e attributes and analysis using XSLT transformations of the documen ts. Another approach is to use data-mining or machine learning al gorithms integrated with visual interfaces so that non-specialists can derive benefit from these algorithms. This approach has been successfully applied in the literature domain in one of our prior project [11]. Literary scholars could use a Naive Bayesian classifier to determine which letters of Emily Dickinson's correspondence cont ained erotic content. It gave users some insights into the vo cabulary used in the letters. While the ability to search for keywor ds or phrases in a collection is now widespread such search only marginally supports discovery because the user has to decide on the words to look for. On the other hand, text mining results can suggest  X  X nteresting X  patterns to look at, and the user can then accep t or reject these patterns as interesting. Unfortunately text mining algorithms typically return large number of patterns which are difficult to interpret out of context. This paper describes Feat ureLens, a system designed to fill a gap by allowing users to interpret the results of the text mining thru visual exploration of the patterns in the text. Interactivity facilitates the sorting out of unimportant information and speeds up the task of analysis of large body of text which would otherwise be overwhelming or even impossible [13]. FeatureLens aims at integrating a set of text mining and visualization functionalities into a powerful tool, which provokes new insights and discoveries. It supports discovery by combining the following tasks: getting an overview of the whole text collection, sorting frequent pa tterns by frequency or length, searching for multi-word patterns with gaps, comparing and contrasting the characteristics of different text patterns, showing __________________________ 
A video and an online demonstration are available from http://www.cs.umd.edu/hcil/textvis/featurelens/ patterns in the context of the text where they appear, seeing their distributions in different leve ls of granularity, i.e. across collections or documents. Availa ble text mining tools show the repetitions of single words within a text, but they miss the support for one or more of the aforemen tioned tasks, which limits their usefulness and efficiency. We start by describing the literary analysis problem that motivated our work and review the related work. We then describe the interface, the text mining algorithms, and the overall system architecture. Finally we present several examples of use with 3 collections and discuss the results of our pilot user studies. This work started with a literary problem brought by a doctoral student from the English department at the University of Maryland. Her work deals with the study of The Making of Americans by Gertrude Stein. The book consists of 517,207 words, but only 5,329 unique words. In comparison, Moby Dick consists of only 220,254 words but 14,512 of those words are unique. The author X  X  extensive use of repetitions (Figure 1) makes The Making of Americans one of the most difficult books to read and interpret in modern literature. Literature scholars are developing hypotheses on the pur pose of these repetitions and their interpretation. Recent critics have attempted to aid interpretation by charting the correspondence between structures of repetition and the novel X  X  discussion of identity and representation. Yet, the use of repetition in The Making of Americans is far more complicated than manual practices or traditional word-analysis could indicate. The text X  X  large size (almost 900 pages and 3183 paragraphs), its particular philosophical trajectory, and its complex patterns of repetition make it a useful case study for analyzing the interplay between the development of text mini ng tools and the way scholars develop their hypotheses in interpre ting literary texts in general. Because this case study used a very unusual text we also tested FeatureLens with other collections: a technical book, a collection of research abstracts, and a collection of presidential addresses which we use here to describe the interface and also used in our pilot user study. Visualizations have been app lied successfully to retrieving, comparing, and ranking whole te xt documents [14, 16] and computer programs [3, 7]. Instead of ranking documents according to their content, FeatureLens ranks text patterns according to their length and frequency, and it provide s a visualization of the text collection at the document level and at the paragraph level. These two levels of granularity allow the user to identify meaningful trends in the usage of text patterns across the collection. It also enables the analysis of the different contexts in which the patterns occur. A recent interactive NY Times display [8] shows the natural line, paragraph, and year categoriza tion. It displays word frequency, location, and distribution inform ation in a very simple manner which seemed to be readily understandable by the literary scholars we have been interacting with. It allows search but does not suggest not support Boolean queries. in sequences. A number of technique s such as arc diagrams, repeat graphs and dot plots have been de veloped and applied to biological sequence analysis [2, 5, 6]. Compared to DNA, literary text has different structural and semantic properties such as division into documents, paragraphs, sentences, and parts of speech that one could use to create a more meaningful visualization. Arc diagrams have been used to visualize mu sical works and text, and have advantages over dot plots [15], though it has not been shown how they can be adapted to large collections of text without creating clutter. TextArc [9] is a related project, which visualizes text by placing it sequentially in an arc and allowing a user to select words interactively and to see where in the text they appear. It does not support ranking of patterns and selec ting longer sequences of words. Most of the tools describe above only handle small datasets and display the collection as a fixed level of granularity. Figure 2 shows the graphical user interface of FeatureLens. The State of the Union Addresses collection consists of eight documents, one for each of President Bush X  X  eight annual speeches (there were two in 2001 because of 9/11). The doc uments are represented in the Document Overview panel. Each rectangular area represents one speech and its header contains the title of the document, i.e. the year the document, each colored line represents a paragraph in this collection. When the document is very large each line may represent compact. FeatureLens computes the default unit of text to be such that the overview fits on the screen, and users can change that value using a control panel. For simplicity we call that arbitrary small unit of text a paragraph in the rest of the paper. The Frequent Patterns panel, located on the left of the screen, displays the pre-computed text patterns generated by the data mining algorithms. Currently we combine only 2 types of patterns: frequent words, and frequent itemset s of n-grams (which capture the repetition of exact or similar expressions in the collection -more the pattern list since the default or dering of the list is by frequency. This also makes it easier for users to learn the interface with simple patterns, then move on to more complex patterns later on as they chose other sorting and filtering options. In Figure 2, the list of patterns has been sorted by decreasing frequency and the user has clicke d on four of the most frequent patterns. The location of the patterns is displayed on the Document Overview . Each pattern has been assigned a different color reflected in the Legend panel. When a paragraph contains one of the selected patterns, the color saturation of the line reflects the score of the paragraph, the more saturated the color. The Collection Overview panel shows a graph of the distribution of the support for each selected pattern. The vertical axis represents the support of the pattern per documen t and the horizontal axis shows the documents of the collection. Wh en the user lets the mouse hover on a specific portion of the graph a popup shows the exact number of occurrences. In Figure 2, the distribution of the word  X  X orld X  is displayed in blue, showing that the word was barely used in the first speech. By looking for lines that contain all the colors in the Documents Overview, it is possible to identify the parts of the text where selected patterns occur together. Clicking on a colored line in the overview displays the text of the corresponding paragraph in the Text View along with five paragraphs before and after the selection, to provide context while maintaini ng fast response time. In Figure 2, the user as selected a paragraph that contains all 4 patterns. A blue horizontal bar indicates which paragraph is currently displayed in the Text View . The text of the selected paragraph has a matching light-blue background. In the right margin of the Text View , small colored tick marks indicate the position of the occurrences of the The occurrences of the patterns in th e text are highlighted in color as well, matching the colors used in the overview and the legend. The Frequent Patterns panel on the left provide many controls to search, filter and sort the list of patterns. A search box allows users to find patterns that include particular keywords or Boolean combinations of keywords. Patterns can be filtered by minimum size (i.e. number of words) a nd minimum frequency within the whole collection. Patterns can be sorted by length or frequency. Above the list of patterns, a check box allows users to append patterns to the current display in order to compare and study correlations between different pattern s (the default option is to show one pattern at a time). Buttons allow users to load the history of previously explored patterns, load another collection, or set options such as the size of the text to be represented as a line in the Document Overview . In the next section, we describe the pattern mining process used in FeatureLens, to explain how patterns other than the trivial single word patterns are mined from the text. For a given collection of texts, a hierarchical structure with two levels is assumed. Each coll ection contains documents which contain at least one paragraph ( our chosen name for a small unit of text in this paper). This doc ument-paragraph hierarchy can be used for a variety of text collections (see Section 7). At this stage of the project, our focus is on the study of repetitions so we chose mining t echniques that look for frequently occurring patterns, but we believe that the interface we developed can be used to interpret the results of other types of data mining techniques that generates lists of patterns. Single words are the simplest form of patterns. For longer expressions, exact repetitions are useful because they often correspond to meaningful concepts or slogans, for example,  X  X he No Child Left Behind Act X  appears several times in President Bush X  X  speeches. Exact repetitions, though, cannot capture language constructions that include some varying parts, such as the ones in  X  X mprove our health care system X  and  X  X mprove the health of our citizens. X  In order to enable the user to study exact repetitions as well as repetitions w ith some slight variations, we resorted to the analysis of frequent closed itemsets of n-grams. For each collection of texts, one set of frequent words and one set of frequent closed patterns of 3-grams are extracted using algorithms implemented in th e Data-to-Knowledge (D2K) framework which leverages th e Text-to-Knowledge (T2K) components [12]. In order to qualify a word or a longer expression as  X  X requent, X  we introduce the definitions of n-gram and the support of a pattern. Definition 1. N-gram: a subsequence of n consecutive words from a sequence of words. Definition 2. Support of an expression: Let C = {p 1 , ..., p collection of n paragraphs, and let e be a text expression. The support of e in the collection C , denoted S (e,C) , is: We consider an expression as  X  X re quent X  if its support is strictly greater than one. In case of large collections of texts, the threshold for the support may be increased in order to limit the number of frequent patterns. D2K/T2K provides the means to perform the frequent words analysis with stemming and we know that humanists are interested in looking at both st emmed and non-ste mmed versions. We also know that sometimes, th e humanist is interested in keeping stop words. In our current scenario, we did not use stemming, but we did rem ove stop words, such as 'a, 'the,' 'of,' etc. using the predefined list provided with T2K. One set of frequent words per collection of documents was computed using a minimum support of 1. Frequent pattern mining plays an im portant role in data and text mining. One relevant example is detecting associations between fields in database tables [1, 10] . We use these ideas in our text pattern analysis. In order to provide repeated expressions that are exact repetitions as well as repetitions with slight variations, we propose to use frequent closed itemsets of n-grams, which we will refer to as frequent patterns of n-grams in the rest of this paper. We first reproduce the general problem definition found in [10] for clarity, we will then define how it can be applied to text pattern analysis. Let I={i 1 , i 2 , ..., i n } be a set of items. An itemset X is a non-empty transaction identifier and X is an itemset. An itemset X is contained in a transaction &lt;tid, Y&gt; if Y X  X  . We are interested in finding all the frequent itemsets in the database. An itemset is called fre quent if its support is above a given threshold. consecutive words (3-grams) in all the paragraphs of the par_id is a paragraph identifier and X is the set of 3-grams of this paragraph. One frequent itemset is a set of 3-grams that occur together in a minimum number of documents (fixed with a support threshold). Such a set of 3-grams may correspond to an exact repetition in the text or may be a repetition with variations, where only parts of a sentence are exactly repeated but where some  X  X oles X  correspond to variations . Let us consider the set of pa ragraphs shown in Table 1. 1 I will improve medical aid in our country 2 I will improve security in our country 3 I will improve education in our country Let us consider I, the set of all 3-grams for these paragraphs: I = { X  X  will improve X ,  X  X ill improve medical X ,  X  X ill improve security X ,  X  X ill improve education X ,  X  X mprove medical aid X ,  X  X mprove security in X ,  X  X mprove education in X  ,  X  X edical aid in X ,  X  X id in our X ,  X  X ecurity in our X ,  X  X ducation in our X ,  X  X n our country X  X  If we consider a support threshold of 3 paragraphs, then the frequent itemsets are: X = { X  X  will improve X ,  X  X n our country X  X  X = { X  X  will improve X  X  X = { X  X n our country X  X  X is an example of a frequent itemset of 3-grams that captures a repetition with slight variations. In the context of a collection of political speeches, we hope that this pattern would invite the user to analyze not only the common parts, but also the differences, which, in this case, are meaningful, i.e. the user may declare:  X  X he speaker is making promises. X  X and X 3 are also frequent itemsets but they seem redundant because X 1 carries the same information in one single itemset. We get rid of such smaller itemsets because in case of real documents the number of such sub-patterns could be dramatically high, making their analysis by the user impossible in practice. According to the following definition, X 1 is a frequent closed itemset but X 2 and X 3 are not. Definition 3. Closed itemset [10]: An itemset X is a closed itemset if there exists no itemset X X  such that: The following definition is adapted to our work from the definition of closed itemsets. Definition 4. Pattern (a closed itemset of 3-grams) : A set of 3-grams X is a pattern if there exists no set of 3-grams X X  such that: In order to help the user interpret patterns made of sets of 3-grams, a tooltip is associated to long patterns in the list. Figure 3 shows an example of the tooltip. The list of 3-grams that compose the pattern is shown in popup window . Each 3-gram is separated by a vertical bar. The first occurrence of the pattern in its context is given as an example to help the user interpret it. 
Figure 3: The list of pattern sorted by length, and the tooltip associated to the longest frequent patterns of 3-grams from When a pattern is selected, the paragraphs that contain all the 3-grams of the pattern are colored in the Documents Overview panel. The corresponding paragraphs can be displayed in the Text View to read the different contexts associated with the pattern. Some paragraphs in the Text View may contain only a subset of the n-grams of the pa ttern; these partial matches are distinguished from exact matches by using different font size. Figure 4 shows three paragraphs that contain the pattern (itemset of 3-grams) shown in Figure 3. A larger font size is used along with coloring to show where an exact match occurs (i.e. all the 3-gram of the selected pattern are contained in the paragraph). Partial matches are also highlighted but they appear with a regular font size. 
Figure 4: Text View panel with two paragraphs that contain an exact match (paragraphs 56 and 57) and one paragraph The system was implemented with OpenLaszlo for the client interface and with Ruby and MySQL for the backend part. OpenLaszlo was chosen in order to provide a zero-install access to the user interface and to make FeatureLens accessible from any web browser. Figure 5 shows a se quence diagram of the different parts of the architecture. The text collections are preprocessed off-line. The frequent words and frequent closed itemsets of 3-grams are computed and stored in a MySQL database together with the text from where they were extracted. OpenLaszlo X  X  visual components are tied to XML files. The XML files may be static or returned by a Web Service over HTTP. The application heavily relies on textual data and full text queries, therefore it needs to: 1) Store the text documents in a structured way, 2) Have an efficient way to make full-text queries and format the output with text coloring, 3) Format the output documents into XML so that OpenLaszlo can use the results. The Ferret package (a Ruby port of the Lucene tool) was used to build text indices for full-text que ries within text documents and the set of frequent patterns. This is very efficient and has a lot of useful features for building te xt indices (stemming, stop-words filtering) or for querying the index (Boolean queries and sort filters). FeatureLens can handle collection of texts which can be represented as a two-level hierar chy. In a collection of multiple books, the two-level hierarchy can use books and chapters; for a collection made of a single book it can be chapters and sections, in a collection of publication abstracts, year and abstract, etc. We experimented with different types of text collections. These texts included two books, The Making of Americans by Gertrude Stein and Gamer Theory by McKenzie Wark, one speech sequence, namely the State of the Union Addresses of the U.S. President Bush for the years 2001 through 2007, and abstracts of the research papers published by the University of Maryland Human-Computer Interaction Lab (HCIL) from 1985 through 2006. Each text collection has its own unique characteristics, and using FeatureLens led to interesting insights for each of them. The book The Making of Americans includes a large number of repetitions. The text is divided into paragraphs and the paragraphs make up the nine sections of the book. Because of the large size of the book, the second level in the hierarchy was chosen to be a unit of five paragraphs instead of one to provide a more compact overview. The second book, Gamer Theory , is a  X  X etworked book X  created by The Institute for the Future of the Book . It is designed to investigate new approaches to writing in a networked environment, when readers and writer are brought together in a conversation about an evolving text . A challenge was set forth to visualize the text, and FeaturesLens participated in it consists of nine chapters. To show FeatureLens X  ability to handle diverse text collection types and to provide interesting but simple examples for our user testing, the State of the Union Addresses 2001 through 2007, and the HCIL technical report abstracts from 1984 to 2006 were preprocessed, as well. They we re both separated into documents by the publication year. FeatureLens was evaluated by pe rforming two pilot user studies. The tool was used by the literary scholar whose doctoral study deals with the analysis of the Making of Americans . In addition a pilot study using the State of the Union Addresses was conducted http://web.futureofthebook.org/mckenziewark/visualizations to identify usability problems and see if FeatureLens X  allowed users to generate interesting in sights about the text collection. The study had eight participants, a ll of them either had advanced degrees, or were graduate stude nts. Seven had experience in writing computer programs, and five had written software for text analysis. The evaluation consiste d of 1) providing the user with background information on FeatureLens and the user study, 2) showing a short demonstration of the interface, 3) allowing the user to explore the text collection with the tool and to comment aloud on the interface and on interesting insights on the text collection. The output from the user study was a list of insights, suggestions for improvement of the tool, and a record of which parts of interface were used during the free exploration. The exploration had two parts, a nd it lasted 20 minutes per user unless the user chose to continue further. In the first part, the users were asked to answer two questions: These questions allowed the users to get acquainted with all the parts of the interface. The users were allowed to ask questions during the exploration, for example, on how to do a particular task or what some part of the interface meant. In the second  X  X ree exploration X  part, the users were asked to explore the text collection freely, and to comment on their findings. The goal of the second question was to check if frequent itemsets of 3-grams could be interpreted. The user could find the correct answer via sorting the list of frequent patterns by decreasing size and then, by reading the first el ement of the list, which is the following set of twelve 3-grams: I = { X  X e has not X ,  X  X hat saddam hussein X ,  X  X hat he has X ,  X  X o evidence that X ,  X  X ot accounted for X ,  X  X addam hussein had X ,  X  X e has given X ,  X  X e has destroyed X  ,  X  X as given no X ,  X  X as not accounted X ,  X  X iven no evidence X ,  X  X vidence that he X  X . By selecting this pattern from the list, the user could identify three consecutive paragraphs, in the 2003 speech (paragraphs 55, 56 and 57), that contained the twelve 3-grams. Figure 4 shows the details of paragraphs 56 and 57 with the corresponding 3-grams highlighted in red. All the eight users found correctly and localized the longest pattern in the text collection. Moreover, all of them were able to provide a meaningful interpretation of the pattern, in this case, a repeated expression with va riations. This repetition was interpreted by all users as either: an accusation, an attempt to persuade or an attempt to accuse. This example shows that large itemsets of 3-grams can lead to valuable insights, and that they can be easily interpreted with the proposed visualization. During the free exploration, the users mostly used text queries to find specific words or patterns of interest, mainly dealing with war, economy and education terms. Some of the user insights dealt with single text patterns and others with many. Most of the insights involve correlations be tween phrases. For example, whenever the President used the phrase  X  X ead the world, X  he was referring to environmental topics and not to the  X  X ar on terrorism. X  Other insights were similar. They related the appearance of a specific term with another. Some other examples include  X  X he President usually means the U.S. economy when mentioning economy,  X   X  security and congress occur once examples illustrate the benefit of visualizing differe nt expressions on the same overview: it helps quickly identify correlations between different expressions and derive new insights and questions. Four out of eight users derived so me questions from the trends in the distribution of support for particular expressions. Some of the comments were:  X  X here is a p eak in 2005 for the expression men and women, X   X  X he term terror has a peak in 2002 and law has one in 2004, X  and  X  X efore 2002, there is no mention of economy whereas that was after the Internet Bubble Crash. X  Most users did elaborate on these comments by analyzing the context of each expression in order to find an explanation of these trends. Interestingly enough, the sorting by pattern length or frequency was not used during the free exploration by most users. The most common work flow consisted of typing an expression in the search field, then selecting a patte rn from the returned list. In this case, patterns made out of large itemsets of 3-grams were useful because they provided some contextual information about the searched expression. Finally, the visualization of the trends in the distributions and of the location of multiple patterns inspired some insights and new questions a bout the text collection. User suggestions for improvement included allowing search for 2-grams and exact n-gram search, and also filtering of paragraphs and color-coding only the ones with co-occurring patterns. One interesting comment concerned th e size of frequent patterns of 3-grams. When patterns are sorted by decreasing size, the size corresponds to the number of 3-gram s in the pattern and not to the actual number of distinct words in the corresponding expression. The second study involves a doctoral student in English who is a literary expert on The Making of Americans . She is particularly interested in frequent patterns w ith variations and in having the ability to share findings about the book with other literary experts. There had been five meetings w ith the user over a period of two and a half months, and the feedback on the interface had been incorporated in its subsequent ve rsions. During the sixth meeting, the user used the tool for th e first time to explore the book The Making of Americans . The free exploration lasted two hours. The output from this user study was a list of comments and insights about the text. The first question addressed by the user was to rediscover and illustrate an existing piece of knowledge about the book. In this case, the question was about the way the author is referring to the bottom nature of her characters. The following text query was used:  X  X ind NOT men NOT women NOT them X  then, from the resulting list of itemsets of 3-grams, the user selected the attacking kind , the resisting kind , independent dependent kind , dependent independent kind and engulfing kind of . The locations of these descriptions of charact ers X  personalities were consistent with the sections where th e corresponding characters are described. In this case, the lis t of frequent itemsets of 3-grams allowed selecting relevant occu rrences of expressions that contained the word kind . Afterwards, the user started to study the way that one of the character's childhood was addressed by the author. The book, The Making of Americans is about two families, and in section 1 and 2, there is a story related to the house and to the childhood of the characters. The user identified this part of the text by selecting several house-related terms, which are depicted in Figure 6. According to what the user already knew, the terms in the house and governess were occurring together at the end of section 1 and in section 2. Then the user noticed that these terms were also occurring together in the first part of section 4. By reading the corresponding paragraphs, the user no ticed that this section was a reintroduction of some of the children from one of the families, the Hersland family, and more pa rticularly, of one girl named Martha Hersland. The user deve loped the hypothesis that this reintroduction of the character of Martha was probably linked to the story of the failure of her marriage with her husband, named Redfern . The display of all the occurrences of the word Redfern showed that his character was mentioned a lot, just after the reintroduction of Martha. Similarly, the expert was able to discover that the author uses the concepts of success and failure when describing marriage in section 6. The words succeeding , failing and married were selected together and the user noticed that the concept of marriage was mentioned in sections 1, 2 and 6, and that it was only associated with the concepts of failure and success in section 6. For the user, this fact supporte d the idea that the author was describing the marriage factually at the beginning of the book, and then the author introduced a judgment in section 6. These examples show that the proposed visualization supports discoveries and re-discoveries by a domain expert. Some pieces of new knowledge were found. As for the first group of users, the frequent itemsets of 3-grams were hardly used by themselves but were useful when used in conjunc tion with text search. Finally, in the context of the Humanities, the proposed visualization can provide facts and illustrations to support the scholar's hypotheses and to share some pieces of knowledge. We described FeatureLens, a system which allows the visual exploration of frequent text pa tterns in text collections. We applied the concepts of frequent words, frequent expressions and frequent closed itemsets of n-grams to guide the discovery process. Combined with the inter active visualization, these text mining concepts can help the user to analyze the text, and to create insights and new hypothese s. FeatureLens facilitates the use of itemsets of 3-grams by providing the context of each occurrence. The display of the frequency distribution trends can be used to derive meaningful information. The display of multiple expressions at a time allows a user to study correlations between patterns. Therefore, FeatureLen s helps to discover and present interesting insights about the text. The user study with The State of the Union collection suggests that at first users use text search as a means of exploration to find patterns of interest, instead of looking at the longest patterns. Being able to display patterns simultaneously was important to make comparisons. In our future work we will investigate better means of exploration of long patterns and look at more di verse kinds of texts, especially large collections of text where our two level hierarchy may not be sufficient. As future work, it would be useful to support the selection of interesting patterns by defining metrics on the distributions of frequency associated with each pattern. These metrics should take into account the temporal properties of the distribution trends, so that meaningful trends, such as peaks or gaps, could be easily identified. We would like to thank the enthusiastic volunteers who participated in the user studies for their time and feedback. [1] Agrawal, R., and R. Srikant, Fast algorithms for mining [2] Church, K.W., and Helfman, J.I., Dotplot: A Program for [3] Eick, S.G. and Steffen, J.L. and Sumner Jr, E.E., Seesoft-A [4] Fekete, J. and Dufournaud, N ., Compus: visualization and [5] Frank, A. C., Amiri, H., Andersson, S., Genome [6] Kurtz, S &amp; Schleiermacher, C. REPuter: fast computation of [7] G. Lommerse, F. Nossin, L. Voinea, A. Telea, The Visual [8] NY Times: The State of the Union in Words. [9] Paley, W.B. TextArc: S howing Word Frequency and [10] J. Pei and J. Han and R. Mao, CLOSET: An Efficient [11] Plaisant, C. and Rose, J. a nd Yu, B. and Auvil, L. and [12] Data to Knowledge (D2K) a nd Text to knowledge (T2K), [13] Thomas, J.J. and Cook, K.A. (eds.), Illuminating the Path: [14] Veerasamy, A. and Belkin, N. Evaluation of a Tool for [15] Wattenberg, M., Arc diagrams: visualizing structure in [16] Wise, J. A. and Thomas, J. J. and Pennock, K. and Lantrip, 
