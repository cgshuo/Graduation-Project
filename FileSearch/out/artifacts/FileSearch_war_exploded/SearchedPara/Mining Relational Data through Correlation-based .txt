 Commercial relational databases currently store vast amounts of real-world data. The data within these relational reposi-tories are represented by multiple relations, which are inter-connected by means of foreign key joins. The mining of such interrelated data poses a major challenge to the data mining community. Unfortunately, traditional data mining algorithms usually only explore one relation, the so-called target relation, thus excluding crucial knowledge embedded in the related so-called background relations. In this pa-per, we propose a novel approach for classifying relational such domains. This strategy employs multiple views to cap-ture crucial information not only from the target relation, but also from related relations. This information is inte-grated into the relational mining process. The framework presented here, firstly, explore the relational domain to par-tition its features space into multiple subsets. Subsequently, these subsets are used to construct multiple uncorrelated views, based on a novel correlation-based view validation method, against the target concept. Finally, the knowledge possessed by multiple views are incorporated into a meta-learning mechanism to augment one another. Based on this framework, a wide range of conventional data mining meth-ods can be applied to mine relational databases. Our ex-periments on benchmark real-world data sets show that the proposed method achieves promising results both in terms of overall accuracy obtained and run time, when compared with two other relational data mining approaches. Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications-Data mining General Terms: Algorithms.
 Keywords: Multi-relational Data Mining, Classification, Multi-view Learning, Relational Database. The number of commercial relational database, which store vast amount of real-world data, is exponentially growing. The normalized data within these relational repositories are Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. represented by multiple relations, which are inter-connected by means of foreign key joins. The development of tech-niques to directly mine such interrelated data poses a major and urgent challenge to the data mining community. One of the key issues of mining relational domains involves how to include essential information from multiple relations. That is, research has shown that, when mining relation domains, it is important to consider not only the knowledge from the target relation, but also essential information from relations which relate to the target, the so-called background rela-tions [7]. Unfortunately, most data mining methods work only with  X  X lat X  data presentations, focusing on one target re-lation. Here we refer to the so-called propositional or single-table approaches. These methods include popular classifica-tion algorithms, such as C4.5 decision trees [18] and Support Vector Machines (SVMs) [3].

The application of such propositional approaches to rela-tionaldomainsisanactiveareaofresearch. Thegeneral idea is that such stable, efficient and accurate algorithms may be applied, without  X  X e-inventing the wheel X . To this end, the extension of propositional approaches, by flatten-ing them into a universal relation has been proposed. These so-called propositionalization approaches subsequently ap-plies single-table methods such as decision trees, to the flat-tened universal relation. Several methods, such as LINUS [7] and RELAGGS [15], have been developed to perform this translation. However, it usually takes considerable time and effort to convert the relations into the required  X  X lat X  format. Another drawback of this method is that the process can cause a loss of information [7] and create a large amount of redundant data. Furthermore, a universal relation with a large amount of entities and attributes often leads to further efficiency and scaling challenges.

A promising new strategy, multi-view learning, has re-ceived significant attention in current literature [2, 16, 6]. This framework has been applied successfully to many real-world applications such as information extraction [2] and face recognition [6]. As in the example given by Blum and Mitchell [2], one can classify segments of televised broadcast based either on the video or on the audio information.
A multi-view learning problem with n views can be seen as n strongly uncorrelated feature sets which are distributed in the multiple relations of a relational database. A rela-tional database normally contains n inter-dependent rela-tions. Each relation usually has a naturally divided dis-joint feature set, providing various attributes (information) contributing to the target concepts to be learned. As an example, consider the loan problem, as shown in Figure 1, from the PKDD 99 discovery challenge [1]. Here a banking database which consists of eight relations is depicted. Each relation describes the different characteristics of a client. For example, the Client relation contains a customer X  X  age, but the Account relation identifies a customer X  X  banking account information. In other words, each relation from this data-base provides different types of information, or views, for the concept to be learned, i.e. whether the customer is a risk or not . Thus, one can explore a relational database to construct multiple strongly uncorrelated feature sets from related re-lations. This problem is therefore a perfect candidate for multi-view learning, as will be discussed in Section 2. Figure 1: A simple database from PKDD 99 [1]
This paper proposes a novel framework for classifying rela-tional domains. The so-called Multi-view Relational Classi-fication (MRC) strategy employs multiple views to capture crucial information not only from the target relation, but also from relations which are related to the target. This information is integrated into the relational mining process. This framework, as shown in Figure 2, firstly, explore the re-lational domain to partition its features space into multiple subsets. Subsequently, these subsets are used to construct multiple uncorrelated views, based on a heuristic view valida-tion method, on the target concept. Thirdly, the knowledge possessed by multiple views are incorporated into a meta-learning mechanism to augment one another. This frame-work enables one to classify relational objects by applying conventional data mining methods, while there is no need to flatten multiple relations to a universal one.
The main contributions of this work are: 1. It provides an initial multi-view framework for mining 2. It develops a new method for partitioning features dis-3. It introduces a novel view validation strategy for multi-
The paper is organized as follows. Section 2 presents the problem settings. Next, a detailed discussion of the MRC algorithm is provided in Section 3. In Section 4, we describe the comparative evaluation performed. Finally, Section 5 concludes the paper and outlines our future work.
Figure 2: The Core Idea of the MRC Approach This paper addresses classification in relational domains. We assume that data are represented as a relational data-base with multiple relations. 2.1 Relational Databases A relational database defines the structure of our relational schema for mining. A schema for a relational database de-scribes a set of tables = { R 1 ,  X  X  X  , R n } .Atable R i of a set of tuples T R and has at least one key attribute, either the primary key attribute and/or the foreign key attribute . The other attributes are descriptive attributes . Foreign key attributes 1 link to key attributes of other tables. This link specifies a join between two tables. A set of joins with n tables R 1  X  X  X  R n describes a join path, denoted as = R 1  X  X  X  X  X  R n (This is known as a slot chain or ref-erence chain in PRM terminology [9]). The length of join path n is defined as the number of joins in n .Foreach entity t in the first relation R 1 of the join path n , each table R i in this join path will only contain a subset of the original tuples (let T t ( R i )= { t 1 ,...,t j } denote the bag of tuples in table R i , corresponding to tuple t in table R 2.2 Relational Classification A relational classification problem deals with classification tasks with information distributed in multiple relations.
Definition 1. In a relational classification setting, we have a database containing a particular target relation R t and a set of background relations R b 1 ,...,R b n . Each tuple x includes a unique primary key attribute x.k ( tuple identifier ) and a categorical variable y . The relational classification task is to find a function F ( x ) which maps each tuple x of the target table R t to the category y : 2.3 Relational Features Relational features are used to pull information about a tuple in the target relation not only from the target relation, but also from entities which are related to the target tuple.
Definition 2. A relational feature is defined using tuple attributes and the standard relational algebra operation  X  (projection) and aggregation operator  X  (e.g., average, min, max). Relational features of tuple t ,denotedas  X  (t) ,is formally defined as follows:
For simplicity we only consider key attribute as a single attribute here.
 for  X  t  X  R t ,  X  R b i , X   X  X   X ,  X  } , then: 2.4 Multi-view Relational Classification Multi-view relational classification uses relational features to construct multiple views.

Definition 3. A multi-view relational classification refers to using relational features to construct a set of n disjoint views { V 1 ,...,V n } . These uncorrelated views are then used to model a target function in order to approximate the target concept to be learned.
 Multi-view learning prefers strongly independent views [5]. That is, views V i and V j (where  X  i, j  X  n and i = j )are conditionally independent given class variable y. In multi-view learning, a tuple t in the target relation R t is viewed as: where  X  i ( t )and  X  j ( t ) are instances in the views V V , respectively. The variable y denotes the class label.
Combining Equation 1 and Equation 3, a multi-view re-lational classification task becomes where x  X  T R t , X  1 ( x )  X  V 1 ,..., X  n ( x )  X  V n and p (  X  i ( x ) , X  j ( x ) | y )= p (  X  i ( x ) | y )  X  p (  X  Here, p (  X  i ( t ) | y ) denotes the conditional probability.
This section describes the definitions related to the Multi-view Relational Classification. The next section discusses, in detail, the proposed MRC approach. The key idea of the MRC approach is to explore a rela-tional domain to construct multiple strongly uncorrelative views. Each is built based on a set of relational features. Then, multiple views are incorporated into a meta-learning mechanism to model the target concept to be learned. In detail, the MRC method includes three key components. Firstly, the MRC algorithm initiates the View Construction element to explore the relational domain to construct mul-tiple views. Subsequently, a View Validation component is launched to select a set of strongly uncorrelated views which are appropriate for multi-view relational learning. Finally, the View Combination mechanism combines the multiple views to form the final classification model. These three key steps are discussed, in detail, in the following sections. The work presented here extends our earlier Multi-view Classi-fication (MVC) algorithm [11], by the introduction of (1) a new method for constructing multiple views and (2) a novel view validation strategy which identifies a set of uncorrela-tive views. 3.1 View Construction The View Construction process constructs multiple views on the target concept. This construction procedure initially ex-plores the relational domain to extract a set of join paths. Subsequently, join operations and aggregation functions are applied to each join path to convert bags of related tuples into a single entity. This results in forming relational fea-tures for each candidate views . 3.1.1 Extracting Join Paths The View Construction component initially converts the re-lational database schema into an undirected graph, in which each node is in one-to-one correspondence with a table in the database and joins are modeled as edges. Subsequently, it traverses the graph to extract a set of unique join paths.
Each join path starts with the target relation and stops with one of the background relations. This ensures that each join path is able to construct a view separately. Also, one constraint is imposed on each join path collected. That is, relation is unique in a join path. The intuitive reason for this strategy is as follows. On the one hand, the number of join paths in a relational domain with many relations is usually very large. It is unaffordable to exhaustively search all join paths. On the other hand, join paths with many relations can quickly decrease the number of entities related to the target tuples. Therefore, to trade off between the effi-ciency and accuracy of the MRC algorithm, this restriction is introduced.

The View Construction component prefers join paths with shorter length. That is, it initially collects unique join paths with two relations, i.e. join path with length of one. Sub-sequently, it keeps collecting join paths with one more re-lation. The reason for this is the follows. The semantic links with too many joins usually becomes very weak in a relational database [22]. Also, the related entities keep de-creasing when a join path grows.

Given the complex structure in a database schema, the length of a join path could be very large. Therefore, the View Construction component sets a maximum length for the join paths collected as stopping criterion. When this number is reach, the entire join path extraction process stops, and the join paths collected are returned.
EXAMPLE 1. In the example database of Figure 1, the join relationships of the database can be represented by the graph on the left hand side of Figure 3. The graph has eight nodes. Each corresponds to one of the eight tables in the database. Also, each edge corresponds to a foreign key join between two tables. The texts on each edges of the graph capture the join attributes between two relations. The right hand side of the figure provides three join paths (with lengths of one, two, and three, respectively) constructed from the relational database schema.

This step returns a set of join paths. The sequence of joins of a join path contains exactly the relations and joins that must be followed to construct relational features for a specific view. This construction process is discussed next. 3.1.2 Constructing Relational Features An attribute derived from a join path will be multi-valued, i.e. a bag of values, if the join path consists of a one-to-many join. Aggregation operators are employed to aggregate in-formation from the multiset, generating relational features. Different aggregation functions are employed to different at-tributes. For a Nominal Attribute , the aggregation COUNT function is applied to calculate the number of occurrences of values of this attribute. For a Binary Attribute in which only two different values are possible, the aggregation COUNT function is applied to each of the two values, respectively. For a Numeric Attribute , the aggregation SUM, AVG, MIN, MAX, STDDEV and COUNT functions are applied.

In this way, each join path separately creates a set of tuples with relational features  X  ( t ), forming a Candidate View V i d (here d indicates that the V i d is a candidate view). Also, the View Construction component uses all descriptive features from the target table to form a special view.
The next section presents the MRC method X  X  second key component, i.e. the View Validation element. 3.2 View Validation The View Validation process evaluates, based on a heuristic correlation measure, all candidate views to select a subset of views which are appropriate for multi-view learning.
The theoretical foundations of multi-view learning are based on the assumptions that the views are independent [2]. Research has shown that disjoint views are preferred by multi-view learning [5]. However, in real-world domains, the ideal assumption of multiple strictly independent views, as described in Section 2.4, is not fully satisfied [16]. Following this line of thought, the MRC algorithm aims to construct a set of highly uncorrelated views .The View Validation el-ement calculates the correlation information among candi-date views, and then selects a subset of candidate views which are strongly uncorrelated to one another. This so-called Correlation-based View Validation (CVV) method is described next. 3.2.1 Correlation-based View Validation The CVV method aims to select a subset of views which are highly correlated with the target concept, but irrelevant to one another. The CVV strategy uses a heuristic measure to evaluate the correlation between views. A similar heuristic principle has been applied in the test theory by Ghiselli [10] and feature selection approach by Hall [12].

Heuristic  X  X oodness X  of feature set . The heuristic  X  X oodness X  of a subset of features is formalized in Equa-tion 4 [12]:
Where C is the heuristic  X  X oodness X  of a selected feature subset. K is the number of features in the selected subset. R cf calculates the average feature-to-class correlation, and R ff stands for the average feature-to-feature dependence.
To calculate C , one need to know R cf and R ff ,whichare discussed next.

Measure of correlation of views . To measure the cor-relations between features and the class, and between fea-tures, we adopt the Symmetrical Uncertainty ( U ) [17] to calculate R cf and R ff . This measure is a modified infor-mation gain ( InfoGain ) measure [18]. It compensates for InfoGain  X  X  bias toward attributes with more values, and has successfully been applied by Ghiselli [17] and Hall [12]. The Symmetrical uncertainty is defined as follows: Given features X and Y , where and In order to apply the heuristic  X  X oodness X  C to select a subset of strongly uncorrected views, we need representatives of multiple views. This method is discussed next.

Represent views with View Features .IntheCVV method, views are represented by View Features . View Fea-tures describe the knowledge, against the target concept, possessed by views.

Definition 4. Let { V 1 ,  X  X  X  ,V n } be n views to be vali-dated. Given a view validation data set D v with m labels { y 1 ,  X  X  X  ,y m } . For each instance t (with label L )in D each view is called upon to produce predictions { f y k V ( i  X  X  1 ,  X  X  X  ,n } and k  X  X  1 ,  X  X  X  ,m } ) for it. Here, f y k notes the probability that instance t belongs to class y k predicted by view V i .Inthisway,a view validation exam-ple D v which consists of a set of f y k V i ( t ) (each describes the hypothesis knowledge possessed by each view), along with the original class labels L , is constructed.
 That is, V 1 , for example, is described and represented by ample .Wecall { f y k V 1 ( t ) } View Features of view V
In this step, a set of view validation examples D v are created. Each instance consists of a set of View Features to describe the corresponding view, along with a class label of the instance. In this way, a subset of view features can be selected based on measure C . Thisisdiscussednext.
Correlation-based View Validation .TheCVVal-gorithm subsequently ranks view feature subsets according to the correlation-based heuristic evaluation measure C .It searches all possible view feature subsets, and constructs a ranking on them. The best ranking subset will be selected, i.e. the subset with the highest value of C .

To search the view feature space, the CVV method uses the best first search strategy [14]. This strategy has success-fully been applied to many feature selection strategies [12, 14]. The best search strategy initiates with an empty set of features, and keeps expanding with one more feature. In each round of the expansion, the best feature subset, namely the subset with the highest  X  X oodness X  value C will be chosen to keep expanding in the same way. Additionally, the best search traversal keeps a ranking of all its visited subsets. If the current expansion results in no improvement in terms of  X  X oodness X  value, the search can go back to the next best path and use it to expand its feature set.

A stopping criterion is usually imposed to a best first search. The CVV algorithm will terminate the search if a number of consecutive non-improvement expansions occur. Based on our experimental observations and the heuristic value found for the CFS method [12], we set the number to five heuristically.

Views are selected based on the final best subset of view features, which are considered highly correlate to the target concept to be learned. If a view has no view features to be considered strongly correlate to the class to be learned, it means that knowledge possessed by this view is not im-portant for the learning. Thus, it makes sense to ignore this view. Therefore, the CVV algorithm selects a view if and only if any of its view features appears in the final best ranking subset of the view feature selection procedure.
EXAMPLE 2. Consider a final view feature subset { f knowledge from views V 1 and V 4 really contributes to build the final model. Thus views V 1 and V 4 are selected by the Correlation-based View Validation method. Views other than V 1 and V 4 are ignored because they are considered weakly relevant to the target concept to be learned. Algorithm 1 Correlation-based View Validation Input: Candidate view set { V 1 d ,  X  X  X  ,V n d } , Output: View set { V 1 ,  X  X  X  ,V n } ( n  X  n ). 1: Let view set V =  X  ; 2: Generate a validation data set D v ,using D v and {V i 3: Select a view feature set A from D v ; 4: for each V i d which has at least one attribute in A do 5: V .add( V i d ); 6: end for 7: return V .

Algorithm 1 shows the CVV method in details. As pre-sented in Algorithm 1, the View Validation element, firstly, generates a view validation data set , where hypothesis knowl-edge possessed by multiple views are represented by view features . Secondly, based on the heuristic correlation eval-uation measure as described in Equation 4, the best first search strategy is employed to select the best subset of view features. Finally, views are filtered out if none of its view features appear in the final view feature subset.
Views that passed the validation process will be used to construct the final model, which is discussed next. 3.3 View Combination The last component of the MRC algorithm, namely the View Combination element constructs the final classification model, using the trained multi-view learners.

Strategies for combining models have been investigated by many researchers. The most popular are those such as bag-ging, boosting and stacking [21]. Voting approaches usually only make sense if the learners perform comparably well [21]. A meta-learner method, such as the one used in stacking, is designed to learn which base classifiers are the reliable ones, using another learning algorithm. In the multi-view learning framework, multiple views may results in various performances, thus it is hard to guarantee the comparable performances of the multiple views. Therefore, the meta-learning method is more suitable to the multi-view learning framework. In meta-learning schemes, a learning algorithm is usually used to construct the function that combines the predictions of the individual learners. The View Combina-tion component applies this scheme in order to combine the classifiers from the multi-view learners. This combination process contains two steps. Firstly, a meta training data set is generated. Secondly, a meta-learner is trained using the meta data constructed. These steps are discussed next.
Construct Meta Data. After training, the multi-view learners {H i } n 1 (formed from multiple views {V i } n used to construct training instances (meta data) for the meta-learner. Each instance of the meta data consists of the predictions made by the learner on a specific training example, along with the original class label for that exam-ple. The details of this procedure are as follows.
For each training tuple x from the target table, each multi-view learner H i will retrieve the related tuple from view V If a corresponding tuple is found, the learner H i is called to produce predictions { P y k v i ( x )) } ( k  X  X  1 , 2 ,...,m Here, P y k v i ( x ) denotes the probability that instance x belongs to class y k )( y has m different values), as defined by multi-view learner H i . Otherwise, an equal probability is assigned for each class label, i.e. P y k v i ( x )equalsto1 /m are returned.
Following these steps, a feature vector, x , consisting of built and used as meta data.
 Train Meta Learner. After building the meta data, the View Combination component calls upon a meta-learner to produce a function to control how the classifiers are used to classify a particular example. The objective of this meta-learner is to achieve maximum classification accuracy from this combination of classifiers. This function, along with the hypotheses {H i } n 1 constructed by each of the multi-view learners constitutes the final classification model. Algorithm 2 Multi-view Relational Classification Input: Relational database = R t  X  R b 1  X  X  X  X  X  R b n , Output: Classification model F . 1: Convert database schema into graph; 2: Extract join path set { n } from graph; 3: Construct relational feature set  X  ( t ) for each join path 4: Select a set {V i } n 1 from { V 1 d ,  X  X  X  ,V n d } (Algorithm 1); 5: Train L with {V i } n 1 , forming hypothesis set {H i } n 6: Form final model F by combining {H i } n 1 ,using M ; 7: return F .

Algorithm 2 describes the entire steps of the MRC strat-egy. As shown in Algorithm 2, the MRC method initially extracts a set of join paths from the relational domain. Sub-sequently, Algorithm 1 is called upon to create a subset of uncorrelated views. After doing so, multiple views are then combined to form a final model.

The next section presents our empirical evaluations. This section presents the results obtained for the MRC al-gorithm on different standard databases. These results are presentedincomparisonwithtwootherwellknownsys-tems for multi-relational data mining, namely the FOIL al-gorithm [19] and the CrossMine method [22]. 4.1 Methodology Seven learning tasks derived from four standard real-world databases were used to compare the predictive performances of the three methods. We implemented the MRC algorithm using Weka [21]. In our experiments, C4.5 decision trees [18] were applied in each of the multi-view learners. We set the maximum length of join path M axJ to two , since this is the smallest number of graph traversals needed to visit all back-ground relations of our experimental databases. In order to compare the effect of the meta-learning phase of the MRC approach, we provide the experimental results when using C4.5 decision tree, SVMs and One Rule (1R) algorithm [13] as the meta-learners. These three meta-learners represent three diverse strategies to construct a model. Each of these experiments produces results using ten-fold cross validation. We report the average running time of each fold for the three methods. The MRC algorithm and the CrossMine method were run on a 3 GHz Pentium 4 PC with 1 GByte of RAM running Windows XP and MySQL. The FOIL approach was run on a Sun4u machine with 4 CPUs. 4.2 Data Sets Financial Database. Our first experiment used the Finan-cial database from the PKDD 1999 discovery challenge [1]. This database is composed of eight tables. The target table includes a class attribute status which indicates the status of the loan, i.e. finished, unfinished, good ,or bad .This database provides us with three different learning problems (denoted as F234AC, F682AC and F400AC respectively), as prepared in [11]. Note that for comparison purpose, all these three learning tasks use the same background relations as prepared in [22].

Mutagenesis Database. Our second experiment (de-noted as MUT188) was conducted against the Mutagenesis data set [20]. This benchmark data set is composed of the structural descriptions of 188 Regression Friendly molecules that are to be classified as mutagenic or not. The back-ground relations consist of descriptions about the atoms and bonds that make up the molecules.

Thrombosis Database. Our third experiment (iden-tified by Throm) uses the Thrombosis database from the PKDD 2001 Discovery Challenge [4]. This database is or-ganized using five relations. We used the Antibody exam relation as the target table, and Thrombosis as the target class. This class describes the patients X  different degrees for Thrombosis, i.e. None, Most Severe, Severe ,and Mild . Our task here is to determine whether or not a patient is thrombosis free. For this task, four background relations are included.

ECML98 Database. Our last experiment used the data-base for the ECML 1998 Sisyphus Workshop [15]. There are two learning tasks derived from this database. The first one (ECML A) classifies 13,322 customers of class 1 or 2. The second problem (ECML B) is to categorize 7,329 households of class 1 or 2 [15]. Eight background relations are pro-vided for each learning task. In this experiment, we used the schemes prepared in [15]. 4.3 Experimental Results In this section we examine the performance of the MRC method both in terms of the accuracy and run time. Table 1: Accuracies obtained for the seven learning tasks using FOIL, CrossMine, MRCs Table 2: Execution time needed for the seven learn-ing tasks using FOIL, CrossMine, MRCs
We present the predictive accuracy obtained for each of the seven learning tasks in Table 1, where CM denotes the CrossMine method. M C4.5, M SVM and M 1R stand for the MRC approaches using C4.5 decision tree, SVMs and 1R algorithm as the meta-learner, respectively. For each data set in Table 1 , the highest results are highlighted in bold .
Accuracy Obtained. The results, as presented in Ta-ble 1, show that the MRC algorithm appears to consis-tently reduce the error rate for almost all of the data set (regardless of meta-learners used), when compared to the FOIL and CrossMine methods. For example, the M SVM method achieved the highest accuracy results against all cases, as shown in Table 1. The MRC 4.5 approach pro-duced equal or higher accuracies against all data sets, ex-cept a slight decrease of 0.1% against the MUT188 data set. When considering 1R algorithm as the meta-learner in the MRC approach, the results also convince that the M 1R outperformed the other two algorithms against almost all cases. Only against one data set (the ECML B data set), the M 1R method obtained slightly lower accuracy than that of the CrossMine method (lower by only 0.2%). Our re-sults as shown in Table 1 also indicate that in many cases, the error rate reduction achieved by the MRC approaches is large. For example, the MRC approaches reduced the er-ror rates by more than 10% against four of the seven data sets (the F400AC, F234AC, ECML AandECML Bdata sets), when compared to the FOIL algorithm. Against the THROM, F682AC and ECML A data sets, the accuracy improvement yielded by the MRC approaches, comparing with the CrossMine algorithm, is at least 10%, 2.8% and 3.3%, respectively. Further analysis of our experimental re-sults also suggests that the MRC approach was capable of producing robust predictive performances regardless of the meta-learning methods applied. As shown in Table 1, the M SVM, M C4.5 and M 1R approaches yielded very compa-rable predictive results again various data sets. For example, in many cases, the predictive results produced by these three approaches differ from one another only within 1.0%.
Execution Time Needed. To evaluate the performance of the MRC strategy in terms of run time, we also provide the execution time needed (in seconds) for each of the seven learning tasks in Table 2, where the best results for each data set are also highlighted in bold . From the experimental results, one can see that the MRC methods achieved very good performance in terms of run time, when compared to the FOIL and CrossMine algorithms. The table shows that the MRC algorithms (regardless of the meta-learners used), in five of the the seven cases, reduced the time when com-pared to the other two approaches. In some cases such as the F682AC, F400AC and F234AC data sets, the reduction was very large. Against these data sets, the MRC approaches were almost twice as fast as the other two methods. Promis-ingly, these outputs imply that the MRC approaches are very efficient for complex database schemes. For example, for the F682AC, F400AC and F234AC data sets which have the most complex structures in this experimental setting, the MRC algorithms were much faster than the other two methods. Further analysis of the results also confirms that the MRC methods, with different meta-learners, resulted in little difference in terms of run time needed.

Impact of the View Validation. In order to more thoroughly evaluate the benefit of the CVV strategy, we also present the performance of the MRC algorithm with and without the CVV method, in terms of accuracy obtained and execution time needed, using the F234AC data set. This data set was chosen since it has a complex database schema and a moderate number of entities. We varied the maximum length of join path M axJ , and presented the results of accu-racies obtained and execution time needed. The results are on the left and right hand sides of Figure 4, respectively. The left hand side of Figure 4 indicates that the MRC method with the CVV strategy consistently improved the predictive performance of the MRC algorithm, regardless the value of M axJ used. Analysis of the right hand side of Figure 4 shows us that the CVV strategy slightly reduced the exe-cution time for the MRC algorithm for all values of M axJ . These results implies that the CVV strategy was able to im-prove the MRC algorithm X  X  performances both in accuracy obtained and execution time needed.

In conclusion, the results shown in Tables 1 and 2 in-dicate that the MRC approach achieves promising results comparing with other techniques, when evaluated in terms of overall accuracy obtained and run time. Promisingly, the results show that the MRC method is capable of automat-ically classifying objects using multiple relations efficiently. Furthermore, the results imply that the MRC framework yields robust models, in terms of predictive performance and run time required, regardless of the meta-learners employed. This paper proposes a Multi-view Relational Classification (MRC) strategy, in which multiple uncorrelated views are employed to include crucial information provided in back-ground relations, for exploring relational domains. We have developed a heuristic method to partition relational features for multi-view learning. Also, we have devised a technique, called Correlation-based View Validation to ensure uncor-related views. The MRC algorithm enables propositional approaches to classify objects across multiple relations effi-ciently, while there is no need to transfer multiple relations to a universal one. Our experimental results show that the MRC method performs well on various datasets in terms of accuracy obtained and run time. The study presented here suggests a new direction to mine relational domains. Our future work will include a thorough investigation of the ro-bustness of the CVV approach. Sophisticated methods to partition relational features will also be further studied.
