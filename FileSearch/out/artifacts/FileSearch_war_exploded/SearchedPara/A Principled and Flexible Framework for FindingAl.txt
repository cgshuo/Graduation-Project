 The aim of data mining is to find novel and actionable insights in data. However, most algorithms typically just find a single (pos-sibly non-novel/actionable) interpretation of the data even though alternatives could exist. The problem of finding an alternative to a given original clustering has received little attention in the liter-ature. Current techniques (including our previous work) are unfo-cused/unrefined in that they broadly attempt to find an alternative clustering but do not specify which properties of the original clus-tering should or should not be retained. In this work, we explore a principled and flexible framework in order to find alternative clus-terings of the data. The approach is principled since it poses a constrained optimization problem, so its exact behavior is under-stood. It is flexible since the user can formally specify positive and negative feedback based on the existing clustering, which ranges from which clusters to keep (or not) to making a trade-off between alternativeness and clustering quality.
 H.2.8 [ Database Management ]: Database Applications X  Data min-ing Algorithms, Experimentation Clustering
The purpose of data mining is to find novel and actionable pat-terns. However, in many situations a practitioner already has knowl-edge of what is not actionable and not novel and unless this is some-how encoded, the algorithm may continue to find those patterns. Consider the clustering of loan applications in order to identify bad loans, but the clusters fall along racial lines. You may wish to find another alternative yet equally good clustering. Similarly, high di-mensional data such as collections of images may naturally con-tain many plausible ways of clustering based on different subsets of pixels. Finally as previously showed, in even low dimensional data (the pen digit data set in our earlier work [5]) multiple expla-nations may exist if the underlying phenomenon is complex and the data is insufficient to justify just one explanation.

The recent innovation of finding an alternative clustering an-swers the question:  X  X iven a clustering  X  , does there exist another clustering  X  0 which is different from  X  but equally good in terms of objective function value? X  Note that in this question there are two key factors of concern: alternativeness and quality. That is, we hope the new clustering not only interprets the data from an alter-native perspective but also is of good quality in terms of the algo-rithm X  X  objective function. Others [9, 2, 4] as well as ourselves [5] have tackled the problem which we term the Singular Alternative Clustering Problem described below.

P ROBLEM 1. Singular Alternative Clustering Problem . Given an objective function f , an existing clustering  X  so that f (  X  ) = x , does there exist another clustering  X  0 that is different from  X  and where f (  X  0 )  X  f (  X  ) ?
Note that the Singular Alternative Clustering Problem is a dif-ferent problem from the one addressed in Jain, Meka and Dhillon X  X  work [10]. In that paper, the authors deal with a problem of finding two disparate (alternative) clusterings simultaneously , while our work deals with finding an alternative clustering given an existing one.
 There are two primary limitations in previous work on this topic. First, existing techniques aside from our prior work [5] are algorithm-dependent [9, 2, 4], as we shall describe in Section 2. Second, all the existing techniques including our own [5] do not specify which properties of the original clustering should be preserved (or not) in the new clustering. Instead, they bluntly find an alternative clus-tering with no guidance other than the new clustering must be an alternative to the original. However, in many circumstances we may not wish to find a complete alternative, but perhaps a partial alternative, and seek to precisely state which parts of the clustering to retain and which parts not to retain.

The main contribution of this paper is to propose a general frame-work for solving the Singular Alternative Clustering Problem where the expected properties of the new clustering can be specified. To find the new clustering in an algorithm-independent way, we cre-ate a transformation matrix to transform the data set into a new space while preserving the properties of the data set and respect-ing the users X  feedback on the previous clustering. This allows any clustering algorithm to be applied to the transformed data. We will formally show that our approach is a solution to a constrained optimization problem. Our formulation minimizes the Kullback-Leibler divergence between two distributions: the original data and the transformed data, so that the data properties are not overly dis-torted. The constraint on the optimization allows us to specify which properties of the clustering should be kept. Formally, our aim is to create an approach that:
Note that this work does not build upon our previous work [5] apart from working on the same problem and also proposing an algorithm-independent approach.
 We begin this paper by describing the related work in Section 2. We present our framework to solve the Singular Alternative Clus-tering Problem in Section 3. In Section 4, we show the flexibility of our approach by discussing the variations to our problem formu-lation. In Section 5, we illustrate experimental results on UCI data sets which show that our approach provides genuinely (non-trivial) alternative clusterings with good quality. The experiments on im-age segmentation applications show that our approach can obtain alternative meaningful image partition results. Finally, we present the experiments where the desirable/undesirable clusters in the new clustering can be explicitly specified, and the results show that our approach not only achieves alternative clusterings but also main-tains the desirable clusters.
The problem of finding alternative clusterings has received lim-ited attention so far. Most of the approaches to address this problem are based on some specific clustering algorithm. Bae and Bailey [2] force an alternative clustering by generating cannot-linked con-straints from all pairs of objects which are in the same cluster in  X  , the original clustering. However, their method is tied to a hierarchi-cal clustering algorithm. Another approach combines k -means and PCA to project the data into an alternative subspace [4]. This has the limitation of not being appropriate for lower dimensional data sets such as spatial data, as we discussed and illustrated in previous work [5]. A third approach [9] explored the idea of using Condi-tional Information Bottleneck (CIB) to find an alternative cluster-ing to a given non-novel clustering. This approach subtracts the background knowledge of the given clustering by maximizing con-ditional mutual information I ( C ; Y | Z ) ( C , Y and Z denote the clus-ters of objects, relevant features and the background knowledge), which is difficult to implement since it requires modeling joint dis-tribution between the cluster labels and the relevant features. The last approach, which is our own [5], first learns a distance metric D  X  from the original clustering  X  and then interprets D  X  from the geometric point of view. It then reverses the transformation of D using Moore-Penrose pseudo-inverse to get the new distance met-ric D 0  X  . Thus in the new data transformed by D 0  X  one will find a different clustering other than  X  .

The area of non-hierarchical clustering with constraints can po-tentially be used to find alternative clusterings. Consider a cluster-ing  X  which can non-ambiguously be represented by a large con-junction of must-linked constraints between every two points in the same cluster and a large conjunction of cannot-linked constraints between every two points from different clusters. Since this repre-sents the clustering  X  , we can guarantee that  X  is not found again by flipping the constraints (making must-linked constraints cannot-linked and vice-versa) and clustering to satisfy these flipped con-ure 1 (a) then this clustering can be uniquely represented as the constraints must-link(a,c), must-link(b,d), must-link(e,f), cannot-link(c,d), cannot-link(c,e) and cannot-link(d,f) (not all entailed con-straints are provided for clarity) shown in Figure 1 (b). However, flipping these constraints for even this simple six point data set produces cannot-link(a,c), cannot-link(b,d), cannot-link(e,f), must-link(c,d), must-link(c,e), must-link(d,f) shown in Figure 1 (c) for which no clustering exists that satisfies all constraints. For sim-ilar reasons it is not desirable to learn a distance function from the flipped constraints due to the many inconsistent constraints that flipping could generate. Furthermore, even if a set of non-contradic-tory constraints could be generated, then trying to find just a single clustering to satisfy them is known to be NP -complete [7] for any constraint type combination involving cannot-linked constraints. Davidson and Ravi have shown that clustering under many cannot-linked constraints is intractable for batch [6], incremental [8] and even pruning-style algorithms [7]. This is a large hurdle since we most certainly wish to generate must-linked constraints from points in the same cluster but flipping them will produce the undesirable cannot-linked constraints. Finally, approaches that can deal with inconsistent constraints/advice in a principled manner were lim-ited. For example, the work of Coleman et al. [3] that deals with embedding constraints into the spectral clustering algorithm only addresses the problem where no object is involved in more than 1 cannot-linked constraint, and only for k = 2 .

As we discussed in the introduction section, the problem of find-ing two clustering simultaneously is a different problem from our problem setting. That problem has been formulated under the frame-work of the EM algorithm [10]. Their two approaches, Decorrelated-kmeans and Convolutional-EM in Jain, Meka and Dhillon X  X  work are based on two separate assumptions. The first one assumes that if the "representative" vectors (which are different from the mean vectors and lack of intuitive interpretation) of the old clustering and the new clustering are mutually orthogonal, then the alternativeness of two clusterings should be guaranteed. The second approach in-terprets each clustering as a partial representation of the data and models the data as a sum of mixture distributions, each mixture corresponding to a clustering. Note that there is no transformation of the data involved in both methods.
Let X = { x 1 , x 2 , ..., x n } X  R d denote the given d -dimensional data set which is represented by a d  X  n matrix. The original clustering  X  is found in X . The transformation matrix D is a d  X  d matrix while Y = { y 1 , y 2 , ..., y n }  X  R d refers to the transformed data set by transformation Y = DX . The alternative clustering  X  is found in Y . Let X and Y follow the probability density functions p ( x ) and p y ( y ) .

The output of a clustering algorithm is a k -block set partition of the data set X which is referred to as a clustering . Each block forms a cluster, and they are referred to as C 1 , C 2 , ..., C of cluster C i is denoted as n i . The cluster centroids are denoted as m 1 , m 2 , ..., m k .

Please refer to Appendix B for the complete table of notations.
To achieve our goal of finding an alternative clustering with good quality in a general purpose manner we explore a data transforma-tion approach that converts the original data X to Y using a dis-tance metric represented by the transformation matrix D . Our for-mulation allows the user to choose any appropriate clustering algo-rithm to run on Y to achieve the new alternative clustering hence we term our approach algorithm-independent. Note that another option is to directly use the transformation matrix D generated in our approach along with the old data X by using the transformation matrix D as the distance metric in any distance based algorithm. The first option is more useful since we do not always have a dis-tance based algorithm, and it is used in the experimental section of this paper.

There are two main factors in our work:
According to these two factors, we formulate the problem as a constrained optimization problem, as shown in Eq.(1), where B = D
T D and k X k B denotes the Mahalanobis distance with this weight matrix B .
The objective function of the Kullback-Leibler divergence is a measure of the difference between two probability distributions. When D KL ( p y ( y ) || p x ( x )) = 0 , the two distributions p p ( y ) of X and Y are the same. We minimize KL divergence in Eq.(1) so that the probability density functions of X and Y are closely matched. This ensures that the inherent properties of X are not destroyed when being transformed to Y .

The constraint in Eq.(1) comes from the characteristics that we expect the new clustering  X  0 to express. We now explain our ini-tial and most general constraint. Variations in this constraint are discussed in Section 4. The constraint is best explained in a prob-abilistic framework. To simplify the problem formulation, assume that the clusters of  X  0 follow a mixture model of multivariate Gaus-sian distributions f 1 ( y ) , ..., f k ( y ) with the same covariance matrix  X   X  but different means  X  m 1 , ...,  X  m k , respectively. In other words, we assume that each cluster in  X  0 follows a multivariate Gaussian distribution with the same covariance matrix  X   X  . Let  X  m be the projection of the original centroids m 1 , ..., m k space. Note that these means are different from the centroids the algorithm will find. Let C 1 , C 2 , ..., C k denote the k clusters in  X  and b C 1 , b C 2 , ..., b C k denote the k new clusters in  X  ability density function of y is p ( y ) = where b n i is the size of cluster b C i in  X  0 . Consequently we have
Suppose object x i belongs to the cluster C j in  X  , which means clustering we must transform the data so that x i is more likely to be assigned to a different cluster other than C j in the new space. Then the probability of object y i belonging to b C j (being closest to b m j ) in the new clustering  X  0 should be small, which is written as ( 0  X   X   X  1 ): This is equal to Eq.(4).
When there is no specific assumption of the sizes of clusters in  X  , we can assume that each cluster has the same size. Thus for (1  X  j  X  k ) . In the multivariate Gaussian model, we assume that b  X  has the same variance along each dimension and dimensions are highly independent. Then the off diagonal entries are very small and can be ignored, and b  X  can be approximated by a multiplication of a scaler and an identity matrix  X  2 I (  X  &gt; 0 ). Therefore Eq.(4) becomes:
Since ( a 1 + a 2 + . . . + a n ) /n  X  n  X  a 1 a 2 . . . a 0 (1  X  i  X  n ) , Eq.(6) must hold if Eq.(7) is true: Therefore, we have constraint becomes (  X  &gt; 0 ):
Since Y = DX ,  X  m = Dm and B = D T D , for x we have
To derive the solution, we define an auxiliary covariance matrix e  X  , shown in Eq.(11). We see that e  X  is a d  X  d matrix and can be interpreted as the variance of the data with respect to k  X  1 centroids since the centroid which each instance is assigned to in  X  is excluded.
Then the solution to our constrained optimization problem de-fined in Eq.(1) is B = e  X   X  1 and since B = D T D we have our transformation matrix D = e  X   X  1 2 . Details of the solution are de-scribed in Appendix A. We see that e  X  is essentially the summation over n (the number of instances) d  X  d matrices. Each of these n matrices is in turn a summation of further ( k  X  1) d  X  d matrices. Each of these n ( k  X  1) matrices measures the variability caused by a point for a given centroid which this point unlikely belongs to in  X  . We then see that the solution to Eq.(1) is to transform the data so as to reduce this variability which in turns satisfies the upper bound in the equation.

The constraint in our formulation (Eq.(1)) is exchangeable with different specifications of the expected properties in the new clus-terings. We will discuss the details of variations of the problems in Section 4.

An illustrative example. We use the following simple example to illustrate our techniques. Figure 2 (a) shows that the data set X is composed of four multivariate Gaussian distributions at four corners of a square with the same variance along each dimension. The given clustering  X  with two horizontal clusters is shown in Figure 2 (b). We see that e  X  e
 X  = indicating that there is more variability between the points along the y -axis than the x -axis. Then the resultant transformation D is to compress more along the y -axis than the x -axis. Therefore, when X is transformed to Y = DX , the new clustering  X  0 with two vertical clusters as shown in Figure 2 (c) is more likely to be found.
Our basic formulation of the constraint part of the optimization problem in Eq.(1) essentially transforms the data but makes sure that each point is not assigned to the same cluster as before. In this section we will discuss other variations to guide the data trans-formation. In particular we shall explore three main variations of general use, but there may be others of more specific use. We will empirically verify our approach to the first and second variations of the problem in Section 5.

The three variations allow: 1. Specifying a trade-off between the alternativeness and qual-2. Specifying which clusters in the original clustering to keep 3. Finding an alternative clustering in a subspace.
 Recall that the constraint in Eq.(1) takes the form Each of the above variations involves changing some aspect of this basic form, as we now describe.
We add the parameter a  X  1 to quantify the trade-off between alternativeness and quality by redefining the constraint as follows:
We see that the larger a is, the stronger the constraint of assigning an object to a different cluster in the new clustering will be (see our probabilistic interpretation of this constraint in the previous sec-tion). Hence the optimization is focused/biased more towards al-ternativeness. Conversely if a is made small then the constraint is weaker. The solution to the modified optimization problem is then B = e  X   X  a 2 , that is, D = e  X   X  a 4 .
To allow this we can have multiple constraints (summations) for different clusters. For a cluster (say C j ) we wish not to keep we employ the same constraint as in Eq.(1) except the summation is limited to points only in C j . For a cluster (say C l ) we wish to retain we then have the constraint: is some small constant value. The new form of the constraint in our problem formulation is shown in Eq.(13) where the clusters of C Y = { C 1 , ..., C r } (1  X  r &lt; k ) are retained and the clusters of C
N = { C r +1 , ..., C k } are not retained. Note that the first summa-tion is related to the points in the clusters to be maintained, and the second summation is related to the points in the clusters not to be maintained.

The solution to the modified optimization problem is then B = e  X  1 , that is, D =
There are some cases where the partial clustering that needs to be kept is not composed of whole clusters but some small chunklets of objects. We can specify the information that objects x should be in the same cluster as a constraint s ( x i , x C
Y = { c 1 , ..., c t } includes all the chunklets c 1 , ..., c is supposed to be retained and the objects in C N = X \ A should change their assignment in the new clustering. We generate a con-that should be in the same cluster. The constraint formulation is as in Eq.(15), and the auxiliary matrix e  X  2 is redefined in Eq.(16).
The solution to the modified optimization problem in Eq.(15) is then B = e  X   X  1 2 , that is, D = e  X   X  1 2 2 , where e  X 
In the original formulation we transformed the data using all en-tries/dimensions in B , but in this variation we normalize over only a subspace in B . For example, we may find that the given clustering  X  is most compact in some subset of dimensions and wish to find an alternative clustering in the complement of this subset. This is effectively finding an alternative clustering in the complementary subspace that  X  is most compact in. It can be achieved by fixing the row and column entries in B to be zero for all dimensions that the clustering  X  is most compact in. Then it makes all the points along each of these dimensions mapped to the dimension origin, ef-fectively making these dimensions useless for differentiating points into clusters.
We present three sets of experimental results. We will now sketch the results and in later subsections provide full details that will al-low their repetition. Note that the source code in MATLAB used to reproduce these results will be made available, and we have posted the source code at www.constrained-clustering.org . In all of the experiments, we use three measurements: Dunn Index (DI), Vector Quantization Error (VQE) and Jaccard Index (JI) to evaluate the results. The DI is a quality measure of the ratio of the minimum distance between two clusters (when measured as the av-erage link distance) to the maximum cluster diameter. The larger the DI the better. We also report the VQE for clusterings as it is the objective function that k -means minimizes. The smaller the VQE the better. Note that the DI is a measure of separation be-tween clusters normalized by the cluster diameters, while the VQE only measures cluster compactness, not their separation. The Jac-card Index (JI) measures the similarity between two clusterings, the smaller the JI, and the more dissimilar the two clusterings are. All of the measurements are calculated based on the original data X .

All of the clusterings related to the UCI data are obtained by the k -means algorithm. The experiments on image segmentation uses the spectral clustering algorithm as defined by Shi and Malik [11]. As before,  X  is the given clustering and the new alternative clustering is  X  0 .
Our first set of results is on standard UCI data [1] sets and com-pares our work against others [2], including our previous work [5]. The comparison to the work of Cui et.al. [4] can be referred to in our previous paper [5], but their approach does not work well for lower dimensional data.

We show (see Table 1) that our work is comparable to similar work with respect to quality of clustering found (when measured by the VQE) and diversity between the original and alternative clus-tering (when measured using the Jaccard index). The approach of Bae and Bailey [2] obtains better DI results but worse VQE results than our own. This can be explained by the fact that the objective function of their algorithm is the DI and for k-means it is the VQE. However, our approach has the advantage of being usable with a variety of distance based clustering algorithms, being able to pro-vide both positive (keep a cluster) or negative (don X  X  keep a cluster) feedback (which will be discussed in Section 5.3) and being able to trade-off the two parts (alternativeness and quality) of the con-strained optimization problem. By modifying the exponent a we can favor making the alternative clustering more different than  X  but typically of worse quality and vice-versa, as shown in Table 2.
These types of experiments are typically performed to show that the approach finds a clustering of reasonable quality and is differ-ent from the original clustering. However, they do not show if the second clustering is truly an interpretable alternative to the original clustering. To show that, we need to focus on data sets where the results are readily interpretable, as we do for our next two sets of experiments.
Our second set of experimental results is for image segmentation using spectral clustering. We focus on several Escher images which are known to have multiple interpretations to the human eye. Con-sider Figure 3 (a) which has two interpretations. If the eye focuses on the black sections then there is a segmentation of the image into black and non-black as found by spectral clustering in Figure 3 (b). This is the dominant segmentation since the contrast between the two clusters is great as one cluster includes the black parts and the other cluster includes the orange and yellow parts. However, our approach is able to discover the second and more subtle two clus-ter segmentation in Figure 3 (c) where the orange is in one cluster and the black and yellow are in the other. Similar results are found for Figure 4 (a) which contains three types of butterflies (red, green and blue). Spectral clustering first finds for k = 2 a clustering of the blue butterflies by themselves and the green and red butter-flies together in Figure 4 (b). The alternative clustering found by our approach is the red butterflies by themselves and the blue and green butterflies together in Figure 4 (c). In Figure 5, the original clustering  X  partitions the image based on the blocked background and subsumes the mandolin, but the new clustering  X  0 separates the object of the mandolin from the background.

We use the normalized spectral method of Shi and Malik [11] as the clustering algorithm. Each object in an image is a pixel with two kinds of information: RGB value and position. The similarity between two pixels is the weighted sum of the Euclidean distance between their RGB values and position. The transformation is only carried out in the RGB value space. In Figure 3 and 4 we see that for the Escher images the new clustering  X  0 finds a different texture in the images. Figure 3: Escher flower example of an image with textures with alternative interpretations, k = 2 .
Finally in our last set of experimental results we show how our approach can focus on which clusters to keep and which not to keep. The cluster C l we chose to maintain is the one with the max-imum size/cardinality in  X  which is derived from the inherent labels in the UCI data set. Because the data set Ionosphere only has two clusters inherently it is not possible to find a much different clus-tering while maintaining one cluster. So we only focus on three data sets: Glass , ESL and Vehicle . Let function g ( C l largest number of points in C l which are in the same cluster in  X  For instance, if the data set has 6 objects X = { x 1 , ..., x { x 1 , x 2 , x 3 } , and the clustering  X  0 partitions X into two clusters C 1 = { x 1 , x 2 } and C 2 = { x 3 , ..., x 6 } , then g ( C size of subset C l is denoted as n l .

We define the hit rate as the percentage of objects kept in the same cluster in  X  0 , as follows: averaged over ten random restarts of each approach).
 ) JI DI(  X  0 ) VQE(  X  0 ) JI DI(  X  0 ) VQE(  X  0 ) 0.18 0.57 5.4* 10 6 0.22 0.77 5.0* 10 6 restarts of our approach). a = 3 2 a = 2 In Table 3, we shows that the new clustering  X  0 in three data sets is of good quality as measured by DI and VQE. Meanwhile,  X  not only different to  X  as compared by JI but also maintains the cluster we want to keep as indicated by the high Hit Rates. We can increase the Hit Rate by increasing the exponent a in Eq.(12).
Data mining aims to find novel and actionable patterns with most algorithms typically returning just one such set of results. However, in some circumstances we wish to find multiple alternative expla-nations of the data. In this paper we study the following problem: given a clustering, find a good quality alternative which we have termed the singular alternative clustering problem. This allows the domain expert who already has a not useful clustering to encode this knowledge so that the algorithm does not find the same clus-tering again. Multiple sequential solutions to this problem can also be used to find many possible alternative patterns in the data.
Previous works to address this problem, including our own, were limited in several ways. Firstly, they were (except our own [5]) algorithm-dependent; secondly, they were all (including our own [5]) unrefined in the sense that they only allowed the domain expert to say "find a completely alternative clustering" but did not allow them to specify what properties of the given clustering to keep or not keep.

In this paper we formulate a solution to this problem as a con-strained optimization problem that minimizes the Kullback-Leibler divergence between the probability density functions of the origi-nal data set and a new transformed data set. This ensures that the properties of the transformed data closely match those of the origi-nal data set. The constraints specify which properties of clustering should or should not be maintained and can be modified to multiple variations of the problem in a principled and flexible way. Varia-tions include finding alternative clusterings in subspaces as well as trading off clustering quality with alternativeness to the original clustering.

There are several advantages to our approach. It is general pur-pose since it can be used with many clustering algorithms based on a distance function, such as k -means and agglomerative algo-rithms. The transformation of the data is specified in closed-form and is easy to implement. Our approach can specify which parts of the clustering are desirable and which are not. Furthermore, the tradeoff between alternativeness and quality can be controlled by the parameter a in Eq.(12).

To validate our approach we performed a set of experiments on the low dimensional UCI data set (see Table 1) to compare our ap-proach against the techniques of others [2] and our previous work [5]. As discussed and shown in our earlier work, the approach of Cui et.al. [4] is not designed for lower dimensional data and hence does not perform well. It is important to note that all of the clus-tering quality measurements we present are calculated based on the original data X . That is, even though we transform X to an alternative space we measure the alternative clusterings X  prop-erties in the original space. We illustrated that our approach not only achieves clusterings of comparable quality but also finds a di-verse set of clusterings. We were able to focus on alternativeness or quality by adjusting the parameter a (see Table 2). Our approach can also find different interpretable clusterings in image segmen-tation applications (see Figures 3, 4 and 5). Finally, we presented the experiments where the desirable and undesirable clusters in the new clustering can be explicitly specified, and the results showed that our approach can find alternative clusterings while maintaining the expected clusters (See Table 3). Our future work will include kernelizing the current approach to find alternative clusterings by a non-linear transformation of the data.
 The authors thank the anonymous reviewers for their excellent com-ments and the NSF for support of this work via GRANT IIS-0801528 CAREER: Knowledge Enhanced Clustering with Constraints. [1] A. Asuncion and D. Newman. UCI machine learning [2] E. Bae and J. Bailey. Coala: A novel approach for the [3] T. Coleman, J. Saunderson, and A. Wirth. Spectral clustering ) VQE(  X  0 ) JI Hit Rate 0.57 2.0  X  10 7 0.31 0.99 Figure 4: Escher butterfly example of textures with alternative interpretations, k = 2 . [4] Y. Cui, X. Z. Fern, and J. G. Dy. Non-redundant multi-view [5] I. Davidson and Z. Qi. Finding alternative clusterings using [6] I. Davidson and S. S. Ravi. The complexity of [7] I. Davidson and S. S. Ravi. Intractability and clustering with [8] I. Davidson, S. S. Ravi, and M. Ester. Efficient incremental [9] D. Gondek and T. Hofmann. Non-redundant data clustering. [10] P. Jain, R. Meka, and I. S. Dhillon. Simultaneous [11] J. Shi and J. Malik. Normalized cuts and image Here we derive the solution to the optimization problem in Eq.(1):
Since Y is transformed from X by the transformation Y = DX , we can find the connection between two density functions p of transformation matrix D . Then we can rewrite the objective function in Eq.(1) as follows: To minimize D KL ( p y ( y ) || p x ( x )) is to maximize log | D | .
Therefore, Eq.(1) is equal to Eq.(19). max
Since B is a positive definite matrix where B = D T D , and it can be rewritten as follows: max
Use the method of Lagrange multiplier, log | B | X   X  ( 1 = log | B | X   X  ( 1 = log | B | X   X  ( 1 = log | B | X   X 
Take the derivative of B to get Eq.(22) and let it be 0. The scaler does not impact the transformation and can be removed. Then we get the solution B = e  X   X  1 i.e. D = e  X   X  1 2 . The following notations are used in this paper.

