 In this paper we focus on the problem of supervised clas-sification learning. In this setting we recei ve a training set of instance-label pairs S = f ( x ness we assume that x by x thus y the set of all threshold linear classifiers, sign ( x ) where 2 IR n . We do not assume howe ver that the data is lin-early separable, thus there exist few examples ( x such that y finding a linear classifier that attains the minimal number of classification errors on S is NP-hard [10]. Therefore, learn-ing techniques that emplo y a smooth and con vex bound on the classification (0-1) error have been widely used. Specif-ically , boosting techniques use losses such as the exponen-account of the two losses for boosting). The problem with con vex losses is that the y must diverge to 1 as the mar gin of an example, y problem, few approaches that emplo y non-con vex losses have been proposed. Unfortunately , such methods do not yield a boosting algorithm in the classical notion of weak-learnability due to an impossibility theorem by Duf fy and Helmbold [6]. Boosting-style methods that do not achie ve a PAC-boosting property were referred to as lever aging al-gorithms by Duf fy and Helmbold. Nonetheless, leveraging methods often yield effecti ve and accurate classifiers and are rob ust to label noise.
 In this work we propose two dif ferent leveraging algo-rithms that emplo y non-con vex losses of the mar gin. The proposed algorithms are as simple to implement as the orig-inal AdaBoost algorithm of Freund and Schapire [9] and rithms can be emplo yed in a sequential manner in which a single feature (or weak-h ypothesis) is chosen on each boosting step, as well as a fully parallel mode in which the weights of all the features are updated together . Last, but not least, when used in the classical sequential mode, the two algorithms induce a criterion (via an objecti ve func-tion) that guides the weak-learner in its search for a new hypothesis to add. This loss criterion is directly related to the decrease of the overall loss due to the addition of a new hypothesis.
 The first algorithm is described in Sec. 2. We start by giv-ing a probabilistic account which assumes the existence of an unobserv ed label noise. We deri ve an algorithm that combines the expectation maximization (EM) estimation procedure with boosting steps. This algorithm weights the examples according to their mar gin and their probability of being outliers. Boosting-style steps are used to update according to these weights. We pro ve that this algorithm con verges to stationary point. The second approach, which is presented in Sec. 3, decomposes the non-con vex loss function into a dif ference of two logistic functions, each of which is clearly con vex. Here we deri ve a leveraging al-gorithm that is based on a recent additi ve update for boost-ing [3]. We analyze the loss functions that both algorithms emplo y in Sec. 4. We deri ve a generalization bound for both algorithms based on the Rademacher comple xity in Sec. 5. We conclude with experiments using synthetic and natural data that demonstrate the rob ustness of the algo-rithms to label noise. These experiments are described in Sec. 6.
 Various pre vious research papers are used as building blocks in this paper . We use L. Baum X  s EM algorithm for maximum lik elihood of incomplete data which was for -mally introduced in [4]. Our boosting steps are based on the boosting algorithms described in [2] and [3]. Our algorith-mic approach of minimizing the dif ference of two con vex functions is described thoroughly in [17 ] and was first em-plo yed for classification learning in [19] to devise rob ust support vector machines. The generalization analysis we emplo y is based on the generalization bounds given in [1]. Our work is also more remotely related to numerous pa-pers in machine learning and statistics. The use of EM for missing labels and corrupted labels has already been men-tioned in the discussion follo wing the paper of Dempster , Laird, and Rubin. An example of an application of EM for missing labels can be found in [13]. The idea of combining EM with an iterati ve minimization procedure in a dif ferent conte xt (and dif ferent analysis) was recently explored by Wang et al. [18]. There are man y works that try to impro ve boosting, and give algorithms that are more rob ust to label noise. We surv ey here only a few of them. An algorithm that directly minimizes the 0-1 loss in k steps using weak learners with kno wn accurac y is described in [7]. This idea is enhanced to an adapti ve algorithm in [8], but using a comple x algorithm. In [16] a PAC boosting algorithm is developed using smooth distrib utions. This algorithm can tolerate low malicious noise rates but it requires the access to a noise tolerant weak learner of a kno wn accurac y. A similar approach to ours is presented by some papers. [12] emplo y the non-con vex normalized sigmoid function as the loss function of their leveraging algorithm DOOM II. Their algorithm includes only a sequential version, and the objec-tive function that the weak-learner has to minimize is not directly related to the decrease in the loss. Finally , the al-gorithm AdaBoost examples that had lar ge influence in pre vious rounds. It has extensions which solv e the LP problem (the LP equi valent of the SVM problem). These algorithms require line searches to compute the weights of the weak-learners whereas the algorithms described in this paper are simple to implement and their updates are computed analytically . 2.1. The Pr obabilistic Model In our probabilistic model we assume that the noise-free (true) label t on x noise is a Bernoulli variable, with parameter , such that with probability the correct label is inverted, and the ob-serv ed label y due to these assumptions are as follo ws,
We define bel y the probability that label noise occurred given x
The goal is to devise a classifier that is more resistant to noise than boosting by incorporating the noise model abo ve. We therefore cast our task as finding the parameters = ( ; ) which minimize the negative log-lik elihood of the observ ed data, which we call the logistic mixture loss,
In the algorithm description belo w and the experiments, we either allo w to be fix ed or estimate its value. 2.2. Combining Boosting Updates with EM The EM algorithm [4] is the standard tool of choice for pa-rameter estimation from incomplete data. The algorithm is composed of two steps: an E (Expectation) step and an M (Maximization) step. By repeating the E and M steps we con verge to a stationary point of the log-lik elihood of the model. We now specify the EM iterations for our set-tings. In the E step we replace each hidden variable (the unobserv ed value of t which we denote In the M step we set maximize the expectation of the log lik elihood of the unobserv ed data and the observ ed data, where prob-abilities are calculated using t +1 = argmax Q ( ; t )
Maximization of Q ( ; "
The abo ve expression implies that maximizing Q ( ; with respect to is equi valent to minimizing a weighted lo-gistic loss with 2 m examples where each original example ( x with weight 1 LLM ( M; "; T; update-"; A ) Requirements: init:
For t=1,.., T End An Appr oximate M-Step: We now focus on adapting a boosting-based algorithm for finding the minimizer of Eq. (2). Since there is no analytical solution for the min-imizer , we need to devise an iterati ve minimization proce-dure. These iterations are interlea ved with the E-step of EM which computes the auxiliary variables To deri ve our approximate M-step we first briefly describe one of the algorithms described by Collins et al. [2]. The algorithm belongs to a whole family of algorithms for find-ing the minimizer of the log-loss, P To find the minimizer of the log-loss Collins et al. devised the follo wing iterati ve procedure. Let mate for the minimizer of the log-loss at the j X  X h iteration. Then, each example ( x on this estimate where, q example reflects the probability of misclassifying the ex-ample according to the logistic model defined by Eq. (1). For ease of reading we define, from here on, the matrix M by M q we denote W +
Informally speaking, these weights reflect the correlation between the probability to misclassify the examples and the absolute value of the j  X  X h feature for each example. The next estimate and the weights as follo ws,
The abo ve update was deri ved for noise-free classification settings with the log-loss. Our setting is howe ver more in-volv ed. We observ e a noisy version of the labels and need to minimize the loss with respect to the, hidden, true la-bels. Therefore, each example is associated with two pos-sible true labels y respecti vely . For each possible label we need to tak e into account its weight and probability according to the logistic model (Eq. (1)) as reflected by q to compute W + and  X  X plit X  the contrib ution of each example between W + and W defined by Eq. (2),
We can now proceed as before to compute as given by Eq. (3). The M-step of an EM algorithm re-quires finding the minimizer of Eq. (2). This can easily be done using man y iterations of the abo ve update rule. Ho w-form a single update as an approximate M-step. In this case we only update 0 such that it decreases Q ( 0 ; ) , yet not minimizes it, and this is in fact a GEM algo-rithm. When performing a single approximate M-step we can further simplify the boosting-based update. In which case we can rewrite Eq. (4) and Eq. (6) and avoid the ad-ditional computation imposed when computing W + W and " is emplo yed for the computation of both q . We now describe the more efficient single approximate M-step. Define ~ W + ~ W j = P is easy to verify that (1 we can rewrite the original variables W + W pseudo code for the version that alternates between an E-step and a single boosting-based M-step is given in Fig. 1. We term the algorithm the Le veraging Logistic Mixture algorithm (LLM ). This version was used in our experiments, howe ver other variants, such as extensions to multiclass, can also be deri ved. The details of these vari-ants and extensions are omitted due to the lack of space. A parametric family of algorithms: This algorithm re-cei ves as input a template set A . Using the template set we describe in a unified vie w both a parallel algorithm and a sequential one. The choice of algorithm to be implemented is done by setting the appropriate template set. By setting gorithm that updates all the indices in each step. If we let sequential boosting algorithm. This algorithm has a simple criterion for choosing the weak learner at each step -choose the weak learner j which maximizes q W + Our proof of con vergence also holds if we change A in each iteration, as long as the constraints defined on A and M are kept satisfied. Therefore we can also implement a com-bined version, in which we sequentially update, yet once in a while we update the weight of the weak learners accumu-lated so far in parallel. 2.3. Con vergence Analysis In this section we discuss the con vergence of the LLM al-gorithm. In short, the LLM algorithm belongs to the class of Generalized EM algorithms [4]. The follo wing lemma sho ws that each approximate M-step is guaranteed to in-crease the value of the auxiliary function Q . Our proof combines the proof techniques from [2] with standard anal-ysis for EM.
 Lemma 2.1 : Let 0 be the set of par ameter s obtained after the update of . Then Q ( 0 ; ) Q ( ; ) , with equality only if is a stationary point of Q .
 Pr oof: To pro ve the theorem we vie w the loss as induced by a sample of 2 m examples where the first m examples are ( x amples are ( x with weights we can rewrite Q as, where c is a constant that depends only on " 0 . We denote by q fore and after the update respecti vely , q q get that, q 0 the inequality 1 + x e x , we get that,
We can rewrite Q ( 0 ; ) using q 0
Note also that using and W
Denoting s change in Q in the M-step as follo ws,
Eq. (9) follo ws from Jensen X  s inequality applied to the con vex function e x while using the condition that P d The function A ( q ) is an auxiliary function that bounds from belo w the increase of Q . Clearly , A ( q ) 0 and A ( q ) = 0 iff 8 j W + j = W j . When A ( q ) = 0 we get by simple deri vation that @ this means that 0 is a stationary point of Q . If we have not reached a stationary point the value of Q will continue to increase on each approximate M-step.
 Using the definition of Q and the abo ve lemma we see that
Thus, the series Q ( con verges in value to a stationary point. It directly follo ws that the loss, L In this section we present a seemingly dif ferent approach to obtain a bounded mar -gin loss function and a cor -responding leveraging algo-rithm. Instead of devising a mixture model of logistic function we devise a bounded mar gin loss by taking the dif-fer ence of two logistic func-tions as follo ws. Let z denote the mar gin of an example. The logistic loss of an example with mar gin z is ` ( z ) = ln(1 + e z ) . Let be a positi ve constant. A  X  X hifted X  version of the loss is obtained by LLD ( M; ; T; A ) Requirements: init:
For t=1,.., T adding to z , ` ( z + ) = ln(1 + e z ) . We now de-fine the loss at mar gin z to be the dif ference between the logistic loss and its shifted version,
The construction of this loss is described in Fig. 3. We term mixture model, we would lik e to find a vector such that the sign of x the logistic-dif ference loss of the sample S w.r.t is,
Our leveraging algorithm for the LD loss is based on a recent additive update for boosting [3]. In our algorithm, each example is given a weight which is equal to minus the deri vative of the loss evaluated at the example X  s mar gin. More formally , let g ( z ) = d the weight of an example attaining a mar gin z is g 0 ( z i ) g ( z i ) . The pseudo code of the resulting lever-aging algorithm, called the Le veraging Logistic Dif ference algorithm (LLD ), is given in Fig. 2. Intuiti vely , on each leveraging iteration, LLD approximates the conca ve part of the LD loss, ` ( z + ) , with a linear function. Since the dif ference between a con vex function and a linear function is con vex, we can now use boosting technology . Specifi-cally , LLD performs an additi ve boosting-step, analogous to the one described in [3] on the con vexified function. We also exploit this construction to deri ve in the sequel a lower bound on the progress of LLD and pro ve its con vergence. Con vergence analysis Our con vergence proof is divided into two parts. In the first part we sho w a simple con vex up-per bound on the logistic dif ference loss. In the second part we sho w that the t  X  X h iteration decreases this upper bound (unless we reached a stationary point). We get that the lo-gistic dif ference loss decreases on each iteration, and there-fore con verges to a stationary point as it is bounded belo w. Recall that every dif ferentiable conca ve function f is bounded by its affine minorization at every point, i.e.
By applying this to the conca ve part of the loss we get ` ( y i t x i + )) . This upper bound is linear and thus also con vex. We add it to the con vex logistic loss and get the follo wing con vex bound on the loss, where d = P m : L LD ( ; S ) ^ L ( ; t ; S ) , with equality for = t . We now concentrate our efforts on sho wing that ^ L ( ; decreases unless we are at a stationary point.
 Lemma 3.1 : The decr ease in ^ L ( Pr oof: We pro ve the lemma by using a quadratic func-tion Q ( ) which upper bounds ^ L ( fix ed Define,
In order to pro ve that Q upper bounds ^ L we define their dif-ference ( ) = Q ( ) ^ L ( ( ) 0 . We do this by sho wing that is con vex and its minimum is attained at = 0 (where we have (0) = 0 ). First note that 0 (0) = 0 . Hence, 0 is a sta-tionary point of . Taking the second deri vative of we get, 00 ( ) = ( r L tions yield that 0 L 00 No w we plug in the definitions we use in the algorithm. We note that W = ( r L
Where we used Cauch y-Schw artz inequality in the first inequality , and the constraint P the last inequality .
 We have sho wn that ( ) 0 and thus Q upper bounds ^ L . Since Q (0) = ^ L ( from belo w as follo ws, Cor ollary 3.2 : The decr ease in the loss at step t satisfies Lemma 3.3 : The algorithm con ver ges by value to a sta-tionary point of its loss.
 Pr oof: As long as rL a positi ve amount (because W = rL sequence of losses decreases. It is bounded belo w by 0 , and hence it must con verge. By continuity the con vergence point must be a point in which rL In this section we sho w that there is a simple bijection from to which mak es the losses of LLM and LLD identical. We also describe a simple affine transformation that nor -malizes either loss so that the resulting loss approaches as the mar gin goes to + 1 while bounding everywhere the 0 1 loss. Let us fix and . We now add a constant to each loss and divide the result by another constant in order to normalize the losses. The resulting normalized losses are,
Examining carefully the abo ve losses, we can see that by setting e = obtain that,
Hence, the two losses are identical subject to the trans-formation of variables, = ln( 1 ) , or equi valently = 1 1+ e . Since the two losses are equi valent, from now on we refer in our analysis only to the logistic mixture loss. For later use we calculate the Lipschitz constant of the loss, that is, find a constant such that for any z; z 0 ,
LM ( z ) In [1] generalization bounds are given for classes of func-tions using their Rademacher comple xity . In short, the Rademacher comple xity of a class of functions F , denoted R m ( F ) and formly from f 1 g . The bounds in [1] are suited for de-cision theoretic settings in which we attempt to minimize a combinatorial loss function (such as the classification er-ror) via the minimization of a dominating cost function whose range is [0 ; 1] . In our case the loss function is the classification error and the dominating cost function is the loss L N order to use the Rademacher bounds from [1] we need to slightly generalize Thm. 8 of [1] to cost functions whose range is [0 ; M ] .
 Theor em 5.1 : [Bartlett and Mendelson, Thm. 8] Con-sider a loss function L : Y A ! [0 ; M ] and a dominat-ing cost function : Y A ! [0 ; M ] . Let F be a class of functions mapping from X to A and let ( X dependently selected according to the probability measure P. Then, for any inte ger m and any 0 &lt; &lt; 1 with proba-bility at least 1 over samples of length m, every f in F satisfies where ~ F = f ( x; y ) 7! ( y; f ( x )) ( y; 0) : f 2 F g To deri ve a bound for our problem, we need to bound the Rademacher comple xity of linear classifiers. Bartlett and Mendelson pro ved a bound on kernel functions. This bound can be rewritten in our setting and no-tation as follo ws: Assume jj x F = f x 7! x jjj jj 2 B g , then ^ R m ( F ) 2 B p algorithm we assumed that jj x that jj x (as described in [3]) implies that jj jj that jj jj ties, we get that in our case ^ R We conclude by adapting the result of theorem 21 in [1] to our setting. Using the bound M and Lipschitz constant which were calculated in Sec. 4 for our cost function L N and by applying Theorem 5.1 we get, Cor ollary 5.2 : With probability 1 every f 2 F satisfies Synthetic data: In this experiment we generated 1000 random points in IR 40 according to the multi variate nor -mal distrib ution N (0 ; I ) . We also randomly pick ed a hyper plane 2 IR 40 using the same normal distrib ution, and as-signed to the i X  X h point the label y vided the points into four groups according to their mar gin. The groups are numbered such that the first group contains the quarter of the points attaining the lar gest mar gin while the fourth group contains the quarter of the points attaining the smallest mar gin. We conducted five experiments. In each experiment a dif ferent subset of the groups was con-taminated with label noise. In the first experiment no group was contaminated with label noise. In the second exper -iment only the first group was contaminated with a label ments we contaminated the first and the second group, and so on until all four groups were contaminated with label noise resulting in a uniform Bernoulli noise for all points. Since we contaminated only a subset of the points, the over-all noise rate in experiment i was p ( i 1) = 4 . In each ex-perimental setup we compared the log-loss version of the parallel boosting algorithm from [2], to the LLM algorithm with " set to be p , and to the LLD algorithm with set to ln((1 p ) =p ) . To compare the performances of the algo-rithms we generated a test set containing 1000 points. The test set, in contrast to the train set, is noise-free. This is compatible with our assumption, in Sec. 2.1, that the data is actually linearly separable, but our train set was contami-nated with label noise. We repeated this experimental setup with four dif ferent values of p , and each experiment was re-peated ten times. Average results are presented in Fig. 4, in which each plot corresponds to a dif ferent value of p . cious noise which flips the labels of points that attain lar ge mar gin values. This can be seen by the high increase of error caused by the contamination of the first quarter (high mar gin examples). As we contaminate the other quarters, the increase of error becomes much smaller . In contrast, LLM actually leverages from this type of noise. The er-ror for LLM is very low when we contaminate only the first two quarters. Only when the last quarter is contami-the overall error of LLM is much lower then that of boost-ing. The error of LLD increases linearly with the amount of contaminated quarters. Thus, LLD is less sensiti ve to the as LLM. The poor performance of boosting is not surpris-ing  X  quite a few papers have formerly noted that boosting algorithms are highly sensiti ve to label noise (see for in-stance [5] and the references therein). This type of noise howe ver is far less crucial for LLM and LLD as it con veys information on the examples whose labels are lik ely to be incorrect. The error rate of LLM and LLD increases as the noise becomes more uniform. Nonetheless, even with an i.i.d Bernoulli noise, LLM and LLD exhibit a far lower test error than boosting.
 USPS: The USPS (US Postal Service) dataset is kno wn to be a challenging classification task particularly since its The dataset contains 7 ; 291 training examples and 2 ; 007 test examples. Each example is represented as a 16 16 matrix where each entry in the matrix is a pix el that can the image. We brok e the 10 -class problem into 10 binary problems. The i  X  X h problem was to discriminate the digit i from the rest. For each binary problem we compared the test error of the follo wing algorithms: log-loss boosting , LLM with fix ed " (at 0 : 08 and 0 : 16 ), LLM with variable " (starting from 0 : 08 and modified every 100 approximate M-steps), and the LLD algorithm with = 4 . A compar -ative representation of these results can be seen in the left side of Fig. 5. In this figure we sho w the dif ference be-tween the test error (percentage wise) of boosting and the test error of our algorithms. Each group of bars describes the results of one of our algorithms on each of the digits. We can see that after 100 boosting steps the LLD algorithm is the best performing algorithm. As we increase the num-ber of iterations LLM , especially with a fix ed value for , tak es the char ge. LLM clearly outperforms the boosting algorithm. This may partially contrib uted to our mar gin loss function which is less prone to outliers, as discussed in Sec. 5. From Fig. 5 we see that LLM impro ves the boost-ing result at least by a small mar gin, and for some digits its error rate is lower than boosting X  s by as much as 0.8%. UseNet: This dataset consists of Usenet articles collected by Lang [11 ] from 20 dif ferent newsgroups. One thou-sand articles were collected for each newsgroup so there are 20 ; 000 articles in the entire collection. In our experiments we randomly divided the articles from each newsgroup into a training set of 750 articles and a test set of 250 articles. The binary classification tasks we check ed discriminate be-tween articles from pairs of topics. Due to lack of space we sho w the results for only four pairs of newsgroups topics. We compared log-loss boosting with the sequential version of LLM using single words as features. (The i th entry in a vector representing a document is 1 if the i th word ap-pears in the document, and is 0 otherwise.) A comparison of the results obtained by LLM and boosting is given on the right side of Fig. 5. When the number of rounds is 100 or 1000 then LLM often outperforms boosting. The error rate of LLM in man y cases is as much as 2% lower . As the number of rounds gets to 10 ; 000 the results of LLM and vanilla boosting become indistinguishable. One possi-ble explanation to this type of beha vior is that LLM finds better base-h ypotheses (i.e. words) than boosting due to its impro ved criterion for choosing features. Alas, as the number of rounds gro ws, boosting has the opportunity to choose man y features and thus close the gap. We plan to examine this angle more thoroughly in future research.
