 data sets with no significant decrease in performance.
 to the performance improvements gained over the standard Nyst r  X  om method. used in the following sections, and then describe our ensemb le Nystr  X  om algorithms. 2.1 Standard Nystr  X  om method of
K [5, 18]. Let C denote the n  X  m matrix formed by these columns and W the m  X  m matrix method generates a rank-k approximation e K of K for k  X  m defined by: where W and  X  = diag(  X  is given by W + complexity of the Nystr  X  om approximation computation is O ( m 3 + nmk ) . 2.2 Ensemble Nystr  X  om algorithm derive an improved hypothesis, typically more accurate tha n any of the original experts. p subsamples S a rank-k Nystr  X  om approximation e K r , e the columns of S further receives a sample V of s columns used to determine the weight expert e K algorithm is The mixture weights assigning equal weight to each expert, e K each expert e K  X  = ( 1 ,..., p ) belongs to the simplex  X  of R p :  X  = {  X   X  R p :  X   X  0  X  Let K the submatrix of e K error  X   X  optimize a regression objective function such as the follow ing: where K to this method as the ridge regression method .
 The total complexity of the ensemble Nystr  X  om algorithm is O ( pm 3 + pmkn + C n of Nystr  X  om algorithm with m samples. of McDiarmid X  X  concentration bound to sampling without rep lacement [3]. Theorem 1. Let Z such that for all i  X  [1 ,m ] and for all z where  X  ( m,u ) = mu defined by S by d K 3.1 Error bounds for the standard Nystr  X  om method the form k K  X  e K k error of the Nystr  X  om approximation of the form k K  X  e K k latter of which is needed to derive tighter bounds in Section 3.2. uniformly at random without replacement from K , and K where  X  ( m,n ) = 1  X  1 where Z = p n bounded by d following inequality: The expectation of  X  can be bounded as follows: Theorem 1 give a bound on k XX  X   X  ZZ  X  k Bounding the term k XX  X   X  ZZ  X  k 2 Theorem 1 yields the result of the theorem. 3.2 Error bounds for the ensemble Nystr  X  om method Nystr  X  om approximations.
 Theorem 3. Let S be a sample of pm columns drawn uniformly at random without replacement from K , decomposed into p subsamples of size m , S rank-k Nystr  X  om approximation of K based on the sample S sample S of size pm and for any  X  in the simplex  X  and e K ens = P p where  X  ( pm,n ) = 1  X  1 Proof. For r  X  [1 ,p ] , let Z to the sample S proof of theorem 2, the following holds: We apply Theorem 1 to  X  ( S ) = P p S subsample S change to k XX  X   X  Z The expectation of  X  can be straightforwardly bounded by E[ X ( S )] = P p Z ensemble Nystr  X  om method.
 general inequality (11), we can write The result follows by the application of Theorem 1 to  X  ( S )= P p similar to the norm-2 case.
 m norm-2 is smaller by a factor larger than performance of various methods for calculating the mixture weights ( 4.1 Ensemble Nystr  X  om with various mixture weights ( s to gain in performance occurs as p increases from 2 to 10 . 4.2 Large-scale experiments for various subsamples of the SIFT-1M dataset, with n ranging from 5 K to 1 M. and s  X  =2 , respectively. imation. ble approximation. Legend entries are the same as in Figure 1 . k s/n  X  100% . time, ensemble Nystr  X  om with ridge weights tends to outperform other techniques. 10% . better insight about the convergence properties of our algo rithms. [1] A. Asuncion and D. Newman. UCI machine learning reposito ry, 2007. [10] Y. LeCun and C. Cortes. The MNIST database of handwritte n digits, 2009. [13] J. C. Platt. Fast embedding of sparse similarity graphs . In NIPS , 2004.
