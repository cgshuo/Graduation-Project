 1. Introduction
Classi fi cation is one of the fundamental tasks in data mining ( Dehuri and Ghosh, 2013 ) and pattern recognition ( Nanda and
Panda, 2013 ). Over the years many models have been proposed. ( Huang and Wang, 2006 ; Chatterjee and Bhattacherjee, 2011 )
However, it is a consensus that the accuracy of the discovered model (i.e., neural networks (NNs) ( Haykin, 1994 ; Yaghini et al., 2013 ), rules ( Das et al., 2011 ), and decision tree ( Carvalho and
Freitas, 2004 )) strongly depends on the quality of the data being mined. Hence inconsistency removal and feature selection brings lots of attention of many researchers ( Battiti, 1994 ; Yan et al., 2008 ; Keynia, 2012 ; Ebrahimzadeh and Ghazalian, 2011 ; Liu et al., 2010 ). If the inconsistent data are simply deleted or classi new category then inevitably some useful information will be lost.
The method used in this paper for making the dataset consistent is based on the Bayesian statistical method ( Wu, 2007 ). Here the inconsistent data are classi fi ed as the most probable one and the redundant data records are deleted as well. So the loss of radial basis function network is modeled using differential evolution for the classi fi cation of both balanced and unbalanced datasets.
In imbalance classi fi cation problems the number of instances of each class that occur can be very different ( Perez-Godoy et al., 2010 ).
Over the decade radial basis function (RBF) networks have attracted a lot of interest in various domains of interest ( Haykin, 1994 ; Novakovic, 2011 ; Naveen et al., 2010 ; Liu et al., 2005 ). The reason is that they form a unifying link between function approx-imation, regularization, noisy interpolation, classi fi cation, and density estimation. Moreover, training RBF networks is usually faster than training multi-layer perceptron networks. RBF network training usually proceeds in two steps: fi rst, the basis function parameters (corresponding to hidden units) are determined by clustering. Second, the fi nal-layer weights are determined by a least square method which reduces to solve a simple linear system. Thus, the fi rst stage is an unsupervised method which is relatively fast, and the second stage requires the solution of a linear problem, which is also fast.

The other advantages of RBF neural networks, compared to multi-layer perceptron networks, is the possibility of choosing suitable parameters for the units of hidden layer without having to perform a nonlinear optimization of the network parameters.
However, the problem of selecting the appropriate number of basis functions remains a critical issue for RBF networks. The number of basis functions controls the complexity, and hence the generalization ability of RBF networks. An RBF network with too few basis functions gives poor predictions on new data, i.e., poor generalization, since the model has limited fl exibility. On the other hand, an RBF network with too many basis functions also yields training data. A small number of basis functions yield a high bias, low variance estimator, whereas a large number of basis functions yield a low bias but high variance estimator. The best general-ization performance is obtained via a compromise between the con fl icting requirements of reducing bias while simultaneously reducing variance. This trade-off highlights the importance of optimizing the complexity of the model in order to achieve the best generalization. However, choosing an optimal number of kernels is beyond the focus of this paper.

In the training procedure of RBFNs revealing center of gravity and width is of particular importance for the improvement of the performance of the networks. There are many approaches along the line with their own merits and demerits ( Stron and Price, 1995 , evolution to reveal hidden centers and spreads. The motivation using differential evolution (DE) over other EAs ( Michalewicz, 1996 ) such as GAs ( Goldberg, 1989 ) is that in DE string encoding is typically represented as real valued vectors, and the perturba-tion of solution vectors is based on the scaled difference of two randomly selected individuals of the current population. Unlike
GA, the resulting step size and orientation during the perturbation process automatically adopt to the fi tness function landscape. The justi fi cation behind combining the idea of feature selection, data inconsistency removal with classi fi cation is to reduce the space, time, and thereby enhancing accuracy.

This paper is set out as follows. Section 2 gives an overview of the RBF network, feature selection, feature consistency, and differ-ential evolution. In Section 3 , the proposed method is discussed. Experimental setup, results, and analysis are presented in Section 4 .
Section 5 concludes the paper with a future line of research. 2. Background
The background of this research work is presented in this section. In Section 2.1, RBF network classi fi er is discussed. Feature connections w i . The output layer, a summation unit, supplies the response of the network to the outside world.

The radial basis function is so named because the value of the function is same for all points which are at the same distance from the center. 2.2. Feature selection
Feature selection (FS) ( Novakovic, 2011 ; Khusba et al. 2008 )is an essential task to remove irrelevant and/or redundant features.
In other words, feature selection techniques study how to select a subset of potential attributes or variables from a dataset. ( Liu and
Setiono, 1995 ). For a given classi fi cation problem, the network may become extremely complex if the number of features used to classify the pattern increases. So the reason behind using FS techniques include reducing dimensionality by removing irrele-vant and redundant features, reducing the amount of attributes needed for learning, improving algorithms  X  predictive accuracy, and increasing the constructed model's comprehensibility. After feature selection a subset of the original features is obtained which retains suf fi cient information to discriminate well among classes. The selection of features can be achieved in two ways ( Yu and Liu, 2004 ):
Filter method : it precedes the actual classi fi cation process. The fi lter approach is independent of the learning algorithm, compu-tationally simple, fast, and scalable. Using fi lter method, feature selection is done once and then can be provided as inputs to different classi fi ers. In this method features are ranked according to some criterion and the top k features are selected.
Wrapper method : this approach uses the method of classi fi tion itself to measure the importance of feature sets; hence the selected feature depends on the classi fi er model used ( Karegowda et al., 2010 ). In this method a minimum subset of features is selected without learning performance deterioration.

Wrapper methods generally result in a better performance than fi lter methods because the feature selection process is optimized for the classi fi cation algorithm to be used. However, wrapper methods are too expensive for large dimensional database in terms of computational complexity and time since each feature set that is considered must be evaluated with the classi fi algorithm used. Filter based feature selection methods are in general faster than wrapper based methods. 2.3. Dataset consistency
Let us de fi ne a few terms pertaining to data inconsistency. Let us de fi ne a few terms pertaining to data inconsistency ( Shin and
Xu, 2009 ; Dash, et al., 2000 ; Dash and Liu, 2003 ; Arauzo-Azofra et al., 2008 ).
 any inconsistent instances or patterns.
 inconsistent. To make the dataset uniform all inconsistent data subsets are made uniform.

Step 5: the most probable class for the inconsistent pattern is found out in this step. The probability of a pattern p s belonging to category c n is given by the following equation:
P  X  c n j p s  X  X f P  X  p s j c n  X  n P  X  c n  X g = P  X  p s  X  where P  X  c n  X  X j c n j = j D j ; P  X  p s j c n  X  X j d m \ c The most probable class for the pattern p s is the one for which
P  X  c j p s  X  is maximum. But if the class distribution of a pattern is quite even then the above probability may give a wrong most probable class. For example, if a pattern P s has three class values c1, c2, and c3 then the corresponding probability is 0.34, 0.33, and 0.33 respectively. In this case it is inappropriate to classify the pattern as c1. For this situation a threshold  X  is introduced. So, only when P  X  c q j p s  X  4 P  X  c n j p s  X  and P  X  c q j p s  X  classi fi ed as c q .

If the probability for all classes for a pattern is below the threshold then the inconsistent data subset containing this pattern is deleted from the dataset. Taking  X   X  .5, the inconsistent data subsets d1 and d3 can be uni fi ed as c1  X  1. 2.4. Differential evolution Differential evolution (DE) ( Storn and Price, 1995 ; Storn and
Price,1997 ; Storn, 1999 ) is a population based meta-heuristic search algorithm which typically operates on real valued chromosome encodings. Like GAs ( Forerest, 1993 ), DE maintains a pool of potential solutions which are then perturbed in an effort to uncover yet better solutions to a problem in hand. In GAs, the individuals are perturbed based on crossover and mutation . However in DE, indivi-duals are perturbed based on the scaled difference of two randomly chosen individuals of the current population. One of the advantages of this approach is that the resulting  X  step  X  size and orientation during the perturbation process automatically adapts to the function landscape.

Over the years, there are many variants of DE algorithms developed ( Price et al., 2005 ; Das and Suganthan, 2011 ), however, we primarily describe a version of the algorithm based on the DE/ rand/1/bin scheme (Storn et al., 1995). The different variants of the
DE algorithm are described using the shorthand DE/ x / y / z ,where x speci fi es how the base vector (of real values) is chosen (rand if it is randomly selected, or best if the best individual in the population is selected), y is the number of difference vectors used, and z denotes the crossover scheme (bin for crossover based on independent binomial experiments, and exp for exponential crossover).
A population of n, d -dimensional vectors x i  X  X  x i 1 ; x i  X  1 ::: n each of which encodes a solution is randomly initialized and evaluated using a fi tness function f ( ). During the search process, each individual ( i ) is iteratively re fi ned. The following three steps are required while execution. (i) Mutation : create a donor vector which encodes a solution, using randomly selected members of the population. (ii) Crossover : create a trial vector by combining the donor vector with i. (iii) Selection : by the process of selection, determine whether the newly created trial vector replaces i in the population or not.
Under the mutation operator, for each vector x i  X  t  X  , a donor vector v i  X  t  X  1  X  is obtained by the following equation: v  X  t  X  1  X  X  x k  X  t  X  X  f m n  X  x l  X  t  X  x m  X  t  X  X  ;  X  4  X  where k, l, m A 1 ... n are mutually distinct, randomly selected
In the fi rst phase we check the dataset for inconsistency. If it is inconsistent then it is made consistent by using the procedure mentioned in Section 2.3 and then divided into training and testing sets.

In the second phase, we rank the features or attributes accord-ing to the information gain ratio and then delete an appropriate number of features which have the least gain ratio ( Aruna et al., 2012 ) and then again check for inconsistency and remove it if present. The exact number of features deleted varies from dataset to dataset. The expected information needed to classify a tuple in D is given by the following equation:
Info  X  D  X  X   X  m where p i is the non-zero probability that an arbitrary tuple in the base 2 is used, because the information is encoded in bits.
Info ( D ) is the average amount of information needed to identify D and is based on only the properties of classes.
 tion still required to classify the tuples in D after partitioning tuples in D into groups only on its basis:
Info A  X  D  X  X   X  v where v is the number of distinct values in the attribute A , D jj is the total number of tuples in D and j D j j is the number of repetitions of the j th distinct value.

Information gain ( Gain ( A )) (Eq. (9) )isde fi ned as the difference between the original information requirement and new require-ment (after partitioning on A): Gain  X  A  X  X  Info  X  D  X  Info A  X  D  X  X  9  X 
Information gain applies a kind of normalization to information gain using split information value de fi ned analogously with Info  X  D  X  as follows:
SplitInfo A  X  D  X  X   X  v weights. Fig. 4 abstractly illustrates the two step tightly coupled learning procedure adopted in this work.

The algorithmic framework of the proposed method is desc-ribed as follows:
Initially, a set of n p individuals (i.e. n p is the size of the population) pertaining to networks centers, width, and bias are created x d  X  2 K max  X  1 where t is the iteration number : randomly and then evaluated using the fi tness function f ( ).
In each iteration, e.g., iteration t , for individual x i mutation, crossover and selection as follows: vector is generated according to the following equation:
V where m f is the mutation factor drawn from (0,2], the indices r ; r 2 and r 3 are selected randomly from {1,2,3, ... , n p }, r r a i .

Crossover : the trial vector is generated as follows: u i  X  u u where j  X  1, 2, ... , d , rand is a random number generated in the range (0,1), c r is the user-speci fi ed crossover constant from the solution vector differs by at least one element from x i  X  t  X  . The resulting trial (child) solution replaces its parent if it has a higher accuracy (a form of selection), otherwise the parent survives 9. FITNESS COMPUTATION: by using Eq. (12) compute the fi tness 10. ENDDO 4. Experimental study
In subsection 4.1 , we brie fl y describe about datasets and parameters required to set in the experimental study. Subsection 4.2 offers results and analysis. 4.1. Description of datasets and parameters
The datasets used in this work were obtained from the UCI machine learning repository ( Frank and Asuncion, 2010 ). Three medical related datasets have been chosen to validate the proposed method. The details about the three datasets are given in Table 4 .
The last two columns of Table 4 indicate whether a dataset is balanced or not. In particular, the datasets like HABERMEN and BLOOD TRANSFUSION are imbalanced, whereas HAMMOGRAPHIC
MASSES are balanced. The imbalanced problem occurs when the number of instances of certain classes is much lower than the have a bias towards the majority classes (e.g., class 1 of HABERMAN contains 225, whereas class 2 contains 81 samples) during the learning process in favor of the accuracy metric which does not take (ii)
Average classi fi cation accuracy of 10, 20, and 30 independent runs with respect to 95% and 98% con fi dence level after removing one-third of the features based on fi lter approach (cf. Tables 8 and 9 ). (iii) Average classi fi cation accuracy of 10, 20, and 30 independent runs with respect to 95% and 98% con fi dence level after adapting data consistency (cf. Tables 10 and 11 ). (iv) Average classi fi cation accuracy of 10, 20, and 30 independent runs with respect to 95% and 98% con fi dence level after adapting data consistency and feature selection (cf. Tables 12 and 13 ). (v) Finally, average accuracy result of the above methods of 10, 20 and 30 independent runs (cf. Tables 14 and 15 ).
 From Tables 6 and 7 it has been noticed that in MAMMO-
GRAPHIC MASSES the average accuracy obtained in the series of different independent runs are varying marginally, whereas in the case of other two datasets it is constant.

From Tables 8 and 9 , it is realized that feature selection is important in Mammographic Masses, whereas it is not mandatory to apply feature selection in the case of other two datasets.
Particularly, in the case of HABERMEN and BLOOD TRANSFUSION no such improvement has been noticed even though one-third feature is removed. The column 2 of Tables 8 and 9 is indicating the feature removed from the dataset (e.g., 5 mean fi fth feature is removed). Based on the empirical study the designated feature is selected for removing from the dataset.
 right hand corner (d) of Fig. 7 shows error vs. iteration for DE  X  RBFN with feature selection.

The top left hand corner (a) of Fig. 8 shows error vs. iteration for DE  X  RBFN; top right hand corner (b) of Fig. 8 shows error vs. iteration for DE  X  RBFN with inconsistency removal and feature selection; bottom left hand corner (c) of Fig. 8 shows error vs. iteration for DE  X  RBFN with inconsistency removal; and bottom right hand corner (d) of Fig. 8 shows error vs. iteration for
DE  X  RBFN with feature selection. 5. Conclusions
In this paper, a synergy of Bayesian statistics based inconsis-tency removal, fi lter based feature selection, and differential evolution trained RBFNs is functioning towards the removal of inconsistency, the reduction of irrelevant features, and the max-imization of predictive accuracy of the classi fi er. The method of encoding an RBF network into an individual is given, where only the centers and spreads of the hidden units are encoded along with the bias of the network. The connection weights between hidden layer and output layer are obtained by pseudo-inverse method. The performance under synergistic approach is promising and very consistent. With 95% and 98% con fi dence level the comparative performance shows that the average accuracy of
DE  X  RBFNs along with feature selection and consistency is super-imbalanced dataset (e.g., HABERMAN) the accuracy of the pro-posed approach almost enhanced 5% than DE  X  RBFNs without feature selection and consistency removal. Hence, we can conclude that that removal of irrelevant features and inconsistency samples may lead to a solution to cope with the class imbalanced problem.
Of course, what we have observed is limited by the datasets, we employed DE  X  RBFNs, Bayesian approach of feature inconsistency removal, fi lter based feature selection, and the simulation tool
MATLAB 6.5. More imbalanced datasets should be examined with the proposed approach to further justify (or refute) our fi
Furthermore, in the future scope of research, lots of avenues are here, e.g., (i) the synergy of fuzzy entropy based feature selection and DE trained RBFNs with inconsistency removal and (ii) a very rigorous comparative analysis with other classi fi ers who are simultaneously reducing the features and maximizing the classi-fi cation accuracy.
 Acknowledgements S.-B. Cho is gratefully acknowledge the support of the Original Technology Research Program for Brain Science through the
National Research Foundation (NRF) of Korea (NRF: 2010-0018948) funded by the Ministry of Education, Science, and Technology. G.-N. Wang acknowledges the support of Defense Acquisition Program Administration and Agency for Defense Development under the contract UD110006MD and the Industrial
Strategic Technology Development Program, 10047046, funded by the Ministry of Science, ICT &amp; Future Planning (MSIP), Korea. References
