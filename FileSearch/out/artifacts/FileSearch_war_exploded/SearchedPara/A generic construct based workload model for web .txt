 1. Introduction 1.1. Research motivation
A benchmark is a standard by which something can be measured or judged. A computer system benchmark is a set of executable instructions to be enforced in controlled experiments to compare two or more computer hardware and software tems, plan capacity, uncover bottlenecks, and govern information resources for various user, developer, and management Peter, &amp; Kathleen, 2001 ).
 Examples are the TREC (Text Retrieval Conference), TPC (Transaction Processing Performance Council), SPEC (Stanford
Performance Evaluation Corporation), SAP, Oracle, Microsoft, IBM, Wisconsin, AS that have been used to assess the system performance. These benchmarks are domain-specific in that they model typical performance for certain pre-determined problem setting. When the user domain differs from the standard problem domain 1992; DeWitt, Futtersack, Maier, &amp; Velez, 1990). 1.2. Research problem
Domain boundness means the performance study is bound by a pre-determined problem set. Because the system perfor-mance depends on the determined problem domain, domain boundness is also called domain dependency issue. Because of domain boundness and dependency, the system performance results cannot be reproduced and sustained and the test re-mance study that is bound by the pre-fixed workload characteristics and components. Because the system performance study depends on the particular application workload, workload boundness is also called workload dependency issue. Be-cause of domain boundness and dependency, the test results are not comparable and portable between different user set-tings and application contexts. The workload is not compatible and scalable. Domain boundness and workload boundness Heiss, &amp; Lee, 1992 ).

As described above, standard benchmarks model certain application types in a pre-determined problem domain. They represent a fixed problem set presented to the proposed system. When the user domain differs from the standard domain load cannot represent the real workload and the test suite cannot accommodate the application requirement. Standard test results (Bar-Ilan, 2005; SanJuan &amp; Ibekwe-SanJuan, 2006 ).

Performance measurement, evaluation, and comparison is vital in the development and improvement of web search ser-method must be domain-independent and workload-independent. It means the core, workload model, ought to be flexible, domain dependency and workload dependency when the benchmark method is domain bound and workload bound ( Jansen &amp; Spink, 2006 ; Kraaij, Westerveld, &amp; Hiemstra, 2002; Vaughan, 2004 ).
In this paper, we present a domain-independent and workload-independent benchmark method which is developed from a user-driven approach which models the benchmark development in a process of workload requirements representation, transformation, and generation. 1.3. Research approach
Benchmarks can be synthetic or empirical. Synthetic benchmarks model the typical applications in a problem domain and Poess &amp; Floyd, 2000; SanJuan &amp; Ibekwe-SanJuan, 2006; Vaughan, 2004 ).

Further, benchmark experiments are composed of the experimental factors and the performance metrics. Experimental and dependent variables to be modeled and formulated in the benchmark.

Workload represents the flux of a benchmark method. Workload model establishes the build-up and constructs in a benchmark method. It is hence the core of benchmarking process. A workload is the amount of work assigned to or per-formed by a worker or unit of workers in a given time period. The workload is the amount of work assigned to or per-formed by a system in a given period of time. The workloads are best described by the amount of work, the rate at with the domain survey, observation, and data collection, and continue with a study of the main components and their of data, operation, and control. That is the reason why the concept of common carrier and user requirements are intro-duced to this research. Workload model development is perceived as a process of workload requirements determination, transformation, and generation.
 distribution of test, and the performance metrics.

Under the web search service context, we develop a workload model that is comprised with a set of common carriers in tices that have been accepted and applied in general.

We develop a workload requirements specification scheme, a scheme translator, and a set of benchmark generators in the research method. We use the common carrier of generic constructs to collect and capture the workload require-ments. Then we apply the scheme, translator, and generator to build a computer-aided web search benchmarking environment.

We extract the generic constructs from the main research literature. We analyze each main web search algorithm. We deduce the generic constructs from the literature and form the common carrier. The web page structure and query struc-ture are decomposed into building blocks. The main idea is to describe the data model and the operation model of work-usage. Instead, a more domain-independent and workload-independent requirements collection and compilation approach can be developed.
 Workload specification scheme workload unit then becomes a building block to develop a larger workload unit.
 Scheme translator the operation specification. Another is the control specification.
 Data generator to the data distribution specification.
 Operation generator queues, gather and report time statistics.
 Control generator to drive and supervise the experiment execution.
 This paper is organized into five sections. Section one introduces this research with motivation, issue, and approach.
Section two reviews the main web search method to establish the basis of generic constructs development. Section three presents the requirements-driven and generic-constructs-based workload model benchmark method. Section four then cusses the contribution and limitation, and concludes the paper with a brief summary and future research work. 2. Literature review
In this section, we review the main web search methods and benchmark approaches to establish the basis of common ments in the algorithm. 2.1. Link structure
The structure of the web is like a directed graph where the web pages represent the nodes of the graph and the links between pages are edges. Every page has some number of forward links (outedges) and backlinks (inedges). This kind of page is recommended by another page linked to it. Therefore, the importance of a page depends on how many backlinks a page has. 2.1.1. PageRank
PageRank is a well-known algorithm of Google in Brin and Page (1998) and Lawrence, Sergey, Rajeev, and Terry, (1998) in-links are important.
 2.1.2. Generic constructs of PageRank
Tag is an operation which operates on the generic construct of  X  X  X eb age X  as shown in Table 1 . 2.1.3. HITS
Kleinberg (1999) presented the HITS algorithm based on link structure between pages to find the authoritative pages, authorities, and hubs which link to many authorities. In detail, the model is based on links between web pages so that inlink pages, it gets higher weighted value as well as being judged as a more important page. HITS is based on hubs result. 2.1.4. Generic constructs of HITS the other hand is the operation of the generic construct in the HITS algorithm as shown in Table 2 . 2.2. Improvement of HITS 2.2.1. The problems of HITS
Bharat and Henzinger (1998) identifies two problems concerning the HITS algorithm. (1) Sometimes a set of documents the original topic. This is called the topic drift problem. 2.2.2. BHITS
In order to improve these two problems, BHITS improved the HITS algorithm in two aspects. Two variables of k and l are 2.2.3. WBHITS
Li, Shang, and Zhang (2002) improved the HITS algorithm and the BHITS algorithm as in the following description of the so-called WBHITS.

For all i 0 e I which points to I , a i  X  P i 0 w a  X  Otherwise, set all w a be more than several hundred.  X  For all i 0 2 I which is pointed by i , h i  X  P i 0 w h 2.2.4. Generic constructs of BHITS and WBHITS In the prior section, we have determined the generic constructs of HITS. After examining the BHITS algorithm and the constructs in BHITS and WBHITS methods as shown in Table 3 . 2.3. Combining the HITS-based algorithms with relevance scoring methods
Li et al. (2002) combined the HITS-based algorithms with the relevance-scoring methods. If s web page i and h i the hub value, s i * h i instead of h i
Similarly, if a i is its authority value, s i * a i instead of a in the following section. 2.3.1. Vector space model (VSM)
Paijmans &amp; Wubben, 2007 ). 2.3.2. Generic constructs of HITS-based VSM reviewing the VSM algorithm, we find that the scoring of VSM is based on the term frequency. Term is the input of web the term occurring in the page, so  X  X  X ount X  is an operation which operates on the generic as shown in Table 4 . 2.3.3. Okapi similarity measurement (Okapi) length of the document under evaluation. In the Okapi method, the similarity between a query q and a document x sim 0  X  q ; x i  X  can also be described as the inner product of the query vector Q and the document vector x where m is the number of unique terms in the document collection; the document weight: where f ij is the term frequency of a term y j in the document x number of documents in the collection that contain the query term y 2.3.4. Generic constructs of HITS-based Okapi stand from the algorithm that the scoring of Okapi is based on the term frequency and the document length. Term is the in Table 5 . 2.3.5. Cover density ranking (CDR) overall ranking. The score of the cover set x ={( p 1 , q 2.3.6. Generic constructs of HITS-based CDR operations of the generic constructs of HITS based  X  CDR as shown in Table 6 . 2.3.7. Three-Level scoring method (TLS)
The TLS method computes the relevance of a web page in two steps. (1) Given a query phrase q with n terms and a web with value 2 for relevant, 1 for partially relevant, and 0 for irrelevant (Kraaij et al., 2002 ). 2.3.8. Generic constructs of HITS-based TLS number of the sub-phrase occurring in the page so  X  X  X ount X  is an operation of generic constructs. We determine that of generic constructs of HITS based  X  TLS as shown in Table 7 . 2.4. TREC Web Track snapshot of the World Wide Web (Nick &amp; David, 2004 ). 2.4.1. The Web Track of TREC 2003
The TREC 2003 Web Track consisted of both a non-interactive stream and an interactive stream. Both streams worked homepages and the queries were derived from the name of the page (SE &amp; SW, 1999 ).
Distillation X  (TD),  X  X  X omepage finding X  (HP), and  X  X  X amed Page Finding X  (NP) (Ji-Rong et al., 2003 ). 2.4.2. The Web Track of TREC 2004
The main experiment in TREC 2004 involved processing a mixed query stream, with an even mix of each query type stud-2.5. Summary
In this section, we summarize the algorithms we have reviewed as shown in Table 8 . The selection of the generic con-constructs and the operations of the algorithms used at Web Track of TREC 2003 and TREC 2004 in Tables 11 and 12 . 3. Research model 3.1. Components of research model
The research model is made up of a page model, a query model, and a control model. The page model and the query model environment. These component models delineate the experimental factors of a workload in a benchmark. The performance metrics measure the test experiments. 3.1.1. Page model as shown in Fig. 2 .
 Single page structure
Web pages are made up of HTML tags. Based on the location of the specific tag where the query terms appear, we define the single page structure to be composed of eight main elements as follows.  X  Frame: &lt;frame&gt;&lt;/frame&gt; tag.  X  Font size:  X  Font color.
 Multi-page structure cation web site, a government web site. 3.1.2. Query model increasing stress on the web search service.
 Query type
Based on TREC 2004 Web Track, there are at least three query types of search.  X  Homepage Finding (HP): the query is the name of a site that a user wishes to reach, and the system should return the Link structure Taking advantage of the links between the web pages helps improve the effectiveness of search (Cecchini et al., 2009; clude in-links and out-links. Therefore, we use these parameters to set up the basic link structure.  X  PageRank: the sequence of search results is determined by the rank value of the algorithm.  X  Authority: an authority page is a web page that is pointed to by a number of hubs.  X  Hub: a hub page is a web page that points to many good authorities.
 Similarity
The similarity analysis is based on four relevance scoring methods.  X  Vector Space Model (VSM): scoring is based on the frequency of each term of the query.  X  Okapi Measurement Method (Okapi): scoring is based on the frequency of each term of the query and the length of the  X  Cover density ranking (CDR): scoring is based on the location of an ordered pair term of the query over a document.  X  Three-Level Scoring Method (TLS): scoring is based on the frequency of sub-phrase of the query. Synonym
A search result should include more relevant web pages by means of synonyms. We generate the synonymous terms for 3.1.3. Control model
The control model defines the environment setup variables in order to execute the experiment. A common set of param-specified as follows.
 several times. The average response time is computed.
 period of time.
 Test sequence: test sequence indicates the order of the queries to be executed.

Number of repetitions: number of repetitions means execution repeated times. 3.1.4. Performance metrics
Response time: response time refers to the time interval between when a request is made and when the response is received by the requester.
 Throughput: throughput refers to the number of operations completed by the system per unit time. Willett, 1997 ).
 of search engines which provide an indication of the relevance of the system.

Relevance criteria:  X  If the page was closely matched the subject matter of the query then it was given a score of 1.  X  Duplicate sites, same URL, same contents, were given a score of 0.  X  If a message appeared which said  X  X  X here was no response X . The server could be down or was not responding for a par-4. Research experiments 4.1. Experiment design eters to be accepted by the APIs. The details of each of the ten tests are described as follows. 4.1.1. Test1: Single Page  X  URL parameters of the APIs are set to  X  X  X uery = inurl:SOA X  and  X  X  X esults = 100 X .
 4.1.2. Test2: Single Page  X  Font Size, Font Color, Frame, Meta, Table two parameters of the APIs are set to  X  X  X uery = SOA X  and  X  X  X esults = 100 X . 4.1.3. Test3: Single Page  X  Title ing selection in the page model and the query model are inputted. And the parameters that are required by the APIs and parameters of the APIs are set to  X  X  X uery = intitle: SOA X  and  X  X  X esults = 100 X . 4.1.4. Test4: Multi Page  X  Company  X  X  X esults = 100 X . 4.1.5. Test5: Query Type  X  Homepage Finding
The fifth test suite is Query Type  X  Homepage Finding. The purpose is to assess  X  X  X omepage Finding X  of query type name: www.ibm.com  X  and  X  X  X esults = 100 X . 4.1.6. Test6: Query Type  X  Named Page Finding 4.1.7. Test7: Query Type  X  Topic Distillation  X  X  X esults = 100 X . 4.1.8. Test8: Link Structure  X  Authority-Hub y = link:ibm.com.tw+soa X  and  X  X  X esults = 100 X . 4.1.9. Test9: Similarity  X  TLS ing selection in the query model is chosen. And the parameters required by the APIs and keywords are entered to query so we select the  X  X  X opic Distillation X  option in the query model. The example query term is  X  X  X ervice Oriented  X  X  X esults = 100 X . 4.1.10. Test10: Synonym
APIs are set to  X  X  X uery = SOA X ,  X  X  X imilar_ok = 1 X  and  X  X  X esults = 100 X . 4.1.11. Experimental metrics are automatically generated after each test is executed. 4.2. Experiment execution and results baseline tests with the selection of input parameters and the output of execution results. 4.2.1. Single Page  X  URL compare with that of Yahoo as shown in Table 17 and Figs. 6 and 7 . 4.2.2. Single Page  X  Font Size, Font Color, Frame, Meta and Table Also, we performed the same test on Google to compare with that of Yahoo as shown in Table 19 and Figs. 10 and 11 4.2.3. Single Page  X  Title in Table 21 and Figs. 14 and 15 . 4.2.4. Multi Page  X  Company
Time X  and  X  X  X hroughput X  as the main metrics to be automatically generated. Based on the search results, we examine the not able to perform the same test at this time. 4.2.5. Query Type  X  Homepage Finding
Time X  and  X  X  X hroughput X  as the main metrics to be automatically generated. Based on the search results, we examine the not able to perform the same test at this time. 4.2.6. Query Type  X  Named Page Finding
Time X  and  X  X  X hroughput X  as the main metrics to be automatically generated. Based on the search results, we examine the
Google, we were not able to perform the same test. 4.2.7. Query Type  X  Topic Distillation the relevance of each URL and compute the precision and recall.
 4.2.8. Link Structure  X  Authority-Hub
Time X  and  X  X  X hroughput X  as the main metrics to be automatically generated. Based on the search results, we examine the relevance of each URL and compute the precision and recall. 4.2.9. Similarity  X  TLS
Time X  and  X  X  X hroughput X  as the main metrics to be automatically generated. Based on the search results, we examine the relevance of each URL and compute the precision and recall. 4.2.10. Synonym
Time X  and  X  X  X hroughput X  as the main metrics to be automatically generated. Based on the search results, we examine the
Yahoo performed poorer than Google in synonym. 5. Research discussion and conclusions
In this research, we have accomplished four main missions. First, an analysis framework of web search and benchmark web search algorithms and the benchmark methods. We have collected and compiled the key characteristics of the main web search algorithms and the benchmark methods. We have created and congealed the steps to extract and formulate 29.
 main components of the method. They include the workload specification scheme, the scheme translator, and the data and operation generators.

The method is domain-independent and domain-representative, and workload-independent and workload-representa-tive, because we model from the user problem domain and characterize from the actual application. The benchmark method tom experiments where users can control the execution through specification scheme instead of ad-hoc manual manipulation.

In the new workload model and method, we have described a common carrier concept to capture and compose the user requirements into three component model. They are the data component model, the operation component model, and the control component model. Web search experiment requires a page model similar to the object model, to abstract web as a benchmark.
 experiments can be further enhanced.
 we can extend the method to provide users and mangers the means to diagnose and detect the strength and weakness point of each benchmark method in web search.
 Acknowledgement This research is sponsored by National Science Council research Grant No. NSC95-2416-H-004-006. References
