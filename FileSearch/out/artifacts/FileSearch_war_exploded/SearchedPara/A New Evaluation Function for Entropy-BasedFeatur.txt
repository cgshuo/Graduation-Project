 Feature reduction has been shown effective in dealing with high-dimensional data for efficient data mining, which refers to the study of methods for reduc-ing the number of dimensions describing data [4, 10]. Its general purpose is to select relevant features to represent data and reduce computational cost, with-out deteriorating discriminative capability. It can bring many potential benefits: alleviating the curse of dimensionality, speeding up the learning process, and im-proving the generalization capability of a learning model. Many feature reduction algorithms have been developed at present. In general, they can be broadly clas-sified into two categories: feature extra ction and feature sel ection [5]. Feature extraction constructs new features wit h a linear or nonlinear transformation by projecting the original feature space to a lower dimensional one. Unlike feature extraction methods, feature selection m ethods preserve the original meaning of the features after reduction, which can be broadly categorized into wrapper [1] and filter [7, 9] methods. The wrapper met hod uses the predictive accuracy of a predetermined learning al gorithm to determine the qua lity of selected features. One drawback of the wrapper method, however, is that it is very expensive to run for data with numbers of features. The filter method separates feature se-lection from classifier learning so that the bias of a learning algorithm does not interact with a feature selection algor ithm. It relies on many feature measures such as distance [3], consistency [11], correlation [2] and so on. Much attention has been paid to filter f eature selection.

Generally speaking, filter feature selection methods work under the framework consisting of four components [4]: subset generation, evaluation, stopping crite-rion and result validation. The main difference among various feature selection algorithms lies in how to evaluate the candidate features. Obviously, evaluation functions have great influence on outputs of feature selection algorithms. Rough set theory offers a formal methodology for filter feature selection. The main ad-vantage of rough set theory is that no additional information about the data is required for data analysis such as thresholds or expert knowledge on a particu-lar domain. It provides a mathematical tool to handle uncertainty in many data analysis tasks [6, 13]. The feature subset obtained by rough set-based feature selection is called a reduct. The features in the reduct are not only strongly relevant to the classification task, but also no redundant with each other, which keep consistency with the obj ective of feature selection.

It is clear that the feature selection wor k in classical rough set theory is based on complete data. However, in many real-world applications, it may happen that some feature values are missing because of many factors such as noise in data, prediction capability [12, 13, 15]. Here we briefly review the state of the art about feature selection algorithms from incomplete data. Sun et al. [12] introduced rough entropy to evaluate the roughness of knowledge in incomplete data, and developed a rough entropy-based feature selection algorithm. Slezak [14] proposed an algorithm based on information entropy to compute a reduct. As the uncertainty measure, conditional entropy, is one key issue in rough set theory, Dai et al. [15] proposed conditional entropy for incomplete data, and studied the application of feature selection based on conditional entropy. Evaluation functions, used to evaluate the quality of features, have great influence on the outputs of feature selection algorithms. However, there are some drawbacks in the existing evaluation functions. On the one hand, the existing evaluation functions only consider the differences of entropy v alues X  variation, but there exists the differences of discernibility abilities for candidate features. As much as we know, the existing research work has not considered this aspect. Even if there are multiple features leading to the same entropy values, we can still compare the discernibility power of the features according to the granularity measure. On the other hand, for the forward greedy search, if the feature with the same entropy values is not only one, we often arbitrarily choose one of them, but the arbitrariness may affect the classific ation performance. Th erefore, the main contribution of this paper is to present a new evaluation function to overcome the above stated drawbacks.

This paper is organized as follows. In S ection 2, we review some basic concepts from the theory of rough sets. In Section 3, a simple example is firstly given to illustrate the drawbacks of existing evaluation functions, and then a new eval-uation function together with an entropy-based feature selection algorithm are presented. In Section 4, comparison exp eriments are made to show the validity of the proposed evaluation function. Finally, the conclusions are presented in Section 5. Data sets are usually given as the form of tables, we call a data table as an infor-mation system, formulated as IS = &lt;U,A,V,f &gt; ,where U is a set of nonempty and finite objects, called the universe; A is the set of features characterizing the objects; V is the union of feature domains, i.e., V =  X  a  X  A V a ,where V a is the value set of feature a , called the domain of a ;and f : U  X  A  X  V is an infor-mation function, which assigns feature values to objects such as  X  a  X  A , x  X  U , and f ( x, a )  X  V a ,where f ( x, a ) denotes the value of feature a for object x .Ifthe feature set is divided into condition feature set C and decision feature set D ,the information system is called a decision system. If there exist x  X  U and a  X  A such that f ( x, a ) is equal to a missing value (a null or unknown value, denoted as  X * X ), i.e.,  X  X  X  V a , then the information system is an incomplete information system (IIS). If  X  /  X  V D but  X  X  X  V C , then the decision system is an incomplete decision system (IDS).

Given a complete information system CIS = &lt; U,A,V,f &gt; ,for  X  B  X  A , the equivalence relation generated by B is defined by IND ( B )= { ( x, y ) | X  a  X  B,f ( x, a )= f ( y,a ) } . The family of all equivalence classes of IND ( B ) is denoted as U/IND ( B ) . An equivalence class of IND ( B ) containing x is denoted by [ x ] B . Since there are missing values for som e objects, the equivalence relation IND ( B ) is not suitable for incomplete information systems.

Given an incomplete information system IIS = &lt;U,A,V,f &gt; ,for  X  B  X  A ,a tolerance relation between objects tha t are possibly indiscernible in terms of B is Itcanbeeasilyshownthat TR ( B )=  X  a  X  B TR ( { a } ) . The tolerance class of object x with reference to a feature set B is denoted as T B ( x )= { y | ( x, y )  X  TR ( B ) } . Let U/TR ( B ) denote the family set { T B ( x ) | x  X  U } , which is the classification induced by B .For X  X  U , the lower and upper approximation of X with respect to B can be defined as B ( X )= { x  X  U | T B ( x )  X  X } and B ( X )= { x  X  U |
T B ( x )  X  X =  X  X  } . The lower approximation is called the positive region, that is POS B ( X )= B ( X ) . X is called B  X  definable iff B ( X )= B ( X ) .Otherwise, B ( X ) = B ( X ) and X is rough.

Given an incomplete decision system IDS = &lt;U,C  X  D,V,f &gt; ,for  X  B  X  C , the objects are partitioned into n mutually exclusion crisp subsets U/IND ( D )= {
D 1 ,D 2 ,  X  X  X  ,D n } by the decision features D . The lower and upper approxima-which is called the positive region of D with respect to B in the IDS. The lower approximation is a description of the domain objects which are known with absolute certainty to belong to the decision classes. In this section, a simple example is firstly given to illustrate the drawbacks of existing evaluation functions, and then a new evaluation function together with a entropy-based feature selection algorithm are presented.

The conditional entropy of Definition 1 can be used as a reasonable infor-mation measure in incomplete decision tables[15], and it is quite representative among other entropies. Correspondingly, the evaluation function in terms of con-ditional entropy is also defined.
 Definition 1. Let IDS = &lt;U,C  X  D,V,f &gt; be an incomplete decision table, U = { x 1 ,x 2 ,...,x n } ,for B  X  C , the classification induced by B is U/TR ( B )= { tition on decision attribute set D . The conditional entropy of D with respect to B is defined by
H ( D | B )=  X  Definition 2. Given an incomplete decision table IDS = &lt;U,C  X  D,V,f &gt; , suppose B  X  C is the selected feature subset, and a  X  C  X  B is a candidate feature. Then the evaluation function of candidate feature a is defined as e ( a )= H ( D | B )  X  H ( D | B  X  X  a } ) .

From Definition 2, the existing evaluation function can be used to evaluate the importance of features. The smaller the evaluation value is, the more important the feature will be. However, the drawbacks of above evaluation function can be explained with reference to the following example.
 Example. Suppose there is an incomplete decision table IDS = &lt;U,C  X  the feature selection process, Definition 2 is applied to compute the evaluation values of features. By computing, the descending sequence of four candidate fea-with the minimum evaluation value are c 3 and c 4 . By direct computation the clas-bility abilities of them are different, feature c 3 can describe the st ronger discerni-bility power than c 4 . However, Definition 2 does not take into consideration this difference. Thus the evaluation function given by Definition 2 is inadequately computed as a result of this aspect.
On the other hand, in the feature selectio n algorithm of forward greedy search, due to e ( c 3 )= e ( c 4 ) , we can select one feature arbitrarily. Consequently, feature c 3 or c 4 are chosen to the selected feature subset. The arbitrariness can surely not guarantee a selected feature subset is a reduct. Suppose that the selected feature subset containing feature c 3 and c 2 exhibit the best performance, but we obtain the final feature subset is { c 4 ,c 2 } due to the arbitrary selection. Ob-viously, this result may affect the classi fication performance. Therefore, we give a new evaluation function from a reaso nable perspective to improve the above mentioned problems.
 Definition 3. Given an incomplete decision table IDS = &lt;U,C  X  D,V,f &gt; , suppose B  X  C is the selected feature subset, and a  X  C  X  B is a candidate feature, the classification induced by a consists of tolerance class A i (1  X  i  X  k ) . Then a new evaluation function of candidate feature a is defined as f ( a )= e ( a )+ g ( a ) ,where g ( a )= 1 | U | 2 feature a .
 Theorem 1. Given an incomplete decision table IDS = &lt;U,C  X  D,V,f &gt; , suppose B  X  C is the selected feature subset, for  X  a, b  X  C  X  B ,thereis f ( a  X  b ) &lt; f ( a ) or f ( a  X  b ) &lt;f ( b ) .
 Proof. Suppose the classification induced by a consists of tolerance classes A i (1  X  i  X  k ) , and the classification induced by a  X  b consists of tolerance classes B (1  X  j  X  l ) , by Definition 2 and the definition of conditional entropy, it is obvious that e ( a  X  b ) &lt;e ( a ) .Since a  X  a  X  b , according to the definition of tolerance class, there is | B j | &lt; | A i | , obviously, it holds that thus g ( a  X  b ) &lt;g ( a ) . Therefore, f ( a  X  b ) &lt;f ( a ) .Inthesameway,itcanproof that f ( a  X  b ) &lt;f ( b ) .

Theorem 1 shows the rationality of the new evaluation function, which states the uncertainty decreases w hen the available knowledge increases. Obviously, the granularity measure can represent discernibility ability of candidate feature a , the smaller g ( a ) is, the stronger its discernibility ability. Through comparison, the selection of survival features can be achieved. From above example, there is candidate feature c 4 is stronger than that of feature c 3 . Therefore, the survival feature is c 4 . It is obvious that the new evaluation function is more reasonable. Combine the new evaluation function into feature selection, a selected feature subset (called reduct) can be charact erized by the following statement. Definition 4. Given an incomplete decision table IDS = &lt;U,C  X  D,V,f &gt; , a selected feature subset B  X  C is called a reduct of the IDS if and only if H ( D | B )= H ( D | C ) ,andfor  X  B  X  B , H ( D | B ) = H ( D | C ) .
In this definition, the first one indicates that the selected feature subset pre-serves the same information measure as the whole set of features; the second one guarantees that all of the features are indispensable, i.e., there is not any redundant feature in the reduct.

In the following, we combine the proposed evaluation function with forward greedy search to construct the f eature selection algorithm.
 Algorithm 1. Entropy-based Feature Selection Algorithm from Incom-plete Data Input: An incomplete decision table IDS = &lt;U,C  X  D,V,f &gt; ; Output: A feature subset Red .
 Begin 1. Initialize Red =  X  ; 2. For each c  X  C do 3. compute H ( D | C  X  X  c } )  X  H ( D | C ) ; 4. if H ( D | C  X  X  c } )  X  H ( D | C ) &gt; 0 ,then Red = Red  X  X  c } ; 5. End for 6. While H ( D | Red ) = H ( D | C ) do 7. compute f ( c ) for all c  X  C  X  Red ; 8. choose the feature c k that minimizes f ( c ) ,andlet Red = Red  X  X  c k } , 9. End while 10. For each c  X  Red do 11. compute H ( D | Red )  X  H ( D | Red  X  X  c } ) ; 12. if H ( D | Red )  X  H ( D | Red  X  X  c } )=0 ,then Red = Red  X  X  c } ; 13. End for 14. Return Red .
 End
The algorithm begins with an empty subset Red , and adds some indispens-able features to Red gradually. Then select the features with the minimal value by the new evaluation function into Red each loop until satisfying the stopping condition. Finally, a redundancy-removing step is carried out to avoid the re-dundancy in the selection result. The feature subset selected by this algorithm obtains the same information as the original feature set from incomplete data. In order to test the validity of the new proposed evaluation function, we conduct some experiments on a PC with Windows 7, Intel (R) Core(TM) Duo CPU 2.93 GHz and 4GB memory. Algorithms are coded in C++ and the software being used is Microsoft Visual 2008. The objective of the following experiments is to show the effectiveness of feature selectio n algorithm based on the new evaluation function. We perform the experiments on six real UCI data sets, which are downloaded from UCI Repository of machine learning databases in [16]. The characteristics of six data sets are described in Table 1. For the complete data sets, we randomly change 5% of the known features values from each original data set into missing values to create incomple te data sets. For the numerical features, we use the data tool Rosetta (http://www.lcb.uu.se/tools/rosetta/index.php) to discretize them.

In what follows, we first make a comparative study on the feature selection algorithms in terms of feature subset size. The results are shown in Table 2 in which PFS represents the proposed featu re selection algorithm, EFS represents the feature selection algorithm constructed in [15] and LFS denotes the lower approximation-based feature selection algorithm in [13]. Note that PFS selects candidate features by Definition 4, while EFS finds candidate features by Defi-nition 2. The main difference between PFS and EFS is the evaluation function.
As shown in Table 2, we can observe th at Algorithm PFS selects fewer fea-tures comparing with EFS and LFS in most data sets. For example, as data set Hepatitis, PFS selects 12 features, while both of EFS and LFS select 14 features. The reason can be attributed to that the total number of objects in the data sets keep invariant, the more objects can be di scerned with the selected features by proposed evaluation function in PFS than that of EFS at certain iterations, such that fewer features needed to discern a ll the objects in the data sets by PFS. And it does shows that there is a decrea se in feature subset size between PFS and LFS, demonstrating that there is other information contained in the entropy other than that in the lower approximation. This phenomenon indicates that the proposed feature selection algorithm can r educe data dimensions effectively, thus it verifies the validity of new evaluation function.

We employ two classifiers NaiveBayes an d J48 to evaluate the classification performance of the selected feature subset. Each data set is divided into two parts: one for training and the other for test. On the basis of the training data, we employ feature selection algorithms to reduce the data sets. By NaiveBayes and J48, the rules are extracted from the t raining set. Using the rules the test set is classified and the classification results are obtained. The average classification accuracies and standard devi ation are acquired based on tenfold cross-validation shown in Tables 3 and 4, where Raw depicts the classification performance on data sets with the original features, and t he average classification accuracies are expressed in percentage. The  X  X verage (ACC) X  row records the average classifi-cation accuracy of the three algorithms on six data sets.
 Cardiotocography 89.79  X  0.61 91.85  X  0.40 88.56  X  0.76 89.23  X  0.31 Average(ACC) 88.73 90.28 89.00 88.76 Cardiotocography 95.07  X  0.84 97.26  X  0.59 94.01  X  1.20 95.92  X  1.03 Average (ACC) 87.74 90.15 88.69 88.78
The results shown in Tables 3 and 4 indicate that PFS produces the better classification performances after featu re selection based on the new evaluation function than those of EFS and LFS as to NaiveBayes and J48. Regarding Naive-Bayes, PFS is better than EFS on all the data sets other than data set Synthetic, and PFS also shows increases in classific ation accuracies co mparing with LFS. As to J48, PFS outperforms EFS on four of six data sets; PFS outperforms LFS on most of the data sets. Considering t he results between PFS and EFS, it can demonstrate the effectiveness of new eval uation function in feature selection. In addition, the three approaches improve the classification capability by selecting a small portion of the original features. From the experimental results, we can confirm that the proposed evaluation function leads to promising improvement on classification performance.

To further explain the reason why the classification performances are improved using the new evaluation function, we conduct the experiments on four large data sets using NaiveBayes classifier with Algorithms PFS, EFS and LFS. Fig.1 displays more detailed change trend of the three algorithms in classification accuracy with the number of selected features.

From Fig.1, the curves between PFS and EFS in the data set Synthetic are overlapping. The reason is that PFS and EFS select the same features, thus the classification accuracies ar e the same for selecting the same number of features. However, most points in the curves of PFS are higher than those of EFS and LFS in the data sets. Take data set Cardiotocography as an illustration, the classification accuracies of PFS are higher than those of EFS and LFS since the beginning of selecting three features. The underlying reason perhaps is that though the number of features is the same by PFS and EFS, the selected fea-tures are different, PFS employed the new evaluation function always find the candidate features that can d iscern more objects for cla ssification learning, such that the classification performance is better than that of EFS. The similar sit-uations can be found in two other data sets. Observing the curves, we can find that PFS can keep a steady increase in accuracy value, whereas EFS and LFS incur a fluctuant increase, even a decrease. This phenomenon may result from one possible reason that PFS has a redundancy-removing step, while EFS and LFS does not consider the redundant information between the selected features. It shows some dispensable features in the selected feature subset are superfluous, which deteriorate the cla ssification performance.
 Furthermore, we conduct the experiments on the four larger data sets using J48 classifier with the three algorithms. Fig.2 displays more detailed change trend of the three algorithms in classificatio n accuracy with the number of selected features.

As shown in Fig.2, the curves between PFS and EFS in the data set Synthetic are overlapping. However, one may observe that there are many points in the curves where the classification perform ance of PFS clearly surpasses those of EFS and LFS. We can see that, as data set Mushroom, when the selected feature number is two, the classification accuracy of PFS is higher than those of EFS and LFS. Though the same number of se lected features, PFS can select the feature that discerns more objects for classification learning, correspondingly, the selected features are different, and the cl assification accuracy is higher than that of EFS. And comparing with LFS, PFS can find some other useful information contained in the entropy other than lower approximation, which would result in better classification performance. For the other three data sets, one may observe that the similar situations.

Based on the aforementioned experime ntal results, we can conclude that the new evaluation function gives an effective way to select satisfactory feature subset in the process of feature sel ection from incomplete data. In this paper, we introduce a new evaluation function to overcome the draw-backs of existing evaluation functions. Based on the new evaluation function, we construct a conditional entropy-based fea ture selection algorithm with forward greedy search from incomplete data. The n umerical experiments show the valid-ity of the new evaluation function. Two main conclusions are drawn as follows. On the one hand, compared with the existing evaluation function, the new eval-uation function reflects not only the conditional entropy values X  variation, but also the discernibility ability of a candidate feature. Thus the new evaluation function is more reasonable than the existing evaluation function to describe the discernibility ability. On the other hand, in feature selection, even if there are more features with same importance in the conditional entropy, our feature se-lection algorithm can select one with the greatest classification ability, while the arbitrary selection in the existing featur e selection algorithm may affect the clas-sification performance. Ther efore, the new evaluation function is more effective in the process of feature sel ection from incomplete data.
 Acknowledgments. This work was supported in part by the Natural Science Foundation of Chin a (61170232), F undamental Research Funds for the Central Universities (2012JBZ0 17), Independent research project of State Key Labora-tory of Rail Traffic Control and Safety (RCS2012ZT011), Innovation Funds of Excellence Doctor of Beijing Jiaotong University (2014YJS040) and Research Initiative Grant of Sun Yat-sen University. The corresponding author is Hong Shen.

