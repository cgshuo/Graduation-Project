 Dipartimento di Automatica e Informatica, Politecnico di Torino, Torino, Italy 1. Introduction
The rapid advance of molecular biology techniques provides measures of gene expression levels (concentration of mRNA) of thousands of genes simultaneously. DNA microarray experiments generate thousands of gene expression measurements and provide a simple way for collecting huge amounts of data in a short time. They are used to collect information from tissue and cell samples regarding gene expression differences. Compared with traditional tumor diagnostic methods, based mainly on the morphological appearance of the tumor, methods relying on gene expression pro fi les are more objective, accurate, and reliable [20]. However, microarray data are highly redundant and noisy. Most genes are uninformative and only a subset of features may present distinct pro fi les for different classes of samples.
Since not all genes provide useful information, a dimensionality reduction process is needed to identify and remove as much of the redundantand unnecessaryin formation as possible. Dimensiona lity reduction is the process of reducing the number of variables under consideration, and can be performed by means space with fewer dimensions. Since from a biological viewpoint it is more important to select real genes than to create arti fi cial features with uncertain biological meaning, we focus our attention on feature selection. Feature selection allows the identi fi cation of the genes that are relevant or mostly associated with a tissue category, disease state or clinical outcome. Furthermore, when a small number of genes additional scienti fi c understanding of the problem.

Traditional methods in gene selection are fi lter-based models and often evaluate genes in isolation without considering correlation among them. They rank genes according to their individual relevance or discriminative power to the target class and select the top-ranked genes. These methods are ef fi cient, but cannot remove redundant genes, because simply selecting two highly ranked genes may not form a better gene set if the two genes are highly correlated. A key challenge in gene selection from microarray in an automatic manner [54].

Another critical issue of feature selection is fi nding the optimal number of features that yield good classi fi cation accuracy. Only few works address the problem of de fi ning the appropriate number of genes to select [12]. While an excessively conservative estimate may cause an information loss due to relevant features exclusion, an excessively liberal estimate may leave noise in the resulting gene set. For example, for a two-class cancer subtype classi fi cation problem, a few tens of genes are usually selected, even if some studies suggest that one or two genes may be already suf fi cient [31].

In this paper we present the MaskedPainter feature selection method. The MaskedPainter method provides two main contributions: (i) it identi fi es the minimum number of genes that yield the best coverage of the training data (i.e. that maximize the correct assignment of the training samples to the corresponding cla ss), and (ii) it ranks the genes according t o a quality sc ore. A density based technique is exploited in the quality score computation to smooth the effect of outliers. The minimum gene subset and the top ranked genes are then combined to obtain the fi nal feature set.
 The name  X  X askedPainter X  originates from the Painter X  X  algorithm used in computer graphics. The the MaskedPainter assigns a priority to genes that is based on the overlaps in their expression intervals. The term masked is due to the fact that the information carried by a gene is represented in a particular format, named gene mask.

We validated our method on different microarray datasets. We mainly focused on multi-category datasets, because classi fi cation problems with multiple classes are generally more dif fi cult than binary ones and give a more realistic assessmentof the proposed methods [22]. We compared the MaskedPainter performance with different feature selectiontechniquesby measuring the classi fi cationaccuracyprovided statistically signi fi cant higher accuracy than the other techniques. All experiments have been performed genes selected by the proposed approach has been assessed.
 The paper is organized as follows. Section 2 discusses related works. Section 3 describes the Masked-Painter feature selection method, while Section 4 presents the experiments we performed to validate our approach, included a biological discussion. Finally, Section 5 draws conclusions. 2. Related work
Feature selection is a fundamental task in the bioinformatics domain to identify the most relevant genes correlated with the considered sample classes. In literature many studies have been addressed to this issue. However, evaluating feature selection techniques and comparing results among different worksisverydif fi cult due to the lack of both standard experimental designs and benchmark datasets way to evaluate feature selection methods is to use as score the accuracy of a classi fi er applied after the feature selection. The higher is the accuracy, the more relevant the selected genes are. For example, the authors [24] analyze several feature selection methods available in the RankGene software [40] and
The feature selection methods can be categorized in three categories: (i) fi lter, (ii) wrapper and (iii) embedded. In the following we describe the main characteristics of these approaches.

Filter methods use general characteristics of the data to to detect differentially expressed genes. A gene is differentially expressed if it shows a certain distribution of expression levels under one condition and a signi fi cantly different distribution under other c onditions. Filter methods aim at evaluating the differential expre ssion of genes and rank them according to t heir ability to disti nguish among classes. For example, a P-metric correlation [18] which measures the difference between the samples relative to the standard deviation of samples was exploited. Moreover, these approaches operate independently of any learning algorithm and require less computation. We can identify two main sub-categories of fi lter methods: (i) univariate and (ii) multivariate.

The univariate approaches are based usually on statistical measures used for detecting differences between two groups (including t-test, Wilcoxon test, and Mann-Whitney test), or among three or more groups (ANOVA, Kruskal-Wallis test, and Friedman test) [27,44]. These methods are easily and very ef fi ciently computed but the disadvantage is that they require some assumptions on the data distribution. For example, the t-test requires expression levels to be normally distributed and homogeneous within groups and may also require equal variances between groups. A Bayesian version of t-test by means of a derivation of point estimates for parameters and hyperparameters was proposed in [14].

Instead, the multivariate approaches are devoted to evaluates the correlations among genes belonging to the selected subset. For example, in [33] a k-means clustering is applied to partition the initial set of genes. A fi lter approach based on the normalized mutual information metric is conducted in each cluster. A sequential forward search in the space of subsets of genes is performed until a prede fi ned number of genes is achieved. Other approaches are based on searching procedures to identify the best subset of genes. In [37] a multivariate score formulated by the CFS algorithm is integrated as evaluation function of a genetic algorithm.

Wrapper methods evaluate the usefulness of a gene by estimating the accuracy of the learning method applied only to selected genes and not to the entire dataset. The aim of this kind of analysis is to select the features that optimize the performance of the target classi fi er. It is computationally very expensive for data with a large number of features and the selected subset is dependent on the considered learning algorithm.

Among the feature selection categories, the wrapper methods typically require extensive computation to search the best features and depend on the considered learning algorithm [25]. For example, the authors [32] proposed an approach based on genetic algorithms. The Silhouette statistic is used to assign a score to each subset. In order to reduce the search space, a pre-selection of genes is done by the BSS/WSS univariate fi lter metric [13]. Moreover, wrapper approaches usually require a further step to avoid the exhaustive search among all the possible solutions, because the number of feature subsets grows exponentially with the number of features, making enumerative search unfeasible. In [35] an univariate rank is fi rst computed for all the genes, and then this ranked list is crossed by a wrapper procedure which incrementally augments the subset of selected genes.

The embedded approaches have the advantage of including the interaction with the classi fi cation model, while at the same time being far less computationally intensive than wrapper methods. For example, in [19] the features are ranked with the magnitude of the weights in the SVM classi fi er. The relevance of procedure. In [15] a variation of this method was proposed. The entropy values of the SVM weights are discretized to eliminate chunks of irrelevant genes. Differently, in [11] the ensemble nature of the decision trees constructed in the random forest framework is exploited to compute the relevance of each feature.

A particular issue of feature selection task is fi nding the optimal number of genes which improve classi fi cation accuracy and show high correlation with disease outcomes. For example, for a two-class which suggest that one or two genes may be enough [31]. The authors [45] test the classi fi cation capability of all simple combinations of the top genes. Firstly, they classify the data set with only one top gene. If the accuracy is not suf fi cient, they consider all 2-gene combinations of top genes, and consider an increasing number of genes until a good accuracy is reached. High accuracy values are provided by only three or four genes.

For a comparison of various feature selection methods on different microarray data see [16,22,29,42]. 3. The MaskedPainter approach
The microarray data E are in the form of a gene expression matrix Eq. (1), in which each row represents a gene and each column represents a sample. For each sample, the expression levels of all the genes in study are measured.

Let e ij be the measurement of the expression level of gene i for sample j where i =1 ,...,N and j =1 ,...,M . Each sample is also characterized by a class label, representing the clinical situation of the patient or tissue being analyzed. The domain of class labels is characterized by C different values and label l j of sample j takes a single value in this domain.

The MaskedPainter method is based on the following idea. Certain genes can identify samples be-longing to a class, because their expression interval in that class is not overlapped with the expression intervals of other classes (i.e., all the samples for which the expression value of the gene is in a given range belong to a single class). For example, Fig. 1(a) shows the expression values of a gene with 12 samples belonging to 3 different classes. The same information can be represented as shown in Fig. 1(b). With the latter representation, it is easy to see that the gene is relevant for class 3, because the expression values of this class are concentrated in a small range, which is different from the range of expression values associated with the other classes. Instead, the same gene is not useful to distinguish between class 1 and 2, because the values for such classes have mostly overlapping ranges.
The MaskedPainter in itially characterizes each gene by means of a gene mask , which represents the gene X  X  capability of unambiguously assigning tr aining samples to the correct class. Next, the method assigns to each gene two values that are then combined in the ranking phase: the overlap score and the dominant class . The overlap score is a quality index that describes the overlap degree of the expression intervals for different classes. Genes with less overlapping intervals are more important because they can unambiguously assign the samples to the correct class. The dominant class of a gene is the class to which the majority of samples without overlapping expression intervals belong.

By exploiting these elements,the MaskedPainter de fi nes (i) the minimum set of genes needed to provide the best sample coverage on training data and (ii) a sort of genes by dominant class and increasing value of overlap score. The fi nal gene set is obtained by combining the minimum gene subset and the top ranked genes in the sort.

The building blocks of the MaskedPainter approach are presented in Fig. 2. The approach is based on the following main phases.

Gene mask computation. A gene mask is computed for each gene. The gene mask shows which training samples the gene can unambiguously assign to the correct class. It is a string of 0s and 1s, generated by analyzing the overlaps among the class expression intervals (i.e., the range of expression values of samples belonging to the same class) for each gene.
Overlap score computation and dominant class assignment. An overlap score is assigned to each gene. It assesses the overlap degree among its core expression intervals. The core expression interval of a gene, separately de fi ned for each class, is the expression interval obtained by smoothing the effect of outliers. To compute the core expression intervals, a density based approach is proposed. A dominant class, i.e., the best distinguished class, is also assigned to each gene. This information is exploited to reduce redundancy when genes are selected.

Minimum gene subset selection. The minimum number of genes needed to provide the best training set sample coverage is selected by analyzing the gene masks and exploiting the overlap scores. A greedy approach is exploited to achieve the best sample coverage with a low computational cost.

Gene ranking. Genes that do not belong to the minimum subset are ranked according to increasing values of overlap score, separately for each dominant class. The fi nal gene rank is composed by selecting the topmost gene from each dominant class in a round robin fashion.

Final gene selection. Selected top ranked genes are added to the minimum gene subset, thus providing the fi nal gene set.

All phases are detailed in the following. 3.1. Gene mask computation
We introduced a compact representation, named gene mask, to represent the discriminative power of sample.

By considering the gene expression matrix in (1),with a class label for each sample,the class expression intervals for each gene are de fi ned. Let i be an arbitrary gene with M samples belonging to C classes. For each gene i we de fi ne C class expression intervals (one for each class). The class expression interval for gene i and class k is expressed in the form: where min i,k and max i,k are the minimum and the maximum gene expression values for class k .A graphical example of class expression intervals is presented in Fig. 3.

Since few samples usually belong to each class, the class expression interval contains the entire expression value range for the corresponding class, thus avoiding any assumption on data distribution. The gene mask exploitation aims at reaching the best coverage of the original data of the training set, thus, no preprocessing (e.g., outlier smoothing) is performed and the min-max expression interval is adopted.

Thegenemaskisanarrayof M bits, where M is the number of samples. Consider an arbitrary gene i .Bit j of its mask is set to 1 if the corresponding expression value e ij belongs to the class expression interval of a single class (i.e., it assigns unambiguously the sample to the correct class), otherwise it is set to 0. Formally, given two classes a, b  X  X  1 ,...,C } ,bit j of gene mask i is computed as follows.
Figure 3 shows the gene mask associated to gene i after the computation of its three class intervals (
I
The gene masks are employed in the minimum gene subset selection step to identify the features which provide the best coverage of the training set samples. 3.2. Overlap score computation and dominant class assignment
An overlap score is assigned to each gene, depending on the amount of overlapping expressionintervals among classes. Differently from the gene mask, which is based on the min-max expression intervals (as discussed in Section 3.1), the overlap score aims at modeling the discrimination power of genes and needs to handle noise and outliers to avoid over fi tting. Hence, to better model the expected values in an unseen test set, the overlap score computation is based on core expression intervals. Core expression intervals are expression intervals obtained by smoothing the effect of outliers by means of a density-based technique.

The dominant class, i.e., the class with the highest number of samples in non-overlapping intervals, is also assigned to each gene.

This phase can be further divided into three steps, which are detailed in the following: (i) core expression interval de fi nition, (ii) overlap score computation, and (iii) dominant class assignment. 3.2.1. Core expression interval de fi nition
Since microarray data may present noisy values, the in fl uence of values far from the high concentration nucleus (typically outliers) must be reduced. Several techniques have been exploited to reduce the in fl uence of outliers in microarray data [9,52], which are variations of the formula mean  X  stdev ,such as 3sigma or the Hampel identi fi er. However, these methods do not consider the density of values. Thus, we propose a density based method to reduce the effect of outliers in the computation of the core expression intervals. The core expression interval of a gene in a class is de fi ned by replacing the mean by the weighted mean and the standard deviation by the weighted standard deviation. It models the possible distribution of unseen expression values. A weight is assigned to each data value by considering the number of its neighbors belonging to the same class. In particular, a higher weight is assigned to values with many neighbors and a lower weight to isolated values.
 Consider an arbitrary sample j belonging to class k and its expression value e ij for an arbitrary gene i . Let the expression values be independent and identically distributed (i.i.d) random variables and  X  i,k be the standard deviation for the expression values of gene i in class k . The density weight d ij measures, for a given expression value e ij , the number of neighboring expression values of samples of the same class. A conservative estimation of the interval width is represented by the interval  X   X  i,k centered in e ij , because  X  i,k identi fi es a neighborhood in which 68% of points should lie when a normal distribution is centered on the considered point.
The density weight for the expression value e ij in class k is de fi ned as where  X  im is de fi ned as:
If an expression value is characterized by many neighboring values belonging to the same class, its density is higher. For example, in Fig. 4 an arbitrary gene i with 10 samples belonging to two classes is shown. The fi rst class 2 sample (denoted as e ia in Fig. 4) is characterized by a density weight d ia equal to 0, because there are no other expression values for gene i in the interval e ia  X   X  i, 2 (represented by a curly bracket). The second class 2 sample ( e ib ) is characterized instead by a density weight d ib equal to 1, because another sample of class 2 belongs to the interval e ib  X   X  i, 2 .

The core expression interval of an arbitrary gene i in class k is given by where the mean  X   X  i,k and the standard deviation  X   X  i,k are based on the density weights and are computed as follows.

The mean  X   X  i,k is de fi ned as where  X  ij is de fi ned as: and D i,k is the sum of density weights for gene i in class k (i.e., M j =1  X  ij  X  d ij ).
The standard deviation  X   X  i,k is given by
The core expression intervals model a normal distribution of expression pro fi les with a con fi dence interval equal to 95%. Thus, the resulting intervals are less affected by outlier values, as shown in Fig. 4. Since the fi rst class 2 sample (on the left) has a density weight equal to zero, its value provides no contribution to the core interval computation, and thus it is not included in  X  I i, 2 .

The proposed approach has been experimentally compared with alternatives which consider as bound-aries  X   X   X  or the minimum and maximum (min-max) values of the expression values. For the min-max microarray data. Using  X   X   X  as boundaries, the fi ltered intervals are excessively reduced and signi fi cant values may be lost. A similar problem affects the Hampel identi fi er [9], also called the median absolute value (MAD). The expression intervals generated by MAD tend to be narrower than the ones obtained by the proposed density weighted method. If intervals are narrower, they tend to be less overlapped. Thus, a gene may be considered more relevant than it actually is, because of a less conservative overlap detection. 3.2.2. Overlap score computation
For each gene we de fi ne an overlap score (denoted as os in the following) that measures the degree of overlap among core expression intervals. Since overlapping intervals may lead to misclassi fi cations due to insuf fi cient discriminative power of the considered gene, the overlap score is exploited for ranking genes. The score is higher for less important genes with many overlapping intervals among different classes. On the contrary, lower scores denote genes with higher discriminating power, because they have few overlaps among their intervals.
 The os depends on the following characteristics of the gene expression values A) the number of samples associated to different classes in the same range, B) the number of overlapping classes, C) the overlapping interval length.

We compute the overlap score os i for each gene i . To ease readability, we will omit the i subscript in the following formulas.

We de fi ne the total expression interval of a gene as the range given by the minimum and maximum among its core expression interval boundaries. We denote such interval as W , and its amplitude as | W | . For example, in Fig. 5, the total expression interval of a gene with samples belonging to three classes (and thus with three core expression intervals) is shown. We divide W in subintervals, where each subinterval is characterized by a different set of overlapping classes with respect to the adjacent subintervals. More expression intervals, as shown in Fig. 5. The amplitude of subinterval w t is denoted as | w t | .
The idea of the overlap score is to assign higher scores to genes that are characterized by more overlaps among expression intervals of different classes. The score is based on both the number of samples of different classes that belong to the same subinterval and the amplitude of the subinterval itself. Thus, the overlap score for an arbitrary gene is de fi ned as follows. where T is the number of subintervals, m t is the number of samples expressed in subinterval t , M is the total number of samples. The function k ( t ) evaluates the set of classes that overlap in the subinterval t and is de fi ned as follows. class (e.g., w 1 in Fig. 5) provide no contribution to the overlap score, because the number of overlapping classes is 0. In the case of subintervals without values (i.e., gaps such as w 4 in Fig. 5), the number of overlapping classes is 0. Thus, also in this case, no contribution is added to the overlap score.
Consider the example in Fig. 5. The total expressioninterval of the gene is divided into fi ve subintervals and the c t components (number of overlapping classes in each subinterval) take the following values: c 1 =0 , c 2 =2 , c 3 =0 , c 4 =0 , c 5 =0 .

The os value ranges from 0 , when there is no overlap among class intervals, to C (i.e., the number of classes), when all intervals are completely overlapped. For example, in Fig. 6, two illustrative cases for a binary problem are reported. Figure 6(a) shows a gene that correctly distinguishes two classes, because and c 3 =0 . Instead, Fig. 6(b) shows a gene unable to distinguish two classes, because the expression intervals associated with the two classes are almost completely overlapped. In this case, the overlap score is close to 2. 3.2.3. Dominant class assignment
Once the overlap score is computed, we associate each gene to the class it distinguishes best, i.e., to its dominant class . To this aim, we consider the subintervals where expressed samples belong to a single class and evaluate the percentage of samples for the considered class in these subintervals. The class with the highest number of samples is the dominant class of the gene. The gene is assigned to the class with the highest number of samples to take into account th e a priori probability of the classes. Associating a gene with the class it distinguishes best will allow us to balance the number of selected genes per class (see Section 3.5).

For example, in Fig. 6(b) the gene dominant class is class 1 , because its samples for the two non-overlapping subintervals w 1 and w 3 are labeled with class 1 . Instead, in Fig. 6(a), since both classes have completely non-overlapping intervals, the gene dominant class is class 2 , according to the number of samples. 3.3. Minimum gene subset selection
The information provided by the gene masks is employed to identify the minimum set of genes which classify correctly the maximum set of samples in the training set. It also allows removing redundant information (e.g., genes with low discriminative power or with similar expression pro fi les). The fi nal objective is the identi fi cation of a set of high quality features covering the given sample set.
Let S be a set of genes. We de fi ne a global mask as the logic OR between all the gene masks belonging to genes in S . The objective is the de fi nition of the minimum set of genes S that holds enough discriminating power to unambiguously assign to the correct class the maximum number of samples in the training set. Thus, given the gene mask of each gene, we search for the global mask with the maximum number of ones. To this aim, we propose a greedy approach.

Greedy approach. The greedy approach identi fi es at each step the gene with the best complementary gene mask with respect to the current global mask. Thus, it adds at each step the information for classifying most currently uncovered samples.

The pseudo-code of the Greedy approach is reported in Algorithm 1. It takes as input the set of gene masks ( M ), the set of overlap scores ( OS ) and produces as output the minimum subset of genes ( G ). all zeros (line 4). Then the following steps are iteratively performed.

A) The gene mask with the highest number of bits set to 1 is chosen (line 8). If more than one gene
B) The selected gene is added to set G (line 18) and the global mask is updated by performing the
C) The gene masks of the remaining genes (gene mask set M , line 20) are updated by performing the
D) If the global mask has no zeros (line 6) or the remaining genes have no ones (line 9), the procedure
At the end of this phase, the minimum set of genes required to provide the best sample coverage of the training set is de fi ned. The genes in the minimum subset are ordered by decreasing number of 1s in the gene mask. 3.4. Gene ranking
Thegenerankisde fi ned by considering both the overlap score and the dominant class. Genes that do not belong to the minimum subset are ranked by increasing value of overlap score separately for each dominant class. The fi nal rank is composed by selecting the topmost gene from each dominant class rank in a round-robin fashion.

A feature selection method considering only a simpli fi ed version of the overlap score was presented in [3]. The overlap score alone ranks high genes with few overlaps, without considering the class they distinguish best. Hence, high-ranked genes may all classify samples belonging to the same class, thus biasing gene selection (typically by disregarding less populated classes). The round-robin gene selection by dominant class allows mitigating this effect. 3.5. Final gene selection
The minimum gene subset includes the minimal number of genes that provide the best sample coverage on the training set, ordered by decreasing number of 1s in the gene mask. However, a larger set of genes by the user. In this case, the minimum gene subset is extended by including the top k ranked genes in the gene ranking, where k is set by the user. Observe that genes in the minimum subset are inserted in the fi nal gene set independently of their overlap score, because these features allow the classi fi er to cover the maximum set of training samples. The effect of this choice is discussed in Sections 4.3 and 4.4. 3.6. Example
As a summarizing example, consider the set of genes represented in Fig. 7(a). Each gene is associated with its overlap score (os), its gene mask (string of 0 and 1), and its dominant class (dc). For example, gene g 1 has a mask of 0100101 (i.e., it classi fi es unambiguously the second, the fi fth and the seventh samples), an overlap score of 0.11, and its dominant class is class 1. For convenience,genes are pre-sorted by increasing overlap score value.

The fi rst gene selected by the greedy method in the minimum subset is g 4 , because it is characterized by the highest number of bits set to 1 (the same as g 6 and g 7 ) and the lowest overlap score. Then, genes with the best complementary masks are g 2 , g 5 ,and g 6 , which all have the same number of bits set to 1. Again, g 2 is selected because of its lower overlap score. Eventually, the only gene with a complementary mask, which is g 5 , is chosen. In this case the minimum number of genes is three. In Fig. 7(b) the genes in the minimum gene subset are reported.

The remaining genes are divided by dominant class and sorted by ascending overlap score. The gene rank is composed by selecting the topmost gene from each dominant class in a round robin fashion (e.g., g genes are required by the user for its biological investigation. Then, the three top ranked genes are added to the three genes of the minimum gene subset. The fi nal gene set is shown in Fig. 7(d). 4. Experimental results
We validated the MaskedPainter method by comparison with other feature selection techniques on public gene expression datasets. Classi fi cation accuracy is used as the performance metric for evaluation. The biological relevance of the selected genes is discussed in Section 4.5. We performed a large set of experiments addressing the following issues.

A) Classi fi cation accuracy. The accuracy yielded by MaskedPainter and several other feature selection
B) Cardinality of the selected feature set. The impact on classi fi cation accuracy of different numbers
C) Minimum gene subset de fi nition. The proposed greedy approach is compared with a set covering The experiments addressing each issue are reported in the following subsections.

We also analyzed the computational cost of our approach. We measured the time required by each approach to extract a high number of features (i.e., 1000 features) from the considered datasets. The MaskedPainter algorithm proved to be as ef fi cient as the competing feature selection methods. In particular, on a Pentium 4 at 3.2 GHz with 2 GByte of RAM, the time required to extract the top 1000 genes on any complete dataset is in the order of few seconds (e.g., less than 1 second on the Alon dataset, 3 seconds on the Brain2 dataset) and very similar to the time required by the other methods. 4.1. Experimental setting
We validated our feature selection approach on 7 multicategory microarray datasets, publicly available on [2,39,47]. Table 1 summarizes the characteristics of the datasets. Five are characterized by 3 to 9 classes, while two are bi-class datasets. Most contain between 60 and 90 samples, whereas one has 34 samples. The number of features ranges from 2 thousands to more than 10 thousands.

For each dataset we selected the best subset of genes according to the 6 feature selection methods reported in Table 2 available in RankGene [40], besides our approach. These feature selection methods are widely used in machine learning [30]. Furthermore, they are used as comparative methods in many feature selection studies on microarray data [21,36].

The experimental design exploits 50 repetitions of 4-fold strati fi ed cross validation for each parameter accuracy on the test set over all the 5 0 repetitions is th en computed. Similar expe rimental designs have been applied in [13,30,55]. Feature selection algorithms have been applied only on the training set to avoid selection bias. The statistical signi fi cance of the results has been assessed by computing the to us (i.e., p-value 0.05) are followed by a * sign, while best absolute values for each row are in bold.
All experiments have been performed by using small sets of features (from 2 to 20) to focus on the capability of the selected features to improve the classi fi cation performance. Using large sets of features allows the classi fi er to compensate for possible feature selection shortcomings by automatically pruning or giving low weights to the least relevant features. 4.2. Classi fi cation accuracy
We computed the classi fi cation accuracy provided by the MaskedPainter approach and by the six The experiments have been performed on all datasets in Table 1 with the J48 decision tree classi fi er [50] and the greedy subset search. The decision tree classi fi er is less capable to compensate for possible feature selection shortcomings by weighting the most/least relevant features. Hence, it allowed us to focus more effectively on the actual contribution of the feature selection. Different choices for these settings are discussed in the following subsections.
 Tables 3, 4 and 5 show the classi fi cation accuracy yielded by the 7 feature selection methods on the Alon, Leukemia, and Srbct test sets. The results obtained on the other four datasets are reported in the Supplemental Material. Each row reports the accuracy for a speci fi c cardinality of the selected feature set (reported in Column 1). The average accuracy value, the maximum value and the standard deviation for each method are reported in the last three rows. 1 The MaskedPainter (MP) approach provides a very good accuracy on all datasets. In particular, on the Alon, Brain1, Brain2, Leukemia, and Welsh datasets, the accuracy on the test set is statistically better than all other feature selection techniques. On the Tumor9 dataset, the MP method shows a performance comparable with the best techniques (IG and SV). Eventually, on the Srbct dataset it is outperformed by the SV technique for larger sets of features (18 and 20). However, its overall average performance is statistically better than all other methods. 4.3. Cardinality of the selected feature set
We analyzed the behavior of the MaskedPainter approach when varying the cardinality of the selected feature set. The average improvement of the proposed approach over the second best method is computed across all datasets, separately for cardinalities of t he feature set ranging in the interval 2 X 20. Table 6 shows the obtained results.

The MaskedPainter approach yields the highest improvements for low numbers of selected features, when the quality of the selected features more signi fi cantly affects classi fi er performance. Since the fi rst few selected features typically belong to the minimum gene subset (see Section 4.4), these results highlight the quality of this small subset with resp ect to the features selected by all other methods. For increasing cardinality of the selected feature set the performance difference decreases, but the MaskedPainter algorithm s till yields highe r accuracy.
 Furthermore, the second best feature selection algorithm is not always the same for all datasets. Hence, our approach can self adapt to the dataset characteristics better than the other methods, whose performance is more affected by the data distribution. 4.4. Minimum gene subset
To evaluate the effectiveness of the minimum gene subset we compared the classi fi cation accuracy and execution time of the greedy with the result achieved with a set covering approach. A preliminary study exploiting a set covering based approach on microarray datasets was presented in [5]. To the best of our knowledge, this is the only work that formalizes the minimum subset selection as an optimization problem. In the following, a brief description of this method is introduced.

The set covering approach considers the set of gene masks as a matrix of N  X  M bits and performs the following three steps. The fi rst two steps aim at reducing the size of this matrix, to reduce the computational time.

A) Sample reduction . Each sample (i.e., column) that contains all 0 or 1 over the N gene masks is
B) Gene reduction . Each gene (i.e., row) whose gene mask is a subsequence of another gene mask is
C) Reduced matrix evaluation . The reduced matrix is evaluated by an optimization procedure that
In Table 7 the average accuracy and the average size 2 of (i) the greedy (Columns 2 and 3) and (ii) the set covering (Columns 5 and 6) minimum subsets are reported for all datasets. Values are averaged over the 50 repetitions of the 4-fold cross validation. The average execution time for one fold, which corresponds to an estimate of the time needed by the fi nal user to perform the feature selection, is also reported for both techniques (greedy in Column 4 and set covering in Column 7).

Independently of the dataset, the greedy minimum subset size is always larger than the set covering size. The greedy approach selects the gene maximizing the number of covered samples at each iteration. The set covering approach, instead, exploits a global optimization procedure to select the minimum number of genes that cover the samples. Hence, the greedy approach may need a larger number of genes to reach the best coverage of the training samples. This larger gene set provides a higher accuracy on most datasets, because it yields a more general model which may be less prone to over fi tting. For instance, on the Leukemia dataset the average accuracy is 85.89% for the set covering approach and 87.00% for the greedy approach.

The greedy algorithm is also characterized by a lower execution time with respect to the set covering algorithm. On average, the set covering completed in tens of seconds, whereas the greedy took from 0.1 to 2.6 seconds, considering all datasets. 4.5. Biological discussion We analyzed the biological information presented in literature for the genes selected by the Masked-Painter technique. In Table 8 we report the fi rst twenty genes selected by our algorithm on the entire Alon dataset, related to colon cancer and commonly used for biological validation [8,15]. Column 4 shows references to published works on colon cancer discussing the genes reported in Column 1. The genes deemed as relevant by the MaskedPainter feature selection technique have been identi fi ed and discussed in previous biological studies.

For example, gene Z50753, named GUCA2B, related to uroguanylin precursor, is shown to be relevant in [38]. Lowered levels of the uroguanylin may interfere with renewal and removal of epithelial cells. This could result in the formation of polyps, which can progress to malignant cancers of the colon and rectum [38]. As a second example, the downregulation of H06524, the GSN gene (gelsolin), combined with that of PRKCB1, may concur in decreasing the activation of PKCs involved in phospholipid sig-nalling pathways and inhibit cell proliferation and tumorigenicity [6]. Furthermore, RTPCR experiments could be exploited to deeply validate the biological meaning of selected genes, and other tools, such as David [10] and GSEA [41], could be potentially applied for further biological enrichment. 5. Conclusions
Feature selection is a well-known approach to identify relevant genes for biological investigation (e.g., tumor diseases). Feature selection techniques have proved to be helpful in tumor classi fi cation and in identifying the genes related to clinical situations.
 In this paper we propose a new method for feature selection on microarray data, the MaskedPainter. It allows (a) de fi ning the minimum set of genes that provides the best coverage of the training samples, and (b) ranking genes by decreasing relevance, thus allowing the user to customize the fi nal size of the feature set. The MaskedPainter method has been compared with six other feature selection techniques on both binary and multiclass mi croarray datasets. In most experimental settings it yi elds the best accuracy, while its computational cost is similar to the other feature selection techniques. The approach has been validated on microarray datasets, but we believe it may be applied to any dataset characterized by noisy and continuously valued features, such as peptide expressions.

On the Alon dataset, the identi fi ed relevant genes are consistent with the literature on tumor classi fi -cation. Hence, the MaskedPainter approach may provide a useful tool both to identify relevant genes for tumor diseases and to improve the classi fi cation accuracy of a classi fi er. The authors will provide the source code or the executable References
