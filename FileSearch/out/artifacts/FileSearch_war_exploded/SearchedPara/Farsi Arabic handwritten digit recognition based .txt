 ORIGINAL PAPER Hamid Salimi  X  Davar Giveki Abstract The problem of handwritten digit recognition has long been an open problem in the field of pattern classification and of great importance in industry. The heart of the problem lies within the ability to design an efficient algorithm that can recognize digits written and submitted by users via a tablet, scanner, and other digital devices. From an engineering point of view, it is desirable to achieve a good performance within limited resources. To this end, we have developed a new approach for handwritten digit recognition that uses a small number of patterns for training phase. To improve the over-all performance achieved in classification task, the literature suggests combining the decision of multiple classifiers rather than using the output of the best classifier in the ensemble; so, in this new approach, an ensemble of classifiers is used for the recognition of handwritten digit. The classifiers used in proposed system are based on singular value decomposition (SVD) algorithm. The experimental results and the literature show that the SVD algorithm is suitable for solving sparse matrices such as handwritten digit. The decisions obtained by SVD classifiers are combined by a novel proposed com-bination rule which we named reliable multi-phase particle swarm optimization. We call the method  X  X eliable X  because we have introduced a novel reliability parameter which is applied to tackle the problem of PSO being trapped in local minima. In comparison with previous methods, one of the significant advantages of the proposed method is that it is not sensitive to the size of training set. Unlike other methods, the proposed method uses just 15 % of the dataset as a train-ing set, while other methods usually use (60 X 75) % of the whole dataset as the training set. To evaluate the proposed method, we tested our algorithm on Farsi/Arabic handwritten digit dataset. What makes the recognition of the handwrit-ten Farsi/Arabic digits more challenging is that some of the digits can be legally written in different shapes. Therefore, 6000 hard samples (600 samples per class) are chosen by K-nearest neighbor algorithm from the HODA dataset which is a standard Farsi/Arabic digit dataset. Experimental results have shown that the proposed method is fast, accurate, and robust against the local minima of PSO. Finally, the proposed method is compared with state of the art methods and some ensemble classifier based on MLP, RBF, and ANFIS with various combination rules.
 Keywords Classifiers combination  X  Two-dimensional PCA (2DPCA)  X  Singular value decomposition (SVD)  X  Particle swarm optimization (PSO)  X  Reliability 1 Introduction Handwritten digit recognition is a subfield of optical char-acter recognition (OCR) which has currently attracted many researchers to OCR applications. It deals with some real-world applications such as postal mail sorting [ 62 ] and bank check processing [ 11 , 32 ]. The task of assigning an unknown object to one of the 10 predefined classes is a hard problem to solve, since not only the diversity of objects within each class is high but also some objects from different classes may be quite similar [ 55 ]. Many methods have been proposed for the recognition of handwritten Latin digits with very On the other hand, research progress has been very limited toward automatic recognition of digit written in scripts other than Latin. In this research paper, Farsi/Arabic handwritten digit is used to test our algorithm.
 Farsi is the main language in Iran, Tajikistan, and Afghanistan, and it is spoken by more than 110 million people [ 47 ]. Like Latin script, handwritten Farsi/Arabic dig-its have large variations in writing styles, sizes, and orienta-tions [ 47 ]. Similar to other scripts, there are 10 numerals in Farsi. In Farsi/Arabic scripts, alphabets are written from right to left but digits are written from left to right. Farsi and Ara-bic numerals are almost the same, but there are some impor-tant differences between the handwriting of digits of these two scripts [ 53 ]. These significant differences are related to numerals of 4 and 6 which are written in different types (see Fig. 1 ). Generally, in Persian digits, there are two types of writing for the digits 0, 2, 3, 4, 5, and 6. These characteristics make the recognition of Farsi numerals more complicated than in other languages (Fig. 2 ).

Several recognition techniques for handwritten Farsi dig-its had been published; for instance, Soltanzadeh and Rah-mati [ 61 ] used outer profiles, crossing counts, and projection histograms from multiple orientations as features. Mowlaei and Faez [ 42 ] used a support vector machine (SVM) as clas-sifier. The authors in [ 42 ] proposed a system for the recog-nition of isolated Farsi numerals and characters. The Haar wavelet was applied to obtain the features and these features were then fed to SVMs for training. A template-based fea-ture extraction method was proposed by Ziaratban et al. [ 72 ]. Twenty templates which are the most significant informa-tion from handwritten Farsi/Arabic numerals were selected heuristically. Feature extraction was conducted via template matching. For each template, it was found the best match in an input image and record the location and the match score of the best match as features. These features were fed into a multi-layer perceptron (MLP) for training.

For handwritten character/digit recognition, three main steps typically should be considered namely, preprocessing, feature extraction, and classification [ 35 ]. Preprocessing is applied to improve image quality for further processing and feature extraction [ 3 ]. Some options for preprocessing include the size and aspect ratio of normalized image, the interpolation technique of pixel values, histogram equaliza-tion, blurring, and other techniques. Feature extraction is one of the most important steps to increase classification performance. A large variety of feature types and technique feature extractions methods have been proposed.

In classification step, a large number of new classifiers have been recently proposed and tested on various OCR databases. Among these classifiers, it is shown that classi-fiers based on SVD algorithm are superior to other methods for handwritten digit recognition. SVD algorithms are strong methods in solving the problem linked to sparse matrices. Since handwritten digit image is a kind of sparse matrix, classifier based on SVD algorithm is one of the good choices to recognize them. In practice, it is hard to find and train a classifier which fits the data distribution sufficiently well [ 8 ]. To improve the overall performance achieved in classifica-tion task, the literature suggests combining the decision of multiple classifiers rather than using the output of the best classifier in the ensemble [ 19 , 29 ]. In an ensemble of classi-fiers, it is hoped that by combining a set of base classifiers, the deficiency of each classifier in the ensemble may be com-pensated by the efficiency of the others. However, the subtle point is that this combination of the multiple base classifiers can be effective only if the individual base classifiers are accurate and diverse [ 7 ]. Two base classifiers are said to be diverse if their errors occur on different parts of the input space. Various techniques have been used to make diversity among ensemble of base classifiers [ 6 ]. These techniques can be classified into implicit and explicit methods [ 45 ]. Among different classifiers, the base classifiers such as MLP can be easily made diverse. Because this type of classi-fiers are initialized by different random weights and different number of hidden layers which may lead to create diversity but base classifiers based on SVD algorithm work in a dif-ferent way. By carrying out several experiments on different dataset, we observed that the base classifiers based on SVD algorithm have no diversity if different numbers of basis are selected. Therefore, a hybrid model based on combining of holistic and local method is used to make diversity.
Recognition techniques are divided into three categories: (1) holistic (global) approach, (2) local approach, and (3) hybrid approach [ 15 , 64 ]. In the holistic approach, input of classifier is an original image. Although these methods work well, they highly depend on image rotation [ 46 ]. The local (component-based) approach is an alternative to the global approach. In this method, instead of using global image as main input space, some local components of images are considered as input space [ 17 , 68 ]. The hybrid (mixture) approaches take advantages of both local and global features to improve the recognition rate of the system, because in this way, more comprehensive information could be utilized [ 28 , 29 ].

In our method, we used a hybrid approach to clas-sify handwritten digits more accurately. At first, to reduce input dimension and being effective in generalization, each image sample was divided into two vertical and horizon-tal parts. The feature extraction based on 2DPCA algo-rithm was then applied separately to original images and each segmented part. After dimension reduction for each image in separated parts and global part, a classifier based on SVD algorithm was considered for each part. It has been shown in the literature whenever a proper weight was assigned to the decision of each classifier, the final per-formance would increase dramatically. Therefore, to find a proper weight for the decision of each classifier, one of the stochastic optimization algorithms called particle swarm optimization (PSO) was applied. To avoid trapping in local minimum, the PSO was executed several times. By using a novel reliability parameter, the best weighting vector among these executions was selected. In compari-son with the ensemble of MLP, RBF, ANFIS classifiers that use popular combination rule such as maximum, produc-tion, averaging, and decision template (DT), the obtained results by our proposed method were more accurate and effective.

The rest of the paper is organized as follows: in Sect. 2 , 2DPCA algorithm is studied as a dimension reduction method. The classifier based on SVD algorithm is introduced in Sect. 3 . In Sect. 4 , the PSO algorithm is explained. Sec-tion 5 is contained proposed method. In Sect. 6 , the experi-mental results are studied and Sect. 7 summarizes the main results of the paper and offers concluding remarks. 2 Two-dimensional PCA (2DPCA) In statistical pattern recognition, high dimensionality is a major cause of the practical limitations of many pattern recognition technologies such as handwritten digit. More-over, it has been observed that a large number of features may actually degrade the performance of classifiers if the number of training samples is small relative to the number of features [ 22 , 51 , 52 ]. In other words, the complexity of a distribution increases rapidly when the dimensionality increases. There-fore, very large data would be needed in general to train a classifier well. It is generally accepted that one needs at least ten times as many training samples per class as the number of features to obtain well-trained classifiers [ 16 , 22 , 52 ]. How-ever, the number of samples is often small because of the limitations on sample availability, identification, time, and cost. Consequently, dimensionality reduction is essential not only to engineering applications but also to the design of clas-sifiers. In fact, the design of a classifier becomes extremely simple if all patterns in the same class hold the same feature vector which is different from the feature vectors held by patterns from other classes. Therefore, many feature extrac-tions are proposed to tackle the above problems. One of the successful kinds of feature extraction methods is based on principal component analysis (PCA) methods.

PCA [ 60 , 65 ] is one of the well-known feature extraction which 2D image matrices are firstly transformed to 1D image vectors by vectorization. The vectorization of a matrix is the column vector obtain by stacking the columns of the matrix on top of one another. The covariance or scatter matrix are formulated from these image vectors. The covariance matrix will be well estimated if and only if the number of available training samples is not far smaller than the dimension of this matrix. In fact, it is too hard to collect this number of sam-ples. Then, normally in 1D subspace analysis, the estimated covariance matrix is not well estimated and not full rank [ 48 ].
Two-dimensional principal component analysis (2DPCA) was proposed by Yang et al. [ 67 ] to apply with face recogni-tion and representation. Unlike PCA, the image covariance matrix is computed directly on image matrices so the spatial structure information can be preserved. This yields a covari-ance matrix whose dimension just equals to the width of the face image. This is far smaller than the size of covariance matrix in PCA. Therefore, the image covariance matrix can be better estimated and will usually be full rank that means, the curse of dimensionality and the small sample size (SSS) problem can be avoided [ 48 ].
 Let A be an arbitrary matrix with dimension m  X  n , and X  X  R n  X  d be a matrix with orthonormal columns, n  X  d . By projecting A onto X , a matrix Y = AX with dimension m  X  d is obtained. In 2DPCA, the total scatter of the projected samples was used to determine a good projection matrix X , that is, the following criterion is adopted: J (
X ) = trace E [ ( Y  X  EY )( Y  X  EY ) T ] where the last term in Eq. ( 1 ) is obtained from the fact that trace ( AB ) = trace ( BA ) , for any two matrices A and B [ 4 ].
Let G = E [ ( A  X  EA ) T ( A  X  EA ) ] is defined as a non-negative matrix with dimension n  X  n . It is referred as image covariance matrix . Suppose there are M images as training set, denoted each image by A k ( k = 1 , 2 ,..., M ) which is an m  X  n matrix, and  X  A = 1 M M k A k is denoted as the average image. Then, G can be computed as follows: G = 1 It was proven that the optimal value for the projection matrix X opt was composed of the orthonormal eigenvectors X ,..., X i.e., X opt =[ X 1 ,..., X d ] . Since G is an n  X  n matrix, com-puting its eigenvectors is efficient [ 67 , 69 ]. Like PCA, the value of d can be controlled by setting a threshold as follow: where  X  1 , X  2 ,..., X  n are the n largest eigenvalues of G and is a preset threshold [ 67 ]. X opt is considered the optimal pro-jection axes which it maximizes the generalized total scatter criterion J(X) in Eq. ( 1 ). Therefore, dimensionality of every image A k in dataset is reduced by post-multiplying image with optimal projection axes as A k  X  X opt . 3 Classification based on SVD algorithm This section describes a classification algorithm that is based on the modeling of the variation within each digit class using orthogonal basis vectors computed using the SVD. This can be seen as a least squares algorithm based on a reduced rank model. To classify based on SVD, suppose each image of the same digit is reshaped into some vectors. The vectorizing for numeral 2 in Persian form is shown in Fig. 3 as follows:
Suppose n digit images of one class has dimension of 30  X  30. Let A 900  X  n , be the matrix consisting of all the training digits of the same type, for example, number 2. The columns of A span a linear subspace of 900 . However, this subspace cannot be expected to have a large dimension, because if it did, then the subspaces of the different kinds of digits would intersect [ 14 ]. To overcome this problem, we used 2DPCA feature extraction to reduce dimension of inputs.
 To explain the SVD classifier, let us define SVD theorem: Theorem 1 (SVD) Any m  X  n matrix A, with m  X  n, can be factorized A = U where U  X  m  X  m and V  X  n  X  n are orthogonal, and  X   X  n is diagonal, = diag ( X   X 
The idea of SVD classification is to model the variation within the set of training (and test) digits of the same type using an orthogonal basis of the subspace. An orthogonal basis can be computed using the SVD, and any matrix A is a sum of rank 1 matrices: Each column in A represents an image of a digit, and there-fore, the left singular vectors u i (denoted as vertical line in Eq. ( 5 )) are an orthogonal basis. It is referred to the left sin-gular vectors as  X  X ingular images X  [ 4 ]. From Eq. ( 5 ), the j th column of A is equal to: a and coordinates of image j in A in terms of this basis are  X  v
The truncated SVD is very important, not only for remov-ing noise but also for compressing data and for stabilizing the solution of problems that are extremely ill-conditioned [ 68 ].
 Theorem 2 Assume that the matrix A  X  m  X  n has rank r &gt; k. The matrix approximation problem min rank ( Z ) = k A  X  has the solution: Z = A where U k = ( u 1 ,..., u k ), V k = (v 1 ,...,v k ) , and ( X 
A  X  A k 2 =  X  k + 1 . (8) (To prove this theorem, you can refer to [ 14 ], 6.3).
The SVD basis classification algorithm will be based on the following assumptions [ 14 ]: 1. Each digit (in the training set and the test set) is well 2. An expansion in terms of the first few singular images 3. If an unknown sample can be better approximated in one
Therefore, it should be computed that how well an unknown digit can be represented in the 10 different bases. This can be done by computing the residual vector in least squares problems of the type: min where z represents an unknown digit and u i represents the singular images, and  X  i is unknown coefficient of singular for basis ith which must be calculated by numerical approx-imation. In order to solve Eq. 9 , it can be rewritten in the form: min where U k = ( u 1 ,..., u k ) . Since the columns of U orthogonal, the solution of this problem is given by U problems is (
I  X  U k U T k ) z that is, the norm of the projection of the unknown digit onto the subspace orthogonal to span ( U k ) [ 14 ]. Therefore, z belongs to the class which has minimum residual among all classes. An SVD basis classification algorithm is illustrated in below.
 4 PSO-based weighting for linear combination The PSO is a stochastic optimization algorithm which devel-oped by Kennedy and Eberhart [ 25 ]. This iterative optimiza-tion was inspired by social behavior of flocks of birds which are searching for their food. The potential solutions in PSO are called particles which search the problem space for better regions. The position of each particle is influenced by the best visited position by itself and the position of the best particle in its neighborhood. The best position in the entire swarm is called global best particle, and it is referred to G best smaller neighborhoods are used, the algorithm is generally referred to as L best of PSO.

The performance of each particle is measured using a pre-defined fitness function, which is related to the problem to be solved. Each particle in the swarm has some characteristics such as current position, velocity (rate of position change), and personal best position.

The personal best position of particle i shows the best reached fitness by that particle at a given time. Let f be the objective function which is going to be minimized. Then, the personal best position of a particle at time step t is updated as y ( t ) = which x i denoted as current position and y i referred as per-sonal best position of particle i .Forthe G best , the best parti-cle is determined from the entire swarm by selecting the best personal of best position. This position is determined by The equation of velocity update is: v ( t + 1 ) = wv where v ij ( t + 1 ) is the velocity updated for the jth dimension, j = 1 , 2 ,... d . c where the first constant moderates the maximum step size toward the personal best position of the particle while the second constant moderates the maximum step size toward the global best position in just one iteration. r 1 j ( t ) and r two random numbers within the range [ 0 , 1 ] and give the PSO algorithm a stochastic search property. Velocity is updated on each dimension, and it can be clamped with a user-defined maximum velocity v max , which would prevent them from exploding, thereby causing premature convergence [ 45 ].
In Eq. 13 , the inertia weight w affects the contribution of v is large, it makes a large step in one iteration (exploring the search space), and when w is small, it makes a small step in one iteration. Typically, the inertia weight is set to 0 w&lt; 0 . 9. The following equation shows the update position of each particle: x ( t + 1 ) = x i ( t ) + v i ( t + 1 ) (14) In swarm terminology, particle flies to its new position. After the new position is calculated for each particle, the iteration counter is increased and the new particle positions are evalu-ated. This process is repeated until some convergence criteria are satisfied [ 45 ]. 4.1 Liner combination rule based on PSO It can be used G best from the PSO algorithm to find the opti-mal weights for linear combination of multiple SVD classi-fiers. The objective function which should be minimized is the total classification error rate of the ensemble system on the validation dataset. Each particle represents a candidate weight for linear combination of multiple classifiers. Each particle x is constructed as x = (w 1 ,w 2 ,...w L ) , where is the weight of the classifier i . For each sample z from the validation data set, the final decision is: Decide z  X   X  k if D k ( z ) = min where D j ( z ) is D The fitness function which should be minimized is f ( x ) = where C is the number of correctly classified samples in the validation dataset and E is the number of misclassified samples [ 45 ]. 5 The proposed method In this section, our proposed method is described in details. First, preprocessing is performed to increase the quality of images for future process. Several preprocessing techniques are used as described in [ 12 , 41 ]. Some usual preprocessing techniques are blurring, histogram equalization, and normal-ization. According to [ 59 ], blurring has a great importance for the identification process in classifying handwritten dig-its. Blurring can be described as smoothing the pattern or making sharp edges and corners softer. For smoothing the images, a commonly used technique is convolving the image with a Gaussian kernel Eq. ( 18 ) G where  X  is the standard deviation. The larger the  X  smoother the image. Based on [ 55 , 59 ], we set  X  = Furthermore, histogram equalization has been applied to enhance the contrast of images.

As stated before, although SVD classifier is proper for recognizing sparse matrices such as handwritten digits, it has no diversity using the selection of different numbers of orthogonal vectors. Therefore, we decided to combine the local and holistic methods to make diversity among SVD classifiers. The main question is how to divide the input space into several new parts to improve the total per-formance. Since Farsi/Arabic handwritten digits have sim-ple structure forms in comparing with other complicated image such as human face image, partitioning into two equal horizontal, and vertical parts lead us to have maxi-mum diversity. Throughout some experiments, we also con-cluded separating digits image into mentioned way gives us a more accurate system. Figure 4 shows the partitioning Persian digit 7 into two equal horizontal and vertical parts (Fig. 5 ).

After performing horizontal and vertical image sepa-rations, to reduce the input dimensions of each part, the 2DPCA algorithm was applied to global images and each separated part for feature extraction. For each part in the training and test sets, first, the average images are com-puted, the image covariance matrix which was stated in Eq. ( 2 ) is then calculated. By selecting a proper preset threshold, X opt the d largest eigenvector of image covariance, G ,inEq.( 2 ) is achieved. X opt is considered the optimal projection axes which it maximizes the gen-eralized total scatter criterion J(X) in Eq. ( 1 ). Therefore, dimensionality of every image A k in dataset is reduced by post-multiplying image with optimal projection axes as A all modified images are vectorized into one dimension (see Fig. 6 ).

To classify each part of the image, an SVD classifier was then used. It is expected that the accuracy of classifi-cation would increase if some weights were reassigned to the decision of classifiers [ 45 ]. Therefore, the PSO-based weighting method is used to assign some weights to classi-fiers.

We tried some different schemes of ensemble systems such as structures called simple PSO and hierarchical PSO. The scheme of these ensemble systems has been illustrated in Fig. 5 . In the simple PSO combination rule structure which is shown in Fig. 5 a, each image in the training set is separated into vertical and horizontal parts. 2DPCA is then applied to each part. Afterward, all of the vertical, horizontal, and original reduced images are used for train-ing by some individual SVD classifiers. The PSO algo-rithm is then applied to find proper weight for the deci-sion of each classifier. This procedure continues until the fitness function in Eq. ( 17 ) is minimized on the training set.

In the case of hierarchical PSO combination rule that is illustrated in Fig. 5 b, two simple PSO combination rules are applied which are referred to as vertical and horizon-tal PSO combination rules. After finding suitable weights for classifier decision profile, another PSO combination rule was employed to assign weights to the final decision of the vertical and horizontal combination rules. In both cases, we observed that the SVD classifier which was defined for origi-nal images assigned much more weights in comparison with others. As a result, we faced with a relatively poor per-formance. As stated before, the purpose of the PSO com-biner is to minimize fitness functions. This purpose is ful-filled using the decisions of classifiers which are chosen, that is, original images and one of the other classifier. As we observed in the final results of our implementations, the PSO combiner eliminates some decisions of other classi-fiers by assigning 0 as a weight to the decisions of those classifiers. As mentioned before, each separated part of the image would lead us to create diversity among SVD classifiers. By assigning 0 as a weight to the decision of a classifier, the diversity among classifies is significantly decreased.

To tackle this problem, a new scheme of ensemble system which we called multi-phase PSO combination rule is suggested. In this scheme, the first phase is find-ing proper weights for the decisions of vertical classi-fiers based on the simple PSO combination rule. This process is continued until the fitness function which is con-sidered for vertical classifiers is minimized. In the sec-ond phase, similar to first phase, the suitable weights are found by PSO algorithm for the horizontal SVD classi-fiers. In the third phase, another simple PSO is applied to find the proper weight for the SVD classifier which is considered for the original images. In this phase, the final result for evaluating the fitness function is computed based on integrating weighted decision of vertical and hor-izontal classifiers with the weighted decision of the clas-sifier which takes the original image into account (see Fig. 7 ).

By using this scheme, we could achieve a high accu-racy rate of recognition but the final results seem to be unstable, that is, in every implementation on the same data, we did not get the same results. Since the size of the training set is small, PSO may easily find some weights in which the proposed method recognizes all sam-ples in the training set, but these weights are not suitable for the test phase. This is because of the nature of PSO and the fact that it may sometimes be trapped in local minima.

Thus, we came with the idea of finding a way for escaping from local minima in PSO. One of the meth-ods to overcome the local minimum is to use random-ized initialization. Using random algorithms, it is expected that these algorithms probably lead us to find global min-imum. As stated before, the PSO algorithm is a ran-dom stochastic optimization algorithm. By executing the PSO algorithm several times, a set of proper weights is obtained, one of them which may guide us to escape from local minima. The main question is: which of these proper weight sets should be selected to lead us to global minima? To answer this question, a simple reliability parameter is introduced to select the best proper weight set.

Let us define the concept of first winner and second win-ner. In classifiers based on the SVD algorithm, the first winner (
O decision profile that is obtained by SVD classifier. It means an unknown sample is assigned to the class which has the minimum value in its decision profile acquired by SVD clas-sifier. The second winner ( O 2  X  w inner ) is the class which has the second minimum value after first winner. Wisely think-ing, whenever the difference between the first winner and second winner is high, the decision of classifier about an unknown sample is more reliable [ 9 ]. There are two ways to consider reliability parameter: one is the difference between the first winner and the second winner (Eq. 19 ), and in the sec-ond, we may consider second winner divided by first winner (Eq. 19 ).  X   X  Both of these parameters can be considered as a reliability parameter but experimental results have shown, averaging of  X  rately (Eq. 21 ).  X  = Let PSO combination rule algorithm be executed n times. Hence, we will have n weighting sets denoted by  X  = {  X  weights for the decision of classifiers, and a value as its fit-ness function over the training set. Among all members of  X  , the members which have minimum value in their fitness function are selected. Let k the members of  X  be gathered into a new set  X   X  ={ X   X  1 ,  X   X  2 ,...,  X   X  k } . For each of the reliability parameter is computed. Let  X  i be defined as a final value of the reliable parameter for  X   X  i . At the begin-ning of algorithm,  X  i is assigned to 0. For each image (i.e., d ) in the training set, if d is recognized correctly by the ensemble system in which the weight of ensemble classi-fiers are tuned by  X   X  i , its reliability parameters,  X  d data are integrated by  X  i .If d is misclassified by the ensem-ble system, the average of the differences between the value of the correct class in the decision profile with first win-ner and the ratio of the correct value of class to first win-ner is subtracted form  X  i . Finally,  X   X  j in  X   X  is selected as the proper weight for ensemble classifiers which has maximum value in its  X  j among others, it means  X   X  j = arg max j The reliable PSO combination rule algorithm is presented as follows:
The reliable PSO combination rule is executed for each phase in the multi-phase process in the ensemble system to select the best weights where it guides us to the best recogni-tion rate. Experimental results demonstrate that multi-phase PSO structure is better than other schemes and the reliable multi-phase PSO combination rule is better than multi-phase PSO since it is accurate, stable, and robust against local min-ima of PSO. 6 Experimental results What makes the recognition of the handwritten Farsi digits more challenging is that some of the digits such as 2, 3, 4, 5, and 6 can be legally written in different shapes [ 47 ]. To evaluate the performance of the proposed method, we implemented it on 6000 samples which were selected by K-nearest neighbor (KNN) from HODA Farsi handwritten digit database [ 27 ].

The HODA dataset is a very large corpus of Farsi hand-written digits. This dataset is extracted handwritten digits from 11,942 forms filled by diploma and bachelor students registered in the Iran X  X  nationwide university entrance exam; 5,393 forms were filled by Diploma students and 6,549 others by BS students. All forms were scanned at 200 dpi resolu-tion in 24 bit color format. After applying a threshold, all scanned digit came into 102,352 binary images which were chosen 60,000 images for train and 20,000 for test (Fig. 8 ). The dataset specifications are shown in Table 1 [ 21 ].
Since each image digit of HODA dataset has different size, in order to achieve constant dimensions, we resize each digit into a matrix with dimension 30  X  30. To create training set, we selected randomly 100 samples per class and the rest of them were considered as test set.

Since the HODA dataset is gathered from preregistration forms of the university entrance exam, it can be consid-ered an easy database, because it was filled precisely. To extract harder samples, the KNN algorithm was applied. Considering K = 8, we selected the samples which were classified into more than three classes. Therefore, we cre-ated a hard handwritten digit from HODA dataset with selecting 6000 samples in which 600 samples were con-sidered for each class. To evaluate the difficulty level of selected samples, the recognition rates based on multiclass least squares SVM (LS-SVM) have been reported for both the PCA and 2DPCA feature extractions. Thus, 5-fold cross-validation scheme was applied to the whole dataset. In 5-fold cross-validation, a multiclass LS-SVM was trained on 4/5 of the whole dataset and the remaining samples were tested. Table 2 shows the average of the recognition rates on 5-fold cross-validation for both PCA (using the first 30 components obtained by PCA) and 2DPCA (using the first 7 column obtained by 2DPCA with considering pre-threshold,  X  = 0 . 9).

The created dataset was then divided into a training set and a test set. For each experiment, 15 % of the whole dataset was considered randomly as training set and the rest as test set. All images in training and test set were then sepa-rated into horizontal and vertical parts. To reduce the dimen-sion for each separated part and global images in training and test sets, the 2DPCA feature extraction was then per-formed.

For ensemble of classifiers based on SVD algorithm, we considered one classifier for each part. Therefore, we have 5 classifiers (2 classifiers for vertical parts, 2 for horizontal parts, and 1 for global images). We set K = 11 as number of basis for the all SVD classifiers, like PSO literature [ 49 ], The PSO parameters were set to c 1 = c 2 = 1 . 49445 ,w = 0 . 7298, and V max = 4. The population size, iterations, and the number of PSO executions for each phase were set to 10, 20, and 10, respectively. Table 3 shows the recognition rates of the proposed method on different 2DPCA compo-nents with various combination rules. In order to ensure that the comparisons are fair enough, some common classi-fiers with various combination rules were considered. Multi-layer perceptron (MLP), radial basis function (RBF), and adaptive neuro-fuzzy interface system (ANFIS) were used as well-known classifiers, and maximum (Max), production (Pro), average (Ave), decision template (DT) [ 30 ], particle swarm optimization (PSO) [ 45 ] were considered as popu-lar combination rules. As demonstrated in Table 3 , the new combination rule, reliable multi-phases particle swarm opti-mization (RMP-PSO) performs better than other popular combination rules which have been suggested in the liter-ature. It is worthy to remark that all the results reported in Tables 3 , 4 , 5 and 6 are based on the average recognition rate of over 20 runs.

For ensemble of MLP classifiers, one classifier was con-sidered for each part. Each classifier contained 50 neurons in the hidden layer with a learning rate of 0.1, a momentum of 0.15 and 100 epochs. Table 4 shows the recognition rates on different PCA components with various combination rules on ensemble of MLPs.

In the case of ensemble of RBF neural network, 50 neu-rons were considered as the hidden layer for each classi-fier. To find the internal weights of RBF neural network, the Moore X  X enrose pseudo inverse algorithm was used. Table 5 shows the recognition rates on the ensemble of RBF with var-ious combination rules. Similar to Table 4 , the novel RMP-PSO had the highest recognition rate in comparison with other methods.

The ANFIS is a fuzzy Sugeno model put in the framework of adaptive systems to facilitate learning and adaptation [ 23 ]. For ensemble of ANFIS classifiers, five ANFIS classifiers were considered (one ANFIS for each part). All ANFIS clas-sifiers were trained with back propagation gradient descent method. The fuzzy rule architectures of the ANFIS classifiers were designed using a generalized bell-shaped membership function. For each ANFIS classifier, samples with target out-puts i was set to 1 and the other classes set to 0 (e.g., (0, 0, 0, 1, 0, 0, 0, 0, 0, 0) was considered as target output for class 4th). This model was implemented by Matlab ANFIS toolbox with version 7.11. In our ANFIS implementation,  X  X enfis3 X  func-tion was used with Sugeno model that consisted five cluster numbers.
 Table 6 shows the recognition rates on the ensemble of ANFIS classifiers with various combination rules. Similar to the previous results, the novel RMP-PSO had the highest recognition rate in comparison with other methods.

The results in Tables 3 , 4 , 5 , and 6 are reported for differ-ent PCA/2DPCA components. In the classification problems related to high dimensionality, since the data dimension is high, many classifiers such as MLP, RBF, and ANFIS usu-ally have low performance. Therefore, we used PCA/2DPCA algorithm to reduce the input dimension. On other hand, dimension reduction algorithm incurs information lost. In most of the dimension reduction algorithms such as PCA and 2DPCA, whenever the number of used component is decrease, the ratio of information lost is increased. Therefore, we need to balance a trade-off between dimension reduction to have a good generalization and increasing the dimension-ality to reduce information lost. Therefore, there is an opti-mal point for selecting feature number in order to have a good classification while minimizing the information lost. In this paper, we evaluated various PCA/2DPCA component numbers to find the optimal used components which lead to good generalization. When the numbers of PCA/2DPCA components are increasing in Eq. 3 , a classifier senses more information from data (  X  % entire variances of transformed features). Hence, the classifier performance will be increased. Increasing classifier performance will be stopped at the par-ticular selected  X  that it will be identified experimentally. As a result, using more eigenvectors not only adds valu-able information to classifier, but also decreases classification performance.

As it can be concluded from these tables, RMP-PSO com-bination rule is more efficient than other well-known com-bination rules. RMP-PSO superiority is based on a reliable stochastic optimal search aimed to find proper weights for increasing generalization in test phase (Fig. 8 ).

To evaluate the efficiency of each combination rule, the average recognition rate of each base classifier is reported in Table 7 . The number of components considered for ensem-ble classifiers were set to optimal numbers which were obtained experimentally. For SVD classifier, we considered K = 9 as selected 2DPCA components; for MLP, RBF, and ANFIS, we selected the first 30, 20, and 10 PCA components, respectively. Table 7 shows the average recognition rates of different base classifiers on each part for over 20 runs. As seen in Table 7 , the best recognition rate for SVD classifier belonged to Global part with recognition rate of 91.70 %. The performance of ensemble of SVD classifiers with consider-ing RMP-PSO combination rule was 97.02 % while the other combination rule had recognition rate less than RMP-PSO. It means 5.32 % improvement on the handwritten recogni-tion in comparison with the recognition rate of the best SVD classifier. Moreover, the best recognition rates of other clas-sifiers, like MLP, RBF, and ANFIS were 85.12, 86.95, and 87.28 % respectively. By considering ensemble of these clas-sifiers with RMP-PSO combination rule, they resulted 88.48, 86.04, and 89.43 % respectively. We observed improvement in the final results for the all case. These experiments prove three remarkable cases which were stated in the previous sections: (1) Classifiers based on SVD are appropriate for solving problems related to spars matrices like handwritten digit recognition. (2) Using hybrid approach incurs making diversity among SVD classifiers. (3) RMP-PSO combination rule is more promising in comparison with the other common combination rules.

We also show the average time for training each base clas-sifier in Fig. 9 . As illustrated in Fig. 9 , in comparison with other well-known classifiers, the time needed for the training of SVD classifier on each segmented part is significantly less than the others.

We also compared reliable multi-phase PSO combination rule with multi-phase PSO on the constant partitioning of training and test set in Table 8 . The results are reported with an average of 10 runs. As stated before, combination rule based on PSO has potential being trapped in local minima. The result in the Table 8 experimentally proves the low gener-alization ability of multi-phase PSO in comparing with reli-able multi-phase PSO combination rule on the test phase.
We also use confusion matrix to realize the distribution of errors across the classes. Figure 10 shows the confusion matrix of recognition results of the proposed model with vari-ous classifiers. In Fig. 10 a, the details of the confusing results of the proposed method are shown. As seen in Fig. 10 a, major confusions in the proposed model are related to Persian digits 3, 4, 5, and 6. This happened because 2, 3, 4, and 6, 9 look like each other. From the Fig. 10 a, it may be noted that from 500 samples of numeral, three (3), 23 (0.5 %) samples were misrecognized by numeral 2 and 14 (0.3 %) samples were misrecognized to numeral 4. In the case of numeral 6, from 500 samples, 20 (0.4 %) samples were misrecognized by numeral 9.

From Fig. 10 b, it can be conducted two important MLPs ensemble X  X  major confusions are related to digits 6 and 4. From 500 samples of number six (6), 50 (1 %) samples were misrecognized by numeral 2 and from 500 samples of number four (4), 36 (0.7 %) samples were misrecognized to numeral 2. Two important major confusions in RBF ensemble are related to digits 2 and 6, and two important major confusions in ANFIS ensemble are related to digits 4 and 3. The supe-riority of proposed method can be concluded from Fig. 10 . One of the main reasons of misclassified samples for ensem-ble of SVD classifiers is that the obtained subspaces by SVD classifier for the similar digits would intersect; so, it can lead us to bad generalization; for instance, the numerals such as 7 and 8 that are not similar to any other digits were classified perfectly by proposed method.

To compare the recognition rates of the proposed method, the performance of the most of the previous studies on Farsi numeral recognition have been given in Table 9 . It should be mentioned that some methods in Table 9 were executed on different datasets and the exact comparison of the pre-sented method with others is not possible, because some of them were created by authors and were not available on the Internet.

As stated before, we did not have access to other datasets to measure the difficulty level of each dataset. As shown in Table 2 , our created dataset was trained and tested with one of the efficient and well-known methods, that is, the SVM. The results in Table 2 insure us about the difficulty level of our selected data. Comparing our method to Ebrahimpour et al. [ 10 ] and Abdi and Salimi [ 20 ] which were performed on the hard sample of HODA dataset, our system only used 1000 samples as training set and it was tested on 5000 samples while the two stated methods used 6000 samples as training set and their methods were tested on 2000 samples. The best and average of the recognition rates of the proposed method were 97.30 and 97.02 % respectively, while the best recog-nition rates of Ebrahimpour et al. [4] and Abdi and Salami [ 20 ] were 95.30 and 97.10 % respectively. We can say, to the best of our knowledge, this method is at least one of the best methods proposed until now for the recognition of Farsi/Arabic handwritten digits which is insensible to the size of the training set. 7 Conclusion In this paper, a new approach for the recognition of Farsi/Arabic handwritten digit is presented which is insensi-ble to the size of training set. Experimental results prove our claim. From 6000 selected hard samples of the Farsi/Arabic handwritten digit dataset, the proposed method was trained with 1000 samples and the rest of the samples were used for the test. In our novel system, the characteristics of singular value decomposition (SVD) algorithm are used as classifiers. An example of these characteristics is that it can be con-sidered as a fast and accurate technique on the recognition of sparse matrices such as handwritten digits in comparison with well-known classifiers such as MLP, RBF, ANFIS, and SVM. As stated in the literature, ensemble of classifiers may improve the classification task, so the proposed method is based on an ensemble of SVD classifiers. Although SVD has significant features, it is unable to make diversity by select-ing different number of bases. To make diversity among SVD classifiers, hybrid method from combining holistic method and component-based method was applied. The combination rule which is used by the ensemble is based on particle swarm optimization. Since the number of the training set is small, our system obtains 100 % recognition rate on the training set but it may achieve low accuracy on the test set which is because of PSO is being trapped in local minimum. To tackle this problem, we have introduced a novel scheme called reli-able multi-phase PSO (RMPPSO) that can prevent the PSO from being trapped in local minima.

In this paper, the proposed scheme is tested using var-ious classifiers, and experimental results show our novel scheme outperforms others. It is also compared with other methods which were performed on Farsi/Arabic handwrit-ten digit. Since standard datasets for Farsi handwritten digit are rare, some studies which have been compared with the proposed method were executed on own datasets which have been gathered by their authors. Therefore, the exact com-parison of the presented method with others is not possible, but the difficulty level of our dataset was evaluated by SVM. Comparing our method with two other methods which have been performed approximately on the same created dataset from HODA dataset, experimental results show; although the proposed method was trained with 1000 samples and tested by 5000 samples, it is better than the two other mentioned studies which were trained by 6000 samples and tested by 2000 samples. This significant difference proves our claim and shows that our novel system is insensible to the size of the training set.
 References
