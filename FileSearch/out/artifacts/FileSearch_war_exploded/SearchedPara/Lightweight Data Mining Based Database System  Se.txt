 Optimizing the DBMS with manual work gets more and more difficult due to the following facts. Firstly, it is time consuming to analyze a lot of performance data before tuning the DBMS. Secondly, it is difficult for DBA to continuously monitors and analyzes fluctuating workload and to react to it properly. In addition, embedded DBMS usually has on DBA at all. Lastly, there is not enough number of experienced database management experts, and the cost for the manpower is prohibitive. Database research community is trying to build database systems [1] with the ability of self-managing and self-optimization. DBMS could perform the work of parameters setting, optimization, system healing, and protecting the system against malicious attacks by itself, without much human intervention. 
We present a lightweight data mining based database system self-optimization scheme in this paper. The lock table is used as the case for study. The training module feeds historical lock table performance data to a neural network to get it trained. The neural network learns from the data through a lightweight data mining process. After trained, the neural network is intelligent enough to predict lock table and system performance according to lock table parameters. A rule engine makes decisions on which parameter to be adjusted during system running, and gets quantitative hints from the neural network predictor to get the job done. Database self-optimization techniques can be categorized into two classes: Global Tuning &amp; Optimization, and Local Tuning &amp; Optimization [2]. 
Local Tuning &amp; Optimization tries to solve specific database tuning problems such programming level setting and so on [1]. On the other hand, Global Tuning &amp; Optimization focuses on the construction of a brand-new database system with the ability of self-optimization. The DBMS can automatically maintain a subtle equilibrium among the resources used by individual system components to achieve the best overall performance. Paper [3] an d paper [4] propose database systems with self-tuning ability. Traditionally successful DBMS systems enjoy a large number of customers and possess of a large code base, radical modifications to the code base is not a good idea. As mentioned in [4], these systems need to cope with the complexity of integrating self-tuning functions into the already existed system code base. No practical systems using this approach exist until now except a prototype is presented in [5]. Other than rewriting the code base of the DBMS, another approach is to view the database system as a black box. The tuning component is decoupled from target database system code base. During DBMS running, the tuning module continuously monitors the performance of the system [6], and tunes some critical parameters that tuning scheme for index selection is proposed, the experiment result shows its feasibility. In [9] Benoit used a decision tree to make tuning decisions, several iterations of parameter adjusting are needed. Our work differs from previous work in that a neural network based predictor is used to avoid several iterations of tuning. 3.1 Lock Table Performance Behavior Granularity: DBMS commonly provides various lock granularities for various concurrencies. Lock granularity directly affects system concurrency. When lots of avoid locking conflicts and increase system concurrency. When memory is in short, the system will elevate some tuple-level locks to a table-level lock, to make room for new lock requests. Table level locks, which are large in granularity, often lead to decreased concurrency and poor system throughput. 2) Lock Table Size and Memory Consumption: In real life when memory budget is limited, Arbitrarily extending lock during data accesses. 3) Solving the Deadlock Problem: DBMS periodically detects deadlocks, if the time interval is too long, maybe some deadlocks cannot be handled timely, transaction response time will get postponed. If the time interval is too short, DBMS maybe do some futile work in deadlock detection, system process power is wasted and performance get decreased. 4) Locking Time Out Parameter: We can set a parameter, namely locking time out for SQL execution. When the transaction cannot get the necessary locks before time out, the transaction is aborted. With some transactions aborted, other blocked transactions can move on, system throughput is ensured. Above-mentioned parameters should be adjusted according to workload fluctuations, data access conflicts, and resource consumption status. 3.2 ABLE Toolkit and Its Application The self-optimization scheme for lock table is implemented using the Java language, the ABLE toolkit (Agent Building &amp; Learning Environment)[10] is used as the underlying framework and running platform. 
We develop a rule engine to solidify expert knowledge on lock table tuning, the rules come from DBA X  X  experience and the database-tuning guide, and are expressed using ABLE rule language [10]. According to current performance data, some parameter may be chose to be adjusted accord ing to the rules. A neural network based predictor is also implemented to help determine the amount of parameter adjustment. 3.3 Self-Optimization System Implementation  X  1  X  System Architecture. Figure 1 shows the architecture the self-optimization system. 1) Performance Data Collecting Module is responsible for gathering lock neural network; during running, the data is handed over to the rule engine for further processing. 2) Training Module uses the performance data to train the neural network to make it capable of predicting. 3) Rule Engine uses rules in the rule set to make decisions on which parameter to be adjusted. 4) The function of the Neural Network based Predictor is to predict lock table performance with provided lock table parameters. To make the neural network capable of the job, it is trained beforehand as mentioned before.  X  2  X  Performance Metrics for the Lock Table. Two performance metrics can be used to indicate the performance of the lock table, namely Deal Lock Rate , and Average Lock Wait Time . We define the Deal Lock Rate to be the number of deadlocks that occur for every 10,000 transactions. The Average Lock Wait Time is calculated from the amount of time spent waiting for locks and the number of locks granted. The two performance metrics can be obtained directly from DBMS management API or calculated from the data obtained from the API. The two metrics maybe conflict with each other, one of them should be discarded.  X  3  X  Prepare Training Data Set. A serial of experiments are conducted to find out the relationship between Deadlock Check Time and lock table performance. The experiment result is shown in figure 2. Figure 2 shows the Deadlock Rate s and Average Lock Wait Times corresponding to various Deadlock Check Time intervals. The X-arises of the figure represents Deadlock Check Time intervals, ranges from 10000ms to 1000ms. The Y-axis on the left side represents Deadlock Rate , and the Y-axis on the right side represents system throughput ( Transactions per Minutes ) and Average Lock Wait Times . From figure 2, we can see that the relationship between Deadlock Check Time and Deadlock Rate is not a monotone relationship. It is hard to find out a commonly acceptable deadlock rate for vari ous OLTP workloads. We choose Average Lock Wait Time to be the major lock table performan ce indicator. When deadlock check time interval is below 3000ms, the curve of Average Lock Wait Time and the curve of system throughput flatten out. For OLTP workloads, we choose 1000ms as a threshold for Average Lock Wait Time . When Average Lock Wait Time is greater than 1000ms, it is necessary to adjust lock table parameters to achieve better performance. 
The relationships between other lock table parameters and lock table performance are also determined through experiments. The parameters include Lock Table Size, SQL Lock Time Out, and Max Locks. When experimenting on specific parameter concerned, other parameters are set to default values, and the concerned parameter is varied by small steps from the low bound of its domain to the high bound, lock table performance and system throughput are measured and recorded. We do not try to experiment with every possible parameter combinations, trained predictor should tell what performance the system will get when using specific parameter combination.  X  4  X  Training the Neural Network --Lightweight Data Mining. After enough performance data is collected, it is used to train the neural network. The data scheme for training is &lt; Lock Table Size, Deadlock Check Ti me, Lock Time Out, Max Locks, Average Lock Wait Time&gt; , the preceding 4 parameters are input values, and the fifth parameters is an output value. 7000 training samples and 3000 testing samples are prepared for training and testing respectively. 
The predictor is a back propagation neural network. We use a 3 layer neural network to ensure the precision of predicting as well as low cost of running, the numbers of neurons in input layer, hidden layer and output layer of the network are 4, becomes more intelligent to predict lock table perfor mance according to provided 5%, the network predicts correctly.  X  5  X  Tuning the Lock Table. During DBMS running, performance data collected is handed over to the rule engine to decide whether any parameter should be adjusted. The rule engine provides the predictor with various parameter values increased by small steps, the parameter setting for the best predicted-performance is chose for actual parameter adjustment. The rule engine evaluates three rules in sequence. 1) When tuple level locks are elevated to table level lock, it indicates that the lock table Lock Wait Time is greater than the preset threshold (i.e. 1000 ms), the rule engine try to decrease deadlock detection interval or SQL lock wait time to reduce lock wait time. The two parameters can both be used to cut down the Average Lock Wait Time , the policy is that adjusting Deadlock Check Time twice for every adjustment of Lock Time Out . 3) When the lock table usage percentage is below the preset threshold, the rule engine tries to decrease the size of lock table to release memory for other using, e.g. database page buffer. The experiment is conducted on a HP Proliant DL 380 G4 with two Intel Xeon 3600MHz CPUs, and 5GB RAM. The operating system is Windows 2003. We use IBM DB2 version 7.2 as the experimenting DBMS. 10 warehouses X  of data is populated into the database and the TPC-C workload is run on the DBMS. The experiment result is shown in figure 3, the system throughput ( Transaction per Minute ) increase by 15.85 percent, from 3250 to 3765, a considerable increase. From inferior to IBM Configuration Wizard. Besides the lock table, IBM Configuration Wizard tunes other critical performance parameters, thus it gets more performance improvement. The result encourages us to extend the scheme to support other DBMS sub systems optimization to achieve more overall performance improvement. A data mining based database system self-optimization scheme for lock table is presented. A neural network is trained with historical performance data of the lock table is used as a predictor. The self-optim ization process is driven by a rule engine, which chooses proper parameters to be adjusted during runtime, and gets quantitative hints from the neural network predictor to accomplish parameter adjusting. With the help of predictor, the self-optimization system avoids several rounds of tuning and evaluating. Experiment result demonstrates the feasibility of the scheme. 
