 1. Introduction tasks performed along several enactments of a transactional system enactments, or to shed light on its actual behavior.

Traditional process mining approaches focus on capturing the arrows between tasks. 2 While this kind of approach naturally hardly be effective in real-life processes that tend to be less structured and more rather useless in practice.

To deal with the inherent fl exibility of real-life processes, recent process mining research [2 requisite for clustering algorithms.
 (2) Predictive Models. A tacit assumption in the approaches to clustering log traces is that the two contexts.
 application scenarios in Section 6 . 2. Related work 2.1. Clustering in process mining applications
Moving from the observation that classical process mining techniques often yield inaccurate model different execution scenarios of the process itself [2 the time of possibly concurrent activities.
 representation, named  X  pro fi le  X  , possibly in a combined way. As speci representation is obtained; and (ii) the transition pro fi accommodate more powerful structural features, such as sets of activities, higher order k-grams (with k the log iteratively via the k-means algorithm. 2.2. Outlier-detection into three categories: model-based, NN-based, and clustering-based.
 some given kind of parametric or non-parametric distribution model is built that conform enough with the model are pointed at as outliers.
 with its neighbors, according to either plain distances measures or to density measures. As an example of the (i.e., data instances falling within a given radius r )  X  th nearest neighbor.
 to outliers.
 been proposed for symbolic sequences: Kernel-Based, Window-Based, and Markovian techniques. In the similarity measure (e.g., edit distance, longest common subsequence) is de on this subject matter.
 anomalies. In fact, most of these earlier approaches simply attempt to make the discovered control-undesirable behaviors. The solution proposed in [30] consists in
L by taking account for the structural changes required to make M trace is an outlier). A major drawback of this method is however that it requires to perform work candidate trace, thereby leading to prohibitive computational costs. 2.3. Supervised classi fi cation via decision trees
Decision Trees are popular logic-based formalisms for representing classi instances space based on the values of one or more attributes tests.
 number of nodes or the average depth of the tree. However, respectively).

The choice of limiting the expansion of the tree is connected with the risk of over [11] , whereas a cost-complexity mechanism is exploited by CART [33] . 3. Outlier detection in process mining applications 3.1. Limitations of existing methods and overview of the approach  X  absolute  X  terms, since outliers show up as individuals whose behavior or characteristics towards process mining applications, some novel challenges come into play: trace, without any a-priori knowledge about the model for the given process. all possible execution paths for the process (but, the anomalies as well), the idea is of capturing the process by simpler (partial) models consisting of frequent structural patterns . Outliers are then identi  X  fi specialize earlier frequent pattern mining approaches to the context of process logs, by (i) de characterizing concurrent processes, and by (ii) presenting an algorithm for their identi  X  than the average.
 patterns introduced in [2] ) completely disregards the concurrent semantics of process logs
Section 3.3 . 3.2. Formal framework for outlier detection execution of the various tasks. Abstracting from the speci 3.2.1. Behavioral patterns over process logs
The fi rst step for implementing outlier detection is to characterize the on the identi fi cation of those features that emerge as complex patterns of executions.
De fi nition 1. S-pattern
A structural pattern (short: S -pattern) over a given set T of tasks is a graph p = either: (i) E p ={ n } X ({ n 1 , ... n k })  X  in this case, p is called a FORK -pattern (ii) E p =({ n 1 , ... n k }) X { n }  X  in this case, p is called a JOIN -pattern.
Moreover, the size of p , denoted by size(p) , is the cardinality of E causal precedence between two tasks. This is, for instance, the case of patterns p account for fork and join constructs, specifying parallel execution (cf. p processes. The crucial question is now to formalize the way in which patterns emerge for process logs.
De fi nition 2. Pattern support of t over T p is a topological sorting of p , i.e., there are not two positions i , j inside t such that i of p w.r.t. t is de fi ned as:
This measure is naturally extended to any trace bag L and pattern set P as follows: supp ( p , L )= 1 ( P , t )= 1 j P j  X   X  p  X  P supp p ; t  X  X  .  X  schemes was con fi rmed by some tests on synthesized logs.
 comply with p 1 . For the remaining traces, the application of the support function de
Thus, given the frequencies in Fig. 1 , the support of p 1 gets full support (i.e 1) by s 1 , ... , s 5 and a support of 0.368 by s
While at a fi rst sight the above notions may appear similar to classical de in order to fi nd all patterns with support greater than a given threshold simply reuse classical level-wise approaches (like the popular Apriori algorithm), which ef process behavior. Instead, the relevance of a pattern is captured in the following de
De fi nition 3. Interesting patterns
Let L be a log,  X  ,  X  be two real numbers. Given two S -patterns p and p subgraph of p  X  and supp ( p , L )  X  supp ( p  X  , L )  X   X  on L and (b) there is no other S -pattern p  X  s.t. size ( p
In other words, we are not interested in a frequent pattern p if it its frequency is not signi other pattern p  X  that includes it; conversely, if p is much more frequent than p
Anyway, the above notion of maximality is expected to suf
Example 2. Let us consider the patterns p 5 and p 1 in Fig. 1 , of them are frequent), the former is still maximal as ( supp ( p g than the one expressed by its super-pattern p 5 . Conversely, no subgraph of p patterns lower than that of p 2 .  X  3.2.2. Clusters-based outliers those individuals that are not associated with any pattern cluster or that belong to clusters whose size is de formalized below.

De fi nition 4. Coclusters and outliers
An  X  -coclustering for a log L and a set P of S -patterns is a tuple C =  X 
P ={ p 1 , ... , p k } is a set of non-empty P 's subsets (named pattern clusters ) s.t.  X 
L ={ l 1 , ... , l h } is a set of non-empty disjoint L 's subsets (named trace clusters ) such that  X 
M : P  X  L is an bijective function that associates each pattern cluster p p
Given  X  ,  X   X  [0..1], a trace t  X  L is an (  X  ,  X  )-outlier w.r.t. an j l j &lt;  X   X  1 mutual correlations, which represent different behavioral classes. More speci condition (a) deems as outlier any trace that is not assigned to any cluster (according to the minimum support (b) estimates as outliers all the traces falling into small clusters (smaller than a fraction one may notice that the traces corresponding to s 1 , ... both patterns p 1 and p 3 . Moreover, s 10 highly supports both p
Finally, sequence s 16 is associated with all of the patterns in Fig. 1 but p correlations between these patterns and log traces, one should hence be able to identify the sequences s 1 , ... , s 5 ; one for s 6 , ... , s 9 , one for s modeling the main behavioral classes of the process. This can be accomplished by setting the threshold approaches).  X  3.3.
 OASC : an algorithm for detecting outliers in a process log the framework described so far.

The algorithm, shown in Fig. 2 , takes in input a log L , a natural number pattSize and four real thresholds fi rst uses the function FindPatterns to compute a set P of ( more than pattSize arcs. Then, an  X  -coclustering for L and P is extracted with the function Clearly enough, the main computation efforts hinge on the functions thoroughly discussed next. 3.3.1. Function FindPatterns
The main task in the discovery of (  X  ,  X  )-maximal S -patterns is the mining of support supp is not anti-monotonic w.r.t. pattern containment. To face this problem, notion of support (denoted supp  X  ) which optimistically decreases the counting of spurious tasks by a size of the pattern at hand: the lower the size the higher the bonus. More precisely, within De we replace the term |{ t [ k ]  X  T p | i b k b j }| with min {|{ t [ k ] and j .

Notice that function supp  X  is both anti-monotonic and  X  safe set L k , for increasing values of the pattern size k (Steps P4 upper bound given as input. In more detail, for each k N 1, we patterns in L k  X  1 and L k , we can single out all (  X  , outcome of FindPatterns . In fact, in Step P7 the exact function supp is actually used for checking ( 3.3.2. Function FindCoClusters log L , a set P of S -patterns and a threshold  X  , FindCoClusters a set of pattern (resp., trace) clusters, while M is a mapping from P to L . similarity between two patterns p i and p j in P provides a sort of estimation for the likelihood that p ( p , t ) measures the correlation between the pattern p and the trace t ), and by threshold  X  . Clearly, different classical clustering algorithms could be used to extract P large datasets and selects autonomously the number of clusters.

In the second phase (Steps C3  X  C13), the preliminary clustering P simultaneously clustering the traces of L :new,  X  high order relate tothesame traces. Moreprecisely,eachtrace t in the loginduces a pattern cluster p in P mcl thatarecorrelatedenoughto t ,stillbasedonthefunction supp andthethreshold
P , for it was induced by some other traces; in this case we retrieve, by using the mapping M , the cluster l 3.3.3. Complexity issues
Assume that a log of N traces over T tasks is given as input. Let P of the loop in function FindPatterns , and let P max be the number of patterns used for the clustering in the complexity of OASC is essentially given by the expression O ( N  X ( T + P size of patterns (i.e., pattSize = S ) and K is a parameter of algorithm MCL [10] (cf. Step C2) routing rules and unfrequent patterns are pruned via the support threshold memory. Indeed, the log can just be scanned S times ( S b huge datasets. 4. Discovery of context-based predictive models 4.1. Formal framework for the induction of predictive models the following. In particular, case attributes can be associated with the starting (or notation, for any attribute a and its corresponding task  X  its association with  X  compactly and intuitively. Each attribute a
At run-time, the enactment of the process will cause the execution of a sequence of tasks, where for each task set of all its activities will be mapped to some values taken from the respective domains.
De fi nition 5. Data-aware logs
Let T be a set of tasks and let A be a set of process attributes. A data-aware log over T and A is a tuple
T , and where data is a function mapping each trace t  X  L to a set of pairs data ( t )={( a i  X  {1, ... , q }, and { a 1 , ... , a q }={ a  X  A | task ( a )= t [ j ],for some task t [ j ] predict membership into the clusters for forthcoming enactments.

De fi nition 6. DADT model decision tree (shortly, DADT ) for L * is a triple D = b H ,  X   X  attr is a function mapping each non-leaf node v in N to an attribute in A ;  X   X  pred : N  X  L *  X  R is a function expressing the probability of any cluster in L additional technical de fi nitions fi rst. We say that a trace t is active in a node v all the split tests de fi ned in the path from the root of H to v . For a threshold node v  X  N if |{ t  X  l | t is active in v }|/| l | N  X   X  . The restriction of L
Moreover, given two tasks  X  and  X   X  , we say that  X  X   X  -precedes before  X   X  , and less than a fraction  X   X  of l 's traces contain
De fi nition 7. DADT temporal compliance
Let D = b H , attr , split , pred N be a DADT for the data-aware log L compliant w.r.t. L * if for each pair of nodes v and v  X  of H such that v (  X   X  , v ), either: (a) task ( attr ( v ))  X   X   X  l task ( attr ( v  X  )) does not hold, or (b) there is an ancestor v  X  X  of v  X  s.t. task ( attr ( v with an attribute of a task that is usually executed after t (w.r.t. the behavioral clusters in L relaxed by the condition (b), which allows to reuse the attributes of a task associated with v in v  X  X  . These constraints guarantee that  X   X  -compliant DADT s are suitable models to support on-the-split ) are all indicated informally via intuitive node/edge annotations. Assuming model in Fig. 3 is  X   X  -compliant, whereas the other is not. This latter fact can be veri
De fi nition 7 do not hold on the root of the tree and its left child. 4.2. LearnDADT : an algorithm for inducing a DADT model
DADT tree of poor accuracy. Consider, as an example, the extreme case where an attribute of the (which precede e in all log traces) cannot appear in any descendants of the root. their capability to discriminate the clusters yet supporting on-the-a preliminary DADT that just consists of one node (named r in the named growDT , which will be discussed in detail later. Once such a (possibly large and over new process instances. The pruned DADT model is returned as the ultimate outcome of the algorithm. 4.2.1. Procedure GrowDT
Let us now provide more details on the recursive procedure (locally) optimal way of partitioning these instances (Steps combination (with weight  X   X  [0..1]) of two components:  X  the reduction of information entropy that descends from splitting S based on the values of a .  X  associated with tasks that occur earlier in the traces corresponding to clusters correlated with v signi
More precisely, denoting by L  X   X  the set of L 's clusters that are signi threshold  X   X  (cf. Line B 2), the latter score is computed as follows: follow task(a) under the ordering relationship  X   X   X  l , i.e., succ ( task ( a ), l )={ t
We pinpoint that when making score coincides with the Gain Ratio measure (i.e., when ep score.

Once a (locally) optimal attribute a * has been chosen for splitting the traces in S , the verify that the constraints in De fi nition 7 are satis fi and L  X   X  will return false iff (i) there is an ancestor v aim, one could think of resorting to some kind of work fl techniques, such as those presented in [1] ). Since the compliance test is done only when
LearnDADT is made to coincide with that of traditional decision tree learning algorithms in the case where selected split formula  X  * and the associated attribute a *, by suitably updating the functions split and
The decision tree is then expanded by adding as many children of v as the groups S partition formula  X  *to S (lines B 7-B 8).

Then, the procedure growDT is recursively applied to each new node v probability of any cluster l i conditioned on node v is estimated as the relative frequency of l 4.2.2. Complexity issues
The computation cost of algorithm LearnDADT is O ( N  X ( H  X  F + T growDT , and N , T and F are the numbers of traces, tasks and attributes, respectively, in the input log
Thus, LearnDADT often takes linear time in N . Nonetheless, in order to deal ef
RainForest ) can naturally combine with our C4.5-like scheme. 5. Putting it all together: a toy application example
Payment), l (Validate Payment), m (Update Reserves), n (Send Noti and closed (task h , Archive claim).
 while PolicyType and Status are nominals taking values from { 5.1. Discovery of behavioral clusters and outliers
Let us fi rst examine the behavior of algorithm OASC against the example log of Fig. 1 , with
The algorithm discovers 4 different structural clusters: one with the traces t
Fig. 1 ), one with the traces t 11 , ... , t 18 (corresponding to s and one with the traces t 29 , ... , t 36 (corresponding to s example log, taking advantage of their respective implementations provided in ProM [14] and several similarity/distance measures for the latter. However, in none of these trials we were able to dendrogram).
 available in the process mining framework ProM [14] , in order to associate a work the associated traces. Fig. 7 shows the resulting work fl process itself, which mainly differ in the kind of policy check performed improves the precision of classical process mining approaches, by preventing the risk of having a single work additional spurious task links (due to the presence of outlier traces t 37 , (anomalous) log traces. Moreover, it does not capture the fact that task n (Send Noti complete check of the claim was accomplished, by way of task c (Check all) 5.2. Discovery of predictive models Let us now apply algorithm LearnDADT to the clusters found by algorithm
CustomerID (which is indeed useless for learning general behavior). Moreover, we considering two different values for  X  , namely  X  =0.35 and fact, differently from the tree that is inferred in this basic case, the topology of the model in Fig. 3 model (w.r.t. the input log)  X  which is maximal for both trees in Figs. 3 and 4 . instances. For example, one can exploit the tree in Fig. 3 to forecast that the uncompleted trace [ (PolicyType,premium)} N ] will fall in Cluster 0 . Conversely, the trace [ given process in term of non-structural aspects of the process itself. 6. Experiments proposal.
  X  on-the-fl y  X  prediction over uncompleted process instance is evaluated in Section 6.4 , while Section 6.5 study the impact of input parameters. 6.1. Evaluation setting
In the evaluation of experimental results we focused on the (i) quality of discovered work illustrated next. 6.1.1. Quality of structural models
The conformance of a work fl ow model W w.r.t. a log L can be measured via two complementary metrics (de work fl ow schemas and do not apply directly to log clusters. Thus, each cluster identi schema, by using the HeuristicMiner plugin [13] provided by the process mining framework ProM [14] actually orthogonal to our approach, mainly descends from the fact that this plugin is robust to noise and ef 6.1.2. Prediction quality is also computed on incomplete log traces, in order to assess the capability of DADT models to carry out sequence of split tests that lead from the root to l . More formally:
De fi nition 8. DADT conformance
Let L bealogovertaskset T andattributeset A ,and D = b H, be the attributes associated with all non-leaf nodes n 1  X  ... path (  X  )= p 1  X  , ... , p k  X  be the sequence of tasks corresponding to a denoted by Conf ( D , L )isde fi ned as follows: and path (  X  ), respectively. 7 Moreover, for any DADT model D =  X  and gives a rough estimate of its ability to make accurate predictions over an ongoing process instance.
Example 5. Consider the decision trees in Figs. 3 and 4 , and the example log in Fig. 6 . Let tree of Fig. 4 and of Fig. 3 , respectively. Let us also denote by t sequence abd fl enmgh . Note that t 1 is assigned to  X  1 a path (  X  1 a )=ad (resp., path (  X  1 b )=da). Therefore, it holds that mismatches ( t the same happens for the whole log, given that in all paths in Fig. 4 leading to leaves task are parallel activities, the log L is likely to contain both some trace t 6.2. Datasets next. 6.2.1. Data from a logistic system (logs A and B) some suitable yard slot for being stocked. Symmetrically, at boarding time, the container is  X  storage space used for containers, and are organized into disjoint sectors . or perishable goods). These data were encoded as attributes of the starting task. 6.2.2. Data from a collaboration process (log CAD)
The second application scenario, studied in the research project TOCAI.it, of events can be traced for each project: Creation , Construction Modify (the project was saved and a new version of it started off), undone), Prototyping (a prototype was built for an item), revision was done for an item), Share (the project was shared with other workers), PilotSeries (a pilot series was produced).

In particular, we focused on the operations performed in the she was playing in the design process ( Role ). 6.3. Experimental results: quality of discovered models particular, Table 1 reports the number of clusters found by
Table 2 at the outcomes of experiments carried out without the removal of outliers (i.e., we set the other thresholds). In particular, we did not fi nd clusters whose size is de
CAD ; yet, we found various outliers over these two logs, whose removal was bene traces.
 traces proposed in [3] (precisely, a balanced combination of the methods on each log, we used their respective implementations provided in ProM [14] , and con work fl ow model of every cluster.
 Table 3 shows instead some features of the DADT models obtained via differ for the value of parameter  X  (while keeping fi xed two different options for setting the parameter  X  : 1.  X  =1, which practically makes our approach coincide with algorithm J48 structural models are completely ignored when inducing the decision tree model 2. classi fi cation accuracy and structural conformance. However, similar results were obtained for 0.3 scenarios, and that precision does not come with a verbose (and possibly over tree induction algorithms, such as C4.5 and its variant J48 [39,11] . In this respect, we attributes, with just one of task attribute (namely the distance covered in the consequence, even when task precedences are ignored in the induction of the classi of log traces is improved when using the precedence-based heuristic in the selection of split attributes ( 6.4. Experimental results: runtime-prediction power  X  instance should be estimated possibly before it has been completed.

In order to conduct the analysis, we measured the accuracy of the classi
Two plots are shown for each log: one for the decision tree discovered by using the algorithm in Fig. 5 with decision tree found with J48  X  this practically corresponds to set results than the classical induction method.
 6.5. Further effectiveness and sensitivity analysis
In this section, we discuss the results of further experimental activities conducted to assess the ef
LearnDADT . 6.5.1. Studying the ef fi cacy and sensitivity of OASC on synthesized data 6.5.1.1. Additional quality metrics. In order to evaluate the ability of and balanced F-measure (i.e. F 1 =(2 X  P  X  R )/( P + R ))) outliers. implemented which produces a trace log according to four main data distribution parameters: N speci fi cally, the log will contain N T traces over an alphabet of N falling into clusters whose size is smaller than the average. Additional p with any cluster at all  X  hence, the total percentage of outliers in the dataset is p a gaussian distribution with mean S P . All these schemas are then combined into a single one W sub-schema is allowed to be run independently of the others. Then, N to p C out )in W P , thereby generating the various clusters of traces over a total of (1 generated by simulating enactments that do not comply with W 6.5.1.3. Experimental results: precision against data distributions. In a different percentages of outliers, by varying both p out (from 0.02 to 0.32) and p N =16000, N C =4, and S P =6. Fig. 10 illustrates the results obtained against these data by applying algorithm  X  outliers, yet producing a higher number of false positives (specially for higher values of p adequately.
 are not recognized at all, thus leading to quite higher FN rates. input parameters. To this end, we generated a log by setting p data parameters. In the following we discuss results of experiments performed with and  X   X  which appeared to require more care in fi nding an appropriate setting. and  X  . The fi rst two parameters considerably impact the purity of the clusters. Probably, extreme values of with outliers, or viceversa. Satisfactory results are obtained with  X  =4 seems to produce additional bene fi ts, mainly as concerns the stability of results.
Fig. 12 sheds light on the ability to discriminate outliers from normal traces, for different con expected, recall scores tend to improve when increasing either sized outlier one). As to F-measure, a good trade-off seems to be found for 6.5.2. Sensitivity tests on LearnDADT
In order to better comprehend the behavior of algorithm LearnDADT prediction accuracy  X  w.r.t. the clusters previously discovered by and consider the results of tests performed with minCard =0 to prevent over fi tting.

The impact of  X  on the metrics is illustrated in Fig. 13 . As expected, in correspondence of higher values of conformance measure is always maximal, independently of LearnDADT that characterize this case. In the other cases, it seems that a good trade-off is achieved around sensible accuracy loss. Fig. 14 lets us conclude that a similar effect is produced by between accuracy and conformance happen for  X   X   X  [0.1..0.15].
 substantially similar to those obtained on real logs, we omit their description here. 7. Discussion and conclusions which are preliminary discovered as an evidence of  X  normal appears to the root. Encouraging results on two complex real-life application scenarios con accurately. 7.1. Comparison with related works manner. In fact, in other tra ce clustering approaches [2 scenarios.
 on proximity notions or on generative models (e.g., [27  X  (e.g., intra-parallelism).
 tree, by integrating into it the constraints needed to support on-the-discovered models, the availability of noise-resistant and ef prediction on the structure of a process instance. 7.2. Future work metrics), as well as to reuse available background knowledge on well-speci based on our empirical sensitivity analysis, we notice that certain key parameters (e.g., according to quasi-monotonic or quasi-convex curves. This behavior leaves space to the design of ef for a semi-automatic setting of the parameters.
 real process management platforms.

References
