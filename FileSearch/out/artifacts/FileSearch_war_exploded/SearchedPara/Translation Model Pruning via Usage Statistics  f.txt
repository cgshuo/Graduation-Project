 A relatively new device for translation systems are small portable devices like cell phones, PDAs and handheld game consoles. The idea here is to have a lightweight and convenient translation device e.g. for tourists that can be easily carried. Other appl i-cations include medical, relief, and military scena r-ios. 
Preferably such a device will offer speech-to-speech translation for both (or multiple) translati on directions. These devices have been researched and are starting to become commercially available (e.g. Isotani et al., 2003). The main challenges here are the severe restrictions regarding both memory and computing power on such a small portable device. 1.1 Statistical Machine Translation Generally statistical machine translation systems have recently outperformed other translation ap-proaches so it seems natural to also apply them in these scenarios. 
A main component of every statistical machine translation system is the translation model. The translation model assigns translation probabilities tracted from a parallel bilingual text. These phras e pairs are applied during the decoding process and their target sides are combined to form the final translation. A variety of algorithms to extract phrase pairs has been proposed. (e.g. Och and Ney, 2000 and Vogel, 2005). 
Our proposed approach now tries to remove phrase pairs, which have little influence on the fi -nal translation performance, from a translation sys -tem ( pruning of the translation model 2 ). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation per-formance too heavily. 
The approach does not depend on the actual al-gorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-extracted before decoding. (in contrast to the proposed approaches to  X  X nline phrase extraction X  (Zhang and Vogel, 2005; Calli-son-Burch et al., 2005)). 
The task now is to remove enough pre-extracted phrase pairs in order to accommodate the possibly strict memory limitations of a portable device while restricting performance degradation as much as possible. 
We will not specifically address the computing power limitations of the portable devices in this paper. Previous work mainly introduced two natural ideas to prune phrase pairs. Both are for example di-rectly available in the Pharaoh decoder (Koehn, 2004). Probability threshold A very simple way to prune phrase pairs from a translation model is to use a probability threshold and remove all pairs for which the translation probability is below the threshold. The reasoning with a very low probability will be chosen (over another translation candidate with a higher prob-ability). Translation variety threshold Another way to prune phrase pairs is to impose a limit on the number of translation candidates for a certain phrase. That means the pruned translation model can only have equal or fewer possible trans-lations for a given source phrase than the thresh-old. This is accomplished by sorting the phrase pairs for each source phrase according to their probability and eliminating low probability ones until the threshold is reached. The approach presented here uses a different idea inspired by the Optimal Brain Damage algorithm for neural networks (Le Cun et al., 1990). 
The Optimal Brain Damage algorithm for neural networks computes a saliency for each network element. The saliency is the relevance for the per-formance of the network. In each pruning step the element with the smallest saliency is removed, and the network is re-trained and all saliencies are re -calculated etc. 
We can analogously view each phrase pair in the translation system as such a network element. The question is of course how to calculate the relevanc e for the performance for each phrase pair. 
A simple approximation was already done in the previous work using a probability or variety threshold. Here the relevance is estimated using th e phrase pair probability or the phrase pair rank as relevance indicators. 
But these are not the only factors that influence the final selection of a phrase pair and most of these factors are not established during the traini ng and phrase extraction process. Especially the fol-lowing two additional factors play a major role in the importance of a phrase pair. Frequency of the source phrase We can clearly say that a phrase pair with a very common source phrase will be much more impor-tant than a phrase pair where the source phrase oc-curs only very rarely. Actual use of the phrase-pair But even phrase-pairs with very common source phrases might not be used for the final translation hypothesis. It is for example possible that it is p art of a longer phrase pair that gets a higher probabil -ity so that the shorter phrase pair is not used. Generally there are a lot of different factors infl u-encing the estimated importance of a phrase pair and it seems hard to consider every influence sepa-rately. Hence the proposed idea does not use a combination of features to estimate the phrase pair importance. Instead the idea is to just apply the translation system to a large amount of text and se e how often a phrase pair is actually used (i.e. infl u-ences the translation performance). If the translat ed text is large enough this will give a good statisti cs of the relevance of this respective phrase pair. Th is leads to the following algorithm: Algorithm Translate a large amount of (in-domain) data with the translation system (tuned on a development set) and collect the following two statistics for each phrase pair in the translation model.  X  c( phrase pair ) = Count how often a phrase pair  X  u( phrase pair ) = Count how often a phrase pair The overall score for a phrase pair with simple smoothing (+1) is calculated as: We use the logarithm function to limit the influ-ence of the c value. The u value is more important as this measures how often a phrase was actually used in a translation hypothesis. This scoring func -tion was empirically found after experimenting with a variety of possible scoring terms. 
The phrase pairs can then be sorted according to this score and the top n phrase pairs can be selected for a smaller phrase translation model. 4.1 Experimental Setup &amp; Baseline Translation system The translation system that was used for the ex-periments is a state-of-the-art statistical machine translation system (Eck et al. 2006). The system uses a phrase extraction method described in Vogel (2005) and a 6-gram language model. Training and testing data The training data for all experiments consisted of the BTEC corpus (Takezawa et al., 2002) with 162,318 lines of parallel Japanese-English text. Al l translations were done from Japanese to English. The language model was trained on the English part of the training data. The test set from the evaluation campaign of IWSLT 2004 (Akiba et al., 2004) was used as test-ing data. This data consists of 500 lines of touris m data. 16 reference translations to English were available. Extracted phrases Phrase pairs for n-grams up to length 10 were ex-tracted (with low frequency thresholds for higher n-grams). This gave 4,684,044 phrase pairs (273,459 distinct source phrases). The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al., 2002) with a 95% confidence inter-val of [57.13, 61.09]. Baseline pruning The approaches presented in previous work served as a baseline. The probability threshold was tested for 8 values (0 (no pruning), 0.0001, 0.0005, 0.001 , 0.005, 0.01, 0.05, 0.1) while the variety threshold 50, 100, 200, 500 (no pruning in this case)) and al l combinations thereof. The final translation scores for different settings are very fluctuating. For th at reason we defined the baseline score for each pos-sible size as the best score that was reached with equal or less phrase pairs than the given size in a ny of the tested combinations. 4.2 Results for For the proposed approach  X  X runing via Usage Statistics X , the translation system was applied to the 162,318 lines of Japanese training data. 
As explained in section 3 it was now counted for each phrase pair how often it occurred in a transla -tion lattice and how often it was used for the fina l translation. The phrase pairs were then sorted ac-cording to their relevance estimation and the top n phrase pairs were chosen for different values of n . The pruned phrase table was then used to translate the IWSLT 2004 test set. Table 1 shows the results comparing the baseline scores with the results us-ing the described pruning. Figure 1 illustrates the scores. The plateaus in the baseline graph are due to the baseline definition as stated above. Table 1: BLEU scores at different levels of pruning (Baseline: Best score with equal or less phrase pairs) For more than 1 million phrase pairs the differ-ences are not very pronounced. However the trans-lation score for the proposed pruning algorithm is still not significantly lower than the 59.11 score at 2 million phrase pairs while the baseline drops slightly faster. For less than 1 million phrase pai rs the differences become much more pronounced with relative improvements of up to 58% at 200,000 phrase pairs. It is interesting to note tha t the improved pruning removes infrequent source phrases and to a lesser extent source vocabulary even for larger numbers of phrase pairs. The proposed pruning algorithm is able to outper-form a strong baseline based on previously intro-duced threshold pruning ideas. Over 50% of phrase pairs can be pruned without a significant loss of performance. Even for very low memory situations the improved pruning remains a viable option while the baseline pruning performance drops heavily. 
One idea to improve this new pruning approach is to exchange the used count with the count of the phrase occurring in the best path of the lattice ac -cording to a scoring metric. This would require having a reference translation available to be able to tell which path is the actual best one (metric-best path). It would be interesting to compare the performance if the statistics is done using the met -ric-best path on a smaller amount of data to the performance if the statistics is done using the model-best path on a larger amount (as there is no reference translation necessary). 
The Optimal Brain Damage algorithm recalcu-lates the saliency after removing each network element. It could also be beneficial to sequentiall y prune the phrase pairs and always re-calculate the statistics after removing a certain number of phras e pairs. This work was partly supported by the US DARPA under the programs GALE and TRANSTAC. 
