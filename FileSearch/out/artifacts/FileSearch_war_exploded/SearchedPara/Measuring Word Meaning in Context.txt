 University of Texas at Austin University of Cambridge University of Texas at Austin
Word sense disambiguation (WSD) is an old and important task in computational linguistics multiple paraphrases, as points in vector space, or as distributions over latent senses. New methods of evaluating and comparing these different representations are needed. context in a graded fashion. In WS sim annotation, the applicability of each dictionary sense the same lemma, again on a scale. We find that the novel annotation schemes show good inter-and give very fine-grained judgments that  X  X ix and match X  senses for each individual usage. similarity as metric.
 sense groupings to identify sense groups that may not match intuitions of untrained native by any static sense grouping. 1. Introduction
Word sense disambiguation ( WSD ) is a task that has attracted much work in computa-tional linguistics (see Agirre and Edmonds [2007] and Navigli [2009] for an overview), including a series of workshops, SENSEVAL (Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) and SemEval (Agirre, M ` arquez, and
Wicentowski 2007; Erk and Strapparava 2010), which were originally organized expressly as a forum for shared tasks in WSD .In WSD , polysemy is typically modeled through a dictionary, where the senses of a word are understood to be mutually disjoint.
The meaning of an occurrence of a word is then characterized through the best-fitting among its dictionary senses.
 aries has been drawn into doubt by lexicographers (Kilgarriff 1997; Hanks 2000), lin-guists (Tuggy 1993; Cruse 1995), and psychologists (Kintsch 2007). Hanks (2000) argues that word senses have uses where they clearly fit, and borderline uses where only a few of a sense X  X  identifying features apply. This notion matches results in psychol-ogy on human concept representation: Mental categories show  X  X uzzy boundaries, X  and category members differ in typicality and degree of membership (Rosch 1975;
Rosch and Mervis 1975; Hampton 2007). This raises the question of annotation: Is it possible to collect word meaning annotation that captures degrees to which a sense applies? that can represent different degrees of similarity to a word sense, as well as different degrees of similarity between occurrences of a word. The SemEval Lexical Substitu-tion task (McCarthy and Navigli 2009) represents each occurrence through multiple weighted paraphrases. Other approaches represent meaning in context through a vector space model (Erk and Pado 2008; Mitchell and Lapata 2008; Thater, F  X  urstenau, and
Pinkal 2010) or through a distribution over latent senses (Dinu and Lapata 2010). Again, this raises the question of annotation: Can human annotators give fine-grained judg-ments about degrees of similarity between word occurrences, like these computational models predict? through annotation in the form of graded judgments? We want to know whether an-notators can provide graded meaning annotation in a consistent fashion. Also, we want to know whether annotators will use the whole graded scale, or whether they will fall back on binary ratings of either  X  X dentical X  or  X  X ifferent. X  Our ques-tion, however, is not whether annotators can be trained to do this. Rather, our aim is to describe word meaning as language users perceive it . We want to tap into the annotators X  intuitive notions of word meaning. As a consequence, we use un-trained annotators. We view it as an important aim on its own to capture lan-guage users X  intuitions on word meaning, but it is also instrumental in answering our first question, of whether word meaning can be described through graded annotator judgments: Training annotators in depth on how to distinguish pre-defined hand-crafted senses could influence them to assign those senses in a binary fashion.
 word meaning in context. In the first task, they rate the applicability of dictionary senses on a graded scale. In the second task, they rate the similarity between pairs of usages of the same word, also on a graded scale. In designing the annotation tasks, we utilize techniques from psycholinguistic experimentation: Annotators give ratings on a 512 scale, rather than selecting a single label; we also use multiple annotators for each item, retaining all annotator judgments. 1 models of word meaning: either to evaluate graded models of word meaning, or to evaluate traditional WSD systems in a graded fashion. They can also be used to ana-lyze existing word sense inventories, in particular to identify sense distinctions worth revisiting X  X e say more on this latter use subsequently.
 notation. It is highly unlikely that ratings on a scale would ever achieve higher exact agreement than binary annotation. Our aim is also not to maximize exact agreement, as we expect to see individual differences in perceived meaning, and want to capture those differences. Still it is desirable to have an end product of the annotation that is robust against such individual differences. In order to achieve this, we average judgments over multiple annotators after first inspecting pairwise correlations between annotators to ensure that they are all doing their work diligently and with similar outcomes. mediate points on the graded scale and do not treat the task as inherently binary. We find that there is good inter-annotator agreement, measured as correlation. There is also a highly significant correlation across tasks and with traditional substitution tasks. This indicates that the annotators performed these tasks in a con-sistent fashion. It also indicates that diverse ways of representing word meaning in context X  X ingle best sense, weighted senses, multiple paraphrases, usage similarity X  yield similar characterizations. We find that annotators frequently give high scores to more than one sense, in a way that is not remedied by a more coarse-grained sense inventory. In fact, the annotations are often inconsistent with disjoint sense partitions.
Gaylord (2009). The current paper extends the previous work in three ways. 1. We add extensive new annotation to corroborate our findings from 2. We test whether the similarity ratings on pairs of usages obey the triangle 3. We examine the extent to which our graded annotation accords with two 2. Background
In this section, we offer an overview of previous word sense annotation efforts, and then discuss alternative approaches to the annotation and modeling of word meaning. 2.1 Word Sense Annotation
Inter-annotator agreement (also called inter-tagger agreement, or ITA) is one indicator of the difficulty of the task of manually assigning word senses (Krishnamurthy and
Nicholls 2000). With WordNet, the sense inventory currently most widely used in word sense annotation, ITA ranges from 67% to 78% (Landes, Leacock, and Tengi 1998;
Mihalcea, Chklovski, and Kilgarriff 2004; Snyder and Palmer 2004), depending on factors such as degree of polysemy and inter-relatedness of the senses. This issue is not specific to WordNet. Annotation efforts based on other dictionaries have achieved similar ITA levels, as shown in Table 1. The first group in that table shows two corpora in which all open-class words are annotated for word sense, in both cases using
WordNet. The second group consists of two English lexical sample corpora, in which only some target words are annotated. One of them uses WordSmyth senses for verbs and WordNet for all other parts of speech, and the other uses HECTOR, with similar
ITA, so the choice of dictionary does not seem to make much difference in this case.
Next is SALSA, a German corpus using FrameNet frames as senses, then OntoNotes, again an English lexical sample corpus. Inter-annotator agreement is listed in the last column of the table; agreement is in general relatively low for the first four corpora, which use fine-grained sense distinctions, and higher for SALSA and OntoNotes, which have more coarse-grained senses.
 (Palmer, Dang, and Fellbaum 2007). ITA is substantially improved by using coarser-grained senses, as seen in OntoNotes (Hovy et al. 2006), which uses an ITA of 90% as the criterion for constructing coarse-grained sense distinctions. Although this strategy does improve ITA, it does not eliminate the issues seen with more fine-grained annotation efforts: For some lemmas, such as leave , 90% ITA is not reached even after multiple re-partitionings of the semantic space (Chen and Palmer 2009). This suggests that the meanings of at least some words may not be separable into senses distinct enough for consistent annotation. 4 Moreover, sense granularity does not appear to be the only question influencing ITA differences between lemmas. Passonneau et al. (2010) found three main factors: sense concreteness, specificity of the context in which the target word occurs, and similarity between senses. It is worth noting that of these factors, only the third can be directly addressed by a change in the dictionary. 514 systems trained on the annotated data. Table 2 shows results for the best systems that participated at four English lexical sample tasks. With fine-grained sense inventories, the top-ranking WSD systems participating in the event achieved precision scores of 73% to 77% (Edmonds and Cotton 2001; Mihalcea, Chklovski, and Kilgarriff 2004). Current state-of-the-art systems have made modest improvements on this; for example, the system described by Zhong and Ng (2010) achieves 65.3% on the English lexical sample at
SENSEVAL -2, though the same system obtains 72.6%, just below Mihalcea, Chklovski, and Kilgarriff (2004), on the English lexical sample at SENSEVAL ture remains the same with systems getting around three out of four word occurrences correct. Under a coarse-grained approach, system performance improves considerably (Palmer, Dang, and Fellbaum 2007; Pradhan et al. 2007), with the best participating system achieving a precision close to 90%. 5 The merits of a coarser-grained approach are still a matter of debate (Stokoe 2005; Ide and Wilks 2006; Navigli, Litkowski, and Hargraves 2007; Brown 2010), however.
 portant implications for improving WSD , we do not focus on this question here. Rather, we propose novel annotation tasks that allow us to probe the relatedness between dictionary senses in a flexible fashion, and to explore word meaning in context without presupposing hard boundaries between usages. The resulting data sets can be used to compare different inventories, coarse or otherwise. In addition, we hope that they will prove useful for the evaluation of alternative representations of ambiguity in word meaning (Erk and Pado 2008; Mitchell and Lapata 2008; Reisinger and Mooney 2010; Thater, F  X  urstenau, and Pinkal 2010; Reddy et al. 2011; Van de Cruys, Poibeau, and
Korhonen 2011). 2.2 Representation of Word Meaning in Word Sense Inventories
One possible factor contributing to the difficulty of manual and automatic word sense assignment is the design of word sense inventories themselves. As we have seen, such difficulties are encountered across dictionaries, and it has been argued that there are problems with the characterization of word meanings as sets of discrete and mutually exclusive senses (Tuggy 1993; Cruse 1995; Kilgarriff 1997; Hanks 2000; Kintsch 2007). 2.2.1 Criticisms of Enumerative Approaches to Meaning. Dictionaries are practical resources and the nature of the finished product depends upon the needs of the target audience, as well as budgetary and related constraints (cf. Hanks 2000). Consequently, dictionaries differ in the words that they cover, and also in the word meanings that they distinguish.
Dictionary senses are generalizations over the meanings that a word can take, and these generalizations themselves are abstractions over collected occurrences of the word in different contexts (Kilgarriff 1992, 1997, 2006). Regardless of a dictionary X  X  granularity, the possibility exists for some amount of detail to be lost as a result of this process. a word and argues that sense distinction only makes sense with respect to a given task.
For example, in machine translation, the senses to be distinguished should be those that lead to different translations in the target language. It has since been demonstrated that this is in fact the case (Carpuat and Wu 2007a, 2007b). Hanks (2000) questions the view of senses as disjoint classes defined by necessary and sufficient conditions. He shows that even with a classic homonym like  X  X ank, X  some occurrences are more typical examples of a particular sense than others. This notion of typicality is also important in theories of concept representation in psychology (Murphy 2002). Theoretical treatments of word meaning such as the Generative Lexicon (Pustejovsky 1991) also draw attention to the subtle, yet reliable, fluctuations of meaning-in-context, and work in this paradigm also provides evidence that two senses which may appear to be quite distinct can in fact be quite difficult to distinguish in certain contexts (Copestake and Briscoe 1995, page 53). 2.2.2 Psychological Research on Lexical and Conceptual Knowledge. Not all members of a mental category are equal. Some are perceived as more typical than others (Rosch 1975;
Rosch and Mervis 1975; and many others), and even category membership itself is clearer in some cases than in others (Hampton 1979). These results are about mental concepts, however, rather than word meanings per se, which raises the question of the relation between word meanings and conceptual knowledge. Murphy (1991, 2002) argues that although not every concept is associated with a word, word meanings show many of the same phenomena as concepts in general X  X ord meaning is  X  X ade up of pieces of conceptual structure X  (Murphy 2002, page 391). A body of work in cognitive linguistics also discusses the relation between word meaning and conceptual structure (Coleman and Kay 1981; Taylor 2003).
 mental representation of word senses. Unlike homonym meanings, the senses of a polysemous word are thought to be related, suggesting that the mental representations of these senses may overlap as well. The psycholinguistic literature on this question 516 is not wholly clear-cut, but by and large does support the position that polysemous senses are not entirely discrete in the mental lexicon. Whereas Klein and Murphy (2001, 2002) do provide evidence for discreteness of mental sense representations, it appears as though these findings may be due in part to the particular senses included in their studies (Klepousniotou, Titone, and Romero 2008).
 ing differences between homonyms and polysemous words, using a variety of experi-mental designs, including eye movements and reading times (Frazier and Rayner 1990;
Pickering and Frisson 2001) as well as response times in sensicality and lexical decision tasks (Williams 1992; Klepousniotou 2002). Brown (2008, 2010) takes the question of shared vs. separate meaning representations one step further in a semantic priming study 6 in which she shows that intuitive meaning-in-context similarity judgments have a processing correlate in on-line sentence comprehension. Response time to the target is a negative linear function of its similarity in meaning to the prime, and response accuracy is a positive linear function of this similarity. In other words, the more similar in meaning a prime X  X arget pair was judged to be, the faster and more accurately sub-jects responded. This provides empirical support for a processing correlate of graded similarity-in-meaning judgments.
 at ways to model word meaning more continuously. Even though there is still some controversy, the majority of studies support the view that senses of polysemous words are linked in their mental representations. In our work we do not make an explicit distinction between homonymy and polysemy, but the data sets we have produced may be useful for a future exploration of this distinction. 2.3 Alternative Approaches to Word Meaning
Earlier we suggested that word meaning may be better described without positing disjoint senses. We now describe some alternatives to word sense inventory approaches to word meaning, most of which do not rely on disjoint senses. 2.3.1 Substitution-Based Approaches. McCarthy and Navigli (2007) explore the use of synonym or near-synonym lexical substitutions to characterize the meaning of word occurrences. In contrast to dictionary senses, substitutes are not taken to partition a word X  X  meaning into distinct senses. McCarthy and Navigli gathered their lexical substitution data using multiple annotators. Annotators were allowed to provide up to three paraphrases for each item. Data were gathered for 10 sentences per lemma for 210 lemmas, spanning verbs, nouns, adjectives, and adverbs. The annotation took the form of each occurrence being associated with a multiset of supplied paraphrases, weighted by the frequency with which each paraphrase was supplied. We make extensive use of the LEXSUB dataset in our work reported here. An example sentence with substitutes from the LEXSUB dataset (sentence 451) is given in Table 3.
 in another language. Resnik and Yarowsky (2000, page 10) suggest  X  X o restrict a word in original]. They argue that such an approach will avoid being too fine-grained, and that the distinctions that are made will be independently motivated by crosslinguistic trends. Although substitution and translation methods are not without their own issues (Kilgarriff 1992, page 48), they constitute an approach to word meaning that avoids many of the drawbacks of more traditional sense distinction and annotation. Some cross-linguistic approaches group translations into disjoint senses (Lefever and Hoste 2010), whereas others do not (Mihalcea, Sinha, and McCarthy 2010). 2.3.2 Distributional Approaches. Recently there have been a growing number of distri-butional approaches to representing word meaning in context. These models offer an opportunity to model subtle distinctions in meaning between two occurrences of a word in different contexts. In particular, they allow comparisons between two occurrences of a word without having to classify them as having the same sense or different senses.
Some of these approaches compute a distributional representation for a word across all its meanings, and then adapt this to a given sentence context (Landauer and Dumais 1997; Erk and Pado 2008; Mitchell and Lapata 2008; Thater, F  X  urstenau, and Pinkal 2010;
Van de Cruys, Poibeau, and Korhonen 2011). Others group distributional contexts into senses. This can be done on the fly for a given occurrence (Erk and Pado 2010; Reddy et al. 2011), or beforehand (Dinu and Lapata 2010; Reisinger and Mooney 2010). The latter two approaches then represent an occurrence through weights over those senses. A third group of approaches is based on language models (Deschacht and Moens 2009;
Washtell 2010; Moon and Erk 2012): They infer other words that could be used in the position of the target word. 7 3. Two Novel Annotation Tasks
In this section we introduce two novel annotation schemes that draw on methods common in psycholinguistic experiments, but uncommon in corpus annotation. Tra-ditional word sense annotation usually assumes that there is a single correct label for each markable. Annotators are trained to identify the correct labels consistently, often with highly specific a priori guidelines. Multiple annotators are often used, but despite the frequently low ITA in word sense annotation, differences between annotator responses are often treated as the result of annotator error and are not retained in the final annotation data.
 many psycholinguistic experiments, such as the ones discussed in the previous section.
Psycholinguistic experiments frequently do not make strong assumptions about how participants will respond, and in fact are designed to gather data on that very ques-tion. Participants are given general guidelines for completing the experiment but these 518 guidelines generally stop short of precise procedural detail, to avoid undue influence over participant responses. All of the psycholinguistic studies discussed earlier used participants na  X   X ve as to the purpose of the experiment, and who were minimally trained.
Responses are often graded in nature, involving ratings on an ordinal scale or in some cases even a continuously valued dimension (e.g., as in Magnitude Estimation). Mul-tiple participants respond to each stimulus, but all participant responses are typically retained, as there are often meaningful discrepancies in participant responses that are not ascribable to error. All of the psycholinguistic studies discussed previously collected data from multiple participants (up to 80 in the case of one experiment by Williams [1992]).
 imental design. We collected responses using a scale, rather than binary judgments; we designed the annotation tasks to be accomplishable without prior training and with minimal guidelines, and we used multiple annotators (up to eight) and retained all responses in an effort to capture individual differences. In the following, we describe two different annotation tasks, one with and one without the use of dictionary senses. Graded Ratings for Dictionary Senses. In our first annotation task, dubbed WSsim (for
Word Sense Similarity), annotators rated the applicability of WordNet dictionary senses, using a five-point ordinal scale. 8 Annotators rated the applicability of every single
WordNet sense for the target lemma, where a rating of 1 indicated that the sense in question did not apply at all, and a rating of 5 indicated that the sense applied completely to that occurrence of the lemma. Table 4 shows the descriptions of the five points on the scale that the annotators were given. By asking annotators to provide ratings for each individual sense, we strive to eliminate all bias toward either single-sense or multiple-sense annotation. By asking annotators to provide ratings on a scale, we allow for the fact that senses may not be perceived in a binary fashion. Graded Ratings for Usage Similarity. In our second annotation task, dubbed Usim (for
Usage Similarity), we collected annotations of word usages without recourse to dic-tionary senses, by asking annotators to judge the similarity in meaning of one usage of a lemma to other usages. Annotators were presented with pairs of contexts that share a word in common, and were asked to rate how similar in meaning they perceive those two occurrences to be. Ratings are again on a five-point ordinal scale; a rating of 1 indicated that the two occurrences of the target lemma were completely dissimilar in meaning, and a rating of 5 indicated that the two occurrences of the target lemma were identical in meaning. The descriptions of the five points on the scale, shown in Table 4, were identical to those used in the WSsim task. Annotators were able to respond  X  X  don X  X  know X  if they were unable to gauge the similarity in meaning of the two occurrences.
Annotation Procedure. All annotation for this project was conducted over the Internet in specially designed interfaces. In both tasks, all annotator responses were retained, without resolution of disagreement between annotators. We do not focus on obtaining a single  X  X orrect X  annotation, but rather view all responses as valuable sources of information, even when they diverge.
 should they desire to include a more detailed response regarding the item in question. They could use this, for example, to comment on problems understanding the sentence.
The annotators were able to revisit previous items in the task. Annotators were not able to skip forward in the task without rating the current item. If an annotator attempted to submit an incomplete annotation they were prompted to provide a complete response before proceeding. They were free to log out and resume later at any point, however, and also could access the instructions whenever they wanted.

Two Rounds of Annotation. We performed two rounds of the annotation experiments, hereafter referred to as R 1and R 2. 10 Both annotation rounds included both a WSsim and a Usim task, labeled in the subsequent discussion as WSsim-1 and Usim-1 for
WSsim-2 and Usim-2 for R 2. An important part of the data analysis is to compare the new, graded annotation to other types of annotation. We compare it to both traditional word sense annotation, with a single best sense for each occurrence, and lexical substitution, which characterizes each occurrence through paraphrases. In annotation data that had previously been labeled with either traditional single sense annotation or with lexical substitutions. R 2 included two additional annotation tasks, one involving traditional WSD methodology (WSbest) and a lexical substitution task (SYNbest). In the SYNbest task, annotators provided a single best lexical substitution, in contrast to the multiple substitutes annotators provided in the original pated in R 2. In R 1, separate groups of annotators participated in WSsim and Usim an-notation, whereas in R 2 the same group of annotators was used for all annotation, so as to allow comparison across tasks for the same annotator as well as across annotators. In 2, therefore, the same annotators did both traditional word sense annotation (WSbest) and the graded word sense annotation of the WSsim task. This raises the question of whether their experience on one task will influence their annotation choice on the other task. We tested this by varying the order in which annotators did WSsim and WSbest. 2 annotators were divided into two groups of four annotators with the order of tasks as follows: mitted the annotators to see one more sentence of context on either side of the target 520 sentence. In R 1 each item was given only one sentence as context. We added more context in order to reduce the chance that the sentence would be unclear. Table 5 summarizes up the annotation tasks and annotation rounds on which we report. Data Annotated. The data to be annotated in WSsim-1 were taken primarily from Semcor (Miller et al. 1993) and the Senseval-3 English lexical sample (
Chklovski, and Kilgarriff 2004). This experiment contained a total of 430 sentences span-ning 11 lemmas (nouns, verbs, and adjectives). For eight of these lemmas, 50 sentences were included, 25 randomly sampled from Semcor and 25 randomly sampled from The remaining three lemmas in the experiment had 10 sentences each, from the data. Each of the three annotators annotated each of the 430 items, providing a response for each WordNet sense for that lemma. Usim-1 used data from lemmas were manually selected, including the three lemmas also used in WSsim-1. We selected lemmas which exhibited a range of meanings and substitutes in the data, with as few multiword substitutes as possible. Each lemma is the target in 10
LEXSUB sentences except there were only nine sentences for the lemma bar.n because of a part-of-speech tagging error in the LEXSUB trial data. For each lemma, annotators were presented with every pairwise comparison of these 10 sentences. We refer to each such pair as an SPAIR . There were 45 SPAIR s per lemma (36 for bar.n ), adding up to 1,521 comparisons per annotator in Usim-1.
 ison of annotator responses on this subset of the two tasks yielded promising results, 2 used the same set of sentences for both Usim and WSsim so as to better compare these tasks. All data in the second round were taken from lemmas with 10 sentences for each. We produced the SYNbest annotation, rather than use the existing LEXSUB annotation, so that we could ensure the same conditions as with the other annotation tasks, that is, using the same annotators and providing the extra sentence of context on either side of the original LEXSUB required that the annotators provide one substitute. As such, there were 260 target lemma occurrences that received graded word sense applicability ratings in WSsim-2, and 1,170 SPAIR s (pairs of occurrences) to be annotated in Usim-2. 4. Analysis of the Annotation
In this section we present our analysis of the annotated data. We test inter-annotator agreement, and we test to what extent annotators make use of the added flexibility of the graded annotation. We also compare the outcome of our graded annotation to traditional word sense annotation and lexical substitutions for the same data. 4.1 Evaluation Measures
Because both graded annotation tasks, WSsim and Usim, use ratings on five-point scales rather than binary ratings, we measure agreement in terms of correlation. Because ratings were not normally distributed, we choose a non-parametric test which uses ranks rather than absolute values: We use Spearmans rank correlation coefficient (rho), following Mitchell and Lapata (2008). For assessing inter-tagger agreement on the
WSbest task we adopt the standard WSD measure of average pairwise agreement, and for R 2 SYNbest, we use the same pairwise agreement calculation used in tion, we use the mean of all annotator ratings in the WSsim or Usim annotation. This is justified because the inter-annotator agreement is highly significant, with respectable rho compared with previous work (Mitchell and Lapata 2008).
 number of annotators and the amount of visible context are different, and did traditional word sense annotation in the WSbest task in addition to the graded tasks) we report the results of R 1and R 2 separately. 12 4.2 WSsim: Graded Ratings for WordNet Senses
In the WSsim task, annotators rated the applicability of each sense of the target word on a five-point scale. We first do a qualitative analysis, then turn to a quantitative analysis of annotation results. 4.2.1 Qualitative Analysis. Table 6 shows an example of WSsim annotation. The target is the verb dismiss , which was annotated in R 2. The first column gives the WordNet sense number (sn). 13 Note that in the task, the annotators were given the synonyms and full description but in this figure we only supply part of the description for the sake of space. As can be seen, three of the annotators chose a single-sense annotation by giving a rating of 5 to one sense and ratings of 1 to all others. Two annotators gave ratings of 1 and 2 to all but one sense. The other three annotators gave positive ratings (ratings of at least 3 [similar], see Table 4) to at least two of the senses. All annotators agree that the first sense fits the usage perfectly, and all annotators agree that senses 3 and 5 do not apply. The second sense, on the other hand, has an interestingly wide distribution of judgments, ranging from 1 to 4. This is the judicial sense of the verb, as in  X  X his case is dismissed. X  Some annotators consider this sense to be completely distinct from sense 1, whereas others see a connection. There is disagreement among annotators, about sense 6. This is the sense  X  X ismiss, dissolve, X  as in  X  X he president dissolved the parliament. X  Six of the annotators consider this sense completely unrelated to  X  X ismiss our actions as irrelevant, X  whereas two annotators view it as highly related (though not completely identical). It is noteworthy that each of the two opinions, a rating of 1 522 and a rating of 4, was chosen by multiple annotators. Because multiple annotators give each judgment, these data seem to reflect a genuine difference in perceived sense. We discuss inter-annotator agreement, both overall and considering individual annotators, subsequently.
 also gets ratings of  X  3 from all annotators. The first annotator seems also to have perceived the  X  X hysical object X  connotation to apply strongly to this example, and has expressed this quite consistently by giving high marks to sense 1 as well as 7. in
R 2. In this case, only one annotator chose single-sense annotation by marking exclu-sively sense 4. One annotator gave ratings  X  3 (similar) to all senses of the lemma. All other annotators saw at least two senses as applying (with ratings is, ratings  X  3) throughout. Senses 1, 2, and 6 have mixed ratings, and senses 3 and 5 have positive ratings only from the one annotator who marked everything as applying.
Interestingly, ratings for senses 1, 2, and 6 diverge sharply, with some annotators seeing them as not applying at all, and some giving them ratings in the 3 X 5 range. Note that the annotators who give ratings of 1 are not the same for these three ratings, pointing to dif-ferent, but quite nuanced, judgments of the  X  X ake reality neater X  usage in this sentence. 4.2.2 Inter-annotator Agreement. We now turn to a quantitative analysis, starting with inter-annotator agreement. For the graded WSsim annotation, it does not make sense to compute the percentage of perfect agreement. As discussed earlier, we report inter-annotator agreement in terms of correlation, using Spearman X  X  rho. We calculate pair-wise agreements and report the average over all pairs. The pairwise correlations are shown in the matrix in Table 9. We have used capital letters to represent the individ-uals, preserving the same letter for the same person across tasks. In the last row we show agreement of each annotator X  X  judgments against the average judgment from the other annotators. The pairwise correlations range from 0.47 to 0.54 and all pairwise correlations were highly significant (p 0 . 001), with an average of rho = 0 . 504. This is a very reasonable result given that Mitchell and Lapata (2008) report a rho of 0.40 on a graded semantic similarity task. 14 The lowest correlation against the average 524 from the other annotators was 0.56. We discuss the annotations of individuals in Sec-tion 4.6, including our decision to retain the judgments of all annotators for our gold standard.
 lations ranged from 0.52 to 0.72. The average value of the pairwise correlations was rho = 0 . 60, and again every pair was highly significant (p 0 . 001). The lowest correla-tion against the average from all the other annotators was 0.58. 4.2.3 Choice of Single Sense Versus Multiple Senses. In traditional word sense annotation, annotators can mark more than one sense as applicable, but annotation guidelines often encourage them to view the choice of a single sense as the norm. In WSsim, annotators gave ratings for all senses of the target. So we would expect that in WSsim, there would be a higher proportion of senses selected as applicable. Indeed we find this to be the case: Table 11 shows the proportion of sentences where some annotator has assigned more than one sense with a judgment of 5, the highest value. Both WSsim-1 and WSsim-2 have a much higher proportion of sentences with multiple senses chosen than the traditional sense-annotated data sets SemCor and SE -3. Interestingly, we notice that the percentage for WSsim-1 is considerably higher than for WSsim-2. In principle, this could be due to differences in the lemmas that were annotated, or differences in the sense perception of the annotators between R 1and R 2. Another potential influencing factor is the order of annotation experiments: As described earlier, half of the tators did WSbest annotation before doing WSsim-2, and half did the two experiments in the opposite order. As Table 11 shows, those doing the graded task WSsim-2 before the binary task WSbest had a greater proportion of multiple senses annotated with the highest response. This demonstrates that annotators in a word meaning task can be influenced by factors outside of the current annotation task, in this case another annotation task that they have done previously. We take this as an argument in favor of using as many annotators as possible in order to counteract factors that contribute noise.
In our case, we counter the influence of previous annotation tasks somewhat by using multiple annotators and altering the order of the WSsim and WSbest tasks. Another option would have been to use different annotators for different tasks; by using the same set of annotators for all four tasks, however, we can better control for individual variation. 4.2.4 Use of the Graded Scale. We next ask whether annotators in WSsim made use of the whole five-point scale, or whether they mostly chose the extreme ratings of 1 and 5.
If the latter were the case, this could indicate that they viewed the task of word sense assignment as binary. Figure 1a shows the relative frequency distribution of responses from all annotators over the five scores for both R 1and R same but for each individual annotator. In both rounds the annotators chose the rating of 1 ( X  X ompletely different, X  see Table 4) most often. This is understandable because each item is a sentence and sense combination and there will typically be several irrelevant senses for a given sentence. The second most frequent choice was 5 ( X  X dentical X ). Both rounds had plenty of judgments somewhere between the two poles, so the annotators do not seem to view the task of assigning word sense as completely binary. Although the annotators vary, they all use the intermediate categories to some extent and certainly the intermediate category judgments do not originate from a minority of annotators. different X ) than the R 1 annotators. One possible reason is again that half our annota-tors did WSbest before WSsim-2. If this were the cause for the lower judgments, we would expect more ratings of 1 for the annotators who did the traditional word sense annotation (WSbest) first. In Table 12 we list the relative frequency of each rating for the different groups of annotators. We certainly see an increase in the judgments of 1 where 526
WSbest is performed before WSsim-2. Again, this may indicate that annotators were leaning more towards finding a single exact match because they were influenced by the WSbest task they had done before. Annotators in that group were also slightly less inclined to take the middle ground, but this was true of both groups of compared with the R 1 annotators. We think that this difference between the two rounds may well be due to the lemmas and data.
 per item for each of the graded annotation tasks. WSsim naturally has less variation compared with Usim because, for any sentence, there are inevitably many WordNet senses which are irrelevant to the context at hand and which will obtain a judgment of 1 from everyone. This is particularly the case for WSsim-2 where the annotators gave more judgments of 1, as discussed previously. The majority of items have a range of less than two for WSsim. We discuss the Usim figures further in the following section. 4.3 Usim: Graded Ratings for Usage Similarity In Usim annotation, annotators compared pairs of usages of a target word ( rated their similarity on the five-point scale given in Table 4. The annotators were also permitted a response of  X  X on X  X  know. X  Such responses were rare but were used when the annotators really could not judge usage similarity, perhaps because the meaning of one sentence was not clear. We removed any pairs where one of the annotators had given a  X  X on X  X  know X  verdict (9 in R 1, 28 in R 2). For R a total of 1,512 SPAIR sandin R 2 we had a resultant 1,142 4.3.1 Qualitative Analysis. We again start by inspecting examples of Usim annotation.
Table 13 shows the annotation for an SPAIR of the verb dismiss . The first of the two sentences talks about  X  X ismissing actions as irrelevant, X  the second is about dismissing a person. Interestingly, the second usage could be argued to carry both a connotation of  X  X shering out X  and a connotation of  X  X isregarding. X  Annotator opinions on this vary from a 1 (completely different) to a 5 (identical), but most annotators seem to view the two usages as related to an intermediate degree. This is adequately reflected in the average rating of 3.125. Table 14 compares the sentence from Table 8 to another sentence 528 with the target neat . The first sentence is a metaphorical use (making reality neater), the second is literal (landscape with neat appearance), but still the ratings of 3 X 5 throughout for an average of 4.0. Note that the WordNet senses, shown in Table 8, do not distinguish the literal and metaphorical uses of the adjective, either.
Table 15 shows two uses of the noun account . The first pertains to accounts on a software system, the second to bank accounts. The spread of annotator ratings shows that these two uses are not the same, but that some relation exists. The average rating for this
SPAIR is 3.0. 4.3.2 Inter-annotator Agreement. We again calculate inter-annotator agreement as the average over pairwise Spearman X  X  correlations. The pairwise correlations are shown in the matrix in Table 16. In the last row we show agreement of each annotator X  X  judgments against the average judgment from the other annotators. For Usim-1 the range of correlation coefficients is between 0.50 and 0.64 with an average correlation of rho = 0.548. All the pairs are highly significantly correlated (p 0 . 001). The smallest correlation for any individual against the average is 0.55. The correlation matrix for Usim-2 is provided in Table 17; the range of correlation coefficients is between 0.42 and 0.73. All these correlations are highly significant (p 0 . 001) with an average correlation of rho = 0.62. The lowest agreement between any individual and the average judgment of the others is 0.58. Again, we note that these are all respectable values for tasks involving semantic similarity ratings.

Use of the graded scale. Figure 1b shows how annotators made use of the graded scale in Usim-1 and Usim-2. It graphs the relative frequency of each of the judgments on the five-point scale. Figures 2b and 3b show the same but for each individual annotator. In both annotation rounds, the rating 1 (completely different) was chosen most frequently.
There are also in both annotation rounds many ratings in the middle points of the scale, indeed we see a larger proportion of mid-range scores for Usim than for WSsim in general, as shown in Table 12. Figures 2b and 3b show that although individuals differ, all use the mid points to some extent and it is certainly not the case that these mid-range judgments come from a minority of annotators. In Usim, annotators com-pared pairs of usages, whereas in WSsim, they compared usages with sense defini-tions. The sense definitions suggest a categorization that may bias annotators towards categorical choices. Comparing the two annotation rounds for Usim, we see that in
Usim-2 there seem to be many more judgments at 5 than in Usim-1. This is similar to our findings for WSsim, where we also obtained more polar judgments for for R 1.
 shown earlier by Table 18. This is understandable given that there are eight annotators 530 for R 2 compared with R 1, 16 and so a greater chance of a larger range per item. There is substantial variation by lemma. In Usim-2, fire.v , rough.a ,and coach.n have an average range of 1.33, 1.76, and 1.93, respectively, whereas suffer.v , neat.a ,and function.n have average ranges of 3.14, 3.16, and 3.58, respectively. The variation in range appears to depend on the lemma rather than POS. This variation can be viewed as a gauge of how difficult the lemma is. Although the range is larger in Usim-2, however, the average variance per item (i.e., the variance considering the eight annotators) is 1.12 and lower than that for WSsim-1.

Usim and the triangle inequality. In Euclidean space, the lengths of two sides of a triangle, taken together, must always be greater than the length of the third side. This is the triangle inequality :
We now ask whether the triangle inequality holds for Usim ratings. If Usim similarities are metric, that is, if we can view the ratings as proximity in a Euclidean  X  X eaning space, X  then the triangle inequality would have to hold. This question is interesting for what it says about the psychology of usage similarity judgments. Classic results due to Tversky and colleagues (Tversky 1977; Tversky and Gati 1982) show that human judgments of similarity are not always metric. Tversky (1977), varying an example by William James, gives the following example, which involves words, but explicitly ignores context:
Note, however, that Tversky was considering similarity judgments for different words, whereas we look at different usages of the same word. The question of whether the triangle inequality holds for Usim ratings is also interesting for modeling reasons.
Several recent approaches model word meaning in context through points in vector space (Erk and Pado 2008; Mitchell and Lapata 2008; Dinu and Lapata 2010; Reisinger and Mooney 2010; Thater, F  X  urstenau, and Pinkal 2010; Washtell 2010; Van de Cruys,
Poibeau, and Korhonen 2011). They work on the tacit assumption that similarity of word usages is metric X  X n assumption that we can directly test here. Third, the triangle inequality question is also relevant for future annotation; we will discuss this in more detail subsequently.
 similarity ratings that the annotators gave to dissimilarity ratings: Let s similarity rating over all annotators, then we use the dissimilarity rating d = 6 (as 5 was the highest possible similarity score).
 (that is, we consider every triple of sentences that share the same target lemma). In those cases where the triangle inequality is violated, we also assess the degree to which it is violated, calculated as the average distance that is missed: Let T for which the triangle inequality does not hold, then we compute m = 1 | This is the average amount by which the longest side is  X  X oo long. X  triples obey the triangle inequality. For the triples that miss it, the average amount by which the longest side is too long is m = 0 . 520. This is half a point on the five-point rating scale, a low amount. In R 2, all sentence triples obey the triangle inequality. One potential reason for this is that we have eight annotators for sample of annotators reduces the variation from individuals. Another reason may be that the annotators in R 2 could view two more sentences of context than those in R 1.
 annotator. Every annotator has at least 93% of sentence triples obeying the principle. For the triples that miss it, they tend to miss it by between one and two points. The results for individuals accord with the triangle inequality principle, though to a lesser extent compared with the analysis using the average, which reduces the impact of variation from individuals.
 annotation triples) is interesting because it contrasts with Tversky X  X  findings (Tversky 1977; Tversky and Gati 1982) that similarity ratings between different words are not metric. And although we consider similarity ratings for usages of the same word, not different words, we would argue that our findings point to the importance of consider-ing the context in which a word is used. It would be interesting to test whether similarity ratings for different words, when used in context, obey the triangle inequality. To reference the Tversky example, and borrowing some terminology from Cruse, evoking the ISLAND facet of Jamaica and Cuba versus the COMMUNIST STATE Russia would account for the non-metricality of the similarity judgments as Tversky 532 points out, and moreover highlight the lack of an apt comparison between Jamaica and
Russia at all. There is some motivation for this idea in the psychological literature on structural alignment and alignable differences (Gentner and Markman 1997; Gentner and Gunn 2001).
 will be useful for future Usim annotation. Usage similarity annotation is costly (and somewhat tedious) as annotators give ratings for each pair of sentences for a given target lemma. Given that we can expect usage similarity to be metric, we can eliminate the need for some of the ratings. Once annotators have rated two usage pairs out of a usages s 1 and s 2 have a distance of 1 (i.e., a similarity of 5), and s of 1, then the distance of s 1 and s 3 can be at most 2. For all usage triples where two pairs have been judged highly similar, we can thus omit obtaining a rating for the third pair. A second option for obtaining more Usim annotation is to use crowdsourcing. In crowdsourcing annotation, quality control is always an issue, and again we can make use of the triangle inequality to detect spurious annotation: Ratings that grossly violate the triangle inequality can be safely discarded. 4.4 WSbest
The WSbest task reflects the traditional methodology in word sense annotation where words are annotated with the best fitting sense. The guidelines more than one sense provided all fit the example equally well. Table 21 shows that, as one would expect given the number of senses in WordNet, there are more unse-lected senses than selected. We again find an influence of task order: When annota-tors did the graded annotation (WSsim-2) before WSbest, there were more multiple assignments (see the last column) and therefore more senses selected. This difference tice that the proportion of multiple sense choice is far lower than the equivalent for
WSsim (see Table 11), as is expected due to the different annotation schemes and guidelines.
 in WSD . There are several ways to calculate pairwise agreement in cases of multiple selection, though these details are not typically given in of the intersection of selections divided by the maximum number of selections from either annotator. This is equivalent to 1 for agreement and 0 for disagreement in cases where both annotators have selected only one sense. Formally, let i sentence. Let A be the set of annotators and let P A = { { annotator pairs. Let a i be the set of senses that annotator a i . Then pairwise agreement between annotators is calculated as: where each annotator only selected one sense (not multiple), the average is 0.626. multiword data (Mihalcea, Chklovski, and Kilgarriff 2004). This annotation exercise used volunteers from the Web (Mihalcea and Chklovski 2003). Like our study, it had taggers without lexicography background and gave a comparable ITA to our 0.626. We calculated pairwise agreement for eight annotators. To carry out the experiment under maximally similar conditions to previous studies, we also calculated ITA for items with only one response and use only the four annotators who performed WSbest first. This resulted in an average ITA of 0.638.
 individual removed to see the change in agreement. The results are in the first row of
Table 22. 4.5 SYNbest
The SYNbest task is a repetition of the LEXSUB task (McCarthy and Navigli 2007, 2009) except that annotators were asked to provide one synonym at most. As in agreement between a pair of annotators was counted as the proportion of all the sentences for which the two annotators had given the same response.

LEXSUB we only include those where at least two annotators have provided at least one substitute: If only one annotator can think of a substitute then it is likely to be a problematic item. As in WSbest, let a i be the set 19 of responses (substitutes) for an item 534 i  X  I for annotator a  X  A .Let P agreement between annotators is calculated as in LEXSUB as: (WSbest), the credit for each item (the intersection of annotations from the annotator pair) is divided by the union of the responses. For traditional divided by the number of responses from either annotator, which is usually one. For lexical substitution this is important as the annotations are not collected over a predeter-mined inventory. In LEXSUB ,the PA figure was 0.278, whereas we obtain PA = 0 . 261 on
SYNbest. There were differences in the experimental set-up. We had eight annotators, compared with five, and for SYNbest each annotator only provided one substitute. Additionally, our experiment involved only a subset of the data used in figures are not directly comparable, but are reasonably in line.
 a substitute for any given item, so there were always at least five substitutes per item. In
LEXSUB there were 16 items excluded from testing in the full data set of 2010 because there was only one token substitute provided by the set of annotators.
 individual removed to see the change in agreement. The results are in the second row of Table 22. 4.6 Discussion of the Annotations of Individuals
We do not pose these annotation tasks as having  X  X orrect responses. X  We wish instead to obtain the annotators X  opinions, and accept the fact that the judgments will vary.
Nevertheless, we would not wish to conduct our analysis using annotators who were not taking the task seriously. In the analyses that follow in subsequent sections, we use the average judgment from our annotators to reduce variation from individuals.
Nevertheless, before doing so, in this subsection we briefly discuss the analysis of the individual annotations provided earlier in this section in support of our decision to use all annotators for the gold standard.
 annotators showed reasonable correlation on the graded task and at a level in excess of that achieved on other graded semantic tasks (Mitchell and Lapata 2008). There will inevitably be one annotator that has the lowest correlation with the others on any given task, but we found that this was not the same annotator on every task. For example,
C on WSsim-2 has the lowest correlation with the average, yet concurs with others much more on Usim-2 and leaving C out would reduce agreement on WSbest and on SYNbest. D has lower correlation with others on several tasks, though higher than C on
WSsim-2. When we redo the triangle inequality analysis in Section 4.3 individually we see from Tables 19 and 20 that annotator D is the highest performing annotator in terms of meeting the triangle inequality principle in R 2 and is a close runner-up in results indicate that although annotators may use the graded scale in different ways, their annotations tally to a reasonable extent. We therefore used all annotators for the gold standard. 4.7 Agreement Between Annotations in Different Frameworks
In this paper we are considering various different annotations of the same underly-ing phenomenon: word meaning as it appears in context. In doing so, we contrast traditional WSD methodology ( SE -3, SemCor, and WSbest) with graded judgments of sense applicability (WSsim), usage similarity (Usim), and lexical substitution as in
LEXSUB and SYNbest. In this section we compare the annotations from these different paradigms where the annotations are performed on the same underlying data. For
WSsim and Usim, we use average ratings as the point of comparison. 4.7.1 Agreement Between WS sim and Traditional Sense Assignment. To compare WSsim ratings on a five-point scale with traditional sense assignment on the same data, we convert the traditional word sense assignments to ratings on a five-point scale: Any sense that is assigned is given a score of 5, and any sense that is not assigned is given a score of 1. If multiple senses are chosen in the gold standard, then they are all given scores of 5. We then correlate the converted ratings of the traditional word sense assignment with the average WSsim ratings using Spearman X  X  rho.
 either SE -3 or SemCor. The correlation of WSsim-1 and SE relation of WSsim-1 with SemCor is rho = 0.425. Both are highly significant (p 0 . 001).
WSbest on the same data. This allows a fuller comparison of traditional and graded tagging because we have a data set annotated with both methodologies, under the same conditions, and with the same set of annotators. We use the mode (most common) sense tag from our eight annotators as the traditional gold standard label for WSbest and assign a rating of 5 to that sense, and a rating of 1 elsewhere. We again used Spearman X  X  rho to measure correlation between WSbest and WSsim and obtained rho = 0.483 (p 0 . 001). 4.7.2 Agreement Between WSsim and Usim. WSsim and Usim provide two graded an-notations of word usage in context. To compare the two, we convert WSsim scores to usage similarity ratings as in Usim. In WSsim, each sense has a rating (aver-aged over annotators), so a sentence has a vector of ratings with a  X  X imension X  for each sense. For example, the vector of average ratings for the sentence in Table 6 is same space, as they share the same sense list. Accordingly, we can compare a pair u , u of sentences that share a target using Euclidean distance: us a dissimilarity rating for u , u . We can now compare these sentence pair dissimilarities to the similarity ratings of the Usim annotation.

The basis of this comparison is small, at three lemmas with 10 sentences each, giving 536 135 sentence pairs in total, because that is all the data available annotated in both paradigms. For R 2 we can perform the analysis on the whole Usim-2 and WSsim-2 data, which gives us 26 lemmas, with 1,142 sentence pairs. rho =  X  0 . 816. The degree of correlation is striking. We conclude that there is a very strong relationship between the annotations for Usim and WSsim. This bodes well for using Usim as a resource for evaluating sense inventories, an idea that we will pursue further in Section 6: It reflects word meaning but is not tied to any given sense inventory. 4.7.3 Agreement of WSsim and Usim with Lexical Substitution. Lexical paraphrases (sub-stitutes) have been used as a means of evaluating WSD systems in a task where the inventory is not predefined (McCarthy and Navigli 2007, 2009). Because the notation was done in part on data that had previously been annotated with lexical substitutions, and R 2 included lexical substitution annotation, we can compare para-phrase annotation with the results of WSsim and Usim. Again, we need to transform annotations to make the comparison feasible. We convert all annotations to a Usim-like format using sentence pair similarity or dissimilarity ratings. For WSsim, we use the transformation described previously, using Euclidean distance between sense rating vectors. We transform lexical substitution annotation using multiset intersection, as the lexical substitution annotation of a sentence is a multiset of substitutes annotators. If sentences s 1 , s 2 have substitute multisets subs freq i ( w ) is the frequency of substitute w in multiset subs intersection as annotators could come up with a substitute. We also did not include any items that were tagged with the wrong POS in LEXSUB . 23 annotation with lexical substitution annotation. The values of Usim and WSsim are based on mean scores averaged over all annotators. The INTER lexical substitution annotation yield similarity ratings for sentence pairs; accordingly, correlations of transformed lexical substitution with Usim are positive, and correlations of transformed lexical substitution with the WSsim-based sentence dissimilarity ratings are negative. All correlations are highly significant (p 0 . 001). We anticipated a higher correlation of SYNbest with R 2 annotation compared with that obtained using and R 1 annotation: In R 2 the set of annotators is larger, the same set of annotators do all experiments, and the SYNbest annotation focuses on obtaining one substitute per annotator (whereas LEXSUB allowed annotators to supply up to three paraphrases). This turned out to be in fact the case, as a comparison of rows 1 and 2 of Table 23 shows, and likewise a comparison of rows 3 and 4. We notice that the correlation is slightly stronger for Usim compared with WSsim, for both annotation rounds. One possible reason for this is that the comparison of lexical substitution data with Usim involves only one transformation of annotation data (the INTER calculation), whereas the com-parison with WSsim involves two ( INTER and also the Euclidean distance transforma-tion). We can expect each transformation of annotation data to be  X  X ossy X  in the sense of introducing additional variance. Furthermore, WSsim relies on WordNet, which may add a layer of structure that does not reflect the overlap in semantic similarity between usages. 4.7.4 Summary. The Usim framework enables us to compare different annotation schemes for word meaning, as it is relatively straightforward to map all annotations to sentence pair (dis-)similarity ratings. We found strong relationships between WSsim and Usim annotation, and between both graded annotation frameworks on the one hand and traditional word sense annotation or lexical substitutions on the other hand.
This provides some validation for the novel annotation frameworks. Also, if all labeling schemes provide comparable results, that opens up opportunities for choosing the best-fitting labeling scheme for each situation. All these tasks pursue the same endeavor, although the graded annotations and substitutions strive to capture the more subtle nuances of meaning that are not adequately represented by the winner takes all ap-proach of traditional methodology. WSsim is closest to the traditional methodology and would suit systems needing to output WordNet sense labels, for example because they want to exploit the semantic relations in WordNet. Usim is application-independent. It allows for evaluation of systems that relate usages, whether into clusters or simply on a continuum. It could, for example, be used as a resource-independent gold standard for word sense induction. Lexical substitution tasks are particularly useful where the application being considered would benefit from lexical paraphrasing, for example, text simplification, summarization, or query expansion in information retrieval. 5. Examining Sense Groupings Emerging from WSsim Annotation Recently there has been considerable work on grouping fine-grained senses, often from WordNet, into more coarse-grained sense groups (Palmer, Dang, and Fellbaum 2007).
The use of coarse-grained sense groups has been shown to yield considerable improve-ments in inter-annotator agreement in manual annotation, as well as in the accuracy of
WSD systems (Palmer, Dang, and Fellbaum 2007; Pradhan et al. 2007). In our WSsim annotation, we have used fine-grained WordNet senses, but we want to check that our results are not an artifact of this fine-grained inventory. Furthermore, the annotation results might be useful for identifying senses that could be grouped or for identifying senses where grouping is not straightforward. 538 perceives two senses of some target word as very similar, they will probably give them similar ratings, and not just for a single sentence but across all the sentences featuring the target word in question. So by looking for pairs of senses that tended to receive similar ratings across all sentences, we can identify sense descriptions that according to our annotators describe similar senses. Conversely, we expect that unrelated senses would have dissimilar ratings. If there were many senses that the WSsim annotators implicitly  X  X rouped X  by giving them similar ratings throughout, we would have to revise our finding that WSsim annotators often perceived more than one sense to be applicable, as they would have perceived only what could be described as one implicit sense group.
 distinctions that would be intuitively plausible to an untrained speaker of the language, then senses in a common group should also be similar according to WSsim annotation.
So when WSsim annotators give very different ratings to senses that are in the same coarse-grained group, or very similar ratings to senses that are in different groups, this can point to problems in a coarse-grained sense group.

Navigli, Litkowski, and Hargraves 2007). Then we test the extent that the annotations accord with sense groupings by: 1. comparing judgments against the existing groupings, and re-examining 2. using the WSsim data to examine the extent that the annotations could be 5.1 Existing Sense Grouping Efforts
OntoNotes. The OntoNotes project (Hovy et al. 2006; Chen and Palmer 2009) annotates word sense, along with coreference and semantic roles. The senses that it uses for verbs are WordNet 2.1 and 2.0, manually grouped based on both syntactic and semantic criteria. Examples of these criteria include the causative/inchoative distinction, and semantic features of particular argument positions, like animacy. Once the sense groups for a lemma are constructed manually, they are used in trial annotation. If an inter-annotator agreement of approximately 90% is reached, the lemma X  X  sense groups are used for annotation; otherwise they are revised. Chen and Palmer report that the sense groups used in OntoNotes have resulted in a rise in inter-annotator agreement as well as annotator productivity. The third column of Table 24 shows OntoNotes groups for the noun account . 5.1.1 The SemEval-2007 English All Words Task (EAW). For the English All Words task at SemEval-2007, WordNet 2.1 senses were grouped by mapping them to the more coarse-grained Oxford Dictionary of English senses. For the training data, this mapping was done automatically; for the test data, the mapping was done by hand (Navigli,
Litkowski, and Hargraves 2007). For our analysis, we used only lemmas that were included in the test data where the mapping had been produced manually. largely agree in the groupings for account . But whereas EAW groups senses 1, 2, 4, and 8 together, OntoNotes splits those senses into two groups. 5.2 Does WSsim Annotation Conform to Existing Sense Groups?
In the WSsim annotation, we have used the fine-grained senses of WordNet 3.0. But annotators were free to give high ratings for a sentence to more than one sense. So it is possible that they implicitly used more coarse-grained sense distinctions. In this and the following section, we will explore the question of whether, and to what extent,
WSsim annotators used implicit coarse-grained sense groups. In this section, we will first ask whether their annotation matched the sense groups of either OntoNotes or
EAW. OntoNotes and EAW differ in the lemmas they cover. Also, as we saw earlier, when they both cover a lemma, they do not always agree in the sense groups that they propose. So we study the agreement of WSsim annotation with the two sense groupings separately. We only study the lemmas which are in both the WSsim data and in either OntoNotes or the EAW test data, listed in Table 25.
 two senses both had high ratings, but are in different groupings in either OntoNotes or
EAW. The first row shows how many sentences there were where two senses received a judgment of  X  3, but the two senses were not in a common OntoNotes/EAW group.
The second row shows the same for judgments  X  4, and the last row for judgments of 5 only. In general, the percentages are higher for EAW than for OntoNotes. This is not due to any difference in granularity between the two resources. The EAW sense groups encompass on average 2.3 fine-grained senses for the 540  X   X   X   X  2 lemmas, and for OntoNotes the mean group sizes are 2.3 ( likely it is due to the individual lemmas. We observe that ratings of  X  X imilar X  or higher are frequent. In all conditions except WSsim-1/OntoNotes, we find percentages over 50%. On the other hand, there are many fewer sentences where two senses received judgments of  X  X ery similar X  or  X  X dentical X  but were not in the same OntoNotes or
EAW group, but these cases do exist. For example, there were five sentences with the target dismiss.v which in WSsim received an average judgment of 4 or 5 for senses from two different OntoNotes groups, 1.1 and 1.2. As dismiss is an only 10 sentences were annotated, this means that this phenomenon was found in half the sentences annotated. The two sense groups are related: One is a literal, the other a metaphorical, use of the verb. OntoNotes group 1.1 is defined as  X  X efuse to give consideration to something or someone, X  and group 1.2 is  X  X ischarge, let go, persuade to leave, send away. X  One such sentence was the second sentence in Table 13. or EAW grouping received widely different ratings in the WSsim annotation. The first row shows how many sentences there were where one sense received a rating of and another sense from the same OntoNotes or EAW group had a rating of second row shows the same for sense pairs in the same coarse-grained group where one received a rating of  X  2 and the other the highest possible rating of 5. (Note that the table considers judgments averaged over all annotators, so this row counts only sentences where all annotators agreed on the highest rating.) An example of such a case is Rich people manage their money well . In WSsim the first sense in WordNet ( possessing material wealth ) received an average score of 5 (i.e. a unanimous verdict), whereas all other senses received a score of less than 2. This included the third sense ( of great worth or quality;  X  X  rich collection of antiques X  ), which had an average of 1.625, and sense 8 Both senses 3 and 8 are in the same group as sense 1 in EAW.
 analysis than the OntoNotes and EAW groups offer. The percentages are substantial:
For the more inclusive analysis in the first row, the numbers are between 20% and 60% of all sentences, and between 2% and 15% of sentences even fall into the more restrictive case in the second row. There is no clear trend in whether we see more of this type of disagreement for OntoNotes or for EAW, or for the first or the second round of WSsim annotation.
 from the same OntoNotes or EAW group have received diverging WSsim ratings, or two senses from different groups have received high ratings. In this way, the WSsim annotation can be used to scrutinize sense groupings: If one aim of the sense groupings is to form groups that would match intuitive sense judgments by untrained subjects, then WSsim annotation would suggest that the senses of dismiss.v that correspond to  X  X ismiss a person X  and  X  X ismiss an idea X  may be too close together to be placed in different groups. 5.3 Inducing Sense Relatedness from WSsim Annotation
In the WSsim annotation, annotators have annotated each occurrence of a target word with a rating for each of the WordNet senses for the target, as illustrated in Tables 6 X 8.
This allows us, conversely, to chart the ratings that a WordNet sense received across all sentences. Table 28 shows this chart for two senses of the noun account .Inthe table, ratings are averaged across all annotators. In this case, the averaged ratings are 542 similar for the two senses: They tend to be high for the same sentences, and low for the same sentences. In general, senses that are closely related should tend to receive similar ratings: high on the same sentences, and low on the same sentences, as illustrated for the two senses in Table 28.
 if the WSsim annotators perceived them to be similar. We compute correlation for any pair of senses for a common lemma, again using Spearman X  X  rho. distribution of rho values obtained for all the sense pairs, as histograms for and R 2 (right). When two senses are strongly positively correlated, this means that the annotators likely viewed them as similar. When two senses are strongly negatively correlated, this means they are probably so different that they tend never to be assigned high ratings for the same sentences. We see that in both rounds, there were roughly as many positive correlations as negative correlations. In R less equally distributed over the range from  X  1to1.In R 2, there were more annotators and the distribution is closer to a normal distribution with more rho values close to 0. now look at the WSsim-derived correlations for the same lemma, shown in Figure 5. The first row in each box shows the WordNet sense number, and the second row shows the
OntoNotes and EAW sense groups. All three labels are those used in Table 24. Each edge represents a correlation in the WSsim annotation. To avoid clutter, only correlations with rho  X  0 . 5 are included, and a sense is only shown if it is correlated with any other sense. Edge thickness corresponds to the value of the correlation coefficient rho between each two senses; rho is also annotated on the edges. The first thing to note is that WSsim-based correlation does not give us sense groups. Correlations are of different strengths, and different cutoffs would result in different link graphs. Even for the chosen cutoff of rho = 0 . 5, the correlations do not induce cliques (in the graph-theoretic sense). For example, the sixth sense of account shows a correlation of rho sense, but not with any of the other senses to which the eighth sense is linked. The figure also shows that there are some senses that are strongly correlated in their annota-tion but are not grouped in one or the other of the existing groupings. For example, senses 3 ( the executive who handles his account )and7( account statement ) are strongly correlated, but are in different groups in OntoNotes as well as in EAW. There are also senses that share the same group in one of the coarse grained inventories, but have a weak or even a negative correlation based on the WSsim annotation. For example, 544 for the lemma paper , senses 1 ( a material made from cellulose pulp )and4( amediumfor written communication ) are in the same EAW group, but have a correlation in WSsim of rho =  X  0 . 52.
 high ratings to more than a single sense could possibly be explained by them im-plicitly using more coarse-grained senses. We answered this question by comparing the WSsim annotation to OntoNotes and EAW sense groups, finding a considerable number of sentences where two senses received a high rating but were not from the same sense group. Now we can repeat the question, but try to answer it using the
WSsim sense relations obtained from correlation: Is it possible that WSsim annotators implicitly used more coarse-grained senses, but just not the OntoNotes or EAW sense groups? senses that were related at a level  X  rho, for rho ranging from that we want to answer is: If annotators give high ratings to multiple senses on the same sentence, is it always to senses that are strongly positively correlated, or do they sometimes pick multiple senses that are not strongly correlated, or even senses that are negatively correlated? The results are shown in Figure 6. First, we can see that there is a sizeable number of sentences where two senses that are negatively correlated have both received a positive judgment. For R 1, the numbers for negatively correlated senses are 135 ( j  X  3), 29 ( j  X  4), and 2 ( j = 5). For lower absolutely and in proportion, with 29 ( j  X  3), 7 ( j interesting to look at a less stringent threshold than rho levels p  X  0 . 05 and p  X  0 . 01 for this. If we look at sense pairs that were not positively correlated at p  X  0 . 05 (p  X  0 . 01), there were 185 (205) sentences in sentences in R 2 where two such senses both received judgments of 3 or higher. Note that the significance levels of p  X  0 . 05, p  X  0 . 01 are here just arbitrary thresholds at which to inspect the data; they are not thresholds that determine the significance of some hypothesis. 26 This brings us back to the question asked above of whether the
WSsim annotators implicitly used more coarse-grained senses. If they had implicitly used more coarse-grained senses, we would have expected to see very few cases where unrelated senses got a high rating on the same sentence. What we found instead was that such cases were relatively frequent, which implies that WSsim annotators in both rounds  X  X ix and match X  senses specifically for each sentence that they evaluate. For example, the senses 1 ( she dismissed his advances )and5( I was dismissed after I gave my report )of dismiss are negatively correlated (rho =  X  0 . 61) yet have average judgments of 3.25 and 4.125 on the second example in Table 13. 5.3.1 Summary. In this section we have analyzed the WSsim annotation in comparison with more coarse-grained sense repositories. One aim was to find out whether anno-tators really used the fine granularity that the WSsim task offered or whether they implicitly used more coarse-grained senses. Both by comparing the WSsim annotation to coarse-grained OntoNotes and EAW sense groups, and by comparing the WSsim annotation to the sense relations implied by WSsim, we find that annotators did make use of the ability to combine sense ratings in a way that was particular to each sentence they annotated. We also conclude that WSsim annotation can be used to evaluate
OntoNotes and EAW groupings with respect to the level to which senses are intuitively distinguishable to untrained subjects. Here, WSsim annotation can uncover senses in different groups that WSsim annotators often conflate, or senses in a single coarse group that WSsim annotators treat differently. 6. Usim and Sense Groupings
One of the major motivations for the Usim task is that it allows us to examine the meanings of words without recourse to a predefined inventory. We have demonstrated in this paper that the data from this task can be compared directly to paraphrase data as well as to data annotated for word sense. In the previous section we have focused on using our WSsim data to examine existing sense groupings. WSsim is useful precisely because it has sense annotations from an existing inventory, WordNet, so we can use the graded annotations to see how these senses relate, and also relationships between coarser grained inventories with mappings to WordNet. Usim does not capture this information, nevertheless it might be useful as a resource for examining sense groupings. We can use it to examine the extent to which sense groupings keep usages together that have a high usage similarity according to Usim, and keep sentences with low usage similarity apart. In this analysis, we use the data from this has Usim judgments for sentences alongside traditional word sense annotations (WSbest). As WSbest annotation, we use the mode of the chosen senses analysis in Section 4.7) for each sentence in R 2, and map it to its coarse-grained sense in 546
EAW and/or OntoNotes. We then compute the average Usim similarity for all pairs of sentences with the same coarse-grained sense, and compare it with the average Usim similarity for sentence pairs with different coarse-grained senses. The results are shown in the first row of figures in Table 29. We see that the OntoNotes and EAW sense groups do indeed partition the sentences such that pairs within the same group have high usage similarity (4 or above) and those in different groups have low usage similarity (2 or below).
 lemmas. A dash ( X ) means that either there was no coarse mapping, or there were no sentence pairs in this category. For example, there were no sentence pairs identified as having different OntoNotes groups or EAW groups for the lemma suffer.v . For the lemmas call.v and dismiss.v , the two sense inventories give rise to the same groupings of sentence pairs.
 judgments. One example is account.n , where the sentence pairs in the same coarse group get high average Usim values, whereas sentence pairs with different coarse groups have low average Usim values. There are, however, a few lemmas where the average Usim values indicate that either the coarse groupings might benefit from another inspection, or that the lemma has meanings with subtle relationships where grouping is not a straightforward exercise. One example is new.a , which has the same high Usim values for both same and different categories in EAW. Another is shed.v , where the sentences annotated with the same OntoNotes groups actually have a lower average Usim value than those with different groups.
 be useful in determining specific groups that might warrant further revision, or that represent meanings which are simply difficult to distinguish. To demonstrate this, we analyzed all coarse-grained sense groups with at least one sentence pair in groups that had at least two R 2 sentences whose WSbest mode mapped to that coarse group. (Naturally, due to the skewed nature of sense distributions and the fact that we only have ten sentences for each lemma, some groups do not meet this criterion.)
We find that the majority of groups that were analyzed have an average Usim rating of over 4. This is the case for 75% of the analyzed EAW groups and 76% of OntoNotes groups. There were, however, groups with very low values. One example was group 1.1 of shed.v in OntoNotes, with an average Usim rating of 2.9. This group includes both literal senses ( trees shed their leaves ) and metaphorical senses ( he shed his image as a pushy boss ) of the verb shed . Another example is group 7 of lead.n in EAW, also with an average Usim of 2.9. This group includes taking the lead as well as lead actor ,soquite a diverse collection of usages. Two example sentences annotated with these two senses are shown here. This pair had an average Usim value of 1.25.
 investigate whether the groupings that Usim identifies as problematic tend to be the same ones that require more iterations in inventory construction (Hovy et al. 2006). We also plan to test whether groupings with low Usim ratings tend to have lower inter-tagger agreement on traditional WSD annotation. 7. Computational Modeling
The graded meaning annotation data from Usim and WSsim annotation can be used to evaluate computational models of word meaning. In this section we summarize existing work on modeling the R 1 data, which has already been made publicly available. as traditional WSD systems. Instead of evaluating only the highest-confidence sense of a
WSD model, we can take a more nuanced look at a model X  X  predictions, and give credit if it proposes multiple appropriate senses. In Erk and McCarthy (2009) we take advantage of this fact to evaluate and compare two supervised models on the WSsim data: a traditional WSD model, and a distributional model that forms one prototype vector for each sense of a given lemma. Both are trained on traditional single-sense annotation, but the prototype model does not see any negative data during training in order to avoid spurious negative data. For training, each word occurrence is represented as either a first-order or a second-order bag-of-words vector of its sentence. In an evaluation using weighted variants of precision and recall, we find that when the traditional is credited for all the senses that it proposes, rather than only the single sense with the highest confidence value, it does much better on both measures. This shows that the model does propose multiple appropriate senses, such that its performance may be underestimated in traditional evaluation. As was to be expected, the prototype models that do not see negative data during training have much higher recall at lower precision, for an overall better F-score (again using weighted variants of the evaluation measures). 548 model. It represents a word sense as the sum of the vectors for all synonyms in its synset, plus the vectors for all hypernyms scaled down by a factor of 10. They also use a more complex, syntax-based model to derive occurrence representations. Unfortunately their results are not directly comparable to Erk and McCarthy (2009) because they evaluate on a subset of the data (verb lemmas only).
 used to evaluate distributional models of word meaning in context. So far, only one type of model has been evaluated on this data to the best of our knowledge: the clustering-based approach of Reisinger and Mooney (2010). They use the Usim data to test to what extent their clusters correspond to human intuitions on a word X  X  senses. Their result is negative, as a low correlation of human judgments and predictions suggests to them that the induced clusters are not a good match for human senses. The Usim data is particularly interesting for a different way of evaluating distributional and vector space approaches for word meaning in context. These have been evaluated on the tasks of lexical substitution (Erk and Pado 2008; Dinu and Lapata 2010; Thater, F  X  urstenau, and
Pinkal 2010; Van de Cruys, Poibeau, and Korhonen 2011), information retrieval, and word sense disambiguation (Sch  X  utze 1998), but Usim, in contrast, offers a different and more direct evaluation perspective. 8. Conclusion
In this paper we have explored the question of whether word meaning can be described in a graded fashion. Our aim has been to use annotation with graded ratings to capture untrained speakers X  intuitions on word meaning. Our motivation has been two-fold. On the one hand we are drawing on current theories of cognition, which hold that mental concepts have  X  X uzzy boundaries. X  On the other hand we wanted to give a basis to current computational models for word meaning in context that predicts degrees of similarity between word occurrences. We have addressed this question through two novel types of graded annotations of word meaning in context that draws on methods from psycholinguistic experimentation. WSsim obtains word sense annotations from a given sense inventory but uses graded judgments for each sense. Usim judges similarity of pairs of usages of the same lemma.
 mative. Annotators can describe word meaning through graded ratings with good inter-annotator agreement, measured through pairwise correlation. Even though no in-depth training on sense distinctions was provided, the pairwise correlations were good in every single case, indicating that all annotators did the tasks in a similar fashion.
In both tasks, all annotators made use of the full graded scale, and did not treat the task as binary. The Usim annotation provides us with a means of comparing different word meaning annotation paradigms. We have used it to demonstrate that there is strong correlation of these new annotations with both traditional overlap of lexical paraphrases. This is as we anticipated, as all of these annotations are describing the same phenomenon of word meaning in context through different means. sentences (between 23% and 46%) in which multiple senses received high positive judgments from the same annotators. At the same time, annotators used the WSsim ratings in a nuanced and fine-grained fashion, sometimes assigning high ratings on the same sentence to two senses that overall patterned very differently. Analyzing Usim annotation, we found that all annotators X  ratings obey the triangle inequality in almost all cases. This can be taken as a measure of intra-annotator consistency on the task.
It also means that current distributional approaches to word meaning in context are justified in viewing usage similarity as metric. Triangle inequality can be used to check the validity of future Usim annotation.
 of systems that represent word meaning in context, but we argue that they provide data sets that better reflect the fluid nature of word meaning and allow us to evaluate questions related to word meaning in a new fashion. In this paper, we have used both WSsim and Usim data to analyze existing coarse-grained sense inventories. We have demonstrated that it is often not straightforward to group sentences into disjoint senses, depending on the lemma. We have also shown how both WSsim and Usim style judgments can be used to identify problematic lemmas, as well as sense groupings that may warrant another inspection to check whether they match naive speakers X  intuitive judgments. The graded annotation can also be used to identify lemmas whose usages are difficult to group into clear distinct senses. This information can in the future be used to handle such lemmas differently when making sense inventories, in annotation, and in computational systems.
 uate computational models of word meaning. As we have shown (Erk and McCarthy 2009), WSsim data can be used to evaluate traditional WSD
We plan to do a more large-scale evaluation to assess to what extent the performance of current WSD systems is underestimated. Also, fine-grained WSsim annotation can be used for a comparison of fine-grained and coarse-grained traditional have also shown (Erk and McCarthy 2009) that WSsim can be used to evaluate graded word sense assignment systems. Although we used a supervised setting, however, we trained on traditional sense annotation. We plan to collect more WSsim annotation in order to be able to train word sense assignment systems on graded data, for example, using a regression model.
 sentences by using crowdsourcing. The use of Usim for supervised training of word meaning models is particularly interesting as all existing usage similarity models are unsupervised; given previous results in WSD, we can expect that supervision will improve the performance of models of usage meaning. One way of using Usim data in training is to learn a similarity metric. Metric learning (see, e.g., Davis et al. 2007) induces a distance measure from given constraints stating similarity or dissimilarity of items.
 through good agreement between those data sets themselves, as well as good agreement between those data sets and traditional word sense annotation and lexical substitutions.
Because all labeling schemes provide comparable results, this allows different ways of evaluating systems providing different perspectives on system output. Furthermore, the different paradigms may suit different types of systems. Lexical substitution tasks (McCarthy 2002; McCarthy and Navigli 2009) are particularly useful where the ap-plication being considered would benefit from lexical paraphrasing, for example, text simplification, summarization, or query expansion in information retrieval. WSsim is closest to the traditional methodology (WSbest) and would suit systems needing to output WordNet sense labels, for example, because they want to exploit the semantic relations in WordNet for tasks such as inferencing or producing lexical chains. Unlike
WSbest, it avoids a winner-takes-all approach and allows for more nuanced sense tagging. Usim is application-independent. It allows for evaluation of systems that relate 550 usages, whether into clusters or simply on a continuum. It could, for example, be used as a resource-independent gold standard for word sense induction by calculating the within and across class similarities. Aside from its use as an enabling technology within a natural language processing application, a system that performs well at the Usim task may be useful in its own right. For example, it could be used to enable lexicographers to work on groups of examples that reflect similar meanings, or find further examples close to the one being scrutinized.
 Acknowledgments References 552
