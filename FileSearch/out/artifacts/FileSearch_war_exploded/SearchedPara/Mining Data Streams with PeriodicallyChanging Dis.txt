 Dynamic data streams are those whose underlying distrib-ution changes over time. They occur in a number of ap-plication domains, and mining them is important for these applications. Coupled with the unboundedness and high ar-rival rates of data streams, the dynamism of the underlying distribution makes data mining challenging. In this paper, we focus on a large class of dynamic streams that exhibit pe-riodicity in distribution changes. We propose a framework, called DMM, for mining this class of streams that includes a new change detection technique and a novel match-and-reuse approach. Once a distribution change is detected, we compare the new distribution with a set of historically ob-served distribution patterns and use the mining results fro m the past if a match is detected. Since, for two highly similar distributions, their mining results should also present hi gh similarity, by matching and reusing existing mining result s, the overall stream mining efficiency is improved while the accuracy is maintained. Our experimental results confirm this conjecture.
 H.2.8 [ Database Management ]: Database Applications  X  Data Mining Algorithms, Performance, Experimentation Data stream, Distribution change
Mining data streams for knowledge discovery, such as se-curity protection [19], clustering and classification [2], and frequent pattern discovery [12], has become increasingly i m-portant. Within this context, an important characteristic of the unbounded data streams is that the underlying dis-tribution can show important changes over time, leading to dynamic data streams . Traditional data mining techniques that make multiple passes over data or that ignore distrib-ution changes are not applicable to dynamic data streams.
In this paper, we address the problem of mining dynamic data streams. This problem is particularly challenging whe n one tries to balance accuracy with efficiency  X  highly accu-rate mining techniques are generally computationally expe n-sive. Many change detection techniques for dynamic streams have been proposed (e.g., [1, 12, 14, 15]). However, some of these techniques can only generate an alarm to indicate that a distribution change has occurred, without providing new mining results corresponding to the new distribution. Oth-ers follow the  X  X etect-and-mine X  approach where the distri -bution change is detected and mining is adjusted between changes, but many of these techniques are ad-hoc in that they are only suitable for one type of mining application, an d they usually perform well only on certain types of streams.
For dynamic data streams, an interesting question is whethe r the distribution changes are entirely random and unpre-dictable, or whether it is possible for the distribution cha nges to follow certain patterns. If some regularity can be de-tected, this can be exploited in mining. An analysis of dif-ferent applications and data streams reveals that there is a large class of data streams that exhibit periodic distri-bution changes; that is, a distribution that has occurred in the past generally reappears after a certain time period. Consider, for example, a scientific stream that records sea surface temperature [17]. There is periodicity to the distr i-bution of collected data that may not be visible within one year, but is apparent over multiple years. Other examples of applications that may present periodically changing dis -tributions include financial data streams such as stock pric e [25], scientific streams such as magnetic resonance imaging (MRI) readings [5], and network traffic.
Based on this observation, we propose a method for min-ing dynamic data streams where we store important (to be defined later) observed distribution patterns, and compare new detected changes with these patterns. For two sub-streams with the same (or highly similar) distributions, mi n-ing results such as a list of all frequent items/itemsets for frequent pattern discovery, and set of clusters/classes fo r clustering and classification, should be the same (or highly similar). Therefore, if a distribution change is detected a nd if a match is found between the new distribution and an archived one, then it is possible to skip the re-mining proce ss and directly output the mining results for the archived dist ri-bution as the results for the newly detected distribution. W e call this the match-and-reuse strategy. Compared with the traditional detect-and-mine stream mining approaches, ou r match-and-reuse strategy is faster for periodically chang ing streams, since pattern matching is usually considerably le ss time consuming than mining.

The key issues that need to be resolved in our approach are the pattern selection, pattern representation, and mat ch-ing. Since the historical patterns need to be stored along with their mining results, typical memory restriction in da ta stream systems make it infeasible to store every pattern tha t has ever occurred in the past. Hence, only important distri-butions, i.e., the ones that have a high probability to reocc ur in the future, should be archived. Being able to determine these distributions is the first important issue. Second, ea ch pattern needs to be stored as succinctly as possible -ide-ally by representing a distribution (pattern) using its den -sity function. However, it is not possible to get an accurate density function for a random set of data elements. Hence, in data stream mining, the most popular approach is to rep-resent the current distribution using a representative dat a set. Intuitively, the larger the size of the representative set, the closer is the distribution of this set to the real distrib -ution. However, memory limitation forces the size of each representative set to be as small as possible. To find such a small representative set that can accurately reflect the tru e distribution of each pattern is a challenge. The third issue is that the matching procedure needs to be efficient for rapid data streams, with an accuracy high enough to meet the application requirements.

Similarity matching over time series has been extensively studied (e.g. [3, 10, 13, 16]). However, the distribution matching problem is far more complicated than time series matching in a number of ways. First, the sample patterns in time series matching are usually predefined. These well-chosen predefined patterns are very representative and usu-ally noise-free. In our case, the patterns are directly ex-tracted from the stream and may contain noise. Second, in time series data, each data element contains only one value. Streams generated from real-world are far more complex and have different forms. Many of the real-world stream elements have the form of relational tuples with multiple attributes. Hence, many of the time series matching tech-niques are too simple to be directly applicable on real-worl d streams. Third, a number of distance functions have been proposed for pattern matching. Each of these distance func-tions has its advantages and disadvantages. It is not clear which distance function would best fit a particular type of application and stream, nor is it known how to tune the distance threshold accordingly.

Our proposed match-and-reuse approach deals with these problems. For each important distribution, a set of repre-sentative sample data is extracted from the stream, which is continuously refined when new data arrive. Experimental results demonstrate that our distribution change detectio n technique using incrementally maintained representative set can achieve high accuracy. Experiments also reveal that, by reusing existing mining results on the newly detected distr i-bution after a match is found, the overall efficiency of the mining application can be greatly improved without losing accuracy.
In summary, the contributions of this paper are as follows:
The rest of this paper is organized as follows. The problem is formalized in Section 2. We propose our stream mining framework in Section 3. In Section 4, we analyze and com-pare some of the popular distance functions. Experiments are presented in Section 5. Section 6 gives a brief review of the related work. We conclude the paper in Section 7.
A data stream S is an unbounded sequence of elements  X  s, t  X  , where s is the data element and t is a monotonically increasing timestamp indicating the arrival time of the ele -ment. s can have different forms, such as a relational tuple, or a set of items, depending on the underlying application that generates S . t can be either an explicit timestamp or an implicit timestamp [4]. In this paper, the exact assign-ment of timestamps is not important as long as they are monotonic. Each stream mining application may only con-sider a few attributes/items of the stream. The unconsidere d attributes/items can be filtered before data are fed to the mining process.

A substream S i = { X  s 1 , t 1  X  ,  X  s 2 , t 2  X  , ...,  X  s S is a finite set of elements that occurs in S , i.e.,  X  s S , j = 1 , ..., k . Similar to [15], we define the probability distribution of a substream S i as the frequency distribution of all values within this substream, without considering th eir arrival time.

The following discussion and definitions assume the exis-tence of two substreams S A and S B , with probability dis-tributions P A and P B , respectively. As usual, we define the similarity of S A and S B as 1  X  dist ( S A , S B ), where dist ( S A , S B ) denotes the distance between S A and S discussed in Section 1, in this work, we consider two sub-streams as two bags of elements observed during different time period. Therefore, we define the similarity between two substreams as the similarity of their distributions, i. e., dist ( S A , S B ) = dist ( P A , P B ).

The distance between two substreams can be computed by a distance function. There are a number of distance functions that can be used. Specific functions for distribu-tion matching in data streams have been proposed [15], but functions that have been defined for time series can also be used by appropriate conversions: the time axis and the at-tribute value axis in these functions should be replaced wit h value axis and probability value axis, respectively. Consi der, for example, the computation of Euclidean distance over the two distributions P A and P B given in Figure 1, where v ( s ) is the value domain of both substreams S A and S B , p A ( v and p B ( v i ) are the probability of each v i  X  v ( s ) occurring in S A and S B , respectively. Then the distance between P and P B can be calculated as where n is the total number of distinct values in v ( s ). Figure 1: Distributions for substream S A and S B
Definition 1. Given S A , S B , P A and P B as defined above, if dist ( P A , P B ) &lt;  X  , where  X  is a predefined threshold re-ferred to as maximum matching distance , then we say that the distribution of substreams S A and S B match each other, denoted as P A  X  P B .

The main idea of this paper (reusing existing mining re-sults for periodically changing data streams) can now be for -malized as follows. Let K be the currently running stream mining application over data stream S with periodically changing distribution. Let P = { P 1 , ..., P m } be a set of preserved important distributions in S for K , and R = { R 1 , ..., R m } be the corresponding mining results where R is the result obtained during the period that distribution P was in effect. Let P 0 be the most recently detected distrib-ution, if P 0  X  P i , then the result of mining S during the period when P 0 is effective is R i , i.e., R ( P 0 ) = R wise, K is executed on P 0 to generate the result.
Due to the unboundedness of data streams and their high arrival rates, at any given time, only a portion of a stream can be monitored. Thus, window models are widely used in many stream techniques, and they are mainly of two types: time-based and count-based.

Time-based window is defined in terms of a time interval  X . It contains a substream of S that arrives within time  X  t, t + X   X  . On the other hand, a count-based window contains a substream of S with fixed number of elements. The size of a window is the total number of data elements within it. The size of a count-based window is fixed, while the size of a time-based window can change, if the arrival rate of S is not stable.

Definition 2. Let W A and W B be two windows (either time-based or count-based) on stream S , containing sub-streams S A and S B , respectively. We define the concate-nation of windows W A and W B , denoted as W A + W B , as the set union of the two substreams within them, i.e., W A + W B = S A  X  S B . Therefore,  X  X  s i , t i  X  in W A + W  X  s , t i  X   X  S A or  X  s i , t i  X   X  S B .

Notice that although we do not consider the arrival time of the elements when calculating the distribution, we do not remove the timestamps attached to them. Therefore, two el-ements  X  s i , t i  X  and  X  s j , t j  X  with the same values, i.e., s are not considered duplicates in W A + W B . This guarantees that the probability of a value that occurs in many elements will be calculated correctly. Thus, | W A + W B | = | W A when W A and W B do not overlap.

Table 1 summarizes the main symbols used in this pa-per. Note that some of the symbols are introduced in later sections.

In this paper, we propose a framework called DMM (which stands for Detect, Match, and Mine) for mining data streams with periodically changing distributions. Briefly, DMM con -sists of four sequential procedures: choosing representat ive set, change detection, pattern matching, and (if pattern matching fails) stream mining.

Change detection procedure (Section 3.3) continuously mon -itors the distribution changes of stream S . Once a new dis-tribution is detected, a representative set is generated fo r this new distribution and is incrementally maintained duri ng its lifespan (Section 3.2). This new distribution is matche d against those that have been seen and archived earlier (Sec-tion 3.4). If a match is found between the new distribution and an archived one, DMM skips the re-mining process and outputs the mining results of the archived distribution, be -cause two substreams with similar distributions should hav e similar data mining results. Consequently, processing tim e is greatly reduced. On the other hand, if pattern matching fails (i.e., no similar distribution has been seen previous ly), then the mining process is activated to generate new results for this new distribution. A set of  X  X mportant X  distributio ns that have already been observed is maintained. Since it may not be possible to store all observed distributions due to space restrictions, a heuristic is applied to determine  X  im-portant X  distributions for storage. When a new distributio n is detected, this set is updated if appropriate.

Note that these four procedures are independent of each other. This provides considerable flexibility as it is possi ble to plug in any change detection and mining technique, and change the distance function for pattern matching at any point if the application requirements are altered.
We maintain two windows on stream S : a count-based window W r and a time-based window W t . As mentioned in Section 1.1, since the distribution of the current stream cannot be directly extracted, we use a set of elements in this stream as a representative set to represent its distributio n. The substream S r in W r represents the current distribution, and substream S t in W t records the set of data elements that have arrived in the last  X  time units. We call W r the reference window and W t the observation window .

The observation window W t is implemented as a tum-bling window. Every  X  time units, W t  X  X umbles X  so that all the elements in it are deleted and a new empty window is opened. We implement W t as a tumbling window rather than the more common sliding window because of perfor-mance considerations. A time-based sliding window moves forward at each clock tick (i.e., when time moves forward), and, all items in the window with a timestamp less than t tection process is triggered every time W t moves, using a sliding window would greatly impact the performance when the stream has high arrival rate.

The size of reference window W r ( | W r | ) and the time in-terval  X  of observation window W t are pre-defined values. | W r | indicates the size of the representative set for each distribution. Hence, if many important distributions (we will precisely define important distributions in Section 3. 5) are expected during the lifespan of S , then | W r | should be smaller. However, intuitively, the larger the representat ive set S r , the closer its distribution is to the real distribution of S . For a given minimum accuracy requirement a , | W r should satisfy where P r and P S are the distributions of substream S r in W and the real current distribution in stream S , respectively. | W r | can be determined either manually or by running a set of training data at the beginning of S .

As mentioned above, the time interval  X  of W t indicates the frequency of triggering the change detection procedure . Smaller  X  values would mean more frequent change detec-tion, and thus, the number of delayed alarms will be fewer. On the other hand, smaller  X  values will reduce the effi-ciency of our change detection technique. Therefore,  X  valu e should be determined according to the minimum efficiency requirement of the relevant application.
Reference window W r is used to store a set of data el-ements that represent current distribution of S . Change detection can be done by comparing the distribution of the substream in W r and the distribution of the newly arrived data, i.e., the substream in W t . This is discussed in detail shortly.

Statistically, the more samples collected (i.e., the large r | W r | ), the better the underlying distribution can be de-scribed. However, in reality, the size of W r needs to be relatively small due to memory limitations. Furthermore, efficiency of distribution matching and change detection is related to the size of the sample sets. Therefore, for a min-ing application with high efficiency requirement, W r cannot be too large. However, using a small data set to represent a distribution could be highly inaccurate, especially when t he distribution is complicated.

In most of the existing work, representative set of the cur-rent distribution is set to be the substream in a window that starts when the distribution change is detected. This may be problematic since the new distribution may not be prop-erly captured within this window (unless the window is very large). Furthermore, for slow distribution changes, it may be a while until the new distribution is stabilized. Hence, the substream captured immediately after the distribution change cannot reflect the real distribution. Therefore, thi s substream is not suitable as a representative set of the cur-rent distribution in S .

Although the size of the representative set has to be lim-ited due to memory and efficiency concerns, if, instead of blindly using the first group of data that arrived after a new distribution is detected, we can carefully choose a sam-ple set with the same size from a large vault of samples, then the distribution of this selected set will be much close r to the true distribution. Based on this idea, we introduce the concept of dynamic reference window W r for solving the problem of representing a (complicated) distribution usin g a small sample set accurately. The overview of generating the dynamic reference window W r is as follows. We concatenate/merge windows W r and W t so that a larger substream with at most | W r | + | W t ements is available. We then carefully select | W r | elements from the concatenated window W r + W t as the latest rep-resentative set, and replace the substream in W r by this new set 1 . This merge-and-select process is triggered every time W t tumbles. Hence, elements in W r can be regarded as a  X  X oncentration X  of all data from the current distribu-tion that have been observed so far. In other words, this representative set is a careful selection from the large vau lt of observed elements since the beginning of the distributio n. Thus, substream S r in W r is highly representative of the current distribution. This merge-and-select process is il lus-trated in Figure 2.

Let a distribution change be detected at time t 1 . Star-ing at t 1 , W r records a substream S r that contains the first | W r | observed elements. Observation window W t contains substream S t with the newest | W t | elements (Figure 2a). At time t 2 , W t is full and ready to tumble forward. If at this point a distribution change is not detected, then we  X  X erge X  the data elements in W t and W r to form a data set with size | W r + W t | , denoted as W t + W r as per Defin-ition 2 (Figure 2b). We then select | W r | elements (will be discussed in Section 3.2.3) from W t + W r , so that the se-lected data set can better represent the current distributi on (Figure 2c). In other words, for the substreams S r and S with distributions P r and P 0 r , respectively, P 0 r is more sim-ilar to the true underlying distribution P of stream S (i.e., dist ( P 0 r , P )  X  dist ( P r , P )). This merge-and-select process continues until the next time a distribution change occurs.
Ideally, we want to find the data set W 0 r in W r + W t , such that its distribution is the closest to the true distributio n of S , i.e., dist ( P 0 r , P ) = min  X  P i distribution of any substream in W r + W t with size | W r However, finding such a substream can be computationally expensive. For stream applications that require high effi-ciency, an approximate algorithm is required.

The goal for our approximation is to find a substream W 0 with size | W r | from W r + W t , such that:
To achieve this goal, we propose a two-step sampling ap-proach. We first estimate the density function of the dis-tribution of data set in W r + W t . The density function is estimated using the popular kernel density estimation [20] : where K is the kernel function , h is the smoothing parameter called bandwidth , and s i is a data element in W r + W t intuition of kernel density estimation is to replace each da ta element in the set with a smoothed  X  X ump X , whose width is determined by h . The resulting density function is the sum of all the bumps. In this paper, we set K to be the standard Gaussian function with mean zero and variance 1. This setting is usually chosen when no pre-knowledge of the distribution is available [22]. Thus,
Hence, equation (2) can be rewritten as
The bandwidth h is a value between 0 and 1. A large h setting is preferable if the distribution has a long lifespa n over the stream, whereas a small h value is suitable for a short trend. If the lifespan of current distribution is unpr e-dictable, then it is reasonable to start from a small setting (e.g., h = 0 . 1), and increase h each time W r is updated until the next distribution change is detected.

Although the kernel density estimation approach can gen-erate an estimated density function of the current distribu -tion, the accuracy of this estimation is usually low. This is because kernel density estimation assumes that the under-lying stream should have an approximately normal distrib-ution. For complex streams, this assumption does not hold. Performing a match by simply calculating the distance of estimated density introduces a large error (as demonstrate d in Sections 5.1 and 5.2). On the other hand, representative sets consist of elements directly extracted from the stream , and thus are more reliable. Hence, we only use kernel den-sity estimation as a preliminary guidance for our sampling strategy.

In the rest of the discussion, we consider only single-dimensional streams, i.e., the data element s in the stream contains only one attribute. Our technique can be extended to N -dimensional streams by replacing each single-attribute element s with a N -dimensional vector  X  s = ( s 1 , ..., s the complexity of the algorithm may increase considerably for streams with high dimensions.

We use Figure 3 to illustrate the idea of our two-step sampling approach. With the density function  X  f h ( s ), we are able to estimate the  X  X hape X  of the current distribution . x -axis is the value of the data s ( v ( s )) in W r + W t y -axis is the probability ( p ( v )) for all data values. Higher p ( v ) values indicate that these values occur more frequent in the stream, and thus more elements s with value v ( s ) should be selected into the new reference window W 0 r , and vice versa. Based on this insight, in the first step, we par-tition the whole area under the density function  X  f h ( s ) into k disjoint groups G 1 , G 2 , ..., G k , where k is a constant for each  X  f h ( s ) (we discuss k value selection shortly). The start and end value for each partition G i is denoted as g S i and g , respectively. Each partition has the same area, i.e., area ( G 1 ) = ... = area ( G k ) = 1 k area (  X  f h ( s )).
The second step is sampling from each partition. For two partitions G i and G j , if g E i  X  g S i &lt; g E j  X  g S value range for G i is  X  X arrower X  than G j ), then the aver-age probability of all values within partition G i is greater than the average probability of values within partition G If we select the same number of data elements with val-ues within G i and G j , respectively, then the selected values within range  X  g S j , g E j  X  are more  X  X parse X  than in  X  g Hence, the shape of this distribution is better captured by the data set selected using this technique. We use S Gi to rep-resent the elements selected from each partition G i . Since the resulting representative set is stored in W r , the total number of data selected from each partition should be | W
To partition W r + W t into a number of groups G i ( i = 1 , ..., k ), we adopt a machine learning method [26] that is used for clustering real-valued attributes into disjoint s ub-regions. We define a discretizer function Q for partition-ing. For each partition G i , every data element s with value v ( s )  X  G i maps (or rounds off) to a representative point s of G i 2 , such that: where 1 G In the optimal case, we want to find a discretizer function Q
O such that the error of mapping sample data to their representative point is minimized, i.e.,
Equation (6) implies that if the number of partitions is the same as the number of data elements in W r + W t , then each data will have itself as its representative point, and hence the distribution of the selected representative set will ha ve the same distribution as the data set in W r + W t . How-ever, since the number of data elements we will have in the new reference window W 0 r is fixed, we will always have k  X  | W r | &lt; | W r + W t | . Hence, this optimal goal will not be achieved in our approach.

Our proposed solution for partitioning W r + W t is as fol-lows. First, we calculate the start and end values ( g S i g ) for each partition G i . Using equations (4),(5),(6), we get the following equation (intermediate steps are omitted due to lack of space):
The solution of equation (7) yields ( g S i , g E i ) pairs for all partitions G i ( i = 1 , ..., k ).

On the second step, we select a number of representa-tive data from each partition. The number of representa-tive data selected from partition G i is determined by k i round ( | W r | /k ). If k 1 k i 6 = | W r | , then we add or remove several data from random groups to ensure | W 0 r | = | W
Within each partition G i , k i data are randomly selected with the indicator function 1 G
The random selection strategy is done to eliminate biases introduced through our kernel density estimation and par-tition process. The final substream S 0 r in the updated refer-ence window W 0 r is the union of all data elements selected from all groups, i.e., S 0 r = S G 1  X  S G 2  X  ...  X  S G k
The number of total partitions k is a predefined constant value for each distribution. A higher k value implies a bet-ter quality of the representative set selection by reducing err ( Q ). On the other hand, computation cost and memory consumption will also increase with higher k . Therefore, for a  X  X mooth X  distribution, i.e., fewer high density areas , or less  X  X umpy X  shape, k can be kept smaller, which means that W r + W t can be partitioned into fewer groups.
A review of change detection techniques for dynamic streams indicates that a large portion of proposed techniques are tightly associated with a mining technique. Hence, those change detection approaches cannot be applied to other min-ing applications, and they may not even perform well if the mining techniques are changed. We propose an online change detection technique for our DMM framework that is not restricted to a specific stream processing applicatio n. This is useful since DMM is a general-purpose method.
As mentioned in Section 3.1, every time W t tumbles, the change detection procedure is triggered. We compare the distributions of substreams in both W r and W t windows. By Definition 1, we evaluate dist ( P r , P t ). If dist ( P  X  , where  X  is the predefined maximum matching distance, then a distribution change is flagged. The choice of distance functions used for calculating dist ( P r , P t ) will be discussed in detail in Section 4.
When a new distribution is detected at time t 1 , we choose a set of data elements as the sample data set representing this new distribution. For applications that require high e ffi-ciency, we directly use the substream in W t from timestamp t to t 1 +  X , so that the matching can start as soon as pos-sible. However, as discussed in Section 3.2.1, the first set o f data that arrive at the beginning of a new distribution may not capture the true distribution, especially when the dis-tribution is complicated or the distribution change is slow . Therefore, if the accuracy is more important than efficiency, we observe the new distribution for a longer time (i.e., wait until W t tumbles several times) and refine the representa-tive set by using the same merging technique discussed in Section 3.2.

This new distribution is then matched with a set of impor-tant historical distributions that have been preserved. Th e representative sets of these distributions are sequences, and thus, we use the appropriate distance measure to check their similarity. If a match is found, then the preserved mining re -sults for the stream with the matching distribution is outpu t as the mining results for the new distribution. The justifica -tion is that for two highly similar distributions, their min ing results should present high similarity as well. This way, th e data mining time is dramatically reduced without reducing the quality of mining results.

The maximum matching distance  X  is important: smaller  X  implies a higher accuracy of the two matching distribu-tions, while a larger  X  increases the possibility of a new dis-tribution to match a pattern in the preserved set leading to higher efficiency, since the time for matching distributions and reusing mining results are far less than the time for re-mining the new distribution. Therefore, the question of finding a balance between accuracy and efficiency for setting  X  value arises. The impact of  X  is empirically studied in Sec-tion 5.5. Our proposal for selecting  X  is to initially start with  X  = 0 and increase it until the accuracy of matching reaches the minimum accuracy requirement of the application.
For a dynamic data stream, there could be a large number of different distributions that are observed during the life s-pan of the stream. Due to limited memory, it is infeasible to record all of these distributions along with their mining results. Furthermore, maintaining a large number of distri -butions could increase the time it takes to match a newly detected distribution. Hence, only important distributio ns, i.e., the ones that have a high probability to be observed again in the future, should be archived. In this section, we discuss how to determine the important distribution set, de -noted by P .

We use the following heuristic rules to determine the im-portance of distributions: 1. Distributions that have occurred in the stream for more 2. The longer a distribution lasts in the stream X  X  lifespan, 3. The more distinctive a distribution is, the higher is 4. A distribution P i that has mining results R i with higher
When a distribution change is detected, whether or not a match is found, the important distribution set P is updated. The distribution P r that was in effect when a change was detected is evaluated to determine whether it should be in-cluded in P . If P r had been matched with pattern P i  X  P , then P r replaces P i in P if its lifespan is longer than P lifespan (rule 2). If P r has no matching distribution in P and P has not reached its maximum memory allowance, we add P r to P . Otherwise, the distribution that is the least important is pruned from P according to rules 1-4.
Both the change detection and distribution matching phases of DMM rely on computing the distance between distribu-tions. A number of distance functions have been proposed in literature for both distribution matching and time serie s pattern matching. Examples include L 1 -norm (also known as Manhattan distance ), L 2 -norm (also known as Euclidean distance ), Dynamic Time Wrapping (DTW) [21], Longest Common Subsequence (LCSS) [23], Edit Distance on Real sequence (EDR) [7], and Relativized Discrepancy (RD) [15]. For most of these distance functions, there is no proof of lower bound accuracy, nor is there a systematic comparison that would suggest the appropriate distance function for a given application when distribution matching (as defined in Definition 1) is performed.

L 1 -norm and L 2 -norm do not have the stretching abil-ity, i.e., they require the two matching substreams to have the same length. Therefore, they are very inflexible and cannot be applied to many stream applications such as the applications using time-based sliding window model. This is because the length of a time-based window is not a con-stant; thus there is no guarantee that two time-based win-dows will always have the same number of data items. On the other hand, L 1 -norm and L 2 -norm are the only two dis-tance functions in the list that are metric, and, therefore, computationally more efficient to compute. LCSS and EDR are robust to noise, making them suitable for mining applica -tions that are not sensitive to noise. L 1 -norm and L 2 -norm are the most efficient distance functions and RD is the one with highest time complexity. There have been individual studies that have analyzed the accuracy of these techniques , but the results, over a number of data sets, are inconclu-sive. However, general patterns indicate that L 1 -norm and L -norm are usually the least accurate, in particular when there is noise.

A proper distance function that can be plugged into DMM should be efficient (preferably linear or quadratic complex-ity), with the ability of stretching (i.e., the ability of ca l-culating the distance between two substreams with differ-ent length). The noise tolerance ability may or may not be preferable depending on the nature of the stream. For example, for ECG streams, outliers/noise are critical info r-mation that must not be ignored, whereas in temperature readings, occasional noise is usually caused by sensor mal-function, and hence should be eliminated when detecting distribution changes or performing distribution matching .
According to these criteria, DTW, LCSS, EDR and RD are all reasonable distance functions to be used in the DMM framework. To evaluate the accuracy of our technique with these distance functions plugged in, we run a set of experi-ments using five synthetic data streams.

Each data stream contains 1,000,000 points with only one numerical attribute for each sample element. The distrib-ution changes occur every 10,000 points. Hence, there are 99 actual changes in each stream. The arrival speed of the stream is stable, with one tuple per unit time. This is for the purpose of gaining control over the window X  X  length, since a time-based sliding window will be equal to a count-based one when the stream speed is stable. However, keep in mind that our technique does not require the stream to have an even speed. Stream S 1 has a normal distribution, and S 2 a mixture of normal distribution with some uniform noise. Streams S 3 , S 4 , and S 5 contain exponential, binomial and poisson distributions, respectively.

We set the size of W r to 500 data items, and the time interval  X  for W t to 200 time units. The maximum matching distance  X  is set to 5% (impact of  X  is studied in Section 5.5). We analyzed all data sets manually and drew the curves of the streams using Matlab to visually observe changes. We manually determined all the changes under the given threshold setting. This determines the ground truth. A mismatch indicates that the compared substreams do not have the same distribution. The recall (R) and precision (P) of our change-detection technique using each distance function are presented in Table 2.

These results indicate that DTW and LCSS have high re-call but very low precision, indicating large number of fals e-alarms. The precision of EDR is satisfying, but the recall is slightly lower than RD. RD has the best overall perfor-mance, with the trade-off of a higher computational cost.
In this section, we present a series of experiments using both synthetic and real data streams to evaluate the pro-posed DMM framework. Our experiments are carried out on a PC with 3GHz Pentium 4 processor and 1GB of RAM, running Windows XP. All algorithms are implemented in C.
We compare our DMM approach with two other generic change detection techniques: the kernel density approach (KD) [1], and another distance function-based approach (DF ) [15]. The Gaussian kernel function used in KD is based on the authors X  recommendation [1]. Several detection strate -gies using different statistics are discussed in [15]. Accor ding to their experiments, the  X  statistics proposed in their paper has the overall best performance. Thus, in our experiments, we only implement DF using  X  statistics.

We use the same test data streams S 1  X  S 5 as in Section 4. The parameter settings are also the same, i.e., | W r | = 500 data items,  X  = 200 time units, and  X  = 5%. Any changes reported after 200 time units (i.e., after the time W t tumbles), from the beginning of the change is considered late. Since DF uses RD as its distance function, we also adopt RD in our change detection technique. The recall (R) and precision (P) of the three techniques are shown in Table 3. The number of on-time (O) and delayed (D) changes detected are presented in Table 4.

These results demonstrate that our proposed change de-tection technique outperforms both DF and KD in terms of precision and recall. Furthermore, DF and KD suffer from a drop in recall, with KD X  X  drop more significant, when the testing stream has a non-normal distribution. The perfor-mance decrease in KD is caused by its assumption that the underlying stream should have approximately normal distri -bution. When this assumption does not hold, performance suffers. The slight drop in DF X  X  recall may be due to the bad representative set selection for complicated distributio ns.
Distribution drifts refer to the slow and gradual changes in the stream, while distribution shifts refer to significan t sudden changes. To study the performance of the DMM, DF, and KD change detection approaches over shifts and drifts, we modify streams S 1 and S 3 to generate four new streams. Streams S 6 and S 7 are the ones with normal distri-bution, while streams S 8 and S 9 follow exponential distrib-ution. In S 6 and S 8 there are only distribution drifts, while in S 7 and S 9 , only distribution shifts occur. The parame-ter settings are the same as those reported in the previous subsection. Table 5 displays the recall and precision of the three techniques over these four streams, and Table 6 shows the number of on-time and delayed detection.

These results suggest that, in general, distribution shift s are easier to detect than distribution drifts, because the Table 5: Recall and precision comparison for distri-bution drifts and shifts Table 6: On-time and delayed comparison for distri-bution drifts and shifts changes are more significant. Our approach greatly outper-forms KD and DF for detecting drifts. This is because, the process of kernel density estimation in KD and representa-tive set selection in DF are performed immediately after a distribution change is detected. In the case of drifts that a re slow by definition, a new distribution will take a long time to stabilize, and, thus, kernel density estimation and repr e-sentative set used in KD and DF are highly unreliable. On the other hand, DMM keeps updating the representative set, and, hence, the next time a distribution change occurs, the reference window contains a substream that can accurately represent the current distribution for comparison.
To evaluate the performance of our distribution matching approach, we carried out a series of experiments on a real data set that exhibits periodically changing distribution s. The data set we use is copyrighted data from the Tropical Atmosphere Ocean (TAO) project. These streams record the sea surface temperature from past decades. Detailed in-formation about this project and the real-time data sets can be found in [17]. The distribution of this stream demon-strates both shifts and drifts. The data set contains 12,218 streams each with a length of 962. Each stream contains readings over different time period. Since these streams are too short, we sorted them by the time period these streams are recorded, and concatenate them to obtain one stream with 11,753,716 data points continuous over years. We an-alyzed Tao manually and drew the curves of the streams using Matlab to visually observe changes. There are a total of 3244 distribution changes in this stream. This determine s the ground truth.
 The arrival rate is set to be one tuple per unit time. Since TAO stream consists of some rapid distribution shifts, the efficiency requirement is high. Hence, we set the reference set size | W r | to 300 data items, and tumbling time interval  X  to 100 time units. Since this stream only contains numer-ical values, assuming each data element takes 1 byte, only 300 bytes are needed for archiving each distribution patter n. Even if there is no periodic distribution in the TAO stream, it only takes less than 1MB memory to store all the distri-butions. However, considering that the mining results will need to be stored along with each distribution, we limit the important distribution list P to 100 distributions.
The results of change detection are shown in Table 7, and the results for periodic distribution matching are shown in Table 8. Table 8: Distribution matching in TAO stream
These results demonstrate that periodically changing dis-tributions do exist in real-world data streams, and our DMM of distribution change detection and matching performs wel l on real data sets.
To demonstrate that the proposed DMM framework can increase the overall efficiency for mining periodically chan g-ing streams, we apply a stream mining application on the TAO stream introduced in the previous section.

For a temperature monitoring stream, a popular mining task is to cluster the readings for further analysis and com-parison. Hence, we adopt a popular decision tree-based clus -tering technique, VFDT [8], to cluster the temperature read -ings in TAO stream. VFDT is recognized as one of the best decision tree generation technique for dynamic data stream s. Many other decision tree techniques cannot deal with the ever-changing distributions and cannot handle continuous numerical values. Every time a distribution changes, the existing decision tree may not be suitable for the new dis-tribution in terms of both accuracy and efficiency. Hence, different decision trees are required for clustering data se ts with different distributions. Rebuilding a decision tree ca n be time consuming. Many newly arrived data elements will be missed during the reconstruction process. However, if th e decision trees for important distributions (i.e., distrib utions that may occur periodically) can be stored and reused, then the overall execution time for clustering TAO stream can be greatly reduced.

Table 9 verifies this insight. DMM reduces total execution time by 31 . 3% (including the time for updating reference window, change-detection, matching and mining).

We did not conduct experiments to test the accuracy of the reused mining results. This is because the accuracy of the reused mining results is solely determined by the accu-racy of the mining technique and the accuracy of our match-ing approach. Any existing mining technique can be plugged into our framework. Therefore, since our matching tech-nique demonstrates a promising accuracy rate, the accuracy of the reused mining results should be close to the accuracy of the adopted mining technique. Testing the accuracy of reuse is not the major point of the paper; the development of the framework and the techniques that enable the reuse of (any) mining technique is the main focus.
As discussed in Section 3.4, the choice of the maximum matching distance  X  can affect both the accuracy and effi-ciency of DMM. To study the effect of  X  , we repeat the ex-periments of DMM on all data sets we introduced in previous sections using different  X  settings. The empirical results are shown in Tables 10  X  13.

These results indicate that with a larger  X  value, fewer dis-tribution changes will be detected. Two distributions with 85% similarity will be considered the same with  X  = 15%, whereas a distribution change will be detected if two distri -butions have less than 98% similarity with  X  = 2%. Hence, fewer distribution changes exist in a stream with larger  X  set-tings. More distribution matches are reported with a larger  X  value, which may lead to a reduction of the overall min-ing time, since archived mining results can be used more frequently. However, the quality of the mining results may drop with increasing  X  values, since the matches are less accurate.

Detecting changes in data streams and adjusting stream mining models accordingly is a challenging issue and has only been recognized as an important one in the past few years. A number of techniques have been proposed for solv-ing these problem [6, 12, 14, 24], but most of them are ad-hoc and cannot be generalized to different mining applications. These change detection technique usually only perform well on certain types of streams.

Aggarwal addresses the data stream change detection prob-lem by providing a framework that uses velocity density es-timation [1]. By estimating the rate at which the changes in the data density occur, the user will be able to analyze the changes in data over different time horizons. A similar technique using density estimation is also proposed for de-tecting outliers in sensor data [18]. However, as mentioned in Section 3.2, without any pre-knowledge of the stream, density estimation can be highly inaccurate, leading to a large number of false and missed changes.

Kifer et al. present another approach to change detection that is not restricted to certain stream mining approaches [15]. Their idea is similar to ours in that two sliding window s are used over a data stream to capture the previous and current distributions. The distribution change is detecte d by calculating the distances between these two distributio ns using RD distance function. However, in their approach, a fixed window is used to maintain a reference to the orig-inal distribution (named  X  X aseline window X ). The window contains the first p elements of the stream that occurred im-mediately after the last detected change. As mentioned in Section 3.2, the first small  X  X hunk X  of data observed after th e change point is usually not representative to the true distr i-bution, especially when the distribution change is slow.
Similarity matching over distributions is a novel idea that has not been proposed previously. However, in time se-ries analysis, similarity pattern matching is well studied . Time sequences can be regarded as simplified data streams, where each data s in stream S usually only contains a single value/field. Hence, some of the strategies for subsequence matching in time series [3, 9, 13, 16, 27] could contribute to distribution matching in data streams.

Ge and Zdonik propose a technique for handling uncertain data in array databases using a similar idea of partitioning distribution [11]. Although a part of their idea is similar t o ours, their focus is entirely different. They consider the al -ready partitioned distribution as an available input. How t o obtain the distribution of a large data set or how to partitio n efficiently is not discussed. These problems are not trivial and no existing work can be adopted directly. By contrast, our input is the raw data observed from a continuously ar-riving stream, and hence the problem is more complicated.
In this paper, we propose a framework, called DMM, for mining data streams with periodically changing distribu-tions. In this framework, we incorporate a novel method for matching new distribution with historical patterns and reuse existing mining results. DMM is flexible, and can be applied to many types of data streams and mining applica-tions. It is shown in the experiments that DMM can greatly reduce the overall mining time over dynamic streams with periodic distribution changes.

In order to detect and match distributions with high accu-racy and efficiency, two tumbling windows are applied that contain two substreams representing the current and new distributions of the stream. An intelligent merge-and-sel ect sampling approach is proposed that can dynamically update the reference window. The representative set generated by this approach is highly representative of the true distribu -tion. Thus, the accuracy of our change detection and match-ing process is greatly improved.

Our work along these lines continue in a number of direc-tions. One issue we are investigating is a method to auto-matically determine the number of partitions k for dynamic reference window selection rather than fixing it a priori. An -other issue is automatically determining threshold  X  and the size of window W r . Finally, it may be interesting to design specific distance functions for matching distributions in s pe-cial types of streams.
