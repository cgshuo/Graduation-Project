 TETSUYA SAKAI Knowledge Media Laboratory, Toshiba Corporate R&amp;D Center 1. INTRODUCTION
Question answering (QA) has received a lot of attention in recent years, but researchers are still exploring how best to evaluate QA. Factoid, list, and defini-tional questions were included in a single task at the TREC 2003 QA track, but different metrics were used for different questions types: the fraction of correct responses was used for the factoid questions (as systems were required to return a single answer for each question), while instance-and nugget-based variations of F-measure (i.e., harmonic mean of recall and precision) were used for the list questions and definitional questions, respectively [Voorhees 2004]. The TREC 2004 QA track used similar evaluation methods for series of questions [Voorhees 2005]. Whereas, at the  X  X A@CLEF-2004 X  Multilingual QA Workshop [Magnini et al. 2004], it was argued that ranked lists of exact answers may be more use-ful than single answers from the user X  X  point of view. At the NTCIR Japanese
QA tasks (QAC1 and 2), reciprocal rank (RR) was used for evaluating ranked lists containing up to five exact answers in Subtask 1 (one-off factoid task), while instance-based F-measure was used for evaluating sets of exact answers in subtasks 2 (list task) and 3 (context task) [Fukumoto et al. 2004]. At NTCIR-4 QAC2, Sakai [2004a] proposed the use of Q-measure for subtask 1 in order to deal with multiple correct answers and answer correctness levels , none of which can be handled by RR.

This paper discusses the reliability of some existing QA metrics based on the QAC2 task settings. By reliability, we mean the stability of pairwise system comparisons with respect to change in the question set used in the evaluation, as well as the resultant sensitivity for discriminating two systems with confi-dence. In order to compare the QA metrics from these viewpoints, we adopt the stability method proposed at ACM SIGIR 2000 [Buckley and Voorhees 2000] and the swap method proposed at ACM SIGIR 2002 [Voorhees and Buckley 2002]. 1 We primarily focus on subtask 1, where systems were required to re-turn ranked lists of exact answers, and examine RR, the fraction (or number) correct5 and NQcorrect1), and Sakai X  X  Q-measure. In addition, we examine
F-measure for QAC2 subtask 2 (i.e., the list task) for a brief comparison with subtask 1.

The remainder of this paper is organized as follows. Section 2 describes the stability and swap methods, which we use, for comparing QA effectiveness met-rics. Section 3 discusses the stability and sensitivity of RR, NQcorrect5, and
NQcorrect1, based on the formal run results at QAC2 subtask 1. Section 4 compares the stability and sensitivity of Q-measure with the above  X  X fficial X  metrics, using our own set of QAC2 subtask 1 results. (This is because we can-not compute Q-measure for the entire set of formal runs, as only the RR values, not the actual system output files, are available to us. However, we can still compare Q-measure with the  X  X fficial X  metrics under the same conditions.) Sec-tion 5 briefly discusses the stability and sensitivity of F-measure, based on the formal run results at QAC2 subtask 2. Section 6 mentions some work related to the present study; Section 7 concludes this paper. In addition, the Appendix contains a brief description of a Japanese QA system used in the experiments described in Section 4. 2. RELIABILITY ASSESSMENT METHODS test collections and effectiveness metrics. The stability method [Buckley and
Voorhees 2000] computes the stability of an experiment with respect to change in the topic set, given an effectiveness metric M , a test collection, and a set of runs submitted to a particular task defined by the collection. The swap method [Voorhees and Buckley 2002] derives the minimum performance dif-ference (as measured by M ) required in order to conclude that a run is better than another with a given  X  X onfidence level, X  given a test collection and a set of runs. 2.1 Stability Method
The essence of the stability method is to compare systems x and y in terms of metric M using B different topic sets and count how often x outperforms y , how often y outperforms x , and how often the two are regarded as equivalent.
We let B = 1000 throughout this paper. Let S denote a set of systems (i.e., runs) submitted to a particular task, and let x and y denote a pair of systems from
S . Let Q denote the entire set of questions used in the task, and let c denote a constant. Let M ( x , Q i ) denote the value of metric M for System x averaged over a topic set Q i (  X  Q ). Then, using the algorithm shown in Figure 1, the minority rate ( MR ) and the proportion of ties ( PT )of M , given a fuzziness value f , can be computed as:
From the algorithm, it is clear that GT M ( x , y ) + GT M ing a wrong conclusion about a pair of runs using a given metric, while the performance metric, both of these values should be small. Note, for example, that a large fuzziness value yields large EQ M ( x , y ) values and, therefore, a large proportion of ties and a small minority rate. As a fixed fuzziness value implies different trade-offs for different metrics, we vary the fuzziness value ( f = 0 . 01, 0 . 02, ... ,0 . 10) and draw MR-PT curves for comparing the stability of different metrics [Sakai 2006b, 2006d]. 2.2 Swap Method
The essence of the swap method is to estimate the swap rate , which represents the probability of the event that two experiments, using two completely different sets of topics, are contradictory given an overall performance difference in terms of metric M . This method can be used to compare the sensitivity of different metrics.

Let d denote a performance difference between two systems. The swap method begins by defining 21 performance difference bins , where the first bin represents performance differences such that 0  X  d &lt; 0 those such that 0 . 20  X  d . Let BIN ( d ) denote a mapping from a difference d to one of the 21 bins where it belongs. The algorithm shown in Figure 2 calculates the swap rate for each bin. Our test is stricter than the original one by Voorhees and Buckley [2002], in that our  X  X wap X  includes cases in which one of d and d M ( Q i ) is zero. This is because Voorhees/Buckley X  X  original test, which in-crements the swap count only when one of the differences is positive and the other is negative, tends to underrate the swap rates for near-zero bins as the differences are actually quite often zero. We have verified that this modification gives graphs that look more stable, but do not affect our conclusions.
Both the stability and the swap method use sampling without replacement from the original topic set Q . That is, no duplicate topics are allowed within each
Q (and Q i ). Moreover, the swap method ensures that Q i and Q because, as we have mentioned earlier, Voorhees and Buckley focused on the worst-case scenario in which the two topic sets are completely different. Note that this disjointness requirement implies that the resampled topic sets Q
Q can only be up to one-half the size of the original topic set Q . Voorhees and
Buckley [2002] have used extrapolation for larger topic set sizes, but we stick to the statistics actually measured in our study, as our objective is to compare the reliability of different metrics under the same conditions. dence level X  (e.g., 95%), we can plot the swap rate (i.e., 1 minus the  X  X onfidence level X ) against the performance difference bins, so that the minimum difference required to guarantee that a system is better than another at that confidence level can be obtained. 3. QAC2 SUBTASK 1: RECIPROCAL RANK, NQCORRECT5, AND
This section examines the results of NTCIR-4 QAC2 subtask 1 [Fukumoto et al. 2004], where systems returned a ranked list containing up to five exact answers for each of the 195 formal run questions. Our analyses here are based on the RR values of the 25 systems submitted to this task, which were included in the
CD-ROM distributed at the NTCIR-4 Workshop. RR is defined as 1 r is the rank of the first correct response within the answer list, or zero if the answer list does not contain a correct answer at all. Thus, for subtask 1, RR is either 1, 1 / 2, 1 / 3, 1 / 4, 1 / 5or0.

The QAC2 organizers have considered the fraction (or number) of questions with a correct answer within top 5 (1) as alternative metrics [Fukumoto et al. 2004], which we denote by NQcorrect5 and NQcorrect1, respectively. However, these metrics are merely coarse reductions of RR: NQcorrect5 is 1 if RR 0 otherwise; NQcorrect1 is 1 if RR = 1 and 0 otherwise. Thus, we can derive NQ-correct5 and NQcorrect1 values from the RR values included in the NTCIR-4
CD-ROM. It is self-evident that NQcorrect5 and NQcorrect1 are highly cor-measure used for the factoid questions at the TREC QA track [Voorhees 2004].
Figure 3 shows the mean RR (MRR), mean NQcorrect5, and mean NQcor-rect1 values for the 25 formal runs. The runs have been sorted by MRR. It can be observed that the system rankings according to the three metrics are simi-lar, but not identical. In terms of MRR, 68% of the 25  X  24 are significantly different at  X  = 0 . 01 and an additional 7% are significantly different at  X  = 0 . 05 (two-sided sign test).

As there are | Q |= 195 official questions, up to c = 97( be resampled for the swap method. Recall that this is because the resampled topic sets Q i and Q i must be disjoint (see Section 2.2). We, therefore, used c = 97 for the stability method as well, to share the resampled topic sets across these two methods. In addition, for the swap method, we tried, c questions we address here is:  X  X hen using approximately 100 (50) questions for comparing systems submitted to QAC2 subtask 1, how stable and sensitive are
RR, NQcorrect5, and NQcorrect1? X  Note that the absolute values in the results reported below depend on the test collection and the set of runs. What we are interested here is how different QA metrics compare to one another in terms of stability and sensitivity under the same experimental conditions. Based on the stability method, Figure 4 shows the MR-PT curves for RR,
NQcorrect5 and NQcorrect1, when c = 97. The horizontal line indicates when the minority rate is 1%. Recall that good metrics should have small MR and small PT values. These results suggest that, not surprisingly, RR is more stable than NQcorrect5, which, in turn, is more stable than NQcorrect1.

Based on the swap method, Figures 5 and 6 plot the swap rates against the performance difference bins for RR, NQcorrect5, and NQcorrect1 when c and c = 50, respectively. The horizontal line indicates when the swap rate
NQcorrect1 are consistently less reliable than RR. For example, in Figure 6 (representing evaluation with only 50 topics), when the overall performance difference between two systems is around 0.17, the swap rate of RR is approxi-mately 5%, while that of NQcorrect1 is approximately 19%. That is, even when there is an overall difference of 0.17 in NQcorrect1 in an experiment, there is approximately 19% chance that a contradictory result would be obtained in a different experiment.

Based on Figures 5 and 6, Table I shows the sensitivity of the metrics at 95% confidence level. For example, if RR is being used with only 97 (i.e., roughly 100) questions to compare the QAC2 subtask 1 formal runs, one should look for a performance difference of at least 0.12 in order to conclude that system x is better than System y with 95% confidence. As the maximum MRR observed among the 50,000 values (1000 trials for 25 systems, each with 2 disjoint topic sets) is 0.7156, this translates to a relative difference of 17%. Of the 300,000 comparisons (1000 trials for 300 system pairs), 59.8% actually had this differ-ence. Thus, the last column of this table represents the sensitivity of a metric.
It can be observed that, for discriminating the systems submitted to QAC2 sub-task 1, using NQcorrect5 instead of RR may suffice. This suggests that most of the differences among the submitted systems arise from whether they managed to include a correct answer somewhere in the list, rather than where in the list the correct answer was.

Table I also shows that NQcorrect1 (i.e., looking at the first response only) is clearly less useful for system discrimination. This is only natural, as NQ-correct1 is completely unaware of what is happening between ranks 2 and 5.
One way to improve the situation with NQcorrect1 is to use a very large topic set (e.g., hundreds of questions), as the TREC 2003 QA track did with factoid questions [Voorhees 2004].
 To sum up, RR is more reliable than NQcorrect5 and NQcorrect1, although
NQcorrect5 may have been sufficient for discriminating the particular 25 sys-tems submitted to QAC2 subtask 1. 4. QAC2 SUBTASK 1: Q-MEASURE, RECIPROCAL RANK, NQCORRECT5,
Section 3 used the formal runs results at QAC2 subtask 1 to verify that RR is more reliable than NQcorrect1 and NQcorrect5. This section examines Q-measure [Sakai 2004a] using similar methods, but uses our own set of runs generated for QAC2 Subtask 1. As was mentioned earlier, this is because we cannot compute Q-measure values for the entire set of runs submitted to QAC2
Subtask 1, since only the RR values of these runs are available to us. As our own runs are generated using a single system (though with substantially different parameter settings), the absolute values (e.g., the minority/swap rates) obtained here may be of little use. Note also that while Section 3 used 25 runs, here we use only 10 runs, which inevitably makes the results less reliable. Nevertheless, this experimental setting should suffice for comparing the stability and sensitivity of different metrics under the same conditions.

Section 4.1 first defines Q-measure as an information retrieval (IR) metric and then describes how it can be applied to QA evaluation involving ranked lists of exact answers [Sakai 2004a]. Section 4.2 describes how we prepared our own set of runs for comparing the stability of Q-measure with the  X  X fficial X  metrics.
Section 4.3 presents the experimental results and provides discussions. 4.1 Q-Measure and Its Application to QA
Q-measure is basically an IR metric, based on graded relevance [Sakai 2004a, 2006b, 2006d]. However, by assigning correctness levels to answer strings (just like assigning relevance levels to documents in IR) and defining answer equiva-lence classes for penalizing inclusion of duplicates in the ranked list, Q-measure can be applied to QA evaluation [Sakai 2004a; Sakai et al. 2004a]. Thus, unlike
RR, Q-measure can handle multiple correct answers and answer correctness levels. The question is how Q-measure compares to RR in terms of stability and sensitivity.
 We first formalize Q-measure as an IR metric based on graded relevance. Let R denote the number of relevant documents for a topic (i.e., question) and let
L (  X  L ) denote the size of a ranked output, where L is the maximum output size allowed. Let count ( r ) denote the number of relevant documents within top r ( Let R ( L ) denote the number of L -relevant documents so that let gain ( L ) denote the gain value (i.e., reward) for retrieving an lingual test collections, which have S-(highly relevant), A-(relevant), and B-relevant (partially relevant) documents, we could let gain ( S ) and Kek  X  al  X  ainen 2002] at rank r of the system X  X  output, where g ( i ) if the document at rank i is L -relevant and g ( i ) = 0 otherwise. In particular, consider an ideal ranked output, such that isrel ( r ) = 1 for 1 rank r . For example, an ideal ranked output for the NTCIR test collections have all S-, A-, and B-relevant documents listed up exhaustively in this order. Q-measure is then defined as follows.
 where BR ( r )isthe blended ratio 3 given by: Q-measure is equal to one if, and only if, the system output is an ideal one;
Q-measure with small gain values behaves like the binary average precision; moreover, Q-measure with flat gain values (e.g., gain ( S ) 1) is equal to average precision if, and only if, there is no relevant document below rank r of the system output. For more details on Q-measure as an IR metric, we refer the reader to Sakai X  X  papers [Sakai 2004a, 2006b, 2006d].
We now describe how to apply Q-measure to QA evaluation involving ranked lists of exact answers. The difficulty of QA evaluation lies in the fact that ar-bitrary answer strings need to be evaluated, in contrast to an IR situation in which only a closed-class, unique document IDs need to be evaluated. To tackle this problem, Sakai [2004a] proposed to prepare answer equivalence classes at the time of QA test collection construction. Using answer equivalence classes, we can handle both  X  X ingle- X  and  X  X ultiple-answer X  questions in an answer-ranking task, and can avoid rewarding systems that return duplicate answers. (In fact, the QAC1 and QAC2 answer files already contain equivalence class data for computing instance-based F-measure. Thus, no extra effort is required for this.) Sakai X  X  second proposal was to assign a correctness level to each an-swer string within each answer equivalence class, so that we can distinguish between  X  X ood X  answers and the  X  X ad X  (but somewhat correct) ones.

Let AS ( i )(1  X  i  X  R ) denote an answer equivalence class and let a ( i , j ) of a ( i , j ) and let L ( i ) = max j l ( i , j ). That is, within AS ( i ). We then define R ( L ) as the number of answer equivalence classes such that L ( i ) = L . We thus extend the NTCIR document relevance levels to answer correctness levels, so that R ( S ) + R ( A ) + R ( B ) Below, we provide some examples (in English translations) of how to prepare
QA test collections in this way. More examples based on the actual QAC2 answer file can be found elsewhere Sakai et al. [2004a].
 Example 1 Q:  X  X ho played in The Beatles? X  ( R = R ( S ) = 4)
AS (1) ={ &lt;  X  X ir Paul McCartney, X  S &gt; , &lt;  X  X aul McCartney, X  S
A &gt; , &lt;  X  X aul, X  B &gt; }
AS (2) ={ &lt;  X  X ohn Lennon, X  S &gt; , &lt;  X  X ennon, X  A &gt;
AS (3) ={ &lt;  X  X eorge Harrison, X  S &gt; , &lt;  X  X arrison, X  A
AS (4) ={ &lt;  X  X ingo Starr, X  S &gt; , &lt;  X  X tarr, X  A &gt;
Some test collection constructors may prefer to add more answer equivalence classes with relatively low correctness levels, representing early/temporary members of The Beatles, such as:
AS (5) ={ &lt;  X  X tuart Sutcliffe, X  B &gt; , &lt;  X  X utcliffe, X  B
If the fifth answer equivalence class is added, then R ( B )
R = R ( S ) + R ( B ) = 5.
 Example 2 Q:  X  X hat does DVD stand for? X  ( R = R ( S ) = 1)
AS (1) ={ &lt;  X  X igital Versatile Disk, X  S &gt; , &lt;  X  X igital Video Disk, X  A
If a system that returns both of the above answer strings is preferrable, then the above data should be broken into two separate answer equivalence classes. Example 3 Q:  X  X hat is love? X  ( R = R ( A ) = 1) AS (1) ={ &lt;  X  X IL, X  A &gt; } The answer data for NIL questions should be prepared as above, although the
QAC2 question set used in this study does not contain NIL questions. The correctness level of the NIL answer does not affect the value of Q-measure, as we shall see later.

One may argue that, for some questions, answer correctness judgments may be much more difficult than traditional document relevance judgments using graded relevance. However, for such questions, assigning flat correctness lev-els should suffice (e.g., treating all answer strings as A-relevant). That is, the assignment of answer correctness levels is not mandatory for every question.
Figure 7 shows an algorithm that reads a ranked list of answers and marks the correct ones with S, A, or B, but avoids marking duplicate answers from the same answer equivalence class. Once this is done, Q-measure can be computed by treating the above marked answer file as if it is a ranked list of documents with relevance level tags. (Figure 7 also includes a special treatment of NIL answers: only a NIL answer at rank 1 is marked as correct, in contrast to the
TREC 2001 evaluation in which systems could be rewarded for including  X  X IL X  somewhere in the ranked list [Voorhees 2002].)
Let us return to Example 1 (without the fifth answer equivalence class), and suppose that the system output was ( X  X cCartney, X   X  X ennon, X   X  X aul, X   X  X eorge Harrison, X   X  X tarr X ). Then, ( g (1), g (2), ... ) = because  X  X cCartney X  is at Rank 1. Whereas, an example ideal ranked out-put for this question would be ( X  X aul McCartney, X   X  X ohn Lennon, X   X  X eorge
Harrison, X   X  X ingo Starr X ), so that ( cg I (1), cg I (2), ... fore, Q -measure = ( BR (1) + BR (2) + BR (4) + BR (5)) / 2) / (6 + 2) + (7 + 3) / (12 + 4) + (9 + 4) / (12 + 5)) / 4 =
For Example 3 (where  X  X IL X  is regarded as A -correct), if the system correctly returns  X  X IL X  at rank 1, then ( g (1), g (2), ... ) = (2, 0, (2, 2, ... ). Whereas, ( cg I (1), cg I (2), ... ) = (2, 2, 1 = (2 + 1) / (2 + 1) = 1. In general, if R = 1 and the answer at rank 1 is correct, then both cg (1) = cg I (1) and count (1) = 1 hold and, hence, BR (1) fore, the NIL answer at rank 1 would receive a Q-measure of 1.0 regardless of whether it is treated as S-, A-or B-correct. 4.2 Ten ASKMi Runs
The ASKMi Japanese QA system Sakai et al. [2004b] was used to generate 10 runs for QAC2 subtask 1 with 195 questions. The relevant features of ASKMi can be found in the Appendix, although it suffices to treat ASKMi as a black-box QA system for the purpose of this study. The 10 ASKMi runs, in order of decreasing MRR, are: 1. An oracle run, with both correct answer types and correct supporting docu-2. An oracle run, with only the correct supporting documents given to the 3. An oracle run, with only the correct answer types given to the system from 4. A formal run actually submitted to QAC2, called TSB-A Sakai et al. [2004a]. 5. This is the same as TSB-A , except that the document score parameter P 6. This is the same as TSB-A , except that the Okapi/BM25 parameter b was 7. This is the same as TSB-A , except that the answer formulator Sakai et al. 8. This is the same as TSB-A , except that pseudorelevance feedback (PRF) was 9. This is the same as TSB-A , except that the candidate score parameter P 10. This is the same as TSB-A , except that top 50 documents, instead of the
Figure 8 shows the performance values of these 10 ASKMi runs evaluated using the official answer file QAC2formalAnsTask1 040308 , again sorted by MRR.
It can be observed that the system rankings according to the four metrics are almost identical, this time, and that the mean Q-measure values happen to be similar to the corresponding mean NQcorrect1 values. However, we shall later show that Q-measure is, in fact, substantially more sensitive than NQcorrect1.
In terms of MRR, 69% of the 10  X  9 / 2 = 45 run pairs are significantly differ-ent at  X  = 0 . 01, and an additional 13% are significantly different at (two-sided sign test). Note that this result is quite similar to that for the ac-tual submitted runs (see Section 3). Thus, although this experiment uses runs generated by a single system and is somewhat artificial, it may be a reasonable mimic of a true QAC2 Subtask 1 environment as the runs are actually quite different from each other. 4 4.3 Stability and Sensitivity of Q-measure
Figure 9 shows the MR-PT curves for RR, NQcorrect5, NQcorrect1, and Q-measure, calculated based on the 10 ASKMi runs when c = 97. The Q-measure values were computed based on answer correctness levels and equivalence classes that were devised for QAC2 as described by Sakai et al. [2004a]. By default, Q-measure uses gain ( S ) = 3, gain ( A ) = 2, gain ( B ) der to separate the effect of introducing correctness levels from that of handling multiple correct answers, we have also tried the flat gain value assignment, i.e., tried using gain ( S ) = 2, gain ( A ) = 1 . 5, and gain ( B ) which represents a mild gain value assignment that lies in between the default and the flat one. It can be observed that:
Q2:1.5:1 and RR are equally stable. Moreover, Q1:1:1 is more stable than these two, while Q-measure (with default gain values) is less so. This means that too much emphasis on the answer correctness levels hurts stability, while handling multiple correct answers improves it.

NQcorrect5 and NQcorrect1 are not as stable as RR, which is in agreement with our formal run results in Figure 4 (although NQcorrect5 appears to be less stable than NQcorrect1 this time).
The minority rates computed based on the 10 ASKMi runs are generally lower than those computed based on the formal runs, shown in Figure 4. Thus, the
ASKMi runs are easier to discriminate than the formal runs. This reflects the fact that we generated the 10 ASKMi runs using substantially different methods, on purpose.

Figures 10 and 11 plot the swap rates against the performance difference bins when c = 97 and c = 50, respectively, calculated based on the 10 ASKMi runs. Table II interprets the figures in a way similar to Table I. Thus, for ex-ample, if Q-measure is being used with roughly 100 questions to compare the 10 ASKMi runs, one should look for a performance difference of at least 0.05 in order to conclude that a run is better than another with 95% confidence. As the maximum Q-measure value observed among the 20,000 values (1000 trials for 10 systems, each with 2 disjoint topic sets) is 0.6860, this translates to a rela-tive difference of 7%. Of the 45,000 comparisons (1000 trials for 10 system pairs), 65.1% actually had this difference. By looking at the last column of Table II, it can be observed that the sensitivity of Q-measure (with different gain value assignments) is comparable to that of RR and that NQcorrect5 and NQcorrect1 are not as good.

By comparing Table II with Table I, it can be observed that the ASKMi runs are easier to discriminate than the formal ones, which is in agreement with the stability results. For example, for the ASKMi runs with c difference of 0.06 in MRR guarantees 95% confidence level, whereas, for the formal runs, the required MRR difference for the same confidence level, is 0.12. Moreover, while NQcorrect5 was comparable to RR in terms of sensitivity in
Table I, it is clearly less sensitive than RR in Table II. This implies that the differences among the 10 ASKMi runs mainly arise from the ranks of the correct answers, rather than the mere presence of them.

In fact, while the ASKMi runs are relatively easy to discriminate, they may still resemble one another in terms of how many different correct answers they contain in each answer list, as well as the correctness levels of these answers, since they all come from the same system that uses the same named entity recognition module, and so on. The set of formal runs may be richer in variety in this respect: For example, one system may be good at including only one correct answer in the ranked list, while another may be good at including many correct answers. Similarly, one system may be good at finding S-correct answers, while another may be good at finding B-correct ones only. If this is indeed the case, Q-measure may well outperform RR in terms of sensitivity if used for ranking the formal runs. Unfortunately, we currently cannot prove this hypothesis, because the formal run system output files are not available to us.

To sum up, Q-measure is at least as reliable as RR for ranking systems, provided that too much emphasis on answer correctness levels is avoided. Us-ing answer correctness levels tends to hurt stability, while handling multiple correct answers improves it. 5. QAC2 SUBTASK 2: F-MEASURE
We finally take a brief look at the formal runs submitted to QAC2 subtask 2 , where systems were required to return sets (as opposed to ranked lists) of answers. The official measure used for ranking systems was instance-based
F-measure [Fukumoto et al. 2004; Voorhees 2004]. Thus, inclusion of dupli-cate answers is penalized, just like Q-measure does with answer equivalence classes for ranked lists of answers. For this  X  X ist task, X  we cannot compute any alternative metrics (e.g., those proposed by the QAC organizers [Kato et al. 2004]), because only the F-measure values, not the actual system output files, are available to us. Thus, we simply measure the stability and sensitivity of
F-measure, based on the actual runs submitted to QAC2 subtask 2, just to check that F-measure was adequate for ranking the submitted systems. Figure 12 shows the mean F-measure values for the 14 runs submitted to
QAC2 subtask 2. In terms of mean F-measure, 63% of the 14 pairs are significantly different at  X  = 0 . 01; additional 15% are significantly different at  X  = 0 . 05 (two-sided sign test).

Figure 13 shows the MR-PT curve for F-measure, based on the 14 submitted runs when c = 97. It can be observed that at 1% minority rate, the proportion of ties is approximately 17%, which is comparable to the case with NQcorrect5 for subtask 1 (Figure 4).

Figure 14 plots the swap rate against the performance difference bins when c = 97. Table III interprets Figure 14. Thus, if the runs submitted to QAC2 subtask 2 are to be compared using roughly 100 questions, one should look for relative differences of at least 18% in order to be 95% confident. Of the total comparisons 62.2% have this difference, which is similar to the situation with
RR for subtask 1. As both QAC2 subtasks 1 and 2 actually used nearly 200 ques-tions, probably more than 70% of the run pairs were distinguishable in these tasks. However, unlike RR, it is known that F-measure is extremely vulnerable to change in the set of correct answers (e.g., addition of newly discovered correct answers), which remains a problem for subtask 2 [Kato et al. 2004]. 6. RELATED WORK
Voorhees [2004] used the swap method for assessing the reliability of instance-based F-measure for list questions and nugget-based F-measure for definitional questions used at the TREC 2003 QA track. To our knowledge, the present study is the first to address the reliability issues based on the QAC Japanese QA evaluation settings. As our experimental methods are language independent, however, we believe that our findings apply to languages other than Japanese as well.
 In the IR contexts, the reliability of reciprocal rank has been examined by
Soboroff [2004] and by Sakai [2006a, 2006c, 2006e, 2006f]. Similarly, the re-liability of Q-measure as an IR metric has been examined by Sakai [2006b, 2006d].

Recently, Sakai [2006a, 2006c, 2006e, 2006f] proposed O, P, and P measure for the task of finding one relevant document in IR. These new metrics are variants of Q-measure and can also be applied to QA evaluation, but they are less stable and sensitive than Q-measure as they do not consider all retrieved relevant documents (or correct answers). 7. CONCLUSIONS AND FUTURE WORK This paper compared existing QA evaluation metrics using the NTCIR-4 QAC2
Japanese factoid QA tasks from the viewpoint of stability and sensitivity. Our main conclusions are:
The fraction of questions with a correct answer within top 5 (NQcorrect5) and that with a correct answer at rank 1 (NQcorrect1) are not as stable and sensitive as reciprocal rank.

Q-measure, which can handle multiple correct answers and answer correct-ness levels, is at least as stable and sensitive as reciprocal rank, provided that a mild gain value assignment is used. Emphasizing answer correctness levels tends to hurt stability, while handling multiple correct answers improves it.
We hope to repeat our experiments using the formal runs from future QAC tracks, and to address the evaluation issues of nonfactoid QA as well. APPENDIX
This appendix describes the features of the ASKMi Japanese QA system [Sakai et al. 2004a, 2004b] that are relevant to the experiments described in Section 4.
Like many other QA systems, ASKMi consists of several modules, including question analyzer for determining answer types, Retriever for matching ques-tions with documents that are used as the knowledge source, and semantic class (or named entity) recogniser for extracting answer candidates from documents.
The ASKMi Retriever uses the Okapi/BM25 term weighting for document a set of query terms) q , the original document score for each document d ( origdscore ( q , d )) is calculated based on the weight of each query term t as follows: where n ( t ) = number of documents containing t ; r ( t ) = number of known relevant documents containing t ; tf ( t , d ) = number of occurrences of t in d ; ndl ( d ) = normalized document length, obtained by dividing the original k 1 = parameter for controlling the effect of tf ( t , d )( k b = parameter for controlling the effect of ndl ( d )(0  X 
As there is no relevance information available in a noninteractive QA envi-ronment, R and r ( t ) are set to zero.

We can optionally modify the original BM25-based document score for QA as follows: where Clearly, P D = 0 implies dscore = 1 (i.e., a constant), and P dscore = origdscore . P D &gt; 1 emphasizes the document score curve and 0 P
An ex-string e is a quadruple &lt; document , answertype , string , position has been extracted through candidate answer extraction. It represents a specific occurrence of a candidate answer within a document. A candidate c is a triple &lt; multiple occurrences) within a document. An answer string a is obtained by con-solidating candidates across documents and across answer types. Let C ( e ) and
A ( c ) represent the mapping from ex-strings to the corresponding candidates, and that from candidates to corresponding answer strings, respectively.
Let e be an ex-string from a document d . Let t  X  q be a query term found in d , and let p be a passage extracted from d . First, we define the cooccurrence function as follows: where pos ( e , p ) = position of ex-string e within p ; pos ( t , i , p ) = position of the i th occurrence of t within p ; P
For a given question q and a document d , the score of an ex-string ( escore ) and that of a candidate ( cscore ) are calculated as follows: where cf atr ( q , e ) cf
The confidence values are currently hard coded in our hand-crafted rules for answer type recognition and candidate answer extraction.

Let D ( c ) represent the mapping from a candidate c to the corresponding document. ASKMi calculates the score of an answer string as follows:
Finally, the answer formulator adjusts the the above answer scores based on some heuristics called answer constraints. Moreover, it performs answer string consolidation in order to prevent returning multiple answers that mean the same thing.

