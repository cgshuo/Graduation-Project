 WEISHI ZHANG and GUIGUANG DING , Tsinghua University CHUNPING LI and CHENGBO ZHANG , Tsinghua University Recommender systems that suggest unknown interesting items to users have been de-veloped rapidly in recent years, among which collaborative filtering (CF) recommenders are those of the broadly applied approaches. The CF approaches principally derive recommendations for a user based on the preferences of other users who were discovered with similar tastes [Breese et al. 1998]. Indeed, most CF-based systems reply on item ratings as explicitly obtained from end users for the calculation of user-user or item-item similarity. However, few of them investigated the potential effect of user reviews (or called textual comments to items), as another type of valuable user-generated sources, on boosting the recommendation accuracy and addressing rating sparsity limitations [Papagelis et al. 2005]. In our work, after surveying existing popular Chinese media-sharing Web sites, such as Youku, 1 Ku6 2 and Tudou, 3 we found that they all possess large amounts of review data that visitors have written for expressing their opinions. The question is then whether/how we could incorporate user reviews into CF-based systems, so as to significantly augment recommendations, especially in the condition that users X  real ratings are not available. In fact, though above mentioned sites enable users to thumb up/down for an item (i.e., providing binary rating to an item), they did not record who did this action. So it is hard to rely on this kind of rating info to obtain users X  preferences, while reviews could be potentially more helpful as they were all attached with user IDs.

To achieve the goal of incorporating user reviews, we have particularly attempted to derive  X  X irtual ratings X  from user reviews through the method of sentiment classification, so that given a piece of text, its latent opinion (represented by sentiment polarity such as positive, neutral, or negative), can be discovered for reflecting the user X  X  preference on the corresponding item. Hereafter, we call such ratings that are derived from user reviews as  X  X irtual ratings, X  to be conceptually different from the user inputted ratings.

More specifically, our system is targeted to provide review-based recommendations for Chinese sites. Thus, we have first investigated Chinese reviews X  characteristics, which include: (1) each user review is usually short and noisy (including advertise-ments, hyperlink text etc.); (2) many reviews contain emoticons such as smiley faces (in our experimental data, 41% reviews have this property); (3) the ratio between positive and negative reviews is not 1:1., though most of related sentiment classification methods are under this 1:1 assumption [Blitzer et al. 2005; Turney 2002; Zagibalov and Carroll 2008a].

Taking into account these characteristics, we have been aiming at developing the SElf-Supervised, Lexicon-based and Corpus-based (SELC) model, which is a self-supervised sentiment classification approach to determining the overall sentiment polarity of a review document that contains both textual words and emoticons. As a result from the model, we use the sentimental polarities of reviews as one kind of resources for recommendation. In the experiments, we have first identified our sentiment classification method X  X  higher precision and recall by comparing it with other typical classification methods. Second, we tested the fusion effect of virtual ratings in two datasets: one is without users X  real ratings, and the virtual ratings were incorporated into user-based and item-based CF algorithms respectively to evaluate which fusion mechanism can better exploit the virtual ratings X  merit; another is with users X  real ratings, and the virtual ratings are evaluated by a matching experiment, then both of the virtual ratings and real ratings are used to predict recommendations to get a comparable results over the user-based and item-based approaches. The rest of this article is organized as follows. We first introduce related work in Section 2, and then given the overview of our approach that is divided into two phases (Section 3). Sections 4 and 5 give the detailed algorithm for each phase, followed by Section 6 with experimental procedures and results analysis. Finally, we conclude this article and indicate its future directions. In supervised sentiment classification methods, standard machine learning techniques such as Support Vector Machine (SVM) and Naive Bayes have been usually used [Pang et al 2002; Alpaydin 2004]. Different factors affecting the machine learning process are investigated. For example, linguistic, statistical, and n-gram features were researched in Dave et al. [2003]. Selected words and negation phrases were investigated in Na et al. [2004]. However, the performance of supervised approaches normally decreases when training data is insufficient [Aue and Gamon 2005; Read 2005].

On the contrary, unsupervised approaches make the assumption that there are cer-tain words people tend to use to express strong sentiment, so that they might suffice to classify the documents. In Turney [2002], an unsupervised sentiment classification ap-proach was proposed by calculating the mutual information between each phrase in a document and the selected two seed words: excellent and poor. Fewer seed words imply less domain-dependency. Zagibalov and Carroll [2008a] only assign one word good as a seed positive word, and use negation words such as  X  X ot X  to find initial negative expres-sions. In Zagibalov and Carroll [2008b], even the one word  X  X ood X  is ignored, and seed words are automatically generated based on a linguistic pattern (called  X  X egated ad-verbial construction X ) like  X  X ot very good X . Experimental results show that this method achieves similar performance to supervised methods on Chinese product reviews.
SELC Model (SElf-Supervised, Lexicon-based and Corpus-based Model) [Qiu et al. 2009] is proposed for self-supervised sentiment classification on Chinese IT product re-views. The model includes two submodels. In the first phase, some reviews are initially classified based on a sentiment dictionary. Then more reviews are classified through an iterative process with a negative/positive ratio control. In the second phase, a su-pervised classifier is learned by taking some reviews classified in the first phase as training data. Then the supervised classifier applies on other unclassified reviews to revise the results produced in the first phase. In this article, we improve this work by considering the special features of real Chinese online reviews and use the sentimental polarities of reviews as resources for recommendations. Since 1990s, recommender systems have been explored in many product domains, that is, movies [Christakou and Stafylopatis 2005], TVs [Setten and Veenstra 2003], Web pages [Balabanovic 1998] with the objective of recommending items matched to users X  profiles [Yang et al. 2007]. In recent years, much more techniques have been developed in recommender systems in order to derive better performance [Gunawardana and Meek 2009; de Gemmis et al. 2008; TsoSutter et al. 2008]. However, most of works are limited when user preference data (i.e., ratings) are hardly obtainable from real sites (e.g., the video-sharing sites). To address this limitation, tags (in form of user-defined keywords) have been utilized as supplementary source to predict user interests [Tso-Sutter et al. 2008]. Tso-Sutter et al. [2008] proposed a generic method that allows tags to be incorporated into standard CF algorithms, by reducing the three-dimensional correlation to three two-dimensional correlations and then applying a fusion method to reassociate these correlations. de Gemmis et al. [2008] have developed a strategy to infer user interests by applying machine learning techniques to learn from both the  X  X fficial X  item descriptions provided by a publisher, and tags that users used to annotate relevant items.

Matrix factorization based techniques have proven to be efficient in recommender systems when predicting user preferences from known user-item ratings. Paterek ap-plied successfully various matrix factorization techniques [Paterek 2007] by adding biases to the regularized MF, post processing the residual of MF with kernel ridge regression, using a separate linear model for each movie, and by decreasing the pa-rameters in regularized MFs. Kurucz et al. [2007] showed the application of expectation maximization based MF methods for Netflix prize. Recently, several matrix factoriza-tion methods [Salakhutdinov and Mnih 2008a, 2008b] have been proposed for collab-orative filtering. These methods all focus on fitting the user-item rating matrix using low-rank approximations, and use it to make further predictions.

However, to the best of our knowledge, only a few papers have considered user reviews and integrated their sentiment analysis results into the generation of recom-mendations. Leung et al. [2006] have attempted to identify features from reviews to infer ratings, but after no detailed description of how the method was implemented. The sentiment analysis approaches in Ganu et al. [2009] are supervised and hence need manually annotated training data. Jakob et al. [2009] proposed three approaches to extract movie aspects as opinion targets and use them as features for the collabora-tive filtering on IMDB dataset. Each of these approaches requires different amounts of manual interaction. However, none of the prior papers has explored the combination of virtual ratings and review sentiment analysis on a Chinese dataset. Our work ex-erts to address this limitation by proposing a self-supervised sentiment classification approach and applying the results to predict the virtual ratings on items, so as to be effectively fused into standard CF algorithm in a Chinese dataset. Our recommender algorithm is proposed to study the roles of online reviews in aug-menting recommenders in current Chinese media-sharing sites. The algorithm con-cretely consists of two phases: (1). SElf-Supervised, Lexicon-based and Corpus-based Model (SELC) for Review Sentiment Classification and (2). Item Recommendation. Figure 1 shows the algorithm flow. Phase 1 and Phase 2 are separated by a dash line. Basedona sentiment word set ,a negation word list ,andan emoticon set , Phase 1 uses a self-supervised approach (SELC) to identify the sentiment polarity of each review. Figure 2 shows the flow chart of the whole self-supervised sentiment classification process.

It concretely consists of two models, that is, unsupervised model and semi-supervised model. In the unsupervised model, an unsupervised approach is applied on the original data to automatically label some data. In the semi-supervised model, a semi-supervised approach is applied on the labeled data to acquire a training model. Finally, the model is applied on the original data to do the sentiment classification. In Figure 2 the solid lines refer to the unsupervised model while dash ones refer to the supervised model. In the unsupervised model of our self-supervised, lexicon-based, and corpus-based (SELC) modeling, a sentiment vocabulary is initialized by a general sentiment dictio-nary. The vocabulary is used to label reviews. Then more sentiment words are found from the labeled reviews for updating the vocabulary. The new vocabulary then helps classify more reviews. By this iterative process, the vocabulary and labeled reviews are updated and enlarged step by step. In the iterative process, the positive/negative ratio is controlled. The algorithm ranks the reviews during each iteration and keeps the same number of top-ranked positive and negative reviews. Additionally, the emoticon scoring analysis is integrated into the iterative process of the unsupervised model to get more accurate results. Specifically, the unsupervised model consists of following steps: 4.1.1. Step 1: Initializing Sentiment Element Sets. The sentiment element sets consist of two sets, that is, sentiment word set and emoticon set . The sentiment word set, denoted by W sen , includes a list of word items, each of which is assigned with a sentiment score. W sen is initialized by a general sentiment dictionary, which usually includes a lot of positive and negative words. A positive word is initially assigned with score + 1 . 0, while a negative word is assigned with score  X  1 . 0. Monosyllabic words are filtered from W sen , because most of them are too ambiguous to provide reliable sentiment. In addition, since the general sentiment dictionary is applicable to many domains, this method has the potential to be domain independent.

The emoticon set, denoted by E , is the set of the emoticons (e.g., smiley or sad faces) used by the users to express their preferences. Because the emoticons are widely used by the users in many resource-sharing Web sites to express their opinions, they play an important role in the task of our item review sentiment classification. First, we manually remove all of the nonesentiment-bearing emoticons, for instance, [Oh ...] and[Well...],fromthewholesetofcrawled expression emoticons. Then we add the remaining part into E and according to the sentiment they express, the emoticons are divided into two kinds: positive and negative. Each positive face in E is initially assigned with score + 1.0, and a negative emoticon is assigned with score  X  1 . 0. We selected 10 positive emoticons and 5 negative emoticons as the initial emoticon set (see details in the experiment Section 6.2.1).

For the generation of the negation word list, we manually selected ten most fre-quently used negation words, such as  X   X  ( X  X ot X ),  X   X  ( X  X ould not X ),  X   X  ( X  X on X  X  have X ),  X   X  ( X  X on X  X  have X ), etc. (see the dataset used in the experiment Section 6.2.1). 4.1.2. Step 2: Identifying Review Sentiment Scores. Through analyzing online reviews, two kinds of reviews have been found in most of view-sharing Web sites: users expressed their opinions on the items, or users expressed their opinions on other users X  reviews. We call the first kind as item-oriented reviews and the second kind as user-oriented reviews. It X  X  easy to differentiate between these two kinds of reviews, since the user-oriented reviews always start with a  X  X reply to] + [other user] X  writing styles. Since the sentiment of user-oriented reviews is usually not directly related to the items, we only focus on item-oriented reviews. We also removed some noise data including advertisements and hyperlink text.

Therefore, at first, a preprocessing was conducted to filter out all the user-oriented reviews and noise data according to their writing styles. Given an item i , all of its related reviews are denoted by Rev(i) . Each review r ( r  X  Rev(i) ) is then divided into clauses by punctuation marks.

Secondly, for each clause, if it contains sentiment word items as appearing in W sen (the sentiment word set), each sentiment word item w of the clause is scored by Equation (1), where L w is the length of the word item, L clause is the length of the clause, S w is the word item X  X  current sentiment score in W sen ,and N w is a negation check coefficient that has a default value of 1.0. If the word item is preceded by a negation within the specified zone, N w is set to  X  1 . 0. We have two assumptions to design Equa-tion (1): (1) at the most time the longer the length of a Chinese word is, the clearer its sentiment polarity is (the square of L w is to enlarge the assumption); (2) the same word in a shorter clause usually expresses stronger sentiment than that in a longer clause. Then the sentiment score of a clause c , denoted by CS(c) , is calculated by CS ( c ) = review taking into account of its contained sentiment words), denoted by RS W ( r ), is subsequently calculated according to Equation (2).

For review r ,the ReviewEmoticonScore , denoted by RS E (r) , is also calculated accord-ing to Equation (3), where S E e is the current sentiment score of emoticon item e as appearing in E .
 Finally, the sentiment score of the review r , denoted by RS(r) can be computed using: where parameter  X   X  [0,1] determines the weight put on each factor, that is, the balance between the review X  X  word sentiment score RS W (r) and its emoticon sentiment score RS E (r) .
 4.1.3. The Bias Caused by the Missing of Ratio Control. Basically, after this step, a review r good but would cause sentiment bias for the following step of updating the sentiment sets (Step 4). In order to explain the bias in a better way, we skip Step 3 and introduce Step 4 first. 4.1.4. Step 4: Updating the Sentiment Element Sets. In Step 4, the sentiment word set W sen and emoticon set E are to be updated (and usually enlarged).

For sentiment word set W sen , each lexical item 4 that occurs at least twice in those classified reviews is taken as a candidate word item. For an candidate word item w , denote the number of positive reviews containing w as N p w , and the number of negative reviews containing w as N n w (preceded by a negation will make the account reduce by one, and N can be negative). The idea of updating sentiment word set W sen is then: if N w is much bigger than N n w , w is very likely to be a positive word item, and vice versa. The following formula is used to be the measure.
If difference(w)  X  1, w is included in W sen (current items in W sen will be removed if they no longer satisfy this condition). The sentiment score of w in W sen is updated as
For updating the emoticon set, for an emoticon item e , denote the number of positive reviews containing e as N p e , and the number of negative reviews containing e as N n e . The idea of updating emoticon set E is similar to the sentiment word set updating: if N e is much bigger than N n e ,then e is very likely to be a positive emoticon item, and vice versa. The following formula is the measure.
If difference(e)  X  1, w is included in E (current items in E will be removed if they no longer satisfy this condition). The sentiment score of w in E is updated as 4.1.5. Step 3: Classifying Reviews based on Ratio Control. After the introduction of Step 4, we can give an example to explain the bias in Step 4 caused by the missing of ratio control. If there are 20 reviews classified as positive and 10 reviews as negative, then the number of words only occurring in the positive reviews is more likely to be bigger than the number of words only occurring in negative ones. If the word  X  X creen X  only occurs in one of the positive reviews, then  X  X creen X  will be assigned with a sentiment score of 1.0 (Step 4 will explain how the score 1.0 is obtained using Equation (6)), and therefore be judged as a positive word item. But in fact, such a word may not have any sentiment polarity. Such bias is caused by unequal number of positive and negative documents. To overcome the bias, a ratio control is designed, which requires the number of positive and negative reviews in the classified sentiment review list to be the same.
 Denote the number of positive and negative reviews in one round of iteration as RN positi v e and RN negati v e respectively. To realize the ratio control, first, rank all reviews according to their sentiment scores RS ( r ). Second, take the smaller value between the positive and negative documents above the threshold in the sentiment review list, and remove others.

Figure 3 shows the process of classifying reviews with ratio control. Those reviews form the sentiment review list. 4.1.6. Step 5: Iteration Control. The unsupervised approach iterates from Step 1 to Step 4. In the SELC model, the iteration completes when  X  % of documents have been labeled. Through the empirical test in one of our prior works [Zhang et al. 2009], we found that the optimal value for  X  is 0.618 for well balancing both accuracy and time efficiency. That is, when 61.8% of documents have been labeled, the iteration procedure can be completed and the labeled documents can be used as the training data for the semi-supervised model. In semi-supervised model, the Support Vector Machine (SVM) classifier with a linear kernel is selected. Specifically, in this model, the items of sentiment element sets as retrieved from the last iteration are used as the feature set. TFIDF measure (see Equation (9)) is used to compute weights for the items in both sentiment word set and emoticon set.
Then the SVM classifier applies the data to do the classification and get the final review sentiment classification results. When Web sites do not support users to give ranking scores for items, we cannot get the real ratings from users. In this condition, virtual ratings as derived from the reviews will be fully incorporated and behave as primary resource for producing recommendations.

Because after the process of Phase 1, each review r will be classified as positive, negative or neutral , at this step, we use the review sentiment classification results to predict the virtual rating matrix, which can be then taken as input to the standard collaborative filtering recommender algorithms (user-based and item-based).
In R UI , each user has a virtual User-Item Vector ,thatis, V UI (u) .Each V UI consists of three parts that is, Like + , Dislike  X  and Unknown . The Like + part of the V UI consists of the items liked by the user u (positive and neutral ones 5 ), while the Dislike  X  and Unknown parts consist of the items disliked or unknown to user u (negative and unknown ones) respectively.

Firstly, given an item i and a user u , the set of all the reviews that user u puts on item i is denoted as Rev(u,i) . The set of all the positive reviews in Rev(u,i) is denoted as Rev(u,i) pos , while the set of all the negative reviews in Rev(u,i) is denoted as Rev(u,i) neg .
Then, for a user u , we calculate the sets of Rev(u,i) pos and Rev(u,i) neg for all the items. and build the User-Item Vector ( V UI (u) )of u in the Rating MatrixR UI according to the following rules.  X  X f the value of (| Rev num (u,i) pos |-|Rev num (u,i) neg | ) is greater or equal than 0, then we add item i into the Like + part of the V UI (u) with the value of + 1.  X  X f the value of (| Rev num (u,i) pos |-| Rev num (u,i) neg |) is less than 0, then we add item i into the Dislike-parts of the V UI (u) with the value of  X  1.  X  X f item i is unknown to user u , then we add item i into the Unknown parts of the V UI (u) with the value of 0.
 After we have built the virtual rating matrix, the standard user-based and item-based collaborative filtering algorithms can be utilized to predict item recommendations in different web applications. 6.1.1. Data and Tools. In order to validate the performance of our methods, we use two sets of data to conduct the experiments, which are respectively Youku dataset (which is without users X  real ratings) and Amazon dataset (which is with real ratings).
To crawl the Youku datasets, we used nine Chinese queries to search videos.
Finally, we got the data 6 including more than 10,320 videos, each of which had more than 20 reviews. All the reviews were written in Chinese.
 The Amazon datasets were crawled from the book section of the Amazon Web site in China, which includes more than 700000 reviews written in Chinese.

In Phase 1, a negation word list that contains ten Chinese negations was used.
For all the experiments, the HowNet Sentiment Dictionary 7 was used as the sen-timent dictionary, which is wellknown in the area of Chinese sentiment classification containing 4,566 positive words and 4,370 negative words.

There are more than 30 emotions provided by Youku for users to use while writing reviews. The following 10 positive emoticons and 5 negative emotions were used as the initial emoticon set and they were classified manually (  X  1or + 1). Then the Updating step (Section 4.1.4) is used to recalculate the strength of the sentiment polarity for each emotion. {  X  X mile X ,  X  X ove X ,  X  X oking X ,  X  X weat X ,  X  X aughty X ,  X  X h-oh X ,  X  X ool X ,  X  X lower X ,  X  X iss X ,  X  X humbs up X  } . {  X  X ad X ,  X  X ick up X ,  X  X ngry X ,  X  X weat X ,  X  X ired X  } . 6.1.2. The Collaborative Filtering Recommendation Algorithm. We use the standard user-based and item-based collaborative filtering algorithms conduct the recommendation experiments.

In user-based CF, to derive the recommendations for a target user u , k most-similar users are selected, which constitute the neighborhood of u , denote by N(u) .When predicting the rating of a given user u for an unknown item i , the rating score of i can be computed by:
In the above equation, R UI (u,i) is rating value user u put on item i ,the R UI ( u )isthe mean rating for the user u and the weight w(u,v) reflects the similarity between each user v and the given user u (i.e., the value of S UI (u,v) ). Then, the Top N items with the highest r UI (u,i) are selected in the recommendation list for the user u .
In the case of item-based CF, the prediction score is the average of the ratings on k most-similar items N(i) rated by the given user u . The prediction for a rating of a given user u for an item i is hence: where the weight w(i,j) reflects the similarity between each item j and the given item i (i.e., the value of S UI (i,j) ). Then, the Top N items with the highest r UI (u,i) are selected in the recommendation list for the user u .
 As the rating matrixes are the input of the CF algorithms, we use Virtual Rating , Real Rating (if included in the datasets), and their averagevalue: Real &amp; Virtual Rating to evaluate the effectiveness of the virtual rating. We first tested the accuracy of our sentiment classification method by using a set of Youku 8 data with 1,085 videos and 6,450 users. Each video has at least 100 video-oriented reviews , and the total number of reviews is 120,174 in this set. Among the 120,174 reviews, there are 49,271 reviews that contain more than one emoticon. In the experiment, we set the value of parameter  X  in Equation (4) as default 0.3, because we considered that the emoticon sentiment score RS E (r) was more important than the word sentiment score RS W (r) for the sentiment classification.

After removing the noisy reviews as mentioned in Section 4, we manually labeled the polarities of 1000 reviews. The numbers of positive, end negative reviews in the labeled set are 653 and 347 respectively. We took the labeled data as the actual polarities of reviews.

We measured two approaches in the comparison, that is, SELC and SVM. In SELC, the method of Phase 1(unsupervised and semi-supervised models) proposed in this article was used to get the sentiment classification result. In SVM , the supervised Support Vector Machine classifier 9 was used to conduct the sentiment classification, and the SVM classifier with a linear kernel was ran in 10-fold stratified cross-validation mode. We used HowNet Sentiment Dictionary and an initial emoticon set as the feature set. Table I shows the sentiment classification X  X  precision and recall results. From Table I, we can see that both the SELC achieves higher F 1 scores (92.8%) on Total reviews than the SVM classifier (87.4%). It is worth noting that SVM has suffered from the unbalance training data (pos:653, neg:347, the common ratio in real online environments) and gets bad recall values on negative reviews (71.8%). On the other side, SELC can still achieve a comparatively good recall on negative reviews (85.9%).
In Equation (4), the parameter  X   X  [0 , 1] determines the weight put on each factor, that is , the balance weight between the review X  X  word sentiment score RS W (r) and its emoticon sentiment score RS E (r) . We have also designed an experiment for parameter sensitivity analysis.

In Equation (4) the default value of  X  if 0.3, we have set the value of  X  as 0.1, 0.3, 0.5 and 0.7 respectively. Figure 4 shows sentiment classification X  X  F1-score results.
From Figure 4 we can see that the sentiment classification achieve the best result when  X  = 0 . 3. When  X  is bigger than 0.3, the performance descends gradually alongwith the growing of  X  , which proves that hypothesis: the emoticon sentiment score RS E (r) is more important than the word sentiment score RS W (r) for the sentiment classification X  is correct. Generally speaking, the performance is not sensitive to the parameter  X  , and the F 1 -scores are close to each other considering different values of  X  .
There are several novel improvements in the SECL used in this article over the method of the original SELC model in Qiu et al. [2009], which affect the performance simultaneously. To check their individual effect, two variant models were implemented. They are referred to as V1 and V2 respectively. In V1 , the new iteration control strategy is replaced by the iteration control method of the original SELC model. In V2 , the emoti-con analysis is removed from both unsupervised model and semi-supervised model.
Figure 5 shows that both the new iteration control strategy and the emoticon analysis have taken effect on the performance improvement, that is, improving 3.1% and 4.5% F -scores respectively. It suggests that the integration of emoticons can be very useful in further increasing the performance of the review sentiment classification, and the new iteration control strategy in the unsupervised model can also provide more accurate training data for the semi-supervised model.

Thus, the above analysis results indicate that our classification approach is capable of overcoming the challenges of online reviews X  special features and providing reliable results for the building of virtual Rating Matrixes in the next phrase of producing item recommendations. 6.3.1. Results of Recommendations on Youku Dataset. To compute recommendations, we classified 68,561 positive, 39,576 negative reviews on 1085 videos. The corresponding rating matrix was established for 6,450 users, with generated 61,137 virtual user-item ratings (the number of + 1and  X  1).

In our experiments, we compared the results for different approaches. Following is the description of labels we used to denote each of these algorithms.  X  YOUKU. The recommendation approach of the Youku Web site, where each video is along with 3 recommended videos mainly based on video popularity.  X  User-SELC. The User-based Collaborative Filtering Approach, where the results of
SELC are used to predict the virtual ratings.  X  Item-SELC. The Item-based Collaborative Filtering Approach, where the results of SELC are used to predict the virtual ratings.

The performance of video recommendations was then measured through statistical evaluation method.

Users are often split into training and test sets. The algorithm is trained over the users from the training set and evaluated over the users in the test set [Shani et al. 2008]. In this article, we evaluated the accuracy of recommendations using a  X  X old-start X  protocol on the dataset. First, we randomly selected 860 (80%) of the items to be training items, leaving 217 (20%) as testing items. Then, we selected 500 users with the least item ratings to be test users.

Since each test user had rated two sets of items, that is, training item set and testing item set, we can evaluate the performance of our approach by calculating the precision of a fixed length of recommendation list [Gunawardana and Meek. 2009]. We first used the algorithm to derive a recommendation list based on the training items rated byatestuser u . We then defined the per-user precision at the recommendation list containing Top N items as: where HitNumber is the number of items in the recommendation list that are hit in the testing item set of user u . Then, we averaged the resulting per-user precisions over all the 500 test users to get an average precision of the Top N recommendation.
It X  X  worth noting that in the process of statistical experimental simulation, since we can X  X  get the real user ratings on the Youku dataset, we used the virtual ratings as the ground truth considering the high performance of the review sentiment analysis, which was validated in experiments of review sentiment classification with the F 1 scores of 92.8%. Additionally, we have also conducted a matching experiment to clarify the validity of using the virtual ratings as ground truth (please see Section 6.3.2).
Figure 6 and Figure 7 show the average precisions, respectively, of Top 3 and Top 10 recommendations for the baseline approaches with varying neighborhood sizes. Since YOUKU doesn X  X  provide results for Top 10 recommendations, Figure 7 gives out only the results of the CF-based approaches.

In Figure 6 and Figure 7 we can see that both the CF-based approaches obviously outperform the YOUKU approach (2.1%). These two figures also show that the User-SELC approach achieves the best results for neighborhood size k = 10, which lead to the precisions of 5.1% at Top 3 recommendation and 4.9% at Top 10 recommendation, while the Item-SELC approach achieves its best results for k = 20, which lead to the precisions of 4.8% at Top 3 recommendation and 4.6% at Top 10 recommendation.
Given a test user u , user-based CF for Top N recommendation relies on similar users who have similar rating patterns. These users are more likely to rate the same test itemsasuser u do. But item-based CF relies on items similar to the training items rated by user u . The test items of user u are not necessarily among the items similar to the training items rated by user u unless the test items are actually similar to the training items rated by the user (which is not always true). So, the results of item-based CF for Top N recommendation are generally poor compared to user-based CF.

Because the top-ranked videos are usually more noticeable to the online users, the precisions at Top 3 recommendations is more worthwhile to be noted in producing better recommendations. From the above two figures, we can see that the precisions at Top 3 recommendations set are also slightly better than those at Top 10 recommendations respectively by the Item-SELC and User-SELC approaches. In particular, User-SELC achieves the best result of 5.1% at Top 3 recommendations.

Experimental results show that the precision of the proposed approach is relatively low with best precision 5.1% (User-SELC at Top 3). That is mainly caused by the character of the date set we used. We have only about 18 (120,174/6,450) items rated for each user in average. So we have less than 4 items in the test set for a user in average. Because of the small size of the test set, the HitNumber in Equation (12) in the recommendation list is also very small, which causes the low precision.
There are other papers also encounter that problem. For example, in Gunawardana and Meek [2009], the precision of experiment on Ta-Feng dataset (with 23 items rated for each user in average) is also low (&lt;4.5%), while the precision of experiment on MovieLens dataset (with 165 items rated for each user in average) is relatively high (best result 35%).

So in the case, we think the results are reasonable and we cannot say the system is not useful in reality only considering the low relatively precision. 6.3.2. Results of Recommendations on Amazon Dataset. Since we cannot get the real user ratings on the Youku Web site, we referred to the dataset of online books (Amazon China), which contains both the reviews and real ratings. The experiment is designed to measure the effectiveness of virtual rating compared to the real rating.
Like the experiments on Youku, the same statistical evaluation approach was used when we process the Amazon dataset; we got 318,730 reviews on 1,805 books and 28,254 reviews replied to other reviews. The number of users is 5502. Each of them has written about 5 reviews in average. The real rating made by user is an integer between 1 and 5, 5 means like the item very much, 1 means very dislike. After the procession of Phase 1 , we also got the virtual rating set of the users. The virtual rating generated from the approach SELC in Phase 1 is a fraction in [0, 5] (just a normalization of the review sentiment score).

First, in order to analyze the matching relationship between virtual rating and real rating, we added an experiment to see the proximity between those two kinds of ratings and clarify the effectiveness of the virtual ratings.

Given a user u and item i , we have the real rating u put on i : r real (u,i)  X  { 1,2,3,4,5 } and c , we get all the real ratings belong to it, and put all the virtual ratings corresponding to these real ratings into the virtual rating set vrset c . After the above steps, we have five virtual rating sets corresponding to the five real rating categories, that is, vrset 1 , vrset 2 , vrset 3 , vrset 4 , vrset 5 .We evaluated the expectation and variance of the virtual ratings in each of the five virtual rating sets separately. Figure 8 shows the result.
From the results we can see that expectation in the each vrset c is close to its real rating category value, and the variances are acceptable. It suggests that the virtual ratings generated from sentiment analysis are efficient and make sense, and we can use virtual ratings as the ground truth for the dataset that does not contain real ratings.
Then, we used the CF-based approach (Section 6.1.2) to predict the recommenda-tion on the dataset. According to the different type of item ratings, we designed three different kinds of subexperiments (using Real Rating , Virtual Rating and their aver-agevalue: Real and Virtual Rating ), to evaluate the effectiveness of the virtual rating. For all the subexperiments, the user X  X  real ratings are used as the ground truth. In other word, we use the user X  X  real ratings to evaluate the predicted results. Figure 9 shows the Top 10 recommendation results of the User-SELC (neighborhood size k = 10) and Item-SELC (neighborhood size k = 20) approaches.

From Figure 9 we can see that the User-SELC approach achieve better results than the Item-SELC approach considering all the three different kinds of ratings. What X  X  more, the results of all the Virtual and Real Rating methods outperform the other two methods only using virtual rating or real rating. It suggests that the virtual ratings has addressed the rating sparsity limitation of current media-sharing sites to some extent and improved the applicability of collaborative filtering (CF) recommender techniques in these sites. In this article, we developed review-aware recommender algorithms that particularly exploited the sentiment classification results to automatically derive virtual ratings, and then fused them into item-based and user-based CF algorithms by which the User-Item Rating Matrix can be inferred by decomposing item reviews that users gave to the items.

Through experiments on two datasets (one is without users X  real ratings and another is with users X  real ratings), we identified the significant impact of virtual ratings on augmenting recommenders. The results of the experiments show that 1) the SELC model achieves high precision on the review dataset and can produce virtual ratings of high quality; 2) the virtual ratings generated from the sentiment classification can be used to improve the recommender system on those resource sharing Web sites regardless of whether there are real ratings or not.

In the future, we will be engaged in classifying the reviews into more delicate cat-egories in addition to  X  X ositive X  and  X  X egative X  and exploring the application of the virtual ratings on the state-of-the-art recommendation algorithms. On the other hand, we will try to extend our method to other product domains, so as to additionally improve its cross-domain applicability.

