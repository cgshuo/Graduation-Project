 Abstract Identifying objects in conversation is a fundamental human capability necessary to achieve efficient collaboration on any real world task. Hence the deepening of our understanding of human referential behaviour is indispensable for the creation of systems that collaborate with humans in a meaningful way. We present the construction of REX-J, a multi-modal Japanese corpus of referring expressions in situated dialogs, based on the collaborative task of solving the Tangram puzzle. This corpus contains 24 dialogs with over 4 h of recordings and over 1,400 referring expressions. We outline the characteristics of the collected data and point out the important differences from previous corpora. The corpus records extra-linguistic information during the interaction (e.g. the position of pieces, the actions on the pieces) in synchronization with the participants X  utterances. This in turn allows us to discuss the importance of creating a unified model of linguistic and extra-linguistic information from a new perspective. Demonstrating the potential uses of this corpus, we present the analysis of a specific type of referring expression ( X  X  X ction-mentioning expression X  X ) as well as the results of research into the gen-eration of demonstrative pronouns. Furthermore, we discuss some perspectives on potential uses of this corpus as well as our planned future work, underlining how it is a valuable addition to the existing databases in the community for the study and modeling of referring expressions in situated dialog.
 Keywords Multi-modal corpus Referring expressions Collaborative task Japanese 1 Introduction Referring expressions are a linguistic device referring to a certain object and play a fundamental role in smooth collaboration between humans and agents where physical operations are involved. Thus, research into the understanding and generation of referring expressions has been a critical research theme in order to develop more efficient and natural human-agent interaction. This has given rise to a substantial international research community within Computational Linguistics. The work presented in this paper is a contribution to the creation and utilization of resources for this field.

From the beginning, research in the field of analysis of referring expressions, by the nature of the problem it deals with (e.g. anaphora resolution), has been faced with the necessity of treating the broader textual context of referring expressions (Hobbs 1978 ; Grosz et al. 1995 ). In contrast, in the field of language generation the expressions dealt with were mainly isolated expressions; hence early work in this area (Dale 1989 ; Dale and Reiter 1995 ) focused largely on studying the generation of isolated referring expressions in a static environment of invariant situations (e.g. an image) and on how those expressions were able to distinguish the target object from the distractors.

Their simplifying assumption was that other factors outside the current (static) situation, such as the context of interaction, would not have an impact on the formulation and understanding of a referring expression. Based on these same assumptions and in order to allow a unified evaluation of such algorithms, the TUNA corpus was recently developed at Aberdeen University (van Deemter 2007 ). It is the most extensive collection of referring expressions to date, containing roughly 2,000 expressions. At the same time, it has the limitation of only taking into account individual expressions in an interaction-free setting. Another recent corpus that has a similar limitation is the GRE3D3 corpus (Dale and Viethen 2009 ). This corpus is focused on recording how humans use relational referring expressions and is significantly smaller (about a third of the size of the TUNA corpus). Both the TUNA and GRE3D3 corpus collect expressions from one individual without any interaction.

Corpora in such static settings allow for the pursuit of interesting specific research-questions in a relatively simple domain. This is reflected for example in the organization and results of competitive events such as the TUNA-Challenge at ENLG 2009 (Gatt et al. 2009 ). However, this type of setting differs significantly from actual human reference behaviour, where the collaborative aspect plays a central role. In fact, this point has been noted very early on (see for example Bolt 1980 ; Clark and Wilkes-Gibbs 1986 ; Heeman and Hirst 1995 ).

As research on referring expressions has made progress, there has been an increasing move away from uniquely looking at the linguistic mode and towards studying the multi-modal phenomenon of reference in domains approximating the complexity of actual human behaviour. Thus there has been ongoing work towards developing algorithms that deal with linguistic and various types of extra-linguistic information in the framework of one computational model (Kelleher et al. 2005 ; Byron et al. 2005 ; Kranstedt et al. 2006 ; Gergle and Kraut 2007 ; van der Sluis et al. 2008 ; Stoia et al. 2008 ; Prasov and Chai 2008 ). This is partly a reflection of a general tendency towards the creation of multi-modal corpora in a number of domains (e.g. Qvarfordt et al. 2005 ; Schiel and Mo  X  gele 2008 ; Blache et al. 2009 ).
Traditionally, many corpora have relied on manual work for their construction from speech and video recordings (e.g. Jokinen 2010 ). In our corpus as well, linguistic information was transcribed and annotated manually, but extra-linguistic information was recorded and processed automatically. While reducing cost and increasing precision of the collected data, this approach might have disadvantages such as imposing limitations on the type of data that can be recorded (only automatically recordable). Overall, at the current stage of research both data collection by hand and automatic recording have their respective advantages.
Along with such progress in multi-modal data collection, recently corpora have been created in order to study referring expressions occurring in situated collaborative tasks. Given that such corpora seek to record increasingly realistic interactions and the role referring expressions play in them, there has been a large variety of research focal points that have been pursued: such as research into the co-occurrence of referring expressions with pointing behaviour (van der Sluis et al. 2008 ), looking into the role of visual information in reference (Gergle and Kraut 2007 ) and the influence of various pragmatic dimensions on the use of proximity markers (Byron and Stoia 2005 ). One question that has not yet received significant attention is the interaction between the referring expressions and the actions performed by participants in collaboration tasks. However, in the context of working on a physical task, referring expressions are particularly important linguistic tools. Hence deepening our understanding of the interaction of linguistic information with extra-linguistic information on the actions in such a setting remains a critical task. The REX-J corpus, which we introduce in this paper, is a resource contributing to research in this area.

One major trend in research on human referring expressions has been based on the  X  X  X entering theory X  X  (Grosz et al. 1983 ; Brennan et al. 1987 ). It employs rules and constraints that govern the relationship between discourse content and the linguistic choices made by the discourse participants, such as the choice of syntactic fundamentally viewed as a dynamic process, where each sentence/utterance is a transition from an input state to an output state. Within this context, there have been studies for example attempting to predict which entities will be most salient at any given time (Poesio et al. 2000 ). Based on the centering theory, there has also been a significant amount of work on Japanese referring expressions, more specifically on the interpretation of anaphora (Walker et al. 1994 ; Kameyama 1998 ). We note that such work has not resulted in the construction of corpora per se.

This paper is organized as follows. Section 2 provides a succinct review of other existing multi-modal corpora of referring expressions in a collaborative task. Section 3 describes the construction of the REX-J corpus, including the annotation policies, and discusses the key characteristics of the collected data. Section 4 introduces two examples of usages of this corpus: an analysis of a specific type of referring expression as well as the generation of demonstrative pronouns in a collaborative task. Section 5 presents the conclusion, some potential research directions that might be pursued exploiting this corpus as well as our plans for future work. 2 Brief review of existing multi-modal corpora of referring expressions in collaboration The construction of corpora in realistic domains is a critical task in order to achieve major progress in our understanding of referring expressions. A fundamental aspect that needs to be captured is the complexity which humans themselves are confronted with, when uttering or hearing referring expressions. Over the last decade, given the recognition that referring expressions are very frequently uttered in the context of a collaborative task (Clark and Wilkes-Gibbs 1986 ; Heeman and Hirst 1995 ), a number of databases have been constructed in order to study referring expressions in such a domain. This tendency is also a reflection of the recognition that this area yields both a challenging, more realistic research domain, as well as promising possibilities for application, such as natural-language collaboration with robots on certain tasks (e.g. Foster et al. 2008 ; Kruijff et al. 2010 ).

The COCONUT corpus (Di Eugenio et al. 2000 ) is collected from keyboard-dialogs with enforced turn-taking and no interruptions allowed. The participants collaborate on a simple 2-D design task, i.e. buying and arranging furniture for two rooms. It resembles the TUNA corpus in tending to encourage very simple types of expressions by the participants. The COCONUT corpus has rich annotations at the linguistic and the intentional level including three kinds of features: (i) problem-solving utterance features, (ii) discourse utterance features and (iii) discourse entity features. However, it does not include extra-linguistic information, e.g. physical actions. Thus, in addition to the limited interaction naturalness, this corpus also has a limitation in terms of the recorded information, particularly extra-linguistic information. As an initial work in the construction of collaborative task corpora, the COCONUT corpus can be characterized as having a rather simple domain as well as a limited annotation. There has been some work utilizing this corpus for attribute selection as well as partner-specific adaptation (Jordan and Walker 2005 ; Gupta and Stent 2005 ).

More recent work has mainly concentrated on seeking to overcome the shortcoming of domain complexity. Thus, the QUAKE corpus (Byron and Fosler-Lussier 2006 ) X  X s well as the more recent SCARE corpus (Stoia et al. 2008 ), which is an extension of QUAKE X  X s based on an interaction captured in a 3-D virtual reality world where two participants collaboratively carry out a treasure hunting task. Byron et al. ( 2005 ) exploits these two resources for research on the generation of noun phrases. While those two corpora deal with a rather complex domain (3-D virtual world), the participating subjects were only able to carry out limited kinds of actions (pushing buttons, picking up or dropping objects) relative to the complexity of the three-dimensional target domain. One of the reasons for this is that they focused on location-based references while we focused on event or action-based references.

As part of the JAST project, a Joint Construction Task (JCT) corpus was created based on two subjects participating in constructing a puzzle (Foster et al. 2008 ). The setting of the experiment is quite realistic and natural in that both participants can equally intervene in the task. Based on this corpus, an analysis of a specific type of referring expression in task-oriented dialog is carried out. Furthermore, the role of eye-gaze in this setting is investigated (Bard et al. 2009 ). While the authors note that the  X  X  X ranscribed speech was precisely time-aligned with all the visual and action components of the construction process X  X , they do not provide further details on exactly what data they have recorded in their corpus. Despite the differences in some particulars of the task setting, further work on both the JCT as well as the REX-J corpus will allow interesting comparisons of specific phenomena between English and Japanese.
 In contrast to these previous corpora, which were all in English, we created a Japanese corpus recording a whole range of information potentially relevant in the collaborative human reference process in a situated setting.  X  X  X ituated dialog X  X  in this context stands for a dialog where the participants are not solely communicating through linguistic means but share a space, and can act upon objects in that space. Participants had to solve the Tangram puzzle collaboratively. While the domain of our corpus is simple compared to the QUAKE and SCARE corpora, we allowed comparatively large flexibility in the actions necessary to achieve a task (such as flipping, turning and moving of puzzle pieces at different degrees), relative to the task complexity. Figure 1 provides a schematic comparison of the recent corpora discussed above in terms of the dimensions of relative operation/annotation complexity as well as domain complexity. Providing this relatively larger freedom of actions to the participants in the REX-J corpus together with recording the related information enables us to pursue research into new aspects of referring expressions that cannot be pursued based on previous corpora.

Among the existing task-oriented corpora, there are important distinctions in terms of the affordances of the experimental set-up of the task; that is whether pointing is possible (e.g. JCT corpus) or not (e.g. COCONUT corpus) and whether the dialog partners share an identical view of the task space (e.g. JCT corpus) or not (e.g. Map Task corpus (Anderson et al. 1991 )). Obviously, the choices regarding these aspects strongly influence the character of the dialog and the types of referring expressions used by the participants. In addition, the number of speakers is an important distinction between corpora. While the corpora in static domains are mostly monolog such as in the TUNA corpus, situated corpora are built based on dialogs. Our corpus is also based on dialog. Furthermore, we chose a set-up where pointing was possible only for one participant while only the other could see the goal shape as well as the position of the mouse cursor. Thus, the two participants did not have an identical view of the task space.

Previous corpora have had a large bias towards English resources. While creating corpora in a language accessible world-wide is surely critical, the construction of language resources in other (and very different) languages is also a significant task that needs to be addressed by the community (Tokunaga et al. 2008 ). Such work will also encourage the pursuit of comparative studies across languages of specific linguistic phenomena (e.g. the use of referring expressions) as well as the interaction between linguistic and extra-linguistic information in such a domain. Furthermore, in the study of referring expressions there has been notable interest over the recent period in the area of intersection of Computational Linguistics and Cognitive Science (van Deemter et al. 2009 ). The REX-J corpus also makes a contribution in this broader sense.

In the construction of a corpus, a question to be considered is the trade-off between on the one side the  X  X  X aturalness X  X  of the domain and on the other the balance of the recorded data in various respects. Within the language analysis research community, the collection of  X  X  X pen domain X  X  corpora X  X uch as considered at the MUC and ACE-conferences (Grishman and Sundheim 1996 ; Strassel et al. 2008 ) X  X as been the standard approach and there has not been a significant discussion on the need to collect balanced databases. On the other hand, within the language generation community there have been some arguments advanced underlining the need to create  X  X  X alanced X  X  corpora. Gatt et al. ( 2007 ) argue that ideally a corpus of referring expressions should both be balanced  X  X  X emantically X  X  (an equal number of situations requiring an equal set of attributes; a  X  X  X alance of minimal descriptions X  X ) and in terms of the target referents (different kinds of referents occur an equal number of times). van der Sluis et al. ( 2008 ) started work towards such a balanced multi-modal corpus. However, while in a static setting  X  X  X alance X  X  is a useful guideline for corpus construction, as van der Sluis et al. ( 2008 ) themselves note, in a collaborative domain this  X  X  X rguably leads to a certain degree of artificiality in the conversational setting X  X . We note that our corpus is not  X  X  X alanced X  X  as defined by Gatt et al. ( 2007 ). In a sense, we approach the question from the opposite end. We intend to collect a corpus enabling the study of various aspects of referring expressions through a comprehensive recording of the reference process in a collaborative task. 3 Construction of the corpus 3.1 The experimental set-up In the process of building the REX-J corpus, we recruited 12 Japanese graduate students (4 females and 8 males) of the Cognitive Science department of Tokyo Institute of Technology, and split them into 6 pairs. All pairs knew each other previously and were of the same sex and approximately same age. 1 Each pair was instructed to solve the Tangram puzzle through collaborating with each other. The Tangram puzzle is a geometrical puzzle that originated in ancient China. The goal of this puzzle is to construct a given shape by arranging seven pieces of simple figures as shown in Fig. 2 . The pieces include two large triangles, a medium-size triangle, two small triangles, a parallelogram and a square.

With the aim of recording the precise position of every piece and every action the participants carried out during the solving process, we implemented a Tangram simulator in which the pieces on the computer display can be moved, rotated and flipped with simple mouse operations. The simulator displays two areas: a goal shape area (the left side of Fig. 2 ) and a working area (the right side of Fig. 2 ) where pieces are shown and can be manipulated.

We assigned a different role to each participant of a pair: a solver and an operator . Given a certain goal shape, the solver thinks of the necessary arrangement of the pieces and gives instructions to the operator how to move them. The operator manipulates the pieces with the mouse according to the solver X  X  instructions. In this interaction, we can expect relatively frequent uttering of referring expressions intended to distinguish certain pieces of the puzzle. In our Tangram simulator, all pieces are of the same color, thus color is useless in identifying a specific piece. Unlike the TUNA corpus, only size and shape are useful object-intrinsic attributes. Instead, we can expect other attributes such as spatial relations and deictic reference to be used more often in the dialogs.

The participants of a pair sit side by side as shown in Fig. 3 . A shield screen was set between the solver and operator to prevent the operator from seeing the goal shape on the solver X  X  screen, and to restrict their interaction to only speech. Each participant has her/his own computer display sharing the working area where the movement of the mouse cursor and the pieces is shown in real-time. The operator (left of the separation) has a mouse for manipulation of pieces, but does not have a goal shape on the screen. In contrast, the solver (right of the separation) has a goal shape on the screen but not a mouse. Each participant pair is assigned 4 trials to each form two symmetric and two asymmetric shapes. The participants exchanged their roles after two trials, i.e. a participant first solves a symmetric and then an asymmetric puzzle as the solver and then does the same as the operator, and vice versa. The order of the puzzle trials is the same for all pairs. If we had a substantially larger pool of participants and the necessary resources, it might be of interest to vary the order of the tasks in order to counterbalance trials. However, because of insufficient number of participants, we maintained the same order throughout the data collection.
 Figure 4 shows the four different goal shapes of the puzzle given to the subjects. In Cognitive Science, a wide variety of different kinds of puzzles have been employed extensively in the field of Insight Problem-Solving. This has been termed the  X  X  X uzzle-problem approach X  X  (Sternberg and Davidson 1996 ; Suzuki et al. 2001 ) and in the case of physical puzzles has relatively often involved puzzle tasks of symmetric shapes (like the so-called T-puzzle, e.g. Kiyokawa and Nakazawa 2006 ). In more recent work the Tangram puzzle has been used as a means to study various new aspects of human problem solving approaches, such as utilization of eye-tracking information (Baran et al. 2007 ). In order to collect data as broadly as possible in this context, we set up puzzle-problems including both symmetrical ((a) and (b) of Fig. 4 ) as well as asymmetrical ones ((c) and (d) of Fig. 4 ).

Before starting the first trial as the operator, each participant had a short training exercise in order to learn the manipulation of pieces with the mouse. The initial arrangement of the pieces was randomized every time. We set a time limit of 15 min for the completion of one trial (i.e. construction of the goal shape). In order to prevent the solver from getting into deep thought and keeping silent, the simulator is designed to give a hint every five minutes by showing a correct piece position in the goal shape area. After 10 min have passed, a second hint is provided, while the previous hint disappears. A trial ends when the goal shape is constructed or the time is up. Utterances by the participants are recorded separately in stereo through headset microphones in synchronization with the position of the pieces and the mouse operations. The positions of the mouse cursor and of all pieces as well as the mouse actions were automatically generated by the simulator. They were recorded in the pixel coordinates with time stamp at intervals of 1/65 s. We collected 24 dialogs (4 trials by 6 pairs) of about 4 h and 17 min. The average length of a dialog was 10 min 43 s (with a standard deviation SD of 3 min 16 s). 3.2 Annotation All 24 recorded dialogs were transcribed with a time code attached to each utterance. Since our central objective is the collection of referring expressions, we defined an utterance to be a complete sentence in order to prevent a referring expression being split across several utterances. In this respect, our  X  X  X tterance X  X  tends to be longer than those of ordinary dialog corpora in which an utterance is usually defined as a segment between pauses with a certain length.

In order to create an annotated corpus of referring expressions, we need to address two distinct steps in the annotation process. First, we have to determine where the referring expressions are (i.e. their span in a text) X  X ere denoted as the expression identification step X  X nd then identify their referent  X  X he referent identification step. For both steps, we utilized the web-based multi-purpose annotation tool SLAT (Noguchi et al. 2008 ). Figure 5 shows a screenshot of the SLAT annotation environment.

Two native Japanese annotators A 1 and A 2 (two of the authors) annotated 4 transcribed dialog texts independently, and resolved discrepancies by discussion. Based on the discussion, we decided on the following annotation guidelines.  X  Only referring expressions whose referent is either one piece or a set of pieces in  X  A minimum span of a noun phrase with necessary information to identify a  X  Demonstrative adjectives are included in expressions, e.g.  X  X  sono sankaku ( that  X  Erroneous expressions are marked with a comment, e.g.  X  X  tiisai sankaku (small  X  An expression without a definite referent (i.e. a group of possible referents or  X  All expressions appearing in muttering to oneself are excluded.

Based on these guidelines, the two annotators annotated the remaining 20 dialog texts independently. We evaluated inter-annotator agreement of these annotations by using the b -coefficient (Artstein and Poesio 2005 ) for the expression identification, and the j -coefficient for the referent identification. Although the j -coefficient is a de facto standard in evaluating annotation reliability of corpora (Carletta 1996 ), it is unweighted and thus does not differentiate between different degrees of disagreement. The fact that all disagreements are treated equally is a  X  X  X erious limitation X  X  in measuring discrepancies of spans of referring expressions (Artstein and Poesio 2008 ). Hence the j -coefficient is an inappropriate measure for annotator agreement on expression identification. 3.2.1 Expression identification In order to measure agreement on the identification of referring expressions, we need to account for a variety of degrees of disagreement, such as total mismatches or small boundary differences of one or two words. The b -coefficient has been used in measuring the inter-annotator agreement of multi-modal corpora, for example in Foster and Oberlander ( 2007 ). It has been noted that the actual value of b highly depends on how the annotation disagreements are categorized and how penalty weights are assigned to the categories. We adopted the same categories and penalty weights as Foster and Oberlander ( 2007 ) to calculate b . Other than the total match, disagreements in the annotation are categorized into three groups, given with their respective penalty weights in brackets: (a) strict subset (1/3), (b) overlap (2/3) and (c) absolute mismatch (1). Since we consider the matching based on the transcribed texts, this categorization can be done automatically.

The b -coefficient is calculated based on the ratio between the expected and observed disagreements in the annotation as follows: where D obs and D exp respectively stand for the arithmetic mean of the observed disagreement values on the annotated spans, and the expected disagreement calculated from the distribution of annotated spans by each annotator. Values of b close to 1 reflect good agreement between the annotators, where the actual disagreement is much smaller in comparison to the expected disagreement. In contrast, values closer to 0 reflect bad agreement, where the observed disagreement approaches the expected disagreement.

The calculation of the observed disagreement is obvious. However, in terms of the expected disagreement, we need to note a difference to previous work on English corpora. The expected disagreement D exp is calculated from the distribution of expression lengths. While in a language such as English, the length of an expression can be measured by the number of words, such a word count is not obvious in languages like Japanese (and many East Asian languages), which does not use explicit word delimiters. In fact, there is a significant amount of research on automatically detecting word boundaries in Japanese which has led to the development of a number of tools such as the morphological analyzer MeCab (Kudo et al. 2004 ). This point indicates the type of issues faced with when building and evaluating non-English resources (see Tokunaga et al. 2008 for a broader discussion of this point in relation to Asian languages).

For the current purpose, we took a simplifying approach of disregarding words and measuring the length of expressions on a character basis. While this obliterates the distinction between semantic units (words) in the calculation and thus will have a tendency to slightly increase the expected disagreement (and hence increasing b -values), it provides a rough approximation. Computation according to the above yields b = 0.664. This means that the observed disagreement was less than half of the expected disagreement. We note that on the b -coefficient, no significance test is possible (Artstein and Poesio 2005 ). 3.2.2 Referent identification We calculated the inter-annotator agreement of the reference identification using the cases categorized as  X  X  X trict subset X  X ,  X  X  X verlap X  X  and  X  X  X otal match X  X  in the previous step. As for the agreement of referent identification, we adopted a rigid agreement judgement, using the j -coefficient for evaluation. Calculation yields j = 0.904, which allows definite conclusions according to Krippendorf X  X  scale to assess j -values (Krippendorff 1980 ).

Cavicchio and Poesio ( 2009 ) discuss advantages and disadvantages of several recently employed statistical metrics for assessing the reliability of annotation of multi-modal corpora, clarifying there is not one  X  X  X est X  X  evaluation metric. As discussion and further work continues on developing more unified reliability measures, one way to deal with this problem at this point would be to measure inter-annotator agreement of a corpus in terms of several different metrics, thus providing evaluations from different standpoints. Based on such considerations, in Table 1 we show inter-annotator agreement in percentages of both annotation steps discussed in this section. The row for A 1 shows the agreement values considering A 2  X  X  annotation as a gold standard, and likewise respectively for the row for A 2 .  X  X  X aw X  X  here denotes the percentage of the cases where one annotator X  X  annotation is exactly the same as the other X  X , i.e. both recognized the same span of a text as a referring expression and identified the same referent(s).  X  X  X /Span X  X  and  X  X  X /Ref X  X  respectively denote the rate of agreement when ignoring discrepancy in span and referent(s) respectively. 3.3 The recorded data We collected a total of 1,443 tokens and 425 types (different surface realizations) of referring expressions in 24 dialogs. The size of our corpus is roughly comparable to both the SCARE corpus (15 dialogs with 1,700 expressions) and the COCONUT corpus (24 dialogs with 1,100 utterances). Our asymmetric experimental setting tended to encourage referring expressions from the solver, while the operator mainly employed referring expressions in order to confirm his understanding of the solver X  X  instructions. This is reflected in the number of referring expressions by the solver (1,243 tokens) largely outnumbering those of the operator (200 tokens).

The annotated referring expressions include indefinite expressions 2 whose referents cannot be determined uniquely. For example, since there are two large triangles in the puzzle, there might be a case where the expression  X  X  o  X  kii sankaku (a large triangle) X  X  can refer to either one of them. We annotated these indefinite expressions with an  X  X  X ndefinite X  X  attribute together with possible referent candidates. We exclude such indefinite expressions as well as expressions explicitly referring to multiple pieces from the further analysis reported hereafter. The number of the excluded expressions is 201, about 15% of the total. Thus, the number of the remaining expressions under consideration which refer to a single piece is 1,242. Tables 2 and 3 further detail the expressions collected by dialog and by speaker.
One of the annotators further annotated these expressions with the syntactic and semantic features listed in Table 4 . The following syntactic/semantic features in referring expressions can be identified: (a) demonstratives (adjectives and pronouns), (b) intrinsic attributes of pieces, (c) spatial relations and (d) actions on an object. Note that multiple features can be used in a single expression. The right-most column shows an example from the corpus with its English translation. The identified feature in the referring expression is underlined.

We utilized the multi-modal annotation tool ELAN 3 in order to merge the manually annotated linguistic information and the extra-linguistic information generated by the Tangram simulator. The extra-linguistic information includes (i) the action on a piece, (ii) the coordinates of the mouse cursor and (iii) the position of each piece in the working area. The actions and the mouse cursor positions are recorded at intervals of 1/65 s. This information is then automatically abstracted into (i) a time span labeled with an action symbol ( X  X  X ove X  X ,  X  X  X otate X  X  or  X  X  X lip X  X ) and its target object number (1 X 7), and (ii) a time span labeled with a piece number which is under the mouse cursor during that span. The position of puzzle pieces is updated and recorded with a time stamp whenever the position of any piece changes. We did not merge the information about piece positions into the ELAN files and kept it in separate files. As a result, we have 11 time-aligned ELAN Tiers. They are represented by the lines marked by colored labels in the bottom half of Fig. 6 . We have Tiers such as  X  X  X V-REX X  X  (referring expressions by the solver) and  X  X  X ouse X  X  (the piece on which the mouse is hovering). 4 Two example uses of the REX-J corpus Multi-modal corpora of referring expressions have been utilized to pursue research in a range of fields, often with a view towards language generation. Jordan and Walker ( 2005 ) utilized the COCONUT corpus and worked on the generation of content descriptions (attribute sets) of referring expressions. Stoia et al. ( 2006 ) sought to generate referring expressions given NP frame slots as input, utilizing various context variables as features in a machine-learning approach. In this section, we provide results from two investigations, one each in the area of language analysis and generation, pointing to some of the various potential uses of the REX-J corpus.
 4.1 Action-mentioning expressions Analyzing the expressions that appear in our corpus, we found a type of referring expression that mentions an action on an object (Spanger et al. 2009a ), such as  X  X  ima hanten sita yatu (the one you just flipped over) X  X . This type of referring expression X  which we call an action-mentioning expression ( X  X  X ME X  X  hereafter) X  X as attracted less attention in previous work in the context of collaborative interaction. AMEs are related in a broad sense to English expressions such as taken up in Novak ( 1986 ), who sought to generate a description of a traffic scene using events (e.g.  X  X  X aking over X  X , etc). Novak ( 1986 ) concentrated on event verbalization (such as  X  X  X he car is moving X  X ) employed in order to distinguish an object in a description task. He provided no further analysis of the relation of this kind of expression with other features in referring expressions. The domain he deals with is a simple observed situation not including any collaboration and thus very different from the domain discussed here. Furthermore, we also note that AMEs are different from haptic-ostensive referring expressions discussed in Foster et al. ( 2008 ) since AMEs are not necessarily accompanied by a simultaneous physical operation on an object.

In contrast to Novak ( 1986 ), we focus on actions carried out on objects in a collaborative task setting. Considering this phenomenon in the context of a situated dialog allows us to investigate in detail the interaction of actions on objects by participants and the use of AMEs. Figure 7 shows an example of an AME from the corpus (Dialog 21) with its context. On the right side of the figure, we show the time-aligned information on the referring expressions (SV-REX), operations on pieces (Action) as well as the time periods when the mouse is over a certain piece (Mouse). The referent of the AME in this example is the recently moved large triangle (2), with its previous position indicated in broken lines. Generally, in a collaborative task as considered here, we can assume actions to be very salient for both participants. In comparison to other shorter expressions such as demonstratives or the simple use of the shape of the piece, an AME might be longer and thus has less ambiguity.

On average, every pair used about 14 AMEs over the four trials. Although there was variation in the usage of AMEs among the pairs ( SD &amp; 9), all 6 pairs of participants used AMEs, indicating that it is a fundamental type of expression for this task setting. 84 out of 85 AMEs were used by the solvers. This is explained by the asymmetric setting of our experiment.

In order to resolve AMEs, factors such as the saliency of actions mentioned in the expressions as well as recency will be useful. Traditionally, recency means that the more recently an object was mentioned, the more salient it will be, thus more likely to be the referent of a referring expression. We note that in other domains, specific actions might be explicitly referred to (e.g.  X  X  X lease take the pink dress I gave you for your birthday last year X  X ), without requiring any notion of recency. However, in the context of a collaborative task domain, such mention of actions taken a long time ago will be rare and rather exceptional. It has been pointed out in previous work that recency is also one of the most significant factors for the resolution of demonstrative pronouns (Mitkov 2002 ). As shown in Table 4 , demonstrative pronouns along with the attribute  X  X  X hape X  X  are the most frequent feature of referring expressions in the REX-J corpus. More generally, demonstrative pronouns such as  X  X  kore (this) X  X ,  X  X  sore (it) X  X  and  X  X  are (that) X  X  are a fundamental device for referring to objects (Halliday and Hassan 1976 ).

Based on the extensive previous research underlining the importance of recency for the resolution of demonstrative pronouns and our intuition that recency will be also useful for the understanding of AMEs, we looked into the relationship between the frequency of AMEs/demonstrative pronouns (DPs) and the time distance to both the last linguistic mention and the last physical action with regard to the referent. Since AMEs were used almost exclusively by the solvers, we considered only DPs used by the solvers for comparison.

Defining recency in terms of the last linguistic mention of an object corresponds to the traditional concept of textual recency as employed in reference resolution. As shown in Fig. 8 , we confirmed the very fast decay of the use of DPs over the first 30 s from the last mention and from the last action. Relative frequencies for those two cases seem to show a similar decay pattern at first glance. However, a v 2 test on the absolute frequencies of  X  X  X P last mention X  X  and  X  X  X P last action X  X  over the time distance showed a significant difference at 0.01% level. This difference suggests that recency in terms of the last action is also useful for reference resolution (Iida et al. 2010 ).

Figure 8 also indicates that the relative frequencies of AMEs over the time distance to the last action show a quite strong decay pattern similar to DPs. In contrast, the relative frequencies of AMEs when measured over the time distance to the last mention decrease much slower. We confirmed a significant difference between the decay patterns of  X  X  X ME last mention X  X  and  X  X  X ME last action X  X  by a v 2 test at 5% level.

The analysis of the frequency decay of DPs and AMEs over the time distance provides new perspectives on the concept of recency. In particular, previous research considered recency mainly in a textual context focusing almost exclusively on linguistic information (Walker et al. 1994 ). Hence, recency was often measured by metrics such as the number of sentences between a referring expression and its antecedent. However, our analysis indicates that the recency of the last linguistic mention shows a different tendency from the recency of the last physical action. There is a possibility that the action-based recency can contribute to resolving AMEs and DPs. Thus, we propose an extended notion of recency in a physical sense, i.e. how far back an object was physically manipulated.

In addition to measuring recency in terms of time distance, we also consider a different metric of recency in terms of the number of distractors. Here  X  X  X istractors X  X  are defined as either those objects manipulated since the last action on the referent or those objects mentioned since the last mention of the referent. Figure 9 shows relative frequencies of DPs and AMEs based on this metric.

As in Fig. 8 , the decay of relative frequencies of DPs over the number of distractors when measured from the last mention and from the last action, seems to show a similar tendency in Fig. 9 . However, a v 2 test revealed a statistically significant difference (at 5% level) between  X  X  X P last mention X  X  and  X  X  X P last action X  X . This result is similar to the result of our analysis of frequencies measured over the time distance. For AMEs,  X  X  X ME last action X  X  seems to decrease faster than  X  X  X ME last mention X  X . However, a v 2 test did not show a significant difference between them.
We examined two factors for recency: the metric for measuring distance (the time distance vs. the number of distractors) and the definition of the starting point of intervals (the last mention vs. the last action). Overall, for both DPs and AMEs, there were different tendencies of relative frequencies when measured in relation to the last action or the last mention. In contrast, the results showed similar tendencies for both distance measuring metrics. This suggests the need to investigate any possible difference in impact of those factors on the resolution of DPs and AMEs.
AMEs consist of a noun modified by an adnominal phrase, which often includes a verb describing an action and temporal adverbials. Considering the surface structure of AMEs, they can be divided into three categories based on the elements constituting the adnominal phrase: (a) combination of a temporal adverbial with a verb like  X  X  sakki kaiten saseta o  X  kii sankaku (the large triangle you just turned around) X  X , (b) a temporal adverbial without a verb, i.e. verb ellipsis such as  X  X  ima no heikousihenkei (the parallelogram you are [verb-ing] right now) X  X  and (c) a verb without a temporal adverbial such as  X  X  sono itiban ue ni oita sankakkei (that triangle [you] put at the very top) X  X . Table 5 shows the frequency distribution of these three categories in the corpus.

The second category, which includes verb ellipsis, would be rare in English, but it is quite natural in Japanese. This is also related to differences in syntax between Japanese and English; while in English expressions such as  X  X  X ow X  X  are temporal adverbials and as such modify a verb, Japanese temporal adverbials can be adnominals with the particle  X  X  no (of) X  X . In addition, comparing the numbers of the second and the third categories, it is interesting to see that temporal adverbials tend to be more explicitly mentioned. This kind of consideration is an example of the comparative analysis fostered by the creation of resources in languages other than English.

The dominant temporal adverbials used by the participants were  X  X  sakki (just before) X  X  and  X  X  ima (now) X  X , e.g. sakki no NP (the NP [ verb -ed] just before) X  X  or  X  X  ima no NP (the current NP/the NP [you are verb -ing] now/the NP [ verb -ed] just before) X  X .  X  X  Ima  X  X  generally refers to the current time point ( X  X  X ow X  X ). It can, however, refer to the recent past as well and thus is ambiguous. We note that in case a verb is explicitly mentioned, disambiguation is possible based on its aspect (progressive vs. perfect). Participants tended to use  X  X  ima  X  X  largely in its perfect meaning (completed action). The distribution of use of  X  X  ima  X  X  in its perfect meaning in comparison to its progressive meaning was about 2:1 (31 instances/15 instances). The distribution of the two types of temporal adverbials  X  X  sakki  X  X  and  X  X  ima  X  X  was about 2:3 (29/46). The higher frequency of  X  X  ima  X  X  might be explained by its dual meanings (progressive and perfect) in contrast to the exclusive use of  X  X  sakki  X  X  for past actions.
Figure 10 shows the distribution of  X  X  sakki (just before) X  X  and that of  X  X  ima (now) X  X  in its perfect meaning, over the time distance to the mentioned action. For actions occurring within a time frame of about 10 s previous to an expression, participants had an overwhelming preference for  X  X  ima  X  X , while for actions further in the past, participants preferred  X  X  sakki  X  X . We carried out a v 2 test over the differences between  X  X  sakki  X  X  and  X  X  ima  X  X , which showed a significant difference at 1% level.
In the above discussion, we saw that the use of temporal adverbials in AMEs is highly dependent on the temporal distance to the action. Considering this phenomenon, we note an interesting parallel with demonstrative pronouns, whose use is very much tied to the concept of spatial distance (i.e. the distinction between kore (this) and sore (that)).

In our analysis of AMEs as shown in Figs. 8 and 9 , we discussed how recency can be useful in terms of a physical action as well as a linguistic mention. We furthermore showed how the distance to these events can be defined in different ways: temporal intervals versus the number of distractors. The comparison of AMEs with pronouns led us to extend the traditional definition of recency in two directions, both emphasizing the importance of physical actions. 4.2 Generation of demonstrative pronouns Pursuing research into new aspects in the generation of referring expressions is a further possible use of the REX-J corpus. In recent work both Jordan and Walker ( 2005 ) and Stoia et al. ( 2006 ) studied the generation of referring expressions in specific dialog settings, with Stoia et al. ( 2006 ) particularly stressing the importance of integrating extra-linguistic information as part of the context. Thus, they introduced  X  X  X patial/visual features X  X  in addition to dialog history. Jordan and Walker ( 2005 ) provide a number of theoretically motivated features such as  X  X  X ntentional influences X  X  (modeling the current state of the task as well as agreement by the participants). The REX-J corpus allows us to reflect on the role of extra-linguistic information in the production of referring expressions (Spanger et al. 2009b ). This section briefly describes our recent work on the generation of demonstrative pronouns in a collaborative task.

In fact, while accounting for certain types of extra-linguistic information, the above approaches neither dealt with information on current operations nor on the actions that have been performed by participants during the course of the collaboration. In previous work, it has been pointed out that in a collaborative task (e.g. the construction of an object), participants X  actions on objects impact their reference behaviour to a significant degree (Foster et al. 2008 ). Thus, we can project that the development of a model integrating information from the linguistic as well as other modes will be vital to deepening our understanding of referring expressions in this domain.

Seeking to replicate human use of demonstrative pronouns in the corpus, we employed a machine learning approach utilizing linguistic as well as extra-linguistic information. This information is then employed as features in a Support Vector Machine (SVM) X  X  supervised learning method for binary classification (Vapnik 1998 ).
We defined the 12 features shown in Table 6 in order to represent a specific situation of the task context, at the time point when a target is mentioned. The features are categorized into three categories: dialog history features (D1 X  X 5), action history features (A1 X  X 5) and the current operation features (O1 and O2). While the features D1, A1 and D4, A4 measure time distances, the features D3 and A3 measure the number of other pieces referred to/acted upon. We recall that overall decay patterns of relative frequencies measured by those two metrics were very similar (see Figs. 8 , 9 ). However, this does not necessarily mean both metrics have an equal impact on the generation of DPs. The addition of features A3 and D3 will allow us to look into this question, in comparison to the time distance metric encoded in features D1, A1 and D4, A4.

The dialog features model the dialog history, while the operation and action features capture key aspects of the physical actions that might have an impact on the accessibility of the target and thus the usage of demonstratives.

In order to provide a more detailed view of the data encoded, all features were then split up according to their respective values. The time distance features (A1, A4, D1 and D4) were discretized into three intervals: (0, 10] sec, (10, 20] sec and (20, ? ] secs based on the analysis of action-mentioning expressions in the previous section, particularly Fig. 8 . This results in a total of 27 features as shown in Table 7 .

Our aim is to automatically decide whether or not to use a demonstrative pronoun to mention a target in a specific situation which is described in terms of the above features. We constructed an SVM classifier which classifies a pair comprised of a target piece and a situation represented by the above features into two classes:  X  X  X emonstrative pronoun X  X  and  X  X  X ther X  X . We employed the SVM-light software (Joachims 1998 ) with 1,242 instances of referring expressions referring to one target object, extracted from our corpus and applied a tenfold cross validation.

Table 8 shows the overall results of the classification, with a separate row each for removing features of a category (dialog history, action history and current operation) from the set of all features, here denoted by  X  X  X ll X  X . The baseline suggests the use of a demonstrative pronoun, whenever the most recently mentioned object is the target, and suggests the non-use of a demonstrative pronoun otherwise. We see that the baseline achieves a smaller F-measure than when using all features. In contrast, removing the operation features leads to a worse F-measure than the baseline.
 While removing either dialog or action history (the rows  X  X  X /o D1 X  X 5 X  X  and  X  X  X /o A1 X  X 5 X  X ) has a negligible effect on the overall F-measure, we note significant performance deterioration when removing the current operation features (O1 and O2). This reflects the fact that information of the ongoing action has a particularly strong impact on the use of demonstrative pronouns.

The asymmetric setting of the REX-J corpus results in most referring expressions being used by solvers (e.g. out of all demonstrative pronouns, 401 are by the solver, 147 by the operator), who are not allowed to point at pieces. In a situation where the mouse cursor is on the target, we are in fact dealing with a joint action , with the solver using a linguistic expression while the operator points to a piece.

Differentiating it from a traditional pointing action, we might be able to call this phenomenon  X  X  X ollaborative pointing X  X . This is closely related to the joint attention effect discussed in Diessel ( 2006 ).
 The results discussed here X  X n spite of the language difference of Japanese vs. English X  X lso support the claim of Piwek ( 2007 ), that speakers tend to utter shorter linguistic expressions when using pointing actions in a similar setting to ours (in their symmetric setting, however, both participants were able to point at objects). This had also been noted much earlier by Brennan and Clark ( 1996 ). In fact, deictic use of pronouns is dominant in our corpus; 402 demonstrative pronouns out of 548 were used with the mouse cursor being on the target.

Table 7 shows the ranked list of the learnt weights of features which were calculated by using a linear kernel and all training instances. The weight of a feature reflects the importance of the respective feature in classification. We note that the two top-weighted features encode information on the current operation (O2) as well as the dialog history (D5). This confirms the point mentioned at the outset; namely the need to integrate linguistic and extra-linguistic information into a unified account.

The relatively high rank of  X  X  X 2=pron X  X  could be interpreted in such a way that a piece mentioned by a pronoun the last time tends to be subsequently mentioned by pronoun. This observation is consistent with past research on anaphora resolution (Mitkov 2002 ).
 Another remarkable tendency can be seen in the ranks of features A1 and A4. Among A1 features, the most recent one ( B 10) has the highest rank (6), while the two more distant ones ( B 20, &gt;20) have lower ranks (19 and 20). In contrast, the ranks of A4 features show the opposite tendency. That is, the most recent one ( B 10) has the lowest rank (22), while the two more distant ones (&gt;20, B 20) have higher ranks (10 and 11). This indicates that in order to use pronouns, the target is better to have been operated recently (high rank of A1 10). In contrast, the other pieces are better to have been operated a long time ago (higher rank of A4 20). Interestingly, there is no such clear tendency for their counterparts among the dialog history features, D1 and D4. In addition, features D3 and A3 reside in close ranks (12 and 15); this means the number of other pieces operated/mentioned during the period from the last mention/operation of the target to the referring expression has a similar effect. We see that 5 out of 12 time distance-based features are higher ranked than either D3 and A3. This indicates that rather than distractor-based features, time based-features are more effective for the generation of DPs.

Given these feature weights, we investigated the impact of each feature by evaluating the performance of feature combinations which were generated by adding one feature at a time in descending order of their weight, i.e. a feature combination K includes feature 1 through feature K . Table 9 shows the development of precision, recall and F-measure over the feature combinations 1 X 27. Following the first two feature combinations, the F-measure grows only very slightly and only at two more places: where features 19 (A1 20) and 27 (D4 &gt; 20) are added. Interestingly, we note that for feature combinations 2 and 3 the F-measure remains the same, while the precision and recall values are different. 5 Conclusions and future work This paper presented the REX-J corpus, which can be used as a resource in order to pursue research on referring expressions occurring in a collaborative task. This corpus captures linguistic information in synchronization with information on the actions carried out by the participants to solve the Tangram puzzle. Through outlining the construction of the corpus, the annotation scheme as well as the collected data, we discussed some of the particularities of Japanese in comparison with English, and how they impact the creation and use of this resource. To show the potential of the corpus, we provided two examples of research on the analysis and generation of referring expression by using this corpus. 4
This corpus can also be a valuable contribution with a view towards stimulating broader research at the intersection of Cognitive Science and Computational Linguistics, since it allows us to study recorded linguistic data in combination with the actions having occurred as well as the current state of the ongoing collaboration. In addition to research from the Computational Linguistics perspective outlined in the previous section, work based on the REX-J corpus is currently being pursued from a Cognitive Science perspective (Kuriyama et al. 2009 ) as well. More broadly, integrating recent work in Cognitive Science on problem solving with an analysis of the referring expressions would help to address questions such as what the impact of a specific task or a state of task is on the use of different types of referring expressions.
In addition, as a non-English linguistic resource, this corpus can contribute to a broadening of research on referring expressions as well as to the development of language-universal models.

Utilizing the REX-J corpus, time-aligned information on linguistic interaction and actions can be analyzed in order to develop more general models for referring expressions by integrating linguistic and extra-linguistic information. Viethen and Dale ( 2008 ) have discussed individual differences in referring expressions in a static setting. As any real application would have to deal with this in a dynamic setting, looking into individual differences within this corpus would be an interesting research direction. Furthermore, as this corpus captures an intensive collaboration on a task by two subjects, questions of alignment that have recently received attention (Janarthanam and Lemon 2009 ; Buschmeier et al. 2009 ) can be addressed in a collaborative setting. Although we focused on expressions referring to a single object, generally reference to a group of objects is also an important linguistic device. We have previously discussed expressions referring to a group of objects in a static setting (Funakoshi and Tokunaga 2006 ). The REX-J corpus not only allows for the pursuit of research analyzing such expressions in a dynamic setting, but also for research on how they interact with the actions by the participants.

In future work, we plan to collect a parallel corpus in English based on the same (or a very similar) task setting, laying the basis for further comparative research of phenomena found in the Japanese corpus (Tokunaga et al. 2010 ). Furthermore, the current setting still excludes many modes of extra-linguistic information that are normally available in a real-world environment, such as information of eye-gaze. We intend to extend the types of data recorded in the current corpus to other modes in order to further approach a real-world environment. References
