 Generally, credit scoring problems are related to classification by statistical methods. Investigating more sophisticated classifiers to match the characteristics of the samples is crucial in providing results that meet the needs of particular credit scoring applica-tions. Techniques for developing classifiers have evolved from simple parametric to nonparametric statistical methods. Altman [1] applied simple parametric discriminant analysis and multiple discriminant analysis (MDA) to the corporate credit granting problem, then compared model X  X  performance using linear discriminant analysis and neural networks. Lawrence and Arshadi [2] us ed a logistic model to analyze the loan management problem using a series of borrower and bank variables. Charitou et al. [3] used logit and neural network models to predict failed and non-failed UK public industrial firms over the period 1988-1997. These studies concluded that there cer-tainly should be further studies and tests using statistical and artificial intelligence techniques and suggested a combined approach for predictive reinforcement. 
Other efforts are leading to the investigation of nonparametric statistical methods bankruptcy prediction and compared the performance of rough sets versus auditor signaling rates. A distinguished classifier, Bayesian networks (BN), has been pro-posed as a probabilistic white-box classifier which permits higher order relationships between the variables of the problem under study. Sarkar and Sriram [5] utilized Bayesian networks for early warning of bank failures. They found the na X ve Bayesian network and the composite attribute Bayesian network classifiers to have superior performance compared to the induced decision tree algorithm. 
The idea of conventional classifiers is generally based on a single classifier or a simple combination of these classifiers, showing moderate performance. However, even exquisitely designed classifiers still have deficiencies which cannot appropri-ately distinguish the samples. One way to alleviate the classifier and data match prob-lem is to use an ensemble of classifiers. A variety of classifiers, either different types of classifiers or different instantiations of the same classifier, are combined before a final classification decision is made. Thus, the ensemble classifier allows different characteristics of the samples to be handled by classifiers suited to their particular needs, and provides an extra degree of bias /variance tradeoff. Recently, the ensemble classifier has been demonstrated to be outp erformed by a single classifier in having greater accuracies and smaller prediction er rors when applied to the credit scoring datasets [6-7]. However, the potential to reduce the generalization error of a single classifier varies among different training data sets. This study investigates the concept of preprocessing the data set into more homogeneous cluster groups first and then fitting the ensemble classifier for stably predicting the categories of applicants. For this reason, we introduce the concept of class-wise classification as a preprocessing step in order to obtain an efficient ensemble classifier. This strategy would work bet-ter than a direct ensemble of classifiers without class-wise classification. The meaning of effective ensemble classifier is twofol d, relating to accuracy and easy interpretation of classified results. 
This study addresses the following research questions. First, a class-wise classifica-tion method is proposed to guide the reclassified samples into more homogeneous cluster groups that can be used to develop a well-performing ensemble classifier for credit scoring applications. Second, the proposed architecture focuses on fusing three types of classifiers  X  Neural network (NN), Bayesian network (BN), and Support vector machine (SVM)  X  which are simple to implement and have been shown to perform well in credit prediction [5], [8]. The rationale of employing these classifiers is that NNs are generally superior to the conventional classifiers, BNs can easily model complex relationships among variables, and SVMs can be used as the bench-mark technique. Third, NNs and SVMs work well for continuous and discrete valued features, but BNs generally use only discrete valued features. Proper discretization of continuous values is critical for the buildin g of a BN classifier. This study employs a heuristic method based on the assumption of linear dependence, as measured by correlations between variables to the target features, so an optimal associate binning technique for discretization of continuous values was gained. Through discretization, continuous values are converted into discrete values with several states. Fourth, the use of the same training data set for all individual classifiers may reduce the diversity among individual classifiers, while different training sets for individual classifiers may decrease the accuracy of individual classi fiers. It is important to construct appro-priate training data sets that maintain a good balance between accuracy and diversity among individual classifiers in an ensemble classifier. We use the class-wise classifi-cation mechanism to reclassify samples into homogeneous cluster groups, and used the class-wise bagging ensemble to train in dividual classifiers. This strategy would increase the diversity and accuracy of an ensemble classifier. Fifthly, the Markov blanket (MB) concept of BN allows for a natural form of feature selection, which provides a basis concept for mining constrained association rules. The learned knowl-edge is presented in multiple forms, including causal diagram and constrained asso-ciation rules. 
For a better understanding of our study, section 2 describes this study X  X  research motivation. Section 3 describes the analysis methodology. Section 4 discusses the findings of this study and offers observations about practical applications and direc-tions for future research. The learning of credit scoring classifiers is based on historical samples with known classes, which are usually  X  X ood credit X  and  X  X ad credit X . In theory, every sample has a fixed class membership in the known classes, but this theory is unachievable in the real world, since samples with known classes are quite limited, and some collected samples are unrepresentative of the population to be analyzed. This study focuses on reclassifying training samples into more homogeneous cluster groups, and predicting whether an acquired applicant categorizes not only as  X  X ood credit X  and  X  X ad credit X  but also  X  X orderline credit X  (samples tending toward different credit status/samples tending away the same credit status) based on initial applicant characteristics. 
Figure 1 shows the ideal classification boundary produced by conventional classi-fiers. The conventional classifier can be a si mple classifier or a combination of classi-fiers. Sometimes, even with highly accurat e classifiers, the classifier X  X  ability to predict new applicants is still limited. Th is is because misclassification patterns may appear in the training data set, and the uncertainty in class membership will increase the learning complexity. If the quality of the sample X  X  class membership can be im-proved, the classification ability of classifi ers would significantly increase. Therefore, the fundamental requirement of the classifiers we build is to give the correct target class membership that conforms to the feature values of samples. If we adopt the idea of unsupervised classification and classify input samples into homogeneous cluster groups while neglecting the original sample X  X  class membership, then we can reassign each sample to proper target class membership by clustering results. This approach allows us to find similarities in samples and to group samples into homogeneous cluster groups, thus alleviating misclassifications of class membership from the origi-nal training data set. clustering technique. The goal of the learni ng problem depicted in this figure is to separate the good credit from the bad credit. However, even line A can separate the good credit and bad credit perfectly, it is still possible that line A is not a good classi-fier due to misclassifications of class membership in the input samples. Suppose that line B further divided the feature space into four regions. The marked samples indi-cate those samples tending toward different credit statuses or samples tending away from the same credit status. Then, referred to sample X  X  original class memberships, we can reassign the marked samples to new class memberships, and use the new input samples for training classifiers. Preprocessing input samples by the hybrid-clustering technique is useful in defining proper class memberships. 3.1 Discretization of Continuous Features A real world German credit data set from the UCI Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets.html) was used to test the prediction effective-ness of the ensemble classifiers. The German credit data set consists of a set of loans given to a total of 1000 applicants, consisting of 700 samples of creditworthy appli-cants and 300 samples where credit should not be extended. For each applicant, 20 variables describe credit history, account balances, loan purpose, loan amount, em-ployment status, and personal information. Each sample contains 13 categorical, 3 continuous, 4 set/binary features, and 1 class feature. The German credit data set is challenging because it is unbalanced and contains a mixture of continuous and cate-gorical values, which confounds the task of classification learning. For a better under-standing of our solution, the overall architecture of the decision making process is shown in Figure 3. classifier X  X  utility by selecting the optimal subset of features which best classifies the training data set. 
Discretization methods are classified as unsupervised and supervised methods. Un-supervised methods do not consider the value of the target features while supervised methods do. The simplest discretization method, equal interval width, divides the intervals for a variable into k equal sized bi ns, where k is a user-defined parameter. A relative method, equal frequency intervals, divides a continuous variable into k bins, where each bin contains m adjacent values (m=k). The unsupervised methods do not consider the associations among independent and target features in setting the parti-tion boundaries; it is likely that information will be lost during the binning process. On the other hand, supervised methods consider the associations among predictive and target features. Kerber [9] proposed a statistically justified heuristic method for supervised discretization. Entropy-based methods have recently been developed for feature discretization. Fayyad and Irani [10] used a recursive entropy minimization heuristic method for discretization with a Minimum Description Length criterion to control the number of intervals produced ov er the continuous space. Dougherty et al. [11] compared several unsupervised and supervised discretization methods. They found that the performance of the Na X ve-Bayes algorithm significantly improved when features were discretized using the entropy-based method. 
For the German credit data set, we kept the original symbols for each categorical feature. The data type of features Telephone and Foreign-worker was changed from categorical to binary, since they have only two values. The data type of features Installment-rate-in-percentage-of-disposable-income, Number-of-existing-credits-at-this-bank and Present-residence-since was changed from categorical to set, since they have several values and with small domains. Therefore, the set data type is not suit-able for using optimal associate binning tech nique to discretize them into categorical values. For features Duration-in-month, Credit-amount and Age-in-years, the corre-sponding continuous value domain is large, and optimal associate binning was used to discretize them into categorical values. After the discretization of continuous features, the average accuracy of a one-layer MLP NN increased from 78.46% to 89.16%. 3.2 Assessing Data Reduction Techniques for Refining the Quality of Samples Typically, the choice of classifier affects the quality of a credit scoring system. Classi-fiers for credit scoring system can use linear/logistic regression, multiple discriminant analysis, neural networks, decision trees, support vector machines and Bayesian net-works [8], [12-14]. However, the quality of training samples leads to misclassification patterns due to unrepresentative samples that drastically reduce the accuracy of the deployed classifier. One way to solve this problem is data reduction. In data reduc-tion, samples are grouped or clustered to reveal hidden patterns, and samples that are not considered representative can be filtered out, leaving only the potentially interest-ing ones. Therefore, overhead can be significantly reduced by storing only the charac-teristics of the clusters instead of the individual data. 
We identify the unrepresentative samples in this study by using class-wise classifi-cation to look for isolated and inconsistent clusters. Isolated clusters are thinly popu-lated clusters or outlier samples; inconsistent clusters are clusters with inconsistent class values. A fundamental assumption made here is that the isolated clusters are far away from the core clusters. To improve the quality of classification, isolated clusters can be eliminated from training samples. However, inconsistent clusters are the clus-ter mixture samples with different class memberships, that is, the samples tend toward different credit status or tend away from the same credit status. These samples can be re-classified into more homogeneous cluster groups. We stand to gain if we can keep the feature values of training samples more homogeneous, would allow an efficient classifier to be built. Since the quality of classification is not easy to control, Punj and Steward [15] suggest that a two-stage clustering method consisting of a hierarchical model, such as Ward X  X  minimum variance method, followed by a non-hierarchical method, such as K-means, can provide a better solution. 
We have chosen to replace Ward X  X  minimum variance with a two-step cluster analysis. Two-step cluster analysis works fine with mixed continuous/categorical types variables and can effectively and accurately handle small and large data sets. Furthermore, the Bayesian information criterion was used to automatically determine the number of clusters. The second stage follows the non-hierarchical K-means method, to determine the final solution, due to its efficiency. For this reason, this study uses a Two-step and K-means hybrid-clustering method to refine the quality of samples. First, we use the discretized samples as a new data set. Based on the shared characteristics of samples, Two-step cluster analysis automatically determines that the optimal number of clusters is three. We then employ K-means cluster analysis to find cluster-1, cluster-2 and cluster-3 three adhered cluster groups. All three cluster groups are inconsistent in that they mix samples with good and bad class memberships, and may also contain samples with borderline credit status. By using the class-wise classification concept, we further divide the three cluster groups into six by cluster identifier and original class membership, so as to avoid the possible misclassified/ uncertainty class membership. status; however, they have dissimilar properties that need to be further separated for the sake of good classification and interpretation. The properties of samples in cluster-12 and cluster-22 are similar, so the behavioral trend of these samples might be tend-ing changed in the near future. Also, the properties of marginal samples in cluster-31 and cluster-32 might be tending toward different credit status in the near future. Cluster groups cluster-11 and cluster-21 are surely the ones with good credit status. 3.3 An Ensemble Classifier for Credit Scoring Analysis To overcome the performance limitation of single classifiers, the multi-classifier sys-tem, also called  X  X nsemble classifier X , has been proposed. The ensemble classifier is valuable due to its ability outperform the best individual classifier X  X  performance [16]. Ensemble classifiers use the idea of combining information from multiple sources to reduce the variance of individual estimation errors and improve the overall classifica-tion results. Two points should be noted about the selection of individual classifiers. First, the choice of classifiers should mainly derive from the fact that these classifiers have been applied successfully in credit scoring analysis. Also, the design of these algorithms should be based on different theoretical concepts. This is particularly criti-cal in combining classifiers, since they might explore complementary information for the classification task. In this study, the employed individual classifiers for the en-semble classifier are NN, BN, and SVM. Th e concept of ensembles appeared in the classification literature since the work by Nilson [17], and then in extensive studies beginning in the 1990s [8-9], [16], [18]. 
Ensemble methods such as cross-validation, bagging, and boosting have been pro-posed for forming ensembles of classifiers. The simplest method is the cross-validation ensemble where all ensemble members are trained on the same data set. Bagging and boosting [19] are other methods for improving the performance of en-semble classifier systems. For selecting tr aining data, bagging generates replicated boot-strap samples of the data, while boosting adjusts the weights of training instances. Both methods can be combined into an individual classifier by voting. Bagging and boosting create perturbed versions of the training set so that ensemble members learn from different variants of the original training data. These ensemble algorithms not only increase the classification accuracy, but also reduce the chances of overtraining since the committee avoids a biased decision by integrating the different predictions from the individual base classifiers. 
Importantly, the key to successfully building an ensemble classifier system is to construct appropriate input training sets and maintain a good balance between diver-sity and accuracy among individual classifiers in an ensemble. When using these classifiers, we confront two problems. The first is how to choose the optimal input feature subset for the classifier and how to se lect the best individual classifier for each ensemble classifier. These two problems are crucial because the feature subset choice influences the selection of an appropriate classifier and vice versa. For this reason, the individual classifiers in this study are pr imarily optimized with respect to functional feature subspace selection. Feature subspace selection is performed by a data reduc-tion technique in order to retain only the feature subset with the highest discrimination information. 
Conventional ensembles train all ensemble members on the same data set. Some works [18] have explored the phenomenon that when a data partitioning strategy is employed, the experimental results vary significantly for different partitions of the same data collection, even when the number of samples in each data set is the same. It is necessary to have a precise specification of the partition in order to replicate an experiment or conduct fair comparisons. To simplify discussion, this study investi-gates the effectiveness of the class-wise bagging ensemble combination strategy, adapted to efficient combination of classi fiers for credit decision applications. 
Figure 3 has shown the proposed final credit scoring system, adopting the ensem-ble approach to generate enhanced results by grouping a set of classifiers of each ensemble: NNs, SVMs, and BNs. Owing to the diversity among individual base clas-sifiers on diverse training data sets, the training data sets of individual base classifiers are derived from class-wise bagging ensemble combination, for the sake of preserving maximum diversity among different classifiers. In the class-wise bagging step, indi-vidual classifiers are trained independently via a bootstrap method. According to the proportion of samples, bootstrapping generates K replicas of training/testing data by repeated random re-sampling with replacement from each of the data sets, T(X c1 ), T(X c2 ),.... Then, to achieve maximum diversity of ensembles, each replicated training set is used to train a NN, a BN, and an SVM classifier of each ensemble. In the sec-ond step, individually created classifiers ar e aggregated by an ensemble combination technique to form a final decision. The con cept of voting is both simple to implement and appealing, and can generally be applied to any type of classifier without relying on specific interpretations of the output. We thus use confidence-weighted voting in our ensemble of classifiers. The average pe rformance of individual classifiers and the ensemble classifier with/without preprocessing by class-wise classification are shown as Gain charts in Figure 4. The results of the experiment show that improvements in performance have been achieved by applying the ensemble of classifiers for the train-ing data set preprocessing both with and without class-wise classification. 3.4 Bayesian Learning and Markov Blanket for Modeling Knowledge Aside from building effective ensemble classifi ers, another aim of this study is to try to discover hidden patterns in the data set so that we can better understand the differ-ent characteristics of applicants and develop new strategies for credit approval analy-sis. A well-known Apriori algorithm has been proposed for mining association rules in a database. An association rule is considered relevant for decision making if it has support and confidence at least equal to some minimum support and confidence thresholds defined by the user. However, the extracted association rules are usually very large, to the present of a huge proportion of redundant rules conveying the same information. If we can employ the built classifier as a basis in mining constrained association rules, it can be of benefit in modeling valuable knowledge. 
The BN model is a powerful knowledge representation and reasoning algorithm under conditions of uncertainty. The major advantage of BNs over many other types of predictive models, such as neural networks, is that unlike  X  X  X lack box X  X  approaches, the BN structure represents the inter-relationships among the data set features. Thus, human experts can easily understand the network structures. Other advantages of BNs include explicit uncertainty ch aracterization, as well as fast and efficient computation. They are highly adaptive and easy to build , and provide explicit representation of domain specific knowledge in human reasoning frameworks. BNs offer good gener-alization with limited training samples and easy maintenance when adding new fea-tures or new training samples. Several works have presented methods for mining knowledge by means of BNs [20-21]. A novel idea of BN for significant feature selection is the Markov blanket (MB). MB is defined as the set of input features such that all other features are probabilisti-cally independent of the target features. That is, based on a general BN classifier, we can get a set of features that are in the MB of the target feature, and features provided by the MB are sufficient for perfectly estimating the distribution and for classifying the target feature classes. Suppose that we build a BN classifier with a complete input training data set. Then, the MB of the target feature forms the feature selection mechanism, and all features outside the MB are ignored from the BN. The remained features are then used to induce constrained association rules. In this study, all asso-ciation rules are in the form, Y  X  X , where Y is the target feature and X is a subset of features from the MB. The rationale of this method is that for any feature x i  X  {MB  X  x ture xi is a minimal set of features MB such that xi is conditionally independent of any other feature x j in MB. Herein, for generating the MB, Pearson chi-square was employed as a technique to test independence, i.e. to assess whether paired observa-tions of two features are independent of each other. The MB structure to be used for BN explores not only the relationship between target and predictive features, but also the relationships among the predictive features themselves. The Pearson chi-square test reported that eighteen predictive features are important to the target feature. Figure 5 depicts a MB for the German credit data set, composed of nineteen features, eighteen predictive features, and one target feature. 
Obviously, the extracted association rules convey redundant information. Interac-tive strategies are proposed for pruning redundant association rules on the basis of equivalence relations, in order to enhance readability. The user will be presented with only the most informative non-redundant association rules, where the union of the antecedents (or consequents) is equal to the unions of the antecedents (or conse-minimal antecedents and maximal consequents in the same equivalence class. The extraction of a set of rules without any loss of information will convey all the infor-mation in a set of association rules that ar e all valid according to the context. Table 1 lists some merged association rules of  X  X ad credit, X  ordered by the length of the rule X  X  antecedent, where each rule represents a property that was dominant or most strongly associated with  X  X ad credit X . The support an d confidence criteria for filtering associa-tion rules are set as 10% and 80%, respectively. This study proposed a complete procedure for designing the architecture of an ensem-ble classifier for credit scoring analysis. Due to frequent uncertainty in the real world credit data set, misclassification patterns from the input samples generally restricted acquired applicant not only as good or bad, but also as borderline credit status based on the initial information provided by the applicant. For this reason, we introduce class-wise classification as a preprocessing step in order to obtain more homogeneous cluster groups and avoid misclassification patterns. Moreover, there exist many classi-fier algorithms for credit scoring analysis. Fo r constructing more efficient classifiers, some other approaches have been proposed to combine different classifiers into an ensemble classifier. An important issue in constructing an ensemble classifier is to make each individual classifier as different from the other classifiers as possible. In our approach, we first constructed the individual NN, BN and SVM classifiers by using class-wise bagging as a data augmentation strategy to obtain good generaliza-tion performance. Then, the final outputs were decided by a confidence-weighted voting ensemble strategy. proach gave significantly better performance than the conventional ensemble classifi-ers. The ensemble approach exploits the differences in misclassification by individual classifiers and improves the overall performance. Specifically, optimal associate binning for the discretization of continuous features and the Markov blanket concept of BN allowed for a useful strategy of feature selection, which provided a basis for mining association rules. The learned knowledge was then represented in multiple forms including causal diagram and constrained association rules. The data-driven nature distinguished the proposed system from those existing credit scoring systems based on hybrid/ensemble of various classifiers. To summarize, the advantage of using the proposed multi-classifier system is that decision makers can have practical aid in their daily credit approval task with relatively high accuracy, and decision mak-ers can figure out meaningful relationships among features by causality networks and constrained association rules. 
