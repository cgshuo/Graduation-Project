 Transforming the data into a suitable representation is the first key step of data analysis, and the performance of any data oriented method is heavily depending on it. We study questions on how we can best learn representations for textual entities that are: 1) precise, 2) robust against noisy terms, 3) transferable over time, and 4) interpretable by human inspection. Inspired by the early work of Luhn [4] , we propose significant words language models of a set of documents that capture all, and only, the significant shared terms from them. We adjust the weights of common terms that are already well explained by the document collection as well as the weight of incidental rare terms that are only explained by specific documents, which eventually results in having only the significant terms left in the model.
 K
EYWORDS : Significant Words; Language Model.
Transformation of raw data to a representation that can be effec-tively exploited is motivated by the fact that data oriented methods often require input that is convenient to process. In this research, we introduce significant words language models (SWLM) as a family of models aiming to learn representations for the set of documents that are not affected by neither general properties nor specific properties. The general idea of SWLM is inspired by the early work of Luhn [4] , in which he argues that to extract significant words by avoiding both common observations and rare observations (Figure1).
In order to estimate SWLM, we assume that terms in the each document in the set are drawn from three models: 1. General model , representative of common observation, 2. Specific model , representa-tive of partial observation, and 3. Significant Words model which is a latent model representing the significant characteristics of the whole set. Then, we try to extract the latent significant words model.
The proposed approach is generally applicable to any system that requires the estimation of an effective model representing the signif-icant features of a group of objects. Until now, we have employed the model in three main applications: Group Profiling for Content Customization: We have proposed to use SWLM to extract the  X  X bstract X  group level latent model from users group that captures all, and only, the essential features of the whole group. We employed the resulting models in the task of contextual suggestion [ 1 , 3 ] and observed improvements in the performance of customization. (Pseudo-)Relevance Feedback: We have presented a variant of SWLM, Regularized SWLM (RSWLM) for estimating a robust language model for a set of feedback documents by incorporating the information from the query. We have conducted extensive ex-periments on the effectiveness of RSWLM for (pseudo-)relevance feedback and demonstrated that it captures the essential terms repre-senting the mutual notion of relevance.
 Hierarchical Classification: We have also extended SWLM to be able to estimate proper models for hierarchical entities which take their position in the hierarchy into consideration [ 2 ]. We have employed SWLM on the task of hierarchical classification and ob-served that since estimated models of entities in the hierarchy are both horizontally and vertically separable, they are precise, robust and transferable over time.
Besides applying the model in further applications, there are several other interesting directions to pursue in this research. These include using SWLM as an analytical tool to investigate and better understanding of the data, extending the method to be applicable to non-textual features, and employing the general idea of SWLM to the representation learning systems like embedding methods. Acknowledgments This research is funded by the Netherlands Orga-
