 Wikification, which stands for the process of linking terms in a plain text document to Wikipedia articles which represent the cor-rect meanings of the terms, can be thought of as a generalized Word Sense Disambiguation problem. It disambiguates multi-word ex-pressions (MWEs) in addition to single words. Existing Wikifica-tion techniques either models the context of a given term as well as the Wikipedia article as bags of words, or compute global con-straints among Wikipedia concepts by the link graph or link distri-butions. The first method doesn X  X  achieve good results because the MWEs can have very different meanings than its constituent words which themselves are ambiguous. The second method doesn X  X  pro-duce high accuracy because the link structure or link distribution is often biased or incomplete by themselves due to the fact that Wikipedia pages are often sparsely linked. In this paper, we present a simple but powerful framework of sense disambiguation using co-occurrences of Wikipedia links in the Wikipedia corpus. We pro-pose an iterative method to enrich the sparsely-linked articles by adding more links and then use the resulting link co-occurrence matrix to disambiguate an input document by a sliding window algorithm. Our prototype system achieves 89.97% precision and 76.43% recall on average for three benchmark data and compares favorably against four state-of-the-art wikification techniques. I.2.7 [ ARTIFICIAL INTELLIGENCE ]: Natural Language Pro-cessing Wikification; phrase sense disambiguation; link co-occurrence; it-erative algorithm
Natural language is rife with ambiguities. A word or a phrase usually has multiple meanings depending on the context. This has Kenny Q. Zhu (corresponding author) is partially supported by NSFC grants 61100050 and 61373031.
 been one of the most significant problems in automatic understand-ing and processing of human text. Consider the following sen-tences.
 E XAMPLE 1. The polarbear is a bear native largely within the Arctic Circle encompassing the Arctic Ocean, its surrounding seas and surrounding land masses.

E XAMPLE 2. The original band, PolarBear , was formed in 1994 by Gary Lightbody who was a student at Dundee University in Scotland.

In Example 1,  X  X olar bear X  refers to a large white carnivorous bear inhabiting the arctic region; in Example 2,  X  X olar Bear X  is the former name of a British rock band. The task of labeling a word or a multi-word expressions (MWE) (collectively called as a term )in a plain text by their explicit meanings is known as phrase sense dis-ambiguation (PSD) problem [6]. PSD generalizes the more well-known open problem, word sense disambiguation (WSD) [21], which seeks to identify the meaning or the sense of the words . The differ-ence is that in WSD, the disambiguated unit is word, not phrase.
PSD is an important generalization because the meanings of MWEs can be independent of the constituent words so traditional WSD techniques cannot be directly applied on PSD problems. For exam-ple, the word  X  X asses X  often means normal people, while the term  X  X undee X  usually reminds us of the hilarious Australian comedy from the 80 X  X , and neither of these meanings have anything to do with  X  X and masses X  or  X  X undee University X  above. In fact, some researchers have already indicated that PSD outperforms WSD in NLP tasks such as statistical machine translation [6]. Moreover, the number of MWEs is massive. For example, if we consider Wikipedia [2], which is the largest online encyclopedia today with over 4 million concepts (articles), 62.8% of the concept terms (ti-tles of the articles) consist of two or more words. A random sample of 10,000 web pages suggests that 66.2% of the phrases in them are ambiguous, because they can be mapped to two or more concepts in Wikipedia.

A particularly important form of PSD is known as wikification [19]. Wikification is an automatic process of linking possible MWEs in a document to an article in Wikipedia. This process disam-biguates the terms by labeling each term with a proper sense (or concept) 2 which is represented by a Wikipedia article. Existing wikification techniques focus on noun-phrases because Wikipedia does not have articles to support all senses of verbs or verb phrases. For this reason, in the rest of the paper, when we mention  X  X erms X  or MWEs, we mean noun phrases.
In this paper, we use the terms  X  X erm X ,  X  X WE X  and  X  X hrase X  in-terchangeably. We also use the terms  X  X ense X  and  X  X oncept X  inter-changeably.
Previous work on wikification often models a Wikipedia concept by its article using a bag of words , models a target term by its con-text which is also a bag of words, and then compare the two bags. The bag-of-words model doesn X  X  work well (e.g. on Example 1 and 2) because: 1) many words themselves are ambiguous and don X  X  offer distinctive signals, resulting in the convoluted signals from a bag. (e.g.,  X  X ircle X , X  X ear X  and  X  X asses X  in Example 1 all have mul-tiple senses, and the words  X  X urrounding X ,  X  X riginal X  and  X  X ormed X  don X  X  contribute much to the meaning of the sentences). 2) it ig-nores other MWEs in the context or article by only treating them as individual words, leading to misunderstanding of some of the MWEs (e.g.  X  X and masses X  in Example 1 and  X  X undee University X  in Example 2).

An improvement over the above bag-of-words approach uses the relatedness between two concepts to constraint the labeling of two neighboring terms simultaneously [26, 22, 16]. The relatedness can be computed from the article inter-link graph structure induced from the corpus[12].While the link structure is an important source to extract relation among concepts, the above approach misses out a more direct and accurate source of information, which is the co-occurrence between Wikipedia concepts in the corpus. Such co-occurrence can be computed by the co-occurrence of links within an article, a paragraph, or a predefined window, since the links are the natural labels of surface terms to the concepts.

In this paper, we propose a simple and novel approach on the wikification problem by using co-occurrence between Wikipedia links, and show that it is more effective and practical than the ex-isting approaches.

Unfortunately, Wikipedia articles can be sparsely linked . Some surface terms don X  X  have a corresponding Wikipedia page to link because it X  X  not created yet; many others are not linked because either they are too common or they have been linked previously in the article before. Either way, the author of the article didn X  X  consider them to be  X  X ink-worthy X . Sparsely linked Wikipedia does not reveal the true link distribution or link graph structure, thus harms the accuracy of the link co-occurrence information. Previous research on WSD using word co-occurrence [24] already indicated that the correct distribution of a term X  X  surrounding environment is critical to the end-to-end disambiguation accuracy. The same argument applies to wikification by concept co-occurrence.
To mitigate this problem, in this paper, we propose an itera-tive enrichment algorithm that adds the missing links to Wikipedia pages. In a nutshell, this algorithm maintains a concept co-occurrence matrix of the Wikipedia snapshot in the current iteration as par-tial information, and use it to disambiguate unlinked terms for the next iteration until no more links can be added. Figure 1 shows a snapshot of the links in a sentence of a Wikipedia article  X  X efore X  and  X  X fter X  the iteration process. We have experimentally verified (in Section 4) that our iterative enrichment algorithm increases the number of links by five times, and the number of co-occurrence by three times, and the final co-occurrence information gives rise to significant improvement (up to 6% increase) in the accuracy of end-to-end wikification.

There are two other technical challenges in our wikification frame-work. First, parsing of plain text into phrases can be ambiguous it-self and common NLP tools are not accurate enough in this chunk-ing process. Considering the sentence in Example 3:
E XAMPLE 3. Snow Patrol are an alternative rock band formed at the University of Dundee in 1994, though at this time as an indie rock band, the band is now based in Glasgow, Scotland.
 Figure 1: Snapshot of a Wikipedia Article  X  X efore X  and  X  X fter X  It-eration. (The source and destination of newly added links are indi-cated at the bottom) Figure 2: Wikification of Plain Text in Example 2 in the Web Demo. (Concepts are enclosed in square brackets) The phrase  X  X niversity X  and  X  X undee X  are treated as independent chunks in the NLP chunker while  X  X niversity of Dundee X  is a cor-rect parse.

Second, wikifying a new, unlinked document is a computation-ally intensive task because there X  X  no support for any sense so all combinations of senses must be attempted. Suppose we have n phrases in a sentence, each has k senses in average, the number of combination is k n ! Thus, we need a method that balances accuracy with complexity.

This paper makes the following contributions. 1. We propose an iterative algorithm that disambiguates un-2. We propose a sliding-window based method to wikify a given 3. We show comprehensive evaluation results that demonstrate
The rest of the paper is structured as follows. Section 2 presents the algorithms of this framework; Section 3 discusses some imple-mentation details; Section 4 demonstrates the experimental results; Section 5 introduces some related work while Section 6 concludes the paper.
Wikification demo website: http://adapt.seiee.sjtu. edu.cn/~luckyvega/demo.html . Figure 3: Architecture of Wikification via Link Co-occurrence
The framework of wikification via link co-occurrence has a three-part architecture (See Figure 3): preprocess of wiki data , co-occurrence matrix generation and document wikification . The first part collects a term-sense mapping from Wikipedia data and parse Wikipedia ar-ticles to identify terms to be linked in the later parts. The second part is an iterative process which, in each iteration, computes the co-occurrence matrix from the current snapshot of the Wikipedia corpus, and then use this matrix to disambiguate (i.e., add links to) noun phrases that have not been linked yet in the corpus. The up-dated corpus will be used in the next iteration. This part can be thought of as an off-line process. The third part is an on-line pro-cess, which makes use of the final co-occurrence matrix and con-verts a plain text to a text with links pointing to relevant Wikipedia articles. Next, we describe each of the three parts in greater details. We start from the generation of the term-sense mapping from Wikipedia corpus. Then we introduce our parsing method on Wikipedia articles by making use of an NLP chunker.
To identify the sense of an unlinked term, we need to know the candidate senses of that term. Each term may map to more than one Wikipedia concepts which are the senses . The list of all Wikipedia concepts associated with an unlinked term is called the candidate sense list. In this paper, we use three sources in Wikipedia to build the term-sense mapping: Wikipedia article ti-tles, redirect pages and disambiguation pages. Specifically, each Wikipedia concept (sense) is mapped to the title of this concept X  X  article, titles of redirect pages linked to this concept, and the title of disambiguation pages that contain this concept. In other works such as Cucerzan X  X [8], anchor text of links in Wikipedia articles are also used as a source of mapping. Our observation is that surface forms in the anchor text and the links between the surface form and the Wikipedia article can both be noisy and unreliable. Anchor texts are not necessarily MWEs and the linked articles are some-times not really the description of the anchor text but are rather just  X  X elated X  information.

E XAMPLE 4. A further 6-7 million were deported and exiled [linkto: Population transfer in the Soviet Union ] to remote areas of the USSR, and 4-5 million passed through  X  X abour colonies X .
Example 4 shows a sentence from a Wikipedia article named  X  X ulag X  [1]. The anchor text  X  X eported and exiled X  is not a term, and the concept  X  X opulation transfer in the Soviet Union X  is not exactly a sense of  X  X eported and exiled. X  Due to this observation, we do not include links as a source for generating the term-sense mapping. Our candidate sense list for  X  X olar bear X  is, for example, Polar bear , Polar Bear (American band) , Snow Patrol , Polar Bear Pass , etc.
With the term-sense mapping generated, we can now parse texts to extract noun phrases. First, we take all the surface forms in the term-sense mapping to form a term list . Only those noun phrases in the term list will be considered later. In a Wikipedia article, we call terms which are already linked linked terms , while noun phrases which are waiting to be linked unlinked terms . Next we introduce the details of parsing.
 Algorithm 1 Parsing a Chunk 1: function P ARSE C HUNK ( Chunk ) 2: Unlinked  X  X  X  3: N  X  wordCount ( Chunk ) ,k  X  N 4: flag  X  False 5: while k&gt; 0 and ! flag do 6: candidateTerm  X  Chunk [ N  X  k,N ] 7: if candidateTerm is NP then 8: if candidateTerm  X  TermList then 9: Add candidateTerm to Unlinked 10: L  X  ParseChunk ( Chunk [0 ,N  X  k  X  1]) 11: Unlinked  X  Unlinked  X  L 12: flag  X  True 13: k  X  k  X  1 14: return Unlinked Parsing Wikipedia articles: The following is a short part of an Wikipedia article about a band.

E XAMPLE 5. Snow Patrol are an alternative rock band formed at the University of Dundee in 1994, though at this time as an indie rock band, the band is now based in Glasgow, Scotland.

To get the noun phrases from the articles, we first treat the article as plain text, i.e. remove all the links. The plain text of Example 5 contains the following noun phrases:  X  X now Patrol X ,  X  X lternative rock band X ,  X  X niversity of Dundee X ,  X  X ime X ,  X  X ndie rock band X ,  X  X he band X ,  X  X lasgow X , and  X  X cotland X . But not all of them can be found in Wikipedia. For example, you cannot find concepts  X  X l-ternative rock band X  or  X  X ndie rock band X  in Wikipedia as of this writing. So instead, our candidates are those terms from the term list, e.g.,  X  X now Patrol X ,  X  X lternative rock X ,  X  X and X ,  X  X niversity of Dundee X ,  X  X ime X ,  X  X ndie rock X ,  X  X lasgow X  and  X  X cotland X .
We achieve the parsing task in two steps. In step 1, we parse the text into linguistic chunks to obtain noun phrases using an NLP chunker. The chunker can detect phrases from a sentence, includ-ing verb phrases, noun phrases, prepositional phrases and adverb phrases. In our framework, we only pick the noun phrases from the chunker. Notice that chunkers are not always correct, there-fore we introduce a method to optimize our chunking result at the end of this section. In step 2, we detect unlinked terms from the resulting noun phrase chunks. To simplify the unlinked term detec-tion, we adopt a simple strategy: remove words one by one from left to right, while the remaining part is a noun phrase and an un-linked term. The intuition here is that longer terms are more likely to be accurate. That is, we prefer to use  X  X lternative rock X  as un-linked term rather than  X  X ock X . The details of the parsing strategy is showninAlgorithm1. wordCount is a function for counting the number of words in the noun phrase chunk. TermList is the list of all terms in the term-sense mapping. Chunk [ i, j ] is the sub-string from word i to word j .
 With the chunks, we then check if the chunks fit the original Wikipedia article which has linked terms. These linked terms may not align properly with the chunking results from the chunker and therefore cause conflicts. Below is the original text from Wikipedia along with the chunking results.

E XAMPLE 6. [ Snow Patrol ] are [ an alternative rock band at [ the University ] of [ Dundee ] in [ 1994 ] , though at [ as [ an indie rock band ] , [ the band ] is now based in [ Glasgow [ Scotland ] .
 The terms with underlines are linked terms, and phrases enclosed in square brackets are the chunks produced by a chunker. The three conflicts are listed as follows: Our conflict resolution policy is that the original links in the Wikipedia article are always respected. In that words, links are natural chunks. Where there X  X  conflict, we break up an offending chunk produced by chunker into smaller chunks. For example the above segments can be re-chunked as:
Optimization on chunks: As we mentioned earlier, the chunker can make mistakes. When a chunk is too wide, i.e., the correct term is properly contained in the chunk, Algorithm 1 can be applied to extract the correct term. When a chunk is too narrow, i.e., it is only a part of a term, we need a way to merge adjacent chunks together to form a term. For example,  X  X he University of Dundee X  was in-correctly chunked into  X  X he University X ,  X  X f X  and  X  X undee X  in Ex-ample 6. Had there been no hyperlink on  X  X niversity of Dundee X , we need a way to reconstruct the term automatically.

We use the following regular expression pattern to capture the potential incorrect segmentations of a noun phrase: where NP stands for noun phrase, PP for preposition and CC for conjunction. In this pattern, we allow prepositions and con-junctions to appear in the compound noun phrases. If the pattern matches an unlinked term, we combine phrases in the pattern to form a new chunk.  X  X he University X ,  X  X f X ,  X  X undee X  are thus com-bined to  X  X he University of Dundee X .

Combining the optimization with the strategy described in pars-ing Wikipedia articles, and applying Algorithm 1, we can produce a more refined chunking for Example 6:
We map the unlinked terms in the parsing result to the corre-sponding candidate sense lists from the term-sense mapping to con-struct the final result of our parsing process which is used to gener-ate the co-occurrence matrix next.
Co-occurrence among Wikipedia concepts provides the knowl-edge for disambiguating terms in a given document. The co-occurrence information of Wikipedia concepts is in theory a K  X  K square ma-trix where K is the total number of concepts in Wikipedia. Each element in the matrix represents the total co-occurrence frequency of the two concepts in any Wikipedia articles. Despite the large number of concepts that exist in Wikipedia, not every pair of them co-occur, and therefore in practice, the matrix is very sparse and manageable. Next, we present the algorithms that compute the co-occurrence matrix, which involves two phases: matrix initialization and matrix enrichment .
In the initialization phase, we take as input the parsed Wikipedia articles which include both linked terms and unlinked terms, to cal-culate the co-occurrence frequency between the concepts of two linked term and the appearance frequency of each concept (i.e., the sum of all co-occurrence frequencies for that concept). We ar-gue that computing the co-occurrence within the whole article is not only computationally demanding, but also counter-productive. Wikipedia articles are often much longer and richer than tradi-tional dictionary definitions. Multiple topics may co-exist within the same article. As a result, co-occurrence of two concepts which are very far away from each other in the article might not be re-lated at all! Therefore we only consider two concepts co-occur if they are less than W c terms (either linked or unlinked) apart in the whole text. In addition, we consider a concept c to co-occur with all the concepts in its own definition page, considering that all the other concepts in the page contribute to the description of c .This actually incorporates the link structure in Wikipedia into the co-occurrence framework.

To illustrate the initialization process, let X  X  consider the parsing result of Example 6 again, with W c equal to three. The first linked term is  X  X lternative rock X . The only linked term within the window is  X  X niversity of Dundee X  which is 2 terms away. So the concept of these two linked terms are considered to co-occur. We then move on to the next linked term which is  X  X niversity of Dundee X . This time the linked terms within the window are  X  X lternative rock X  and  X  X ndie rock X . The process continues till the last linked term.
The initial co-occurrence matrix doesn X  X  have enough informa-tion because Wikipedia articles are often sparsely linked. We de-velop the algorithm that bootstraps from the initial matrix and iter-atively adds links to the current Wikipedia articles and updates the matrix concurrently. The pseudo-code of this algorithm is shown in Algorithm 2 which consists of two procedures U PDATE A RTICLES
In Algorithm 2, S u stands for the candidate sense list of an un-linked term t u . The list of its linked neighbors X  senses are denoted as S l . A neighbor here means a term that is fewer than W away. The co-occurrence frequency of two concepts (or senses) is denoted as Co ( c i ,c j ) . The appearance frequency of a con-cept c is denoted as Tf ( c ) , which is equal to the times c occurs in Wikipedia corpus. UT is list containing all the updated (dis-ambiguated) terms in the U PDATE A RTICLES process. Conditional concept score ( S CC ) is a score used to determine the sense of t based on conditional probability. Given c i  X  S l , the conditional probability that a concept c  X  S u is selected as the correct sense is defined as P ( c | c 1 ,c 2 ,...,c n ) . According to the Bayes X  theorem Algorithm 2 Enrich Co-occurrence Matrix 1: procedure E NRICH M ATRIX 2: InitMatrix () 3: while UpdateArticles () &gt; 0 do 4: UpdateMatrix () 5: function U PDATE A RTICLES 6: updatedCount  X  0 , UT  X  X  X  7: for a in Wikpedia Corpus do 8: for t u in a do 9: Initialize Score [ sizeof ( S u )] 10: for i  X  0 ,sizeof ( S u )  X  1 do 11: Score [ i ]  X  S CC ( S u [ i ]) 12: Sort Score Descending 13: if Score [1] /Score [0]  X   X  then 14: Assign S u [0] to t u 15: updatedCount  X  updatedCount +1 16: UT  X  UT  X  t u 17: return updatedCount 18: procedure U PDATE M ATRIX 19: for t u in UT do 20: for c l in S l do 21: c u  X  Concept of t u 22: Update Co ( c u ,c l ) and Tf ( c u ) and assuming independence of c 1 ,...,c n ,wehave: In Equation (1), we can replace P ( c i | c ) with Co ( c,c proportional to Tf ( c ) , therefore S CC of a candidate concept c in S u can be defined as: To prevent S CC from being 0 when Co ( C, c i ) is 0, we add a smoothing term to Equation (2) and obtain:
U PDATE A RTICLES processes all unlinked terms in all articles and attempts to disambiguate and convert unlinked terms to linked terms. For each candidate concept of an unlinked term, we cal-culate S CC of it then sort the concepts in the candidate list by this score. If there is only one concept in the candidate list and the score is non-zero, the term is disambiguated and linked to this concept. If there are more concepts in the candidate list and the ratio between the scores of the top two concepts is less than a threshold  X  ,the number one concept will be chosen to disambiguate the term.
In Example 6, the first  X  X and X  is an unlinked term. Term  X  X and X  has many candidate concepts such as Belt (clothing) , Band (radio) , Band society ,and Musical ensemble . With window size equal to three, linked terms inside the window are  X  X lternative rock X  and  X  X niversity of Dundee X , whose concepts are Alternative rock and University of Dundee . According to S CC , Musical ensemble is ranked number one, while Band (radio) is the number two concept. Since the ratio between Band (radio) and Musical ensemble is less than 0.5 (a  X  value determined empirically in Section 3), Musical ensemble is picked to be the correct concept of  X  X and X  and  X  X and X  is inserted into UT .

In U PDATE M ATRIX , for each term in UT , we update the co-occurrence matrix by the co-occurrence between this term X  X  con-cept and its neighboring linked terms X  concepts within the window of size W c . This is very similar to Matrix Initialization .Once the co-occurrence matrix is updated, there X  X  more knowledge that enables the disambiguation of other unlinked terms in the next it-eration. The iterative process continues until no more linked term can be linked and the final co-occurrence matrix is stored.
To wikify an entire document, our general idea is to disambiguate several unlinked terms together within a local context by optimiz-ing the likelihood of co-occurrence among a particular combination of senses for these terms. We could use the whole document as the context but that will be computationally infeasible if the document is large. Instead we make the size of the context parameterized and tunable. However, a local context can be misleading and can introduce errors in the disambiguation. We solve this problem by creating a sliding window that allows us to aggregate the disam-biguation results from neighboring contexts together and then make a pseudo-global decision about which sense each term ultimately should have.

We illustrate this idea using Example 2 and show three steps in sliding a window of size three in Figure 4. We parse the plain text using the NLP chunker discussed in Algorithm 1 and get the follow-ing terms:  X  X olar Bear X ,  X  X ary Lightbody X ,  X  X tudent X ,  X  X undee University X  and  X  X cotland X . We create a candidate sense list for each term. Step 1 shows a window containing  X  X olar Bear X ,  X  X ary Lightbody X  and  X  X tudent X . Given that each term has a few senses as candidates, there are many combinations of senses for these three terms, such as: Snow Patrol , Gary Lightbody and Student turn out to be the best sense combination since the sum of the pair-wise co-occurrence is the largest at 54. Note here that we are simulating the overall co-occurrence with pair-wise co-occurrence, which is a reasonable approximation under computation constraints. We call the maxi-mum sum sliding window score ( S SW ), and every term within this window is associated with this disambiguation result (in the form of sense-S SW pair) in a data structure called Preferred Concept List ( PCL ).

In Step 2, the window is slided to the right by one term, and the best combination of senses is again computed while the PCL is updated with three more entries. This time S SW is 60.

In Step 3, within the last window of this sentence. The best combination contains the Student (degree) sense, which is differ-ent from the result of  X  X tudent X  in the 2 previous steps.
After we have finished sliding the window from start to finish, we group the results for each term by senses and sum the S the group. In the case of  X  X tudent X , Student sense has a combined S
SW of 114 while Student (degree) sense has a combined S SW 25. The first sense wins and becomes the final sense for the term  X  X tudent X . Other terms are similarly disambiguated.
 The detailed sliding window wikification algorithm is given in Algorithm 3. L u stands for the list of unlinked terms. W for the size of the sliding window. By picking one concept for each unlinked terms inside the sliding window, we get a Concept Combi-nation ( CC ). Picking different candidate concepts from unlinked terms gives different CC s, thus forms the Concept Combination List ( CCL ). Therefore, S SW can be defined as The CC associated with S SW is called Best Concept Combination ( BCC ). PCL L u [ i ] denotes the PCL of the i th term in L
In this section we first attempt to experimentally determine the value for three key paramaters in our framework and then describe a baseline system for comparison with our framework.
The key parameters in our framework include  X  which deter-mines whether to disambiguate a given term in our matrix enrich-ment process, W c , the co-occurrence window size in the iterations, and W s , the sliding window size for wikifying new documents.
For threshold  X  , we randomly pick 100 paragraphs from Wikipedia corpus. We use function UpdateArticles in Algorithm 2 to add links Algorithm 3 Sliding Window Method 1: procedure S LIDING W INDOW ( L u , W s ) 2: for i  X  0 ,sizeof ( L u )  X  W s do 3: CCL  X  getCombination ( L u [ i ] ,W s ) 4: Calculate S SW and BCC 5: for j  X  0 ,sizeof ( BCC )  X  1 do 7: for i  X  0 ,sizeof ( L u )  X  1 do 8: Initialize hash [ concept  X  score ] 9: for j  X  0 ,sizeof ( PCL L u [ i ] )  X  1 do 10: key,value  X  PCL L u [ i ] [ j ] 11: hash [ key ]  X  hash [ key ]+ value 12: concept  X  argmax key ( hash [ key ]) 13: if hash [ concept ] &gt; 0 then 14: Assign concept to L u [ i ] to these paragraphs using the matrixes generated by the enrichment process on different thresholds. We also manually add links to these 100 paragraphs, as ground truth labels. We compare the different linking results with the ground truth then calculate the precision and recall, which is shown in Table 1. Table 1: Result on Different Thresholds (with co-occurrence win-dow W c = 15)
We can see that threshold 0.5 achieves the best precision and also reasonable recall. Since our matrix enrichment process is an iterative process, precision in each iteration is more important, we therefore choose 0.5 as threshold  X  .

For W c , we follow the same experiment described above, since these two parameters are both used in the matrix generation part. Instead of using different thresholds, we change the window size this time. Table 2 shows the linking precision and recall using dif-ferent W c . 5 and 15 both achieve precision higher than 90%. To optimize the recall, we set W c =15 .
 Table 2: Result on Different W c and W s (with threshold  X  = 0.5)
For W s , we build another test data with 100 randomly picked paragraphs from web and wikify them using the sliding window al-gorithm. The matrix used in this experiment is generated by enrich-ing 10,000 sample Wikipedia articles, with parameters  X  =0 W c =15 . Table 2 compares the results on different W s with the manually created ground truth. Both precision and recall increase with growing W s .When W s is larger than 5, the whole process takes too much time and is thus not practical. Consequently we set W s =5 .
Besides the algorithm introduced in Section 2, for comparison purpose, we also implemented a baseline system which wikifies a document by the co-occurrence between Wikipedia concepts and plain words. This system can be thought of as a direct port of WSD from using WordNet to using Wikipedia, and it also uses a common bag-of-words approach. In this baseline system, the co-occurrence vector of each Wikipedia concept is constructed from words and frequencies in the article of this concept itself. With the vectors of all Wikipedia concepts, we can wikify a document by comparing the co-occurrence vectors with the context of each term in the doc-ument. Given a document, we parse it into terms in the same way as our wikification framework. Each term has a list of candidate Wikipedia concepts. We compute the cosine similarity between the vector of each candidate concept and the vector built from the input document. The concept whose vector has the best similar-ity with the document vector is chosen to disambiguate that term. We compare the result of this baseline system with our wikification framework in Section 4.
In this section we evaluate the framework of wikification via link co-occurrence. We first explain how we prepare Wikipedia cor-pus samples and the test data. Then we discuss four experiments. The first experiment shows the effects of using corpora of different sizes; the second one evaluates the iterative algorithm which gen-erates the co-occurrence matrix; the third one compares the end-to-end wikification results from our framework with the baseline as well as four state-of-the-art methods; while the last experiment measures the time performance of our system. The Wikipedia cor-pus we use was a complete dump [2] (27GB) published in May 2011 which contains 8.56 million surface forms and 3.28 million unique concepts (senses). All experiments in this section are con-ducted on a 64-bit workstation with 3.10 GHz quad-core Intel CPU and 16 GB memory running Windows Server 2008.
One of the important points to show in the evaluation is that our framework doesn X  X  require all of the Wikipedia articles to be effec-tive. But instead, samples of highly popular and information-rich articles are enough to provide the co-occurrences we need for wik-ification. Completely random samples don X  X  guarantee information richness. Our approach is to first collect a set of information-rich articles, then apply random sampling on the set. We obtain the set of information-rich articles by taking the top 3 k articles ordered by PageRank [5] within the Wikipedia network. PageRank ranks documents by their popularity. We argue that articles which are referenced by many others are supposed to have richer content.
To compute PageRank on the Wikipedia network, each Wikipedia article is treated as a web page, and links to the peers are treated as hyperlinks in web pages. As a bias towards longer articles, we set the initial PageRank to be proportional to the article length. Finally from the 3 k informative articles, we randomly pick k articles as our sample corpus.

Next we describe our test data sets. Though there has been pre-vious work on wikification, no standard test data sets are available yet. Bartunov et al.[3] ran a project called  X  X ikifyMe" to create a standard test data for wikification. Unfortunately, the web site doesn X  X  provide sufficient data for our test purposes. In our evalua-tion, we use three test data sets. The first two are publicly available data sets for wikification, reported by Dai et al.[9]. They are created by Cucerzan[8] and Kulkarni et al.[16], respectively. Cucerzan X  X  test data comes from Wikipedia articles and news articles. Because Wikipedia articles may have been modified since Cucerzan finished his experiment, we only use the news articles. Kulkarni X  X  data set is complete with test corpus and labeled ground truth. The third test set is our own creation which is extracted from 25 articles of New York Times and China Daily covering five topics: world, business, sports, entertainment and technology. We parsed these paragraphs into unlinked terms and then manually linked these terms to the appropriate Wikipedia articles as the ground truth labels. In the rest of this section, the three test sets are referred to as Cucerzan X  X , Kulkarni X  X  and Cai X  X , respectively. Examples from each of the three test set (along with our wikification results) are shown in Figure 5. The phrases disambiguated are marked with underlines, with the disambiguated senses surrounded by square brackets.
In the first part of the evaluation, we would like to check the ef-fect of using different corpus size. We run our experiment on five different samples. Figure 6a and Figure 6b show the final number of pairs of co-occurring concepts and linked terms after iteration on different samples. We can see that both numbers are sub-linear to the corpus sizes, indicating that increasing the sample size which increases the cost in time and space doesn X  X  give us proportional gain. Our hypothesis is that, with a proper sampling strategy, we can obtain sufficient knowledge in a small corpus. To this end, we introduce matrix coverage as a measure to evaluate the quality of sampling. Matrix coverage measures the number of concepts appearing in the matrix. It X  X  important because we can possibly disambiguate a term into a concept only if the concept exists in our matrix. Considering the fact that different concepts differ in their popularity and importance, we also calculate a weighted matrix coverage, i.e., we multiply the PageRank score by the frequency of a concept. Figure 7a and Figure 7b show the two versions of matrix coverage. The values are sub-linear to sample sizes, which supports our hypothesis.
To get a straightforward view of the effect on wikification using different corpus sizes, we conduct the next experiment. We take the final matrix produced by different corpus size with W c =15  X  =0 . 5 to wikify the three test data set. Figure 9 shows the av-erage precision, recall and F1-measure of our wikification method on them. Result in different corpus sizes are similar to each other, indicating that a small number of popular articles is good enough to bootstrap the matrix generation process, leading to a good wiki-fication result.
In the second part of the evaluation, we verify the correctness of the new links added during the iterative enrichment algorithm. The accuracy of the additional links is important because it affects the distribution of the final matrix, thus affects the end-to-end wik-ification result. We sample 20,000 articles from Wikipedia cor-pus and run our iterative process to adding links to Wikipedia arti-cles. When the iteration completes, we manually check whether the added links are correct or not. The number of added links and the accuracy in each iteration is shown in Figure 8. We sample 100 ar-ticles from those 20,000 articles. For each article, we pick the first 50 added links in each iteration to form the data set, forming about 5,000 links to be examined in each iteration. From Figure 8 we can see that, our iterative process significantly increases the links in Wikipedia corpus while the accuracy in each iteration decreases slightly but stabilizes above 0.9. Figure 8: Number and Accuracy of New Links in Wikipedia
Next is to test the effectiveness of our iterative process in im-proving the end-to-end wikification result. We use the original ma-trix and final matrix to wikify the text in the three test data sets. Sample size here is 10,000. Table 3 shows the result of our wik-ification method on them, before and after the iterations. In each cell, the three numbers from top to bottom are precision, recall and F1-measure, respectively. We can see that, after iterations, which provides us more concept co-occurrence information by adding ac-curate links to Wikipedia corpus, the matrix achieves better preci-sion and F1-measure in all the three test data set.
In the third part of the evaluation, we compare our result with 4 state-of-the-art methods, namely, Cucerzan X  X [8], Kulkarni X  X [16], WikiMachine[26] and Ratinov[22] as well as the baseline algo-rithm in Table 4. Our system (Co-occur.) outperforms all the com-Matrix after Iterations R 71.26% 71.05% 86.99% peting methods on F1-measure and has better precisions on Kulka-rni X  X  and Cai X  X  data.

Finally, we measure the system performance of both the matrix generation process and the end-to-end wikification process. The execution times of each iteration with different sample sizes are shown in Figure 10a. Figure 10a shows that the iterative method converges quickly after a few iterations, indicating that it is possi-ble to use a stopping criterion to terminate the process after a few iterations and yet produce a reasonable co-occurrence matrix.
We also collect data to see how those two parameters, threshold  X  and co-occurrence window size W c , affect the total number of it-erations in matrix generation process. It X  X  interesting that  X  doesn X  X  affect the number of iterations much but W c does. Our explanation is that in a small window, an unlinked term has fewer linked terms to help disambiguate it. It is possible that there is even no linked term in a small window. Thus, the links added in each iteration is fewer when using small W c , resulting in more iterations. It X  X  true that  X  also affects the speed of adding link, but our experiment shows that it X  X  not as obvious as W c .

To evaluate the online wikification performance, we use the 20 articles in Cucerzan X  X  test data set. We produce article segments of various sizes by chopping the articles in the data set into paragraphs before merging the consecutive ones together. The correlation be-tween file size and time cost is showed in Figure 10b. Note that the y-axis is on logarithmic scale so the scatter plot clearly indicates that our wikification time is roughly linear to the input document size. In addition, all of the articles, including the longer ones with over 1000 words, can be effectively wikified under 1 second.
In the following, we first present the state of the affairs in WSD research, and then put our work into perspective by discussing var-ious approaches in wikification before briefly introducing several additional pieces of work related to wikification.
Traditional methods for WSD are either dictionary-based or ma-chine learning methods. Co-occurrence information between words is also used in some WSD work [13, 17, 7, 24, 10, 27]. Among them, Guthrie [13], Fernandez-Amoros [10] and V X ronis [27] in-troduced unsupervised methods. Guthrie [13] proposed a two-level WSD on subject (domain) level and sense level (within a subject). In each disambiguation level, they chose the subject/sense with the highest similarity between the context and the description of the subject/sense in a dictionary. Fernandez-Amoros [10] used co-occurrence to compute a relative matrix using mutual information (MI) measure. They used the MI between the target word and words of its context as the weight of each context words, and select the sense whose WordNet definition is most similar to the context by the bag-of-words model. V X ronis [27] clustered words into hubs on a co-occurrence graph mined from a large corpus. The hubs that contain a target word define its different senses. The disambigua-tion of a word is done by computing the similarity between the con-text of this word and its various hubs by bag-of-words again. All of the above methods disambiguate words only whereas in this pa-per, we proposed a method for the more general problem of phrase sense disambiguation, and instead of relying on bags-of-words ap-proach, our method takes advantage of the link co-occurrences of Wikipedia concepts.
There are generally two broad approaches to the wikification problem [22]: local algorithms which labels the terms in a doc-ument one by one using the local context of each term only; and the global algorithms which use global information of the sense configuration in the whole sentence to improve the previous local methods. Next we discuss these two approaches separately.
Most local algorithms use bag-of-words similarity between the context of the target term and the context of each candidate sense to identify the correct sense. Cucerzan [8] applied vector space model on linking named entities to Wikipedia concepts. Each Wikipedia concept is modeled as a vector. The concept with the most simi-lar vector with the document vector is chosen to link a term. Since document vector is constructed by merging the vectors of all terms X  candidate concepts, important signals may be diluted and weaken. Furthermore, this work only focuses on disambiguation of named entities. Ferragina and Scaiella [12] applied similar framework, but instead of calculating vector similarity, they calculated a relat-edness score between concepts. Since this disambiguation process is very time-consuming for an online system, they restricted the in-put documents to be short texts. Milne and Witten applied machine learning methods on wikification [20]. Using part of the Wikipedia corpus as the training set, they combined three features together, which were commonness, relatedness and quality of text, to train a classifier that can distinguish correct concept from irrelevant ones for a given term. Fernando and Stevenson limited their wikification work on cultural heritage [11]. Their system is based on Milne and Witten X  X  work, but uses category information in Wikipedia to filter irrelative concepts both in the training set and the result. They also used the link structure in Wikipedia to help find more proper links. Skaggs [23] and Boston [4] proposed topic models for wikifica-tion. Skaggs used an LDA-based topic model while Boston applied a simple generative model which selects the sense with the highest conditional probability given the context of the term.

Several recent attempts were made to combine both local and global information in wikification. Global information is the rela-tion or constraints between Wikipedia concepts in the whole sen-tence, it usually comes from the context similarity between two Wikipedia articles. The context can either be the content of the ar-ticle or the surrounding words of links which point to the article. Kulkarni et al. [16] proposed a method that considers compati-bility and relatedness in disambiguating a term. Compatibility is measured based on similarity features between the context of the term and the concept X  X  context. Relatedness between two con-cepts is computed by the category structure in Wikipedia and the cross references between the two articles. The above two factors are combined to form an optimization model. which is approxi-mately solved by greedy hill-climbing. Tonelli [26] developed Wiki Machine based on an SVM model by combining local and global kernels. Local kernels are built from non-contiguous n-grams and part-of-speech tags while global kernels use bag-of-words and la-tent semantic to capture the topical information. Ratinov [22] did a similar work to Tonelli, using both local and global information to train an SVM model. Different from Tonelli X  X  work the local in-formation they used is context similarity, while the global informa-tion is the Wikipedia article/concept similarity. They also trained a linear SVM classifier to decide whether a term should be tagged or not. This classifier is trained based on the link distribution of Wikipedia corpus, attempting to avoid labeling general terms which are usually not labeled by people in Wikipedia.

Strictly speaking, the link co-occurrence approach adopted in this paper can be categorized as a global approach because the link co-occurrence matrix that we obtain from the iterative algorithm is indeed global information among the concepts, while at the same time during wikification time we disambiguate several neighboring terms together in a sliding window which forms a local context. Our method differs from all existing methods in: 1) it doesn X  X  use a bag-of-words (or bag-of-terms) model at all; 2) it relies on co-occurrences between Wikipedia links or concepts rather than words or surface terms; and 3) wikification algorithms which depends on the Wikipedia link structures or the link distribution face the prob-lem of link sparsity in Wikipedia, our iterative link enrichment pro-cess solves this problem and can be used as a complementary pre-processing step for many of these algorithms. There are some other work related to Wikification. Strube and Ponzetto [25] proposed path based, information content based and text overlap based measures on the semantic relatedness between two Wikipedia articles/concepts. These relatedness features can be used to train some statistical models [16, 22]. Different from them, our approach is based on co-occurrence, which is a more natural re-lation between concepts. To help users better understand the com-plex medical terminology, He et al. [14] tried to generate links from narrative radiology reports to Wikipedia automatically. He X  X  work didn X  X  share the same goal with ours since their main concern is to resolve the ambiguity of anchor medical terms other than semantic ambiguity. Lui et al. [18] applied wikification on the generation of hypertext for web based learning. Given a set of documents, af-ter wikification on them, they tried to replace the destination of the link with a document from the set which is semantically related to the original destination(Wikipedia article). Jadidinejad et al. [15] implemented the wikification approach by Milne et al. [20] and applied it on structured query generation. Miao and Li proposed sentence wikification for query oriented summarization, but only applied an exact match strategy. The proposal in this paper com-plements much of the above work as it improves the accuracy of wikification, a key component in these systems.
Phrase Sense Disambiguation is an important problem in natu-ral language processing. In this paper, we show the possibility to use Wikipedia as a reliable and comprehensive source to bootstrap a process that disambiguates unlinked noun phrases in Wikipedia articles and then use the obtained co-occurrence information to disambiguate noun phrases in new documents, a process known as wikification. This approach extends the original term-level co-occurrence to concept/sense-level co-occurrence. Our evaluation shows that the co-occurrence based wikification can achieve high accuracy (about 82.58% on F1) efficiently (over 1000 words per second) using just 10,000 popular Wikipedia articles and moderate computation resources. [1] Gulag: Wikipedia article. [2] Wikipedia. http://www.wikipedia.org . [3] S. Bartunov, A. Boldakov, and D. Turdakov. Wikifyme: [4] C. Boston, S. Carberry, and H. Fang. Wikimantic: [5] S. Brin and L. Page. The anatomy of a large-scale [6] M. Carpuat and D. Wu. How phrase sense disambiguation [7] Y.-J. Chung, S.-J. Kang, K.-H. Moon, and J.-H. Lee. Word [8] S. Cucerzan. Large-scale named entity disambiguation based [9] H. Dai, C. Wu, R. Tsai, and W. Hsu. From entity recognition [10] D. Fern X ndez-Amor X s, R. H. Gil, J. A. C. Somolinos, and [11] S. Fernando and M. Stevenson. Adapting wikification to [12] P. Ferragina and U. Scaiella. Tagme: on-the-fly annotation of [13] J. A. Guthrie, L. Guthrie, Y. Wilks, and H. Aidinejad. [14] J. He, M. de Rijke, M. Sevenster, R. C. van Ommering, and [15] A. Jadidinejad and F. Mahmoudi. Query wikification: [16] S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti. [17] H. Li and N. Abe. Word clustering and disambiguation based [18] A. Lui, V. Ng, E. Tsang, and A. Ho. Generation of hypertext [19] R. Mihalcea and A. Csomai. Wikify!: linking documents to [20] D. Milne and I. Witten. Learning to link with wikipedia. In [21] R. Navigli. Word sense disambiguation: A survey. ACM [22] L.-A. Ratinov, D. Roth, D. Downey, and M. Anderson. Local [23] B. Skaggs. Topic Modeling for Wikipedia Link [24] C. Stokoe, M. P. Oakes, and J. Tait. Word sense [25] M. Strube and S. P. Ponzetto. Wikirelate! computing [26] S. Tonelli, C. Giuliano, and K. Tymoshenko.
 [27] J. V X ronis. Hyperlex: Lexical cartography for information
