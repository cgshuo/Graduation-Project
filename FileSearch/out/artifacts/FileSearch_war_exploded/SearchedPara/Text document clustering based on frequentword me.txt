 1. Introduction
Nowadays in every industry, almost all the documents on papers have their electronic copies. This is because the electronic format provides safer storage and occupies much smaller space. Also, the electronic files provide a quick access to these documents. The text database which consists of documents is usually very large. The Word Wide Web is such a database, and how to explore and utilize this kind of text data-base is a major question in the areas of information retrieval and text mining. With the development of the World Wide Web, it is getting more and more popular to use web search engines to get information.
When a user submits a query, the search result is usually a long list of ranked documents. The users may not find what they want from the top 10 documents on the list. It is time-consuming and annoying to browse the result documents one by one. Thus, when the users cannot find matching one after 10 X 20 clicks, they may give up. This is why the precision of the retrieval for a given query is important for the search engine.

In order to increase the precision of the retrieval result, many methods have been proposed [2] . One result usually covers several topics and the user may be interested in just one of them. By clustering the text documents, the documents sharing the same topic are grouped together. When the clusters are returned, the user can select the group that interests him/her most. This method makes the search engine more efficient and accurate. Text clustering is known as an unsupervised and automatic grouping of text documents into clusters, so that documents within a cluster have high similarity between them, but they are dissimilar to documents in documents.
 special requirements for the clustering of the text retrieval result.  X  Finding a suitable model to represent the document is a non-trivial issue. Most of the text documents are written in a human language, which is context-sensitive. As the accurate meaning of a sentence has close relationship with the sequential occurrences of words in it, the document model better preserves the sequen-tial relationship between words in the document.  X  The high dimension of text documents should be reduced. Usually there are about 200 X 1000 unique words in a document such as a newspaper article. In order to efficiently process a huge text database like WWW, the text clustering algorithm should have a way to reduce the high dimension.  X  Overlapping between document clusters should be allowed because a document can cover several topics.
For example, morning news may have information about a war followed by information about the popu-larity of teas in US. cluster is about since the label can provide an adequate description of the cluster. However, it is time-con-suming to determine the labels after the clustering process is finished.  X  The number of clusters is unknown prior to the clustering. It is difficult to specify a reasonable number of clusters for a data set when you have little information about it. Instead of telling the number of clusters to the clustering algorithm, it makes more sense to let the clustering algorithm find it out by itself.
Our two new text clustering algorithms, named Clustering based on Frequent Word Sequences (CFWS) and Clustering based on Frequent Word Meaning Sequences (CFWMS) are designed to meet the above spe-cial requirements of text clustering to varying degrees. A  X  X  X ord X  X  is the word form showing in the documents. our CFWS and CFWMS algorithms are: they treat the text document as a sequence of words (meanings), instead of a bag of words, and whether documents share frequent word (meaning) sequences or not is used as the measurement of their closeness.

A frequent word ( meaning ) sequence is defined as a sequence of frequent words (word meanings) appearing in at least a certain number (or percentage) of documents, and we developed algorithms to find the frequent word (meaning) sequences from a text database.

Finding frequent itemsets is an important data mining topic, and it was originated from the association rule mining of transaction data set. Recently, some text clustering algorithms used frequent word sets to compare the distance between documents. Considering the difference between text documents and transaction data set, using the frequent word sequences is more appropriate for text clustering than using the frequent word sets.
Since the order of words in a document is important, we did not adopt the vector space model. In our algo-rithms, each document is reduced to a compact document by keeping only the frequent words (meanings). In the compact document, we keep the sequential occurrence of words (meanings) untouched to explore the fre-quent word (meaning) sequences. By building a Generalized Suffix Tree (GST) for all the compact documents, the frequent word (meaning) sequences and the documents sharing them are found. Then, frequent word (meaning) sequences are used to create clusters and describe their content for the user. The difference between
CFWS and CFWMS is: in CFWMS, we convert words into word meanings by exploring the synonymy, poly-semy, and hyponymy/hypernymy relationships between words by using an ontology  X  WordNet [11] .We found CFWMS is more accurate than CFWS, and both of them are more accurate than bisecting k -means (BKM) [33] , a modified bisecting k -means using background knowledge (BBK) [19] and Frequent Itemset-based Hierarchical Clustering (FIHC) [12] algorithms.

The rest of this paper is organized as follows: Section 2 introduces the related work on text clustering, min-ing of frequent word sequences, and the application of background knowledge to text clustering. Section 3 describes the method to find frequent word sequences from a text database in details, and Section 4 describes the CFWS clustering algorithm. Section 5 describes the CFWMS clustering algorithm based on CFWS and an ontology, named WordNet [11] . The experimental results of CFWS and CFWMS as well as their performance comparison with other clustering algorithms are presented in Section 6 . Section 7 contains some conclusions. 2. Related work
There are two general categories of clustering methods: agglomerative hierarchical and partitioning meth-ods. In the previous researches, both of them are applied to text clustering. Agglomerative hierarchical clus-tering (AHC) algorithms initially treat each document as a cluster, use different kinds of distance functions is repeated until the desired number of clusters is obtained. Overlapping of clusters is not allowed in AHC.
Comparing with the bottom-up method of AHC algorithms, the family of k -means algorithms [5,22,24] , which belong to the partitioning category, create a one-level partitioning of the documents. The k -means document is assigned to a cluster based on a distance measure, then k centroids are recalculated. This step is repeated until an optimal set of k clusters are obtained based on a heuristic function. These two categories are compared in [33] .

Unweighted Pair Group Method with Arithmetic Mean (UPGMA) [8] of AHC is reported to be the most accurate one in its category. Bisecting k -means algorithm, combining the strengths of partitioning and hierar-chical clustering methods, is reported to outperform the basic k -means as well as the agglomerative approach ing step is repeated until the desired number of clusters is obtained. Since these two categories of clustering are not taken care of well. First, there is no description given to each cluster. Each text document cluster ond, the number of unique words in a document may be in the order of several hundreds or more, but these algorithms do not have steps to reduce the high dimension of the text documents during the clustering. Third, the user must specify the desired number of clusters before the clustering process. However, it is difficult to specify a reasonable number of clusters before you closely study the text database. We may run the algorithm several times with different number of clusters to choose a appropriate one, but this is too time-consuming.
Recently, there is a new category of text clustering algorithms developed. They address the special charac-
The FTC algorithm introduced in [3] used the shared frequent word sets between documents to measure their closeness in text clustering. The Frequent Itemset-based Hierarchical Clustering (FIHC) algorithm proposed sets, such that the documents in the same cluster are expected to share more frequent word sets than those in different clusters. FIHC uses frequent word sets to construct clusters and organize clusters into a topic hier-archy. These new algorithms are reported to be comparable to bisecting k -means in terms of clustering accu-racy. An advantage of these algorithms is that a label is provided for each cluster. The label is the frequent word sets shared by the documents in each cluster. A problem of these algorithms is that they strongly depend on the frequent word sets, which are unordered and cannot represent text documents well in many cases.
The concept of the frequent word set is based on the frequent itemset of the transaction data set. The items result of the data mining performed on the database. But the text document is different. The sequential order of words in a document plays an important part of delivering the meaning of the document. The change of the relative positions of two words may change the content of a document. For example,  X  X  X ssociation rule X  X  is a concept of data mining. If these two words,  X  X  X ssociation X  X  and  X  X  X ule X  X , appear in the reverse order within a set {association, rule} is used, it cannot differentiate these two cases.

Since all the clustering algorithms mentioned above treat each document as a bag of words, they use the vector space model to represent a document after the preprocessing steps, such as the removal of the stop-words and the stemming of words. In this model, each text document is represented by a vector of the frequen-cies of the remaining terms. The information about the position of the words in the text documents is not stored in this model, thus it is not good enough for text documents. In this paper, we propose a new model to represent the document. The sequential relationship between words (meanings) in the document is pre-served in the model and utilized for the text mining.

Since frequent word sequences can represent the document well, clustering text documents based on fre-quent word sequences is meaningful. In [37] , the idea of using word sequences (phrases) for text clustering was proposed; and then the Suffix Tree Clustering (STC) based on this idea was proposed in [38] . STC does words. STC builds a suffix tree to identify sets of documents that share common phrases, and uses this infor-mation to create clusters and summarize their contents. However, STC does not reduce the high dimension of the text documents, hence its complexity is quite high for large text databases. And STC just performs the word form matching, which ignores the semantic and lexical relationships between words.

On the other hand, our CFWS algorithm uses only the frequent word sequences, not all the phrases in the documents, hence the dimension of the documents is reduced dramatically. Moreover, a phrase is meaningful to the clustering result only when it is shared by at least a certain number of documents. For example, suppose that there are only two documents, say A and B , sharing a phrase  X  X  X anana Republic X  X  (a fashion brand name) in a document collection, and all other documents in the collection do not have this phrase, while 20 docu-is more desirable than a cluster about  X  X  X anana Republic X  X  in the final clustering. From this example, we can
Thus, we remove these infrequent phrases at the early stage of the process, so that the dimension of the doc-uments is reduced, and the clustering result would not be affected by them.

Ahonen-Myka et al. also pointed out in [7,28,29] that the sequential aspect of word occurrences in docu-ments should not be ignored to improve the information retrieval performance. They proposed to use the max-imal frequent word sequence, which is a frequent word sequence not contained in any longer frequent word sequence. They claimed that maximal frequent word sequences provide a rich computational representation of the document, which makes future retrieval easy and gives a human-readable description of the document.
However, all frequent word sequences in a text database are as important as the maximal frequent word sequences. Frequent word sequences of all length contain more information about the database than the max-imal frequent word sequences.

Due to the usage of synonyms, many phrases having the same meanings may not be recognized during the clustering process. However, there are only few methods proposed to utilize the semantic relationships between words in text clustering. In [6] , the Universal Networking Language (UNL) is used for the semantic representation of sentences, and the clustering is performed by using a neural network, called Kohonen X  X  self-organizing feature maps. The algorithms proposed in [19,32] use the vector space model, apply the back-ground knowledge from WordNet in the preprocessing of the text documents, and perform the bisecting k -means algorithm for clustering. The modified bisecting k -means using the background knowledge [19] , which is denoted as BBK algorithm in this paper, enriches the text representation by adding synonyms and up to five levels of hypernyms for all nouns in the documents. In [32] , they studied the impact of tagging the documents with the part-of-speech tags and adding all hypernyms to the information available for each document. The techniques proposed in [19,32] add all available information to the representation of the text documents, so that the dimension of the text database is increased, and additional noise is added by the incor-rect senses retrieved from WordNet.

Our CFWMS algorithm treats a document as a string of word meanings instead of a bag of words, and uses frequent word meaning sequences to cluster the documents. Since nouns and verbs are important members of meaningful word sequences in natural language, the word forms of nouns and verbs are converted into word meanings in the preprocessing step of CFWMS. In this way, a document is treated as a sequence of word meanings which represent nouns and verbs, and only the frequent word meaning sequences contribute to the final clustering result. Since one word meaning may represent several word forms, and infrequent word meanings are removed from the documents, this technique can reduce the dimension of the text databases for the clustering process. 3. Finding frequent word sequences 3.1. Term definitions
In our algorithm, a text document d is viewed as a sequence of words, so that it can be represented as minimum support. That means, there are at least the specified minimum number (or percentage) of documents containing this word set. A frequent k-word set is a frequent word set containing k words. In the rest of this paper, all the words in frequent 2-word sets will be called frequent words .

An ordered sequence of two or more words is called a word sequence . A word sequence S is represented as h w , w 2 , ... i .A frequent word sequence is denoted by FS in this paper. For example, FS = h w which w 2 is not necessarily following w 1 immediately in a text document. There could be words between them as long as w 2 is after w 1 and the words between them are not frequent. A text document d supports this word when there are at least the specified minimum number (or percentage) of documents supporting S . Multiple occurrences of a sequence in the same document is counted as one. The term phrase is defined in [38] as an ordered sequence of one or more words, and no gaps are allowed between words. Thus, our definition of fre-quent word sequence is more adaptable to the variations of human languages. For example,  X  X  X oys play bas-ketball X  X  may be a frequent word sequence supported by both of the following two sentences:  X  Young boys like to play basketball .  X  Almost all boys play basketball .

A frequent k-word sequence is an FS with length k , such as FS = h w sequences of length k 1, which are h w 1 , w 2 , ... , w k 1 cannot omit it.
 frequent k-word set. But a member of a frequent k-word set is not necessarily a member of a frequent k-word sequence, where the order of the k words matters. This is very straightforward.
 from the definition of the subsequence.
 3.2. Algorithm details
Finding the frequent word sequences has two steps: finding frequent 2-word sets first, then finding frequent word sequences of all length by using the Generalized Suffix Tree (GST) data structure. 3.2.1. Finding frequent 2-word sets
The goal of this step is to reduce the dimension of the database (i.e., the number of unique words) by eliminating those words that are not frequent enough to be in a frequent k -word sequence, for k P 2. This step is simple and straightforward. We use an association rule miner to find the frequent 2-word sets that satisfy the minimum support. All the words in frequent 2-word sets are put into a set WS . Based on The-orems 1 and 2 , we know that members of the frequent word sequences of all length k , k P 2, must be in WS .

After finding the frequent 2-word sets, we remove all the words in the documents that are not in WS . After the removal, the resulting documents are called compact documents . Let us consider an example database
D ={ d 1 , d 2 , d 3 }:  X  d 1 : Young boys like to play basketball.  X  d 2 : Half of young boys play football.  X  d 3 : Almost all boys play basketball.
 young}. If we specify the minimum support as 60%, the minimum support count is 2 for this case. The set of frequent 2-word sets is { { young, boys}, {boys, play}, {boys, basketball}, {young, play}, {play, basketball}}; and WS = {young, boys, play, basketball}. After removing those words not in WS , the database D becomes
D  X  d 0 1 : Young boys ( like to ) play basketball;  X  d 0 2 :( Half of ) young boys play ( football );  X  d 0 3 :( Almost all ) boys play basketball;
In this example, we can see that the dimension of D is reduced from 11 to 4. This reduction has a big impact on our next step of building the generalized suffix tree for D 3.2.2. Building a generalized suffix tree (GST)
Our goal is to find the frequent word sequences of the database. We adopted the suffix tree [36] , a well-known data structure for sequence pattern matching, to find all the frequent word sequences. Each compact document is treated as a string of words and inserted into a generalized suffix tree (GST) one by one. Finally, the database.

A suffix tree for a string S is actually a compressed trie for the non-empty suffixes of S . A GST is a suffix the text database, so some modification are made on the structure of the GST to meet our needs. In the rest of this paper, we will use the terms  X  X  X uffix tree X  X  and  X  X  X ST X  X  interchangeably.  X  A suffix tree is a rooted, directed tree.  X  There are two kinds of nodes: internal nodes and suffix nodes.  X  Each internal node has at least two children.  X  Each edge is labeled with a non-empty substring of a string. The label of a node n is the concatenation of the labels on the path from the root to this node. This label is represented as string n . string L .  X  The labels of different edges coming from the same node must have different starting words.  X  For each suffix s of string S , there exists a suffix node whose label is s .  X  There is a document id set associated with each suffix node. If a substring of a document ends at a node, then the document id is inserted into the document id set of the node. These document ids are used to check the multiple occurrences of a sequence in the same document.

Fig. 1 shows the GST built for the previous example. The nodes of the GST are drawn as circles, each with an assigned node number in it for later references. Each suffix node has a box attached, and it contains the document id set of the suffix node. After building the GST, we traverse it by depth-first. On the way down, the labels of the edges are concatenated to become the string child node sends its document id set to its parent. The support count of the label (i.e., string node is the size of the union of all the document id sets of its children. By checking the support count and the length of the label of each node, we can get the information about all the frequent word sequences in the database. In our example shown above, we have seven nodes in the GST, and the details are given in Table 1 .

Since the minimum support for frequent words, h , is set to 60% in this example, the minimum support for frequent word sequences could not be smaller than 60%. Only those words whose support is at least h are kept in the compact documents, so that we can find only the frequent word sequences with that minimum support h . In this example, as the minimum length of the word sequence is set to 2, we can get four frequent word sequences, which are represented by nodes 1, 2, 6, and 7. The maximal frequent word sequences of this data-base are represented by nodes 1 and 6.

The word sequence of node 2 is a subsequence of the word sequence of node 1, but its id set is a superset of that of node 1. If we find only maximal frequent word sequences, some of the information would be lost. As mentioned in [28] , maximal frequent word sequences can be used as content descriptors for documents. How-ever, if we want to summarize the content of this example database, a frequent word sequence  X  X  X oys play X  X  is the best description. A maximal frequent word sequence  X  X  X oung boys play X  X  covers only the content of first two documents, d 1 and d 2 , and another maximal frequent word sequence  X  X  X oys play basketball X  X  covers only can have some useful information about the database for further information retrieval and data mining operations.

In [38] , the Suffix Tree Clustering (STC) algorithm was proposed, which clusters text documents by con-for the example database D . Comparing this suffix tree of the STC algorithm with our suffix tree shown in
As the number of unique words and the size of the compact database are much smaller than those of the ori-ginal database, our algorithm is clearly more efficient than STC. 4. Clustering based on frequent word sequences (CFWS) 4.1. Term definitions
A cluster candidate is a set of text documents supporting the same frequent word sequence ( FS ). A cluster candidate i is represented by cc i [ FS i , Ids i ], in which FS ter candidate and Ids i is the set of document ids. A cluster is a set of text documents covering the same topic. A cluster i is represented as C i [ T i , Ids i ], where T and Ids i is the set of document ids that cover the topic T cluster whose topic contains only one frequent word sequence. The final clustering C is a set of clusters { C empty. 4.2. CFWS algorithm
Our CFWS algorithm has three steps: building a GST to find frequent word sequences, merging cluster can-didates based on the k -mismatch concept, and then combining the overlapping clusters to obtain the final clusters. 4.2.1. Finding frequent word sequences and collecting the cluster candidates
We use the method explained in Section 3 to build a GST for the database. The minimum support of the frequent word sequences is usually in the range of 5 X 15%. When the minimum support is too large, the total number of frequent words would be very small, so that the resulting compact documents would not have enough information about the original data set. In this case, a lot of documents will not be processed because they do not support any frequent word, and the final clustering result will not cover these documents.

After building the GST, we perform the depth-first traverse to collect all the cluster candidates for the data-base. Only the suffix nodes representing frequent word sequences can produce the cluster candidates. Since the in Table 1 , we can collect four cluster candidates for our example database: cc
Ids 1 = {1,2}], cc 2 [ FS 2 =  X  X  X oys play X  X , Ids 2 = {1,2,3}], cc cc [ FS 4 =  X  X  X lay basketball X  X , Ids 4 = {1,3}]. 4.2.2. Merging the cluster candidates based on the k-mismatch concept
The FS of a cluster candidate describes what the cluster candidate is about. For example, from FS play X  X , we know that cc 2 covers the documents regarding which game the boys play. However, sometimes we do not need fine clusters, instead we may be interested in a more general topic, such as the popular sports among teenagers. For that purpose, we can merge the related cluster candidates. The agglomerative clustering algorithm also merges two clusters closely related to each other. However, instead of using a distance function to measure the closeness between two cluster candidates, we use the k -mismatch concept of the sequential patterns.

The original k -mismatch concept is about characters and strings: Given a pattern p , a text t , and a fixed number k that is independent of the lengths of p and t ,a k -mismatch of p in t is a j p j -substring of t that matches ( j p j k ) characters of p . That is, it matches p with k mismatches. In our case, the given pattern p closeness between two cluster candidates could be measured by their FS s. The idea is that two cluster candi-dates with k -mismatched FS s may cover similar topics. For example, we can say that the topic covered by cc with FS 2 =  X  X  X oys play X  X  is similar to the topic covered by cc them to have a bigger cluster, which covers the topic about sports.

First, we check the mismatches between the frequent word sequences found by building the GST of the doc-ument collection. We are interested in three types of mismatches that can happen between two frequent word shorter pattern FS i , it becomes the longer pattern FS j longer pattern FS i , it becomes the shorter pattern FS j
FS i and FS j , of the same length, such that by substituting k words in FS examples of three cases when k =1:  X  Insertion: { j FS i j = j FS j j k j FS i =  X  X  X oys play X  X ; FS
By inserting a word  X  X  X asketball X  X  in FS i , FS i = FS j ;  X  Deletion: { j FS i j = j FS j j + k j FS i =  X  X  X oys play X  X ; FS
By deleting a word  X  X  X oys X  X  from FS i , FS i = FS j ;  X  Substitution: { j FS i j = j FS j jj FS i =  X  X  X oys play X  X ; FS
By substituting a word  X  X  X oys X  X  of FS i with a word  X  X  X irls X  X , FS Starting with the longest frequent word sequence, denoted by FS rithm [23] to find all the k -mismatched patterns for FS L eral topics. Our goal is to merge the cluster candidates with subtopics into clusters with more general topics.
Second, we merge cc L [ FS L , Ids L ] with all cc k [ FS a new cluster. Then, cc L and all cc k are removed from the set of all initial cluster candidates, S parameter k determines how fine the final clustering would be. If k is 0, the cluster candidates are the final by each cluster of the final clustering would be more general. In our experiments of clustering text documents, we found the clustering results are better when k is set to 1. These two steps are repeated until S empty.
 4.2.3. Combining the overlapping clusters
After we merge cluster candidates into clusters, we may find some clusters have too much overlap between their document id sets. The overlap of two clusters, C i and C ter. Obviously, the range of d is [0,1]: When d = 0, these two clusters are disjoint; and when d = 1, these two clusters have the same set of documents, which does not mean these two clusters are identical because this set of documents may cover two different topics. This step is controlled by two parameters: the overlap threshold clusters with the highest overlap value are merged repeatedly until the number of clusters becomes KCluster .
Otherwise, two clusters can be merged only when their overlap is larger than d . The experimental result given in Section 6.4 shows that, when KCluster is not specified, our CFWS algorithm performs the best when d = 0.5, and the clustering accuracy is not sensitive to d if d is higher than 0.5.

In the end, we collect those documents that are not in any cluster because they do not contain a frequent word sequence. These documents form a cluster by themselves, and their topic could be specified as  X  X  X nrelated issues X  X .

Our text clustering algorithm CFWS is summarized as follows: 1. Given a collection of text documents D ={ d 1 , d 2 , d the user-specified minimum support. Obtain WS , the set of all frequent words, each of which is a member of a frequent 2-word set. 2. Reduce each document d i ,1 6 i 6 n , into a compact document d w 6 2 WS . 3. Insert each compact document into the GST. 4. Using the depth-first traversing, visit every node in the GST. If a node j has a frequent word sequence FS with a set of document ids Ids j , create a cluster candidate cc 5. Merge similar cluster candidates into clusters based on the k -mismatch concept for a given k . 6. Combine clusters based on the overlap threshold d and the optional user-specified number of final clusters. 5. Clustering based on frequent word meaning sequences (CFWMS)
In the previous CFWS algorithm, in order to find frequent word sequences in the text database, we count the occurrences of each word in the documents first. This is a word form matching process, where a word form refers to a literal term in text documents. On the other hand, a word meaning refers to the lexicalized concept that a word form can be used to express [11] . We believe that word meanings are better than word forms in terms of representing the topics of documents. In order to improve the quality of clustering, we propose a new algorithm named Clustering based on Frequent Word Meaning Sequences (CFWMS), which uses frequent word meaning sequences as the measurement of the closeness between documents. 5.1. Term definitions
In the real world, people may use different word forms to express the same word meaning, and those word forms are called synonyms. A word meaning can be represented by a synonym set, or shortly synset , a set of word forms which are synonyms. In this paper, a synset is denoted by SS , e.g. SS relation between word forms may affect our clustering result. For example,  X  X  X uto X  X  is a synonym of  X  X  X ar X  X , so they are interchangeable in documents. When the word form matching is performed to find frequent words in a text database, the support counts of  X  X  X ar X  X  and  X  X  X uto X  X  could be 6 and 4, respectively. If the minimum sup-than the minimum support count if 6 documents refer to the automobile by using  X  X  X ar X  X  and 4 other documents use  X  X  X uto X  X . If we treat these two word forms as one, these 10 documents may be grouped into one cluster.

Hyponymy/hypernymy is a semantic relationship between word meanings, which is also very important for between a specific word meaning and a general word meaning. For example, the hypernym of a synset {car, railcar} is {vehicle}, and the hypernym of {vehicle} is {conveyance, transport}. Documents containing  X  X  X ar X  X  may share the same topic with other documents containing  X  X  X ehicle X  X  or  X  X  X ransport X  X . If we perform only the word form matching, we may lose this information. If we use  X  X  !  X  X  to represent this relationship, these three synsets could be linked as {car, railcar} ! {vehicle} ! {conveyance, transport}. In this case, {vehicle} is a direct hypernym of {car, railcar} and {conveyance, transport} is a inherited hypernym of {car, railcar}. Such said to belong to the synset link, and the synset link is said to contain these synsets, which is denoted as
SS
In CFWMS, we want to convert word forms in the documents to the word meanings they express, so that documents containing synonyms, hyponyms, and hypernyms all contribute to the support count of the same word meaning. In this way, the number of documents containing  X  X  X ar X  X ,  X  X  X uto X  X  and  X  X  X ehicle X  X  are all counted as the support count of one word meaning. After the conversion, each text document is treated as a sequence of word meanings, then a text document d can be viewed as d = h SS is considered frequent if there are more than certain number (or percentage) of documents containing it. A frequent word meaning sequence is denoted by FMS in this paper.

Since most words have multiple word meanings, identifying the right word meaning that a word form expresses in a certain lexical environment is not a trivial problem. In our algorithm, we use a meaning union ( MU ), which is a union of synset links, to predict the real word meaning. The synset link members are said to belong to the meaning union, and the meaning union is said to contain these synset links, which is denoted as
SL j 2 MU h . For a synset SS i ,if SS i 2 SL j and SL j 2 MU
SS , which is denoted as SS i 2 MU h . For example, for a word form  X  X  X ox X  X  in a document, there is a meaning union MU 1 ={ SL 1 , SL 2 }, where SL 1 ={ SS 1 ! SS 2 }, SL
SS 3 = {box, loge}, and SS 4 = {compartment}. We expect that one of the synsets belonging to MU word meaning expressed by the word form  X  X  X ox X  X  in this document.

In CFWMS, the word forms in each document are converted into meaning unions, like d user-specified minimum support, and a frequent set of k meaning unions is the one containing k meaning unions. In the rest of this paper, each meaning union appearing in any frequent set of 2 meaning unions is called a frequent meaning union . A frequent word meaning sequence could also be represented as a sequence are called frequent synsets . 5.2. CFWMS algorithm
Our CFWMS algorithm has three steps: preprocessing of documents to convert word forms into meaning unions, finding frequent word meaning sequences and collecting cluster candidates, and then combining clus-ter candidates to obtain the final clusters. 5.2.1. Document preprocessing using WordNet
The most important procedure in the preprocessing of documents is to convert the word forms into mean-ing unions by using an ontology. As an ontology, we used WordNet [11] , an on-line lexical reference system.
WordNet covers semantic and lexical relations between word forms and word meanings, such as synonymy, polysemy, and hyponymy/hypernymy. WordNet contains only nouns, verbs, adjectives and adverbs. Since nouns and verbs are more important in representing the content of documents and also mainly form the fre-quent word meaning sequences, we focus only on nouns and verbs and remove all adjectives and adverbs from the documents. For those word forms that do not have entries in WordNet, we keep them in the documents since these unidentified word forms may capture unique information about the documents.

For each document, two passes are required for the conversion. The first pass is to retrieve the meaning union ( MU ) from WordNet for every noun and verb in the document. In WordNet, a word form X  X  multiple word meanings represented by synsets are ordered from the most to the least frequently used. For every noun and verb, we select the first two synsets containing the word form. For each synset selected, one direct hyper-nym synset is retrieved, too. If the word form has only one synset, one inherited hypernym synset is retrieved as well. In this way, each word form has its meaning union which contains at least one synset link. We tried different numbers of synsets and hypernyms for each word form, and found these selections produce a good clustering result in most cases. For example, a document d meaning unions as d 0 1  X h MU 1 ; MU 2 ; MU 3 i , where MU SS 9 ! SS 10 }, MU 3 ={ SS 1 ! SS 5 , SS 2 ! SS 3 }.
 In the second pass, we try to reduce the number of unique meaning unions in the converted document.
After the retrieval in the first pass, many meaning unions have more than one synset link since WordNet assigns more than one word meaning to a word form. Multiple synset links introduce noise to the database, and affect the accuracy of the clustering. Thus, we need to estimate the real word meaning for each word form.
In natural language, the word meaning of a word form is determined by its lexical environment; in other words, the words around it. And the word forms in one document may tend to express similar word meanings.
Our estimation strategy is to keep those synset links containing the synsets with high frequencies in the doc-ument. Thus, if different meaning unions share a synset, we replace them with a single meaning union contain-the document. The support count of a synset in a document is defined as the number of meaning unions con-taining the synset in the document. For the replacement of meaning unions, the occurrences of each synset in the meaning unions of the document is counted, and a list of synsets with their supporting meaning unions is created. Table 2 shows the details of each replacement step for the given example. The first three columns in Table 2 show the synsets, their support counts and supporting meaning unions.
 We perform the replacement as follows:
Step 1 : Select the synset with the largest support count in the list. Step 2 : Repeat Step 1 until the support count of every synset in the list becomes 0.

Since all the meaning unions sharing the same synset are replaced by one meaning union which contains only one SL , the number of unique meaning unions in the document can be reduced. 5.2.2. Finding frequent word meaning sequences and collecting the cluster candidates After the preprocessing step, every document in the database is converted to a string of meaning unions.
For example, we may have an example preprocessed database D  X  d 0 1  X h MU 4 ; MU 2 ; MU 4 i  X  MU 4 ={ {SS 4 , SS 1 } ! SS 5 }  X  MU 2 ={ SS 3 ! SS 8 , SS 9 ! SS 10 }  X  d 0 2  X h MU 5 ; MU 6 i  X  MU 5 ={ SS 2 ! SS 5 , SS 7 ! SS 11 }  X  MU 6 ={ {SS 8 , SS 6 } ! SS 13 }  X  d 0 3  X h MU 1 ; MU 3 ; MU 7 i  X  MU 1 ={ SS 12 ! SS 13 }  X  MU 3 ={ SS 14 ! SS 15 }  X  MU 7 ={ SS 16 ! SS 17 }
In order to find frequent word meaning sequences, we use an association rule miner to find the frequent sets of two meaning unions first, and then the frequent synsets. If we perform exact matching between meaning unions, we cannot find any frequent sets of two meaning unions in D union, we can find that synset SS 5 is shared by MU 4 of d and MU 6 of d 0 2 ; and synset SS 13 is shared by MU 6 of d ( FMS ) found is h SS 5 , SS 8 i .

In order to find the frequent word meaning sequences, we need to check and collect the counts of synset links, instead of just matching the word meaning unions. The support count of a synset link is defined as the number of meaning unions which contain a synset that belongs to the synset link. For instance, when we check MU 2 of d 0 1 , there are two synset links in MU by SL A and SL B , respectively. Since a synset link could be treated as a meaning union containing only one synset link, we record SL A and SL B as unique meaning unions and increase their counts by one, respectively. When MU 6 of d 0 2 is checked, we meet SS 8 . Since SS 8 is in SL its count by one. In the same way, when we check MU 1 of d increase the count of SL A by one again. Table 3 shows the support counts of unique meaning unions (as synset links) in database D 0 after we checked all three documents. In this example, { SL meaning unions, SL C and SL A are frequent meaning unions, and the set of frequent synsets is { SS 1 , SS 2 , SS 4 , SS 5 , SS 3 , SS 8 , SS 12 , SS 6 , SS
After finding the frequent synsets, in order to reduce each document into a compact document, we remove the meaning unions which do not contain a frequent synset, and replace each meaning union containing a fre-quent synset with the frequent meaning union containing that frequent synset. Thus, each resulting compact document contains only the frequent meaning unions. In our example, the resulting database D of compact documents d 00 1 , d 00 2 , and d 00 3 :
Then, we can build the Generalized Suffix Tree (GST) for the compact documents and collect the cluster can-didates the same way as in the CFWS algorithm described in Section 4 . By traversing the GST, the frequent meaning union sequences are found. They are used to represent the frequent word meaning sequences and serve as the labels of the cluster candidates collected. In our example, a cluster candidate we obtain is cc [ FMS = h SL C , SL A i , Ids = {1,2}]. 5.2.3. Combining the cluster candidates The frequent word meaning sequence of a cluster candidate describes the topic those documents cover. In
CFWMS, we do not merge the cluster candidates based on the k -mismatch concept. The reason is because the cluster candidates we found are general enough since word meanings were used, instead of word forms. We only combine the cluster candidates based on the overlap threshold d and the optional user-specified number of final clusters, as described in the CFWS algorithm.

Our text clustering algorithm CFWMS is summarized as follows: 1. Given a collection of text documents D ={ d 1 , d 2 , d (b) In order to reduce the number of unique MU s in each document, if different MU s contain the same syn-
After steps 1a and 1b, we get D 0  X f d 0 1 ; d 0 2 ; d 0 3 d 0 i  X h MU 1 ; MU 2 ; MU 3 ; ... i . 2. Find frequent sets of 2 MU sof D 0 based on the user-specified minimum support. Instead of counting MU s, we check the support counts of SL s belonging to MU s. Obtain the set of all frequent synsets, each of which belongs to a frequent set of 2 MU s. 3. Reduce each document d 0 i ,1 6 i 6 n , into a compact document d tain a frequent synset, and replace every MU with a frequent MU if both contain the same frequent synset. 4. Insert each compact document into the GST. 5. Using the depth-first traversing, visit every node in the GST. If a node j has a frequent word meaning sequence FMS j with a set of document ids Ids j , create a cluster candidate cc 6. Combine the overlapping cluster candidates based on the overlap threshold d and the optional user-spec-ified number of final clusters. 6. Experimental evaluation
In this section, we evaluate the performance of our clustering algorithms in terms of the scalability of find-ing frequent word sequences, and the accuracy of clustering. We implemented our algorithms in C++ on a
SuSE Linux PC with a Celeron 500 MHz processor and 384 MB memory. 6.1. Data sets
For the performance evaluation, two groups of data sets are used. One group is typical text document sets number of classes, dimension of database and document distribution. We chose this group of data sets to test our algorithms X  performance on typical text documents. There are nine data sets in this group. They are Re1,
Re2, Re3, Re4 and Re5 from the EXCHANGES, ORGS, PEOPLE and TOPICS category sets of the Reuters-21578 Distribution 1.0; and Ce1, Ce2, Ce3 and Ce4 from the CISI, CRAN and CACM abstracts of Classic database [4] .

Another group of data sets are prepared by ourselves. We tried to simulate the case of using a search engine to retrieve the desired documents from a database, and we adopted the Lemur Toolkit [25] as the search engine. The English newswire corpus of the HARD track of the Text Retrieval Conference 2004 [16] is used as the database. This corpus includes about 652,309 documents (in 1575 MB) from eight different sources, and there are 29 test queries. Among those 29 queries, HARD-306, HARD-309, and HARD-314 queries were sent
Se1, Se2 and Se3, for our evaluation. The reason why we chose only top 200 documents is that usually users do not read more than 200 documents for a single query.

Each document of the test data sets has been pre-classified into one or more classes. This information is hidden during the clustering processes and used to evaluate the clustering quality of each clustering algorithm in terms of the accuracy. Table 4 summarizes the characteristics of all the data sets used for our experiments. 6.2. Evaluation of the finding frequent word sequences
We evaluated our method of finding frequent word sequences in terms of its scalability. Before the mining, preprocessing steps including the stop-words removal and the stemming were performed on the data sets. The whole mining process has two steps: finding frequent 2-word sets, then building and traversing the GST. Any frequent itemset mining algorithm can be used to find frequent 2-word sets in our method. In our experiment, we adopted Apriori algorithm [1] , which is the most representative frequent itemset mining algorithm. There formance to find only frequent 2-itemsets. The efficiency of Apriori is sensitive to the minimum support level. When the minimum support is decreased, the runtime of Apriori increases as there are more frequent itemsets.
The most time-consuming part is building and traversing the GST. As reported in [14,34] , the GST con-struction time can be linear with the size of the whole database, and it can be constructed incrementally as the documents are read from files. The number of unique words affects the construction and traverse times of the GST. Thus, by keeping only the frequent words which are members of frequent 2-word sets in the data-base, the construction of the GST of large databases becomes more feasible. In our method, the size of whole database is dramatically reduced by removing infrequent words from the documents. For example, the Re2 data set has 807 documents with 116,816 total words and 6488 unique words. Its average document size and length are 1.85 KB and 144, respectively. The Apriori algorithm finds 1049 frequent 2-word sets with 175 unique words when the minimum support is 10%. After removing infrequent words from the documents, the total number of words is reduced to 52,792, and the average document length is reduced to 51. Documents in Re2 are about organizations. By using our method, 64 frequent word sequences are found when the min-
Even though the construction time of the GST is reduced dramatically when the compact documents are used, there is a trade-off between the construction time and the memory space requirement when the GST is large. When the average length of the compact documents is big, we may have the memory bottleneck prob-lem as the GST size becomes larger than the available memory size. To handle this problem, many efficient in-memory suffix tree construction algorithms were proposed [9,10,21,27] .

Since the number of unique words in text databases is relatively large, we used a hash table in our imple-mentation for efficient searching of the child nodes during the construction of the GST. In order to test the scalability of our method, we increased the number of documents in the test data set by duplicating Re1,
Re2 and Re3, and the execution time is plotted against the data set size in Fig. 3 , without including the time to read the data file initially. As we can see, the execution time increases linearly with the data set size.
We also tested our method for various minimum support levels on the Re2 data set, and the result is shown in Fig. 4 . As the minimum support level is increased, the execution time decreases since less frequent 2-word sets are found, and it leads to a smaller GST. From these experimental results, we can conclude that our method of finding frequent word sequences is very scalable. 6.3. Evaluation method of the text clustering
We used the F-measure and purity values to evaluate the accuracy of our clustering algorithms. The F -mea-query, whereas each pre-classified set of documents can be considered as the desired set of documents for that
If n i is the number of the members of class i , n j is the number of the members of cluster j ,and n number of the members of class i in cluster j , then P ( i , j ) and R ( i , j ) can be defined as The corresponding F -measure F ( i , j ) is defined as Then, the F -measure for the whole clustering result is defined as the clustering result is [33] .

The purity of a cluster represents the fraction of the cluster corresponding to the largest class of documents assigned to that cluster, thus the purity of a cluster j is defined as The overall purity of the clustering is a weighted sum of the cluster purities:
In general, the larger the purity value is, the better the clustering result is [39] . 6.4. Comparison of CFWS with other algorithms
For a comparison with our CFWS, we also executed bisecting k -means (BKM) and FIHC on the same data sets. We chose BKM because it has been reported to produce a better clustering result consistently compared to k -means and agglomerative hierarchical clustering algorithms [33] . FIHC is chosen because, like CFWS, it uses frequent word sets. For a fair comparison, we did not implement BKM and FIHC algorithms by our-selves. We downloaded the CLUTO toolkit [20] to perform BKM, and obtained FIHC version 1.0 [17] from the inventor of FIHC.

Data sets were preprocessed before they were used in our experiments. First, we removed the stop-words from the documents. Then, the words were stemmed by using the Porter X  X  suffix-stripping algorithm [33] .Itis important to do the stemming since it can eliminate the minor difference between words with the identical meaning.

Fig. 5 shows the average F -measures of three algorithms: BKM, FIHC and CFWS. For all three algo-rithms, we specified the desired number of final clusters, denoted by KCluster , to be the same as the number of classes in each data set. We executed BKM 11 times on each data set with randomly selected initial cent-roids and calculated the average F -measure. FIHC and CFWS were also executed 11 times on each data set; and for each run, we specified the same minimum support level, in the range of 5 X 15%, for both algorithms so that the comparison would be fair. Based on the average F -measures, it is clear that our CFWS algorithm con-query results. Fig. 6 shows the effect d on the F -measure of CFWS when KCluster is not specified. As we can see, the F -measure is not sensitive to d if d is higher than 0.5.

The F -measure represents the accuracy of clustering. Our CFWS algorithm has better F -measures because we use a better model for text documents. Both BKM and FIHC use the vector space model for text docu-ments. However, the vector space model cannot capture the order of words, which is important in representing the context in the text document. On the contrary, our model represents the words as well as their orders, which provide more valuable information for clustering.

Both CFWS and FIHC use the frequent words to cluster documents, and FIHC X  X  measurement of the closeness between clusters is similar to ours. However, FIHC uses the frequent word sets to cluster documents, whereas CFWS uses the frequent word sequences. If a word is a member of a frequent k -word sequence, it must be a member of a frequent k -word set. But a member of a frequent k -word set is not necessarily a mem-ber of a frequent k -word sequence. It is also true that a frequent k -word set is not necessarily a frequent k -word sequence. As a result, FIHC has a higher probability of grouping unrelated documents into the same cluster.

Table 6 shows the F -measures of FIHC and CFWS on Re1, Ce1 and Se1 data sets when KCluster is not specified and the minimum support levels are 5%, 10% and 15%, respectively. We can see that CFWS produces more accurate clusters at different minimum support levels.

Fig. 7 shows the average purity values of the three clustering algorithms when the KCluster value specified is the same as the number of classes in each data set. As we can see, BKM performs better than both FIHC and
CFWS for almost all of the typical text document sets: Re1, Re2, Re3, Re4, Re5, Ce1, Ce2, Ce3 and Ce4. The reason is that people may use different words to express the same meaning, but both FIHC and CFWS per-form the exact word matching during the procedure of finding frequent word sets and sequences, respectively.
Our CFWMS algorithm is designed to solve this problem. In CFWMS, we first apply an ontology, WordNet, to convert word forms to word meanings. Then, we match the word meanings instead of word forms. The frequent word meaning sequences are used as the measurement of the closeness between documents in
CFWMS. In typical text documents, the same word meanings may not be expressed by the same word forms because synonyms, hypernyms, and polysemous words are used. By using WordNet, the lexical and semantic relations between word forms and word meanings are explored. The word meanings expressed by different word forms are also captured successfully in our CFWMS algorithm. Moreover, the topics covered in the typ-ical text documents are represented by the frequent word meaning sequences. The experimental results of CFWMS are shown in the next section.

For the search query results (i.e., Se1, Se2 and Se3), CFWS performs better than BKM and FIHC. The unique characteristics of these document sets can explain the performance results. These document sets were obtained from the retrieval lists of a search engine for user queries. We used the simple TFIDF retrieval model of the Lemur Toolkit [25] to retrieve the documents for each query. The retrieved documents are more likely to have common words between them compared to the set of typical text documents. In other words, the topics covered by this type of document sets are much closer than those of the typical text document sets. For exam-ple, top 200 documents on the retrieval list for the query of  X  X  X ea consumption increased in US X  X  cover three specifying a lower minimum support level, we can obtain finer clusters to cover the subtopics in the data set, which are shown in Table 7 . The class labels are the frequent word sequences we found during the clustering process. The words in the frequent word sequences are in the original form before the stemming. For this type of data sets, our CFWS algorithm can work better as it can group the documents into much finer clusters. The reason is that our algorithm can tell the minor differences between subtopics by recognizing frequent word sequences in the documents. By clustering the retrieved documents, our CFWS algorithm enables the web search engines to provide more accurate search results to the user. 6.5. Comparison of CFWMS with other algorithms
For a comparison with our CFWMS algorithm, we implemented the modified bisecting k -means using background knowledge (BBK) algorithm proposed in [19] . BBK uses the vector space model and enriches the text representation by adding synonyms and up to five levels of hypernyms for all nouns based on the con-text in the document. For both CFWMS and BBK, WordNet was used as the ontology.

The stop-words were removed as the first step of preprocessing. However, since WordNet provides the mor-phological capability, we did not perform the stemming for CFWMS and BBK.

In CFWMS, the knowledge from WordNet is used to convert word forms to word meanings. As a result, the dimension of text database is reduced further than the case of CFWS, and Fig. 8 shows the changes of the dimension in test data sets. On the other hand, BBK adds synsets to the vector space model of the database, hence the dimension of the text database is increased after the preprocessing step.

For a fair comparison, we specified the desired number of clusters, KCluster , to be the same as the number of classes in each data set. We executed BBK 11 times on each data set with randomly selected initial cent-roids. CFWS and CFWMS were also executed 11 times on each data set with the same minimum support level, in the range of 5 X 15%. The performances of CFWS, CFWMS, and BBK are compared in Figs. 9 and 10 . CFWMS has better average F -measures than that of CFWS in most cases because it can identify the same word meaning sequences represented by different word form sequences. The average purity values show that CFWMS can improve the clustering quality for both typical text documents and search query results.

This comparison shows that our CFWMS outperforms BBK on all three groups of test data sets. CFWMS utilizes the frequent sequences of word meanings in the clustering process, which can achieve more accurate clustering result than the vector space model of BBK. Moreover, our method of converting word forms into word meanings is better than the one used in BBK. Both CFWMS and BBK retrieve the hypernyms of word forms, and the noise created during this process could affect the accuracy of clustering. CFWMS retrieves only up to two levels of hypernyms, while BBK retrieves up to five levels, so CFWS introduces less noise into the text database than BBK. 7. Conclusion In this paper, first we proposed a new text document clustering algorithm named CFWS, which stands for
Clustering based on Frequent Word Sequences. Unlike the traditional vector space model, our model utilizes the sequential patterns of the words in the document. Frequent word sequences discovered from the document set can represent the topics covered by the documents very well, and the documents containing the same fre-quent word sequences are clustered together in our algorithm.
 To facilitate the discovery of the frequent word sequences from documents, we use the Generalized Suffix
Tree (GST) built on the frequent words within each document. The performance of our frequent word sequence mining algorithm is quite scalable. For very large data sets, we can adopt some of the efficient in-memory GST construction algorithms proposed [9,10,21,27] .

Most existing clustering algorithms do not satisfy the unique requirements of the text document clustering, such as handling high dimension and context-sensitive languages, and providing overlapped clusters and self-explanatory labels of the clusters. Our CFWS algorithm explores unique characteristics of text documents by using the frequent word sequences to reduce the high dimension of the documents and to measure the close-ness between them. Our experimental results show that CFWS performs better than bisecting k -means (BKM) [33] and Frequent Itemset-based Hierarchical Clustering (FIHC) [12] algorithms in terms of accuracy, espe-cially for the fine clustering of documents in the same category, which is a very useful feature for modern web search engines.
 Then, we proposed another new text document clustering algorithm named CFWMS, which stands for
Clustering based on Frequent Word Meaning Sequences. CFWMS enhanced CFWS by using frequent word meaning sequences to measure the closeness between documents. Frequent word meaning sequences can cap-ture the topics of documents more precisely than frequent word sequences. To find frequent word meaning sequences, we used the synonyms and hyponyms/hypernyms provided by the WordNet ontology to preprocess the documents. CFWMS has a better accuracy than CFWS on most of our test data sets. Also, CFWMS out-performs the modified bisecting k -means using background knowledge (BBK) [19] , which also uses an ontol-ogy to explore the lexical relationships between words.

References
