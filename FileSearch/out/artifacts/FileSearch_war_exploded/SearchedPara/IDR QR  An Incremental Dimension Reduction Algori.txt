 Dimension reduction is critical for many database and data mining applications, such as efficient storage and retrieval of high-dimensional data. In the literature, a well-known di-mension reduction scheme is Linear Discriminant Analysis (LDA). The common aspect of previously proposed LDA based algorithms is the use of Singular Value Decompo-sition (SVD). Due to the difficulty of designing an incre-mental solution for the eigenvalue problem on the product of scatter matrices in LDA, there is little work on design-ing incremental LDA algorithms. In this paper, we pro-pose an LDA based incremental dimension reduction algo-rithm, called IDR/QR, which applies QR Decomposition rather than SVD. Unlike other LDA based algorithms, this algorithm does not require the whole data matrix in main memory. This is desirable for large data sets. More impor-tantly, with the insertion of new data items, the IDR/QR algorithm can constrain the computational cost by apply-ing efficient QR-updating techniques. Finally, we evaluate the effectiveness of the IDR/QR algorithm in terms of clas-sification accuracy on the reduced dimensional space. Our experiments on several real-world data sets reveal that the accuracy achieved by the IDR/QR algorithm is very close to the best possible accuracy achieved by other LDA based algorithms. However, the IDR/QR algorithm has much less computational cost, especially when new data items are dy-namically inserted.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms Keywords: Dimension reduction, Linear Discriminant Anal-ysis, incremental learning, QR Decomposition
Department of Computer Science &amp; Engineering, Uni-versity of Minnesota, Minneapolis, MN 55455, U.S.A. { jieping,huix,hpark,janardan,kumar } @cs.umn.edu Department of Computer Science, University of Delaware, Newark, DE, U.S.A. qili@cis.udel.edu
Efficient storage and retrieval of high-dimensional data is one of the central issues in database and data mining re-search. In the literature, many efforts have been made to design multi-dimensional index structures [2], such as R -trees, R -trees, X -trees, SR-tree, etc, for speeding up query processing. However, the effectiveness of queries using any indexing schemes deteriorates rapidly as the dimension in-creases, which is the so-called curse of dimensionality .A standard approach to overcome this problem is dimension reduction, which transforms the original high-dimensional data into a lower-dimensional space with limited loss of in-formation. Once the high-dimensional data is transfered into a low-dimensional space, indexing techniques can be ap-plied effectively to organize this low-dimensional space and facilitate efficient retrieval of data [14].

A well-known dimension reduction scheme is Linear Dis-criminant Analysis (LDA )[7, 9], which computes a linear transformation by maximizing the ratio of the between-class distance to the within-class distance, thereby achieving max-imal discrimination. LDA has been applied to many appli-cations such as text retrieval [3] and face recognition [1, 17, 21]. In the past, many LDA extensions have been developed to deal with the singularity problem encountered by clas-sical LDA. There are three major extensions: regularized LDA, PCA+LDA, and LDA/GSVD. The common point of these algorithms is the use of Singular Value Decomposi-tion (SVD )or Generalized Singular Value Decomposition (GSVD). The difference among these LDA members is as follows: Regularized LDA increases the magnitude of the diagonal elements of the scatter matrix by adding a scaled identity matrix; PCA+LDA first applies PCA on the raw data to get a more compact representation so that the singu-larity of the scatter matrix is decreased; LDA/GSVD solves a trace optimization problem using GSVD.
 The above LDA algorithms have certain limitations. First, SVD or GSVD requires that the whole data matrix is stored in main memory. This requirement imposes difficulties on making the LDA algorithms scalable to large data sets. Also, the expensive computation of SVD or GSVD can signifi-cantly degrade the computational performance of the LDA algorithms when dealing with large data sets. Finally, since it is difficult to design an incremental solution for the eigen-value problem on the product of scatter matrices, little ef-fort has been made to design incremental LDA algorithms. However, in many practical applications, acquisition of rep-resentative training data is expensive and time-consuming. It is thus common to have a small chunk of data available over a period of time. In such settings, it is necessary to de-velop an algorithm that can run in an incremental fashion to accommodate the new data.

The goal of this paper is to design an efficient and in-cremental dimension reduction algorithm while preserving competitive performance. More precisely, when queries are conducted on the reduced dimension data from the proposed algorithm, the query accuracy should be comparable with the best possible query accuracy achieved by other LDA based algorithms.

To resolve these issues, we design an LDA based incremen-tal dimension reduction algorithm, called IDR/QR, which applies QR Decomposition rather than SVD or GSVD. This algorithm has two stages. The first stage is to maximize the separability between different classes. This is fulfilled by QR Decomposition. The distinct property of this stage is its low time and space complexity. The second stage incorporates both between-class and within-class information by applying LDA on the  X  X educed X  scatter matrices resulting from the first stage. Unlike other LDA based algorithms, IDR/QR does not require the whole data matrix in main memory. This is desirable for large data sets. Also, our theoreti-cal analysis indicates that the computational complexity of IDR/QR is linear in the number of the data items in the training set as well as the number of dimensions. More im-portantly, the IDR/QR algorithm can work incrementally. When new data items are dynamically inserted, the compu-tational cost of the IDR/QR algorithm can be constrained by applying efficient QR-updating techniques.

Finally, we have conducted extensive experiments on sev-eral well-known real-world datasets. The experimental re-sults show that the IDR/QR algorithm can be an order of magnitude faster than SVD or GSVD based LDA algorithms and the accuracy achieved by IDR/QR is very close to the best possible accuracy achieved by other LDA based algo-rithms. Also, when dealing with dynamic updating, the computational advantage of IDR/QR over SVD or GSVD based LDA algorithms becomes more substantial while still achieving comparable accuracy.
 Overview: The rest of this paper is organized as follows. Section 2 introduces related work. In Section 3, we give a review of LDA. The batch implementation of the IDR/QR algorithm is presented in Section 4. Section 5 describes the incremental implementation of the IDR/QR algorithm. A comprehensive empirical study of the performance of the proposed algorithms is presented in Section 6. We conclude in Section 7 with a discussion of future work.
Principal Component Analysis (PCA )is one of the stan-dard and well-known methods for dimension reduction [13]. Because of its simplicity and ability to extract the global structure of a given data set, PCA is used widely in com-puter vision [22].

Most previous work on PCA and LDA requires that all the training data be available before the dimension reduc-tionstep. Thisisknownasthe batch method .Thereissome recent work in vision and numerical linear algebra litera-ture for computing PCA incrementally [4, 11]. Despite the popularity of LDA in the vision community, there is little work for computing it incrementally. The main difficulty is the involvement of the eigenvalue problem on the product of scatter matrices, which is hard to maintain incrementally. Although iterative algorithms have been proposed for neural network based LDA [5, 16], they require O ( d 2 )time for each update, where d is the dimension of the data. This is still expensive, especially when the data has high dimension.
For convenience, we present in Table 1 the important no-tations used in the paper.

This section gives a brief review of classical LDA, as well as its three extensions: regularized LDA, PCA+LDA, and LDA/GSVD.

Given a data matrix A  X  IR d  X  n , we consider finding a linear transformation G  X  IR d  X  that maps each column a j of A ,for1  X  j  X  n ,inthe d -dimensional space to a vector y = G T a j in the -dimensional space.

Classical LDA aims to find the transformation G such that class structure of original high-dimensional space is preserved in the reduced space. Let the data matrix A be partitioned into c classes as A =[ A 1 ,A 2 ,  X  X  X  ,A c ], where A
Let I i be the set of column indices that belong to the i th class, i.e., a j ,for j  X  I i , belongs to the i th class.
In general, if each class is tightly grouped, but well sep-arated from the other classes, the quality of the cluster is considered to be high. In discriminant analysis, two scatter matrices, within-class and between-class scatter matrices are defined to quantify the quality of the cluster, as follows [9]: where m i is the centroid of the i th class and m is the global centroid. Define the matrices where e i =(1 ,  X  X  X  , 1) T  X  IR n i . Then the scatter matrices S w and S b canbeexpressedas S w = H w H T w , S b = H b H T b . The traces of the two scatter matrices can be computed as follows, Hence, trace ( S w )measures the closeness of the vectors within classes, while trace ( S b )measures the separation between classes.

In the lower-dimensional space resulting from the linear transformation G , the within-class and between-class scatter matrices become
An optimal transformation G would maximize trace ( S L b ) and minimize trace ( S L w ). A common optimization in clas-sical LDA [9] is
G =arg max where g i is the i th column of G .

The solution to the optimization in Eq. (3 )can be ob-tained by solving the eigenvalue problem on S  X  1 w S b ,if S non-singular, or on S  X  1 b S w ,if S b is non-singular. There are at most c  X  1 eigenvectors corresponding to nonzero eigen-values, since the rank of the matrix S b is bounded above by c  X  1. Therefore, the reduced dimension by classical LDA is at most c  X  1. A stable way to solve this eigenvalue problem is to apply SVD on the scatter matrices. Details on this can be found in [21].

Classical LDA requires that one of the scatter matrices be non-singular. For many applications involving under-sampled data, such as text and image retrieval, all scatter matrices are singular. Classical LDA is thus not applicable. This is the so-called singularity or undersampled problem . To cope with this challenge, several methods, including two-stage PCA+LDA, regularized LDA, and LDA/GSVD have been proposed in the past.

A common way to deal with the singularity problem is to apply an intermediate dimension reduction stage, such as PCA, to reduce the dimension of the original data be-fore classical LDA is applied. The algorithm is known as PCA+LDA, or subspace LDA. In this two-stage PCA+LDA algorithm, the discriminant stage is preceded by a dimension reduction stage using PCA. A limitation of this approach is that the optimal value of the reduced dimension for PCA is difficult to determine.

Another common way to deal with the singularity problem is to add some constant value to the diagonal elements of S as S w +  X I d ,forsome  X &gt; 0, where I d is an identity matrix [8]. It is easy to check that S w +  X I d is positive definite, hence non-singular. This approach is called regularized LDA (RLDA). A limitation of RLDA is that the optimal value of the parameter  X  is difficult to determine. Cross-validation is commonly applied for estimating the optimal  X  [15].
The LDA/GSVD algorithm in [12, 23] is a recent work along the same line. A new criterion for generalized LDA was presented in [23]. The inversion of the matrix S w is avoided by applying the Generalized Singular Value Decom-position (GSVD). LDA/GSVD computes the solution ex-actly without losing any information. However, one limita-tion of this method is the high computational cost of GSVD, which limits its applicability for large datasets, such as im-age and text data.
In this section, we present the batch implementation of the IDR/QR algorithm. This algorithm has two stages. The first stage maximizes the separation between different classes via QR Decomposition [10]. Without the concern of min-imizing within-class distance, this stage can be used inde-pendently as a dimension reduction algorithm. The second stage addresses the concern on within-class distance, while keeping low time/space complexity.

The first stage of IDR/QR aims to solve the following optimization problem, Note that this optimization only addresses the concern of maximizing between-class distance. The solution can be ob-tained by solving the eigenvalue problem on S b .Thesolution can also be obtained through QR Decomposition on the cen-troid matrix C , which is the so-called Orthogonal Centroid Method (OCM )[19], where consists of the c centroids. More specifically, let C = QR be the QR Decomposition of C ,where Q  X  IR n  X  c has orthonor-mal columns and R  X  IR c  X  c is upper triangular. Then solves the optimization problem in Eq. (4), for any orthog-onal matrix M . Note the choice of orthogonal matrix M is arbitrary, since trace( G T S b G )=trace( M T G T S b GM ),for any orthogonal matrix M . In OCM [19], M is set to be the identity matrix for simplicity.

The second stage of IDR/QR refines the first stage by addressing the concern on within-class distance. It incor-porates the within-class scatter information by applying a relaxation scheme on M in Eq. (6 )(relaxing M from an or-thogonal matrix to an arbitrary matrix). Note that the trace value in Eq. (3 )is the same for an arbitrary non-singular M , however the constraints in Eq. (3 )will not be satisfied for arbitrary M . In the second stage of IDR/QR, we look for a transformation G such that G = QM ,forsome M .Note that M is not required to be orthogonal. The original prob-lem on computing G is equivalent to computing M .Since the original optimization on finding optimal G is equivalent to finding M ,with B = Q T S b Q and W = Q T S w Q as the reduced between-class and within-class scatter matrices, re-spectively. Note that B has much smaller size than the original scatter matrix S b (similarly for W ).

The optimal M can be computed efficiently using many existing LDA based methods, since we are dealing with ma-trices B and W of much smaller size c by c .Akeyobser-vation is that the singularity problem of W will not be as Algorithm 1: Batch IDR/QR Input: data matrix A ;
Output: optimal transformation matrix G ; /* Stage I: */ 1. Construct centroid matrix C ; 2. Compute QR Decomposition of C as C = QR ; /* Stage II: */ 3. Z  X  H T w Q ; 4. Y  X  H T b Q ; 5. W  X  Z T Z ; /*Reduced within-class scatter matrix*/ 6. B  X  Y T Y ; /*Reduced between-class scatter matrix*/ 7. Compute the c eigenvectors  X  i of ( W +  X I c )  X  1 B , 8. G  X  QM ,where M =[  X  1 ,  X  X  X  , X  c ].
 Table 2: Complexity comparison: n is the number of training data points, d is the dimension, and c is the number of classes. severe as the original S w ,since W has much smaller size than S w . We can compute optimal M by simply apply-ing regularized LDA, that is, we compute M ,bysolvinga small eigenvalue problem on ( W +  X I c )  X  1 B , for some posi-tive constant  X  . The pseudo-code for this algorithm is given in Algorithm 1 .
We close this section by analyzing the time and space complexity of the IDR/QR algorithm.

It takes O ( dn )for the formation of the centroid matrix C in Line 1. The complexity for QR Decomposition in Line 2 is O ( c 2 d )[10]. Lines 3 and 4 take O ( ndc )and O ( dc tively for matrix multiplications. It then takes O ( c 2 n )and O ( c 3 )for matrix multiplications in Lines 5 and 6, respec-tively. Line 7 computes the eigen-decomposition of a c by c matrix, hence takes O ( c 3 )[10]. The matrix multiplication in Line 8 takes O ( dc 2 ).

Note that the dimension, d , and the number, n ,ofpoints are usually much larger, compared with the number, c ,of classes. Thus, the most expensive step in Algorithm 1 is in Line 3, which takes O ( ndc ). Therefore, the time complexity of IDR/QR is linear in the number of points, linear in the number of classes, and linear in the dimension of the dataset.
It is clear that only the c centroids are required to reside in the main memory, hence the space complexity of IDR/QR is O ( dc ). Table 2 lists the time and space complexity of several dimension reduction algorithms discussed in this paper. It is clear from the table that IDR/QR and OCM are more efficient than other methods.
The incremental implementation of the IDR/QR algo-rithm is discussed in details in this section. We will adopt the following convention: For any variable X , its updated version after the insertion of a new instance is denoted by  X  X . For example, the number, n i ,ofelementsinthe i th class is changed to  X  n i , while centroid m i is changed to  X  m
With the insertion of a new instance, the centroid ma-trix C , H w and H b will change accordingly, as well as W and B . The incremental updating in IDR/QR proceeds in three steps: (1 )QR-updating of the centroid matrix C = [ m 1 ,  X  X  X  ,m k ]inLine2of Algorithm 1 ; (2 )Updating of the reduced within-class scatter matrix W in Line 5; and (3 )Updating of the reduced between-class scatter matrix B in Line 6.

Let x be a new instance inserted, which belongs to the i th class. Without loss of generality, let us assume we have data from the 1st to the k th class, just before x is inserted. In general, this can be done by switching the class labels between different classes. In the rest of this section, we con-sider the incremental updating in IDR/QR in two distinct cases: (1) x belongs to an existing class, i.e., i  X  k ;(2) x belongs to a new class, i.e., i&gt;k . As will be seen later, the techniques for these two cases are quite different.
Recall that we have data from the 1st to k th classes, when anewinstance x is being inserted. Since x belongs to the i th class, with 1  X  i  X  k , the insertion of x will not create a new class. In this section, we show how to do the incremental updating in three steps.
Since the new instance x belongs to the i th class,  X  C = [ m 1 ,  X  X  X  ,m i + f,  X  X  X  ,m k ], where f = x  X  m i  X  n i ,and  X  n Hence,  X  C canberewrittenas  X  C = C + f  X  g T ,for g =
The problem of QR-updating of the centroid matrix C can be formulated as follows: Given the QR Decomposition of the centroid matrix C = QR ,for Q  X  IR d  X  k and R  X  IR k compute the QR Decomposition of  X  C .

Since  X  C = C + f  X  g T , the QR-updating of the centroid matrix C can be formulated as a rank-one QR-updating. However, the algorithm in [10] cannot be directly applied, since it requires the complete QR Decomposition, i.e., the matrix Q is square. While in our case, we use the skinny QR Decomposition, i.e. Q is rectangular. Instead, we apply a slight variation of the algorithm in [6] via the following two-stage QR-updating: (1 )A complete rank-one updating as in [10] on a small matrix; (2 )A QR-updating by an insertion of a new row. Details are given below.

Partition f into two parts: the projection onto the orthog-onal basis Q , and its orthogonal complement. Mathemati-cally, f can be partitioned into f = QQ T f +( I  X  QQ T ) f .It is easy to check that Q T ( I  X  QQ T ) f = 0, i.e. ( I  X  QQ orthogonal to, or lies in the orthogonal complement of, the subspace spanned by the columns of Q . It follows that where f 1 = Q T f , f 2 =( I  X  QQ T ) f .Next,weshowhow to compute the QR Decomposition of  X  C in two stages. The first stage updates the QR Decomposition of Q ( R + f 1  X  It corresponds to a rank-one updating and can be done at O ( kd )[10]. This results in the updated QR Decomposition as Q ( R + f 1  X  g T )= Q 1 R 1 ,where Q 1 = QP 1 ,and P 1 is orthogonal.
 to the subspace spanned by the columns of Q ,itisalso orthogonal to the subspace spanned by the columns of Q 1 = QP 1 , i.e. [ Q 1 ,q ] has orthonormal columns.

The second stage computes QR-updating of whichcorrespondstothecasethat || f 2 || g T is inserted as a new row. This stage can be done at O ( dk )[10]. The updated QR Decomposition is where [  X  Q,  X  q ]=[ Q 1 ,q ] P 2 , for some orthogonal matrix P
Combining both stages, we have  X 
C = Q 1 R 1 + || f 2 || q  X  g T =[ Q 1 ,q ] R 1 || f as the updated QR Decomposition of  X  C , assuming || f 2 If || f 2 || =0,then  X  C = Q 1 R 1 is the updated QR Decompo-sition of  X  C .Notethat f 2 can be computed efficiently as f = f  X  ( Q ( Q T f )), by doing matrix-vector multiplication twice. Hence, the total time complexity for the QR-updating of the centroid matrix C is O ( dk ).
Next we consider the updating of the reduced within-class scatter matrix W = Q T H w H T w Q (Line 5 of Algorithm 1 ). Let  X  W =  X  Q T  X  H w  X  H T w  X  Q be its updated version.
Note that H w =[ A 1  X  m 1  X  e T 1 ,  X  X  X  ,A k  X  m k  X  e T Its updated version  X  H w differs from H w on the i th block. block of its updated version  X  H w is where u = x  X  m i , v =  X  m i  X  m i and  X  e i = e i 1  X  IR
The product  X  H i  X  H T i canbecomputedas  X  H H T i =([ H i ,u ]  X  v  X   X  e T i )([ H i ,u ]  X  v  X   X  e where the third equality follows, since ( H i ,u )  X  e i = m )+ u = u ,and( v  X   X  e T i )(  X  e i  X  v T )= vv T (  X  e T Since H w H T w = It follows that  X  W =  X  Q T  X  H w  X  H T w  X  Q where  X  u =  X  Q T u ,and  X  v =  X  Q T v . The assumption of the approximation in Eq. (9 )is that the updated  X  Q with the insertion of a new instance is close to Q .

The computation of  X  u and  X  v takes O ( dk ). Thus, the com-putation for updating W takes O ( dk ).
Finally, let us consider the updating of the reduced between-class scatter matrix B = Q T H b H T b Q (Line 6 of Algorithm 1 ). Its updated version is B =  X  Q T  X  H b  X  H T b  X  Q .
The key observation for efficient updating of B is that canberewrittenas where F = D  X  h T , D = diag( [  X   X  n 1 ,  X  X  X  ,
By the updated QR Decomposition  X  C =  X  Q  X  R ,wehave  X  Q
T  X  H b =[  X  Q T  X  C,  X  Q T  X  m ] F =[  X  R,  X  Q T  X  m ] F = It is easy to check that  X  m = 1  X  n  X  C  X  r ,where r =(  X  n Hence,  X  Q T  X  m =  X  Q T 1  X  n  X  C  X  r = 1  X  n  X  R  X  B =  X  Q T  X  H b  X  H T b  X  Q =(  X  RD  X   X  Q T  X  m  X  h Therefore, it takes O ( k 3 )time for updating B .
Overall, the total time for QR-updating of C and updating of W and B with the insertion of a new instance from an existing class is O ( dk + k 3 ). The pseudo-code is given in Algorithm 2 .
Recall that we have data from the 1st to k th classes, upon the insertion of x .Since x belongs to i th class, with i&gt;k , the insertion of x will result in a new class. Without loss of generality, let us assume i = k +1. Hence the ( k + 1)th centroid  X  m k +1 = x . Then the updated centroid matrix [ m 1 ,m 2 ,  X  X  X  ,m k ,x ]=[ C,x ]. In the following, we focus on Algorithm 2: Updating Existing Class Input: centroid matrix C =[ m 1 ,m 2 ,  X  X  X  ,m k ], its
Output: updated matrix  X  W , updated centroid matrix 1.  X  n j  X  n j ,for j = i ;  X  n i  X  n i +1; f  X  x  X  m i  X  2.  X  m i  X  m i + f ;  X  m j  X  m j ,foreach j = i ; 3.  X  C  X  [  X  m 1 ,  X  X  X  ,  X  m i ,  X  X  X  ,  X  m k ]; 4. f 1  X  Q T f ; f 2  X  ( I  X  QQ T ) f ; 5. do rank-one QR-updating of Q ( R + f 1  X  g T ) 6. if || f 2 || =0 7.  X  Q  X  Q 1 ;  X  R  X  R 1 ; 8. else 10. do QR-updating of [ Q 1 ,q ] R 1 || f 11.  X  Q  X  Q 2 ;  X  R  X  R 2 ; 12. endif 13. u  X  x  X  m i ; v  X   X  m i  X  m i ; 14.  X  u  X   X  Q T u ;  X  v  X   X  Q T v ; 15.  X  W  X  W +(  X  u  X   X  v )(  X  u  X   X  v ) T + n i  X  v  X  v 16. D  X  diag( 17. r  X  (  X  n 1 ,  X  X  X  ,  X  n k ) T ;  X  r  X  1  X  n  X  R  X  r ; 18.  X  B  X  (  X  RD  X   X  r  X  h T )(  X  RD  X   X  r  X  h T ) T the case when x does not lie in the space spanned by the k centroids { m i } k i =1 .
Given the QR Decomposition C = QR , it is straightfor-ward to compute the QR Decomposition of  X  C as  X  C =  X  by the Gram-Schmidt procedure [10], where  X  Q =[ Q,q ], for some q . The time complexity for this step is O ( dk ).
With the insertion of x from a new class ( k +1), the ( k + 1)th block  X  H k +1 is created, while H j ,for j =1 , keep unchanged. It is easy to check that  X  H k +1 =0. It follows that  X  H w  X  H T w = H w H T w .Hence  X  W =  X  Q T  X  H w  X  H T w  X  Q =  X  Q T H w H T w  X  Q =[ Q,q ] The assumption in the above approximation is that W is the dominant part in  X  W .
The updating of B follows the same idea as in the previous case. Note that canberewrittenas Algorithm 3: Updating New Class
Input: centroid matrix C =[ m 1 ,m 2 ,  X  X  X  ,m k ],
Output: updated matrix  X  W , updated centroid matrix 1.  X  n j  X  n j ,for j =1 ,  X  X  X  ,k ;  X  n k +1  X  1;  X  n  X  2. do QR-updating of  X  C =[ C,x ]as  X  C =  X  Q  X  R ; 3.  X  W  X  W 0 00 ; 4. D  X  diag 5. r  X  (  X  n 1 ,  X  X  X  ,  X  n k +1 ) T ;  X  r  X  1  X  n  X  Rr ; 6.  X  B  X  (  X  RD  X   X  r  X  h T )(  X  RD  X   X  r  X  h T ) T ; where the matrix F = D  X  h T ,and D is an diagonal ma-trix D =diag(
By the updated QR Decomposition  X  C =  X  Q  X  R ,wehave
Since  X  m = 1  X  n  X  C  X  r ,where r =(  X  n 1 ,  X  X  X  ,  X  n  X  Q
Then  X  B can be computed by similar arguments as in the previous case. Therefore, it takes O ( k 3 )for updating B .
Thus, the time for QR-updating of C and updating of W and B with the insertion of a new instance from a new class is O ( dk + k 3 ). The pseudo-code is given in Algorithm 3 .
With the above two incremental updating schemes, the incremental IDR/QR works as follows: For a given new in-stance x , determine whether it is from an existing or new class; If it is from an existing class, update the QR Decom-position of the centroid matrix C and W and B by applying Algorithm 2 ; otherwise update the QR Decomposition of the centroid matrix C and W and B by applying Algo-rithm 3 ; The above procedure is repeated until all points are considered. With the final updated  X  W and  X  B ,wecan compute the  X  k eigenvectors {  X  i }  X  k i =1 of (  X  W +  X I of classes (  X  k equals k ,if x is from an existing, and k +1 otherwise). Then the transformation G =  X  QM , assuming  X  C =  X  Q  X  R is the updated QR Decomposition.

The incremental IDR/QR proposed obeys the following general criteria for an incremental learning algorithm [20]: (1 )It is able to learn new information from new data; (2 )It does not require access to the original data; (3 )It preserves previously acquired knowledge; (4 )It is able to accommo-date new classes that may be introduced with new data.
In this section, we evaluate both the batch version and the incremental version of the IDR/QR algorithm. The per-formance is mainly measured by the computational cost in terms of the classification accuracy and execution time. In the experiment, we applied the K-Nearest Neighbor (K-NN) method [7] as the classification algorithm and classification accuracies are estimated by 10-fold cross validation.
Experimental Platform: All experiments were per-formed on a PC with a P4 1.8GHz CPU and 1GB main memory running a Linux operating system.

Experimental Data Sets: Our experiments were per-formed on the following four real-world data sets, which are from two different application domains, including face recog-nition and text retrieval. Some characteristics of these data sets are shown in Table 3. 1. AR 1 is a popular face image data set [18]. The face 2. ORL 2 is another popular face image data set, which http://rvl1.ecn.purdue.edu/  X  aleix/aleix face DB.html http://www.uk.research.att.com/facedatabase.html 3. tr41 document data set is derived from the TREC-5, 4. re1 document data set is derived from Reuters-21578
In this experiment, we compare the performance of the batch IDR/QR with several other dimension reduction algo-rithms including PCA+LDA, LDA/GSVD, OCM, and PCA. Note that IDR/QR applies regularization to the reduced within-class scatter, i.e., W +  X I c .Wechose  X  =0 . 5inour experiments, while it produced good overall results.
Figure 1 shows the classification accuracies on our four test data sets using five different dimension reduction algo-rithms. Main observations are as follows: http: // trec.nist.gov http: // www.research.att.com /  X  lewis Figure 2: Comparing the efficiency of computing the transformation (measured in seconds in log-scale)
Figure 2 shows the execution time (in log-scale )of differ-ent tested methods for computing the transformation. Even with log-scale presentation, we can still observe that the ex-ecution time for computing the transformation by IDR/QR or OCM is significantly smaller than that by PCA+LDA, LDA/GSVD, and PCA.
Here, we evaluate the effect of small reduced dimension on the classification accuracy using the AR data set. Recall that the reduced dimension by the IDR/QR algorithm is c , where c is the number of classes in the data set. If the value c is large (such as AR, which contains 126 classes), the reduced representation may not be suitable for efficient indexing and retrieval. Since the reduced dimensions from IDR/QR are ordered by their discriminant powers (see Line 7 of Algo-rithm 1 ), an intuitive solution is to choose the first few dimensions in the reduced subspace from IDR/QR. The ex-perimental results are shown in Figure 3. As can be seen, the accuracy achieved by keeping the first 20 dimensions only is still sufficiently high.
In this experiment, we compare the performance of incre-mental IDR/QR with that of batch IDR/QR in terms of Figure 3: The effect of small reduced dimension on classification accuracy using AR classification accuracy and the computational cost. We ran-domly order the data items in the data set and insert them into the training set one by one incrementally with the given order. The remaining data is used as the test set. Initially, we select the first 30% data items as the training set. Incre-mental updating is then performed with the remaining data items inserted one at a time.

Figure 4 shows the achieved classification accuracies by batch IDR/QR and incremental IDR/QR on four data sets. In the figure, the horizontal axis shows the portion of train-ing data items, and the vertical axis indicates the classifica-tion accuracy (as a percentage). We observe a trend that the accuracy increases when more and more training data items are involved. Another observation is that the accuracy by in-cremental IDR/QR is quite close to that by batch IDR/QR. Indeed, on four data sets, the maximal accuracy deviation between incremental IDR/QR and batch IDR/QR is within 4%. Recall that incremental IDR/QR is carried through QR Decomposition in three steps: (1 )QR-updating of the centroid matrix C ; (2 )Updating of the reduced within-class scatter W ; and (3 )Updating of the reduced between-class scatter B . The first and third steps are based on the exact scheme, while the second step involves approximation. Note that the main rationale behind our approximation scheme in updating W is that the change of Q matrix is relatively small and can be neglected for each single updating, where C = QR is the QR Decomposition of C .
 To give a concrete idea of the benefit of using incremental IDR/QR from the perspective of efficiency, we give a com-parison on the compuational cost between batch IDR/QR and incremental IDR/QR. The experimental results are given in Figure 5. As can be seen, the execution time of incre-mental IDR/QR is significantly smaller than that of batch IDR/QR. Indeed, for a single updating, incremental IDR/QR takes O ( dk + k 3 ), while batch IDR/QR takes O ( ndk ),where k is the number of classes in the current training set and n is the size of the current training set. The time for a sin-gle updating in incremental IDR/QR is almost a constant O ( dc + c 3 ), when all classes appear in the current training set, and the speed-up of incremental IDR/QR over batch IDR/QR keeps increasing when more points are inserted into the training set. Note that we only count the time for Lines 1 X 6 in Algorithm 1 , since each updating in in-cremental IDR/QR only involves the updating of the QR Decomposition (Line 2), W (Line 5 )and B (Line 6).
In this paper, we have proposed an LDA based incremen-tal dimension reduction algorithm, called IDR/QR, which applies QR Decomposition rather than SVD. The IDR/QR algorithm does not require whole data matrix in main mem-ory. This is desirable for large data sets. More importantly, the IDR/QR algorithm can work incrementally. In other words, when new data items are dynamically inserted, the computational cost of the IDR/QR algorithm can be con-strained by applying efficient QR-updating techniques. In addition, our theoretical analysis indicates that the compu-tational complexity of the IDR/QR algorithm is linear in the number of the data items in the training data set as well as the number of classes and the number of dimen-sions. Finally, our experimental results show that the accu-racy achieved by the IDR/QR algorithm is very close to the best possible accuracy achieved by other LDA based algo-rithms. However, the IDR/QR algorithm can be an order of magnitude faster. When dealing with dynamic updat-ing, the computational advantage of IDR/QR over SVD or GSVD based LDA algorithms becomes more dramatic while still achieving the comparable accuracy.

As for future research, we plan to investigate the appli-cations of the IDR/QR algorithm on searching extremely high-dimenional multimedia data, such as video.
 Acknowledgement This research was sponsored, in part, by the Army High Performance Computing Research Center under the aus-pices of the Department of the Army, Army Research Lab-oratory cooperative agreement number DAAD19-01-2-0014, and the National Science Foundation Grants CCR-0204109, ACI-0305543, IIS-0308264, and DOE/ LLNL W-7045-ENG-48. The content of this work does not necessarily reflect the position or policy of the government and the National Science Foundation, and no official endorsement should be inferred. Access to computing facilities was provided by the AHPCRC and the Minnesota Supercomputing Institute. [1] P.N. Belhumeour, J.P. Hespanha, and D.J. Kriegman. [2] C. B  X  ohm, S. Berchtold, and D. A. Keim. Searching in [3] S. Chakrabarti, S. Roy, and M. Soundalgekar. Fast [4] S.Chandrasekaran,B.S.Manjunath,Y.F.Wang, [5] C.ChatterjeeandV.P.Roychowdhury.On [6] J.W. Daniel, W. B. Gragg, L. Kaufman, and G. W. [7] R.O. Duda, P.E. Hart, and D. Stork. Pattern [8] J. H. Friedman. Regularized discriminant analysis. [9] K. Fukunaga. Introduction to Statistical Pattern [10] G. H. Golub and C. F. Van Loan. Matrix [11] P. Hall, D. Marshall, and R. Martin. Merging and [12] P. Howland, M. Jeon, and H. Park. Structure [13] I. T. Jolliffe. Principal Component Analysis . [14] K. V. Ravi Kanth, D.t Agrawal, A. E. Abbadi, and [15] W.J. Krzanowski, P. Jonathan, W.V McCarthy, and [16] J. Mao and K. Jain. Artificial neural networks for [17] A. Martinez and A. Kak. PCA versus LDA. In IEEE [18] A.M. Martinez and R. Benavente. The AR face [19] H. Park, M. Jeon, and J.B. Rosen. Lower dimensional [20] R. Polikar, L. Udpa, S. Udpa, and V. Honavar. [21] D. L. Swets and J.Y. Weng. Using discriminant [22] F.D.L. Torre and M. Black. Robust principal [23] J. Ye, R. Janardan, C.H. Park, and H. Park. An
