 The problem of sequential pattern mining (SPM) , or finding frequent sequences of events in data with a temporal component, has been studied extensively [23,17,4] since its introduction in [18,3]. In classical SPM, the data to be mined is deter-ministic, but it it recognized that data obtained from a wide range of data sources is inherently uncertain [1]. This paper is concerned with SPM in proba-bilistic databases [19], a popular framework for modelling uncertainty. Recently several data mining and ranking problems have been studied in this framework, including top-k [24,8] and frequent itemset mining (FIM) [2,5,6,7]. In classical SPM, the event database consists of tuples eid, e,  X  ,where e is an event ,  X  is a source and eid is an event-id which incorporates a time-stamp . A tuple may record a retail transaction (event) by a customer (source), or an observation of an object/person (event) by a sensor/camera (source). Since event-ids have a time-stamp, the event database ca n be viewed as a collection of source sequences ,one per source, containing a sequence of eve nts (ordered by time-stamp) associated with that source, and classical SPM problem is to find patterns of events that have a temporal order that occur in a si gnificant number of source sequences.
Uncertainty in SPM can occur in three di fferent places: the source, the event and the time-stamp may all be uncertain (in contrast, in FIM, only the event can be uncertain). In a companion paper [16] the first two kinds of uncertainty in SPM were formalized as source-level uncertainty (SLU) and event-level un-certainty (ELU) , which we now summarize.

In SLU, the  X  X ource X  attribute of each tuple is uncertain: each tuple contains a probability distribution over possible sources ( attribute-level uncertainty [19]). As noted in [16], this formulation applies to scenarios such as the ambiguity arising when a customer makes a retail transaction, but the customer is either not identified exactly, or the customer database itself is probabilistic as a result of  X  X eduplication X  or cleaning [11]. In ELU, the source of the tuple is certain, but the events are uncertain. For example, the PEEX system [13] aggregates unreliable observations of employees using RFID antennae at fixed locations into uncertain higher-level events su ch as  X  X ith probability 0.4, at time 103, Alice and Bob had a meeting in room 435 X . Here, the source (Room 435) is deterministic, but the event ( { Alice, Bob } ) only occurred with probability 0.4.
Furthermore, in [16] two measu res of  X  X requentness X , namely expected support and probabilistic frequentness , used for FIM in probabilistic databases [5,7], were adapted to SPM, and the four possible combinations of models and measures were studied from a computational complexity viewpoint. This paper is focussed on efficient algorithms for the SPM problem in SLU probabilistic databases, under the expected support measure, and the contributions are as follows: 1. We give a dynamic-programming (DP) algorithm to determine efficiently the 2. We give depth-first and breadth-first methods to find all frequent sequences 3. To speed up the computation, we give subroutines for: 4. We empirically evaluate our algorithm s, demonstrating their efficiency and Significance of Results. The source support probability algorithm ((1) above) shows that in probabilistic databases, FIM and SPM are very different  X  there is no need to use DP for FIM under the expected support measure [2,6,7].
Although the proof that source support probability allows the computation of the expected support of a sequence in an SLU database is simple, it is unexpected, since in SLU databases, there are dependencies between different sources  X  in any possible world, a given event can only belong to one source. In contrast, determining if a given sequence is probabilistically frequent in an SLU event database is #P-complete because o f the dependencies between sources [16].
Also, as noted in [16], (1) can be used to determine if a sequence is frequent in an ELU database using both expected support and probabilistic frequentness. This implies efficient algorithms for enumerating frequent sequences under both frequentness criteria for ELU databases, and by using the framework of [10], we can also find maximal frequent sequential patterns in ELU databases.
 The breadth-first and depth-first algorithms (2) have a high-level similarity to GSP [18] and SPADE/SPAM [23,4], but checking the extent to which a sequence is supported by a source requires an expensive DP computation, and major modifications are needed to achieve good performance. It is unclear how to use either the projected database idea of Pr efixSpan [17], or bitmaps as in SPAM); we instead use the ideas ((3) above) of incremental computation, and probabilistic pruning. Although there is a high-level similiarity between this pruning and a technique of [6] for FIM in probabilistic databases, the SPM problem is more complex, and our pruning rule is harder to obtain.
 Related Work. Classical SPM has been studied extensively [18,23,17,4]. Modelling uncertain data as probabilistic databases [19,1] has led to several rank-ing/mining problems being studied in this context. The top-k problem (a rank-ing problem) has been studied intensively ( see [12,24,8] and ref erences therein). FIM in probabilistic databases was studied under the expected support measure in [2,7,6] and under the probabilistic frequentness measure in [5]. To the best of our knowledge, apart from [16], the SPM problem in probabilistic databases has not been studied. Uncertainty in the t ime-stamp attribut e was considered in [20]  X  we do not consider time to be uncertain. Also [22] studies SPM in  X  X oisy X  sequences, but the model proposed ther e is very different to ours and does not fit in the probabilistic database framework. Classical SPM [18,3]. Let I = { i 1 ,i 2 ,...,i q } be a set of items and S = { 1 ,...,m } be a set of sources .An event e  X  X  is a collection of items. A database D = r 1 ,r 2 ,...,r n is an ordered list of records such that each r i  X  D is of the are ordered by this time-stamp), e i is an event and  X  i is a source.
A sequence s = s 1 ,s 2 ,...,s a is an ordered list of events. The events s i in the sequence are called its elements .The length of a sequence s is the total number of Let s = s 1 ,s 2 ,...,s q and t = t 1 ,t 2 ,...,t r be two sequences. We say that s is a such that s k  X  t i j ,for k =1 ,...,q .The source sequence corresponding to a s and source i ,let X i ( s, D ) be an indicator variable, whose value is 1 if s is a subsequence of the source sequence for source i , and 0 otherwise. For any sequence s , define its support in D , denoted Sup ( s, D )= m i =1 X i ( s, D ). The objective is to find all sequences s such that Sup ( s, D )  X   X m for some user-defined threshold 0  X   X   X  1.
 Probabilistic Databases. We define an SLU probabilistic database D p to be an ordered list r 1 ,...,r n of records of the form ( eid ,e, W )where eid is an event-id, e is an event and W is a probability distribution over S ; the list is ordered by eid . The distribution W contains pairs of the form (  X , c ), where  X   X  X  and 0 &lt;c  X  1 is the confidence that the event e is associated with source  X  and (  X ,c )  X  W c = 1. An example can be found in Table 1(L).
 The possible worlds semantics of D p is as follows. A possible world D  X  of D p is generated by taking each event e i in turn, and assigning it to one of the possible sources  X  i  X  W i . Thus every record r i =( eid i ,e i ,W i )  X  D p takes the form r i =( eid i ,e i , X  i ), for some  X  i  X  X  in D  X  . By enumerating all such possible combinations, we get the complete set of possible worlds. We assume that the distributions associ ated with each record r i in D p are stochastically independent; example, a possible world D  X  for the database of Table 1 can be generated by As every possible world is a (deterministic) database, concepts like the support of a sequence in a possible world are well-defined. The definition of the expected support of a sequence s in D p follows naturally: The problem we consider is: Since there are potentially an exponential number of possible worlds, it is infea-sible to compute ES ( s, D p ) directly using Eq. 1; next we show how to do this computation more efficiently using linearity of expectation and DP. p-sequences. A p-sequence is analogous to a source sequence in classical SPM, as ( a, d :0 . 4)( a, b :0 . 2). An SLU database D p can be viewed as a collection of p-of those events in D p that have non-zero confidence of being assigned to source i , ordered by eid , together with the associated confidence (see Table 1(R)). How-ever, the p-sequences correspo nding to different sources are not independent, as illustrated in Table 1(R). Thus, one may view an SLU event database as a collection of p-sequences with dependencies in the form of x-tuples [8]. Never-theless, we show that we can still process the p-sequences independently for the purposes of expected suppport computation: where E denotes the expected value of a random variable. Since X i is a 0-1 variable, E[ X i ( s, D p )] = Pr[ s D p i ], and we calculate the right-hand quantity, which we refer to as the source support probability . This cannot be done naively: which ( a )( a, b ) ... ( a )( a, b ) % &amp;' ( Computing the Source Support Probability. Given a p-sequence D p i = matrix A i,s [0 ..q ][0 ..r ] (we omit the subscripts on A when the source and sequence are clear from the context). For 1  X  k  X  q and 1  X   X  r , A [ k, ] will contain Pr[ s 1 ,...,s k ( e 1 ,c 1 ) ,..., ( e ,c ) ], so A [ q, r ] the desired value of Pr[ s D i ]. We set A [0 , ] = 1 for all ,0 compute the other values row-by-row. For 1  X  k  X  q and 1  X   X  r , define: The interpretation of Eq. 3 is that c  X  k is the probability that e allows the element s k to be matched in source i ;thisis0if s k probability that e is associated with source i . Now we use the equation: Table 2 shows the computation of the source support probability of an exam-ple sequence s =( a )( b )forsource X in the probabilistic database of Table 1. Similarly, we can compute Pr[ s D p Y ]=0 . 08 and Pr[ s D p Z ]=0 . 35, so the ex-the same value obtained by direct application of Eq 1.

The reason Eq. 4 is correct is that if s k  X  e then the probability that s 1 ,...,s k e 1 ,...,e is the same as the probability that s 1 ,...,s k e 1 ,...,e  X  1 (note that if s k erwise, c  X  k = c , and we have to consider two disjoint sets of possible worlds: those where e is not associated with source i (the first term in Eq. 4) and those where it is (the second term in Eq. 4). In summary: Lemma 1. Given a p-sequence D p i and a sequence s , by applying Eq. 4 repeat-edly, we correctly compute Pr[ s D p i ] . We now describe three optimized sub-routines for computing all frequent 1-sequences, for incremental support computation, and for probabilistic pruning. Fast L 1 Computation. Given a 1-sequence s = { x } , a simple closed-form expression for Pr[ s D p i ]is1  X  r =1 (1  X  c  X  1 ). It is easy to verify by induction (1 compute ES ( s, D p )for all 1-sequences s in just one (linear-time) pass through D p . Initialize two arrays F and G ,eachofsize q = |I| , to zero and consider (1  X  c G [ x ]:= G [ x ]+ F [ x ]andreset F [ x ] to zero (we use a linked list to keep track of which entries of F are non-zero for a given source). At the end, for any item x  X  X  , G [ x ]= ES ( x ,D p ).
 Incremental Support Computation. Let s and t be two sequences of length j and j + 1 respectively. Say that t is an S-extension of s if t = s  X { x } for some item x ,where  X  denotes concatenation (i.e. we obtain t by appending a single item as a new element to s ). We say that t is an I-extension of s if s = s 1 ,...,s q and t = s 1 ,...,s q  X  X  x } for some x  X  s q ,and x is lexicographically not less than any item in s q (i.e. we obtain t by adding a new item to the last element of s ). and ( a )( b, c, d ) respectively. Similar to classi cal SPM, we generate candidate sequences t that are either S-or I-extension s of existing frequent sequences s ,and compute ES ( t, D p ) by computing Pr[ t D p i ] for all sources i . While computing Pr[ t D p i ]forsource i , we would like to exploit the similarity between s and t to compute Pr[ t D p i ] more rapidly.

Let i be a source, D p i = ( e 1 ,c 1 ) ,..., ( e r ,c r ) ,and s = s 1 ,...,s q be any sequence. Now let A i,s be the ( q +1)  X  ( r + 1) DP matrix used to compute for =0 ,...,r . We now show that if t is an extension of s , then we can quickly compute B i,t from B i,s , and thereby obtain Pr[ t D p i ]= B i,t [ r ]: Lemma 2. Let s and t be sequences such that t is an extension of s ,andlet i be a source whose p-sequence has r elements in it. Then, given B i,s and D p i ,we can compute B i,t in O ( r ) time.
 Proof. We only discuss the case where t is an I-extension of s , i.e. t = s 1 ,...,s q  X  { x } for some x  X  s and t are pairwise equal, the first q  X  1rowsof A i,s and A i,t are also equal. The ( q  X  1)-st row of A B i,s ,the q -th row of A i,s .If t q = s q and we can move on to the next value of .If t q  X  e ,then s q  X  e and so: Since we know B i,s [ ]= A i,s [ q, ], B i,s [  X  1] = A i,s [ q,  X  1] and c ,wecan compute A i,s [ q  X  1 ,  X  1]. But this value is equal to A i,t [ q  X  1 ,  X  1], which is the value from the ( q  X  1)-st row of A i,t that we need to compute A i,t [ q, ]. Specifically, we compute: if t q  X  e (otherwise B i,t [ ]= B i,t [  X  1]). The (easier) case of S-extensions and an example illustrating incremental computation can be found in [15]. Probabilistic Pruning. We now describe a technique that allows us to prune non-frequent sequences s without fully computing ES ( s, D p ). For each source i , we obtain an upper bound on Pr[ s D p i ] and add up all the upper bounds; if the sum is below the threshold, s can be pruned. We first show (proof in [15]): Lemma 3. Let s = s 1 ,...,s q be a sequence, and let D p i be a p-sequence. Then: We now indicate how Lemma 3 is used. Suppose, for example, that we have a candidate sequence s =( a )( b, c )( a ), and a source X . By Lemma 3:
Pr[( a )( b, c )( a ) D p X ]  X  Pr[( a )( b, c ) D p X ]  X  Pr[( a ) D p X ] Note that the quantities on the RHS are computed for each source by the fast L 1 computation, and can be stored in a small data structure. However, the last line is the least accurate upper bound bound: if Pr[( a )( b, c ) D p X ] is available when pruning, an tighter bound is Pr[( a )( b, c ) D p X ]  X  Pr[( a ) D p X ]. We now describe two candidate generation methods for enumerating all frequent sequences, one each based on breadth-first and depth-first exploration of the sequence lattice, which are similar to G SP [18,3] and SPAM [4] respectively. We first note that an  X  X priori X  property holds in our setting: Lemma 4. Given two sequences s and t , and a probabilistic database D p ,if s is a subsequence of t ,then ERS ( s, D p )  X  ERS ( t, D p ) .
 Proof. In Eq. 1 note that for all D  X   X  PW ( D p ), Sup ( s, D  X  )  X  Sup ( t, D  X  ). Breadth-First Exploration. An overview of our BFS approach is in Fig. 1(L). We now describe some details. Each e xecution of lines (6)-(10) is called a phase . Line 2 is done using the fast L 1 computation (see Section 4). Line 4 is done as in [18,3]: two sequences s and s in L j are joined iff deleting the first item in s and the last item in s results in the same sequence, and the result t comprises s extended with the last item in s . This item is added the way it was in s i.e. either a separate element ( t is an S-extension of s ) or to the last element of s ( t is an I-extension of s ). We apply apriori pruning to the set of candidates in the ( j + 1)-st phase, C j +1 , and probabilistic pruning can additionally be applied to C j +1 (note that for C 2 , probabilistic pruning is the only possibility).
In Lines 6-7, the loop iterates over all sources, and for the i -th source, first consider only those sequences from C j +1 that could potentially be supported by source i , N i,j +1 ,( narrowing ). For the purpose of narrowing, we put all the sequences in C j +1 in a hashtree , similar to [18]. A candidate sequence t  X  C j +1 is stored in the hashtree by hashing on each item in t upto the j -th item, and the leaf node contains the ( j +1)-st item. In the ( j + 1)-st phase, when considering source i , we recursively traverse the hashtree by hashing on every item in L i, 1 until we have traversed all the leaf nodes, thus obtaining N i,j +1 for source i .
Given N i,j +1 we compute the support of t in source i as follows. Consider s = s 1 ,...,s q and t = t 1 ,...,t r be two sequences, then if s and t have a common prefix, i.e. for k =1 , 2 ,...,z , s k = t k , then we start the computation of Pr[ t D p i ]from t z +1 . Observe that our narrowing method naturally tends to place sequences with common prefix es in consecutive positions of N i,j +1 . Depth-First Exploration. An overview of our depth-first approach is in Fig. 1 (R) [23,4]. We first compute the set of frequent 1-sequences, L 1 (Line 1) (assume L 1 is in ascending order). We then explore the pattern sub-lattice as follows.
Consider a call of TraverseDFS( s ), where s is some k -sequence. We first check that all lexicographically smaller k -subsequences of t are frequent, and reject t as infrequent if this test fails (Line 7). We can then apply probabilistic pruning to t ,andif t is still not pruned we compute its support (Line 8). If at any stage t is found to be infrequent, we do not consider x , the item used to extend s to t , as a possible alternative in the recursive tree under s (as in [4]). Observe that for sequences s and t ,where t is an S-or I-extension of s ,ifPr[ s D p i ]=0,then Pr[ t D p i ] = 0. When computing ES ( s, D p ), we keep track of all the sources where Pr[ s D p i ] &gt; 0, denoted by S s .If s is frequent then when computing ES ( t, D p ), we need only to visit the sources in S s .
 Furthermore, with every source i  X  X  s , we assume that the array B i,s (see Section 4) has been saved prior to calling TraverseDFS( s ), allowing us to use incremental computation. By implication, the arrays B i,r for all prefixes r of s are also stored for all sources i  X  X  r ), so in the worst case, each source may store up to k arrays, if s is a k -sequence. The space usage of the DFS traversal is quite modest in practice, however. We report on an experimental evaluation of our algorithms. Our implementations are in C# (Visual Studio .Net), executed on a machine with a 3.2GHz Intel CPU and 3GB RAM running XP (SP3). We begin by describing the datasets used for experiments. Then, we demonstrate the scalability of our algorithms (reported running times are averages from multiple runs), and also evaluate probabilis-tic pruning. In our experiments, we use both real ( Gazelle from Blue Martini [14]) and synthetic (IBM Quest [3]) data sets. We transform these deterministic datasets to probabilistic form in a way similar to [2,5,24,7]; we assign probabili-ties to each event in a source sequence using a uniform distribution over (0 , 1], thus obtaining a collection of p-seque nces. Note that we in fact generate ELU data rather than SLU data: a key benefit of this approach is that it tends to preserve the distributio n of frequent sequences in the deterministic data.
We follow the naming convention of [23]: a dataset named C i D j K means that the average number of events per source is i and the number of sources is j (in thousands). Alphabet size is 2K and all other parameters are set to default.
We study three parameters in our ex periments: the number of sources D , the average number of events per source C , and the threshold  X  .Wetestour algorithms for one of the three parameters by keeping the other two fixed. Ev-idently, all other paramete rs being fixed, increasing D and C , or decreasing  X  , all make an instance harder. We choose our algorithm variants according to two  X  X xes X :  X  Lattice traversal could be done using BFS or DFS.  X  Probabilistic Pruning (P) could be ON or OFF.
 We thus report on four variants in all, for example  X  X FS+P X  represents the variant with BFS lattice traversal and with probabilistic pruning ON. Probabilistic Pruning. To show the effectiveness of probabilistic pruning, we kept statistics on the number of candidates both for BFS and for DFS. Due to space limitations, we report statistics only for the dataset C10D20K here. For more details, see [15]. Table 3 shows that probabilistic pruning is highly effective at eliminating infrequent candidates in phase 2  X  for example, in both BFS and DFS, over 95% of infrequent candidates were eliminated without support computation. However, probabilistic pruning was less effective in BFS compared to DFS in the later phases. This is because we compute a coarser upper bound inBFSthaninDFS,asweonlystore L i, 1 probabilities in BFS, whereas we store both L i, 1 and L i,j probabilities in DFS. We therefore, turn probabilistic pruning OFF after Phase 2 in BFS in our experiments. If we could also store L i,j probabilities in BFS, a more refined upper bound could be attained (as mentioned after Lemma 3 and shown in (Section 6) [15]).

In Fig. 2, we show the effect of probabilistic pruning on overall running time as  X  decreases, for both synthetic ( C10D10K ) and real (Gazelle) datasets. It can be seen that pruning is effective particularly for low  X  , for both datasets. Scalability Testing. We test the scalability of our algorithms by fixing C =10 and  X  = 1%, for increasing values of D (Fig. 3(L)), and by fixing D =10 K and  X  = 25%, for increasing values of C (Fig. 3(R)). We observe that all our algorithms scale essentially linearly in both sets of experiments. We have considered the problem of finding all frequent sequences in SLU databases. This is a first study on efficient algorithms for this problem, and naturally a number of open directions remain e.g. expl oring further the notion of  X  X nterest-ingness X . In this paper, we have used t he expected support measure which has the advantage that it can be computed efficiently for SLU databases  X  probabilis-tic frequentness [5] is provably intractable for SLU databases [16]. Our approach yields (in principle) efficient algorithms for both measures in ELU databases, and comparing both measures in terms of computational cost versus solution quality is an interesting future direction. A number of longer-term challenges remain, including creating a data generator that gives an  X  X nteresting X  SLU database and considering more general models of uncertainty (e.g. it is not clear that the assumption of independence between successive uncertain events is justified).
