 Many retrieval functions are using the length-normalization to penalize the term fre-quencies of long-length documents, since documents are diversely distributed in lengths and topics [1-4]. The pivoted length normalization has been the most popular normalization technique which was applied to traditional retrieval models such as vector space model and probabilistic retrieval model [1]. The length-normalization is inherent in the recent language modeling approaches as well, within its smoothing methods. However, the length-normalization problem is still a non-trivial issue. For example, term frequencies in documents having redundant information will be high in documents may not be high in proportional to the lengths. 
The passage-based document retrieval (we call it passage retrieval) has been re-garded as alternative method to resolve the length-normalization problem. The pas-document, and sets the similarity between the document and the query by the similar-ity between the query-relevant passage and the query [5-8]. Different from docu-the passage retrieval has been the promising technique to appropriately handle the length-normalization problem. issue, several types of passages have been proposed; a semantic passage using a para-graph or a session, a window passage which consists of subsequent words with fixed-length, and an arbitrary passage which consists of subsequent words with fixed length the more effective [7]. From extensive experiments on the arbitrary passage, Kaszkiel and Zobel showed that as passage is less restricted, the passage retrieval becomes more effective [7]. Their result is intuitive, since as the passage is less-restricted, the more query-specific parts in a document can be selected as the best passage. General-performance, while a more query-independent passage provides worse performance. such as using 50, 100, ..., 600. If we drop this restriction, then a more query-specific fectiveness. 
Regarding that, this paper proposes a new type of passage, namely completely-arbitrary passages by eliminating all possible restrictions of passage on both lengths and starting positions which still has remained in original arbitrary passage. Continu-ity of words comprising a passage is the only remaining restriction in the completely-that a more query-specific passage can be selected as the best passage, resulting in a better retrieval effectiveness. 
As another advantage, completely-arbitrary passages can well-support the prox-imity between query terms which has been a useful feature for a better retrieval per-formance. Note that the length of passage can play a role for supporting the proximity passage). Based on this passage length, original arbitrary passage cannot well-support the proximity feature, since original arbitrary passage has a lower bound on its length such as at least 50 words. On completely-arbitrary passage , extremely short passages including two or three terms only are possible, thus passages of query terms alone can be selected as the best passage. For example, suppose that a query consisting of  X  X earch engine X , and the following two different documents (Tao X  X  example [11]). D 1 :  X ............... search engine ....................... X  D 2 :  X  X ngine ....................... search X . 
Normally, we can agree that D 1 is more relevant than D 2 , since D 1 represents more exactly the concept of the given query. When two query terms in a D 2 are less-distant than 50 words, Kaszkiel X  X  arbitrary passage cannot discriminate D 1 and D 2 at passage-retrieval level. On the other hand, completely-arbitrary passage can discriminate them, since it can construct the best passage of the length of 2. Tao and Zhai X  X  method (simply, Tao X  X  method) [11]. However, Tao X  X  method heuristi-cally models the proximity, with an unprincipled way by using a function which cannot does not consider the importance of each query term, but simply representing a distance including common terms are processed, the distance cannot be reasonably estimated, so Tao X  X  method, the passage retrieval has an advantage that the importance of query terms can be considered, resulting in robustness even for verbose queries. 
When we use completely-arbitrary passages, an efficiency problem is raised since the number of completely-arbitrary passages is exponentially increasing according to algorithm by which the ranking problem of completely-arbitrary passages can be efficiently implemented. We will provide a simple proof of equivalence between cover-set ranking and finding the best passage among the set of completely-arbitrary passages. 
Although the proposed passage retrieval can be utilized to any retrieval models, we result is encouraging, showing that the proposed passage retrieval using completely-arbitrary passage significantly improves the baseline, as well as that using the previ-ous arbitrary passage, in the most of test collections. guage modeling approaches. To our best knowledge, Liu X  X  work is a unique study for the passage retrieval in the language modeling approaches [8]. However, Liu X  X  work worse performances over the baseline in some test collections. 2.1 Passage Retrieval Based on Best-Passage Evidence In this paper, we assume that the passage-level evidence is the score of the best pas-cally well-performed. Here, the best passage indicates the passage that maximizes its relevance to query. Let Score( Q , P ) be the similarity function between passage P (or document P ) and query Q . The passage-level evidence using the best passage is for-mulated as follows: use the interpolation of the document-level evidence -Score( Q , D ) -and the passage-level evidence -Score passg ( Q , D ) -as follows: evidence on the final similarity score. When  X  is 1, the final document score is com-pletely substituted to the passage-level evidence only. We call it pure passage re-trieval. Score( Q , D ) is differently defined according to the retrieval model. 
Of Eq. (1), note that SP( D ) is dependent to types of passage. As for previous sev-semantic passage [9], for window passage [6], and for arbitrary passage [7], respec-tively. SP SEM, SP WIN and SP VAR are defined as follows. 
SP SEM ( D ): It consists of paragraphs, sessions, and automatically segmented seman-tic passages in D . overlapped window and a non-overlapped window . In the overlapped window, over-lapping among passages is allowed. For example, Callan used the overlapped window by overlapping N/2 sub-window in original N -length window to the previous and the next passage [6]. The non-overlapped wi ndow does not make any overlapping be-tween passages. Note that the final passage in the end of document may have a shorter length than N for either a non-overlapped passage or an overlapped passage. An over-lapped window includes a specialized type of passage  X  a completely-overlapped window which allows overlapping N -1 sub-window to previous and next passages, thus all passages have the same length of N , without having a different length of the passage at the end of document. This completely-overlapped passage is called fixed-will refer to window passage as completely-overlapped passage. SP
WIN(Ni) ( D ). Kaszkiel called it variable length arbitrary passage [7]. His experimental results of SP VAR(N1,...Nk) (D) showed the best performance over the other methods. Now, completely-arbitrary passage, the proposed type of passage, is denoted as SP COMPLETE ( D ). Formally, SP COMPLETE ( D ) is defined as follows. special type of arbitrary passage. 
Although completely-arbitrary passage is conceptually simple, the number of pos-length of document. Therefore, it may be intractable to perform the passage retrieval based on completely-arbitrary passages, since we cannot pre-index all possible arbi-time. to equivalently find the best passage by comparing cover sets only. Whenever the retrieval function produces the score of docum ent inversely according to the length of the document, we can always use the cover-set ranking algorithm. 2.2 Cover Set Ranking A cover is defined as follows: Definition 1 ( Cover ). A cover is a special passage such that all terms at boundary of passage are query terms. 
For example, let us consider the following document  X  abbbcccbbddaaa  X , where we ddaaa are not covers, while cccbbdd , cbbdd , c , d , cc , dd are. Let SC( D ) be a set of all covers in D . Then, the following equation indicates that the best passage is a cover. Now, what is the class of retrieval functions which satisfies the above equation? For this question, we define the new class of retrieval function by the length-normalized scoring function . Definition 2 ( Length-normalized scoring function ): Let c( w , D ) be the frequency of term w in document D , and | D | be the length of document D . Let us suppose that term we call Score( Q , D ) as a length-normalized scoring function. We assume the following reasonable statement for the best passage. Assumption 1 ( Minimum requirement of query te rm occurrence in the best pas-sage ). The best passage should contain at least one query term. Now, we can simply prove the following theorem. Theorem 1. If Score( Q , D ) is a length-normalized scoring function, then the best pas-sage is cover. boundary of passage, at least one term is not a query term. According to Assumption 1, this best passage contains a query term. We can construct a new passage by de-Then, the length of this new passage will be shorter than that of the best passage. But, definition of the best passage, since the best passage should have the maximum score. 
Fortunately, most of the retrieval methods such as pivoted normalized vector space, model, probabilistic retrieval model, and language modeling approaches be-most cases. 
From Theorem 1, the problem of finding the best passage among completely-arbitrary passages is equivalently converted to the ranking of covers, namely cover-set ranking . When m is the number of occurrences of query terms in documents i.e. m = For practical implementation of cover-set ranking, we should pre-index term position from Clarke X  X  cover [10]. For a given subset T of query terms, Clarke X  X  cover is de-fined as a minimal passage that contains all query terms in T , but does not contain a shorter passage contain T [10]. on the language modeling approach. The model-dependent part is the scoring function -Score( Q , D ) for document D , and Score( Q , P ) for passage P . In the language model-query of passage P . Thus, the same smoothing techniques as document models are utilized to estimate passage models. From now on, we will briefly review two popular smoothing techniques [4]. In this section, we assume that a document also becomes a document model from MLE (Maximum Likelihood Estimation) and the collection model as follows: where 
Dirichlet-prior smoothing enlarges the original document sample by adding a pseudo document of length  X  , and then uses the enlarged sample to estimate the smoothed model. Here, the pseudo document has the term frequency, which is in proportional to the term probability of the collection model. 
We can easily show that the Score( Q , D ) using two smoothing methods is a length-normalized scoring function. The following presents the brief proof that Dirichlet-prior smoothing method is a length-normalized scoring function. Proof. Dirichlet-prior smoothing induces the length-normalized scoring function. The retrieval scoring function of Dirichlet-prior smoothing is written by. c( q , D 1 ) is c( w , D 2 ), and | D 1 | &lt; | D 2 |, then a length-normalized scoring function. Similarly, we can show that Jelinek-Mercer 4.1 Experimental Setting For evaluation, we used six TREC test collections. Table 1 summarizes the basic information of each test collection. TREC4-AP is the sub-collection of Associated Press in disk 1 and disk 2 for TREC4. In columns, #Doc , avglen , #Terms , #Q , and #R are the number of documents, the average length of documents, the number of terms, and the number of topics, and the number of relevant documents, respectively. 
The standard method was applied to extract index terms; We first separated words based on space character, eliminated stopword s, and then applied Porter X  X  stemming. For all experiments, we used Dirichlet-prior for document model due to its superiority over Jelinek-Mercer smoothing. For passage models, Jelinek-Mercer and Dirichlet-prior smoothings were applied, namely PJM and PDM, respectively. Similarly, for document model, we call JM for Jelinek-Mercer smoothing and DM for Dirichlet-prior smoothing. Interpolation methods using passage-level evidence are denoted by DM+PJM when DM and PJM are combined, and by DM+PDM when DM and PDM are combined. 
As for retrieval evaluation, we use MAP (Mean Average Precision), Pr@5 (Preci-sion at 5 documents), and Pr@10 (Precision at 10 documents). 4.2 Passage Retrieval Using DM+PDM, DM+PJM First, we evaluated DM+PDM based on PDM using different  X  s (among 0.5, 1, 5, 10, field. Table 2 shows retrieval performances of DM+PDM when the best interpolation parameter  X  is used, compared to DM 1 . DM indicates the baseline using document-level evidence based on Dirichlet-prior smoothing for estimating document model, in different values between 100 and 30000). Bold-face numbers indicate the best per-formance in each test collection. To check whether or not the proposed method (DM+PDM) significantly improves the baseline (DM), we performed the Wilcoxon sign ranked test to examine at 95% and 99% confidence levels. We attached  X  and  X  to the performance number of each cell in the table when the test passes at 95% and 99% confidence level, respectively. As shown in Table 2, DM+PDM clearly shows the better performance than DM on many parameters as well as the best parameter. We can see that the best performances for each test collection are obtained for extremely sensitivity of  X  is not much serious. 
In the second experiment, we evaluated DM+PJM by using PJM across different smoothing parameters  X  between 0.1 and 0.9. Table 3 shows the retrieval performance of DM+PJM when the best interpolation parameter  X  for each  X  is used. As shown in Table 3, the best  X  value is small, except for TREC4-AP. This exception is due to the characteristics of the description field of query topic used for TREC4-AP. 
The performance improvement is similar to DM+PDM. At the best, there is no se-rious difference between DM+PDM and DM+PJM. In TREC4-AP, the retrieval per-formance is highly sensitive on smoothing parameter for DM+PDM and DM+PJM, since the description type of queries requires the smoothing which mainly plays a query modeling role. 4.3 Pure Passage Retrieval Versus Interpolated Passage Retrieval In this experiment, we evaluated the pure passage retrieval (  X  is fixed to 1 in Eq. (2)) by comparing it to the previous result of the interpolation method (DM+PDM or DM+PJM). We used the same queries as section 4.2. Table 4 shows the retrieval performance of the pure passage retrieval (i.e. PDM) among different smoothing parameters. 
We can see from Table 4 that pure passage retrieval based on PDM does not im-prove full document retrieval using DM. Although there is some improvement of pure passage retrieval on WT2G and WT10G, this result is not statistically significant. For making a good retrieval function into document models and passage models by dif-ferentiating smoothing parameters. However, the pure method should reflect all rele-vant features into a single smoothing parameter only. In the pure method, smoothing parameter of the passage model is moved towards the best parameter of document model, such that it also plays the roles of document model. 4.4 Comparison with Kaszkiel X  X  Variable-Length Arbitrary Passages and Tao X  X  We compared different passage retrievals using completely-arbitrary passage and 12 different lengths of passages from 50 to 600. According to our notation, Kaszkiel X  X  we allowed the length of passage to the full length of the document. This slight modi-fication is trivial, because it does not lose the original concept of arbitrary passage. 
We formulate Kaszkiel X  X  arbitrary passages to SP VAR2(U) ( D ), U -dependent set of increment unit of passage length. In our notation, Kaszkiel X  X  arbitrary passages corre-(SP COMPLETE ( D )) corresponds to SP VAR2(50) ( D ) using U of 1. Note that parameter U in SP words, as U is changed from 50 to 1, SP VAR2(U) ( D ) becomes more close to completely-arbitrary passages from si mple arbitrary passage. Table 5 and Table 6 show results of passage retrievals (DM+PJM) for different scription field), respectively. For smoothing parameter of PJM, we used 0.1 for key-word queries, and 0.99 for verbose queries. Exceptionally, we used 0.9 for keyword queries in WT10G-2, and 0.9 for TREC4-AP, in order to obtain a more improved performance. 
As shown in tables, Kaszkiel X  X  arbitrary passages (when U is 50), statistically sig-nificant improvement is found in only WT2G test collection. Overall, as U is smaller, the performances become better, and show more frequently statistically significant improvements. 
As mentioned in introduction, the superiority of the completely-arbitrary passage to Kaszkiel X  X  arbitrary passage can be explained by effects of proximity of query terms. We know that this proximity can be well-reflected in the completely-arbitrary kiel X  X  arbitrary passage has the restriction of lengths (i.e. such as at least 50 words), so it cannot fully support effects of proximity. Overall, as U becomes larger, the passage retrieval will more weakly reflect effects of the proximity. From the viewpoint of proximity, the passages with fixed-lengths such as Kaszkiel X  X  one has limitation to obtain a better retrieval performance. 
For further comparison, we re-implemented Tao X  X  work [11], which is a recent work using the proximity. Tao X  X  approach can be described by modifying Eq. (2) as follows [11]. formed several different runs using various parameters  X  from 0.1 to 0.9, and  X  from proximity-based result, comparing with two passage retrievals using Kaszkiel X  X  arbi-trary passage ( U = 50) and our completely-arbitrary passage ( U = 1), respectively. 
For Tao X  X  proximity method and Kaszkiel X  X  method (arbitrary passage), statistical baseline. We can see from Table 7 that Tao X  X  proximity-based method does show significant improvement over the baseline (DM) only in TREC4-AP, TREC8, and WT10G, showing less-significant effectiveness in other test collections. In terms of effectiveness, Tao X  X  proximity method is better than Kaszkiel X  X  method. Remarkably, the proposed approach showed the significant improvement for all test collections. From the viewpoint of improvement over DM, the proposed approach is clearly better than these two methods. This paper proposed a completely-arbitrary passage as a new type of passage, pre-new type, and formulated the passage retrieval in the context of language modeling approaches. Experimental results showed that the proposed passage retrieval consis-tently shows the improvement in most standard test collections. From comparison of polation method is better than the pure version. The smoothing role in passage lan-guage models tends to be similar to the role of document language models, differenti-ating the best smoothing parameters for keyword queries and verbose queries. In addition, we showed that our passage retrieval using completely-arbitrary passage is better than those using Kaszkiel X  X  arbitrary passage [7], as well as Tao X  X  method [11]. 
The strategy which this work adopts is to use the best passage only. However, the best passage cannot fully cover all contents in documents, since query-relevant con-tents may separately appear by multiple-passages in documents, not by single best passage. It is especially critical to long-length queries, which consists of several num-ber of query terms, since this type of query causes more possibility of such separation. In this regard, one challenging research issue would be to develop a new passage retrieval method using multiple-passages for reliable retrieval performance, and to examine its effectiveness. This work was supported by the Korea Science and Engineering Foundation (KOSEF) through the Advanced Information Technology Research Center (AITrc), also in part by the BK 21 Project and MIC &amp; IITA through IT Leading R&amp;D Support Project in 2007. 
