 Recent work has seen widespread use of syn-chronous probabilistic grammars in statistical ma-chine translation (SMT). The decoding problem for a broad range of these systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008)) corresponds to the intersection of a (weighted) hypergraph with resents a large set of possible translations, and is created by applying a synchronous grammar to the source language string. The language model is then used to rescore the translations in the hypergraph.
Decoding with these models is challenging, largely because of the cost of integrating an n-gram language model into the search process. Exact dy-namic programming algorithms for the problem are well known (Bar-Hillel et al., 1964), but are too ex-decoding for syntax-based SMT has therefore been focused primarily on approximate search methods.
This paper describes an efficient algorithm for ex-act decoding of synchronous grammar models for translation. We avoid the construction of (Bar-Hillel et al., 1964) by using Lagrangian relaxation to de-compose the decoding problem into the following sub-problems: 1. Dynamic programming over the weighted hy-2. Application of an all-pairs shortest path al-Informally, the first decoding algorithm incorporates the weights and hard constraints on translations from the synchronous grammar, while the second decod-ing algorithm is used to integrate language model scores. Lagrange multipliers are used to enforce agreement between the structures produced by the two decoding algorithms.

In this paper we first give background on hyper-graphs and the decoding problem. We then describe our decoding algorithm. The algorithm uses a sub-gradient method to minimize a dual function. The dual corresponds to a particular linear programming (LP) relaxation of the original decoding problem. The method will recover an exact solution, with a certificate of optimality, if the underlying LP relax-ation has an integral solution. In some cases, how-ever, the underlying LP will have a fractional solu-tion, in which case the method will not be exact. The second technical contribution of this paper is to de-scribe a method that iteratively tightens the underly-ing LP relaxation until an exact solution is produced. We do this by gradually introducing constraints to step 1 (dynamic programming over the hypergraph), while still maintaining efficiency.
We report experiments using the tree-to-string model of (Huang and Mi, 2010). Our method gives exact solutions on over 97% of test examples. The method is comparable in speed to state-of-the-art de-coding algorithms; for example, over 70% of the test examples are decoded in 2 seconds or less. We com-pare our method to cube pruning (Chiang, 2007), and find that our method gives improved model scores on a significant number of examples. One consequence of our work is that we give accurate estimates of the number of search errors for cube pruning. A variety of approximate decoding algorithms have been explored for syntax-based translation systems, including cube-pruning (Chiang, 2007; Huang and Chiang, 2007), left-to-right decoding with beam search (Watanabe et al., 2006; Huang and Mi, 2010), and coarse-to-fine methods (Petrov et al., 2008).
Recent work has developed decoding algorithms based on finite state transducers (FSTs). Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al. (2010) show that exact FST decoding is feasible for a specific class of hierarchical gram-mars (shallow-1 grammars). Approximate search methods are used for more complex reordering mod-els or grammars. The FST algorithms are shown to produce higher scoring solutions than cube-pruning on a large proportion of examples.

Lagrangian relaxation is a classical technique in combinatorial optimization (Korte and Vygen, 2008). Lagrange multipliers are used to add lin-ear constraints to an existing problem that can be solved using a combinatorial algorithm; the result-ing dual function is then minimized, for example using subgradient methods. In recent work, dual decomposition  X  X  special case of Lagrangian relax-ation, where the linear constraints enforce agree-ment between two or more models X  X as been ap-plied to inference in Markov random fields (Wain-wright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an in-put sentence in the source language, and from this to create a hypergraph (sometimes called a transla-tion forest) that represents the set of possible trans-lations (strings in the target language) and deriva-tions under the grammar. The second step is to integrate an n-gram language model with this hy-pergraph. For example, in the system of (Chiang, 2005), the hypergraph is created as follows: first, the source side of the synchronous grammar is used to create a parse forest over the source language string. Second, transduction operations derived from syn-chronous rules in the grammar are used to create the target-language hypergraph. Chiang X  X  method uses a synchronous context-free grammar, but the hyper-graph formalism is applicable to a broad range of other grammatical formalisms, for example depen-dency grammars (e.g., (Shen et al., 2008)).

A hypergraph is a pair ( V, E ) where V = hyperedges. A single distinguished vertex is taken as the root of the hypergraph; without loss of gener-ality we take this vertex to be v = 1 . Each hyper-edge e  X  E is a tuple hh v v 0  X  V vertex v ordered sequence h v the tail of the edge; in addition, we sometimes refer to v 1 , v 2 , . . . v k as the children in the edge. The num-ber of children k may vary across different edges, but k  X  1 for all edges (i.e., each edge has at least one child). We will use h ( e ) to refer to the head of an edge e , and t ( e ) to refer to the tail.
We will assume that the hypergraph is acyclic: in-tuitively this will mean that no derivation (as defined below) contains the same vertex more than once (see (Martin et al., 1990) for a formal definition).
Each vertex v  X  V is either a non-terminal in the hypergraph, or a leaf . The set of non-terminals is V N = { v  X  V :  X  e  X  E such that h ( e ) = v } Conversely, the set of leaves is defined as
V L = { v  X  V : 6  X  e  X  E such that h ( e ) = v }
Finally, we assume that each v  X  V has a label l ( v ) . The labels for leaves will be words , and will be important in defining strings and language model scores for those strings. The labels for non-terminal We now turn to derivations. Define an index set I = V  X  E . A derivation is represented by a vector y = { y r : r  X  I} where y v = 1 if vertex v is used in the derivation, y edge e is used in the derivation, y Thus y is a vector in { 0 , 1 } |I| . A valid derivation satisfies the following constraints:  X  y 1 = 1 (the root must be in the derivation). We use Y to refer to the set of valid derivations. The set Y is a subset of { 0 , 1 } |I| (not all members of
Each derivation y in the hypergraph will imply an ordered sequence of leaves v refer to this sequence. The sentence associated with the derivation is then l ( v
In a weighted hypergraph problem, we assume a parameter vector  X  = {  X  any derivation is f ( y ) =  X  y = P ple bottom-up dynamic programming X  X ssentially the CKY algorithm X  X an be used to find y  X  = arg max y  X  X  f ( y ) under these definitions.
The focus of this paper will be to solve problems involving the integration of a k  X  X h order language model with a hypergraph. In these problems, the score for a derivation is modified to be where v parameters score n-grams of length k . These parameters are typically defined by a language model, for example with k = 3 we would have  X  ( v i  X  2 , v i  X  1 , v i ) = log p ( l ( v i ) | l ( v The problem is then to find y  X  = arg max under this definition.

Throughout this paper we make the following as-sumption when using a bigram language model: Assumption 3.1 ( Bigram start/end assump-tion. ) For any derivation y , with leaves v 1 = 2 appear at any other position in the strings s ( y ) for y  X  Y ; (3) l (2) = &lt;s&gt; where &lt;s&gt; is the start symbol in the language model; (4) l (3) = &lt;/s&gt; where &lt;/s&gt; is the end symbol.

This assumption allows us to incorporate lan-guage model terms that depend on the start and end symbols. It also allows a clean solution for boundary We now give a Lagrangian relaxation algorithm for integration of a hypergraph with a bigram language model, in cases where the hypergraph satisfies the following simplifying assumption: Assumption 4.1 ( The strict ordering assumption. ) For any two leaves v and w , it is either the case that: 1) for all derivations y such that v and w are both in the sequence l ( y ) , v precedes w ; or 2) for all derivations y such that v and w are both in l ( y ) , w precedes v .

Thus under this assumption, the relative ordering of any two leaves is fixed. This assumption is overly that does not require this assumption. However de-riving the simple algorithm will be useful in devel-oping intuition, and will lead directly to the algo-rithm for the unrestricted case. 4.1 A Sketch of the Algorithm At a high level, the algorithm is as follows. We in-troduce Lagrange multipliers u ( v ) for all v  X  V with initial values set to zero. The algorithm then involves the following steps: (1) For each leaf v , find the previous leaf w that maximizes the score  X  v =  X  (  X  est scoring derivation using dynamic programming over the original (non-intersected) hypergraph, with leaf nodes having weights  X  the output derivation from step 2 has the same set of bigrams as those from step 1, then we have an exact solution to the problem. Otherwise, the Lagrange multipliers u ( v ) are modified in a way that encour-ages agreement of the two steps, and we return to step 1.

Steps 1 and 2 can be performed efficiently; in par-ticular, we avoid the classical dynamic programming intersection, instead relying on dynamic program-ming over the original, simple hypergraph. 4.2 A Formal Description We now give a formal description of the algorithm. Define B  X  V h v, w i such that there is at least one derivation y with v directly preceding w in s ( y ) . Extend the bit-vector y to include variables y ( v, w ) for h v, w i  X  B where y ( v, w ) = 1 if leaf v is followed by w in s ( y ) , 0 otherwise. We redefine the index set to be I = V  X  E  X  B , and define Y  X  { 0 , 1 } |I| to be the set of all possible derivations. Under assumptions 3.1 and 4.1 above, Y = { y : y satisfies constraints C0 , C1 , C2 } where the constraint definitions are:  X  (C0) The y v and y e variables form a derivation  X  (C1) For all v  X  V L such that v 6 = 2 , y v =  X  (C2) For all v  X  V L such that v 6 = 3 , y v = C1 states that each leaf in a derivation has exactly one in-coming bigram, and that each leaf not in the derivation has 0 incoming bigrams; C2 states that each leaf in a derivation has exactly one out-going bigram, and that each leaf not in the derivation has 0
The score of a derivation is now f ( y ) =  X  y , i.e., f ( y ) = X where  X  ( v, w ) are scores from the language model. Our goal is to compute y  X  = arg max Next, define Y  X  as In this definition we have dropped the C2 con-straints. To incorporate these constraints, we use Lagrangian relaxation, with one Lagrange multiplier u ( v ) for each constraint in C2 . The Lagrangian is L ( u, y ) = f ( y ) + X where  X   X  ( v, w )  X  u ( v ) .

The dual problem is to find min Figure 1 shows a subgradient method for solving this problem. At each point the algorithm finds y Lagrange multipliers from the previous iteration. If y t satisfies the C2 constraints in addition to C0 and C1 , then it is returned as the output from the algo-rithm. Otherwise, the multipliers u ( v ) are updated. Intuitively, these updates encourage the values of y and P updates correspond to subgradient steps.

The main computational step at each iteration is to compute arg max solved, as follows (we again use  X  to refer to the parameter values that incorporate La-grange multipliers):  X  For all v  X  V L , define  X   X  ( v ) =  X  Using dynamic programming, find values for  X  Set y ( v, w ) = 1 iff y ( w ) = 1 and  X   X  ( w ) = v . The critical point here is that through our definition of
Y  X  , which ignores the C2 constraints, we are able to do efficient search as just described. In the first step we compute the highest scoring incoming bi-gram for each leaf v . In the second step we use conventional dynamic programming over the hyper-graph to find an optimal derivation that incorporates y ( v, w ) values. Each iteration of the algorithm runs in O ( | E | + |B| ) time.

There are close connections between Lagrangian relaxation and linear programming relaxations. The most important formal results are: 1) for any value of u, L ( u )  X  f ( y  X  ) (hence the dual value provides an upper bound on the optimal primal value); 2) un-der an appropriate choice of the step sizes  X  t , the subgradient algorithm is guaranteed to converge to the minimum of L ( u ) (i.e., we will minimize the upper bound, making it as tight as possible); 3) if at any point the algorithm in figure 1 finds a y t that satisfies the C2 constraints, then this is guaranteed to be the optimal primal solution.

Unfortunately, this algorithm may fail to produce a good solution for hypergraphs where the strict or-dering constraint does not hold. In this case it is possible to find derivations y that satisfy constraints C0 , C1 , C2 , but which are invalid. As one exam-ple, consider a derivation with s ( y ) = 2 , 4 , 5 , 3 and y (2 , 3) = y (4 , 5) = y (5 , 4) = 1 . The constraints are all satisfied in this case, but the bigram variables are invalid (e.g., they contain a cycle). We now describe our full algorithm, which does not require the strict ordering constraint. In addition, the full algorithm allows a trigram language model. We first give a sketch, and then give a formal definition. 5.1 A Sketch of the Algorithm A crucial idea in the new algorithm is that of paths between leaves in hypergraph derivations. Previously, for each derivation y , we had de-fined s ( y ) = v of leaves in y . In addition, we will define where each p leaves v terminals that are between the two leaves in the tree.
As an example, consider the following derivation (with hyperedges hh 2 , 5 i , 1 i and hh 3 , 4 i , 2 i ):
For this example g ( y ) is h 1  X  , 2  X  X  h 2  X  , 3  X  X  h 2  X  , 5  X  X  h 5  X  X  , 5, h 5  X  X  h 5  X  , 1  X  X  . States of the form h a  X  X  and h a  X  X  where a is a leaf appear in the paths respectively before/after the leaf a . States of the form h a, b i correspond to the steps taken in a top-down, left-to-right, traversal of the tree, where down and up arrows indicate whether a node is be-ing visited for the first or second time (the traversal in this case would be 1 , 2 , 3 , 4 , 2 , 5 , 1 ).
The mapping from a derivation y to a path g ( y ) can be performed using the algorithm in figure 2. For a given derivation y , define E ( y ) = { y : y 1 } , and use E ( y ) as the set of input edges to this algorithm. The output from the algorithm will be a set of states S , and a set of directed edges T , which together fully define the path g ( y ) .

In the simple algorithm, the first step was to predict the previous leaf for each leaf v , under a score that combined a language model score with a Lagrange multiplier score (i.e., compute arg max w  X  ( w, v ) where  X  ( w, v ) =  X  ( w, v ) + u ( w ) ). In this section we describe an algorithm that for each leaf v again predicts the previous leaf, but in addition predicts the full path back to that leaf. For example, rather than making a prediction for leaf 5 that it should be preceded by leaf 4 , we would also predict the path h 4  X  X h 4  X  , 2  X  X  h 2  X  , 5  X  X h 5  X  X  be-tween these two leaves. Lagrange multipliers will be used to enforce consistency between these pre-dictions (both paths and previous words) and a valid derivation. 5.2 A Formal Description We first use the algorithm in figure 2 with the en-tire set of hyperedges, E , as its input. The result is a directed graph ( S, T ) that contains all possible paths for valid derivations in V, E (it also contains additional, ill-formed paths). We then introduce the following definition: Definition 5.1 A trigram path p is p = h v 1 , p 1 , v 2 , p 2 , v 3 i b) p h v path between nodes h v ( S, T ) . We define P to be the set of all trigram paths in ( S, T ) .

The set P of trigram paths plays an analogous role to the set B of bigrams in our previous algorithm.
We use v to the individual components of a path p . In addi-tion, define S form h a, b i (as opposed to the form h c  X  X  or h c  X  X  where c  X  V We now define a new index set, I = V  X  E  X  S
N  X  P p  X  P . If we take Y  X  { 0 , 1 } |I| to be the set of valid derivations, the optimization problem is to find y  X  = arg max f ( y ) = X In particular, we might define  X  and  X  p ( w 3 | w 1 , w 2 ) is a trigram probability.
The set P is large (typically exponential in size): however, we will see that we do not need to represent the y to leverage the underlying structure of a path as a sequence of states.

The set of valid derivations is Y = { y : y satisfies constraints D0  X  D6 } where the constraints are shown in figure 3. D1 simply states that y iff there is exactly one edge e in the derivation such that s  X  S ( e ) . Constraints D2  X  D4 enforce consis-tency between leaves in the trigram paths, and the y values. Constraints D5 and D6 enforce consistency between states seen in the paths, and the y
The Lagrangian relaxation algorithm is then de-rived in a similar way to before. Define We have dropped the D3  X  D6 constraints, but these will be introduced using Lagrange multipliers. The resulting Lagrangian is shown in figure 3, and can be written as L ( y,  X ,  X , u, v ) =  X  y where  X   X  +  X  v +  X  v ,  X  s =  X  s + u s + v s ,  X  p =  X  p  X   X  ( v 2  X  ( v 1 ( p ))  X  P s  X  p
The dual is L (  X ,  X , u, v ) = max y  X  X   X  L ( y,  X ,  X , u, v ) ; figure 4 shows a sub-gradient method that minimizes this dual. The key step in the algorithm at each iteration is to compute arg max y  X  X   X  L ( y,  X ,  X , u, v ) = arg max y  X  X   X   X  y where  X  is defined above. Again, our definition efficiently, as follows: 1. For each v  X  V 2. Find values for the y 3. Set y The first step involves finding the highest scoring in-coming trigram path for each leaf v . This step can be performed efficiently using the Floyd-Warshall all-pairs shortest path algorithm (Floyd, 1962) over the graph ( S, T ) ; the details are given in the appendix. The second step involves simple dynamic program-ming over the hypergraph ( V, E ) (it is simple to in-tegrate the  X  step, the path variables y 5.3 Properties We now describe some important properties of the algorithm:
Efficiency. The main steps of the algorithm are: eration, dynamic programming over the hypergraph ( V, E ) ; 3) at each iteration, all-pairs shortest path al-gorithms over the graph ( S, T ) . Each of these steps is vastly more efficient than computing an exact in-tersection of the hypergraph with a language model.
Exact solutions. By usual guarantees for La-grangian relaxation, if at any point the algorithm re-turns a solution y t that satisfies constraints D3  X  D6 , then y t exactly solves the problem in Eq. 1. Upper bounds. At each point in the algorithm, L (  X  t ,  X  t , u t , v t ) is an upper bound on the score of the optimal primal solution, f ( y  X  ) . Upper bounds can be useful in evaluating the quality of primal so-lutions from either our algorithm or other methods such as cube pruning.

Simplicity of implementation. Construction of the ( S, T ) graph is straightforward. The other steps X  X ypergraph dynamic programming, and all-pairs shortest path X  X re widely known algorithms that are simple to implement. The algorithm that we have described minimizes the dual function L (  X ,  X , u, v ) . By usual results for Lagrangian relaxation (e.g., see (Korte and Vygen, 2008)), L is the dual function for a particular LP re-laxation arising from the definition of Y  X  and the ad-ditional constaints D3  X  D6 . In some cases the LP relaxation has an integral solution, in which case the algorithm will return an optimal solution y t . 7 In other cases, when the LP relaxation has a frac-tional solution, the subgradient algorithm will still converge to the minimum of L , but the primal solu-tions y t will move between a number of solutions.
We now describe a method that incrementally adds hard constraints to the set Y  X  , until the method returns an exact solution. For a given y  X  Y  X  , for any v with y ous two leaves (the trigram ending in v ) from ei-ther the path variables y ables y preceding v in the trigram path p with y v ( p ) = v , and v  X  2 ( v, y ) to be the leaf two posi-tions before v in the trigram path p with y v ( p ) = v . Similarly, define v  X   X  1 ( v, y ) and v  X   X  2 to be the preceding two leaves under the y ables. If the method has not converged, these two trigram definitions may not be consistent. For a con-sistent solution, we require v and v Unfortunately, explicitly enforcing all of these con-straints would require exhaustive dynamic program-ming over the hypergraph using the (Bar-Hillel et al., 1964) method, something we wish to avoid.
Instead, we enforce a weaker set of constraints, which require far less computation. Assume some function  X  : V set of leaves into q different partitions. Then we will add the following constraints to Y  X  : for all v such that y y under this new definition of Y  X  can be performed using the construction of (Bar-Hillel et al., 1964), with q different lexical items (for brevity we omit the details). This is efficient if q is small. 8
The remaining question concerns how to choose a partition  X  that is effective in tightening the relax-ation. To do this we implement the following steps: 1) run the subgradient algorithm until L is close to convergence; 2) then run the subgradient algorithm for m further iterations, keeping track of all pairs of leaf nodes that violate the constraints (i.e., pairs v  X  2 ( v, y ) ing algorithm to find a small partition that places all pairs h a, b i into separate partitions; 4) continue run-ning Lagrangian relaxation, with the new constraints added. We expand  X  at each iteration to take into ac-count new pairs h a, b i that violate the constraints.
In related work, Sontag et al. (2008) describe a method for inference in Markov random fields where additional constraints are chosen to tighten an underlying relaxation. Other relevant work in NLP includes (Tromble and Eisner, 2006; Riedel and Clarke, 2006). Our use of partitions  X  is related to previous work on coarse-to-fine inference for ma-chine translation (Petrov et al., 2008). We report experiments on translation from Chinese to English, using the tree-to-string model described in (Huang and Mi, 2010). We use an identical model, and identical development and test data, to is trained on 1.5M sentence pairs of Chinese-English data; a trigram language model is used. The de-velopment data is the newswire portion of the 2006 NIST MT evaluation test set (616 sentences). The test set is the newswire portion of the 2008 NIST MT evaluation test set (691 sentences).

We ran the full algorithm with the tightening method described in section 6. We ran the method for a limit of 200 iterations, hence some exam-ples may not terminate with an exact solution. Our method gives exact solutions on 598/616 develop-ment set sentences (97.1%), and 675/691 test set sentences (97.7%).

In cases where the method does not converge within 200 iterations, we can return the best primal solution y t found by the algorithm during those it-erations. We can also get an upper bound on the difference f ( y  X  )  X  f ( y t ) using min per bound on f ( y  X  ) . Of the examples that did not converge, the worst example had a bound that was 1.4% of f ( y t ) (more specifically, f ( y t ) was -24.74, and the upper bound on f ( y  X  )  X  f ( y t ) was 0.34).
Figure 5 gives information on decoding time for our method and two other exact decoding methods: integer linear programming (using constraints D0  X  D6 ), and exhaustive dynamic programming using the construction of (Bar-Hillel et al., 1964). Our method is clearly the most efficient, and is compara-ble in speed to state-of-the-art decoding algorithms.
We also compare our method to cube pruning (Chiang, 2007; Huang and Chiang, 2007). We reim-plemented cube pruning in C++, to give a fair com-parison to our method. Cube pruning has a parame-ter, b , dictating the maximum number of items stored at each chart entry. With b = 50 , our decoder finds higher scoring solutions on 50.5% of all exam-ples (349 examples), the cube-pruning method gets a strictly higher score on only 1 example (this was one of the examples that did not converge within 200 it-erations). With b = 500 , our decoder finds better so-lutions on 18.5% of the examples (128 cases), cube-pruning finds a better solution on 3 examples. The median decoding time for our method is 0.79 sec-onds; the median times for cube pruning with b = 50 and b = 500 are 0.06 and 1.2 seconds respectively.
Our results give a very good estimate of the per-centage of search errors for cube pruning. A natural question is how large b must be before exact solu-tions are returned on almost all examples. Even at b = 1000 , we find that our method gives a better solution on 95 test examples (13.7%).

Figure 5 also gives a speed comparison of our method to a linear programming (LP) solver that solves the LP relaxation defined by constraints D0  X  D6 . We still see speed-ups, in spite of the fact that our method is solving a harder problem (it pro-vides integral solutions). The Lagrangian relaxation method, when run without the tightening method of section 6, is solving a dual of the problem be-ing solved by the LP solver. Hence we can mea-sure how often the tightening procedure is abso-lutely necessary, by seeing how often the LP solver provides a fractional solution. We find that this is the case on 54.0% of the test examples: the tighten-ing procedure is clearly important. Inspection of the tightening procedure shows that the number of par-titions required (the parameter q ) is generally quite small: 59% of examples that require tightening re-quire q  X  6 ; 97.2% require q  X  10 . We have described a Lagrangian relaxation algo-rithm for exact decoding of syntactic translation models, and shown that it is significantly more effi-cient than other exact algorithms for decoding tree-to-string models. There are a number of possible ways to extend this work. Our experiments have focused on tree-to-string models, but the method should also apply to Hiero-style syntactic transla-tion models (Chiang, 2007). Additionally, our ex-periments used a trigram language model, however the constraints in figure 3 generalize to higher-order language models. Finally, our algorithm recovers the 1-best translation for a given input sentence; it should be possible to extend the method to find k-best solutions.
 Acknowledgments Alexander Rush and Michael
