 Keyphrases of a given document often refer to a number of phrases which are able to capture the main topics of that document. Due to the briefness and informativeness, keyphrases could make readers quickly obtain a rough under-standing of document without going through the text. Besides, keyphrases have been utilized in a variety of natural language processing tasks such as digital library [1] and search engine [2].
 Actually, authors are not always required to manually provide keyphrases. Therefore it would be helpful to automatically extracting keyphrases from doc-uments, which is the aim of keyphrase extraction. Generally speaking, the task of keyphrase extraction consists of two stages: candidate phrase generation and keyphrase selection. In the first stage, the document is segmented into phrases, each of which is viewed as a keyphrase candidate. In the second but the key stage, each of the candidate phrases is assigned with a saliency score which is often calculated by leveraging supervised or unsupervised learning techniques, to indicate whether it could be a keyphrase or not. In most conventional keyphrase extraction approaches, the candidates with the top-k scores will be selected as the keyphrases individually.

However, we observe that the saliency scores of semantically or lexically sim-ilar candidates also tend to be similar with each other, which may give rise to undesirable extracted results for conventional approaches. Take this paper for ex-ample, keyphrase extraction and structural learning are the desirable keyphrases. However, the candidates like extracting keyphrases , keyphrase extraction ap-proach may share similar saliency scores with the true keyphrase keyphrase ex-traction for their lexical similarity. That would bring another true keyphrase structural learning down from the top ranked list of candidates from which the keyphrases are selected, if its saliency score was a bit lower than that of keyphrase extraction . Obviously, the keyphrase set of keyphrase extraction and structural learning is preferred to that of keyphrase extraction , extracting keyphrases and keyphrase extraction approach , as the former covers more aspects of the topics in this document while being more diverse to carry less redundant information.
In this paper, we propose a novel keyphrase extraction approach that explore the diversity and coverage characteristics of keyphrases extracted from a given document. The main idea is to regard the task of keyphrase extraction as a problem of subset learning where the keyphrases of a given document is a subset of the set of candidate phrases generated from that document. We formulate subset learning problem in the framework of structural learning and employ a large margin structural learning algorithm, i.e., structural SVM, with a spe-cific loss function to learn extraction model. We evaluate our approach using a dataset composed of research articles with manually assigned keyphrases by the authors. The experimental results show the improvement of our approach to several state-of-the-art keyphrase extraction approaches, which results from the explicit enhancement of the diversity and coverage of keyphrase set. 2.1 Keyphrase Extraction Most recent studies have investigated th e problem of keyphrase extraction from a machine learning point of view, which can be categorized into two major ap-proaches: supervised and unsupervised.

Supervised approaches usually involve building a binary classifier using a collection of documents with manually assigned keyphrases as training set. The accuracy of the classifier relies heavily on the features describing the saliency of candidate phrase. Turney [3] calculated nine features such as the phrase frequency, the relative position of the phrase in document to represent each candidate phrase. More types of features have been explored, including linguistic knowledge such as PoS tag patterns [4], semantic information gleaned from a domain-specific the-saurus [5], and etc. Rather than employing classification methods, Jiang et al. [6] applied another machine learning technique, namely learning to rank to keyphrase extraction. Supervised approaches have been applied to extract keyphrases from many particular types of documents such as product landing pages [7] and social snippets [8].

Unsupervised approaches calculate the saliency score of each keyphrase can-didates mainly based on the co-occurrence characteristics of words appearing in the source documents. Mihalcea et al. [9] proposed a graph-based ranking model, referred to as TextRank, for keyphrase extraction. Recently, a number of extensions of graph-based methods have been proposed. For example, Liu et al. [10] calculated the saliency score of each word w.r.t various topics via multiple random walks; Wan et al. [11] built three types of graphs to reflect a variety of relationships between sentences and wor ds. Clustering techniques have also been used in keyphrase extraction [12].

Our approach falls into the first category, but neither of the previous work explicitly takes the diversity and coverage of keyphrases into account which are key requirements of desirable keyphrases. 2.2 Structural SVM Structural SVM [13] is a large margin approach to the problem of learning with structured and interdependent output sp ace. It first extracts combined features from the input and output space, then learn a discriminant function using a generalized maximum-margin principle to derive prediction for a given input. Recently, some improvements have been conducted. For example, Joachims et al. [14] introduced a 1-slack version of the cutting plane algorithm with the time complexity linear in the number of training examples. Yu et al. [15] used approx-imate cutting planes and random sampling to enable efficient training structural SVM with kernels. Sarawagi et al. [16] proposed an improved training algorithm that generates more informative violated constraints during learning process. Recently, structural SVM has found incr easing applications such as document retrieval [17], visual object localization [18]. To the best of our knowledge, it was not applied to the task of keyphrase extraction. 3.1 Formulations Let x = { x 1 ,  X  X  X  ,x m } X  X  denote a given document composed of m candidate phrases x i , y  X  X  denote a subset of phrases selected as the keyphrases. As in supervised learning scenario, we are given a sample of input (document) and output (keyphrase set) pairs S = { ( x i , y i )  X  X  X Y : i =1 ,  X  X  X  ,n } as training set, and aim to find a hypothesis function h : X X  X  that minimize the empirical risk: set  X  y approaches the ground truth y i . The loss function for keyphrase extraction will be described in detail in Sect. 3.3.
 In the framework of structural learning, a linear discriminant function f : X X Y X  R is employed to measure the compatibility of a predicted keyphrase set with the given document. Thus, the hypothesis function h takes the form of where w is a parameter vector, and  X ( x , y ) is a combined feature representa-tion of document and the corresponding predicted keyphrase set, which will be described in detail in Sect. 3.4.

Using the structural SV M, the parameter vector w is learned through opti-mizing the following convex quadratic programming problem: The object of the QP problem is a tradeoff between the structural risk 1 2 w 2 and an upper bound of empirical risk, which is controlled by a parameter C .The constraints imply that the re-scaled margin between the ground truth y i and the best runner-up  X  y =argmax y  X  X \ y that there are 2 n subsets of a set of size n , the number of constraints in (2) is intractably large. In the paper, we employ cutting plane algorithm to give a approximate solution of which the convergence can be theoretically guaranteed. For details of the algorithm please refer to [13].

After the QP problem in (2) is solved, the keyphrase set of a new document can be predicted using (1). Since exhaustive search over Y is NP-hard, motivated by the algorithm in [17], we propose a greedy algorithm to derive an approximate optimal keyphrase set for a given document. 3.2 Diversity Measure The element of our approach to enhancing diversity and coverage of keyphrase set is to define a measure to quantify the diversity between keyphrases.
The existing similarity measures such as knowledge-based measures [19], corpus-based measures [20], web-based measures [21], give evidences of the similarity be-tween two pieces of text from different points of view. However, neither of them is proposed for the application of measuring the topical diversity within a phrase set. It is thus not appropriate to utilize any single existing measure in our approach directly. In the paper, we take the existing similarity measures as meta measures and define the diversity measure between keyphrases as weighted combination of the meta measures. Based on the assump tion that the keyphrases of a given doc-ument have large diversity, the combination weights are obtained supervised by a set of given keyphrase set as training set. Formulations of Diversity Measure Learning. Let f ( x 1 ,x 2 )  X  [0 , 1] de-note a function measuring the dissimilarity between two phrases x 1 and x 2 . Given v off-the-shelf dissimilarity functions F = { f 1 ,  X  X  X  ,f v } as meta measures, is the keyphrase set of the i -th document and x j ( j =1 ,  X  X  X  ,n i )isoneofthe keyphrases, the aim is to find a normalized weight vector u = { u 1 ,  X  X  X  ,u v } which maximizes the diversities between al l keyphrases in the training set: Meta Measures. In the paper, we calculate the following four different types of dissimilarity measures between phrases, which are used as the meta measures. 1. Lexical-based measure. The measure basically counts the commonly appear-2. Knowledge-based measure. WordNet[22]isusedasexternalsemanticknowl-3. Corpus-based Measure. Given a corpus represented by a word-by-document 4. Web-based measure. The main idea of the measure is to leverage web search Learning Algorithm. We employ a hill climbing algorithm for solving the optimization problem in (3). Algorithm 1 describes the details of the algorithm.
During each iterations, we search for a weight for each meta measure while persevering that of others and update only one weight. This helps the algorithm partially avoid overfitting from which conventional hill climbing algorithms often suffer. Besides, we run the algorithm with k random initializations and select the final weight vector with the maximum target value of (3) from the k runs. 3.3 Loss Function In order to enhance diversity and coverage, we define the loss function in Struc-tural SVM as the penalty of extraction errors w.r.t. diversity and coverage. Intuitively, the diversity error of a pred icted keyphrase set refers to extracting topically similar phrases as the keyphras es, and the coverage error refers to miss-ing important topics in the extracted results. Figure 1 illustrates the two types of errors on a keyphrase set consists of three keyphrases. Figure 1(a) shows the ideal case that the extracte d keyphrases (denoted by circles) are identical to the true keyphrases (denoted by triangles) . Figure 1(b) shows a mis-extracted case that two similar phrases 1 and 3 are extracted on accoun t of a true keyphrase 1, while another true keyphrase 3 is missing in the extracted set. In the paper, we simply assume the topics are delivered by keyphrases. Therefore it can be regarded that both of the diversity and coverage errors occurs in Fig. 1(b). the predicted keyphrases of a document, respectively. Before defining the loss function, we first obtain a partition of  X  y according to the similarities between the predicted keyphrases and the ground truth, i.e., on account of the true keyphrase y i ,and d (  X  ,  X  )  X  [0 , 1] is the diversity measure learnedthroughAlgorithm 1. Note that  X  y ( y i ) may consist of more than one phrases or be null, if the diversity and coverage errors occur on y i , respectively. Using the reorganized form of predicted keyphrases in (4), the loss function  X  ( y ,  X  y )isdefinedasfollows:
As for the above loss function, we can easily find that: 1.  X  ( y ,  X  y ) = 0 if and only if  X  y ( y i )= { y i } holds for each y i  X  y . 4. If more than one phrases is extracted on account of a true keyphrase y i , i.e., 3.4 Features Our approach use a variety of features to represent each possible keyphrase sets rather than possible keyphrases. Given an input (document) and output (keyphrase set) pair ( x , y )where y = { y 1 ,  X  X  X  ,y k } , we first extract a set of features  X  p ( x ,y i )( p =1 ,  X  X  X  ,d ) from each keyphrase candidates y i  X  y ,re-ferred to as basic features, then compute the feature  X  p,op ( x , y ) on the basis of  X  ( x ,y i )( i =1 ,  X  X  X  ,k ) by using a predefined feature operator op . Finally, all the features  X  p,op ( x , y ) are concatenated to fo rm the feature vector  X ( x , y ). Basic Features. We make use of the following features widely used in conven-tional keyphrase extraction approaches as the basic features in our approach. 1. TF-IDF. Phrase and word level TF, IDF and TF  X  IDF are computed as 2. Phrase length. The features includes the numbers of words and letters in the 3. Occurrence. We compute binary features indicating whether the phrase oc-4. Part of speech. We compute binary features indicating whether the phrase 5. PageRank. Phrase and word level PageRank values are computed as features. Feature Operators. After extracting a set of bas ic features on every phrases in a phrase set, a number of feature operators are defined to compute the feature values of the keyphrase set. Each of them takes a set of values of a basic feature as input, and output a feature value of the corresponding keyphrase set. The feature operators used in our approach are listed in Table 1.

Note that the operators in Table 1 would be applied to different types of basic features. Specifically, the first four operators are applied to real-valued basic features and the others are applied to binary ones. 4.1 Experimental Setup Dataset. To avoid manually annotation of keyphrases which is often labori-ous and erroneous, we constructed an evaluation dataset using research articles with author provided keyphrases. Specific ally, we collected the full-text of pa-pers published in the proceedings of two conferences, namely ACM SIGIR and SIGKDD from 2006 to 2010. After removing the papers without author pro-vided keyphrases, there are totally 3461 keyphrases appearing in 997 papers in our evaluation dataset. For each paper, tokenization, pos tagging, stemming and chunking were performed using NLTK (Natural Language Toolkit). We observed that the keyphrases make up only 0.31% of the total phrases in the dataset, i.e., the dataset is highly imbalanced. To compensate for this, we filtered out a por-tion of negative instances (non-keyphrases) from the dataset using some heuristic rules.
 Baselines. Two state-of-the-art unsupervised and supervised keyphrase extrac-tion approaches, namely TextRank [9] and Kea [24] are selected as the baselines. We also compared our approach with SVM that made use of all the basic fea-tures introduced in Sect 3.4 for training. The SVM baseline and our approach are implemented using the SVM light toolkit 1 . As for the parameters C in SVM and SVM-struct, the default values given by SVM light are used.
 Evaluation Measures. The traditional metrics, namely Precision , Recall and F1-score are utilized to evaluate our approach and the baselines in the experi-ments. 4.2 Experimental Results We random split the dataset into five even parts and conducted 5-fold cross evaluation. The extraction results of our approaches and the baselines averaged over the 5 trails are shown in Fig. 2.
From the figure, we can see that our approach (denoted by SVM-struct) out-performs nearly all the baselines in terms of Precision, Recall and F1-score. We also conducted significance test (paired t -test) on the differences between our approach and the baselines. The results show that the improvement of our ap-proach over SVM in terms of Precision and F1-score is statistically significant ( p  X  0 . 05) and the improvement of our approach over TestRank and Kea in terms of all measures is statistically significant ( p  X  0 . 05). Since our approach uses the same set of basic features as does SVM , we can conclude that structural learning is benifical to the task of keyphrase extraction. 4.3 Analyses Experimental analyses were conducted to give better understandings of the ef-fectiveness of our approach. Size of Keyphrase Set. The size of predicted keyphrase set specified in the greedy algorithm for keyphrase set prediction using (1) is the main parameter that may influence the performance of our approach. We ranged the parameter from 2 to 10 in our experiments and plotted the performance variation of the our approach together with that of the SVM baseline in Fig. 3. From the figure, we can see that our approach outperforms SVM in terms of F1-score when the size equals 3 , 4 , 5 , 6 , 7 , 9. Since there are average 4 keyphrases in each document in our evaluation set, it can be concluded that the parameter of the size of keyphrase set could be relia bly set to be slightly higher than the general average number of keyphrases per document in the target domain. Diversity of Extraction Results. We investigated the ability of our ap-proach to enhance the diversity and coverage of the extracted keyphrase sets. Given a set of documents with corresponding extracted keyphrase sets S = { sets as follows: set computed using the diversity measure learned by Algorithm 1. We randomly sampled 100 documents from the evaluation set and compared the diversity of the extracted keyphrase sets of our approach with that of the ground truth and SVM baseline in Fig. 4.

From the figure, we can see that the average diversity of the extracted keyphrase sets by our approach is higher than that of the ground truth and SVM baseline. Besides, the diversity differences are all statistically significant in paired t -test ( p  X  0 . 01). The results indicate that our approach is capable of enhancing the diversity of extracted keyphrase set. It is also not difficult to conclude that the coverage of extracted k eyphrase set can be enhanced resulting from the high diversity between extrac ted keyphrases and a proper setting of parameter of the size of ex tracted keyphrase set. In this paper, we have proposed a novel keyphrase extraction approach that takes the diversity and coverage of the keyphrases into accounts. In our approach, the issues of keyphrase extraction was formulated as subset learning which can be viewed as a structural learning problem. We have defined a loss function to reflect the diversity and coverage characteristics of the keyphrase set and preformed training using structural SVM. Experimental results on a scientific literature dataset have shown the ability of our approach to enhancing diversity and coverage which results in the perform ance improvement to s tate-of-the-art approaches.

As future work, we will experimentally verify the effectiveness of our approach with more evaluation sets constructed from widely domains. In addition, we plan to leverage topic models such as LDA and LSA to derive latent topics in document so as to enhance the diversity and coverage of extracted keyphrases. Acknowledgments. This paper is supported partly by Chinese National Nat-ural Science Foundation (61170079), Research Proj ect of  X  X DUST Spring Bud X  (2010AZZ179), Sci. &amp; Tech. Development Fund of Shandong Province of China (2010GSF10811), Specialized Research Fund for the Doctoral Program of Higher Education of China (20103718110007).

