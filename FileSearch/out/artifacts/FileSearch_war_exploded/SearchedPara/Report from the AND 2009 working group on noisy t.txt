 ORIGINAL PAPER Simone Marinai  X  Dimosthenis Karatzas Abstract This document is a report of the discussions by the participants to the working group on  X  X oisy Text Data-sets X  organized during the Third Workshop on Analytics for Noisy Unstructured Text Data (AND 2009) held in Barcelona (Spain) July 23, 24, 2009. 1 Introduction Twelve researchers participated in the working group rep-resenting two main areas of expertise. Some researchers are more familiar with document image analysis techniques (dealing with the theory and applications of machine read-ing systems), whereas other people come from the field of text analytics (dealing with the post-processing of informa-tion already in textual form). While sharing some application areas, it turned out that some remarkable differences exist between the two groups.

Naturally, given the composition of the working group, the discussion was focused around the common ground of auto-matic document conversion into an electronic form and par-ticularly on the identification of a roadmap for the definition of the functionalities needed to facilitate research on noisy text analysis. Such an infrastructure is expected to be multi-faceted (due to the heterogeneity of the sub-tasks involved) and to allow to archive and share information about available resources on (noisy) data, such as text images, audio streams, lexica, corpora, evaluation tools, datasets, and ground truths. Important benefits from such an infrastructure should come from cross-fertilization of related tasks. For instance, we foresee that techniques and approaches adopted in annotation tools used to collect ground-truth data could be considered also in other domains. 2 Background In an attempt to circumscribe the topic of the discussion, we focused our analysis mainly on Historical documents that present challenges for both the document image analysis and the text analytics fields. This focus was due to the prevailing research interest of most participants, but we anticipate that several of our findings could be translated to other areas. One first concern about Historical documents is to try to define the temporal extent covered by these collections. In the most general sense, any document can be deemed to be an histori-cal one, since any piece of information could, in principle, be of interest in a given historical research context. From a more pragmatic point of view, most of us agreed that one broad bor-derline can be set by including in the  X  X istorical X  category the documents that are out of copyright and can therefore be freely distributed in digital format either as images or as transcriptions. One positive aspect of working with histori-cal documents is clearly the easy distribution of benchmark datasets. On the other hand, one important limitation is that we leave out from our analysis many digital-born pieces of information, such as SMS and Email, that are a substantial part of AND-related research but have been only marginally considered in the Working Group discussion.

Automatic conversion of historical documents is in most cases not perfect, and the result when faced with degraded documents is rarely correct or dependable. As such, doc-uments with historical content are good candidates to test noise resilient techniques for two main reasons. First, these documents are written in an  X  X istorical language X  that can be considered noisy with respect to dictionaries of contem-porary languages. Second, historical documents are some-times printed with non-standard fonts and old typographic techniques and are prone to aging problems that reduce the quality of the scanned images and therefore influence the per-formance that can be achieved by standard OCR packages. Another important feature of historical documents (shared by various types of documents) is that they contain in addition to textual areas also objects such as equations, tables, illustra-tions, and other elements that are traditionally not handled by  X  X ure X  text analytics techniques. All these objects should be addressed as well since they potentially carry relevant infor-mation. The structure of the document (e.g., the page lay-out), which ties all these semantic elements together, is also important to be preserved in addition to the textual content.
Overall, there are two broad areas where new solutions are required: the document analysis part (all the tasks that come after document digitization up to and including char-acter recognition) and the text analytics part (post-processing algorithms once a first textual description has been obtained). There is merit in advancing the state of the art in both areas, as different problems can be resolved at each stage. Moreover, there is merit in better communicating between the two fields, as information over and above the textual transcription that can be extracted automatically can inform post-processing algorithms.

Three aspects were identified as important during the dis-cussion: datasets collection, ground-truth information, and benchmarks. The views of the working group about these topics are detailed, as recorded by the authors of this report, in the next sections. The rest of this report, as far as DIA is concerned is mostly focused on the problem of character rec-ognition, as it constitutes the main overlapping area between DIA and TA. 3 Datasets Both fields of document image analysis (DIA) and of text analytics (TA) are in need of datasets that are representative of real-world objects and are statistically adequate to test any initial research hypotheses.

It is nevertheless difficult to agree on a single dataset that fulfills the above requirements simultaneously for both fields. For example, a representative and statistically ade-quate dataset for text analytics would be judged in terms of the number of words it contains, the epoch it covers, whether it contains all the usual linguistic structures of its epoch etc. From the point of view of document image analysis though, a representative and statistically adequate dataset would con-tain images of pages that were preserved under different conditions, skewed in different angles, contain samples of all possible page layouts in the collection at hand etc.
In the sub-sections below, we provide a discussion on raw data collection, ongoing mass digitization methods, the suit-ability of these efforts to AND-related research, and how they could be enriched to produce useful collections for research. 3.1 Raw data collections In the last few years, mass digitization of texts has been an active area. Probably the most well-known project in this domain is the  X  X oogle Book Library X  project that started around 2004 with the aim of scanning and making search-able the collections of several large research libraries. Other projects began earlier with similar targets. For instance, the Million Book project started in the late 1990s with the aim to digitize a million books by 2007 (it now has more than 1.5 million books) and make them available on the Internet. The Gutenberg Project began in 1993 (even though the first digitized books date earlier) with the aim of making available large collections of full texts of public domain books. Most of these books are in plain text format, but several are also available in e-book formats.

If we include also public efforts to digitize cultural her-itage works, such as the Gallica Digital Library managed by the French National Library, we can estimate that sev-eral millions of books are nowadays freely available in the Internet.

One natural question that arose in the Working Group was how mass digitization can help our research. In most cases, these data are digitized images of pages with no ground-truth associated. Sometimes the text in the books has been converted by suitable OCR packages and the un-corrected text has been made available as well. In few cases the text images are carefully annotated by manual transcriptions or by checking the OCR outputs.

Given these data, how can we use them to help future research? The data can be of interest as they are or could be enriched by adding appropriate annotations corresponding both to ground-truth information and metadata. At this point, we should note that there is an implicit differentiation made between metadata and ground-truth data. The most impor-tant difference is that metadata are collected at the time the dataset is created and are usually tightly structured within the dataset itself. On the opposite, ground truth can exist as a separate entity to the dataset. As such ground-truth data can be specified, created, and submitted by a different person or organization separately from the dataset submission.
For DIA-oriented research, the raw data can be used to test any processing task that deals with document images. Another possibility is to use the publicly available document imagesasinputdata,relyingonadditionalgroundtruthadded by researchers in the DIA community. For the text analytics area(un-)correctedOCRoutputscanbeofsignificantinterest as well; for instance to extract statistics of most common OCR errors or to identify task specific lexica of a given time period.

Problems that are likely to be encountered in the defi-nition and implementation of ground-truth annotations are analyzed later. 4 Benchmarks According to the Longmann Dictionary of Contemporary English, a Benchmark is  X  X omething that is used as a stan-dard by which other things can be judged or measured X . In our context, a benchmark is made by three main components:  X  A collection of objects, such as document images or text  X  A collection of annotations that define some aspect(s) of  X  An evaluation protocol that provides some measures of
Competitions, such as those organized in ICDAR 2009, are some of the best ways to gather a consensus on a given benchmark with related performance metrics. Usually, the contests are organized around popular tasks that are already addressed by several research groups. However, the contests can also act as catalysts and open the way to new topics. Sev-eral researchers collected personal benchmark sets over the years for specific research and would need an instrument to share these data with other researchers. To this purpose, an active group (like a Wiki) where knowledge and data can be shared should be of real interest to the community. 5 Groundtruth Provided that very large collections of documents are now freely available, the most important problem to address is the manual annotation of these data that is required to generate accurate ground-truth information. Since this is a very expen-sive task, we should aim at obtaining a  X  X niversal X  annota-tion that could be valuable for several research topics. For instance, the text annotation of a digitized book could be used bybothresearchersonOCRandresearchersontextanalytics. Therefore, when producing the ground truth, we should pro-vide as much information as possible, so that it could be used by different researchers with different needs. There may be one general ground-truth corpus to test the end-to-end text reading process, or we can imagine different ground-truth data for each step (e.g., for the character segmentation and recognition). In the former case, the cost of manual anno-tation can be shared by several groups, but making a very detailed annotation could be more complex. Task-oriented ground truths can be defined as well. For instance, one book can be annotated either considering the high level hierarchi-cal structure (e.g., organized in chapters and sections) or at the page level. In the latter case, we can annotate the Char-acter, Word, Sentence, or even the page Layout. Worse than this, sometimes what is relevant for one topic can be noise for others. As an example, if there is a typing error in the original text, in some cases it should be corrected in the ground truth: with correct information, it is possible to have a good starting model for linguistic tasks. However, if the data are used for research on character classifiers for OCR, then the  X  X rror X  should be left as it is because otherwise we will teach in the wrong way the character recognition module.

Likewise, if a word is hyphenated at the end of a line, the ground truth could either consider the two parts of the word as a single unit or as two separate units depending on the application domain. This sort of dichotomy is related to the most appropriate resolution of annotation that could be at the word level, at the text line level, or even at the paragraph level.

In any case generating the ground truth implies several  X  X osts X  that can be summarized in three main areas: collec-tion of raw data (e.g., document images), development of tools for annotation, and use of (skilled) human resources to perform the actual labeling.

One emerging approach is the use of tools for coopera-tive annotation such as those used in the ICDAR 2009 Book Structure Extraction competition. In this case, the annotation is spread among various teams that are interested to contrib-ute so as to obtain the final annotated files. Tools to be used in this task can be designed and implemented with limited effort. However, the annotation should be of interest to var-ious groups so as to reach an appropriate critical mass. In general, providing the whole ground truth only to groups that actively produce part of it is probably a reasonable way to motivate the volunteers that annotate the data. Another point that should be addressed is how to include new groups and whether commercial companies are in general allowed to get access to these data.

Crowdsourcing platforms such as the Mechanical Turk or reCAPTCHA offer good alternatives for achieving collabo-rative annotation of large collections. The users involved in the annotation process are usually not experts; therefore, the process should be extremely well defined or broken down to small easy steps. Typically, correct annotation is ensured through the agreement of a minimum number of users (hence it can be an expensive and timely process) and quite often an extra verification step is needed.
A different way to obtain large datasets is to create syn-thetic ground truth. In this case, both the data and their ground truth are automatically created at the same time, taking into account specific degradation models to simulate real-world objects. The advantage is that large datasets can be easily created and accurate ground truth is readily available. The disadvantage of such methods though is that they are rarely capable of producing datasets that truly reflect the variety of actual data.

From the point of view of the Document Image Analysis field, there are ongoing efforts to maintain a collection of existing datasets online at the web pages of IAPR Technical Committee 10 (Graphics Recognition, http://www.iapr-tc10. org ) and 11 (Reading Systems, http://www.iapr-tc11.org ). Maybe this would be a good starting point to define some-thing useful for both communities. 6 Conclusions We attempted to do our best to summarize the main discus-sions we had in the AND Working Group on Noisy Text Databases. However, the opinions reported here are our unique responsibility. The point of view followed when writ-ing this document is mostly oriented toward the Document Image Analysis community both because the report is pub-lished in IJDAR and because of the personal background of the authors. Last but not least, we would like to thank all the people that participated to the discussion 1 and in particular Annette Gotscharek for sharing with us the notes collected during the meeting.
