 jl@yahoo-inc.com We are concerned with machine learning over lar ge datasets. As an example, the lar gest dataset we use in this paper has over 10 7 sparse examples and 10 9 features using about 10 11 bytes. In this setting, man y common approaches fail, simply because the y cannot load the dataset into memory or the y are not suf ciently efcient. There are roughly two approaches which can work: one is to parallelize a batch learning algorithm over man y machines ( e.g. , [3]), the other is to stream the examples to an online-learning algorithm ( e.g. , [2, 6]). This paper focuses on the second approach. Typical online-learning algorithms have at least one weight for every feature, which is too expensi ve in some applications for a couple reasons. The rst is space constr aints : if the state of the online-learning algorithm overo ws RAM it can not efciently run. A similar problem occurs if the state overo ws the L2 cache. The second is test-time constr aints : reducing the number of features can signicantly reduce the computational time to evaluate a new sample.
 This paper addresses the problem of inducing sparsity in learned weights while using an online-learning algorithm. Natural solutions do not work for our problem. For example, either simply adding L 1 regularization to the gradient of an online weight update or simply rounding small weights to zero are problematic. Ho we ver, these two ideas are closely related to the algorithm we propose and more detailed discussions are found in section 3. A third solution is black-box wrapper ap-proaches which eliminate features and test the impact of the elimination. These approaches typically run an algorithm man y times which is particularly undesirable with lar ge datasets.
 Similar problems have been considered in various settings before. The Lasso algorithm [12] is commonly used to achie ve L 1 regularization for linear regression. This algorithm does not work automatically in an online fashion. There are two formulations of L 1 regularization. Consider a con vex-constraint formulation where s is a tunable parameter . The other is soft-re gularization with a tunable parameter g : With appropriately chosen g , the two formulations are equi valent. The con vex-constraint formu-lation has a simple online version using the projection idea in [14 ]. It requires the projection of weight w into an L 1 ball at every online step. This operation is dif cult to implement efciently for lar ge-scale data with man y features even if all features are sparse, although important progress was made recently so the comple xity is log arithmic in the number of features [5]. In contrast, the soft-re gularization formulation (2) is efcient for a batch setting[8] so we pursue it here in an online setting where it has comple xity independent of the number of features. In addition to the L 1 regu-larization formulation (2), the family of online-learning algorithms we consider also includes some non-con vex sparsication techniques.
 The Forgetron [4] is an online-learning algorithm that manages memory use. It operates by decay-ing the weights on pre vious examples and then rounding these weights to zero when the y become small. The Forgetron is stated for kernelized online algorithms, while we are concerned with the simple linear setting. When applied to a linear kernel, the Forgetron is not computationally or space competiti ve with approaches operating directly on feature weights.
 At a high level, our approach is weight decay to a def ault value. This simple method enjo ys strong performance guarantees (section 3). For instance, the algorithm never performs much worse than a standard online-learning algorithm, and the additional loss due to sparsication is controlled contin-uously by a single real-v alued parameter . The theory gives a family of algorithms with con vex loss functions for inducing sparsity X one per online-learning algorithm. We instantiate this for square loss in section 4 and sho w how this algorithm can be implemented efciently in lar ge-scale prob-lems with sparse features. For such problems, truncated gradient enjo ys the follo wing properties: (i) It is computationally efcient : the number of operations per online step is linear in the number of nonzero features, and independent of the total number of features; (ii) It is memory efcient : it maintains a list of acti ve features, and can insert (when the corresponding weight becomes nonzero) and delete (when the corresponding weight becomes zero) features dynamically .
 Theoretical results stating how much sparsity is achie ved using this method generally require addi-tional assumptions which may or may not be met in practice. Consequently , we rely on experiments in section 5 to sho w truncated gradient achie ves good sparsity in practice. We compare truncated gradient to a few others on small datasets, including the Lasso, online rounding of coef cients to zero, and L 1 -re gularized subgradient descent. Details of these algorithms are given in section 3. We are interested in the standard sequential prediction problems where for i = 1 ; 2 ; : : : : We want an update rule f allo ws us to bound the sum of losses, P t sparsity . For this purpose, we start with the standard stochastic gradient descent (SGD) rule, which is of the form: is often referred to as the learning rate . In the analysis, we only consider constant learning rate for simplicity . In theory , it might be desirable to have a decaying learning rate i which becomes smaller when i increases to get the so-called no-r egret bound without kno wing T in adv ance. Ho we ver, if T is kno wn in adv ance, one can select a constant accordingly so the regret vanishes as T ! 1 . Since the focus of the present paper is on weight sparsity , rather than choosing the learning rate, we use a constant learning rate in the analysis because it leads to simpler bounds.
 The abo ve method has been widely used in online learning ( e.g. , [2, 6]). Moreo ver, it is argued to be efcient even for solving batch problems where we repeatedly run the online algorithm over training data multiple times. For example, the idea has been successfully applied to solv e lar ge-scale standard SVM formulations [10, 13]. In the scenario outlined in the introduction, online-learning methods are more suitable than some traditional batch learning methods. Ho we ver, the learning rule (3) itself does not achie ve sparsity in the weights, which we address in this paper . Note that variants of SGD exist in the literature, such as exponentiated gradient descent (EG) [6]. Since our focus is sparsity , not SGD vs. EG, we shall only consider modications of (3) for simplicity . In this section, we rst examine three methods for achie ving sparsity in online learning, including a novel algorithm called truncated gradient . As we shall see, all these ideas are closely related. Then, we pro vide theoretical justications for this algorithm, including a general regret bound and a fundamental connection to the Lasso. 3.1 Simple Coefcient Rounding In order to achie ve sparsity , the most natural method is to round small coef cients (whose magni-tudes are belo w a threshold &gt; 0 ) to zero after every K online steps. That is, if i=K is not an inte ger , we use the standard SGD rule (3); if i=K is an inte ger , we modify the rule as: T standard stochastic gradient descent, and then round the coef cients to zero. The effect is to remo ve nonzero and small weights. In general, we should not tak e K = 1 , especially when is small, since in each step w i is modied by only a small amount. If a coef cient is zero, it remains small after one online update, and the rounding operation pulls it back to zero. Consequently , rounding can be done only after every K steps (with a reasonably lar ge K ); in this case, nonzero coef cients have suf cient time to go abo ve the threshold . Ho we ver, if K is too lar ge, then in the training stage, we must keep man y more nonzero features in the intermediate steps before the y are rounded to zero. In the extreme case, we may simply round the coef cients in the end, which does not solv e the storage problem in the training phase at all. The sensiti vity in choosing appropriate K is a main dra wback of this method; another dra wback is the lack of theoretical guarantee for its online performance. These issues moti vate us to consider more principled solutions. 3.2 L In the experiments, we also combined rounding-in-the-end-of-training with a simple online subgra-dient method for L 1 regularization with a regularization parameter g &gt; 0 : sgn( v j ) = 1 if v j &lt; 0 , and sgn( v j ) = 0 if v j = 0 . In the experiments, the online method (5) with rounding in the end is used as a simple baseline. Notice this method does not produce sparse weights online simply because only in very rare cases do two oats add up to 0 . Therefore, it is not feasible in lar ge-scale problems for which we cannot keep all features in memory . 3.3 Truncated Gradient In order to obtain an online version of the simple rounding rule in (4), we observ e that direct round-ing to zero is too aggressi ve. A less aggressi ve version is to shrink the coef cient to zero by a smaller amount. We call this idea truncated gradient , where the amount of shrinkage is controlled by a gravity parameter g i &gt; 0 : where for a vector v = [ v 1 ; : : : ; v d ] 2 R d , and a scalar g 0 , T 1 ( v; ; ) = [ T 1 ( v 1 ; ; ) ; : : : ; T 1 ( v d ; ; )] , with Ag ain, the truncation can be performed every K online steps. That is, if i=K is not an inte ger , we let g i = 0 ; if i=K is an inte ger , we let g i = Kg for a gra vity parameter g &gt; 0 . The reason for doing so (instead of a constant g ) is that we can perform a more aggressi ve truncation with gra vity parameter Kg after each K steps. This can potentially lead to better sparsity . We also note that when Kg , truncate gradient coincides with (4). But in practice, as is also veried by the theory , one should adopt a small g ; hence, the new learning rule (6) is expected to dif fer from (4). In general, the lar ger the parameters g and are, the more sparsity is expected. Due to the extra truncation T 1 , this method can lead to sparse solutions, which is conrmed empirically in section 5. A special case, which we use in the experiment, is to let g = in (6). In this case, we can use only one parameter g to control sparsity . Since Kg when K is small, the truncation operation is less aggressi ve than the rounding in (4). At rst sight, the procedure appears to be an ad-hoc way to x (4). Ho we ver, we can establish a regret bound (in the next subsection) for this method, sho wing it is theoretically sound. Therefore, it can be regarded as a principled variant of rounding. Another important special case of (6) is setting = 1 , in which all weight components shrink in every online step. The method is a modication of the L 1 -re gularized subgradient descent rule (5). The parameter g i 0 controls the sparsity achie ved with the algorithm, and setting g i = 0 gives exactly the standard SGD rule (3). As we sho w in section 3.5, this special case of truncated gradient can be regarded as an online counterpart of L 1 regularization since it approximately solv es an L 1 regularization problem in the limit of ! 0 . We also sho w the prediction performance of trun-cated gradient, measured by total loss, is comparable to standard stochastic gradient descent while introducing sparse weight vectors. 3.4 Regr et Analysis Throughout the paper , we use kk 1 for 1 -norm, and kk for 2 -norm. For reference, we mak e the follo wing assumption regarding the loss function: Assumption 3.1 We assume L ( w; z ) is con vex in w , and ther e exist non-ne gative constants A and B suc h that ( r 1 L ( w; z )) 2 AL ( w; z ) + B for all w 2 R d and z 2 R d +1 .
 For linear prediction problems, we have a general loss function of the form L ( w; z ) = ( w T x; y ) . The follo wing are some common loss functions ( ; ) with corresponding choices of parameters A and B (which are not unique), under the assumption sup binary classication where y 2 1 , but the last one is more often used in regression where y 2 R : Logistic : ( p; y ) = ln(1 + exp ( py )) , with A = 0 and B = C 2 ; SVM (hinge loss): ( p; y ) = max (0 ; 1 py ) , with A = 0 and B = C 2 ; Least squar es (square loss): ( p; y ) = ( p y ) 2 , with A = 4 C 2 and B = 0 .
 The main result is Theorem 3.1 which is parameterized by A and B . The proof will be pro vided in a longer paper .
 Theor em 3.1 (Spar se Online Re gret) Consider spar se online update rule (6) with w and &gt; 0 . If Assumption 3.1 holds, then for all w 2 R d we have wher e k v I ( j v 0 j ) k 1 = P d The theorem is stated with a constant learning rate . As mentioned earlier , it is possible to obtain a result with variable learning rate where = i decays as i increases. Although this may lead to a no-re gret bound without kno wing T in adv ance, it introduces extra comple xity to the presentation of the main idea. Since the focus is on sparsity rather than optimizing learning rate, we do not include such a result for clarity . If T is kno wn in adv ance, then in the abo ve bound, one can simply tak e = O (1 = In the abo ve theorem, the right-hand side involv es a term g i k w I ( w i +1 ) k 1 that depends on w i +1 which is not easily estimated. To remo ve this dependenc y, a trivial upper bound of = 1 can be used, leading to L 1 penalty g i k w k 1 . In the general case of &lt; 1 , we cannot remo ve the w i +1 dependenc y because the effecti ve regularization condition (as sho wn on the left-hand side) is the non-con vex penalty g i k w I ( j w j ) k 1 . Solving such a non-con vex formulation is hard both in the online and batch settings. In general, we only kno w how to efciently disco ver a local minimum which is dif cult to characterize. Without a good characterization of the local minimum, it is not possible for us to replace g i k w I ( w i +1 ) k 1 on the right-hand side by g i k w I ( w ) k 1 because such a formulation implies we could efciently solv e a non-con vex problem with a simple online update rule. Still, when &lt; 1 , one naturally expects the right-hand side penalty g k w I ( w i +1 ) k 1 is much smaller than the corresponding L 1 penalty g i k w k 1 , especially when w j has man y components close to 0 . Therefore the situation with &lt; 1 can potentially yield better performance on some data.
 Theorem 3.1 also implies a tradeof f between sparsity and regret performance. We may simply consider the case where g i = g is a constant. When g is small, we have less sparsity but the regret term g k w I ( w i +1 ) k 1 g k w k 1 on the right-hand side is also small. When g is lar ge, we are able to achie ve more sparsity but the regret g k w I ( w i +1 ) k 1 on the right-hand side also becomes lar ge. Such a tradeof f between sparsity and prediction accurac y is empirically studied in section 5, where we achie ve signicant sparsity with only a small g (and thus small decrease of performance). No w consider the case = 1 and g i = g . When T !1 , if we let ! 0 and T !1 , then follo ws from Theorem 3.1. In other words, if we let L 0 ( w; z ) = L ( w; z ) + g k w k 1 be the L 1 -regularized loss, then the L 1 -re gularized regret is small when ! 0 and T ! 1 . This implies truncated gradient can be regarded as the online counterpart of L 1 -re gularization methods. In the stochastic setting where the examples are dra wn iid from some underlying distrib ution, the sparse online gradient method proposed in this paper solv es the L 1 regularization problem. 3.5 Stochastic Setting SGD-based online-learning methods can be used to solv e lar ge-scale batch optimization problems. In this setting, we can go through training examples one-by-one in an online fashion, and repeat multiple times over the training data. To simplify the analysis, instead of assuming we go through example one by one, we assume each additional example is dra wn from the training data randomly with equal probability . This corresponds to the standard stochastic optimization setting, in which observ ed samples are iid from some underlying distrib utions. The follo wing result is a simple conse-quence of Theorem 3.1. For simplicity , we only consider the case with = 1 and constant gra vity g = g . The expectation E is tak en over sequences of indices i 1 ; : : : ; i T .
 Theor em 3.2 (Stoc hastic Setting) Consider a set of training data z be the L 1 -regularized loss over training data. Let ^ w 1 = w 1 = 0 , and dene recur sively for t 1 : T and w 2 R d : Observ e that if we let ! 0 and T ! 1 , the bound in Theorem 3.2 becomes E [ R ( ^ w T ; g )] E the batch L 1 -re gularization problem inf w 1 choose a random stopping time T , then the abo ve inequalities say that on average w T also solv es this L 1 -re gularization problem approximately . Thus, we use the last solution w T instead of the ag-batch learning setting, the connection of truncated gradient to L 1 regularization can be regarded as an alternati ve justication for the sparsity ability of this algorithm. The truncated descent update rule (6) can be applied to least-squares regression using square loss, We altered an efcient SGD implementation, Vowpal Wabbit [7], for least-squares regression according to truncated gradient. The program operates in an entirely online fashion. Features are hashed instead of being stored explicitly , and weights can be easily inserted into or deleted from the table dynamically . So the memory footprint is essentially just the number of nonzero weights, even when the total numbers of data and features are astronomically lar ge.
 In man y online-learning situations such as web applications, only a small subset of the features have nonzero values for any example x . It is thus desirable to deal with sparsity only in this small subset rather than in all features, while simultaneously inducing sparsity on all feature weights. The approach we tak e is to store a time-stamp j for each feature j . The time-stamp is initialized to the inde x of the example where feature j becomes nonzero for the rst time. During online learning, at each step i , we only go through the nonzero features j of example i , and calculate the un-performed shrinkage of w j between j and the current time i . These weights are then updated, and their time stamps are reset to i . This lazy-update idea of delaying the shrinkage calculation until needed is the key to efcient implementation of truncated gradient. The implementation satises efcienc y requirements outlined at the end of the introduction section. A similar time-stamp trick can be applied to the other two algorithms given in section 3. We applied the algorithm, with the efciently implemented sparsify option, as described in the pre vious section, to a selection of datasets, including ele ven datasets from the UCI repository [1], the much lar ger dataset rcv1 [9], and a pri vate lar ge-scale dataset Big_Ads related to ad interest prediction. While UCI datasets are useful for benchmark purposes, rcv1 and Big_Ads are more interesting since the y embody real-w orld datasets with lar ge numbers of features, man y of which are less informati ve for making predictions than others. The UCI datasets we used do not have man y features, and it seems a lar ge fraction of these features are useful for making predictions. For comparison purposes and to better demonstrate the beha vior of our algorithm, we also added 1000 random binary features to those datasets. Each feature has value 1 with prob . 0 : 05 and 0 otherwise. In the rst set of experiments, we are interested in how much reduction in the number of features is possible without affecting learning performance signicantly; specically , we require the accurac y be reduced by no more than 1% for classication tasks, and the total square loss be increased by no more than 1 % for regression tasks. As common practice, we allo wed the algorithm to run on the training data set for multiple passes with decaying learning rate. For each dataset, we performed 10 -rate , the gra vity g , number of passes of the training set, and the decay of learning rate across these passes. This set of parameters was then used on the whole training set. Finally , the learned classi-er/re gressor was evaluated on the test set. We x ed K = 1 and = 1 in this set of experiments. The effects of K and are included in an extended version of this paper . Figure 1 sho ws the fraction of reduced features after sparsication is applied to each dataset. For UCI datasets with randomly added features, truncated gradient was able to reduce the number of features by a fraction of more than 90% , except for the ad dataset in which only 71% reduction was observ ed. This less satisfying result might be impro ved by a more extensi ve parameter search in cross validation. Ho we ver, if Figure 1: Left: the amount of features left after sparsication for each dataset without 1% perfor -mance loss. Right: the ratio of AUC with and without sparsication. we tolerated 1 : 3% decrease in accurac y (instead of 1% ) during cross validation, truncated gradient tional accurac y loss of 0 : 3% . Ev en for the original UCI datasets without articially added features, some of the less useful features were remo ved while the same level of performance was maintained. For classication tasks, we also studied how truncated gradient affects AUC (Area Under the ROC Curv e), a standard metric for classication. We use AUC here because it is insensiti ve to threshold, unlik e accurac y. Using the same sets of parameters from 10 -fold cross validation described abo ve, we found the criterion was not affected signicantly by sparsication and in some cases, it was ac-tually impro ved, due to remo val of some irrele vant features. The ratios of the AUC with and without sparsication for all classication tasks are plotted in Figure 1. Often these ratios are abo ve 98% . The pre vious results do not exercise the full power of the approach presented here because the y are applied to datasets where the standard Lasso is computationally viable. We have also applied this approach to a lar ge non-public dataset Big_Ads where the goal is predicting which of two ads was click ed on given conte xt information (the content of ads and query information). Here, accepting a 0 : 9% increase in classication error allo ws us to reduce the number of features from about 3 10 9 to about 24 10 6  X a factor of 125 decrease in the number of features.
 The next set of experiments compares truncated gradient to other algorithms regarding their abilities to tradeof f feature sparsication and performance. Ag ain, we focus on the AUC metric in UCI classication tasks. The algorithms for comparison include: (i) the truncated gradient algorithm with K = 10 and = 1 ; (ii) the truncated gradient algorithm with K = 10 and = g ; (iii) the rounding algorithm with K = 10 ; (iv) the L 1 -re gularized subgradient algorithm with K = 10 ; and (v) the Lasso [12 ] for batch L 1 regularization (a publicly available implementation [11 ] was used). We have chosen K = 10 since it work ed better than K = 1 , and this choice was especially important for the coef cient rounding algorithm. All unspecied parameters were identied using cross validation. Note that we do not attempt to compare these algorithms on rcv1 and Big_Ads simply because their sizes are too lar ge for the Lasso and subgradient descent. Figure 2 gives the results on datasets ad and spambase. Results on other datasets were qualitati vely similar . On all datasets, truncated gradient (with = 1 ) is consistently competiti ve with the other online algorithms and signicantly outperformed them in some problems, implying truncated gradient is generally effecti ve. Moreo ver, truncated gradient with = g beha ves similarly to rounding (and sometimes better). This was expected as truncated gradient with = g can be regarded as a principled variant of rounding with valid theoretical justication. It is also interesting to observ e the qualitati ve beha vior of truncated gradient was often similar to LASSO, especially when very sparse weight vectors were allo wed (the left sides in the graphs). This is consistent with theorem 3.2 sho wing the relation between these two algorithms. Ho we ver, LASSO usually performed worse when the allo wed number of nonzero weights was lar ge (the right side of the graphs). In this case, LASSO seemed to overt while truncated gradient was more rob ust to overtting. The rob ustness of online learning is often attrib uted to early stopping, which has been extensi vely studied (e.g., in [13 ]).
 Finally , it is worth emphasizing that these comparison experiments shed some light on the relati ve strengths of these algorithms in terms of feature sparsication, without considering which one can be efciently implemented. For lar ge datasets with sparse features, only truncated gradient and the ad hoc coef cient rounding algorithm are applicable. This paper covers the rst efcient sparsication technique for lar ge-scale online learning with strong theoretical guarantees. The algorithm, truncated gradient, is the natural extension of Lasso-style regression to the online-learning setting. Theorem 3.1 pro ves the technique is sound: it never harms performance much compared to standard stochastic gradient descent in adv ersarial situations. Furthermore, we sho w the asymptotic solution of one instance of the algorithm is essentially equi v-alent to Lasso regression, thus justifying the algorithm' s ability to produce sparse weight vectors when the number of features is intractably lar ge. The theorem is veried experimentally in a num-ber of problems. In some cases, especially for problems with man y irrele vant features, this approach achie ves a one or two orders of magnitude reduction in the number of features.

