 With the explosion of user-generated web2.0 content in the form of blogs, wikis and discussion forums, the Internet has rapidly become a massive dynamic repository of public opin-ion on an unbounded range of topics. A key enabler of opin-ion extraction and summarization is sentiment classification: the task of automatically identifying whether a given piece of text expresses positive or negative opinion towards a topic of interest. Building high-quality sentiment classifiers using standard text categorization methods is challenging due to the lack of labeled data in a target domain. In this paper, we consider the problem of cross-domain sentiment analy-sis: can one, for instance, download rated movie reviews from rottentomatoes.com or IMBD discussion forums, learn linguistic expressions and sentiment-laden terms that gener-ally characterize opinionated reviews and then successfully transfer this knowledge to the target domain, thereby build-ing high-quality sentiment models without manual effort? We outline a novel sentiment transfer mechanism based on constrained non-negative matrix tri-factorizations of term-document matrices in the source and target domains. We report some preliminary results with this approach. Categories and Subject Descriptors: I.2 [Artificial In-telligence]:Learning General Terms: Algorithms, Experimentation Keywords: Sentiment analysis, Transfer learning, Nonneg-ative matrix factorization
Suppose that we download movie reviews from an online review site and compile a large term-document matrix, X 1 representing m terms and n documents. Assume further that the sentiment associated with at least some of these documents is known from the explicit ratings given by movie enthusiasts that visit these sites. From such a dataset, we can hope to identify general linguistic indicators of positive  X 
The work is partially supported by NSF grants IIS-0546280 and DMS-0844513.
 and negative opinions, e.g., sentiment-laden terms such as  X  X reat X  and  X  X wful X  respectively. The end goal of this exer-cise would be to apply this knowledge to guage sentiment around documents in a new target domain X 2 ,whichfor example may be a collection of blogs posts talking about products and services of keen interest to a company. In this poster, we propose a mechanism to transform document la-bel information in one domain to another via words ,usinga principled matrix factorization framework.

Transfer learning and domain adaptation (see e.g., [1]) are quite natural tools for sentiment analysis applications, but most efforts in these directions have been fairly recent (see e.g., [2] and references therein). A in-depth survey of sentiment analysis literature [5] shows emphasis on standard text classification or lexical rule based approaches mainly geared towards single-domain applications. In this regard, our main objective in this poster is to report progress on a new contribution to cross-domain sentiment analysis.
We first recall nonnegative matrix factorization models [3] to give a background for our methodology. We build on tri-factorization based co-clustering models closely related to probabilistic Latent Semantic Indexing (pLSI) and proba-bilistic Topic models. Given a m  X  n term-document matrix X from a domain, we construct a tri-factorization, X  X  FSG T ,where G is an n  X  k nonnegative matrix whose i th row represents the posterior probability that the i th docu-ment belongs to one of k classes or topics. Similarly, F an m  X  k matrix specifying posterior class-probabilities for words. The k  X  k core factor S gives a condensed view of X  X  in the pLSI context, it may be understood as class proba-bility distribution. Under orthogonality constraints on F, G and non-negativity constraints on F, S, G the factorization above can be computed using simple iterative update rules. See [3] for applications to clustering. In the sentiment anal-ysis case, we set k = 2 with the intention of interpreting the first and second columns of F, G as positive and negative sentiment probability respectively. In order to enable trans-fer learning, we also need to add further constraints that express prior knowledge. These are discussed next.
Let X 1 and X 2 be term-document matrices in the source and target domains respectively. We will shortly consider constrained factorizations X 1  X  F 1 S 1 G 1 and X 2  X  F 2 respectively. We assume that the number of terms is the same: if the vocabularies differ, we simply pad zero columns and re-express the matrices under the same unified vocabu-lary so that the column indices in both matrices correspond to the same word. Moreover, let V represent a m  X  m diag-onal matrix with V ii =1if i is a shared word, i.e., it occurs in both domains, or in other words the associated column is non-zero for both X 1 and X 2 .

In domain X 1 ,wehavesomelabeleddocuments. The partial labels on documents can be described using G 0 1 where ( 1 ) i 1 = 1 if the document expresses positive sentiment, and ( G 0 1 ) i 2 = 1 for negative sentiment. Note that one may also use soft sentiment polarities though our experiments are conducted with hard assignments. We first transfer this document label knowledge to words by learning F in a 2-way clustering, min where the notation Tr( A ) means trace of the matrix A . Here,  X &gt; 0 is a parameter which determines the extent to which we enforce G 1  X  G 0 1 , C 1 is a n  X  n diagonal matrix whose entry ( C 1 ) ii = 1 if the category of the i -th document is known (i.e., specified by the i -th row of G 0 1 )and( C 1 otherwise. Note that if C 1 = I , then we know the class orientation of all the documents and thus have a full specifi-cation of G 0 1 .Letthesolutionbe F 1 ,S 1 , which contains the knowledge to be transferred to X 2 . This transfer is achieved by next solving the following 2-way clustering: The key part here is the second term which enforces the word sentiment polarity on X 2 to be approximately close to F 1 which is learnt from X 1 ; the extent of this approximation is determined by parameter  X &gt; 0. The constraint only applies to the common terms in X 1 ,X 2 as enforced by the diagonal matrix V . The solution for Eq.(2) gives ( F 2 ,S 2 ,G 2 mechanism therefore transfers the document sentiment G 1 of domain X 1 to the document sentiment G 2 in domain X 2 via word sentiment polarities in F 1 , schematically shown below.
The optimization in Eq.(1) can be solved using the follow-ing update rules:
S
The optimization in Eq.(2) can be solved using the follow-ing update rules (updates for S are same as in Eq.(3)): G The algorithm consists of an iterative procedure using the above three rules until convergence. The correctness and convergence of the updating rules can be rigorously proved using the standard auxiliary function approach [4].
The Figure at the top of this page presents preliminary experiments on cross domain sentiment analysis on Ama-zon.com reviews for 4 product types: Houseware-Kitchen (HK), Books, DVDs, and Electronics [2]. Results are shown for two source domains Books and DVDs, and four target domains. As can be seen, as supervision is increased in the source domain in the form of labeled data, performance gains effectively show up in the target domains also. We also note the effect of task relatedness: for example, DVD transfers more effectively than Books to Electronics but less effectively to Houseware-Kitchen. For each source domain, the top 10 highest entropy words as per F 1 are as follows: (1) HK  X  easy return love poor perfect excellent waste disappointed broke clean , (2) Electronics: return excellent price terrible waste perfect highly easy poor returned ,(3)DVD: waste worst bad boring horrible love wonderful enjoy excellent ridiculous ,(4) BOOKS: boring disappointing waste excellent bad wonder-ful poor poorly love easy . Our initial results are promising but more empirical studies need to be done before we can accurately evaluate the potential of our method.
