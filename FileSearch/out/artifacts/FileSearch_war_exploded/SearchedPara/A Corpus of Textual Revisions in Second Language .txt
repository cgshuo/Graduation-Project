 Learner corpora have been playing an increasingly important role in both Second Language Acquisition and Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the lan-guage (Granger et al., 2009); many also annotate text segments where there are errors, and the cor-responding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sen-tences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to sup-port the training of automatic text correction sys-tems (Dale and Kilgarriff, 2011).

Less attention has been paid to how a language learner produces a text. Writing is often an iterative and interactive process, with cycles of textual revi-sion, guided by comments from language teachers. Understanding the dynamics of this process would benefit not only language teachers, but also the de-sign of writing assistance tools that provide auto-matic feedback (Burstein and Chodorow, 2004).
This paper presents the first large-scale corpus that will enable research in this direction. After a re-view of previous work (  X  2), we describe the design and a preliminary analysis of our corpus (  X  3). In this section, we summarize previous research on feedback in language teaching, and on the nature of the revision process by language learners. 2.1 Feedback in Language Learning Receiving feedback is a crucial element in language learning. While most agree that both the form and content of feedback plays an important role, there is no consensus on their effects. Regarding form, some argue that direct feedback (providing correc-tions) are more effective in improving the quality of writing than indirect feedback (pointing out an er-ror but not providing corrections) (Sugita, 2006), but others reached opposite conclusions (Ferris, 2006; Lee, 2008).

Regarding content, it has been observed that teachers spend a disproportionate amount of time on identifying word-level errors, at the expense of those at higher levels, such as coherence (Furneaux et al., 2007; Zamel, 1985). There has been no large-scale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been ana-lyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners  X  for example, whether learners mostly focus on correcting me-chanical, word-level errors, or also substantially re-organize paragraph or essay structures  X  has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the ex-isting learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the  X  X inal version X . Recently, two cor-pora with text revision information have been com-piled (Xue and Hwa, 2010; Mizumoto et al., 2011), but neither contain feedback from language teach-ers. Our corpus will allow researchers to not only examine the revision process, but also investigate any correlation with the amount and type of feed-back. We first introduce the context in which our data was collected (  X  3.1), then describe the kinds of com-ments in the drafts (  X  3.2). We then outline the conversion process of the corpus into XML format (  X  3.3), followed by an evaluation (  X  3.4) and an anal-ysis (  X  3.5). 3.1 Background Between 2007 and 2010, City University of Hong Kong hosted a language learning project where English-language tutors reviewed and provided feedback on academic essays written by students, most of whom were native speakers of Chi-nese (Webster et al., 2011). More than 300 TESOL students served as language tutors, and over 4,200 students from a wide range of disciplines (see Ta-ble 1) took part in the project.

For each essay, a student posted a first draft 1 as a blog on an e-learning environment called Black-board Academic Suite; a language tutor then directly added comments on the blog. Figure 1 shows an ex-ample of such a draft. The student then revised his or her draft and may re-post it to receive further com-ments. Most essays underwent two revision cycles before the student submitted the final version. 3.2 Comments Comments in the draft can take one of three forms: Code The tutor may insert a two-digit code, repre-Open-ended comment The tutor may also provide Hybrid Both a code and an open-ended comment.
For every comment 2 , the tutor highlights the prob-lematic words or sentences at which it is aimed. Sometimes, general comments about the draft as a whole are also inserted at the beginning or the end. 3.3 Conversion to XML Format The data format for the essays and comments was not originally conceived for computational analysis. The drafts, downloaded from the blog entries, are in HTML format, with comments interspersed in them; the final versions are Microsoft Word documents. Our first task, therefore, is to convert them into a machine-actionable, XML format conforming to the standards of the Text Encoding Initiative (TEI). This conversion consists of the following steps: Comment extraction After repairing irregularities Comment-to-text alignment Each comment is Title and metadata extraction From the top of the Sentence segmentation Off-the-shelf sentence Evaluation Precision Recall
Comment extraction -code 94.7% 100% -open-ended 61.8% 78.3% Comment-to-text alignment 86.0% 85.2% Sentence segmentation 94.8% 91.3% Sentence alignment Sentences in consecutive ver-3.4 Conversion Evaluation To evaluate the performance of the conversion algo-rithm described in  X  3.3, we asked a human to manu-ally construct the TEI XML files for 14 pairs of draft versions. These gold files are then compared to the output of our algorithm. The results are shown in Table 3.

In comment extraction, codes can be reliably identified. Among the open-ended comments, how-ever, those at the beginning and end of the drafts severely affected the precision, since they are of-ten not quoted in brackets and are therefore indistin-guishable from the text proper. In comment-to-text alignment, most errors were caused by inconsistent or missing highlighting and background colors.
The accuracy of sentence alignment is 89.8%, measured from the perspective of sentences in Ver-sion 1. It is sometimes difficult to decide whether a sentence has simply been edited (and should there-fore be aligned), or has been deleted with a new sen-tence inserted in the next draft. 3.5 Preliminary Analysis As shown in Table 4, the tutors were much more likely to use codes than to provide open-ended com-ments. Among the codes, they overwhelmingly em-phasized word-level issues, echoing previous find-ings (  X  2.1). Table 2 lists the most frequent codes. Missing articles, noun number and subject-verb agreement round out the top errors at the word level, similar to the trend for Japanese speakers (Lee and Seneff, 2008). At the sentence level, conjunctions turn out to be challenging; at the paragraph level, paragraph organization, sign posting, and topic sen-tence receive the most comments.

In a first attempt to gauge the utility of the com-ments, we measured their density across versions. Among Version 1 drafts, a code appears on aver-age every 40.8 words, while an open-ended com-ment appears every 84.7 words. The respective fig-ures for Version 2 drafts are 65.9 words and 105.0 words. The lowered densities suggest that students were able to improve the quality of their writing af-ter receiving feedback.
 We have presented the first large-scale learner cor-pus which contains not only texts written by non-native speakers, but also the successive drafts lead-ing to the final essay, as well as teachers X  comments on the drafts. The corpus has been converted into an XML format conforming to TEI standards.

We plan to port the corpus to a platform for text visualization and search, and release it to the re-search community. It is expected to support stud-ies on textual revision of language learners, and the effects of different types of feedback. We thank Shun-shing Tsang for his assistance with implementing the conversion and performing the evaluation. This project was partially funded by a Strategic Research Grant (#7008065) from City Uni-versity of Hong Kong.

