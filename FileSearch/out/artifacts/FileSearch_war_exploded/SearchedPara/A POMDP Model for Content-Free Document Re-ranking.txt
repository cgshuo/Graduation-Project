 Log-based document re-ranking is a special form of session search. The task re-ranks documents from Search Engine Results Page (SERP) according to the search logs, in which both the search activities from other users and personal-ized query log for a user are available. The purpose of re-ranking is to provide the user with a new and better order-ing of the initial retrieved documents. We test the system on the WSCD 2014 dataset, in which the actual content of the queries and documents are not available due to privacy concerns. The challenge is to perform effective re-ranking purely based on user behaviors, such as clicks and query reformulations rather than document content. In this pa-per, we propose to model log-based document re-ranking as a Partially Observable Markov Decision Process (POMDP). Experiments on the document re-ranking task show that our approach is effective and outperforms the baseline rankings provided by a commercial search engine.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Session search; POMDP; Retrieval model
The task of session search [6] is to perform document re-trieval by taking in account all queries and user interactions in a session. During a session, the user writes new queries after interacting with the search engine. The interactions include clicking and reading the retrieved documents. Usu-ally all preceding queries, previously retrieved documents, and user clicks are provided in a query log; the search en-gine X  X  task is to retrieve the most relevant documents and rank them by decreasing relevance with a good use of the provided information in the log.

Recently, studies on how to utilize interaction data within a session to improve search engine X  X  retrieval effectiveness have generated a great deal of research [1, 2, 4]. One par-ticular research activity is the Yandex Personalized Web Search Challenge 1 , which is also part of the Web Search Click Data workshop (WSCD 2014). In the WSCD 2014 task, participants are asked to re-rank the top 10 URLs from the Search Engine Results Page (SERP). Both the within-session query logs and the global query logs from the entire corpus are provided, containing document clicking informa-tion, the clicking orders, and dwell time. Specially, due to privacy concerns, there is no actual  X  X ext X  provided in the dataset. Query terms are replaced by unique numeric iden-tification numbers. Documents only show document identi-fication numbers with no content at all (even no numeric ids for terms). For instance, in session #1690, the first query is  X #3791236#2452511#2637985 X  and the top 3 retrieved doc-uments are #25823561 , #14641317 , #23498956. We thus call this dataset a content-free document set.

Such log-based content-free document re-ranking [1, 13] is a special form of session search. The re-ranking task is not to find new documents for a query but to re-order the already retrieved documents to better reflect the most cur-rent information need in the search session. The research challenge is that even with no content and no documents, how we can still be able to model the dynamics in a session and improves session search effectiveness.

In this paper we propose to model document re-ranking in sessions as a Partially Observable Markov Decision Process (POMDP) [12]. The unknown hidden states of the POMDP are the user X  X  decision making states, the belief state is up-dated by user interactions in the session, and the rewards are IR metrics evaluating documents based on the user clicks.
Markov Decision Process (MDP) and POMDP have re-cently attracted attentions from IR researchers to solve a range of IR tasks [2, 4, 8, 10, 12]. While most existing work requires availability of textual content in documents, it is not a requirement in our model. As far as we know, this work is the first to apply POMDP on pure log-based document re-ranking task involving no document content.

The remainder of this paper is organized as follows. In section 2 we introduce our approach that models document re-ranking as a POMDP. Section 3 presents the experimen-tal setup and section 4 shows the results and a discussion. Section 5 concludes the paper.
In this work, we model document re-ranking as a POMDP [5]. A POMDP is composed of agents, states, actions, re-wards and transition functions. An agent takes inputs from the environment and outputs actions ; the actions in turn influence the other states of the environment according to the transition function . A POMDP assigns immediate re-https://www.kaggle.com/c/yandex-personalized-web-search-challenge wards for taking an action at a state. In a POMDP, the states are not known and can only be guessed through a probabilistic distribution with randomness. A set of belief states are used to model the probability that an agent is at a particular state, which are updated through receiving more observations. Below we describe the states, actions, and model optimization in POMDP for document re-ranking. Specifically, we propose to use two types of rewards during optimization; which are the Global reward from the entire query log from all other users and the Local reward based on the current session for one user only.
User behavior in sessions could be driven by different rea-sons. These can be reflected in query reformulations and clicks. Guan et al. [2] have shown how to model these user interactions using an MDP with queries as the states for ses-sion search. Here we propose two hidden decision states for session search. The two states are determined by whether the user is satisfied with the documents yielded from pre-vious run of search or not. They are judged as Relevant (REL) or Irrelevant (IREL). Most prior research approaches user behaviors in a session with quite complex user modeling where the focus is on analysis on task types and user intent [9, 10]. POMDP could be computationally demanding if the user modeling is too complex. This research simplifies the user modeling in session search and largely reduces the scale of the state space. Hence, it is able to model document re-ranking as a POMDP with a low computational complexity.
The notations for the two states are:  X  REL (Relevant): the user finds at least one relevant doc-ument from the previous SERP.  X  IREL (Irrelevant): the user finds the previous SERP ir-relevant.

We estimate the search relevance by the satisfactory clicks (SAT-Clicks) on the documents. A search interaction with at least one SAT-Click whose dwell time exceeds the thresh-old will be considered as relevant, otherwise irrelevant. We notice there are some recent novel modeling of dwell time to predict click-level satisfaction [7], however in experiment, we use a threshold of 400 time units for long dwell time, because it was given by the task as a threshold for relevance.
Generally, we define the re-ranked list of the returned doc-uments in SERP as the action of the search engine agent. In the WSCD 2014 task, each SERP contains 10 documents to be re-ranked. Therefore, the action can be mathematically defined as an ordered vector a = [ d 1 ,...,d 10 ], where d i th document after re-ranking. In other words, each vector a stands for a new ranked list of documents.
 We further group the actions into high-level strategies. These strategic rank actions are:  X  Action of Consistency. Promote ranks for previously retrieved relevant (SAT-Clicked) documents. This action puts emphasis on the whole session consistency of the session-wise information need.  X  Action of Novelty. Demote ranks for previously re-trieved relevant (SAT-Clicked) documents. This action puts emphasis on the document novelty within a session.  X  Action of Demotion Significantly demote ranks for pre-viously clicked non-relevant documents. This action aims to avoid ranking the previous examined non-relevant doc-uments high.  X  Action of No Change. Keep the original ranks of doc-uments.
Without loss of generality, POMDP as a decision process aims to find the best decision of what action to take, based on the rewards estimation at the current state. Therefore, our model optimization is to find the optimal rank action a that maximizes the estimated total rewards R T OT AL , which is also the total estimated document relevance: Generally, we consider the total rewards as a combination of two parts, the long-term Global rewards R G and the short-term Local rewards R L . Hence we re-write the optimization function as:
The Global reward R G represents the estimation of a gen-eral relevance score of the document, which can be estimated using prior knowledge across the entire log from multiple users and multiple sessions. We take the following steps to estimate the general reward of d when it is retrieved accord-ing to query q and by taking action a .  X  First, we consider how much accumulated relevance score have document d gained from the training set with query q , and represent it as P(d | q). This is because the relevance score of a document is judged according to the SAT-Clicks in the dataset. Therefore we can use the likelihood of d being SAT-Clicked by other users in the training set as the estimation of the global reward.  X  Second, if P(d | q) is unavailable, which implies d has not been clicked in the entire training set by any user in any session for q , we use a prior P(d) to estimate the global re-ward. P(d) represents the probability for d to be relevant to any query in the training dataset. In this case, although we cannot directly calculate the relevance between d and q , we use P(d) to estimate the general importance of doc-ument d , which suggests how likely d will be clicked in the entire log.  X  Last, if even P(d) is unavailable, which means d has not been clicked ever in the training set. We use the original rank of document d in the current session to generate its rewards. We calculate the global reward R G using where P ( d a,i | q ) stands for the rewards gained from the i document.

The Local rewards R L represents the document relevance estimation based on the current session. R L depends on be-lieve state s and action a in the POMDP. Therefore, the reward function can be defined as R L = P s  X  S b ( s ) R ( s,a ), where s is the believe state and a is the current action. Q-learning [11] is used to calculate the local rewards. Particu-larly, we calculate R L as Table 2: Document re-ranking example: session #9101134, where REL stands for the relevance scores of the documents according to relevance judgment ground truth.
 where d a,i is the i th ranked document under action a , and docList n  X  j is the previously retrieved document list in the ( n  X  j ) th interaction within the session.  X  ( s ) is a pa-rameter function which will empirically adjusts the rewards assigned to previous SAT-Clicked documents depending on the believe states. We calculate the probabilities of be-lief states b ( s ) as the probabilities of the state generated from the training set. We observe that 5.7% queries lead to SATClicks. We uses SATClicked documents as relevant documents. Hence 5.7% is the maximum likelihood estimate of the belief of State  X  X elevant X . In the similar way, we can calculate the beliefs of state  X  X rrelevant X .

Overall, the optimization for this document re-ranking task can be concluded as : where d a,i , docList n  X  j ,  X  ( s ) are the same as in equation 4.  X  controls the contribution ratio of the previously accumulated rewards. More details about the parameters are illustrated in section 3.
The original dataset contains an entire search log of 27 days from an industry search engine. We separate the dataset into a training set and a test set, while we use the training data for parameter tuning and global reward calculation, and we evaluate the results on the test set. More details about the dataset are showed in Section 3.1.

Our document re-ranking experiments are performed on the test set, in which the click information according to the last SERP for each session is unavailable. The 10 urls of each of these SERPs are given as the original ranking. Our goal is to re-rank these 10 urls such that the relevant ones receive higher positions. In each search session, we use SAT-Clicks in the last SERP as ground truth for evaluation, while all these evaluations are based on the nDCG@10 [3] and precision@1 metrics.
 Table 3: Document re-ranking example: session # 9101134. Global Rewards R G , Local Rewards R L &amp; Total Rewards R T otal for each documents are listed.

Table 1 presents the definition of the relevance judgment used for the dataset. In the dataset, the URLs (documents) are labeled using three grades of relevance. All these rel-evance labels are done automatically based on dwell time. It is noteworthy that, based on the user privacy considera-tion, all the dwell time in the dataset is presented in time units rather than actual seconds, while the data provider did not disclose how many milliseconds exactly each time unit stands for.
Our experiments are based on an anonymized query log dataset supplied by the WSCD2014 workshop 2 . This dataset includes a one-month web search log from a well-known com-mercial search engine. Information available in this dataset includes user ids, queries, query terms, SERP URLs with do-mains, URL rankings and the click informations including dwell time. This query log is fully anonymized, therefore, only meaningless numeric IDs of users, query terms, sessions, URLs and their domains are released. The session log for the first 27 days in the dataset is fully released, hence we do the data partitioning and use the first 24 days X  data as our training set to calculate global rewards, and use the follow-ing 3 days X  data as test set. The following are statistics of the dataset:
Baseline systems that we compare our approach with are shown in the following list.  X  Baseline-Random. Randomly re-rank the 10 URLs in the test SERPs.  X  Baseline-SE The original search engine returned rank list from Yandex. It is a strong baseline since Yandex is a top commercial search engine.
 The runs from our approaches are:  X  Global Rewards A document re-ranking approach only based on the global rewards as defined in Eq. 3. The local rewards estimation in POMDP are not included. This https://www.kaggle.com/c/yandex-personalized-web-search-challenge/data is to evaluate the effect of our Total Rewards model by comparing it with this run.  X  Local Rewards A document re-ranking approach only based on the local rewards as defined in Eq. 4. The global rewards are not included. This is to evaluate the effect of our Total Rewards model by comparing it with this run.  X  Total Rewards Our major run modeling the document re-ranking task as a POMDP, which considers both the global rewards and the local rewards estimations as de-fined in Eq. 5. The original Yandex document score is not provided in the log dataset; we hence use a docu-ment X  X  rank in the Yandex returned rank list divided by 10 as its score. Based on the training data, the optimal weights for P ( d | q ), P ( d ) and Yandex document score are 1, 0.5 and 0.95. For the local reward, the optimal value of  X  is 0.9.The optimal value for  X  is 0.8, and the opti-mal value of  X  ( s ) are  X  ( REL ) =  X  0 . 5 and  X  ( IREL ) =  X  1 which means at state IREL , a document X  X  previous re-ward is treated as a stronger negative feedback to demote document rank compared with state REL .  X  Upper Bound In this run, we use the ground truth to help choosing the strategic action with the most local re-wards. Noticing this run uses the ground truth of docu-ment relevance. The goal of this run is to show the upper bound of how good this framework could achieve if we can always make the best local rewarded strategies.
Table 4 presents the main evaluation results. Improve-ments over the original search engine ranking (Baseline-SE) are also shown. We can see our approach using TotalRewards achieves improvements over the baselines in both Precision@1 (0.014%) and in nDCG@10 (0.006%). However a significant t-test(p &lt; 0.01, one-sided) indicates that the improvement is not statistically significant.

Moreover, this run using total rewards performs better than the approaches using either only global rewards or lo-cal rewards alone. It supports our approach that models the entire rewards function for session search as a combination of both the global rewards and the local rewards. In ad-dition, the UpperBound run boosts the performance even more however with a limited amount and the improvement is statistically significant(t-test, p &lt; 0.01, one-sided). This extra improvement shows the potential ability of our frame-work, while on the other hand the limitation of improve-ments reflects the difficulty of the problem itself, especially when the baseline is the rank results from a mature com-mercial search engine.
 Table 2 and 3 shows an example in more details. Table Table 4: Experimental Results (  X  indicates a significant im-provement over the baseline at p &lt; 0.01 (t-test, one-sided)) 2 shows an example of we are re-ranking the relevant doc-ument #4192947 into a higher position, while table 3 illus-trates how we achieve it by reward calculation considering both Global Rewards R G and Local Rewards R L .
In this paper, we give a POMDP approach to the docu-ment re-ranking task while the document contents are un-available in the search log. We propose two hidden decision states REL and IREL to model user behavior, and success-fully reduce POMDP computational complexity. By con-sidering document re-ranking as a decision making process, we build up this framework handling both global rewards and local rewards to find the optimal rank action of docu-ments, and hence to improve the re-ranking effectiveness in the testing dataset. Experiments show that our approaches outperform the strong baseline  X  Yandex  X  by 0.014%. We believe our approach gives a good novel attempt to utilize POMDP for content-free document re-ranking in sessions. This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations ex-pressed in this paper are of the authors, and do not neces-sarily reflect those of the sponsor.
