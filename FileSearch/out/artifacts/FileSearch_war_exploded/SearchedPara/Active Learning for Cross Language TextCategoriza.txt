 Due to the explosive growth of electronic documents in different languages, there X  X  an urgent need for effective multilingual text organizing techniques. Cross Language Text Categorization (CLTC) is the task of assigning class labels to doc-uments written in a target language (e.g. Chinese) while the system is trained using labeled examples in a source language (e.g. English). With the technique of CLTC, we can build classifiers for multiple languages employing the exist-ing training data in only one language, thereby avoiding the cost of preparing training data for each individual language.

The basic idea under CLTC is the documents in different languages may share the same semantic information [14], although they X  X e in different representations. Previous works on CLTC have tried several methods to erase the language barrier and show promising results. However, despite language barrier, there X  X  another problem for CLTC. That is the differences between cultures, which may cause topic drift between languages. For example, news of the category sports from China (in Chinese) and US (in English) may concern different topics. The former may talk more about table tennis and Liu Xiang while the later may prefer NBA and NFL. As a result, even if the langu age barrier is perfectly erased, some knowledge of the target language still can X  X  be learned from the training data in the source language. This will inevitably affect the performance of categorization. To solve this problem, making use of the unlabeled data in the target language will be helpful. Because these data is often easy to obtain and contains knowledge of the target language. If we can provide techniques to learn from it, the resulting classifier is expected to get more fit for the target language, thereby give better categorization performance.

In this paper, we propose an active learning algorithm for cross language text categorization. Our algorithm makes use of both labeled data in the source lan-guage and unlabeled data in the target language. The classifier first learns the classification knowledge from the source language, and then learns the cultural dependent knowledge from the target language. In addition, we extend our al-gorithm to double viewed form by considering the source and target language as two views of the classification problem. Experiments show that our algorithm can effectively improve the cross language classification performance. To the best of our knowledge, this is the first study of applying active learning to CLTC.
The rest of the paper is organized as fo llows. First, related works are re-viewed in Section 2. Then, our active l earning approach for CLTC is presented in Section 3 and its extension to double viewed form is introduced in Section 4. Section 5 presents the exper imental results and analysis. Finally, Section 6 gives conclusions and future work. Several previous works have addressed the task of CLTC. [2] proposes practical approaches based on machine translation. In their work, two translation strate-gies are considered. The first strategy translates the training documents into the target language and the second strategy translates the unlabeled documents into the source language. After translation, monolingual text categorization is performed. [12] introduces a model translation method, which transfers classifi-cation knowledge across languages by translating the model features and takes into account the ambiguity associated wi th each word. Besides translation, in some other studies multilingual models are learned and used for CLTC, such as the multilingual domain kernels learned from comparable corpora [5] and the multilingual topic models mined from Wikipedia [8]. Moreover, there are also some studies of using lexical databases (e.g. WordNet) for CLTC [1].

All the previous methods have somehow solved the language barrier between training documents and unlabeled documents. But only a few have considered the culture differences between languages. In these works, authors try to solve this problem by employing some semi-supervised learning techniques. [12] em-ploys a self-training process after the model translation. This process applies the translated model to predict a set of unlabeled documents in target language and iteratively chooses the most confident classified documents to retrain the model. As a result, the model is adapted to better fit the target language. [9] proposes an EM based training algorithm. It consists of an initialization step that trains a classifier using translated labeled documents and an iteration step that repeats the E and M phases. In the E phase, the classifier predicts the unknown labels of a collection of documents in the target language. In the M phase, these documents with labels obtained in E phase are used to estimate the parameters of the new classifier. [16] investigates the use of co-training in cross language sentiment categorization. In their work, Chinese and English features are considered as two independent views of the categorization problem.
The common idea of above methods is to automatically label and use the documents in target language. To reduce the noises introduced by classification errors, the documents with low prediction certainty are usually underutilized. In this paper, we consider such documents to contain important information and will explore them through our active learning algorithm. 3.1 Cross Language Text Categorization Given a collection TR e of labeled documents in language E and a collection TS c of unlabeled documents in language C , in the scenario of CLTC, we would like to train a classifier using TR e to organize the documents in TS c . E and C is usually referred as the source language and target language respectively. In this paper, we suppose E is English and C is Chinese. In practice, they can be replaced with any other language pairs.

To solve the language barrier between training and test documents, we can employ a machine translation tool. The translation can be performed in two directions: the first direction translates all training documents into Chinese and the second direction translates all test documents into English. Both approaches convert the cross language problem into monolingual one. In this section, we choose the training set translation approach. First, we translate TR e into Chi-nese, denoting it by TR e -c . Then, we learn a classifier C e -c based on TR e -c . Suppose the translation process gives accurate enough results, C e -c obtains the classification knowledge transferred from English.

C e -c can be applied to the unlabeled Chinese documents directly. Since the documents of same topic in different languages may share some common se-mantics, C e -c may be able to make very certain predications for some Chinese documents by the classification knowledge transferred from English. However, for some other documents, C e -c may get confused, as the class-discriminative in-formation of these documents can X  X  be de tected. The latter case is usually caused by culture differences. For instance, a classifier trained using English sports sam-ples may not be able to recognize Liu Xiang, a famous Chinese hurdle athlete, in a Chinese document. We can then make an observation that Chinese documents, which are uncertain to be classified by C e -c , usually contain culture dependent classification knowledge that can X  X  be learnt from the translated training data. From this observation, we derive the active learning algorithm to improve C e -c . 3.2 Apply Active Learning to CLTC Active learning [11] is a form of learning algorithm for situations in which un-labeled data is abundant but labeling data is expensive. In such a scenario, the learner actively selects examples fro m a pool of unlabeled data and asks the teacher to label.

In the context of CLTC, we can assume an additional collection U c of unla-beled documents in target language (Chinese in this paper) is available, since the unlabeled data is usually easy to obtain. Our algorithm consists of two steps. In the first step, we train a classifier using the translated training set TR e -c ,this classifier can be considered as an initial learner which has learnt the classification knowledge transferred from the source language. In the second step, we apply this classifier to the documents in U c and select out the documents with lowest classification certainty. Such documen ts are expected to contain most culture dependent classification knowledge. We label them and put them into the train-ing set. Consequently, the classifier is r e-trained. The second step is repeated for several iterations, in order to let the classifier learn the culture dependent knowledge from the target language. Figure 1 illustrates the whole process.
In our approach, a basic classification algorithm is required to train the initial classifier. We employ support vector ma chine, as it has been well studied in pre-vious work for active learning [15,6,11]. Note that our algorithm is independent on specific classification techniques.

Given the translated labeled set TR e -c , each example can be represented as ( x, y ), where x  X  R p is the feature vector and y  X  X  1 , 2 ,...k } is the corresponding class label. A classifier learnt from TR e -c can predict the unknown class label for a document d in U c . To measure the prediction certainty, we can refer to the membership probabilities of all possible classes.

However, SVM can X  X  give probabilistic outputs directly. Some tricks have been proposed in [7]. For binary-clas s SVM, given the feature vector x  X  R p ,andthe label y  X  X  X  1 , 1 } , the membership probability p ( y =1 | x ) can be approximated using a sigmoid function, where f ( x ) is the decision function of SVM, A and B are parameters to be estimated. Maximum likelihood estimation is used to solve for the parameters, N + and N  X  are the number of positive and negative examples in the training set. Newton X  X  method with backtracking line search can be used to solve this optimization problem [7]. For multi-class SVM, we can obtain the probabilities through pair coupling [18]. Suppose that r ij is the binary probability estimate of P ( y = i | y = iorj,x ), and p i is the probability P ( y = i | x ), the problem can be formulated as where k denotes the number of classes. This optimization problem can be solved using a direct method such as Gaussian elimination, or a simple iterative algo-rithm [18].

In practice, we employ the toolbox LibSV M [4], which is widely used in data mining tasks [13]. It implements the above methods for multi-class probability estimation. After obtaining the class membership probabilities of a document, we use the best against second best (BVSB) approach [6] to estimate the classi-fication certainty. This approach has been demonstrated to be effective for multi class active learning task [6]. It measu res the certainty by the difference between the probability values of the two classes having the highest estimated proba-bilities. The larger the difference, the higher the certainty is. Suppose c is the classifier, d is the document to be classified, i and j are the two classes with highest probabilities, then we calculate the certainty score using
Based on the discussions above, we describe the proposed algorithm in Algo-rithm 1. Algorithm 1. Active learning algorithm for CLTC In this section, we extend our algorithm to double viewed form. In chief, the source and target language are considered as two views of the classification problem. The same idea was utilized in [16]. 4.1 Two Views of the Problem In Section 3, we convert the cross language problem into monolingual one with the help of a machine translation tool. As illustrated in Figure 2, the translation can be performed in two directions. The first direction translates the training set TR e into Chinese and the second direction translates both the test set TS c and additional unlabeled set U c into English. Each direction gives us a monolingual view of the problem. In Section 3, we apply our active learning algorithm based on the Chinese view. In this section, we will show how to take advantage of both views and extend our algorithm to double viewed active learning.

First, we perform the translation following both directions. As a result, each document is associated with two views: the Chinese view and the English view. We denote the double viewed training set, test set and additional unlabeled set by TR , TS and U respectively. Then, two initial classifiers are trained using TR based on Chinese view and English view individually. We apply both of them to the unlabeled set U .

Since predictions made by the two classifiers are based on individual views of one document, they may have different certainties. As illustrated in Figure 3, the pool of unlabeled documents is split into four regions. In region C ,the English classifier is certain on the documents, while the Chinese classifier is not. In region D , it X  X  the opposite. For these scenar ios, we can employ a co-training [3] approach, which labels documents according to the confident classifier and generate new training examples for the unconfident one. In other words, the two learners can teach each other in some t imes, needn X  X  always ask the teacher. Based on this idea, we present the double viewed active learning algorithm in the next section. 4.2 Double Viewed Active Learning Given a document d and two classifiers C e and C c , we measure whether both classifiers are certain about its prediction by the average certainty, Average Certainty ( d, C e ,C t )=( Certainty ( d, C e )+ Certainty ( d, C c )) / 2 . (5) To measure whether a classifier is more certain than the other, we refer to the difference between their certainties, Certainty Difference ( d , C e , C c )= Certainty ( d, C e )  X  Certainty ( d, C c ) . (6) Our double viewed active learning algorithm is described by Algorithm 2.
After the learning phase, we get two classifiers C e and C c .Asaresult,inthe classification phase we can obtain two predictions for a document. Since both classifiers output class membership probabilities, they can be combined in the following way to give the overall prediction, 5.1 Experimental Setup We choose English-Chinese as our experimental language pair. English is re-garded as the source language while Chinese is regarded as the target language. Algorithm 2. Double viewed active learning algorithm for CLTC Since there is not a standard evaluation benchmark available for cross language text categorization, we build a data set from the Internet. This data set contains 42610 Chinese and English news pages during the year 2008 and 2009, which fall into eight categories: Sports, Military, Tourism, Economy, Information Technol-ogy, Health, Autos and Education. The main content of each page is extracted and saved in plain text.

In our experiments, we select 1000 English documents and 2000 Chinese doc-uments from each class. The set of Englis h documents is treated as the training set TR e . For the Chinese documents, we first randomly select 1000 documents from each class to form the test set TS c , and leave the remaining documents as the additional unlabeled set U c .

As we will use the two views of each document in our algorithm, we em-ploy Google T ranslate 1 to translate all Chinese documents into English and all English documents into Chinese. Th en, for all Chinese or Chinese trans-lated documents, we segment the text with the tool ICTCLAS 2 ,afterwards remove the common words. For all Englis h or English translated documents, the EuropeanLanguageLemmatizer 3 is applied to restore each word in the text to its base form. Then we use a stop words list to eliminate common words.
Each document is transformed into an E nglish feature vector and a Chinese feature vector with TF -IDF format. The LibSV M package is employed for the basic classifier. We choose linear kernel due to its good performance in text classification task. Since we need probabilistic outputs, the b option of LibSV M is selected for both training and classification. The cost parameter c is set to 1 . 0 as default. We use Micro-Average F1 score as the evaluation measure, as it X  X  a standard evaluation used in most previo us categorization research [10,17]. 5.2 Results and Discussions In this section, we present and discuss the experimental results of the proposed algorithms.
 Single Viewed Active Learning. In the first experiment, we would like to verify the effectiveness of our active learning algorithm described in Algorithm 1. An initial classifier is trained using the translated labeled set TR e-c and then applied to the Chinese unlabeled set U c . In each iteration, 10 documents with the lowest prediction certainty are selected and labeled by the teacher. To validate this selecting strategy, we also implemen t another strategy which selects 10 doc-uments randomly for comparison. In each iteration, a new classifier is retrained on the expanded labeled set and its perfo rmance is evaluated on the testing set TS c . The corresponding micro average F1 curves are plotted in Figure 4.
We can observe that, the initial classifier doesn X  X  perform well on the Chinese test set. As the number of iterations increases, the performance is significantly improved. The certainty-based strategy shows an obvious advantage over the random strategy. This verify our assumption that documents with low predic-tion certainty usually contain culture dependent classification knowledge and therefore are most informative for the lea rner. After 20 iterations, the Micro av-erage F1 measure on the 8000 test documents is increased by about 11 percents while the additional cost is to label 200 selected examples.
 Double Viewed Active Learning. In the following experiments, we verify the double viewed algorithm described in Algorithm 2. First, two initial classifiers are trained using the labeled set based on English and Chinese view individually. Then the active learning process i s performed. We set the parameter n to 10, which means in each iteration 10 examples having lowest average certainty are selected and labeled by the teacher; and we set m to 5, which means each classifier labels 5 examples for the other. The certainty threshold h is set to 0 . 8, in order to reduce the error introduced by automatically labeled examples. In each iteration, the two classifiers are retrained and applied to the test set. We combine their predictions based on each view to get the overall prediction. Figure 5 shows the micro average F1 curves of the Chinese, English and overall classifiers. The curve of the single viewed algorithm is plotted as well for comparison.

We can observe that, the English classi fier generally has better performance than the Chinese one, a possible reason is that more noises are introduced in Chinese view due to the text segmentation process. The overall classifier has highest accuracy, as it combines the information from both views. All the three classifiers generated by double viewed algorithm outperform the one of the single viewed algorithm. Because in each iteration they get 10 more labeled examples (each classifier automatically labels 5 examples for the other).

In our double viewed algorithm, the classifiers learn from each other and the teacher. We would like to investigate the effect of the two approaches individually. This can be done by set the parameter n and m in Algorithm 2. We first set n to 10 and m to 0, then set n to 0 and m to 5. The corresponding curves are showed in Figure 6.

As we can see, learning from the teach er makes a significant contribution to the improvement of the performance, while the effect of learning from the partner is weaker. The latter maybe ca used by two reasons: first, there may be some errors introduced by the automatically labeled examples; second, since the Chinese and English views of one document are not completely independent, the C and D region illustrated in Figure 2 may be very limited. However, learning from the partner is still helpful, and it reduces the labor of the teacher to achieve the same performance.
 Comparison. In Table 1, we present the detailed classification results of our algorithms, comparing with two basic machine translation based methods. The first one, denoted as MTC , translates the training set TR e into Chinese and trains a classifier; the second one, denoted as MTE , trains a classifier in English and translates the test set TS c into English. In addition, we also build a mono-lingual classifier ( ML ) by using all documents in U c as training data. The ML method plays the role of an upper-bound, since the best classification results are expected when monolingual tr aining data is available.

We can observe that, the ML classifier has the best per formance as expected, since it X  X  trained on the labeled data in the target language, so that there X  X  no drawback caused by language barrier or cultural differences. Comparing with the two basic machine translation methods MTE and MTC , our active learn-ing algorithms, both single viewed and double viewed, significantly improve the classification performance of each class. The double viewed algorithm has better performance than the single viewed one, as it combines the information from both views and makes use of the automatically labeled examples. In this paper, we proposed the active learning algorithm for cross language text categorization. The proposed method can effectively improve the cross language classification performance by learning from unlabeled data in the target lan-guage. For the future work, we will incorporate more metrics in the selecting strategy of active learning. For instance, can we detect the scenario in which the classifier is pretty certain but actually wrong? If such examples can be detected and labeled for retraining, the classifier will be further adaptable for the target language.
 Acknowledgments. This work is supported by National Natural Science Foun-dation of China (60803050, 61132009)and BIT Team of Innovation.
