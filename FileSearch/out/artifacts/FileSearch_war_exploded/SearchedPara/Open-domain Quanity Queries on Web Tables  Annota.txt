 Over 40% of columns in hundreds of millions of Web ta-bles contain numeric quantities. Tables are a richer source of structured knowledge than free text. We harness Web tables to answer queries whose target is a quantity with natural variation, such as net worth of zuckerburg , bat-tery life of ipad , half life of plutonium , and calo-ries in pizza . Our goal is to respond to such queries with a ranked list of quantity distributions, suitably represented. Apart from the challenges of informal schema and noisy ex-tractions, which have been known since tables were used for non-quantity information extraction, we face additional problems of noisy number formats, as well as unit specifica-tions that are often contextual and ambiguous.

Early  X  X ardening X  of extraction decisions at a table level leads to poor accuracy. Instead, we use a probabilistic con-text free grammar (PCFG) based unit extractor on the ta-bles, and retain several top-scoring extractions of quantity types and numerals. Then we inject these into a new collec-tive inference framework that makes global decisions about the relevance of candidate table snippets, the interpretation of the query X  X  target quantity type, the value distributions to be ranked and presented, and the degree of consensus that can be built to support the proposed quantity distribu-tions. Experiments with over 25 million Web tables and 350 diverse queries show robust, large benefits from our quantity catalog, unit extractor, and collective inference.
Web search engines enhance  X  X rganic X  search results with data from structured knowledge bases (KBs), curated from diverse sources using information extraction [13] and entity annotation [5] techniques. With very few exceptions [10, 19, 3, 1], a vast majority of work on extracting typed text segments, entities, attributes and relations involve discrete symbols and not measurable quantities 1 . And yet, the Web is as rich a source of quantities as it is of symbolic knowledge. There are hundreds of millions of information-rich HTML ta-bles on the Web. In our sample, a full 40% of their columns exclusively contain quantities. On the other hand, quantity-seeking Web queries are well-served only in verticals like  X  X hen you can measure what you are speaking about, and express it in numbers, you know something about it. X   X  Lord Kelvin.
 shopping, food and travel where structured databases con-tain columns that are matched against the query, aggre-gated, and presented as quantities throughout. With one mild exception [20], the emerging Web tables literature [17, 4, 7, 11] does not regard quantity extractions as any different from discrete symbolic value extractions.
Our goal is to bridge the gap between the noisy presen-tation of quantities in Web tables and open-domain, one-off quantity-seeking queries. Asking for an attribute of an entity is a common case, e.g., average revenue of microsoft , half-life of plutonium and mass of pluto . Note that we are primarily interested in seeking quantities with uncer-tainty and imprecision , unlike  X  X osmic truths X  like the num-ber of primes under 1000, or the value of  X  . For this reason, our queries cannot be answered from Wikipedia Infoboxes, which mostly offer point answers, and without directly accu-mulating evidence from the Web. To this end, we present a new, robust, open-domain, quantity search system QEWT.
QEWT is a large system with very many details. Here we distill three major contributions: a quantity catalog and a new table column unit annotator based on a probabilis-tic context free grammar (PCFG), a query response model based on ranked value distributions, and a new algorithm for collective consensus inference. Our unit catalog, query workloads, labeled data, and code are publicly available at http://www.cse.iitb.ac.in/~sunita/wwt . tables are notoriously challenging to extract. Unit expres-sions may be ambiguous ( X  X . X ) or even missing, syntactic clues though present might be noisy, and units may not fol-low clear-cut representations in table headers. We compiled a quantity catalog QuTree, and designed a new unit anno-tator for table columns based on probabilistic context free grammars (PCFGs). The PCFG technique exploits a di-verse set of clues, including co-occurrence statistics between quantity types, units and phrases mined from an unlabeled corpus of table headers.
 URLs or documents. In expert or entity search [2], it is a list of entities. Queries that seek uncertain quantities require a very different treatment. In particular, extracted quantities can be distinct from each other for extraneous reasons, or there may be systematic variations (which isotope of Pluto-nium, or Microsoft revenue in which year). A more intuitive uniform output representation is a distribution over quan-tities . We describe techniques to represent, score and rank such distributions h , given the query.

Earlier work [3,  X  X CQ X  X  is a restrictive special case of our framework. Each response in QCQ is a single interval , and a model was trained to score these using labeled data. Here we can create distributions and intervals in a query-driven manner without labeled data. Since our target is open-domain query answering, we ensure that our response model can handle data at arbitrary scales and from arbitrary distributions via a data-driven distance metric.
 approach is a collective model for extracting quantities from raw Web tables. Instead of  X  X ardening X  extraction decisions on quantity columns eagerly, we keep around multiple un-certain values and units, which are finally collectively re-solved over multiple tables at query time. We depend on a consensus model that defines a joint distribution h over all candidate extractions to assign collectively resolved proba-bilities on the relevance of each snippet and the extracted unit and value. Another valuable input is from a classifier that provides a distribution over the target quantity type. We train the classifier via a novel method of tapping our huge unlabeled table corpus.
 350 queries spanning three query benchmarks compiled from QCQ [3], InfoGather [20] and World Bank data [18]. The base corpus tables is accessed through the Google API at re-search.google.com/tables and our corpus of 25 million tables extracted [11] from a commercial Web crawl of 500 million pages. Our experimental results can be summarized as: We present an overview of our system QEWT in Figure 1. The user submits a quantity query q which has two parts. The first part a q is a sequence of word/s that are a ver-bal description of the response quantity type (e.g., distance from sun, speed, annual revenue). The second part e q is a free-form sequence of words that indicate an entity for which a quantity attribute is being sought (e.g. Pluto, Concorde, Microsoft). The query words are submitted to indices over Web table corpora and a set of tables retrieved. Figure 3 shows sample tables for the query co2 emissions of china with e q = china and a q = co2 emissions . A snippet gener-ator module processes the retrieved tables for potential an-swer snippets and attaches a relevance score to each. Next, a number/unit extractor parses the noisy snippets to output an uncertain list of values and units based on a unit ontology. The uncertain extractions from all tables are collectively re-solved to get a distribution over the target quantity. Finally the response as a continuous distribution or ranked quantity intervals is output to the user. We next present an overview of the main components.
 cial crawl of 500 million Web pages, from which we ex-tracted and indexed 25 million non-decorative HTML ta-bles, and tables collected per-query via the API provided by research.google.com/tables. We extract from each table zero or more top rows as the header for each of its columns, and selected text from the page embedding the table as the context as described in [11]. A type interpreter labels table columns as numeric or textual; in our corpus of 25 million tables we recognized 40% of the table columns as numeric. Unit catalog QuTree. Starting from Category:Units of -measurement in Wikipedia, we created a unit catalog we call QuTree. It has 44 quantity types such as Length , Area , Speed , and 750 units. Each quantity type has a canonical unit, and other units come with conversion factors to/from the canonical unit. A fragment of QuTree is shown in Fig-ure 2. Each catalog unit is associated with one or more full names (e.g., kilometre per hour), one or more symbols (e.g., kmph, km/h) and an optional list of lemmas to ac-count for the common variant names of a unit, (e.g., meter-metre, kmph-kilometre per hour). QuTree also includes the concept of a  X  X ultiplier X  to denote dimensionless quantities, with unit instances like thousand, million and billion, which capture scales of measurements. QuTree can be downloaded from goo.gl/542L2Y. It is to quantities what YAGO [14] is to discrete entities.
 Query target type. In the user query, a q is a textual hint at a quantity type t from the quantity catalog. We must build an estimate of the distribution Pr( t | q ) = Pr( t | a example, for attribute co2 emissions Pr( t | a q ) is expected to assign high probability to the quantity type mass . In Sec-tion 3.1 we show how to train Pr( t | a q ) via an innovative use of the table corpus, and without requiring tedious manual labeling. We will see in Section 6.4 that this signal is very useful to the collective extractor in finding relevant answers. Figure 3: Sample tables responding to query e q = china and a q = co2 emissions .
 didate tables that match the query keywords q in the header, context, or body. On each candidate table, we use a snip-pet generation module to match e q to a row header, a q to a column header, and identify candidate cells for quantity extraction. Observe from Figures 3 and 4 the challenges of identifying potentially relevant rows, e.g.,  X  X hina X  vs.  X  X hina (mainland) X , and relevant columns, e.g. multiple columns might be relevant and column headers may not match a q at all as shown in Figure 4. Therefore, relevance of a snippet is uncertain. We use a random variable R s to denote the uncertainty in the relevance of a snippet s and associate a score r s  X  [0 , 1] to denote Pr( R s = 1). We will describe snippet generation and relevance score assignment further in Section 3.2.
 Number and unit extraction. From each snippet s , we ex-tract a float value from the table cell, and annotate it with a unit from QuTree based on signals in the column header and the table cell. Note that a unit node in QuTree be-longs to exactly one quantity type, so a unit annotation also gives its quantity type. Both these tasks are highly chal-lenging because of the extreme diversity of writing down quantities on the web: multipliers expressed separately in the column header, locale-dependent use of commas, peri-ods, and spaces, scientific notation, etc. can confound any simplistic local extractor. Unit extraction from headers has its own challenges of dealing with drastic formatting dif-ferences in both column headers and cell quantities E.g., one column header may say  X  X otal Emissions (1000 X  X  tons carbon) X  whereas another may say  X (billion metric tons of CO 2 ) X   X  these are different, whereas yet another table say-ing  X  X O2 emissions (kt) X  is comparable to the second case. In Section 5 we present our design of a parser based on a context free grammar and a rich feature set. The output of this step is a weighted list of possible extractions of values and units from each snippet.
 to the centerpiece of QEWT: a novel collective inference pro-cedure that simultaneously estimates the relevance of each snippet and each of its possible extractions, and the target quantity type basing its judgement on a global estimate of the distribution of values around the quantity. We describe this in Section 3.5.
 input variation is the issue of answer representation. The vast body of work on search has to rank discrete items such as URLs (pages) [8] or entities [2]. Here our true response is an uncertain quantity, but how best is this shown to the user? Rather than take an inflexible stand, QEWT sup-ports three different types of answer formats each of which is suited for a different kind of quantitative queries. QEWT X  X  internal response representation is a quantity distribution, but QEWT can provide simplified digests in the form of ranked value intervals [3], or degenerated to ranked point values (in case the query has little or no uncertainty, such as physical constants, or are categorical queries in disguise, such as the number of USB ports in a laptop).
As shown in Figure 1, the collective inference module, which we describe in this section, mediates between the type distribution, snippet relevance uncertainty, and the uncer-tainty of snippet unit and value extractions. Section 3.1 is about estimating the target type of the query. Section 3.2 discusses scoring snippets wrt the query. Section 3.3 de-scribes how a single snippet can lead to many possible ex-tractions of a unit and value, and how to score these ex-tractions. Section 3.4 deals with the design of distributions for consensus inference. Finally, Section 3.5 describes the consensus inference algorithm itself.
Given query q = ( a q ,e q ), its target quantity type, which is uncertain, is modeled as a distribution Pr( t | a q ). A good estimate of Pr( t | a q ) is vital for generating relevant responses.
A baseline method may look for matches of the attribute words with type and unit names/lemmas in our quantity catalog. It may correctly find the target type of queries like length of nile or year of crash . But many queries like usa co2 emissions , walton net worth , miami rain-fall , and ebay revenue have no match in the catalog, lead-ing to loss of recall. Surprisingly, there is also precision loss. E.g., query fan speed which matches well the type speed in QuTree but misses the correct target type frequency .
Ideally, we would like to recognize that query word rev-enue targets type money amount and that distance , length , height and width all target the type length (dimension).
This form of association between (potential query) words and quantity types is evident in a fraction (but still a large absolute number) of tables, with column headers like  X  X idth in inches X ,  X  X o2 emissions in kiloton X ,  X  X verage rainfall in inches X ,  X  X an speed (rpm) X , and  X  X nnual revenue ($ mil-lion) X . To take advantage of these headers, they must be mapped to types, which is, in general, a highly nontrivial job (see Section 5). However, starting from over 25 mil-lion tables, we could find 1.1 million headers which could be mapped with high precision (99%) to types, using sim-ple rules discussed in Section 5.1, and restricting to headers with an exact and unique match with a unit in QuTree.
We thus get (automatically) labeled instances with ob-served features consisting of the words x k in the header not included in the unit, and a quantity type t k derived by gen-eralizing its extracted unit. E.g., from the header text  X  X n-nual revenue ($ million) X  we extract an instance with bag of words x = { annual , revenue } and type money amount . These labeled instances are used to train a logistic regres-sion classifier for Pr( t | q ). 3-fold cross validation gave 94% accuracy, higher than using PMI models [16]. If the posterior entropy was large (e.g., for a query on refractive index ), we prefered the  X  X imensionless X  type in favor of other types.
Each table retrieved from the index generates one or more snippets. We represent the uncertainty of relevance of a snippet s to q by R s,q , or R s if q is fixed. The uncertainty of relevance is reflected in a score r sq or r s  X  [0 , 1]. Based on query q , the local evidence in favor of relevance ( R is r s , and the local evidence in favor of irrelevance ( R is 1  X  r qs .

Given query q = ( a q ,e q ) and a candidate table T , we describe how we match these two parts of the query to gen-erate one or more snippets from T . We view web tables as vertical stores of entity and their attributes 2 . Accordingly, a candidate snippet is generated by matching e q to a row r in a column c e and matching a q to a different column c a generating the snippet from cell ( r,c a ). For most queries, we find a single snippet per table but sometimes the entity e could match multiple rows of a table, or a q could match multiple columns of T . For example, in Figure 3 the first three web tables generate a single snippet for query co2 emissions of china . In contrast, for the query refrac-tive index of flint glass , in Figure 4, the second table matches the entity  X  X lint glass X  in the last two rows whereas the first web table provides the attribute  X  X efractive index X  in three different forms over columns 2, 3, and 4. We next elaborate on our method for generating such snippets.
The candidate tables are required to match all high-IDF terms in the query in either the body, header, or context. We omit a discussion of horizontal tables for simplicity [6]. This ensures that our candidate snippets are minimally rele-vant. Thereafter, we separately measure the similarity score sim( e q ,r,c e ,T ) of entity e q to cell ( r,c e ) and similarity score sim( a q ,c a ,T ) of attribute string a q to column c a  X  X  header. These match scores are custom designed to depend on matches beyond the immediate cells to include T  X  X  context, title, and other parts of its body. In [11] we presented the design of a segmented similarity function that shows how to measure the relevance of a table X  X  column to a query keyword while combining column-specific match with matches from the rest of the table. We use this segmented similarity function as the match function: sim( e q ,r,c e ,T ) and sim( a q ,c
The snippets from a table are generated as follows. We first find the ( r,c ) with the best value of sim( e Call it ( r  X  ,c  X  e ). Fix c  X  e as the entity column. We then find the column c that is numeric and has maximum similarity sim( a q ,c,T ). Call it c  X  a . Next, as snippets we select those cells r s ,c s for which: This criteria makes sure that we select all snippets s = ( r ,c s ) whose match is close enough (within ) of the best possible match from T , provided the entity match is at least  X  . In our experiments we used 0.2 for both and  X  . The reason we have a minimum match threshold for entities and not for attributes is because the number of numeric columns in a table is typically much smaller than the number of rows. We depend on the global consensus model to prefer the columns that are correct. We use the entity and attribute match scores to assign the relevance score of a snippet as r = (sim( a q ,c s ,T ) + sim( e q ,r s ,c  X  e ,T )) / 2.
We model each snippet s as extracting exactly one value and one unit (therefore, type). However, given the noise in extraction, we model the extracted value and type as uncer-tain, but drawn from a finite, usually small set of alternative extractions. This set of possible extractions is indexed by j . For example, consider the web table in Figure 1 obtained in response to query height of washington monument .
 Table 1: An example web table for query height of washington monument .

We get one snippet from this table at the cell (1,1), but we have uncertainty over its value: does 168,7 equal 168.7, or a list of two numbers 168 and 7. This gives rise to two possible values: 168.7 and 168? Also, from the header Height (m), the unit parser extracts three possible units: meter, million, and mile. So, we consider all six combinations of unit and value as possibilities for j . Each j is attached with a score g sj that we generate as follows: First, parse the value in the cell based on different locale specific parsers and get the set of successful parses of the value v s 1 ,...,v sn . Second, use the CFG unit parser described in Section 5 to get a list of units along with their scores: ( u s 1 ,w 1 ) ,..., ( u sm set j consists of the mn possible cross product of value,unit combination where each j is associated with: a value v sj unit u sj and therefore its type t sj , and a confidence score g extracted unit, and each unit maps to a unique quantity type. For convenience, we can convert a specific extraction to some canonical unit that we associate with the quantity type.) Summarizing, for each snippet s , there are two hidden random variables:
In abstract terms, a quantity query should be answered with a type (unit) and a value distribution h . For each potential response type  X  , we will build a distribution h with the usual constraint R  X  h  X  (  X  ) d  X  = 1. h  X  will be estimated from a set of values { v i } , but, based on the previous sections, each value v i is associated with a probability  X  i that the value should actually contribute to h . This is expressed with the notation h  X  (  X |{ ( v i , X 
Simple parametric distributions are not a good choice for h , given that we have to deal with numbers of arbitrary scale, coming from arbitrary domains, with a high level of extraction noise. Therefore, we focus on non-parametric dis-tributions. We briefly mention two standard forms of h  X  the sake of completeness.
A natural option is a kernel density estimate: where  X  i is a kernel width parameter that may be the same for all i as a special case, and Z = P i  X  i . A problem that we faced with a fixed width is that they do not adapt well to numbers of arbitrary scales. Consider the query that seeks to find the half-life of Plutonium. Plutonium has many iso-topes with extremely diverse half-lives: 14, 88, 6560, 24100, and 376000 years. Naturally these are stated approximately, with errors that are commensurate with the magnitude of the quantity being expressed. We associated a different width  X  i around each data point that increases with the scale of the point v i as max(10% of v i , minimum non-zero gap between numbers).
Another popular non-parametric density estimator is based on Wavelets. Wavelets are believed to outperform kernel density estimators at representing discontinuities and local variations, features rampant in our data. We use the Haar wavelet as our basis function. Due to lack of space, we re-fer the reader to standard textbooks on the topic and omit further details.
Now we are in a position to verbally express how collective consensus is formed, given a query. We will state that var-ious scores  X  X hould be large X , with the understanding that, in a collective scheme, we want some sort of aggregate (say their product) to be large. We will then translate this de-scription into a formal model. In the rest of this section, we fill in the details of the above framework.

Each snippet s that potentially contributes information to the response of a query has the associated hidden variables R s and J s . The collective inference algorithm will build estimates of the posterior distributions over R s ,J s through the procedure shown in Figure 5. Specifically, let  X  r s be the posterior probability of relevance of snippet s . If the snippet is relevant, then the probability of extraction j being valid is  X  g sj , with P j  X  g sj = 1 for each s . We also maintain, for all possible response types  X  , the value distributions denoted h (  X  ).

The consensus update  X  sj  X  g sj r s Pr( t sj | q ) h \ s also be interpreted as a  X  X eave-one-out X  validation of v sj
Multiple extractions from a table are not iid. QEWT han-dles them, but we skip discussing them for simplicity. the backdrop of other contributing values, as explained be-low. (Think of index i here as  X  s,j  X  .) If v i is dropped while making the estimate h  X  , we will write it as h \ i  X  (  X |{ ( v A natural notion of consensus among { ( v i , X  i ) } can be ob-tained by dropping each ( v i , X  i ) in turn, forming the esti-mate h \ i  X  (  X |{ ( v i , X  i ) } ), and evaluating h \ i tells us how strongly v i itself is supported by the rest of the observed values.

The lines after the consensus are to update  X  r s and  X  g normalized posterior probabilities. After (sufficient) conver-gence, we are left with these posterior probabilities, from which we construct h  X  on all values (with their posterior probabilities). The resulting h  X  and/or values with their posterior probabilities are then sent to the answer interval representation module, described next.
Expressive densities described in the previous section give us a great deal of power to fit the extracted values. How-ever, the user may wish to view something simpler than such a general density. Following QCQ [3], we propose ap-proaches to present ranked intervals as the query response. We associate each interval I with a lower limit ` I and up-per limit u I and a p I that denotes the probability that the answer lies within [ ` I ,u I ]. The final answer is a set I of non-overlapping intervals I 1 ,...,I k such that P I  X  X  p A baseline approach would be to cluster the relevant val-ues, but the relevance of a value is not known for sure, and the number of clusters is also unknown. Therefore, we need more sophisticated methods.

For this part snippet boundaries are not relevant, so we denote our input as a set of ( v i , X  i ) pairs where a i corre-sponds to some sj and  X  i =  X  g sj .
I can also be thought of as a mixture of uniform distri-bution with density Thus, one method of obtaining I is to approximate a more expressive density (Section 3.4) to a mixture of uniform dis-tributions. Let h (  X |{ ( v i , X  i ) } ) be a density (such as a Ker-nel density). We find our desired intervals I 1 ,...,I that within each I = [ ` I ,u I ], the error of approximating density h by a constant is within a tolerance while min-imizing the number of such intervals. We measure error in terms of KL divergence between h and h I ; this makes the error per interval as R u I v = ` points ` I = u I , and assign an error of 0 for such intervals.
Since we restrict the interval boundaries to values in V , we can easily find the optimal set of intervals in O ( | V | ) time using a simple range segmentation algorithm.
One problem with the above method is the difficulty of finding one tolerance to fit all query types. We next present a method based on the minimum description length (MDL) principle [12] that is more adaptive to per-query variations in value distributions.

Assume we have a set S q of values sampled from all rows of the snippet columns for query q . For example, in Fig-ure 3 the set S q will be sampled from all distinct co2 emis-sion values over all countries. Our goal is to find intervals I = I 1 ,...,I k to represent the point answers V = { ( v Note that each v i  X  S q . Using MDL, we interpret this as a compression problem from a hypothetical sender of V to a receiver, with the intervals serving as a compression model for V . We assume S q is known to both parties. The cost of sending V has two parts: the cost of the model, which in our case is the set of intervals, and the cost of sending the data V given model.

The cost of sending the data V given intervals I = I 1 ,...I is the sum of the cost of data in each interval. Let S I the set of numbers from S q that lie in interval I = [ ` I As per I , all values within [ ` I ,u I ] have probability r of being relevant. Each entry v i in V is relevant with prob-ability  X  i , thus the expected number of bits for sending v is:  X   X  i log r I  X  (1  X   X  i ) log(1  X  r I ). Other values in S irrelevant and require  X  log(1  X  r I ) bits each. Summing up, data cost over all intervals is  X  P I  X  I  X  log(1  X  r I )  X  X  S I | log(1  X  r I ).

The model cost is the cost of sending the parameters r I and the boundary [ ` I ,u I ]. Following MDL, we encode r by finding a good fit distribution p ( r I ), and setting the cost as  X  log p ( r I ). Since r I is between 0 and 1, a natural choice is the Beta distribution whose density is Pr( p ; m,n ) = and m,n are parameters. We need to choose m,n so as to minimize the cost of sending r I s over all intervals and all queries. Since S q is typically larger than V , we choose the prior parameters as m = 1 ,n = 2 so that smaller values of r get lower cost. The set of intervals that minimize the sum of data and model cost can be found in O ( | V | 2 ) time using the same segmentation algorithm as in Section 4.1.
So far we have abstracted the role of the extractor that finds likely units from table columns. Given the noise in tables bearing quantities, the module that annotates table columns with units needs to be fairly sophisticated. textual header x our goal is to extract the units (if any) from x that associate with the numbers in the column. Figure 6 Figure 6: Example unit annotations (in yellow) on table columns. shows some example unit annotations to table columns. As shown, we handle four types of unit occurrences:
We initially attempted to extract based on a set of intu-itive rules capturing various lexical clues and matches with the unit catalog, since all previous work on quantities have relied on rule-based unit extractors [3, 20].

Let the term match refer to the longest sequence of to-kens in a header x that matches a unit name, symbol, or lemma in QuTree. Annotating a unit based purely on a match leads to many false positives because there are sev-eral words like  X  X n X ,  X  X t X ,  X  X ast X ,  X  X  X ,  X  X tone X ,  X  X oint X  that are unit names/symbols but are commonly used as non-unit words. Therefore, we defined rules that require additional evidences for a match in x to be tagged a unit: 1. after-in : A match after in is a unit e.g.  X  X rice in $ X , 2. bracketed : A match within brackets is a unit e.g 3. type-name : A match tied to a unit U whose par-Figure 7: An example parse for the column header Wind speed (mph / km/h) . SimpleUnit is abbreviated to SU, and unary nodes like CUnit and AtomUnit have been deleted for compactness.

On deploying these rules on our table corpus we were sur-prised by their large number of errors. For example, the rule after-in wrongly labels the header  X  X cores in last match X  as unit Last (a unit of Volume), and cannot disambiguate between units Carat, Knot, and Kiloton in header  X  X a-pacity in kt X  since  X  X t X  is a symbol for each of these three units. The rule bracketed wrongly labels word  X  X ec X  in header  X  X opulation (Dec 2006) X  as unit Decade. Also, it cannot differentiate between units in headers  X  X uration(s) X  and  X  X ear(s) X  where the unit annotation of the first should be Seconds but the second should have no unit. The rule type-name wrongly annotates header  X  X ength of song(m:s) X  as Meter whereas the correct annotation for  X  X  X  is Minute . The rule is particularly bad for compound units, for example in  X  X nergy density by volume (MJ/L) X , volume helps anno-tate L as Liter . Rules for compound units are not easy because they require simultaneous labeling of many differ-ent parts of the headers. Finally, these set of rules have poor recall, for example, the header  X  X O2 Emissions 2000 thousands of metric tons of carbon X  from the third table in Figure 3 is not covered by any of the three rules.

We therefore explored alternative models that combine multiple soft evidence from additional resources and are more expressive in their modeling of compound units and other unit patterns like multipliers. The rampant ambiguity of a mention with language words and a unit and with other units, implied that just depending on clues derived from the header string may not be adequate. Further, since the sys-tem of derived units is based on a well-defined grammar, it seemed natural to use a grammar to drive the extrac-tion. Feature-based context free grammars seemed perfect for the task. Regular grammars cannot capture patterns that encourage alternative units to be of the same type, as in  X  X eters per second (m/s) X . In this approach we use a discriminative Context Free Grammar (CFG) with scores attached to each possible pro-duction in the grammar [15]. In Table 8 we show the CFG (without scores) for unit extraction from table headers. The grammar supports all four different types of compound units illustrated in Figure 6. In Figure 7 we present an example parse tree for the header:  X  X ind speed (mph / km/h) X  that this grammar supports.

In general, the grammar allows many possible parses of a header x . Each parse-tree has a score that is additive over each of the productions in the tree. A production P of the form: R ::= R 1 R 2 is scored as: where ( i,j ) and ( j + 1 ,k ) are the text spans in x that R and R 2 cover, respectively. When R 2 is empty ( j = k ), we have a unary production. The feature vector f can be used to capture various clues that help identify units. We list the set of features we used in sections 5.2.1,. . . 5.2.5. The weight vector w corresponding to the features could be trained using the discriminative framework of [15], but since the number of features in our case was small (seven) and easily interpretable, we fixed their values manually.
The task of annotating units in an input x reduces to the task of finding the tree of production with the highest sum of scores, and outputing the list of units under the  X  X nit X  nodes in the tree. Since we have scores, as against hard rules, we can output multiple extractions each weighted with a score. The well-known Inside-outside algorithm [13, 9] for inference in PCFGs can be easily extended to output the top-K highest scoring extractions in polynomial time.
We next list the features used in f ( P, x ,i,j,k ). We first list the features for leaf-level productions where R is a unit name U which could be either a Multiplier or an AtomicUnit, and finally in Section 5.2.5 present features for other internal nodes of the parse tree. We will use the short form x ij denote the token subsequence x i ...,x j of x .
The TF-IDF similarity of x ij to various parts of unit U 0 entry in QuTree, including U  X  X  name, lemmas, and symbol is an important feature for productions that tag x ij as a unit name. Another feature analogous to the type-name rule, is the TF-IDF similarity of the words in the parent quantity type of U and tokens in x excluding x ij . For example, in the header  X  X ength (m) X , this feature will apply for production Meter := x 22 since the word  X  X ength X  matches the parent type of unit Meter. However, it does not fire for production million := x 22 .
When each of the after-in and bracketed rules apply on a x ij we fire a feature when R is the unit state  X  X nit X . To allow for occasional extraneous tokens, (e.g. length in approx meter), we also fire these tokens with a tolerance of one token.
We exploit ontologies such as WordNet that provide rel-ative frequency of word usage to get disambiguation clues between units and non-units and between different symbols of the same unit. WordNet provides relative frequency of various senses of nouns in its ontology. Each noun sense s is associated with a list of word-forms and a frequency f ( s ). Let S = { s 1 ,...,s m } be the set of senses whose word-forms match x ij . If one of these senses, say s , is a descen-dant of the Quantity type in Wordnet and matches the base name of the unit U in QuTree, we fire a feature with value matching senses in Wordnet, with more than 99% frequency on the non-unit meaning of last as  X  X inish X  or  X  X nd X . This provided strong evidence to not tag  X  X ast X  as a unit.
Another strong clue for correctly assigning a unit U to x is obtained from the presence of strongly co-occurring words in x outside unit words x ij . For example, for x =  X  X idth in m X , and unit U = metre for token x 22 , we can exploit the fact that  X  X idth X  often co-occurs with Length units. We use the unlabeled corpus of table headers to train a query type classifier as described in Section 3.1.2. Using this, we assign a feature with value Pr( t U | x ) where t U is the parent type of unit U . We next add a set of features to handle compound units. For each compound unit type: unit-multiplier pair (e.g. dol-lar [million]), and ratio/product of two units (MJ/L) we add bias terms so that atomic units are preferentially related via these operations instead of being treated as a list of unre-lated units. Finally, when two units belong to the same type, we add a bias term so that they are treated as alternatives (e.g. metre | feet).
After describing our testbed in Section 6.1, we perform controlled studies on collective extraction (  X  6.2), choice of value distribution h (  X  6.3), query type inference (  X  6.4), re-sponse interval generation approaches (  X  6.5), and the effect of various unit extractors (  X  6.6). two sources: a commercial Web crawl with 500M pages sim-ilar to ClueWeb09 4 from which we extracted 25 million ta-lemurproject.org/clueweb09 bles [11], and tables returned by research.google.com/tables in response to queries.
 QCQ: 28 diverse queries used in [3].
 WorldBank: 172 queries on four quantity attributes of coun-InfoGather: 146 queries used in InfoGather [20], on three Queries with ground truth are at goo.gl/542L2Y.
 the performance of QEWT is measured. Most commonly, ground truth G is available as a set of values, or one or a few ranges. Similar to QCQ, we assume that the query specifies a multiplicative confidence band . I.e., if a true point value is v , the user would be satisfied with a value in [(1  X  ) v, (1 + ) v ] (Our results are with = 0 . 02). Mean-while, QEWT presents a distribution h . The probabilistic precision of h is which is the total area matching a ground truth value band. Recall is defined as the fraction of G supported by h 5 . The probabilistic F1 score is the harmonic mean of probabilistic precision and recall. Given the total area under h is always 1, any attempt to recall all ground values by  X  X mearing out X  h will result in large swathes of h never being touched by a ground truth band. Since the form of h shown to the user is a set probability weighted set intervals, we use h (Equation 3) for h . QEWT potentially improves upon two simpler baselines. In CollectiveHard , for each snippet s , we greedily and lo-cally choose the extraction with the best scoring unit and value. I.e., J s is pinned. The only uncertainty is in rel-evance R s , for which h is used. For Pr( t | q ) we use the hard scores from Section 3.1.1 which is 1 when a q matches the catalog, and uniform otherwise. In Independent , h is missing but the rest of the setting is exactly the same as in Collective . The default representation for h is variable width kernel density. Figure 9 shows that the full power of QEWT X  X  collective extraction is vital. In workload In-foGather, CollectiveHard suffers particularly severely be-cause of the hard method of query type assignment on the corporate tax rate queries.
With the value of h established, in Figure 10 we com-pare choices for h : variable and fixed width kernel density, wavelets, and a degenerate distribution with impulse prob-abilities only at point values seen in the snippets, essen-tially treating quantities as discrete symbolic extractions. Whereas there is no clear winner among the continuous dis-tributions, they are all better than point histograms, which
Note, an alternative definition of probabilistic recall is pre-cision times the fraction of G supported by h . But that causes precision to be counted twice in F1 and is redundant.
Figure 9: Comparing collective extraction models. are bad for queries like net worth of a celebrity, or revenue of a company that are often stated approximately; it gets no benefit of consensus from values that are close by (e.g. $ 111.5 billion versus $ 111.05 billion).
Figure 10: Comparing different models for h(.)
We compare the dictionary match approach (  X  3.1.1) with the data-driven approach (  X  3.1.2). Out of the 35 distinct attribute names spanning the three workloads, dictionary match correctly identified only ten of the query types. In contrast, our data-driven approach correctly classified 34 of them with confidence at least 0 . 66. Some example attributes and their type and score from this method appear in Fig-ure 11. When the data-driven unit extractor is plugged into the collective answer extractor, the result is a significant boost to end-to-end accuracy, as shown in Figure 12.
Figure 13 compares MDL-intervals (  X  4.2) vs. uniform mix-ture approximations (  X  4.1). The x-axis is proportional to the (assigned) marginal cost of an additional segment (one uni-form distribution). The y-axis shows F1. At low segment cost, responses are over-fragmented, losing recall. At high segment cost, precision is lost.

Part of the reason for MDL X  X  superiority is the guidance from the reference value distribution. Figure 14 shows an example, for the query  X  X orporate tax rate of united states X . There are 12 distinct correct answers in the tight interval [39 . 05 , 39 . 34] (%), shown as green crosses. The candidate ex-tracted values V = { ( v i ,p i ) } are shown as red squares (some wide off the mark). About 100 reference values forming S sampled from tables on corporate tax rates of several coun-tries are between 20% and 40%, shown as blue diamonds. Because of the large reference density in [35 , 45], the inter-vals (with probabilities) created by MDL are [39.2,39.4]: 0.61 , [35,35]: 0.1, [40,40]: 0.09, and [39.0,39.1]: 0.09. The most likely one is shown as a green box, containing most true values. In contrast, the top response from Uniform-Mix is the low-precision interval [35.0,45]: 0.91, because it Figure 11: Query attributes a q , labeled type  X  and Pr(  X  | a q ) via logistic classifier. Figure 12: Query F1 accuracy under different query-type classification models. Figure 13: Comparing the MDL and UniformMix models for generating final intervals in the answer for increasing Segment cost. has no query-specific method of measuring relative distances between points.
Here we evaluate different parsers for extracting units dis-cussed in Section 5 and also study their impact on end-to-end query processing.

In order to estimate the extraction accuracy of unit parsers, we created a dataset of 617 table headers from our corpus that have been manually labeled with the correct units. In Figure 15 we plot the extraction accuracy of different parsers under varying settings. Rule is a rule-based parser that in-cludes among others the rules of Section 5.1 refined to label only unambiguous matches. This has an accuracy of only Figure 14: An example showing how a reference set of numbers helps the MDL approach select the cor-rect intervals. 40%, and almost all of this error is due to poor recall  X  the precision of the rule-based parser is 99%. In contrast, our proposed CFG-based parser that we call QuantCFG achieves an accuracy of 82%. We created another parser called Sequence that uses all the features of QuantCFG but not its grammar, and chooses the unit with highest score w . f among all word sequences that match QuTree. The drop in accuracy to 74% establishes the importance of the grammar. The next three bars establish the importance of features in Sections 5.2.2, 5.2.3, 5.2.4 by reporting accuracy of QuantCFG without various subsets of them.
We next assess the impact of the unit extraction accuracy on the final answer quality in the end-to-end system. In Figure 16, we compare F1 scores of the answer on the three workloads on three different parsers described above: the rule-based parser, the feature-based sequence parser, and our QuantCFG parser. We observe that it is necessary to use parsers both with high precision and high recall for getting quality answers. Figure 16: Comparing different parsers on their im-pact on the answer quality in the end to end system.
Moriceau [10] was among the earliest to formalize quan-tity search, and provide some initial notions of temporal trends and aggregation of values. A more extensive sys-tem was built by Wu and Marian [19,  X  X &amp;M X  X . Banerjee et al. [3,  X  X CQ X  X  proposed the quantity interval ranking problem. None of these exploited source HTML tables, none delayed per-snippet extraction decisions, and none proposed an inference procedure that collectively estimated snippet relevance, snippet extractions, and the value distribution. SCAD [1] collected quantities while satisfying domain-guided numeric constraints between them (e.g., a laptop screen is wider than it is tall). But SCAD did not use a unit ex-tractor or consensus inference as in QEWT. Zhang et al. [20,  X  X nfoGather X  X  also extract units and values from Web tables. They focus on identifying correspondences among tables based on column types. They do not model uncertain value distributions. Their aggregation/consensus is based on exact match of values, which we demonstrate as weaker than QEWT X  X  value distribution model. InfoGather extracts column units using rules, which cannot handle compound units and noisy headers, unlike our PCFG unit extractor. Their  X  X uery X  resembles a table completion task, with  X  100 entities, units, and scales explicitly provided. In contrast, QEWT is a robust, open-domain system for ad-hoc, single-entity queries.
 search grants from the Indo-German Max Planck Centre for Computer Science (IMPECS) and from Yahoo! Research.
