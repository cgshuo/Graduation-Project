 We consider the general k -way clustering problem in signed social networks where relationships between entities can be either positive or negative. Motivated by social balance the-ory, the clustering problem in signed networks aims to find mutually antagonistic groups such that entities within the same group are friends with each other. A recent method proposed in [13] extended the spectral clustering algorithm to the signed network setting by considering the signed graph Laplacian. This has been shown to be equivalent to finding clusters that minimize the 2-way signed ratio cut. In this paper, we show that there is a fundamental weakness when we directly extend the signed Laplacian to the k -way clus-tering problem. To overcome this weakness, we formulate new k -way objectives for signed networks. In particular, we propose a criterion that is analogous to the normalized cut, called balance normalized cut, which is not only theoretically sound but also experimentally effective in k -way clustering. In addition, we prove that these objectives are equivalent to weighted kernel k -means objectives by choosing an appropri-ate kernel matrix. Employing this equivalence, we develop a multilevel clustering framework for signed networks. In this framework, we coarsen the graph level by level and re-fine the clustering results at each level via a k -means based algorithm so that the signed clustering objectives are opti-mized. This approach gives good quality clustering results, and is also highly efficient and scalable. In experiments, we see that our multilevel approach is competitive to other state-of-the-art methods, while it is much faster and more scalable. In particular, the largest graph we have considered in our experiments contains 1 million nodes and 100 million edges  X  this graph can be clustered in less than four hun-dred seconds using our algorithm.
 Category : I.5.3 -Computing Methodologies -Pattern Recognition -Clustering GeneralTerms : Algorithms, Experimentation Keywords : Clustering, Signed Networks, Social Balance Theory, Signed Graph Kernels
Social network analysis has gained considerable attention in recent years. These networks can be modeled as graphs with nodes and edges, where nodes indicate individual ac-tors and edges indicate the relationships between actors. In many real social networks, there are not only positive rela-tionships but also negative relationships. For example, on-line news and review websites such as Epinions and Slashdot allow users to approve or denounce others. These kinds of networks can be modeled as signed networks where a posi-tive relationship is denoted by a positive edge weight while a negative relationship is denoted by a negative edge weight.
It has been shown that signed networks tend to have a particular structure that is derived by the fact that rela-tionships between entities tend to follow so-called X  X alanced X  patterns in signed networks [3, 9]. Motivated by social bal-ance theory, the clustering problem in signed networks is to find antagonistic clusters such that entities within the same cluster have a positive relationship with each other and en-tities between different clusters have a negative relationship with each other. While much research on graph clustering has been conducted on unsigned graphs which contain only positive relationship information, it is yet to be explored how many existing clustering algorithms can be extended to signed networks. In particular, since the goal of clus-tering changes when we consider signed networks, existing unsigned clustering methods cannot be directly applied to signed graph clustering.

Due to the massive size of real networks, scalability is an important issue. For unsigned networks, several scalable clustering algorithms have been proposed based on the mul-tilevel framework [7, 11]. However, to the best of our knowl-edge, there is no scalable multilevel clustering algorithm for signed networks.

In this paper, we formulate new k -way objectives for signed networks based on social balance theory. In particular, we propose a criterion that is analogous to the normalized cut, called balance normalized cut, which is not only theoretically sound but also experimentally effective in k -way cluster-ing. Furthermore, we show that these objectives are equiv-alent to a general weighted kernel k -means objective. Using this equivalence, we develop a fast multilevel clustering al-gorithm for signed networks. In this multilevel algorithm, clustering can be optimized level by level very efficiently.
The following are the contributions of our paper:
This paper is organized as follows. In Section 2, we briefly review some related work, and in Section 3, we state some preliminaries. In Section 4, we show a weakness of the signed graph Laplacian and introduce our new k -way objectives and kernels for signed networks. Also, we prove the equivalence between these objectives and a general weighted kernel k -means objective. In Section 5, we embed our algorithm into a multilevel framework, and explain our multilevel clustering algorithm for signed networks. We show our experimental results in Section 6, and state our conclusions in Section 7.
The clustering problem in signed networks can be dated back to the 1950s. Cartwright and Harary [3, 9] introduced the notion of social balance. They defined balanced triads, and showed that a network which follows the balance notion can be clustered into two perfect antagonistic groups. This balance notion was generalized by Davis [4], who defined the concept of a weakly balanced network, and showed that a network can be partitioned into k antagonistic groups if it is weakly balanced. In Section 3.3, we formally state balance theory and weak balance theory.

Motivated by (weak) balance, many researchers have tried to develop algorithms for clustering signed networks. For example, Doreian et al. [8] proposed a local search strat-egy which is similar to the Kernighan-Lin algorithm [12]. Yang at el. [17] proposed an agent-based method which ba-sically conducts a random walk on the graph. Anchuri at el. [1] proposed hierarchical iterative methods that solve 2-way frustration and signed modularity objectives using spectral relaxation at each hierarchy. On the other hand, Nikihil at el. also consider the signed graph clustering problem, though their formulation is motivated by correlation cluster-ing [2]. They proposed two approximation algorithms and proved approximation bounds. Different from these works, our work starts with considering k -way signed cut and as-sociation objectives which directly measure the quality of clusters. Thereafter, we obtain k clusters by globally opti-mizing these signed objectives.

The most relevant work to ours is a spectral method pro-posed by Kunegis et al. [13], who showed that a signed ver-sion of the 2-way ratio-cut problem can be solved by consid-ering the so-called signed graph Laplacian. However, there is a fundamental weakness when we directly extend the signed Laplacian to the k -way clustering problem, which we will discuss in detail in Section 4.1.

Another state-of-the-art framework for signed graph clus-tering is based on a low rank model proposed by Hsieh et al. [10]. They observe that matrix completion on signed net-works can be thought as recovering the missing structure of the network. Thus, in this framework they first complete the network and derive low rank factors (for example, top eigenvectors) as representations of the signed network. Af-terward, they run k -means on the low rank factors to derive the clustering. We will compare this state-of-the-art ap-proach with our proposed multilevel clustering algorithm in Section 6.

Scalability is always an important issue for graph clus-tering algorithms. For unsigned networks, many scalable graph clustering algorithms make use of a multilevel frame-work, such as Metis [11] and Graclus [7]. In the multilevel framework, the input graph is repeatedly coarsened level by level until the graph size becomes small enough. An initial clustering is then performed on the coarsest graph. Finally, the clustering result is refined as the graph is uncoarsened level by level. Furthermore, Dhillon et al. [7] showed that a weighted graph clustering objective is mathematically equiv-alent to a general weighted kernel k -means objective, and utilizing this equivalence into the multilevel framework can make the clustering procedure very efficient. However, the algorithms used in [7] cannot be directly applied to signed networks, and the modifications are not trivial either.
In this paper, we use capital letters to denote matrices, lowercase bold letters to denote vectors, and lowercase italics to denote scalars. We use x c ( i )todenotethe i th element of the vector x c when the vector has a subscript; otherwise, we use x i to denote the i th element of the vector x .
In this section, we first review spectral clustering of un-signed networks in Section 3.1. In Section 3.2, we introduce weighted kernel k -means which has been shown to be effec-tive in optimizing graph cut objectives of unsigned networks. In Section 3.3, we state some basic concepts for signed net-works. Finally, in Section 3.4, we introduce a state-of-the-art spectral method that solves a 2-way signed network cluster-ing problem. Let us consider k -way clustering on unsigned networks. One of the most popular approaches to solving this prob-lem is to consider some objective, such as graph association or graph cuts, that quantifies the quality of clusters, and derive a clustering result by maximizing or minimizing the objective function. Here are some well-known objectives:
In addition, under the special case k = 2, the ratio cut objective (1) is equivalent to the following problem: where the 2-class indicator x has the following form: with |  X  1 | , |  X  2 | &gt; 0. See [5] for detailed proof.
Note that all the above formulated problems are NP-hard problems [15]. In practice, these problems are usually solved with spectral relaxation, which is known as spectral cluster-ing [16]. Specifically, if we consider the relaxed problems by dropping the combinatorial constraint { x 1 , ..., x k then the top k eigenvectors will yield the optimal solution. One way to round { x 1 , ..., x k } to a valid indicator set, we 1  X  i  X  n , and do k -means clustering on these n vectors.
Though spectral clustering works quite well in practice, it suffers a scalability issue since computing eigenvectors can be costly. In particular, the normalized objective usually squeezes the spectrum of the matrix to a small range, which makes the computation much more expensive due to the smaller eigengaps.
Weighted kernel k -means is a generalized version of k -means [7]. Given a set of vectors v 1 ... v n ,theweighted kernel k -means objective is defined as follows: where w i is a nonnegative weight of each vector v i ,  X  is a non-linear mapping, and m c is the weighted centroid of  X  which is defined by:
Like the traditional k -means algorithm, weighted kernel k -means computes the closest centroid for every node, and Algorithm 1 : Weighted Kernel k -means
Input : v 1 ... v n : data vectors, K : kernel matrix, k :
Output : {  X  c } k c =1 : final clustering assigns the node to the closest cluster. After all the nodes are considered, the centroids are updated. Given the Ker-between  X  ( v i )and m c , denoted as D ( v i , m c ), can be sim-plified as follows:
D ( v i , m c )= K ii  X  This procedure is summarized in Algorithm 1.

It has been shown that all the objectives stated in Sec-tion 3.1 are equivalent to the weighted kernel k -means ob-jective by choosing appropriate kernels [7]. Therefore, we can use the weighted kernel k -means algorithm to attempt to optimize these graph clustering objectives without com-puting eigenvectors.
A signed network can be represented as an adjacency ma-trix A that describes relationships between entities. For-mally, the adjacency matrix A is defined as follows: We can break A into its positive part A + and negative part A  X  . Formally, A + ij =max( A ij , 0) and A  X  ij =  X  min( By this definition, we have A = A +  X  A  X  . For convenience, in the remaining part of this paper, we will use the term  X  X etwork X  as an abbreviation of signed network unless we specify it is unsigned. In addition, similar to the conven-tion in spectral clustering, we will focus our discussion on undirected signed networks, i.e., A is symmetric.
The most prevailing theory related to signed networks is social balance . The intuition of social balance can be inter-preted as  X  X  friend of my friend is my friend X ,  X  X n enemy of my friend is my enemy X , and  X  X n enemy of my enemy is my friend X . We say that a graph is balanced if any part of the graph does not violate this intuition. It is known that if the network is balanced, then it has a clusterable global structure:
Theorem 1 (Balance Theory [3, 9]). A network is balanced iff (i) all of its edges are positive, or (ii) nodes can be clustered into two groups such that edges within groups are positive and edges between groups are negative.
In addition, Davis [4] proposed a weaker notion of balance, called weak balance, that generalizes social balance. They relax the balanced relationships by allowing an enemy of one X  X  enemy to still be an enemy. Under such relaxation, they show that a network which satisfies weak balance rules can be clustered as follows:
Theorem 2 (Weak Balance Theory [4]). A network is weakly balanced iff (i) all of its edges are positive, or (ii) nodes can be clustered into k groups such that edges within groups are positive and edges between groups are negative. We then say that a network is k -weakly balanced iff it can be divided into k antagonistic groups.

Motivated by Theorem 2, we now formally state the k -way clustering problem in signed networks as follows. Given a signed network, we are asked to partition the network into k clusters such that most edges within clusters are positive and most edges between clusters are negative. In addition, to avoid partitioning where most clusters contain only a few nodes, we prefer that clusters are of substantial size/volume. A similar desire occurs in unsigned network clustering when we consider normalized cut as the criterion [15].
Recently, Kunegis et al. [13] proposed a spectral method for clustering of signed graphs by defining a signed graph Laplacian. Let  X  D be the diagonal absolute degree matrix, i.e.,  X  D ii = n j =1 | A ij | , then the signed Laplacian in [13] to be  X  D  X  A . The signed Laplacian  X  L is always positive semidefinite by the fact that  X  x  X  R n , Now let us define k -way ratio cut for signed networks. Given a signed network G , we define k -way ratio cut to be equal to the sum of positive edge weights of edges that lie between different clusters and the sum of negative edge weights of all edges lie within the same cluster, normalized by each cluster X  X  size as in (1) (also see (13) in Section 4.1).
In [13], it has been shown that the 2-way signed ratio cut objective can be formulated as an optimization problem with a quadratic form: where the 2-class indicator x has the following form: with |  X  1 | , |  X  2 | &gt; 0. This objective has a similar form to the original unsigned ratio cut objective (3). However, by examining (7), we can verify that only negative edges within the same cluster ( A ij &lt; 0and x i = x j ) and positive edges between clusters ( A ij &gt; 0and x i = x j ) will contribute to the objective function. See [13] for more details.
In this section, we propose new criteria and objectives, in-cluding graph association and cut, for general k -way signed graph clustering. We can derive the signed graph kernel cor-responding to each objective. In addition, we show that we can use the weighted kernel k -means algorithm to optimize these objectives, by selecting the proper kernel matrix.
Before we introduce our proposed objectives, we show why the signed Laplacian is hard to extend to the k -way cluster-ing problem. While the signed Laplacian can be properly used in 2-way clustering, it is not clear how we can extend this definition to general k -way clustering. One intuitive solution is to directly extend (7) to the following k -way ob-jective: However, unlike in unsigned networks, this direct extension suffers a weakness. To explain this weakness more clearly, let us first consider the unsigned ratio cut. We can observe that we use different representation for indicators ( x in (3) and { x 1 , ..., x k } in (1)) for 2-way objective and k -way objec-tive. This generalization is valid because based on a 2-way objective (3) with well-defined indicator x , we can properly generalize it to k -way objective (1) by selecting another ap-propriate representation for indicator set { x 1 , ..., x that the cut criterion remains the same. Similarly, now given a 2-way signed ratio cut objective (7), we aim to find an ap-propriate setting of the indicator set { x 1 , ..., x k } the k -way objective (8) with such setting also minimizes the signed ratio cut. Nevertheless, the following theorem proves that this generalization is hopeless no matter how we choose as our indicators { x 1 , ..., x k } :
Theorem 3. There does not exist any representation of { x 1 , ..., x k } such that the objective (8) minimizes the general k -way signed ratio cut (defined in Section 3.4).

Proof. Recall that to define a proper representation of { x 1 , ..., x k } , we need to pick two representatives a, b  X  R a = b , x c ( i )= a if i  X   X  c and x c ( i )= b if i/  X   X  c the objective (8) minimizes the signed ratio cut.
To begin with, we rewrite the k -way objective (8) by plug-ging(6)intoitasfollows: We can prove the theorem by showing that no matter how we choose a, b , we will incorrectly punish some favorable clustering patterns when minimizing (9).

First we argue that b must be 0. This is because when two nodes, i and j , are both not in  X  c , we will never know whether i and j are in the same cluster or not by only seeing information about x c . Therefore, in this case, we have to assign x c ( i )= x c ( j ) = 0 to ensure a zero penalty, or other-wise we may possibly penalize some patterns that conform to weak balance.

However, given b =0,any a =0cannotmaketheob-jective (9) correctly minimize signed ratio cut. To see this, consider that i  X   X  c and j/  X   X  c , in which case we should put penalty only when sgn( A ij ) = 1. However, any fixed choice of a will lead to a penalty regardless of sgn( A ij ). Thus, no matter how we select a, b , we will always punish some desir-able clustering patterns. This proves the impossibility of a direct generalization of signed ratio cut objective by using (8).
Notice that the above proof, however, does not apply if we restrict k = 2. If there are only two clusters, i, j /  X   X  implies that i, j are in the other cluster simultaneously. In this case, we can directly punish this clustering assignment if sgn( A ij )=  X  1. Therefore, under the special case k =2, the first argument that b needs to be 0 in the proof is no longer valid. However, for general k&gt; 2, b needs to be 0 as shown in the proof. This condition precludes the direct extension of the signed Laplacian to the k -way clustering problem.

To fix this basic theoretical flaw, we now propose alter-nate objectives that (i) follow the weak balance clusterabil-ity stated in Theorem 2, and (ii) are valid for general k clustering.
One way to solve the signed cut/association objectives is to use spectral relaxation as traditional spectral clustering does. However, as described in Section 3.1, this approach fails to scale to very large networks due to computational issues. Thus, we now argue that similar to unsigned network clustering, we can use k -means like algorithm to optimize the objectives, by showing the following theorem:
Theorem 5 (Equivalence of Objectives). For any signed cut or association objective, there exists some corre-sponding weighted kernel k -means objective (with properly chosen kernel matrix), such that these two objectives are mathematically equivalent.
Proof. We prove this by showing that both weighted ker-nel k -means objective and signed cut/association objectives can be represented as trace maximization problems. We will also show that given a signed objective, we can construct the kernel matrix such that the corresponding weighted kernel k -means objective is equivalent to the signed objective. First we consider the weighted kernel k -means objective (4). As proved in [7], this objective can be rewritten as the fol-lowing trace maximization problem: where Y is an (weighted) indicator matrix, W is the diagonal weight matrix for the nodes, and K is the kernel matrix.
Now we show how to derive the corresponding trace max-imization given the graph cut/association objectives in Sec-tion 4.1. Generally, every objective can be written in the following form: k where W is some diagonal weight matrix, and the goal is to either maximize or minimize (17).

First, we represent the problem as a maximization prob-lem. We use the following equivalence when the original problem is a minimization problem:
Next, we enforce the positive semi-definiteness on  X  K in (18) by the fact that: Therefore, every problem in form (18) can be rewritten as: where  X  K is a kernel matrix. This is because we can always choose some sufficiently large  X  in (19) such that  X  K =  X W  X  K is positive semi-definite.

Finally, (20) is in fact equivalent to a trace maximization problem since: (20)  X  max y where Y  X  R n  X  k ,whose c th column is also the normalized indicator  X  y c for cluster  X  c , i.e.  X  y c = y c / ( y
Hence, by choosing the kernel matrix to be K = W  X  1  X  KW  X  1 (21) is equivalent to (16). This proves the theorem.
This equivalence implies that we can define some implicit kernel matrix  X  K given a signed cut/association objective by choosing a proper  X  . In addition, running weighted kernel means with W  X  1  X  KW  X  1 as the input kernel matrix is equiv-alent to optimizing the corresponding signed cut/association objective, since both of them optimize the same trace max-imization problem.

To explain how to derive the corresponding signed graph kernel more clearly, we take balance normalized cut (15) as an example. With the notations in the proof, the objective is to minimize (17) with  X  K = D +  X  A and W =  X  D .By (18), we can rewrite it as a maximization problem by setting  X  K = W  X   X  K =  X  D  X  ( D +  X  A ). The problem is further equivalent to (20) by setting  X  K =  X W +  X  K =  X   X  D  X  ( This becomes our balance normalized cut kernel, since we can always choose a large enough  X  such that  X  K is positive semi-definite. Also, by (21), if we set another kernel matrix K as the input of weighted kernel k -means, we can optimize balance normalized cut efficiently.
In Section 4.2, we have seen that there are two general approaches to optimize signed graph clustering objectives -spectral clustering and weighted kernel k -means algorithm. Typically, k -means based algorithm is more efficient and scalable than spectral clustering. However, k -means algo-rithm is also easier to be trapped into qualitatively poor lo-cal optima. In this section, we will use a popular multilevel framework which allows kernel k -means based algorithms to converge to better local optima.

Our approach can be viewed as a generalization of Graclus [7] for the signed network setting. The multilevel approach is a divide-and-conquer method that includes three phases: coarsening, base clustering and refinement.
In the coarsening phase, given the input graph G 0 ,we generate a series of graphs G 1 ...G , such that | V i +1 for all 0  X  i&lt; .Givenagraph G i , the coarsened graph G i +1 is generated as follows: At the beginning every node is unmarked. We then visit each node in a random order, and try to merge two nodes in G i into one supernode in G i +1 with the following strategy: If the node, say x ,isalready marked then we skip it. Otherwise, we consider all of x  X  X  unmarked neighbors y , and select the one such that the edge weight A i xy is the largest (where A i is the adjacency matrix of
G i ). We then merge x and y as a supernode z ,andall the neighbors of x and y are added to neighbors of z .We mark both x and y . If all neighbors of x are marked, then we simply mark x and do not merge it into any node. After all nodes are visited, the supernodes and non-merged nodes become the vertex set of G i +1 .

The matching strategy is quite intuitive since the larger the edge weight between x and y , the more likely that x and y are in the same cluster. There can be some variants of the matching strategy such as visiting nodes ordered by their positive degree and so on.
When the original graph is coarsened to a small enough graph, we can directly perform clustering on the coarsest graph G . We take two kinds of approaches to do base clus-tering on G : Minimize balance normalized cut of A with spectral relaxation, or perform unsigned graph clustering on A + using region-growing algorithm as in Metis [11]. Here are some pros and cons between these two approaches. Considering balance normalized cut using spectral relax-ation usually gives a better initialization, but it could be slow if G is still very large. The latter might occur if the original network is power-law so that the size reduction at each level might not be significant. On the other hand, the region-growing algorithm used in Metis is very efficient since it requires no eigenvector computation. However, since this method does not consider signed edges in G , it is possible to derive a base clustering result such that both positive and negative edges are dense within clusters. This problem is not critical, however, since we will refine the clustering results with consideration of signed edges in the refinement phase. Therefore, for the base clustering method, we use Metis on A + in our experiments.
After we derive the base clustering result in G ,werun the refinement algorithm to derive clustering results in G G  X  2 , ..., G 0 . Thus, given a clustering result in G i ,the goal is to get a clustering result in G i  X  1 . To do this, we first project the clustering result in G i to G i  X  1 as the initial clus-ters. In other words, for x  X   X  c in G i , all the nodes in which were merged to x at the coarsening phase are assigned to  X  c . After having the initial clusters, we refine the cluster-ing result by running weighted kernel k -means. As shown in Theorem 5, we can choose a suitable kernel of A i such that optimizing the weighted kernel k -means objective is equiv-alent to optimizing the appropriate signed graph criterion. In our implementation, we choose balance normalized cut kernel as our kernel matrix. Notice that since at each level i we have a good initialization by projecting the clustering from level i  X  1, weighted kernel k -means usually converges quickly.

The quality of clusters is dominated by the local opti-mal of the weighted kernel k -means objective. To make the clustering result better, we can use a local search strategy, which allows k -means to converge to a better local optimal. We incorporate this local search strategy with the standard batch k -means algorithm. Basically, we alternatively run the batch k -means and incremental k -means at each refine-ment level. More details of this ping-pong procedure can be found in [6].
In Theorem 5 we have seen that we can choose a suffi-ciently large  X  to construct a kernel matrix K . However, in practice, if  X  is too large, it makes the k -means algorithm harder to escape a poor local optimal. As a result, the clus-tering result could be bad. On the other hand, a small  X  which does not make K positive semi-definite can still be used for k -means algorithm since K might still possibly pro-vide a pretty accurate distance measurement. Observing this fact, we develop a procedure for finding an appropriate  X  for K .
 We begin with some small  X  and derive a corresponding K as the input of our ping-pong procedure. This K could be effective for distance measurement in k -means even if it is not positive semi-definite. However, it is also possible that k -means algorithm cannot converge with such K due to the lack of positive definiteness. If we observe lack of convergence, we change K by increasing  X  and repeat the ping-pong procedure.

On the other hand, if no move occurs in batch k -means, it might be a signal that the  X  is too large to make k -means jump out from the local optimal. So, in this case, we change K by decreasing  X  and repeat the ping-pong procedure. We do not change  X  in other cases.
Now let us discuss the computational complexity of our k -means based refinement procedure. In Algorithm 1, the bot-tleneck of computation is to compute the weighted distance D ( v i , m c ) for every pair ( i, c ). If we observe the weighted distance formula in Equation (5), the first term K ii is a constant for fixed i , so we only need to compute last two terms when reassigning nodes to clusters. Now consider the balance normalized cut kernel K =  X   X  D  X  1  X   X  D  X  1 D +  X  D  X  1  X  D  X  1 A  X  D  X  1 . We notice that the first two terms in diagonal matrices, which only contribute values for all K these to (5) and simplifying all expressions, we have:
D ( v i , m c )  X  where  X  c is a constant for fixed c :
Therefore, when running weighted kernel k -means in Al-gorithm 1, we can first compute and store each  X  c in O ( time, where m is the number of edges in the network. Af-ter that, for each node we compute D ( v i , m c )foreach using (22), which takes O ( m + nk ) time. Finally it takes additional O ( n )timetocompute C  X  ( v i ). If the number of iterations is t , the time complexity of Algorithm 1 is only O ( t ( m + nk )). The same time complexity can be derived similarly for incremental k -means.

We can furthermore make refinement stage more efficient by only considering refinement on boundary nodes. Specifi-cally, at each level, we only refine nodes that have neighbors in different clusters in initial clustering. Considering only boundary nodes usually leads to a similar clustering result to one derived by considering all nodes, but this makes the whole procedure much faster.
In the first part of this section, we demonstrate the ef-fectiveness of the graph objectives and kernels proposed in Section 4.1. In the second part, we show that the multi-level approach is faster than other state-of-the-art methods, and it yields satisfactory clustering results. In addition, we show that our approach is scalable to networks with millions of nodes and edges.
We now compare the graph objectives and kernels pro-posed in Section 4.1. In particular, we show that the pro-posed balance ratio cut and balance normalized cut kernels areeffectiveinsolvingthe k -way clustering problem.
Criteria and Kernels . In Section 4, we showed that each proposed objective corresponds to a kernel by repre-Criterion Kernel Signed Laplacian  X I  X   X  L Normalized Signed Laplacian  X   X  D  X  1 +  X  D  X  1 A  X  D  X  1 Positive Ratio Association  X I + A + Positive Ratio Cut  X I  X  L + Ratio Association  X I + A Balance Ratio Cut  X I  X  ( D +  X  A ) Balance Normalized Cut  X   X  D  X  1  X   X  D  X  1 ( D +  X  A )  X  D  X  1
Table 1: Criteria and kernels considered in experiments. senting the objective as in (19). Now we select the fol-lowing kernels for comparison: Positive Ratio Association ( PosRatioAssoc ), Ratio Association ( RatioAssoc ), Pos-itive Ratio Cut ( PosRatioCut ), Balance Ratio Cut ( Bal-RatioCut ) and Balance Normalized Cut ( BalNorCut )as representatives of our proposed methods, and Signed Lapla-cian ( SignLap ), Normalized Signed Laplacian ( NorSign-Lap ) as the current state-of-the-art spectral methods [13]. We summarize these kernels in Table 1. For each kernel, we properly choose the smallest  X  such that K is positive semi-definite, by computing the spectrum of the matrix.
Experimental Setup and Metrics . To compare the effectiveness of each kernel, we create some synthetic net-works with the following procedure. We begin with a com-plete 5-weakly balanced network A com , in which group sizes are 100 , 200 ,... 500 respectively. We then uniformly sample some entries from A com to form a weakly balanced network A , with two parameters: sparsity s and noise level .The sparsity s represents the percentage of edges sampled from A com , and the noise level specifies the probability of flip-ping the sign of an edge when sampling. Afterwards, we consider each kernel and its corresponding objective in (19), and derive the clustering using spectral clustering. When we perform spectral clustering, we run k -means ten times with different initializations and select the one that gives us the smallest objective value since a bad initialization of k -means might influence the clustering result.

To evaluate a clustering result (or an indicator set { x 1 in other words), we first consider the following objective value: k We call (23) the ratio objective if W = I and normalized objective if W =  X  D . Both objectives measure the degree of imbalance of clusters, with size or volume normalization. Typically, if the underlying kernel is ratio normalized, we use ratio objective to measure the clustering result; otherwise, we use normalized objective to measure the clustering result. Note that the normalized objective is upper bounded by k . In addition, since we also have the  X  X eal X  clustering as the ground truth in the synthetic datasets, we can calculate the error rate of clustering based on the ground truth. Specif-ically, we calculate the percentage of misclassified edges if we apply the clustering to A com . We consider an edge to be misclassified if the edge is a between-cluster edge and it is positive, or if the edge is a within-cluster edge and it is negative. The precise definition of error rate is: where n is the number of nodes in A com .Tomakethecom-parison fair, all results are averaged over ten times trials.
Results. In the first experiment, we sample from A com with different sparsity levels and a fixed = 0, i.e., no noise. Since in most cases we expect a network to be sparse, we zoom in the range of sparsity s  X  [4  X  10  X  3 , 10  X  1 ]. The results are shown in Figure 1. A lower error rate and objec-tive implies a more effective method. We can see that Pos-RatioAssoc, NegRatioAssoc and PosRatioCut, which only consider one of positive or negative criterion, perform worse than others. This confirms that both positive and negative relationships are essential when we cluster signed networks. Furthermore, we see that BalRatioCut and BalNorCut out-perform SignLap and NorSignLap under every sparsity level, and the difference becomes significant when a graph becomes more sparse. This is also not surprising since SignLap and NorSignLap are not optimizing the desired clustering pat-terns when k = 5, as explained in Theorem 3.

In the second experiment, we fix the sparsity at 10  X  2 and increase noise level from 0 to 0 . 5. The results are shown in Figure 2. Here we observe that for &lt; 0 . 17, BalRatioCut and BalNorCut still give the lowest error rate, but their error rates go higher for larger . However, if we see in Figures 2b and 2c, we observe that their objectives are still quite small compared with others. The possible reason is that when noise level goes too high, antagonistic clusters are no longer significant. In such case, ground-truth clusters cannot be captured by minimizing these objectives.

In summary, we see that BalRatioCut and BalNorCut are very effective in k -way clustering and improve the state-of-the-art signed Laplacian based methods.
We now show that the quality of results using the multi-level clustering method discussed in Section 5 is comparable to recent state-of-the-art methods. Furthermore, multilevel clustering is much faster than other methods, so it can scale up to large-scale signed networks. We have implemented our multilevel clustering algorithm in C++. In refinement phase, we use balance normalized cut kernel for weighted kernel k -means, since it gives us the most favorable result as shown in Section 6.1. For efficiency, we do not apply local search strategy and we only consider boundary nodes in refinement phase (See Section 5.3 for details). Similarly, to make a fair comparison, all results are averaged over ten trials.

Methods . To compare our multilevel clustering with other existing state-of-the-art methods, we consider the nor-malized signed Laplacian ( NorSignLap ) [13]. In addition, we also consider two more approaches, MC-SVP and MC-MF , which are based on the low rank model proposed in [10]. MC-SVP uses SVP to complete the network and runs k -means on k eigenvectors of the completed matrix to get the clustering result. while MC-MF completes the network using matrix factorization and derives two low rank factors U, H  X  R n  X  k , and runs k -means on both U and H .We then select the clustering that gives us smaller normalized balance cut objective. See [10] for more details.
Results . First we see the quality of clustering results. Us-ing the same procedure conducted in Section 6.1, we create some networks sampled from a complete 10-weakly balanced network, in which each group contains 1 , 000 nodes. We fix = 0, and compare the objective and error rate of each method under different sparsity. To make the comparison clearer, here we consider the balance normalized objective for all methods. The result is shown in Figure 3. As a graph becomes more sparse, MC-MF and MC-SVP perform worse than NorSignLap and multilevel clustering. However, MC-MF requires much less running time than NorSignLap. We will discuss timing details issue in the next subsection. In addition, the multilevel clustering outperforms other state-of-the-art methods in most cases. It usually achieves the lowest error rate and normalized objective. Figure 3: Clustering results for multilevel clustering and other state-of-the-art methods on weakly balanced networks, with different sparsity.
 Now let us compare the efficiency of different methods. We consider A com to be a large balanced network, which contains 20 groups, with 50 , 000 nodes in each group. By choosing a proper sparsity, we randomly sample some edges from A com to form A with desired number of edges. We then measure the running time required for each clustering algo-rithm under different A . Both NorSignLap and MC-SVP are too costly to apply on this large-scale network, since both of them require eigenvectors computation of top k eigenvectors. Therefore, here we only compare the multilevel algorithm with MC-MF method. While we report the running time of whole procedure for multilevel clustering, we only report the time for computing two factors U and H for MC-MF. Thus, the MC-MF time we report is just an under-estimate of the real clustering time, since we ignore the time for doing k means clustering on U and H . To solve matrix factorization efficiently, we implement alternating least squares (ALS) al-gorithm in C++ with MKL library, and run ALS iterations 3 times to derive U and H . In practice, ALS usually needs more than 3 iterations to obtain converged U and H ,which will take more time than we report.

The running time of the multilevel clustering and MC-MF is shown in Figure 4. We can see that the multilevel cluster-ing is faster than MC-MF even if we only consider the time for matrix factorization procedure of MC-MF. In particular, we can obtain the clustering result of a 20-weakly balanced network, with 1M nodes and 100M edges (so, sparsity = 10  X  4 ), in only 398.77 seconds. We further plot the clustering result of this network in Figure 5. By reordering nodes based on clustering result, we can see A + becomes nearly block-diagonal while A  X  is pretty dense in the between-cluster part. This reconfirms that our multilevel algorithm is both efficient and effective.

Finally, we apply our multilevel clustering algorithm to large-scale real networks considered by Leskovec et al. [14], in which the authors argue that there are no significant (two) nearly antagonistic groups, though local structure in these networks tends to be balanced. We now extend the consideration from balance to weak balance, and examine whether there exist k nearly antagonistic groups in these networks. We first make the network symmetric by consid-ering sgn( A + A ), and take the largest connected component as the input of the multilevel clustering algorithm. We then obtain several clustering results by considering k from 3 to 30. Since we do not have the ground truth of clustering in real networks, we can only compute the  X  X mpirical X  error rate on A , i.e., the percentage of misclassified edges in under such clustering. Figure 4: Running time of multilevel clustering and MC-MF on weakly balanced networks, with 1 million nodes and 100 million edges. For MC-MF we report the time to solve matrix factorization only, while in multilevel clustering we report the time for the whole procedure.
 Figure 5: Clustering result of a 20-weakly balanced network, with 1M nodes and 100M edges. We see that most positive edges are within clusters and most negative edges are be-tween clusters. The normalized objective of this clustering is 0 . 032, which is much smaller than 20.

We find the empirical error rate for each k is very close to each other, so we report the average error rate for each dataset, which is summarized in Table 2. Similar to the observation in [14], the empirical error rates of k -way clus-tering result are trivially achievable (by putting all nodes into one cluster). This implies that we can hardly find sig-nificant k nearly antagonistic groups in these networks even if we extensively consider weak balance.
In this paper, we study the k -way clustering problem in signed networks. We first find that the state-of-the-art signed graph kernels, (normalized) signed Laplacian, suffer from a weakness when we apply them to the k -way clustering prob-lem. Thus, we propose some new signed cut and association objectives and kernels. In particular, we show one of our proposed objectives, balance normalized cut, is both theo-retically sound and experimentally effective. Furthermore, we prove that our proposed objectives are mathematically Table 2: Multilevel clustering results on large-scale real net-works. The (empirical) error rate is the percentage of mis-classified edges for k = 3 to 30; the above results support observation in [14] that these networks do not have signifi-cant clustering structure. equivalent to the weighted kernel k -means objective. Based on this equivalence, we propose a multilevel clustering algo-rithm, which is competitive to recent state-of-the-art meth-ods in terms of the quality of the clustering result, while it is much more efficient and scalable.
 NSF grants CCF-0916309, CCF-1117055 and DOD Army grant W911NF-10-1-0529. [1] P. Anchuri and M. Magdon-Ismail. Communities and [2] N. Bansal, A. Blum, and S. Chawla. Correlation [3] D. Cartwright and F. Harary. Structure balance: A [4] J. A. Davis. Clustering and structural balance in [5] I. S. Dhillon. Co-clustering documents and words using [6] I. S. Dhillon, Y. Guan, and J. Kogan. Iterative [7] I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph [8] P. Doreian and A. Mrvar. A partitioning approach to [9] F. Harary. On the notion of balance of a signed graph. [10] C.-J. Hsieh, K.-Y. Chiang, and I. S. Dhillon. Low rank [11] G. Karypis and V. Kumar. A fast and high quality [12] B. W. Kernighan and S. Lin. An efficient heuristic [13] J. Kunegis, S. Schmidt, A. Lommatzsch, J. Lerner, [14] J. Leskovec, D. Huttenlocher, and J. Kleinberg. [15] J. Shi and J. Malik. Normalized cuts and image [16] U. von Luxburg. A tutorial on spectral clustering. [17] B. Yang, W. Cheung, and J. Liu. Community mining
