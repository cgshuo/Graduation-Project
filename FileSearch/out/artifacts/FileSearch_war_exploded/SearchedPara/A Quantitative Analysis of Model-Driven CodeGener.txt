
Wanderley Lopes de Souza 1 , Carlos Eduardo Cirilo 1 ,andLu  X   X s Ferreira Pires 2 Model-Driven Development (MDD) [1] has been quite popular in the academic community in the last years, and different approaches based on MDD have been proposed. Most recent studies related t o MDD focus on the transformation of domain models to different kinds of software applications. These studies show that software development supported by code generation from abstract mod-els provides several benefits, such as, e.g., increase of productivity, facilitated maintenance and documentation, and portability [2 X 5].

Although the impact of MDD-based approaches has already been established to some extent, there are still some questions that should be answered, like: What is the average time development reduction that can be obtained with code genera-tion from models compared to traditiona l development? How can code generation be applied to reduce the difficulties faced by developers? What are the practical advantages of using code generation from models perceived by developers? This paper aims at answering these questions by means of experimentation.
In this paper we report on the results of a quantitative study we performed by applying a systematic experimental methodology [6]. This study evaluates the impact of using model-driven code generation on the time spent by groups of students to develop software systems. In this experiment, at a certain time, the groups developed a web application following the disciplines of the classic life-cycle [7] without using any form of code generation, and at another time, the groups developed the same application by using code generation from models. After that, we analysed the data on the time spent by the participating groups that we collected during the experiment . The paper also reports on the results of this analysis.

This paper is further organised as follows : Section 2 presents some concepts re-lated to the principles of experimentation applied in this work, Section 3 discusses our experiment in detail, Section 4 disc usses some related work, and Section 5 gives our final conclusions, including some topics for possible future work. Empirical validation should help establish Software Engineering as a science [8]. Experiments allow the validation in practice of theories developed in a scien-tific process. Through experimentation, this validation can be performed in a systematic, disciplined, controlled and computable way [9].

In Software Engineering, experimentation enables one to test hypotheses about a particular object of study (e.g., a proce ss, method, tool, feature, model or the-ory) and observe the effects of adopting t hese objects in practical situations. From measurements, collected data ar e analysed and hypotheses can be vali-dated or refuted [6, 9]. Figure 1 shows th e general structure of an experiment.
The null hypothesis (H0) is the main hypothesis of the experiment, and indi-cates that there is no statistically significant relationship between the cause and the effect being investigated. Generally, the goal of an experiment is to refute this hypothesis in favour of one or more alternative hypotheses (H1, H2, H3, ..., Hn).

In an experiment we can identify independent and dependent variables. Inde-pendent variables, which are also called factors , refer to the elements that are manipulated or maintained during the experiment, so that the causes that affect the outcome of the experiment can be iden tified. In contrast, dependent vari-ables refer to the elements that are test ed when the experiment is performed. The value assigned to a given factor is called a treatment . Therefore, treatments should be applied on the objects of study and on the set of participants in order to identify possible results.
 Our experiment followed the phases described in Wohlin et al. [6], and was con-ducted in the second half of 2011. The experiment aimed at performing a com-parative analysis of the time spent impl ementing the CRUD (Create, Retrieve, Update, Delete) functions of web system s from models of the application entity classes described in UML class diagrams. Software development was performed both manually (based on the classic lif e-cycle) and by using code generation from models.

In the experiment, we used a part of the ProgradWeb 1 system of the Fed-eral University of S  X  ao Carlos (UFSCar). ProgradWeb is an medium/large size academic system in operation since 1998, which stores all information about students, teachers, registration, certificates, diplomas and other issues related to undergraduate courses at UFSCar. 3.1 Definition The experiment was defined as follows: Analyse the use of mechanisms for code generation from models in the con-For the purpose of evaluation; With respect to efficiency in terms of time; From the point of view of software developers; In the context of Computer Science and Computer Engineering undergradu-
The experiment was conducted in a university environment with the collabo-ration of undergraduate students of the Computer Science and Computer Engi-neering programs at UFSCar. The context of our study corresponds to multi-test within object study [6]. Thus, the experiment consisted of various experimental tests, in which different groups of subjects implemented a single application in different ways. 3.2 Planning After the definition, the experiment was planned according to the steps discussed below.
 Context Selection. The experiment was conducted in a university environ-ment, in a teaching laboratory of the Computer Department of UFSCar, within the  X  X opics in Computer Science 2011 X  course. The experiment involved the participation of students of the third and fourth academic years in Computer Science and Computer Engineering at UFSCar.
 Hypothesis Formulation. In order to determine the effect of the development approach (with and without code generation) in the results, three hypotheses were formulated for the experiment. The following metrics were considered when formulating hypotheses:  X  Total time spent by the groups to develop the Web application with CRUD  X  X  Average time spent by the groups to develop the Web application with CRUD The hypotheses formulated for the experiment are: Null Hypothesis (H0). In general, there is no difference in efficiency (  X  )be-Alternative Hypothesis (H1). When groups apply code generation from Alternative Hypothesis (H2). When groups apply manual coding based on Variables Selection. In this step we chose the variables that allowed us to analyse the efficiency of the groups when us ing different development approaches. The variables were chosen as follows: Independent Variables. We selected the following independent variables in Dependent Variable. In accordance with the hypotheses, we chose group ef-Selection of Subjects. Subjects were selected according to convenience sam-pling [6]. In this non-probabilistic technique, the selected participants were the closest and most convenient to conduct the experiment. Voluntarily, 29 students from the third and fourth years of Computer Science and Computer Engineering undergraduate academic programs of UFSCar participated in the experiment, in the scope of their attendance to the  X  X opics in Computer Science 2011 X  course. Experiment Design. The experiment followed the general design principle of grouping the participants in homogeneous blocks [6], avoiding a direct impact of the experience level of the participa nts in the treatment outcomes of the devel-opment approach factor, increasing in this way the accuracy of the experiment.
The participants were divided into 9 homogeneous groups (blocks), where 7 groups contained 3 participants and 2 groups contained 4 participants. The groups were divided so that the average experience level of the groups was as similar as possible.
 To determine the experience level of each participant, we used a Participant Characterization Form . The participants answered multiple-choice (closed) ques-tions to assess their knowledge regarding the topics covered by the experiment (e.g., Java, Hibernate, JSF, UML), allowing the experience level of each partici-pant according to the technologies used in experiment to be quantified. Figure 2 displays the individual experience levels of each participant P i and the average experience level of each group G j , according to the information given by the stu-dents in their characterization forms. Th ese levels were obtained by calculating the average of knowledge degree in a 5-point scale for each question of the form answered by the participants.
With respect to the design type, the exp eriment consisted of a paired com-parison [6] of one factor (development approach) with two treatments (code generation and manual coding). In this design type, each subject (or a group of subjects) uses both treatments on the same object of study. The order in which the participants apply the treatments is set at random. Therefore, in our ex-periment, the application was developed by all participating groups using both code generation and manual coding. Table 1 shows the randomly selected order in which the treatments have been assigned to the groups, where  X 1 X  and  X 2 X  indicate which treatment was applied first and second, respectively. Instrumentation. All the material required to support the participants through-out the experiment was made available beforehand, including the preparation of objects, instruments and guidelines for d ata collection used during the execution of the experiment. 3.3 Experiment Execution Once the experiment was defined and planned, we executed the experiment ac-cording to the following steps: preparation, operation and validation of the col-lected data.
 Preparation. At this stage, the students got committed and involved with the experiment and were made aware of its pur pose and research objectives. During the preparation of the experiment, all p articipants were inf ormed and accepted the terms regarding the confidentiality of the provided data, which would be only used for academic purposes, and their freedom to withdraw, by signing a Consent Form .

At this stage, the following objects were produced to be used in the experi-ment: Participant X  X  Characterization Form. Questionnaire in which the partici-Consent Form. Document signed by the partici pants, stating the objectives Task Description. Document describing the task to be performed in the ex-Support Material. Roadmap describing the steps to be used in the develop-Data Collection Form. Document containing empty spaces to be filled in by Evaluation Form. Questionnaire containing multiple-choice questions for the In order to avoid the interference of the time spent in learning the approaches to code generation and manual coding, a training with a duration of two weeks was planned and provided to all participants, so that everybody was able to develop the kind of application proposed in the experiment.

The platform adopted for developing both applications consisted of Java as implementation language, the JSF framework to support the MVC pattern im-plementation, JPA and Hibernate to support persistence in a MySQL 2 database and the Eclipse IDE 3 as development environment. This platform was installed and configured beforehand in the computers where the experiment was con-ducted. Operation. On the day in which the experiment was performed, the tools and materials used in experiment were stored in a package that was made available to the groups. This package contained the following elements:  X  Eclipse IDE configured with the Papyrus UML 4 plug-in.  X  Library files (Hibernate, JSF and MySQL Connector).  X  Folder with the Apache Tomcat Serve r to run the developed application.  X  Class diagram of the developed application.  X  All the documents described in experiment preparation step.
 In addition, when following the code g eneration approach, the groups received a code generator previously developed for t he experiment. This generator consisted of the M2C transformations to generate Java code from UML class diagrams.
The groups performed the experimental task as defined in the Task descrip-tion . All groups implemented the proposed application with all required func-tions. The applications resulting from using both approaches were equivalent, and had the same quality and performance levels.

During the experiment, the groups recorded on the Data Collection Form the start time and end time of each activity performed to implement the application. Table 2 summarises the collected data, showing the time spent on the implemen-tation activities. Table 3 gives a description of the acronyms used in Table 2.
Table 2 shows that the data divided into two blocks, for the manual coding and code generation approaches. Table 2 also shows average and total values. Data Validation. In general, we observed that the groups developed the pro-posed tasks satisfactorily, and the coll ected data was within the expected limits. This means that the treatments were ex ecuted correctly and in accordance with the planning. Therefore, we can claim that the obtained data was valid to con-duct the proposed evaluation. 3.4 Results Analysis and Interpretation Table 2 shows that when using the manual coding approach, the groups spent an average of 2 hours and 2 minutes to complete the proposed activities, whereas when following the code generation approach this time dropped to an average of 11 minutes (reduction of 90.98%). Therefore, it is fair to conclude that the groups completed the implementation task more quickly when following the code genera-tion approach than when following the manual coding approach. This difference is justified by the model-to-code (M2C) transformations, which automatically generated a substantial amount of source code from a predefined model.
In the case of applications with more classes and attributes in each class in their models than the application used in our experiment, the amount of time and effort required to perform the implementation activities following the man-ual coding approach, tends to be even longer. Although not all the necessary application code is automatically generated when using the code generation ap-proach, this approach can already provide a significant productivity gain, which justifies the effort and time spent on building generators based on model-to-code transformations, as the one used in this experiment. This also means that any application that uses the same platform and technologies as the ones for which these generators are built can be devel oped with much less effort than if they would be implemented using manual coding.
 Descriptive Statistics. Descriptive statistics deals with the presentation and processing of a numerical data set [ 6]. Once an experiment is performed, descriptive analysis allows the collected data to be analysed, grouped and graph-ically presented, to view the resul ts from different perspectives.
Figure 3 shows the dispersion of time spent by the groups that participated in the proposed experiment in a boxplot chart. Figure 3 shows clearly that when the groups used code generation they were faster than when they applied manual coding.
 Furthermore, the normality of the data set was verified using the Shapiro-Wilk nonparametric test [10]. The test results confirmed that the sample sets obtained with both approaches consisted of normally distributed sets, as shown in Figure 4. Data Set Reduction. Before applying statistical methods, it is necessary to check the quality of the input data [6]. The representation and correctness of data have a direct impact on the conclusions that can be drawn from the results, since if incorrect data is used in the statistical analysis, wrong conclusions will probably be drawn. Incorrect data sets can be obtained due to systematic errors or the presence of outliers , which are data values that are much higher or much lower than expected when compar ed with the remaining data.

After a careful analysis of the forms filled by the participants, we have not found any systematic errors (e.g., transcription errors by the participants) nor outliers in the input data set. Therefore, there was no need to reduce the data set, and the data were considered valid for applying statistical methods of data analysis.
 Hypothesis Testing. When analysing the data produced by the groups that participated in the experiment, we no ticed that when groups followed the code generation approach they were more efficient than when they followed the manual coding approach. In order to verify and quantify the actual efficiency gain from the collected data, we applied the paired t-test [11] (Equation 4). where: n Number of paired samples.  X  d Difference of averages of each samples set.
 S d Standard deviation of the differences of the samples (Equation 5). where: d i Difference between each pair of sample.
 case, t  X ,f is the upper  X  percentage point of the t-distribution with f degrees of freedom.

Based on the samples, n =9and d = { 113 , 111 , 108 , 111 , 121 , 114 , 102 , 110 , 109 } (in minutes). The average values of each data set are  X  Classic = 122 , 222 and  X  S d =5 . 099 and t 0 =65 . 30667.
 The number of degrees of freedom is f = n  X  1=8.Wetake  X  =6 . 8  X  10  X  12 . In the table of statistical probability of the Student X  X  t-distribution ,wecould H 0 can be rejected with a significance level of 6 . 8 To perform the calculations mentioned above we used the software R 5 , with RKWard 6 as a graphical user interface. 3.5 Discussion of Results After we performed hypothesis testing and we verified the possibility of reject-ing the null hypothesis (H 0 ), we could draw some conclusions regarding the influence of independent variables on the dependent variable, the validity of the experiment and the treatment of the validity threats (in Section 3.7).
Concerning the rejection of the null hy pothesis, we can conclude that the differences in development times have st atistical significance, as evidenced by the samples provided by the groups that followed the manual coding and code generation approaches. This indicates that the difference in development time spent by the groups was due to the development approach they used during the experiment, and not by any accident or mi stakes in the collection of samples.
Table 2 shows that when the groups used code generation, the average total time was consistently lower than when they used manual coding (  X  X  Generation &lt;  X  X 
Classic ). This is an evidence that hypothesis H 1 can be validated instead of hypothesis H 2 . Therefore, we can safely argue that groups that use code genera-tion from models usually spend less time in software development. This result is within our expectations for this experim ent, since we expected that automated code generators would speed up parts of the development task. This expectation was confirmed in practice by our experiment.

Furthermore, the conclusions on the results of this study are limited to the scope of CRUD web applications implem ented by software developers in a uni-versity environment, since the experiment was performed in vitro and under controlled conditions. In our data set, only the time spent on implementation activities was collected, which means that the time spent on other steps of soft-ware development (e.g., modeling and testing) was not covered by the exper-iment reported in this paper. To exten d our results to a broader context, we would have to perform new experiments with an increased number of subjects, and performed in in vivo environments, in which we could compare the use of code generation from models with other approaches for software development beyond manual coding according to the cl assic life-cycle, as addressed in this study.

Replication of our experiment in an industrial environment could increment the validation of our work, as it would extend the results to new contexts, where model-driven approaches could be compared with other development approaches (e.g., software product lines and agile methods), also considering factors that do not play a role in an academic environmen t. In addition, other effects related to software development may be studied, suc h as, e.g., effectiveness with respect to faults during development. In this case, empirical evaluations with users and in-spection tests should be planned in order to collect relevant metrics for assessing the degree to which the developed applications reach the goals of effectiveness, efficiency and subjective satisfaction f rom the point of view of their end users.
A package containing the tools, materials and more details about the experi-mentstepsisavailableat www.dc.ufscar.br/  X  paulo.papotti/EXPERIMENT.zip and can be used by researchers and practi tioners to help them perform new exper-iments related to our study. 3.6 Participants X  Opinion We analysed the participant X  X  opinion in order to evaluate the impact of using the approaches considered in the experiment. After the experiment operation, all students received two evaluation form s with multiple-choice questions with empty spaces for them to report on their perception of the manual coding and code generation approaches.

After the participants filled in both questionnaires, the answers were analysed and some interesting results were obtained. When asked if they encountered difficulties in the development of the proposed tasks when they followed the manual coding approach, 54% of the participants reported having difficulties, 32% mentioned partial difficulties and only 14% had no difficulties. In contrast, when asked the same question with re spect to the code generation approach, 71% reported not having any difficulty, 11% mentioned partial difficulties, and only 18% of all participants had total difficulty in completing the tasks. Figure 5 shows charts that visualise the levels of difficulty faced by the participants.
Figure 5 shows a decrease in the percei ved difficulty when the participants used code generation. When following the manual coding approach, 86% of the participants had some kind of difficulty (total or partial), while for the code generation approach this value fell to 29%. Therefore, we believe that the mech-anisms for code generation were essential to facilitate the task of the participants, since there has been an increase of 57% in the percentage of participants who developed the proposed task without any difficulties.

The most common difficulties pointed out by the participants when they fol-lowed the manual coding approach are: (1) too much effort spent on coding; (2) problems related to the language; and (3) mistakes they made due to lack of attention. In contrast, the most common difficulties faced by participants when using code generation are: (1) lack of practice with the use of generators; and (2) poor integration between the generat ors and the IDE used for development.
Since all participants stated that the code generation approach assisted them in performing the development task, the y were also asked to mention the advan-tages of this approach when compared with manual coding. Figure 6 summarises the results of this enquiry, showing that 92.86% of the participants mentioned the generation of part of the application code as the biggest advantage of the code generation approach. Other advantages that have been mentioned are the increased focus on modeling (71.43%), the ease to perform future maintenance (53.56%) and others (7.14%).
 3.7 Validity Threats Whenever an experiment is performed, the validity of its results should be as-sessed. An experiment may have its results put at risk and invalidated for differ-ent reasons depending on the way it was conducted. Therefore, the conditions for validity should be considered since the initial stages of the experiment. The main types of validity that should be considered are: conclusion validity, inter-nal validity, construct validity and external validity. All these validities were considered in our experiment.
 Conclusion Validity. Different precautions were taken to ensure the conclu-sion validity of the experiment. We used a parametric statistical test (paired t-test), which is suitable for evaluating the factor  X  development approach  X  with treatments  X  code generation  X  X nd X  manual coding  X .

The normality of the collected data was c onfirmed before using the paired t-test through the Shapiro-Wilk normality test (Figure 4) with a significance level of at least 5%. However, to reinforce and ensure the validity of results, we applied the Wilcoxon non-parametric test, an alternative test to the paired t-test, which does not require that the collected data are normally distributed. In this case, the results pointed to the same direction as the paired t-test, confirming the rejection of the null hypothesis.

Furthermore, to increase the validity o f the conclusion, the data collected by participants (hour and minutes) do not dependent on human judgement and are quite reliable, even though they were co llected by the participants themselves.
Finally, the heterogeneity of the experience level of the participants was treated by grouping them in blocks (groups) with similar average experience level.
 Internal Validity. The experiment was conducted by undergraduate students of the Computer Science and Computer Engineering academic programs at the final stage (third and fourth academic years) of their study, who are well ac-quainted with software development. Therefore, we are confident that these stu-dents are representative for the population of software developers.
Furthermore, a questionnaire was used to characterize the participants of the experiment by capturing their profile and their average experience level. This information was used to assign them to homogeneous groups of similar average experience level.
 Construct Validity. The goal of the proposed experiment was to compare two different approaches to software development and their impact on the time spent on the development task by groups of participants. The data on the time spent by the groups were collected during the development of a sample application using both approaches in order to perform this comparison.

In order to avoid any interference in the behaviour of the participants, the metrics and calculations that were used on the collected data were not disclosed to the participants, so that they would keep their focus on the development task in the most spontaneous way as possible, instead of seeking opportunities to get results that would favour or harm the experiment.
 External Validity. Our experiment was conducted in a computerized labo-ratory, equipped with the items necessary to perform the development task, including the tools and technologies used in software development in industrial environments, such as Java tools and the Eclipse IDE. The experiment was com-pletely performed in a period of about 3 h ours, so that the results have not been affected by excessive fatigue or b oredom of the participants. A study of the impact of MDD adoption in large scale projects is presented in [12], in which different characteristics of a large industrial project were in-vestigated in which MDD was applied in its pure form. The study focused on the analysis of the size and complexity o f models produced in the project, con-sidering metrics related to quality and modeling efforts. Similarly, three case studies related to the implementation of the MDE in a industrial environment are presented in [13]. The study highlights the importance of social, technical and organizational requirements for the successful adoption of model-driven soft-ware development in practice. The autho rs used questionnair es, interviews and observation to collect data from industrial professionals. Challenges and barriers related to the adoption of MDD were discussed in [14], based on the authors X  experience in different industrial and academic projects. A quantitative analy-sis with preliminary results from the adoption of a tool intended for software maintenance in the MDD context is reported in [15].

Our work is closely related to the work on practical experiments with model-driven software development mentioned above, and especially the ones related to the use of code generation mechanisms. However, unlike most related work, our study has a strong focus on quantitative statistical analysis of real data collected by the participants of the experiment. Furthermore, the results of our work have a more limited scope, and are specifically related to the use of code generation from models, aiming at a deeper analysis within the proposed scenario. This paper presented the results of a quantitative analysis of an experiment in which the use of mechanisms for code generation from models in software devel-opment has been evaluated. The population of participants in the experiment was composed of undergraduate students of the last years from UFSCar who devel-oped an application based on ProgradWeb system, following both a development approach based on manual coding, as with code generation from models. We col-lected data about the time spent on the development task in both cases. From the data collected, the analysis revealed an average of 90.98% reduction in devel-opment time, considering the conditions of the experiment. In addition, the code generation approach contributed to reduce the difficulties encountered by 57% of the participants.

The results achieved in this paper reaffirm the benefits of approaches that use model-driven code generation, esp ecially for the development of CRUD web applications. Productivity increase and difficulties reduction during development were the main points we explored in our experiment. Our study directly evaluates the benefits of MDD and quantifies the achieved gains. Although the study was conducted in a university environment, the results are valid and may be used in other research projects (respecting our adopted conditions) that aim at making further progress in the related research areas.

Finally, some topics for further work include: perform the experiment de-scribed in this paper again in an industrial environment; perform other experi-ments to compare code generation from models with other approaches that are not model-based; and evaluate the effect iveness of development by analysing the faults found by the groups during the experiment.
 Acknowledgments. We thank the National Counsel of Technological and Sci-entific Development (CNPq) for sponsoring our research in the context of the INCT-MACC.

