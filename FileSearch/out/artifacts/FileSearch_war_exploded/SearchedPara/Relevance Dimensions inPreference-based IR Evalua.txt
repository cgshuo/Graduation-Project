 Evaluation of information retrieval (IR) systems has recently been exploring the use of preference judgments over two search result lists. Unlike the traditional method of collecting relevance labels per single result, this method allows to consider the interaction between search results as part of the judging criteria. For example, one result list may be preferre d over another if it has a more diverse set of relevant results, covering a wider range of user intents. In this paper, we investigate how assessors determine their preference for one list of results over another with the aim to understand the role of various rele vance dimensions in preference-based evaluation. We run a series of experiments and collect preference judgments over differen t relevance dimensions in side-by-side comparisons of two search result lists, as well as relevance judgments for the indivi dual documents. Our analysis of the collected judgments reveal s that preference judgments combine multiple dimensions of relevance that go beyond the traditional notion of relevance ce ntered on topicality. Measuring performance based on single do cument judgments and NDCG aligns well with topicality based preferences, but shows misalignment with judges X  overall preferences, larg ely due to the diversity dimension. As a judging method, dimensional preference judging is found to lead to improved judgment quality. H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval Diversity, relevance criteria, user preferences, When collecting relevance judgments for the Cranfield [7] style evaluation of IR systems, the established method is to assess the relevance of individual documents to a given query independently of other documents retrieved for the same query [14]. While this method eliminates several sources of variability that may influence assessors' relevance criteri a, it ignores aspects that may better reflect user satisfaction with search engine result pages (SERP) in the real-world. For example, a SERP that contains diverse relevant results satisfying multiple aspects of a user's need may be preferred by users compared to a SERP with relevant, but redundant individual results. Diversit y has in fact been recognized as an important aspect of search engine evaluation and has been investigated, for example, by the TREC Web Track X  X  diversity task [6]. The track tackled the issue of evaluating diversity by structuring the test topics into subtopics, reflecting different intents, and obtaining relevance labe ls for each intent separately. Recently, a number of studies explored a new way of comparing IR systems [13,11]. Instead of judging retrieved documents in isolation, the use of preference judgments over two search result lists, produced by two IR systems, has been proposed. Unlike the traditional method of assessing re levance per single result, this method enables comparing search results in context [13]. Using this method, it becomes possibl e to consider the interaction between search results as part of the evaluation criteria. Moreover, the benefits of user preference based evaluation has been demonstrated by Sanderson et al. who found high correlations between user preferences and diversity measures such as intent-aware precision or  X  -NDCG [11]. However, comparing two lists of search results is a complex cognitive task for assessors, where multiple dimensions of relevance intermix and contribute to varying degree to the final preference decision. Some of the well-studied aspects of relevance include topicality, freshness, and authority, which characterize individual documents, and novelty or diversity, which considers a set or list of documents [8,15]. These aspects have been studied extensively in the traditional relevance evaluation setting, but not within a user preference based evaluation methodology. In this paper, we adopt a user preference based evaluation method and investigate how users determine their preference for one list of results over another . To this end, we propose a method to collect dimensional preference la bels and conduct experiments in which we ask assessors to compare two search result lists and indicate their overall preference as well as their preferences along five relevance dimensions: Relevance (topicality), Freshness, Authority, Caption quality and Di versity. We employ professional judges and obtain preference judgments for 523 queries extracted from a commercial search engine X  X  query logs. Our goal is to investigate:  X  How do different dimensions of relevance relate to user  X  How do the different dimensions re late to traditional measures  X  How does dimensional preference judging affect the judging Our contributions are: 1) a me thod for collecting dimensional preference labels that can improve judgment quality, 2) analyses that provide insights into the different relevance dimensions at play in preference based IR evaluation, 3) finding showing that preferences go beyond topicality relevance that is not captured with traditional single docum ent relevance judgments and evaluation metrics, like NDCG. A range of alternative methods to IR evaluation has been proposed in recent years that aim to go beyond the traditional methods that treat each retrie ved document in isolation. For example, Bailey et al. [2] proposed a method that allows investigating aspects such as co herence, diversity and redundancy among the search results displayed in a SERP. Carterette et al. [4] studied methods to evaluate search engines using preference judgments over pairs of documents retrieved for a query. They found significant improvement in inter-assessor agreement when collecting preference la bels as opposed to independent absolute labels. Chandar and Carterette [5] employed a new preference based design for the evaluation of novel document retrieval methods, in which users gave preference judgments, given an already observed document. Thom as and Hawking [13] proposed a preference method that displays two sets of search results side-by-side and asks users to indicate which side they preferred. In their experiments, comparing Google's first and second page results, they reported high levels of accuracy in users preferring the top ranked results. User preference based evaluation has also been shown to correlate highly w ith diversity based evaluation measures. Sanderson et al. [11] in a large-scale study, involving nearly 300 users, 30 search topics and 19 TREC runs, provided compelling evidence for the utility and reliability of user preferences. The study in [10] demonstrated that even subtle differences in retrieval results could be measured with preference based comparisons. Following this line of work, we adopt a user preference based evaluation method and collect preference labels for a range of relevance dimensions over pairs of search result lists. Unlike Bailey et al. [2], we allow assessors to interact with the search results and visit the result web pages. Extending the work of [11] we investigate th e underlying criteria and relevance dimensions upon which user preference decisions may rest. The concept of relevance has been extensively studied in IR. It is widely recognized for being multi-faceted and subjective. It is also well known that human judgments are influenced by various situational, cognitive, perceptual and motivational biases [3,9] as well as by document variables, judgment conditions and scales, and personal factors [12]. In spite of these, relevance has been proven to be a reliable quantity in comparative IR evaluation [14]. Among the numerous aspects of relevance identified in the literature, the most dominant dimensions are topicality, authority, and, more recently, freshness [15]. Other aspects, which consider the interaction of multiple results, include novelty and diversity [5,1]. The latter has gained increased attention with the launch of the diversity task at the TREC Web track [6]. The task required participating systems to retrieve a ranked list of documents that collectively satisfied multiple information needs, explicitly defined by the subtopics of a gi ven test topic. The retrieved documents were assessed separate ly for each subtopic using a traditional judging procedure and binary relevance. In this paper, we propose a method to collect dimensional preference labels, including diversity, and examine how these dimensions relate to the overall preference over two search result lists. We conduct experiments in which we gather user preference judgments from professi onal assessors over pairs of search result lists shown side-by-side, similarl y to [13]. We randomly sample 550 queries from the query log of a commercial search engine. From this query-set that is a mix of head and tail queries, we scrape the SERPs returned by two commercial search engines that we will refer to as Engine Base line and Engine Experiment. From each SERP, we extract the top 10 search results. A pairing of the top 10 results from the two engines along with the query comprises one sample that is used as the input of one assessment task in our experiments. We ran domly assign the two engines to the left or right, unbeknown to the assessors, such that they occur on both sides with the same probab ility. Each search result is represented by the web page's title, URL, and a snippet. We run two parallel experiments on the same data set, differing only in the kind of judgment we ask assessors to make. In one experiment (control), we only requi re assessors to indicate their overall preference over the two result lists for a given query. In the other experiment (treatment), in addition to the overall preference judgment, we collect additional preferences along five dimensions: Relevance, Divers ity, Authority, Freshness and Caption quality. We ask assessors to consider each of these aspects in turn and indicate which side they prefer according to that single dimension. Assessors were given judging guidelines with explanations of each of the dimensions and the interface also provided tooltip reminders. We collected 2 judgments per sample in both conditions. Both groups of assessors were from a pool of trained professional judges with similar judging experience and performance quality. All the preference judgments were collected on a 7 point scale, with the mid-point reflecting no pr eference between the two sides, and with increasing levels of prefer ence for the left or right side towards the two ends of the scale. We map the collected preference judgments to the following scale {-3,-2,-1,0,1,2,3}, where the negative values indicate preference for the Baseline engine X  X  results and positive values reflect preferences for the Experiment engine X  X  results. In addition to the two preference judging experiments, we collect graded relevance labels, based on a 5 point scale, for the individual query and search result pairs (query-URL pairs) using a traditional relevance judging met hod (isolated, absolute judging). From our original sample of 550 queries, 27 were discarded either due to errors in the scrapes or judges later selecting the ``can X  X  judge X  X  option. For the remaining 523 queries, a total of 20,862 search results (with 9,306 unique URLs), forming 9,522 unique query-URL pairs, were obtaine d from the two engines: 5,134 unique query-URL pairs (4,963 unique URLs) from the Baseline engine and 5,153 unique query-URL pairs (5,054 unique URLs) from the Experiment engine. The number of unique query-URL pairs common to the two engines is 765 (711 unique URLs). Table 1. Data sets in the Control and Treatment conditions Table 1 summarizes the collected judgments. We can see that both the judging tasks were completed in around 105 seconds on average, even though the treatment group judges had to make additional dimensional judgments and left longer comments. We may hypothesize that the dimens ions allowed judges to better structure their preference decision process, which is confirmed in the judges X  feedback comments, see Section 4.5. In this section, we report Win-Loss analysis results for the two engines, calculated based on th e different preference judgments. We define our WinLoss measure as the number of times engine A is preferred over engine B minus the number of times engine B is preferred, divided by the total number of comparisons: Results in Table 2 show the WinLoss statistics for the Experiment engine over the Baseline for the different preference judgments. Note that there are some missing dimensional judgments (holes), as judges were asked to only provide them when they were confident in their judgment. We can observe that the overall preference in both conditions is for the Experiment engi ne. However, we also see that the different dimensions show different behavi ors and trends. For example, the winning engine outperforms the baselin e in terms of diversity, but loses on freshness. The overall Wi nLoss value lies somewhere in-between the different dimensional scores. Control Overall Preference 8.89 0% Treatment Overall Preference 8.32 0% Caption Quality 4.19 47.5% To investigate the relationship between the different preferences, we plot their distribution in Figure 1. It is clear that Relevance aligns the closest with the overa ll preference labels, followed by Diversity. On the other hand, Au thority, Freshness and Caption quality behave very differently a nd are much less discriminating. Note that this does not mean that they are not important criteria of user satisfaction, but that in this case, both engines performed similarly along these dimensions. 
Figure 1. Distribution of the different preference labels in Spearman correlation tests confirm that Relevance is the most correlated dimension with judges X  overall preferences, followed by the Diversity and Authority di mensions; see Table 3. Looking at the correlation between Relevance and the other dimensions, we see that Relevance and Authority are the closest, followed by Diversity. This means that while these notions may be inherently related to some degree, they ar e also distinguishably different dimensions. So far, we have seen that preference judgments are largely predicated on Relevance, but that Relevance alone does not explain the overall preference decisions. This makes sense, since if the retrieved results are not relevant in the first place, then no matter how diverse or fresh the resu lts are, users are not likely to be satisfied. On the other hand, once Relevance cannot differentiate between two search engines, then other aspects such as diversity become important. Th is is confirmed by the WinLoss scores calculated for cases when Relevance was judged as tie: the overall preference based WinLoss score of 11.06 is matched with a score of 22.86 for Diversity, 0.78 for Authority, -3.33 for Freshness and 0 for Caption quality. From this we can conclude that Diversity has the main impact in users X  overall preference when both sides are equally relevant. 
Caption Quality 0.435 0.404 In this section, we report on the extent to which judges agree in their preferences. Since we coll ected two judgments per sample, we use Cohen X  X  Kappa (  X  ) as our inter-judge agreement measure. We calculate kappa values both for the original 7 point scale (7P) and for the 3 point win-loss-tie scale (3P). In addition, the raw Jaccard agreement ratio for the 3P labels are also shown, see Table 4. Statistical significance (p&lt;0.05) is indicated with *. Control Overall 0.046* 0.19* 0.55 Treatment Overall 0.088* 0.22* 0.55 Diversity 0.063 0.10* 0.42 Caption quality 0.018 0.11 0.50 Results show that the treatment group has better inter-assessor agreement levels, suggesting a benefit of usi ng dimensional preferences for improving judgment quality. Although user preference judgments can capture diffe rent aspects of search result quality, in this section, we compare the different preference judgments with the established measure of NDCG, calculated over the individual judgm ents collected separately for the query-URL pairs in our data set. While a similar comparison was conducted in [11], here we al so consider how the individual relevance dimension based preferences correlate with NDCG. More specifically, we calculate the difference between the two engine X  X  NDCG scores up to a given rank i , dNDCGi, and then 0 20 40 calculate the Spearman correlation between the obtained dNDCGi scores and the 7P preference judgments. The obtained correlations are summarized in Table 5. We can see that the correlation between the different preference judgments and dNDCG increases when dNDCG is calculated for lower ranks. This suggests that pref erence labels are more holistic, reflecting the (relative) quality of the whole result set. Comparing the control and treatment groups X  overall preferences and the dNDCGi scores, we observe higher correlations in the treatment condition. This could be a result of the fact that judges had to explicitly consider the Relevance (topicality) dimension in these experiments, which is the domina ting criterion for the judgments that NDCG is calculated over (individual query-document pairs judged independently of each other). Looking at the dimensional preferences, we see that the Relevance dimension shows the highest correlation with dNDCGi, even above that for the overall pref erence. Diversity and Caption quality are the least correlated with dNDCGi. This is interesting, especially since the overall preference is less correlated with dNDCGi than the Relevance preference, confirming that other dimensions that contribute to the overall preference cannot be measured this way. This indicates a potential gap between real user satisfaction and search engine performance when optimized for relevance (and NDCG) alone. One consideration when collecti ng relevance judgments is the assessors X  experience with the task. Since in a preference based evaluation, judges need to evaluate many different signals, an interface that provides a shortlist of possible criteria to consider may be helpful. Indeed some of the judges in our treatment experiments commented on this ve ry fact. For example, one judge noted that  X  X t helped me to remember the different aspects that I should take into consideration wh ile judging X . Another said that  X  X the dimensions] were helpful, because I take all the factors into consideration. It helped put focus on some of the smaller differences such as captions... X  In this paper, we proposed a preference-based evaluation method to collect both overall and pe r relevance dimension based preferences (Relevance, Diversity, Authority, Freshness and Caption quality). We found that considering the dimensions separately increases judgment quality, while also allows experimenters to measure differen t aspects of a system. Based on judges X  comments, it seems that the addition of dimensions provided some structure to their judging process, which resulted in better quality. It is plausible that the process of considering dimensional preferences allowed judges to better think through their decisions, which may have contributed to the observed higher inter-assessor agreement levels. Our main finding is that overall preferences capture a range of dimensions, most dominantly Releva nce and Diversity. Authority, Freshness and Caption quality were much less discriminating in our data set. Thus, we see that preference judgments combine multiple dimensions of relevance that go beyond the traditional notion of relevance that is centered on topicality. This is further confirmed by our finding that search engine performance scores, calculated using traditional single document judgments and NDCG correlates with our Releva nce dimension preferences, but shows misalignment with the overall preferences, largely due to the diversity dimension. This is cr itical and demonstrates the need to consider additional measures that go beyond relevance in IR evaluation. 
