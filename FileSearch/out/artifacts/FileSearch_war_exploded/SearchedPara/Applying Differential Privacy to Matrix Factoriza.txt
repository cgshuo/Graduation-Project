 Recommender systems are increasingly becoming an inte-gral part of on-line services. As the recommendations rely on personal user information, there is an inherent loss of pri-vacy resulting from the use of such systems. While several works studied privacy-enhanced neighborhood-based recom-mendations, little attention has been paid to privacy pre-serving latent factor models, like those represented by ma-trix factorization techniques. In this paper, we address the problem of privacy preserving matrix factorization by utiliz-ing differential privacy, a rigorous and provable privacy pre-serving method. We propose and study several approaches for applying differential privacy to matrix factorization, and evaluate the privacy-accuracy trade-offs offered by each ap-proach. We show that input perturbation yields the best recommendation accuracy, while guaranteeing a solid level of privacy protection.
 Differential privacy; matrix factorization
In the last decade, recommender systems have become a fundamental tool in on-line services. One of the domi-nant recommendation approaches is collaborative filtering (CF), which can be partitioned into two families. Neighbor-hood methods learn correlations between items or users [5] and generate predictions based on their similarity. In con-trast, latent factor models [12] derive models that charac-terize users and items with respect to a set of latent factors.
Matrix factorization (MF) methods [12] have evolved as the state-of-the-art latent factor technique. There, the rat-ing matrix is decomposed into two low-dimensional matrices,
NICTA is funded by the Australian Government through the Department of Communications and the Australian Re-search Council through the ICT Centre of Excellence Pro-gram. c  X  c  X  capturing latent factors of users and items, respectively. MF has been shown to provide a higher predictive accuracy than the neighborhood methods, it is computationally cheaper, and easier to extend, for example, to consider temporal ef-fects or ratings with varying levels of confidence.
Recommender systems rely on personal user information and raise privacy concerns related to the misuse of the col-lected data for inferring personal information [10]. The raw user ratings, even if anonymized, pose a privacy risk: the data can be de-anonymized using information obtained from other sources and then be used to infer sensitive in-formation [18], e.g., gender, political views, or sexual ori-entation. Moreover, it was shown that even without direct access to user ratings, personal user information could be inferred from recommendations provided by the system to other users [2]. These inherent privacy risks of recommender systems motivated research of privacy-preserving recommen-ders [8, 10]. However, this body of research has mainly fo-cused on neighborhood methods, with limited work on pri-vacy preserving latent factor recommenders [14, 15].
In this paper, we approach the problem of privacy pre-serving MF by utilizing the concept of differential privacy [7], a rigorous and provable approach to privacy in statis-tical databases, previously applied to neighborhood based CF [13, 14]. While differential privacy sets constraints on privacy preserving computations, these computations can be carried out in various ways, which result in different privacy-accuracy trade-offs. We propose a number of approaches to alter MF, such that it maintains differential privacy guaran-tees. We study the privacy guarantees that can be achieved by the following approaches: (i) obfuscating the input data before applying the MF algorithm; (ii) adding noise within a stochastic gradient descent solver of the MF problem; and (iii) obfuscating the output of an alternating least squares MF mechanism. For these approaches, we provide a theoret-ical analysis of the (calibrated) noise level introduced in the algorithms, and empirically evaluate the resulting privacy-accuracy trade-offs by observing the effect of the noise on the computed rating predictions.

The contributions of our work are three-fold. We provide an analysis and evaluation of three differentially private MF approaches . The evaluation demonstrates that the best performing method, yielding the highest predictive ac-curacy while still ensuring a solid level of privacy protection, is input obfuscation. We further conduct an investigation of the design choices that affect the privacy-accuracy trade-off, showing the impact of the data pre-processing, the de-pendencies between the characteristics of a dataset (size, density, number of user/item ratings) and the choice of the algorithm, and the influence of the privacy constraints on pa-rameter tuning. Finally, we compare the accuracy of the predictions generated by differentially private MF with that of privacy-preserving neighborhood based methods. Our experiments demonstrate that neighborhood methods are more resilient to the noise introduced by the differential privacy constraints, and are more appropriate when high levels of privacy protection are required. However, when weaker privacy levels are acceptable, privacy preserving MF techniques achieve higher levels of predictive accuracy than neighborhood based methods.
Personalization and recommender systems inherently bring to the fore the issue of privacy [8, 11]. Privacy hazards in recommender systems are aggravated by the fact that gen-eration of quality recommendations requires large amounts of user data. For instance, the accuracy of CF recommen-dations is correlated with both the number of users in the system and the number of their ratings [5]. Hence, there is a trade-off between the accuracy of recommendations pro-vided to users and the degree of user privacy.

We divide prior works on privacy enhanced recommender systems into two categories: distributed recommenders and data modification techniques. In the distributed group, user profiles are stored across several repositories. Canny pro-posed a decentralized storage of user profiles, which requires the adversary to compromise multiple systems when attack-ing a distributed recommender [3]. Vallet et al. [17] have shown how MF techniques can be leveraged to allow a cen-tral server to provide accurate recommendations without re-tention of user data, storing it on the client side instead.
Data modification techniques include approaches such as encryption [15], obfuscation [1], and randomization [16]. Po-lat and Du proposed to add uncertainty to user ratings through randomized data perturbation [16]: users substitute ratings in their profiles with modified ratings, resembling the real ones. Hence, if user data is exposed to an adver-sary, only the modified ratings will leak. Nikolaenko et al. showed how secure multiparty computation could be utilized in MF [15], so that the recommender learns only the item profiles, but not the user ratings. Such techniques, however, do not prevent the inference of user ratings from the output of MF, and are orthogonal to the techniques studied in this paper, as they address a different threat model.

Calandrino et al. [2] studied the privacy risks imposed by recommenders, such as Hunch, Last.fm, and Amazon. In item-to-item CF, when a user makes a transaction involving an item, this results in an increase of the similarity of the item to other items in the user X  X  transaction history. There-fore, the attacker can track the similarity lists of items asso-ciated with a target user, and identify new items in the lists. When the same item appears in a number of tracked lists, the attacker can infer that the item was added to the target user X  X  record. The authors pointed to differential privacy as a possible solution to this problem.

Differential privacy has drawn much research attention; it makes no assumptions about the adversary X  X  background knowledge and computation power, and provides formally provable privacy guarantees [7]. To the best of our knowl-edge, only two works have investigated the application of differential privacy to recommender systems, although not in the immediate context of MF.

Machanavajjhala et al. studied the problem of privacy-preserving social recommendations on the basis of a graph linking between users and items, e.g., items purchased by users [13]. A utility vector derived from the graph captures the utility of each item for the target user, and the goal is to induce a probability distribution over the items, such as to maximize the user X  X  utility, while keeping the vector private. It was found that good recommendations were achievable only under weak privacy parameters, or only for a small fraction of users.

McSherry and Mironov applied differential privacy to CF [14]. They used the Laplace mechanism to compute a differen-tially-private item-to-item covariance matrix, which was used to find neighbors and compute SVD recommendations. Their solution involved breaking the recommendation process into a learning phase, in which the private covariance matrix was derived, and a recommendation phase, in which the pre-dictions were computed. In contrast, we consider direct privacy-preserving derivation of the latent factor models. Overall, our work explores additional approaches beyond those investigated in [14], and compares their performance.
The input to MF is typically a sparse rating matrix R n  X  m containing the ratings of n users for m items. Each matrix element r ui reflects the rating of user u for item i . MF fac-torizes R n  X  m into two latent matrices of a lower dimension d : user-factor matrix P n  X  d and item-factor matrix Q The factorization is done such that R is approximated as a product of P and Q , i.e., each known rating r ui is approxi-mated by  X  r ui = p u  X  q | i . To obtain P and Q , MF minimizes the regularized squared error: where  X  regularizes the factors and prevents overfitting.
Two common ways to solve the resulting non-convex op-timization problem are stochastic gradient descent (SGD) and alternating least squares (ALS). In SGD, the factors are learned by iteratively evaluating the error e ui = r ui  X  p for each rating r ui , and updating the user and item vectors by taking a step in the direction opposite to the gradient of the regularized loss function: The constant  X  determines the rate of minimizing the error and is often referred to as the learning rate.

In ALS, the optimization problem is solved iteratively. In each iteration, one latent matrix (say, P ) is fixed, resulting in a convex optimization problem, where the solution (for Q ) can be found efficiently. Then, the other matrix ( Q ) is fixed, and the optimization problem is solved again (this time for P ). These steps are repeated until convergence.
Differential privacy is based on the principle that the out-put of a computation should not allow inference about any particular record in the input [7]. This is achieved by re-quiring that the probability of any computation outcome is insensitive to small input changes. We denote two datasets A and B as adjacent , A  X  B , if they are identical in all records but one. Formally, there exist a user u and an item i such that A = B \{ r ui } X  X  r 0 ui } , where r 0 ui and r ratings that u assigned to i in A and B , respectively. The guaranteed privacy level is measured by a parameter . For-mally, a randomized computation K maintains -differential privacy if for any two datasets A  X  B , and any subset S of possible outcomes in Range( K ), where the probability is over the randomness of K . Low values of correspond to a high degree of privacy. Setting the bounds for the acceptable value of is an open question. In the literature, privacy settings of = ln 2 or = ln 3 are considered as providing acceptable levels of privacy, al-though Dwork suggested that in some cases much higher values of could provide meaningful guarantees [6].

A common way to obtain differential privacy is by ap-plying random noise to the measurement. The amount of noise added depends on the L 1 -sensitivity of the evaluated function, which is the largest possible change in the mea-surement given a change in a single record in the dataset. In general, the L k -sensitivity of a function g is given by: where || X || k denotes the L k -norm.

The Laplace mechanism [7] obtains -differential privacy by adding noise sampled from Laplace distribution, with a calibrated scale b . The probability density function of Laplace distribution with mean 0 and scale b ( x  X  Laplace ( b )) is f b ( x ) = 1 2 b exp(  X  | x | b ). Given a function g : D  X  following computation maintains -differential privacy [7]:
For example, consider the function COUNT c ( A ), which counts the number of records in dataset A that satisfy condi-tion c . It has sensitivity 1, since changing a single record af-fects the count by at most 1. Hence, K ( A ) = COUNT c ( A )+ Laplace (1 / ) maintains -differential privacy. Consider also the function SUM( A ), where a i  X  [0 ,  X ]. It has sensitivity  X , which is the maximal change in the sum by changing one element of A . Hence, K ( A ) = SUM( A ) + Laplace ( X  / ) maintains -differential privacy.

We also rely in this work on the K -norm mechanism [9], which allows to calibrate noise to the L 2 -sensitivity of the evaluated function. Given a function g : D  X  R d , the com-putation K ( x ) = g ( x ) + r X  maintains -differential privacy, where r is a d -dimensional vector uniformly sampled from a d -dimensional sphere with radius 1, and  X   X   X ( d,S 2 ( g ) / ).
Differential privacy sets the conditions that should be maintained to preserve privacy, but within these constraints it is often possible to implement various mechanisms that evaluate the same computation, resulting in different privacy-accuracy trade-offs. Considering the stages of the MF pro-cess, we highlight a number of possible approaches for adding differentially private noise, as shown in Figure 1. Figure 1: Various noise application points in regards to the input, output and the solver within the MF mechanism
Input Perturbation . The original MF input ratings are perturbed with a calibrated noise, and then the algorithm is trained using the noisy input ratings. Since input per-turbation is performed before training the recommender, it can be followed by any recommendation algorithm, and in particular by (any variant of) MF.

In-process Mechanisms . In this approach, the algo-rithms used to decompose the rating matrix R into the la-tent matrices P and Q are adapted to maintain differential privacy. We focus in this work on two MF algorithms, and propose their differentially private variants:
SGD . In the training process of MF with SGD, in each iteration, the gradient of the regularized loss function deter-mines the direction of the update and its magnitude. In the gradient perturbation approach the gradient is perturbed with noise in each iteration.

ALS with Output Perturbation . In each step of ALS, two optimization problems are solved to update the matrices P and Q . These empirical risk minimization problems can be solved in a differentially-private manner using the techniques proposed in [4]. In particular, we apply the output pertur-bation approach to obtain noisy versions of P and Q . Output Perturbation . In this approach, a non-private MF algorithm is executed, and then the resulting latent fac-tors are perturbed to maintain differential privacy. Unfortu-nately, the optimization problem in MF is non-convex, and a small change in the input could lead to a large change in the factors. Consequently, the sensitivity of the optimization problem would require introducing large noise, potentially resulting in poor utility.

Hence, in this work we restrict the evaluation to three vari-ants of differentially private MF X  X nput perturbation, SGD perturbation, and ALS with output perturbation X  X nd do not consider the output perturbation approach, where noise is added to the latent factors, after a non-private MF. We first outline the data pre-processing steps that were taken before applying these approaches. For the pre-processing, we utilize the private versions of the following aggregate val-ues, based on the training dataset (will be described in detail in Section 5.1): global average GAvg ( R )  X  average of all the ratings for all items; item average IAvg ( i )  X  average rat-ing for item i ; and user average UAvg ( u )  X  average rating of user u . We will now describe the three aforementioned differentially private MF approaches.
Prior to applying differential privacy to MF, we prepro-cess the inputs as in [14]. A notable exception is that we Algorithm 1: Evaluation of item averages Input : Output : 2: for j = 1 to m do 3: Let R j = { r ui  X  R | i = j } 5: Clamp IAvg( j ) to [ r min ,r max ].
 Algorithm 2: Evaluation of users effects Input : Output : 1: Let R 0 = { r ui  X  IAvg( i ) | r ui  X  R } 3: for v = 1 to n do 4: Let R v = { r 0 ui  X  R 0 | u = v } 6: Clamp UAvg( v ) to [  X  2 , 2] incorporate the user averages in rating predictions, as this allows to derive more accurate predictions when using MF. The preprocessing consists of the following three steps.
Firstly, we compute the (differentially-private) average item ratings according to the process described in Algorithm 1. We add a number of fictitious ratings  X  i with the global av-erage GAvg to stabilize the item averages X  X his limits the effect of noise for items with few ratings, while only slightly affecting the average for items with many ratings. If the added noise causes an item average to go beyond the range of ratings [ r min ,r max ], the average is clamped to fit the range. Differential privacy is guaranteed by adding noise calibrated to the L 1 -sensitivity of ratings, given by  X  r = r max  X  r
Secondly, we follow the same technique to compute the user averages, as outlined in Algorithm 2. The basis for the user averages is the ratings after the item average discount-ing. We stabilize the user effects with the addition of  X  fictitious ratings with the newly computed global average. The user averages are also clamped to a bounded range (in the experiments 1 we used [  X  2 , 2] for user averages).
Finally, the item and user averages are discounted from the rating matrix R , and the resulting ratings are clamped. The clamping reduces the L 1 -sensitivity of the computa-tions conducted during the MF process, and results in a
In the evaluation we used the MovieLens dataset with the rating scale of 1 to 5 stars.
 Algorithm 3: Differentially Private Input Perturbation Input : Output : 1: Let R 0 = { r ui + Laplace(  X  r ) | r ui  X  R } 2: Clamp the ratings in R 0 to the range [  X  B,B ] 3: ( P,Q ) = min 4: return P and Q lower magnitude of noise being introduced in the differen-tially private computation. We denote the clamping param-eter B (set to 1 in the experiments), i.e., r ui  X  [  X  B,B ]. The pre-processed matrix R is passed to the MF algorithm to derive the matrices P and Q . Predicted ratings are then obtained through  X  r ui = IAvg( i ) + UAvg( u ) + p u q |
Differential privacy maintains the composability property: if each computation in a series of computations is i -differen-tially private, then the overall algorithm is P i i = -differen-tially private. Accordingly, the overall privacy budget is divided between the computation of global averages, item effects, user effects, and, lastly, MF. Note that it is possi-ble to predict ratings using only the user and item averages,  X  r ui = IAvg( i ) + UAvg( u ), which is referred to as Global Ef-fects (see Comparison Baselines in Section 5.1). This leads to its differentially-private counterpart, as described in the above three pre-processing steps. In this case, as MF is not applied, the privacy budget is divided between three com-putations: global averages, item averages and user averages. We refer to this technique as Private Global Effects .
In input perturbation the Laplace mechanism is applied directly to each input rating. Following data pre-processing, the sensitivity of the inputs is  X  r = r max  X  r min = 2 B , and perturbing each rating with noise sampled from the distribu-tion Laplace( X  r/ ) ensures -differential privacy. 2 The noisy ratings can then be clamped again, to limit the influence of excessive noise. Algorithm 3 summarizes this process.
The gradient perturbation approach, outlined in Algo-rithm 4, guarantees privacy throughout the MF process by introducing noise in the SGD step in each iteration of the algorithm. The error calculation conducted in each step is carried out with the Laplace mechanism to maintain differ-ential privacy, and consequently the SGD step maintains dif-ferential privacy. Optionally, the noisy error can be clamped to constrain the effect of noise (in our experiments we used e max = 2). The number of iterations k should be known in advance, so the noise introduced in each iteration is cal-ibrated to maintain /k -differential privacy. Composability ensures that the k iterations maintain the overall bound of -differential privacy.
Proofs of the differential privacy properties of algorithms in Sections 4.2-4.4 are omitted due to space limitations. Algorithm 4: Differentially Private SGD Input : Output : 1: Initialize random factor matrices P and Q . 2: for k iterations do 3: for each r ui  X  R do 4: e 0 ui = r ui  X  p u q | i + Laplace( k  X  r/ ) 5: Clamp e 0 ui to [  X  e max ,e max ] 6: q i  X  q i +  X  ( e 0 ui  X  p | u  X   X   X  q i ) 7: p u  X  p u +  X  ( e 0 ui  X  q | i  X   X   X  p u ) 8: return P and Q .
The basic idea of ALS is to alternately fix one of the la-tent matrices P and Q , and optimize the regularized loss function for the other matrix. Once one matrix is fixed, the optimization problem becomes convex and can be solved an-alytically. For example, once Q is fixed, the overall regular-ized loss function can be minimized by considering for each user u the following loss function defined over the subset of ratings R u = { r vi  X  R | v = u } : where n u = | R u | . Each user vector p u is then obtained by solving the risk minimization problem
The problem of differentially private empirical risk min-imization (ERM) was studied by Chaudhuri et al. [4]. An adaptation of their techniques shows that the L 2 -sensitivity of p u ( R,Q ) in Equation 7, is  X  p u = q max  X  r n the upper bound on the L 2 -norm of each row q i in Q . Simi-larly, when fixing P and optimizing Q based on the regular-ized loss function J P ( q i ,R ) = [ P R the L 2 -sensitivity of each row q i is p max  X   X  r  X n processed ratings, we have  X  r = 2 B , where B is the clamp-ing parameter. Since we calculated the L 2 -sensitivity of the user-vector p u and item-vector q i , the noise added to these vectors is taken from the Gamma distribution.

Following the above analysis, Algorithm 5 outlines a dif-ferentially-private ALS algorithm with output perturbation. Similarly to the SGD approach, we calibrate the noise so that each optimization problem is / 2 k -differentially private and the overall ALS computation is -differentially private due to composability.
In this section, we present the results of the evaluation of the proposed differentially private MF approaches.
 Algorithm 5: Differentially Private ALS with Output Perturbation Input : Output : 1: Initialize random factor matrices P and Q . 2: for k iterations do 3: for each user u , given Q do 4: Sample noise vector b with pdf 5: p u  X  arg min p u J Q ( p u ,R u ) + b 7: for each item i , given P do 8: Sample noise vector b with pdf 9: q i  X  arg min q i J P ( q i ,R i ) + b 11: return P and Q .

Table 1: Statistical properties of the MovieLens datasets We use in the evaluation the 100K, 1M and 10M Movie-Lens datasets. Table 1 summarizes selected statistical prop-erties of the datasets.

We use 10-fold cross validation 3 to train and evaluate the recommender system. We measure the accuracy of the predicted ratings  X  r ui using the Root Mean Square Error (RMSE) metric (averaged over all the ratings), computed by RMSE = P R ( r ui  X   X  r ui 2 / | R | ) crepancies in the introduction of noise, the reported RMSE is averaged across multiple runs.
 We compare the performance of the privacy-preserving MF approaches against the following baselines:
Global average: the average rating is computed over the entire training set, and used as the prediction for all the ratings in the test set, i.e.,  X  r ui = GAvg ( R ). We treat the global average RMSE as the upper bound for error.

Item average: the average rating for each item is com-puted over all the available training item ratings, and used as the prediction for all the ratings for that item in the test
We used Matlab and, specifically, crossvalind . Table 2: Summary of the experimental settings and results. set, i.e.,  X  r ui = IAvg ( i ). This baseline reflects the RMSE score attainable without personalization.

Global effects: the average ratings IAvg( i ) for each item i and UAvg( u ) for each user u are computed over the entire training set. The item and user biases are both used when predicting the test ratings,  X  r ui = IAvg( i ) + UAvg( u ). We treat this baseline as the most simple way to obtain person-alization, and we consider RMSE scores below this baseline to represent effective personalization.

Clean MF: ALS is executed to solve the MF problem with-out any noise. These RMSE scores reflect the lower bound for error attainable with no privacy constraints.

We use the item average and global effects baselines to assess the privacy-accuracy trade-off offered by the private approaches. To this end, we measure the values of the pri-vacy parameter for which the RMSE scores attained by each algorithm cross the RMSE scores of these baselines, where low values of indicate that the algorithm can pro-vide the same level of accuracy as the baseline with a low cost in privacy. Thus, the focus is on the privacy-accuracy trade-offs of the approaches, rather than on evaluating their performance for certain values of . Also, we investigate several factors that may affect the system performance.
In each experiment, given the overall -differential pri-vacy constraint, we allocated 0 . 3 to pre-processing. Out of this, 0 . 02 was used to compute the global averages (split between the user and item average calculations), whereas the user and item averages were computed with 0 . 14 each. The remaining privacy budget of 0 . 7 was allocated to MF. This distribution of the privacy budget is based on an offline optimization, which is beyond the scope of the paper.
Where applicable, we bounded the L 2 -norm of the user vectors to p max = 0 . 4, and of the item vectors to q max In both the SGD and ALS experiments, we set the number of iterations to k = 5. The number of iterations for input perturbation was set to k = 20. Table 2 details several other dataset specific parameters, which were set in an offline optimization. We note that the selected number of factors and the number of iterations were lower than typical for these algorithms, to limit the amount of noise introduced by differential privacy. Table 2 shows also the baseline RMSE scores measured for each dataset, and the values of for which each approach crossed those baselines.

Figures 2a and 2b show the privacy-accuracy trade-offs for all the approaches observed for the MovieLens-1M and MovieLens-10M datasets, respectively. 4 In addition to the aforementioned baselines, the figures show the results for the Input Perturbation approach followed by a non-private SGD algorithm (ISGD), the Private SGD approach (PSGD), and the Private ALS approach (PALS).

In general, the performance of all the approaches improves with the size of the dataset. For example, ISGD crosses the IA baseline for MovieLens-100K, MovieLens-1M, and MovieLens-10M at = 2, = 0 . 9 and = 0 . 7, respectively. This is not surprising; the larger the dataset, the more re-silient it is to the noise introduced through differential pri-vacy. Since the noise is calibrated to mask the effect of a single rating, larger datasets provide a higher signal-to-noise ratio, thereby allowing better performance with respect to the baseline for any value of .

As expected, crossing of the IA is observed for lower val-ues of than crossing of the GE baseline. This is explained by the lower degree of personalization offered by IA, which is achievable with higher levels of noise and, therefore, a higher degree of privacy. For all the datasets, the IA crossing val-ues of the approaches are similar, but there is a substantial difference between the GE crossings. Specifically, the IA crossings of PSGD and PALS are very close, and both are
Results obtained for MovieLens-100K exhibit a similar trend and are not shown, but are summarized in Table 2. slightly lower than that of ISGD. However, the GE crossing of ISGD is much lower than those of PSGD and PALS.
 For example, consider the MovieLens-10M dataset. The PSGD and PALS approaches both cross the IA baseline at = 0 . 6, whereas ISGD crosses it at = 0 . 7. This is ex-plained by the fact that the matrices P and Q in PALS and PSGD are bounded with L 2-norm bounds p max and q max . Bounding the L 2-norm provides a small improvement for low values of and gives PALS and PSGD a slightly ear-lier crossing. However, for higher , ISGD achieves a better performance; it crosses the GE baseline at = 2 . 1, whereas PSGD and PALS cross it at = 5 . 5 and = 6, respectively. Similar trade-offs were observed also for other datasets.
Figure 3 shows two variants of the PSGD approach, eval-uated using the MovieLens-1M dataset: the RMSE curve of PSGD extracted from Figure 2a and the curve of PSGD with exactly the same parameters but with no data pre-processing. Data pre-processing has a substantial effect on the RMSE, as it reduces the sensitivity and the required lev-els of noise X  X n particular for low , when the IA baseline crossing is considered. Similar trends were observed also for the PALS and ISGD approaches, and for other datasets. We also demonstrate the effect of bounding q max in PSGD. Specifically, we set p max to 80% of the q max value, while the regularizer  X  and the number of factors d are fixed to  X  = 0 . 03 and d = 7. We conduct the experiment using three values of q max : q max = 0 . 5, q max = 1, and q max = 2. Fig-ure 4 shows the results obtained for MovieLens-10M. While the value of q max does not affect much the crossing of the IA baseline, it changes the value of the GE crossings and
Figure 5: Comparison of private MF and kNN algorithms the accuracy achieved for higher values of . In PSGD, the L 2-norm bounds do not affect the noise added to P and Q and are used only to control the L 2-norm of the latent vec-tors. For low , more noise is added and a small bound is preferable over greater bounds, since it removes the noisy elements. However, for higher values of , a small bound prevents MF from fully realizing the potential of the seven factors, and, therefore, a higher bound achieves a better pre-dictive accuracy when less noise is added.
In this experiment we compare the results of the privacy-preserving MF approach to two other privacy preserving CF algorithms: a private version of the GE baseline and private k-Nearest Neighbors (kNN) algorithm [5].

For private GE, we used the following allocation for the privacy budget: 0 . 02 for the global average, 0 . 54 for item average and 0 . 44 for user average. For the private kNN rec-ommendation algorithm, we followed the approach of McSh-erry and Mironov [14]. 5 We applied a different privacy bud-get allocation: 0 . 9 was allocated to data pre-processing, out of which 0 . 02 was used for the global average, while the item and user averages were computed with 0 . 44 each, and the remaining 0 . 1 was used for the identification of nearest neighbors. It should be highlighted that kNN com-bines the differentially private item-to-item covariance ma-trix with the private user ratings, giving it an a-priori advan-tage over the proposed differentially private MF algorithms. Figure 5 shows the comparison of the private versions of MF and kNN for the MovieLens-1M dataset. 6 For low val-ues of , computing only the private GE was more effective than the MF approaches (both kNN and private GE cross the IA baseline at = 0 . 18), since it makes the smallest number of computations and introduces the lowest amount of noise. However, this approach cannot outperform the GE baseline, and therefore cannot take advantage of weaker pri-vacy constraints, when available.

While latent factors models typically outperform neigh-borhood based approaches in terms of predictive accuracy [5, 12], surprisingly this is not the case in the presence of privacy constraints. For lower values of , the improved ac-
The differentially private implementation of kNN outlined in [14] is not publicly available, such that we were not able to reproduce the exact results reported therein. Due to memory limitations, kNN implementation for the MovieLens-10M dataset was not feasible. curacy offered by MF in the non-private settings does not compensate for the higher noise required to meet the pri-vacy constraints. However, for higher and weaker privacy, the predictive accuracy advantage of MF becomes apparent, and it outperforms the private kNN algorithm.

We posit that the superiority of private neighborhood based approaches over the MF approaches is explained by their better resilience to the noise introduced by differential privacy. Linking the average number of user ratings (Table 1) with the number of latent factors (Table 2), we observe that each factor relies, on average, on a few dozens of rat-ings. Hence, applying even moderate noise deteriorates the signal-to-noise ratio and affects the predictions. In contrast, the private item-to-item covariance matrix relies on thou-sands of ratings and is more resilient to noise. Due to this, kNN outperforms MF for lower values of (stringent privacy constraints). However, higher values of (lenient privacy constraints) allow decreasing the level of noise applied, such that MF approaches outperform kNN.
To address privacy concerns of recommender systems, we investigated the application of differential privacy to MF, the state-of-the-art recommendation approach. Differential privacy does not dictate a specific way to conduct a compu-tation, but is rather a property that should be maintained. Hence, it is possible to design various approaches that carry out the same computation in a differentially private man-ner, with different levels of effectiveness. We proposed and evaluated three approaches reflecting the stages of MF: in-put perturbation, and differentially private variants of ALS and SGD. We also analyzed the sensitivity of the proposed approaches and compared private MF to other privacy pre-serving recommender approaches, namely, GE and kNN.
We showed that input perturbation yields the best perfor-mance amongst the three evaluated private MF approaches. However, when privacy is a priority and high degree of noise is applied, private kNN outperforms MF. We believe this observation is inherent to sparse datasets and stringent pri-vacy requirements, as kNN is not as sensitive to noise as MF. On the other hand, when weaker privacy settings are acceptable, MF offers a better alternative: in that case, the predictive accuracy of the private algorithms gets closer to that of the respective non-private variants, and MF is shown to outperform other private recommendation approaches.
Following our evaluation, we identified the following de-sign choices that should be considered when applying differ-ential privacy to recommender systems.

Contextual considerations . Data characteristics, such as size, density and the distribution of ratings, may affect the privacy-accuracy trade-offs of the approaches. Beyond these, additional factors need to be considered. For example, the scalability and flexibility of the model-based approaches may outweigh the advantage of neighborhood methods in privacy protection, making privacy-preserving MF algorithm a vi-able option. Also, methods like input perturbation may be more amenable to the processing of streaming data, since each new rating can be perturbed independently, whereas for other approaches further work is required to adapt in-cremental learning models to the private setting.

Mind your parameters . Typically, MF parameters such as the number of factors, the regularizer, and the learning rate are tuned to increase prediction accuracy, while pre-venting over-fitting and ensuring convergence. In the private setting, these considerations should be augmented to incor-porate their impact on the introduced noise. For example, increasing the number of factors results in larger L 2 -norms of the latent vectors, and requires larger magnitudes of noise to obtain the same level of privacy. This noise abolishes the increased accuracy driven by the additional factors, and pa-rameter tuning is needed to balance these effects. [1] S. Berkovsky, T. Kuflik, and F. Ricci. The impact of [2] J. A. Calandrino, A. Kilzer, A. Narayanan, E. W. [3] J. Canny. Collaborative filtering with privacy. In IEEE [4] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. [5] C. Desrosiers and G. Karypis. A comprehensive survey [6] C. Dwork. Differential privacy: a survey of results. In [7] C. Dwork, F. Mcsherry, K. Nissim, and A. Smith. [8] A. Friedman, B. Knijnenburg, K. Vanhecke, [9] M. Hardt and K. Talwar. On the geometry of [10] A. J. Jeckmans, M. Beye, Z. Erkin, P. Hartel, R. L. [11] A. Kobsa. Privacy-enhanced web personalization. In [12] Y. Koren, R. Bell, and C. Volinsky. Matrix [13] A. Machanavajjhala, A. Korolova, and A. D. Sarma. [14] F. McSherry and I. Mironov. Differentially private [15] V. Nikolaenko, S. Ioannidis, U. Weinsberg, M. Joye, [16] H. Polat and W. Du. Achieving private [17] D. Vallet, A. Friedman, and S. Berkovsky. Matrix [18] U. Weinsberg, S. Bhagat, S. Ioannidis, and N. Taft.
