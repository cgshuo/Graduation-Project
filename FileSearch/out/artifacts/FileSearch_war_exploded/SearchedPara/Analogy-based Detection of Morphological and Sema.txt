 The recent boom of research on analogies with word embedding models is largely due to the striking demonstration of  X  X inguistic regularities X  (Mikolov et al., 2013b). In the so-called Google analogy test set (Mikolov et al., 2013a) the task is to solve analo-gies with vector offsets (a frequently cited example is king -man + woman = queen ). This test is a pop-ular benchmark for word embeddings, some achiev-ing 80% accuracy (Pennington et al., 2014).

Analogical reasoning is a promising line of re-search, since it can be used for morphological anal-ysis (Lavall  X  ee and Langlais, 2010), word sense dis-ambiguation (Federici et al., 1997), and even for broad-range detection of both morphological and semantic features (Lepage and Goh, 2009). How-ever, it remains to be seen to what extent word em-beddings capture the  X  X inguistic regularities X . The Google analogy test set includes only 15 relations, and K  X  oper et al. (2015) showed that lexicographic relations such as synonymy are not reliably discov-ered in the same way.

This study systematically examines how well var-ious kinds of linguistic relations can be detected with the vector offset method, and how this pro-cess is affected by window size and dimensional-ity of count-based word embeddings. We develop a new, more balanced test set (BATS) which includes 99,200 questions in 40 morphological and semantic categories. The results of this study are of practical use in real-world applications of analogical reason-ing, and also provide a more accurate estimate of the degree to which word embeddings capture linguistic relations. Current research on analogical reasoning in word embeddings focuses on the so-called  X  X roportional analogies X  of the a:b::c:d kind. The task is to detect whether two pairs of words have the same relation. A recent term is  X  X inguistic regularity X  (Mikolov et al., 2013b), used to refer to any  X  X im-ilarities between pairs of words X  (Levy et al., 2014). Analogies have been successfully used for detect-ing different semantic relations, such as synonymy and antonymy (Turney, 2008), ConceptNet relations and selectional preferences (Herdadelen and Baroni, 2009), and also for inducing morphological cate-gories from unparsed data (Soricut and Och, 2015).
The fact that analogies are so versatile means that to make any claims about a model being good at analogical reasoning, we need to show what types of analogies it can handle. This can only be de-termined with a comprehensive test set. However, the current sets tend to only include a certain type of relations (semantic-only: SAT (Turney et al., 2003), SemEval2012-Task2 (Jurgens et al., 2012), morphology-only: MSR (Mikolov et al., 2013b)). The Google analogy test (Mikolov et al., 2013a) contains 9 morphological and 5 semantic categories, with 20-70 unique word pairs per category which are combined in all possible ways to yield 8,869 seman-
None of the existing tests is balanced across dif-ferent types of relations (word-formation getting particularly little attention). With unbalanced sets, and potentially high variation in performance for different relations, it is important to evaluate results on all relations, and not only the average.

Unfortunately, this is not common practice. De-spite the popularity of the Google test set, the only study we have found that provides data for indi-vidual categories is (Levy et al., 2014). In their experiments, accuracy varied between 10.53% and 99.41%, and much success in the semantic part was due to the fact that the two categories explore the same capital:country relation and together consti-tute 56.72% of all semantic questions. This shows that a model may be more successful with some re-lations but not others, and more comprehensive tests are needed to show what it can and cannot do.
Model parameters can also have a major impact on performance (Levy et al., 2015; Lai et al., 2015). So far they have been studied in the context of se-mantic priming (Lapesa and Evert, 2014), semantic similarity tasks (Kiela and Clark, 2014), and across groups of tasks (Bullinaria and Levy, 2012). How-ever, these results are not necessarily transferable to different tasks; e.g. dependency-based word embed-dings perform better on similarity task, but worse on analogies (Levy and Goldberg, 2014a). Some stud-ies report effects of changing model parameters on general accuracy on Google analogy test (Levy et al., 2015; Lai et al., 2015), but, to our knowledge, this is the first study to address the effect of model parameters on individual linguistic relations in the context of analogical reasoning task. We introduce BATS -the Bigger Analogy Test Set. It covers 40 linguistic relations that are listed in ta-ble 1. Each relation is represented with 50 unique word pairs, which yields 2480 questions (99,200 in all set). BATS is balanced across 4 types of rela-tions: inflectional and derivational morphology, and lexicographic and encyclopedic semantics.
 A major feature of BATS that is not present in MSR and Google test sets is that morphological cat-egories are sampled to reduce homonymy. For ex-ample, for verb present tense the Google set includes pairs like walk:walks , which could be both verbs and nouns. It is impossible to completely elimi-nate homonymy, as a big corpus will have some cre-ative uses for almost any word, but we reduce it by excluding words attributed to more than one part-of-speech in WordNet (Miller and Fellbaum, 1998). After generating lists of such pairs, we select 50 pairs by top frequency in our corpus (section 4.2).
The semantic part of BATS does include homonyms, since semantic categories are overall smaller than morphological categories, and it is the more frequently used words that tend to have mul-tiple functions. For example, both dog and cat are also listed in WordNet as verbs, and aardvark is not; an homonym-free list of animals would mostly con-tain low-frequency words, which in itself decreases performance. However, we did our best to avoid clearly ambiguous words; e.g. prophet Muhammad was not included in the E05 name:occupations sec-tion, because many people have the same name. The lexicographic part of BATS is based on SemEval2012-Task2, extended by the authors with words similar to those included in SemEval set. About 15% of extra words came from BLESS and EVALution. The encyclopedic section was com-piled on the basis of word lists in Wikipedia and Table 1: The Bigger Analogy Test Set: categories and examples are based on the Google test, and category E09 -on the color dataset (Bruni et al., 2012). In most cases we did not rely on one source completely, as they did not make the necessary distinctions, included clearly ambiguous or low-frequency words, and/or were sometimes inconsistent 3 (e.g. sheep:flock in EVA-Lution is a better example of member:collection re-lation than jury:court ).
 Another new feature in BATS, as compared to the Google test set and SemEval, is that it contains sev-eral acceptable answers (sourced from WordNet), where applicable. For example, both mammal and canine are hypernyms of dog . 4.1 The vector offset method As mentioned above, Mikolov et al. (2013a) sug-gested to capture the relations between words as the offset of their vector embeddings. The answer to the question  X  a is to b as c is to ? d  X  is represented by hid-den vector d , calculated as argmax d  X  V ( sim ( d, c  X  a + b )) . Here V is the vocabulary excluding words a , b and c and sim is a similarity measure, for which Mikolov and many other researchers use angular dis-tance: sim ( u, v ) = cos ( u, v ) = u  X  v
Levy and Goldberg (2014b) propose an alterna-tive optimization objective: argmax d  X  V ( cos ( d  X  c, b  X  a )) They report that this method produces more accurate results for some categories. Essentially it accounts for d  X  c and b  X  a to share the same direc-tion and discards lengths of these vectors.

We supply the BATS test set with a Python eval-report results calculated by the Mikolov X  X  method for the sake of consistency, but some authors choose the best result for each category from each method (Levy and Goldberg, 2014b). 4.2 Corpus and models One of the current topics in research on word em-beddings is the (de)merits of count-based models as compared to the neural-net-based models. While some researchers find that the latter outperform the former (Baroni et al., 2014), others show that these approaches are mathematically similar (Levy and Goldberg, 2014b). We compare models of both types as a contribution to the ongoing dispute. Our count-based model is built with Pointwise Mutual Information (PMI)frequency weighting. In the dimensionality reduction step we used the Sin-gular Value Decomposition (SVD), raising  X  matrix element-wise to the power of a where 0 &lt; a  X  1 to give a boost to dimensions with smaller variance Caron (2001). In this study, unless mentined oth-erwise, a = 1 . The co-occurrence extraction was performed with the kernel developed by Drozd et al. (2015). As a representative of implicit models we chose GloVe (Pennington et al., 2014) that achieved the highest performance on the Google test set to this date. Our source corpus combines the English Wikipedia snapshot from July 2015 (1.8B tokens), Araneum Anglicum Maius (1.2B) (Benko, 2014) and ukWaC (2B) (Baroni et al., 2009). We discarded words occurring less than 100 times, resulting in vo-cabulary of 301,949 words (uncased).

To check the validity of our models we evaluate it with the Google test set for which there are nu-merous reported results. For GloVe we used the parameters from the original study (Pennington et al., 2014): 300 dimensions, window 10, 100 iter-ations, x max = 100, a = 3/4, sentence borders ig-nored. For comparison we also built an SVD model with 300 dimensions and window size 10. On our 5 B corpus GloVe achieved 80.4% average accuracy (versus 71.7% on 6 B corpus in the original study). The comparable SVD model achieved 49.9%, as op-posed to with 52.6% result reported by Levy et al. (2015) for 500 dimensions, window size 10 on 1.5 B Wikipedia corpus.

To evaluate effects of window size and dimen-sionality we built 19 SVD-based models for win-dows 2-8 at 1000 dimensions, and for dimensions 100-1200 for window size 5. 5.1 Word category effect Figure 1 presents the results of BATS test on the GloVe model (built with the parameters from the original study (Pennington et al., 2014)), and the best performing SVD model, which was the model with window size 3 at 1000 dimensions. The model built with the same parameters as GloVe achieved only 15.9% accuracy on BATS, and is not shown.
While GloVe outperforms the SVD-based model on most categories, neither of them achieves even 30% accuracy, suggesting that BATS is much more difficult than the Google test set. Many categories are either not captured well by the embedding, or cannot be reliably retrieved with vector offset, or both. The overall pattern of easier and more dif-ficult categories is the same for GloVe and SVD, which supports the conclusion of Levy and Gold-berg (2014b) about conceptual similarity of explicit and implicit models. The overall performance of both models could perhaps be improved by parame-ters that we did not consider, but the point is that the current state-of-the-art in analogical reasoning with word embeddings handles well only certain types of linguistic relations, and there are directions for im-provement that have not been considered so far.
The high variation we observe in this experiment is consistent with evidence from systems competing at SemEval2012-Task2, where not a single system was able to achieve superior performance on all sub-categories. Fried and Duh (2015) also showed a sim-ilar pattern in 7 different word embeddings.
As expected, inflectional morphology is overall easier than semantics, as shown even by the Google test results (see Skip-Gram (Mikolov et al., 2013a; Lai et al., 2015), GloVe (Pennington et al., 2014), and K-Net (Cui et al., 2014), among others). But it is surprising that derivational morphology is signifi-cantly more difficult to detect than inflectional: only 3 categories out of ten yield even 20% accuracy. The low accuracy on the lexicographic part of BATS is consistent with the findings of K  X  oper et al. (2015). It is not clear why lexicographic rela-tions are so difficult to detect with the vector offset method, despite numerous successful word similar-ity tests on much the same relations, and the fact that BATS make the task easier by accepting sev-eral correct answers. The easiest category is binary antonyms of the up:down kind -the category for which the choice should be the most obvious in the semantic space.

A typical mistake that our SVD models make in semantic questions is suggesting a morphologi-cal form of one of the source words in the a:b::c:d analogy: cherry:red :: potato:?potatoes instead of potato:brown . It would thus be beneficial to exclude from the set of possible answers not only the words a , b and c , but also their morphological forms. 5.2 Window size effect Evaluating two count-based models on semantic and syntactic parts of the Google test set, Lebret and Collobert (2015) shows that the former benefit from larger windows while the latter do not. Our exper-iments with SVD models using different window sizes only partly concur with this finding.

Table 2 presents the accuracy for all categories of BATS using a 1000-dimension SVD model with window size varying between 2 and 8. The codes and examples for each category are listed in table 1. All categories are best detected between win-dow sizes 2-4, although 9 of them yield equally good performance in larger windows. This indicates that there is not a one-on-one correspondence be-tween  X  X emantics X  and  X  X arger windows X  or  X  X or-Table 2: Accuracy of SVD-based model on 40 BATS categories, window sizes 2-8, 1000 dimensions phology X  and  X  X maller windows X . Also, different categories benefit from changing window size in dif-ferent ways: for noun plurals the difference between the best and the worse choice is 13%, but for cate-gories where accuracy is lower overall there is not much gain from altering the window size.

Our results are overall consistent with the evalu-ation of an SVD-based model on the Google set by Levy et al. (2015). This study reports 59.1% average accuracy for window size 2 yields, 56.9% for win-dow size 5, and 56.2% for window size 10. How-ever, using window sizes 3-4 clearly merits further investigation. Another question is whether changing window size has different effect on different models, as the data of Levy et al. (2015) suggest that GloVe actually benefits from larger windows. 5.3 Vector dimensionality effect Intuitively, larger vectors capture more information about individual words, and therefore should in-crease accuracy of detecting linguistic patterns. In our data this was true of 19 BATS categories (I01-02, I04, I06, D02-03, D05-07, E01, E03, E07, E10, L03-04, L07-10): all of them either peaked at 1200 dimensions or did not start decreasing by that point.
However, the other 20 relations show all kinds of patterns. 14 categories peaked between 200 and 1100 dimensions, and then performance started de-creasing (I03, I05, I07-10, D01, D04, D09, E02, E05, E09, L1, L6). 2 categories showed negative effect of higher dimensionality (D08, E04). Finally, 2 categories showed no dimensionality effect (E08, L05), and 3 more -idiosyncratic patterns with sev-eral peaks (D10, E02, L06); however, this could be chance variation, as in these categories performance was generally low (under 10%). Figure 2 shows sev-
The main takeaway from this experiment is that, although 47.5% of BATS categories do perform bet-ter at higher dimensions (at least for SVD-based models), 40% do not, and, like with window size, there is no correlation between type of the relation (semantic or morphological) and its preference for a higher or low dimensionality. One possible ex-planation for lower saturation points of some rela-tions is that, once the dimensions corresponding to the core aspects of a particular relation are included in the vectors, adding more dimensions increases noise. For practical purposes this means that choos-ing model parameters would have to be done to tar-get specific relations rather than relation types. 5.4 Other parameters In scope of this study we did not investigate all pos-sible parameters, but our pilot experiments show that changing the power a for the  X  matrix of the SVD transformation can boost or decrease the perfor-mance on individual categories by 40-50%. Smaller value of a gives more weight to the dimensions which capture less variance in the original data, which can correspond to subtle linguistic nuances. However, as with windows and dimensions, no set-ting yields the best result for all categories.
A big factor is word frequency, and it deserves more attention than we can provide in scope of this paper. Some categories could perform worse be-cause they contain only low-frequency vocabulary; in our corpus, this could be the case for D01 and not yield higher accuracy even if the frequency dis-tribution is comparable with that of an  X  X asier X  cat-egory (e.g. D8 and E10). Also, SVD was shown to handle low frequencies well (Wartena, 2014). This study follows up on numerous reports of suc-cessful detection of linguistic relations with vector offset method in word embeddings. We develop BATS -a balanced analogy test set with 40 morpho-logical and semantic relations (99,200 questions in total). Our experiments show that derivational and lexicographic relations remain a major challenge. Our best-performing SVD-based model and GloVe achieved only 22.1% and 28.5% average accuracy, respectively. The overall pattern of  X  X asy X  and  X  X if-ficult X  categories is the same for the two models, of-fering further evidence in favor of conceptual sim-ilarity between explicit and implicit word embed-dings. We hope that this study would draw atten-tion of the NLP community to word embeddings and analogical reasoning algorithms in context of lexico-
Our evaluation of the effect of vector dimension-ality on accuracy of analogy detection with SVD-based models shows that roughly half BATS cate-gories are best discovered with over 1000 dimen-sions, but 40% peak between 200 and 1100. There does not seem to be a correlation between type of linguistic relation and preference for higher or low dimensionality. Likewise, our data does not confirm the intuition about larger windows being more ben-eficial for semantic relations, and smaller windows -for morphological, as our SVD model performed best on both relation types in windows 2-4. Further research is needed to establish whether other models behave in the same way.
