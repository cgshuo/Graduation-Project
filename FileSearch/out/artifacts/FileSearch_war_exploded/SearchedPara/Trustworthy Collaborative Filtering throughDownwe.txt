 The recent growth of online shopping has e nabled an active interaction between customers and merchants. This increa se is fueled not only by more products and service providers but also by the byproduct of online transactions: the huge amount of purchase history and customer reviews, which promote a better un-derstanding of customer behaviors and preferences and lead to an effective rec-ommender system.
 Existing approaches try to exploit the user feedback from two perspectives. Collaborative Filtering (CF)-based methods recommend items through discover-ing purchasing patterns of users who purchase similar products and items which were purchased by similar group of users. Various techniques have been pro-posed to solve the problem. The underlying intuition is to reveal the  X  X eople who bought this item also bought X  patterns hidden in prior transactions. An-other stream of recommendation algorithms is to exploit the similarity between users and products. Their aim is to measure the distance in terms of content features, such as the profile of users and specifications of products. Due to the explosive increase of user reviews, there has been an increasing interest in lever-aging the opinions of customer comments.

As the online market extends greater and greater, many problems of the tra-ditional recommender systems have been exposed. A fundamental advantage of EC is its openness. It motivates users to contribute more contents and opinions to the platform. However, the openness makes such websites a main channel for fake reviews and spams, and it also makes the ratings highly biased.
As reported in [29], nearly one third of online reviews of some products are found to be fake and from spammers. Such reviews are for different malicious purposes and are definitely not helpful for recommendation. Various work has been done to investigate how to identify such spam in review. Thus, it is favor-able if a recommender system can automatically reduce information brought by spams. Without considering the importance of ratings and reviews from different users, recommender systems will be entangled with such noise.

Since products in different departments are of different popularity, traditional recommendations suffer from a biased rating data. Ratings on products from popular departments are over emphasized, as many users X  ratings are overlapped and thus are not that informative, while the useful ratings from less popular categories are ignored. It will be favorable if such information redundancy from the same group can be reduced.

In this paper, we aim to investigate how to solve the problem and propose a more trustworthy recommendation fram ework. Inspired by recent progress of review quality analysis, we first examine the reviews of customers to reveal their credibility. Through analyzing the links between customers and products, we further induce the importance of different products. The user credibility and product importance is combined into a unified framework. In order to incor-porate both factors, several research c hallenges need to be addressed: 1) User credibility and product utility is an implicit index and is not available for most EC platforms; 2) Information redundancy widely exists between homogeneous customers.

To cope with the challenges mentioned above, we put forward a trustworthy framework which actively selects and upweights customers and items of higher utility simultaneously. Contributions of this work can be summarized as follows: 1)By studying the quality of user review as well as links between users and items, we calculate customer credibility and product utility. 2)A unified framework which actively considers the importance of customers and products is proposed. It is formulated in a decent mathematica l form and can be optimized efficiently. 3)Various experiments are employed to test the effectiveness of our model and several competitive baselines. The data is obtained from real world websites and is publicly available.

The remaining sections of the paper are organized as follows: In Section 2, some related research results are discu ssed. The heuristics we employed to esti-mate user and product weight are discussed in Section 3. The proposed model is presented in Section 4. In Section 5 we in troduce the experimental settings and results and the paper is finally concluded in Section 6. In this section, we will describe some related work on recommender systems [4]. There are various kinds of recommendation algorithms, here we briefly review two main genres.

Collaborative Filtering : Collaborative filtering (CF) may be the most suc-cessful tool in leveraging the user prior transactions to predict future interests. There are two types of collaborative filte ring techniques, i.e., user-based and item-based CF. The user-based appr oach recommends an item to a user based on the opinions of other similar users who purchased it; Item-based CF approach recommends items to users based on the other items with high correlations. For both approaches, similarity measurem ent between users and items is of great significance. The widely used distance met rics include Pearson correlation coef-ficient, cosine-based similarity, vector space similarity and so on [1]. There are also several hybrid approaches [21]. In addition, associative retrieval techniques are applied in [12]. Authors of [11] try to propose algorithms for processing im-plicit feedbacks. An approach for incorporating externally specified aggregate ratings information into CF methods are introduced in [28].

Collaborative filtering has been frequently reduced into a latent factor prob-lem in recent years. Models based on matrix factorization [17] have have proven to be useful and dominated leader board of recommendation challenges. Variants of matrix factorization using flexible regression priors [30] were also proven to be useful. However, it is difficult to explain p redictions in latent factor models such as SVD. Supervised random walks for predicting and recommending links in social networks is adopted by Backstrom et al. [3]. A method called functional matrix factorization is proposed by Zhou et al. [31] to handle the cold-start problem. A collective probabilistic factor model is utilized [22] for Web site rec-ommendation. On the basis of co-clustering, Leung et al. propose a collaborative location recommendation framework [19].

Our work differs from the existing CF methods from incorporating the weights of items and users. Most CF algorithms treat all instances as equal weight. However, the assumption does not hold i n real applications as some experienced users X  ratings are clearl y more useful than others.

Content-based Recommender System : Another main ca tegory of recommen-dation algorithms are Content-based (CB) recommender systems. They try to recommend items similar to those a given user liked in the past. Specifically, the process of CB recommender system is p erformed in three steps [20]. In the content analyzer phase, the content of ite ms is represented in a form suitable for the next processing steps, i.e., the items profiles. There are mainly three kinds of retrieval models for extracting items profiles from Web pages, news articles or product descriptions: keyword-based model (e.g., [2]), semantic analysis by using ontologies [5], semantic analysis by using encyclopedic knowledge sources (e.g., [18]). The profile learner module c ollects data representative of the user preferences and generalizes this data, so that the user profile can be constructed. The most used learning algorithms in generalization include naive Bayes classi-fier (e.g., [7]), relevance feedback [26] [2], and other methods, such as decision tree and nearest neighbor algorithms (e.g., [23],[25]). The final module X  X iltering component recommends relevant items to u sers based on the similarity [5] be-tween the user profile and item profiles.

Recently, user tagging activity has been taken into account within content-based recommender systems. In [9], tag-based user profiles are exploited for producing music recommendations. In this section, we introduce our algorithms to predict the customer credibility and product utility. 3.1 Customer Credibility Modeling In online customer review forum, users can often provide two kinds of informa-tion: a numerical rating, often a scalar from one to five/ten or  X  X umber of stars X , together with a small paragraph describing their feedback. Our aim is to learn a credibility score from the user comments. Here, the credibility means how useful the users X  ratings are for the recommendation.
 Here we denote user-review matrix as C  X  R m  X  n , user-rating matrix as R  X  R m  X  p and ground truth as y  X  R m ,where m is the number of users and n is the number of features of user comment (normally the size of vocabulary of reviews) and p is the number of products. In our work, y is captured through measuring how close the user X  X  rating is to the average rating. In order to avoid spamming, a product X  X  rating will be used only if its rating number exceeds a predefined threshold. To better model the user revie ws, we design different kinds of features based on previous work of review spam det ection [14]. As related research has a long literature history and numerous res earch work has been proposed, different existing techniques can be used to exten d our system. However, since our work focuses mainly on the learning framework, the optimal solution of evaluating utility of customer and product remains to be found in future work.
Although our solution of modeling customer credibility is not optimal, even near optimal solutions will also bring in some noises. The key issue is to reduce negative effect in the learning framework, which will be discussed later. 3.2 Product Utility Modeling Since users will purchase products they are interested in, the transactions can build up links upon different customers and products. Such links can be regarded as an iterative reinforcement. In this work, in order to capture product utility, we adopt the HITS algorithm [15].

We regard each user as a hub node and product as an authority. The ini-tial value of users is their credibility. By employing HITS on the user-product network, a utility score can be achieved for every product. Similarly, we refer utility of product as how useful the product X  X  ratings are for the recommendation algorithm.

To avoid bringing noise into the model, we remove two kinds of links: If a user X  X  rating is below average rating of the product, the links are removed; Links from low credible users are removed. The removal of edges is similar to the heuristics for detecting spammers in social media websites[10]. In this section, we will introduce the proposed Trustworthy Collaborative Fil-tering. The proposed framework enables the recommendation to be more trust-worthy by downweighting nodes with lower credibility and products with less utility. Exclusive group regularizer is further introduced to reduce redundancy from homogeneous users. 4.1 Dual Weighted Dimension Reduction Existing recommender systems often re gard people equally weighted. In real applications, a user weight T  X  R 1  X  m and a product weight Q  X  R 1  X  p are often available, which are always ignored during building up recommendation models.
Eq. 1 is a general matrix factorization form of collaborative filtering, which aims to build up two low rank factors through reducing the reconstruction loss. K is the number of latent dimensions, which is normally smaller than p. Clearly, each user vector u i is regarded as equally weighted. In order to incorporate T and Q , Eq.1 can be reformulated as follows: where T i is the weight of user i and Q j is the utility score of product j. By assigning small weight to less credible users and less useful products, their neg-ative effect will be reduced. Since a negative value is difficult to be explained, similar to previous research, here we enforce the two factors W and H to be nonnegative.

Since Eq. 2 is not jointly convex to W and H , a local minimum can be achieved through alternatively updating them.
We first fix H and update W .When H is fixed, the problem can be reduced to where tr (  X  ) calculates the trace of a matrix and A i = diag ( E i  X  ). E is the weight matrix which records weight of each entry E ij = T i  X  Q j .Inordertosolvethe problem, we introduce a Langrange multiplier  X  =[  X  ik ] for the nonnegative constraint. The partial d erivatives with respect to W ik are: Then the derivatives can be reduced to the following form through using the KKT condition  X  ik U ik =0: Then we can get the update rule of W : where is the element wise product between matrices.

Similarly, when W is fixed, the update rule of H can be reduced to the following formulation:
Here, the update rules are similar to the weighted NMF [8] when we denote the weight of users and products as a dual weight matrix E . 4.2 Redundancy Reduction A problem of directly using the weight is, members from communities with higher popularity and activeness will dominate the calculation, which is not desirable. In order to upweight the nodes from less active communities, we introduce the intra-group competition in this section.

In order to find the community structure of online users, we leverage the rela-tionships between different users. We adopt the influence maximization approach [27] based on modularity. In order to build links between users, we calculate the number of products on which they both commented. If the number exceeds the predefined threshold, a link is built between the two users.

Modularity is introduced to measure the community structure of complex networks [6] [24].Modularity measures to what extent the links between different nodes are deviating from a random graph. Given degree of two nodes d i and d j , the expected number of links is d i d j 2 m . Here 2m is the normalizing factor which represents the number of total edges in a graph.
 Eq. 8 depicts the formulation of modularity, where s i is the community mem-bership of node i and  X  ( s i ,s j )=1if s i = s j and 0 otherwise. J is the adjacency matrix of the underlying graph. Thus, the more active the intragroup interaction is, the larger modularity will be achieved. Through maximizing the modularity of a network, optimal group membershi ps can be induced from the links between nodes.
 For simplicity, here we introduce B : Then the modularity can be reformulated in the matrix form: where S i,j denotes the probability of assigning node i to affiliation j. Finding the optimal group memberships maximizing the modularity can be induced into finding the top-r eigenvectors of B .

Then the group membership detection p roblem is reduced as a soft clustering problem. We employ standard K-means algorithm on the top-r eigenvectors of B to get the group memberships. For simplicity, we also use S  X  R m  X  k to denote such group memberships. We denote the number of total groups as k.

In order to reduce information redundancy from similar users, we introduce the exclusive regularizer here: lates the sum of squares of different groups. The minimization process is similar to group LASSO, which first calculates 2 norm and then 1 . The distinct feature of 1 , 2 norm is that it achieves sparsity on the intra-group level. Concretely, it enforces more entries in a group to be zero. In the scenario of recommendation, such property reduces overlapped information from users in the same group.
By the definition of 1 , 2 norm, Eq. (11) can be formulated as:
The objective in Eq. 2 can then be reformulated as:
For simplicity, we replace SS T by D .Eachentry D i,j denotes the correlation between two users. If the correlation D i,j is very large, it avoids user i and j X  X  weights T i and T j to be large simultaneously, which finally results in the intra-group sparsity. 4.3 Optimization The optimization objective is not jointly convex to W , H and T . Again, we update them in an alternative manner.

When W and H are fixed, the objective function can be reformulated as: a standard quadratic programming (QP) problem and can be solved efficiently. Here we introduce two parameters, the upper bound ub and lower bound lb , to control the intra-group sparsity. A higher ub gives more freedom to those influencers and makes more entries to zero; A higher lower bound lb alleviates the sparsity and forces the weight distribution to tend to the uniform distribution.
When T is fixed, the optimization has the same form with Eq.(2). Thus, W and H can be updated iteratively as d enoted in Eq.(6) and Eq.(7).

The updating process keeps iterating unti l convergence or reaches the maximal number of iterations. 5.1 Dataset To validate our algorithm on real world applications, we selected the data set from Douban 1 . It contains various entities including books, users and user ratings and reviews towards different items. There are over 10 thousand users and 1 million ratings in our data set. We extract a dataset which contains both rating and reviews from the data set. 5.2 Evaluation In order to measure how the algorithm makes mistakes during recommendation, first, we adopt Mean Absolute Error (MAE): Here, a smaller MAE score means the algorithm makes fewer mistakes and thus is a better model.

In real applications, top-k performance is one of the most important criteria, since users may not spend that much time looking at low ranked recommended items. So we also adopt nDCG@5[13] in our experiment. 5.3 Settings The main aim of our experiment is to test whether the incorporated user and product weights are useful for recommendation. So we adopt three baselines to compare with. 1) CF : The Collaborative Filtering method which only uses the rating data between users and products. Since there a re various variants, we selected Low-rank matrix factorization [16] for our work. 2) PU : The method that only uses the Product Utility (PU). 3) UC : The method that only uses the User Credibility (UC). 4) TCF : The proposed Trustworthy Collaborative Filtering framework.
More complex and advanced variants of co llaborative filtering are not selected in our work, since our work is a generalized framework of exploiting user credibil-ity and product utility and can easily be extended and embedded with other CF algorithms. For the experiment, ten fold cross validation is employed for getting the final results. Data is randomly split into ten folds and one fold is selected as testing data while the rest as training data in each round. After ten rounds, the average MAE and nDCG are reported. 5.4 Experimental Results Figure 1 illustrates the Mean Absolute Error of different methods. The lowest error is achieved by our proposed framework, which outperforms the methods that ignores either user credibility or product utility or both. This proves that the incorporated components are effective in improving recommendations. Col-laborative Filtering achieves worst among all methods. PU method which only considers the product utility is the second worst method. This is possibly due to the noises brought by the user and product modeling algorithms. The UC method which only takes user credibility into consideration is the runner-up, which is most closest to the proposed framework. It proves that the proposed exclusive regularizer is useful for den oising and improving the recommendation.
Figure 2 illustrates the nDCG@5 score of different methods with varying num-ber of latent dimensions (k). First, it fur ther proves that our proposed framework is useful for recommending items. Second, it shows that the proposed framework increases steadily when the number of dim ensions increases. The optimal dimen-sionality may be set based on different applications. In this work, we put forward a new recommendation framework, which can jointly take user and product utility into consideration during recommendation. The proposed framework is proven to be useful in reducing negative effect of noises and information redundancy. Since the incorporated user and product weights can be learnt separately, our model can easily be extended with different kinds of external knowledge, such as social network and knowledge base.
Since evaluating users and products is not our main focus, the optimal weight-ing methods are interesting problems and remain to be done in possible future work. We would like to investigate leveraging links and explicit group member-ships in social networks in the future.

