 This paper presents a novel method for mining product reviews, where it mines reviews by identifying product features, expressions of opinions and relations between them. By taking advantage of the fact that most of product features are phrases, a concept of shallow dependency parsing is introduced, which extends traditional depen-dency parsing to phrase level. This concept is then implemented for extracting relation between product features and expressions of opinions. Experimental evaluations show that the mining task can benefit from shallow dependency parsing.
 I.2.7 [ Artificial Intelligence ]: Natural Language Processing -text analysis Algorithms, Experimentation Product Mining, Dependency Parsing
Managers and researchers acknowledge that product quality is highly correlated to several outcome variables such as market share and stock valuation. However, product quality and its dimensions are difficult to assess objectively. One solution is to mine the World-Wide Web with its ever-increasing repository of product reviews generated by consumers, experts, and rating agencies. Yet this vast richness of content has made it difficult to extract useful informa-tion such as product quality from the web. Retrieving this informa-tion and analyzing this content is an impossible task if it were to be manually done. However, advances in machine learning and nat-ural language processing present us with a unique opportunity to automate the decoding of quality dimensions from online reviews.
Much work has been done on sentimental classification at differ-ent levels [4, 3]. In this paper, we study the problem of mining on-line reviews, which consists of identifying product features, iden-tifying expressions of opinions, and discovering relations among them. Simple inspection of the data reveals that product features usually contain more than one word. This suggests that depen-dency parsing might not be the best approach here unfortunately, because it provides dependency relations only between words. To solve this issue, inspired by shallow phrase-structure parsing, we introduce the concept of shallow dependency parsing and propose an approach to construct it. Shallow dependency parsing segments an input sentence into  X  X hrases X  and links the segments with di-rected arcs. The parsing focuses on the  X  X hrases X  and the relations between them, rather than on the single words inside each phrase.
The approach performs the opinion mining task in three main steps: (1) constructing a shallow dependency tree based on shal-low phrase-structure parsing and dependency parsing; (2) extract-ing candidate product features and candidate opinion expressions; (3) extracting relations between product features and opinion ex-pressions.
 Shallow dependency parsing is dependency parsing with phrase nodes. A dependency relationship, which is an asymmetric binary relationship, holds between two phrases. One phrase is called the head , which is the central phrase in the relation, while the other the dependent , which modifies the head. A label representing the rela-tion type is assigned to each dependency relationship, such as subj (subject) and obj (object). Figure 1 shows an example of shallow dependency parsing.
 A bottom-up process is used for shallow dependency parsing. We use a shallow parser, Sundance, for segmenting English sen-tences into syntactic chunks and Stanford Parser for generating dependency parsing trees. Phrases extracted from Sundance shal-low parser are treated as nodes in shallow dependency parsing tree. The dependency relationships among words inside a phrase are re-served. The algorithm is shown in Alg. 1.
 The product features are defined as parts, properties, properties of parts, product, company name and related objects which the eval-Algorithm 1 Pseudocode for constructing the shallow dependency pars-2: P = shallow-parse (S) 3: for each phrase P t  X  P do 8: end for 9: return OUTPUTS
Figure 1: Example of Shallow Dependency Parsing Tree. uation is made on. We introduce a language model to shrink the candidates based on the assumption that the more likely a phrase to be a product features, the more closely it related to the product reviews. In practice, for a certain domain of product reviews, the language model is built on unlabeled data and each candidate NP or VP chunk in the output of shallow parser is scored by the model and cut off by a threshold.

Opinion expressions are spans of text that express an evaluation or attitude of the opinion holder, which are usually evaluative or subjective phrases. We use a dictionary which contains 8221 opin-ion expressions to select candidates [6]. Another assumption we use to filter candidate opinion expressions is that opinion expres-sions tend to appear closely with product features, which is also used to extract product features by Hu and Liu [2]. In our ex-periments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the shallow depen-dency parsing tree.
 The relation extraction is treated as a classification task, and kernel methods is used to address the problem. All combinations between candidate product features and candidate opinion expressions are potential relations. Given a shallow dependency parsing tree, the subtree rooted at the lowest common parent of the opinion expres-sion and the product feature is used to represent the relation. De-pendency tree kernels proposed in [1] is used to model this task.
We conducted two sets of experiments to empirically evaluate the performance of the proposed approach. In the first set of exper-iments, we compare the performances of different relation extrac-tion methods on the corpus used by Hu and Liu [2] which contains reviews of Cell phone and Digital camera domains. Table 1 shows the comparison results on the performances of different methods in Methods P R F P R F different domains of the corpus. For each domain, we use all the sentences belonged to it, and conduct a 5-fold cross validation on them. The candidate product features and opinion expressions are filtered by the methods discussed in Section 2. All combinations between candidate product features and opinion extractions are po-tential relations. SVM-Tree denotes the results of our tree-kernel based SVM. In comparison with our tree-kernel based approach, we evaluated SVM 1 with two kinds of kernel functions: linear and radial basis function, whose results are denoted by SVM-Linear and SVM-RBF in the Table 1. The features are the context and part-of-speech tags. We also compared with the Adjacent method which extracts relations between the product features and its nearest opin-ion expression [2]. In both domains, Adjacent performed better than SVM-Linear and SVM-RBF and SVM-Tree outperforms the other three methods. The experiment comparison demonstrates the effectiveness of our proposed methods.

In the second set of experiments, we present a case study on ap-plying our proposed approach on fifty reviews written by Walter Mossberg on the Thursday issues of the Wall Street Journal from 1991 to 2001 [5]. Two annotators were asked to independently an-notate product features, opinion expressions, and relations between them. We combined the data from cell phone and digital camera domain in the corpus of Hu and Liu [2] and used them for training. The F-score of SVM-Tree on fifty reviews drops to 21.6%. The main reason is that the fifty reviews covered different domains in-cluding computers, printers, DVD/TV products and MP3 Players. One of future work is to develop robust techniques for identifying opinion expressions across multiple different domains.
 This work was partially funded by Chinese NSF 60673038, Doc-toral Fund of Ministry of Education of China 200802460066, and Shanghai Science and Technology Development Funds 08511500302. [1] A. Culotta and J. Sorensen. Dependency tree kernels for [2] M. Hu and B. Liu. Mining and summarizing customer [3] N. Kobayashi, K. Inui, and Y. Matsumoto. Extracting [4] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sentiment [5] G. J. Tellis and J. Johnson. The value of quality. Marketing [6] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing contextual libsvm 2.88 is used in our experiments
