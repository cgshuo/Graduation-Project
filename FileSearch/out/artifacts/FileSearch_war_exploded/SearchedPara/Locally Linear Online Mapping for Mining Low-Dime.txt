 Previous research has demonstrated that im age variations of many objects can be ef-fectively modeled by low-dimensional manifolds embedded in the high-dimensional data space [1, 2]. The embedded manifolds are generally nonlinear and can be ap-proximated by mixtures of linear models [2, 3]. Some methods have been proposed to coordinate pre-learned mixture models [4] or propagate pre-learned manifolds to new data points [5]. However, a universal coordinate system may not be applicable when separate manifolds are involved. 
State-of-the-art mixture models usually treat the input data in a batch mode, and estimate model parameters by using the expectation-maximization (EM) algorithm. However, when the input data are not all available at once, only online algorithms can be used, which also have advantages including adaptability, low storage requirement, and fast training. The adaptive-subspace self-organizing map (ASSOM) proposed by Kohonen [6] is able to discover online an ordered set of linear subspaces. It extends the self-organizing feature map (SOFM) [7] by replacing the single weight vector at each neuron with a set of basis vectors spanning a subspace. However, the ASSOM may not be adequate when data distributions are deviated from the origin. As an improvement, the adaptive-manifold self-organizing map (AMSOM) proposed by Liu [8] learns mean vectors of local models in addition to the conventional subspaces. The linear manifold self-organizing map (LMSOM) proposed by Zheng et al . [9] learns the offsets of local models by using a stochastic gradient-descent algorithm. Nonetheless, since local subspaces extend infinitely in the data space, confusion between local models is common in these mixture models, which is harmful for dimension reduction and data compression because the variance observed by local models could be larger than nec-essary and data are more likely to be assigned to the wrong prototypes. 
In order to achieve truly local representation, in this paper, we propose a mixture model which implements a new distortion measure between the input data and local linear manifolds. The objective function at each neuron has not local extremum, and the mean vector and the local principal subspace is the only global minimum solution. Adaptive online learning of local linear manifo lds is then achieved through a stochastic gradient-descent algorithm. Mixture of local models is realized through competitive and cooperative learning. We demonstrate by experiments that: 1) our model largely highly nonlinear manifolds, including separate manifolds. As a potential real-world application, this model is also used to discover low-dimensional manifolds in hand-written digit images and shows promising results. Let x be the input, L be an H -dimensional local linear manifold with an offset vector m and orthonormal basis vectors { b 1 , b 2 , ..., b H }. The proposed distortion measure is: where  X  x L and % x L are respectively the projection of x on L and the residual of projec-where 0  X   X   X  1 is a parameter which shall be shown to control the desired extension of creasing the data likelihood. This would eventually correspond to two conditionally independent Gaussian distributions according to equation (1), one in L and the other one in the residual subspace orthogonal to L . We consider 1-D linear manifold L in the 2-D space first. When  X  = 0, e ( x , L ) is the usual reconstruction error, and the equi-distortion surface is the two infinitely extend-ellipse as shown in Fig. 1, whose major axis lies on L with length 2 C /  X  , and whose minor axis is perpendicular to L with length 2 C . The larger  X  is, the shorter the major axis will be, and therefore the more local the representation along the linear manifold L will be. When  X  = 1, e ( x , L ) degenerates to the usual Euclidean distance from x to the prototype m , and the ellipse in Fig. 1 degenerates to a circle centered at m . The dis-cussion can be easily generalized to higher dimensional cases. 
Let x (1) , ..., x ( N ) be N samples available to a neuron for learning. The objective of learning at the neuron is to minimize the average distortion measure principal subspace of the samples. This is because J ( m , B ) is also the implied objective function of principal component analysis (PCA): PCA minimizes 2 () except the global minimum are saddle points, i.e. J ( m , B ) has no local extremum. Due to limited space, rigorous proof s will be presented elsewhere. 
We use Robbins-Monro stochastic approximation [10] to optimize J ( m , B ). Com-pared to their deterministic counterparts, stochastic algorithms have the advantages of higher computing efficiency at lower memory cost, better suitability to sequentially acquired data, and stronger robustness to local extrema or saddle points due to  X  X oisy X  presentation of input samples. Therefore, we aim to minimize a  X  X ample X  objective maintaining orthonormality of { b h ; h = 1, 2, ..., H } 
The stepwise updating rules of m and b h are as follows: where ) ( t  X  &gt; 0 is a learning-rate parameter and should satisfy regarded as a force moving m along the linear manifold L towards the projection of x . Its average effect in the stochastic process is to find the mean vector m . When  X  = 0, the ()  X  t x L term diminishes, and m is no more guaranteed to move to the center, which is the case of the LMSOM [9]. The basis vectors should be orthonormalized after equa-tions (8) and (9) have been applied. The solution obtained through stochastic optimi-zation converges in probability to the optimal solution. 
For mixture of local models we use a self-organizing network. Such networks are also been observed to consist of self-organ ized areas each being specialized in certain tasks. Self-organizing maps have been extensively and successfully applied to various areas of machine learning [7]. In particular, our model first partitions the data space to a number of regions corresponding to the neurons, and then learns local models at neu-rons. The partition and learning is performed online for the input at each time instant in network determines the winning neuron c via competition: where Q is the set of neurons and L q the linear manifold of the q -th neuron. 
The winner and the neurons in its neighborhood then update their local linear manifolds following the previously developed stochastic optimization algorithm. The updating  X  X orce X  of the neighboring neurons of the winner is attenuated by a neighborhood function v ( t ) ( c , q ), which is a decreasing function of the distance between learning, each neuron only learns the data in its partition. The updating formulae for each local model q  X  Q are: neuron, for h = 1, 2, ..., H . 
The locally linear online mapping (LLOM) algorithm is summarized as follows: 1. Initialize the parameters ( m , B ) of all the neurons in the network (e.g. in a random way), with the columns of B being orthonormal; 2. For the input sample x ( t ) at instant t , the winner c of the neurons is determined ac-cording to equation (10); (11) and (12) and then orthonormalize the basis vectors; 4. Repeat steps 2 and 3 until certain stop criterion is met, e.g. when some predeter-mined maximum learning steps have been met (when there are not sufficient sam-ples, the bag of samples should be repeatedly presented). Different networks are trained to learn th ree 1-D linear manifolds corresponding to the three clusters in Fig. 2. Each cluster contains 600 samples. Each network has three neurons. The number of learning steps T = 10,000. The neighborhood function is Gaussian. The learning-rate parameter is () value of  X  is not crucial when it is in the range (0, 1) according to our experiments, and  X  = 0.1 is used here. As shown in Fig. 2, the ASSOM can not learn these clusters sufficiently. The mean vectors learned by the AMSOM have been  X  X ttracted X  away from the cluster centers due to confusion between the local models. The orientations of the linear manifolds learned by the LMSOM have evident deviation from the local principal directions of the clusters. The LLOM has shown the best result. 
We have also compared the classification accuracies of the different networks. Each neuron of a network is labeled with the cluster whose samples the neuron wins the most during the training phase. In the test phase, each input sample compares its label to the label of the winning neuron. The training and test were repeated 20 times. Each time all the data were regenerated and all the networks re-initialized with different random seeds. Table 1 summarizes the mean values and the standard deviations of the accura-cies for different networks. The LLOM shows obviously better performance than the other networks. We remark that while the linear discriminant analysis (LDA) may also correctly separate these clusters, it does not give a faithful representation of the data as shown in Fig. 2(d).  X 
Figure 3 shows the results of mapping a 2-D spiral of 1,000 samples onto mixtures of 1-D linear manifolds by using the different networks. Each network contains 15 neu-extending linear manifolds and therefore could provide stronger data generalization, according to our experiments, a larger  X  such as the one used here seems necessary for a more local learning of highly nonlinear manifolds. The other parameters are defined in the same way as in the previous experiment. The LLOM shows obviously the best performance thanks to the local property of neurons. It can also map separate nonlinear manifolds as shown in Fig. 4, where both of the distributions contain 30,000 samples. Note that many nonlinear dimensionality reduction algorithms, e.g. the locally linear coordination (LLC) algorithm [4], do not work in such cases since a universal low-dimensional coordinate system does not exist. 
We have also applied the LLOM to mining low-dimensional manifolds embedded in the MNIST handwritten digit images [11] for recognition. The MNIST database con-tains 60,000 training images and 10,000 test images. Our system consists of ten LLOM rectangular topology. The dimension of local linear manifolds is H = 1, 2, 3. The neighborhood function and the learning-rate parameter are defined in the same way as image, the image variant rotated -10  X , and the image variant rotated 10  X . 
For n = 3 and H = 2, each network has learned 3  X  3 2-D linear manifolds (defined by the mean vectors m kq , basis vectors b kq 1 and b kq 2 ) embedded in the high-dimensional distributions of handwritten digit images, as shown in Fig. 5. In this way, the LLOM also helps to visualize the underlying low-dimensional structures. 
For recognition of an input image x , each network N k builds a reconstructed image x for each variant x ( s ) of x . The network with the minimum average error de-termines the class of x . ) (  X  s models of N k [9]. The recognition accuracy of the LLOM networks on the MNIST database is summarized in Fig. 6. The accuracy increases with the number of neurons and the dimension of the local linear manifolds, which reaches 98.26% on the training set and 97.74% on the test set. For a comparison, the system based on the LMSOM achieved 98% on the training set and 97.3% on the test set [9]. 
Although the accuracy of handwritten digit recognition could be further improved by using more specific methods to character recognition [11], in this paper our interest is not limited to the specific task, but in a general application of the LLOM to automatic feature extraction through manifold mining. The LLOM has the potential to be applied to many other areas where nonlinear dimensionality reduction is desirable. The neural model proposed in this paper is able to discover low-dimensional nonlinear manifolds embedded in data online through mixtures of local linear models. It can be applied to areas where nonlinear dimensionality reduction is desirable. The key prop-erty of this mixture model is the truly local representation of each sub-model resulted from a new distortion measure, which has largely alleviated confusion between local sub-models, as demonstrated by experiments. The objective function based on the new distortion measure does not contain local extremum, which is also desirable. Some related issues to be addressed in the future include learning temporally dependent data and automatically determining the number of sub-models. This work was supported by the Sun Yat-sen University Science Foundation under Grant 3171910, by the National Natural Science Foundation of China (NSFC) under Grant 60473109, and by the Distinguished Young Scholars of NSFC Grant 60525111. The authors would like to thank the anonymous reviewers for their helpful comments. 
