 Finding information about people on the Web using a search en-gine is difficult becaus e there is a m any -to-m any mapping be-tween person names and specific pe rsons (i.e. referents). This paper describes a person resolution sy stem , called WebHaw k . Given a list of pages obtained by subm itting a person query to a search engine, WebHaw k facilitates person search in three steps: tion about any person. Secondly , a clus ter groups the remaining pages into different clusters, each for one specific person. To make the resulting clusters more meaningful, an extr actor is used to induce query -oriented personal inform ation from each page. Finally , a namer generates an inform ative description for each clus ter s o that us ers can find any specific pers on eas ily . The archi-tecture of WebHaw k is presented, and the four components are discussed in detail, with a sepa rate evaluation of each com ponent presented where appropriate. A user study shows that WebHaw k com plem ents m ost exis ting s earch engines and successfully im-proves users X  experience of person search on the Web. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  clustering, information filtering.
 Design, Experimentation Person Resolution, Person Sear ch, Clustering, Junk Filtering Person search is one of the most popular search ty pes on the Web. For example, according to [6] , 5-10% of the English queries from AllTheWeb include person names. However, most of the current search engines (e.g. Google , Yahoo , MSN and AllTheWeb , etc.) do not provide any specific function aiming at person search. They treat a person query the same as a general query , and return all web pages that contain one or more terms in the person query . It is difficult for users to find the expected person in such retrieval results due to following reasons. 
First of all, some pages may not contain any person information, referred to as junk pages in this paper, becaus e pers on nam es m ay refer to non-person entities, such as products, companies, or places. For exam ple,  X  Ford X  m ay refer to the Ford Com pany . Those junk pages should be remo ved from person search results. Secondly , there are a lot of am biguities in person search. As Taffet [17] pointed out, those am biguities can be categorized into two kinds. One is called the multi-referent am biguity due to the fact that many persons (i.e. refe rents) may have the same name. The other is called the multi-morphic am biguity due to the fact that one nam e m ay have different written form s. 
Multi-referent ambiguity is very common. For example, given a pers on query  X  X avid Lee X , there are m ore than ten referents in the top-100 pages retrieved by MSN . S ome exam ples are s hown below: 
To investigate how severe the m ulti-referent am biguity is in re-ality , we selected the 200 most-frequent person queries from the log of MSN . W e is sued thes e queries to MSN , and m anually counted for each query the num ber of different referents that oc-cur in top-100 retrieved pages. Th e statistics are shown in Figure 1. We see that 68% of person qu eries retrieve two or more refer-ents, and about 20% retrieve more than ten referents. The worst cas e is the query  X  X ichael X , which retrieves 48 referents . Figu re 1: Dis trib ution of th e n umbers of referen ts, es timated from th e retrieved res ults of 200 p ers on queries using MSN . This work was done while the authors were visiting Microsoft Res earch As ia.

Multi-m orphic am biguity is also com mon because the sam e person usually goes by different na mes in different contexts such as  X  X usan Dum ais X  and  X  Susan T. Dum ais X . This would als o an-noy users when they browse search results. 
Unfortunately , the issues in person search mentioned above have not been dealt with successfully in m ost of the existing search engines . They treat pers on queries the same as other ge-list of non-person (or junk) pages and person pages with different referents. Thus, users have to brow se those pages one at a time to find the expected person, making person search a painful experi-ence. Usually , veteran users can get refined results by adding appropriate contextual words (e.g. title, organization, etc.) into the query and elaborating the query expression. However, most users are not experienced search experts and the inappropriate contex-tual words and query expression w ill ever deteriorate the results e.g., lose relevant web pages a nd introduce irrelevant web pages. Recently , some meta-s earch engines have been developed to handle am biguities in search results. Some representative exam-ples include Vivis imo (www.vivisimo.com), Dogpile (www.dogpile.com) and iBoogie (www.iBoogie.com). However, they focus on general information organization, and can only partially resolve the two kinds of am biguities in person search. In this paper we present a pe rson resolution sy stem, called WebHaw k . Given a list of pages obtained by subm itting a person query to a search engine (i.e. MSN in this study ), WebHaw k fa-cilitates person search by re-organizi ng the search results in three ste ps: First of a ll, a filter removes junk pages that contain no per-son information. Secondly , a clus ter groups the remaining pages into different clusters, each for one specific person (i.e. referent). To make the resulting clusters m ore meaningful in the context of person search, an extr actor has been developed for inducing query -oriented personal inform ation from each page. Finally , a namer generates an informative desc ription (which consists of a nam e and a set of key words) for each cluster so that users can Note that the sy stem is curren tly focusing on English name reso-lution. 
Consider the exam ple of  X  David Lee X  aforem entioned. The re-sult of WebHaw k , generated from the top-100 retrieved pages, is shown in Figure 2. We see that there are 11 junk pages being removed. For the remaining 89 pages, WebHaw k groups them into a list of clusters, each for one referent. The clusters are ranked by the number of pages c ontained. The cluster named  X  X ther topics  X  is a collection of s ingle-page clus ters . Each clus ter phy  X ) and a set of key words that can best distinguish the referent from all of the others (e.g. David Lee Murphy is an  X  X rtist X ). professor nam ed  X  David M. Lee X  in Figure 2) without browsing pages. 
In the rest of this paper, we first review related work in Section 2. Section 3 describes the corpora we used in our study . In Sec-tions 4 to 7, we des cribe each of the four com ponents (i.e., filter , clus ter , extr actor and namer ) in detail, presenting a separate evaluation of each com ponent where appropriate. In S ection 8, we pres ent a pilot us er s tudy , where WebHaw k is com pared with another state-of-the-art sy stem. Results show that WebHaw k com plem ents m ost exis ting s earch engines and succes sfully im -proves users  X  experience of pers on s earch on the W eb. F inally , we conclude the paper in section 9. 
Mann and Yarowsky [11] employ a bottom-up centroid ag-glomerative clustering algorithm to generate person clusters based on extracted biographic features . However, the algorithm can only handle a small num ber of clus ters , and has only been tes ted on som e artificial test data. So it is questionable how well the m ethod generalizes to real world problems. 
Al-Kam ha and Em bley [1] als o us e an approach that clus ters tributes, links and page sim ilar ity . Then, a so-called relatedness confidence matrix is constructed for each page-pair using a prob-abilistic m odel. Clustering is perform ed by m erging pairs whose matching confidence value is larger than a pres et thres hold. 
Guha and Garg [6] follow a different scenario. Instead of the clustering algorithm, they use a re-ranking algorithm to disam-biguate people. The algorithm requires a us er to s elect one of the returned web pages as a start point. Then, through comparing the person descriptions, the algorith m re-ranks the entire s earch re-sults in such a way that pages re ferring to the same person de-scribed in the us er-s elected page are ranked higher. In this sce-nario, a user needs to browse the documents in order to find one which m atches the user X  X  intended referent, which is inconvenient to the user. 
Bekkerman and McCallum [3] present two unsupervised frameworks for finding those web pa ges referring to a particular person: one based on link structure and another using Agglomera-tive/Conglomerative Double Clustering (A/CDC). But their sce-nario focuses on simultaneously di sam biguating an existing social network of people, or lists of people who are known to be some-what connected, which is not the cas e for person search in reality . 
On the one hand, WebHaw k can be viewed as an extens ion of the above approaches . F or exam ple, WebHaw k als o us es a clus -tering algorithm to group pages based on extracted personal in-other hand, WebHaw k differs from the above work in that it has been des igned as a pragmatic system where we take into account more pragmatic factors, among which three of the most important ones are as follows. First, we remove junk pages by a filter . This problem is not discussed thoroughly in previous works, but we think that it is very critical for any pragmatic person search sys-tem that is based on re-organizing search results of general search engines . Second, we perform an in-depth s tudy of us er-interface issues, such as how to provide an inform ative title/description for each clus ter. F inally , WebHaw k is tes ted on real data, and evalu-ated by real us ers , in com paris on with other s earch s ystem s. 
In practice, s ome com mercial s ystem s, s uch as Zoominfo (www.zoominfo.com), have been de veloped to find people infor-mation. But these systems have high cost and low scalability be-cause the person information in the systems is collected mainly by human labors .

In addition, our work is generally related to a number of other researches, e.g. object identification in [14, 16], citation matching in [12], and name matching in [7] . Search result clustering has been discussed in [4, 19, 20]. Pe rson name resolution can also be formulated as a general co-refere nce resolution problem discussed in [8]. In particular, Wacholder et al. [18] describe how to resolve person names within one document. It has also been extended to the multi-document case in [2, 5, 13, 15]. 
We have selected the 200 most -frequent person queries from the log of MSN . For each page, we collected the top 100 web pages that are retrieved successfully by MSN . 
Those pages have been manually annotated as follows. All junk pages (as defined in Section 4) ha ve been labeled. We also as-sumed that one page refers to one referent, and removed all pages that refer to two or more referents. As a result, 2% of pages have been removed. All remaining person pages have been grouped into different clusters, each for one referent. For each person page, we have manually extracted a set of personal information features, including full person name, title, organization, email address, and phone number. The distribution of the numbers of referents for the 200 person queries is shown in Figure 1. The average number is 6.88. 
We have separated the person que ries into two data sets: 160 queries as the training set and the remaining 40 queries as the test set. The following evaluations are based on these two data sets, unless stated otherwise. 
A junk page, in the context of person search, refers to a re-trieved web page, with respect to a person query, where no occur-rence of the person query in the page occurs as a (part of) person name. 
Junk pages are retrieved because person names may refer to non-person entities such as products (e.g. Abercrombie, Bloomberg), companies (e.g. Ford, Disney), places (e.g. Ed-munds), and nature/law systems (e.g. Claudette, White, Aarne). According to our study, junk pages amount to 35% in the top-100 retrieved pages, with respect to a person query. Therefore, junk page filtering is not only a critical initial step for subsequent processes such as clustering, but also prevents users from being detracted. Unfortunately, this problem has not been studied thor-oughly in a systematic manner previously. 
In WebHawk , a component called filter has been developed for this purpose, where junk page f iltering is formulated as a bi-nary classification problem. In our implementation, the filter em-ploys a particular SVM classifier, SVM light [9]. In the next subsec-tion, we will discuss the features used in the classifier. 
We evaluate the performance of the filter using the data set de-scribed in Section 3. In particular, we denote manually labeled junk pages as positive examples a nd other pages as negative ex-amples. The evaluation metrics are standard precision ( p ), recall ( r ) and F 1 =2pr/(p+r) . We explore the effectiveness of different feature sets in the filter . These feature sets are 1. Simple lexical features : These features include title words, 2. Stylistic lexical features : These features include words in 3. Query-relevant lexical features : These features include 4. Linguistic features : These features include three real-5. Query-relevant possessive feature : This feature refers to 6. Query-relevant abbreviation feature : This feature refers to 
In our experiments, only the simple lexical features are used in the baseline system. Table 1 shows the results, where  X  X /o X  means that we use all features except the specified features. We can observe from that both the lexical features (1-3) and the lin-guistic features (4) have a substan tial positive impact on the per-formance while the query-relevant features (5 and 6) bring only marginal improvements. To furt her improve the performance of the filter , we will also employ new features such as link informa-tion and annotate more data for training. Feature Set P R F 1 Baseline (w/ 1) 0.740 0.751 0.745 
All features 0.803 0.774 0.788 w/o 1 0.712 0.738 0.725 w/o 2 0.751 0.802 0.776 w/o 3 0.778 0.773 0.776 w/o 4 0.746 0.775 0.760 w/o 5 0.783 0.777 0.780 w/o 6 0.785 0.775 0.780 
Personal information includes person name, title, organization, email address and phone number. Intuitively, a combination of the above five biographical features can almost uniquely characterize a specific referent, and thus contribute most to the subsequent processes in WebHawk, such as clustering and naming. Meta words denote the words in the description metadata and keywords metadata in a web page, e.g. the italic words in the following html source: &lt;meta name="description" content=" car company... "&gt; &lt;meta name="keywords" content=" car... "&gt; 
Extracting personal information is similar to the problem of named entity recognition (NER), which is a long-standing re-search topic in natural language processing (NLP). However, it is challenging to extend traditional NER techniques to our case for two reasons. First, our task is query -oriented. That is, we only extract biographical attributes of a specific referent in a page, with respect to a person query . The second reason is that web data is very noisy , so traditiona l NLP techniques may not be ro-bust enough. Our method uses a hy brid approach based on two techniques: pattern matching and mutual reinforcement learning . 
Now, we dis cuss in detail how to extract each of the five bio-graphical features. 
We assume that terms in a person query form (partially ) the person name and are key clues for person name extraction. Full English person names are usually composed of three fields: first name, middle name and last name , e.g.  X  X ichael Jeffrey Jordan X . Person names with two words or one word also appear frequently in documents, e.g.  X  X ichael Jordan X  and  X  X ordan X . We prefer person nam es with m ore words because those nam es are more like full names and have a better distinguishing capability . 
We can simply extract the full names from web pages given person queries with three words, but the difficult case lies in the person queries composed of only one or two words (e.g.  X  X avid X  and  X  X  X avid Lee X ). W e em ploy the technique of mutual rein-forcement learning . Person names are extracted from a web page in the following three steps. (1) To extract candidate pers on names from web text. Candi-words and contain the query term s. These candidate person names may include incorrect nam es (e.g.  X  David Lee Hom epage X ) and nam es of different pers ons (e.g.  X  David H. Lee X ,  X  X avid M. Lee X  and  X  David Lee Arnold X ). (2) To assign each candidate pers on nam e a saliency score. The score is a linear combination of heuristic score (see Table 2) and normalized frequency , with equal weights. The higher the score, the better the candidate name is. 3) To adjust the scores of candida te names and choose the one with the highest score. We employ mutual reinforcement learning to simultaneously compute the scores of the bi-names (person name with two words) and the tr i-names (person name with three words). The scores of uni-names (pe rson name with one word) are not changed in this step. The technique of mutual reinforcement learning is based on the following assum ption. 
A bi-name should have a high saliency score if it appears in many tri-nam es with high saliency scores and if its two term s are boundaries of a tri-name with high saliency scores. Conversely , a tri-nam e should have a high saliency score if it contains m any bi-names with high saliency scores and if its boundary words are just the two term s of a bi-nam e with high saliency score. 
For each web page, we generate two sets of candidate nam es: T= { t 1 ,...,t m }. We build a weighted bipartite graph from B and T as follows : we create an edge between b i and t j and s pecify nonnega-tive weights w ij on the edge indicating the relation between them ( w ij =0.1 if b i appears in t j and w ij =0.5 if term s in b of t j ). We denote the weighted bipartite graph by G(B,T,W) where edge weights . For each bi-nam e b i and each tri-nam e t com pute their s aliency scores u(b i ) and v( t j above principle is rendered as
We collect the saliency scores for bi-names and tri-names into two vectors u and v , res pectively . The above equation can then be written in the following matrix format where  X  1 is the proportionality constant. 
We alternate com puting and no rmalizing scores of bi-names and tri-nam es according to the above equation until convergence. At last we choose the person name with the highest saliency score. 
Different from person name ex traction, title, organization, em ail addres s and phone num ber are extracted us ing pattern matching. We manually author a set of extraction patterns for email address and phone number due to their simplicity . As for title and organization, we em ploy a sim ple learning m ethod to collect extraction patterns . First, we s elect s entences in the anno-tated training corpus and tokeni ze them , s mooth variations , and then subject them to sim ple generalization to constitute candidate patterns . For exam ple, in this sentence  X  David Lee is a painter X ,  X  X ainter X  is annotated as a title, so we replace  X  David Lee X  by &lt; title &gt;  X . Then, all thes e patterns are applied to the s ame training corpus to evaluate their accuracies . Thos e patterns with high ac-curacy are adopted to extract title and organization. The num bers of patterns for title, organization, email address and phone num-ber are 13, 11, 5 and 4, respectively . 
Some patterns can handle m ost cas es except a few peculiar in-stances , e.g.  X  webm aster@x..x X  is not an appropriate em ail ad-dress though it conforms to the pattern of  X  X ..x@x..x X . Here, x repres ents any character. Thes e negative exam ples are m anually induced to form total 25 patterns, named anti-patterns , and we use these patterns to avoid sim ilar extraction errors. 
With these patterns, we m ight extract m ore than one title or or-ganization for a web page in that various patterns could be applied appropriately . Then we use anti-patterns to filter the inappropriate instances. In order to im prove the accuracy of title extraction, we have built a person title gazetteer beforehand, and those extracted titles not in this gazetteer will be excluded. Lastly we rank the rem aining ins tances by frequency and s elect the bes t ones . In our experiments, standard precision ( p ), recall ( r ) and F =2pr/( p+r) are used as evaluation m etrics . The bas eline m ethod of extracting personal information is to select the most frequent one from the corresponding set of candidate entities. The candi-date sets for different ty pes of personal information are produced as follows, respectively : z Person name candidates are thos e nam es which are ex-z Organization candidates are all organizations extracted by z Title candidates are those terms co-occur both in the web z Email address candidates are those substrings containing z Phone number candidates are those substrings containing 
Table 3 compares the performance of the baseline methods with our methods. In the first co lumn, # denotes the number of names (or titles, etc.) in the test set. For name extraction,  X  X /o saliency scores to candidate name s, and  X  X / s tep3 X  m eans that mutual reinforcement learning is em ploy ed to adjust those scores. We can see that the learning step improves the performance of name extraction. 
As shown in Table 3, our approaches significantly outperform the baseline methods. In our pilot study , we have found that though the in-house NER tool achieves a good performance on the WSJ corpus, it performs much worse on web pages due to the complexity and irregularity of web pages. For example, a web page has a person name  X  X shton Kutcher X  in heading text, but there are no contextual words s urrounding it and the terms in this tool cannot recognize it as a person name. While in our approach, the query  X  X shton Kutcher X  is a good contextual clue for distin-guishing this person name. Over all, the performance of the pattern matching approach to extracti on of title, email address and phone number benefits from the use of que ry terms as contextual clues and the use of anti-patterns for filte ring noisy answers. However, organization extraction is still a difficult task because it is hard to determine the boundary of organization. 
Cluster is used to group person pages into different clusters, each for one s pecific pers on. W e us e the agglom erative clus tering algorithm to produce clus ters in a bottom -up way as follows: 
Initially , each web page is an individual cluster; then we itera-tively merge two clusters with the largest similarity value to form a new cluster until this sim ilarity value is below a pre-set merging threshold. The merging threshold can be determined through cros s-validation. We em ploy the widely us ed average-link m ethod to com pute the sim ilarity betw een two clusters as follows: where p i , p j are web pages in clus ter c 1 and clus ter c pages in cluster c 2 . 
The principal problem of using the clustering algorithm de-scribed above is how to m easure the sim ilarity between two web pages p i and p j . Different types of features extracted from web pages are explored in the experime nts, including lexical features, linguistic features and personal information (i.e. PersonInfo) ex-tracted as described in Section 5. The lexical features include title words, meta words and text words (We consider both unigrams and bigrams). The linguistic feat ures include bas eNP (bas e Noun Phrase) and NE (Nam ed Entity , including person, organization and location), which are produced by the in-house NER and bas eNP recognizer. Here, the NE features refer to all nam ed enti-ties extracted from the web page by the tool, no matter whether refer to all baseNPs extracted from the web page. 
For each ty pe of features , we generate a feature vector for a web page and the weight of a feature unit is its frequency . Take NE features as an exam ple. The vector is com pos ed of all nam ed entities in a web page. Having gene rated those feature vectors for two web pages , we us e the cos ine m eas ure to calculate similarity value between each pair of the s ame ty ped feature vectors . W e then linearly com bine such sim ilarity values to get the final sim i-larity value. The weights for diffe rent ty pes of features are hard to be es tim ated em pirically , so we us e the perceptron algorithm with uneven margins (PAUM) [10] to estimate them. Th e PAUM is an extens ion of the perceptron algorithm specially des igned to cope with two class problem s where positive exam ples are rare com -pared to negative ones, which is su itable for the clustering context. 
In the test set, each query corresponds to several non-overlapping clusters annotated by hand. For sim plicity , the m anu-ally annotated clus ters are called clas ses and the autom atically generated clusters are called clus ters. Our evaluation involves two steps: First, we evaluate the perform ance for each query . Second, we average the res ults over the 40 queries in tes t set. 
For each clus ter of one query , we calculate the recall and preci-sion of that clus ter for each given clas s. More s pecifically , for clus ter j and clas s i where n ij is the num ber of com mon m embers in clas s i and clus ter j , n j is the num ber of m embers of clus ter j and n i is the num ber of members of clas s i . The F measure of cluster j and clas s i is then calculated by
For an entire clustering for the query , the F m eas ure of any class is the m axim um value it attains at any cluster and an overall value for the F measure is computed by taking the weighted aver-age of all values for the F m eas ure as follows . where the max is taken over all clusters and n is the number of all web pages for the query . 
After we get the performance values of all 40 queries, we aver-age the values to produce the overall performance value. 
In the experiments, we compare the performance of the clus ter s using different ty pes of features and explore the influence of the filter on the performance. The compar ison results are shown in Figure 3. In the figure,  X  X ll data X  re fers to all retrieved web pages including junk pages,  X  X lean data X  refers only to person pages and  X  X uto-cleaned data X  refers to th e remaining web pages after apply -ing filtering, which may include bo th person pages and junk pages. 
We igh ted Av era ge F
Figu re 3: Performan ce comp aris on for d ifferen t featu re s ets From Figure 3 we can see that linguistic features slightly im-prove the performance, while automatically extracted personal information contributes substan tially to the performance over all kinds of data. With linguistic f eatures and personal information, the performance on clean data is 7% higher than that with only lexical features . The res ults validate the intuition that pers onal information can almost uniquely characterize a specific referent. 
We can observe that given the same feature set, the gap be-tween the performance on clean data and the performance on all data is large. It demonstrates th at junk pages deteriorate signifi-cantly the perform ance and junk page filtering is necessary for person resolution. The filter that we have developed, though by no means perfect, can already improve the visible performance across all ty pes of features. 
Namer is used to generate for each cluster an inform ative de-scription so that users can find any specific person easily . 
Here we propose a method to name the cluster concisely and informatively . We define a name template which consists of two parts: full person name and i nformative term. The informative term is content-focused and contai ns information unique to a par-The filling of this tem plate consis ts of two steps: candidate gen-eration and ranking. 1) In the candidate generation p roces s, we collect thos e nam es and titles, which have been extracted from web pages, as the can-didates. 2) In the ranking process, we rank the candidate names and ti-tles by their frequencies in the cluster and then fill the tem plate with the m ost frequent ones. Note that the extr actor ma y not ex-tract any title from the web pages in some clusters. In such a case, as backoff, we use the most salient word or phrase in the cluster as the inform ative term . For exam ple, for  X  David Lee Murphy  X , we use the title  X  X rtist X  as th e informative term, but for  X  X avid Lee Sm ith X , we use the phrase  X  Fan Sites X  as the backoff inform a-the set of uni-grams and bi-grams extracted from those web pages, after excluding stop words and all bi-grams containing stop words. We assign the weight of a uni-gram as its frequency and the weight of a bi-gram as the double of its frequency . 
The nam e for a clus ter is very concis e. Us ers m ay want to take a look in more detail at the cluster. So we provide a short sum-mary as a s upplem ent. The s ummary is produced bas ed on a s im-ple s entence extraction m ethod. The details are skipped due to the page lim it. The sum mary is located in the top of the right fram e of the us er interface of WebHaw k as shown in Figure 4. 
In order to as sess how eas ily and effectively to us e We bH aw k for person search on the web, we perform a pilot user study . We com pare WebHaw k with traditional MSN and the award-winning Vivis imo . MSN returns a ranked list of search results while both Vivis imo and WebHaw k show clusters in the left pane and a ranked list of documents in the right pane (as shown in Figures 4-5). Becaus e Vivis imo can combine various results returned by a on the same data s ource, we s elect MSN as the underly ing s earch engine for Vivis imo in the user study . Note that previous work in [1, 3, 6, 11] does not provide available pragmatic sy stems for pers on s earch, the com paris on between WebHaw k and thos e works through user study is impossible. The WebHaw k  X  X  syste m im plem entation details, which are im portant for real-tim e person search, are om itted due to the space lim it. 
Prior to the us er s tudy , a lis t of 12 tas ks was developed. Each of person. Each task had only one correct answer. The person que-ries were selected from MSN X  X  logs and the specific information need for each person query was designed. It was guaranteed that the correct answer could be found from the top 100 web pages returned by MSN . For example, one of the tasks was to find the name of the college where a person named  X  X ichael Williams X , correct ans wer was Lynchburg College . 
The usability study m etrics were as follows: 1) Effectivenes s m etric -accuracy : This was the percentage of tasks suc cessfully comple ted by a sy ste m. A ta sk c ould be com-pleted successfully , completed unsuccessfully or aborted. All these occurrences were recorded. A successfully com pleted task was the one where a us er com pleted the tas k and obtained the correct ans wer. An uns ucces sfully com pleted tas k was the one where a us er com pleted the tas k but obtained the incorrect ans wer. An aborted task was the one that a user quit while performing the task. 2) Efficiency m etric -com pletion tim e: This was the tim e taken for a user to complete a task using the sy stem. 3) S ubjective acceptance: This relates a us er X  X  subjective satis-factory level by asking the user to fill out pre-designed question-naires. 36 students from different depart ments were employ ed as sub-jects for performing person searches and filling out questionnaires. A Graeco-Latin Square was used to establish task order for each participant and to confound task order effects. According to the task order, each subject carried out all 12 searches, four on MSN , four on Vivis imo and four on WebHaw k . Note that besides the provided person name query , subject s were allowed to design and use any other queries they wished on MSN or Vivis imo during a search tas k. Each s earch tas k had to be finis hed in 10 minutes otherwise the task was considered to be aborted. If the user al-ready knew the answer, she or he still had to perform the search and find it. The s tart tim e and the end tim e of each tas k were re-The time limit of 10 minutes is defined in the TREC 2002 Inter-active Track Guidelines (http://trec.nist.gov/data/t11_interactive/guidelines.html). corded to m easure the com pletion tim e if the task was not aborted. The obtained answer for each task was saved and then com pared with the correct ans wer. The av eraged com pletion tim e and the accuracy were obtained for each pa rticipant and then the values were averaged acros s all the par ticipants, as shown in Table 4. 
As a com plem ent to the objective m eas ures , an exit-s ystem ques tionnaire was des igned to gauge a us er X  X  overall acceptance tionnaire asked subjects to as sess each s ystem in the following four aspects: 
Subjects were required to expres s an opinion over a 5-point all  X , 3 for  X  somewhat  X  and 5 for  X  extr emely  X . W e collected the res pons es of s ubjects and averaged them , as shown in Table 5. 
Table 4 shows that com pletion tim e for WebHaw k was the lowes t and the accuracy for WebHaw k was a little higher than other s ystem s. As can be s een in Table 5, both Vivis imo and WebHaw k were a little m ore difficult to use than traditional MSN in that the two-pane-bas ed us er interface was m ore com plex than the sim ple ranked list. WebHaw k produced an informative per-pers on s earch. Las tly , us ers were more satisfied with WebHaw k and preferred to use WebHaw k for person search. 
This us er s tudy showed that WebHaw k was efficient and ef-fective and could improve users X  search experience for person search, which could be attribut ed to its good performance for person resolution and its ability to provide an informative inter-face. que stionnair es ave raged ac ross que ry and subje ct on a sc ale of 
We have presented an effective system for person resolution in person search results, called WebHaw k . It is composed of four com ponents : filter , extr actor , clus ter and namer . The experi-ments and results have shown the perform ance of each com ponent and demonstrated that personal information extracted by the ex-tractor improves the performance substantially and the filter does benefit the sy stem by removing noisy data. It is verified through a user study that users X  person s earch experience is indeed im-proved by WebHawk . Either by simply providing a search option on the search interface indicting whether the search is a person search or a general search, or by providing an automatic person query identification mechanism, general search engines can inte-grate with WebHawk easily and issue person queries to the per-son search engine-WebHawk . 
We have attempted a new met hod where we categorized person pages into more elaborate sub-cat egories (e.g. newswire, citation list, short bios, etc.), clustere d web pages within each category, and combined these clusters. Th e intuition underlying this method is that each kind of web pages has their own characteristics and has better be clustered using th eir own distinguishing features. This method however did not improve the performance as we expected in our pilot experiments. We will explore new clustering scenarios in our future work. New features, e.g. link information, will also be explored for dif-ferent components in WebHawk . At present, WebHawk focuses only on English person names and we will adapt the system to other languages in the near future. 
We thank Tom Huang, Chuan Lin and John Chen for their ear-lier work and are grateful to Jian-Yun Nie and the anonymous reviewers for their helpful sugges tions. We also thank Mary D. Taffet for her kindly help to provi de her dissertation proposal and other valuable references. Lastly , we thank those part-time stu-dents for data annotation and user study. [1] R. Al-Kamha and D. W. Embley. Grouping search-engine [2] A. Bagga and B. Baldwin. Entity-based cross-document co-[3] R. Bekkerman and A. McCallu m. Disambiguating web ap-[4] D. Cutting, D. Karger, J. Pedersen and J. W. Tukey. Scat-[5] M. B. Fleischman and E. Hovy. Multi-document person [6] R. Guha and A. Garg. Disambiguating people in search. [7] H. Han, L. Giles, H. Y. Zha, et al. Two supervised learning [8] J. R. Hobbs. Resolving pronoun references. Readings in [9] T. Joachims. Making large-scale svm learning practical. In [10] Y. Y. Li, H. Zaragoza, R. Herbrich, et al. The perceptron [11] G. S. Mann and D. Yarowsky. Unsupervised personal name [12] A. McCallum, K. Nigam, and L. H. Ungar. Efficient c [13] C. Niu, W. Li and R. K. Srihari. Weakly supervised learning [14] H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser. [15] Y. Ravin and Z. Kazi. Is H illary Rodham Clinton the P [16] S. Russell. Identity uncertainty. In Proceedings of IFSA X 01 , [17] M. D. Taffet. Person resolution: resolving multireferent and [18] N. Wacholder, Y. Ravin, and M. Choi. Disambiguation of [19] O. Zamir and O. Etzioni. Web document clustering: a f [20] H. J. Zeng, Q. C. He, Z. Chen, W. Y. Ma and J. W. Ma. 
