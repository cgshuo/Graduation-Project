 C  X  esar Ferri CFERRI @ DSIC . UPV . ES Department of Computer Science, University of Bristol, UK. Jos  X  e Hern  X  andez-Orallo JORALLO @ DSIC . UPV . ES Many recent approaches in machine learning have bene-fited from the idea that the predictions of a committee or ensemble of models will be usually better than the predic-tions of one single model. The errors of one can be coun-teracted by the hits of the other models. This collaborative view of learning can occur without interaction between the learning agents, known as ensemble learning , or with in-teraction during the learning stage, known as co-learning . In this paper we explore the novel approach of delegation , which can be summarised by the motto: let others do the things that you cannot do well. We introduce the notion of a cautious classifier as one that only classifies the examples for which its predictions have high confidence, leaving the rejected examples for another classifier. We demonstrate in this paper that the overall performance of such a hierar-chical tandem of classifiers is usually better than a single classifier. By delegating rather than combining models, we avoid some of the problems of ensemble methods, in partic-ular the loss of comprehensibility and the use of excessive computational resources.
 From a general point of view, the idea of delegation leads to several questions, around two main issues. First, we have to determine a threshold or decision rule to decide when to apply the first classifier and when to delegate to the second one. Secondly, and more crucially, we have to determine good techniques to generate classifiers that perform better than the first one for the examples that the first one has del-egated. With these two main issues in mind, we propose a simple method, with the following features. (1) The deci-sion whether an example has to be tackled by the first clas-sifier or by the second classifier is made by the first clas-sifier itself, by using its own estimated reliability. Because of this, we will use a good ranker (e.g., a good probability estimator) as a first classifier. (2) The second classifier is specialised on the examples for which the first classifier be-haves worst by training this second classifier solely with the examples rejected by the first classifier. As we will see in subsequent sections, these issues and the way in which we deal with them still allow a range of alternative approaches, many of them explored in this paper.
 We also explore iterated delegating classifiers, which sug-gests a relationship with other ensemble or combination methods, such as boosting (Freund &amp; Schapire, 1996) and stacking (Wolpert, 1992). Boosting assigns higher weight to incorrectly classified examples and lower weight to the examples which are classified correctly for each iteration. Delegation, on the other hand, removes the examples which are classified with high confidence and leaves the examples which are classified with lower confidence for subsequent iterations. Stacking builds a second-stage meta-classifier which decides which base classifier (from an independent ensemble of classifiers) to use. Other multiple classifier methods, such as cascading (Gama &amp; Brazdil, 2000), and, specially, local cascading, generate new attributes from the class probability estimations given by the base classi-fiers or by previous decision tree splits. In contrast, dele-gation produces models which are completely and exclu-sively defined in terms of the original attributes and class. Arbitrating (Ortega et al., 2001) and grading (Seewald &amp; F  X  urnkranz, 2001) are also related to delegation, but both learn external referees to assess the probability of error of each classifier from the pool of base classifiers, and their areas of expertise. No new attributes are generated. More-over, there is only one base classifier and this one decides which examples to accept and which to reject.
 The closest idea to delegation comes from the general separate-and-conquer technique and, specifically, a variant introduced by the PART algorithm (Frank &amp; Witten, 1998), which learns a decision tree, selects the branch with largest coverage, removes the rest of the tree and retrains a second tree with the remaining examples. The process continues until all the examples are covered. Delegation is indepen-dent from the base classifier used and it is based on the idea of a confidence threshold, which in the case of decision trees could select several branches with large coverage. Delegation is hence a serial (not parallel or hierarchical), transferring (no combination), attribute-preserving, self-refereeing, multi-classifier method. The advantages of the delegation approach are manifold. First, since each classi-fier is keeping part of the examples, the next classifier has fewer examples for training and, hence, the process can be much more efficient than other ensemble methods. Sec-ondly, the resulting overall classifier is not a combination of classifiers, but a decision list; if we use decision trees as base classifiers, the overall classifier is a decision tree, and its decisions can be traced and understood. Thirdly, since some parts of the models will not be used for any example at all, we can, in some cases, simplify the models; e.g., in the case of decision trees, we could prune a subtree if all its leaves lead to delegation.
 The rest of the paper is organised as follows. Section 2 defines the notion of cautious classifiers, how they can be constructed from a soft classifier and how they are used to perform delegation. In Section 3 we analyse delegating classifiers as a generalised separate-and-conquer method. Section 4 starts the experimental evaluation with an anal-ysis of the behaviour for a delegation tandem of just two classifiers. Section 5 discusses the round rebound tech-nique, which bounces part of the delegated examples back to the first classifier, an iterated scenario, and the general al-gorithm. The approach is extensively evaluated for several configurations and compared to classical ensemble meth-ods. Section 6 discusses the overall results. Section 7 closes the paper with a summary and future work. In many application areas, a classifier that abstains from making a prediction when it is not sure of being able to make the right decision is preferable over a greedy clas-sifier that always makes a classification. While under 0-1 loss accuracy and error are complementary, this is not the case when the classifier has the ability to abstain. We define a cautious classifier as a classifier that gives predictions for the subset of inputs for which it is more confident (that may still be right or wrong) but abstains for the rest of its inputs. In other words, a cautious classifier is a partial function. Any soft classifier, that is, a classifier that estimates the class probabilities or the reliability of each prediction, can be converted to a cautious classifier. We use the follow-ing definitions. For a classifier f we consider the asso-ciated functions f CLASS e , f CONF e , and f PROB c e (for each class c from a total of C classes). The function f
CLASS e returns the class assigned by classifier f to exam-ple e , the function f CONF e returns the confidence (i.e., an estimate of the reliability) of the prediction given by clas-sifier f to example e , and f PROB c e returns the probability of class c for example e . Unless stated otherwise, we as-Given these definitions, a cautious classifier f can be ob-tained from a soft classifier using a confidence threshold. A soft classifier will be converted into a good cautious clas-sifier if the reliabilities are well estimated, as achieved by, for instance, a good class probability estimator, or, for bi-nary problems, a good ranker. Note that for two-class prob-lems, a cautious classifier gives predictions for the reliable positives and the reliable negatives, abstaining for the rest. This represents an abstention window (see Figure 1). The notion of a cautious classifier is an interesting concept in itself, but in this paper we are concerned with the design of good complete classifiers. It is the idea of completing these cautious classifiers that leads to the concept of del-egation . If a cautious classifier f 1 decides that it is not competent to classify an example with enough confidence, but wants to complete the work, then it can delegate the example to another classifier. If we have this second classi-fier, denoted by f 2 , and a confidence threshold  X  , then the delegating decision rule is as follows.
 A key issue is to establish a good way for obtaining clas-sifier f 2 . A natural way to obtain classifier f 2 train it only on the training examples for which f 1 has low confidence. In this way, the second classifier will be specialised for these examples. More formally, if we have a training set Tr , a soft classifier f and a confidence threshold  X  , then we just divide this set into two data sets Tr f e Tr : f CONF e formally, we will refer to Tr f as the  X  X etained X  or  X  X igh-confidence X  examples and Tr f as the  X  X elegated X  or  X  X ow-confidence X  examples.
 The same threshold is used for training and for prediction (the delegating decision rule). The question arises how to determine this threshold. One approach we consider in this paper is that a classifier retains a fixed percentage of the ex-amples. For instance, we may stipulate that the first classi-fier should retain 60% of the most highly ranked examples, delegating the rest to the second classifier. More formally, given a fraction  X  , a classifier f and a training set Tr ,we can obtain the threshold  X  in the following way: That is,  X  is the greatest threshold such that at least a pro-portion  X  of the training examples have higher confidence. It does not ensure a proportion of exactly  X  , because there may be many examples with the same confidence, but it returns an upper approximation of this proportion. The method is called Global Absolute Percentage .
 We also introduce an alternative decision rule for delegat-ing classifiers, in order to handle imbalanced data sets. In this case, we have a different threshold  X  c for each class c . If we denote by Tr c the examples in Tr of class c ,wecan obtain each threshold as follows:  X  The retained examples in this case are: Tr f e Tr : c f fied Absolute Percentage .
 The above outlines well-defined techniques for training and testing delegating classifiers. In particular, the predicted class and associated confidence for each test instance is ob-tained from one of the base classifiers. Computing the ac-curacy of the overall delegating accuracy is thus straightfor-ward. In addition, we use the Area Under the ROC Curve (AUC). AUC is not only suitable in contexts with vari-able misclassification costs or class distributions (Provost &amp; Fawcett, 2001), it also provides an estimate of a soft classifier X  X  ranking performance, as it is equivalent to the Whitney-Mann-Wilcoxon sum of ranks test. For multi-class problems we employ the approximation presented in (Hand &amp; Till, 2001), which averages the AUC of all 1-vs-1 ROC curves. It may happen that the first classifier retains all the examples of one class and hence the second classi-fier has fewer classes than the first one. To calculate AUC in this case, we simply assume that the second classifier assigns 0 probability for non-delegated classes. In this section we put delegation in a general machine learning context and discuss its relation with separate-and-conquer methods such as the sequential covering algorithm for learning sets of rules. Clearly, the behaviour of a tan-dem of delegating classifiers depends on the machine learn-ing method used for the base classifiers. Here, we analyse the use of fence-and-fill methods (i.e., methods that parti-tion the instance space into regions). In that case, the first classifier will retain the bigger, relatively purer areas of the instance space and delegate the smaller, less pure areas to the second classifier. Interestingly, since the Tr f examples are removed for the second classifier, the removed areas can  X  X lear the space X  so that the remaining small areas may be joined into bigger areas for the second classifier. This will be particularly useful in the presence of real patterns that were obscured by the patterns extracted by the first classi-fier. Figure 2 illustrates the process with two decision tree learners. Here, we assume some form of smoothing (e.g., Laplace correction), so that the confidences associated with the smallest leaves drop below the threshold and thus these areas are delegated to the second classifier.
 In this particular example we can simplify the first classi-fier, replacing the subtrees that lead to only low-confidence leaves with  X  X elegation nodes X  that lead into the second tree. On the meta-level, the entire delegating classifier is again a decision tree. In fact, we can change the tree into a graph by combining all delegation nodes into a single node, so that we only represent the second tree once (Figure 3). This process, which we refer to as grafting , is a very natural way to learn decision graphs.
 There is an analogy between delegating classifiers and separate-and-conquer rule learners such as CN2 (Clark &amp; Niblett, 1989) and FOIL (Quinlan, 1990). With the separate-and-conquer approach, a single rule is learned in each iteration and the area it covers is removed from the training set. Delegating classifiers perform a similar strat-egy but work at the meta-level, learning a full classifier in each step. This entails several important differences. First, the area covered in each iteration contains examples of multiple classes. Secondly, the area can have a much more sophisticated shape than the axis-parallel hyper-rectangles covered by a single rule. Thirdly, the reliability estimates over the area are in general not constant. Consequently, delegation, especially under the iterative scenario explored in Section 5, can be seen as a powerful generalisation of separate-and-conquer methods. This connection highlights that the potential applications and kinds of problems where delegation can work well will be those where separate-and-conquer methods are applicable (see e.g. (F  X  urnkranz, 1999)). The use of a complete classifier in each iteration makes delegation a much more powerful method.
 In the next sections we experimentally validate the method in three different scenarios: the two-stage scenario con-sidered above, a round-rebound scenario where the second classifier can delegate examples back to the first, and an it-erative scenario with a variable number of base classifiers. The two-stage scenario was introduced in Section 2. In this section we experimentally analyse the behaviour of this type of delegating classifiers. Table 1 lists the 22 datasets from the UCI dataset repository (Blake &amp; Merz, 1998) we used. We applied 20 5-fold cross-validation instead of a more usual 10 10-fold cross-validation because for com-puting the AUC we need examples of every class and some datasets have small minority classes. For a set of datasets we use arithmetic means unless stated otherwise.
 In the experiments, we employ Probability Estimation Trees (PETs), which estimate the probability of class mem-bership for every class. A trained decision tree can be eas-ily adapted to be a probability estimator by using the ab-solute class frequencies of each leaf in the tree. However, the probability estimates obtained by PETs can be poor in comparison with other probability estimators. In order to obtain better estimates, we employ the improvements pre-sented in (Ferri et al., 2003), using the SMILES system. These improvements involve a new splitting criterion and a new smoothing method (mbranch smooth), both devoted to improve the AUC of the learned trees. Pruning is not en-abled, since it is not beneficial for increasing the estimated probabilities (Domingos &amp; Provost, 2003).
 Table 2 shows a mean increase of about one point in accu-racy with respect to a single tree, obtained by just retraining a second tree with around 50% of the initial dataset. Ac-cording to t-tests there are 8 significant wins and 1 loss in accuracy, and 7 wins and 1 loss in AUC. It can be seen (see columns AcT 1 and AcT 2) that the second classifier has higher accuracy than the first one precisely on the low-confidence examples delegated by the first classifier. This is the key to the overall improvement in accuracy achieved by the delegating classifier.
 To analyse the effect of the first classifier X  X  ranking per-formance, we considered four configurations for the first classifier: pruning and no smoothing, no pruning and no smoothing, no pruning and Laplace smoothing as used in (Domingos &amp; Provost, 2003), and no pruning and m-branch improvements presented in (Ferri et al., 2003). Ta-ble 3 gives mean results (over 22 datasets) of these four configurations for the global absolute percentage method (50%). As we can see, it is the ranking performance rather than the accuracy of the first classifier which is crucial for accuracy and particularly AUC of the delegating classifier. Table 4 demonstrates that accuracy is not very sensitive to the percentage of examples retained, although it seems that around 50% is a good compromise. With lower percent-ages most of the work is left to the second classifier, which is then very similar to the first one and not specialised suffi-ciently to improve the results. A high retention percentage lowers the influence of the second classifier and may also lead to overfitting. The method appears to be robust in the sense that, with several configurations, mean accuracy is never worse than for a single classifier. The AUC of the stratified method is worse than for the global method. This may be due to the fact that the non-stratified method usually levels the proportion of classes for the second sample since examples of the majority class are usually better ranked than the examples of the minority class. Hence, the sec-ond classifier can pay more attention to minority classes. In this section we consider variants of the two-stage delega-tion scenario. The first scenario we consider is that the sec-ond classifier delegates its low-confidence examples back to the first; we call this round rebound . The rationale is that if an example is rejected by both the first and the sec-ond classifier, it would be best classified by the first rather than the second because the first classifier is more general and potentially less overfitting. This leads to the following decision rule.
 For the second threshold  X  2 , we can again select a per-centage of the complete training set (Absolute Percentage), or we can select a percentage of the examples delegated by the first classifier ( Relative Percentage ):  X  2 max t : e Tr The difference is that, given a percentage of 40%, using the absolute percentage method both classifiers retain ap-proximately 40% of all examples, while with the relative percentage method, from the 60% delegated to the second classifier, 24% would be retained.
 We now have four different methods of determining the thresholds for the first and second classifiers: global abso-lute percentage, stratified absolute percentage, global rel-ative percentage and stratified relative percentage. Table 5 shows the mean accuracy and AUC results (over the 22 datasets) of the round rebound scenario with the four dif-ferent methods and several retention percentages. Compar-ing these results with previous results, the mean accuracy is slightly better (from 84.73 to 85.04 in the best case, with 10 significant wins and 0 losses) and for AUC it almost stays constant (from 91.31 to 91.33, with the same 7 wins and 1 loss). While this is a small improvement, it should be noted that it is achieved without additional learning cost. The second variation we consider is an iteration of several delegating classifiers with the following decision rule. It is again possible to include the round rebound; however, if the number of examples in the last iteration are few the effect of rebound will be negligible, so we will not use it in the iterative scenario.
 We thus arrive at the following general algorithm for learn-ing a delegating classifier. The algorithm has several pa-rameters, some of them already explained (the method and percentage for obtaining the threshold), but also the max-imum number of iterations, which is especially important when using the relative methods of obtaining the threshold. Table 6 shows the mean results (accuracy, AUC and num-ber of iterations) for different retention percentages for the absolute methods, and Table 7 shows similar results for the relative methods (stratified not shown since results are worse). These tables demonstrate that the results improve with the number of iterations. This is easy to do and to con-trol with the absolute methods, although since the thresh-olds are upper approximations, a percentage of 1% in the global absolute does not translate into 100 iterations but only a mean of 31.31. The important thing about the abso-lute methods, apart from the fact that the results are better, is that there seems to be no saturation point; whereas with the relative method we have to find a good combination of the relative proportion of examples retained and the maxi-mum number of iterations. In the absolute case, just reduc-ing the percentage increments the number of iterations and the results. With respect to the two-stage scenario, now the best improvement in accuracy is considerable (from 83.72 to 85.93) and the improvement in AUC begins to be impor-tant (from 90.78 to 91.82).
 Finally, we compare delegating (global absolute percent-age of 1% and 2%) with two ensemble methods: boost-ing and bagging. For this last experiment, we use Weka (Witten &amp; Frank, 1999), using J4.8 for all the experiments (Laplace smoothing enabled), with pruning only enabled for boosting. In this case, in order to use the same base algorithm for the comparison, we use an implementation of our delegation algorithm in Weka, without the AUC im-provements (no m-branch smoothing and no special split-ting criteria). The results of delegation are hence slightly different to those shown in previous tables, but are consis-tent with them. We again used 20 5-fold cross-validation. From the results in Table 8, delegation is not far behind other ensemble methods in terms of accuracy. The impor-tant point is that delegation requires much more modest re-sources, as at each iteration there is fewer data for training. For instance, delegation with global absolute percentage at 2% requires 21.64 iterations on the average and a total of 8933.67 examples handled ( Tr 1 Tr 2 Tr n ). with an average dataset size of 1195.41 examples ( Tr 1 ). This means with 20 iterations, the time complexity is only around 8 times higher. Similarly, for delegation at 1% there are 31.31 mean iterations and a total of 11923.40 examples handled. This means that the execution time is only around 10 times the time it takes to generate a single tree. In Table 9 we show a pairwise comparison of the number of win/losses (t-test 99% significance) between the follow-ing algorithms: Delegating (2%), J48, Bagging (10 itera-tions), and Boosting (10 iterations). Although delegating improves significantly the performance of J48, it does not reach the level of Bagging or Boosting.
 The experimental results in the previous sections demon-strate that delegation is a technique with a very good trade-off between the quality of models and the resources needed to obtain them. The efficiency of the method derives from its separate-and-conquer philosophy and the serial multi-classifier topology, resulting in a sub-linear increase of re-sources with respect to the number of classifiers. We have also demonstrated a clear improvement in accu-racy. A formal analysis is beyond the scope of this paper, particularly since the delegating technique is not a combi-nation technique, hence we cannot justify the improvement from a reduction of variance as with other ensemble meth-ods. Here, we discuss a number of factors that seem to play an important role. First and foremost, since classifiers decide themselves which examples to retain and which to delegate, the quality of the reliability estimation of each delegating classifiers is crucial. This is why we used un-pruned decision trees optimised for probability estimation. Secondly, the idea of iteratively removing patterns (from easier to more difficult patterns, see), common to separate-and-conquer methods, is likely to assist in obtaining good accuracy. Thirdly, class distribution is modified by the non-stratified threshold, usually balancing classes as the process evolves, which also intuitively seems beneficial. Fourthly, there is the reduction of training sets as a result of delega-tion. A smaller training dataset means better specialisation but may also lead to overfitting. However, the results are improved by the iterated scenario, at least if retention per-centages are small, showing that overfitting may be com-pensated by better specialisation or by other factors. Fi-nally, the kind of base classifier is also relevant. Divide-and-conquer methods, such as decision trees, particularly benefit from delegation because there will be several del-egating nodes which can be joined into a graft node, as discussed in section 3. We would argue that the good be-haviour of the round rebound configuration demonstrates some of these factors. Although the second classifier gener-ally improves classification accuracy on the delegated sam-ple, there is also a small subsample which is better bounced back to the original classifier, thus avoiding overfitting. Although AUC is also increased, the results are less con-clusive. One explanation could be found in the fact that the base probability estimation trees we are using are re-ally specialised for AUC, and hence improving their per-formance is difficult. Combining the predictions of the del-egating classifiers, or selecting the classifiers globally in-stead of hierarchically could improve the results in AUC. Another reason might be that some classes could have no delegated examples, and this may affect the overall AUC. The experimental results are inconclusive as to whether delegating works better with large datasets than smaller ones, but there does seem to be a preference for larger datasets. This and the better efficiency of delegating versus other combination techniques could justify the following modus operandi : (1) If comprehensibility and efficiency are crucial, use delegated decision trees with a no-rebound two-stage scenario. (2) If only efficiency is crucial, use the two-stage scenario with round-rebound or the iterated sce-nario (possibly employing efficient classifiers first and pos-sibly more computation-intensive classifiers for subsequent models). (3) If neither comprehensibility nor efficiency are crucial, try other classical ensemble methods. Finally, re-garding robustness, we have seen that for all the schemes discussed in this paper, delegation only degrades perfor-mance for at most 2 of the 22 datasets. The key idea of a delegating classifier is that it only makes predictions with a minimum level of confidence and del-egates the prediction to another classifier otherwise. We have argued that delegation is a fundamental notion in ma-chine learning, and we have analysed the similarities and differences with other general techniques such as separate-and-conquer rule learning and ensemble methods. We have investigated the performance of different configurations of delegating classifiers. In general, the results show that del-egating classifiers can significantly improve the accuracy of the predictions, without some of the disadvantages of other multiclassifiers. In particular, we do not combine the predictions of classifiers: each instance is classified by a single classifier. This does not degrade the comprehensi-bility of the model as ensemble methods do. Secondly, the resulting multi-classifier can often be simplified. For in-stance, delegating decision trees can be simplified to deci-sion graphs. Finally, our approach is considerably more ef-ficient than classical ensemble methods, because each sub-sequent classifier is learned using fewer examples than the previous one.
 As future work, we will investigate the use of different methods for the base classifiers at each stage. For exam-ple, the first classifier can be an efficient classifier such as Naive Bayes, while further down the delegation chain we use more data-intensive methods. We also plan to investi-gate combination of the predictions of the base classifiers, e.g., by weighting their prediction depending on the confi-dence and iteration. We would lose comprehensibility and the possibility of simplifying the partial solutions but we would maintain the same efficiency, and we could possibly come even closer to the accuracy of boosting or bagging. Another idea to pursue, when using decision trees, is a safe pre-pruning, which would detect when a node is not leading to leaves with confidence greater than the threshold. This would increase efficiency. We also plan to investigate the applicability of delegation beyond classification. Most of the presented ideas can be applied to regression and cluster-ing, provided each learned model has a good estimated re-liability for predictions or cluster membership. Finally, for multi-class problems, we can use the first classifier to deter-mine the most likely classes for an example, and then use several binary classifiers in the second stage. This would be specially interesting for support vector machines. Acknowledgements
