 Result merging is an important research problem in fed-erated search for merging documents retrieved from multi-ple ranked lists of selected information sources into a single list. The state-of-the-art result merging algorithms such as Semi-Supervised Learning (SSL) and Sample-Agglomerate Fitting Estimate (SAFE) try to map document scores re-trieved from different sources to comparable scores accord-ing to a single centralized retrieval algorithm for ranking those documents. Both SSL and SAFE arbitrarily select a single centralized retrieval algorithm for generating compa-rable document scores, which is problematic in a heteroge-neous federated search environment, since a single central-ized algorithm is often suboptimal for different information sources.

Based on this observation, this paper proposes a novel approach for result merging by utilizing multiple central-ized retrieval algorithms. One simple approach is to learn a set of combination weights for multiple centralized retrieval algorithms (e.g., logistic regression) to compute compara-ble document scores. The paper shows that this simple approach generates suboptimal results as it is not flexible enough to deal with heterogeneous information sources. A mixture probabilistic model is thus proposed to learn more appropriate combination weights with respect to different types of information sources with some training data. An extensive set of experiments on three datasets have proven the effectiveness of the proposed new approach.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Design, Performance Federated Search, Result Merging, Mixture Model
Federated search (also known as distributed information retrieval) [17, 23, 29] is an important research area of infor-mation retrieval. Unlike traditional information search sys-tems such as Google or Bing, which index webpages or doc-uments that can be crawled and collected, federated search targets on information distributed in independent informa-tion providers. Many contents in this environment may not be arbitrarily crawled and searched by traditional search en-gines, due to various reasons such as copyright, security and data protection. Only the owners of those documents can provide a full searching service to their set of documents. We refer to a collection of documents with its own and cus-tomized search engine as an information source. The size of this type of information (i.e., hidden Web contents) has been estimated to be many times larger than Web contents searchable by traditional search engines [3].
 Federated search offers a solution for searching hidden Web contents by building a bridge between users, who have little knowledge about which kind of information sources she is looking for, and the information sources that reveal limited information about their documents through source-specific search engines. To achieve this goal, federated search includes three main research problems: resource representa-tion, resource selection and result merging. Resource rep-resentation learns important information about the sources such as their contents and their sizes. Resource selection se-lects a subset of information sources which are most useful for users X  queries. Result merging combines documents re-trieved from selected sources into a single ranked list before presenting the list to the end users.

Among the above main problems, result merging substan-tially suffers from the heterogeneity of information sources. Each information source may adopt a different, customized retrieval model. A query can also be processed in many ways. Even if different sources use similar retrieval algo-rithms, they may have different source statistics (e.g., dif-ferent values of inverse document frequencies). All of those make it difficult to compare documents of different sources. A simple solution that downloads all document contents and ranks them with a single method for each user query may yield good results, but it is also costly in an online setting. Other solutions such as downloading parts of the documents [6] or incorporating scores from the resource selection com-ponent into source-specific document scores (e.g., CORI [4]) also suffer when information sources do not provide enough information or vary greatly in their scales of document rank-ing scores.

The state-of-the-art result merging algorithms merge doc-uments by learning how to map document scores in ranked lists of multiple information sources to comparable docu-ment scores. The basic idea is to utilize a centralized sample database created with all sample documents obtained in re-source representation. For each query, these algorithms rank documents in the centralized sample database with a single retrieval algorithm, and then build a mapping function be-tween source-specific document scores (or ranks) and compa-rable document scores. By mapping document scores/ranks returned from all selected sources to a common scale, it is possible to construct the final ranked list. Algorithms of this class such as SSL [27], SAFE [24] and WCF [12] have shown promising results. However, despite using various learning algorithms, those methods still do not fully address the het-erogeneity of retrieval algorithms in different information sources. The problem lies in the fact that all these existing methods arbitrarily select a single fixed centralized retrieval algorithm for learning the mapping, which is problematic in a heterogeneous federated search environment, as a single algorithm is often suboptimal for learning comparable scores for different sources.

In this paper, we propose a novel result merging algo-rithm that utilizes multiple centralized retrieval algorithms. This method can generate more accurate results in result merging due to the flexibility of using multiple types of cen-tralized retrieval algorithms for estimating comparable doc-ument scores. In particular, the paper shows that it is not desirable to learn a fixed set of weights (e.g., with a logistic regression approach) for different centralized retrieval algo-rithms in estimating comparable document scores. A mix-ture probabilistic model is proposed to automatically learn the appropriate weights for different types of information sources with some training data. The mixture model ap-proach is more flexible in calculating comparable document scores for a heterogeneous set of information sources. Empir-ical studies have been conducted with three federated search datasets to show the advantages of the proposed result merg-ing algorithm. In particular, one new dataset is created from the Wikipedia collection of ClueWeb data.

The rest of the paper is organized as follows. Section 2 discusses some research work generally related with the work in this paper. Section 3 discusses two specific state-of-the-art results merging algorithms (SSL and SAFE) as they are directly related with the proposed research. Section 4 proposes the novel result merging algorithm with multiple centralized retrieval algorithms. Section 5 introduces exper-imental methodology. Section 6 presents the detailed ex-perimental results and provides some discussions. Section 7 concludes and points out some future research directions.
Federated search includes three main research problems: resource representation, resource selection and result merg-ing. There is a large volume of previous research work in all of those research problems. This section first briefly intro-duces most related prior research in resource representation and resource selection. Then it will provide more details about the literature of result merging.

Resource representation is to collect information about each information source. Such information usually includes sources X  sizes, document frequencies, term frequencies, and other statistics. The START protocol [9] is one of the first attempt to standardize the communication between infor-mation sources and a broker (or centralized agent) in or-der to collect, search and merge documents from individual sources. However, this approach can only work in cooper-ative environments. In an uncooperative environment, it is more practical to collect source statistics with sampling al-gorithms. The query-based sampling method [4] is a popular algorithm for sampling documents from a set of information sources. In principal, query-based sampling sends randomly generated terms as queries to a source, and downloads the top documents as sample documents for each query. When this process is done, the set of all sample documents can be collected in a centralized sample database to build a sin-gle index. The centralized sample database is often a good representative of the (imaginary) complete database of all documents in a federated search environment.

Resource selection is to select a subset of information sources most relevant to the user X  X  query. Resource selec-tion has been studied intensively during the last two decades. Many algorithms have been developed, such as GlOSS [10], CORI [4], ReDDE [26], CRCS [22], topic models [2], the classification-based model [1] and many others. The Rele-vant Document Distribution Estimation (ReDDE) resource selection algorithm and its variants have been shown to gen-erate good and robust resource selection results in differ-ent types of federated search environments. ReDDE selects relevant sources by first ranking sample documents in the centralized sample database. Then, each document among the top of the list can contribute a score to its containing source. The magnitude of the score depends on both doc-ument X  X  rank and the source X  X  size. Finally, the relevance of a source is measured by the combined score of all of its sample documents.

Result merging is to collect the ranked list of documents from each selected source and combine them into a single ranked list to present to users. Result merging in federated search is similar to data fusion [32, 25], or merging process in multilingual information retrieval [30]. In data fusion, different retrieval models are applied to a single information source, and the problem is to get the best combination of retrieval algorithms. Whereas, in federated search, there are multiple information sources with different (often unknown) retrieval models. Similar to information fusion, multilingual information retrieval also assumes that the whole collection index is available to the merger during the process, which is not always the case in federated search.

One scenario is that the broker can download all returned documents from selected sources, and apply a centralized retrieval algorithm to produce the final ranked list. How-ever, in practice, this method is rarely used since the high cost of communication and time may impair user experi-ence. In another simple case, when all sources implement the same retrieval model, documents X  scores (or ranks) re-turned by the source may be comparable with each other. Thus, merging their scores (or ranks) directly (also known as Raw Score Merging), or in a round-robin fashion may give good results with low cost. However, it is noticed that even if all sources share the same model, some statistics such as document frequency of a term are still different across dif-ferent sources. It is generally not practical to assume that all independent sources share such a same set of collection statistics.

Some other algorithms in the early generation of feder-ated search also relied on term statistics for making decision. Craswell et al. suggested that by partially downloading a part of the top returned documents, we can approximate term statistics to build the final rank list [6]. Xu and Croft requested that document statistics of query terms should be provided to the broker, in such a way that they can calcu-late the global inverse document frequencies [34]. However, these algorithms again require some type of collaboration from the independent sources, which is often unavailable.
CORI result merging algorithm [4] is a relatively simple, yet effective algorithm. The intuition is that comparable document scores should depend on two factors: (i) how good a document is compared to other documents from the same source; and (ii) how good the source containing a particular document is compared to other sources. CORI makes a lin-ear combination of those two factors and gets the final score of a document as: where D 0 is the global score, D is the original score within the source, and C 0 is the normalized source score from the resource selection step.

The merging algorithm proposed by Rasolofo et al. [19] also explores the combination between document scores and source weights. Unlike CORI, their source weights are not directly related with the sources X  relevance scores. Rather, the weight of a source depends on the total number of docu-ments that it returns. The algorithm assumes that a source containing more relevant documents may return a longer ranked list, which is not always the case for information sources using different types of ranking algorithms.
The intuition of combining document and resource scores can also be seen in a variant of the PageRank algorithm in distributed environments [31]. In this work, Wang and De-Witt employed the source X  X  ServerRank and the document X  X  LocalRank to derive the global PageRank values.

Semi-Supervised Learning (SSL) [27] and the Sample Ag-glomerate Fitting Estimate (SAFE) [24] result merging algo-rithms offer a better trade-off in efficiency and effectiveness. Both methods try to map source-specific document ranks into comparable document scores generated by a single cen-tralized retrieval algorithm. We will provide more detailed information about SSL and SAFE in the following section as they are directly related with the new research in this paper.
Semi-Supervised Learning Merging (SSL) [27] uses curve fitting model to calculate comparable document scores from different sources for result merging. Specifically, given a user X  X  query, SSL sends the query to the centralized sam-ple database and retrieves the sample ranked list with rel-evance score of each document. Upon receiving documents from a selected information source, SSL checks for overlap-ping documents exist in the sample database. Those over-lapping documents are characterized by two features: the relevance scores in the central sample database, and the rel-evance ranks in the specific source. The task is to estimate the relevance scores of all non-overlapping documents in the centralized complete database (the imaginary dataset of all documents of all sources). Assume that there is a linear mapping between centralized relevance scores and source-specific document ranks, then that mapping can be inferred by using a regression method on the overlapping documents. Having said that, let R ij be the source-specific rank of doc-ument d i in source C j , and S ij be the relevance score of doc-ument d i in the centralized sample database, we can build a linear relationship.
 where a j ,b j are two parameters depending on each pair of an information source and a query.

With enough overlapping documents for a source and a query, we can train a regression matrix In the above equation, let us denote the first matrix by X , the second matrix by W , and the third matrix by Y . By minimizing the square loss error, we can derive the solution to the parameters W as
One main problem of SSL is that if there is not enough overlapping documents (three requested in the original SSL work) for building a linear mapping, the model will back off to the CORI result merging formula, which is often much less effective.
Sample-Agglomerate Fitting Estimate (SAFE) [24] over-comes the SSL X  X  problem of not having enough overlapping documents by estimating the ranks of unoverlapping docu-ments in the centralized sample database. If we assume that the sampling process is uniform, then each sample document will represent the same number of unseen documents in the selected information source. Therefore, a sample document ranked at position i -th in the source-specific sample ranked list will have an approximate rank i  X  | C | | C specific full rank, where | C | is the source X  X  estimated size, and | C s | is the source X  X  sample size. By using the estimated source-specific ranks together with true centralized ranks (of overlapping documents), SAFE could apply regression with more information than SSL. A problem may occur when there are not enough sample documents of a selected infor-mation source in the centralized sample database. However, this is rarely the case, if ReDDE (or its variants) is used for selecting information sources, since this method usually selects a source if it has a significant number of documents in the centralized ranked list with respect to the query.
Another contribution of SAFE to SSL is that, instead of using the raw rank information of documents, SAFE applies different transformation functions to the rank, in order to find the best regression. More specifically, there are four different transformations as in Table 1. Each transforma-tion function is applied to the source-specific ranked list to learn the set of parameters ( a ij ,b ij ). Then, SAFE se-lects the best transformation by comparing the goodness of curve-fitting of all models based on their coefficient of deter-mination R 2 values [11]. Specifically, for a linear regression equation X  X  w = Y , the coefficient of determination is calculated as follows.
 where P = X ( X T X )  X  1 X T
SSL and SAFE are state-of-the-art algorithms for result merging in federated search. However, because of their choos-ing of a single centralized retrieval algorithm for calculat-ing comparable document scores, these algorithms still do not fully address the heterogeneity of different information sources in federated search environments . A single cen-tralized retrieval algorithm may have good curve-fittings for some information sources, but may also be less fit for some others. This paper proposes to use multiple centralized re-trieval algorithms to retrieve a set of ranking scores for each document. Moreover, rather than assigning a fixed set of weights to combine the above scores, our model learns a more appropriate combination of weights with respect to dif-ferent types of information sources. We assume that there is an underlying distribution (i.e., latent groups) of sources according to their adopted retrieval models. Learning the proposed model thus becomes learning the distribution of groups and the combination weights associated with each group. This model is called the Mixture of Retrieval Models (MoRM) for result merging. MoRM is related with SSL and SAFE in the way that it uses the centralized sam-ple database to learn the comparable document scores by curve-fitting. However, unlike SSL and SAFE, MoRM em-ploys multiple retrieval algorithms for the centralized sam-ple database. Therefore it is more flexible to address the heterogeneity of information sources in federated search en-vironments for improving the accuracy of result merging.
In this section, we describe the general framework of MoRM for document merging. The following steps are applied when a query comes:
In the following sections, we will propose a simple logistic regression model (for learning a single set of combination weights) and then propose the mixture of retrieval models (for learning multiple sets of combination weights) for the task of estimating documents X  comparable scores.
A learning algorithm such as logistic regression may ad-dress the problem of combining different document scores seamlessly. We chose logistic regression to demonstrate the approach of learning a single set of combination weights for ranking documents. Logistic regression is a discriminative model that models the probability that a binary event hap-pens by a sigmoid function. In this case, our predictive functions are:
P ( y q cd = 1 | ~w,~x q cd ) = 1 and
P ( y q cd =  X  1 | ~w,~x q cd ) = 1  X  P ( y q cd = 1) =  X  (  X  ~w  X  ~x where our notations for this model are as follows: let the superscript q refer to the query q , and the subscript cd refer to the d -th document of source c (such a document is called D cd ). We also use ~x q cd to denote the feature vector of docu-ment D cd (the set of comparable scores of D cd according to different centralized retrieval algorithms), and ~w for the set of weights associated with ~x q cd . Our target is to predict y which is the relevance of document D cd with respect to the query q . The possible values of y q cd are:
Finally, in the equations (1) and (2) above, we also use  X  ( z ) to indicate the sigmoid function and apply this property:  X  (  X  x ) = 1  X   X  ( x ).

Given C , the number of sources; Q , the number of queries, and all the returned documents D cd with respect to the training queries, we can write the likelihood function of the model as
L ( ~w ) = where we have combined equations (1) and (2) above. Learn-ing the combination weight ~w can be done by maximizing the log-likelihood function using the iterative re-weighted least squares method [8].
We now describe the mixture model of retrieval algorithms (MoRM) for result merging. MoRM offers more prediction capability by automatically learning multiple sets of com-bination weights, each of them is associated with a  X  X oft X  information source cluster. The word  X  X oft X  means that we use probability to assign a source to its cluster, rather than fixing a hard assignment. Specifically, assuming that there are K of such clusters, and let  X  ck be the probability that the source c belongs to group k , then the following constraints must be hold:
To make our formulations simpler, in this section, we will first derive the formulations for only one query, and drop the superscript q of y cd and ~x cd . At the end of this sec-tion, we will extend the formulations for the set of training queries. Furthermore, we will denote  X  cdk for P ( y cd | ~w (the probability that the document D cd has relevance y cd the query in question, given that the collection c belongs to cluster k . In short,  X  cdk =  X  ( y cd ~w k  X  ~x cd )).
Let ~ w = { ~w k | k = 1 ,  X  X  X  ,K } , and  X  = {  X  ck | c = 1 ,  X  X  X  , C ; k = 1 ,  X  X  X  ,K } . Given a query q , let ~  X  = { ~ w ,  X  } denote the set of parameters, in which each combination weight ~w k is associated with the k -th cluster. MoRM assumes the same combination weight for all sources of a cluster for building robust combination model with a limited amount of training data, hence we will set  X  k =  X  ck =  X  c 0 k for all sources c,c
The probability that a document D cd has relevance y given all parameters is calculated as follows.

The component  X  k acts as the prior of the clusters X  dis-tribution, which adjusts the belief of relevance according to each cluster. This equation is also known as the mixture of logistic regression. Given that model, the likelihood func-tion for the training dataset with respect to one query is as follows.
 where D c is the number of documents returned by the source c .

It is difficult to optimize the above function directly, since taking its logarithm still presents the summation inside the log. Therefore, we will utilize the Expectation Maximization (EM) algorithm [7] to learn the parameters. The derivation of EM algorithm is discussed in the following section.
Let z c be a K -dimensional latent variable associated with source c . z c has only one element which equals to 1 and the all other elements equal 0 (i.e., a 1-of-K representation). Therefore, z c must satisfy the following constraints. where z ck is the k -th element of z c .

We then use z c as the indicator of the membership of source c . If c belongs to cluster k , then z ck = 1, and the other elements of z c equal 0. Given that  X  k is the prior distribution of cluster k as above, and note that  X  ck =  X  for all c , we can write the prior distribution of z ck as follows. Then the prior distribution of the whole vector z c can be written as
Define another random variable Z = { z c | c = 1 ,  X  X  X  , C} associated with all sources. Since each source is independent of each other, the prior of Z is just the multiplication over all sources.

Similarly, if we know that the source c has the member-ship vector z c , then the probability that the document D of that source has relevance y cd is Q K k =1  X  cdk z ck , since  X  the conditional probability with respect to cluster k . There-fore, the likelihood function of the model is obtained by multiplying the above term over all sources and documents. where X denotes all document feature vectors, and Y de-notes the relevance vector of all documents. Multiplying the equations (7) and (8) above, one can calculate the complete likelihood function Taking the logarithm of the above function yields the com-plete log-likelihood as follows. log P ( X , Y , Z | ~  X  ) =
The EM algorithm involves two steps. For the E-step, we need to calculate the posterior probability P ( Z | X , Y , Using (9), we can derive the following relation.

P ( Z | X , Y , ~  X  ) = P ( X , Y , Z | where we can use the proportional sign  X  because the de-nominator P ( X , Y | ~  X  ) does not depend on Z .

We wish to calculate the expectation of the variable Z under the above posterior distribution, since that term will be useful in the following M step. Given that all z independent, and the right-hand side of equation (11) can be factorized over c , we can derive the expectation of each variable z ck as where we have defined a new variable  X  ( z ck ).

In the M-step, the updated parameters ~  X  new are calcu-lated according to the following formula where
Taking the expectation of log P ( X , Y , Z | ~  X  ) (as derived in equation (10)) with respect to the posterior distribution gives us the following objective function for the M-step. {  X  new , ~ w new } = arg max
To find the new value of  X  k , we only need to maximize the first part of the above function subject to the constraint P K k =1  X  k = 1
Using Lagrange multiplier and setting the gradient to 0, one can solve the optimal values of  X  k as
Searching for the value of ~w new k is a bit trickier, since we have to solve the following optimization problem.
In fact, the gradient of the above objective function with respect to ~w k is equal to: Therefore, one can apply a gradient descent algorithm to find the optimal value of ~w k .

In the implementation of the algorithm discussed so far, there is an issue about  X  ( z ck ). As equation (12) has shown, computing  X  ( z ck ) involves calculating the product Q D This could lead to numerical underflow since  X  cdk is a proba-bility smaller than 1. Therefore, we need to calculate  X  ( z under the log space. Let and  X  max = max K k =1  X  ( z ck ). Therefore  X  ( z ck ) =  X  ( z ck )
Since each log  X  ( z ck ) is computable under the log space, the above equation will avoid the underflow problem. Fi-nally, we extend our formulations to the set of training queries. In this case, the E-step becomes: In the M-step, the update formula of  X  k remains the same (equation (14)), while the optimization function in equation (15) becomes and the objective gradient with respect to ~w k is
In this section, we will describe the methodology and datasets of this work. The experiments were conducted on three datasets: two standard TREC datasets, and one Wikipedia dataset for federated search based on the ClueWeb. number of documents ranges from 4,400 to 434,525.
Table 2 provides more statistics of the three datasets, in-cluding sizes, number of information sources; the max, min and average of the number of documents of each source. We also provide the query statistics of each dataset, including
The partition assignments are available at http://www.cs.purdue.edu/homes/dthong/clueweb the number of queries; the max, min and average of the number of relevant documents per query.

The ClueWeb 09 is a large-scale collection of web doc-uments that was collected in January and February 2009. The entire dataset consists of about one billion web pages in ten languages. For its tremendous size, the ClueWeb has been used in several tracks of the Text REtrieval Conference (TREC), most notably in the Web track. For distributed environment (in a different problem setting), ClueWeb has been used in [15]. It is desired to construct a new dataset based on ClueWeb for experiments in federated search.
Three Web tracks of TREC (from 2009 to 2011) have been using the ClueWeb so far. Each track has provided 50 queries based on which we build the new testbed. Within the full ClueWeb dataset, Wikipedia is the main contributor of relevant documents for Web track queries. The total size of Wikipedia is about 6 million documents, which is rea-sonable for creating a separate testbed. We extract all Wiki documents, and apply the same K-means algorithm that was used for creating the TREC4-Kmeans. We also select only 106 queries which contain at least one relevant Wikipedia document (out of the 150 provided queries) for training and testing. In the end, we constructed 100 information sources for the testbed ClueWeb-Wiki, with statistics provided in Table 2. The distribution of source sizes is also shown in Figure 1. Most of the sources have less 70,000 documents, and there are 12 sources of more than 100,000 documents.
Given a set of information sources, we assign each source a retrieval algorithm chosen from a set of: vector space TF.IDF with  X  X tc X  weighting [21], a unigram statistical lan-guage model with linear smoothing (with smooth parameter as 0.5) [16] and Okapi [20] in a round robin manner. This choice is based on the fact that those models are commonly and widely used in information retrieval. Each information source is set to return at most 200 documents for each query. At the centralized sample database, we utilize five models: the three above models, the Inquery [5] and the Indri [28] algorithm. All retrieval algorithm implementations use the Lemur Toolkit [14]. We randomly select a set of queries for training, and used the other set for testing. For the TREC123, there are 50 training queries out of 100; those numbers of TREC4-Kmeans and ClueWeb-Wiki are 25 out of 50 and 50 out of 106.

We choose K , the number of latent groups, to be 3 in our main results. Some experimental results with different K values are also presented. For each information source, we sample at most 300 documents for creating the central-ized sample database. For each query, we use ReDDE to select the top 5 sources for TREC123, TREC4-Kmeans and ClueWeb-Wiki.

Our metrics for the performance is the high-precision at document level, which is the percentage of the number of relevant documents in the final merged ranked list. Given that list, we measure the precision at top 5, 10, 15, 20 and 30 respectively. In next section, we will present our experi-mental results of all datasets.
We now present the high-precision results on the above three testbeds. Tables 3-5 show the high-precision results on TREC123, TREC4-Kmeans and ClueWeb-Wiki respec-tively. The first column is our baseline using SAFE algo-rithm with Indri [18] as the single centralized retrieval algo-rithm. SAFE has been demonstrated to generate accurate and robust results compared with SSL and other results merging algorithms. We denote this method as SFI. The LR column presents the results using the logistic regression model to learn the combination weights of all centralized retrieval methods. The last column MoRM presents the re-sults using the proposed mixture of retrieval algorithms. All precision results of LR and MoRM are compared with the baseline SFI using paired t-tests at level p &lt; 0 . 05.
For TREC123, the performance of MoRM is significantly better than that of SFI. MoRM is also consistently better than the performance of logistics regression model. In gen-eral, both learning methods show improvements over the baseline method of one single feature. For TREC4-Kmeans, MoRM also outperforms SFI, although the differences are not significant as in TREC123. This can be explained as in TREC4-Kmeans, we only train on 25 queries, whereas in TREC123, we trained on 50 queries. These above results Table 3: High-precision result on TREC123 with 300 sample documents and top 5 information sources selected for each query. A * denotes a significant difference at level p &lt; 0 . 05 compared to the original SAFE algorithm using Indri as the only centralized retrieval algorithm.
 Doc TREC123
Rank SFI LR MoRM @5 0.268 0.332 (+ 23.88 %) 0.344 (+ 28.36 %) * @10 0.246 0.272 (+ 10.57 %) 0.304 (+ 23.58 %) * @15 0.229 0.267 (+ 16.31 %) 0.279 (+ 21.54 %) * @20 0.208 0.251 (+ 20.67 %) * 0.261 (+ 25.48 %) * @30 0.208 0.229 (+ 9.95 %) 0.232 (+ 11.54 %) Table 4: High-precision result on TREC4-Kmeans with 300 sample documents and top 5 information sources selected for each query. A * denotes a significant difference at level p &lt; 0 . 05 compared to the original SAFE algorithm using Indri as the only centralized retrieval algorithm.
 Doc TREC4-Kmeans
Rank SFI LR MoRM @5 0.272 0.280 (+ 2.94 %) 0.296 (+ 8.82 %) @10 0.244 0.252 (+ 3.28 %) 0.256 (+ 4.92 %) @15 0.211 0.227 (+ 7.59 %) 0.227 (+ 7.59 %) @20 0.192 0.212 (+ 10.42 %) 0.216 (+ 12.50 %) @30 0.177 0.193 (+ 9.02 %) 0.192 (+ 8.29 %) Table 5: High-precision result on ClueWeb-Wiki with 300 sample documents and top 5 information sources selected for each query. A * denotes a significant difference at level p &lt; 0 . 05 compared to the original SAFE algorithm using Indri as the only centralized retrieval algorithm.

Rank SFI LR MoRM @10 0.146 0.163 (+ 11.00 %) 0.173 (+ 18.31 %) @15 0.139 0.164 (+ 17.95 %) 0.168 (+ 20.53 %) @20 0.132 0.150 (+ 13.55 %) 0.153 (+ 15.59 %) @30 0.111 0.121 (+ 9.67 %) 0.123 (+ 10.75 %) have shown the advantage of using multiple centralized re-trieval algorithms for learning comparable document scores, over the previous model that uses only one single central-ized retrieval algorithm. It also demonstrates the advantage of using the mixture model of multiple sets of weights over the logistic regression model that uses only one single set of combination weights.

For the Wikipedia dataset based on ClueWeb, the pro-posed model also consistently outperforms SFI and LR. The differences however are not significant. Such a significance may be harder to achieve, since on average, this dataset contains less relevant documents per query than the other datasets, as shown in Table 2.
In this section, we discuss the experimental results when the number of latent variable K changes. We only report TREC123 and ClueWeb-Wiki for these experiments, and try different configuration of K = { 1 , 3 , 5 , 10 } . Similar pattern can be observed on the TREC4-Kmeans. K = 1 is actually equivalent to the logistic regression model. Figure 2 shows the results of this experiment. It can be seen that the mix-ture of retrieval algorithms model is quite consistent with a small range of K values. For ClueWeb-Wiki, the perfor-mance of the mixture model with K &gt; 1 is at least equal or higher than that of the logistic regression. For TREC123, the performances with different values of K are also stable for most of the test levels.
This paper proposes a novel method of mixture model with multiple centralized retrieval algorithms for result merg-ing in federated search. Existing result merging algorithms do not fully address the issue of heterogeneity of informa-tion sources in federated search. Their arbitrary choices of a single centralized retrieval algorithm suffer from the fact that information sources are inherently different in source statistics, query processing techniques, and/or document re-trieval algorithms. The proposed model attempts to com-bine various evidence from multiple centralized retrieval al-gorithms in a mixture model framework, in order to map source-specific document ranks to comparable scores for re-sult merging. We have shown that a single set of combi-nation weights of the evidence do not offer enough flexi-bility in dealing with such a heterogeneous environment. A mixture model that learns multiple sets of combination weights according to the clusters of sources proves to be a better choice. A set of experiments has been conducted with two traditional TREC datasets and a new dataset based on the ClueWeb. The empirical results in three datasets have demonstrated the effectiveness of the proposed mix-ture model with multiple centralized retrieval algorithms.
This model could be extended in many ways. For in-stance, we could add more flexibility to the model by cus-tomizing the prior distribution  X  k independently for each source, which means a source will be associated with a set of combination weights independent of the others. However, this model could require a larger training dataset to learn the parameters. A hybrid model where a cluster of similar sources independently uses multiple sets of weights is more feasible. The similarity between sources will play an impor-tant factor in creating those clusters. Another direction is to build a mixture model based on cluster of queries instead of cluster of sources, in which each query will trigger a differ-ent set of combination weights of all features. Furthermore, we can combine both of the above methods. It is also inter-esting to explore other types of evidence, such as the links between documents from different sources and incorporate them into the learning model. This work is partially supported by NSF research grants IIS-0746830, CNS-1012208 and IIS-1017837. This work also partially supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370. [1] J. Arguello, J. Callan, and F. Diaz.
 [2] M. Baillie and M. Carman. A multi-collection latent [3] M. Bergman. The deep web: surfacing the hidden [4] J. Callan. Distributed information retrieval. Advances [5] J. Callan, W. B. Croft, and S. M. Harding. The [6] N. Craswell, D. Hawking, and P. Thistlewaite. Merging [7] A. P. Dempster, N. M. Laird, and D. Rubin.
 [8] R. Fletcher. Practical methods of optimization, volume [9] L. Gravano, C.-C. K. Chang, H. Garcia-Molina, and [10] L. Gravano, H. Garcia-Molina, and A. Tomasic. Gloss: [11] J. Gross. Linear regression , volume 175. Springer [12] C. He, D. Hong, and L. Si. A weighted curve fitting [13] http://lemurproject.org/clueweb09/. The clueweb09 [14] http://www.lemurproject.org/. The lemur toolkit. [15] A. Kulkarni and J. Callan. Document allocation [16] J. Lafferty and C. Zhai. Document language models, [17] W. Meng and C. Yu. Advanced metasearch engine [18] D. Metzler and W. B. Croft. Combining the language [19] Y. Rasolofo, F. Abbaci, and J. Savoy. Approaches to [20] S. Robertson, S. Walker, S. Jones, [21] G. Salton, E. Fox, and H. Wu. Extended boolean [22] M. Shokouhi. Central-rank-based collection selection [23] M. Shokouhi and L. Si. Federated search. 2011. [24] M. Shokouhi and J. Zobel. Robust result merging [25] X. M. Shou and M. Sanderson. Experiments on data [26] L. Si and J. Callan. Relevant document distribution [27] L. Si and J. Callan. A semisupervised learning method [28] T. Strohman, D. Metzler, H. Turtle, and C. W. B. [29] P. Thomas. Server selection in distributed information [30] M. Tsai, H. Chen, and Y. Wang. Learning a merge [31] Y. Wang and D. J. DeWitt. Computing pagerank in a [32] S. Wu, Y. Bi, and X. Zeng. The linear combination [33] J. Xu and J. Callan. Effective retrieval with [34] J. Xu and W. B. Croft. Cluster-based language models
