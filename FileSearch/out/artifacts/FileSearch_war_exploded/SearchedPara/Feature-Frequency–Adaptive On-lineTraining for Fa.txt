 Peking University Hong Kong Polytechnic University Peking University Hong Kong Polytechnic University achieving state-of-the-art scores on the tasks with different characteristics. 1. Introduction Training speed is an important concern of natural language processing (NLP) systems.
Large-scale NLP systems are computationally expensive. In many real-world applica-tions, we further need to optimize high-dimensional model parameters. For example, the state-of-the-art word segmentation system uses more than 40 million features (Sun,
Wang, and Li 2012). The heavy NLP models together with high-dimensional parameters lead to a challenging problem on model training, which may require week-level training time even with fast computing machines.
 has high accuracy. Typically we need to make a tradeoff between speed and accuracy, to trade training speed for higher accuracy or vice versa. In this work, we have tried to overcome this problem: to improve the training speed and the model accuracy at the same time.
 gradient descent methods are normally batch training methods, in which the gradient computed by using all training instances is used to update the parameters of the model.
The batch training methods include, for example, steepest gradient descent, conjugate gradient descent (CG), and quasi-Newton methods like limited-memory BFGS (Nocedal and Wright 1999). The true gradient is usually the sum of the gradients from each individual training instance. Therefore, batch gradient descent requires the training method to go through the entire training set before updating parameters. This is why batch training methods are typically slow.
 with batch training methods. A representative on-line training method is the stochastic 1998; Vishwanathan et al. 2006). The model parameters are updated more frequently compared with batch training, and fewer passes are needed before convergence. For large-scale data sets, on-line training methods can be much faster than batch training methods.
 enough for training large-scale NLP systems X  X robably because those methods are not well-tailored for NLP systems that have massive features. First, the convergence the existing on-line training methods typically require more than 50 training passes before empirical convergence, which is still slow. For large-scale NLP systems, the training time per pass is typically long and fast convergence speed is crucial. Second, the accuracy of the existing on-line training methods is not good enough. We want to further improve the training accuracy. We try to deal with the two challenges at the same time. Our goal is to develop a new training method for faster and at the same time more accurate natural language processing.
 d escent based on feature f requency information (ADF), 1 on-line training of NLP systems. Other than the high training accuracy and fast train-ing speed, we further expect that the proposed training method has good theoretical 564 properties. We want to prove that the proposed method is convergent and has a fast convergence rate.
 updating. This learning rate vector is automatically adapted based on feature frequency information in the training data set. Each model parameter has its own learning rate adapted on feature frequency information. This proposal is based on the simple intu-rate that decays faster. This is because a higher frequency feature is expected to be well optimized with higher confidence. Thus, a higher frequency feature is expected to sound training algorithm, ADF.
 2. Related Work Our main focus is on structured classification models with high dimensional features.
For structured classification, the conditional random fields model is widely used. To illustrate that the proposed method is a general-purpose training method not limited to imum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996) is widely used. Here, we review the conditional random fields model and the related work of on-line training methods. 2.1 Conditional Random Fields
The conditional random field (CRF) model is a representative structured classification model and it is well known for its high accuracy in real-world applications. The CRF model is proposed for structured classification by solving  X  X he label bias problem X  (Lafferty, McCallum, and Pereira 2001). Assuming a feature function that maps a pair of (Lafferty, McCallum, and Pereira 2001): where w w w is a parameter vector. parameter estimation is performed by maximizing the objective function,
The first term of this equation represents a conditional log-likelihood of training data. The second term is a regularizer for reducing overfitting. We use an L 2.2 On-line Training The most representative on-line training method is the SGD method (Bottou 1998;
Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). The SGD method uses a randomly selected small subset of the training sample to approximate the gradient of an objective function. The number of training samples used for this approximation is called the batch size . By using a smaller batch size, one can update the parameters more frequently and speed up the convergence. The extreme case is a batch size of 1, and it gives the maximum frequency of updates, which we adopt in this work. In this case, the model parameters are updated as follows: where t is the update counter,  X  t is the learning rate or so-called decaying rate, and
L of SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], and
Sun et al. [2013].) Following the most recent work of SGD, the exponential decaying rate works the best for natural language processing tasks, and it is adopted in our implementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). and Schapire 1999), averaged perceptron training (Collins 2002), more recent devel-opment/extensions of stochastic gradient descent (e.g., the second-order stochastic gradient descent training methods like stochastic meta descent) (Vishwanathan et al. method requires the computation or approximation of the inverse of the Hessian matrix fication models. Usually the convergence speed based on number of training iterations is still large.
 Sperduti and Starita 1993; Dredze, Crammer, and Pereira 2008; Duchi, Hazan, and
Singer 2010; McMahan and Streeter 2010), our work is fundamentally different. The proposed ADF training method is based on feature frequency adaptation, and to the best of our knowledge there is no prior work on direct feature-frequency X  X daptive on-line 566 training. Compared with the confidence-weighted (CW) classification method and its variation AROW (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza, and Dredze 2009), the proposed method is substantially different. While the feature frequency information is implicitly modeled via a complicated Gaussian distribution framework in Dredze, Crammer, and Pereira (2008) and Crammer, Kulesza, and Dredze (2009), the frequency information is explicitly modeled in our proposal via simple learning rate adaptation. Our proposal is more straightforward in capturing feature frequency information, and it has no need to use Gaussian distributions and KL divergence, which are important in the CW and AROW methods. In addition, our proposal is a probabilistic learning method for training probabilistic models such as CRFs, whereas the CW and AROW methods (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza, and Dredze 2009) are non-probabilistic learning methods extended from perceptron-of the conference version (Sun, Wang, and Li 2012). Sun, Wang, and Li (2012) focus on the specific task of word segmentation, whereas this article focuses on the proposed training algorithm. 3. Feature-Frequency X  X daptive On-line Learning
In traditional on-line optimization methods such as SGD, no distinction is made for different parameters in terms of the learning rate, and this may result in slow conver-gence of the model training. For example, in the on-line training process, suppose the high frequency feature f 1 and the low frequency feature f sample and their corresponding parameters w 1 and w 2 are to be updated via the same learning rate  X  t . Suppose the high frequency feature f 1 and the low frequency feature f 2 has only been updated once. Then, it is possible that the weight w 1 is already well optimized and the learning rate  X  updating w 1 . Updating the weight w 1 with the learning rate  X  causes fluctuations in the on-line training and results in slow convergence speed. On speed.
 convergence speed, the proposed method is easy to implement. The proposed method with feature-frequency X  X daptive learning rates can be seen as a learning method with specific diagonal approximation of the Hessian information based on assumptions of feature frequency information. In this approximation, the diagonal elements of the diagonal matrix correspond to the feature-frequency X  X daptive learning rates. Accord-ing to the aforementioned example and analysis, it assumes that a feature with higher frequency in the training process should have a learning rate that decays faster. 3.1 Algorithm In the proposed ADF method, we try to use more refined learning rates than traditional the learning rate scalar to a learning rate vector, which has the same dimension as the frequency information. By doing so, each weight has its own learning rate, and we will show that this can significantly improve the convergence speed of on-line learning.
The update term g g g t is the gradient term of a randomly sampled instance:
In addition,  X   X   X  t  X  R f + is a positive vector-valued learning rate and component-wise (Hadamard) product of two vectors.
 training process has a learning rate that decays faster. This is because a weight with higher frequency is expected to be more adequately trained, hence a lower learning rate is preferable for fast convergence. We assume that a high frequency feature should have a lower learning rate, and a low frequency feature should have a relatively higher ically sound training algorithm. The proposed method with feature-frequency X  X daptive learning rates can be seen as a learning method with specific diagonal approximation of the inverse of the Hessian matrix based on feature frequency information. this window. Given a feature k , we use u to record the normalized frequency: information, as follows: itively, the upper bound  X  corresponds to the adaptation factor of the lowest frequency features, and the lower bound  X  corresponds to the adaptation factor of the highest frequency features. The optimal values of  X  and  X  can be tuned based on specific real-world tasks, for example, via cross-validation on the training data or using held-out real-world natural language processing tasks:  X  around 0.995, and  X  around 0.6. This different natural language processing tasks.
 linear approximation. Finally, the learning rate is updated as follows: 568
With this setting, different features correspond to different adaptation factors based on feature frequency information. Our ADF algorithm is summarized in Figure 1. mization based on sparse features (Shalev-Shwartz, Singer, and Srebro 2007). Similarly, features. Note that although binary features are common in natural language processing valued features. 3.2 Convergence Analysis
We want to show that the proposed ADF learning algorithm has good convergence properties. There are two steps in the convergence analysis. First, we show that the
ADF update rule is a contraction mapping. Then, we show that the ADF training is asymptotically convergent, and with a fast convergence rate.
 function of traditional classification or regression problems: where f f f i is the feature vector generated from the training sample ( x x x tion in w w w  X  f f f i , such as 1 2 ( y i  X  w w w  X  f f f classification.
 introduce several mathematical definitions. First, we introduce Lipschitz continuity: Definition 1 (Lipschitz continuity)
A function F : X  X  R is Lipschitz continuous with the degree of D if
D | x  X  y | for  X  x , y  X  X . X can be multi-dimensional space, and between the points x and y .
 Lipschitz constant || F || Lip as follows:
Definition 2 (Lipschitz constant) that makes the function F Lipschitz continuous.
 contraction mapping as follows: Definition 3 (Contraction mapping)
A function F : X  X  X is a contraction mapping if its Lipschitz constant is smaller than 1: || F || Lip &lt; 1.
 Lemma 1 (SGD update rule is contraction mapping)
Let  X  be a fixed low learning rate in SGD updating. If  X   X  the SGD update rule is a contraction mapping in Euclidean space with Lipschitz con-tinuity degree 1  X   X / X  2 .
 one dynamic update process. In this dynamic process, if we use the same update rule F , we have w w w t + 1 = F ( w w w t ) and w w w t + 2 = F ( w w w update is a contraction mapping restricted by this one-following-one dynamic process. contraction mapping . We formally define dynamic contraction mapping as follows. Definition 4 (Dynamic contraction mapping)
Given a function F : X  X  X , suppose the function is used in a dynamic one-following-one process: x t + 1 = F ( x t ) and x t + 2 = F ( x t + 1 dynamic contraction mapping if  X  D &lt; 1, | x t + 2  X  x t + 1 a dynamic contraction mapping is not necessarily a contraction mapping. We first show 570 a dynamic contraction mapping.
 Theorem 1 (ADF update rule with fixed learning rates)
Let  X   X   X  be a fixed learning rate vector with different learning rates. Let  X  imum learning rate in the learning rate vector  X   X   X  :  X  max mapping in Euclidean space with Lipschitz continuity degree 1 The proof is sketched in Section 5.
 rate vector is a dynamic contraction mapping, because the real ADF algorithm has a decaying learning rate vector. In the decaying case, the condition that  X  || X  after a number of training passes (denoted as T )  X   X   X  T that  X  max : = sup {  X  i where  X  i  X   X   X   X  T } and  X  max  X  losing generality, our convergence analysis starts from the pass T and we take  X   X   X  in the following analysis. Thus, we can show that the ADF update rule with a decaying learning rate vector is a dynamic contraction mapping: Theorem 2 (ADF update rule with decaying learning rates)
Let  X   X   X  t be a learning rate vector in the ADF learning algorithm, which is decaying over the time t and with different decaying rates based on feature frequency infor-mation. Let  X   X   X  t start from a low enough learning rate vector  X   X   X  ( || update rule with decaying learning rate vector is a dynamic contraction mapping in
Euclidean space with Lipschitz continuity degree 1  X   X  max The proof is sketched in Section 5.
 demonstrate the convergence properties of the ADF training method. First, we prove the convergence of the ADF training.
 Theorem 3 (ADF convergence) ADF training is asymptotically convergent.
 The proof is sketched in Section 5.
 lowest learning rate  X   X   X  t + 1 =  X  X   X   X  t , the expectation of the obtained w w w 1998; Hsu et al. 2009): function. The rate of convergence is governed by the largest eigenvalue of the function
C C C derive a bound of rate of convergence, as follows. Theorem 4 (ADF convergence rate)
Assume  X  is the largest eigenvalue of the function C C C t proposed ADF training, its convergence rate is bounded by  X  , and we have where  X  is the minimum eigenvalue of H H H ( w w w  X  ).
 The proof is sketched in Section 5.
 frequency-adaptive learning rates is convergent and the bound of convergence rate is analyzed. It demonstrates that increasing the values of  X   X   X  bound of the convergence rate. Because the bound of the convergence rate is just an up-bound rather than the actual convergence rate, we still need to conduct automatic practice. The ADF training method has a fast convergence rate because the feature-frequency-adaptive schema can avoid the fluctuations on updating the weights of high low frequency features. In the following sections, we perform experiments to confirm the fast convergence rate of the proposed method. 4. Evaluation
Our main focus is on training heavily structured classification models. We evaluate the proposal on three NLP structured classification tasks: biomedical named entity recogni-tion (Bio-NER), Chinese word segmentation, and noun phrase (NP) chunking. For the structured classification tasks, the ADF training is based on the CRF model (Lafferty,
McCallum, and Pereira 2001). Further, to demonstrate that the proposed method is not limited to structured classification tasks, we also perform experiments on a non-structured classification task, the ADF training is based on the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996). 4.1 Biomedical Named Entity Recognition (Structured Classification)
The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004
DNA , RNA , protein , cell line , and cell type , on the MEDLINE biomedical text mining labeling task with the BIO encoding.
 abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data (2009), and Tsuruoka, Tsujii, and Ananiadou (2009).
 et al. 2009), we use word token X  X ased features, part-of-speech (POS) based features,
Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package), the edges features usually contain only the information of y 572 {  X { {  X { {  X { ization of edge features in traditional CRF implementation is to reduce the dimension features just like the implementation of node features. A detailed introduction to rich edge features can be found in Sun, Wang, and Li (2012). Using the feature templates, we extract a high dimensional feature set, which contains 5 . 3
Following prior studies, the evaluation metric for this task is the balanced F-score defined as 2 PR / ( P + R ), where P is precision and R is recall. 4.2 Chinese Word Segmentation (Structured Classification)
Chinese word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word
Segmentation Bakeoff. In this data set, there are 8 . 8  X  for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and and Zhao and Kit (2011).
 and Li 2012). Rich edge features are used. For the classification label y
Li 2012):
The extracted feature set is large, and there are 2 . 4  X  evaluation metric for this task is the balanced F-score. 4.3 Phrase Chunking (Structured Classification)
In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs, are identified. The phrase chunking data is extracted from the data of the CoNLL-2000 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n -grams and part-of-speech n -grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4 . 8 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is the balanced F-score. 4.4 Sentiment Classification (Non-Structured Classification)
To demonstrate that the proposed method is not limited to structured classification, we select a well-known sentiment classification task for evaluating the proposed method on non-structured classification.
 {  X { {  X { 574 ative opinion. This task (Blitzer, Dredze, and Pereira 2007) consists of four subtasks based on user reviews from Amazon.com. Each subtask is a binary sentiment clas-sification task based on a specific topic. We use the maximum entropy model for classification. We use the same lexical features as those used in Blitzer, Dredze, and
Pereira (2007), and the total number of features is 9 . 4 evaluation metric is binary classification accuracy. 4.5 Experimental Setting As for training, we perform gradient descent with the proposed ADF training method.
To compare with existing literature, we choose four popular training methods, a rep-resentative batch training method, and three representative on-line training methods. The batch training method is the limited-memory BFGS (LBFGS) method (Nocedal and like CRFs. The on-line training methods include the SGD training method, which we introduced in Section 2.2, the structured perceptron (Perc) training method (Freund and Schapire 1999; Collins 2002), and the averaged perceptron (Avg-Perc) training method (Collins 2002). The structured perceptron method and averaged perceptron method are non-probabilistic training methods that have very fast training speed due to the avoidance of the computation on gradients (Sun, Matsuzaki, and Li 2013). All training methods, including ADF, SGD, Perc, Avg-Perc, and LBFGS, use the same set of features.

Pereira 2008) and the AROW method (Crammer, Kulesza, and Dredze 2009). The CW and AROW methods are implemented based on the Confidence Weighted Learning
Library. 2 Because the current implementation of the CW and AROW methods do not utilize rich edge features, we removed the rich edge features in our systems to make more fair comparisons. That is, we removed rich edge features in the CRF-ADF setting, and this simplified method is denoted as ADF-noRich. The second-order stochastic gradient descent training methods, including the SMD method (Vishwanathan et al. 2006) and the PSA method (Hsu et al. 2009), are not considered in our experiments because we find those methods are quite slow when running on our data sets with high dimensional features.
 of training samples). It means that feature frequency information is updated 10 times that the following setting is sufficient to produce adequate performance for most of the real-world natural language processing tasks:  X  around 0.995 and  X  around 0.6. many different natural language processing tasks.
 that requires careful tuning. We perform automatic tuning for c based on the training segmentation, phrase chunking, and sentiment classification tasks, respectively. 1999) for the ADF, LBFGS, and SGD training methods. We vary the  X  with different and LBFGS in the phrase chunking task; and  X  = 1 . 0 for all training methods in the sentiment classification task. Experiments are performed on a computer with an Intel(R)
Xeon(R) 2.0-GHz CPU. 4.6 Structured Classification Results 4.6.1 Comparisons Based on Empirical Convergence. First, we check the experimental re-sults of different methods on their empirical convergence state. Because the perceptron training method (Perc) does not achieve empirical convergence even with a very large number of training passes, we simply report its results based on a large enough number of training passes (e.g., 200 passes). Experimental results are shown in Table 4. methods, either the on-line ones or the batch one. It is a bit surprising that the ADF method performs even more accurately than the batch training method (LBFGS). We notice that some previous work also found that on-line training methods could have 576 better performance than batch training methods such as LBFGS (Tsuruoka, Tsujii, and
Ananiadou 2009; Schaul, Zhang, and LeCun 2012). The ADF training method can achieve better results probably because the feature-frequency X  X daptive training schema
SGD training may over-train high frequency features and at the same time may have insufficient training of low frequency features. The ADF training method can avoid such problems. It will be interesting to perform further analysis in future work. 0.05. Significance tests demonstrate that the ADF method is significantly more accurate than the existing training methods in most of the comparisons, whether on-line or batch. For the Bio-NER task, the differences between ADF and LBFGS, SGD, Perc, and Avg-Perc are significant. For the word segmentation task, the differences between
ADF and LBFGS, SGD, Perc, and Avg-Perc are significant. For the phrase chunking between ADF and LBFGS and SGD are non-significant.

ADF method is about one order of magnitude faster than the LBFGS batch training method and several times faster than the existing on-line training methods. 4.6.2 Comparisons with State-of-the-Art Systems. The three tasks are well-known bench-mark tasks with standard data sets. There is a large amount of published research on those three tasks. We compare the proposed method with the state-of-the-art systems. The comparisons are shown in Table 5.
 word segmentation, and NP-chunking tasks. Many of the state-of-the-art systems use extra resources (e.g., linguistic knowledge) or complicated systems (e.g., voting over multiple models). Thus, it is impressive that our single model X  X ased system without extra resources achieves good performance. This indicates that the proposed ADF training method can train model parameters with good generality on the test data. 4.6.3 Training Curves. To study the detailed training process and convergence speed, we show the training curves in Figures 2 X 4. Figure 2 focuses on the comparisons between the ADF method and the existing on-line training methods. As we can see, the ADF method converges faster than other on-line training methods in terms of both training passes and wall-clock time. The ADF method has roughly the same training speed per pass compared with traditional SGD training.
 (Dredze, Crammer, and Pereira 2008) and the AROW method (Crammer, Kulesza, and
Dredze 2009). Comparisons are based on similar features. As discussed before, the ADF-noRich method is a simplified system, with rich edge features removed from the CRF-
ADF system. As we can see, the proposed ADF method, whether with or without rich edge features, outperforms the CW and AROW methods. Figure 3 (Bottom Row) focuses on the comparisons with different mini-batch (the training samples in each stochastic we find larger mini-batch sizes will slow down the convergence speed. Results demon-strate that, compared with the SGD training method, the ADF training method is less sensitive to mini-batch sizes.
 training method LBFGS. As we can see, the ADF method converges at least one order 578 magnitude faster than the LBFGS training in terms of both training passes and wall-clock time. For the LBFGS training, we need to determine the LBFGS memory parameter m , which controls the number of prior gradients used to approximate the Hessian information. A larger value of m will potentially lead to more accurate estimation of the Hessian information, but at the same time will consume significantly more memory. Roughly, the LBFGS training consumes m times more memory than the ADF set m = 10 for the word segmentation and phrase chunking tasks, and m = 6 for the
Bio-NER task due to the shortage of memory for m &gt; 6 cases in this task. data in one pass. For example, some Web-based on-line data streams can only appear once so that the model parameter learning should be finished in one-pass learning (see
Zinkevich et al. 2010). Hence, it is important to test the performance in the one-pass learning scenario.
  X  X n the fly X  during on-line training. As shown in Section 3.1, we only need to have is updated when observing training instances one by one. Then, the learning rate instances again. This is the same algorithm introduced in Section 3.1 and no change is required for the one-pass learning scenario. Figure 5 shows the comparisons between the ADF method and baselines on one-pass learning. As we can see, the ADF method the ADF training method. 4.7 Non-Structured Classification Results
In previous experiments, we showed that the proposed method outperforms existing baselines on structured classification. Nevertheless, we want to show that the ADF method also has good performance on non-structured classification. In addition, this task is based on real-valued features instead of binary features.
 580 shown in Table 6. As we can see, the proposed method outperforms all of the on-line and batch baselines in terms of binary classification accuracy. Here again we observe that the ADF and SGD methods outperform the LBFGS baseline.
 verges quickly. Because this data set is relatively small and the feature dimension is much smaller than previous tasks, we find the baseline training methods also have fast convergence speed. The comparisons on one-pass learning are shown in Fig-ure 7. Just as for the experiments for structured classification tasks, the ADF method outperforms the baseline methods on one-pass learning, with more than 12.7% error rate reduction. 5. Proofs This section gives proofs of Theorems 1 X 4.
 w w w +  X   X   X   X   X   X  g g g t . For  X  w w w t  X  X , where a i and b i are the i th elements of the vector  X   X   X  and g g g
SGD update rule with the fixed learning rate  X  max such that  X   X   X   X  } . In other words, for the SGD update rule F SGD , the fixed learning rate  X  from the ADF update rule. According to Lemma 1, the SGD update rule F contraction mapping in Euclidean space with Lipschitz continuity degree 1 given the condition that  X  max  X  ( || x 2 i || X || X  y  X  y  X 
Thus, according to the definition of dynamic contraction mapping, the ADF update rule is a dynamic contraction mapping in Euclidean space with Lipschitz continuity degree 1  X   X  582 w w w where a i is the i th element of the vector  X   X   X  t + 1 . b
Similar to the analysis of Theorem 1, the third step of Equation (8) is valid because  X  is the maximum learning rate at the beginning and all learning rates are decreasing when t is increasing. The proof can be easily derived following the same steps in the proof of Theorem 1. To avoid redundancy, we do not repeat the derivation.
To prove the convergence of the ADF, we need to prove the sequence M t  X  X  X  . Following Theorem 2, we have the following formula for the ADF training: where  X  max is the maximum learning rate at the beginning. Let d q : = 1  X   X  max / X  2 , then we have:
Thus, M t is upper-bounded. Because we know that M t is a monotonically increasing function when t  X  X  X  , it follows that M t converges when t proof.  X  X  X 
Then, we have
This is because 1  X  a j  X  e  X  a j given 0  X  a j &lt; 1. Finally, because t  X  X  X  , we have
This completes the proof.  X  X  X  6. Conclusions
In this work we tried to simultaneously improve the training speed and model accuracy of natural language processing systems. We proposed the ADF on-line training method, decays faster. We demonstrated that the ADF on-line training method is convergent and has good theoretical properties. Based on empirical experiments, we can state the following conclusions. First, the ADF method achieved the major target of this work: faster training speed and higher accuracy at the same time. Second, the ADF method was robust: It had good performance on several structured and non-structured classifi-cation tasks with very different characteristics. Third, the ADF method worked well on both binary features and real-valued features. Fourth, the ADF method outperformed existing methods in a one-pass learning setting. Finally, our method achieved state-of-the-art performance on several well-known benchmark tasks. To the best of our knowledge, our simple method achieved a much better F-score than the existing best reports on the biomedical named entity recognition task.
 Acknowledgments 584 References
