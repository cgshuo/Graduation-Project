 Giorgos Borboudakis borbudak@ics.forth.gr Ioannis Tsamardinos tsamard@ics.forth.gr Qualitative causal knowledge, such as X causally af-fects Y (denoted as X 99K Y ) or X does not causally affect Y (denoted as X  X  99K Y ) is often available in many domains. It may stem from expert or domain knowledge (the methylation levels of a gene X  X  promoter X causally reduces its expression Y ) or known seman-tic or temporal constraints (e.g., demographic vari-ables do not causally affect gender). Such knowledge may also come from small-sample experiments where a quantity is manipulated: if temperature X is var-ied in a yeast culture, then all (non-)differentially ex-pressed genes are (not) causally affected by tempera-ture. These relations can be identified by simple hy-potheses tests, even if one cannot robustly induce a complete causal model due to a small sample size. In this paper, we devise theory and algorithms for in-corporating a given set K of X 99K Y and X  X  99K Y relations into a causal model. As causal models we consider Bayesian Networks (BNs) and Maximal An-cestral Graphs (MAGs) and their respective Markov equivalence classes Partially Directed Acyclic Graphs (PDAGs) and Partially Oriented Ancestral Graphs (PAGs). Such models can be induced from data by learning algorithms such as the PC and the FCI ( Spirtes et al. , 2000 ). MAGs are a generalization of BNs that admit possible latent confounders. Typically, when learning from observational data, several statis-tically indistinguishable models are consistent with the data forming a Markov Equivalence class.These mod-els share the same edges but may disagree on their orientations. In these models causal facts of the form X 99K Y and X  X  99K Y correspond to the presence and absence of a directed path, respectively.
 First, we characterize the Markov equivalence class of all BNs (MAGs) that belong in the given PDAG (PAG) and at the same time are consistent with K . It turns out that this type of equivalence class cannot be represented with a PDAG (PAG) but a simple exten-sion of these formalisms is required that we name Path-Constrained PAG (PDAG) (PC-PAG, PC-PDAG). A PC-PDAG (PC-PAG) is similar to a PDAG (PAG) with the addition of new types of edges denoting the presence or absence of a directed path. In general, the incorporation of K into a PDAG (PAG) forces the ori-entation of certain edges and results in a correspond-ing PC-PDAG (PC-PAG) with fewer structural uncer-tainties. As a simple example consider that given the PAG X  X   X   X  Y  X   X   X  Z and knowledge K = { X 99K Z } one can infer the PC-PAG X  X  Y  X  Z (which also happens to be a PAG and a MAG in this case). Subsequently, we develop algorithms that given a PDAG (PAG) P and a set of knowledge facts K dis-cover all implied edge orientations and return the cor-responding PC-PDAG (PC-PAG), if P and K are con-sistent. We show that the algorithms are computa-tionally more efficient than brute force algorithms that enumerate all BNs (MAGs) in the equivalence class of P to identify the ones that are also consistent with K . Later on, we extend the algorithms to deal with cases where P and K are inconsistent.
 In simulated experiments with randomly generated networks as well as real networks appearing in the literature, we show that often, even for small | K | , a large number of orientations is made possible. This provides evidence for the utility of identifying and us-ing this type of prior knowledge in causal discovery. We also present a case study where we incorporate causal knowledge induced from real biological data to a known biological network.
 Several other methods that address prior knowl-edge for causal discovery have appeared in the lit-erature. These methods can incorporate knowledge on the parameters of the network ( Niculescu et al. , 2006 ), on the presence or absence of direct relations ( Meek , 1995 ), on a total ordering of the variables ( Cooper &amp; Herskovits , 1992 ), or the complete struc-ture of the network ( Heckerman et al. , 1995 ). Di-rect causal relations in a model (i.e., not mediated by any other modeled variable) correspond to edge in the model; being  X  X irect X  depends on the context (i.e., the modeled variables). In contrast, path-constraints do not depend on the context and are semantically dif-ferent. In the yeast example of the first paragraph, one may deduce that temperature is causally affecting a gene expression, but not necessarily directly: other genes may mediate the effect. In ( O X  X onnell R. T. , 2008 ) a method is presented for incorporating possi-bly indirect relations, but relies on computationally expensive Markov Chain -Monte Carlo (MCMC) sim-ulations. No prior algorithm (see ( Borboudakis et al. , 2011 ) for an early effort) can incorporate causal knowl-edge of possibly indirect relations for MAGs. We briefly review some background preliminaries, as-suming the reader X  X  familiarity with causal model-ing. Maximal Ancestral Graphs (MAGs) are graphical models that represent causal relations among a set of variables, as well as probabilistic properties, such as conditional independencies. A key property of MAGs is that they are able to model latent confounders and selection variables without explicitly introducing them into the model, using bi-directed and un-directed edges respectively. For this paper, we do not consider cases of selection variables (i.e., we actually consider what is called Directed Maximal Ancestral Graphs (DMAGs) that do not have un-directed edges) ( Spirtes et al. , 2000 ; Richardson &amp; Spirtes , 2002 ). We will refer to DMAGs as MAGs for ease of notation.
 MAGs contain two kinds of edges: directed edges (  X  ) and bi-directed edges (  X  ). Each edge has two marks (or orientations), tails (-) and/or arrowheads ( &gt; ). A wildcard mark (  X  ) can be a tail or an arrowhead. Edge A  X  X  X  B is into B , and edge A  X  B is out of A . A path in a MAG M is a sequence of distinct vertices  X 
V 0 ; V 1 ; : : : ; V n adjacent in M . A path is directed if for 0  X  i &lt; n , V  X  V B a descendant of A if A = B or there is a directed path from A to B in M . A directed cycle occurs in M if B  X  A and A is an ancestor of B . An almost directed cycle occurs in M if B  X  A and A is an ancestor of B . A triple  X  X; Y; Z  X  is said to form a collider if X and Z are into Y .
 MAGs, by definition, do not contain any directed or al-most directed cycles. As a consequence, an arrowhead denotes non-ancestry, whereas a tail denotes ancestry. Specifically, a directed edge A  X  B denotes that A is a causal parent of B , whereas a bi-directed edge A  X  B denotes that neither of the two variables is a causal ancestor of each other; in addition, when faithfulness holds (defined below) the bi-directed edge denotes A and B share a latent common cause (confounder). Next a graphical criterion called m -separation is de-fined, which connects the graph with properties of the joint distribution of the data.
 Definition 2.1 (m-separation) . In a MAG, a path p between vertices A and B is m -connecting relative to (condition to) a set of vertices Z , ( A , B  X  X  Z ) if: (i) every non-collider on p is not a member of Z , (ii) every collider on p is an ancestor of some member of Z . A and B are said to be m -separated by Z if there is no m -connecting path between them relative to Z . We assume that the Markov Condition and the Faith-fulness Condition hold for MAGs, i.e., A and B are m-separated by Z if and only if A and B are indepen-dent given Z . So, one can graphically determine which independencies hold in the data distribution. In addi-tion, it is required by definition of MAGs that for every missing edge between A and B there exists a subset of the variables Z s.t. A and B are m-separated by Z . It may be the case that two or more different MAGs share the same m -separations. Those MAGs are said to be Markov equivalent .
 Definition 2.2 (Markov Equivalence) . Two MAGs M 1 ; M 2 , with the same set of vertices, are Markov where A and B are not empty, A and B are m -separated by Z in M 1 if and only if they are m -separated by Z in M 2 .
 All Markov equivalent MAGs form a Markov equiv-alence class. A Partially Oriented Ancestral Graph (PAG) represents such a Markov equivalence class. PAGs contain three kinds of marks: arrowheads ( &gt; ), tails (-) and circles (  X  ). It has the same adjacencies as any member of the equivalence class, and every non-circle mark is invariant in any member of the equiva-lence class. Arrowheads and tails have the same se-mantics as in MAGs. Circles denote uncertainties ; both orientations appear in some MAGs of the equiv-alence class. A path is possibly directed if there is an orientation of the uncertainties of the PAG which cre-ates a directed path. A triple  X  X; Y; Z  X  forms a defi-nite non-collider if X and Z are not adjacent and X; Z are not both into Y . FCI ( Spirtes et al. , 2000 ) is an asymptotically correct algorithm for learning a PAG. Bayesian Networks (BNs) are special cases of MAGs with no bi-directed edges; thus, latent causes of two or more modeled variables (confounders) cannot be rep-resented so that Faithfulness holds in a way that is also consistent with the causal semantics of the edges. The Markov equivalence class of BNs is called Partially Di-rected Acyclic Graphs (PDAG; some authors use the term essential graph instead and PDAG for a different type of graph). In the rest of the paper, we develop the theory and algorithms for the general case (PAGs) and discuss specializations for PDAGs. We assume that a PAG P defined over variables V is given representing a Markov equivalence class of MAGs faithful to some distribution over V . P may contain structural uncertainties about the direction of some edge-points. P could be induced from data by a learning algorithm such as FCI, or be otherwise known and fixed. In addition, we are given a set of prior knowledge constraints K of the form X 99K Y (we call these positive constraints ) or X  X  99K Y ( negative constraints ), where X; Y  X  V . Thus, knowledge con-straints must concern modeled variables . A constraint X 99K Y ( X  X  99K Y ) implies that X is (is not) a causal ancestor of Y , i.e., there must (must not) be a directed path X  X   X  X  X   X  Y in P . Conversely, given that the network is faithful to some distribution, if a path is present (absent) then X is causing (not causing) Y . Thus, each piece of knowledge in K corresponds to a path constraint about the presence or absence of a di-rected path in P . Finally, we assume that P and K are consistent, i.e., the path-constraints induced by the latter can be satisfied by at least one MAG in P (later we develop an algorithm removing this assumption). As an example, assume that we are given PAG X  X   X   X 
Y  X   X   X  Z . Incorporating knowledge K = { X 99K Z } one can infer that X  X  Y  X  Z . Instead, if K = { X  X  99K Z } then one can infer X  X  X  X  Y  X   X   X  Z . Notice that in this setting knowledge is qualitative (the strength of the causal effect is not represented) and both P and K are assumed correct (their uncertainty is not represented). Since P is assumed correct and it encodes all conditional dependencies and indepen-dencies in the joint distribution: (a) there is no point representing and including in K knowledge about the data dependencies and independencies; they are either already represented in P or are inconsistent with it. (b) neither positive nor negative constraints add to our knowledge about the independencies in P and thus they cannot affect the skeleton of P , and only reduce our uncertainty about edge marks .
 We now consider how to represent the set of MAGs that are both Markov equivalent with PAG P and satisfy prior knowledge K . We call this set Path-Constrained PAG : PC-PAG ( P , K ). It is easy to check that a PC-PAG is an equivalence class. As we show next, a PC-PAG cannot be represented by a PAG, thus we need to define a new graphical object to represent the class .
 A simple example is now described. Consider the PAG P and K = { X 99K Y } in Figure 1 (a). Any MAG in PC-PAG( P , K ) must have the orientations V  X  Y  X  W : if Y  X   X  V , then V  X  X because  X 
X; V; Y  X  is a definite non-collider. Since X 99K Y the only remaining option is to orient X  X  W  X  Y , in which case either a cycle or an almost directed cycle is created. Thus, Y  X  X  X  V does not hold, i.e., V  X  Y . Symmetrically, we get W  X  Y . Figure 1 (b) (ignoring the dash edge momentarily) shows a PAG P  X  consis-tent with P and the new orientations. P  X  admits 19 possible MAGs, out of which only 9 are also consistent with K . It is impossible to further orient any edge in a way that the PAG admits these 9 MAGs and only those. Thus, PAGs are not closed under the addition of path-constraints . We now present a type of graph that can represent a PC-PAG equivalence class; as for PAGs we overload the term PC-PAG to indicate both the class and its graphical representation. The new type of graph contains additional edges to represent the path constraints.
 Definition 3.1. A Path-Constrained PAG of PAG P and knowledge K , PC-PAG( P , K ) is a graph with two types of edges: solid and dashed edges, s.t., (i) the skeleton of the solid edges is the same as P , (ii) each solid edge-mark is either tail (-) or arrowhead ( &gt; ) if the corresponding feature is invariant in all MAGs in P also consistent with K , and  X  otherwise, and (iii) an indirect edge X 99K Y ( Y  X  99K X ) is present if X 99K Y ( X  X  99K Y ) is in K but there is not a directed path (there is a possible directed path) from X to Y using solid edges only.
 The vertices and the solid edges form a PAG that we call the underlying PAG . We similarly define Path-Constrained PDAG and its corresponding underlying PDAG, when P is a PDAG instead of a PAG.
 Figure 1 (b) is the PC-PAG of the PAG and knowl-edge in Figure 1 (a). The independencies shared by all member MAGs in a PC-PAG can be read off the un-derlying PAG. The dashed edges also denote ancestral relations so the graph remains ancestral. There can be at most one edge between a pair of vertices. For a given P and K the corresponding PC-PAG is unique. We now define the following problem: Problem 1. Given a PAG P over variables V and a set of causal prior knowledge K = {K} M i =1 , where each K i is of the form A induce the PC-PAG( P , K ). We now develop algorithms that identify the PC-PAG or PC-PDAG given a baseline P and knowledge K . We will be referring to the general case of PC-PAGs, unless we need to explicitly differentiate. By definition, the Algorithm 1 Find-PC-PAG( P , K ) 1: Input: PAG P ; set of causal prior knowledge K 2: Output: boolean sat; PC-PAG C 4: global Found 5: for each un-oriented mark X  X  X  X  X  Y in P do 6: Found( X; Y; &gt; ) = f alse 7: Found( X; Y;  X  ) = f alse 8: end for 9: sat = Search( P; K ) 10: if  X  sat then return  X  sat;  X  X  X  end if 11: C = P 12: for each un-oriented mark X  X  X  X  X  Y in P do 13: if Found( X; Y; &gt; )  X  X  Found( X; Y;  X  ) then 14: Orient( C; X; Y; &gt; ) 15: else if  X  Found( X; Y; &gt; )  X  Found( X; Y;  X  ) then 16: Orient( C; X; Y;  X  ) 17: end if 18: end for 19: for each non-satisfied K i  X  K in P do 20: if K i is of type X 99K Y then 21: Add edge X 99K Y to C 22: else 23: Add edge X L99  X  Y to C 24: end if 25: end for 26: return  X  sat; C  X  Algorithm 2 Search( P , K ) 1: Input: PAG P ; set of causal prior knowledge K 2: Output: boolean sat 4: if  X  V alid ( P; K ) then return False end if 5: if PruneRule(P) then return True end if 6:  X  X; Y  X  = any X  X   X   X  Y in P 7: if there is no such edge  X  X; Y  X  then 8: UpdateFound( P ) 9: return True 10: end if 11: P 1 = ApplyOrientation( P; X; Y; &gt; ) 12: sat 1 = Search ( P 1 ; K ) 13: P 2 = ApplyOrientation( P; X; Y;  X  ) 14: sat 2 = Search ( P 2 ; K ) 15: return sat 1  X  sat 2 skeleton of the solid edges of PC-PAG is the same as the one in P ; once the edge marks of the solid edges are determined, the indirect edges are trivially determined by K . Thus, the main objective should be to determine the edge marks of the solid edges, i.e., the orientations shared by all MAGs in P also consistent with K . Algorithm 1 starts from a given PAG P and keeps adding orientations until it is converted to a MAG. It does so recursively so as to explicitly or implicitly enumerate all consistent MAGs and identify the invari-ant edge marks. The data structure Found( X; Y; m ) stores a flag indicating whether a MAG has been found where the right end-point of edge X  X  Y is marked as m . The procedure Search performs the actual search and computes the values of the identified edge-marks in Found. If all MAGs identified agree on a given edge-mark m of edge X  X  Y , m is transferred to the output graph C by calling procedure Orient( C ; X; Y; m ). Once the edge-marks of C are determined the algorithm in-serts the dashed edges by applying the PC-PAG defi-nition. The algorithm is sound and complete provided the search procedure identifies all edge-marks that be-long in at least one consistent MAG.
 We now focus on the search procedure. The search strategy is essential for the computational efficiency of the algorithm. We actually present 4 differ-ent search procedures, that are all encoded in the same pseudo-code of Algorithm 2 due to space limitations. The algorithms perform search with and without pruning. In addition, sub-procedures of Algorithm 2 may be specialized for PDAGs or PAGs. Hence, there are 4 different specialization of the pseudo-code for each of the above combinations. Algo-rithm 2 accepts parameters P and K and returns the corresponding PC-PAG, or a flag indicating no consis-tent MAG was found.
 Search-No-Pruning . This version of Algorithm 2 does not include line 5. For each  X  mark in an edge X  X   X   X  Y , the edge may be oriented as X  X  Y or X  X  X  X  Y in a MAG of the class. The procedure per-forms a chronological backtracking search with forward checking ( Dechter , 2003 ), i.e., it recursively calls itself for each possible way to place an edge mark (Lines 12 and 14), while propagating these decisions to eliminate inconsistent choices. Thus, in the worst case the algo-rithm calls itself at most 2 # u , where # u is the number of uncertainties (  X  marks in the input graph P ). The procedure stops in two cases: (i) an orientation has been made that leads to an in-valid MAG. Procedure Valid ( P  X  , K ) determines va-lidity by checking the following three conditions: (1)There are no directed cycles nor almost directed cycles (the latter is only checked for PC-PAGs). (2)No prior knowledge constraint is violated. (3)The set of m -separations justifying each missing edge in P remains the same in P  X  . This set of m -separations can be stored by FCI or similar algorithms as P is induced; otherwise it can be found from P in a preprocessing step. The reason one needs to check this condition is because orientations imposed during search may change the set of discriminating paths ( Spirtes et al. , 2000 ), and thus the independence model of P  X  may be different (in our implementation this conditions is only checked if there are no more uncertainties in the graph). This condition is checked only for PC-PAGs and not for PC-PDAGs (as we will see later in the ApplyOrientation procedure). (ii) there are no more uncertainties in the graph and we have found a MAG P  X  (line 7); if it is valid, then procedure UpdateFound ( P  X  ) sets Found( X; Y;  X  ) = T rue or Found( X; Y; &gt; ) = T rue , if the edge X  X  Y or X  X  X  X  Y is present in P  X  , respectively.
 Forward Checking . During search, an orientation of an edge mark may imply other orientations. For ex-ample, if vertices A; B; C form a definite non-collider triple and A is oriented into B , the edge between B and C has to be oriented out of B and into C . This implicitly prunes the search tree, similar to unit propagation in SAT solving algorithms. Procedure ApplyOrientation ( P , X , Y , m ) applies mark m (ei-ther  X  or &gt; ) to edge X  X   X   X  Y and propagates the ori-entation. For the case of PC-PDAGs one can sim-ply apply Meek X  X  rules ( Meek , 1995 ) until conver-gence to find all implied orientations . Unfortunately, for the case of PC-PAGs there is no known com-plete procedure (Zhang notes this as an open prob-lem ( Zhang &amp; Spirtes , 2005 ), p. 81). In this case, we use rules R 1-R 3 of FCI ( Zhang &amp; Spirtes , 2005 ) to do some, but not all of possible propagations. As a result it is possible to generate a MAG that does not belong to the Markov equivalence class represented by the ini-tial PAG. Thus, Condition 3 in procedure Valid above is necessary to check. Application of the propagation rules takes polynomial time.
 Search-with-Pruning . We now present a condi-tion that allows early stopping of the search with-out sacrificing completeness and significantly im-proves the efficiency. For a call of Search( P  X  , K ), let us call with A (ssigned) the set of as-signed orientations so far in the search path, i.e., A = { X  X; Y; m  X  s.t. the end-point at Y is oriented } ; let U (nassinged) be the set of orientations remaining, i.e., U = { X  X; Y  X  s.t. X  X   X   X  Y  X  X   X  } . Then note that, Rule 1 (Prune Rule) . If for each mark in A , a MAG has already been found, and for each unassigned mark in U a MAG has been found for all possible orienta-tions , there is no need to proceed with search and the procedure returns True . No matter what the orienta-tions in U we end up with, our knowledge of possible orientations will not increase. This check is performed by procedure PruneRule .
 When pruning, selecting to recurse on the marks for which a consistent MAG has not been found yet may lead to earlier pruning. Thus, as a heuristic in line 6, we give preference to edges with such marks. In section 6 we present results showing that pruning leads to an exponential speed up of the algorithm. Algorithm 1 returns False if the given PAG P and prior knowledge K are inconsistent. Ideally in this case, one should express the uncertainty in both and infer new orientations in a probabilistic yet efficient way. Such a procedure however, is still eluding us. As an approximation, we now present Algorithm 3 that identifies a subset K  X   X  K that is consistent with P and maximizes a score function denoting preferences on the prior knowledge.
 For each piece of knowledge K i  X  K we denote with u the utility of satisfying it in a MAG, and c i the cost (penalty) of not satisfying it. We can then define the score function of satisfying knowledge K  X   X  K : By setting all u i to 1 and c i to 0, the algorithm will find the largest subset of consistent prior knowledge constraints. With the given setup one can also handle cases where each prior knowledge constraint has a prior belief p i assigned to it. Specifically, if u i = log( p i c = log(1  X  p i ) and one assumes these probabilities are independent, then the Sc ( K  X  ; K ) corresponds to the prior probability P ( K  X  ). In general of course, these probabilities will not be independent since P ( X 99K Y 99K Z ) = 1 =  X  P ( X 99K Z ) = 1.
 Algorithm 3 is a branch-and-bound algorithm that does not branch if the current search path cannot pos-sibly lead to a MAG with higher score than what has already been found. Given a PAG P  X  in the current search node, let K s be the set of currently satisfied prior knowledge constraints, K v the set of violated constraints and K r the set of all remaining constraints in K . An upper bound on the best score to find under this search path can then be computed as follows: Algorithm 3 SearchBnB( P , K ) Input: PAG P ; set of causal prior knowledge K
Output: score maximizer K  X   X  K stored globally global S  X  , maximum score found, initialized to  X  X  X  global K  X  , score maximizer, initialized to  X  if  X  Valid( P; K )  X  MaxPosScore( P; K )  X  S  X  then end if  X  X; Y  X  = any X  X   X   X  Y in P if there is no such edge  X  X; Y  X  then end if P 1 =ApplyOrientation( P; X; Y; &gt; ) SearchBnB( P 1 ; K ) P 2 =ApplyOrientation( P; X; Y;  X  ) SearchBnB( P 2 ; K ) Procedures Score and MaxPosScore compute score Sc and upper bound ScBound given the PAG P in the current search node and knowledge K . Once a leaf of the search has been reached, i.e., all orientations are determined and we have reached a MAG P  X  in the current node, procedure FindSatisfied is called to identify the subset of K that is satisfied in P  X  . Speed-Up of Pruning Rule . We evaluated the per-formance gain of our method when the pruning is on. We randomly generated Bayesian Networks for varying numbers of vertices and edges. For PAGs 20% of the vertices were randomly picked to be hidden. The net-works were then converted to their corresponding PAG or PDAG. The numbers of vertices were { 5,10,15 } for PAGs and { 50,100,150 } for PDAGs. Smaller net-works were generated for PAGs than PDAGs because: (a) PAGs usually have many more uncertainties than PDAGs for the same size and settings of the genera-tion process and (b), the algorithm for PAGs is slower in general than for PDAGs given the same number of uncertainties, because orientations lead to more prop-agations in PDAGs than PAGs. For PAGs we set an upper limit of 50 on the number of uncertainties to avoid computationally intractable problems. The edge density varied between 10-90%, with steps of 1 (a to-tal of 81 different densities). The number of prior knowledge constraints were { 1,2,3,5,7,10 } for PAGs and { 1,2,3,5,7,10,15,20 } for PDAGs. The constraints were sampled from the set of unknown and consistent pairwise causal relations of the given model (i.e., they were not already satisfied). Figure 2 shows the mean number of invocations of the algorithm (search nodes) vs the number of uncertainties of each PAG. The y-axis is logarithmically scaled. The effective branch-ing factor b when pruning is on is smaller than when it is not used, leading to exponential computational savings. We also compared our algorithm with a one-sample t-test (using the difference of invocations of the methods), and obtained p-values of 9  X  10  X  3 and 8  X  10  X  for the PDAG and PAG methods respectively, show-ing that using pruning offers statistically significant improvements.
 Evaluation of Inference Capabilities . We eval-uated the ability of our methods to infer new ori-entations. We randomly generated networks as be-fore. The number of vertices was { 10,15,20,25,30 } and { 50,100,150,200,300,500 } , whereas the number of prior knowledge facts was { 1,2,3,5,7,10,15,20,25 } and { 1,2,3,5,7,10,15,20,30,50 } for PAGs and PDAGs re-spectively. We also ran experiments for 3 real networks which are commonly used in the literature (Alarm, Hailfinder and Child). For the real networks, the number of prior knowledge constraints were within { 1,2,3,5,7,10,15,20,30,50 } , both for PDAGs and PAGs. The selection of prior knowledge was repeated 100 times. That is a total of 3  X  10  X  100 = 3000 runs, for each type of model (PDAGs and PAGs). We mea-sure the ability to make novel inferences, called Infer-the number of mark orientations inferred by incorpo-rating the knowledge and #uncertainties the number of  X  marks in the input PAG or PDAG. Figures 3 (a) and 3 (b) show the mean inference rate for PAGs and PDAGs respectively, as the number of prior knowledge constraints increases. In general, (a) inference rate is significant (more than 30%) even for a small number of constraints (e.g., 10) and (b) inference rate is higher in PDAGs than PAGs everything else being equal. A Case-Study with Real Data . We obtained two flow-cytometry datasets of ( K. Sachs , 2005 ). Both datasets measure a set of 11 protein concentrations on the same biological pathway under different experi-mental conditions. The first dataset contains 707 sam-ples and the (indirectly) manipulated variable is PKA, whereas the second contains 913 samples and the (in-directly) manipulated variable is PKC. Causal prior knowledge is inferred as follows: for manipulated pro-tein M , we infer the constraint M 99K X for every X with a p-value of a Spearman correlation less than 0.01, and M  X  99K X , when the p-value is greater than 0.5 This leads to 11 causal constraints as knowledge K . We also consider a portion of the biological path-way as our gold standard MAG (Figure 4 (a)) that we convert to its corresponding PAG P (Figure 4 (b)). K and P are inconsistent in this case because two con-straints in K are due to statistical errors. To select a consistent set of prior knowledge, we ran Algorithm 3 with weights u i = log(1  X  p i ) and c i = log( p i ), where p is the p-value of a positive constraint, or 1-p-value for a negative constraint. The algorithm selected a consistent subset K  X  of size 6 that was then incorpo-rated into P via Algorithm 1 . K  X  still included one statistical error. The resulting PC-PAG is shown in Figure 4 (c). It orients 11 out of 22 initial edge mark uncertainties, out of which one is erroneous due to the false negative constraint in K  X  with a p-value of 0.86. We present algorithms for incorporating path-constraints to PDAGs and PAGs corresponding to known, possibly indirect, causal relations. The algo-rithms use chronological backtracking search, with for-ward checking and a pruning rule stemming from the semantics of the graphs. A branch-and-bound varia-tion of the algorithms for dealing with knowledge in-consistent with the given PDAG or PAG is also pre-sented. Our experimental results show that typically even a few causal constraints can orient a significant number of edges. In a case study we show how experi-mental studies (where some variables are manipulated) can be used to infer such causal constraints and be in-corporated into an incomplete (i.e. with structural un-certainties) causal model. The algorithms could form a basis for extensions that take into consideration de-grees of belief on each constrain or network feature. We thank the reviewers for their helpful comments. This work was partially funded by the REACTION GA 248590 EU project.

