 Consider a set of input vectors x 1 , . . . , x n  X  R d , with corresponding desired output variables y , . . . , y n . The task of supervised learning is to estimate the functional relationship y  X  f ( x ) The quality of prediction is often measured through a loss function  X  ( f ( x ) , y ) . We assume that  X  ( f, y ) is convex in f throughout the paper. In this paper, we consider linear prediction model f ( x ) = w T x . As in boosting or kernel methods, nonlinearity can be introduced by including non-linear features in x .
 We are mainly interested in the scenario that d n . That is, there are many more features than the number of samples. In this case, an unconstrained empirical risk minimization is inadequate because the solution overfits the data. The standard remedy for this problem is to impose a constraint on w to obtain a regularized problem. An important target constraint is sparsity , which corresponds to the (non-convex) L 0 regularization, defined as k w k 0 = |{ j : w j 6 = 0 }| = k . If we know the sparsity parameter k for the target vector, then a good learning method is L 0 regularization: If k is not known, then one may regard k as a tuning parameter, which can be selected through cross-validation. This method is often referred to as subset selection in the literature. Sparse learning is an essential topic in machine learning, which has attracted considerable interests recently. It can be shown that the solution of the L 0 regularization problem in (1) achieves good prediction accuracy if the target function can be approximated by a sparse  X  w . However, a fundamental difficulty with this method is the computational cost, because the number of subsets of { 1 , . . . , d } of cardinality k (corresponding to the nonzero components of w ) is exponential in k .
 Due to the computational difficult, in practice, it is necessary to replace (1) by some easier to solve formulations below: where  X  &gt; 0 is an appropriately chosen regularization condition. We obtain a formulation equiv-alent to (2) by choosing the regularization function as g ( w ) = k w k 0 . However, this function is discontinuous. For computational reasons, it is helpful to consider a continuous approximation with ing the closest approximation with p = 1 , one obtain Lasso , which is the standard convex relaxation formulation for sparse learning. With p  X  (0 , 1) , the L p regularization k w k p is non-convex but con-tinuous. In this paper, we are also interested in the following capped-L 1 approximation of k w k 0 , with g ( w ) = P d j =1 min( | w j | ,  X  ) , where for v  X  R : This is a good approximation to L 0 because as  X   X  0 , P j min( | w j | ,  X  ) / X   X  k w k 0 . Therefore when  X   X  0 , this regularization condition is equivalent to the sparse L 0 regularization upto a rescaling of  X  . Note that the capped-L 1 regulariza-tion is also non-convex. It is related to the so-called SCAD regularization in statistics, which is a smoother version. We use the simpler capped-L 1 regularization because the extra smoothness does not affect our algorithm or theory.
 For a non-convex but smooth regularization condition such as capped-L 1 or L p with p  X  (0 , 1) , standard numerical techniques such as gradient descent leads to a local minimum solution. Unfor-tunately, it is difficult to find the global optimum, and it is also difficult to analyze the quality of the local minimum. Although in practice, such a local minimum solution may outperform the Lasso so-lution, the lack of theoretical (and practical) performance guarantee prevents the more wide-spread applications of such algorithms. As a matter of fact, results with non-convex regularization are dif-ficult to reproduce because different numerical optimization procedures can lead to different local minima. Therefore the quality of the solution heavily depend on the numerical procedure used. The situation is very difficult for a convex relaxation formulation such as L 1 -regularization (Lasso). The global optimum can be easily computed using standard convex programming techniques. It is known that in practice, 1-norm regularization often leads to sparse solutions (although often sub-optimal). Moreover, its performance has been theoretically analyzed recently. For example, it is known from the compressed sensing literature that under certain conditions, the solution of L 1 re-laxation may be equivalent to L 0 regularization asymptotically even when noise is present (e.g. [3] and references therein). If the target is truly sparse, then it was shown in [9] that under some restric-tive conditions referred to as irrepresentable conditions , 1-norm regularization solves the feature selection problem. The prediction performance of this method has been considered in [4, 8, 1]. Despite of its success, L 1 -regularization often leads to suboptimal solutions because it is not a good approximation to L 0 regularization. Statistically, this means that even though it converges to the true sparse target when n  X   X  (consistency), the rate of convergence can be suboptimal. The only way to fix this problem is to employ a non-convex regularization condition that is closer to L 0 regularization, such as the capped-L 1 regularization. The superiority of capped-L 1 is formally proved later in this paper.
 Because of the above gap between practice and theory, it is important to study direct solutions of non-convex regularization beyond the standard L 1 relaxation. Our goal is to design a numerical pro-cedure that leads to a reproducible solution with better theoretical behavior than L 1 -regularization. This paper shows how this can be done. Specifically, we consider a general multi-stage convex re-laxation method for solving learning formulations with non-convex regularization. In this scheme, concave duality is used to construct a sequence of convex relaxations that give better and better approximations to the original non-convex problem. Moreover, using the capped-L 1 regularization, we show that after only two stages, the solution gives better statistical performance than standard Lasso when the target is approximately sparse. In essence, this paper establishes a performance guarantee for non-convex formulations using a multi-stage convex relaxation approach that is more sophisticated than the standard one-stage convex relaxation (which is the standard approach com-monly studied in the current literature). Experiments confirm the effectiveness of the multi-stage approach. Given a continuous regularization function g ( w ) in (2) which may be non-convex, we are interested in rewriting it using concave duality. Let h ( w ) : R d  X   X   X  R d be a map with range  X  . It may not be a one-to-one map. However, we assume that there exists a function  X  g h ( u ) defined on  X  such that g ( w ) =  X  g h ( h ( w )) holds.
 We assume that we can find h so that the function  X  g h ( u ) is a concave function of u on  X  . Under this assumption, we can rewrite the regularization function g ( w ) as: using concave duality [6]. In this case, g  X  h ( v ) is the concave dual of  X  g h ( u ) given below Moreover, it is well-known that the minimum of the right hand side of (3) is achieved at This is a very general framework. For illustration, we include two example non-convex sparse regularization conditions discussed in the introduction.
 L p regularization We consider the regularization condition g ( w ) = P In this case,  X  g h ( u ) = P d j =1 u p/q j on  X  = { u : u j  X  0 } . The solution in (4) is given by  X  v j = ( p/q ) | w j | p  X  q .
 Capped-L 1 regularization We consider the regularization condition g ( w ) = P d j =1 min( | w j | ,  X  ) . defined on the domain { v : v j  X  0 } , where I (  X  ) is the set indicator function. The solution in (4) is given by  X  v j = I ( | w j | X   X  ) . We consider a general procedure for solving (2) with convex loss and non-convex regularization g ( w ) . Let h ( w ) = P j h j ( w ) be a convex relaxation of g ( w ) that dominates g ( w ) (for example, it can be the smallest convex upperbound (i.e., the inf over all convex upperbounds) of g ( w ) ). A simple convex relaxation of (2) becomes This simple relaxation can yield a solution that is not close to the solution of (2). However, if h satisfies the condition of Section 2, then it is possible to write g ( w ) as (3). Now, with this new representation, we can rewrite (2) as This is clearly equivalent to (2) because of (3). If we can find a good approximation of  X  v that improves upon the initial value of  X  v = [1 , . . . , 1] , then the above formulation can lead to a refined convex problem in w that is a better convex relaxation than (5). Our numerical procedure exploits the above fact, which tries to improve the estimation of v j over the initial choice of v j = 1 in (5) using an iterative algorithm. This can be done using an alternating optimization procedure, which repeatedly applies the following two steps: The general procedure is presented in Figure 1. It can be regarded as a generalization of CCCP (concave-convex programming) [7], which takes h ( w ) = w . By repeatedly refining the parameter v , we can potentially obtain better and better convex relaxation, leading to a solution superior to that of the initial convex relaxation. Note that using the L p and capped-L 1 regularization conditions in Section 2, this procedure lead to more specific multi-stage convex relaxation algorithms. We skip the details due to the space limitation.
 Tuning parameters:  X  Input: training data ( x 1 , y 1 ) , . . . , ( x n , y n )
Output: weight vector  X  w initialize  X  v j = 1
Repeat the following two steps until convergence: Although the reasoning in Section 3 is appealing, it is only a heuristic argument without any formal theoretical guarantee. In contrast, the simple one-stage L 1 relaxation is known to perform reasonably well under certain assumptions. Therefore unless we can develop a theory to show the effectiveness of the multi-stage procedure in Figure 1, our proposal is mere yet another local minimum finding scheme that may potentially stuck into a bad local solution.
 This section tries to address this issue. Although we have not yet developed a complete theory for the general procedure, we are able to obtain a learning bound for the capped-L 1 regularization. In particular, if the target function is sparse, then the performance of the solution after merely two-stages of our procedure is superior to that of Lasso. This demonstrates the effectiveness of the multi-stage approach. Since the analysis is rather complicated, we focus on the least squares loss only, and only for the solution after two-stages of the algorithm.
 For a complete theory, the following questions are worth asking: The first question answers whether it is beneficial to use a non-convex penalty function. The second question answers whether we can effectively solve the resulting non-convex problem using multi-stage convex relaxation. The combination of the two questions leads to a satisfactory theoretical answer to the effectiveness of the multi-stage procedure.
 A general theory along this line will be developed in the full paper. In the following, instead of trying to answer the above questions separately, we provide a unified finite sample analysis for the procedure that directly addresses the combined effect of the two questions. The result is adopted from [8], which justifies the multi-stage convex relaxation approach by showing that the two-stage procedure using capped-L 1 regularization can lead to better generalization than the standard one stage L 1 regularization.
 The procedure we shall analyze, which is a special case of the multi-stage algorithm in Figure 1 with capped-L 1 regularization and only two stages, is described in Figure 2. It is related to the adaptive Lasso method [10]. The result is reproducible when the solution of the first stage is unique because it involves two well-defined convex programming problems. Note that it is described with least squares loss only because our analysis assumes least squares loss: a more general analysis for other loss functions is possible but would lead to extra complications that are not central to our interests. Tuning parameters:  X ,  X  Input: training data ( x 1 , y 1 ) , . . . , ( x n , y n ) Output: weight vector  X  w 0
Stage 1: Compute  X  w by solving the L 1 penalization problem:
Stage 2: Solving the following selective L 1 penalization problem: This particular two-stage procedure also has an intuitive interpretation (besides treating it as a spe-cial case of multi-stage convex relaxation). We shall refer to the feature components corresponding to the large weights as relevant features , and the feature components smaller the cut-off threshold  X  as irrelevant features . We observe that as an estimation method, L 1 regularization has two impor-tant properties: shrink estimated weights corresponding to irrelevant features toward zero; shrink estimated weights corresponding to relevant features toward zero. While the first effect is desirable, the second effect is not. In fact, we should avoid shrinking the weights corresponding to the relevant features if we can identify these features. This is why the standard L 1 regularization may have sub-optimal performance. However, after the first stage of L 1 regularization, we can identify the relevant features by picking the components corresponding to the largest weights; in the second stage of L 1 regularization, we do not have to penalize the features selected in the first stage, as in Figure 2. A related method, called relaxed Lasso , was proposed recently by Meinshausen [5], which is similar to a two-stage Dantzig selector in [2]. Their idea differs from our proposal in that in the second stage, the weight coefficients w 0 j are forced to be zero when j /  X  supp 0 (  X  w ) . It was pointed out in [5] that if supp 0 (  X  w ) can exactly identify all non-zero components of the target vector, then in the second stage, the relaxed Lasso can asymptotically remove the bias in the first stage Lasso. However, it is not clear what theoretical result can be stated when Lasso cannot exactly identify all relevant features. In the general case, it is not easy to ensure that relaxed Lasso does not degrade the performance when some relevant coefficients become zero in the first stage. On the contrary, the two-stage penalization procedure in Figure 2, which is based on the capped-L 1 regularization, does not require that all relevant features are identified. Consequently, we are able to prove a result for Figure 2 with no counterpart for relaxed Lasso.
 Definition 4.1 Let w = [ w 1 , . . . , w d ]  X  R d and  X   X  0 , we define the set of relevant features with threshold  X  as: Moreover, if | w i 1 | X  X  X  X  X  X | w i d | are in descending order, then define  X  k ( w ) = P j&gt;k | w i j | 2 as the 2 -norm of the largest k components (in absolute value) of w .
 For simplicity, we assume sub-Gaussian noise as follows. Assumption 4.1 Assume that { y i } i =1 ,...,n are independent (but not necessarily identically dis-tributed) sub-Gaussians: there exists  X   X  0 such that  X  i and  X  t  X  R , Both Gaussian and bounded random variables are sub-Gaussian using the above definition. For Gaussian:  X   X  N (0 ,  X  2 ) , then E  X  e t X   X  e  X  2 t 2 / 2 .
 Consider the two-stage method in Figure 2. Given  X   X  (0 , 0 . 5) , with probability larger than 1  X  2  X  : if  X / 48  X   X   X  12  X  p 2 ln(2 d/ X  ) /n , then where q = | supp 1 . 5  X  (  X  w ) | .
 The proof of this theorem can be found in [8]. Note that the theorem allows the situation d n , which is what we are interested in. The condition M  X  A s  X  1 / 6 , often referred to as mutual coherence , is also quite standard in the analysis of L 1 regularization, e.g., in [1, 3]. Although the condition is idealized, the theorem nevertheless yields important insights into the behavior of the two-stage algorithm. This theorem leads to a bound for Lasso with  X  =  X  or q = 0 . The bound has the form This bound is tight for Lasso, in the sense that the right hand side cannot be improved except for the constant. In particular, the factor O ( verified with an orthogonal design matrix. It is known that in order for Lasso to be effective, one has to pick  X  no smaller than the order  X  p ln d/n . Therefore, the generalization of standard Lasso is of the order  X  k (  X  w ) +  X  p k ln d/n , which cannot be improved. Similar results appear in [1, 4]. Now, with a small  X  , the bound in Theorem 4.1 can be significantly better than that of the standard Lasso result if the sparse target satisfies  X  k (  X  w ) of  X  w in supp  X  (  X  w ) are relatively large in magnitude and the rest is small in 2-norm. That is, when the target  X  w can be decompose as a sparse vector with large coefficients plus another (less sparse) vector with small coefficients. In the extreme case when q = k = | supp 0 (  X  w ) | (that is, all nonzero which is superior to the standard one-stage Lasso bound k  X  w  X   X  w k 2 = O ( p k ln( d/ X  ) /n ) . Again, this bound cannot be improved for Lasso, and the difference can be significant when d is large. In the following, we show with a synthetic and a real data that our multi-stage approach improves the standard Lasso in practice. In order to avoid cluttering, we only study results for the two-stage procedure of Figure 2, which corresponds to the capped-L 1 regularization. We shall also compare it to the two-stage L p regularization method with p = 0 . 5 , which corresponds to the adaptive Lasso approach [10]. Note that instead of tuning the  X  parameter in Figure 2, in these experiments, we is the number of features that are not regularized in stage-2). This is clearly more convenient than tuning  X  . The standard Lasso corresponds to q = 0 .
 In the first experiment, we generate an n  X  d random matrix with its column j corresponding to [ x 1 ,j , . . . , x n,j ] , and each element of the matrix is an independent standard Gaussian N (0 , 1) . We then normalize its columns so that P n i =1 x 2 i,j = n . A truly sparse target  X   X  , is generated with k nonzero elements that are uniformly distributed from [  X  10 , 10] . The observation y i =  X   X  T x i + i , where each i  X  N (0 ,  X  2 ) . In this experiment, we take n = 25 , d = 100 , k = 5 ,  X  = 1 , and repeat the experiment 100 times. The average training error and 2-norm parameter estimation error are reported in Figure 3. We compare the performance of the two-stage method with different q versus the regularization parameter  X  . As expected, the training error becomes smaller when q increases. Compared to the standard Lasso (which corresponds to q = 0 ), substantially smaller estimation error is achieved with q = 3 for Capped-L 1 regularization and with p = 0 . 5 for L p regularization. This shows that the multi-stage convex relaxation approach is effective. Figure 3: Performance of multi-stage convex relaxation on simulation data. Left: average training squared error versus  X  ; Right: parameter estimation error versus  X  .
 In the second experiment, we use real data to illustrate the effectiveness of the multi-stage approach. Due to the space limitation, we only report the performance on a single data, Boston Housing . This is the housing data for 506 census tracts of Boston from the 1970 census, available from the UCI Machine Learning Database Repository : http://archive.ics.uci.edu/ml/ . Each census tract is a data-point, with 13 features (we add a constant offset on e as the 14th feature), and the desired output is the housing price. In the experiment, we randomly partition the data into 20 training plus 456 test points. We perform the experiments 100 times, and report training and test squared error versus the regularization parameter  X  for different q . The results are plotted in Figure 4. In this case, q = 1 achieves the best performance. This means one feature can be reliably identified in this example. In comparison, adaptive Lasso is not effective. Note that this dataset contains only a small number ( d = 14 ) features, which is not the case where we can expect significant benefit from the multi-stage approach (most of other UCI data similarly contain only small number of features). In order to illustrate the advantage of the two-stage method more clearly, we also consider a modified Boston Housing data, where we append 20 random features (similar to the simulation experiments) to the original Boston Housing data, and rerun the experiments. The results are shown in Figure 5. As expected from Theorem 4.1 and the discussion thereafter, since d becomes large, the multi-stage convex relaxation approach with capped-L 1 regularization ( q &gt; 0 ) has significant advantage over the standard Lasso ( q = 0 ).
 [1] Florentina Bunea, Alexandre Tsybakov, and Marten H. Wegkamp. Sparsity oracle inequalities [2] Emmanuel Candes and Terence Tao. The Dantzig selector: statistical estimation when p is [3] David L. Donoho, Michael Elad, and Vladimir N. Temlyakov. Stable recovery of sparse over-Figure 4: Performance of multi-stage convex relaxation on the original Boston Housing data. Left: average training squared error versus  X  ; Right: test squared error versus  X  . Figure 5: Performance of multi-stage convex relaxation on the modified Boston Housing data. Left: average training squared error versus  X  ; Right: test squared error versus  X  . [4] Vladimir Koltchinskii. Sparsity in penalized empirical risk minimization. Annales de l X  X nstitut [5] Nicolai Meinshausen. Lasso with relaxation. ETH Research Report, 2005. [6] R. Tyrrell Rockafellar. Convex analysis . Princeton University Press, Princeton, NJ, 1970. [7] Alan L. Yuille and Anand Rangarajan. The concave-convex procedure. Neural Computation , [8] Tong Zhang. Some sharp performance bounds for least squares regression with L 1 regulariza-[9] Peng Zhao and Bin Yu. On model selection consistency of Lasso. Journal of Machine Learning [10] Hui Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical
