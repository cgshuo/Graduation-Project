 Searching for people on the Web is one of the most com-mon query types to the web search engines today. However, when a person name is queried, the returned webpages of-ten contain documents related to several distinct namesakes who have the queried name. The task of disambiguating and finding the webpages related to the specific person of inter-est is left to the user. Many Web People Search (WePS) approaches have been developed recently that attempt to automate this disambiguation process. Nevertheless, the disambiguation quality of these techniques leaves a major room for improvement. This paper presents a new server-side WePS approach. It is based on collecting co-occurrence information from the Web and thus it uses the Web as an ex-ternal data source. A skyline-based classification technique is developed for classifying the collected co-occurrence infor-mation in order to make clustering decisions. The cluster-ing technique is specifically designed to (a) handle the domi-nance that exists in data and (b) to adapt to a given cluster-ing quality measure. These properties allow the framework to get a major advantage in terms of result quality over all the latest WePS techniques we are aware of, including all the 18 methods covered in the recent WePS competition [2]. Categories and Subject Descriptors: H.3.3 [ Informa-tion Storage and Retrieval ]:Information Search and Re-trieval; H.3.5[ Information Storage and Retrieval ]:Online Information Services-Web-based services General Terms: Algorithms, Experimentation, Measurement Keywords: Clustering, Skyline based classifier, WEPS, Web People Search, Named Entity Co-Occurrences, Social Network, Web Querying
The rapid growth of the Internet has made the Web a 0331707 and 0331690.
 popular place for collecting information. Today, Internet users access billions of web pages online using search engines. Searching for a specific person is one of the most popular search queries. Some statistics suggest that such searches account for as much as 5-10% of all search queries [14].
When a web search engine is queried with a person name, it returns a collection of webpages. Let D = { d 1 , d 2 , . . . , d be the set of the top k returned results. The webpages in D are related to the set of one or more namesakes who have the queried name, where these namesakes are unknown be-forehand. The goal of a Web People Search (WePS) system is to automatically cluster the webpages in D such that each cluster corresponds to a namesake.
 Most popular search engines of today such as Google and Yahoo! do not yet provide WePS capabilities, leaving the disambiguation task of finding the webpages of a person of interest to the user. For instance, when Google is queried with the name  X  X illiam Cohen X , the top 100 returned web-pages refer to at least 10 different namesakes whose name is  X  X illiam Cohen X . Thus the task of locating all the webpages about a specific William Cohen, for instance the CMU pro-fessor, can present a challenge to the user. Frequently the problem does not disappear even if the context keywords are provided along with the person name. For instance, the returned webpages for the above query with the additional keyword  X  X MU X  (a) will not include his webpages that do not include X  X MU X , making the recall lower, and (b) will ac-tually still refer to more than one namesake, thus not leading to perfect precision either. Such issues explain the moti-vation and the ever increasing interest in WePS systems, whose goal is to provide the user with more structured and more powerful web search capabilities for this specific type of query.

One of the key challenges that needs to be overcome to make the WePS functionality a reality, is to build a WePS system that is capable of reaching high disambiguation qual-ity. This has proven to be a nontrivial task, as corroborated by the results of a recent WePS competition [2].

Given that there is already a significant number of WePS techniques it is important to understand what distinguishes this work from other solutions. The proposed approach is a two-step clustering method. First the Named Entities (NEs) are extracted from each webpage d  X  D . The extracted NEs are people names and organizations that represent the social network of the namesake referred to by d . Then, for each distinct pair of webpages d i , d j  X  D the framework computes TF/IDF similarity based on NEs only. If this similarity is sufficiently large, the two pages are merged into one cluster.
In the second step of the approach, for each d i , d j pair that is still unmerged, the algorithm forms queries to a Web search engine that combine the NEs from d i and d j . The web search engine results are the co-occurrence counts that tell how often elements of the two social networks co-occur on the web and thus how strongly they are related. These counts are computed both, in the context of the queried name and without it. The counts are then transformed to form similarity features for the d i , d j pair. While there is research efforts on using related feature types for some do-mains, e.g. [7,11,19], we are unaware of any work that em-ploys such features for WePS. Collecting such features is a time consuming operation, unless it is performed at a server (web search engine) side. Furthermore, web search engine APIs have significant restrictions on the number of queries allowed per day. Thus, the proposed approach in its current form is inherently a server-side solution.

Proper features selection is one of the important contribu-tions of this paper. However, the key is to develop an algo-rithm to utilize the chosen features well in order to build a WePS system that gets high disambiguation quality results. We have developed a skyline-based feature classification al-gorithm for this purpose. This algorithm is the main con-tribution of this paper. The algorithm gains its advantage by (a) taking into account dominance that exists in the co-occurrence data and (b) by using a supervised learning to tune itself to both, the WePS domain and a given quality metric.

The rest of this paper is organized as follows. Section 2 summarizes the related research work. Section 3 provides an overview of the proposed approach. It is followed by Sec-tion 4 that covers the steps of the algorithm in more detail. The proposed approach is empirically evaluated in Section 5 and compared to some of the state of the art solutions. Sec-tion 6 concludes the paper by highlighting the impact of the proposed solution.
The Web People Search challenge is closely related to the well-studied Entity Resolution problem. In our previ-ous work we also have developed interrelated techniques to solve various Entity Resolution challenges, e.g. [9,15 X 18,23]. The approach covered in the paper, however, is not related to those techniques. The key algorithm for training the skyline-based classifier is new. In fact, we are unaware of any Entity Resolution technique that would use a similar approach un-der any context.
 There are several research efforts that address specifically Web Person Search and related challenges [1 X 3,5,6,8,22,25]. The approach of [5] is based on exploiting the link structure of pages on the Web, with the hypotheses that Web pages be-longing to the same real person are more likely to be linked together. Three algorithms are presented for disambigua-tion, the first is just exploiting the link structure of Web pages and forming clusters based on link analysis, the second algorithm is based on word similarities between documents and does clustering using Agglomerative/Conglomerative Dou-ble Clustering (A/DC), the third approach combines link analysis with A/DC clustering. The work in [21] clusters documents based on the entity (person, organization, and location) names and can be applied to the disambiguation of semi-structured documents, such as Web pages. The pri-mary new contribution is the development of a document generation model that explains, for a given document how entities of various types (other person names, locations, and organizations) are  X  X prinkled X  onto the document. In Sem-Eval 2007, a workshop on WePS task was held [2]. Sixteen different teams from different universities have par-ticipated to the task. The participated systems utilized named entities, tokens, URLs, etc that exist in the docu-ments. It has been shown that as the extracted information increases the quality of the clustering increases. Use of dif-ferent NE recognition tools affects the results of clustering as well. The NE based single link clustering was the one of the top three systems [12], because the named entity network alone enables to identify most of the individuals.
There are also a few publicly available Web search engines that offer related functionality, in that Web search results are returned in clusters. Clusty (http://www.clusty.com) from Vivisimo Inc. and Kartoo (http://www.kartoo.com) are search engines that return clustered results. However the clusters are determined based on intersection of broad topics (for instance research related pages could form one cluster and family pages could form another cluster) or page source, also the clustering does not take into account the fact that multiple persons can have the same name. For all of these engines, clustering is done based on entire Web page content or based on the title and abstract from a standard search-engine result. ZoomInfo (http://www.zoominfo.com/) and Spock (http://www.spock.com/) are commercially available people search engines.

Recently, researchers have started to use external databases, such as ontology and Web Search engine in order to im-prove the classification and clustering qualities in different domains [7,11,13,19]. For example, querying the Web and utilizing the search results are used for Word Sense Dis-ambiguation (WSD) [7] and record linkage in publications domain [11, 19]. The number of queries to a search engine is a bottleneck in these approaches. Hence, the study in Kanani and McCallum [19] tries to solve the problem in the case of limited resources. The suggested algorithm increased the accuracy of data cleaning while keeping the number of queries to a search engine minimal. Similarly the approach in [11] uses the web as a knowledge source for data cleaning. The study proposed a way to formulate queries and used some standard measures like TF/IDF similarity to compute the similarity of two different references to an entity. The work in [12] is a complementary work to the one in [19]. In [7] the authors proposed to use the co-occurrence counts for disambiguation of different meanings of words. The au-thors used web-based similarity measures like WebJaccard, WebDice, and so on. These measures are also utilized as features for the SVM based trainer along with a set of token based features, where the trainer learns the probability of two terms being the same.

In this paper, we study a similar approach, where our aim is to increase the quality of the Web Person search. Our study differs from the others in the way we utilize the web search results. In addition, we have used a different way to formulate queries.
The main task of a WePS system is to accurately clus-ter the webpages in D = { d 1 , d 2 , . . . , d k } , such that each resulting cluster corresponds to a namesake. There are sev-eral possible ways of implementing a WePS engine, such as client-side, third party proxy, or server-side approaches. In a third party proxy approach, the user query is issued first to a proxy, which in turn quires a web search engine and then clusters the returned results. The advantage of such an approach is that a third (independent) party could im-plement it. A client-side solution is similar, except for the software installed on the client acts as the proxy.
In a server-side approach, the user queried a web search engine directly. The server also does the clustering and returns the result to the user. The advantage of such an approach is that it is likely to be more efficient, as many webpage preprocessing steps can be done before any user query is issued. In addition, web querying is done internally instead of querying over the Internet. The disadvantage is that only a web search company could do that, or alterna-tively a third party should maintain a snapshot of the entire Web, e.g. using technology similar to that of WebBase de-veloped at Stanford [10].

The solution proposed in this paper can potentially work with any of these architectures. However, currently it is more feasible as a server-side approach.
The pseudo code in Figure 1 demonstrates the webpage preprocessing steps carried out on the server. They are per-formed in advance for the entire web collection, before the system starts accepting user queries. For each webpage, its Named Entities (NEs) are extracted and processed. The TF/IDF factors are precomputed. This is done to speed up the future computations of TF/IDF similarities between pairs of webpages. TF/IDF will be computed during actual query processing and will use these precomputed values. Fi-nally, the classifier is trained on the training data via super-vised learning. The latter is a key step which will be covered in detail in Section 4.3.

The pseudo code demonstrates that the extracted NEs are first cleaned. This is done using a set of filters. The idea is that NEs will be used by the framework as the context that identifies a namesake to some degree. However, certain types of NEs are too ambiguous for that purpose, in which case they are filtered out from further consideration. Given that the goal is to minimize the number of queries to the web search engine, the filters are specifically designed not to use any extra queries.

Filter 1 . For instance, location names have been found to be too ambiguous to be used as context, as too many namesakes may be mentioned in the context of the same location. Therefore, the algorithm does not use location information as one of the social network components. More-over, NE extractors sometimes wrongly extract the location names as organization names. This also creates the same problem mentioned above. Thus, to eliminate the ambigu-ity, the preprocessing step filters out the locations extracted as organizations. It does so by performing a lookup in a locally stored gazetteer with the organization name as the query. If there is a matching location in the gazetteer, then the organization name is simply filtered out.

Filter 2 . The second filter deals with the NEs that consist of one-word person names, such as  X  X ohn X . Such NEs are highly ambiguous since they can appear in the context of many namesakes on the Web. Consequently, the algorithm prunes away the NEs consisting of one-word names. This filter works by performing a lookup into the dataset that store first names.

Filter 3 . Similarly, the third filter handles NEs that are common English words. For example, word  X  X efense X  might be extracted from a webpage as an organization by the ex-traction software. However, it is a commonly used word, which can appear in the context of many namesakes. To de-tect common words the algorithm selects the most frequent 5000 terms from Wikipedia (http://www.wikipedia.org) as common English words. If an NE is a common English word, it is filtered out.

Filter 4 . Suppose that we are disambiguating webpages for  X  X ack Smith X . It is not rare to find out that two or more distinct  X  X ack Smith X  namesakes are related to two distinct namesakes X  X ohn Smith X . These two Jacks might for example be relatives of the two Johns. Thus,  X  X ohn Smith X  cannot serve as a good context to identify a particular  X  X ack Smith X  namesake. To capture this intuition, the algorithm filters out people NEs whose last name is the same as the last name specified in the original query.
A user query processing consists of three logical steps illus-trated in pseudo code in Figure 2: (1) TF/IDF Clustering (Lines 1 X 8), (2) WebFeature Clustering (Lines 9 X 14), and (3) Visualizing the results to the user (Lines 15).
The first step of the algorithm merges all pairs of web-pages whose TF/IDF similarities exceed a threshold. The threshold value is learned during the supervised learning process. TF/IDF is computed only on NEs extracted from the webpages, using the standard cosine similarity formula [24]. This initial clustering achieves two purposes: it de-creases the number of queries to the search engine [19], while at the same time it improves the quality of the final cluster-ing, as will be discussed in Section 5.

The second step employs the Web to gain additional data about the interactions between the webpages in D . For each distinct unprocessed and unmerged pair of webpages d , d j  X  D , it utilizes the search engine to collect the co-occurrence information c ij of the social networks of d i d . This will be explained in more detail in Section 4.1. The algorithm then transforms the co-occurrences into the cor-responding similarity features (Section 4.2). It then uses the skyline-based classifier on those features to predict whether the d i , d j pair should be merged (Section 4.3). The algo-rithm continues such iterations until no unprocessed pairs are left to be considered. After the second step, the final clustering is ready. The third step presents the computed final clustering results to the user.
Figure 2 demonstrates that for each unprocessed pair of webpages d i and d j the algorithm forms queries to the Web Search engine to compute co-occurrence statistics. This sec-tion explains the motivation behind using the queries as well as the procedure for forming such queries.

The purpose of using Web queries is to evaluate the degree of interaction of the social networks for two namesakes rep-resented by webpages d i and d j . If there is evidence on the web that two social networks are closely related, then two webpages are merged into one cluster. The guiding princi-ples in formulating the queries are:
The pseudo code in Figure 3 illustrates the procedure for formulating the queries. It demonstrates that two major types of queries are utilized: 1. N AND C i AND C j 2. C i AND C j .
 Here, C i represents the context for d i . It can be either the set of people NEs P i , or organization NEs O i . Context C is defined similarly for document d j . Since C i and C j have two possible assignments each, this creates 4 context combinations. Given that there are 2 types of queries, this leads to 8 queries in total, shown in Figure 3.

For example, assume that the user searches for the web-pages related to  X  X illiam Cohen X . Suppose that the algo-rithm extracts 2 namesakes of each type per webpage, that is, m = 2. Assume that webpage d i contains names  X  X amie Callan X  X nd X  X om Mitchell X , and webpage d j contains names  X  X ndrew McCallum X  X nd X  X ndrew Ng X . Then the first query will be: The web search engine API has a function call that computes the number of webpages relevant to the query, without ac-tually retrieving those webpages.

Observe that dataset D alone might not have any evidence to merge d i and d j . For instance, the TF/IDF similarity be-tween d i and d j might be low. Also, among the webpages in D , names  X  X amie Callan X  and  X  X om Mitchell X  might be only mentioned in d i , whereas X  X ndrew McCallum X  X nd X  X ndrew Ng X  X nly in d j , and otherwise D might not contain any infor-mation revealing interactions among these people. However, querying the Web allows the algorithm to gain additional in-formation to support the merge. In this case, the counts will be high enough to indicate that the people mentioned in the query are closely related.
To estimate the degree of overlap of two contexts C i and C for webpages d i and d j for the queried name N we can com-pute the co-occurrence count | N  X C i  X C j | for query N  X C However, it might be difficult to interpret this absolute value without comparing it to certain other values. For instance, if this count value is high, does it mean the contexts overlap significantly and thus d i and d j should be merged? Or, is it simply because N is a common name and thus there are lots of webpages that contains it under many contexts? Or, is it because contexts C i and C j are too unspecific, and thus too many webpages contain them?
Similar questions have been studied in the past [7]. The solution proposed there advocates normalizing such values, based on either Jaccard or Dice similarities. The Jaccard similarity between two sets A and B computes the fraction of common elements in A and B among all the distinct ele-ments:
The Dice similarity between two sets A and B computes the fraction of common elements in A and B among all the elements (including non distinct) in A and B , and normalizes it to [0 , 1] interval: There have been studies showing that the differences in re-trieval quality, when using these measures, is insignificant and furthermore these measures are monotone with respect to each other [20].

The proposed algorithm uses the Dice similarity to get the normalized version of | N  X C i  X C j | count. Two ways to normalize it have been examined: and
The algorithm employs the second formula, as it has proven to capture the ambiguity of context better. The following example provides just one type of scenario to illustrate the choice of the formula. Assume that the namesakes men-tioned in two webpages d i and d j are different. Suppose that the extractor wrongly extracts  X  X his X  as the only per-son NE from d i , and  X  X hat X  as the only person NE from d . Assume that all (or, most of) the webpages of the two namesakes contain both  X  X his X  and  X  X hat X . Then, Dice 1 sim-ilarity will be 1 (or, very high), causing the wrong merge of the webpages. However, Dice 2 similarity will be low, since |C i  X C j | will be large. That is, Dice 2 will automatically cap-ture that  X  X his X  and  X  X hat X  is not a good choice to be used as the contexts.

This observation raises an interesting point. At first glance it might seem the algorithm employs two different strategies to achieve the same goal. That is, it detects ambiguous con-text by using the filtering strategy covered in Section 3.1. But, frequently the same can be achieved by simply using the |C i  X C j | part of the Dice similarity. Notice, however, the Dice similarity can be low, for instance, because there is no evidence on the web of the context co-occurrence. But it also can be low because some of the context terms were too ambiguous. In the latter case, perhaps if some of the am-biguous terms are removed, there actually will be sufficient evidence to merge d i and d j . This is what precisely the fil-tering strategy attempts to do, by filtering out potentially ambiguous context terms upfront.

Hence, for each of the 4 possible N  X C i  X C j combinations (i.e., P-P, P-O, O-P, O-O), the algorithm generates two fea-tures. The first one is the raw | N  X C i  X C j | count. The second is its normalized version computed using the Dice 2 formula. Therefore, there are 8 features in total.

Observe that the features are chosen such that there is dominance in data in terms of merge decisions. First, let us make a few auxiliary definitions. We will say point f = ( f 1 , f 2 , . . . , f 8 ) up-dominates point g = ( g 1 , g 2 f 1  X  g 1 , f 2  X  g 2 , . . . , f 8  X  g 8 , and will denote it as f  X  g . Similarly, we will say f down-dominates g , if f 1  X  g 1 , f g , . . . , f 8  X  g 8 , and will denote it as f  X  g . Similarly, there is a notion of strict domination where  X   X   X  and  X   X   X  are substituted with  X  &lt;  X  and  X  &gt;  X .

Assume that d i , d j pair is characterized by the feature vector f = ( f 1 , f 2 , . . . , f 8 ). Suppose that based on f the al-gorithm decides that d i and d j should be merged. Assume that there is another pair of webpages d k and d  X  , which is then d k and d  X  should also be merged, since their social net-works contain even more evidence that the two namesakes are the same person. Thus, there is dominance in feature data in terms of merge decisions.
 The proposed algorithm uses a well-studied notion of Sky-Line for classifying points as  X  X erge X  or  X  X o not merge X . A skyline of a set of points P is the subset S of all points from P such that each point p  X  S is not strictly dominated by any other point from P . Figure 4(a) illustrates an example of a down-dominating skyline consisting of points { a, b, g, j } .
Figure 4(b) plots the up-dominating skyline for all the merge ( X + X ) points, plotted as filled circles. The do-not-merge ( X   X   X ) points are plotted as empty circles in that figure. A classification skyline is an up-dominating skyline on all the points the classifier declares to be  X  X erge X  points. Given a classification skyline, the classifier will classify any point this skyline up-dominates (i.e., any point that is  X  X n X  or  X  X bove X  the skyline) as a  X  X erge X  point, and any other point  X  as a do-not-merge point. Figure 4(b) illustrates one possible classification skyline for the plotted dataset. As any classifier, a skyline-based classifier can make a mistake. The figure shows that this choice of skyline will cause a do-not-merge point k to be wrongly classified as a merge point.
The clustering algorithm works by merging pairs of web-pages whose co-occurrence features are above the classifi-cation skyline. This section covers a greedy algorithm for learning such a skyline, illustrated in Figure 5.
The training algorithm is given a training dataset wherein each point that should be merged is labeled with  X + X , and each point that should not  X  with  X   X   X . It is also given a quality metric, such as the pairwise F measure or B-cubed, so that at any point in time it can measure which quality it can get by making certain types of decisions.

S clas initially consists of one point (  X  ,  X  , . . . ,  X  ), which causes no points to be classified as  X + X . At each iteration, the algorithm tries to greedily locate the best point to add to S clas , to make it better. For that it examines all the points from a certain pool of  X + X  points, S pool . Among them it first select the subset of points P lsterr such that adding a point p  X  P lsterr to S clas would cause the least error, see Figure 6. It does so by examining the quality of the clustering when using S clas and using p  X  S pool to classify only the  X   X   X  points. Then among the points in P lsterr selects a point p best adding which to S clas would lead to the best overall quality, see Figure 7. Point p best is then added to the classification skyline S clas . It keeps track of the best skyline observed so far by storing and updating it in S best
Adding point p best to the classification skyline S clas can make several (at least one) yet-unclassified points to be clas-sified as  X + X  by the algorithm. That can happen for two reasons: The algorithm maintains in P actv the set of yet-unclassified points. We will use notation P + actv and P  X  actv to denote the  X + X  and  X   X   X  points in this set. The pool of points S pool which P lsterr is constructed is the skyline of P + actv . The choice of S pool is motivated by several factors. First, for the efficiency reasons we want to avoid using large sets, like for instance P actv . The skyline tends to be much smaller than P actv . Second, since it is a set of + X  X , this guarantees making at least one correct merge decision. Third, if we look to add a  X + X  point to S clas that would minimize the number of misclassifications of  X   X  X  (false positives), then that skyline will contain all such points. Figures 6, 7, 8, 9, and 10 demonstrate the steps of the learning algorithm in more detail. The feature space is indexed for efficient processing, such as incremental maintenance of the skylines instead of rebuilding them from scratch.

The example in Figure 11 illustrates one iteration of the algorithm. Assume that the current classification skyline is S clas = { q, d, c } , as shown in Figure 11(a). Then S will be the down-dominating skyline on yet-unclassified  X + X  points, which is S pool = { f, e, u, h } . Notice that among yet-unclassified  X   X   X  points (the ones below the classification sky-line) f dominates 1 point ( k ), e  X  2 points, u  X  1 point, and h  X  2 points. If the goal of our quality measure to minimize the number false positives, then P lsterr will be chosen as P lsterr = { f, u } since they introduce only 1 new error. Now, given a choice of adding f or u to the skyline S clas , assume that adding f would lead to the best classification quality. Then the algorithm will choose p best = f . Figure 11(b) demonstrates what happens after adding f to the skyline.
Discussion. Observe that one of the solutions would be just to use any classifier, such as SVM, to classify the points as + X  X  and  X   X  X . Unlike that solution, the skyline-based classier utilizes several additional factors to its advantage. First, it explicitly takes into account the dominance that ex-ists in data. Second, it is aware that there is a clustering underneath the + X  X  and  X   X  X  and that classifying a point as a + can cause several other points be classified as + due to transitivity. Third, it greedily fine-tunes itself to a given quality metric; whereas the first approach is more geared to-ward specifically pairwise F-measure. As we shall see in the experimental section, these qualities allow the skyline-based solution to outperform SVM/DTC based classifiers.
In this section we empirically evaluate our approach and compare it with some previously proposed algorithms.
Datasets. We tested our algorithms using three different publicly available datasets. The first one, WWW05 , which is used in [5] contains 12 different people. The second one, SI-GIR05 , which is used in [3] is composed of 9 different people. This dataset was also a trial dataset for [2]. The third data set WePS is the dataset that is used in the WEPS task [2]. This dataset is composed of training and testing datasets. In the training there are 49 different people, whereas in test-ing dataset there are 30 people. On average, each person dataset has 100 web pages.

We first combined both data sets WWW05 and SIGIR05 to have more training examples with different aspects. Dataset SIGIR05 contains web pages for very common person names, which are formed using the census data. Dataset WWW05 is composed of the people who are related to each other due to their research areas. Most of the people in this dataset are well represented on the Web. The names in SIGIR05 are very common English names, as a result each person is rep-resented with 1 or 2 pages, on the other hand dataset WWW05 is more diverse. For example, there are two sub datasets,  X  X dam Cheyer X  X nd X  X eslie Kaelbling X  X n WWW05 , where pages represent only two people with the same name. Another sub dataset,  X  X illiam Cohen X , contains a popular person with a very common name, majority of the pages are about the secretary of defense William Cohen. We then applied leave-one-out cross validation to demonstrate the effectiveness of our algorithm. We learned the skyline point with the 20 dif-ferent names and tested the accuracy of those skyline points on the left-out dataset. We used Yahoo! API to collect the co-occurrence statistics.

Quality Measures. We use two measures that are com-monly used for assessing the quality of WePS applications: F P and F B [2,3]. Here, F P is the harmonic mean of Purity , Inverse Purity . F B is the B-cubed F-measure [4].
Baseline Methods. We use two methods as base-line: the Named Entity based Clustering (NE) and Web based co-occurrence similarity measure (WebDice). NE is a TF/IDF merging scheme that uses only Named Entities from the webpages to compute the TF/IDF similarity val-ues, which are compared against a threshold. This algo-rithm was the second runner-up in the WEPS competition. The second baseline, WebDice computes the similarity be-tween two pages by summing up the individual dice simi-larities of People-People, People-Organization similarities of web pages. Single-link clustering approach is used and the optimal threshold is selected by applying leave-one-out cross validation. The threshold for the NE-clustering is learned to be between 0 . 1 and 0 . 2 on the WWW05  X  X  cross-validation experiments. On the other hand the threshold for the Web-Dice clustering is 0 . 001 for WWW05 . On the WePS dataset the threshold is learned to be 0 . 002 and 0 . 2 for WebDice Clus-tering and NE-Clustering respectively.

Quality on WePS dataset. We used the training data of WePS to learn the skyline points and tested it on its test portion, as required by the WEPS task. Figure 12 compares the quality of our algorithm with the quality of the systems participated to the WEPS task. The figure shows that the proposed approach outperforms the best solution by 7%.
Detailed Clustering Results Table 1 compare the re-sults of the proposed algorithms and of the baselines solu-tions. We used paired t-test to measure the statistical signif-icance of the improvement. As Table 1 shows improvement We performed another set of experiments with the WePS . First we compute the cross-validation results on the train-ing dataset, and after that we used the training and testing algorithms to test the effectiveness of our algorithm on the algorithms with respect to NE based clustering method is statistically significant for  X  = 0 . 01. The improvement of skyline algorithm with respect to dice similarity is statisti-cally significant for  X  = 0 . 05. Figure 13: Initial Clustering effect on final Cluster-ing.
 WEPS dataset. Table 2 shows the cross validation results of WEPS training dataset. The improvement to the baseline algorithms is also statistically significant. In the table we also show the clustering quality results for the test dataset. It improves the quality as well. The threshold for initial clustering is empirically estimated on the training data and then used on the testing. The threshold is chosen as 0 . 2 for all three datasets.

Effect of Initial Clustering. In this experiment, we demonstrate the effect of the threshold chosen for the initial TF/IDF clustering to the overall quality. While this thresh-old is learned from data in the previous experiments, in this experiment it is varied to analyze the dependence. Figure 13 shows the quality change as the threshold increases. As the threshold approaches 0 . 2 the skyline based algorithm signif-icantly outperforms the dice similarity approach. We also compare the clustering quality of our algorithm with a De-cision Tree based Classifier (DTC) and with SVM. SkyLine based method outperforms them both, since it takes into account the dominance in data and tunes itself to F P .
Efficiency. The proposed approach is likely to be less efficient than any of the 16 WePS task approaches, if imple-mented as a third party proxy solution. The reason is that it requires on average 4360  X  8 web search queries to the API per person name. Many search engines do not allow that many queries per day. Notice, however, the number of queries is relatively small, and thus a server-side solution is a real possibility.
This paper proposes a Web People Search approach that is based on collecting the co-occurrence information from the Web in order to get a better disambiguation quality. A skyline based supervised learning methodology has been developed. The proposed methodology takes into account the dominance that exits in the feature space as well as greedily fine-tunes itself to a given quality measure. These qualities allow it to outperform other state of the art WePS solutions. The limitation of the proposed solution is that as of currently it is more feasible a server-side approach due to the present day restrictions of web search engine querying APIs.
