 Taxonomies, especially the ones in specific domains, are becoming indispensable to a growing number of applications. State-of-the-art approaches assume there exists a text corpus to accurately charac-terize the domain of interest, and that a taxonomy can be derived from the text corpus using information extraction techniques. In reality, neither assumption is valid, especially for highly focused or fast-changing domains. In this paper, we study a challenging problem: Deriving a taxonomy from a set of keyword phrases. A solution can benefit many real life applications because i) keywords give users the flexibility and ease to characterize a specific domain; and ii) in many applications, such as online advertisements, the do-main of interest is already represented by a set of keywords. How-ever, it is impossible to create a taxonomy out of a keyword set it-self. We argue that additional knowledge and contexts are needed. To this end, we first use a general purpose knowledgebase and key-word search to supply the required knowledge and context. Then we develop a Bayesian approach to build a hierarchical taxonomy for a given set of keywords. We reduce the complexity of previous hierarchical clustering approaches from O ( n 2 log n )to O ( n log n ), so that we can derive a domain specific taxonomy from one mil-lion keyword phrases in less than an hour. Finally, we conduct comprehensive large scale experiments to show the e ff ectiveness and e ffi ciency of our approach. A real life example of building an insurance-related query taxonomy illustrates the usefulness of our approach for specific domains.

Taxonomy plays an important role in many applications. For ex-ample, in web search, organizing domain -specific queries into a hi-erarchy can help better understand the queries and improve search results [23] or help with query refinement [17]. In online adver-tising, taxonomies about specific domains (e.g., insurance, which is the most profitable domain in online ads) are used to decide the relatedness between a given query and bidding keywords.

Recently, much work has been devoted to taxonomy induction, particularly with respect to automatically creating a domain-specific ontology or taxonomy [14, 15, 16]. The value of automatic taxon-omy constructing is obvious: Manual taxonomy construction is a laborious process, and the resulting taxonomy is often highly sub-jective, compared with taxonomies built by data-driven approaches. Furthermore, automatic approaches have the potential to enable hu-mans or even machines to understand a highly focused and poten-tially fast changing domain.

Most state-of-the-art approaches for domain-specific taxonomy induction work as follows : First, it selects a text corpus as its in-put. The assumption is that the text corpus accurately represents the domain. Second, it uses some information extraction methods to extract ontological relationships from the text corpus, and uses the relationships to build a taxonomy. For instance, to derive a medical or biological taxonomy, a commonly used corpus is the entire body of biomedical literature from MEDLINE, life science journals, and online books (e.g., those on PubMed).

Although these text-corpus based approaches have achieved some success, they have several disadvantages. For example , for a highly focused domain, it is very di ffi cult to find a text corpus that accu-rately characterizes that domain. Intuitively, it is easier to find a text corpus (e.g., the entire set of ACM publications) for big topics such as  X  X omputer science,  X  but it is much more di ffi cult to find one for a specific topic such as  X  X ig data for business intelligence,  X  as articles about such topics are likely to be dispersed in many dif-ferent fields and forums. Furthermore, we are often interested in new domains or fast changing domains, which makes it even more di ffi cult to find a characterizing text corpus.

Furthermore , even if we can find a corpus that accurately char-acterizes the domain, we may still have a problem of data sparsity. Intuitively, the more highly focused the domain, the smaller the text corpus that is available for that domain. The problem is exac-erbated by our limited power in understanding natural language text for identifying ontological relationships. To build a taxonomy from a text corpus, we usually bootstrap from a limited set of heuristic patterns. However, high quality patterns typically have very low recalls. For instance, it is well known that Hearst patterns [8] (i.e.,  X  X uch as X  patterns) have a high level of accuracy, but it is unrealistic to assume that the text corpus expresses every ontological relation-ship in  X  X uch as X  or its derived patterns, especially when the text corpus is not large enough.

Instead of building taxonomies from a text corpus, we can also consider extracting a domain-specific taxonomy from a big, gen-has low coverage on a highly focused domain, and it may also pro-duce ambiguous interpretations for highly specialized terms in the domain.

In this paper, we consider the challenge of inducing a taxonomy from a set of keyword phrases instead of from a text corpus. The problem is important because a set of keywords gives us the flex-ibility and ease to accurately characterize a domain, even if the domain is fast changing. Furthermore, in many cases, such a set of keywords is often readily available. For instance, search engine companies are interested in creating taxonomies for specific adver-tising domains. Each domain is described by a set of related ad keywords (bid phrases).

The problem of inducing a taxonomy from a set of keywords has one major challenge. Although by using a set of keywords we can more accurately characterize a highly focused, even fast-changing domain, the set of keywords itself does not contain explicit rela-tionships from which a taxonomy can be constructed. One way to overcome this problem is to enrich the set of keyword phrases by aggregating the search results for each keyword phrase (i.e., throw-ing each keyword phrase into a search engine, and collecting its top k search result) into a text corpus. Then it treats the text cor-pus as a bag of words and constructs a taxonomy directly out of the bag of words using some hierarchical clustering approach [5, 18]. The problem with this approach is that the corpus represents the context of the keyword phrases, rather than the conceptual re-lationships that exist among the keyword phrases. For instance, the search results for the query  X  X uto insurance X  contain a very limited number of articles on how  X  X uto insurance X  is defined or charac-terized. The majority of articles either introduce a certain type of car insurance or talk about car insurance in passing. From such ar-ticles, we can extract context words such as  X  X lorida, X   X  X ords, X  or  X  X ccident. X  The resulting context can be arbitrary and not insurance related.

To tackle this challenge, we propose a novel,  X  X nowledge + approach for taxonomy induction. We argue that, in order to cre-ate a taxonomy out of a set of keywords, we need knowledge and contexts beyond the set of keywords itself. For example, given two phrases  X  X ehicle insurance X  and  X  X ar insurance , X  humans know im-mediately that  X  X ar insurance X  is a sub-concept of  X  X ehicle insur-ance,  X  because humans have the knowledge that a car is a vehicle. Without this knowledge, the machine can hardly derive this rela-tionship unless the extended corpus from the keywords (e.g., using keyword search) happens to describe this relationship in a syntac-tic pattern that the machine can recognize (e.g.,  X  X ehicles such as cars X ). But the context is also important. It is unlikely that the knowledgebase (especially a general purpose one) knows about ev-ery subsuming relationship in the specific domain. For example, it has no idea that x is a sub-concept of y . However, through the con-text, we may find that x is highly related to z , and in the knowledge-base z is a sub-concept of y . Thus, using knowledge and contexts, we can establish the relationship between x and y .

Incorporating both knowledge and contexts in taxonomy build-ing is not easy. To this end, we formalize our approach as hier-archical clustering over features generated from a general knowl-1 http: // www.dmoz.org / http: // www.wikipedia.org / edgebase and search context. More specifically, we first deduce a set of concepts for each keyword phrase using a general purpose knowledgebase [19] and then obtain its context information from a search engine. After enriching the keyword set using knowledge and contexts, we make use of Bayesian-based hierarchical cluster-ing to automatically induce a new taxonomy. Our paper presents three technical contributions: First, we illustrate how to derive the concepts and context from each keyword. Second, we present the way to formulate the taxonomy building to a hierarchical clustering problem based on their concepts and context. And finally, we scale up the method to millions of keywords.

The rest of the paper is organized as follows. In Section 2, we introduce the background of hierarchical clustering and Bayesian rose trees. We then present algorithms, analyze their complexity, and discuss some implementation issues in Section 3. Section 4 describes experiments that demonstrate the e ff ectiveness and e ciency of our approach. In Section 5, we conclude our work and present several future works. Hierarchical clustering is a widely used clustering method [9]. The advantage of hierarchical clustering is that it generates a tree structure (a dendrogram) which is easy to interpret. Two strate-gies are used for hierarchical clustering , agglomerative and divi-sive, where the agglomerative approach builds a tree from bottom up by combining the two most similar clusters at each step, and the divisive approach builds a tree from top down by splitting the current cluster into two clusters at each step.

Traditional hierarchical clustering algorithms construct binary trees. However, binary branches may not be the best model to de-scribe asetof data X  X  intrinsic structure in many applications. Fig. 1 gives an example. The goal here is to create a taxonomy from a set of insurance-related keyword phrases. It assumes we have a good similarity measure that enables us to group related keywords in an optimal way. Still, the outcome might be sub-optimal or unnatu-ral. In the figure,  X  X ndiana cheap car insurance, X   X  X entucky cheap car insurance,  X  and  X  X issouri cheap car insurance X  are on di levels, but apparently they belong to the same cluster. This dis-advantage is introduced by the model not the similarity measure, which means that no matter how accurately we understand the data, the result will still be sub-optimal.

To remedy this, multi-branch trees are developed. Compared with binary trees, they have a simpler structure and better inter-pretability. An example of multi-branch clustering is shown in Fig. 2, where nodes such as  X  X ndiana cheap car insurance, X   X  X en-tucky cheap car insurance , X  and  X  X issouri cheap car insurance X  are grouped under the same parent node.

Currently, there are several multi-branch hierarchy clustering ap-proaches [1, 3, 10]. The methods proposed by Adams et al [1] and Knowles et al [10] are based on the Dirichlet di ff usion tree, which use an MCMC process to infer the model. Blundel et al [3] adopts a simple, deterministic, agglomerative approach called BRT (Bayesian Rose Tree). In this work, we adopt BRT as a basic algo-rithm for taxonomy induction. The major steps of BRT are shown in Algorithm 1.

Algorithm 1 is a greedy agglomerative approach. In the begin-ning, each data point is regarded as a tree on its own: T where x i is the feature vector of i th data. For each step, the algo-rithm selects two trees T i and T j and merges them into a new tree T . Unlike binary hierarchical clustering, BRT uses three possible merging operations [3]: Specifically, in each step, Algorithm 1 greedily finds two trees T and T j to maximize the ratio of probability: where p ( D m | T m ) is the likelihood of data D m given the tree T D m is all the leaf data of T m , and p (
D where f ( D m ) is the marginal probability of the data D is the  X  X ixing proportion . X  Intuitively,  X  T m is the prior probability that all the data in T m is kept in one cluster instead of partitioned into sub-trees. In BRT [3],  X  T m is defined as: where n T m is the number of children of T m ,and0  X   X   X  1isthe hyperparameter to control the model. A larger  X  leads to coarser partitions and a smaller  X  leads to finer partitions.

The major cost of this bottom-up hierarchy construction approach is dominated by the following two steps: (1) looking for pairs of clusters to merge; (2) calculating the likelihood associated with the merged cluster
Assume in the current round there are c clusters. The two steps above take O ( c 2 ) time. At the start of the algorithm, we have c the number of data points. For all the clusters merging into one cluster, if we first compute the likelihood by Eq. (1), and sort them before we search for merging pairs, it will take O ( n 2  X  time complexity, where C V is the maximum number of non-zero elements in all the initial vectors x i  X  X . For high-dimensional text data, the document will have hundreds of words on average. There-fore, C V cannot be ignored when we analyze the overall complexity compared to log n . This is not applicable for any large-scale data set. Moreover, this approach will have O ( n 2 ) memory cost. For Algorithm 1 Bayesian Rose Tree ( BRT ).
 100 , 000 data points, this will take 3  X  8  X  10 10 = 240 G bytes mem-ory to contain all the pairs X  likelihood, if we use 8 bytes to store double-precision value and have three types of merging likelihoods ( X  X oin, X   X  X bsorb , X  and  X  X ollapse X ).
In this section, we introduce our approach for building a domain specific taxonomy from a set of keyword phrases augmented with knowledge and contexts . First , we obtain knowledge and contexts related to the keywords. In our approach, we obtain knowledge (concepts that correspond to each keyword phrase) using a tech-nique called short text conceptualization [19] and a general purpose knowledgebase called Probase [11, 24] , and we obtain contexts by submitting the queries to a commercial search engine to retrieve all snippet words. Second, we build the taxonomy using a mul-tiple branch hierarchical clustering approach. In this section, we initially introduce how to leverage concepts and contexts , and then we present the methods that speed up the taxonomy building for a large set of keywords.
We use information obtained from a general-purpose knowledge-base and a search engine to augment the given set of keyword phrases.

The knowledgebase we use is Probase [11, 24], which has been demonstrated useful for Web search [21, 22]. The core of Probase consists of a large set of isa relationships, which are extracted from a text corpus of 1.68 billion web pages. For example,  X ... Spanish artists such as Pablo Picasso ... X  is considered a piece of evidence for the claim that  X  X ablo Picasso X  is an instance of the concept Spanish artist . Probase also contains other information. For ex-ample, for each concept, it contains a set of attributes that describe the concept. One unique feature of Probase is the broadness of its coverage. Probase contains millions of concepts, from well-known ones such as  X  X ountry X  and  X  X rtists X  to small but concrete ones such as  X  X edding dress designers X  and  X  X enewable energy techniques . X  The richness of Probase enables us to identify and understand mil-lions of concepts people use in their daily communication.
The knowledgebase facilitates us in deriving concepts from key-word phrases and we can then use the concepts to enrich the key-word phrase. For instance, given  X  X icrosoft and apple , X  X ederive concepts such as IT companies , big companies , etc., and given  X  X p-ple and pear , X  we derive concepts such as fruit or tree .However, these are still not enough for understanding the keyword phrase. We need to enhance the knowledgebase in two aspects.
Although Probase contains milli ons of concepts, we cannot ex-pect it to cover everything. To remedy this, we collect the  X  X on-text X  of a keyword phrase, and use the context to supplement the Figure 3: Conceptualization of  X  X ndiana cheap car insurance X . concepts related to the phrase. To obtain the context, we submit the keyword phrase to a search engine, and collect the top ten snippets in the search results. The context is represented as a bag of words. The quality of the context is much lower than that of the concepts obtained from Probase, in the sense that for a given word in the bag-of-words context, we do not know its semantic relationship with the keyword. But still, they can be useful especially when the concepts we obtained are insu ffi cient. As an example, consider two key-word phrases,  X  X ww.monster.com X  and  X  X onster.com . X  Probase knows that  X  X onster.com X  is a job site, but it knows nothing about  X  X ww.monster.com . X  Thus, conceptualization will report that the two phrases have zero similarity. Through the search engine, we find that  X  www. monster.com X  is associated with phrases such as web site, online job site, job board , etc. Thus, by adding the context information, the query containing  X  X ww.monster.com X  will have greater similarity to the query containing  X  X onster.com . X 
To cluster the data into a hierarchy, we first need to model the data. This corresponds to calculating f ( D ) in Eq.(2), the marginal distribution of data D (Section 2). The original BRT approach [3] assumes that the data can be modeled by a set of binary features that follow the Bernoulli distribution. In other words, features are not weighted. In our approach, we use features that consist of con-cepts and contexts to represent the data. Since even a short piece of text may contain multiple topics or concepts, it is important to rank them by their significance. Thus, unlike BRT, we incorporate weights into the marginal distribution f ( D ).

Given a set of keyword phrases { keyword 1 ,  X  X  X  , keyword derive a list of ( term , weight ) pairs for each keyword, where the term is either a concept produced by the knowledgebase, or a con-text word generated by search, and the weight is derived as follows: weight j =  X   X  freq ( term j ) + (1  X   X  )  X  where  X  is a parameter that controls how much we value the con-text compared to concepts; freq ( term j ) is the frequency of a term j in the context derived from search results; P ( term j | is the probability of the term as a concept given keyword phrase keyword i ,and is provided by the knowledgebase (as in Eq (5)); C is the frequency of keyword i in the knowledgebase, and C P ( term j | keyword i ) is used as the frequency of the term as a con-cept. We then set the feature vector x i with the term frequencies weight j  X  X  for keyword i .

In the hierarchical clustering algorithm, once two keywords or keyword clusters are grouped together, the grouped keyword clus-ter can contain multiple topics. For example, initially we have four keywords  X  X hina, X   X  X ndia, X   X  X ermany , X  and  X  X rance. X  Although these four keywords share some common concepts such as coun-try and nation , we can still distinguish them based on the concepts with smaller weights. First,  X  X hina X  and  X  X ndia X  will be grouped together since they share many concepts like Asian country and emerging market .  X  X ermany X  and  X  X rance X  will also be grouped together because they share concepts like European country and Western nation . After that, these two clusters will be grouped to-gether. The final cluster actually contains multiple topics, i.e. both Asian country and European country . Therefore, we need a distri-bution to better capture this characteristic.

To this end, we use the DCM distribution [13] to represent the marginal distribution f ( D ). DCM is derived based on multino-mial and Dirichlet conjugate distributions. Multinomial distribu-tion can naturally characterize the co-occurrence counts of terms, while the prior distribution, i.e., Dirichlet distribution, can be re-garded as smoothing over counts. The generative process of a docu-ment underlying this modeling is that we first sample a multinomial distribution from Dirichlet distribution, and then sample a docu-ment based on the multinomial distribution. Multinomial distribu-tion can be regarded as a document-specific sub-topic distribution, which makes certain words appear more likely in a particular docu-ment [13]. DCM integrates out the intermediate multinomial distri-bution, and thus it represents either more general topics or multiple topics. In hierarchical clustering, we incrementally merge clusters. Therefore, DCM is more appropriate for evaluating whether two clusters (with multiple topics) should be merged.

Specifically, the likelihood of multinomial distribution p ( x defined by: Algorithm 2 Nearest-neighbor-based BRT .
 m of multinomial distribution.

The Dirichlet distribution prior is: p (  X  |  X  ) = has the property  X  ( x + 1) = x  X  ( x ). The  X  X irichlet delta function X   X  (
Then the marginal distribution f ( D ) is given by: f Using this marginal distribution f DCM ( D ), we can seamlessly inte-grate the weights into the term feature vector. Next we will intro-duce how to construct the tree more e ffi ciently. As we show in Section 2, BRT has time complexity O ( n 2  X  n 2 log n ), and space complexity O ( n 2 ), which is not applicable to large-scale problems. The most time consuming process in ag-glomerative clustering lies in searching all candidate cluster pairs to find the best pairs to merge. If we can reduce the search space for BRT by pruning the pairs of keyword clusters that are most unlikely to be merged, then we can reduce the cost of searching agglomerative clustering. We propose to  X  X ache X  a set of nearest neighbors for each data point. Then the search complexity only depends on the number of the nearest neighbors. However, search-ing for nearest neighbors still incurs acost of O ( n ) for each data point. The time complexity of finding k nearest neighbors can be reduced to O (log n ) using techniques such as KD-trees [2] and Met-ric trees [20]. However, these techniques are not suitable for high-dimensional data, since they partition the data space dimension by dimension. Appr oximation methods such as LSH (Locality-Sensitive Hashing) [7] can be used when the number of dimensions is large. Particularly , Spilltree [12] relaxes the non-overlapping constraints of Metric trees and incorporates the technique used in LSH, and thus combines the benefits of both methods.

In this section, we focus on adapting two major types of near-est neighbor approaches for e ffi cient taxonomy construction: k -nearest-neighbor ( kNN )and -ball-nearest-neighbor ( NN )[4]. kNN finds k nearest neighbors for each data and is not concerned the density of the data. NN uses a spherical ball to bind all the nearest 3 For integer variables , Gamma function is  X  ( x ) = ( x numbers, it is  X  ( x ) =  X  0 t x  X  1 e  X  t d t . neighbors within the ball. In our taxonomy construction approach, we significantly reduce the time complexity by using methods such as Spilltree [12] and PPJoin+ [25].
We introduce two k NN -based approximation approaches based on BRT .Aflowchart for using the nearest neighbors to approximate the construction procedure of BRT is shown in Algorithm 2. k NN -BRT : Using the k NN approach, we first find the k nearest neighbors for each data point, and then we ch eck the possibility of merging within the neighborhood set. We denote N k ( x )asthe k nearest neighbor set of data x .Tofind k nearest neighbors of a data, we keep a minheap with size k to maintain the data with largest similarities scores. When new data comes and the similarity score is larger than the top value (the smallest one in the minheap), we replace the top index with the new data. Compared to BRT ,the space cost is significantly reduced from O ( n 2 )to O ( nk ). The time complexity is also reduced to O ( n 2  X  C V + n 2 log k ).
Spilltree -BRT : Using the k nearest neighbors to construct BRT is still time consuming. We then use the Spilltree algorithm [12] to further reduce the time complexity.

Spilltree is a generalization of metric trees [20]. Metric trees partition the data space into binary trees, and retrieve the nearest neighbors by DFS (depth first search). Metric trees will be less ef-ficient when the number of dimensionality is large (e.g., larger than 30 [12]). The Spilltree algorithm o ff ers two major modifications : 1. Introduce random projection bef ore partitioning the data space. 2. Introduce the overlapping / non-overlapping regions between
According to the Johnson-Lindenstrauss Lemma [6], embedding a data set of dimension n to an O (log n ) dimensional space has little distortions for pairwise distances. As a result, brute-force search in the projected space provides a (1 +  X  )-NN [12] in the original space. Thus, by projecting the data onto a much lower dimensional space, high precision can be guaranteed while the time cost is significantly reduced, especially when the original data has millions of dimen-sions.

Moreover, original metric trees perform a DFS to find the nearest neighbors. By introducing the overlapping nodes, Spilltree adopts a combined strategy of a defeatist search and DFS. The defeatist search may fail for non-overlapping nodes if a query and its nearest neighbors belong to di ff erent branches. However, it is guaranteed to be successful for the overlapping nodes when the shortest distance in the overlapping regions is larger than or equal to the distance be-tween the query and its nearest neighbor. By setting an appropriate tolerance parameter  X  , the accuracies of both overlapping and non-overlapping nodes are ensured [12]. Overall, the time complexity of search for Spilltree is O (log n ) [12].

The random projection to d -dimensional space has the time com-plexity of O ( nd  X  C V ). Building a Spilltree costs O ( nd log n ). To use Spilltree to search the k nearest neighbors, we also keep a min-heap to maintain k data points when traversing the Spilltree .This step will cost O ( nd log n log k ) time for all the data. In summary, using Spilltree to build BRT costs O ( nd  X  C V + nd log n log k ). Com-pared to the k NN -BRT algorithm, using Spilltree will cost addi-tional O ( Vd + nd ) memory to store the random projection matrix and the Spilltree .
 Table 1: Comparison of computational complexity and mem-ory requirements of di ff erent algorithms. ( C V is the number of non-zero elements in the vector x and L = j x ( j ) )
In NN -approximation, for each data point, we keep its nearest neighbors whose similarity with the data point is larger than a pre-defined threshold . This reduces the time complexity to O ( n 2 The storage of NN depends on the number of the neighbors that satisfy the threshold. However, we might need to re-run the NN algorithm in order to ensure we can find candidates to be merged, since when the threshold is too large, NN will not return any nearest neighbors.

PPJoin -BRT : To support NN -approximation, we need to find neighbors of a data point e ffi ciently. We adopt the PPJoin+ [25] approach for this purpose. PPJoin+ uses two types of filtering, prefix filtering and su ffi x filtering, to filter out the data points that do not satisfy certain constraints . In prefix filtering , it has been proven that the cosine similarity is larger than a threshold if and only if the number of overlapped terms between the two sets is larger than = L i  X  L j ,where L i = k x ( k ) i is the length of the document x i . Therefore, we can quickly filter out pairs of doc-uments as if their overlap is larger than . The time complexity of this step is reduced to (1  X  2 ) j x ( j ) ,&lt; 1 [25]. In su tering ,it first derives an upper bound of hamming distance H corresponding to the pre-defined threshold .Thenwe filter out the data if the lower bound of the hamming distance between two doc-uments H min ( x i , x j ) is larger than the upper bound H implemented an algorithm based on the binary search for the lower bound of the hamming distance. The overall time complexity of PPJoin+ is O ( n 2 [(1  X  2 ) L + log L ]), where L is the average length of documents. PPJoin -BRT takes O ( nf ( )) memory to store the likelihood values of the nearest neighbors. Moreover, it needs an inverted index to facilitate prefix filtering, and the memory cost is O ( nL ).

A comparison of all the algorithms is shown in Table 1. We can see that Spilltree -BRT has the least time complexity; however, it requires more memory .
To demonstrate the e ff ectiveness and scal ability of th e proposed taxonomy building algorithms, we test the original BRT and the nearest-neighbor-based methods on three di ff erent data sets. The nearest-neighbor-based methods include k NN -BRT , Spilltree -BRT , and PPJoin -BRT . In this section, we introduce the evaluation re-sults in detail.
In this experiment, we use the 20-newsgroups data to evaluate the correctness of the algorithms. This data set forms a hierarchy with two levels. The first level has six clusters, and the second level contains 20 clusters. We randomly sample 2,000 documents to compare the clusters generated by di ff erent hierarchical clustering algorithms. We set all the parameters to be the same for Bayesian rose trees. In addition, k in k NN -BRT and Spilltree -BRT is set to 10. The dimension of random projection used in Spilltree -BRT is (a) Speedup over BRT with dif-ferent parameters in Spilltree for query data. set to 10. The overlapping parameter  X  used in Spilltree [12] is set to 0.001.

We first compare the likelihood of di ff erent algorithms. As shown in Table 2, the nearest-neighbor-based methods are comparable with the original BRT . Particularly, the likelihood of k NN -BRT is better than the original BRT . The reason is that searching the candidate pairs from the nearest neighbors can significantly reduce the noise of the data. Thereby it leads to a better local optima . This phe-nomenon has also been observed in [4].

Then we leverage the NMI metric to compare the clusters of dif-ferent levels in the generated trees with the human annotated ones (Table 3). All the results are based on ten trials of di ff cutions. It can be seen that all the nearest-neighbor-based methods have similar performance with the original BRT .The k NN -BRT al-gorithm performs best with levels one and two, while the original BRT performs best with the third level. This observation is consis-tent with the likelihood results.
In this section, we investigate the scalability of the algorithms based on two data sets. The first one is a query data set consist-Table 2: Comparison of likelihood for 20-newsgroups data. ing of 114,076 queries, where the concept and context information are extracted using the methods introduced in Section 3.1. The vocabulary size is 912,792 and the average number of words in the documents is 935. We also evaluate the algorithms on a larger news data set. It contains one million news articles. The vocabulary size is 252,174 and the average number of words is 242. All the experi-ments are conducted on a 64-bit server with 2.5GHz CPU and 32G of memory. The memory cost for the news data set is around 10G.
We first conduct the experiments to examine the  X  X nd-to-end X  time cost of di ff erent algorithms. We set k = 10 for k NN -BRT and Spilltree -BRT . The dimension of random projection used in Spill-tree -BRT is set 10. The overlapping parameter  X  used in Spilltree is set to 0.001. As shown in Figs. 4(a) and 4(d), the Spilltree -BRT al-gorithm is the fastest. k NN -BRT also performs faster than BRT and PPJoin -BRT . PPJoin -BRT is not as fast as expected because the process of finding the nearest neighbors must be re-run if there is no candidate cluster pair to merge. Particularly, for the query data set, PPJoin -BRT is even worse than the original BRT algorithm. This is due to the fact that PPJoin is significantly a ff average length of documents. The average length of documents in query data is about four times that of the news data. For the news data set, PPJoin -BRT performs better than BRT .

To further analyze the performance of each algorithm in detail, we divide the execution into two parts: preprocessing and build . The detailed time costs for di ff erent algorithms are shown in Figs. 4(b), 4(c), 4(e), and 4(f). For BRT , during the preprocessing, we com-pute the likelihood values between data samples and then sort them. For the nearest neighbor based methods, the preprocessing mainly focuses on finding the nearest neighbors and computing the likeli-hood values. The building part of each algorithm gradually merges clusters. For di ff erent algorithms, merging clusters di searching for the candidate pair sets of di ff erent sizes. Moreover, after a new cluster is generated, di ff erent number of likelihood val-ues will be computed, as illustrated in Algorithms 1 and 2. It can be concluded from the results that the time costs of k NN -BRT and Spilltree -BRT in these two parts is consistently lower than BRT . Specifically, the preprocessing of PPJoin -BRT is faster than k NN -BRT .However,for build is much slower. This again demonstrates our explanation that PPJoin -BRT will re-run the process of finding the nearest neighbors.

Since Spilltree -BRT is the fastest algorithm, we also investigate how di ff erent Spilltree parameters a ff ect the time cost. The follow-ing experiments are conducted with 10,000 data samples. The com-parison results between the nearest-neighbor-based methods and the original BRT are shown in Figs. 5(a) and 5(b). In the figures, di ff erent curves represent the time costs of the Spilltree algorithms with di ff erent overlapping tolerance parameters  X  . This parame-ter controls how many data points we allow to be overlapped by di ff erent tree nodes in a Spilltree . As shown in the figures, the larger overlapping parameter results in slower algorithm execution, since it makes the algorithm backtrack more times back to the par-ent nodes [12]. Moreover, in each curve, we also show how the projected dimensions a ff ect the time cost. Generally, fewer di-mensions lead to faster algorithm execution. Spilltree -BRT can be 200 X 300 times faster than BRT when we project the data onto a ten-dimensional space. Finally, we evaluate how the number of the nearest neighbors a ff ects the time cost. The results are shown in Figs. 5(c) and 5(d). We present the time costs of both k NN -BRT and Spilltree -BRT algorithms. It is shown that Spilltree -BRT per-forms better when the number of the nearest neighbors is small. This is also because it backtracks fewer times to the parent nodes when fewer neighbors are needed.
To investigate the quality of the proposed taxonomy building method, we also apply it to the top K NN search problem. This problem is defined as: given a query in the form of short text, we retrieve similar queries based on the concept and context features Table 4: Comparison of the accuracy of top K search. ( d is the dimension of random projection, and k is the number of the nearest neighbors.) from an already built tree . We use K here to distinguish the no-tation of k used for k NN -based construction of taxonomy. In this experiment, we investigate whether Spilltree , k NN -BRT ,and Spill-tree -BRT can bring benefits to this problem.

We first select 1,000 queries to build the taxonomy trees by lever-aging the Spilltree algorithm and di ff erent algorithms of BRT s. The experiments of Spilltree are conducted by doing a grid search for tolerance parameter  X  = { 0 . 2 , 0 . 3 , 0 . 4 } . The experiments of all BRT -related algorithms are conducted by doing a grid search for parameters  X  = { 5 , 10 , 15 } (Eq. (7)) and  X  = { 0 . 1 , We choose the best results to be shown in this experiment. Then we randomly select 100 queries to search the K NN of each query. The ground truth is generated based on the cosine similarity. We evaluate the precision of the top K search problems for di gorithms, which is the proportion of the overlapped top K keywords retrieved by the generated trees and the brute-force search using the cosine similarity metric. The results are shown in Table 4. From the analysis of the results, we draw the following conclusions: First, the dimension of random projection significantly a ff ects the accu-racy of Spilltree . In general, the more random projection features we use, the better the accuracy is. The search accuracy can be im-proved from 30% to more than 90%. Second, the number of the nearest neighbors used to accelerate the building procedure does not a ff ect the accuracy very much. For k NN -BRT , the accuracy does not show a significant change when changing the number of k . Third, Spilltree -BRT can further improve the accuracy of Spilltree . With the same projection dimensionality d , Spilltree -BRT results are significantly better than Spilltree . Increasing d or k can both improve the accuracy. This is because larger d results in a better searching accuracy of the nearest neighbors, while larger k leads Table 5: Comparison of the time (in 10  X  6 seconds) of top K search. ( d is the dimension of random projection.) to more candidate cluster pairs which increase the opportunity for finding the right clusters to merge.

We also test the searching time for Spilltree and Spilltree -BRT . 110K data is used to build the trees and 200 queries are randomly selected to test the retrieving time. As shown in Table 5, Spilltree performs faster with lower dimension of random projection. How-ever, the accuracy is not good enough (see Table 4). With higher dimensions, the time cost of Spilltree increases very quickly. The major reasons are: (1) It cost more time to compare the similarity between two points for higher dimensionality; (2) Given the same parameter configuration, Spilltree will search more points in high dimensional space. We also perform a grid search for Spilltree -BRT . It can be seen that all results are achieved in acceptable time. The search time of Spilltree -BRT depends on the structure of the tree. Thus, changing d and k only a ff ects the building time of Spill-tree -BRT and does not monotonically a ff ect the searching time.
In this paper, we present an approach that can automatically de-rive a domain-dependent taxonomy from a set of keyword phrases by leveraging both a general knowledgebase and keyword search. We first deduce concepts with the technique of conceptualization and extract context information from a search engine, and then in-duce the new taxonomy using a Bayesian rose tree. We provide three nearest-neighborhood-based methods to speed up the original Bayesian rose tree algorithm. Particularly, the Spilltree -based al-gorithm reduces the time and memory cost significantly. We also conducted a set of experiments to demonstrate the e ff ectiveness and e ffi ciency of the proposed algorithms.

We regard the work presented as initial, as there are improve-ments to be made as well as many directions to pursue. First, we will investigate how to use a set of random projections to expo-nentially reduce the errors of k nearest neighbor searches in the Spilltree algorithm. Second, we would like to use other locality sensitive hashing methods or latent semantic sensitive dimension-ality reduction methods to improve the search accuracy in Spilltree . Third, we would like to apply our taxonomy building method to real-world applications (e.g., adve rtisement or personalized search) to demonstrate its e ff ectiveness and usefulness.
 We would like to thank Charles Blundell and Yee Whye Teh for their help on the implementation of the Bayesian rose tree, and thanks to Ting Liu for help on the implementation of Spilltree. [1] R. P. Adams, Z. Ghahramani, and M. I. Jordan.
 [2] J. L. Bentley. Multidimensional binary search trees used for [3] C. Blundell, Y. W. Teh, and K. A. Heller. Bayesian rose trees. [4] W.-Y. Chen, Y. Song, H. Bai, C.-J. Lin, and E. Y. Chang. [5] S.-L. Chuang and L.-F. Chien. Towards automatic generation [6] S. Dasgupta and A. Gupta. An elementary proof of the [7] A. Gionis, P. Indyk, and R. Motwani. Similarity search in [8] M. A. Hearst. Automatic acquisition of hyponyms from large [9] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a [10] D. A. Knowles and Z. Ghahramani. Pitman-Yor di ff usion [11] T. Lee, Z. Wang, H. Wang, and S. Hwang. Web scale [12] T. Liu, A. W. Moore, A. Gray, and K. Yang. An investigation [13] R. E. Madsen, D. Kauchak, and C. Elkan. Modeling word [14] I. Mani, K. Samuel, K. Concepcion, and D. Vogel.
 [15] R. Navigli, P. Velardi, and S. Faralli. A graph-based [16] H. Poon and P. Domingos. Unsupervised ontology induction [17] E. Sadikov, J. Madhavan, L. Wang, and A. Y. Halevy. [18] D. Shen, M. Qin, W. Chen, Q. Yang, and Z. Chen. Mining [19] Y. Song, H. Wang, Z. Wang, H. Li, and W. Chen. Short text [20] J. K. Uhlmann. Satisfying general proximity / similarity [21] J. Wang, H. Wang, Z. Wang, and K. Q. Zhu. Understanding [22] Y. Wang, H. Li, H. Wang, and K. Q. Zhu. Toward topic [23] R. W. White, P. N. Bennett, and S. T. Dumais. Predicting [ 24] W. Wu, H. Li, H. Wang, and K. Q. Zhu. Probase: A [25] C. Xiao, W. Wang, X. Lin, J. X. Yu, and G. Wang. E ffi
