 Assessing the trustworthiness of reviews is a key issue for t he main-tainers of opinion sites such as TripAdvisor, given the rewa rds that can be derived from posting false or biased reviews. In this p aper we present a number of criteria that might be indicative of su spi-cious reviews and evaluate alternative methods for integra ting these criteria to produce a unified  X  X uspiciousness X  ranking. The crite-ria derive from characteristics of the network of reviewers and also from analysis of the content and impact of reviews and rating s. The integration methods that are evaluated are singular value d ecompo-sition and the unsupervised hedge algorithm. These alterna tives are evaluated in a user study on TripAdvisor reviews, where v ol-unteers were asked to rate the suspiciousness of reviews tha t have been highlighted by the criteria.
 E.0 [ Data ]: General  X  Data quality; H.4 [ Information Systems ]: Miscellaneous Algorithms User-generated content, Credibility, Shilling
A significant challenge in the administration of review and r at-ing sites such as Amazon and TripAdvisor is the management an d presentation of the user-generated content. For popular it ems, there will often be many more reviews available than a user might re ad, so identifying helpful or informative reviews is an interes ting chal-lenge [6]. At the other end of the spectrum, it is vital for the credi-bility of rating sites to identify and filter fraudulent  X  X hi ll X  reviews  X  that is the subject of this paper.

In the work described here, we present a number of criteria th at might be indicative of hotels with suspicious reviews (see S ec-tion 3). This is not the central contribution of the paper how ever. The main contribution is the comparison of methods for combi n-ing good sets of features to produce useful overall suspicio usness rankings. These methods are compared in a user study that ide n-tifies a good subset of features for identifying suspicious r eviews. Our evaluation shows that reviews highlighted as suspiciou s corre-spond with the opinions of real users.

The two aggregation methods evaluated are: singular value d e-composition (SVD), and the unsupervised hedge algorithm (U H) [8]. Our evaluation demonstrates that these aggregation me thods behave quite differently, resulting in rankings that do not correlate strongly. Notably, the top-ten suspicious hotels identifie d by both SVD and UH agree on just two of ten hotels. The assessment of these top-ten sets in the user trial suggests that SVD provid es a more effective means of aggregating the various review filte ring criteria.

In the next section, we present a very brief review of some rel -evant research, before presenting details of the suspiciou sness cri-teria in Section 3. Details on the two aggregation methods ar e de-scribed in Section 4, where the details of the evaluation are also presented.
The related research relevant to this work falls into two cat e-gories: the work that directly addresses this problem, and t he work on spam detection and authoritativeness in related domains . This second category is extensive, as there are many related or an alo-gous problems that have received attention  X  in particular e -mail spam [2], link spam (search engine spam) [1], detecting atta cks on recommender systems [5], and assessing authoritativeness on sites such as Wikipedia [4].

If we consider the identification of spam reviews as a subset of the larger problem of identifying reviews that are author itative, credible or helpful, then there is some interesting researc h to draw on. Both O X  X ahony &amp; Smyth [6] and Hsu et al. [3] cast the prob-lem of ranking reviews in a supervised learning framework, a nd show impressive results. O X  X ahony &amp; Smyth use customer feed -back on the helpfulness of reviews on Amazon to provide the su-pervision , while Hsu et al. use feedback provided from Digg. Un-fortunately in the TripAdvisor scenario there is no user fee dback to support a supervised learning approach. This presents a p articu-lar problem for feature subset selection and aggregation. H owever, the features employed by O X  X ahony &amp; Smyth to identify helpfu l reviews have influenced the selection of the criteria descri bed in Section 3.

The most relevant work we have found on aggregation is that by Tan &amp; Jin [8]. They modify the original Hedge algorithm for rank/score aggregation so that it can operate in an unsuperv ised setting (see Section 4.1). They find this is better for opinio n aggre-gation than a number of alternative techniques, including S VD. It is interesting that in our evaluation SVD proves to be more effe ctive.
In this section, we define a number of criteria (features) tha t we expect will be predictive of suspicious reviews. These crit eria are influenced by some of the criteria used in [6] but also by the ex pec-tation that shill reviewers will have unusual social networ k charac-teristics  X  we think of the bipartite network of reviews and h otels as a simple social network.

Following the practice in [6], we split hotel reviews into posi-tive and negative categories according to the overall ratings where a positive review is one assigned 4 or 5 stars and reviews with less than 4 stars are negative. For all criteria, a higher value is more indicative of potential shilling activity  X  we rank hotels i n descend-ing order, where those at the top of the list are deemed to be mo st suspicious.
This criterion is motivated by two principles, that reviewe rs who have posted many reviews have more authority, and that revie wer profiles with may reviews take a lot of effort to create. Based on this a positive singleton is a positive review from a reviewer who has posted no other reviews. Thus, the PPS score for hotel H is the proportion of reviews on that hotel that are positive single tons: where N ps is the number of positive singleton reviews, and N is the total review count for the hotel.
We expect that, typically multiple shill reviews will be inj ected to boost a hotel X  X  popularity. We suggest that this activity may occur over a short time period, in which multiple user accoun ts are created and strongly-positive reviews for the target ho tel are submitted in quick succession. 1 The greater the degree of temporal clustering between a batch of positive reviews, the more sus picious these reviews appear.

Given the list of of positive singleton reviews { r 1 , . . . , hotel H arranged in ascending order by submission date, we de-fine a score for H as a function of the average date distance D ( i.e. number of days) between each review r i and its temporally nearest neighbour:
For r 1 and r P the time to the beginning and end of the evalua-tion period is considered. This score is based on a Gaussian k ernel where  X  is a bandwidth parameter that controls the influence of the proximity of reviews. A higher value of  X  will emphasise pairs of reviews that are very close in time. We examined a range of val -ues for this parameter, but found that a value of  X  = 1 was most effective on the TripAdvisor data.
In an attempt to recover from negative reviews the managemen t of a hotel may react by posting some positive shill reviews. T hese
The review spam recently discovered on Apple X  X  App Store had this characteristic http://edition.cnn.com/2009/TECH/ 12/09/wired.apple.apps/index.html will show up as positive singletons that closely follow genu ine neg-ative reviews. The strength of evidence for these can be quan tified as the dataset, and t i is the reaction time associated with shill i ( i.e. the number of days between the negative review and the subsequen t shill).

If a hotel has n such reactive positive singletons, then we can accumulate this evidence into an RPS score as follows: where T H is a normalisation factor for each hotel which is the elapsed time before the 1 st and n th RPS. This employs what Shafer calls Hooper X  X  rule of concurrent testimony [7] to accumula te the evidence. The evaluation in Section 4.3 suggests that RPS is a strong indicator of suspicious activity.
With this criterion we seek to assess the impact on the hotel X  s av-erage star rating of reviewers with little track record. To d o this, we produce an alternative average rating where each reviewer X  s contri-bution is weighted by the number of reviews they posted ( n difference between the unweighted and weighted rating is th e RWR score: where R H is the set of ratings for hotel H . The mean difference will be greater for hotels that have received high scores from mem bers who have infrequently reviewed hotels.
This criterion elaborates on the RWR score by considering al l contributions by the reviews. TripAdvisor allows members t o make contributions other than reviews: this includes photos, vi deos, fo-rum posts, articles, and travel itineraries. We extract the total num-bers of contributions for members, and define CWR as follows: where c r is the number of contributions from the reviewer who pro-duced the rating. It is interesting that neither CWR and RWR f are well in the evaluation in Section 4.3.
The idea here is to remove a portion of the most positive revie ws for a hotel and recalculate the average star rating to see if i t devi-ates much from the simple average. This TR score is calculate d as follows: where R tr H is the truncated rating set. In our evaluation the top 20% of ratings were removed. Hotels where the average rating fal ls more than average after deleting the top 20% of reviews are su s-picious, they would have a high TR score.
An alternative strategy is to look at the change in hotel rati ngs over time. We split each hotel X  X  reviews into  X  X arly X  and  X  X a te X  sets. These correspond to the first and second year of our data set. SS is a measure of the change in popularity between the first an d second period: where R e and R l are the early and late rating/review sets. A positive SS score may indicate shilling in the second period.
In our attempts to identify fraudulent reviews, we also wish to consider the text content of the reviews. Creating long revi ews that appear to be genuine is a time-consuming process, so we expec t that spam reviews might be shorter than normal. The PRLD fea-ture identifies hotels with reviews that deviate a lot from th e mean. The PRLD score for a hotel H is the average absolute difference between the length of its positive reviews and the mean lengt h. where PosRev H is the set of positive reviews, len p is length of re-view p , and len is the mean length of positive reviews.
A major challenge for research in this area is the lack of an-notated datasets for assessing the effectiveness of shill d etection strategies. For this reason, we gathered a dataset of 26,903 reviews from 21,440 unique reviewers, covering hotels from all regi ons of Ireland over a two-year time window from September 2007 to September 2009. A total of 741 hotels are covered in our evalu a-tion  X  we have eight criteria to score these hotels (PPS, CPS, RPS, RWR, CWR, TR, SS and PRLD). This gives us a 741  X  8 score matrix where the rows are the hotels and columns are the featu res.
Given the score matrix, we need a means of aggregating the 8 columns to produce a single suspiciousness ranking. To do th is we consider two alternative techniques:
Singular Value Decomposition: SVD is a well established tech-nique for projecting high-dimensional data into a lower dim ension space. The standard form is X n  X  m  X  T n  X  k S k  X  k V k  X  m a matrix describing n items in terms of m features and S is a di-agonal matrix of k singular values. In order to produce an aggre-gated ranking we use just one singular value so the decomposi tion of the hotels.

Unsupervised Hedge Algorithm: This is an unsupervised vari-ant of an older supervised rank aggregation algorithm, adap ted by Tan &amp; Jin [8]. In the absence of supervision, it sets out to pro duce a ranking with maximal agreement with the component rankings . To do this it produces an aggregate score that is a weighted sum o f the component scores f comb ( x ) =  X  m i = 1 w i f i ( x ) where w assigned to the i th score. The algorithm works iteratively with the weights updated at each step. The parameter  X  is effectively a learning rate that controls conver-gence, and  X  X oss X  is a simple function that quantifies the dis agree-ment between the aggregate score and the component score -se e [8] for details. This weight update strategy has the effect o f de-emphasising component scores that disagree with the aggreg ate.
For each of the aggregation methods describe previously, we evaluated two variants, since aggregation can be performed either on the raw feature scores or on the rankings derived from thes e scores. The rank correlations between the four resulting ra nkings of the 741 hotels are listed in Table 1. It is interesting to obse rve that both UH variants are very strongly correlated. This is becau se, in both cases, the aggregation is dominated by a single feature , RWR. Table 1: The rank correlations between the rankings from the four aggregation alternatives.

To produce a ground truth for our dataset, we conducted a user study. We firstly selected 41 hotels corresponding to the uni on of the top-ten sets of hotels selected by a variety of aggregati on and feature selection alternatives. Five unsuspicious hotels were added to this set to act as a control. Users were presented with a ran dom selection of six of these hotels, and asked to mark any review s that might appear suspicious. Based on the judgements provided b y 55 users who completed the task, we calculated a suspiciousness score for each of the 46 hotels  X  the mean of the fraction of rev iews marked as suspicious by each user. Note that the average leve l of review annotation agreement between users in the trial was  X  76%.
The chart in Figure 1 shows that the five control hotels have th e lowest suspiciousness score, as might be expected. The top-ten lists produced by the two SVD-based aggregations scored equally w ell, while the UH results are disappointing. Note that the lists f or the ranking and scored-based UH methods were identical. The poo r performance of UH seems to be because the consensus is domi-nated by the RWR feature, a feature that is not very informati ve  X  see next sub-section.
 Figure 1: Suspiciousness scores for the top-ten lists produ ced by three aggregation alternatives.
It is interesting to assess which of the features are most pre dic-tive of suspiciousness as determined by the annotators. To d o this, we compare the feature-based rankings for the 46 hotels with the suspiciousness ranking according to the annotators  X  see Fi gure 2. This analysis shows that RPS and TR are the strongest feature s while review length (PRLD) and the weight of established rev iew-ers (RWR and CWR) seem to provide little information. The poo r Figure 2: The correlation of feature-based rankings and the suspiciousness ranking by annotators.
 Figure 3: Suspiciousness scores for the top-ten lists produ ced by SVD on the strongest features. performance of RWR also explains why UH does not perform well in the aggregation as the UH ranking is dominated by RWR.
To investigate how effective the strongest features alone p roved in identifying suspicious reviews, we applied the SVD score s ag-gregation method based on subsets of the top 3 and 5 features f rom Figure 2. The results in Figure 3 show that using the 3 best fea tures (RPS,TR,PPS) leads to a significant improvement over the ful l set of 8 features. This indicates that this combination of crite ria pro-vides an effective means of identifying shill hotel reviews .
We have presented a number of criteria for highlighting susp i-cious hotel reviews on TripAdvisor. A natural question aris es re-garding how best to integrate their outputs. We have evaluat ed two alternative strategies for aggregating the criteria into a single rank-ing. Surprisingly, we have found that SVD outperforms UH in t his aggregation  X  this is the opposite of the findings in [8].
Our evaluation suggests that positive reviews which quickl y fol-low negative reviews are suspicious, as are hotels whose ran king deteriorated dramatically when the 20% most positive revie ws are removed. Both of these criteria will focus on hotels with sig nificant variance in their review sets. This research was supported by Science Foundation Ireland ( SFI) Grant No. 08/SRC/I1407. [1] L. Becchetti, C. Castillo, D. Donato, S. Leonardi, and [2] P. O. Boykin and V. P. Roychowdhury. Leveraging social [3] C. Hsu, E. Khabiri, and J. Caverlee. Ranking Comments on [4] N. Korfiatis, M. Poulos, and G. Bokos. Evaluating [5] M. P. O X  X ahony, N. J. Hurley, and G. C. M. Silvestre. [6] M. P. O X  X ahony and B. Smyth. Learning to recommend [7] G. Shafer. The combination of evidence. International Journal [8] P. Tan and R. Jin. Ordering patterns by combining opinion s
