 Pradeep Ravikumar  X  pradeepr@stat.berkeley.edu Alekh Agarwal  X  alekh@eecs.berkeley.edu Martin J. Wainwright  X  , X  wainwrig@stat.berkeley.edu University of California, Berkeley A key computational challenge associated with dis-crete Markov random fields (MRFs) is the problem of maximum a posteriori (MAP) estimation: comput-ing the most probable configuration(s). For general graphs, this MAP problem includes a large number of classical NP-complete problems, including MAX-CUT independent set, and satisfiability problems, among various others.
 This intractability motivates the development and analysis of methods for obtaining approximate solu-tions. The ordinary max-product algorithm is a form of non-serial dynamic-programming, exact for trees, and also widely used as a heuristic for obtaining ap-proximate solutions to the MAP problem, but it suf-fers from convergence failures, and despite some local optimality results (Freeman &amp; Weiss, 2001), it has no general correctness guarantees. For certain MRFs aris-ing in computer vision, Boykov et al. (2001) studied graph-cut based search algorithms that compute a lo-cal maximum over two classes of moves. A related class of methods are those based on various types of convex relaxations, in which the discrete MAP problem is re-laxed some type of convex optimization problem over continuous variables. Examples include linear pro-gramming (LP) relaxations (Wainwright et al., 2005; Chekuri et al., 2005), as well as quadratic, semidefinite and other conic programming relaxations (Ravikumar &amp; Lafferty, 2006; Kumar et al., 2006; Wainwright &amp; Jordan, 2003).
 Among convex relaxations, LP relaxation is the least computationally expensive and best understood. The primary focus of this paper is a well-known tree-based LP relaxation (Chekuri et al., 2005; Wainwright et al., 2005) of the MAP estimation problem for pairwise Markov random fields, based on optimizing over a set of locally consistent pseudomarginals on edges and ver-tices of the graph. In principle, this LP relaxation can be solved by any standard solver, including sim-plex or interior-point methods (Bertsimas &amp; Tsitsik-ilis, 1997). However, such generic methods fail to ex-ploit the graph-structured nature of the LP, and hence do not scale favorably to large-scale problems. Wainwright et al. (2005) established a connection be-tween this tree-based LP relaxation and the class of tree-reweighted max-product (TRW-MP) algorithms, showing that suitable TRW-MP fixed points specify optimal solutions to the LP relaxation. Subsequent work has extended this basic connection in various in-teresting ways. For instance, Kolmogorov (2005) de-veloped a serial form of TRW-MP with some conver-gence properties but as with the ordinary TRW-MP updates, no guarantees of LP optimality. Weiss et al. (2007) connected convex forms of sum-product and exactness of reweighted max-product algorithms. Globerson and Jaakkola (2007) developed a conver-gent dual-ascent algorithm, but its fixed points are guaranteed to be LP-optimal only for binary problems, as is also the case for the TRW-MP algorithm (Kol-mogorov &amp; Wainwright, 2005), and the rate of con-vergence is not analyzed. Other authors (Komodakis et al., 2007; Feldman et al., 2002) have proposed sub-gradient methods, but such methods typically have sub-linear convergence rates.
 The goal of this paper is to develop and analyze vari-ous classes of message-passing algorithms that always solve the LP, and are provably convergent with at least a geometric rate. The methods that we develop are flexible, in that new constraints can be incorporated in a relatively seamless manner, with new messages introduced to enforce them. All of the algorithms in this paper are based on the notion of proximal mini-mization : instead of directly solving the original linear program itself, we solve a sequence of so-called prox-imal problems, with the property that the sequence of associated solutions is guaranteed to converge to the LP solution. We describe different classes of al-gorithms, based on different choices of the proximal function: quadratic, entropic, and reweighted Bethe entropies. For all choices, we show how the interme-diate proximal problems can be solved by message-passing updates on the graph, guaranteed to converge but with a distributed nature that scales favorably. An additional desirable feature, given the wide variety of lifting methods for further constraining LP relax-ations (Wainwright &amp; Jordan, 2003), is that additional constraints are easily incorporated within the frame-work. We begin by introducing some background on Markov random fields, and the LP relaxations that are the focus of this paper. Given a discrete space X = note a p  X  dimensional discrete random vector. We assume that its distribution P is a Markov random field, meaning that it factors according to the struc-ture of an undirected graph G = ( V, E ), with each variable X s associated with one node s  X  V , in the following way. Letting  X  s : X  X  and  X  st : X  X  X  X  be singleton and edgewise potential functions respec-tively, we assume that the distribution takes the form P ( x ;  X  )  X  exp The problem of maximum a posteriori (MAP) esti-mation is to compute a configuration with maximum probability X  X .e., an element x  X   X  arg max This problem is an integer program, since it involves optimizing over the discrete space X p . The functions  X  ( ) and  X  st ( ) can always be represented in the form where the m -vectors {  X  s ; j , j  X  X } and m  X  m matrices {  X  st ; jk , ( j, k )  X  X  X  X } parameterize the problem. The basic linear programming (LP) relaxation of this problem is based on a set of pseudomarginals s and st , associated with the nodes and vertices of the graph. These pseudomarginals are constrained to be non-negative, as well to normalize and be locally con-sistent in the following sense: The polytope defined in this way is denoted LOCAL( G ), or L ( G ) for short. The LP relaxation is based on solving maximizing the linear function X subject to the constraint  X  L ( G ). In the se-quel, we write this LP more compactly in the form guaranteed to be exact for any problem on a tree-structured graph (Wainwright et al., 2005), so that it can be viewed as a tree-based relaxation. The main goal of this paper is to develop efficient and dis-tributed algorithms for solving this LP relaxation, as well as strengthenings based on additional constraints. For instance, one natural strengthening is by  X  X ift-ing X : view the pairwise MRF as a particular case of a more general MRF with higher order cliques, define higher-order pseudomarginals on these cliques, and use them to impose higher-order consistency constraints. This particular progression of tighter relaxations un-derlies the Bethe to Kikuchi (sum-product to general-ized sum-product) hierarchy.
 We begin by defining the notion of a proximal min-imization scheme, and the Bregman divergences that we use to define our proximal sequences. Instead of re-ferring to the maximization problem max  X  L ( G )  X  T , it is convenient to consider the equivalent minimiza-3.1. Proximal Minimization The class of methods that we develop are based on the notion of proximal minimization (Bertsekas &amp; Tsitsik-lis, 1997). Instead of attempting to solve the LP di-rectly, we solve a sequence of problems of the form where for each n = 0 , 1 , 2 , . . . , n denotes current iter-ate, {  X  n } denotes a sequence of positive weights, and D f is a certain type of generalized distance, known as the proximal function. The purpose of introducing the proximal function is to convert the original LP X  X  convex optimization problem but non-differentiable in dual space  X  X nto a strictly convex optimization prob-lem that can be solved relatively easily. This scheme appears similar to an annealing scheme, in that it in-volves a choice of weights {  X  n } . However, although the weights {  X  n } can be adjusted for faster conver-gence, they can also be set to a constant, unlike for annealing procedures, which would typically require that 1 / X  n  X  0. The reason is that D f ( k ( n ) ), as a generalized distance, itself converges to zero when the method gets closer to the optimum, thus provid-ing an  X  X daptive X  annealing. For appropriate choice of weights and proximal functions, these proximal min-imization schemes converge to the LP optimum with at least geometric and possibly superlinear rates (Bert-sekas &amp; Tsitsiklis, 1997; Iusem &amp; Teboulle, 1995). In this paper, we focus exclusively on proximal func-tions that are Bregman divergences (Censor &amp; Zenios, 1997), a class that includes various well-known diver-gences (e.g., quadratic norm, Kullback-Leibler diver-gence etc.). More specifically, we say that a function f is a Bregman function if it is continuously differen-tiable, strictly convex, and has bounded level sets. It then induces a Bregman divergence This function satisfies D f ( k  X  )  X  0 with equality iff =  X  , but need not be symmetric or satisfy the trian-gle inequality, so it is known as a generalized distance. We study the sequence { n } of proximal iterates (4) for the following choices of Bregman divergences. Quadratic Distances: This choice is the simplest, corresponding to the quadratic norm across nodes and edges Q ( k  X  ) : = where we have used the shorthand and similarly for the edges. The Bregman function this corresponds to is the quadratic function, Weighted Entropic Distances: Here we consider a (possibly weighted) sum of Kullback-Leibler (KL) divergences across the nodes and edges: D ( k  X  ) = where D ( p k q ) : = the KL divergence, and {  X  s ,  X  st } are positive node and edge weights, respectively. An advantage of the KL distance, in contrast to the quadratic norm, is that it automatically acts to enforce non-negativity con-straints on the pseudomarginals. The Bregman func-tion this corresponds to is the entropy function, where H s and H st are singleton and edge-based en-tropies, respectively.
 An extension of define a Bregman function based on a convex combination of tree-structured entropy func-tions (Wainwright &amp; Jordan, 2003), and using expres-sions such as the reweighted Bethe entropy which are equivalent to the convex combination of tree entropies within the local polytope, we can derive an iterative procedure involving tree-reweighted message passing to solve the outer proximal steps. We defer further details to a full-length version. 3.2. Proximal Sequences via Bregman The key in designing an efficient proximal minimiza-tion scheme is ensuring that the proximal sequence { n } can be computed efficiently. In this section, we first describe how the sequence of each proximal mini-mization can be reformulated as a particular Bregman projection. We then describe how this Bregman pro-jection can itself be computed iteratively, in terms of a sequence of cyclic Bregman projections based on a de-composition of the constraint set LOCAL( G ). In the sequel, we then show how this cyclic Bregman projec-tions reduce to very simple message-passing updates. Given a Bregman divergence D , the Bregman projec-tion of the vector  X  onto a convex set C is given by By taking derivatives and using standard conditions for optima over convex sets (Bertsekas &amp; Tsitsiklis, 1997), the defining optimality condition for b is for all  X  C . Now consider the proximal minimization problem to be solved at step n , namely the strictly convex problem By taking derivatives and using the same convex op-the conditions for all  X  C . Note that these optimality conditions are of the same form as the Bregman projection con-ditions (11), with the vector  X  f ( n ) +  X  n  X  taking the role of  X  f (  X  ); in other words, with (  X  f )  X  1 (  X  f ( ) +  X  n  X  ) being substituted for  X  . Consequently, efficient algorithms for computing the Bregman projection (11) can be leveraged to compute the proximal update (12). In particular, our algorithms leverage the fact that Bregman projections can be computed efficiently in a cyclic manner  X  X hat is, by decomposing the constraint set C =  X  i C i into an intersection of simpler constraint sets, and then performing a sequence of projections onto these simple constraint sets (Censor &amp; Zenios, 1997).
 To simplify notation, for any Bregman function f , let us define the operator and for any Bregman divergence D with Bregman function f and any convex set C , define the projec-tion operator With this notation, we can write the proximal update in a compact manner as a type of projection Now consider a decomposition of the constraint set as an intersection X  X ay L ( G ) =  X  T k =1 L k ( G ). By the method of cyclic Bregman projections (Censor &amp; manner, by performing the sequence of projections onto constraint set L i (  X  ) ( G ), where i (  X  ) =  X  mod T , for instance. This procedure is summarized in Algo-rithm 1.
 Algorithm 1 Basic proximal-Bregman LP solver Given a Bregman distance D , weight sequence {  X  n } and problem parameters  X  :  X  Outer loop: For iterations n = 0 , 1 , 2 , . . . , As shown in the following sections, by using a de-composition of L ( G ) over the edges of the graph, the inner loop steps correspond to local message-passing updates, slightly different in nature depend-ing on the choice of Bregman distance. Iterating the inner and outer loops yields a provably convergent message-passing algorithm for the LP. Convergence follows from the convergence properties of proximal minimization (Bertsekas &amp; Tsitsiklis, 1997), combined with convergence guarantees for cyclic Bregman pro-jections (Censor &amp; Zenios, 1997). In the following section, we derive the message-passing updates corre-sponding to various Bregman functions of interest. We also give rates of convergence for the cyclic projection updates in the inner loop. 3.3. Quadratic Projections Consider the proximal sequence with the quadratic distance Q from equation (6); the Bregman function inducing this distance is the quadratic function f ( y ) = 1 2 y 2 , whose gradient is given by  X  f ( y ) = y . The Map  X  = J f ( ,  X  X  ) : In this case, it can be derived as, whence we get the initialization in Equation 18. The Projections n, X  +1 =  X  Q ( n, X  , L i ( G )) : onto the individual constraints L i ( G ); the associated local update takes the form Consider the edge marginalization constraint for edge ( s, t ), L i ( G )  X  the dual (Lagrange) parameter corresponding to the constraint by  X  st ( x s ), the KKT conditions for (15) are given by while the constraint itself gives Solving for  X  st ( x s ) yields equation (20). The node marginalization follows similarly, so that overall, we obtain message-passing algorithm (2) for the inner loop. 3.4. Entropic Projections Consider the proximal sequence with the Kullback-Leibler distance D ( k  X  ) defined in equation (8); the Bregman function inducing the distance is a sum of negative entropy functions f ( ) = log , and its gradient is given by  X  f ( ) = log( ) + 1 .
 The derivation of the updates mirrors the previ-ous section, and defering the details to a full-length version, we get the message passing algorithm (3) for the inner loop.
 There are also interesting similarities between our cor-responding dual updates and sum-product updates X  which are updates to the dual parameters X  X etails of which we defer to a full-length version of this paper due to lack of space. 3.5. Reweighted Entropy Projections The message passing updates here are  X  X eweighted X  versions of those in the previous section for the un-weighted entropy induced Kullback-Leibler divergence Algorithm 2 Quadratic Messages for n +1
Initialization: repeat until convergence proximal iterates.
 Initialization of Proximal Steps: Projections: The node normalization update re-mains the same as in the previous section, while the marginalization update changes as, A key practical issue in applying LP relaxation is how round the fractional solution; a standard approach is Algorithm 3 Entropic Messages for n +1
Initialization: repeat until convergence to round the node marginals to the nearest integer so-lution. However, in general, such rounding procedures need not always output the optimal integer configura-tion. An attractive feature of our proximal Bregman procedures is the existence of rounding schemes which, assuming that the LP relaxation is tight, can produce the LP integral optimum and certify that it is correct, even before the pseudomarginals converge to the LP solution. Here we describe two rounding schemes, and state the optimality certificate associated with each. Node-based Rounding: This method applies to of pseudomarginals at iteration n , define an integer configuration x n by choosing, for each vertex s  X  V , a value x n s  X  arg max x ing is edgewise-consistent if for all edges ( s, t )  X  E , we have ( x n s , x n t )  X  arg max Tree-based Rounding: We describe this method in application to the unweighted entropic proximal up-dates. Let T 1 , . . . , T M be a set of spanning trees that cover the graph (meaning that each edge appears in at least one tree); for each edge ( s, t ), define the edge weight  X  st = 1 M i = 1 , . . . , M : (a) Define the tree-structured energy function E i ( x ) : (b) Run the ordinary max-product problem on en-Say that such a rounding is tree-consistent if the tree MAP solutions { x n ( T i ) , i = 1 , . . . , M } are all equal. The following result characterizes the optimality guar-antees associated with these rounding schemes: Theorem 1 (Rounding with optimality certificates) . At any iteration n = 1 , 2 , . . . , any edge-consistent con-figuration obtained from node-rounding, or any tree-consistent configuration obtained from tree-rounding is guaranteed to be MAP optimal for the original prob-lem.
 The proof is based on a certain energy-invariance prop-erty of the proximal updates; in particular, at any it-eration n , the pseudomarginals n have an associated function F ( x ; n ) which is proportional to the energy E (  X  ; x ) = ical model. For instance, for the entropic proximal scheme, at any iteration n , the function F ( x ; n ) : = to the exponential of E (  X  ; x ). (See the technical re-port (Ravikumar et al., 2008) for full details.) Both rounding schemes require relatively little compu-tation. Of course, the node-rounding scheme is purely local, and so trivial to implement. With reference to the tree-rounding scheme, many graphs can be cov-ered with a small number M of trees (e.g. M = 2 for grid graphs). Consequently, the tree-rounding scheme requires running the ordinary max-product al-gorithm twice, certainly more expensive than node-rounding but doable. In practice, we find that tree-rounding tends to find LP optima more quickly than node rounding. The convergence of our message passing updates fol-lows from two sets of results: (a) convergence of prox-imal algorithms (Bertsekas &amp; Tsitsiklis, 1997) and (b) convergence of cyclic Bregman projections (Censor &amp; Zenios, 1997). Our outer loop is a proximal algorithm; which has been well-studied in the optimization liter-ature. A sequence ( t ) is said to have superlinear con-Note that such convergence is faster than a multi-Bertsekas and Tsistiklis (1997) show that a proximal algorithm with a quadratic proximity has a superlin-ear convergence under mild conditions, whereas Iusem and Teboulle (1995) show the same for the entropy proximity. Under the assumption that inner loops are solved exactly, these convergence results then show that our outer iterates converge superlinearly. Our in-ner loop message updates use cyclic Bregman projec-tions; Censor and Zenios (1997) show that with dual feasibility correction, projections onto general convex sets are convergent. For Euclidean (quadratic) projec-tions onto linear constraints (half-spaces), Deutsch et al. (2006) establish a geometric rate of convergence, dependent on angles between the half-spaces. The in-tuition is that the more orthogonal the half-spaces are, the faster the convergence; for instance, a single it-eration suffices for completely orthogonal constraints. Our inner updates thus converge geometrically to an  X   X  suboptimal solution for any outer proximal step. As noted earlier, the proximal convergence results as-sume that the inner loop has been solved exactly, while the Bregman projection results yield geometric conver-gence to but an  X   X  suboptimal solution. While with  X  small enough, e.g. 10  X  6 as in our experiments, this issue might not be practically relevant, there has been some recent work, e.g. (Solodov &amp; Svaiter, 2001), showing that under mild conditions, superlinear rates still hold for  X   X  suboptimal proximal iterates. We performed experiments on a 4-nearest neighbor grid graphs with sizes varying from p = 100 to p = 900, in all cases using models with 5 labels. The edge potentials were set to Potts functions,  X  st ( x s , x t ) =  X  st I [ x s = x t ], which penalize disagreement of labels by  X  st . The Potts weights on edges  X  st were chosen randomly as Uniform(  X  1 , +1), while the node poten-tials  X  s ( x s ) were set as Uniform(  X  SNR , SNR), where the parameter SNR  X  0 controls the ratio of node to edge strengths, and thus corresponds roughly to a signal-to-noise ratio.
 Figure 1 shows plots of the logarithmic distance be-for grids of different sizes. In all cases, note how the curves have an inverted quadratic shape, correspond-ing to superlinear convergence.
 Figure 2 shows the fraction of edges for which the node-based rounding is edgewise inconsistent for grids of different sizes. Note how the fraction converges to zero in a small number of iterations. Figure 3 shows the fraction of the energy of the rounded solution to the energy of the MAP optimum, or the suboptimality factor. Note again, the small number of iterations for convergence. In this paper, we have developed distributed algo-rithms, based on the notion of proximal sequences, for solving graph-structured linear programming (LP) re-laxations. Our methods respect the graph structure, and so can be scaled to large problems, and they ex-hibit a superlinear rate of convergence. We also devel-oped rounding schemes that can be used to generate integral solutions along with a certificate of optimality. These optimality certificates allow the algorithm to be terminated in a finite number of iterations. The structure of our algorithms naturally lends it-self to incorporating additional constraints, both linear and other types of conic constraints. It would be inter-esting to develop an adaptive version of our algorithm, which selectively incorporated new constraints as nec-essary, and then used the same proximal schemes to minimize the new conic program.
 Acknowledgements Work supported by NSF grants CCF-0545862 and DMS-0528488. We thank the anonymous reviewers for helpful comments.

