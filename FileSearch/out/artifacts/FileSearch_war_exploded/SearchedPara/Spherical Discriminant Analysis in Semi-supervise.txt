 Speaker clustering is a critical part of speaker di-arization (a.k.a. speaker segmentation and cluster-ing) (Barras et al., 2006; Tranter and Reynolds, 2006; Wooters and Huijbregts, 2007; Han et al., 2008). Unlike speaker recognition, where we have the training data of a set of known speakers and thus recognition can be done supervised, speaker cluster-ing is usually performed in a completely unsuper-vised manner. The output of speaker clustering is the internal labels relative to a dataset rather than real speaker identities. An interesting question is: Can we do semi-supervised speaker clustering? That is, can we make use of any available information that can be helpful to speaker clustering?
Our answer to this question is positive. Here, semi-supervision refers to the use of our prior knowledge of speakers in general to assist the un-supervised speaker clustering process. In the form of an independent training set, the prior knowledge helps us learn a speaker-discriminative feature trans-formation, a universal speaker prior model, and a discriminative speaker subspace, or equivalently a speaker-discriminative distance metric. A general pipeline of speaker clustering consists of four essential elements, namely feature extrac-tion, utterance representation, distance metric, and clustering. We incorporate our prior knowledge of speakers into the various stages of this pipeline through an independent training set. 2.1 Feature Extraction The most popular speech features are spectrum-based acoustic features such as mel-frequency cep-stral coefficients (MFCCs) and perceptual linear pre-dictive (PLP) coefficients. In order to account for the dynamics of spectrum changes over time, the basic acoustic features are often supplemented by their first and second derivatives. We pursue a dif-ferent avenue in which we augment the basic acous-tic features of every frame with those of the neigh-boring frames. Specifically, the acoustic features of the current frame and those of the K L frames to the left and K R frames to the right are con-catenated to form a high-dimensional feature vec-tor. In the context-expanded feature vector space, we learn a speaker-discriminative feature transforma-tion by linear discriminant analysis (LDA) based on the known speaker labels of the independent training set. The resulting low-dimensional feature subspace is expected to provide optimal speaker separability. 2.2 Utterance Representation Deviating from the mainstream  X  X ag of acoustic fea-tures X  representation where the extracted acoustic features are represented by a statistical model such as a Gaussian mixture model (GMM), we adopt the GMM mean supervector representation which has emerged in the speaker recognition area (Campbell et al., 2006). Such representation is obtained by maximum a posteriori (MAP) adapting a universal background model (UBM), which has been finely trained with all the data in the training set, to a particular utterance. The component means of the adapted GMM are stacked to form a column vector conventionally called a GMM mean supervector. In this way, we are allowed to represent an utterance as a point in a high-dimensional space where tra-ditional distance metrics and clustering techniques can be naturally applied. The UBM, which can be deemed as a universal speaker prior model inferred from the independent training set, imposes generic speaker constraints to the GMM mean supervector space. 2.3 Distance Metric In the GMM mean supervector space, a naturally arising distance metric is the Euclidean distance metric. However, it is observed that the supervec-tors show strong directional scattering patterns. The directions of the data points seem to be more indica-tive than their magnitudes. This observation moti-vates us to favor the cosine distance metric over the Euclidean distance metric for speaker clustering.
Although the cosine distance metric can be used in the GMM mean supervector space, it is optimal only if the data points are uniformly spread in all di-rections in the entire space. In a high-dimensional space, most often the data lies in or near a low-dimensional manifold or subspace. It is advanta-geous to learn an optimal distance metric from the data directly.

The general cosine distance between two data points x and y can be defined and manipulated as follows. d ( x , y ) = 1  X  x T A y  X 
The general cosine distance can be casted as the cosine distance between two transformed data points W learning an optimal distance metric is equivalent to learning an optimal linear subspace of the original high-dimensional space. Most existing linear subspace learning techniques (e.g. PCA and LDA) are based on the Euclidean distance metric. In the GMM mean supervector space, we seek to perform discriminant analysis in the cosine distance metric space. We coin the phrase  X  X pherical discriminant analysis X  to denote discrim-inant analysis on the unit hypersphere. We define a projection from a d -dimensional hypersphere to a d
We note that such a projection is nonlinear. How-ever, under two mild conditions, this projection can be linearized. One is that the objective function for learning the projection only involves the cosine dis-tance. The other is that only the cosine distance is used in the projected space. In this case, the norm of the projected vector y has no impact on the objective function and distance computation in the projected space. Thus, the denominator term of Equation 2 can be safely dropped, leading to a linear projection. 3.1 Formulation The goal of SDA is to seek a linear transformation W such that the average within-class cosine similar-ity of the projected data set is maximized while the average between-class cosine similarity of the pro-jected data set is minimized. Assuming that there are c classes, the average within-class cosine similarity can be written in terms of the unknown projection matrix W and the original data points x
S where | D i | denotes the number of data points in the i sine similarity can be written in terms of W and x S where | D m | and | D n | denote the number of data
The SDA criterion is to maximize S W while min-imizing S B
Our SDA formulation is similar to the work of Ma et al. (2007). However, we solve it efficiently in a general dimensionality reduction framework known as graph embedding (Yan et al., 2007). 3.2 Graph Embedding Solution In graph embedding, a weighted graph with vertex set X and similarity matrix S is used to characterize certain statistical or geometrical properties of a data set. A vertex in X represents a data point and an entry s ij in S represents the similarity between the data points x i and x j . For a specific dimensional-ity reduction algorithm, there may exist two graphs. The intrinsic graph { X, S ( i ) } characterizes the data properties that the algorithm aims to preserve and the penalty graph { X, S ( p ) } characterizes the data properties that the algorithm aims to avoid. The goal of graph embedding is to represent each vertex in X as a low dimensional vector that preserves the simi-larities in S . The objective function is where f ( x , W ) is a general projection with param-eters W . If we take the projection to be of the form in Equation 2, the objective function becomes
It is shown that the solution to the graph embed-ding problem of Equation 7 may be obtained by a steepest descent algorithm (Fu et al., 2008). If we expand the L 2 norm terms of Equation 7, it is straightforward to show that Equation 7 is equiva-lent to Equation 5 provided that the graph weights are set to proper values, as follows. s s
That is, by assigning appropriate values to the weights of the intrinsic and penalty graphs, the SDA optimization problem in Equation 5 can be solved within the elegant graph embedding framework. Our speaker clustering experiments are based on a test set of 630 speakers and 19024 utterances se-lected from the GALE database (Chu et al., 2008), which contains about 1900 hours of broadcasting news speech data collected from various TV pro-grams. An independent training set of 498 speak-ers and 18327 utterances is also selected from the GALE database. In either data set, there are an aver-age of 30-40 utterances per speaker and the average duration of the utterances is about 3-4 seconds. Note that there are no overlapping speakers in the two data sets  X  speakers in the test set are not present in the independent training set.

The acoustic features are 13 basic PLP features with cepstrum mean subtraction. In computing the LDA feature transformation using the independent training set, K L and K R are both set to 4, and the di-mensionality of the low-dimensional feature space is set to 40. The entire independent training set is used to train a UBM via the EM algorithm, and a GMM mean supervector is obtained for every utterance in the test set via MAP adaptation. The trained UBM has 64 mixture components. Thus, the dimension of the GMM mean supervectors is 2560.

We employ the hierarchical agglomerative clus-tering technique with the  X  X ard X  linkage method. Our experiments are carried out as follows. In each experiment, we perform 4 cases, each of which is as-sociated with a specific number of test speakers, i.e., 5, 10, 20, and 50, respectively. In each case, the corresponding number of speakers are drawn ran-domly from the test set, and all the utterances from the selected speakers are used for clustering. For each case, 100 trials are run, each of which involves a random draw of the test speakers, and the average of the clustering accuracies across the 100 trials is recorded.

First, we perform speaker clustering in the orig-inal GMM mean supervector space using the Eu-clidean distance metric and the cosine distance met-ric, respectively. The results indicate that the cosine distance metric consistently outperforms the Eu-clidean distance metric. Next, we perform speaker clustering in the reduced-dimensional subspaces us-ing the eigenvoice (PCA) and fishervoice (LDA) approaches, respectively. The results show that the fishervoice approach significantly outperforms the eigenvoice approach in all cases. Finally, we perform speaker clustering in the SDA subspace. The results demonstrate that in the SDA subspace, speaker clustering yields superior performance than that in other reduced-dimensional subspaces (e.g., PCA and LDA). Table 1 presents these results. This paper proposes semi-supervised speaker clus-tering in which we learn a speaker-discriminative feature transformation, a universal speaker prior Metric Subspace 5 10 20 50 model, and a speaker-discriminative distance metric through an independent training set. Motivated by the directional scattering patterns of the GMM mean supervectors, we peroform discriminant analysis on the unit hypersphere rather than in the Euclidean space, leading to a novel dimensionality reduction technique  X  X DA X . Our experiment results indicate that in the SDA subspace, speaker clustering yields superior performance than that in other reduced-dimensional subspaces (e.g., PCA and LDA).

