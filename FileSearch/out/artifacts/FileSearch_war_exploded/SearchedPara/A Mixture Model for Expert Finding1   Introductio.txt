 Expert finding, aiming at answering the question:  X  X ho are experts on topic X? X , is becoming one of the biggest challenges fo r information management [15]. Recent years, expert finding has attracted much attention due to the rapid flourish of the Web 2.0 applications and the advancement of information retrieval technologies from the traditional document-level to the object-level [20]. Many challenging questions arise, for example, How to find the most appropriate collaborators for a project? How to find the important scientists on a research topic? How to find an expertise consultant? 
Much research work has been done to deal with the challenges. For example, [2][21] propose using conventional language models for finding experts from an en-terprise corpora or a domain-specific document collection. TREC has provided a common platform for researchers to empirically assess methods and techniques de-from the list of candidate names for each of these topics. 
Previously, the language model like method or information retrieval based method combining relevance scores between the query and different support documents re-lated to each expert candidate. Based on th e combination methods, the approach can be again classified into two categories:  X  composite  X  and  X  hybrid  X . Composite combines different support documents into a single formula (cf. Section 3 for details of the two methods). However, preliminary experiments show that simply applying these two categories of models on the task of expert finding does not achieve satisfactory re-sults. In traditional IR models, documents are taken as the retrieval units and the con-tent of documents are considered reliable. However, the reliability assumption is no longer valid in the expert finding context. This is because: (1) Composite model (cf. Section 3.2.1) suffers from the limitation that all the (2) Hybrid model (cf. Section 3.2.2) is a bit more flexible. However, it still re-
The language model-based methods are lexical-level and suffer from lacking se-mantics. A question, thus, arises:  X  X an we search for experts in a semantic-level? X . 
In this paper, we focus on the above problems. We propose a mixture model based on Probabilistic Latent Semantic Analysis (PLSA) [16] for the expert finding task. In this model, we do not model the relevance between a query and a document directly. Instead, we propose to use a hidden theme layer to model the semantic relations be-tween the query and the support documents of candidate experts. In this way, an ex-pert whose support documents associated with the same themes as that of a query can be ranked higher, although they may not co ntain the query terms. We evaluated the proposed approach in ArnetMiner system. We compared our appr oach with the tradi-tional language models for expert finding. We also carried out the comparison with several existing systems. Experimental resu lts show that our proposed approach per-forms better than the baseline methods and also outperforms the existing systems. 
Our contributions in this paper include: (a) formalization of the expert finding problem in a semantic-level, (b) proposal of a mixture model to the problem based on Probabilistic Latent Semantic Analysis (PLSA), and (c) empirical verification of the effectiveness of the proposed approach. To the best of our knowledge, no previous work has been done on a semantic-level model for expert finding. 
The rest of the paper is organized as follows. In Section 2, we formalize the task of expert finding. In Section 3, we briefly introduce the language model and propose our mixture model for expert finding. In Section 4, we give the experimental results and in Section 5, we introduce the related work. We conclude the paper in Section 6. We denote a candidate expert as e and a query as q . A general process of expert find-P ( e | q ), and then return the experts with the highest probabilities on the top. 
Based on the Bayes rule, we can obtain the following formula: usually viewed as uniform and thus can be ignored. The probability P ( e ) reflects the query-independent expertise. A variety of techniques can be used to compute P ( e ), for example, we can simply use the number of one X  X  publications to estimate the probability; more complicated, we can calculate it by using a propagation scheme like the state-of-the-art PageRank algorithm. Also, some work assumes it uniformly and only focuses on estimating the probability P ( q | e ) using language models [2] [21]. 
Figure 1 shows an example of expert finding. The left part of the figure gives three queries:  X  X emantic web X ,  X  X achine learning X , and  X  X atural language processing X  and the right part of the figure show s the found experts for each query. several existing language models for expe rt finding, namely a hybrid model and a composite model. Finally, we propose a mixture model for finding experts. 3.1 Language Models for Document Retrieval In document retrieval, language model de scribes the relevance between a document and a query as the generating probability of the query from the document X  X  model: 
For a query q , we usually assume that terms appear independently in it, thus: from the language model of document d . A common method for estimating P ( t i | d ) is maximum likelihood estimation and Dirichlet smoothing [1], as follows: quency of term t i in D ;  X  is a parameter ranging in [0, 1] and is often set based on the document length in D . 3.2 Language Models for Expert Finding The simplest method to apply language mode l for expert finding is to merge all sup-then employ the language model described in Section 3.1 to estimate the relevance between the virtual document and the query . However, this model has obvious disad-vantages: it cannot differentiate the contributions of different support documents. Based on the consideration, two extended la nguage models have been proposed (we call them as composite language model and hybrid language model). 3.2.1 Composite Language Model Let D e ={ d j } denotes the collection of support documents related to a candidate e . In the composite language model, each support document d j is viewed as a unit and the estimations of all the documents of a candidate e are combined. We have: 
The model consists of two components: 1) a document that is related to a candidate (3) and (4) based on the independent assumption. Finally, we obtain: 
We call this model as composite model becau se it first integrates the probability of document d j generating each term t i and then combines the different document models together. The nature of the composite model is that it views documents as a  X  X idden X  variable separating the query from a candidate such that the candidate is not directly modeled. It is based on the assumption that terms are independent in d j . Accordingly, the model emphasizes the co-o ccurrence of all the query te rms in the same document query  X  X emantic web X . However, it does not work well for the other two queries  X  X a-chine learning X  and  X  X atural language processing X . 3.2.2 Hybrid Language Model The hybrid language model (cf. Equation (7)) is similar to the composite model, ex-and then uses a language model to integrate them together. 
The two models are not equivalent mathematically since the product and the sum information from all documents associated with the given candidate and models the candidate directly. It is based on the assump tion that terms are independent in all sup-port documents of e . Thus the model does not care much about the co-occurrence of the query terms in the same support document [2] [21].As for the example in figure 1, the hybrid model works well for both the queries  X  X emantic web X  and  X  X achine learn-ing X , as the query terms appear in the support documents of experts. Unfortunately, it cannot find the two expe rts for  X  X atural language processing X  because it is still based on lexical-level rele vance assumption. 3.3 A Mixture Model for Expert Finding We propose a mixture model for expert finding. We assume that there is a hidden hidden theme  X  m is semantically associated with multiple queries and support docu-ments. Similarly, each support document or query is also associated with multiple directly model the relevance between them. Instead, we use the hidden themes associ-ated to them as the bridge to model th e relevance. More accurately, we have: Here, P ( q |  X  m ) denotes the probability of generating a query given a theme and P (  X  m | d ) denotes the probability of generating a theme given a document. 
We assume that a query q and a document d are conditional independent given a theme  X  m . Then the problem becomes, for each document, how to estimate the prob-parameter estimation. Following we introduce the method for parameter estimation. 
Let T as all terms occurring in the whole document collection D . Suppose there are k hidden themes. The generative process of the data set can be described as: (1) Select a document d with probability P ( d ); (2) Pick a latent theme  X  m with probability P (  X  m | d ); (3) Generate a term t with probability P ( t |  X  m ). As a result, we obtain an observed pair ( t , d ) without  X  m . 
The above generative process can be expressed as a joint probability model: 
Equation (9) sums over all  X  m from which the observations could have been gener-ated, which is based on the assumption that t and d are conditional independent on  X  m . We use Bayes X  formula to transform Equation (9) to get its symmetric form: P (  X  m ) by maximizing of the log-likelihood function: where n ( d , t ) denotes the co-occurrence times of d and t . 
We use Expectation-Maximization (EM) algorithm [5] to estimate the maximum P (  X  m ) and runs an iterative process to obtain new values based on updating formulas. The update formulas contain expectation (E) step and maximization (M) step. 
In E-Step, we aim to compute the posterior probability of latent theme  X  m , based on the current estimates of the parameters: 
In M-Step, we aim to maximize the expectation of the log-likelihood of Equation (11). By introducing Lagrange multipliers and solving partial derivative, we can ob-tain the following equations for re-estimated parameters: 
The E-step and M-step run iteratively until the log-likelihood function converges to a local maximum. Then we obtain the parameters: P ( t |  X  m ), P ( d |  X  m ), and P (  X  m ). 3.4 Find Experts Using the Model We can make inferences based on the estimated probabilities. Given a query, the probability P ( q |  X  m ) can be estimated by 
Then Equation (8) can be rewritten as: 
Therefore we obtain Equation (18) by substituting P ( q | d j ) into Equation (5): where P (  X  m |d j ) can be estimated by Bayes X  formula: However, we have found that final results sometimes are sensitive to the probability. In this work, we employ the propagation approach we have proposed in [25] to esti-mate P ( e ). The approach is based on the social relationship analysis. The basic idea is that if a person knows many experts on a topic or if the person X  X  name co-occurs accordingly. perimental results. Finally we give some discussions. 4.1 Experimental Setting We evaluate the work in the context of ArnetMiner[22]. ArnetMiner contains 448,289 researchers and 725,655 publications extracted from the Web database, pages, and files. As performing PLSA on the full data collection will take an extreme long time, we created a subset of the data for evaluati on purpose. Specifically, we first selected the most frequent queries from the log of ArnetMiner (by removing the specific que-ries or too long queries, e.g.,  X  X  convergen t solution to tensor subspace learning X ). We obtained seven queries:  X  X nformation extraction X  (IE),  X  X achine learning X  (ML),  X  X e-mantic web X  (SW),  X  X atural language processing X  (NLP),  X  X upport vector machine X  (SVM),  X  X lanning X  (PL), and  X  X ntelligent agents X  (IA). Next, for each query, we gath-ered the top 30 persons from Libra author search , Rexa authors search, and Arnet-Miner 1 . We merged all the persons together by removing ambiguous names (e.g., L. Liu) and names that do not exist in ArnetMiner. Finally we got 421 person names. We collected 14,550 publications of the 421 persons from ArnetMiner as the support document collection. 
For evaluation, it is difficult to find a standard data set as the ground truth. As a re-sult, we use the method of pooled relevance judgments [8] together with human above three systems (Libra, Rexa, and ArnerMiner) into a single list. Then, one fac-ulty and two graduates, from the authors X  lab, provided human judgments. Assess-ments were carried out mainly in terms of how many publications he/she has published, how many publications are related to the given query, how many top con-ference papers he/she has published, what distinguished awards he/she has been awarded. Finally, the judgment scores were averaged to construct the final ground truth. The data set is available on line. We conducted evaluation in term s of P@5, P@10, P@20, P@30, R -prec, Mean Average Precision ( MAP ) and P-R curve [8] [10]. 
We used the language models introduced in Section 3.2 as baselines. Hereafter, we respectively call them CM and HM. For comp arison purpose, we also report the re-sults obtained by Libra and Rexa. 
We implemented our proposed model (shortly MM) in two stages. In the first stage, we use PLSA algorithm (equations (12)-(15)) to estimate the probabilities note publications. Terms are extracted from the titles and conference names of the publications after word segmentation and stop words filtering. We empirically set the the second stage, we rank experts using equation (18) for each query. 4.2 Experimental Results of Expert Finding Table 1 shows the performances on the 7 queries by our proposed model, the two lan-guage models, and the two systems (Libra and Rexa). Figure 2 shows the average 11-point precision recall curves on the 7 queries for the different approaches. We see that in terms of most of the measures, the proposed model outperforms the two baseline language models. We also present top 9 example experts for  X  X atural language proc-essing X  ranked by different approaches in Table 2. 4.3 Discussions (1) Improvements Over Baselines. Our proposed model outperforms the two lan-guage models in terms of P@5, P@10 and MAP . From the PR curve, we can also see that our model outperforms the language models in most of the 11 points, which con-firms the effectiveness of our approach. The proposed model can retrieve experts whose support documents do not contain the query terms but  X  X emantically X  related to the query, therefore our approach can impr ove the performance significantly. For ex-ample, in Table 2, our model MM ranks higher for  X  X aymond J. Mooney X  than the language models. This is because many of Mooney X  X  papers do not exactly contain the query terms although they are related to  X  X atural language processing X . We rank higher for  X  X an Roth X  and  X  X ragomir R. Radev X  due to the similar reason. (2) Effect of the Number of Themes. The best number of themes is difficult to de-termine. In our experiment, we tried to t une the parameter to get better performance. As Figure 3 shows, the number of themes systematically varies from 10 to 100 with interval 10 and from 100 to 1000 with interval 100. In general, the best results were obtained when setting the number of themes as 300. 
An intuitive explanation to Figure 3 is that when the number of theme is small, the estimated mixture model prefers to very gene ral queries; with the number increasing, the model prefers to specific queries. The number 300 seems to be a best balance in #theme=10 and #theme=300. (3) Language Models. We also analyze the retrieval results of two language models. From table 1, we see that for queries  X  X  W X ,  X  X E X  and  X  X VM X , CM performs better than HM, because the word  X  X eb X ,  X  X nfor mation X  and  X  X achine X  may slightly drive the topic of documents drift away when using HM. For the queries of  X  X L X ,  X  X A X ,  X  X L X , and  X  X LP X , HM performs better than CM, due to the limitation in CM that all the query terms should co-occur in one document. (4) Decline Over Baselines. In terms of p@20, p@30 and R -prec, we must note that our model underperforms the two language models. The reason lies in that our model P (  X  m ) in the first stage. 5.1 Language Model for Expert Finding With the launch of expert finding task in TREC 2005, more and more researchers begin focusing on the research topic. Previous work for expert finding usually makes use of language models. For example, Cao et al. [9] propose a two-stage language model which combines a co-occurrence model to retrieve documents given a query, and a relevance model to find experts in those documents. Balog et al. [2] propose a model which models candidate using suppor t documents directly and another model sparse data environments and proposes se veral advanced models based on the are probabilistically equivalent and the differences lie in the independent assumptions. As far as we know, expert finding by using latent semantic analysis has not been in-vestigated previously. 5.2 Probabilistic Latent Semantic Analysis and Its Applications The idea of using latent semantic structure in information retrieval traces back to [13]. They propose latent semantic analysis (LSA) method, which is mostly used in auto-matic indexing and information retrieval [4]. The main idea is to map data using Sin-gular Value Decomposition (SVD) from a high-dimensional vector space repre sentation to a reduced lower representati on, also called latent semantic space. 
A new approach to discover latent variables is Probabilistic latent semantic analy-sis (PLSA) proposed by Thomas Hofmann [16]. The difference between LSA and The core of PLSA is a statistical model called aspect model, which assumes there exists a set of hidden factors underlying the co-occurrences among two sets of ob-jects. Expectation Maximization (EM) algorithm [5] is used to estimate the probabili-ties of the hidden factors generating the two sets of objects. 
Probabilistic Latent Semantic Analysis has been used to solve problems in a vari-ety of applications on account of its flexibility. Such applications include information retrieval [16], text learning and mining [6] [7] [14] [18] [24], co-citation analysis [11] [12], social annotation analysis [23], web usage mining [17] and personalize web search [19]. In this paper, we have proposed a mixture model for expert finding. We assume that there is a latent theme layers between te rms and documents and employ the themes to help discover semantically related expert s to a given query. A EM based algorithm has been employed for parameter estimation in the proposed model. Experimental results on real data show that our proposed model can achieve better performances than the conventional language models. As future work, we plan to investigate how to automatically determine the number of themes based on the input query. 
