 When labeled examples are limited and di ffi cult to obtain, trans-fer learning employs knowledge from a source domain to improve learning accuracy in the target domain. However, the assump-tion made by existing approaches, that the marginal and condi-tional probabilities are directly related between source and target domains, has limited applicability in either the original space or its linear transformations. To solve this problem, we propose an adap-tive kernel approach that maps the marginal distribution of target-domain and source-domain data into a common kernel space, and utilize a sample selection strateg y to draw conditional probabili-ties between the two domains closer. We formally show that under the kernel-mapping space, the di ff erence in distributions between the two domains is bounded; and the prediction error of the pro-posed approach can also be bounded. Experimental results demon-strate that the proposed method outperforms both traditional induc-tive classifiers and the state-of-the-art boosting-based transfer al-gorithms on most domains, including text categorization and web page ratings. In particular, it can achieve around 10% higher accu-racy than other approaches for the text categorization problem. The source code and datasets are available from the authors. H.2.8 [ Database Management ]: Database Applications  X  Data Mining Algorithms
It is expensive or impractical for many applications to obtain large number of labeled examples. When this happens, most in-ductive learners perform poorly. The idea of transfer learning is to borrow labeled examples from a source-domain to improve learn-tribution of target-domain and source-domain respectively. Super-vised transfer learning is to use small number of labeled example target-domain. The main challenge is to identify regions in P ( x either in original space or its transformation, where P t P ( x , y ) are similar and knowledge can be transferred.
Much work has been proposed to solve transfer learning problem assumes that conditional probabilities r t ( y | x )and r lar in regions of the latent space where marginal distribution q and q s ( x ) of corresponding examples are close. Other works, such as [12], assumes q ( x ) is related to r ( y | x ). They both implicitly assume that marginal distribution and conditional probability are directly related. In summary, either of the following is assumed to be true and adopted to design transfer learning strategies: (1) where marginal distribution q ( x ) of target-domain and source-domain are similar, conditional probability r ( y | x ) also ought to be similar, or (2) vice versa. However, those assumptions may be too strict to be practical. For some problems, both the marginal and conditional distributions between target-domain and source-domain could be significantly di ff erent. When this happens, neither of the two as-sumptions is true anymore, in either the original space, scaled space or latent space using linear transformation.

However, non-linear transformation, such as the kernel manipu-lation, can make these assumptions plausible. A suitable kernel is able to map the input space into a convenient feature space where a linear boundary can be easily found [3]. Importantly, this also sheds light on transfer learning. First, a suitable kernel, such as Gaussian kernel [13], can make di ff erent input data form similar marginal distribution in the kernel space. Second, in the kernel space, some examples have very si milar conditiona l probabilities and can be used to construct transfer learning models. Third, the er-ror rate of the transfer classifier can be bounded (Section 3). Thus, under a suitable kernel mapping space, two domains significantly di ff erent in their original space can both have similar marginal and conditional distributions, leading to e ff ective knowledge transfer. Consider a synthetic example in Figure 1 where /  X  denotes pos-itive / negative. Figure 1(a) plots the target-domain data,  X  X wo cir-cles X , and the maximal margin decision boundary is the dashed el-lipse. Figure 1(b) shows a source-domain data set,  X  X wo moons X , where the decision boundary is the dashed curve. Obviously, two moons and two circles have significantly di ff erent distributions in the original space. However, after we map them into the kernel Figure 1: The /  X  refers to positive / negative. The  X   X  denotes examples with similar r ( y | x ) . space (details in Section 2.1), the marginal distributions become close and samples from two domains are both in cloud-like appear-ance, as shown in Figure 1(c) and (d). In the kernel space, though conditional probabilities between source and target domains are still di ff erent, those examples highlighted by  X   X  in (d) have similar conditional distribution as the target-domain. In other words, these examples lie on the same region as those target-domain data with corresponding labels.

The observation is, if one can find a proper kernel as the bridge, the marginal distributions can be made reasonably close. Those source-domain examples with similar conditional probabilities as target-domain data can be selected to build transfer learning mod-els. As follows, we propose an iterative framework to transfer knowledge based on kernel mapping. In each iteration, we perform kernel mapping followed by sample selection of source-domain data with similar distribution to target-domain, and then use these examples to build transfer classifier. Finally, we combine the clas-sifiers from di ff erent iterations to form an ensemble in order to re-move the bias due to any single mapping space.
We introduce the kernel-based feature mapping methods for trans-fer learning. For ease of discussion, the notations are summarized in Table 1. Suppose that labeled target-domain data L = { X contains instances, X L = { x 1 ,..., x } , Y L = { y 1 ,..., ber of class labels is NC . For the unlabeled data, we set domain data O = { X O , Y O }has o instances. We assume the source-domain data has the same class labels as the target-domain data. And the number of classes is NC . As follows, we first discuss how to perform kernel-based feature mapping through Kernel Dis-criminant Analysis(KDA) to make the marginal distributions q ( x ) from two domains similar, then discuss the cluster criterion to se-lect source-domain data whose conditional probabilities r ( y likely to be similar to target-domain data within the mapping space. These are followed by the details of the proposed algorithms imple-mented in two di ff erent ways: KMapEnsemble (Kernel-based fea-ture Mapping with Ensemble) as well as KMapWeighted (Kernel-based feature Mapping with Weighted criterion).

We first show that how to employ KDA to find a proper map-ping space, perform feature mapping and then bridge two di ent distributions. In practice, we center the scatter matrix before we perform discriminant analysis, thus sample mean of X is zero or m s = 1 i = 1 x i = 0 ,where is the number of samples. Now, we define the within-class scatter matrix S W and the between-class scatter matrix S B as follows.
 Definition 1. Scatter matrix (1) (2) where m i is the mean of the samples of class i in X , i is the number of instances with class i . The objective of discriminant analysis, within-class scatter matrix S W , and maximize the between-class scatter matrix S B simultaneously. In particular, for a two class problem, the solution of the objective is the same as: (3) S W v = m 1  X  m 2 where m 1 and m 2 are the means of the two classes.

As an important step, we show how to perform discriminant anal-ysis in the Gaussian RKHS (or Reproducing Kernel Hilbert Space) to find a proper kernel feature mapping between source and target domains. Let  X  : x  X  X  beafunctiontomapthedatafromthe original space to RKHS. We use the inner product to avoid explicit orem [19], let x ij is the j th instance of class i and then any solution  X  ( v )  X  X  can be presented as: (4)  X  ( v ) = where  X  ij is the j th component in  X  i = {  X  i 1 ,..., X  i coe ffi cient of the vector  X  ( v ) for the class i . So, the discriminant analysis object in kernel space can be written as ([3]) (5) max  X  {  X  = ( where W = ( W i ) i = 1 ,..., NC is a block diagonal matrix, and W (  X  an eigenvalue decomposition problem. First, we use the eigenvec-tors decomposition of the matrix K and obtain K = P  X  P T ,where  X  is a diagonal matrix of non-zero eigenvalues and P is a matrix of normalized eigenvectors associated to  X  . Thus, by substituting K in Equation Eq.(5), we get: (6) max  X  {  X  = ( where  X  = X  P T  X  .As P is orthogonal, Equation Eq.(6) can be simplified, and the solutions of  X  are found by maximizing (7)  X  X  = P T WP  X  Thus the first step of the system resolution consists in finding accordance with Equation Eq.(7), which corresponds to a classical eigenvector system resolution. Once  X  are calculated, we compute  X  . Details of the decomposition process is standard and can be found in [3]. Then the projection can be obtained to perform feature mapping between target-domain and source-domain. When a new instance z comes, we get its projection as: (8) v  X  ( z ) = where x ij is the j th instance of class i . In this paper, we use the Gaussian kernel and set the kernel-width using a heuristic strategy defined in [24]. In Section 3, we discuss how the di ff erence in marginal distribution between target-domain and source-domain is bounded by the kernel mapping process.
KDA, as described above, is to learn a feature mapping space in order to make the marginal distribution between two domains similar. However, the conditional probabilities r ( y | x ) between two domains may be still di ff erent. Thus, in the following, under the mapping space, we propose a  X  X luster-based criterion X  to select those source-domain data whose conditional probability is similar to target-domain. To achieve this, we introduce a cluster method, Bisecting K-means, with a self-adapting modification as adopted from [16]. We propose two criteria to control self-adapting cluster process. If the sum of squared error of two sub-clusters is smaller than that of the whole cluster, then the cluster is to be splitted into two sub-clusters. In addition, data in the same cluster should mostly belong to the same class, i.e.,  X  X urity X  of the cluster is high. Formal definitions are as follows:
Definition 2. Given a cluster C and its two sub-clusters C where C 1  X  C 2 = C and C 1  X  C 2 = .Then (9) Par ( C , C 1 , C 2 ) = SSE ( C )  X  where x returns 1 if x is positive, 0 if not and SSE is sum of squared error. Given a cluster C i whose data are all labeled and labels in it are  X  +  X  X nd X - X  X hen (10) Purity ( C i ) = max(
The clustering procedure is summarized in Algorithm 1. It min-imizes the distance of data within the cluster and prefers clusters having mostly the same label. Based on the above description, we propose cluster-based example selection. We first cluster the data (both the labeled target-domain data and source-domain data) into clusters. Then, we set the  X  X abel of each cluster X  as the majority class of those labeled target-domain examples in this cluster. We define the label of cluster C i as CL i : (11) CL i = arg max Algorithm 1 Self-adapting Bisecting K-means Input: Labeled Data: L Output: Cluster Index of data in L : idx
C = L, idx = {1,...,1} for each C i in C do end for
Return cluster index of L where nc ij is the number of labeled target-domain points with label j in cluster C i . After that we only select those data from source-domain whose labels are the same as the corresponding cluster X  X  label. Formally, let SO denotes the selected source-domain data. Then we obtain SO  X  O and  X  x k  X  SO ,if x k  X  C i then y
Both kernel-based feature mapping and cluster-based example selection are adopted into two ensemble-based frameworks. As shown in Algorithm 2, KMapEnsemble uses an iterative procedure to generate multiple mapping and uses model averaging to compute the final prediction. In summary, the labeled target-domain data is first used to perform KDA and obtain an original mapping space. The marginal distribution q ( x ) of data from both domains are simi-lar inside this mapping space, according to the analysis in Section 3. At later iterations, we select those source-domain data on the ba-sis of the  X  X luster-based criterion X . Those source-domain data have similar conditional probabilities r ( y | x ) to the target-domain data. Afterwards, these selected source-domain data and labeled target-domain data are used to construct a new feature space using KDA, and train a new classifier. In order to take advantage of each in-dividual kernel space and avoid their individual bias, we combine their predictions with simple model averaging.

KMapWeighted takes advantage of a re-weighting scheme to use source-domain data, similar to that of TrAdaboost [8]. It first em-ploys KDA to obtain a new mapping feature space, and then adopts a weighting scheme based on training error to control the impacts of source-domain data. Weights for wrongly predicted source do-main data are reduced to weaken their impacts. Since their marginal distributions are similar to target-domain in kernel space, the pre-diction errors are obviously ascribed to di ff erent conditional distri-bution. The complete process is summarized in Algorithm 3. All the labeled data from target-domain and source-domain are used to learn the initial mapping space and train a base classifier. At later iterations, the weight of each labeled data is updated according to the training error. For the source-domain data, their contributions are controlled through multiplying the weights by  X  | C ( x viously when source-domain examples are misclassified, they are likely to conflict with target-domain data and their weights are re-duced as a result. We re-sample instances from the labeled data set in proportion to their weights. These sampled data are used to learn a new mapping space and new base model at the next iteration. The prediction results from each iterations are averaged.

Below is the analysis of computational complexity. Let m be the number of all instances of source-domain and target-domain, n the number of features and N be the number of iterations. For ker-nel mapping, we need O ( m 2 n f + m 3 ) to compute the mapping space and project data. For the sample selection, KMapEnsemble needs O ( m 2 n f ) complexity while KMapWeighted relies on the complex-ity of the base classifier. For example, if we use KNN, it needs O ( m 2 n f ). Thus, considering the iterations, the entire complexity of each approach is O (( m 2 n f + m 3 )  X  N ). Algorithm 2 KMapEnsemble Input: L , U , O ,max number of iteration: N , base classifier
Output: Predicted labels of U : Lo f U for i = 1 to N do end for
For each element j in Lo f U : Lo f U j = max k { ik con f Return Lo f U Algorithm 3 KMapWeighted Input: L , U , O ,max number of iteration N , base classifier
Output: Predicted labels of U : Lo f U of each instances as 1.0 for i = 1 to N do end for Lo f U j = 0, otherwise
Return Lo f U
A major challenge facing transfer learning is the di ff erence be-tween target and source distributions. The classic theory of learn-able does not apply, where training and test data must follow the same (but unknown) distribution. The di ff erence between training and test distributions can be classified into two types: covariate shift and functional relation change [22]. Covariate shift is related to the marginal distribution q ( x ), while functional relation is related to the conditional probability r ( y | x ).

Our approach to dealing with this problem is as follows. Theo-rem 1 establishes the convergence for our kernel discriminant anal-ysis. Theorem 2 and the analysis thereafter show that the selected source-domain data are similar to the target-domain data in that their marginal and conditional distributions are similar. Specif-ically, Theorem 2 states that the data in a Gaussian reproducing kernel Hilbert space (RKHS) are distributed approximately Gaus-sian under some suitable conditions . Thus, the source and target domain marginals in a Gaussian RKHS share a similar intrinsic ge-ometry. This knowledge of the marginals can be exploited for bet-ter transfer learning by assuming the conditionals r s ( y are not completely di ff erent, rather they are related. We then state that these similar data can be exploited for transfer learning under the  X  X luster-manifold X  assumption [4]. Theorem 3 provides an er-ror bound for the proposed transfer learning algorithm. Based on the bound, we show which situations can not be handled by the pro-posed approach. Finally, Theorem 4 shows the error rate estimation using our ensemble classifier.
As stated earlier, we pool the labeled target and source domain data to perform KDA and train classifiers. The following analysis establishes the error bound for discriminant analysis, which is sim-ilar to [23]. Based on the notations described in Section 2, let us consider the target mapping function f  X  : (12) f  X  = arg min Assume for the moment that our hypothesis space is linear. That is, H = { f | f ( x ) = v T x , x  X  X } . Consequently we have (13) v  X  = arg min where is the number of training instances.

The lemma below is for two class problems, but it can extend to multi-class problems as well.

Lemma 1. The solution derived from Equation Eq.(3) is equiva-lent to the one obtained from Equation Eq.(13).

P roof .Let X L be the labeled data set and Y L be the correspond-ing labels. First, we can rewrite Equation Eq.(13) as: (14) Optimize this objective respect to v , we obtain S m v = ( where 1 is the number of labeled instances in class 1, 2 is the one in class 2 and S m = S B + S W .When S W is full rank, this equation has the same solution as Equation Eq.(3) if the overall mean is 0. The detail can be found in [23].
 However, solving Equation Eq.(12) directly often leads to over-fitting the data. Instead, we minimize the following regularized functional for a positive parameter  X  in a hypothesis space (15) f + = arg min We note that the error bound for f + provides an error bound for the estimation (Equation Eq.(3)). The following theorem states for the bound for f + .

Theorem 1. Let f  X  is the best function that minimizes the mean squared error. That is (16) f  X  = arg min . With the confidence 1  X   X  , the error bound for the solution of Equation Eq.(15) is given by: (17) ( f +  X  f  X  ) 2 d  X  X  X  S (  X  ) + A (  X  ) L  X  1 / 4 represents a simple operator, M and C v ( ,  X  ) is the unique solution of an equation whose coe ffi contain sample size and confidence parameter  X  . Note that there exists an unique  X  that minimizes S(  X  ) + A(  X  ).
 A(  X  ) represents the approximation error, and S(  X  ) represents the sample error. They correspond to classic bias and variance trade-o ff . The detail of the proof can be found in [7]. The kernelized version of the theorem can be similarly derived.
First, we will show that under some suitable conditions, the data in a Gaussian RKHS induced by a kernel function are distributed approximately Gaussian [13]. Let  X  be the kernel window width. The kernel associated with Hilbert space H  X  is  X  . They are all de-pendent on the number of examples m . Also, let where  X  o denote the baseline kernel with window width 1. Suppose that there exits a constant  X  2 &gt; 0 such that for any &gt; (18) lim (19) lim Eq.(18) states that most kernel data have H m -norm near  X  Eq.(19) states that most kernel data are nearly orthogonal in
Theorem 2. Let h be a random element with zero mean function and covariance operator  X  m . If Equation Eq.(18) and Eq.(19) hold, as m  X  X  X  , the empirical distribution  X  m ( h ) converges weakly to The proof of this theorem can be found in [13]. Notice that the theorem is established for one dimensional projection. However, it can be extend to an n f -dimensional projection for an arbitrary but fixed n f . Also, one can observe that projected data at lower dimensions can be well approximated by Gaussian distributions. As shown in our approach, we use KDA to achieve this projection.
From the above theorem, we can see that both the target and source domain data follow Gaussian distributions in a Gaussian RKHS. That is, q t ( x ) = N ( 0 ,  X  t )and q s ( x ) = N ( 0 our approach to transfer learning selects for training those source domain data that are close to the target-domain data through clus-tering. A source-domain instance is close to a target-domain in-stance if they reside in the same cluster. Thus, it seems reasonable to assume that the selected source-domain data follow a Gaussian distribution with similar variance. That is, q so ( x ) = the generalized variance |  X  so | X  X   X  t | . This implies that the selected data have a similar marginal distribution to that of target-domain data in the induced space.

A recent work made a specific assumption about the relation be-tween the marginal and conditional distributions [4]. It assumes that if two point x 1 and x 2 are close in the intrinsic geometry of q ( x ), then the conditionals r ( y | x 1 )and r ( y | x is, r ( y | x ) is a smooth function along the geodesics in the intrin-sic geometry of q ( x ). This assumption allows the knowledge of the marginal distribution q ( x ) to be exploited. Here, for the purpose of our analysis, we will make a similar assumption. We will as-source and labeled target domain data are close (i.e., in the same clusters and have the same labels) and they follow closely related Gaussian distributions, their marginals should share a very similar intrinsic geometry. Therefore, the conditionals r t ( y | are the same. The above assumptions allow us to state that the dif-ference between the source and target domain distributions can be attributed to co-variate shift.
According to [6], if the di ff erence between the two distributions can be bounded, the generalization error can also be bounded when we combine the labeled data from the target-domain and source-domain to predict the unlabeled data from target-domain. Follow-ing [6], let m = + o denote the total number of labeled data. That is, we obtain =  X  m labeled data from the target-domain and o = (1  X   X  ) m from the source-domain independently. Consider the ideal labeling function f  X  : X  X  Y ,where Y = [0 , 1]. For a hypoth-esis h : X  X  Y , the probability of h disagreeing with f  X  to the target domain distribution q t ( x ) can be defined as (20) t ( h , f  X  ) = E x  X  q t ( x ) [ | h ( x )  X  f  X  ( x ) This is also the risk for a hypothesis. And we use t ( h ) for short and  X  ( h ) for the empirical risk. We use the parallel definitions ( h )and X  s ( h ) for the source-domain. This allows us to measure the distance between two distributions using a hypothesis class-specific criterion. Let H be a hypothesis class for X and A H be a subset for H where for every hypothesis h  X  X  , { x : x  X  X , h ( x ) = Then the distance between the target and source distributions is (21) d H ( q t ( x ) , q s ( x )) = 2sup According to [6], d H can be computed approximately from a finite-sample when H has a finite VC dimension. After that, for a hy-pothesis space H , we define the symmetric di ff erence hypothesis space H  X  H as H  X  H = { h ( x )  X  h ( x ): h , h  X  X } ,where the XOR operator. It says that every hypothesis g  X  X   X  H labels all data as 1 when a given pair of hypotheses in H disagree against each other. It can be shown that for any hypotheses h , h following inequality holds (22) | t ( h , h )  X  s ( h , h ) | X  Moreover, we define the ideal hypothesis h  X  that minimizes the combined target-domain and source-domain risk. (23) h  X  = arg min Thus, the combined risk can be denoted as  X  = t ( h ) + s sider that we have a few labeled data from the target-domain and a large number of labeled data from the source-domain. In this situ-ation, we train a classifier that minimizes a convex combination of empirical target-domain and source-domain risk: (24)  X   X  ( h ) =  X   X  t ( h ) + (1  X   X  ) X  s ( h ) We use  X  ( h ) as a generalized one with respect to q t ( x )and q Similar to in [6], our transfer algorithm can be bounded as follows. Specifically, Lemma 2 shows the the di ff erence between the target risk s ( h ) and weighted risk  X  ( h ) can be bounded and Lemma 3 shows the the di ff erence between the true and empirical weighted risks can also bounded.
Lemma 2. Let h be a hypothesis in H (25) |  X  ( h )  X  t ( h ) | X  (1  X   X  )(
Lemma 3. Let H be a hypothesis space of VC-dimension d , then with probability at 1  X   X  ,forevery h  X  X  (26) |  X   X  ( h )  X   X  ( h ) | &lt; The detailed derivation can be founded in [6]. From these two Lem-mas, we obtain the bound for our domain transfer learning.
Theorem 3. Let H be a hypothesis space of VC-dimension d , and U t and U s be unlabeled samples of size m each, drawn accord-ing to q t ( x )and q s ( x ), respectively. Let  X  d H  X  H tance on U t and U s .Let S be a labeled set of size m , which is con-structed from  X  m points drawn from q t ( x )and(1  X   X  ) m points from q ( x ). If  X  h  X  X  is the minimizer of  X   X  ( h )on S and h  X  is the target domain risk minimizer, then with probability at least 1  X   X  (over the choice of the samples), for every h  X  X  , t (  X  h ) 2(1  X   X  )( 1 In a situation where we have many labeled source domain data and a few labeled target domain data,  X  will be small and  X  d play an important role in the bound. As stated in Section 3.2, P ( x , y )and P s ( x , y ) are similar after we perform kernel mapping and cluster-based sample selection. That means a classifier h H can be found to maximally discriminate between unlabeled in-stances from the target and source domains at the same time. Thus, the empirical distance  X  d H  X  H will become small and the bound will be tight. Follow these and the analysis in Section 3.2, the transfer-ability of the pr oposed approach relies on the selected data in the induced space. If two domains are unrelated in the kernel space, few source domain examples can be selected by the sample selec-tion criteria. When these happen, the proposed approach may fail, as shown by the error bound of Theorem 3. If the di ff erence be-tween the two distributions is large, the bound is loose.
In Section 3.3, we have shown the error bound for individual classifier. In this section, we will obtain the upper bound of the generalization error for our majority vote ensemble. The analysis is similar to [1]. Let Q be a posterior distribution over the Q  X  weighted majority vote classifier or Bayes classifier is (27) B Q ( x ) = sgn [ E h  X  Q h ( x )]  X  x  X  X where sgn [ x ] = 1if x &gt; 0and  X  1 otherwise. We denote the asso-ciated individual Gibbs classifier that for classifying any example x  X  X chooses randomly a classifier h according to the distribution Q . For a finite set of unlabeled data U , the risk of G Q (28) ( G Q ) = 1 where,  X  = 1 if predicate  X  holds and 0 otherwise, and y i true unknown label of x i . Similarly, the risk of B Q is (29) ( B Q ) = margin function. From these, the error of our ensemble can be bounded based on the risk of individual Gibbs classifiers and the margin of unlabeled data.

Theorem 4. B Q is the Bayes classifier and then for all Q and all  X   X  (0 , 1] with probability at least 1  X   X  : (30) ( B Q ) inf  X   X  where T  X  u ( Q ) =  X  ( Q ) + 1 2 ( E u m Q ( x i )  X  1), K  X  ), E u z is the expectation of a random variable z with respect to the uniform distribution over U , P u is the uniform probability over U ,and x + denotes the positive part of x that means x + = x if x &gt; 0 and 0 otherwise. This theorem states that if the individual classifier has error bound  X  ( Q ) with probability at least 1 ensemble classifier can also be bounded.
Three real-world data collections from two di ff erent domains are used to empirically evaluate the proposed algorithms. The perfor-mance is compared with both traditional non-transfer single clas-sifiers, and the state-of-art transfer learning algorithm TrAdaBoost [8]. Two sets of studies are also conducted to further examine the sensitivity of the proposed methods with respect to di ff ber of iterations, and varied sizes of labeled target-domain data.
As summarized in Table 2, the data collections involved in our study are Reuters-21578 [2], 20-Newsgroups [9] and SyskillWe-bert [2]. Among them, Reuters-21578 and 20-Newsgroups are the benchmarks of text categorization, and SyskillWebert is the stan-dard used to test web page ratings. The important statistics and pre-processing procedures of these collections are presented below.
Data Set Description With a hierarchical structure, the Reuters-21578 collection contains Reuters news wire articles organized into Three categories,  X  X rgs X ,  X  X eople X  and  X  X laces X  are selected in our study. From the category  X  X laces X , we remove all the docu-ments of  X  X SA X  to make the sizes of the three categories nearly even. For each category, all of the subcategories are then orga-nized into two parts, and each part is of the di ff erent distribution and approximately equal size. Therefore, one part can be treated as the target-domain data and the other is used for the source-domain purpose. According to the method described in [8], three cross-domain learning tasks are generated as listed in Table 2, and the learning objective aims to classify articles into top categories. Sim-ilar to Reuturs-21578 data set, 20-Newsgroups corpus contains 7 top categories and these top categories contain 20 subcategories which have approximately 20,000 newsgroup documents. We se-lect four top categories  X  X om X ,  X  X ec X ,  X  X alk X  and  X  X ci X  in this exper-iment. Thus, three other cross-domain tasks are formed as listed in Table 2. SyskillWebert database contains the HTML source of web pages plus the ratings of a user on those web pages. The web pages are on four separate subjects. Associated with each web page are the HTML source as well as a user X  X  rating in terms of  X  X ot X ,  X  X edium X  or  X  X old X . As demonstrated in Table 2, all of the four subjects are involved in our study.  X  X ands-recording artists X  is re-served as the set of source-domain and the others are used as the target-domain data. Compared to the  X  X old X  pages, the total num-ber of pages rated as  X  X edium X  or  X  X ot X  is fewer. Therefore, we combine the  X  X edium X  and  X  X ot X  pages together, and change the labels of those pages as  X  X on-cold X  to form a binary classification problem. The learning task is to predict the user X  X  preferences for the given web pages.

Experiment Setting For each target-domain data set employed in the experiment, we further split it into two parts: target-domain data with labels( L ) and target-domain data without labels( ratio between L and U is 1:9. All of the target-domain data without labels ( U ) are used as the test sets; while the training sets consist of the data points with labels from both the target-domain ( source-domain ( O ). Several popular algorithms, such as Naive-Bayes, KNN and SMO with polynomial kernel, are used as not only single classifiers by themselves, but also the base classifiers of KMapEnsemble, KMapWeighted and TrAdaBoost respectively. For these three ensembles, we set the number of iterations to be 10. The algorithm implementations are based on Weka [21].
Using accuracy as the evaluation metric, we systematically com-pare the proposed algorithms to single classifiers and TrAdaBoost. All of the results reported below are averaged over 10 runs. Due to the space limitation, we abbreviate KMapEnsemble as  X  X E X , and KMapWeighted as  X  X W X  in all of the tables.
 Table 3 summarizes the accuracies of KMapEnsemble, KMap-Weight, single classifiers and TrAdaboost on the three databases. For the Reuters-21578 collection, as highlighted in bold, KMapEnsem-ble consistently outperforms TrAdaBoost and the three single clas-sifiers on all of the 3 tasks. On the  X  X rgsVsPeople X  data set with KNN as the base classifier, we notice that the averaged accuracy of KMapEnsemble is over 24% higher than TrAdaBoost and KNN as the single classifier. If we overlook the model di ff erences, com-pared to its rivals, KMapEnsemble on average achieves at least 15.4%, 7.4% and 8.6% higher accuracy on  X  X rgsVsPeople X ,  X  X rgsVs-Places X  and  X  X eopleVsPlaces X  respectively. The better performance of KMapEnsemble over TrAdaBoost can be ascribed to the kernel-based feature mapping and cluster based example selection imple-mented in KMapEnsemble, which utilizes the predictions from dif-ferent feature spaces to transfer the similar data from the source-domain to the target-domain. Moveover, on 7 out of 9 cases, KMap-Weight outperforms TrAdaBoost which conducts the cross-domain transfer only in the original feature space. This, from the empir-ical perspective, provides justification to the theoretical analysis of kernel-based feature mapping in Section 3. The worst perfor-mances of single classifiers are mainly due to the inferior strength of the single model which can not capture the useful information existing in the source-domain. For the 20-Newgroup data set, re-markably, when the base classifier is KNN, SMO and NavieBayes in the order, the win-lose-tie statistics of KMapEnsemble is 3-0-0, 3-0-0 and 1-0-2 respectively. The similar performance explanation provided to KMapEnsemble on Reuters-21578 can be also applied here. For those data sets KMapEnsemble fails, KMapWeighted performs best on 1 out of 2 such data sets. In particular, on data set  X  X omVsRec X , KMapWeighted achieves the highest accuracy when NavieBayes is used as the base classifier. This suggests that, besides the new mapping feature space technique integrated into KMapWeighted, the sophisticated weighting scheme also helps to enhance the ab ility of KMapWei ghted by dynamically controlling the impacts of source-domain data according to the training error. On the SyskillWebert database, among the total of 3 tasks with respect to di ff erent base classifiers, the win-lose-tie statistics of KMapEnsemble is respectively 3-0-0 for KNN, 2-1-0 for SMO and 3-0-0 for NaiveBayes. For the  X  X ands-Sheep X  task, the accuracy of KMapEnsemble is at least 13% higher than other approaches when the base classifier is KNN. In addition, compared to TrAdaBoost, the win-lose-tie statistics of KMapWeighted is still 2-1-0 for KNN, 1-2-0 for SMO and 3-0-0 for NaiveBayes in the order. The similar performance explanation provided on the above two collections can be also applied here. On the other hand, the superior performance of KMapEnsemble over KMapWeighted on all of the 9 scenarios could be due to the e ff ectiveness of model averaging over boosting. Please refer to [10] for related discussion.
As shown in Section 2, several inputs need to be specified before the execution of KMapEnsemble. Among those parameters, di ent numbers of iterations and varied sizes of labeled target-domain data, are of critical importance for the performance of the proposed algorithms. As the result, we carried out two additional sets of ex-periments to test the sensitivity and adaptiveness of the proposed algorithms with respect to these parameters.

Di ff erent Numbers of Iterations As the number of iterations varies from 1 to 10, Figure 2 plots the accuracy learning curves of KMapEnsemble, TrAdaBoost and traditional non-transfer single classifiers on the Reuters-21578 collection. Although the perfor-mances of traditional non-transfer single classifiers are not a by the values of iteration, we still deliberately include those straight lines in the plots to acquire a global inspection. As shown in the Figure 2(a-e), KMapEnsemble outperforms its competitors for 5 out of 6 scenarios. The only exception is on data set  X  X eople vs places X  when the base classifier is KNN. In general, compared to TrAdaBoost, KMapEnsemble underperforms at the first iteration. That is because the feature space at the first iteration is learned merely through the small size of labeled target-domain data. As more iterations proceed, the predictive ability of KMapEnsemble is significantly boosted due to the constant accumulation of the source-domain data. These data are similar to the labeled data from the target-domain, whereby they can assist in the learning. For example, KMapEnsemble consistently outperforms TrAdaBoost in accuracy by at least 12% on data set  X  X rgsVsPeople X  when the base classifier is KNN. Therefore, we conjecture that, if more iter-ations are carried out, KMapEnsemble could perform better in the sense that the combination of more feature spaces can significantly reduce the bias and boost the accuracy.

Varied Sizes of Labeled Target-domain data This study is con-ducted on SyskillWebert collection. Two sets of experiments are performed with  X  X ands X  or  X  X oats X  as the target-domain data and others for the source-domain purpose. The results are demonstrated in Figure 3. It is evident that, as the size of the labeled target-domain data increases, KMapEnsemble consistently performs bet-ter than or as equal as its competitors. For example, as shown in Figure 3(c), KMapEnsemble achieves at least 10% higher accuracy than other methods on each size of labeled target-domain data. As a general trend, the accuracy of KMapEnsemble steadily improves when the number of labeled target-domain data increases from 2 to 12. Consequently, we infer that, better performances can be ob-tained if more labeled target-domain data are provided.
As analysesed in Section 3, kernel mapping makes the margin distributions between source-domain and target-domain close, and cluster based sample selection further reduces the di ff erence of con-ditional distributions between source-domain and target-domain. In this part, we perform an extend experiment to show that both of them play an important role on the proposed approach. For com-parison, we propose two other methods. One is called  X  X oCluster X  which use labeled target-domain data and all source-domain data to learn a mapping space and build a classifier, the other called  X  X oKMap X  is to build a classifier using labeled target-domain data and selected source-domain under the original feature space. We notice that noCluster only performs kernel mapping while noKMap only performs cluster based sample selection. Figure 4 plots the ac-curacies of noCluster, noKMap and kMapEnsemble on the Reuters-21578 collection as the number of iterations varies from 1 to 10. Both noCluster and noKMap are not a ff ected by the values of iter-ation. We observe that kMapEnsemble outperforms noCluster and noKMap consistently after the second iteration on all three data set. It boosts at least 5% in accuracy comparing two base line methods at all three cases. noKMap fails because it does not cover the gap of margin distribution between source and target domains. And the reason behind poor performance of noCluster is that it does not deal with conditional distributions of source and target domain which are still di ff erent in the kernel mapping space.
One main challenge of transfer learning is how to resolve and, in the same time, take advantage of the di ff erence between two do-mains. Several methods use instance weighting or to increase the weights of similar instances and reduce those dissimilar ([5, 8, 11, 16]). For example, [8] adopts the boosting weight formula as the re-weighting scheme. Many others approaches attempt to change the representation of instances through mapping them into other spaces where the data from two domains are similar(e.g., [18, 20]). Most recently, [12] proposes a locally weighted ensemble framework to combine multiple mode ls for transfer learning by dynamically as-signing weights of a model according to a model X  X  predictive power on each test example. [17] uses a set of predefined kernels to find a suitable kernel for the new data. [20] improves the e ff of unsupervised dimension reduction with the help of related prior knowledge from other classes in the same type of concept.
The main challenge of transfer learning is to bridge the di in distribution between target-domain and source-domain. The com-mon assumption that the marginal and conditional probabilities are directly related between source and target domains may fail in the original or linearly transformed space. In this paper, we have ex-plored a new approach to solve this problem by (1) exploiting ker-nel space feature mapping to connect two distributions, (2) clus-ter criterion-based source-domain sample selection (3) instance re-weighting, and (4) ensembles. Based on the feature spaces of source and target domains, we use KDA to seek a third feature space to make the marginal distributions from two domains similar. Then under the clustering manifold assumption, we select those data from source-domain whose conditional probability is likely to be simi-lar to target-domain inside the kernel space. This process is re-peated in an iterative manner in order to remove the bias of any single kernel mapping as well as to re-select or re-weight source domain examples based on their contribution to expected accuracy. Formal analysis show that in the kernel space, both target-domain and source-domain data are approximately Gaussian, the di ence in conditional distribution between target-domain and selected source-domain data is bounded, and importantly, the error of pre-diction on target-domain data by the kernel space ensemble trained with source-domain data is also bounded. Empirical studies have shown that when two domains are di ff erent, the proposed approach increases the accuracy of state-of-the-art transfer learning methods by as much as 10%.
 We would like to thank Ulrich R X kert and Stefan Kramer of Institut f X r Informatik / I12 at Technische Universit X t M X nchen for sharing the TechTC-300 data set and Wenyuan Dai from Department of Computer Science and Engineering at Shanghai Jiao Tong Univer-sity for sharing the preprocessed Reuters-21578 data set. Jiangtao Ren is supported by the National Natural Science Foundation of China under Grant No. 60703110.
