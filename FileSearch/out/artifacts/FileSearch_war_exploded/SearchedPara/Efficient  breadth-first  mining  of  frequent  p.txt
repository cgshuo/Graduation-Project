
Constrained itemsets mining, i.e. finding al l itemsets included in a transaction data-base that satisfy a given set of constraints, is an active research theme in data mining (Srikant et al. 1997; Ng et al. 1998; Han et al. 1999; Lakshmanan et al. 1999; Grahne et al. 2000; Pei et al. 2001; Boulicaut and Jeudy 2002). The most studied constraint is the frequency constraint, the antimonotonicity of which is used to reduce the expo-nential search space of the problem. Explo iting the antimonotonicity of the frequency constraint is known as the Apriori pruning method (Agrawal et al. 1993; Agrawal and
Srikant 1994): it dramatically reduces the s earch space, thus making the computation feasible. Frequency is not only computationally effective, it is also semantically im-portant because frequency provides support t o any discovered knowledge. For these reasons, frequency is the base constraint of what is generally referred to as frequent itemsets mining. However, many other constraints can facilitate user-focussed ex-ploration and control as well as reduce the computation. For instance, a user could be interested in mining all frequently purch ased itemsets having a total price greater than a given threshold and containing at least two products of a given brand. Among these constraints, classes that exhibit nic e properties have been characterised. The class of antimonotone constraints is the most effective and easy to use in order to prune the search space. Because any conjunction of antimonotone constraints is in turn antimonotone, we can use the Apriori pruning method : the more antimonotone constraints are available, the more selective the Apriori pruning method will be. The dual class, monotone constraints, has been considered more complicated to exploit and less effective in pruning the search sp ace. The problem of mining itemsets that satisfy a conjunction of antimonotone and monotone constraints has been studied for a long time (Ng et al. 1998; Pei et al. 2001; Boulicaut and Jeudy 2002), but all these studies have failed in finding the real synergy between the two opposite pruning opportunities. All the authors have sta ted that this is the inherent difficulty of the computational problem: when d ealing with a conjunction of monotone and antimonotone constraints, we face a tradeoff between antimonotone and monotone pruning.

Our observation is that this prejudice holds only if we focus exclusively on the search space of the itemsets, which is the approach followed in the work done so far.
In Bonchi et al. (2003(b)), we have shown that a real synergy of the two opposite pruning exists and can be exploited by reasoning on both the itemsets search space and the transactions input database together. In this way, pushing monotone con-straints does not reduce antimonotone pruni ng opportunities; on the contrary, such opportunities are boosted. Dually, pushing antimonotone constraints boosts monotone pruning opportunities: the two components s trengthen each other. On the basis of these considerations; we have introduced ExAnte, a preprocessing data-reduction al-gorithm that reduces dramatically both the search space and the input dataset in con-strained frequent patterns mining. The experimentation of the algorithm has pointed out how effective the reduction is and which potential benefits it offers to the sub-sequent frequent pattern computation.

In this article, we show how the basic ideas of ExAnte can be generalized in a levelwise, Apriori-like computation. The resulting algorithm can be seen as the real generalization of Apriori, able to exploit both kinds of constraints to reduce the search space. We named our algorithm E xAMiner (ExAnte Miner in contrast with ExAnte preprocessor, but also Min er, which exploits antimonotone and mono-tone constraints together). Because th e most appropriate way of exploiting monotone constraints in conjunction with frequency i s to reduce the problem input, which in turn induces a reduction of the search sp ace, the mining philosophy of ExAMiner is to reduce as much as possible the problem dimensions at all levels of the computa-tion. Therefore, instead of trying to explore the exponentially large search space in some smart way, we massively reduce the search space as soon as possible, obtain-ing a progressively easier mining problem. Experimental results confirm that this is, at this moment, the most efficient way of attacking the computational problem in analysis. Moreover, ExAMiner makes feasible the computation of extreme patterns, i.e. extremely long or extremely costly patterns, which can be found only at very low support levels, where all the other known mining approaches cannot always complete the computation. Even if the support threshol d is very low, ExAMiner, exploiting the extremely strong selectivity of the monotone constraint, reduces drastically the prob-lem dimensions and makes the computation affordable. is a nonempty subset of .If | X |= k ,then X is called a k -itemset. A transaction is a couple tid , X ,where tid is the transaction identifier and X is the content of the transaction (an itemset ). A transaction database D is a set of transactions. An itemset X is contained in a transaction tid , Y if X  X  Y . A constraint on itemsets is a function, C : 2  X  X  true , false } . We say that an itemset I if and only if C ( I ) = true . The support of an itemset X in database D , denoted by supp D ( X ) , is the number of transactions in D that contain X .Givenauser-defined minimum support,  X  , an itemset X is called frequent in D if supp
Let Th ( C ) ={ X | C ( X ) = true } denote the set of all itemsets X that satisfy con-straint C . The frequent itemset mining problem requires computing the set of all frequent itemsets Th ( C freq ) . In general, given a conjunction of constraints C ,the constrained itemset mining problem requires to computing Th frequent itemset mining problem requires to computing Th
Definition 1.1. Given an itemset X , a constraint C AM If C AM holds for X , then it holds for any subset of X .

Apriori algorithm with the following heuristic: if an itemset X does not satisfy C then no superset of X can satisfy C freq and hence they can be pruned. This pruning can affect a large part of the search space because itemsets form a lattice. There-fore, the Apriori algorithm operates in a levelwise fashion moving bottom-up on the supersets.

Definition 1.2. Given an itemset X , a constraint C M If C M holds for X , then it holds for any superset of X .
 that satisfy a conjunction of monotone and antimonotone constraints:
Because any conjunction of antimonotone cons traints is an antimonotone constraint, and any conjunction of monotone constraints is a monotone constraint, we just con-sider the general constraints C AM and C M ,where C freq is always present in the con-junction C AM unless explicitly stated otherwise.
 pendently of the given transaction database. In this article, we will mainly focus on those strong constraints, together with the minimum frequency constraint. This is necessary because we want to distingui sh between simple monotone constraints and global constraints such as the infrequency constraint: supp straint is still monotone but has different properties because it is dataset dependent and it requires dataset scans in order to be computed. Obviously, because our algo-rithm reduces the transaction dataset, we w ant to exclude such data-dependent con-straints from our study. Thus, our study focuses on strongly monotone constraints, in the sense that they depend exclusively on the properties of the itemset and not on the underlying transaction database. For example, Table 1 shows some data in-dependent monotone constraints. In the rest of this article, we write, for the sake of brevity, monotone and antimonotone instead of strongly monotone and strongly antimonotone, respectively. The antimonot one constraints that are not strong will be represented by the minimum frequency c onstraint, without loss of generality.
The first proposal of an algorithm for mi ning frequent itemsets with monotone con-straints is FC M , an FP-growth-based algorithm, introduced in Pei et al. (2001).
This algorithm is particularly studied for those hard constraints, named convertible, that are neither antimonotone nor monotone but that can be converted in these kinds of simple constraints by sorting the items in some specific order in the header table of the FP tree. Consider, for instance, the constraint a stable relation between subsets, supersets and satisfaction of such a constraint. But if we arrange the items in price-ascending order, the average of prices of an itemset is always more than the average of its prefix itemset. Therefore, w.r.t. this order, the constraint is monotone. The authors s tate that, because a monotone constraint, it is also a convertible monotone constraint; this technique can be used also to push simple monotone constraints in the frequent itemsets computation.

Strictly speaking, this algorithm cannot be considered a constraint-pushing tech-nique because it generates the complete set of frequent itemsets, no matter whether or not the satisfy the monotone constraint. The only advantage of FC a pure generate and test algorithm is that FC M only tests some of frequent item-sets against the monotone constraint. Once a frequent itemset satisfies the monotone the constraint. The main drawback of this proposal is that sorting items by prices makes us lose the compacting effect of the ordering by frequency, and thus we have to manage much greater FP trees, requiring a lot more main memory, which might not be available.
 lead to a reduction of antimonotone pruning opportunities. Theref ore, when dealing with a conjunction of monotone and antimonotone constraints, we face a tradeoff between antimonotone and monotone pruning. Suppose that a pattern has been re-moved from the search space because it does not satisfy a monotone constraint. This pruning avoids checking support for this pattern, but on the other hand, if we check its support and find it smaller than the frequency threshold, we may prune away all the supersets of this itemset. In other words, by monotone pruning, we risk los-ing antimonotone pruning opportunities given by the pruned itemset. The tradeoff is clear: pushing monotone constraint can save frequency tests; however, the results of these tests could have lead to more effective antimonotone pruning.
 pruning is DualMiner (Bucila et al. 2002). However, DualMiner does not compute all solution itemsets with their support. It just finds the family of solution subal-gebras, representing them with their top and bottom elements. This is an analog with maximal frequent pattern mining. In maximal frequent pattern mining, one is interested in mining only frequent itemsets that are maximal w.r.t. set inclusion. DualMiner does not compute the whole Th ( C freq )  X  set of all couples T , B ,where T and B are itemsets that represent the top and the bottom, respectively, of a solution subalgebra. Therefore, DualMiner can not be used for constrained frequent pattern mi ning in contexts where the support of each solution is required, for instance, when we want to compute association rules. More-over, the dual top-down bottom-up exploration of the search space, performed by
DualMiner, faces many practical limitations . First, DualMiner uses multiple scans of the database, even to compute supports of itemsets of the same size. This is due to the inherent divide-et-impera strategy characterizing the DualMiner computa-tion. Moreover, the dual top-down bottom-up computation performs very poorly on real-world problems. In real-world problems, even on dense datasets or at very low support levels, frequency is always a very selective constraint: it is quite hard to find frequent patterns of large size. The DualMiner top-down computation, exploiting C pruning, will usually perform a huge number of useless tests on very large itemsets, inducing a degradation of the performance. The same reasoning, i.e. frequency is often much more selective than any other reasonable constraint, represents the main drawback also for the Version Space (De Raedt and Kramer 2001) and ACP (Bonchi et al. 2003(a)) algorithms.
In this article, we introduce ExAMiner, a breadth-first algorithm that exploits the real synergy of antimonotone and monotone constraints: the total benefit is greater than the sum of the two individual benefits. ExAMiner generalizes the basic idea of the preprocessing algorithm ExAnte, embedding such idea at all levels of a levelwise
Apriori-like computation. The resulting algorithm brings to the research field the following interesting contributions:  X  ExAMiner is the generalization of the Apriori algorithm, when a conjunction of  X  There are some data-reduction techniques known in the literature (Park et al.
Introducing ExAMiner, we show how t his techniques can be coupled with data reduction based on a monotone constraint, obtaining dramatically effective opti-mizations.  X 
In the presence of a cardinality monotone constraint, ExAMiner exploits en-hanced data-reduction techniques that fur ther amalgamate antimonotonicity and monotonicity. Moreover, we show that this improvement can be induced, even if in weaker form, also for the other forms of monotone constraint.  X 
We introduce a pruning technique that reduces directly the generator itemsets at each iteration, i.e. the frequent items ets that are not going to produce solu-tions. This is more effective than reducing the candidate itemsets, as in ordinary
Apriori-like algorithms.  X 
ExAMiner can be used with any constraint that has a monotone component: therefore, also succinct monotone constraints (Ng et al. 1998) and convertible monotone constraints (Pei et al. 2001) can be exploited.  X 
ExAMiner maintains the exact support o f each solution itemsets: a necessary condition if we want to compute association rules.  X 
ExAMiner can be used to make feasible the discovery of extreme patterns that can be discovered only at very low support level, for which the computation is unfeasible for traditional algorithms.  X 
A thorough experimental study has been performed with different monotone con-straints on various datasets (both real-world and synthetic datasets): ExAMiner performs as the most efficient algorithm so far for constrained frequent pattern mining.

The rest of the article is organized as follows. In the next section, we review the ExAnte property and the ExAnte preprocessing algorithm. In Sect. 3, we intro-duce some data reduction techniques that rel y exclusively on the antimonotonicity of frequency, then we argue that, coupling s uch techniques with the ExAnte monotone data reduction, we can obtain dramatically ef fective optimizations. In Sect. 4, we show that, in the presence of a cardina lity monotone constraint, antimonotone and monotone data-reduction techniques can be further amalgamated and their pruning power enhanced. Moreover, we show that this improvement can be induced, even if in weaker form, also for the other kinds of monotone constraint. In Sect. 5, we de-scribe in detail the ExAMiner algorithm and we provide detailed pseudo-code, then we explain its computation by means of an example, and we discuss the cost of dataset rewriting. In Sect. 6, we report experimental results. In Sect. 7, we argue that the general breadth-first data-reduc tion approach of ExAMiner can be exploited to mine more complex kinds of patterns from more structured data: an example de-scribing the mining of frequent connected subgraphs from a dataset of graphs is provided. Finally, in Sect. 8 we conclude with a discussion on further improvements to the proposed algorithm.
As already stated, if we focus only on the itemsets lattice, pushing monotone con-straint can lead to a less effective antimonotone pruning. Suppose that an itemset has been removed from the search space because it does not satisfy some mono-tone constraints C M . This pruning avoids checking support for it, but it may be supersets could be pruned away. By monotone pruning an itemset, we risk to los-ing antimonotone pruning opportunities given from the itemset itself. The tradeoff is clear (Boulicaut and Jeudy 2002): pushing monotone constraint can save tests on antimonotone constraints; however, the results of these tests could have lead to more effective pruning.
 to consider the constrained frequent patterns problem in its whole, not focussing only on the itemsets lattice but considering it together with the input database of transactions.
 order to produce all the itemsets satisfying all the given constraints. Hence, it really doesn X  X  matter which mining algorithm is used, the ExAnte property can always be used as a preprocessing step before mining.
 operator.

Definition 2.1 (  X  reduction). Given a transaction database D and a conjunction of monotone constraints C M , we define the  X  reduction of D as the dataset resulting from pruning the transactions that do not satisfy C M
In other words, using the ExAnte property, we put the focus on the input data instead of on the search space. Indeed, the tradeoff between antimonotone and monotone pruning exists only if we focus exclusively on the search space of the problem, which is the approach followed in the current state-of-the-art algorithms. But if we take a step back and look at the overall problem, reasoning on both the search space and the input database together, we can find the real synergy of antimonotonicity and monotonicity constraints, as explained in the rest of this section. Instead of applying monotone constraints to the patte rns, we apply them to the input database (recall that a transaction is nothing more than an itemset). The ExAnte property states that a transaction that does not satisfy the given monotone constraint can be deleted from the input database because it will never contribute to the support of any itemset satisfying the constraint, and thus of any solution itemset. This is stated more formally in the following theorem.

Theorem 2.1 (ExAnte property (Bonchi et al. 2003(b))). Given a transaction data-base D and a conjunction of monotone constraints C M ,wehave Proof. Because X  X  Th ( C M ) , all transactions containing X will also satisfy C for monotonicity. In other words, no transaction containing X will be pruned. This implies the thesis.
A major consequence of reducing the input database in this way is that it implicitly reduces the support of a large amount of itemsets that do not satisfy the monotone constraint as well, resulting in a reduced number of candidate itemsets generated dur-ing the mining algorithm. Even a small reduction in the database can cause a huge cut in the search space because all superset s of infrequent itemsets are pruned from the search space as well. In other words, monotonicity-based data reduction of trans-actions strengthens the antimonotonic ity-based pruning of the search space.
This is not the whole story, in fact, infrequent singleton items can not only be removed from the computation: for the same antimonotonicity property, they can be deleted also from all transactions in the input database. This antimonotonicity-based data reduction has been named  X  reduction.

Removing items from transactions has got another positive effect: reducing the size of a transaction that satisfies a monotone constraint can make the transaction vi-olate the monotone constraint. Consider, for instance, the monotone constraint based on the minimum sum of prices: a transaction that satisfies the constraint having a total sum of prices greater than the giv en threshold might end up not satisfy-ing the constraint anymore after its in frequent items are removed because its total sum of prices might go below the threshold. Therefore, a growing number of trans-actions that do not satisfy the monotone constraint can be found. Obviously, we are inside a loop where two different kinds of pruning ( duce the search space and the input datase t, strengthening each other step by step until no more pruning is possible (a fix point has been reached). This is the key idea of the ExAnte preprocessing method. In the end, the reduced dataset result-ing from this fix-point computation is usually much smaller than the initial dataset (obviously depending on the selectivity of the antimonotone and monotone con-straints).

Definition 2.2. Given a transaction database D , a conjunction of monotone con-straints, C M , and a conjunction of antimonotone constraints C duced dataset obtained by the fix-point application of
ExAnte starts the first iteration as any frequent patterns mining algorithm: counting the support of singleton items. Items that are not frequent are thrown away once and for all. But during this first count, only transactions that satisfy C
The other transactions are assigned to be pruned from the dataset (
Doing so, we reduce the number of interesting 1-itemsets. Even a small reduction of this number represents a huge pruning of the search space. At this point, ExAnte deletes from alive transactions all infrequent items ( reduce the monotone value (for instance, the total sum of prices) of some alive trans-actions, possibly resulting in a violation o f the monotone constraints. Therefore, we have another opportunity of  X  reducing the dataset. But in we create new opportunities for  X  reduction, which can turn in new opportunities for  X  reduction, and so on, until a fix point is reached.

Clearly, a fix point is eventually reached after a finite number of iterations, as at each step, the number of alive items strictly decreases.

In Fig. 1, we show experimental results of ExAnte: input transactions reduc-tion and search space reduction (measured in term of candidate itemsets consid-ered).

In ExAnte, pruning away infrequent 1-itemsets gives us the opportunity of the transaction database in input, which in turn creates more infrequent 1-itemsets. In a levelwise computation, we collect new information level by level. If we can exploit such information to prune items from the transactions, we obtain new opportunities to  X  reduce again the transaction database.
 gorithms, sharing the same levelwise structure, have been proposed. Even if usually proposed as new algorithms with their own na mes, they can essentially be considered optimizations of the basic Apriori schema. Some of these proposed optimizations are data-reduction techniques, which, if exploited alone, bring little benefit to the com-putation. But if we couple such antimonotoni city-based optimizations with ExAnte X  X   X  reduction, we can obtain dramatically effective optimizations.
In the rest of this section, we review all such data-reduction strategies that are based on the antimonotonicity of frequenc y. In the next section, we couple them with a monotone constraint, and thus with the  X  reduction, obtaining the ExAMiner algorithm.

In the following propositions, we indicate with k the actual iteration, where fre-quent k -itemsets are computed.

Proposition 3.1 (Antimonotone global pruning of an item). At the iteration k , a singleton item that is not a subset of at least k frequent k -itemsets will not be a subset of any frequent ( k + 1 ) -itemset, and thus it can be pruned away from all transactions in the input database.

Proof. Every frequent ( k + 1 ) -itemset X has got k + 1 frequent subsets of size k , which can be obtained by removing a singleton item i item i  X  X is contained in exactly k subsets of X having size k .

Note that the ExAnte X  X   X  reduction, i.e. the removal of infrequent singleton items from all transactions, is just the instantiation of this property for k code of the algorithm in Sect. 5, we use an array of integers, V each item the number of frequent k -itemsets in which it appears.

While the last proposition induces a pruning that is global to the whole database, the next two propositions (Park et al. 1995) induce prunings that are local to a trans-action. In particular, the first one removes the entire transaction (similarly to the reduction), while the second one removes items from a single transaction.
Proposition 3.2 (Antimonotone pruning of a transaction). Given a transaction tID , X ,if X is not a superset of at least k + 1 frequent k -itemsets, then the transac-tion can be pruned away because it will never be a superset of any frequent itemset.

Proof. If a transaction tID , X contains a frequent ( contain all k + 1 frequent k -itemsets that are subsets of Y .

Unfortunately, this property cannot be exploited transaction by transaction be-cause it requires information that is available only at the end of the actual iteration, when all frequent itemsets have been counted. However, because the set of frequent version of the previous property, transaction by transaction.

Corollary 3.1 (Weak antimonotone pruning of a transaction). Given a transaction tID , X ,if X is not a superset of at least k + 1 candidate k -itemsets, then the transaction can be pruned away because it will never be a superset of any frequent ( k + 1 ) -itemset.

Proof. Trivially from Proposition 3.2 because the set of frequent itemsets at level k is a subsets of the set of candidate itemsets.

This property can be checked locally for each transaction, when the transaction is used to count support for candidate itemsets. If a transaction does not participate the algorithm, for each transaction t , we use a counter t candidate k -itemsets covered by t .

Actually, we can perform more local pruning in a transaction tID we know, for each item i  X  X , the number of candidate k -itemsets that are supersets of { i } and subsets of X .

Definition 3.1 (Multiplicity of an item w.r.t. a transaction). Given a transaction tID , X and an item i , we define the multiplicity of i w.r.t. X at the iteration k : as the number of candidate k -itemsets that are supersets of
Proposition 3.3 (Antimonotone local pruning of an item). Given a transaction tID , X , for each i  X  X ,if M ( i , X ) k &lt; k ,then i can be removed from X after the support counting phase of iteration k .

Proof. If M ( i , X ) k &lt; k ,then i will not be contained in any candidate that is a subset of X . Thus, i can be removed from X after the support counting phase of iteration k .
 for each item in t , we use a counter i . count for keeping track of M tunity of pruning the transaction (  X  reduction of the dataset). The basic philosophy of ExAMiner is to write a new reduced tran saction database for each iteration, ex-ploiting all possible antimonotone and monotone pruning opportunities.
In the previous section, we introduced a set of possible pruning strategies, known in the literature, that rely on antimonotoni city of frequency. Then we have indicated that the basic idea of ExAMiner is to couple this pruning with the the database, induced by the monotone constraint, providing strong data reduction because the two components strengthen each other. In this section, we introduce novel powerful pruning strategies, which couple more tightly the antimonotone and monotone pruning.
 the following proposition can be used to obtain a stronger pruning at a very low computational price. The following i s a generalization of Proposition 3.1.
Proposition 4.1 (Enhanced global pruning of an item). At iteration k , a singleton not be subsets of any frequent n -itemsets.

Proof. Let X = i 1 ... i n be a frequent n -itemset. For the antimonotonicity of fre-of size k . A generic item i p  X  X is a subset of exactly n number of possible subsets of X \{ i p } of size k  X  1. Adding i we obtain the thesis.
 a generic levelwise computation. At the end of the count and reduce phase, we have generated the set of frequent k -itemsets, L k , which normally would be used to generate the set of candidate itemsets for the next iteration. For each item i that does not satisfy the condition in the above proposition, we can prune from the set of generators L k , each k -itemset containing it, before the generation phase starts. The following corollary of Proposition 4.1, repre sents the first proposal of a strategy for pruning the generators because usually th ey are defined for pruning the candidate itemsets (the generated ones).

Corollary 4.1 (Generators pruning). Consider the computational problem Th  X 
Th ( C M ) ,where C M  X  card ( S )  X  n . At the iteration k putation, consider the set S k of itemsets in L k that contain at least a singleton item
In order to generate the set of candidates for the next iteration, C as generators the itemsets in L k \ S k without losing solutions to the given problem.
Analogously, the same kind of enhancement can be applied to the local prun-ing of an item from a transaction. The n ext corollary of Proposition 4.1, enhances Proposition 3.3, when we have to deal with a cardinality constraint.
Corollary 4.2 (Enhanced lo cal pruning of an item). Consider the computational problem Th ( C freq )  X  Th ( C M ) ,where C M  X  card ( S )  X  levelwise computation, consider a transaction, tID , X , for each i Then i can be pruned from X .

Similar pruning enhancements can be obtained also for all monotone constraints, inducing weaker conditions from the above propositions on cardinality.

Consider, for instance, the computational problem Th C  X  sum ( S . price )  X  m , and suppose that we are at the end of iteration k of the levelwise computation. As usual, we have recorded in V of frequent k -itemsets in which it appears. Consider an item i that, so far, is not in any solution. It is always possible to compute the maximum value of n for which continues to hold:
This value of n represents an upper-bound for the maximum size of frequent itemsets containing i . In the pseudo-code in the next section, we use a function determine _ max _ n ( V k [ i ] , k ) to determine such upper-bound.

Therefore, if we sum the price of i with the prices of the n sive items that are still alive, we can obtain an upper-bound for the total sum of prices of a frequent itemset containing i . In the pseudo-code, we use a function optimistic _ monotone _ value ( i , , n , C m ) to compute such upper-bound. If this sum is less than the monotone constraint threshold m , the item i can be globally pruned away, with all its supersets that are in L k because they cannot be solutions.
This generator pruning technique is twofold beneficial. In fact, the proposed tech-nique not only reduces the generators, and hence the number of candidates at the next iteration, but it also reduces the number of checkings of C itemsets from L k that cannot be solution before the checking of C action for all kinds of monotone constraints.

Essentially ExAMiner is a breadth-first Apriori-like algorithm that exploits antimono-tone and monotone constraints to reduce the problem dimensions levelwise. Each transaction, before participating to the support count, is reduced as much as pos-didate itemsets. Each transaction that arrives to the counting phase at iteration k is then reduced again as much as possible, and only of it survives to this second set of reductions, is it written to the transaction database for the next iteration. Therefore, in order to describe the proposed algorithm, it is sufficient to provide the pseudo-code for the procedure that substitutes the counting phase of the Apriori algorithm.
This new procedure is named count&amp;reduce . In the following with D the transaction database at iteration k .

D k , the set of candidates C k , the minimum support threshold straint C M and the array V k  X  1 . As already stated, each transaction t in the database passes through two series of reductions and tests. The first one brings the transac-tion, if still alive, to be exploited in the supports count. The second one brings the transaction, if still alive, to be written in the transaction database from the next iter-ation. The first reduction (lines 4 X 6 of Algorithm 4) is the global pruning of items (Proposition 3.1 and 4.1), which expl oits the information in the array V tested against the monotone constraint (line 10,  X  reduction). Only if both tests are passed is the transaction t matched against candidate itemsets to increase their sup-port counts (lines 11 X 18). During this phase, we collect other information regarding t and each item i in t : the number of candidates itemset contained in t (line 12), number of frequent itemsets containing i (line 18).

If the transaction t has arrived alive to this point, it has still to pass some tests in order to enter into the database for the next iteration. First, we check if its cardinality is at least k + 1 (line 19). Then we check if, during the counting phase, it has partic-ipated in the count of at least k + 1 candidate k -itemsets (line 20, Proposition 3.1). pruning of items (lines 22 X 24). After this reduction, we check again the size of the we collect information for the enhanced global pruning (line 28), and we perform the pruning of generators (lines 29 X 32).
The proposed data-reduction strategy, when instantiated at the first iteration ( k corresponds to the ExAnte algorithm, with th e unique difference that, in ExAnte, the computation starts again and again from the same level until there are pruning opportunities.
 computation. In fact, because during a count&amp;reduce round, we reduce the input dataset as well as the search space, we could start again with another count&amp;reduce round at the same level with the reduced tr ansaction dataset. Therefore, we face a choice between different strategies.

On the other hand, we can stand for more than one count&amp;reduce round on each level. Between these two extremes, we can have a whole range of computational strategies.
  X  how many count&amp;reduce rounds we admit  X  and for which levels of the levelwise computation.
 we could also decide to go back to some previous level. For instance, after the counting phase of the third level, before ge nerating candidates for the fourth level, we could go back to the first level and start again from the beginning but with a much smaller problem. We can go on this way until the input dataset is so reduced that it only contains the data essential for the solutions. However, this opportunity does not seems appealing from the point of view of efficiency.

AMiner:  X  ExAMiner  X  ExAMiner  X  ExAMiner of the usual levelwise algorithm are the most efficient because the count can be performed directly, without using complex data structures. From the third iteration, the count become very expensive. Therefore, we have decided to reduce as much as possible the problem during the first two iterations and then go on directly to compute the solution of the problem.
We now show an example of execution of ExAMiner 2 . Suppose that the transac-tion and price dataset in Table 2 are given. Suppose that we want to compute frequent itemsets (  X  = 3) with a sum of prices  X  30. During the first iteration, the total price of each transaction is ch ecked to avoid using transactions that do not satisfy the monotone constraint. All transaction with a sum of prices carded.

At the end of the count, we find items b , d and h to be infrequent. Note that, if the first transaction had not been discarded, item h would have been counted as frequent. At this point, we perform an  X  reduction of the dataset: this means removing b , d and h from all transactions in the dataset.

After the  X  reduction, we have more opportunities to fact, transaction 3, which at the beginning has a total price of 34, now has its total price reduced to 29 due to the pruning of h . This transaction can now be pruned away. The same reasoning holds for transactions 3 and 5.

At this point, we count once again the support of alive items with the reduced dataset. The item f , which initially has got a support of 3, now has become in-frequent due to the pruning of transaction 5. We can removing f from all transactions. Then we can  X  reduce again. In fact, after the removal of f , transactions 9 and 10 have a total price that is less than 30, and thus they are pruned too. We count once again the support of all itemsets and no one has become infrequent. We have reached a fix point for level one. At this
L ={ a , c , e , g , i , j , k , l } ,and C from L 1 .

We start with the second level of the levelwise computation. The first set of reductions and tests (lines 4 X 6 of Algorithm 4) produce no pruning at the beginning of the second level. The support counting phase starts. At the end of this phase, we have L 2 ={ ak , ce , cg , cj , eg , ei , gi , ij , jk , jl
The second part of the reductions and test (lines 11 X 14) produces no prun-ing because we are at the second level. We start another count&amp;reduce round at the second level. Items a and l are globally pruned from all transactions because they appear in only one frequent 2-itemset: V 2 [ a gives us more opportunities of  X  reducing the dataset. Transaction numbers 2, 12 and 14 can be pruned away because they no longer satisfy the monotone constraint. Thanks to this data reducti on, at the end of another counting phase, we have a smaller set of frequent 2-itemsets: L 2 ={ this point, item k appears in only one frequent 2-itemset and hence is globally pruned from the transactions. Due to this pruning, transaction numbers 4, 6 and 7 no longer satisfy the monotone constraint and they are pruned too. The input trans-action database has reduced from 14 to 3 transactions. We count using only these 3 transactions and find that L 2 ={ eg , ei , gi } .Items c and j do not appear in any frequent 2-itemsets and they can be pruned away. The resulting transaction database database.
ExAMiner at each iteration rewrites a dataset of smaller size. The next iterations has thus to cope with a smaller input dataset tha n the previous one. The benefits are not only in term of I/O costs but also of reduced work in subset counting due to the reduced number and size of transactions.
 sequentially scanned by reading fixed blo cks of data, one can take advantage of the
OS prefetching . Overlapping between computation and I/O activity can occur, pro-vided that the computation granularity is large enough. Moreover, OS buffer caching and pipelining techniques virtualize I/O disk accesses, and, more importantly, small datasets (that we can obtain after few iterations from the initial dataset) can be com-pletely contained in buffer cache or in main memory.
 quentially scanned, prefetching and buffer cache are able to hide I/O time. tion, no matter whether the benefit of the data reduction is worth the I/O cost. This choice was done to concentrate our study on data reduction. However, we can im-prove our algorithm by forcing it to rewrite the dataset only when the reduction is substantial. For instance, the data-reduction rate shown in Fig. 3 for the ExAMiner ation.
The test bed architecture used in our experiments was a Windows2000 PC with a Pentium III processor running at 1000 MHz and 320 MB RAM.
 In our experiment, we used two different datasets with different characteristics.
The first dataset, named POS, was used in the KDD-Cup 2000 competition and it is described in Zheng et al. (2001). The dataset is available from the KDD-Cup 2000 home page 1 . POS is a real-world dataset containing several years X  worth of point-of-sale data from a large electronic retailer, aggregated at the product-category level. The second dataset, named Synt, is a synthetic dataset obtained with the most commonly adopted dataset generator, available from IBM Almaden a price with each item using a uniform distribution.

Figures 3 and 4(a) report the number of transactions considered, iteration by itera-tion, by different algorithms. G&amp;T (generate and test) is a usual Apriori computation, followed by the filtering based on the monotone constraint; G&amp;T does not exploit any data-reduction technique, as it uses the whole initial dataset during the levelwise computation. ExAnte-G&amp;T is a computation composed by an ExAnte preprocessing followed by a G&amp;T computation: it reduces the dataset as much as possible at the first iteration. With Ampruning, we denote a levelwise computation, which uses the antimonotone pruning only: it corresponds to ExAMiner with a trivial monotone constraint, e.g. sum ( prices )  X  0. The difference in behavior of this computation and
ExAMiner 2 indicates how the pruning is boosted when monotone constraints are coupled with the frequency constraints.
 tone cardinality constraint, is reported in Fig. 4(b). We have experimented also FC and ExAnte-FC M (i.e. ExAnte preprocessing followed by FC experimenting with DualMiner on this dat aset because its dual pruning strategy per-forms very poorly with the cardinality cons traint. It is interesting that, even with a scarcely selective monotone constraint, s uch as the cardinality constraint, ExAM-iner outperforms significantly all other competitors.
 come more selective and/or the transactio n database larger. In Fig. 5(a) and (b), run-time comparisons on the Synt dataset with a sum reported. On this dataset, in our test-bed, FC M was not able to complete the computation due to the excessive memory request to allocate the FP tree. The Ex-
AMiner computation cut times by half, and performance improved as the monotone constraint got more selective (Fig. 5(a)) or the minimum support threshold got more selective (Fig. 5(b)).
In this article, we have described the ExAMiner algorithm for the mining of fre-quent itemsets with monotone constraint s. However, the same methodology works as well for the mining of other kinds of more complex patterns from more structured data. Indeed, in any mining domain where the antimonotone frequency constraint is conjoined with some monotone constraint s, we can exploit the same breadth-first data-reduction-based approach.
 of graphs. Inokuchi et al. (2000) and Kuramochi and Karypis (2001) have been pro-posed breadth-first Apriori-like algorithms to solve this problem. Analogously to what was done for frequent itemsets, we can conjoin the antimonotone pruning to the  X  reduction given by some monotone constraints.

Suppose that we want to mine frequent connected subgraphs containing at least four vertices from a database of graphs. The constraint of containing at least four vertices is clearly monotone. Suppose that the graph in Fig. 6(a) is one of the graphs in the input database (that is, the corre sponding of a transaction in the frequent itemsets problem). This graph contains 18 vertices and is far from not satisfying the monotone constraints. Suppose now that vertices H and L are found infrequent. We can  X  reduce this graph by pruning the two infrequent vertices. After this we obtain the graph in Fig. 6(b). The graph now contains 16 nodes but there is no connected substructure of at least 4 vertices: this graph will never participate in the support count of any interesting (solution) subgraph. Therefore, we have discovered and can be deleted (  X  reduced) from the input database.

In this last example, we had a stronger synergy between antimonotonicity and monotonicity due to the requirement of being a connected subgraph.
This article introduced ExAMiner, a fast algorithm for frequent patterns mining that exploits antimonotone and monotone constraints to optimize a levelwise, Apriori-like computation. The strength of the algorithm lies in the fact that the two kinds of constraints are used synergistically to reduce both the itemset search space and the input database as the computation proceeds. As observed in the suite of experiments,
ExAMiner exhibits a sensible improvement with respect to existing algorithms, from the different points of view of performance, reduction of the search space, and the ability to cope with extremely large transac tion databases and extremely low support thresholds. Also, in its distinctive features of dynamically reducing the transaction database, the new algorithm exhibits such dramatic reduction to make it convenient to pay for the extra I/O overhead. We found this result striking, especially in view of the fact that the current implementation of ExAMiner is rather naive, and can be engineered, in principle, to further improve its performance. One possible improve-ment would be to avoid dynamically changing the transaction database if its reduction rate falls below some given threshold. Another improvement could be obtained by dynamically shifting to the vertical representation of the transaction database (TID lists) as soon as it fits into main memory, as done in Orlando et al. (2002). Also, the new algorithm is, in principle, well suited to p arallel or distributed implementations, which are worth considering in the future. The achieved results and the possibil-ity for further improvements bring us to the conclusion that ExAMiner may enable dealing with frequent patterns queries that are, to date, considered untractable.
