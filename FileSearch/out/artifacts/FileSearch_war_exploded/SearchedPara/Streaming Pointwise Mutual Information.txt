 Recent work has led to the ability to perform space efficient counting over large vocabularies [ Talbot, 2009; Van Durme and Lall, 2009 ] . As online extensions to previous work in randomized storage [ Talbot and Osborne, 2007 ] , significant space savings are enabled if your application can tolerate a small chance of false positive in lookup, and you do not require the ability to enumerate the contents of your collection. 1 Recent interest in this area is motivated by the scale of available data outpacing the computational resources typically at hand.
 We explore what a data structure of this type means for the computation of associativity scores, or pointwise mutual information, in a streaming context. We show that approximate k -best PMI rank lists may be maintained online, with high accuracy, both in theory and in practice. This result is useful both when storage constraints prohibit explicitly storing all observed co-occurrences in a stream, as well as in cases where accessing such PMI values would be useful online. Throughout this paper we will assume our data is in the form of pairs  X  x,y  X  , where x  X  X and y  X  Y . Further, we assume that the sets X and Y are so large that it is infeasible to explicitly maintain precise counts for every such pair on a single machine (e.g., X and Y are all the words in the English language).
 We define the pointwise mutual information (PMI) of a pair x and y to be where these (empirical) probabilities are computed over a particular data set of interest. 2 Now, it is often the case that we are not interested in all such pairs, but instead are satisfied with estimating the subset of Y with the k largest PMIs with each x  X  X . We denote this set by PMI k ( x ) . Our goal in this paper is to estimate these top-k sets in a streaming fashion, i.e., where there is only a single pass allowed over the data and it is infeasible to store all the data for random access. This model is natural for a variety of reasons, e.g., the data is being accessed by crawling the web and it is infeasible to buffer all the crawled results.
 As mentioned earlier, there has been considerable work in keeping track of the counts of a large number of items succinctly. We explore the possibility of using these succinct data structures to solve this problem. Suppose there is a multi-set M = { m 1 ,m 2 ,m 3 ,... } of word pairs from X  X  Y . Using an approximate counter data structure, it is possible to maintain in an online fashion the counts is the length of the stream. The challenge for this problem is determining how to keep track of the set PMI k ( x ) for all x  X  X in an online fashion. Pointwise mutual information underlies many experiments in computational (psycho-)linguistics, going back at least to Church and Hanks [1990], who at the time referred to PMI as a mathematical formalization of the psycholinguistic association score . We do not attempt to summarize this work in its entirety, but give representative highlights below.
 Trigger Models Rosenfeld [ 1994 ] was interested in collecting trigger pairs ,  X  A,B  X  , such that the presence of A in a document is likely to  X  X rigger X  an occurrence of B . There the concern was in finding the most useful triggers overall, and thus pairs were favored based on high average mu-P (  X  As commented by Rosenfeld, the first term of his equation relates to the PMI formula given by Church and Hanks [ 1990 ] . We might describe our work here as collecting terms y , triggered by each x , once we know x to be present. As the number of possible terms is large, 3 we limit ourselves to the top-k items.
 Associated Verbs Chambers and Jurafsky [ 2008 ] , following work such as Lin [ 1998 ] and Chklovski which they termed narrative event chains ; for example, if in a given document someone pleaded , admits and was convicted , then it is likely they were also sentenced , or paroled , or fired . Prior to enforcing a temporal ordering (which does not concern us here), Chambers and Jurafsky acquired clusters of related verb-argument pairs by finding those that shared high PMI.
 Associativity in Human Memory Central to their rational analysis of human memory, Schooler and Anderson [ 1997 ] approximated the needs odds , n , of a memory structure S as the product of recency and context factors, where the context factor is the product of associative ratios between S If we take x to range over cues, and y to be a memory structure, then in our work here we are storing the identities of the top-k memory structures for a given cue x , as according to strength of associativity. 4 We first discuss the difficulty in solving the online PMI problem exactly. An obvious first attempt at an algorithm for this problem is to use approximate counters to estimate the PMI for each pair in the stream and maintain the top-k for each x using a priority queue. This method does not work, as illustrated by the examples below.
 Example 1 (probability of y changes) : Consider the stream lg (5 / 4) and PMI( x,z ) = lg 1 / 5 lg (1 . 25) . However, during the second half of the stream we never encounter x and hence never update its value. So, the naive algorithm behaves erroneously.
 What this example shows is that not only does the naive algorithm fail, but also that the top-k PMI of some x may change (because of the change in probability of y ) without any opportunity to update PMI Next, we show another example which illustrates the failure of the naive algorithm due to the fact that it does not re-compute every PMI each time.
 Example 2 (probability of x changes) : Consider the stream ever, since we didn X  X  re-compute PMI( x,y ) , we erroneously output xy .
 We next formalize these intuitions into a lower bound showing why it might be hard to compute every PMI k ( x ) precisely. For this lower bound, we make the simplifying assumption that the size of the set X is much smaller than N (i.e., | X | X  o ( N ) ), which is the usual case in practice. Theorem 1: Any algorithm that explicitly maintains the top-k PMIs for all x  X  X in a stream of length at most n (where | X | X  o ( n ) ) in a single pass requires  X ( n | X | ) time.
 We will prove this theorem using the following lemma: Lemma 1: Any algorithm that explicitly maintains the top-k PMIs of | X | = p + 1 items over a stream of length at most n = 2 r + 2 p + 1 in a single pass requires  X ( pr ) time.
 Proof of Lemma 1: Let us take the length of the stream to be n , where we assume without loss of stream: Suppose that we are interested in maintaining only the top-PMI item for each x i  X  X (the proof easily generalizes to larger k ). Let us consider the update cost for only the set X p = { x 1 ,...,x p } X  X p have a higher PMI with y 2 than y 1 . However, after we see two copies of x p +1 y 2 , the PMI of y 1 is higher than that of y 2 for each x  X  X p . Similarly, the top-PMI of each element of X p alternates between y 1 and y 2 for the remainder of the stream. Now, the current PMI for each element of X p must be correct at any point in the stream since the stream may terminate at any time. Hence, by construction, the top PMI of x 1 ,...,x p will change at least r times in the course of this stream, for a total of at least pr operations. The length of the stream is n = 2 p + 2 r + 1 . This completes the proof of Lemma 1.
 Proof of Theorem 1: Taking | X | = p + 1 , we have in the construction of Lemma 1 that r = (  X ( operations.
 Hence, there must be a high update cost for any such algorithm. That is, on average, any algorithm must perform  X ( | X | ) operations per item in the stream. The lower bound from the previous section shows that, when solving the PMI problem, the best one can do is effectively cross-check the PMI for every possible x  X  X for each item in the stream. In practice, this is far too expensive and will lead to online algorithms that cannot keep up with the rate at which the input data is produced. To solve this problem, we propose a heuristic algorithm that sacrifices some accuracy for speed in computation.
 Besides keeping processing times in check, we have to be careful about the memory requirements of any proposed algorithm. Recall that we are interested in retaining information for all pairs of x and y , where each is drawn from a set of cardinality in the millions. Our algorithm uses approximate of all x and y since this takes considerably less space. Given these values, we can (approximately) estimate PMI( x,y ) for any  X  x,y  X  in the stream.
 We assume C xy to be based on recent work in space efficient counting methods for streamed text data [ Talbot, 2009; Van Durme and Lall, 2009 ] . For our implementation we used TOMB coun-ters [ Van Durme and Lall, 2009 ] which approximate counts by storing values in log-scale. These log-scale counts are maintained in unary within layers of Bloom filters [ Bloom, 1970 ] (Figure 1) that can be probabilistically updated using a small base (Figure 2); each occurrence of an item in the stream prompts a probabilistic update to its value, dependent on the base. By tuning this base, one can trade off between the accuracy of the counts and the space savings of approximate counting.
Figure 1: Unary counting with Bloom filters. Now, to get around the problem of having stale PMI values because the count of x changing (i.e., the issue in Example 2 in the previous section), we divide the stream up into fixed-size buffers B and re-compute the PMIs for all pairs seen within each buffer (see Algorithm 1).
 Updating counts for x , y and  X  x,y  X  is constant time per element in the stream. Insertion into a k -best priority queue requires O ( lg k ) operations. Per interval, we perform in the worst case one insertion per new element observed, along with one insertion for each element stored in the previous rank lists. requires O ( n + n lg k ) = O ( n lg k ) time, where n is the length of the stream. Note that when | B | = n we have the standard offline method for computing PMI across X and Y (not withstanding approximate counters). When | B | &lt; | X | k , we run afoul of the lower bound given by Theorem 2. Regarding space, | I |  X  | B | . A benefit of our algorithm is that this can be kept significantly smaller than | X | X | Y | , 6 since in practice, | Y | lg k . Algorithm 1 F IND -O NLINE -PMI 5.1 Misclassification Probability Bound Our algorithm removes problems due to the count of x changing, but does not solve the problem that the probability of y changes (i.e., the issue in Example 1 in the previous section). The PMI of a pair  X  x,y  X  may decrease considerably if there are many occurrences of y (and relatively few occurrences of  X  x,y  X  ) in the stream, leading to the removal of y from the true top-k list for x . We show in the following that this is not likely to happen very often for the text data that our algorithm is designed to work on.
 In giving a bound on this error, we will make two assumptions: (i) the PMI for a given x follows a Zipfian distribution (something that we observed in our data), and (ii) the items in the stream are drawn independently from some underlying distribution (i.e., they are i.i.d.). Both these assumptions together help us to sidestep the lower bound proved earlier and demonstrate that our single-pass algorithm will perform well on real language data sets.
 We first make the observation that, for any y in the set of top-k PMIs for x , if  X  x,y  X  appears in the final buffer then we are guaranteed that y is correctly placed in the top-k at the end. This is because we recompute PMIs for all the pairs in the last buffer at the end of the algorithm (line 13 of Algorithm 1). The probability that  X  x,y  X  does not appear in the last buffer can be bounded using the i.i.d. assumption to be at most where for the last inequality we use the bound | B | X | X | k that we assumed in the previous section. the top-k PMI for x with high probability. The proof for general c ( x,y ) is given next. We study the probability with which some y 0 which is not in the top-k PMI for a fixed x can displace some y in the top-k PMI for x . We do so by studying the last buffer in which  X  x,y  X  appears. The only way that y 0 can displace y in the top-k for x in our algorithm is if at the end of this buffer the following holds true: where the t subscripts denotes the respective counts at the end of the buffer. We will show that this event occurs with very small probability. We do so by bounding the probability of the following three unlikely events.
 If we assume all c ( x,y ) are above some threshold m , then with only small probability (i.e., 1 / 2 m ) will the last buffer containing  X  x,y  X  appear before the midpoint of the stream. So, let us assume that the buffer appears after the midpoint of the stream. Then, the probability that  X  x,y 0  X  appears more than (1 +  X  ) c ( x,y 0 ) / 2 times by this point can be bounded by the Chernoff bound to be at most point can be bounded by exp(  X  c ( y 0 )  X  2 / 4) . Putting all these together, we get that We now make use of the assumption that the PMIs are distributed in a Zipfian manner. Let us take the rank of the PMI of y 0 to be i (and recall that the rank of the PMI of y is at most k ). Then, by the Zipfian assumption, we have that PMI( x,y )  X  ( i/k ) s PMI( x,y 0 ) , where s is the Zipfian parameter. together to bound the probability of the event Hence, the probability that some low-ranked y 0 will displace a y in the top-k PMI of x is low. Taking a union bound across all possible y 0  X  Y gives a bound of 1 / 2 m + | Y | (exp(  X  c ( x,y 0 )  X  2 / 8) + exp(  X  c ( y 0 )  X  2 / 4)) . 7 We evaluated our algorithm for online, k -best PMI with a set of experiments on collecting verbal stemmed; e.g., wrote::ruled , fighting::endure , argued::bore . For each unique verb x observed in the stream, our goal was to recover the top-k verbs y with the highest PMI given x . 8 Readers may peek ahead to Table 2 for example results.
 Experiments were based on 100,000 NYTimes articles taken from the Gigaword Corpus [ Graff, 2003 ] . Tokens were tagged for part of speech (POS) using SVMTool [ Gim  X  enez and M ` arquez, 2004 ] , a POS tagger based on SVM light [ Joachims, 1999 ] .
 Our stream was constructed by considering all pairwise combinations of the roughly 82 (on average) verb tokens occurring in each document. Where D  X  D is a document in the collection, let D v refer to the list of verbal tokens, not necessarily unique. The length of our stream, n , is therefore: P While research into methods for space efficient, approximate counting has been motivated by a desire to handle exceptionally large datasets (using limited resources), we restricted ourselves here to a dataset that would allow for comparison to explicit, non-approximate counting (implemented through use of standard hashtables). 10 We will refer to such non-approximate counting as perfect counting . Finally, to guard against spurious results arising from rare terms, we employed the same c ( xy ) &gt; 5 threshold as used by Church and Hanks [ 1990 ] .
 We did not heavily tune our counting mechanism to this task, other than to experiment with a few different bases (settling on a base of 1.25). As such, empirical results for approximate counting should be taken as a lower bound, while the perfect counting results are the upper bound on what an approximate counter might achieve.
 We measured the accuracy of resultant k -best lists by first collecting the true top-50 elements for at different ranks of the gold standard. For example, the elements of a proposed 10-best list will optimally fully intersect with the first 10 elements of the gold standard. In the case the list is not perfect, we would hope that an element incorrectly positioned at, e.g., rank 9, should really be of rank 12, rather than rank 50.
 Using this gold standard, Figure 3(a) shows the normalized, mean PMI scores as according to rank. This curve supports our earlier theoretical assumption that PMI over Y is a Zipfian distribution for a given x . 6.1 Results In Table 1 we see that when using a perfect counter, our algorithm succeeds in recovering almost all top-k elements. For example, when k = 5 , reading 500 documents at a time, our rank lists are 97 . 31% accurate. Further, of those collected triggers that are not truly in the top-5, most were either in the top 6 or 7. As there appears to be minimal impact based on buffer size, we fixed | B | = 500 documents for the remainder of our experiments. 11 This result supports the intuition behind our misclassification probability bound: while it is possible for an adversary to construct a stream that would mislead our online algorithm, this seems to rarely occur in practice.
 Shown in Figure 3(b) are the accuracy results when using an approximate counter and a buffer size of 500 documents, to collect top-5 rank lists. Two results are presented. The standard result is based on comparing the rank lists to the key just as with the results when using a perfect counter. A problem with this evaluation is that the hard threshold used for both generating the key, and the results for perfect counting, cannot be guaranteed to hold when using approximate counts. It is possible that Table 2: Top 5 verbs, y , for x = bomb, laughed and vetoed . Left columns are based on using a perfect some  X  x,y  X  pair that occurs perhaps 4 or 5 times may be misreported as occurring 6 times or more. In this case, the  X  x,y  X  pair will not appear in the key in any position, thus creating an artificial upper bound on the possible accuracy as according to this metric. For purposes of comparison, we instrumented the approximate solution to use a perfect counter in parallel. All PMI values were computed as before, using approximate counts, but the perfect counter was used just in verifying whether a given pair exceeded the threshold. In this way the approximate counting solution saw just those elements of the stream as observed in the perfect counting case, allowing us to evaluate the ranking error introduced by the counter, irrespective of issues in  X  X ipping below X  the threshold. As seen in the instrumented curve, top-5 rank lists generated when using the approximate counter are composed primarily of elements truly ranked 10 or below. 6.2 Examples Figure 2 contains the top-5 most associated verbs as according to our algorithm, both when using a perfect and an approximate counter. As can be seen for the perfect counter, and as suggested by Table 1, in practice it is possible to track PMI scores over buffered intervals with a very high degree k -best lists are near perfect matches to those computed offline.
 When using an approximate counter we continue to see reasonable results, with some error intro-duced due to the use of probabilistic counting. The rank 1 entry reported for x = laughed exempli-fies the earlier referenced issue of the approximate counter being able to incorrectly dip below the threshold for terms that the gold standard would never see. 12 In this paper we provided the first study of estimating top-k PMI online. We showed that while a precise solution comes at a high cost in the streaming model, there exists a simple algorithm that performs well on real data. An avenue of future work is to drop the assumption that each of the top-k PMI values is maintained explicitly and see whether there is an algorithm that is feasible for the streaming version of the problem or if a similar lower bound still applies. Another promising approach would be to apply the tools of two-way associations to this problem [ Li and Church, 2007 ] . An experiment of Schooler and Anderson [ 1997 ] assumed words in NYTimes headlines operated as cues for the retrieval of memory structures associated with co-occurring terms. Missing from that report was how such cues might be accumulated over time. The work presented here can be taken as a step towards modeling resource constrained, online cue learning, where an appealing description of our model involves agents tracking co-occurring events over a local temporal window (such as a day), and regularly consolidating this information into long term memory (when they  X  X leep X ). Future work may continue this direction by considering data from human trials.
 Acknowledgements Special thanks to Dan Gildea, as well as Rochester HLP/Jaeger-lab members for ideas and feedback. The first author was funded by a 2008 Provost X  X  Multidisciplinary Award from the University of Rochester, and NSF grant IIS-0328849. The second author was supported in part by the NSF grants CNS-0905169 and CNS-0910592, funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5), and by NSF grant CNS-0716423.
