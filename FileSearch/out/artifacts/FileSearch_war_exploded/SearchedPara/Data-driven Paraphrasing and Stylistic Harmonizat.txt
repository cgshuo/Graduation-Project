 One of the key research questions in semantic under-standing of natural language is bridging the lexical gap; i.e. in absence of lexical overlap between a pair of text segments, judging their semantic content with respect to semantic similarity, entailment, or equiv-alence. The term paraphrase is used to describe se-mantic equivalence between pairs of units of text, and can be loosely defined as being interchange-able (Dras, 1999). Being able to decide if two text units are paraphrases of each other, as well as the reverse direction -generating a paraphrase for a given phrase, are ongoing efforts. Both components are useful in a number of downstream tasks. One guiding use case for the methods developed in the scope of this thesis is their applicability to automatic summarization (Nenkova et al., 2011). In extrac-tive summarization , a good summary should select a subset of sentences while avoiding redundancy. This requires detecting semantic equivalences be-tween sentences. Abstractive summarization re-quires a system to further rephrase the summary, to match space constraints, achieve fluency, or unify stylistic differences in multiple source documents. Here, a paraphrasing component can modify the ex-tracted source sentences to meet such external re-quirements. The primary focus of this work will be the development of novel methods for both detect-ing and generating paraphrases of natural language text. In the wider setting of this thesis, we are par-ticularly interested in multi-document summariza-tion (MDS). To scale to the requirements of multi-domain content, our main interest is in knowledge-free and unsupervised methods for these tasks.
The remainder of this paper is structured as fol-lows. In Section 2 we will briefly cover related work in different subareas pertaining to paraphrasing. In Section 3 we will define a number of research ques-tions, which are central to the thesis. Section 4 will then present some ongoing work in lexical substitu-tion and first steps to move towards a knowledge-free unsupervised approach. Finally, Section 5 will give a conclusion and an outlook to future work be-ing addressed in the thesis. Paraphrase-related research can roughly be catego-rized into three areas: 1. Paraphrase identification -deciding or ranking the degree of how paraphrastic two given elements are; 2. Paraphrase generation -given a text element, generate a meaning-preserving reformulation; and 3. Paraphrase extraction -given an input corpus, extract meaningful pairs of para-phrastic elements. We will cover each area briefly; an extensive, high-level summary can be found in (Androutsopoulos and Malakasiotis, 2010). 2.1 Paraphrase Identification The task of paraphrase identification is strongly re-lated to Semantic Textual Similarity (STS) and Rec-ognizing Textual Entailment (RTE) tasks. STS has most recently been addressed as a shared task at SemEval-2015 (Agirre et al., 2015), which gives a good overview of current state-of-the art methods. For the specific task of identifying pairs of para-phrases, the use of discriminative word embeddings (Yin and Sch X tze, 2015) have recently been shown to be effective. 2.2 Paraphrase Generation Paraphrase generation, being an open generation task, is difficult to evaluate. However, as a prelim-inary stage to full paraphrasing a number of lexi-cal substitution tasks have become popular for eval-uating context-sensitive lexical inference since the SemEval-2007: lexsub task (McCarthy and Navigli, 2007). A lexical substitution system aims to predict substitutes for a target word instance within a sen-tence context. This implicitly addresses the prob-lem of resolving the ambiguity of polysemous terms. Over the course of a decade, a large variety of super-vised (Biemann, 2013) and unsupervised (Erk and Pad X , 2008; Moon and Erk, 2013; Melamud et al., 2015a) approaches have been proposed for this task. 2.3 Paraphrase Extraction One of the earlier highly successful approaches to paraphrase extraction was shown by Lin and Pan-tel (2001). The main idea is an extension of the distributional hypothesis from words sharing simi-lar context to similar paths between pairs of words sharing the same substituting words. Thus, a set of similar paths are obtained which can be regarded as prototypical paraphrases. A notable later method to extract a large database of paraphrases makes use of parallel bilingual corpora. The bilingual pivoting method (Bannard and Callison-Burch, 2005) aligns two fragments within a source language based on an overlap in their translation to a  X  X ivoting X  language. The paraphrase database, PPDB (Ganitkevitch et al., 2013) was obtained by applying this approach to large corpora. We define a number of research questions (RQ), which have been partially addressed, and shall also provide a guiding theme to be followed in future work.
 RQ 1: How can the lexical substitution task be solved without prior linguistic knowledge? Ex-isting approaches to lexical substitution rely fre-quently on linguistic knowledge. A lexical resource, such as WordNet (Fellbaum, 1998), is used to ob-tain a list of candidate substitutes, and focus is then shifted towards a ranking-only task. State-of-the-art unsupervised systems can still be improved by leveraging lexical resources as candidate selection filters 1 . We argue that this is related mostly to se-mantic word relations . Whereas some semantic re-lations (synonymy, hypernymy) are well suited for substitution, other relations (antonymy, opposition) are indicators for bad substitutes. Unsupervised dis-tributed methods are susceptible to not recognizing these different word relations, as they still share sim-ilar contexts. As part of this research question, we investigate how knowledge-free approaches can be used to overcome this lack of semantic information. We elaborate on this RQ in Section 4.2.
 RQ 2: What is the gap between lexical substi-tution and full paraphrasing? We aim to fur-ther examine the remaining gap to a full paraphras-ing system that is not restricted to single words. As a first step, we extend lexical substitution to multi-word expressions (MWE). As most existing research considers only the restricted case of single words, the adaptation of existing features and methods to nominal phrases, and more complex MWEs, will be investigated in detail. Furthermore, the lexical sub-stitution task is conventionally only defined as pro-viding a ranked list of lemmas as target substitutes. In general, directly replacing the target word in the existing context results in a syntactically incorrect sentence. This is is the case for languages with in-flection, but also for words with discontinuous ex-pressions, which may require restructuring the sen-tence. As a next step we plan on leveraging mor-phological tagging (Schmid and Laws, 2008) to ap-ply syntactic reformulation, by adapting a rule-based transformation framework (Ruppert et al., 2015). RQ 3: How can a paraphrasing system be em-ployed for stylistic harmonization? In multi-document summarization, source documents fre-quently originate from different text genres. E.g. a news document employs a different writing style than a blog post or a tweet. Detecting such stylis-tic variation across genres has received some atten-tion (Brooke and Hirst, 2013). Recently, stylistic in-formation has successfully induced for paraphrases within PPDB (Pavlick and Nenkova, 2015). Using a simple log ratio of observation probability of a given phrase across distinct domains, the style of the phrase could be mapped in a spectrum for multiple dimensions, such as formal / casual or simple / com-plex . When generating a summary containing such different genres, fluency and coherence of the result-ing document have to be considered. To improve summaries, a system could perform the following steps 1. Given an input corpus, identifying different 2. Given an input sentence and its source style , We can achieve this by considering the difference of distributional expansions across multiple domains. For example, the trigram context  X  X our _ passen-gers X  might frequently be expanded with  X  X ircraft X  in a news-domain corpus, whereas a tweet domain more frequently uses  X  X irplane X , with both expan-sions being distributionally similar. We can thus learn that  X  X ircraft X  could be a substitution to adapt towards news-style language and selectively per-form such replacements.
 RQ 4: Can we exploit structure in monolingual corpora to extract paraphrase pairs? Para-phrase databases, such as PPDB (Ganitkevitch et al., 2013), are constructed from bilingual parallel cor-pora. Here an assumption is used that equivalent text segments frequently align to the same segment in a different  X  X ivoting X  language. The center of this RQ is the goal to extract paraphrase pairs, similar to PPDB, from monolingual corpora by exploiting different structure. One such structure can be seen in news corpora. When given a document times-tamp, it is possible to exploit the notion of bursti-ness to find out if two documents are related to the same or different events. We aim to adapt tech-niques aimed at summarization to extract pairs of paraphrases (Christensen, 2015). 4.1 Delexicalized lexical substitution In a first work we address RQ 1 and perform lexi-cal substitution in a previously unexplored language. With GermEval-2015 (Miller et al., 2015), the lexi-cal substitution challenge was posed for the first time using German language data. It was shown that an existing supervised approach for English (Szarvas et al., 2013) can be adopted to German (Hintz and Biemann, 2015). Although the wider focus of the thesis will be the use of fully unsupervised meth-ods, in this first step lexical semantic resources are utilized both for obtaining substitution candidates as well as extracting semantic relation features between words. The suitability of various resources, Ger-maNet (Hamp and Feldweg, 1997), Wiktionary 2 , and further resources crawled from the web, are eval-uated with respect to the GermEval task. It was found that no other resource matches the results ob-tained from GermaNet , although its coverage is still the primary bottleneck for this system. As lexical substitution data is now available in at least three languages (English, German, and Italian), we also explore language transfer learning for lexical substi-tution. Experimental results suggest that delexical-ized features can be extended to not only generalize across lexical items, but can further train a model across languages, suggesting the model to be lan-guage independent. For this, we adapt existing fea-tures from (Szarvas et al., 2013) and extend the fea-ture space based on more recent approaches. We fol-low a state-of-the-art unsupervised model (Melamud et al., 2015b) to further define features in a syntac-tic word embedding space. As a preliminary result, feature ablation tests show that the strongest features for lexical substitution are semantic relations from multiple aggregated lexical resources. This insight motivates the next step towards a knowledge-free system. 4.2 Unsupervised semantic relation extraction Semantic relations have been identified a strong fea-tures for lexical substitution (Sinha and Mihalcea, 2009); selecting candidates based on aggregated in-formation of multiple resources usually results in good performance. Consequently, when obtain-ing substitution candidates from different sources, such as a distributional thesaurus (DT), a key chal-lenge lies in overcoming a high amount of related but not substitutable words. Prime examples are antonyms , which are usually distributionally simi-lar but no valid lexical substitutions (replacing  X  X ot X  with  X  X old X  alters the meaning of a sentence). Figure 1 illustrates this challenge at the example of an in-stance obtained from the SemEval-2007 data. Here, candidates from a DT are compared against candi-dates obtained from WordNet . Both resources yield related words (e.g.  X  X ask X ,  X  X age X ,  X  X omputer sci-ence X  are all related to the target  X  X ob X ) -however, for lexical resources we can leverage semantic re-lations as a much more fine-grained selection fil-ter beyond relatedness (in the example, entries such as  X  X omputer science X  can be excluded by discard-ing the topic relation). On the other hand, obtain-ing candidates only from a lexical resource neces-sarily limits the system to its coverage. Whereas WordNet is a high-quality resource with good cov-erage, alternatives for other languages may be of in-ferior quality or are lacking altogether. To quantify this, we have evaluated the overlap of semantic re-lations present in GermaNet (Hamp and Feldweg, 1997) with the gold substitutes in the GermEval-2015 lexsub task. Figure 2 illustrates this overlap and further shows the stages at which the resource fails. Whereas all target words are contained in Ger-maNet, only 85% of the substitutes are contained as lexical items. When considering only synonyms , hyponyms and hypernyms as relations, only 20% of all gold substitutes can be retrieved. This number constitutes an upper bound for the recall of a lexi-cal substitution system. If, instead, candidates are obtained based on distributional similarity, we can obtain a much higher upper bound on recall of sub-stitution candidates. Figure 3 shows the recall of the top-k similar words, based on a distributional the-saurus computed from a 70M sentence newspaper corpus (Biemann et al., 2007). Even when consid-ering only the top-50 most similar words, a recall of 29% can be achieved, whereas this value plateaus at about 45% -improving over the lexical resource baseline more than twofold. In summary, we make two observations: 1. Similarity-based approaches, such as distribu-2. The semantic relation between target and sub-These observations motivate a similarity-based se-lection of substitution candidates, which does not rely on knowledge-based resources. We argue that the second key component to lexical substitution is an unsupervised extraction of semantic relations. For this we follow (Herger, 2014), who leverages the extended distributional hypothesis , stating that  X  X f two paths tend to occur in similar context, the meanings of the paths tend to be similar X  (Lin and Pantel, 2001). The original motivation for this is obtaining inference rules, or equivalences between paths. For example, it can be discovered that  X  X is author of Y  X   X   X  X wrote Y  X . In reverse however, we can also discover pairs of words ( X, Y ) , which tend to be connected with the same paths. We can thus compute a DT on pairs of words rather than single words, using their path as context features. Our al- X  extract pairs ( (  X  compute path features ( ( ( gorithm for semantic relation extraction can thus be described as follows: 1. Extract pairs of words from background cor-2. Compute context features for each pair based 3. Compute the similarity between pairs of pairs, 4. Cluster pairs of words based on their similarity For step 1, we experimented with different filter-ing strategies, based on word frequency, sentence length, token distance, and POS tags. As context features we aggregate multiple variants to generalize a path: we replace the occurrence of the pair ( X, Y ) with the literal strings  X  X  X  and  X  X  X  and then extract the token substring, as well paths defined by syntac-tic dependency edges. Steps 1 and 2 are visualized in Figure 4. For steps 3 and 4, we adapt the DT computation from (Biemann and Riedl, 2013) and obtain similarity scores based on the overlap of the most salient context features, i.e. generalized paths. At this stage, we obtain a distributional similarity between pairs of words, e.g. the pair (famine, epi-demic) is distributionally similar to ( problem , cri-sis ). This resembles the notion of word analogies, which can be obtained in embedding spaces (Levy et al., 2014), however our model results in discrete notions of relations as opposed to non-interpretable vectors. For this, we cluster word pairs by applying Chinese Whispers (Biemann, 2006). Table 1 shows the final output of the semantic relation clustering exemplified for four resulting clusters. Although the data is not perfectly consistent, clusters tend to represent a similar relation between each respective pairs of words. These relations often correspond to those found in lexical resources, such as hyponymy or antonymy. However, the relations are frequently fragmented into smaller, domain-specific clusters. In the above example, Cluster 1 and Cluster 2 both correspond to a relation resembling hypernymy -however, in Cluster 1 this relation is mostly clustered for professions (e.g. a  X  X elder X  is-a-kind of  X  X lec-trician X ), whereas Cluster 2 corresponds roughly to vehicles or items (a  X  X ailboat X  is-a-kind-of  X  X oat X ). Cluster 3 can be reasonably considered as containing antonymous terms ( X  X ain X  is-opposite-of  X  X rought X ). In some cases, clusters contain relations of words not generally found in semantic resources. Cluster 4 contains word pairs having a causation relation (e.g.  X  X laucoma X  causes  X  X lindness X ); it is further inter-esting to observe that items in this cluster contain exclusively negative outcomes ( X  X llness X ,  X  X tress X ,  X  X looding X , etc.). Previous work has conventionally evaluated semantic relation extraction intrinsically with respect to a lexical resource as a gold stan-dard (Panchenko and Morozova, 2012). However, we are interested in utilizing semantic relations for paraphrasing tasks and will therefore follow up with an extrinsic evaluation in a lexical substitution sys-tem. Our goal is to leverage unsupervised clusters, e.g. as feature input, to overcome the need for lexi-cal semantic resources. In this paper we have outlined the guiding theme of a thesis exploring data-driven methods for paraphras-ing and defined a set of research questions to sketch a path for future work. We addressed the first step of lexical substitution. We showed that a supervised, delexicalized framework (Szarvas et al., 2013) can be successfully applied to a previously unexplored language. We make a number of observations on multiple language lexsub tasks: Obtaining substitu-tion candidates from lexical resources achieves best system performance, despite incurring a very low upper bound on substitution recall. Obtaining can-didates in an unsupervised manner by considering distributionally similar words increases this upper bound more than twofold, at the cost of more noise. We further observe that the strongest features in this setting are semantic relations between target and substitute, obtained from the aggregated lexical re-sources. Hence, we conclude that obtaining seman-tic relations in an unsupervised way is a key step to-wards knowledge-free lexical substitution. We con-tinue to present an unsupervised method for obtain-ing clusters of semantic relations, and show prelim-inary results. As a next step we aim at integrating such relation clusters into a lexical substitution sys-tem. We also plan on extending lexical substitu-tion towards a full paraphrasing system, by moving from single-word replacements to longer multiword expressions, as well as applying syntactic transfor-mations as a post-processing step to the substitu-tion output. In related branches of this thesis we will also explore methods for extracting paraphrases from structured corpora, and ultimately apply a two-way paraphrasing system to a multi-document sum-marization system, supporting both selection of non-redundant sentences as well as sentence rephrasing to perform harmonization of language style. This work has been supported by the German Re-search Foundation as part of the Research Training Group  X  X daptive Preparation of Information from Heterogeneous Sources X  (AIPHES) under grant No. GRK 1994/1.

