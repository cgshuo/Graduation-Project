 A basic logistic regression model includes individual effects of feature variables to the individual effects, interactions between the feature variables are impor-tant in a wide range of applications. Examples include identifying important single nucleotide polymorphism (SNPs) in genome-wide association studies [ 6 ], pathway analysis of gene-expression or metabolomic data [ 14 ], regulatory motif identification [ 7 ], and association studies of gene-gene interactions [ 16 ]. Pairwise and higher order interactions between the features can be modelled by explicitly including the interactions as feature variables. In other words, a k th order interaction can be modelled by merging a subset of k variables into a new variable whose domain becomes the Cartesian product of the domans of the merged variables, and including indicator variables for each of the values in the new domain. High order interactions pose statistical and computational challenges since the number of interaction terms grows exponentially with the order, k . Different techniques for dealing with these challenges have been pro-posed. These include, for instance, forward and backward selection (see e.g. [ 5 ]) and more recently, Lasso [ 15 ].
 We consider situations where the interactions can be expressed as Boolean formulas, each of which is composed of a subset of the original feature variables connected by logical operations such as and , or ,and xor example, if we have a vector of m binary variables x = { x model may involve terms such as  X  x 1 and x 2 ,or x 3  X  X r X  x We define the model formally as a generalized linear model where y is a binary response variable, E ( y )=Pr[ y =1 | tation of y conditional on the regressors x , g is a link function,  X  regression coefficients, and L 1 ,...,L t are Boolean functions of x . The right-hand side of Eq. ( 1 ) is called a linear predictor . Depending on how we define the link function g , this framework includes a range of different model fam-ilies, such as linear regression, logistic regression, Poisson regression, etc. In this study, we focus on the logistic case where the link function is defined as g ( E ( y )) = logit( E ( y )) = log( E ( y ) / (1  X  E ( y ))).
 define the order of the function as the minimum number of variables x that are sufficient to determine the value of L i . For example, a function that depends on only one of the variables is said to be first order, and so on. The order of the model is defined as the maximum order of all the functions involved. As mentioned above, the basic logistic regression model is usually defined as a first order model that includes all the m first order functions L 1  X  i  X  t = m . 1.1 Related Work An important prior work regarding logical features was done by Ruczinski et al. in [ 11 ] who also provide an implementation of their method in the R package LogicReg . 1 It uses a greedy algorithm to search through the space of possi-ble Boolean functions, with additional simulated annealing step to avoid local optima. It shows better performance than the tree based or rule based methods that are used by CART [ 2 ] and MARS [ 4 ]. However, as the number of covariates and the order of interactions are increased, the number of possible Boolean func-tions grows significantly. This makes greedy search heuristics computationally inefficient and prone to converge to local optima.
 defined using the and operator. To allow more types of Boolean operations, the to guide the learning process. However, these methods need expert knowledge that is not always available. They are also unable to handle complex interactions. Both methods are only suitable for cases when all coefficients inside a group are either zero or non-zero. Our new Lasso based method can efficiently and effectively learn arbitrary logical functions and deal with situations when the covariates have multiple values.
 The least absolute shrinkage and smoothing operator (Lasso) for linear regressions was proposed by Tibshirani in 1996 [ 15 ] and has since then gained a lot of popularity in subset selection problems. Lasso aims to minimize the sum of squared errors subject to a bound on the sum of absolute values of the regression coefficients, i.e., the L 1 norm of the coefficient vector  X  =(  X  where Y =( y (1) ,...,y ( n ) ) T is a vector of n responses and X is a matrix whose rows are n observation vectors x (1) ,..., x ( n ) ,and  X &gt; 0 is a regularization parameter.
 Lasso encourages sparsity in the estimated coefficient vector. As the value of  X  is decreased, more and more coefficients are set to zero. This feature is especially useful when we need to identify a small set of relevant variables out of a large collection of candidates. Thus Lasso is suitable for discovering interactions in regression problems. Furthermore, there are plenty of well-developed tools for solving Lasso problems efficiently.
 If we encode logical features constructed using the original variables as a new set of regressor variables, we can use Lasso to select the significant ones out of all possible interactions. However, the number of all possible logical expres-sions grows too rapidly to be handled efficiently. Furthermore, the redundancy caused by different combinations of logical formulas expressing the exact same model may lead to numerical and statistical instability. To restrict the number of predictors within a manageable size, previous work in [ 13 ] confines the logic expressions to include only two variables and the and operator. 1.2 Contributions In this paper we introduce a Lasso-based method for learning sparse logistic regression models with logical features. Technically, the method is implemented as a transformed Lasso [ 10 ] which involves a transformation matrix that multi-plies the original design matrix X . The transformed Lasso can deal with any type of logical interactions. Here we also extend it to handle multivalued covariates. In the following sections, we first propose a transformation of the original data involving a generalized Walsh-Hadamard matrix. The transformation increases the dimension of the data but enables the learning of arbitrarily complex logical dependencies. We demonstrate the power of the proposed method by comparing its performance to that of the greedy method in LogicReg with different settings of sample sizes and model complexities. Finally, we propose several possible future extensions.
 As is well known, any Boolean formula can be decomposed as a linear combina-tion of xor functions of the same or lower orders as the formula itself. For exam-ple, to represent and or or formulas over a subset of indices I by using linear combinations of xor formulas, we can write them respectively as functions of subsets of the covariates, it is convenient to use the discrete Walsh-Hadamard transform, see [ 10 ]. To construct the design matrix, we first expand the original covariate matrix into a larger matrix with columns corresponding to indicator variables for each possible covariate vector (e.g., in the case of two binary variables: 00, 10, 01, and 11), and then (pre-)multiply this matrix by the Walsh-Hadamard matrix. For a more complete explanation including a detailed example, see [ 10 ]. As we explain below, in practice the matrices need not be explicitly constructed.
 vectors composed of the m covariates. The columns of a such matrix are functions on all subsets of the covariates given by the corresponding row. For example, the rows of a fourth order Walsh-Hadamard matrix correspond to all combinations of two binary elements x 1 and x 2 , while the columns are xor ( x is then matrix in the similar way as forming the Walsh-Hadamard matrix. Each column corresponds to a xor function of a subset of possible sequences while each vari-with values { 0 , 1 , 2 } , the columns relating to two variables x xor ( x xor ( x the xor functions when x 1 or x 2 equals 2, because we can represent them as linear combinations of xor functions when x 1 or x 2 equals 0 or 1. Such lin-ear dependencies would significantly complicate the parameter estimation stage. The design matrix W 9 based on the Walsh-Hadamard matrix for two ternary variables is In practice, when the number of covariates is large, it becomes more likely that we only observe a small subset of all possible combinations of variables. In this case we do not need to build the full Walsh-Hadamard matrix. For each observed combination, we can map it to a vector of the corresponding functions directly.
 For example, assume that we have five binary variables { x the linear predictor only depends on the first three variables: g ( E ( y )) =  X   X  x +  X  2 and ( x 2 ,x 3 ). If we restrict the maximum order of the interactions to be three, the design matrix has 5 0 + 5 1 + 5 2 + 5 3 = 1 + 5 + 10 + 10 = 26 columns corresponding to the xor functions with at most third order interactions. If the predictors: xor (0), xor ( x 1 ), xor ( x 2 ), xor ( x 3 ), and zero. Then we can apply the xor functions with non-zero coefficients in new data sets for prediction. In the following experiments on simulated data sets, we show that even when the number of xor functions is relatively large, the learned models quickly converge toward the generating model as the sample size is increased. We compare the greedy search method in the R package LogicReg with trans-formed Lasso for different models and sizes of training data in three experiments. For Lasso regression, we use the popular R package glmnet . For each model and size of training data, we generated 100 different training data sets based on the true model. Later we compare the log-losses of learned models by LogicReg and transformed Lasso by evaluating them on another 100 new data sets with sample sizes 1024. We ran all the experiments on computers with 32 GB RAM and 2 . 53 GHz CPUs using only a single core at a time.
 3.1 Experiment 1 First, we use the true models: logit( E ( Y )) = 0 . 5  X  1 . 3 or [ and (  X  x 1 ,  X  x 2 ) ,x logit( E ( Y )) = 0 . 5  X  1 . 3 or [ and (  X  x 1 ,  X  x 2 ) ,x logit( E ( Y )) = 0 . 5  X  1 . 3 or [ and (  X  x 1 ,  X  x 2 ) ,x The linear predictors contain three, five or seven separate terms (not includ-ing intercepts) that have no xor operators. The terms included in the simpler linear predictors (three and five terms) are subsets of the terms in the more com-plex linear predictors. Each term in the linear predictor involves three covariates. The covariates are independent from each other and generated with equal prob-abilities for 1s and 0s.
 For the LogicReg method, we restrict the search space by no more than three variables in a term and no more than five separate terms. is performed by 10-fold cross validation. For the method based on the Lasso framework, we also consider only up to third order interactions. The best tuning parameter  X  in Lasso is also determined by 10-fold cross validation. The sam-ple size ranges from 64 to 8192. The total number of covariates is 30 or 40 of which all but the ones appearing in Eq. 7 (a) X (c) have no effect on the response. For example, the response of model in Eq. ( 7 a) only depends on nine variables, x ,...,x 9 , while covariates x 10 ,...,x m ( m = 30 or 40) are irrelevant. We repeat the experiment on 100 different training sets for each combination of sample size and number of covariates. The log-likelihoods achieved by both methods are compared to the log-likelihoods under the true model on 100 independently generated samples of size 1024.
 can see that when the sample size reaches 8192, both methods have almost converged to the same estimated models with small log-loss. The Lasso X  X  ability to shrink most of the unimportant coefficients plays a key role in achieving a similar level of performance already from the small samples sizes, unlike the LogicReg method. For instance, with 40 covariates, selecting the required number of covariates (under 100) out of the 10 701 candidates seems to be very hard for LogicReg up until sample size 1024.
 the sense of both smaller average log-losses and smaller variance between the rep-etitions. But when the sample size is relatively small, the greedy search method is very unstable. This is because it needs to pick the right Boolean terms out of a much larger number of possible terms  X  recall that the number of features considered in the LogicReg method is significantly greater than in transformed Lasso because LogicReg includes all possible Boolean operators while the latter method only includes xor operators. Moreover, the Lasso method has no local optima unlike the greedy search applied in LogicReg.
 covariates. This is because the number of terms to be considered in both methods grows with the number of covariates. For example, the number of included in the design matrix in transformed Lasso for 30 variables is 4526 and for 40 variables 10 701. On the other hand, the (negative) effect of increasing the number of covariates is not very significant compared to the (positive) effect of increasing the sample size except in the sense that as the number of variables is increased, the computational cost of both methods increases sharply. 3.2 Experiment 2 In the second experiment, we replace the and and or operators in Eq. ( 7 )bythe xor operator while keeping the coefficients unchanged. The new data generating models are logit( E ( Y )) = 0 . 5  X  1 . 3 xor (  X  x 1 ,  X  x 2 ,x 3 ) logit( E ( Y )) = 0 . 5  X  1 . 3 xor (  X  x 1 ,  X  x 2 ,x 3 ) All other experiment settings are the same as in the first experiment. Comparing iment, we find that for small sample sizes, the transformed Lasso method has difficulty in fitting the true models if they contain many ever, it performs better for sample sizes larger than 1024. On the other hand, the greedy search method fails to find acceptable models in the second experiment even when the sample size is 8196.
 For the models in the second experiment, we need less non-zero coefficients in the transformed Lasso method, because it contains only instance, if we have three input terms, we only need three predictors with non-tor of three terms in the first experiment, we need to decompose it into fifteen xor functions. There are less correct xor terms in the second experiment, thus each xor term is comparatively more important. For small sample sizes, when there is not enough information, it becomes much harder to identify the correct xor functions. For the linear predictors in the second experiment, any incorrect choice of xor term decreases the accuracy much more significantly than in the first experiment. Therefore, transformed Lasso performs worse when the sample size is small in the second experiment than in the first experiment. On the other coefficients on responses. When we have enough training data, it becomes easier for transformed Lasso to identify the right terms in the second experiment. On the other hand, the greedy search method in LogicReg represents Boolean functions only by and , or or negation operators. Thus, it needs to construct much more complex expressions in the second experiment. For example, the term xor ( x with the repeating use of x 1 , x 2 and x 3 . It requires exploring an extremely large model space. Moreover, the greedy search method starts by picking up the most significant variables. However, a single variable in a much weaker effect in responses, which makes it very hard for the greedy search method to find a good starting point. Furthermore, in the following process, it can only modify a current model by adjusting one variable or one operator at each step. Although the simulated annealing method are incorporated to avoid local optima, an updated model should be reachable by a single move from the previous one. Therefore, a proper starting point is crucial for the greedy search method, which is difficult to find in the second experiment. Even when the sample size is as large as 8192, the learned models by the greedy search method are far from close to the true ones. 3.3 Experiment 3 For the last experiment, we show how the two methods work when variables have multiple values. We use the similar data generating models as in the first experiment, but allow the variables to take one of the three values: true models are: actions between ternary covariates as described in Sect. 2 . On the other hand, because the method used by LogicReg requires binary input, we add dummy variables to indicate when a covariate takes one of the three values. than the greedy search method under all sample sizes and numbers of covariates. The ternary case is more difficult for both methods than the binary case because it has much larger search spaces for both methods. However, both methods still show their power to learn good model if there are enough data. When the sample size reaches 4096, both methods converge to true models for ternary covariates as well as for binary covariates.
 forms better than the greedy search method in all the cases with models of differ-ent complexities and training sample sizes. The greedy search method achieves reasonable results only when there is a decent number of samples and input functions are simple. However, it has very large log-losses as well as large vari-ances when the sample size is less than 512 in all different settings in the three experiments. But in real life studies, a relative small number of training samples is very common. Moreover, the greedy search method fails when the interaction includes complex operators like xor , which makes the responses less affected by any single covariate involved in the interaction.
 ple case with three terms in the linear predictor, a total of 30 covariates and sample size 128 in the first experiment, we need on average 373 s to perform model learning by the greedy search method in LogicReg, but only 6 . 0s for the transformed Lasso method. For the more complex model with seven terms including xor operators, a total of 40 covariates and sample size 8196 in the second experiment, LogicReg uses on average 15 920 s, whereas the Lasso based method needs only 1 580 s. In this study, we propose on approach to learning sparse logistic models with logical features of multivalued inputs. The same approach can also be applied in other types of regression models such as Poisson regression and Cox proportional hazards models. In our experiments with simulated data, our Lasso based method was able to estimate different models based on and , or ,and their combinations more effectively than the earlier LogicReg method. More extensive experiments, including a comparison to other types of state-of-the-art classification techniques will provide more detailed information about the performance of the proposed method.
 In future work, the proposed approach can be extended in several directions. Firstly, for handling a large number of variables, we can use the LIBLINEAR [ 3 ] library that scales better for large sparse data. Even then, a very large number of covariates will necessarily pose problems to methods that include high order interactions. For example, most genome wide studies may include hundreds of thousands genomic covariates. Many existing approaches include a screening stage to narrow down the set of candidate covariates to a manageable number. This can be done either by exploiting expert knowledge or by other statistical methods, see, e.g. [ 6 , 12 ]. Exploring suitable screening methods for the trans-formed Lasso with very high dimensional data is an interesting research direction that is necessary for many genomics applications.
 Moreover, it can be helpful to integrate additional assumptions concerning be achieved by modifying the Lasso penalization in various ways. For instance, it may be reasonable to assume that if a given high order coefficient takes a non-zero value, the lower order interactions among the variables that are included in the higher order interactions are more likely to be non-zero as well. Different group Lasso techniques are available for achieving this [ 9 ]. Furthermore, Lasso tends to select most significant variables out of a group of correlated variables. Integrating the structure of predictors can also be beneficial in the case when the covariates are highly correlated.

