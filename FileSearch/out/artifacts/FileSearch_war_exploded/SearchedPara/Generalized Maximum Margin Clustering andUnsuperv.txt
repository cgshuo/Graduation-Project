 Data clustering, the unsupervised classification of samples into groups, is an important re-search area in machine learning for several decades. A large number of algorithms have been developed for data clustering, including the k-means algorithm [3], mixture models [4], and spectral clustering [5, 6, 7, 8, 9]. More recently, maximum margin clustering [1, 2] was proposed for data clustering and has shown promising performance. The key idea of maxi-mum margin clustering is to extend the theory of support vector machine to unsupervised learning. However, despite its success, the following three major problems with maximum margin clustering has prevented it from being applied to real-world applications: Figure 1: The scalability of the original maximum margin clustering algorithm versus the generalized maximum margin clustering algorithm Figure 2: Clustering error of spectral clustering using the RBF kernel with different kernel width. The horizonal axis of Figure 2(b) represents the percentage of the distance range (i.e., the difference between the maximum and the minimum distance) that is used for kernel width. In this paper, we propose  X  generalized maximum margin clustering  X  framework that resolves the above three problems simultaneously. In particular, the proposed framework reformulates the problem of maximum margin clustering to include the bias term in the classification boundary, and therefore remove the assumption that clustering boundaries have to pass through the origins. Furthermore, the new formulism reduces the number of parameters to be linear in the number of examples, and therefore significantly reduces the computational cost. Finally, it is equipped with the capability of unsupervised kernel learning, and therefore, is able to determine the appropriate kernel matrix and clustering memberships simultaneously. More interestingly, we will show that spectral clustering, such as the normalized cut algorithm, can be viewed as a special case of the generalized maximum margin clustering.
 The remainder of the paper is organized as follows: Section 2 reviews the work of maximum margin clustering and kernel learning. Section 3 presents the framework of generalized maximum margin clustering. Our empirical studies are presented in Section 4. Section 5 concludes this work. The key idea of maximum margin clustering is to extend the theory of support vector machine can be written as: where K  X  R n  X  n is the kernel matrix and diag( y ) stands for the diagonal matrix that uses the vector y as its diagonal elements. To apply the above formulism to unsupervised learning, the maximum margin clustering approach relaxes class labels y to continuous variables, and searches for both y and  X  that maximizes the classification margin. This leads to the following optimization problem: where  X  stands for the element wise product between two matrices. To convert the above problem into a convex programming problem, the authors of [1] makes two important relax-ations. The first one relaxes yy &gt; into a positive semi-definitive (PSD) matrix M  X  0 whose diagonal elements are set to be 1. The second relaxation sets  X  = 0, which is equivalent to assuming that there is no bias term b in the expression of classification boundaries, or in other words, classification boundaries have to pass through the origins of data. These two assumption simplify the above optimization problem as follows: Finally, a few additional constraints of M are added to the above optimization problem to prevent skewed clustering sizes [1]. As a consequence of these two relaxations, the number of parameters is increased from n to n 2 , which will significantly increase the computational cost. Furthermore, by setting  X  = 0, the maximum margin clustering algorithm requires clustering boundaries to pass through the origins of data, which is unsuitable for clustering data with unbalanced clusters.
 Another important problem with the above maximum margin clustering is the difficulty in determining the appropriate kernel similarity matrix K . Although many kernel based clustering algorithms set the kernel parameters manually, there are several studies devoted to automatic selection of kernel functions, in particular the kernel width for the RBF kernel, i.e.,  X  in exp 20% of the range of the distance between samples. However, in our experiment, we found that this is not always a good choice, and in many situations it produces poor results. Ng et al. [9] chose kernel width which provides the least distorted clusters by running the same clustering algorithm several times for each kernel width. Although this approach seems to generate good results, it requires running seperate experiments for each kernel width, and therefore could be computationally intensive. Manor et al. in [10] proposed a self-tuning spectral clustering algorithm that computes a different local kernel width for each data point x . In particular, the local kernel width for each x i is computed as the distance of x i to its k th nearest neighbor. Although empirical study seems to show the effectiveness of this approach, it is unclear how to find the optimal k in computing the local kernel width. As we will see in the experiment section, the clustering accuracy depends heavily on the choice of k .
 Finally, we will briefly overview the existing work on kernel learning. Most previous work focus on supervised kernel learning. The representative approaches in this category include the kernel alignment [11, 12], semi-definitive programming [13], and spectral graph parti-tioning [6]. Unlike these approaches, the proposed framework is designed for unsupervised kernel learning. We will first present the proposed clustering algorithm for hard margin, followed by the extension to soft margin and unsupervised kernel learning. 3.1 Hard Margin In the case of hard margin, the dual problem of SVM is almost identical to the problem in Eqn. (1) except that the parameter  X  does not have the upper bound C . Following [13], we further convert the problem in (1) into its dual form: where e is a vector with all its elements being one. Unlike the treatment in [13], which rewrites the above problem as a semi-definitive programming problem, we introduce vari-ables z that is defined as follows: Given that  X   X  0, the above expression for z is essentially equivalent to the constraint | z follows: Note that the above problem may not have unique solutions for z and  X  due to the translation invariance of the objective function. More specifically, given an optimal solution z and  X  , we may be able to construct another solution z 0 and  X  0 such that: Evidently, both solutions result in the same value for the objective function in (4). Fur-to the problem in SVM where the bias term b may not be unique [14]. To remove the trans-into the objective function, i.e. where constant C e weights the important of the punishment factor against the original further define Then, the problem in (4) becomes where e 0 is a vector with all its elements being 1 except its last element which is zero. We then construct the Lagrangian as follows diagonal element which is 1. Hence, the dual problem of (6) is Finally, the solution w can be computed using the KKT condition, i.e.,  X  In other words, the solution w is proportional to the eigenvector of matrix  X  Remark I It is important to realize that the problem in (5) is non-convex due to the non-convex constraint w 2 i  X  1. Thus, the optimal solution found by the dual problem in (7) is not necessarily the optimal solution for the prime problem in (5). Our hope is that although the solution found by the dual problem is not optimal for the prime problem, it is still a good solution for the prime problem in (5). This is similar to the SDP relaxation made by the maximum margin clustering algorithm in (2) that relaxes a non-convex programming problem into a convex one. However, unlike the relaxation made in (2) that increases the number of variables from n to n 2 , the new formulism of maximum margin does not increase the number of parameters (i.e.,  X  ), and therefore will be computational more efficient. This is shown in Figure 1, in which the computational time of generalized maximum margin clustering is increased much slower than that of the maximum margin algorithm. D is a diagonal matrix whose diagonal elements are computed as D i,i = operator of pseudo inverse. More interesting, we have the following theorem showing the relationship between generalized maximum margin clustering and the normalized cut. Theorem 1. The normalized cut algorithm is a special case of the generalized maximum 1 , 2 , . . . , n , and (3) C e  X  1 .
 Proof sketch: Given the conditions 1 to 3 in the theorem, the new objective function in (7) becomes: max of  X 
L ( K ) . 3.2 Soft Margin We extend the formulism in (7) to the case of soft margin by considering the following problem: where C  X  weights the importance of the clustering errors against the clustering margin. Similar to the previous derivation, we introduce the slack variable z and simplify the above problem as follows: as: The main difference between the above formulism and the formulism in (7) is the introduc-tion of the upper bound C  X  for  X  in the case of soft margin. In the experiment, we set the parameter C  X  to be 100 , 000, a very large value. 3.3 Unsupervised Kernel Learning As already pointed out, the performance of many clustering algorithms depend on the right choice of the kernel similarity matrix. To address this problem, we extend the formulism in (10) by including the kernel learning mechanism. In particular, we assume that a set of combination of kernel matrices, i.e., K = accuracy. More specifically, we need to solve the following optimization problem: Unfortunately, it is difficult to solve the above problem due to the complexity introduced by ( structed from the kernel similarity matrix K i . We then defined the inverse of the combined By solving the above problem, we are able to resolve both  X  (corresponding to clustering memberships) and  X  (corresponding to kernel learning) simultaneously. We tested the generalized maximum margin clustering algorithm on both synthetic datasets and real datasets from the UCI repository. Figure 3 gives the distribution of the synthetic datasets. The four UCI datasets used in our study are  X  X ote X ,  X  X igits X ,  X  X onosphere X , and  X  X reast X . These four datasets comprise of 218, 180, 351, and 285 examples, respectively, and each example in these four datasets is represented by 17, 64, 35, and 32 features. Since the  X  X igits X  dataset consists of multiple classes, we further decompose it into four datasets algorithm [8] and the maximum margin clustering algorithm [1] are used as the baseline. The RBF kernel is used throughout this study to construct the kernel similarity matrices. In our first experiment, we examine the optimal performance of each clustering algorithm by using the optimal kernel width that is acquired through an exhaustive search. The opti-mal clustering errors of these three algorithms are summarized in the first three columns of Table 1. It is clear that generalized maximum margin clustering algorithm achieve similar or better performance than both maximum margin clustering and normlized cut for most datasets when they are given the optimal kernel matrices. Note that the results of maxi-mum margin clustering are reported for a subset of samples(including 80 instances) in UCI datasets due to the out of memory problem.
 Table 1: Clustering error (%) of normalized cut (NC), maximum margin clustering (MMC), generalized maximum margin clustering (GMMC) and self-tuning spectral clustering (ST). Dataset Optimal Kernel Width Unsupervised Kernel Learning Two Circles 2 0 0 0 0 50 Two Jointed Circles 7 6.25 0 0 1 45 Two Gaussian 1.25 2.5 1.25 3.75 5 7.5 Vote 25 15 9.6 11.90 11 40 Digits 3-8 35 10 5.6 5.6 5 50 Digits 1-7 45 31.25 2.2 3 0 47 Digits 2-7 34 1.25 .5 5.6 1.5 50 Digits 8-9 48 3.75 16 12 9 48 Ionosphere 25 21.25 23.5 27.3 26.5 48 Breast 36.5 38.75 36.1 37 37.5 41.5 In the second experiment, we evaluate the effectiveness of unsupervised kernel learning. Ten kernel matrices are created by using the RBF kernel with the kernel width varied from 10% to 100% of the range of distance between any two examples. We compare the proposed unsupervised kernel learning to the self-tuning spectral clustering algorithm in [10]. One of the problem with the self-tuning spectral clustering algorithm is that its clustering error usually depends on the parameter k , i.e., the number of nearest neighbor used for computing the kernel width. To provide a full picture of the self-tuning spectral clustering, we vary k from 1 and 15 , and calculate both best and worst performance using different k . The last three columns of Table 1 summarizes the clustering errors of generalized maximum margin clustering and self-tuning spectral clustering with both best and worst k . First, observe the big gap between best and worst performance of self-tuning spectral clustering with for most datasets, generalized maximum margin clustering achieves similar performance as self-tuning spectral clustering with the best k . Furthermore, for a number of datasets, the unsupervised kernel learning method achieves the performance close to the one using the optimal kernel width. Both results indicate that the proposed algorithm for unsupervised kernel learning is effective in identifying appropriate kernels. In this paper, we proposed a framework for the generalized maximum margin clustering. Compared to the existing algorithm for maximum margin clustering, the new framework has three advantages: 1) it reduces the number of parameters from n 2 to n , and therefore has a significantly lower computational cost, 2) it allows for clustering boundaries that do not pass through the origin, and 3) it can automatically identify the appropriate kernel similarity matrix through unsupervised kernel learning. Our empirical study with three synthetic datasets and four UCI datasets shows the promising performance of our proposed algorithm.
 [1] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum margin clustering. In [2] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vec-[3] J. Hartigan and M. Wong. A k-means clustering algorithm. Appl. Statist. , 28:100 X 108, [4] R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the em [5] C. Ding, X. He, H. Zha, M. Gu, and H. Simon. A min-max cut algorithm for graph [6] F. R. Bach and M. I. Jordan. Learning spectral clustering. In Advances in Neural [7] R. Jin, C. Ding, and F. Kang. A probabilistic approach for optimizing spectral clus-[8] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on [9] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. [10] L. Zelnik-Manor and P. Perona. Self-tuning spectral clustering. In Advances in Neural [11] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. S. Kandola. On kernel-target [12] X. Zhu, J. Kandola, Z. Ghahramani, and J. Lafferty. Nonparametric transforms of [13] G. R. G. Lanckriet, N. Cristianini, P. L. Bartlett, Laurent El Ghaoui, and Michael I. [14] C. J. C. Burges and D. J. Crisp. Uniqueness theorems for kernel methods. Neurocom-
