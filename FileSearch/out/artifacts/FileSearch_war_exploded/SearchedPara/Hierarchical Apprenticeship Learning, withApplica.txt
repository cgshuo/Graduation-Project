 In this paper we consider apprenticeship learning in the setting of large, complex domains. While most reinforcement learning algorithms operate under the M arkov decision process (MDP) formal-ism (where the reward function is typically assumed to be giv en a priori), past work [1, 13, 11] the trade off between many features. Apprenticeship learni ng is based on the insight that often it is easier for an  X  X xpert X  to demonstrate the desired behavio r than it is to specify a reward function that induces this behavior. However, when attempting to app ly apprenticeship learning to large do-to demonstrate complete trajectories in the domain, and we a re specifically concerned with domains that are sufficiently complex so that even this task is not fea sible. Second, these past algorithms require the ability to solve the  X  X asier X  problem of finding a nearly optimal policy given some can-didate reward function, and even this is challenging in larg e domains. Indeed, such domains often As a motivating application, consider the task of navigatin g a quadruped robot (shown in Figure 1(a)) over challenging, irregular terrain (shown in Figure 1(b,c)). In a naive approach, the dimen-ing to an 18-dimensional state space that is well beyond the c apabilities of standard RL algorithms. Fortunately, this control task succumbs very naturally to a hierarchical decomposition: we first plan to achieve these footsteps. However, it is very challenging to specify a proper reward, specifically for the higher levels of control, as this requires quantifyi ng the trade-off between many features, neath its feet, etc. Moreover, consider the apprenticeship learning task of specifying a complete set a highly non-trivial task.
 Motivated by these difficulties, we present a unified method f or hierarchical apprenticeship learn-ing . Our approach is based on the insight that, while it may be dif ficult for an expert to specify employ a hierarchical control scheme to solve our problem, i t is much easier for the expert to give advice independently at each level of this hierarchy. At the lower levels of the control hierarchy, our method only requires that the expert be able to demonstra te good local behavior, rather than even when the expert is entirely unable to give full trajecto ry demonstrations. Thus the approach allows for apprenticeship learning in extremely complex, p reviously intractable domains. learning algorithm. This algorithm extends the apprentice ship learning paradigm to complex, high-dimensional control tasks by allowing an expert to demonstr ate desired behavior at multiple levels of abstraction. Second, we apply the hierarchical apprentice ship approach to the quadruped locomotion problem discussed above. By applying this method, we achiev e performance that is, to the best of The remainder of this paper is organized as follows. In Secti on 2 we discuss preliminaries and notation. In Section 3 we present the general formulation of the hierarchical apprenticeship learning algorithm. In Section 4 we present experimental results, bo th on a hierarchical multi-room grid world, and on the real-world quadruped locomotion task. Fin ally, in Section 5 we discuss related work and conclude the paper. A Markov decision process (MDP) is a tuple ( S,A,T,H,D,R ) , where S is a set of states; A is a set of actions, T = { P distribution upon taking action a in state s ); H is the horizon which corresponds to the number of time-steps considered; D is a distribution over initial states; and R : S  X  R is a reward function. As we are often concerned with MDPs for which no reward functi on is given, we use the notation MDP \ R to denote an MDP minus the reward function. A policy  X  is a mapping from states to a prob-ability distribution over actions. The value of a policy  X  is given by V (  X  ) = E h P H where the expectation is taken with respect to the random sta te sequence s stating from the state s Often the reward function R can be represented more compactly as a function of the state. Let  X  : S  X  R n be a mapping from states to a set of features. We consider the c ase where the reward we have that the value of a policy  X  is linear in the reward function weights where we used linearity of expectation to bring w outside of the expectation. The last quantity defines the vector of feature expectations  X  We now present our hierarchical apprenticeship learning al gorithm (hereafter HAL). For simplicity, we present a two level hierarchical formulation of the control task, referred to g enerically as the 3.1 Reward Decomposition in HAL At the heart of the HAL algorithm is a simple decomposition of the reward function that links the form of two MDP \ Rs  X  a low-level and a high-level MDP \ R, denoted M and M low level states to high-level states (the assumption here i s that | S locomotion problem the low-level MDP \ R describes the state of all four feet, while the high-level MDP \ R describes only the position of the robot X  X  center of mass. A s is standard in apprenticeship learning, we suppose that the rewards in the low-level MDP \ R can be represented as a linear function of state features, R ( s state is equal to the average reward over all its correspondi ng low-level states. Formally
R ( s h ) = where  X   X  1 ( s While this may not always be the most ideal decomposition of th e reward function in many cases X  for example, we may want to let the reward of a high-level stat e be the maximum of its low level state rewards to capture the fact that an ideal agent would al ways seek to maximize reward at the lower level, or alternatively the minimum of its low level state rewards to be robust to worst-case outcomes X  X t captures the idea that in the absence of other pri or information, it seems reasonable to assume a uniform distribution over the low-level states c orresponding to a high-level state. An important consequence of (2) is that the high level reward is now also linear in the low-level reward weights w . This will enable us in the subsequent sections to formulate a unified hierarchical appren-ticeship learning algorithm that is able to incorporate exp ert advice at both the high level and the low level simultaneously. 3.2 Expert Advice at the High Level policies demonstrated by the expert. However, because the h igh-level MDP \ R can be significantly  X  constraint, which states that the expert X  X  policy outperfo rms all other policies: Equivalently, using (1), we can formulate this constraint a s follows: While we may not be able to obtain the exact feature expectatio ns of the expert X  X  policy if the high-level transitions are stochastic, observing a single exper t demonstration corresponds to receiving a sample from these feature expectations, so we simply use th e observed expert features counts  X   X   X  (  X  shown that a sufficient number of observed feature counts wil l converge to the true expectation. To slack variables (similar to standard SVM formulations), wh ich results in the following formulation: known algorithms that are able to solve this optimization pr oblem; however, we defer this discussion until after presenting our complete formulation. 3.3 Expert Advice at the Low Level Our approach differs from standard apprenticeship learnin g when we consider advice at the low target domain, we allow for an expert to specify single, gree dy actions in the low-level domain. Specifically, if the agent is in state s to state s 0 for all other states s 00 the current state s constraints on the reward function parameters w , for all s 00 provide noisy advice, we use regularization and slack varia bles. This gives: where s 00 tions provided by the expert. 3.4 The Unified HAL Algorithm From (2) we see the high level and low level rewards are a linea r combination of the same set of reward weights w . This allows us to combine both types of expert advice presen ted above to obtain the following unified optimization problem This optimization problem is convex, and can be solved effici ently. In particular, even though the optimization problem has an exponentially large number of c onstraints (one constraint per policy), the optimum can be found efficiently (i.e., in polynomial tim e) using, for example, the ellipsoid method, since we can efficiently identify a constraint that i s violated. 5 However, in practice we found the following constraint generation method more effic ient: 4.1 Gridworld In this section we present results on a multi-room gridworld domain with unknown cost. While this is not meant to be a challenging control task, it allows us to c ompare the performance of HAL to average the cost over each room, we can form a  X  X igh-level X  ap proximation of the grid world. Our hierarchical controller first plans in this domain to choose a path over the rooms. Then for each room along this path we plan a low-level path to the desired ex it.
 Figure 2(b) shows the performance versus number of training examples provided to the algorithm (where one training example equals one action demonstrated by the expert). 6 As expected, the flat apprenticeship learning algorithm eventually converges t o a superior policy, since it employs full value iteration to find the optimal policy, while HAL uses the (non-optimal) hierarchical controller. However, for small amounts of training data, HAL outperform s the flat method, since it is able to leverage the small amount of data provided by the expert at bo th levels of the hierarchy. Figure 2(c) shows performance versus number of MDPs in the training set f or HAL and well as for algorithms which receive the same training data as HAL (that is, both hig h level and low level expert demon-strations), but which make use of only one or the other. Here w e see that HAL performs substantially better. This is not meant to be a direct comparison of the diff erent methods, since HAL obtains more training data per MDP than the single-level approaches. Rat her, this experiment illustrates that in situations where one has access to both high-level and low-l evel advice, it is advantageous to use both. This will be especially important in domains such as th e quadruped locomotion task, where we have access to very few training MDPs (i.e., different ter rains). 4.2 Quadruped Robot In this section we present the primary experimental result o f this paper, a successful application of hierarchical apprenticeship learning to the task of quadru ped locomotion. Videos of the results in this section are available at http://cs.stanford.edu/  X kolter/nips07videos . 4.2.1 Hierarchical Control for Quadruped Locomotion The LittleDog robot, shown in Figure 1, is designed and built by Boston Dynamics, Inc. The robot consists of 12 independently actuated servo motors, three o n each leg, with two at the hip and one at the knee. It is equipped with an internal IMU and foot force se nsors. We estimate the robot X  X  state using a motion capture system that tracks reflective markers on the robot X  X  body. We perform all computation on a desktop computer, and send commands to the r obot via a wireless connection. path planner , that plans an approximate trajectory for the robot X  X  cente r of mass over the terrain; footsteps that follow this path. The footstep planner uses a reward function that specifies the rel-form the high-level cost, we aggregate features from the foo tstep planner. In particular, for each foot we consider all the footstep features within a 3 cm radiu s of the foot X  X   X  X ome X  position (the desired position of the foot relative to the center of mass in the absence of all other discriminating count for stochasticity of the domain. After forming the cos t function for both levels, we used value iteration to find the optimal policy for the body path planner , and a five-step lookahead receding horizon search to find a good set of footsteps for the footstep planner. 4.2.2 Hierarchical Apprenticeship Learning for Quadruped Locomotion icantly more challenging terrain for testing. To give advic e at the high level, we specified complete body trajectories for the robot X  X  center of mass, as shown in Figure 3(a). To give advice for the low level we looked for situations in which the robot stepped in a suboptimal location, and then indicated the correct greedy foot placement, as shown in Fig ure 3(b). The entire training set con-demonstrations on this terrain; it took about 10 minutes to c ollect the data.
 Even from this small amount of training data, the learned sys tem achieved excellent performance, not only on the training board, but also on the much more diffic ult testing board. Figure 4 shows snapshots of the quadruped crossing the testing board. Figu re 5 shows the resulting footsteps taken for each of the different types of constraints, which shows a very large qualitative difference be-tween the footsteps chosen before and after training. Table 1 shows the crossing times for each of the different types of constraints. As shown, he HAL algorit hm outperforms all the intermediate methods. Using only footstep constraints does quite well on the training board, but on the testing board the lack of high-level training leads the robot to take a very roundabout route, and it performs much worse. The quadruped fails at crossing the testing terr ain when learning from the path-level demonstration only or when not learning at all.
 Finally, prior to undertaking our work on hierarchical appr enticeship learning, we invested several weeks attempting to hand-tune controller capable of pickin g good footsteps across challenging ter-rain. However, none of our previous efforts could significan tly outperform the controller presented here, learned from about 10 minutes worth of data, and many of our previous efforts performed substantially worse. The work presented in this paper relates to many areas of rein forcement learning, including ap-prenticeship learning and hierarchical reinforcement lea rning, and to a large body of past work in quadruped locomotion. In the introduction and in the formul ation of our algorithm we discussed the connection to the inverse reinforcement learning algorith m of [1] and the maximum margin plan-ning algorithm of [13]. In addition, there has been subseque nt work [14] that extends the maximum margin planning framework to allow for the automated additi on of new features through a boosting procedure; There has also been much recent work in reinforce ment learning on hierarchical rein-forcement learning; a recent survey is [2]. However, all the work in this area that we are aware of deals with the more standard reinforcement learning formul ation where known rewards are given to the agent as it acts in a (possibly unknown) environment. I n contrast, our work follows the ap-prenticeship learning paradigm where the model, but not the rewards, are known to the agent. Prior work on legged locomotion has mostly focused on generating g aits for stably traversing fairly flat terrain (see, among many others, [10], [7]). Only very few le arning algorithms, which attempt to considered in this paper go well beyond the difficulty level c onsidered in prior work. We gratefully acknowledge the anonymous reviewers for help ful suggestions. This work was sup-ported by the DARPA Learning Locomotion program under contr act number FA8650-05-C-7261.
