
Faculty of Computer Science, Dalhousie University, Halifax, Nova Scotia, Canada School of Management, Dalhousie University, Halifax, Nova Scotia, Canada 1. Introduction
Traditional document clustering is an unsupervised classification of a given document collection into clusters. Such methods work by either (a) optimizing some loss function, such as K -Means [6], over all document assignments or (b) fitting a probabilistic model, such as the multinomial na X ve Bayes model [19] onto the document collection. Unsupervised processes minimize user effort during clustering perception of the document collection [14].

In this paper, we seek to determine whether clusters better matching user expectation may be generated with some supervision by the user. User supervision can be used in the two components of clustering: tering methods employ user-provided constraints between documents such as  X  X ust-link X  and  X  X annot-Through optimizing the constrained loss function or forming the probabilistic model with constraints, by selecting the feature set to represent the documents. Document category information, which is not has been shown to cost less time than labeling documents [20].

In this paper, we explore how user supervision performs when it is used for feature selection. The than labeling document constraints. Traditional semi-supervised clustering algorithms and our frame-performance is not directly comparable because it is difficult to establish a common quantification of may take less time than labeling documents as reported in the active learning setting [20].
An overview of our framework is as follows. We first obtain document clusters using the current feature set. Then, cluster-based feature selection is performed based on the obtained clusters serving that the user is asked to label as few features as possible.
 normally used with document classification algorithms but our framework performs in the document clustering setting. Compared to a document clustering algorithm, a classification algorithm requires labeled documents for training a classifier. Second, the user labels documents in AL but they label features in our framework. Third, uncertain sampling [17] is used in AL to find the most uncertain of the most promising features for labeling.

To explore whether user supervision at the feature level can generate clusters better matching user expectation, we propose an interactive framework for feature selection, in which the feature set ob-tained from the interactive feature selection is used for clustering. This framework includes several components: an underlying clustering algorithm, unsupervised feature selection, cluster-based feature evaluate whether the generated clusters better conform to user expectation. We also use this framework a human user may make mistakes. More importantly, the simulated user can be employed repeatedly. In this paper, we use K Means and Multinomial Na X ve Bayes model as the underlying clustering algo-rithms. However, we believe that other clustering algorithms also work because our interactive feature selection framework does not depend on any specific algorithm. In addition, we use unsupervised mean-TFIDF feature selection and  X  2 , cluster-based feature selection method. We conducted experiments on some real-word datasets to investigate: (1) the effectiveness of the proposed framework, and (2) the effect of feature reweighting and user effort in terms of labeling features.
 implications of this work and the opportunities for further investigations in Section 7. 2. Related work
Existing semi-supervised clustering makes use of user supervision in the form of document-level constraints. Those methods are generally grouped into four categories. First, constraints are used to learning of document constraints such as [15], most semi-supervised clustering algorithms involve the user supervision outside the clustering process. In this way, all the document constraints are defined clustering process and label the presented features.

Interactive feature selection in the context of active learning is studied in [20], which used linear classification setting and operates at the document level. It normally uses uncertainty sampling [17] our framework, we explore the interactive feature selection for document clustering and no document labeling is required.

A set of representative features for each class is labeled in [18]. These features are then used to Maximization (EM) algorithm [9] is applied iteratively to build new classifiers. The features are only proposed in [21]. The main idea of this work is to label a few documents for cluster seeds and for unsupervised feature selection methods for document clustering, such as document frequency (DF), mean TFIDF , and term frequency variance (TfV) [22]. Despite their simplicity, they are effective in selecting good features. All those techniques are completely unsupervised and employ term (feature) frequency and/or document frequency to rank features based on their suitability for clustering. Our interactive framework aims to involve the user in the feature selection process. 3. Background
In this section, we present the underlying clustering algorithms and feature selection techniques used model clustering algorithm, i.e., K Means and Multinomial Na X ve Bayes respectively. For traditional document clustering, we employ mean-TFIDF feature selection technique to select feature subset for clustering. For class-based feature selection, we use the  X  2 feature selection technique. 3.1. K Means data points by locally optimizing a loss function or distortion measure defined as:  X  the value of J is minimized. { r ij } is defined as This is usually achieved by an iterative procedure in which each iteration has two alternating steps time complexity of K Means is O ( IKNM ) ,where I is the number of iterations that K Means runs until convergence, K is the number of clusters, N is the number of data points, and M is the dimensionality its variants ( K Means based methods) are very efficient. 3.2. Multinomial Na X ve Bayes model
Multinomial Na X ve Bayes model [19] is a commonly used probabilistic model for text clustering, which assumes a document as a vector of words, with each word generated independently by a multino-mial probability distribution of the document X  X  class or cluster.

Now suppose we have a labeled training set D and |D| is the size of D . In the Na X ve Bayes classifier Based on Bayesian probability and the multinomial model, we have the prior probability Algorithm 1 K Means [6] {  X  1 , X  2 ,..., X  K } Output : Data point assignments { r ij } Method : 1: Randomly initialize the cluster centroids {  X  j } based on the given seed s 2: repeat 3: for all i =1 to N do 4: Compute all distances dist ij between data point d i and each cluster centroid  X  j 5: Assign data point d i to the cluster c j when dist ij is the smallest, namely, r ij =1 when j = 6: end for 7: Update cluster centroids {  X  j } based on the new data point assignments { r ij } 8: until No data point assignments change or maximum # of iterations is reached and with Laplacian smoothing, we have word conditional probability for each class, classify documents: cluster are computed based on the most recent document distributions in the clusters.
The Multinomial Na X ve Bayes clustering algorithm, also called EM-NB algorithm, is formed by ap-plying EM algorithm [9] to Na X ve Bayes classifier. In EM-NB algorithm, Eqs (3) and (4) are evaluated 0 and 1 . 3.3. Mean-TFIDF feature selection technique
Mean-TFIDF feature selection technique [22] is based on the principle that a good feature has high term frequency but low document frequency. It ranks all features by their mean-TFIDF values which are while inverse document frequency idf of a feature j ( ft j )isdefinedas idf j =log |D| |{ d : ft denotes the document collection, n ( i,j ) denotes occurrences of term i in document j and d denotes a mean-TFIDF value of a feature j is the average value of TFIDF s over the documents in the collection Algorithm 2 EM-NB , i.e. Multinomial Na X ve Bayes [19] Input : Data point vectors { d 1 ,d 2 ,...,d N } and, initial probability that a document belonging to a class (cluster), P initial ( c j | d i ) Output : P data point assignments { r ij } Method : 1: repeat 2: for all j =1 to | C | do 3: Based on current P ( c j | d i ) , compute 4: end for 5: for all i =1 to N do 6: for all j =1 to K do 7: Compute P new ( c j | d i ) given the document using Eq. 5 8: end for 10: end for 11: until No data point assignments change or maximum # of iterations is reached 3.4.  X  2 class-based feature selection technique probabilities from t he expected probabilities assuming i ndependence. Assuming random variable C  X  { probability estimations. Assume there are N documents in the collection. If there are N c documents in class c ,then Pr ( C = c )= N c /N .Ifthereare N i documents with/without feature j indicated by the  X  for feature selection of document clustering, we treat clusters as classes. 4. Methodology of user effort, and cluster evaluation measures. We also give details about the simulated user. Our clustering framework with interactive feature selection is summarized as follows: 4.1. Notations
Notations used in this paper are summarized in Table 1. Particularly, m denotes the size of feature set used for document clustering while f denotes the number of features presented to a user at each 4.2. Interactive document clustering framework
After a new feature set with user supervision is obtained (as described in Algorithm 4), the documents Algorithm 3 Interactive Document Clustering Framework with Feature Selection (Notations also in Table 1) clustering, FS basic  X  all features extracted.
 Output : { r ij }  X  assignments of document to clusters, i.e. final clusters.
 Method : 5: repeat 6: t  X  t +1 7: Perform feature Selection with User Supervision, Algorithm 4 8: Initialize the underlying clustering algorit hm with previous iteration X  X  parameters 9: Cluster documents using the new feature set and the initialized underlying clustering algorithm 11: until No data point assignment changes or maximum # of iterations is reached or the user chooses set selected by mean-TFIDF [22]. There are no user accepted features at the beginning. It is worth the generated clusters do not change, or (3) when the maximum number of iterations is reached. The user may choose to stop when either generated clusters or the feature set is satisfactory. 4.3. Interactive feature selection framework
The high dimensionality of the document text reduces the clustering algorithm performance. Feature features after he sees more other features and therefore knows more about the topics. All features ac-to the ranking obtained by the cluster-based feature selection based on the most recent clusters. Algorithm 4 Interactive feature selection with user supervision (notations also defined in Table 1) Input : m  X  size of feature set for document clustering, f  X  # of features presented to the user each time, FS t  X  1 accepted  X  set of features accepted until t  X  1 iteration, FS basic  X  all features extracted, y c  X  intermediate clusters.
 Output : FS t accepted  X  set of features accepted until t iteration, FS m  X  m features selected for next clustering iteration.
 Method : 3: { //accepted features and  X  X on X  X  know X  features are presented to the user only once and multiple 5: for all i =1 to f do 6: Present i th feature in FL to the user, get reply 7: if reply ==  X  accept  X  then 9: end if 10: end for 12: size  X  size of FS m 13: for i  X  1 to m  X  size do 14: FS m  X  FS m  X  X  ( f + i ) th feature } 15: end for 4.4. Cluster-Based feature selection
When document class labels are available, class-based feature selection can be performed. Such ex-amples are the  X  2 , information gain, and gain ratio. In our work, we apply those techniques without the label of the document. We make use of the class-based feature selection and the cluster labels to user-supervision in the document class labels.

The cluster-based (class-based) feature selection ranks the features according to the corresponding measures [8]. Take the  X  2 as an example and suppose there are K clusters. There is one  X  2 value for can be defined either as the sum of the local values or the maximum of the local values. Since we only long as it is able to rank features based on the underlying document class labels. In our research, we clusters. 4.5. The simulated user
In our research, user supervision is used to identify useful features for clustering, namely, feature goal in this paper is to compare our interactive framework with unsupervised feature selection. More must be performed, which is very costly in terms of human effort required. Like other text mining concept demonstration. At the same time, we realize there are many properties of a human user that an simulated user cannot simulate. For example, different users have different domain expertise, in-depth knowledge of the documents, etc. Therefore, a proper user study should be conducted for the evaluation of the proposed framework after this initial proof-of-concept study.

Based on the document class labels, a ranking of all features is obtained using class-based feature the performance of the clustering algorithm by comparing computed clusters against the underlying interactive framework terminates when the generated clusters do not change or the maximum number of iterations is reached. 4.6. Feature sets
By using the underlying clustering algorithms, we compare interactive feature selection framework The various feature sets are listed in Table 2. All feature sets in Table 2 have the same size m = 600 except FS basic whose size depends on the number of all extracted features. 4.7. Effect of user effort
In this section, we investigate the effect of user effort on document clustering. To the best of our interactive feature selection and clustering framework. The f value can be thought of as a measure of defined as: ing. 4.7.1. Feature reweighting
Since feature reweighting can boost classification performance in active learning [20], feature reweighting is adopted in the interactive clustering framework. Different underlying clustering algo-rithms have their own method of integrating feature re-weighting. In this paper, we use K Means and Multinomial Na X ve Bayes model or EM-NB .For K Means, the TFIDF values of the accepted features is multiplied by the given weight g and then the vector of TFIDF values is normalized. In EM-NB ,the posterior probability of a class is for a given document [18]. g affects word conditional probability through the feature term frequency: a given feature is defined as: In our experiments, g is an integer between 1 and 10 . Using the above definitions, we investigate how 5. Experiments 5.1. Datasets
In this work, we use six datasets to test our newly proposed algorithm: (1) news-diff-3 ,(2) news-related-3 ,(3) news-similar-3 ,(4) D2-D2&amp;D3-D3 ,(5) D-H-I ,and(6) 3-classic-abstract .Thefirst three datasets are derived from the widely used 20-Newsgroups collection 1 for text classification and clustering. Three reduced datasets, News-Different-3 , News-Related-3 ,and News-Similar-3 ,arede-rived according to [4]. News-Different-3 covers topics from 3 quite different newsgroups (alt.atheism, rec.sport.baseball, and sci.space). News-Related-3 contains 3 related newsgroups (talk.politics.misc, groups (comp.graphics, comp.os.ms-windows, comp.windows.x). Since News-Similar-3 has significant conceptual overlap between groups, it is the most difficult one to cluster.

The fourth and fifth datasets are collections of papers in full text, which were manually collected by the authors from Association for Computing Machinery (ACM) Digital Library. 2 We use the 1998 ACM Computing Classification System to label the categories. 3 In this paper, we use the categories listed in Table 3. H and I are related as they have overlapping areas such as  X  X ata Mining X  and  X  X ext are all from D category. The second, D-H-I , consists of 100 papers from each of D , H , I categories.
The sixth dataset 3 -classic is made by combining the CISI, CRAN, and MED from the SMART from the Cranfield collection. One hundred documents from each category are sampled to form the reduced 3 -classic dataset. The topics are quite different across categories, like News-Different-3 .
We pre-process each document by tokenizing the text into bags-of-words. 5 Then, we remove the stop words and stem all other words. The top m features ranked either by mean-TFIDF or the  X  2 method are employed for clustering. For the K -Means-based algorithms, a feature vector for each document is constructed with TFIDF weighting and then normalized. For EM-NB-based algorithms, the term frequency of the selected features is directly used in the related algorithms. 5.2. Evaluation measures
We use three evaluation measures: (1) Clustering Accuracy, (2) NMI , and (3) Jaccard Coefficient. 5.2.1. Clustering accuracy
Assume we have a clustering T and the underlying classes C . To estimate the clustering accuracy, we map each cluster t  X  T to one underlying class c  X  C if the documents from c dominate t ,i.e.,the number of documents from c is maximum. Then we define n ( t ) as the number of dominating documents in t from c . The clustering accuracy CACC of T with respect to C is defined as: number of clusters K is very large. For example, CACC is 1 when K equals N , the number of documents as the number of underlying classes in the datasets. 5.2.2. Normalized mutual information
Normalized mutual information ( NMI ) [10] measures the shared information between the cluster as-signments S and class labels L of documents. It is defined as: and the entropy of L respectively. Assuming there are K classes and K clusters, N documents, n ( l i ) n ( l i ,s j ) denotes the number of documents in both class l i and cluster s j ,wedefine: interval [0 , 1] . 5.2.3. Jaccard coefficient
Jaccard coefficient [5] is usually used to measure similarity between two clusterings with no under-between y c 1 and y c 2 is defined as: Note that the values of Jaccard Coefficient are in the interval [0 , 1] . 5.3. Experimental setup Two underlying algorithms, K Means and Multinomial Na X ve Bayes Model ( EM-NB )areemployed. However, we expect that other clustering algorithms will also work because our interactive framework does not depend on any specific algorithm. We use unsupervised mean-TFIDF feature selection and the  X  2 method for the cluster-based feature selection. We first present the results of the underlying effect of feature set size on document clustering. Third, we study how clustering performance depends on user effort. Fourth, we explore the effect of weight g for feature reweighting. Fifth, we compare K Means (or EM-NB) with feature supervision on different newsgroup datasets. Finally, we compare K Means with EM-NB (both with feature supervision) on the same datasets. In this paper, we present patterns. 5.4. Performance on different feature sets
In this section, we compare and discuss the performance of the same underlying algorithm with differ-framework without user supervision.

Each pair of one underlying algorithm and one feature set was run 36 times 6 with different initial-ture set, we take the average performance when the performance stabilizes with the number of features f displayed to the user, e.g. f is between 100 and 300.

As shown in Tables 4 and 5, the interactive feature selection framework can produce better clusters than other unsupervised feature selection methods with some user effort. In these tables, the perfor-umn FS reference except where the performance measures are bold. In Table 5, the exception is between not always perform better than the unsupervised feature set, the feature set selected with some user supervision does. It is especially true when the automated feature set performs much worse than the framework with some user effort achieves comparable performance to the underlying algorithm with of Jaccard Coefficient. That is because the accuracy and NMI are calculated based on the underlying class labels while Jaccard Coefficient is computed based on the clustering produced with the reference Coefficient [11]. 5.5. Effect of feature set size
Sometimes, the interactive clustering framework with user interaction obtains better performance than m for clustering may affect clustering performance. The above two reasons motivated exploration of on the underlying document class labels. Therefore, we run the base clustering algorithms K Means and EM-NB to cluster documents with the reference feature sets with different sizes. The effect of feature set size in terms of clustering accuracy is illustrated in Figs 1(a) and (b). The performance of both Maximum performance can be reached with different feature set sizes but 200 m 400 usually gives the maximum performance. The clustering performance of both K Means and EM-NB on datasets with are added, the  X  X ood X  features dominate at first but noisy features take over later on. Comparing EM-NB (Fig. 1(b)) to K Means (Fig. 1(a)) on news-related-3 and news-similar-3 datasets, we observe that EM-NB has smaller performance change than K Means when noisy features are introduced later on. 5.6. Effect of user effort
We only show a portion of the figures we obtained from the experiments here. All the figures can K Means and Fig. 4(b) for EM-NB .
 the user in each iteration as seen in Fig. 2(a). We also note that the effort efficiency declines when more features displayed in each iteration, as seen in Fig. 2(b). This may be due to the fact that the (see Section 5.5 for more discussion). The observation that the effort efficiency is about 1 when f is on the intermediate clusters even when the  X  2 is not perfect and the intermediate clusters are noisy.
Generally speaking, the clustering performance improves with more effort provided from the user (Figs 3(a) and 4(a)). However, when the interactive clustering framework with K Means works with news-related dataset and ACM ( D-H-I ) dataset, the clustering performance declines beyond a certain reference feature set FS reference .

One important finding is that the algorithm converges very quickly when f is very small so that the total number of features accepted is only a small portion of the reference feature set. When weight g emphasized, which has a negative effect on interactive clustering framework with EM-NB (Figs 3(a) and 4(b)). For the interactive framework with EM-NB , probabilities of features in the feature set for when more features are accepted by the user. 5.7. Selection of weight g
In our experiments, we tried different values of weight g (see details in Section 4.7.1) from 1 to 10 found that feature reweighting helps to improve the document clustering performance. It can either im-which saves user effort. When the interactive framework with EM-NB works with g&gt; 1 ,itimproves performance when applied to the news-similar-3 dataset (which represents the dataset that is the hard-est to cluster) although it achieves comparable performance when applied to other datasets. We suggest g =5 to avoid over-emphasis on accepted features. 5.8. K Means and EM-NB on the same datasets
We also compare the interactive clustering framework with K Means versus EM-NB as the underlying algorithm on the same datasets (Fig. 5). It is found that the framework with EM-NB is more stable than with K Means once maximum performance is reached. In particular, the framework with K Means declines more strongly after maximum performance is reached when applied to news-related dataset and ACM ( D-H-I ) dataset. Within the interactive clustering framework, EM-NB performs better than K Means on news-diff-3 dataset. When applied to news-related-3 and news-similar-3 datasets, K Means outperforms EM-NB when only a few features are confirmed by the user, e.g., f total &lt; 100 .Withmore features confirmed, EM-NB can achieve better performance than K Means. It is mainly due to the fact for clustering. The noisy features have more negative effect on EM-NB than on K Means. 5.9. K Means or EM-NB on different datasets
We compare the interactive clustering framework with K Means or EM-NB on the three newsgroups sub-datasets with respect to the same number of features. When the same user effort in terms of number difficult one to be grouped and the news-diff-3 dataset remains the easiest one (Fig. 6). 6. Guidelines for designing interactive framework
Based on our experiments on different datasets, several guidelines for applying interactive framework can be derived. Developing a interactive clustering tool or framework is not trivial task and one needs lots of domain knowledge/expertise. The ontology of the domain should be incorporated into the tool to help users to make decisions. 7. Conclusions
In this paper, we designed and created a new framework that enables the user to guide the clustering process by selecting features which are meaningful to them. The framework interleaves interactive fea-avoid early convergence of the clustering algorithm at a local optimum. After a certain amount of user same or decline a little. Our results show that reweighting of previously  X  X ccepted X  features can also improve clustering performance. However, large weights (greater than 10) should be avoided to prevent over-emphasizing the accepted features for some datasets, which might make the clustering algorithms group the documents only based on these few over-emphasized features.
 Acknowledgment
We would like to thank the anonymous reviewers for their insightful comments. This research was supported in part by the NSERC (Natural Sciences and Engineering Research Council) Business Intelli-gence Network, and by the MITACS NCE.
 References
