 Continuous active learning achieves high recall for technology-assisted review, not only for an overall informa-tion need, but also for various facets of that information need, whether explicit or implicit. Through simulations using Cormack and Grossman X  X  TAR Evaluation Toolkit (SIGIR 2014), we show that continuous active learning, ap-plied to a multi-faceted topic, efficiently achieves high recall for each facet of the topic. Our results assuage the concern that continuous active learning may achieve high overall re-call at the expense of excluding identifiable categories of relevant information.
 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Search process, relevance feedback. Keywords: Technology-assisted review; TAR; predictive coding; electronic discovery; e-discovery; test collections; relevance feedback; continuous active learning; CAL.
The objective of technology-assisted review ( X  X AR X ), first described in the context of electronic discovery ( X  X Discov-ery X ) in legal matters [6], is to bring to the attention of a document reviewer substantially all relevant documents, and relatively few non-relevant ones, thereby maximizing recall and minimizing reviewer effort. The best reported results for TAR employ continuous active learning ( X  X AL X ), in which a learning method presents the most-likely relevant docu-ments to the reviewer in batches, the reviewer labels each document in each successive batch as relevant or not, and the labels are fed back to the learning method [6]. While CAL has been shown to achieve high recall with less effort than competing methods (including exhaustive manual re-view [7] and non-interactive supervised learning [6]), it has been suggested that CAL X  X  emphasis on the most-likely rel-evant documents may bias it to prefer documents like the  X 
The views expressed herein are solely those of the author and should not be attributed to her firm or its clients. ones it finds first, causing it to fail to discover one or more important, but dissimilar, classes of relevant documents [11, 8].

In legal matters, an eDiscovery request typically comprises between several and several dozen requests for production ( X  X FPs X ), each specifying a category of information sought. A review effort that fails to find documents relevant to each of the RFPs (assuming such documents exist) would likely be deemed deficient. In other domains, such as news ser-vices, topics are grouped into hierarchies, either explicit or implicit. A news-retrieval effort for  X  X ports X  that omits arti-cles about  X  X ricket X  or  X  X occer X  would likely be deemed inade-quate, even if the vast majority of articles  X  about baseball, football, basketball, and hockey  X  were found. Similarly, a review effort that overlooked relevant short documents, spreadsheets, or presentations would likely also be seen as unsatisfactory.

We define a  X  X acet X  to be any identifiable subpopulation of the relevant documents, whether that subpopulation is defined by relevance to a particular RFP or subtopic, by file type, or by any other characteristic. Our objective is to determine whether CAL is able to achieve high recall over all facets, regardless of how they are identified. To this end, we used Cormack and Grossman X  X  TAR Evaluation Toolkit ( X  X oolkit X ), 1 grouping together the four RFPs supplied with the Toolkit as one overall topic, and treating each of the four RFPs as facets. We then computed recall as a function of review effort for the overall topic, as well as for each of the facets. We also computed recall separately for facets consist-ing of short documents, word-processing files, spreadsheets, and presentations.
 We repeated our experiments, importing the Reuters RCV1-v2 dataset [10] into an adapted version of the Toolkit, using each of the RCV1-v2 top-level subject categories as an overall information need, and treating each of the 82 bottom-level subject categories as a facet.
In the Toolkit, we created a new overall topic,  X  X ll, X  by combining the topics supplied with the Toolkit (topics 201, 202, 203, and 207 from the TREC 2009 Legal Track [9]) as follows: As a seed set, we used 1,000 documents selected at random from the  X  X eed query X  hits in the Toolkit for the four topics [6, Table 3, p. 155]; as training and gold standards, we used the union of the respective standards supplied for the four topics, with the effect that, for both training and http://cormack.uwaterloo.ca/cormack/tar-toolkit/ . documents, word-processing files, presentations, and spreadsheets. evaluation, a document was considered relevant to the over-all topic,  X  X ll, X  if it was relevant to any of the four facets represented by the Toolkit-supplied topics. We ran the  X  X c-tkeysvm X  CAL implementation, without modification, and captured the ordered list of documents presented for review. We computed recall for the  X  X ll X  topic, and for each facet, at each position in the list.

We further evaluated recall with respect to four other facets:  X  X hort, X   X .doc, X   X .xls, X  and  X .ppt, X  representing short documents ( &lt; 1 K bytes), word-processing files, spread-sheets, and presentations, respectively. Our results are shown in Figure 1. While it is apparent that, at the out-set, topics 201 and 203, as well as short documents and presentations, lag behind; at high recall levels, there is little variance among the recall levels of the facets.
We next adapted the Toolkit to work with the RCV1-v2 dataset [10]. We used tf-idf Porter-stemmed word features, and SVM light , following accepted practice [10]. We used as information needs the four top-level categories in the RCV1-v2 subject hierarchy, titled  X  X orporate, industrial, X   X  X co-nomics, X   X  X overnment and social, X  and  X  X arkets, X  with the corresponding subject codes,  X  X CAT, X   X  X CAT, X   X  X CAT, X  and  X  X CAT, X  respectively. As facets, we used the bottom-level categories in the RCV1-v2 subject hierarchy. We ig-nored the intermediate levels, as they are simple unions of the bottom-level categories. For seed queries, we used the ti-tles of the top-level categories. We used the RCV1-v2 labels as both the training and gold standards.

Figure 2 shows overall and facet recall, as a function of re-view effort, for each of the four RCV1-v2 overall information needs. The results mirror those for the TREC 2009 dataset; however, a much higher level of recall is achieved, and con-vergence occurs as that higher level is approached. It ap-pears that convergence coincides with a decline in marginal precision which, for these experiments, takes place in the neighborhood of 90% recall.

Among the results in Figure 2, one facet  X   X  X MIL X   X  is an obvious outlier. Its recall reaches 80% only with double the review effort required for all other facets. In the RCV1-v2 collection, only five documents are labeled relevant to GMIL (bottom-level topic title:  X  X illennium Issues X ). We examined these documents and found that four of them con-tain the phrase  X  X illennium bug X ; see, for example, the left panel of Table 1. We then searched the dataset and found 141 documents containing this same phrase, of which four were labeled relevant to GMIL, 48 were labeled relevant to some other facet of GCAT, and 93 were not labeled rele-vant to any facet. The right panel of Table 1 shows one such document. On reviewing these and other examples, we were unable to glean the criteria used to distinguish relevant from non-relevant documents, and thus attribute this outlier result to apparent mislabeling in the RCV1-v2 dataset.
The objective of finding substantially all relevant docu-ments suggests that CAL  X  or any other review effort  X  should continue until high recall has been achieved, and achieving higher recall would require disproportionate ef-fort. Measuring recall is problematic, due to imprecision in the definition and assessment of relevance [3, 12, 8], and the effort, bias, and imprecision associated with sampling [2, 1, 8]. Accordingly, it is difficult to specify an absolute threshold value that constitutes  X  X igh recall, X  or to determine reliably that that such a threshold had been reached. Arguably, 75% to 80% recall would be sufficient for the TREC review de-tailed in Figure 1, because at that level, the recall for the facets is uniformly high, and a higher level can be achieved only with disproportionate effort. On the other hand, 90% or higher recall would be necessary to establish the adequacy of the RCV1-v2 reviews detailed in Figure 2. Only above 90% overall recall is the recall for facets uniformly high, and 90% recall is achievable with proportionate effort.
We suggest that, as an alternative to a fixed recall tar-get, marginal precision might be a better indication of the completeness of a CAL review. In all of our experiments, we observed that the precision of each successive batch of 1,000 documents rose rapidly to nearly 100%, was sustained at nearly 100%, and then fell off. Table 2 illustrates that, in these experiments, stopping the review when marginal pre-cision falls below one-tenth of its previously sustained value is a good predictor of high recall for the overall information need, as well as the facets, with proportionate effort.
A sign test shows our result to be significant (p &lt; 0.03), by virtue of being observed for six of six separate experiments.
CAL is a greedy method that always chooses the most-likely relevant documents for review. It is to be expected that, at the outset, it chooses documents representing the easiest-to-identify facets, due to their subject matter, file properties, or abundance. As those documents are ex-hausted, others representing new facets become the most-likely relevant documents, until no more likely relevant doc-uments remain. Only when the most-likely relevant docu-ments from all facets have been exhausted, does marginal precision drop to a de minimus level.

While our findings suggest that it may be unnecessary, neither our theory of CAL X  X  operation nor our results sug-gest that it would be harmful to train the learning method using additional seed documents  X  found by ad hoc means  X  to represent important facets that are are known to the reviewer at the outset, or become known during the course of the review process [5].

Our experiments suggest that when a review achieves sus-tained high precision, and then drops off substantially, one may have confidence that substantially all facets of relevance have been explored. In addition to offering a potentially better prediction of completeness, precision can be readily calculated throughout the review, while recall cannot. Fur-ther research is necessary to determine the extent to which marginal precision may afford a reliable quantitative esti-mate of review completeness, including coverage of different facets of relevance.

While sharing general motivation with efforts to achieve novelty and diversity in ad hoc retrieval [4], CAL seeks to achieve high recall rather than to reduce redundancy, and does so using a depth-first rather than breadth-first approach. We conducted an auxiliary experiment to in-vestigate whether a strategy of using separate reviews for each facet would improve on the combined review strat-egy reported here. We found that the overall effort re-quired to achieve 75% recall for every facet was higher for the separate review strategy. It remains to be seen whether other diversity-focused methods might improve on the purely depth-first results presented here. Labeled  X  X elevant X  in RCV1-v2 Labeled  X  X ot relevant X  in RCV1-v2
Okura up on millennium bug software demand.

TOKYO 1997-08-15 Shares of Okura &amp; Co Ltd surged on Friday afternoon due to the expectation that its software business would benefit from the so-called millennium bug problem. The stock was the top percentage gainer on the Tokyo Stock Exchange X  X  first section in the afternoon session. Okura X  X  shares were up 55 yen at 455 yen as of 0435 GMT. (c) Reuters Limited 1997-06-16  X  X elevant, X  while 137 were labeled  X  X ot relevant. X  is measured in terms of overall precision, recall, and F , as well as the lowest recall obtained for any facet.
For all experiments, our results are the same: CAL achieves high overall recall, while at the same time achiev-ing high recall for the various facets of relevance, whether topics or file properties. While early recall is achieved for some facets at the expense of others, by the time high over-all recall is achieved  X  as evidenced by a substantial drop in overall marginal precision  X  all facets (except for a single outlier case that we attribute to mislabeling) also exhibit high recall. Our findings provide reassurance that CAL can achieve high recall without excluding identifiable categories of relevant information. [1] M. Bagdouri, D. D. Lewis, and D. W. Oard.
 [2] M. Bagdouri, W. Webber, D. D. Lewis, and D. W. [3] D. C. Blair. STAIRS redux: Thoughts on the STAIRS [4] C. L. A. Clarke, M. Kolla, G. V. Cormack, [5] G. Cormack and M. Mojdeh. Machine learning for [6] G. V. Cormack and M. R. Grossman. Evaluation of [7] M. R. Grossman and G. V. Cormack. Technology-[8] M. R. Grossman and G. V. Cormack. Comments on [9] B. Hedin, S. Tomlinson, J. R. Baron, and D. W. Oard. [10] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [11] K. Schieneman and T. Gricks. The implications of [12] E. M. Voorhees. Variations in relevance judgments and
