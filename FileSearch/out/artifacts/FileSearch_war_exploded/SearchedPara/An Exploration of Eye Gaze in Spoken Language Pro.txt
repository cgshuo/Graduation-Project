 } @cse.msu.edu Psycholinguistic experiments have shown that eye gaze is tightly linked to human language process-ing. Eye gaze is one of the reliable indicators of what a person is  X  X hinking about X  (Henderson and Ferreira, 2004). The direction of gaze carries infor-mation about the focus of the users attention (Just and Carpenter, 1976). The perceived visual context influences spoken word recognition and mediates syntactic processing (Tanenhaus et al., 1995; Roy and Mukherjee, 2005). In addition, directly before speaking a word, the eyes move to the mentioned object (Griffin and Bock, 2000).

Motivated by these psycholinguistic findings, we are currently investigating the role of eye gaze in spoken language understanding during human ma-chine conversation. Through multimodal interfaces, a user can look at a graphic display and converse with the system at the same time. Our assumption is that, during human machine conversation, a user X  X  eye gaze on the graphical display can indicate salient entities on which the user X  X  attention is focused. The specific domain information about the salient enti-ties is likely linked to the content of communication and therefore can be used to constrain speech hy-potheses and influence language understanding.
Based on this assumption, we carried out an ex-ploration study where eye gaze information is in-corporated in a salience model to tailor a language model for spoken language processing. Our prelim-inary results show that eye gaze can be useful in im-proving spoken language processing and the effect of eye gaze varies among different users. Because eye gaze is subconscious and involuntary in human machine conversation, our work also motivates sys-tematic investigations on how eye gaze contributes to attention prediction and its implications in auto-mated language processing. Eye gaze has been mainly used in human machine interaction as a pointing mechanism in direct manip-ulation interfaces (Jacob, 1990; Jacob, 1995; Zhai et al., 1999), as a facilitator in computer supported human human communication (Velichkovsky, 1995; Vertegaal, 1999); or as an additional modality dur-ing speech or multimodal communication (Starker and Bolt, 1990; Campana et al., 2001; Kaur et al., 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work.

In the context of speech and multimodal commu-nication, studies have shown that speech and eye gaze integration patterns can be modeled reliably for users. For example, by studying patterns of eye gaze and speech in the phrase  X  X ove it there X , researchers found that the gaze fixation closest to the intended object begins, with high probability, before the be-ginning of the word  X  X ove X  (Kaur et al., 2003). Re-cent work has also shown that eye gaze has a poten-tial to improve reference resolution in a spoken dia-log system (Campana et al., 2001). Furthermore, eye gaze also plays an important role in managing dia-log in conversational systems (Qvarfordt and Zhai, 2005).

Salience modeling has been used in both natural language and multimodal language processing. Lin-guistic salience describes entities with their accessi-bility in a hearer X  X  memory and their implications in language production and interpretation. Linguistic salience modeling has been used for language in-terpretations such as reference resolution (Huls et al., 1995; Eisenstein and Christoudias, 2004). Vi-sual salience measures how much attention an en-tity attracts from a user based on its visual proper-ties. Visual salience can tailor users X  referring ex-pressions and thus can be used for multimodal refer-ence resolution (Kehler, 2000). Our recent work has also investigated salience modeling based on deic-tic gestures to improve spoken language understand-ing (Chai and Qu, 2005; Qu and Chai, 2006). We conducted user studies to collect speech and eye gaze data. In the experiments, a static 3D bedroom scene was shown to the user. The system verbally asked a user a list of questions one at a time about the bedroom and the user answered the questions by speaking to the system. Fig.1 shows the 14 questions in the experiments. The user X  X  speech was recorded through an open microphone and the user X  X  eye gaze was captured by an Eye Link II eye tracker. From 7 users X  experiments, we collected 554 utterances with a vocabulary of 489 words. Each utterance was tran-scribed and annotated with entities that were being talked about in the utterance.
The collected raw gaze data consists of the screen coordinates of each gaze point sampled at 4 ms. As shown in Fig.2a, this raw data is not very use-ful for identifying fixated entities. The raw gaze data are processed to eliminate invalid and saccadic gaze points, leaving only pertinent eye fixations. Invalid gaze points occur when users look off the screen. Saccadic gaze points occur during ballis-tic eye movements between fixations. Vision stud-ies have shown that no visual processing occurs dur-ing saccades (i.e., saccadic suppression). It is well known that eyes do not stay still, but rather make small, frequent jerky movements. In order to best determine fixation locations, nearby gaze points are averaged together to identify fixations. The pro-cessed eye gaze fixations can be seen in Fig.2b.
Fig.3 shows an excerpt of the collected speech and gaze fixation with fixated entities. In the speech stream, each word starts at a particular timestamp. In the gaze stream, each gaze fixation f has a starting timestamp t f and a duration T f . Gaze fixations can have different durations. An entity e on the graphi-cal display is fixated by gaze fixation f if the area of e contains the fixation point of f . One gaze fixation can fall on multiple entities or no entity. Our goal is to use the domain specific information about the salient entities on a graphical display, as indicated by the user X  X  eye gaze, to help recognition of the user X  X  utterances. In particular, we incorporate this salient domain information in speech recogni-tion via salience driven language modeling.
We first briefly introduce speech recognition. The task of speech recognition is to, given an observed spoken utterance O , find the word sequence W  X  such that W  X  = arg max p ( O | W ) is the acoustic model and p ( W ) is the language model. The acoustic model provides the probability of observing the acoustic features given hypothesized word sequences while the language model provides the probability of a word sequence. The language model is represented as: Using first-order Markov assumption, the above lan-guage model can be approximated by a bigram model:
In the following sections, we first introduce the salience modeling based on eye gaze, then present how the gaze-based salience models can be used to tailor language models. 4.1 Gaze-based Salience Modeling We first define a gaze fixation set F t 0 + T t contains all gaze fixations that fall on entity e within a time window t 0  X  ( t 0 + T ) :
F t 0 ( e ) = { f | f falls on e within t 0  X  ( t 0 + T ) } We model gaze-based salience in two ways. 4.1.1 Gaze Salience Model 1
Salience model 1 is based on the assumption that when an entity has more gaze fixations on it than other entities, this entity is more likely attended by the user and thus has higher salience: Here, p focusing on entity e within time period t 0  X  ( t 0 + T ) based on how many gaze fixations are on e among all gaze fixations that fall on entities within t 0  X  ( t 0 + T ) . 4.1.2 Gaze Salience Model 2
Salience model 2 is based on the assumption that when an entity has longer gaze fixations on it than other entities, this entity is more likely attended by the user and thus has higher salience: where Here, p focusing on entity e within time period t 0  X  ( t 0 + t ) based on how long e has been fixated by gaze fixa-tions among the overall time length of all gaze fixa-tions that fall on entities within t 0  X  ( t 0 + T ) . 4.2 Salience Driven N-gram Model Salience models can be incorporated in different lan-guage models, such as bigram models, class-based bigram models, and probabilistic context free gram-mar. Among these language models, the salience driven bigram model based on deictic gesture has been shown to achieve best performance on speech recognition (Qu and Chai, 2006). In our initial in-vestigation of gaze-based salience, we incorporate the gaze-based salience in a bigram model.

The salience driven bigram probability is given by: p s ( w i | w i  X  1 ) = (1  X   X  ) p ( w i | w i  X  1 ) + where p eled in equations (3) and (4). In applying the salience driven bigram model for speech recogni-tion, we set t 0 as the starting timestamp of the ut-terance and T as the duration of the utterance. The priming weight  X  decides how much the original bigram probability will be tailored by the salient entities indicated by eye gaze. Currently, we set  X  = 0 . 67 empirically. We also tried learning the priming weight with an EM algorithm. However, we found out that the learned priming weight per-formed worse than the empirical one in our exper-iments. This is probably due to insufficient devel-opment data. Bigram probabilities p ( w i | w i  X  1 ) were estimated by the maximum likelihood estimation us-ing Katz X  X  backoff method (Katz, 1987) with a fre-quency cutoff of 1. The same method was used to es-timate p ( w i | w i  X  1 ,e ) from the users X  utterance tran-scripts with entity annotation of e . The salience driven language models can be inte-grated into speech processing in two stages: an early stage before a word lattice (n-best list) is generated (Fig.4a), or in a late stage where the word lattice (n-best list) is post-processed (Fig.4b).

For the early stage integration, the gaze-based salience driven language model is used together with Figure 4: Integration of gaze-based salience driven language model in speech processing the acoustic model to generate the word lattice, typ-ically by Viterbi search.

For the late stage integration, the gaze-based salience driven language model is used to rescore the word lattice generated by a speech recognizer with a basic language model not involving salience mod-eling. A* search can be applied to find the n-best paths in the word lattice. The evaluations were conducted on data collected from user studies (Sec. 3). We evaluated the gaze-based salience driven bigram models when applied for speech recognition at early and late stages. 6.1 Evaluation Results Users X  speech was first segmented, then recognized by the CMU Sphinx-4 speech recognizer using dif-ferent language models. Evaluation was done by a 14-fold cross validation. We compare the per-formances of the early and late applications of two gaze-based salience driven language models:  X  S-Bigram1  X  salience driven language model  X  S-Bigram2  X  salience driven language model
Table 1 and Table 2 show the results of early and late application of the salience driven language mod-els based on eye gaze. We can see that all word error rates (WERs) are high. In the experiments, users were instructed to only answer systems questions one by one. There was no flow of a real conversa-tion. In this setting, users were more free to express themselves than in the situation where users believed they were conversing with a machine. Thus, we ob-serve much longer sentences that often contain dis-fluencies. Here is one example:
The high WER was mainly caused by the com-plexity and disfluencies of users X  speech. Poor speech recording quality is another reason for the bad recognition performance. It was found that the trigram model performed worse than the bigram model in the experiment. This is probably due to the sparseness of trigrams in the corpus. The amount of data available is too small considering the vocabu-lary size.

The S-Bigram1 and S-Bigram2 achieved similar results in both early application (Table 1) and late application (Table 2). In early application, the S-Bigram1 model performed better than the trigram model ( t = 5 . 24 , p&lt; 0 . 001 , one-tailed) and the bigram model ( t = 3 . 31 , p&lt; 0 . 001 , one-tailed). The S-Bigram2 model also performed better than the trigram model ( t = 5 . 15 , p&lt; 0 . 001 , one-tailed) and the bigram model ( t = 3 . 33 , p&lt; 0 . 001 , one-tailed) in early application. In late application, the S-Bigram1 model performed better than the trigram model ( t = 2 . 11 , p&lt; 0 . 02 , one-tailed), so did the S-Bigram2 model ( t = 1 . 99 , p&lt; 0 . 025 , one-tailed). However, compared to the bigram model, the S-Bigram1 model did not change the recogni-tion performance significantly ( t = 0 . 38 , N.S., two-tailed) in late application, neither did the S-Bigram2 model ( t = 0 . 50 , N.S., two-tailed).

We also compare performances of the salience driven language models for individual users. In early application (Fig.5a), both the S-Bigram1 and the S-Bigram2 model performed better than the baselines of the bigram and trigram models for all users except user 2 and user 7. T-tests have shown that these are significant improvements. For user 2, the S-Bigram1 model achieved the same WER as the bigram model. For user 7, neither of the salience driven language models improved recognition compared to the bi-gram model. In late application (Fig.5b), only for user 3 and user 4, both salience driven language models performed better than the baselines of the bi-gram and trigram models. These improvements have also been confirmed by t-tests as significant.
Comparing early and late application of the salience driven language models, it is observed that early application performed better than late applica-tion for all users except user 3 and user 4. T-tests have confirmed that these differences are significant.
It is interesting to see that the effect of gaze-based salience modeling is different among users. For two users (i.e., user 3 and user 4), the gaze-based salience driven language models consistently out-performed the bigram and trigram models in both early application and late application. However, for some other users (e.g., user 7), this is not the case. In fact, the gaze-based salience driven language mod-els performed worse than the bigram model. This observation indicates that during language produc-tion, a user X  X  eye gaze is voluntary and unconscious. This is different from deictic gesture, which is more intentionally delivered by a user. Therefore, incor-porating this  X  X nconscious X  mode of modality in salience modeling requires more in-depth research on the role of eye gaze in attention prediction during multimodal human computer interaction. 6.2 Discussion Gaze-based salience driven language models are built on the assumption that when a user is fixat-ing on an entity, the user is saying something re-lated to the entity. With this assumption, gaze-based salience driven language models have the potential to improve speech recognition by biasing the speech decoder to favor the words that are consistent with the entity indicated by the user X  X  eye gaze, especially when the user X  X  utterance contains words describing unique characteristics of the entity. These particular characteristics could be the entity X  X  name or physical properties (e.g., color, material, size).
 Figure 6: N-best lists of utterance  X  a tree growing from the floor  X 
Fig.6 shows an example where the S-Bigram2 model in early application improved recognition of the utterance  X  a tree growing from the floor  X . In this example, the user X  X  gaze fixations accompany-ing the utterance resulted in a list of candidate enti-ties with fixating probabilities (cf. Eqn. (4)), among which entities bedroom and plant willow were as-signed higher probabilities. Two n-best lists, the Bi-gram n-best list and the S-Bigram2 n-best list, were generated by the speech recognizer when the bigram model and the S-Bigram2 model were applied sep-arately. The speech recognizer did not get the cor-rect recognition when the bigram model was used, but got the correct result when the S-Bigram2 model was used.

Fig.7a and 7b show the word lattices of the ut-terance generated by the speech recognizer using the bigram model and the S-Bigram2 model respec-tively. The n-best lists in Fig.6 were generated from those word lattices. In the word lattices, each path forms a recognition hypothesis. The bigram proba-bilities along the edges are in the logarithm of base 10. In the bigram case, the path  X  &lt; s &gt; a tree X  has a higher language score (summation of bigram prob-abilities along the path) than  X  &lt; s &gt; sheet X , and  X  X  floor X  has a higher language score than  X  X  full X . However, these correct paths  X  &lt; s &gt; a tree X  and  X  X  floor X  (not exactly correct, but better than  X  X  full X ) do not appear in the best hypothesis in the result-ing n-best list. This is because the system tries to find an overall best hypothesis by considering both language and acoustic score. Because of the noisy speech, the incorrect hypotheses may happen to have higher acoustic confidence than the correct ones. Af-ter tailoring the bigram model with gaze salience, the salient entity plant willow significantly increases the probability of  X  X  tree X  (from -1.3594 to -0.9913) and  X  X ree growing X  (from -3.1009 to -1.1887), while it decreases the probability of  X  X heet growing X  (from -3.0962 to -3.4534). This probability change is made by the entity conditional probability p ( w i | w i  X  1 ,e ) in tailoring of bigram by salience (cf. Eqn. (6)). Probability p ( w i | w i  X  1 ,e ) , trained from the anno-tated utterances, reflects what words are more likely to be spoken by a user while talking about an entity e . The increased probabilities of  X  X  tree X  and  X  X ree growing X  show that word  X  X ree X  appears more likely than  X  X heet X  when the user is talking about entity  X  plant willow . This is in accordance with our com-mon sense. Likewise, the salient entity bedroom , of which floor is a component, makes the probability of the correct hypothesis  X  X he floor X  much higher than other hypotheses ( X  X he for X  and  X  X he forest X ). These enlarged language score differences make the cor-rect hypotheses  X  X  tree X  and  X  X he floor X  win out in the searching procedure despite the noisy speech. Figure 8: N-best lists of utterance  X  I like the picture with like a forest in it  X 
Unlike the active input mode of deictic gesture, eye gaze is a passive input mode. The salience in-formation indicated by eye gaze is not as reliable as the one indicated by deictic gesture. When the salient entities indicated by eye gaze are not the true entities the user is referring to, the salience driven language model can worsen speech recogni-tion. Fig.8 shows an example where the S-Bigram2 model in early application worsened the recogni-tion of a user X  X  utterance  X  I like the picture with like a forest in it  X  because of wrong salience informa-tion. In this example, the user was talking about a picture entity picture bamboo . However, this entity was not salient, only entities bedroom and chande-lier 1 were salient. As a result, the recognition with the S-Bigram2 model becomes worse than the base-line. The correct word  X  X icture X  is missing and the wrong word  X  X edroom X  appears in the result.
The failure to identify the actual referred entity picture bamboo as salient in the above example can also be caused by the visual properties of entities. Smaller entities on the screen are harder to be fix-ated by eye gaze than larger entities. To address this issue, more reliable salience modeling that takes into account the visual features is needed. This paper presents an empirical exploration of in-corporating eye gaze in spoken language processing via salience driven language modeling. Our prelim-inary results have shown the potential of eye gaze in improving spoken language processing. Neverthe-less, this exploratory study is only the first step in our investigation. Many interesting research ques-tions remain. During human machine conversation, how is eye gaze aligned with speech production? How reliable is eye gaze for attention prediction? Are there any other factors such as interface design and visual properties that will affect eye gaze behav-ior and therefore attention prediction? The answers to these questions will affect how eye gaze should be appropriately modeled and used for language pro-cessing.

Eye-tracking systems are no longer bulky, sta-tionary systems that prevent natural human ma-chine communication. Recently developed dis-play mounted gaze-tracking systems (e.g., Tobii) are completely non-intrusive, can tolerate head motion, and provide high tracking quality. These features have been demonstrated in several successful appli-cations (Duchowski, 2002). Integrating eye tracking with conversational interfaces is no longer beyond reach. We believe it is time to conduct systematic investigations and fully explore the additional chan-nel provided by eye gaze in improving robustness of human machine conversation. This work was supported by a Career Award IIS-0347548 and IIS-0535112 from the National Sci-ence Foundation. The authors would like to thank Zahar Prasov for his contribution on data collection and thank anonymous reviewers for their valuable comments and suggestions.

