 Recovering matrices from incomplete and corrupted obser-vations is a fundamental problem with many applications in various areas of science and engineering. In theory, under certain conditions, this problem can be solved via a natural convex relaxation. However, all current provable algorithms suffer from superlinear per-iteration cost, which severely lim-its their applicability to large scale problems. In this paper, we propose a robust principal component analysis (RPCA) plus matrix completion framework to recover low-rank and sparse matrices from missing and grossly corrupted obser-vations. Under the unified framework, we first present a convex robust matrix completion model to replace the lin-ear projection operator constraint by a simple equality one. To further improve the efficiency of our convex model, we also develop a scalable structured factorization model, which can yield an orthogonal dictionary and a robust data repre-sentation simultaneously. Then, we develop two alternating direction augmented Lagrangian (ADAL) algorithms to ef-ficiently solve the proposed problems. Finally, we discuss the convergence analysis of our algorithms. Experimental results verified both the efficiency and effectiveness of our methods compared with the state-of-the-art algorithms. H.2.8 [ Database Management ]: Database applications-Data Mining RPCA, robust matrix completion, low-rank, ADAL
In recent years, high dimensional data are becoming in-creasingly ubiquitous such as digital photographs, surveil-lance videos and social network data. Such high dimen-sional data are becoming more commonly available due to the advance in data collection technologies. However, the  X  X urse of dimensionality X  has rendered many tasks such as inference, learning, and recognition, impractical. Principal component analysis (PCA) is arguably the most widely used technique for dimensionality reduction in statistical data analysis, mainly because it is simple to implement, can be solved efficiently, and is often effective in real-world applica-tions such as latent semantic indexing, face recognition and text clustering. However, one of the main challenges faced by PCA is that the observed data is often contaminated by outliers or missing values.

This problem has drawn much attention from researchers in various communities such as data mining, machine learn-ing, signal/image processing, and computer vision [21, 30, 5, 27, 34, 7, 16]. Based on compressive sensing and rank mini-mization, many methods for recovering low-rank and sparse matrices (also called robust principal component analysis or RPCA [30]) with incomplete or grossly corrupted observa-tions have been proposed, such as principal component pur-suit (PCP) [5] and outlier pursuit [31]. In principle, those methods aim to minimize a hybrid optimization problem in-volving both the l 1 -norm and trace norm minimization. It is recognized that the l 1 -norm and the trace norm as the convex surrogates for the l 0 -norm and the rank function are powerfully capable of inducing sparse and low-rank, respec-tively [24, 5]. In addition, Xu et al. [31] used the l 1 ; 2 to model corrupted columns.

Although RPCA has been well studied in recent years, there is little work focusing on RPCA plus matrix comple-tion (also called robust matrix completion or RMC [7]). In this paper, we are particularly interested in the trace norm regularized problem for RMC: where  X  X  X  is the trace norm of the desired matrix X , i.e., the sum of its singular values,  X  0 is a regularization parameter, and f (  X  ) denotes the loss function such as the l -norm loss or the l 1 -norm (or l 1 ; 2 -norm) loss functions. In the following, we give a few examples of applications where the RMC is useful.
In this paper, we aim to recover both low-rank and sparse matrices from missing and grossly corrupted observations. Our solution provides a good approximation to the origi-nal data contaminated with both outliers and missing val-ues. Unlike existing RPCA methods, our approach not only takes into account the fact that the observations are contam-inated by additive outliers or missing values, but can also identify both low-rank and sparse components from missing and grossly corrupted observations. We develop a provable and scalable solution framework for RPCA and RMC prob-lems, which is particularly useful in this X  X ig data X  X ra where many real-world applications need to deal with large, high dimensional data with missing and corrupted values. We conduct extensive experiments that verify both the efficiency and effectiveness of our methods.

We summarize the main contributions of our work as fol-lows:
This paper is organized as follows. We review background and related work in Section 2. In Section 3, we present a convex and a non-convex scalable trace norm regularized models. We develop two efficient ADAL algorithms in Sec-tion 4. We provide the theoretical analysis of our algorithms in Section 5. We report empirical results in Section 6, and conclude this paper in Section 7.
Given a matrix, some of its entries may not be observed due to problems in the acquisition process, e.g., loss of in-formation or high cost of experiments to obtain complete data [1]. For the matrix completion problem, many itera-tive thresholding algorithms [25, 19] have been proposed to solve the trace norm regularized linear least squares problem wh ere P  X  ( Z ) is defined as the projection of the matrix Z on the observed entries  X : P  X  ( Z ) ij = Z ij if ( i; j ) P
 X  ( Z ) ij = 0 otherwise. In other words, f (  X  ) is the l loss function, i.e., f ( X ) = 1 2  X P  X  ( X )  X  X   X  ( Z )  X  2
A lo w-rank matrix X can be recovered from highly cor-rupted matrix Z = X + E via the following trace norm and l -norm minimization problem where  X  X  X  X  1 indicates the element-wise l 1 -norm, i.e.,  X   X  ij | e ij | and E = Z  X  X . The model (3) is called the RPCA problem [30], where f (  X  ) is the l 1 -norm loss function, i.e., f ( X ) =  X  Z  X  X  X  1 . Several algorithms have been developed to solve the convex optimization problem (3), such as PCP [5] and IALM [17].

A more general RMC model in [7] and [16] aims to si-multaneously recover both low-rank and sparse components from incomplete and grossly corrupted observations via the convex optimization problem, Chen et al. [7] and Li [16] provided theoretical performance guarantees when minimizing trace norm plus l 1 -norm suc-ceeds in exact recovery. Although the RMC model (4) is a convex optimization problem, and can be solved by some convex algorithms, some additional variables need to be in-troduced. In addition, all current provable algorithms for RMC involve the singular vector decomposition (SVD), and thus they suffer from high computational cost of full or par-tial SVDs, which severely limits their applicability to large scale problems. To address the issues, we propose a scalable RMC method to recover matrices with missing and grossly corrupted observations.
From the optimization problem (4), we can find that the optimal solution E  X  C = 0 , where  X  C is the complement of  X , i.e., the index set of unobserved entries. Consequently, we have the following lemma.

Lemma 1. The RMC problem (4) is equivalent to the fol-lowing convex optimization problem,
To efficiently solve the RMC problem (4) and avoid in-troducing some auxiliary variables, we can assume without loss of generality that the unobserved data Z  X  C = 0 , and E may be any values such that Z  X  C = X  X  C + E  X  C . Therefore, the constraint with a linear projection operator P  X  in (4) is simplified into Z = X + E . It is worth noting that at last E
 X  C will be set to 0 for the expected output E . Hence, we replace the constraint P  X  ( Z ) = P  X  ( X + E ) with Z = X + E , and achieve the following equivalent form:
To further improve the efficiency of our convex model (6) and the scalability of handling large data sets, we also pro-pose a scalable non-convex model.
The desired low-rank matrix X is factorized into two much smaller matrices G  X  R m d ( G T G = I ) and H  X  R n d whose product is equal to X , i.e., X = GH T , where d is an upper bound for the rank of the matrix X , i.e., d  X  r = rank( X ). We have the following lemma.

Lemma 2. Let G and H be two matrices of compatible dimensions, where G has orthogonal columns, i.e., G T G = I , then we have  X  GH T  X  =  X  H  X  :
By substituting GH T = X and  X  H  X  =  X  X  X  into (6), we obtain a much smaller-scale matrix trace norm minimization problem,
Theorem 1. Suppose ( X ; E ) is a solution of the con-vex problem (6) with rank ( X ) = r , then there exists the solution G  X  R m d , H  X  R n d and E  X  R m n to the prob-lem (7) with d  X  r and E  X  C = 0 such that GH T = X , and ( GH T ; E ) is also a solution to the problem (6).
Proof. If we know that ( X ; E ) is a solution to the convex optimization problem (6), it is also a solution to Since for any ( X ; E ) with rank( X ) = r , we can find G  X  R m d and H  X  R n d satisfying GH T = X and P
 X  ( Z  X  GH T ) = P  X  ( E ) = P  X  ( E ), where d  X  r . More-over, according to Lemma 2, we have where P  X  C ( E )= 0 .

In the following, we will discuss how to solve our convex and non-convex models (6) and (7). It is worth noting that the RPCA problem (3) can be viewed as a special case of both (6) and (7), where all entries of the corrupted matrix are directly observed. We will develop two efficient alter-nating direction augmented Lagrangian (ADAL) solvers for solving our convex model (6) and non-convex model (7), respectively. It is also worth noting that although our non-convex algorithm will produce different estimations of G and H , the estimation of GH T is stable as guaranteed by Theo-rem 1 and the convexity of the problem (6).
In this section, we will develop two efficient alternating di-rection augmented Lagrangian (ADAL) algorithms for solv-ing both problems (6) and (7). First, we design a general convex ADAL scheme for solving the convex problem (6). Then we propose a similar procedure for solving the non-convex problem (7). The convergence analysis of our algo-rithms is provided in the next section.
 Algori thm 1 Solving the problem (8) via ADAL Input: 0 .
 Initi alize: x 0 = 0 , z 0 = 0 and 0 = 0 . for k = 0 ; 1 ;  X  X  X  ; T do end for Output: x k and z k .
We describe our optimization algorithm based on the ADAL method (also known as the alternating direction method of multipliers) for solving (6). The ADAL method was intro-duced for optimization in the 1970 X  X , and its origins can be traced back to techniques for solving partial difference equa-tions in the 1950 X  X . It has received renewed interest due to the fact that it is efficient to tackle large scale problems and solve optimization problems with multiple non-smooth terms in the objective function [17]. The ADAL can be considered as an approximation of the method of multipli-ers. It decomposes a large global problem into a series of smaller subproblems, and coordinates the solutions of sub-problems to compute the globally optimal solution. The problem solved by ADAL takes the following generic form where both f (  X  ) and g (  X  ) are convex functions. ADAL re-formulates the problem using a variant of the augmented Lagrangian method as follows: L ( x; z; ) = f ( x )+ g ( z )+ T ( Ax + Bz  X  c )+ wh ere is the Lagrangian multiplier and is a penalty pa-rameter. ADAL solves the problem (8) by iteratively min-imizing L ( x; z; ) over x; z , and then updating , as out-lined in Algorithm 1 [2]. Our RMC optimization problem (6) can be solved by ADAL. The augmented Lagrangian of (6) is given by App lying ADAL, we carry out the following updating steps in each iteration.
With all other variables fixed, the optimal X is the solu-tion to the following problem: To solve the problem (10), the spectral soft-thresholding op-eration [6, 4] is considered as a shrinkage operation on the singular values and is defined as follows: Algo rithm 2 Solving RMC problem (6) via ADAL Input: Giv en data P  X  C ( Z ) and .
 Initialize: X 0 = E 0 = Y 0 = 0 , 0 = 10 4 , max = 10 10 , while not converged do end while Output: X k and E k , where ( E k )  X  C is set to 0 . where T := Z  X  E + Y = , max { X  ;  X } should be understood element-wise, U  X  R m r , V  X  R n r , and = ( 1 ; 2 ; : : : ;  X  R r 1 are obtained by SVD of T , i.e., T = U diag( ) V T
The optimal E with all other variables fixed is the solution to the following problem, To solve the problem (12), we introduce the following well-known shrinkage (soft-thresholding) operator [8]: According to the soft-thresholding operator S , the closed-form solution E  X  to the problem (12) is given by
We can easily obtain the closed-form solution by zeroing the gradient of (12) with respect to E  X  C , i.e.,
We can replace the l 1 -norm loss function in the sparse component learning problem (12) with the l 2 ; 1 -norm loss function for a sparse solution, such as outlier pursuit [31] or low-rank representation [18] problems. The optimal solution to the problem with l 2 ; 1 regularization can be obtained by the soft-thresholding operator in [32].

Based on the description above, we develop an ADAL iter-ative algorithm for solving the RMC problem (6), as outlined in Algorithm 2 . In addition, E  X  C should be set to 0 for the expected output E . Moreover, an O (1 =k ) convergence rate of Algorithm 2 can be established following the conclu-sion in [11]. A fixed is commonly used. But there are some schemes of varying the penalty parameter to achieve better convergence. This algorithm can also be accelerated by adaptively changing . An efficient strategy [17] is to let = 0 (the initialization in Algorithm 2) and increase k iteratively by k +1 = k , where  X  (1 : 0 ; 1 : 1] in general and 0 is a small constant.

Algorithm 2 can be easily applied to solve the RPCA prob-lem (3), where all entries of the corrupted matrix are directly observed. Although we also use the PROPACK package [14] to compute a partial SVD as in [25, 4, 17], Algorithm 2 em-ploys the SVD for the spectral soft-thresholding operation, and becomes slow or even impractical for large-scale prob-lems. Therefore, we further propose an efficient ADAL al-gorithm for solving the non-convex problem (7) in Section 4.3. In addition, several researchers [13, 28] have provided some matrix rank estimation strategies to compute a good value r for the rank of the involved matrix. Thus, we only set a relatively large integer d such that d  X  r . Our non-convex RMC problem (7) can also be solved by ADAL. The augmented Lagrangian of (7) is given by W e will derive our scheme for solving the following subprob-lems with respect to G , H and E , respectively,
By fixing H and E at their latest values, and removing the terms that do not depend on G and adding some proper terms that do not depend on G , the optimization problem (16) with respect to G is reformulated as follows: where P := Z  X  E + Y = . This is actually the well-known orthogonal procrustes problem [22], the optimal solution can be given by the SVD of the matrix P H , i.e., where  X  U and  X  V are given by the SVD of P H , i.e., P H =  X  U  X 
S  X  V T .
By fixing G and E , the optimization problem (17) with respect to H can be rewritten as: Ac cording to Theorem 2.1 in [4], the closed-form solution to the problem (21) is given by the following theorem.
Theorem 2. The trace norm minimization problem (21) has a closed-form solution given by:
Proof. The first-order necessary and sufficient optimal-ity condition for the convex problem (21) is given by Algo rithm 3 Solving RMCMF problem (7) via ADAL Input: Giv en data P  X  C ( Z ) and .
 Initialize: G 0 = eye( m; d ), H 0 = 0 , E 0 = Y 0 = 0 , 0 10 4 , max = 10 10 , = 1 : 10, and tol. while not converged do end while Output: G k , H k and E k , where ( E k )  X  C is set to 0 . where @  X  H  X  deno tes the set of subgradients of the trace norm (optimality conditions for trace norm are given in [4]). Since ( G ) T G = I , the optimality condition for the prob-lem (21) is rewritten as follows: (23) is also the first-order optimality condition for the fol-lowing problem, Ac cording to Theorem 2.1 in [4], the optimal solution of the problem (24) is given by (22).
By fixing all other variables, the optimal E is the solution to the following problem: min The updating steps for E are very similar to (13) and (14), where X is replaced by G ( H ) T as follows: and
Following the above analysis, we obtain an ADAL algo-rithm to solve the matrix factorization based RMC (RM-CMF) problem (7), as outlined in Algorithm 3 . In addi-tion, E  X  C should be set to 0 for the output E . Algorithm 3 can also be easily applied to solve the RPCA problem (3).
We now provide convergence analysis and complexity anal-ysis for our algorithms.
The convergence of ADAL to solve the standard form (8) was studied in [9, 2]. We establish the convergence of Algo-rithm 2 by transforming the RMC problem (6) into a stan-dard form (8), and show that the transformed problem sat-isfies the condition needed to establish the convergence. In Algorithm 2, we state that our algorithm alternates between two blocks of variables, X and E . Let x denote the vectoriza-tion of X , i.e., x = vec( X )  X  R mn 1 , e = vec( E )  X  R mn 1 and z = vec( Z )  X  R mn 1 , and f ( X ) :=  X  X  X  and g ( E ) :=  X P  X  ( E )  X  1 . We can write the equivalence constraint in (6) as the following form: where both A  X  R mn mn and B  X  R mn mn are the identity matrices. By the definition f ( X ) and g ( E ), it is easy to verify that the problem (6) and Algorithm 2 satisfy the con-ditions in Algorithm 1. Hence, the convergence of Algorithm 2 is given as follows:
Theorem 3. Consider the RMC problem (6), where both f ( X ) and g ( E ) are convex functions, and A and B are both identity matrices and have full column rank. The sequence {
X k ; E k } generated by Algorithm 2 converges to an optimal solution { X ; E } of the problem (6).
 Hence, the sequence { X k ; E k } converges to an optimal solu-tion to the RMC problem (4), where ( E k )  X  C = 0 . Moreover, the convergence of our derived Algorithm 3 for the non-convex problem (7) is guaranteed, as shown in the following theorem.
 Theorem 4. Let ( G k ; H k ; E k ) be a sequence generated by Algorithm 3, then we have the following conclusions: 1. ( G k ; H k ; E k ) approaches to a feasible solution, i.e., 2. Both sequences G k H T k and E k are Cauchy sequences. 3. ( G k ; H k ; E k ) converges to a KKT point of the problem The proof of this theorem can be found in APPENDIX.
For the convex problem (6), the running time of Algorithm 2 is dominated by that of performing SVD on the matrix of size m  X  n . For the non-convex problem (7), Algorithm 3 performs SVD on much smaller matrices of sizes m  X  d and d  X  n , and some matrix multiplications in (22). Hence, the total time complexity of Algorithm 2 and Algorithm 3 are O ( tmn 2 ) and O ( t ( d 2 m + mnd )) ( d  X  n  X  m ), respectively, where t is the number of iterations.
Our non-convex method is the scalable version of our con-vex method for both RPCA and RMC problems. In addi-tion, the computational complexity of existing convex algo-rithms is at least O ( mn 2 ). It means that common RPCA (e.g., PCP [5]) and RMC (e.g., SpaRCS [27]) methods can-not handle large-scale problems, while our non-convex method has a complexity practically linear to the input matrix size and scales well to handle large-scale problems.

From the problems (3) and (6), we can see that in essence our convex method for RPCA problems is equivalent to IALM [17]. The models in [10] and [20] are the special cases of our model (7) when  X  X  X  . Moreover, the models in [33] and [3] focus only on the desired low-rank matrix. In this sense, them can be viewed as the special cases of our model (7). From the above complexity analysis, both schemes have the same theoretical computational complexity. However, from the experimental results in the next section, we can Fig ure 1: Image used in the text removal experi-ment: (a) Input image; (b) Original image; (c) Out-lier mask. see that our non-convex method usually runs much faster than the methods in [33] and [3]. The following bilinear reg-ularized matrix factorization formulation in [3] is one of the most similar model to our model (7),
In this section, we evaluate both the effectiveness and ef-ficiency of our RMC and RMCMF algorithms for solving RMC problems such as text removal, face reconstruction and background modeling. All experiments were performed using Matlab 7.11 on an Intel(R) Core (TM) i5-4570 (3.20 GHz) PC running Windows 7 with 8GB main memory.
We first conduct an experiment by considering a simulated image processing task on artificially generated data, and the goal is to remove some generated text from an image. The ground-truth image is of size 256  X  222 with rank equal to 10 for the data matrix. We then add to the image a short phase in text form which plays the role of outliers. Fig. 1 shows the image together with the clean image and outliers mask. For fairness, we set the rank of all the algorithms to 20, which is two times the true rank of the underlying matrix. The input data are generated by setting 30% of the randomly selected pixels of the image as missing entries. We compare our two methods, RMC and RMCMF, with the state-of-the-art methods, PCP [5], SpaRCS 1 [27], RegL1 2 [33] and BF-ALM [3]. We set the regularization parameter = 1 = for all algorithms in this experiment.

The results obtained by different methods are visually shown in Fig. 2, where the outlier detection accuracy (the score Area Under the receiver operating characteristic Curve, AUC) and the error of low-rank component recovery (i.e.,  X 
X  X  Z  X  F =  X  Z  X  F , where Z and X denote the ground-truth image matrix and the recovered image matrix, respectively) are also presented. As far as low-rank matrix recovery is con-cerned, the five RMC methods including SpaRCS, RegL1, BF-ALM, RMC and RMCMF, outperform PCP, not only visually but also quantitatively. For outlier detection, it can be seen that our methods RMC and RMCMF perform better than the other methods. In short, RMC and RMCMF sig-nificantly outperform PCP, SpaRCS, RegL1 and BF-ALM in terms of both low-rank matrix recovery and spare outlier identification. Moreover, the running time of PCP, SpaRCS, http ://www.ece.rice.edu/~aew2/sparcs.html https://sites.google.com/site/yinqiangzheng/ Figure 3: Performance of PCP, SpaRCS, RegL1, RMC and RMCMF in terms of AUC (left) and Er-ror (right) with varying ranks (the first row) or reg-ularization parameters (the second row).
 RegL1, BF-ALM, RMC and RMCMF is 15.39sec, 5.74sec, 3.86sec, 2.62sec, 1.10sec and 0.23sec, respectively.
Moreover, we further evaluate the robustness of our meth-ods, RMC and RMCMF, with respect to the regularization parameter and the given rank changes. We conduct some experiments on the artificially generated data, and illustrate the outlier detection accuracy (AUC) and the error (Error) of low-rank component recovery, where the rank parame-ter of our RMCMF method, SpaRCS and RegL1 is cho-sen from { 30 ; 35 ;  X  X  X  ; 70 } , and the regularization parame-ter of RMC, RMCMF, PCP and RegL1 is chosen from tice that we only report the results of RegL1 due to the similar performance of RegL1 and BF-ALM. The average AUC and Error results of 10 independent runs are shown in Fig. 3, from which we can see that our RMCMF method per-forms much more robust than SpaRCS and RegL1 in terms of AUC and Error with respect to the given rank. Fur-thermore, our RMC and RMCMF methods are much more robust than PCP and RegL1 in terms of AUC and Error against the regularization parameter.
We also test our methods for the face reconstruction prob-lems. The face database used in this experiment is a part of Extended Yale Face Database B [15] with large corruptions. The part of Extended Yale-B consists of 320 frontal face im-ages of the first 5 classes, and each subset contains 64 images with varying illumination conditions and heavily  X  X hadows X  as outliers. The resolution of all images is 192  X  168 and the pixel values are normalized to [0, 1], then the pixel values are used to form data vectors of dimension 32,256. The input data are generated by setting 40% of the randomly selected pixels of each image as missing entries.
 Fig. 4 shows some original and reconstructed images by RegL1 and CWM 3 [20], and our methods, RMC and RM-CMF, where the average computational time (in seconds) htt p://www4.comp.polyu.edu.hk/~cslzhang/papers.htm Figure 4: Face reconstruction results. From left column to right column: input corrupted images (black pixels denote missing entries), original im-ages, reconstruction results by CWM (1554.91sec), RegL1 (2710.68sec), RMC (54.19sec) and RMCMF (21.18sec). of all these algorithms on each people X  X  faces is presented. It can be observed that RMC and RMCMF perform much better than the other methods visually, as they effectively eliminate the heavy noise and X  X hadows X  X nd simultaneously complete the missing entries. In other words, our RMC and RMCMF methods can achieve the latent features underly-ing the original images even though the observed data is corrupted by both outliers and missing values. And impres-sively, both RMC and RMCMF are also significantly faster than RegL1 and CWM.
In this experiment we test our methods on real surveil-lance videos for object detection and background subtrac-tion as a robust matrix completion problem. Background modeling is a crucial task for motion segmentation in surveil-lance videos. A video sequence satisfies the low-rank and sparse structures, because the background of all the frames is controlled by few factors and hence exhibits low-rank property, and the foreground is detected by identifying spa-tially localized sparse residuals [30, 5]. We test our methods on real surveillance videos for object detection and back-ground subtraction on four color surveillance videos: Boot-strap, Lobby, Hall and Mall databases 4 . The data matrix Z consists of the first 400 frames of size 120  X  160. Since all the original videos have colors, we first reshape every frame of the video into a long column vector and then collect all the columns into a data matrix Z with size of 57600  X  400. Moreover, the input data is generated by setting 10% of the randomly selected pixels of each frame as missing entries.
Figs. 5 and 6 illustrate the foreground and background separation results on the Bootstrap and Mall databases, where the first and fourth columns represent the incom-plete input images, the second and fifth columns show the low-rank recoveries, and the third and sixth columns show the sparse components. It is clear that the background can be effectively extracted by RMC, RMCMF, BF-ALM, and GRASTA 5 [12]. Notice that SpaRCS [27] could not yield ex-perimental results on these databases because it ran out of memory. We can see that the decomposition results of RM-CMF and RMC are slightly better than that of GRASTA and BF-ALM. As pointed out in [23], the theoretical reason for the unsatisfactory performance of the l 1 -penalty is that the irrepresentable condition is not met. Hence, RMCMF incorporating with matrix factorization is more accurate in recovering the low-rank matrix than RMC. Furthermore, we also provide the CPU time consumption of these algorithms on all four databases, as shown in Table 1, from which we can see that RMCMF is more than 7 times faster than RMC, more than 4 times faster than GRASTA, and more than 2 times faster than BF-ALM. This further shows that RM-CMF has good scalability and can address large-scale prob-lems. We proposed a unified RMC framework for RPCA and RMC problems. We first presented two matrix trace norm regularized models that replace the linear projection op-erator constraint by a simple equality one. Then we de-htt p://perception.i2r.a-star.edu.sg/bkmodel/bkindex https://sites.google.com/site/hejunzz/grasta BF-ALM, RMC and RMCMF, respectively.
 Table 1: Comparison of time costs in CPU seconds of GRASTA, BF-ALM, RMC and RMCMF on back-ground modeling datasets.
 velo ped two efficient ADAL algorithms to solve our con-vex and non-convex low-rank and sparse matrix decompo-sition problems. Finally, we analyzed the convergence of our algorithms. Experimental results on synthetic and real-world data sets demonstrated the superior performance of our methods compared with the state-of-the-art methods in terms of both efficiency and effectiveness.

Both our algorithms are essentially the Gauss-Seidel schemes of ADAL, and their Jacobi-type update schemes can be easily implemented in parallel. Hence, our algorithms are well suited for parallel and distributed computing and are particularly attractive for solving certain large-scale prob-lems. Moreover, our methods can easily extended to the general nonconvex low-rank inducing penalty problem [26]. For future work, we will consider the compressing RMC (also called compressive principal component pursuit) prob-lem with the general linear operator as in [29].
We thank the reviewers for giving us many constructive comments, with which we have significantly improved our paper. This research is supported in part by SHIAE Grant No. 8115048, MSRA Grant No. 6903555, GRF No. 411211, and CUHK direct grant Nos. 4055015 and 4055017. [1] E. Acar, D. Dunlavy, T. Kolda, and M. M X rup. [2] S. Boyd, N. Parikh, E. Chu, B. Peleato, and [3] R. Cabral, F. Torre, J. Costeira, and A. Bernardino. [4] J. Cai, E. Cand`es, and Z. Shen. A singular value [5] E. Cand`es, X. Li, Y. Ma, and J. Wright. Robust [6] E. Cand`es and B. Recht. Exact matrix completion via [7] Y. Chen, A. Jalali, S. Sanghavi, and C. Caramanis. [8] I. Daubechies, M. Defrise, and C. DeMol. An iterative [9] J. Eckstein and D. Bertsekas. On the tively. [10] A. Eriksson and A. van den Hengel. Efficient [11] B. He and X. Yuan. On the O(1/n) convergence rate [12] J. He, L. Balzano, and A. Szlam. Incremental gradient [13] R. Keshavan, A. Montanari, and S. Oh. Matrix [14] R. Larsen. Propack-software for large and sparse svd [15] K. Lee, J. Ho, and D. Kriegman. Acquiring linear [16] X. Li. Compressed sensing and matrix completion [17] Z. Lin, R. Liu, and Z. Su. Linearized alternating [18] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma. [19] S. Ma, D. Goldfarb, and L. Chen. Fixed point and [20] D. Meng, Z. Xu, L. Zhang, and J. Zhao. A cyclic [21] K. Min, Z. Zhang, J. Wright, and Y. Ma.
 [22] H. Nick. Matrix procrustes problems. 1995. [23] Y. She and A. Owen. Outlier detection using [24] M. Tao and X. Yuan. Recovering low-rank and sparse [25] K.-C. Toh and S. Yun. An accelerated proximal [26] S. Wang, D. Liu, and Z. Zhang. Nonconvex relaxation [27] A. Waters, A. Sankaranarayanan, and R. Baraniuk. [28] Z. Wen, W. Yin, and Y. Zhang. Solving a low-rank [29] J. Wright, A. Ganesh, K. Min, and Y. Ma.
 [30] J. Wright, A. Ganesh, S. Rao, Y. Peng, and Y. Ma. [31] H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA [32] J. Yang, W. Yin, Y. Zhang, and Y. Wang. A fast [33] Y. Zheng, G. Liu, S. Sugimoto, S. Yan, and [34] T. Zhou and D. Tao. GoDec: Randomized low-rank &amp; We first prove that the boundedness of the multiplier and some variables of Algorithm 3, and then analyze the conver-genc e of Algorithm 3. To prove the boundedness, we first give the following lemmas.

Lemma 3. Let X be a real Hilbert space endowed with an inner product  X  X  X  and a corresponding norm  X  X  X  X  (the trace norm or the l 1 norm), and y  X  @  X  x  X  , where @  X  X  X  X  denotes the subgradient. Then  X  y  X  = 1 if x  X  = 0 , and  X  y  X   X  1 if x = 0 , where  X  X  X  X  is the dual norm of the norm  X  X  X  X  .  X  Y ( Z  X  G k +1 H T k  X  E k ) , where G k +1 is the optimal solution of the problem (19). Then the sequences { Y k } , {  X  Y k { H k } and { E k } produced by Algorithm 3 are all bounded.
Proof. By the first-order optimality condition of the prob-lem (18) with respect to E k +1 , we have Furthermore, by substituting Y k +1 = Y k + k ( Z  X  G k +1 E k +1 ) into (18), we have ( Y k +1 )  X  C = 0.

By Lemma 4, we have where  X   X   X  1 denotes the matrix l 1 -norm, i.e.,  X  M  X  1 max i;j | M i;j | . Thus, the sequence { Y k } is bounded. By the iteration procedure of Algorithm 3, we have  X L due to the boundedness of { Y k } . Then is upper bounded, i.e., { H k } and { E k } are bounded. Since  X 
G k H T k  X  =  X  H k  X  , { G k H T k } is also bounded.
We next prove that { e Y k } is bounded. Since G k +1 is the optimal solution of the problem (19), then we have By the definition of e Y k +1 , and k +1 = k , thus, By the boundedness of H k and Y k , then the sequence { e is bounded.

The first-order optimality condition of the problem (21) with respect to H k +1 is rewritten as follows: According to Lemma 4, we have  X  G T k +1  X  Y k +1  X  2  X  1, where  X 
M  X  2 denotes the spectral norm of M and is equal to the maximum singular value of M . Thus, G T k +1  X  Y k +1 is bounded. Let G ? k +1 denote the orthogonal complement of G k +1 , i.e., G Thus, { ( G ? k +1 ) T  X  Y k +1 } is bounded due to the boundedness of { e Y k } . Then we have sequence {  X  Y k } is bounded.
 Proof ofTheorem4 : the boundedness of { Y k } and lim k !1 k =  X  , we have Thus, ( G k ; H k ; E k ) approaches to a feasible solution. 2. We prove that the sequences { E k } and { G k H T k } are Cauchy sequences.
 thus, { E k } is a Cauchy sequence, and it has a limit E .
Similarly, { G k H T k } is also a Cauchy sequence, therefore it has a limit { G ( H ) T } . 3. According to Algorithm 3, the first-order optimality conditions of the problems (25) and (21) at the k -th iteration is formulated as follows: and Since both { E k } and { G k H T k } are Cauchy sequences, let E and G ( H ) T be limit of { E k } and { G k H T k } , respec-tively. By the definition of Y k , we have and ( G ; H ; E ) is a feasible solution, i.e., and G ( G ) T = I .

Thus, ( G ; H ; E ) is a KKT point of the problem (7).
