 Matrix completion latent factors models are known to be an effective method to build recommender systems. Cur-rently, stochastic gradient descent (SGD) is considered one of the best latent factor-based algorithm for matrix comple-tion. In this paper we discuss GASGD, a distributed asyn-chronous variant of SGD for large-scale matrix completion, that (i) leverages data partitioning schemes based on graph partitioning techniques, (ii) exploits specific characteristics of the input data and (iii) introduces an explicit parame-ter to tune synchronization frequency among the computing nodes. We empirically show how, thanks to these features, GASGD achieves a fast convergence rate incurring in smaller communication cost with respect to current asynchronous distributed SGD implementations.
 G.4 [ Mathematics of Computing ]: Mathematical Soft-ware  X  Parallel and vector implementations Recommender system; Matrix completion; Stochastic gradi-ent descent; Distributed computing; Graph partitioning
Many of todays internet businesses strongly base their suc-cess in the ability to provide personalized user experiences. This trend, pioneered by e-commerce companies like Ama-zon [8], has spread to many different sectors. As of today, personalized user recommendations are commonly offered by internet radio services, social networks, media sharing plat-forms (e.g. YouTube) and movie rental services.
Building user recommendations can be abstracted as a matrix completion problem ; given a set of users and a set of available items (e.g. songs, movies, tweets, etc.), these can be mapped to rows and columns of a rating matrix whose values represent preferences expressed by users on items. Building recommendations boils down to estimate, as pre-cisely as possible, all missing values in this matrix.
Recent experiences in this field [4, 2] has promoted algo-rithms based on latent factors as the best approach to the matrix completion problem. These algorithms map users and items to a latent feature space and define their affinity as the product of their latent feature vectors. The basic idea is that latent features, albeit non clearly definable, contain useful information on user and item similarity and can thus be exploited to predict missing ratings. Stochastic Gradi-ent Descent (SGD) [7, 12] is considered, as of today, the best latent factor based algorithm for matrix completion in all settings where the rating matrix is huge (e.g. millions of users with hundreds of thousands items and billions of available ratings). The SGD success stems from the avail-ability of efficient parallel implementations [16, 9, 10] that make it possible to efficiently exploit modern multi-processor and multi-core computing architectures. However, as the problem size grows to massive scale the memory occupa-tion of such algorithms become the limiting factor: the cost of computing infrastructures skyrockets, while the perfor-mance tend to grow non linearly due to more frequent con-tention on data shared among running processes. Thereby, the research trend is now focussed on distributed SGD imple-mentations whose aim is to make the analysis of huge data sets feasible on possibly large clusters of cheap commodity machines. The shift toward distributed implementations is a key move to make recommendation algorithms a service easily and cheaply available through cloud platforms.
Recent proposals [13, 5] represent important steps in the right direction, but still suffer from some limitations that hinder their performance. Firstly, all these solutions divide the input dataset among the available SGD instances lever-aging, at best, randomized partitioning approaches; parti-tioning is performed by subdividing the rating matrix in blocks of the same size; however, no attention is payed to the amount of data (user preferences) in each block, even if this value drives the real load of the SGD instance that will compute on that matrix block; as a consequence, this  X  X lind X  approach to partitioning, albeit easily applicable, can cause skewed load on the computing nodes. Furthermore, it is in-efficient as it does not appropriately exploit some intrinsic characteristics of the input data. Secondly, all these solu-tions employ a bulk synchronous processing approach where shared data values are concurrently updated on local copies and then periodically resynchronized. The synchronization frequency is a fundamental parameter for these solutions as it regulates a crucial tradeoff between the convergence rate of the algorithm toward the solution and the communica-tion cost incurred during its functioning. To the best of our knowledge, none of the existing works in this field explored and leveraged this tradeoff.

Our proposal is to address the aforementioned problems with three distinct contributions. On one side we mitigate the load imbalance among SGD instances by proposing an input slicing solution based on graph partitioning algorithms (contribution 1). This approach looks at the input data as a graph where each node represents either a user or an item and each edge represents a rating expressed by the former on the latter. The graph is then greedly partitioned edge by edge thus providing a substantially more balanced load among the available computing nodes. A better balanced load has a positive impact on the overall efficiency of the system as less loaded processes will not wait idly while more loaded processes delay the bulk data synchronization phase (assuming homogeneous computing nodes).

On the other side we attack the efficiency problem with two distinct solutions: 1. By properly leveraging known characteristics of the in-2. By tuning the frequency used by SGD instances to
All our three contributions nicely integrate with the cur-rent state of the art as they don X  X  require modifications to ex-isting asynchronous distributed SGD algorithms, but rather complement them. In particular, contributions 1 and 2 are integrated in a input partitioner that must be used to pre-process input data before running the SGD algorithm, while contribution 3 is provided as an empirical evaluation on well known publicly-available data traces.

In the following we will first introduce the reader to the the matrix completion problem (Section 2) and then detail the SGD algorithm with its parallel and distributed vari-ants (Section 3). Our novel input partitioner included in GASGD is then described in Section 4, followed by an ex-perimental evaluation (Section 5) that backups our claims. Section 5 also contains a description of contribution 3. Fi-nally, an analysis of the related work (Section 6), and some final remarks (Section 7) will conclude this paper.
We consider a system constituted by U = ( u 1 ,  X  X  X  ,u users and X = ( x 1 ,  X  X  X  ,x m ) items. Items represent a gen-eral abstraction that can be case by case instantiated as news, tweets, shopping items, movies, songs, etc. Users can rate items with values from a predefined range. Without loss of generality, here we assume that ratings are repre-sented with real numbers. By collecting user ratings it is possible to build a n  X  m rating matrix R that is usually a sparse matrix as each user rates a small subset of the avail-able items. Denote by O  X  { 1 ,...,n } X { 1 ,...,m } the set of indices identifying observed entries in R ; ( i,j )  X  O implies that user u i rated item x j with vote r ij . The training set is defined as T = { r ij : ( i,j )  X  O } . The goal of a matrix com-pletion algorithm is to predict missing entries b r ij in R using ratings contained in the training set. As demonstrated by the Netflix competition [2] and the KDD-Cup 2011 [4], col-laborative approaches (i.e., collaborative filtering techniques [11]) based on latent factors are today considered the best models for large-scale recommender systems.

Denote by k min ( n,m ) a rank parameter. These kind of solutions aim at finding an n  X  k row-factor matrix P  X  ( user vectors matrix ) and an k  X  m column-factor matrix Q  X  ( item vectors matrix ) such that R  X  P  X  Q  X  . The ap-proximation is performed by minimizing an application de-pendent error function L ( P,Q ), that measures the quality of the reconstruction. We call p i , the i -th row of P , the k -dimensional vector of user u i and q j , the j -th column of Q , the k -dimensional vector of item x j . Unobserved entries b r ij  X  R, ( i,j ) /  X  O are predicted by multiplying the corre-sponding user and item vectors p i q j . There exists a wide range of objective functions for matrix completion and fac-torization. The most used error function is the regularized squared loss [7, 12, 13, 16]: where || X || F is the Frobenius norm and  X   X  0 is a regulariza-tion coefficient used to avoid overfitting. In accordance with [13], we observe that Equation 1 is in summation form as it is expressed as a sum of local losses L ij for each element in R:
The most popular techniques to minimize the objective function are Alternating Least Squares (ALS) and Stochastic Gradient Descent (SGD). ALS [15] alternates between keep-ing P and Q fixed. The idea is that, although both these values are unknown, when the item vectors are fixed, the system can recompute the user vectors by solving a least-squares problem (that can be solved optimally), and vice versa. In this paper we will focus on SGD [7, 12], since it has been shown that it performs better on large-scale data [13]. SGD, in fact, was the approach chosen by the top three solutions of KDD-Cup 2011 [4]. SGD works by iteratively updating current estimations of P and Q with values proportional to the negative of the gradient of the error function. The term stochastic means that P and Q are updated, at each iteration, by a small step for each given training case toward the average gradient descent. For each observed entry ( i,j )  X  O , the objective function is expressed by Equation 2 and the variables are Figure 1: The items and users degree distributions in the considered datasets (log-log scale). tion achieves nearly perfect load balance on large graphs, but provides results with very high replication factors. A bet-ter solution is represented by a greedy approach [6, 1] that breaks the randomness of the hashing solution by maintain-ing some global status information. In particular, the system stores the set of computing nodes A ( v ) to which each already observed vertex v has been assigned 2 . The algorithm, when processing edge e i,j  X  E operates as follows: Case 1 If neither u i nor x j have been assigned to a node, Case 2 If only one of the two vertices has been already Case 3 If A ( u i )  X  A ( x j ) 6 =  X  , then the edge e i,j Case 4 If A ( u i ) 6 =  X  , A ( x j ) 6 =  X  and A ( u i )  X  A ( x We implemented the above procedure in a parallel fashion. In particular, the input data is randomly divided between the computing threads/nodes. All the threads/nodes have
The memory footprint needed to run such algorithm is or-ders of magnitude smaller than the input data. Figure 2: Replication factor and load RSD achieved by the various partitioning schemes (log-log scale). access to a shared key-value storage that maintains as key the id of the vertex v and as value A ( v ). To manage concur-rent access we define fine-grained locks, one for each entry.
The novelty of our solution lies on the idea to exploit the bipartite nature of the considered graph. A bipartite graph is a graph where the vertex set V can be split in two disjoint subsets W 1 and W 2 such that all edges connect vertices placed in distinct subsets: if { v,w } X  E , either v  X  W 1 and w  X  W 2 or v  X  W 2 and w  X  W 1. Such graphs are natural in the area of recommender systems, where vertices represent distinct classes of objects, e.g. users and items.
Our algorithm exploits the fact that in real word datasets the size of the two sets constituting the bipartite graph are often significantly skewed: one of the two sets is much bigger that the other. If this is the case, by perfectly splitting the bigger set it is possible to achieve an average replication factor smaller than the one obtained through the greedy approach (although we will show in Section 5 that this is not always enough to achieve a smaller communication cost).
It is therefore possible to identify two dual approaches:
Dataset MovieLens Netflix Yahoo! Table 1: Statistics and parameters for each dataset.
To achieve this result we modify Case 4 of the greedy algorithm, that is the only case where replicas are gener-ated. Assume without loss of generality a user partitioned strategy, so | A ( u i ) | = 1 ,  X  u i  X  U .

Case 4 If A ( u i ) /  X  A ( x j ) then e i,j is assigned to A ( u
In this section we report the results of the experimental evaluation we conducted on a prototype implementation of GASGD. The goal of this evaluation was to show how the GASGD characteristics provide a solution able to reach high quality results in an efficient manner.

We used three datasets for the experiments: MovieLens 10M 3 , Netflix [2] and Yahoo! [4]. Dataset statistics as well as simulation parameters are synthesized in Table 1. The re-ported learning rate  X  only represents the starting value, as we adopted an adaptive mechanism called bold driver [13] to automatically vary the parameter during the computation. Furthermore, in our implementation each computing node shuffles the training points it analyzes before each epoch.
We implemented five different input partitioning scheme: grid , hashing , greedy , greedy -item partitioned ( GIP ), greedy -user partitioner ( GUP ). The grid partitioner, commonly used by DSGD (Section 3), simply shuffles rows and columns of the rating matrix R , and then divides it in | C | identical blocks. The other four schemes are based on graph parti-tioning techniques (Section 4). As a baseline for comparison we used the solution obtained by running SGD on a sin-gle machine. For this centralized approach we employed the parallel-SGD algorithm (with locks) described in Section 3.
We begin our experimental analysis by studying the qual-ity of the various partitioning schemes, in terms of achieved replication factor (average number of copies for each vec-tor) and load balancing (expressed as the Relative Standard Deviation (RSD) of the load among the nodes). Figure 2 reports the obtained results. As expected GUP achieves the smallest RF in those scenarios where the number of users is much bigger than the number of items (MovieLens and Netflix), sporting, moreover, a perfect load balance among the computing nodes. However, in the Yahoo! dataset users and items are comparable in number and the classical greedy solution outperforms its user partitioned variant. Interest-ingly, the grid solution always outperforms the greedy ap-proach in RF , even if its load balancing performance are inferior. Intuitively, by shuffling rows and columns of R http://grouplens.org/datasets/movielens Figure 3: Algorithms convergence rate (loss vs epochs), with f = 1 and f = 100 (y-axis in log-scale). and then grouping them in blocks, grid achieves the same objective of greedy , that is to replicate users or items with high degree, but does so in a more effective way, since it has complete knowledge of the rating matrix R while greedy processes the training set in a streaming fashion.

To assess the quality of the solution obtained from the asynchronous distributed SGD variants, we compare the convergence curves of the objective function expressed by Equation 1. Note that here our goal was not to assess the absolute solution quality achieved by the algorithm (i.e., RMSE), but rather to show how fast the solutions provided by the distributed solutions approach the centralized one. Figure 3 shows the loss curves for the various algorithms, applying two different synchronization frequencies: f = 1 and f = 100. In general, by increasing f the ASGD curves tend to reproduce the centralized SGD trend, while with f = 1 the distributed convergence rates are quite far from the centralized one. This was expected, since with f = 1 all the vector copies are free to diverge during the compu-tation of an epoch while with f = 100 they are frequently synchronized, closely reproducing the behavior of a central-ized implementation. The worst performance, with small synchronization frequencies, is provided by the grid solu-tion: by shuffling R , in fact, grid uniformly distributes the ratings of an entity (i.e. user or item) among the available nodes, thereby favoring the divergence of vector copies.
To synthesize these results we used an aggregate value for the convergence curves difference: the sum of squared errors (SSE) between the ASGD curve points and the cen-tralized ones (limited to the first 30 epochs). The top half of Figure 4 (Figures 4a, 4b and 4c) reports the obtained SSE values varying the synch frequency f . In datasets with significantly more users than items (MovieLens and Netflix) the GUP strategy convergence curve quickly approaches the centralized one (respectively with f = 8 and f = 16) while other solutions achieve the same result with larger frequen-cies. Furthermore, while increasing the number of nodes degrades the performance of both greedy and grid (dashed lines in the figures), the GUP solution is not affected by this problem; it actually sports a performance improvement to its convergence rate in the Netflix dataset (Figure 4b). Also in the Yahoo! dataset the GUP approach outperforms the other variants in convergence rate (Figure 4c).

The GUP loss curve is the one more quickly approaching the centralized SGD curve for the smallest value of f . Now we will show how, at that frequency f , the GUP solution is also the least expensive with respect to communication cost. We express the communication cost ( CC ) as the total num-ber of messages that the system exchanges in an epoch. This cost depends on three factors: the number | C | of processing nodes, the synchronization frequency f and the replication factor ( RF ). We remark that RF is a weighted average of the replication factor of item ( RF X ) and user ( RF U ) vectors.
More formally, each node c  X  C owns a slice T c of the training set T , where | T c | = | T | | C | assuming perfect load bal-ance. At the beginning of each epoch all computing nodes shuffle the content of their slice T c and cut it in f folds such that T c = { K c 1  X  ...  X  K c f } . Each fold K roughly | T c | f ratings. We define U c y = { u i  X  U :  X  r (resp. X c y = { x j  X  X :  X  r ij  X  K c y } ) the set of users (resp. items) with at least a rating in the fold. We denote with U y = { U 1 y  X  ...  X  U | C | y } (resp. X y = { X 1 y  X  ...  X  X set of users (resp. items) with at least one rating in the y -th fold on any computing node. The communication cost for an epoch is then defined as:
The first part of Equation 7 considers messages that the working copies send to the master, while the second one considers messages that the vector master copies send to all the replicas after the update. For the sake of simplicity the equation does not consider the fact that working and master copies overlap on master nodes, and so they don X  X  need a messages exchange.

The bottom half of Figure 4 (Figures 4d, 4e and 4f) shows the CC incurred by the various solutions varying f . To better understand the charts we show what happens to CC for the extreme values of f : f = 1, single synchronization at the end of each epoch, and f = | T | | C | , vectors are synchronized after each observation. f = By looking at (8), it is clear that, at low frequencies, the solution with the smallest RF achieves the lowest CC (i.e. GUP in MovieLens and Netflix, grid in Yahoo!). The sit-uation changes by increasing f , since, for instance, in the Netflix dataset (Figure 4e) the GUP solution gradually be-comes the most expensive. This behavior is explained by Equation 9. At high frequencies, in fact, the CC basically depends on the sum of RF U and RF X . The GUP strategy, by partitioning the user set, is forced to highly replicate all items. This behavior has a small impact on RF , as the num-ber of users is larger than the number of items; however, GUP sports large RF X and thus incurs highly expensive synchronization phases for high sync frequencies.

The positive aspect highlighted by these tests is that it X  X  not necessary to massively increase f in order to achieve a convergence rate similar to a centralized implementation. A dozen synchronizations per epoch, in fact, are enough for GUP to reach this goal in both the MovieLens and Netflix datasets (Figures 4a and 4b), and at such frequency it is the most efficient solution (Figures 4d and 4e). For the Yahoo! case, instead, a slightly higher frequency is required for GUP to reach the centralized convergence rate (Figure 4c). At that frequency, GUP is just a bit more expensive in CC than grid (Figure 4f), but with the advantage of (i) an almost perfect load balance and (ii) the streaming implementation, that makes it applicable to huge input data (i.e. exceeding the memory capacity of a single node).
The scalability of collaborative filtering algorithms is a research topic that was recently interested by a huge growth of contributions from the scientific community [13, 5, 16, 9, 10]; we already discussed most of them in Section 3 as they represent a cornerstone for the work proposed in this paper, so this section is devoted to a short description of a few further works that are connected to this paper.
The vertex-cut distributed graph placement problem is addressed by PowerGraph [6], a distributed graph-parallel computation paradigm, in which each vertex in parallel exe-cutes some software, respecting the Gather-Apply-Scatter (GAS) programming model. We already described their greedy partitioning algorithm in Section 4.

In the context of graph factorization, [1] proposes an asyn-chronous SGD version where the global explicit synchroniza-tion phase is eliminated. Vector copies, indeed, are synchro-nized continuously (a thread loops in local memory) in an asynchronous fashion and independently. We think that also in this case it X  X  possible to tune the frequency of the synchro-nization, in a fine-grained fashion, and we leave this idea as part of our future work.
In this paper we described three distinct contributions aimed at improving the efficiency and scalability of Asyn-chronous distributed SGD algorithms. In particular, we proposed a novel input slicing solution based on graph par-titioning approach that mitigates the load imbalance among SGD instances (i.e. better scalability), while, at the same time, providing lower communication costs during the algo-rithm execution (i.e. better efficiency) by exploiting specific characteristics of the training dataset. The paper also in-troduces a synchronization frequency parameter driving a tradeoff that can be accurately leveraged to further improve the algorithm efficiency. [1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, [2] J. Bennett and S. Lanning. The netflix prize. In [3] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, [4] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer. [5] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis. [6] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and [7] Y. Koren, R. Bell, and C. Volinsky. Matrix [8] J. Mangalindan. Amazon X  X  recommendation secret. [9] F. Niu, B. Recht, C. R  X e, and S. J. Wright. Hogwild!: [10] B. Recht and C. R  X e. Parallel stochastic gradient [11] X. Su and T. M. Khoshgoftaar. A survey of [12] G. Tak  X acs, I. Pil  X aszy, B. N  X emeth, and D. Tikk. [13] C. Teflioudi, F. Makari, and R. Gemulla. Distributed [14] J. N. Tsitsiklis, D. P. Bertsekas, M. Athans, et al. [15] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan. [16] Y. Zhuang, W.-S. Chin, Y.-C. Juan, and C.-J. Lin. A
