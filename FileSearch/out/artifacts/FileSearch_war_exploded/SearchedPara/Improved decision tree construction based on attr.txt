 1. Introduction increasing with the complexity of installed equipment. This phenomenon affects product quality, causes the immediate shut-down of a machine, and undermines the proper functioning of an entire production system. Rotating machines are a major class of mechanical equipment, and need the utmost care and continuous monitoring to ensure optimal operation. Traditionally, vibration analyses and many signal processing techniques have been used to extract useful information for monitoring the operating condition.
Khelf et al. (2013) analysed the frequency domain to extract information and diagnose faults. Cepstral analysis has been used to construct a robust gear fault indicator ( Badaoui et al., 2004 ), and a short-time Fourier transform representation was derived ( Mosher et al., 2003 ). Other techniques have also been employed, such as the Wigner  X  Ville distribution ( Baydar and Ball, 2001 ), continuous wavelet analysis ( Kankar et al., 2011 ), and discrete wavelet analysis ( Djebala et al., 2008 ).
 condition-monitoring diagnostic systems. For example, neural networks ( Chen and Chen, 2011 ), support vector machines ( Deng been applied. However, decision tree techniques are still preferred in engineering applications, because they allow users to easily understand the behaviour of the built models against the above-in numerous research papers, e.g. Sugumaran and Ramachandran (2007) , Zhao and Zhang (2008) , Sakthivel et al. (2010) ,and Sugumaran et al. (2007) .

The construction of a decision tree (DT) includes growing and pruning stages. In the growing phase, the training data (samples) are repeatedly split into two or more descendant subsets, accord-ing to certain split rules, until all instances of each subset wrap the same class (pure) or some stopping criterion has been reached.
Generally, this growing phase outputs a large DT that includes the learning examples and considers many uncertainties in the data (particularity, noise and residual variation). Pruning approaches based on heuristics prevent the over-fi tting problem by removing all sections of the DT that may be based on noisy and/or erroneous data. This reduces the complexity and size of the DT. The pruning phase can under-prune or over-prune the grown DT. Moreover, many existing heuristics are very challenging ( Breiman et al., 1984;
Niblett and Bratko, 1987; Quinlan, 1987 ), but, unfortunately, no single method outperforms the others ( Mingers, 1989; Esposito et al., 1997 ).

In terms of growing phase problems, there are two possible solutions: the fi rst reduces DT complexity by reducing the number of learning data, simplifying the decision rules ( Piramuthu, 2008 ). The second solution uses attribute selection to overcome over-fi tting problems ( Yildiz and Alpaydin, 2005; Kohavi and John, 1997 ). To overcome both the DT size and over-fi tting risks, we propose to combine attribute selection and data reduction to construct an Improved Unpruned Decision Tree IUDT . The opti-mal DT construction (DTC) problem will thus be converted into an exploration of the combinatorial graph research space problem. The key feature of this proposition is to encode each subset of attributes A i and a samples subset X j into a couple  X  A show that the proposed schematic largely improves the tree performance compared to standard pruned DTs, as well as those based solely on attribute selection or data reduction.
The rest of the paper is organized as follows: In Section 2 , some previous studies on DTC are brie fl y discussed. Section 3 introduces the main notions used in this work. In Section 4 , we describe our approach based on attribute selection and database sampling to outperform conventional DTC. Section 5 reports the experimental results using 10 benchmark datasets. In Section 6 , IUDT is applied to the problem of fault diagnosis in rotating machines. Finally, Section 7 concludes the study. 2. Related work
This section describes post-pruning approaches that have been proposed to improve DTC. Their common aim was to decrease (1) the tree complexity and (2) the error rate of an independent test dataset. Pruning methods have various differences that can be summarized as follows: 1. the necessity of the test dataset; 2. the generation of a series of pruned sub-trees or the processing of a single tree; 3. the pruning determination criteria.
 Breiman et al. (1984) developed error-complexity pruning, which uses the cost-complexity risk. The pruning measure uses an error rate penalty based on the sub-tree size. The errors and the size of the tree's leaves (complexity) are both considered in this pruning method. The cost-complexity risk measurement of all possible added to the product of a factor  X  and the number of leaves j t j in trees with the smallest value of  X  are selected to be pruned. Finally, the correctly pruned sub-tree t is selected from the  X  sequence of sub-trees using an independent test dataset. The fi nal selection is based on the error rate or standard error (assuming a binomial distribution).

Reduced-error pruning, proposed by Quinlan (1987) , produces a series of pruned DTs using the test dataset. A complete DT T fi rst grown using the training dataset. A test dataset is then used, and for each node in T 0 , the number of classi fi cation errors made on the pruning set when the sub-tree t is kept is compared with the number of classi fi cation errors made when t is turned into a leaf. Next, the positive difference between the two errors is assigned to the sub-tree root node. The node with the largest difference is then pruned. This process is repeated until the pruning increases the misclassi fi cation rate. Finally, the smallest version of the most accurate tree with respect to the test dataset is generated.

In contrast to reduced-error pruning, the necessity of separate test datasets can be avoided using pessimistic error pruning (PEP, Quinlan, 1987 ). This uses the binomial continuity correction rate to obtain a more realistic estimate of the misclassi fi cation rate. The misclassi fi cation correction depends on the number of leaves and misclassi fi cations.

Error-based pruning (EBP, Quinlan, 1993 ) is an improved ver-sion of PEP that traverses the tree according to a bottom-up post-order strategy. No pruning dataset is required, and the binomial continuity correction rate of PEP is used. Therefore, the difference is that, in each iteration, EBP considers the possibility of grafting a branch t y in place of the parent of y itself. The estimation errors t ; t are calculated to determine whether it is convenient to prune node x (the tree rooted by x replaced by a leaf), replace it with t (the largest sub-tree), or keep the original t x .

Recently, Luo et al. (2013) developed a new pruning method based on the structural risk of the leaf nodes. This method was developed under the hypothesis that leaves with high accuracies mean that the tree can classify the training data very well, and a large volume of such leaves implies generally good performance.
Using this hypothesis, the structural risk measures the product of the accuracy and the volume of leaf nodes. As in common pruning methods, a series of sub-trees are generated. The process visits each node x on DT T 0 ( t x is a sub-tree whose root is x ). For each sub-tree t x , feasible pruning nodes are found (their two children are leaves), and the structural risks are measured. Finally, the sub-tree that maximizes the structural risk is selected for pruning.
Additional post-pruning methods have been proposed, such as critical value pruning ( Mingers, 1987 ), minimum error pruning ( Niblett and Bratko, 1987 ), and DI pruning (which balances both the Depth and the Impurity of nodes) ( Fournier and Cr X milleux, 2002 ). The choice of DT has also been validated ( Karabadji et al., 2012 ), and genetic algorithms used to pull out the best tree over a set of different models (e.g. BFTree, J48, LMT, Hall et al., 2009 ). To select the most robust DT, all models were generated and their performances measured on distinct training and validation sets. In this work, the main objective is to construct DTs without under-pruning or over-fi tting the training dataset, and without choosing between different pruning methods. Two prior works have shown that unpruned DTs give similar results to pruned trees when a
Laplace correction is used to calculate the class probabilities ( Bradford et al., 1998; Provost and Domingos, 2003 ).
The identi fi cation of smaller sets of highly predictive attributes has been considered by many learning schemes. Attribute selec-tion shares the same objective as pruning methods, namely the elimination of irrelevant, redundant, and noisy attributes in the building phase to produce good DT performance. Many studies have investigated and improved classi fi cation models ( Bermejo et al., 2012; Maca  X  et al., 2012 ). In these works, wrapper techni-ques have been applied to attribute selection. A target learning algorithm is used to estimate the value of attribute subsets. The process is driven by the binary relation  X  D  X  between attribute subsets. The search process can be conducted on a depth-fi breadth-fi rst basis, or a combination of both (e.g.  X  A star algorithm). Wrappers are generally better than fi lters, but the improved performance comes at a computational cost  X  in the worst case, 2 m subsets of attributes must be tested ( m is the number of attributes) ( Kohavi and John, 1997 ).

Similar to attribute selection, DTs can be improved by reducing the data complexity, as well as reducing the effects of unwanted data characteristics. Data reduction essentially involves dimen-sionality reduction and/or example reduction ( Piramuthu, 2008 ).
Generally, reduction methods use sampling (e.g. random, strati-fi ed) to select examples for consideration in the learning phase ( Ishibuchi et al., 2001; Liu, 2010 ).

In conclusion, different pruning techniques have been studied, but none is adequate for all varieties of problem. There has been a recent focus on attribute selection and sampling data to improve
DTC. To realize a better DT for a speci fi c application, we propose the IUDT algorithm, which combines a novel scheme of random database sampling with an attribute selection wrapper process.
The main objective of our study is to reduce the effective number of examples and training data attributes, and thus minimize the size of the DT. 3. Preliminaries relevance and redundancy problems, and fi nally DTC. The de presented in this section use the same semantics as in Davey (2002) . 3.1. Partial order relation.
 re fl exive, transitive, and anti-symmetric. where  X  de fi nes the following order on X :  X   X f X  a ; b  X  ,  X  a  X  c ; d  X  ,  X  c ; e  X  ,  X  d ; e  X  ,  X  a ; a  X  ,  X  b ; b  X  ,  X  c
We can represent an ordered set P as an oriented graph whose nodes correspond to the X elements and whose edges denote the relation  X  , with loops representing the couples  X  x ; x  X  . Fig. 1 illustrates Example 1 .

De fi nition 2 ( Graph ). A graph G  X  X  V ; E  X  consists of a set of vertices V and an edges set E D V V . Each edge e A E is associated with an unordered pair of vertices.

In the case of an oriented graph, each edge e A E is associated with an ordered pair of vertices. In the rest of this section, we denote a research graph L and an element in L (i.e. a vertex v A G )asa pattern. 3.1.1. Specialization and generalization patterns partial order  X  on the patterns in L . A pattern  X  is more general than another pattern  X  if  X   X   X  . Similarly,  X  is more speci , i.e. for the relation a  X  e in Example 1 , a is more general than e and e is more speci fi c than a . 3.1.2. Specialization and generalization operators operator P s associates each pattern  X  A L with a pattern set that is more speci fi c: P g  X   X   X  X f  X  A L j  X  !  X  g . Similarly, we can de generalization operator P g such that P g  X   X   X  X f  X  A L j
P is said to be direct (immediate) if it only associates  X  with the most general patterns of the set of patterns that are more speci
P  X  ate) if it only associates  X  with the most speci fi cpatternsofthesetof patterns that are more general P g  X   X   X  , P g  X   X   X  X  max  X  P 3.1.3. Minimum and maximum patterns
Let be a specialization relation in L , and  X  D L be a patterns set. min  X   X   X  is the most general patterns set of  X  , and max  X  most speci fi c patterns set of  X  . min  X   X   X  X f  X  A  X  j  X   X  A  X  s : t :  X  !  X  g max  X   X   X  X f  X  A  X  j  X   X  A  X  s : t :  X  !  X  g
Using a direct order relation, we can represent a partially ordered set by a directed graph and acyclic Hasse diagram. Fig. 2 illustrates Example 1 as a Hasse diagram. 3.1.4. Research graph traversing strategy
Graph specialization is generated by the specialization order relation de fi ned on L , which can be traversed in several modes:
Breadth-fi rst search: traverses the research space (specializa-tion graph) elements in a bottom-up manner and generates patterns level-by-level. Known to be an apriori-like where all motifs with the same level are explored before the more speci fi c ones.

Depth-fi rst search: achieves the quickest possible solution by exploring the immediate successor of any generated pattern, and specializes as much as possible the pattern level before exploring patterns of the same level. 3.2. Over-fi tting
Consider the sample data illustrated in Fig. 3 . Let us assume observations from two classes and from  X  a ; b ; c  X  different classi models.

Classi fi er (a) simply classi fi es the data according to a straight line. This gives a very poor classi fi cation, considered as a hazard becoming increasingly dependent on the samples. Thus, its capa-city to correctly predict other data classes decreases. Finally, classi fi er (b) gives the best generalization of the learning process, and has the smallest probability of misclassifying new data. De exists h 0 A H that has a higher training set error but lower test error on the test data. (More speci fi cally, if learning algorithm A explicitly considers and rejects h' in favour of h, we say that A has over-fi tted the data.) 3.3. Attribute particularity
The identi fi cation (elimination) of relevant (redundant) attri-butes is the main purpose of an attribute selection algorithm. 3.3.1. Relevance
In machine learning, relevance includes three disjoint cate-gories: strong relevance, weak relevance, and irrelevance ( Kohavi and John, 1997 ), in order of importance. Strongly relevant attri-butes should be conserved by any attribute selection algorithm; however, Ruiz et al. (2006) state that there is no guarantee that a feature will necessarily be useful to an algorithm just because of its relevance (or vice versa). Weakly relevant attributes could be conserved or not, depending on the evaluation measure (e.g. accuracy, simplicity) and other selected attributes. Irrelevant attributes should be eliminated. 3.3.2. Incremental relevance
Caruana and Freitag (1994) de fi ne incremental relevance by considering the monotonicity of the accuracy and order of the set of subsets P  X  R ; D  X  .
 De fi nition 4 ( Incremental usefulness ).  X  Given data D , a learning algorithm T , and a subset of attributes X , the attribute e is incrementally useful to T with respect to X if the accuracy of the hypothesis that T produces using the group of attributes f e g[ X is better than the accuracy achieved using just the subset of attributes X  X  ( Caruana and Freitag, 1994 ).
 To obtain a predictive feature subset of attributes, the above de fi nition is useful in the present work. 3.3.3. Redundancy
Attribute redundancy in machine learning is widely accepted as the correlation between attributes. Two attributes are redundant to each other if their values are completely correlated. Correlation between two variables can be checked based on the entropy, or the random variable uncertainty ( Xing et al., 2001; Liu and Yu, 2005 ). 3.4. Decision trees
DTs are built recursively, as illustrated in Fig. 4 , following a top-down approach. They are composed of a root, several nodes, branches, and leaves. DTs grow according to the use of an attribute sequence to divide training examples into n classes. Tree building can be described as follows. First, the indicator that ensures the best split of the training examples is chosen, and population subsets are distributed to new nodes. The same operation is repeated for each node (subset population) until no further split operations are allowed. Terminal nodes are composed of popula-tions in the same class (increased proportion in some types of DT).
The classi fi cation operation assigns an individual to a terminal node (leaf) by satisfying the set of rules oriented to this leaf. The set of rules forms the DT.

It is clear that the size and uncertainty of the training examples are the central issue in DTC, and so we aim to make it less complex while retaining high performance. DT performance is mainly based on determining its size ( Breiman et al., 1984 ). It has been proved that the tree size grows with the number of training data observations ( Oates, 1997 ). 4. The IUDT approach
This section describes the idea of replacing the post-pruning phase by attribute selection and dataset reduction. The main objective is to illustrate that the performance of an unpruned DT can be improved using these two steps. Wrapper attribute selec-tion can eliminate irrelevant and redundant attributes, and data reduction reduces the size complexity problem. We must therefore determine both the best attribute subset and the subset of training examples used to build the best unpruned decision tree t n features of the proposed method are as follows: (i) data prepro-cessing, (ii) de fi nition of attribute combinations as a level-wise research space (specialization graph), and (iii) application of an oriented breadth-fi rst exploration in the research space to best unpruned decision tree t n . Fig. 5 presents a schematic of the proposed method. 4.1. Data preprocessing
Experimental analyses on DT pruning methods give the follow-ing split of training and test data: 25 instances of 70% training and 40% test data ( Mingers, 1989 ), and 10 instances of 25% training and 75% test data ( Bradford et al., 1998 ). Multiple instances were used to avoid the assumption that  X  a single random split of data may give unrepresentative results  X  . To obtain representative results in the experimentation stage, and to reduce the size of the target robust unpruned DT, the dataset was randomly split into a 50% training set and a 50% test set fi ve times. Each training set was further split at random into three sub-training sets containing 50% of the whole training set. This results in 15 training sets of 25% of the dataset, as in Bradford et al. (1998) , but the test set remains at 50%. Note that for each of the fi ve test sets, there are three sub-training sets. Fig. 6 illustrates the splitting process, where i varies from 1 to 5.
 estimation prediction error. The principal justi fi cation is that the chosen process overlaps the test set because of the random sampling. 4.2. Encoding where X is the attribute subset of the attribute set E and i is a , and the problem must be represented as a research graph. The research graph is an ordered set P of the couples set R according to the binary relation D . The use of the proposed data preprocessing step will increase the number of subsets traversed by the wrapper algorithm, i.e. 15 2 m . The set of couples R is de fi ned as R  X f X  X ; i  X j X D E ; i A I g X  1  X 
Example 2. For a dataset that contains two attributes f a couples set is represented as follows:
R  X f X  a ; 1  X  ; ... ;  X  a ; 15  X  ;  X  b ; 1  X  ; ... ;  X  b ; Fig. 7 illustrates the research graph P  X  R ; D  X  .

Formally, the task of fi nding t n (the best unpruned DT) can be described as follows. Consider a database D , and a language LT for expressing R elements (couples ( X , i )) which are used to build DTs.
Let a set I of 15 sub-training sets, and a set S of fi ve testing sets respectively, and, as well as an objective function F . The main objective function evaluates each constructed tree with i using the other learning sub-sets 8 a A S \ i and the b -test set. This is calcu-the size of X . The function w : I - X  1 .. 5 is given by w  X  c  X  X  4.3. Research space exploration
A breadth-fi rst search is adopted to traverse the graph. The proposed search method has similar characteristics to apriori-based frequent itemsets mining algorithms ( Agrawal et al., 1994 ).
The search for the best unpruned tree t n starts with the empty subset | . Exceptionally, the size increases by two as we go from to couples containing one attribute and one sub-training index i .
The search process then proceeds in a bottom-up manner. At each iteration, the size of the newly discovered subsets  X  X ; increases by one with respect to the incremental relevance property ( IRP ), which is a predicate IRP : L k  X  1 -f 0 intermediate candidates L k  X  1 are generated by joining two similar but slightly different (by one attribute) subsets that have already been discovered, C k . The new candidates C k  X  1 are the L satisfy the proportional relevance property ( PRP ), which is a alternating between candidate generation and evaluation phases, until there are no new candidates in C k  X  1  X  C k  X  1  X  | explored couple  X  X ; i  X  A R , a DT is built using only the subset of examples i that is divided over the attributes subset X , i.e. a particular permutation of the attributes in X . As illustrated earlier, the attributes sequence is ordered based on the split criteria (e.g.
Gini index, impurity-based criteria, twoing criterion). At this stage, the proposed approach features two opportunities: a personalized DT model de fi nition and a prede fi ned model (e.g. BFTree, J48, REPTree, SimpleCart), which is subject to the constraint of avoid-ing the pruning phase. Every constructed DT is evaluated accord-ing to formula (3) . Algorithm 1 gives the pseudo-code of the proposed traversal method.
 Algorithm 1. j The best unpruned DT search function.
 Input: A dataset D , a language LT , predicates IRP and PRP .
Output: The optimal DT t n  X  X ; i  X  . 1: L 1  X f X  e ; i  X j e A E ; i A I g 2: C 1  X f X  e ; i  X j X  e ; i  X  A L 1 ; IRP  X  t  X  e ; i  X  X  and PRP  X  t  X  e 3: t n  X  Best of  X  C 1  X  4: i  X  1 5: while C i is not empty do 6: L i  X  1  X f X  Y ; i  X j8 X  X ; i  X  A C i ; 8 e A E ; Y  X  X [ e g 7: C i  X  1  X f X  X ; i  X j X  X ; i  X  A L i  X  1 ; IRP  X  t  X  X ; 8: t 0  X  Best of  X  C i  X  1  X  9: if Accuracy  X  t n  X  o Accuracy  X  t 0  X  then 10: t n  X  t 0 11: end if 12: i  X  i  X  1 13: end while 14: return t n
Until recently, every wrapper algorithm that considered more than 30 attributes was computationally expensive. There are many methods of speeding up the traversal process. The principal concepts are based on minimizing the evaluation subsets ( Hall and Holmes, 2003; Bermejo et al.; Ruiz et al., 2006 ), or using a randomized search ( Stracuzzi and Utgoff, 2004; Huerta et al., 2006 ). To tackle this problem, the incremental relevance and a proportional of 5% of the best subset's accuracy properties are adopted. The accuracy of the DT constructed using couple ( X , i )is calculated as follows: Accuracy  X  t  X  X ; i  X  X  X  average  X  I
Fig. 8 shows an instance of IRP and PRP. IRP aims to eliminate the generated candidates L k  X  1 by the addition of the gray and attribute subsets that have an accuracy that is greater than that of the best of the intermediate candidates  X  L k  X  1  X  minus 5% (e.g. on the right of Fig. 8 , the black candidate is the best, white candidates are eliminated, and only the gray ones are kept). 5. Validation of IUDT
The proposed method is implemented and tested on 10 standard machine learning datasets, which were extracted from the UCI collection ( Blake and Merz, 1998 ). The code is implemen-ted in Java using the WEKA framework ( Hall et al., 2009 ) and
GUAVA Google library ( Bourrillion et al., 2010 ). Experiments were conducted and compared with the results from the original WEKA pruned DTs, i.e. DTP ( D ecision T ree construction technique with use of P runing phase), an  X  attribute selection  X  algorithm imple-mentation that behaves like a wrapper algorithm, i.e. IUDTAS ( I mproved U npruned D ecision T ree construction using only
A ttribute S election), and a second algorithm that considers only the sampling step, i.e. IUDTSD ( I mproved U npruned D ecision T ree construction using only S ampling D ata).

The datasets are described in Table 1 . The  X  % Base error column refers to the percentage error obtained if the most frequent class is always predicted.

In addition, the selected datasets represent various applica-tions. Three DTs, namely J48, SimpleCart, and REPTree, were chosen to apply different pruning methods within their standard
WEKA implementation. The main DT characteristics considered in the experiments are reported in Table 2 .

The experiments are based on the DTs reported in Table 2 , implemented in their standard settings but without the pruning phase (unpruned DTs).

Tables 3, 4 and 5 list the classi fi cation accuracy for J48, REPTree, and SimpleCart, respectively, when the IUDT algorithm is applied to the 10 experimental datasets. The tables show that the number of attributes used to construct the DT is less than the number of data attributes, and only about one-sixth are used in the case of large attribute sets. The accuracy results show that the DTs outperform the %-based accuracy when only the most frequent class is continually predicted.
 Table 6 shows WEKA's DTs built using the pruning phase ( DTP ).
The design of these trees is mostly based on the construction (growing and pruning) phase, where 50% of the examples (instances) are used as the training set.

As in the proposed approach, each dataset was randomly split into training and test sets fi ve times (50% split). The results reported in Table 6 represent the mean prediction accuracy over the fi ve i -test datasets, which gives a comparison of accuracy and size between the proposed approach and the standard implemen-tation DTP , i.e. using the pruning phase.
 Table 6 shows that results differ from one model to another. Generally, the J48 trees are larger than the REPTree and SimpleCart
DTs. This phenomenon is due to the pruning technique applied. In contrast to the Reduced-error pruning (REPTree) and Error Complex-ity Pruning (SimpleCart) techniques, the EBP technique employed by J48 (and discussed in Section 2 ) grafts one of the sub-trees of a sub-tree X that was selected to be pruned. Furthermore, the accuracy of the J48 DTs is the best for six datasets. Clearly, these results provide a picture of the over-pruning and under-pruning encountered in some databases. Over-pruning can be observed in the case of the Zoo and Breast-cancer datasets using REPTree and SimpleCart. An under-pruning case is illustrated in the case of the
Ecoli dataset, where the accuracy of the J48 and REPTree models is the same when the size of J48 is much larger. Consequently, we can conclude that J48 is an under-pruned DT.
 against an algorithm that only applies a sampling process without attribute selection, we generated DTs using the 15 sub-learning sets of examples i  X  j train ( i A  X  1 ... 5 and j A  X  1 ... reported in Table 7 give the mean prediction accuracy over the i -test datasets. We can observe that, although the sizes of the trees are greater than the pruned DTs, their accuracy is much better. The accuracy of algorithms that only apply attribute selection without data sampling is reported in Tables 8  X  10 . These results were obtained by implementing a wrapper algorithm that traverses the research graph space, as in the proposed method, using a random 50% subset (instances) as learning data. The best DTs validated by the same preprocessing output used in the proposed approach are selected (the fi ve i -test datasets).

Tables 11  X  13 compare the results from IUDT based on a combination of sampling and attribute selection with the original
WEKA pruned DT construction ( Table 11 ), an approach that only applies data sampling, i.e. IUDTSD ( Table 12 ), and an approach that only applies attribute selection i.e. IUDTAS ( Table 13 ). The symbol signi fi es that the proposed approach produces better results than the comparative method. Otherwise, we use the symbol  X  X  X  . It is important to note that the over-pruned DTs of size one (i.e. Zoo and Breast-cancer) are always considered to be worse, and the accuracy is considered to be equal if the difference between the IUDT and the comparative approach (i.e. DTP ; IUDTSD ,or IUDTAS ) is in the interval  X  1 ;  X  1 % . Otherwise, the tree with the greatest accuracy is considered to be better.
The comparison in Table 11 clearly indicates that the proposed approach is generally more accurate than DTP (in the case of
REPTree and SimpleCart). In the case of J48, each approach performs better on four datasets. The pruned standard DTs are smaller in REPTree and SimpleCart, but J48 gives the smallest DTs for
Table 12 compares the results of IUDT with those from the application of data sampling IUDTSD . It is clear that the accuracy of
IUDT is much better than that of IUDTSD ,butincontrasttothe accuracy results, the size results show that the sampling approach has some advantages. In general, the table indicates that the accuracy results of IUDT are better when using the REPTree and SimpleCart models, but the DTs constructed are larger than the sampling approachresults.InthecaseofJ48,wehaveequalitybetweenthe accuracy and size results. This shows that using data sampling certainly provides a less complex DT, albeit at the cost of robustness.
Table 13 compares the results of IUDT with IUDTAS , in which only attribute selection is applied. Clearly, the accuracy and size results from the proposed approach are better. In general, the table indicates that IUDT gives better accuracy and size results with the
REPTree and SimpleCart models, except in the case of J48 DT sizes, which are larger than those of the attribute selection approach.
These results show that using a combination of attribute selection and data sampling provides a robust and less complex DT. 6. Application to fault diagnosis in a rotating machine
We now consider the application of the proposed method to fault diagnosis in rotating machines. Some of the main faults affecting the proper functioning of such machines were produced experimentally on a test rig. The condition-monitoring task can be converted to a classi fi cation task, where each condition (good and defective) is considered as a class. The target is to extract informa-tion from vibration sensors to indicate the machine's current condition (class). The approaches investigated in this paper are then used to seek the non-complex construction of effective decision rules, following the schematic shown in Fig. 9 . 6.1. Experimental study gears (one with 60 teeth and the other with 48 teeth), six bearing housings, a coupling, and a toothed belt. The system is driven by a
DC variable-speed electric motor with a rotational speed ranging from 0 to 1500 rpm.
 test rig. The vibration signals were acquired using an acceler-ometer fi xed on the bearing housing, connected to a data acquisition system equipped with OROS software. Vibration sig-natures were recorded under three different rotational speeds (300, 900, and 1500 rpm) under a normal operating condition and with three different faults: mass imbalance, gear fault, and faulty belt. 6.2. Signal processing
The wavelet transform has been widely studied over the past two decades, and its use has seen a signi fi cant growth and interest in vibration analysis. The formulation of its discrete variant ( DWT ), which requires less computation time than the continuous form, is shown in the following equation:
DWT  X  j ; k  X  X  1 ffiffiffiffi
Mallat (1989) introduced an effective use of the discrete wavelet transform by applying a succession of fi lters on several levels. The resulting signals are called approximation coef fi cients and detail coef fi cients. To overcome the down-sampling encountered throughout the decomposition, the coef fi cients are subjected to reconstruction fi lters to create new signals called approximations (A) and details (D).
Fig. 11 illustrates the principle of DWT decomposition. In the present study, Daubechies wavelets with two levels of decomposition were used to extract the approximations and details of the original signals. The frequency space was transformed by applying a Fast Fourier
Transform (FFT) to each original signal, as well as to each of the example of mass imbalance under a rotational speed of 900 rpm. 6.3. Extraction of Indicators
Thirty-fi ve original signals were recorded under four different operating conditions (classes) and t hree different rotational speeds, skewness, and variance were extracted from the temporal forms of these signals.

From the frequency spectra, we derived the maximum ampli-tude, its frequency, the frequency of the second highest amplitude, the interval between the two highest amplitude frequencies, the mean interval between the four highest amplitude frequencies, and the root mean square. These 11 indicators were also extracted from each of the four signals from the DWT decomposition application, giving a total of 55 indicators from the original signal. 6.4. Results and discussion The experimental results using three DT algorithms are given in Table 14 . These results indicate a global superiority in terms of classi fi cation accuracy. Slightly less complex trees are constructed with the IUDTSD approach, but these exhibit much lower perfor-mance. IUDTSD can also be seen to give an over-generalization, as explored in Section 3.2 .

The best results were obtained using REPTree, and the decision rules produced by the experimental algorithms are given in the appendix. In terms of the IUDT output, REPTree gave the best classi fi cation accuracy. Table 15 lists the selected indicators used for its construction in appearance order in the tree (AOT), showing the signal from which each indicator originated.

From Table 15 , we can see that only the indicators extracted from the original signal and the fi rst-level decomposition were used to construct the best tree, where four decomposition levels were done. By exclusively extracting the retained indicators in the
DTC, a signi fi cant saving in both storage memory and computation time can be achieved, particularly in diagnostic and maintenance tasks that require the archiving of data over long periods. 7. Conclusion
The popularity of DTs is strongly related to their simplicity, ease of understanding, and close resemblance to human reasoning.
However, each DT model has its own speci fi c advantages and
The model choice depends strongly on the performance of the pruning approach, which forces users to select a model according to their requirements. This implies that DT users must study all growing and pruning techniques so as to choose the most appro-priate model, which is a dif fi cult task.

When constructing DTs, the pruning phase is useful for redu-cing the model's complexity and size, but often imparts a penalty on the accuracy, particularly for small datasets. In this paper, we proposed an improved algorithm without the pruning phase that combines the attribute selection and data sampling processes. proposed approach improves the accuracy of DTs and effectively reduces their size, avoiding the problems of over-or under-pruning. application of fault diagnosis in rotating equipment. This demon-complexity. Moreover, extracting only indicators selected by the proposed approach allows a signi fi cant gain in storage memory and computation time.

We conclude that the proposed method is feasible, especially in the case of small datasets, and is an effective approach when exhaustive knowledge on data characteristics is missing. Acknowledgments In memoriam, we would like to gratefully acknowledge Prof.
Lhouari Nourine and Prof. Engelbert Mephu Nguifo for their kind advice on this research.
 Appendix A
REPTree IUDTAS  X  X  X  X  X  X  X  X  X  X  X  X  X  X  sMeanFFT-A2s o 0 j sRMS-D2s o 1.28 : 2.000000 (15/0) [0/0] j sRMS-D2s 4  X  1 : 28 jj sRMS-D2s o 3.94 jjj sMeanFFT-A2s o 0 jjjj sRMS-D2s o 1.41 jjjjj sMeanFFT-A2s o 0 jjjjjj sMeanFFT-A2s o 0 : 2.000000 (5/1) [0/0] jjjjjj sMeanFFT-A2s 4  X  0 : 1 : 000000 (4/0) [0/0] jjjjj sMeanFFT-A2s 4  X  0 : 7 : 000000 (8/0) [0/0] jjjj sRMS-D2s 4  X  1 : 41 jjjjj sMeanFFT-A2s o 0 : 1.000000 (21/0) [0/0] jjjjj sMeanFFT-A2s 4  X  0 jjjjjj sMean-Freqdsit-A1s o 10.83 : 7.0000 (2/0) [0/0] jjjjjj sMean-Freqdsit-A1s 4  X  10 : 83 : 1 : 000 (8/0) [0/0] jjj sMeanFFT-A2s 4  X  0 : 7 : 000000 (24/1) [0/0] jj sRMS-D2s 4  X  3 : 94 jjj sMean-Freqdsit-A1s o 10.17 : 1.000000 (19/0) [0/0] jjj sMean-Freqdsit-A1s 4  X  10 : 17 : 2 : 000000 (14/1) [0/0] sMeanFFT-A2s 4  X  0 j sMeanFFT-A2s o 0.01 jj sCF-D1s o 0.14 jjj sRMS-D2s o 15.12 : 7.000000 (17/0) [0/0] jjj sRMS-D2s 4  X  15 : 12 : 3 : 000000 (2/0) [0/0] jj sCF-D1s 4  X  0 : 14 jjj sCF-D1s o 0.17 : 3.000000 (10/1) [0/0] jjj sCF-D1s 4  X  0 : 17 : 3 : 000000 (39/0) [0/0] j sMeanFFT-A2s 4  X  0 : 01 : 2 : 000000 (22/0) [0/0]
REPTree IUDT  X  X  X  X  X  X  X  X  X  X  X  X  sSkew-Sigs o 0.23 j sMeanFFT-Sigs o 0.01 jj sMeanFFT-Sigs o 0 jjj sRMS-A1s o 2.18 : 2.000000 (9/0) [0/0] jjj sRMS-A1s 4  X  2 : 18 jjjj sRMS-A1s o 8.46 jjjjj sMeanFFT-Sigs o 0 jjjjjj sMeanFFT-Sigs o 0 : 1.000000 (9/0) [0/0] jjjjjj sMeanFFT-Sigs 4  X  0 jjjjjjj sMean-Freqdsit-Sigs o 11.83 : 7.00000 (7/0) [0/0] jjjjjjj sMean-Freqdsit-Sigs 4  X  11 : 83 : 1 : 0000 (8/0) [0/0] jjjjj sMeanFFT-Sigs 4  X  0 : 7 : 000000 (10/0) [0/0] jjjj sRMS-A1s 4  X  8 : 46 jjjjj sMean-Freqdsit-Sigs o 9.67 : 1.000000 (8/0) [0/0] jjjjj sMean-Freqdsit-Sigs 4  X  9 : 67 : 2 : 000000 (6/0) [0/0] jj sMeanFFT-Sigs 4  X  0 jjj sCF-D1s o 0.15 : 7.000000 (11/0) [0/0] jjj sCF-D1s 4  X  0 : 15 : 3 : 000000 (5/1) [0/0] j sMeanFFT-Sigs 4  X  0 : 01 : 2 : 000000 (13/0) [0/0] sSkew-Sigs 4  X  0 : 23 : 3 : 000000 (19/0) [0/0] References
