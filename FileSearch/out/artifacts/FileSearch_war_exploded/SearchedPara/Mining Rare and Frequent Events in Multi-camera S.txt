 This paper describes a method fo r unsupervised classification of events in m ulti-cam era indoors surv eillance video. This research is a part of the Multiple Sensor Indoor Surveillance (MSIS) project which uses 32 AXIS-2100 webcams that observe an office environm ent. The res earch was ins pired by the following practical problem : how autom atically clas sify and vis ualize a 24 hour long video captured by 32 cameras? Raw data are sequences of JPEG im ages captured by webcam s at the rate 2-6 Hz. The following features are extracted from the im age data: foreground pixels X  spatial distribution and color histogram. The data are integrated by event by averaging motion and color features and creating a  X  sum mary X  fram e which accum ulates all foreground pixels of frames of the event into one image. The self-organizing map (SOM) approach is applied to event data for clustering and visualization. One-level and two-level SOM clustering are used. A tool for browsing results allows exploring units of the SOM maps at different levels of hierarchy , clusters of units and dis tances between units in 3D s pace. A s pecial technique has been developed to visualize rare events. The results are presented and discussed. I.2.6 [ Artificial Intelligence ]: Learning: I.5.2 [ Pattern Recognition ]: Design Methodology -classifier design and evaluation : I.5.5 [ Pattern Recognition ]: Implementation -special architectur es . Algorithms, Experimentation. Indoor surveillance, rare event de tection, unsupervised learning, self-organizing maps, visualization. The proliferation of video cam eras in public places and bus ines s facilities, such as airports, str eets, parking lots, shopping m alls, hospitals, hotels and governmental buildings can create many opportunities for business and pub lic safety applications, including surveillance for threat detection, m onitoring parking movement, detecting unusual even ts in a hospital, monitoring elderly people at hom e, etc. Thes e applications require the ability autom atically detecting and classify ing events by analy zing video or imagery data. While video surveillance has been in use for decades, the developm ent of s ystem s that can autom atically detect and classify events is still the active research area. A significant num ber of papers have been publis hed in recent y ears . Most of them deal with developing s pecific clas sifiers that allow recognizing objects such as people and vehicles and tracking them [1-3] or recognizing relationships between objects (for example, a person standing at a vending machine) and actions (for example, a person picks up a cup) [4] . The others propose general approaches for event iden tification using clustering. In [5] the authors segm ent raw surveillance video into sequences of frames that have motion, c ount the proportion of foreground pixels for segm ents of various lengths, and use a m ulti-level hierarchical clustering to group the segments. The authors also propose an abnorm ality m easure for a segm ent that is a relative difference between average dis tance for elem ents of the clus ter and average distance from the sequence to its nearest neighbors. The weaknesses of the approach are (1) segments of higher motion often are subsequences of segm ents of lower average motion and when they are clus tered the subsequences of the same event belong to different clusters; (2) location and direction of movement of the objects are not taken into account; and (3) the other features , s uch as color, texture and shape, which could be useful for distinguishing events, are not taken into account. The authors of [6] describe an approach that uses a 2D foreground pixels X  histogram and color histogram as features for each fram e. The features are m apped into 500 feature prototy pes using the vector quantization technique. A surveillance video is represented by a number of shor t (4 seconds) overlapping video segments. The relationship am ong video segments and their features and among features them selves is represented by a graph which edges connect the segm ents to features and features to features . The weights on the edges reflect how many times each feature occurred in each video segment, and similarity among features. To visualize the graph it is embedded in a 3D space using the spectral graph method. To categorize the video segments the authors use the k-means clustering on the video segments X  projections. The larger clusters are defined as usual events, but small and more isolated clusters as unusual events. A new video segment can be classified by embedding it into common space and applying k -nearest neighbor classifier. The advantages of this approach are (1) taking into account similarity among features, and (2) attractive visualization of results. The disadvantages are (1) high computational complexity of the graph embedding method, and (2) dependence of the results on the length of video segments. Developing techniques for visuali zation of huge amount of data always attracted attention of data mining researchers. Visualization is an indispensabl e part of the data exploration process. Finding efficient algor ithms for event detection and summarization, skimming and br owsing large video and audio databases are the major topics of multimedia data mining [7,8]. Many visualization technique s have been developed in traditional data mining. They use clustering that follows by a mapping into 2D or 3D space. For example, using the principal component analysis the data ma pped into principal component space and then visualized using two or three first components. Another well-known and widely used approach is Self-Organizing Maps (SOM) or Kohonen Neural Networks [9]. This approach has been applied for analysis and visualization of variety of economical, financial, scientific, and manufacturing data sets [10]. From the view point of our research the most interesting application is the Pi cSOM, which is a content-based interactive image retrieval system [11]. It clusters images using separately color, texture and shape features. A user chooses what kind of features he would like to use and picks up a set of images that are similar to his query. The system uses SOM maps to select new images and presents them back to the user. The feature SOM maps highlight the areas on the map that correspond to the features of the set of currently selected images. The interaction continues until the user reaches his goal. Our research described below is devoted to creating a method for unsupervised classification of events in multi-camera indoors surveillance video and visu alization of results. It is a part of the Multiple Sensor Indoor Surveillance project that is described in the next section. Th en we describe sequentially our data collection and preprocessing procedure, one-and two-level clustering using SOM, techniques for detecting rare events, an approach to classification new events using Gaussian mixture models (GMM) that are derived from SOM map, and a tool for visualization and browsing resu lts. Finally, we summarize the results and speculate on the future work. This research is a part of the Multiple Sensor Indoor Surveillance (MSIS) project. The backbone of the project consists of 32 AXIS-2100 webcam s, two PTZ cameras with infrared mode, fingerprint reader and infrared badge system that are sensing an office floor for Accenture Technology Labs. The webcams and infrared badge syst em cover two entrances, seven laboratories and demonstration r ooms, two meeting rooms, four major hallways, four open-space cube areas and two discussion areas. Some areas overlap with up to four cameras. The total area covered is about 18,000 sq. ft. (1,670 sq. m). The fingerprint reader is installed at the entrance and allows matching an employee with his/her visual representation. The backbone architecture also includes several computers, with each computer receiving signals from 3-4 webcams, detecting  X  X vents X  and recording the images for that event in JPEG format. The event is defined as any movement in the camera X  X  field of view. The signal sampli ng frequency is not stable and on average is about 3 frames pe r second. Another computer collects events X  pictures, convert s them into MPEG-1 movie and creates an event record in an SQL database. Events detected by the infrared badge system go to the other database. The event databases serves as a common repository for both people who are doing manual search of events and automatic analysis. The objectives of the MSIS project are to: The following analyses and prototypes have been developed or are currently under development: The described below research was inspired by the following practical problem: how automatically classify and visualize a 24 hour video captured by 32 cameras? First we implemented a Web ba sed tool that allows users searching and browsing the Event Repository by specifying the time interval and a set of cameras of interest. The tool X  X  output consists of a sequence of events sorted by time or by camera and time. Each event is represented by a key frame and has links to the event X  X  MPEG-1 movie and sequence of frames. Using the tool a user can quickly sort out events for a tim e interval that is as short as 1-2 hours, but can be overloaded with a large number of events that occurred during 24 hours , which counts from 300 to 800 events per camera. The tool does not give the user a  X  X ig picture X  and is useles s for s earching for rare events . Thes e reas ons m otivated our res earch for uns upervis ed clas sification of events . Our raw data are JPEG images of size 640 by 480 pixels that are captured by AXIS-2100 webcams at the rate 2 -6 Hz. Each image has a time stam p in seconds passed from the midnight of the day under consideration. The background subtraction algorithm is applied to each im age to extract foreground pixels. We use two approaches for b ackground modeling -an adaptive single fram e selection and es tim ating m edian value for each pixel using a pool of recent im ages. After subtracting the background, morphological operati ons are applied to remove nois e. Then the following features are extracted from the im age: motion features that characterize the foreground pixels X  distribution (64 values), and color histogram of the foreground pixels in RGB color space (3*8=24 values). The foreground pixels X  distribution is calculated on 8 by 8 grid, and the value for each cell of the grid is the num ber of foreground pixels in the cell divided by the cell X  X  area. Then thes e data are integrated up to one second in our case. The notion of tick and its value is im portant because we deal with multiple non-sy nchronized cameras with overlapping fields of view. Tick allows loos ely  X  up to the tick  X  sy nchronizing cameras X  data. It also allows regularizing fram e tim e series taken with vary ing sampling rate. The data integration by tick and event consists of averaging motion and color data. For visual representation of a tick or an event a  X  X um mary X  fram e is created. It accum ulates all foreground pixels of all images from the tick/event into one im age. The s ummary fram es serve as key frames for representing ticks/events. Figure 1 gives an example of a summary fram e of an event. Before presenting details of our approach for es tim ating an event boundaries let us consider what kind of events we can expect to find in an indoor o ffice environment. A camera can watch a hallway , a m eeting room , a working s pace s uch as cubicles or laboratories , a recreational area such as a coffee room , or a m ulti-purpose area that is used differently at different tim e of the day . Let us assume that we are using a percentage of foreground pixels in the im age F as an integral m eas ure of motion. If cam era watches a hallway then most events pres ent people walking along the hallway , getting in and out of offices, or standing and talking to each other. Most events last for seconds and some for several minutes. In this case the plot of F in tim e looks as number of peaks that re present short events and some trapezoidal  X  X umps X  that corre spond to longer events. Some bumps can have peaks that correspond to combinations of transient and long-term events . If a camera watches a meeting room then we have long periods of time when the room is empty ( F =0) that interchange with periods when a m eeting is in proces s. The latter has a trapezoidal F -plot with a long base and som e volatility that corresponds to people m ovem ent during the meeting and some small peaks th at correspond to events when participants arriving and leav ing the room . In cas e of a recreational area camera the events ty pically longer than for a hallway but m uch s horter than for a meeting room cam era. They correspond to events when people getting into the area for drinking coffee, having lunch, read ing or talking to each other. If a cam era watches a bus y working area s uch as cubicles then we have such events as people arriving and leaving their working places, sitting down and standing up, m oving and com municating with each other. The F -plot for s uch cam era never equal to zero at working hours and looks like a long volatile meeting. Having watched F -plots for several cameras we cam e to the conclus ion that, firs t, F -m eas ure can be us ed for event boundaries estimation, and, second, the notion of event is a relative one. F or exam ple, on the one hand, the whole meeting can be considered as an event, but, on the other hand, it may be cons idered as a s equence of events , such as , people arriving for the m eeting, participating in the m eeting, and leaving the room . Each of thes e events can be divi ded into shorter events down to a noticeable m ovem ent. Thes e obs ervations encouraged us to use the wavelet decomposition of F -signal to detect events at different levels. We used the wavelet decomposition of levels from 2 to 5 with Haar wavelet for calculating approximation and details. Then we applied a threshold to approximation signal that slightly exceeds the noise level to find the boundaries of major events s uch as m eetings (m ega-events ), and another threshold to the absolute value of highest detail signal to find internal events (micro-events) . The boundaries of micro events are used for data integration. After integrating data by tick and by event we have two sets of data for each cam era. The tick-level data s et record cons ists of the following elements: tick value from the beginning of the day , names of the first and last frames of the tick, and integrated motion and color data. The event-le vel data set record consists of a unique event identification number, first and last tick values, names of the first and las t fram es of the event, and integrated motion and color data. We used both of these sets for unsupervised classification and vi sualization presented below. We applied the self-organizing map (SOM) approach to tick/event data for clustering a nd visualization. We use 2D rectangular maps with hexa gonal elements and Gaussian neighborhood kernels. In one-level clustering approach we use both motion and color data to build the map. For creating maps we used the SOM Toolbox for MATLAB developed at the Helsinki University of Technology [12] . The toolbox allows display ing maps in many way s vary ing the units X  sizes and colors. The size of maps for tick data can reach 1400 units (70 by 20) and for event data  X  about 200 units (25 by 8). Figure 2 presents the map for event data of a cam era that observes a m ulti-purpose area. It presents 717 events on 22 by 6 lattice (132 units). The unit X  X  size reflects the num ber of events attracted by this unit (the number of hits ). The unit X  X  size is calculated us ing the form ula (1). where s(u) is the size of unit u , and hits( u) is the number of data points attracted by the unit u . It means that if a unit has at leas t one hit then its size is not less than 0.5, and the size of the unit is proportional to the number of hits . The units with zero hits are not display ed. The units X  colors show the topological similarity of the prototy pe vectors. A visualization tool that is desc ribed below allows exploring the contents of each unit. However, the num ber of units can be large that makes unit browsing very labor ious. Next step is to apply the k-means algorithm for clustering map units (prototy pe vectors) [13] . As the number of units is about one order smaller than the number of raw data, we can run the k-means algorithm with different number of intende d clusters, sort the results accordingly their Davies -Bouldin indexes [14] , and allow users browsing clusters of units. The Davies-Bouldin index of a formula (2). the within-clus ter dis tance of the clus ter C, which has elem ents centroids . (between-clus ter dis tance). We do k-means clustering of units for the number of clusters from 2 to 15 for event data and from 2 to 20 for tick data. Then the visualization tool allows the user to pick up a clustering from a m enu where each clus tering is shown with its Davies -Bouldin index. Figure 3 presents the clustered map for the event data of the s ame cam era and a legend that shows the number of events in each clus ter. The centroid unit of each clus ter is pres ented in com plem entary color. W hen the num ber of clus ters is large som e clusters m ay consis ts of several transitional units that have no raw data (events or ticks ) as sociated with them . The visualization tool allows browsing events or ticks by cluster. Our raw data have two kinds of features: motion and color features . In two-level clus tering we explore thes e features consequentially . On the top level only motion features are used to create the m ain m ap. Then we us e k-means clus tering for the map units as we described above. After this, for each obtained cluster we build a SOM map using color features. Such separation of features allows differentiating more precisely spatial events and easier det ecting unusual events. In indoor environment, where most of moving objects are people, who change their clothes every day , the variance of color features is higher than the variance of motion features. Separating motion features allows collecting them over longer periods and creating more robust classifiers. To create a classifier we accum ulated motion data during a week, built a SOM m ap and applied k-means clustering to its units. Each cluster is a set of weighted Gaus sian kernels , which can be cons idered as a Gaus sian Mixture Model (GM M). A new piece of data can be clas sified by calculating probability using GMM associated with each cluster and assigning the new data to the cluster with m axim al probability . The sam e procedure can be applied to color features. Combining motion based classifiers on top level with color based classifiers on the second level we obtain a hierarchical clas sifier. In some cases finding unusual events is of special interest. But what we should count as an unusual event is often uncertain and requires additional considerati on. An event can be unusual because it happened at unusual tim e or at unusual place or had unusual appearance. For example, finding a person working in his office at midnight is an unusual event, but the same event happened at noon is not. A person standing on a table would be considered as an unusual event in most office environments. Many people wearing clothes of the same color would be considered as an unusual event unless every body is wearing a uniform. Every body agrees that an unusual event is a rare event at given tim e and s pace point. But a rare event m ay not be an unusual one. For exam ple, a person sitting in his office on weekend could be a rare but not surprising event. After thoughtful deliberation we decided to use computers for finding rare and frequent events leaving humans to decide how unusual or usual they are. In our research we distinguish between local rare/frequent events  X  thes e are events that happened during one day  X  and gl obal rare/frequent events  X  those that happened during longer period of time and the surveillance system accum ulated data about these events. W e also distinguish between events that happened during regular working hours and out of them. For visualizing local rare events we use a 3D surface that shows distances between units of the SOM map and indicates how many data points (hits) belong to each unit using markers which size is proportional to the number of hits. Figure 4 shows the 3D visualization for event data. A user can rotate the axes searching for  X  X ighlanders X   X  small unit markers that are located on the top of peaks or for  X  X solated villages X  which are sets of small unit markers that are located in closed mountain valleys. For detecting global rare events the following procedure is proposed. First, the GMM classifier is applied to a new event/tick motion data. If it gives a high probability for a small cluster or for a cluster with zer o data points (as it was discussed above) then the system declares th at a rare event of particular type is found. If the classifier gets low probabilities for all clusters then the system indicates that it is a new (and rare) spatial event. Such events are accumulated and can be used for building a new version of GMM classifier. In case when the event belongs to a moderate or frequent event cluster, the system applies the corresponding color-based GMM classifier to detect rare or new events regarding their color features. The described above techniques have been integrated into an event visualization tool. Figures 2 and 3 show snapshots of the tool. The tool X  X  GUI consists of four panels  X  Data , Map , Image Browser and Unit/Cluster Summary . Using the Data panel the user selects and loads data. Currently each camera has a separate file for its tick and event data. The file contains preprocessed raw data and SOM data for chosen SOM architecture  X  one or two-level SOM map. The user selects the desired map from a menu. The map is displayed in the Map panel or in a separate window. The Map panel displays the current SOM map, which could be the top level map that shows co lor coded units with unit sizes reflecting the number of hits, or a cluster map of different number of clusters with color coded units. The 3D surface that presents distances between units and the number of hits in each unit is displayed in a separate window (see Figure 4). The user can rotate the 3D surface for hunting for local rare events. When the user clicks on a unit or a cluster of units on the SOM map the contents of the unit or cluster is displayed in the Unit/Cluster Summary panel. This panel presents information about the current item (unit or cluster). It shows the number of events and/or ticks in the current item, the name of image displayed in the Image Browser panel and its time. It also has two plots. The left plot shows the distribution of event or tick data in time. The right plot shows all data (ticks or events) that related to the current item. A small square corresponds to each piece of data. The color of the square indicates the time interval that the piece of data belongs to. When the user clicks on a square the corresponding summary frame is displayed in the Image Browser panel. The Image Browser panel displays the visual information related to the current selected tick or event in the Unit/Cluster Summary panel. Using the browser X  X  control buttons the user can watch the first or last frame of the tick/event, go through the tick/event frame by frame forward and backward, and watch a slide show going in both directions . The speed of the slide show is controlled by the speed slid er. Clicking on the image brings the current frame in full size into a separate window allowing the user to see details . This feature proved to be very useful for exploring busy summary images. We described an approach to unsupervised classification and visualization of surveillance data captured by multiple cameras. The approach is based on self-o rganizing maps and enables us to efficiently search for rare and frequent events. It also allows us creating robust classifiers to identify incoming events in real time. A pilot experiment with several volunteers who used the visualization tool for browsing events and searching for rare events showed both its high efficiency and positive feedback about its GUI. Although we applied this approach for indoor surveillance in office environment, we believe that it is applicable in the larger context of creating robust and scalable systems that can classify and visualize data for any surveillance environment. The real bottleneck of the approach is not creating SOM maps (it takes just several minutes to create a map for 24 hour tick data with 86400 records), but feature extraction and data aggregation. In the future we plan to extent our approach to visualize data of a set of cameras with overlappi ng fields of view, embed the GMM-based classifier into visu alization tool for detecting global rare events and improve the graphical user interface. [1] Kanade, T., Collins, R.T., and Lipton, A.J. Advances in [2] Siebel, N.T. and Maybank, S. Fusion of Multiple Tracking [3] Krumm, J., Harris, S., Meyers, B., Brumitt, B. , Hale, M. [4] Ayers, D and Shah, M. Monitoring Human Behavior from [5] J.-H. Oh, J.-K. Lee, S. Kote, B. Bandi Multimedia Data [6] H. Zhong and J. Shi Finding (U n)Usual Events in Video, [7] Amir, A., Srinivasan, S., and Ponceleon, D. Efficient Video [8] Gong, Y. Audio and Visual Content Summarization of a [9] T. Kohonen Self-Organizing Maps . Springer-Verlag, 1997. [10] E. Oja and S. Kaski (Eds.) Kohonen Maps . Elsevier, 1999. [11] J.T. Laaks onen, J .M. Kos kela, S .P. Laaks o, and E. Oja [12] J. Vesanto, J. Himberg, E. Alhoniemi, J. Parhankangas, [13] J. Vesanto and I. Alhoniemi Cl ustering of Self-organizing [14] D.L. Davies and D. W. Bouldin A cluster separation 
