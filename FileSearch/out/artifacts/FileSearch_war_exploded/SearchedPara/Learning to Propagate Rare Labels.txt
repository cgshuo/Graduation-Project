 Label propagation is a well-explored family of methods for train-ing a semi-supervised classifier where input data points (both la-beled and unlabeled) are connected in the form of a weighted graph. For binary classification, the performance of these methods starts degrading considerably whenever input dataset exhibits following characteristics -(i) one of the class label is rare label or equivalently, class imbalance (CI) is very high , and (ii) degree of supervision (DoS) is very low  X  defined as fraction of labeled points. These characteristics are common in many real-world datasets relating to network fraud detection. Moreover, in such applications, the amount of class imbalance is not known a priori. In this paper, we have proposed and justified the use of an alternative formulation for graph label propagation under such extreme behavior of the datasets. In our formulation, objective function is the difference of two convex quadratic functions and the constraints are box constraints. We solve this program using Concave-Convex Procedure (CCCP) . Whenever the problem size becomes too large, we suggest to work with a k -NN subgraph of the given graph which can be sampled by using Locality Sensitive Hashing (LSH) technique. We have also discussed various issues that one typically faces while sampling such a k -NN subgraph in practice. Further, we have proposed a novel label flipping method on top of the CCCP solution, which improves the result of CCCP further whenever class imbalance information is made available a priori. Our method can be easily adopted for a MapReduce platform, such as Hadoop. We have conducted experiments on 11 datasets comprising a graph size of up to 20K nodes, CI as high as 99 . 6% , and DoS as low as 0 . 5% . Our method has resulted up to 19 . 5 -times improvement in F -measure and up to 17 . 5 -times improvement in AUC-PR measure against baseline methods.  X  A part of the work was done when the author was an intern at IBM Research, New Delhi, while being a student at IIT Delhi. H.2.8 [ Database Management ]: Database Applications X  Data mining ; I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier design and evaluation Algorithms, Design, Performance, Experimentation Label Propagation; Class Imbalance; Semi-Supervised Classifica-tion; Concave-Convex Procedure
The presence of fraudulent users is inevitable in any e-Commerce, m-Commerce, and online social networking applications. For exam-ple, the presence of fraudulent users in mobile payment networks [23], fake or malicious user accounts in an online social network [10, 25], and fraudsters in online auction networks [22] are commonly occurring problems in real-life. Such users pose serious threats to these services and society at large. Detecting the fraudulent users in such networks is a daunting challenge from the perspective of both modeling as well as computational requirements. Almost any outlier detection technique, which is based on just user centric features, would cease to perform well here because it is quite easy for these users to tweak the values of the features in a way to circumvent the filter. On the other hand, the users in these applications inherently carry out transactions among themselves (e.g. money exchange, tweets, posts, etc.) resulting in a natural graph structure that is quite informative about users X  behavior and contain much less noise. Thus, it would be apt to leverage these data in a critical manner for the purpose of identifying fraudulent users. In addition, for these applications, the number of fraudulent users (positive class) is much smaller than the normal users (negative class); and the available labeled dataset is much smaller in the size than the given unlabeled dataset. To summarize, the datasets in such applications comprise of the following signature characteristics. 1. Sparse Graph Structure: For the given dataset, there is an underlying (typically explicit) sparse graph structure among the labeled and the unlabeled data points which is quite informative. 2. Rare Labels (aka Extremely High Class Imbalance): The class distribution is highly skewed. That is, positive label is a rare label, say of the order of 1% of total dataset or 99% negative label data points. For such a setting, we define N/n as the class imbal-ance (CI) where N and n denote the number of negative and total data points, respectively. The class imbalance is typically not known a priori. 3. Extremely Low Degree of Supervision: The size of the labeled dataset is quite small relative to the size of unlabeled dataset, say of the order of 0.5% of total dataset. We define the degree of supervi-sion as l/n where l and n denote the number of labeled points and the total number of points, respectively. 4. Very High Volume of Dataset: Generally a dataset contains a huge number of data points, say of the order of 100K, which are represented in the form of a graph.

Any classification problem comprising of all the above data char-acteristics naturally calls for an efficient (both in terms of memory and time) semi-supervised graph label propagation method for rare labels . Such a method, surprisingly, is not well addressed in the existing literature and is precisely the focus of our paper. In what follows, we begin with summarizing state-of-the-art on graph label propagation methods with a special attention on rare labels.
Graph label propagation (aka graph transduction) [4, 1] relates to the problem of learning a classifier where input data points (labeled and unlabeled) are connected in the form of a graph and the edge weights represent similarities between the points. The existing popular methods for label propagation include Gaussian fields and harmonic function (GFHF) based method [29], local and global consistency (LGC) method [27, 28], and graph transduction via alternating minimization (GTAM) method 1 [24]. The key idea behind these methods is to propagate the information about known labels across the whole graph in a smooth manner. This is achieved by striking a right balance between the accuracy on labeled nodes and a regularizer term that encourages smooth propagation. The underlying formulation for most of these methods is some form of a quadratic convex program .

An important point to note here is that all these methods ex-hibit a promising performance when input dataset behaves normally, whereby normal means moderate values for both class imbalance (CI) and degree of supervision (DoS). However, the performance of these methods get compromised when input dataset behave abnor-mally  X  extremely high CI and extremely low DoS. We have not only experimentally validated but also argued analytically about the com-promised performance of these methods under such circumstances in a subsequent section.

If the input data points were not connected in the form of a graph, the problem could have been formulated as either semi-supervised outlier detection [12] or supervised classification from imbalanced data [6, 16, 5, 19, 15, 11] depending on whether degree of supervi-sion is low or high. The existing literature, however, appears to be quite slim when it comes to address the problem of semi-supervised graph label propagation for rare labels .
The key contributions of this paper are summarized below. 1. We have experimentally validated and analytically argued that 2. We have proposed a novel label propagation method for the
There is another family of methods based on the theory of manifold learning [3]; but they are beyond the scope of this paper. 3. In our non-convex quadratic formulation, the objective func-4. The space and the time complexities of each iteration in gra-5. We have also addressed different issues that one might face 6. We have developed a novel and intuitive label flipping method 7. GTAM [24] is one of the recent scheme to address the prob-
Let D = { ( x 1 ,y 1 ) ,..., ( x ` ,y ` ) ,x ` +1 ,...,x ` + u tially labeled input dataset for the binary classification problem, where ` points are labeled, u points are unlabeled, and y  X  1 } for i = 1 , 2 ,...,` . Let us assume that the total number of data points is n so that we have n = ` + u . We also assume that the labeled set follows the same class distribution as the set of unlabeled samples. Let G = ( V,E ) be an undirected weighted graph representing the given dataset. This graph contains a vertex set V = { v 1 ,v 2 ,...,v n } , where each vertex v i corresponds to a unique data point x i . Further, each edge ( v i ,v j )  X  E of this graph is associated with a non-negative weight w ij  X  0 which measures the similarity between nodes v i and v j , and is computed using an weighted adjacency matrix of graph G . The degree of a vertex v  X  V is defined as d i = P n j =1 w ij . The degree matrix D is defined as a diagonal matrix with d 1 ,d 2 ,...,d n as diagonal entries. The normalized and unnormalized graph Laplacian matrices 2 defined as follows.

Given partially labeled dataset D and similarity matrix W , the goal of a typical semi-supervised label propagation method is to learn a vector f  X  R n and assign sign ( f i ) as the label for the vertex v i . As shown in [1] and references therein, most of the exist-ing methods for label propagation solve a variant of the following convex quadratic optimization problem: where,  X  0 is a constant, F  X  R n , and L is a normalized graph Laplacian. In practice, these methods work with the La-grangian of the above formulation, which is given by L ( f , X  ) = f tiplier. In this Lagrangian form, the first term is a regularizer and corresponds to the smoothness of the label propagation (aka struc-tural risk) and the second term corresponds to the empirical risk. These methods essentially try to minimize this Lagrangian subject to the constraint that f  X  F . In what follows, we show how the existing methods, namely GFHF, LGC, and GTAM, become special cases of the convex quadratic program (1). For more details, one can refer to [1] and references therein. 1. GFHF based Method [29]: It solves a variant of (1) with quan-tities ( L ,, F ) assigned to ( L u , 0 , R n ) . 2. LGC Method [27]: It solves yet another variant of (1) with quan-tities ( , F ) assigned to ( / X , R n ) . The constant  X  corresponds to the optimal Lagrange multiplier. Additionally, this method begins with an assumption of y i = 0 for every unlabeled point and the constraint gets modified to P n i =1 ( f i  X  y i ) 2  X  . Therefore, this method essentially tries to minimize the following Lagrangian. 3. GTAM Method [24]: This method was designed for multi-class label propagation in the presence of class imbalance. The underlying formulation, ideally speaking, is not a variant of (1) but it is a mixed integer augmentation of (1). For the case of binary label propagation, this method solves the following mixed integer program. where f + i = f i y i , f  X  i = f i (1  X  y i ) , c + i = d c i = d i / P i d i (1  X  y i ) . For every point x i , it is required that we have y i = 1 if its label is +1 and y i = 0 , otherwise. The key innovation in this method lies in making y also as the binary decision variable and then solving the optimization problem by fixing the value of one variable (either y or f + , f  X  ) and minimizing for the other one. Note, for fixed y , this formulation becomes a convex quadratic program. At the optimal solution, the vector y itself is treated as the final labels of the vertices.
L := I  X  D  X  1 W is also known as normalized graph Laplacian matrix [1, 18, 7] but we will avoid using this form here. Consider the convex quadratic program (1) and its Lagrangian supervision (DoS) is very low, the second term in the Lagrangian would become quite insignificant and the first term would be a dominating term. In such a scenario, the first term would drive the f values towards zero for majority of the unlabeled data points in an optimal solution because matrix L is always a positive semi-definite (PSD) matrix. Further, in practice, due to limited machine precision, these f i values would indeed be equal to zero. Given that GFHF and LGC methods assign labels to the unlabeled points by means of the formula y i = sign ( f i ) , the labeling confusion at optimal solution would prevail at the same level as it was before solving the optimization problem. Further, if the class imbalance also happens to be excessively high then it would worsen the performance in following way  X  if one chooses to assign +1 label for f i it would result in high recall but very bad precision. On the other hand, if  X  1 label is picked for f i = 0 then it would result in very bad recall and moderate precision . Note, the Lagrangian of LGC method comprises an additional third term of P n i = ` +1 this term would also support driving the f i values towards zero for majority of the unlabeled data points in an optimal solution. In what follows, we unearth the ways by which one can fix such an undesirable behavior of convex methods under extreme values of CI and DoS. This discussion paves the way for our proposed non-convex label propagation (NCLP) method.
Ideally speaking, one should go about solving an integer version of the convex quadratic program (1) where we have f  X  X  +1 ,  X  1 } This is because methods such as GFHF and LGC eventually use sign ( f i ) as the labels. However, due to the computational complex-ity, these methods implicitly seek to solve a relaxed version, where they relax the integer constraints through either f  X  R n Under the extreme behavior of the dataset, an optimal value of such a relaxed integer program is quite far from the corresponding integer optimal value. That is, it results in f i = 0 for pretty much all the points as discussed before. The presence of integrality constraints avoids such degenerate point as being the optimal solution. Thus, it is this large integrality gap that plays a lead role behind poor classi-fication performance of the convex methods in scenarios where DoS is extremely low and CI is excessively high. For such scenarios, the performance of the convex methods degrades in the sense that either precision takes a hit or recall takes a hit and thereby resulting in low value of F -measure.
Any semi-supervised classification method, which uses a classifi-cation score f i to classify the unlabeled data point x i a balance across three quantities by means of underlying optimiza-tion formulation  X  empirical loss for labeled data, loss for unlabeled data , and generalization error . For vast majority of graph label prop-agation methods, the generalization error remains the same and is given by the graph regularizer term f &gt; Lf . It is the loss function for unlabeled data which plays a crucial role in determining the performance of the method under extreme behavior of the dataset. Figure 1 shows the unlabeled loss function used by different meth-ods. Note, GFHF function uses no loss function for unlabeled data points, whereas LGC method uses convex quadratic loss function. The loss function used by LGC method encourages the classification scores f i to be as close to 0 as possible, and thereby resulting in poor performance under extreme behavior of the data. This fig-ure also shows the loss function used by a popular non-graphical
Figure 1: Different Loss Functions for Unlabeled Data Points semi-supervised classification method, namely Transductive SVM (TSVM) [13]. This loss function encourages classification scores f to be as close to  X  1 as possible. We are inspired by this loss function to modify the convex quadratic criterion and obtain a non-convex quadratic criterion (as described in the next section). Figure 1 also captures the unlabeled loss function used by our proposed non-convex label propagation (NCLP) method which we describe in next section.
The idea behind non-convex label propagation (NCLP) is to choose a middle ground between two extreme scenarios, namely f  X  { +1 ,  X  1 } n and f  X  R n . Our non-convex criterion basi-cally solves the following non-convex quadratic program with box constraints and then chooses sign ( f i ) as the label of x minimize subject to  X  1  X  f i  X  +1  X  i = 1 ,...,n where,  X  and  X  are hyper-parameters. Notice, there are two key differences between the formulation used by any of the convex quadratic criterion based methods and the proposed non-convex criterion based formulation: (i) we have restricted the values of f in the interval [  X  1 , +1] for non-convex criterion, and (ii) the last term captures the loss for unlabeled points which is a concave function. This makes the objective function as a difference of two convex functions. The above modifications yield better results for the extreme scenario of very low supervision and very high class imbalance. To gain a better understanding and appreciation behind why above trick works, in what follows, we have highlighted some important properties of this non-convex program.
Consider any integer solution point, say f  X  { +1 ,  X  1 } serve, non-convex objective (4) value for this integer solution would vex objective (2) value would be equal to f &gt; Lf +  X  P y ) 2 +  X  ( n  X  l ) , if we choose  X  as the hyper-parameter instead of  X  . For any such integer solution point, following relation holds true: Given that  X  ,  X  , l and n are constants, this relation implies that if we restrict our attention to only integer solutions, that is f  X  { +1 ,  X  1 } n , then the optimal solution would be the same for all the three methods  X  namely, GFHF, LGC, and NCLP. However, both convex and the non-convex criterion based methods solve relaxed versions of the problems, and their fractional optimal solutions are very different from each other. As discussed before, for the extreme scenario of low DoS and high CI, the fractional solution of the convex method has almost all its entries as zero. On the other hand, this is not the case for the non-convex method. The fractional solution of the non-convex method is closer to the integer optimal solution than the fractional solution of the convex method; primarily due the presence of the last term in its objective function. This last term, that is  X  P n i =( l +1) f 2 i , pushes more and more entries of the solution vector towards integer boundaries ( +1 or  X  1 ) and thereby resulting in a better quality solution. A better quality fractional solution is precisely the reason behind non-convex criterion based method outperforming the convex criterion based method in our experiments (Section 5) under extreme conditions. In what follows, we justify why the presence of the last term in non-convex objective function pushes the f i values towards integer boundaries.
The last term in the objective function of (4) forces more f to be closer to the integer boundary, that is either  X  1 or +1 . This means that despite the second term in (4) being insignificant under low DoS, the first term, unlike convex methods, can X  X  drive all the f values to zero. This alleviates the limitations of the convex quadratic criterion based methods as described in section 2.2. In order to understand this effect more closely, let us consider a hypothetical graph as shown in Figure 2. The red and the green labeled nodes correspond to the +1 and  X  1 labeled data points, respectively. La-bels of all the other points (nodes) need to be predicted. Remember, we always assume +1 as the rare class label. For illustration pur-pose, we have assumed a true class separation boundary as shown in this figure. The true label for each point on the left side of this boundary is +1 and that on the right side is  X  1 . The aim of any label propagation method is to predict labels for all the unlabeled points.
The general idea behind any label propagation can be understood by means of following influence propagation phenomenon on this graph  X  the red nodes and the green nodes simultaneously start influencing their adjoining nodes in a manner similar to disease or influence spread. This is shown by red and green arrows in Figure 2. The quantum of influence depends on the weight of the edge joining two nodes. At the end, the class label/color of an unlabeled node gets determined depending on which color X  X  overall influence is stronger. Further, by design, it is ensured that such an influence propagation is as smooth as possible.

In view of this metaphor, it is not hard to believe that both convex and non-convex quadratic criterion based methods would do a good job of predicting labels for the points that lie far from the separation boundary. It is the points closer to the separation boundary which make all the difference. In Figure 2, points corresponding to the node and are denoted as black nodes. When a convex quadratic criterion based method, such as GFHF or LGC, is applied on this graph, it tries to regularize f i scores and hence these scores gradually shift from positive values to negative values as we move from the left side of the boundary to the right side of the boundary. This results in f whenever CI is very high and DoS is very low. The reason being the following  X  under very high CI and low DoS, influence of the red nodes dominate the green nodes at these boundary points . Therefore, all these boundary points get mapped to +1 labels. The end result is separation boundary getting pushed further towards right as shown in Figure 2. On the contrary, if we let labels propagate under the non-convex quadratic criterion, there would be a tendency for any two points to acquire same signed f scores only when they are connected with very high weight edges for otherwise, it will push the objective function value high (due to the presence of last term). This results in f 8 ,f 9 ,f 10 ,f 14 and f 15 taking negative values as the weights of the edges connecting these nodes to their +Ve class side neighbors are outweigh by the weights of the edges connecting them to their -Ve class side neighbors.
A little thought would suggest that whenever given graph G has multiple connected components, the formulation (4) can be decom-posed into multiple independent formulations  X  one for each con-nected component. This feature adds to the parallelization prospects of any solution methodology. However, one must keep in mind that each of these connected components must have labeled data points within itself for otherwise this formulation will recommend the same label (either +1 or -1) to all the data points.
The k -fold cross-validation coupled with grid search is a common trick to choose the values of hyper-parameters. In our context, however, such an approach would be infeasible due to its time complexity for large size problems. Therefore, in what follows, we suggest a rule of thumb to choose hyper-parameters. We have found these rules working extremely well in our experiments.

For choosing  X  , we advocate that the empirical loss arising out of labeled data should never be dominated by the structural risk. The reason being that empirical loss will anyways be quite small when degree of supervision is very low. For any graph, if we assume an integer solution f  X  X  +1 ,  X  1 } n , the structural risk can be bounded as follows:
X The empirical loss for the labeled points can be at most 4  X l , where l corresponds to the number of labeled points. Therefore, we suggest 4  X l  X  2 n . In our experiments, we have chosen  X  = n/ 2 l .
For choosing  X  , we again advocate that the highest possible (ab-solute) value of the loss for unlabeled points should never dominate the structural risk. The reason being that this loss remains the same (equal to  X u ) across all possible integer solutions and hence we don X  X  want this term to dictate our final solution. For this, we need to compute a lower bound on f &gt; Lf where f i  X  { +1 ,  X  1 } . We get an approximate lower bound by solving the convex program corresponding to LGC method using an iterative scheme proposed by [27]. We use the  X  value as suggested before while applying this iterative method. Let f  X  R n be the solution of this iterative method. We define g i = sign ( f i ) and assign  X  = [ g &gt;
As discussed earlier, the formulation (4) is a difference of convex (DC) program and hence we use the well known Concave-Convex Procedure (CCCP) [26, 14] to solve this optimization problem in an approximate manner. CCCP is an iterative method which takes the linear approximation of the second function in each iteration and thereby reducing the overall problem into a convex quadratic approximation. Algorithm 1 depicts the pseudo code for the CCCP based label propagation method assuming there is one single con-nected component (CC) . If there are multiple CC then we need to run this method independently on each of the CC. The most critical step of this method involves solving a convex quadratic program using gradient projection method which is discussed in the next section.

Algorithm 1: Non-Convex Label Propagation (NCLP) 1 Choose an appropriate value for  X  and  X  ; 2 begin Initialize 4 repeat 6 Solve following convex quadratic program using 7 until k f new  X  f old k X  threshold ; 8 y  X  i  X  sign ( f new i )  X  i = 1 ,...,n
We set the initial guess f 0 by setting f 0 i as 0 for all the unlabeled points and given label y i for all the labeled points.
The gradient projection method is one of the popular methods for solving a convex quadratic program having simple constraints such as box constraints [20, 17]. This method, in spirit, is very much similar to the method of steepest descent direction for unconstrained optimization except that the negative gradient is projected onto the surface of the box constraints in order to define the direction of movement. Exact details about this method can be found in [20, 17] but in what follows, we present a simplified form of this method when applied to our problem context.

The Algorithm 2 depicts the pseudo code for the gradient pro-jection method applied to the convex quadratic program appearing in Algorithm 1. This method takes f old as the input which is the current approximate solution of the overall problem and outputs an optimal solution f new for the convex quadratic program appearing in the Algorithm 1. The matrix S in this algorithm is a 0 / 1 diagonal matrix of size n  X  n , having non-zero diagonal entry corresponding to each training data point. The gradient projection method is an it-erative procedure where we maintain the set Q of active constraints X  indices. In every iteration, we find a descent direction d in the space orthogonal to the space of active constraints. This ensures that the active constraints remain active when we move in this descent direc-tion. If this direction is zero then we test whether the optimality has been achieved (or KKT conditions have been satisfied). If not then we drop the constraint from active set whose lagrange multiplier is the most negative and continue for the next iteration. If the descent direction d is nonzero then we first compute the range of the step size, denoted by  X  , such that by taking this much step size in the direction d , we still remain in the feasible region. Next, we find out the optimal step size  X   X  within this feasible range  X  which would minimize the objective function as a function of step size  X 
Algorithm 2: Gradient Projection Method 1 f  X  f old ; 2 5 f = 2 Lf + 2  X  S ( f  X  y )  X  2  X  ( I  X  S ) f old ; 3 Q  X  X  i | constraint f i is tight } ; 4 Define an indicator vector  X   X  [  X  1 , 1] n as follows 5 Define a decent direction d  X  R n as follows 6 if d = 0 then 7 i  X   X  argmin 8 if  X  i  X   X  0 then 9 f new  X  f and Stop 10 else 11 Q  X  Q \ { i  X  } and go to Step 4 12 else 13 Compute feasible step size  X  along direction d as 14 Compute the optimal step size  X   X  as follows 15 f  X  f +  X   X  d and go to Step 2
The Steps 2 and 13 of Algorithm 2 are the most expensive, O ( n steps in terms of both memory and time requirements. All other operations are of the order of O ( n ) . In what follows, we discuss ways to deal with this problem.

Recall, if the application has an explicit graph, e.g. social network or mobile payment network, then typically such graphs are quite sparse and one can cope with the scale by employing the sparse representations of the matrices L and S . One can also consider using MapReduce platform where we already have efficient algo-rithms available for large matrix multiplication [21]. However, the challenge arises when there is no explicit graph given among the data points. In such a scenario, one can always construct a complete graph among the data points by means of defining an appropriate similarity (or distance) function between a pair of data points. As dealing with such a huge graph is quite expensive, we advocate to work with a sub-graph instead. That is, we suggest to generate a k -NN graph of the data points. The use of k -NN graph brings down the space and the time complexities of the steps 2 and 13 to O ( kn ) . The generation of a k -NN graph, however, takes O ( n naive manner. To deal with this problem, we further suggest to work with an approximate k -NN graph by employing a hashing technique such as Locality Sensitive Hashing (LSH) [2] or more recent tech-nique proposed by [9]. In LSH based technique, it is required to first hash (aka project) each of the given n data points  X  having d -dimension  X  into a p -dimensional space, where p &lt;&lt; d . To boost the approximation quality, LSH performs T different projections in a parallel and independent manner. Once this data structure is ready, we can generate an approximate k -NN efficiently. The space and the time required to hash n data points are of the order of O ( pTn ) where, p and T are known as LSH parameters. Once this hashing is completed, the time required to generate approximate k -NN graph reduces to O ( pTn 1+ ) where 0 &lt; &lt; 1 . LSH technique has an advantage that one can control the tradeoff between the quality of a k -NN graph and the resource requirement (memory and time) by means of adjusting the tunable parameters.
As discussed above, in order to cope with the size of a problem in real world settings, it would be necessary to generate an approximate k -NN graph of the given dataset before even applying NCLP method. LSH is an efficient data structure for such purpose. Typically, the implementation of LSH involves setting up three tunable parameters  X  (i) the bucket width w for a hash function, (ii) the lower dimension p into which the data are projected under each hash table, and (iii) number of hash tables T . While, the choice of parameters p and T directly impact the tradeoff between resource requirements and the quality of k -NN, there is no rule of thumb or guidelines when it comes to choosing them. Depending on one X  X  appetite for resources (memory and time), the values of p and T need to be chosen in pretty much hit and trail manner. On the other hand, for choosing the bucket width w , one can be more systematic as discussed below.
Suppose, the original data points are having d -dimensional fea-tures. Then, by normalizing them in a range of [0 , 1] , the distance between any two points in the d -dimensional space can be made at max dimensional space under LSH, the maximum value for each of these p -coordinates would be  X  d/w number of hash bucket along each projection dimension if we decide to choose w as the bucket width for each hash function. Assuming that given n data points are uniformly distributed, the projected coordinates of these points are also likely to be nearly uniformly distributed along any dimension. Thus, along each projec-tion dimension, each of these hash bucket should comprise nw/ number of points on an average. Given that we want to generate a k -NN graph, we should make sure that for any point, there are at least k -NN points in the same bucket along each projection di-mension. This would mean that we should set nw/ experiments, we opt to set nw/ also gives us decent performance in terms of scaling.
We would also like to highlight a difficulty that one typically runs into while using k -NN graph to cope with the scale of the problem. Note, as we decrease k value, we are likely to get more number of connected components in the graph. In such a case, as described earlier, we need to run NCLP method independently on each of these connected components. For this to be feasible, however, each of these connected components should contain both +Ve and -Ve labeled data points for otherwise NCLP method will output the same label across all data points in that component. We call this problem as label starvation problem in k -NN graph . If all the connected components in the generated k -NN graph satisfy this feasibility requirement then we can simply proceed by applying NCLP method independently on each of these components. On the contrary, if this is not the case then we would require to add some extra edges in the k -NN graph so that we reconnect all these components and get one single connected component having both types of labeled points within it. While there could be many ways to accomplish it, we have adopted a minimum spanning tree (MST) based strategy for this problem while conducting experiments. This strategy is summarized in the form of Algorithm 3. At high level, the idea behind this strategy is to add minimum possible edges that are most effective in terms of connecting label starved components with their nearby components who have labeled nodes within them.

Algorithm 3: Fixing Label Starvation Problem 1 for every pair of ( C i ,C j ) where C i has at least one labeled 2 for every labeled point x i  X  C i do 3 { x j 1 ,x j 2 ,...,x j k } X  k -NN points of x i in C j 5 dist ( C i ,C j )  X  min 6 for every pair of ( C i ,C j ) where C i or C j has at least one 7 d ij  X  min { dist ( C i ,C j ) ,dist ( C j ,C i ) } 8 Build an MST by treating these connected components as 9 for each edge ( i,j ) of this MST do 10 Include all the edges between each labeled point of C i
In this section, we describe a label flipping method which can improve the quality of any given labeling, including output of NCLP method, whenever class imbalance information is also known a priori. This method is simple, fast, and greedy in nature. It starts with a given labeling of the data points, say y old , and selectively flips the labels of some points until a desired number of +Ve labels, say P , is achieved. At any given point in time, this method compares the class imbalance of the current solution in hand with the target imbalance. If the current class imbalance is higher than the target one then it picks one -Ve labeled point and flips its label. The other case is symmetric. While picking a -Ve labeled point, it picks the point whose label flipping would result in the least increase in value of objective function (4). For this, we define a notion called potential of a data point. The potential of a data point measures the increase in the value of the objective function (4) in case its label is flipped. Therefore, this method chooses a -Ve labeled point having minimum potential and flips its label. This iterative process continues until a desired level of class imbalance is achieved.

The pseudo code for this method is given in Algorithm 4. Observe, the label flipping method can start with any initial labeling and it outputs an improved labeling with a desired level of class imbalance. The quality of the output, however, may vary depending on the starting point as this approach is greedy in nature. One would see a huge difference in quality of the output if the label flipping is triggered with the output of NCLP method versus any other random label assignment. Finally, we would like to highlight the use of two heaps H + and H  X  as the data structure in our implementation for this method. This data structure in conjunction with the idea of node potential adds in lots of computational benefits and makes this method really fast in practice. For a k -NN graph, the space and the time complexities of this method turn out to be O ( kn ) assuming the class imbalance for starting solution is not too far from the target class imbalance.

Algorithm 4: Greedy Label Flipping 1 y  X  y old ; 2 P  X  X  i | y i = +1 } ; 3 N  X  X  i | y i =  X  1 } ; 4 For every point x i , compute its potential as 5 Using potentials  X  i , construct a heap H + for the set P and 6 Compute the target number of positive points as 7 if | P | &lt; P  X  then 8 i  X   X  Root node of heap H  X  ; 9 Flip the label y i  X  and update the set P accordingly; 10 Flip the sign of  X  i  X  ; 11 Move i  X  from H  X  into H + ; 12 for each j  X  Nbr ( i  X  ) do 14 Update node j in its heap; 15 Go to Step 7 16 else 17 if | P | &gt; P  X  then 18 Perform similar operations as in Steps 8 through 15 19 else 20 y new  X  y ;
We evaluated the performance of our approach over existing meth-ods by conducting a set of experiments on eleven datasets, including KDD Cup X 99 dataset 3 related to intrusion detection problem and ten other datasets from the UCI repository 4 . KDD Cup X 99 dataset has 494K data points with 38 features each. We randomly sampled 20K points out of these 494K points, including 1K points from positive class. Here positive class represents intrusion. This way, we could get 95% class imbalance. For other datasets, we marked the smallest size class as +Ve class, and all other classes as -Ve class. The summary statistics  X  number of instances, number of attributes , and percentage of negative instances (class imbalance)  X  of these datasets are captured in the first four columns of Table 1. http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html http://archive.ics.uci.edu/ml
As described in sections 4.2, we generated an approximate k -NN graph for every dataset using LSH technique. In our experiments, we used k = 5 ,p = 7 , and T = 20 for all the datasets. The bucket width w for every dataset was computed using formula mentioned in section 4.3. We mainly conducted two types of experiments. The first set of experiments was conducted to observe the effect of degree of supervision, and the second set of experiments was performed to observe the effect of class imbalance. In what follows, we describe each of these two sets of experiments in detail.
In this set of experiments, for every dataset, we kept the class imbalance fixed and varied degree of supervision across a range. For each value within this range, we randomly sampled an appropriate number of points whose labels were retained and all other labels were masked. While sampling, we ensured the class imbalance across the labeled data is roughly the same as the class imbalance across the whole dataset, so as to mimic the realistic scenario. For every dataset and each value of degree of supervision, we repeated the experiment 50 times  X  each time with a different sample of labeled points  X  so as to get an idea behind average behavior of different methods. For each such experiment, we compared the performance of following four methods  X  out of which the first two serve as the baseline and the last two are the proposed ones  X  (i) LGC [27, 28], (ii) GTAM [24], (iii) NCLP, and (iv) NCLP followed by label flipping (call it as NCLF).
 Tables 1 and 2 below show the average value of F -measure and Area Under Precision-Recall Curve (AUC-PR) , respectively, for each experiment in this setting. Note, AUC-PR values can X  X  be computed for GTAM and NCLF methods as these methods directly assign labels to the data points without making use of classification score f i . Further, we prefer to use AUC-PR instead of Area Under ROC curve (AUC-ROC) because AUC-PR is known to be a better metric than AUC-ROC while comparing different classifiers  X  es-pecially under high class imbalance [8]. It has been shown that a classifier which performs good in terms of AUC-PR would definitely perform good in terms of AUC-ROC, but not vice-a-versa.

While actual numbers are listed in Tables 1 and 2, we have sketched heatmap plots for these tables (Figures 3 and 4, respec-tively) to serve as visual aid while interpreting the results. Figure 3 shows the heatmap for F -measure based performance gain in NCLP method over GTAM method. Similarly, Figure 4 shows the heatmap for AUC-PR based performance gain in NCLP method over LGC method. In each of these heatmaps, the numeric score inside a cell indicates the superiority (or inferiority) level of the proposed method as compared to the baseline method. The score in each cell is equal to the ratio of superior method X  X  performance to the inferior method X  X  performance . We assign a green label to the cell and a positive sign to its score if the proposed method performs superior in that cell. Otherwise, we assign a red label to the cell and a negative sign to its score. A cell X  X  color sweeps from red through green as its score swings from negative value to positive value. For our purposes, greener a cell means better the performance of the proposed method as compared to the baseline method. In what follows, we have summarized some interesting insights from these results.
In the second set of experiments, we evaluated the performance of NCLP and NCLF methods over GTAM and LGC methods by varying class imbalance in datasets. For this, we worked with KDD Cup X 99 dataset. We sampled 20K points from this dataset in a way to ensure 95% class imbalance. It formed our first dataset for this set of experiments. Similarly, we created nine more datasets by sampling 20K points with 90% , 85% , . . . , 50% class imbalance. For each of these ten different class imbalance valued datasets, we further varied DoS in the range of 0 . 5% to 10% and computed F -measures using NCLP, NCLF, GTAM, and LGC methods.

We have plotted 3D graphs to capture the variation of F -measures as a function of class imbalance and DoS. Figure 5 shows such 3D graphs for LGC, GTAM, NCLP, and NCLF methods.

It follows from Figure 5 that LGC and GTAM methods are very sensitive to class imbalance as these methods see a steep decline in their performance with an increase in class imbalance. Contrary to this, NCLP method remains almost unaffected by the class im-balance. More precisely, for any given DoS, the F -measure of NCLP method remains pretty much the constant over a range of class imbalance. The numbers show that NCLP method outperforms LGC method, especially under the scenario when class imbalance is extremely high. Finally, NCLF method X  X  performance remains flat for a substantial range of high class imbalance ( 75%  X  95% ).
We have highlighted the limitations of the existing convex crite-rion based graph label propagation methods such as GFHF, LGC, and GTAM under the scenario when class imbalance is quite high and degree of supervision is quite low. We have suggested a non-convex extension to this criterion and proposed an efficient scheme to propagate labels under this new criterion. The questions that are still unanswered include: (i) how to generalize NCLP and NCLF methods to multi-class classification, (ii) can we develop a fully com-binatorial label flipping algorithm to solve the non-convex program approximately, (iii) can we develop a sound theoretical understand-ing behind this shift in the behavior of label propagation method, especially, when class imbalance is high and degree of supervision is low, and (iv) can we rigorously analyze what values of hyper-parameters  X , X  would make the objective function as convex and whether those values are good choices. [1] A. Agovic and A. Banerjee. A unified view of graph-based [2] A. Andoni and P. Indyk. Near-optimal hashing algorithms for [3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [4] Y. Bengio, O. Delalleau, and N. Le Roux. Label propagation [5] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: [6] N. V. Chawla, N. Japkowicz, and A. Kotcz. Editorial: special [7] F. R. K. Chung. Spectral Graph Theory . American Degree of Supervision (%) Figure 4: A Heatmap for AUC-PR Comparison of NCLP with LGC [8] J. Davis and M. Goadrich. The relationship between [9] W. Dong, C. Moses, and K. Li. Efficient k -nearest neighbor [10] M. Egele, G. Stringhini, C. Kruegel, and G. Vigna. COMPA: [11] W. Fithian and T. Hastie. Local case-control sampling: [12] J. Gao, H. Cheng, and P.-N. Tan. Semi-supervised outlier [13] T. Joachims. Transductive inference for text classification [14] B. K. Sriperumbudur and G. R. G. Lanckriet. On the [15] S. Li and I. W. Tsang. Maximum margin/volume outlier [16] W. Liu and S. Chawla. A quadratic mean based supervised [17] D. G. Luenberger. Linear and Nonlinear Programming . [18] U. Luxburg. A tutorial on spectral clustering. Statistics and [19] A. K. Menon, H. Narasimhan, S. Agarwal, and S. Chawla. On Figure 5: 3D Plots for F -Measure Based Performance Comparison of Baseline Methods and Proposed Method [20] J. Nocedal and S. J. Wright. Numerical Optimization . [21] J. Norstad. A MapReduce algorithm for matrix multiplication, [22] S. Pandit, D. H. Chau, S. Wang, and C. Faloutsos. Netprobe: [23] I. N. C. S. Report. Mobile payments X  X  growing threat. [24] J. Wang, T. Jebara, and S.-F. Chang. Graph transduction via [25] H. Yu, P. B. Gibbons, M. Kaminsky, and F. Xiao. Sybillimit: [26] A. L. Yuille and A. Rangarajan. The concave-convex [27] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch X lkopf. [28] D. Zhou and B. Sch X lkopf. A regularization framework for [29] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised
