 Graphics processors (GPUs) have developed very rapidly in recent years. GPUs have moved beyond their originally-targeted graphics applications and increasingly become a viable choice for general purpose computing. In fact, with many light-weight data-parallel cores, GPUs can often provide substantial computational power to accelerate general purpose applications at a much lower capital equipment cost and much higher energy efficiency, which means much lower operating cost while contributing to a greener economy. 
There have been a number of research works on using GPUs to speed up database management systems (DBMS) and business intelligence analytics [2-7, 13-14]. As the GPGPU technology advances, and as multi-core systems gain popularity, recent studies have begun to focus on measuring how GPGPUs compare with multi-core systems in speeding up data analytics. addresses the issue of column scan in a main memory on column store-based data warehouse. Column scan refers to predicate evaluation and filtering over a column of data in a database table. In [11], it is reported that a GPU implementation running on Nvidia GeForce 8800 GTX achieved marginally better performance than the CPU version running on a quad-core Intel CPU. However, the performance gain is not significant, and, given the higher energy consumption of GPU compared to CPU, it is difficult to conclude that GPGPU has a competitive edge over pure multi-core systems for such tasks. 
Recognizing the importance of the column scan primitive, our study aims at exploring design alternatives for GPGPU implementations to determine if one could achieve a much higher level of performance so as to establish a more viable edge for performance impact of various alternative designs that more effectively utilize GPU X  X  thread model, memory hierarchy and bandwidth, and also study the impact of the use of the newer atomic access feature of Nvidia GTX 280 X  X  shared memory. Our study confirms that GPUs can deliver a much higher level performance and offer significant speedup over pure multi-core systems. accelerate graphics pipeline. Recognizing the huge potential performance gains from these GPUs, many efforts have been made to use them to perform general purpose computing. With the introduction of Nvidia X  X  CUDA solution, which provides both software support (the CUDA programming language extended from the popular C language), and the CUDA-enabled hardware compute engine (a highly parallel architecture with hundreds of cores and very high memory bandwidth); programming GPUs for general purpose computing has become much easier than before [8]. 
CUDA extends C by allowing the programmer to define C functions, called kernels, which are executed in parallel by different CUDA threads. From the programmer's point of view, CUDA threads are arranged in a two-level hierarchy. Many threads together form a thread block, and many thread blocks form a thread grid. Threads in the same thread block are run on the same CUDA processor, and can the device X  X  global memory. 
Figure 1 is a diagram of CUDA's memory hierarchy. Each thread has a private block and with the same lifetime as the block. Finally, all threads have access to the same global memory. There are also two read-only memory spaces accessible by all threads: the constant and texture memory. 
While CUDA has made it much easier to program GPGPU, the program should be designed with its memory hierarchy in mind to achieve high performance. 
The (device) global memory space is not cached, so it is very important to follow the pattern of coalesced memory access to get maximum memory bandwidth. Non-coalesced memory is still allowed but at the cost of wasting memory bandwidth [10]. 
On the other hand, because it is on-chip, the shared memory is much faster than the memory into shared memory first, and then write it back to global memory after processing [10]. 
Many researchers are now able to use GPUs to accelerate their applications, and many have shown respectable speedup performance compared to CPU-only implementations. Popular commercial applications, such as Adobe X  X  creative suite, Mathematica, etc., have new releases of their GPU-enabled version; researchers have used GPUs in cloud dynamics, N-Body and molecular dynamics simulations, etc, with results that show good promise [8]. The column scan primitive has its root in column-based database systems and column of a data table. The scan operator scans the bit stream, extracts the b-bit value and compares it against either a point value or a range of values. The output is also a bit stream, in which one bit is set when the corresponding input data element is found to satisfy the predicate evaluation. Figure 2 illustrates an example, in which 8 bits are used to encode the value of a data element. In this example, the predicate is  X  X qual to 1 X , and the second data element in the input column is found to satisfy the predicate. 
Depending on the size of the dictionary used, it is common to pick the smallest b which is sufficient to encode all values, to reduce the storage and I/O. Because of this, has to process the non-aligned values efficiently. 3.1 Experiment Design and Hardware and GPU. The CPU version was developed and compiled using Microsoft Visual C++ 2005, with OpenMP enabled for multi-thread support. The GPU version was developed in CUDA 2.0 on top of Microsoft Visual C++ 2005. The machine used in our experiments is an HP XW8400 workstation with dual quad-core Intel Xeon 5345 CPU running at 2.33GHz, Nvidia GeForce GTX 280 graphics board with 1GB onboard device memory, and 20GB of main memory. The operating system is Windows XP x64. The connection between the GPU and the host was via PCIe 1.1 bus, which has 4GB/s of theoretic bandwidth. 
Our experiments have shown that the measured bandwidth was about 3GB/s when copying data between the host memory and the pinned global memory on the device. This presented a potential bottleneck for the overall system performance. However, it offered the theoretic maximum bandwidth of 8GB/s. The newer standard, PCIe 3.0, currently under development and will be available at 2010, will double the bandwidth again. Furthermore, there are streaming techniques which allow data transfer and data techniques are beyond the scope of this paper. 
Since our primary interest is on the performance acceleration ratios by GPUs, we used randomly generated data sets. To make it easier to compare with previously reported results in [11], the selectivity has been fixed at 1/255 unless specially noted, time averaged from 10 experimental runs. Th e time does not include time to load the GPU X  X  device memory. Both CPU-and GPU-versions give identical results and they confirm the algorithmic correctness of our implementations. 3.2 CPU-Only Implementation The CPU only version was developed as the performance reference. The algorithm itself is straightforward but special attention has been devoted to make it as optimized as possible. It consists of a tight loop of the following steps: (a) Read a 32-bit input word. (b) Extract the encoded values in the input word, and, if necessary, the next 32-bit (c) If an extracted value matches the query value, the corresponding bit in the memory bandwidth since this operation is highly memory bound. Compiler flags and other optimization techniques have also been used here to maximize the performance [9]. Table 1 shows the performance of the CPU version running on a single CPU core. For the 8-bit encoding case, the single CPU core can process 128 MB of data in 0.416 throughput increases to 0.466 GB/s. It should be pointed out that even with selectivity of 1/255, more than 3/4 of the time has been spent on setting the output bitmap. 
Figure 3 gives the performance of the CPU implementation on multi-cores; up to 8 cores (8 threads) are shown here. The CPU ve rsion with 8 cores can process at a rate of 2.4GB/s for the 8-bit case, and 3.7GB/s for the 15-bit cases. The performance of cores. However, our CPU implementation scales largely linearly to the number of communication overhead between CPUs. 
The scan rate achieved did not change much when the dataset size increased from 128 million data values to 256 millions. In the following sections, we have fixed the dataset size to 128 million data elements only. 4.1 The Baseline The first GPU implementation, our baseline, has the following characteristics:  X  Both input and output are kept in GPU X  X  device global memory; no other type of  X  Every thread works on a data block, which has b * 32 bits, where b is the  X  Threads are arranged into thread blocks , with t threads per block. We have  X  Each thread will process 32 values of b bits each and generate one 32-bit word Figure 4(a) is an overview of this algorithm and Table 2 shows its performance. For the 8-bit case, the GPU baseline algorithm can process 128m data points in 32 ms, or a throughput rate of 3.91 GB/s. For the 15-bit case, the throughput rate dropped to 2.89 GB/s.
 from its non-coalesced memory reads, which prevents the algorithm from utilizing the GPU X  X  memory bandwidth fully. In the following sections, we will discuss various ways for improving this algorithm. 4.2 Bandwidth and Memory Hierarchy Modern GPUs have very high memory bandwidth. For example, the Nvidia GeForce GTX 280 has the theoretic maximum memory bandwidth of 141.7 GB/s. However, to thread structures carefully. The first opportunity for improvement lies in better utilization of the global memory bandwidth by using the  X  X hared X  memory of a thread bits. Since every 32-bit word may contain more than one data element, multiple when accessing the shared memory, it is much cheaper than the non-coalesced access to global memory in our baseline design. Note that the loading from global memory to data element, extracts it from the shared me mory, compares it to the query value, and sets its output bit if the match is found. 
Setting the output bit-vector is a harder problem. The early CUDA device did not ensure the threads don X  X  write to the same address at the same time; otherwise the race condition may occur. 
To avoid the race condition, our implementation allocates a 32-bit temporary instead to a single bit only; then a separate process is used to reduce these words into bits. In contrast with writing results directly to global memory, and having a separate kernel to reduce them, we can keep the 32-b it output per thread in the shared memory and merge them in the shared memory right after the predicate evaluation is done. 
A na X ve implementation, in which one loop scans through each of the 32-bit words and sets its corresponding bit in the resulting word, turns out to be very inefficient. In evaluation processing itself. 
We solved this problem by introducing parallel merge. By designing the data structures properly, the parallel reduction is no more than simply OR across 32 words, a much more efficient implementation. 
Figure 4(b) contains an illustration of the second algorithm. Double circles are performance results are shown in the GPU V2 column in Figure 4(d). 
The throughput rate for the 15-bit case increased dramatically, reaching 8.27 GB/s, rate is increased by 25%, to about 5 GB/s. It is worth noting the difference on throughput rates between 8-bit and 15-bit cases. The reason behind is that memory bandwidth utilization is better in 15-bit case. This can be explained as follow. 
Let X  X  say we are processing 8-bit input value. For a thread block of t threads, 8t bits will be processed. To load these 8t bits data from global memory, only 8t/32 threads will be needed for loading, since each thread will load one 32-bit word. This leaves t  X  8t/32 threads, or  X  of threads, idle. In the 15-bit case, only about half of them are idle during the loading phase. 
Certainly, this presents another opportunity for further optimizing our algorithm and it is discussed in next section. 4.3 Bandwidth for Copying Data In the previous section, we have shown that the algorithm is less optimal in the sense To better utilize the memory bandwidth, it is desirable to load a few data elements per thread. In 8-bit case, four elements would be preferred. For 15-bit case, two would be we have a more balanced algorithm and use memory bandwidth better. 
Results of this version are shown in the GPU V3 column in Figure 4(d). For 8-bit case, the throughput rate is now about 7.2GB/s, compared to 5GB/s in previous version. For 15-bit case, the throughput rate has reached to 9.2GB/s. 4.4 Utilizing the Newer Atomic Shared Memory Feature CUDA 1.2 supports atomic shared memory access, which was not supported in previous releases, including GTX 8800 on which [11] was based. For the memory throughput memory access, we can set the bits inside shared memory directly by each thread, and eliminate the extra effort in parallel reductio n. Figure 4(c) illustrates the algorithm that uses atomic shared access, as well as the throughput rate for all GPU algorithms. The throughput rate for this version was shown in GPU V4 column in Figure 4(d). performing parallel reduction. The throughput rates have now jumped to 12.8 GB/s and 13.7 GB/s for 8-bit and 15-bit data, respectively. 4.5 Analysis of Throughput Perfor mance against Theoretic GPU Bandwidth The Nvidia GTX 280 GPU used in our experiments has the theoretic maximum bandwidth of 141.7 GB/s, which is a much higher number than the throughput rate that our optimized GPU implementation has achieved. We discuss the gap between the two in the following. 
First of all, one should note that the maximum bandwidth quoted above was the theoretic value; the realizable bandwidth is usually smaller than that. Previous works have reported that the best bandwidth achieved in copying within GPU's global memory, on a 8800GTX, was 76GB/s, or 88% of its maximum pin-bandwidth at 86GB/s [12]. 
On the GTX 280, the best bandwidth we obtained was 113GB/s, or 80% of the cudaMemcpy(), as shown in the bandwidthTest sample program shipped with CUDA SDK 2.0. Using custom kernel, instead of the system function, the best bandwidth dropped to 107GB/s, or 75% of the maximum. 
Secondly, the raw memory bandwidth is not the most appropriate measure for our purpose of comparing performance. The more appropriate measure is the copy rate, which is defined as the number of bytes copied per second. For memory copy between device's global memory, the best copy rate was 53.3GB/s. Copying from the global memory to device's shared memory is faster, at rate of 78GB/s. 
To explain the gap between the copy rate and the throughput rate achieved in our algorithm, a series of experiments was done to measure exactly how the time was spent in the processing. Since there is no profiling tools available at device level, this was done by gradually removing parts of the logic and observing the change in throughput rate. Table 3 shows the results of these experiments. All experiments here are based on GPU V4, on 15-bit dataset with 1/255 of selectivity. 
Under the GPU v4 framework, loading the data from global memory to shared accordingly, even without using atomic operation (and so not guarantee correct result), the throughput rate dropped to 17.1 GB/s. With atomic operation, the throughput rate further dropped to 13.7 GB/s. 
Overall the majority of the time was spent on extracting values from the data compared to 8-bit case. Also noted by using atomic shared memory write, the than performing the parallel reduction step separately. 4.6 Comparison with the CPU Implementation Figure 5 give the comparison between CPU and GPU implementation. The CPU and 15-bit data respectively. The optimized GPU version, however, can process at the rate of 12.8 GB/s and 13.7GB/s, which are about 10.5x and 7.2x faster than CPU counterpart on 4 cores. 
The speedup is achieved not only due to GPU X  X  massively parallel processing designs that take advantage of the underlying hardware. These experiments indicate that GPU has the potential to deliver consid erable performance edge over pure multi-core systems. 4.7 Comparison with Previous Results Previous work [11] on using GPU for column scan was positive but the performance investment. Their GPU implementation, running on an Nvidia GeForce 8800 GTX, can process 128MB 8-bit data in 0.091 seconds, or about 1.37GB/s. This is just 29% faster than their CPU version on a quad-core CPU, which was about 1.06GB/s. 
The Nvidia Geforce 8800 GTX has 128 cores r unning at 1.35GHz, and a memory bandwidth of 86.4GB/s. In contrast, the GeForce GTX 280 used in this work has 240 cores running at 1.29GHz, and a memory bandwidth of 141.7 GB/s. GTX 8800 is a CUDA 1.0 device, and does not support atomic share memory access. 
Since the hardware we used in this paper is not identical to the one used in [11], it is difficult to have a direct comparison. However, based on extrapolation of the hardware capabilities, we believe that our baseline GPU implementation would deliver comparable or slightly faster performance on comparable hardware than that reported in [11], while our V3 implementation, which does not use the atomic shared memory feature (which was not available in GeForce 8800 GTX) would be about 2-3 times faster than the implementation reported in [11]. Another angle of looking at this is the speed-up obtained over the CPU version. Their CPU version, running on a quad-core Intel CPU Xeon 5355 @ 2.67 GHz, was able to process 120MB of 8-bit data in 0.118 second; a throughput rate of 1.06GB/s. Our CPU implementation, running on a workstation with dual Intel Xeon 2345 @ as a reference point for speedup computation. 4.8 Time for Data Transferring memory and our results have showed great performance boost from using GPUs -something that has not been done/proved in prior publication. If we treat GPUs as strict accelerators for existing systems, it is important to include the time for transferring the data from the host to GPU device and the time for transferring the results back. 
Table 4 gives the throughput rate when we take time for data transferring into account. The hardware we used here has PCIe 1.1 bus, which provides around 3GB/s of measured bandwidth. data transferring time via streaming  X  essentially to have CPU, GPU and data transfer all run concurrently. The performance data for both non-streaming and streaming version are presented here. 
Noted that time for both transferring data from host to device, and transferring has approached the measured bandwidth. In other words, large portion of the calculation have been hide and the throughput rate is limited by the system X  X  PCIe bus which connect the GPU device to the host. 
Compare the throughput rate without data transferring time, it is clear that the PCIe like to argue here,  X  The throughput rate is still much faster than CPU version on four cores, shifting  X  The machine we used has PCIe 1.1. Modern PC/workstation have commonly  X  In most real world scenarios, the data is most likely stored in disks. And the disk interesting direction for further research. We have reported our research on using GPU to accelerate column scan in data warehousing and analytics. We have shown that with algorithm and data structure achieve significant speed-up over pure multi-core implementations, making GPU a more viable alternative to be considered for column scan operations in data intensive analytics than previously reported. 
