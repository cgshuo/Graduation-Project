 With billions of database-generated pages on the Web where consumers can readily add priced product offerings to their virtual shopping cart, several opportunities will become pos-sible once we can automatically recognize what exactly is being offered for sale on each page. We present a case study of a deployed data-driven system that first chunks individ-ual titles into semantically classified sub-segments, and then uses this information to improve a hyperlink insertion ser-vice.

To accomplish this process, we propose an annotation structure that is general enough to apply to offering titles from most e-commerce industries while also being specific enough to identify useful semantics about each offer. To automate the parsing task we apply the best-practices ap-proach of training a supervised conditional random fields model and discover that creating separate prediction mod-els for some of the industries along with the use of model-ensembles achieves the best performance to date.

We further report on a real-world application of the trained parser to the task of growing a lexical dictionary of product-related terms which critically provides background knowl-edge to an affiliate-marketing hyperlink insertion service. On a regular basis we apply the parser to offering titles to produce a large set of labeled terms. From these candidates we select the most confidently predicted novel terms for re-view by crowd-sourced annotators. The agreed on terms are then added into a dictionary which significantly improves the performance of the link-insertion service. Finally, to contin-ually improve system performance, we retrain the model in an online fashion by performing additional annotations on titles with incorrect predictions on each batch.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  text analysis Algorithms, Experimentation, Case-study shallow semantic parsing; automated terminology extrac-tion; composite CRF ensembles; product offer titles; hyper-link insertion
With billions of database-generated pages on the Web where consumers can readily add priced product offerings to their virtual shopping cart, several opportunities will be-come possible once we can automatically recognize what ex-actly is being offered for sale on each page. While there has been a significant amount of work reported on how to ex-tract aggregate-level information about products (and the sentiments expressed about them) from webpage content, especially for electronic products, far less work has been re-ported on the task of automatically reading and recognizing all of the characteristics of any individual offering. When feasible, this capability will significantly aid in such tasks as terminology extraction and cross-seller product offering searches.

A natural place to begin is to characterize the information available within product offering titles such as:  X  The Show (Album Version)  X  X nd X  Pro Digital Lens Hood for VIXIA HF S10, S100 Flash Camcorders (1-year wrty from e-sekuro) . X  These titles, along with price, product category, and seller name, are prevalent pieces of information available on the Web 1 . While titles are a form of unstructured text, their central role in consumer decision making and search engine rank performance, sellers are motivated to ensure that its text is rich in relevant information while also being easy to read/parse. Further, we will show, these titles can be inter-preted to have structure in the form of semantic terms that refer to brands, features and a few other semantic classes, along with some syntactic terms (such as  X  for  X ). Titles, as with full sentences, can also be understood at a higher semantic parsing structure, but simply being able to auto-matically identify the structure at shallow-level can be use-ful to real-world applications. In this paper we present a case study of a deployed data-driven system that grows a these four data items, for example, are available in a product search such as google.com/search?tbm=shop&amp;q=Lauren+black+dress, and also in Amazon X  X  product API service http://www.google.com/search?q=RG Small.html product-related terms dictionary by parsing product offer-i ng titles in order to improve an affiliate-marketing link in-sertion service that inserts affiliated hyperlinks to relevant offers. 2 .

Although the semantic chunking of offering titles is a sim-pler task than the chunking of natural language sentences, even in this constrained domain there is no preexisting for-mal parser available to apply to the task; nor, as we will show in the experiments section, does it appear feasible to manually develop one. Given our pragmatic ambitions, we followed the best-practices approach of training a sequence BIO tagging model [19]. Further, we pursue the approach taken in natural language processing (NLP) of first chunk-ing a text string as a preprocessing step to additional tasks [1]. As Abney writes:  X  [I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time]  X . For us the task of shallow semantic offering title parsing appears to also be naturally commenced with a chunking step. The offering shown earlier, for example, could be semantically segmented as follows:  X  [Pro Digital] [black] [Lens Hood] [for] [Canon] [VIXIA] [HF S10], [HF S100] [Flash Camcorders] ([1-year wrty.]) [from] [e-sekuro] . X . Further, just as sub-sequent work in NLP added the requirement to label seg-ments with their phrase type (e.g. NP-chunk, VP-chunk, etc.) [14], the semantic segments in a title could also be mapped to semantic classes. Given a set of semantic classes such as product category / PC , product feature / PF , etc. (which we define in section 2) the sample title can now be further annotated as follows:  X  X  Pro Digital ] BN [ black ] PF [ Lens Hood ] PF
In NLP a best-practices approach to perform chunking is to use supervised sequence tagging models, such as a linear chain conditional random fields (CRFs) [18]. Because state-of-the-art systems do not generally approximate expert-level performance, even with a significant amount of labeled and unlabeled data [8], most real-word applications need to pre-pare for error rates that are higher than those achieved by human annotators. One way to address this challenge is to associate a confidence score with each predicted chunk so that only high-confidence predictions are provided to con-suming processes. In our case we apply a set of models (a model ensemble) to naturally produce a confidence score.
Given a system that can chunk offerings titles into rank-able terms one can begin to apply it to real-world tasks, such as the growing of a domain-specific dictionary of cat-egorized consumer product-related terms. Having such a terms database can be very useful in automation of many consumer e-commerce related tasks. Once a system knows that  X  Canon  X  is a brand ( BN ), that  X  Vixia  X  is a product line ( PL ), that  X  flash camcorder  X  is a product category ( PC ) and that  X  high def  X  is a product feature ( PF ), then a data-driven system can more easily infer the meaning of a phrase such as  X  Canon Vixia high-def flash camcorder  X . Unfortunately, updating and managing a terminological dictionary remains a time-consuming process [5]. With hundreds of thousands of product-related terms, it is essential to enhance the dic-tionary in a cost-effective manner. By parsing hundreds of a n affiliated hyperlink is encoded with sufficient informa-tion for a seller to reward the website publisher with some commission related to the value of  X  X ntroducing X  the con-sumer to the seller. Additional information on the service can be found here http://viglink.com/products/insert thousands of titles, we believed, many high-quality candi-date terms could be cheaply discovered and inserted into the dictionary.

The remainder of this paper is structured as follows: Sec-tion 2 defines the proposed annotation style in detail; Sec-tion 3 describes our algorithmic solution to the chunking task; Section 4 describes our solution to the addition of parsed terms into the dictionary, including the use of a crowdsourcing-based filter; Section 5 analyses the parser X  X  intrinsic performance on manually annotated data; Section 6 analyses the return-on-investment from the added terms; and Section 7 concludes the paper with future possible di-rections.
In this section we define our proposed annotation frame-work to the task of chunking product offering titles into their granular terms. Our main challenge is to define a term typ-ing system that balances the requirement to handle titles from a diverse set of industries; to also identify helpful se-mantic characterization of the offer; and to provide intuitive quidelines to human annotators.

Based on prior work in the modeling of product databases and product data management [9] and through iterative ex-perimentation, we propose that product offering titles can be exhaustively and beneficially decomposed into token sub-strings from the eight term types described below.
We observe in advance that any given title can reference zero or more of the eight term classes -no class is mandatory, though at least one must be present. Further, a given term can belong to more than one term type (famously the term  X  apple  X  can refer to a food category, a brand, a merchant, and even a scent-type product feature). 1. A product identifying term ( PI ) is a term that identifies 2. A product feature term ( PF ) is a term that refers to 3. A product category term ( PC ) is a term that refers to a 4. A product brand term ( BN ) is a term that refers to a terms 5. A p roduct line term ( PL ) is a term that refers to a 6. A merchant term ( ME ) is a term that refers to a mer-7. An offering feature term ( OF ) is a term added by the 8. A functional term ( FT ) is a term that plays a syntac-
Several examples of annotated product offering titles are presented in table 1.
We opted to apply the best-practices approach of BIO tagging, in which a string is first tokenized, and then a se-quential classification/tagging model is required to tag each token with either a B , I , or O depending on whether the given token either begins a chunk, is inside a chunk, or is outside of any chunk. When the task also requires that chunks be classified then the labels are updated encode the relevant term class. In our case, with eight term types, each token must be labeled with one of seventeen possible labels: { O , B-PI , I-PI , B-PF , I-PF , B-PC , ..., I-FT } . Specifically, we train a linear-chain conditional random field model on a manually annotated training dataset, as originally proposed in [18].
The remaining decision for applying a supervised model is to select the predictor features associated with each to-ken and the size of the before/after token window. Here again, we leverage the best-practices of features reported to be helpful in NP-chunking [18] and named entity recogni-tion (NER) [11, 15]. Indeed, most of features that we use are contained in the published baseline recognizer for the ICDM-2012 CPROD1 contest 3 [10].

For our task, which involves shorter text items and more varied patterns, we modified the eosTokFeat.pl 4 from the contest with three additional features. Two of the new fea-tures simply the offset of the token from the edge of the token string. This feature-type is used in sentence-centric chunking and was not used in the CPROD1 contest. The other new feature is global feature based on the offerings industry (such as Electronics, Books, or Automotive). As seen in ta-ble 1, the annotation pattern for an offering X  X  title can differ significantly based on their industry -book and movies can have long product identifiers. By introducing this feature we hope that the model can account for these patterns.
The new features are further described below: 1. Previous Tokens ( LEFTOFF ): The number of tokens into 2. Remaining Tokens ( RIGHTOFF ): The number of tokens 3. Industry ( INDUSTRY ): The code of the title  X s industry.
Clearly there are additional state-of-the-art techniques that we could apply, but we believed (given our Agile develop-t he original program to generate those features eosTokFeat.pl can be found at http://kaggle.com/c/cprod1/forums/t/2287/crf-based-baseline2-published the updated featurization program and all code used in the experimental study is available at https://dropbox.com/sh/q8cyv0wfuyg0han/sXZ95rOUC8 ment mindset) that this best-practices approach should be g ood enough to prove the business value of the solution.
As presented, the trained sequence tagging model does not provide a confidence likelihood estimate with each pre-dicted segment. Such an estimate can be helpful for many downstream tasks that may use title parser. For our dictio-nary population task, for example, where we require high-precision predictions, a confidence ranking score enables us to better expend our resources on predictions that will be more likely to be accepted into the dictionary. While there are methods to extract likelihood estimates from a trained CRF model, such as the Constrained Forward Backward algorithm (CFB) [3] that computes the total weight of all paths constrained by the states of our segments, these meth-ods are not generally available in CRF software libraries and can increase the processing time.

An alternative approach that we investigate is to train an ensemble of models, and to report a confidence score based on the number of models that agree for each predic-tion [12]. Unfortunately, there is no agreed-on best-practice approach to combining the predictions of a set of sequence tagging models 5 . However, a simple voting approach can be naturally implemented by ranking predictions based on the proportion of models in agreement. If all models agree on a segment then associate a score of 1; if only four fifth  X s of the models agree then associate a confidence score of 0 . 8. In our case, we opted to use the five models trained during a five-fold cross validation analysis which reports an in-sample F 1 (our  X  X nit test X  of each iteration is that the F 1 be higher than a threshold of 50).
Now that we can parse offering titles and rank the pre-dictions, we are ready to apply it to a real-world problem. Several business processes could benefit from this capability at our organization. The first that we explore is the addition of terms into a domain-specific dictionary that is referenced by our hyperlink-insertion service. A more complete dictio-nary improves the chances of recognizing mentions of prod-uct related terms in text. Managing such a large dictionary however can be a time-consuming process [16], especially if a low error-rate is required. In our case errors can result in bad user experiences to the visitors of our customer X  X  web-pages. If a term such as  X  Christmas  X  were accidentally in-serted into our dictionary as a product category ( PC ) rather than, say, of a product feature ( PF ), then the term becomes a viable candidate for hyperlink insertion when encountered in a webpage, and then be linked to some random (if still Christmassy) product. Further, the value of adding terms diminishes over time as fewer and fewer high-value terms remain and more esoteric terms are added. Given the ex-pectation of erroneous predictions, we opted to include a manual review step but of such high-confidence terms that the time spent per term would be very low.

We start the addition process by retrieving the titles of the most popularly clicked links to seller offerings 6 because pop-ular destinations are more likely to contain high-value terms t his topic appears to be an open research problem our click-log contains billions of seller URLs from which we can readily identify popular offerings that are more likely to result in the insertion of additional high-value links. Each week we parse a set of approximately ten-thousand titles and retain the terms with a confidence score higher than 50% (more than half the ensemble mod-els agree on the prediction) that are not yet present in the dictionary. This step currently produces approximately two-thousand candidate terms per week, such as: { term=  X  table saw  X , type = PC , industry = HG , score =0.80 quantity =72 } , ranked in descending order of their quantity (more popular terms should be entered ahead of less popular ones).
Because of the requirement for low-error rate of dictionary term addition and the low marginal value per individual ad-dition, we use a web-based crowdsourcing service to manu-ally review each of these terms to further filter the terms that are likely to be correct 7 . These online marketplaces match human intelligence micro-tasks (HITs) posted by organiza-tions with workers, and one of the areas that these services have been successfully applied to is natural language annota-tion [2]. A best-practice approach is to have w -workers, with typically 2  X  w  X  3, perform each micro-task in order to (as with an ensemble) determine which predictions have high-confidence and to more readily identify free-riding workers. We opted for w = 3 workers per task.

The basic questionnaire format that we selected was for the worker to provide feedback on whether a given term was of the given offering-term type (feature, category, etc.) within a given industry (electronics, books, etc.). For ex-ample:  X  Is the term two-disk a product feature in the Arts &amp; Entertainment industry?  X  We further experimented with whether to make it a binary-classification task ( TRUE or FALSE ) or to make it a multi-label classification one ( Definitely , Likely , Maybe , Unlikely , and No ). We opted for the multi-label approach because it resulted in slightly higher F 1 where agreement required two of the workers to select ei-ther Definitely or Likely for accepted predictions. Ap-proximately six hundred terms currently survive this filter-ing step. The accepted terms are now manually reviewed by an internal annotator to our organization, but by now all of them are entered into the dictionary.

In terms of the fine-tuning of the crowdsourced solution, we tested three different levels of payments per micro-task: $0.01, $0.02 and $0.05 and found that a payment of $0.01 achieved the highest number of additional terms per invest-ment. In the hope of detecting and rejecting free-riders (who sloppily speed through HITs) we identify and block any worker who replies differently than the two other anno-tators more than half of the time on a batch in which they attempt more than thirty HITs.
As with many real-world predictive modeling applications, our situation benefited from continuous improvement of our models [17]. Rather than creating a single training dataset to train our model from, we iteratively updated the training set with titles in which the existing model performed poorly on. Every week we select approximately one-hundred titles that contain a term confidently predicted by the ensemble-based parser but that a majority of the crowdsourced work-ers agreed on as being an incorrect prediction. The dataset h ttp://mturk.amazon.com produced by this process is further discussed in the next s ection on experiments.
This section evaluates the performance of the shallow-semantic parser on a pre-annotated dataset, while the next section evaluates the impact of using the parser on a real-world application.

Our focus of the analysis is on ways to better structure the trained parser such that it produces more accurate pre-dictions from the provided training data. 8
For the evaluation we used a dataset with 2,437 annotated product titles 9 . As in most real-world settings the annotated dataset evolved and grew over time, from a starting seed of records to test the feasibility of training a parser with useful accuracy. Afterwards, titles were added as described earlier in the section on iterative retraining 10 . Each record is cat-egorized into one of nineteen typically high-level industries based on an internally managed mapping table (for example,  X  X omputers X   X   X  X E X ).

Table 2 shows the distribution of titles and of term types by the different industry labels 11 . The table shows that there is a clear skew in record distributions -with the cat-egories of Consumer Electronics and Book accounting for more than half of the records (750+566), and product fea-tures (PF) dominating the types of terms annotated. From the product term distributions by industry we further notice a few outliers: Books and Arts &amp; Entertainment are dom-inated by PI terms; while alternatively Fashion and Food records have relatively few PI terms.
A standard measure of performance for systems that must segment and label text is the harmonic mean between the precision ( P ) and recall ( R ) of all predictions: F 1 = T o calculate this measure we used the widely use evaluation tool from the CoNLL-2000 Shared Task 12 . We performed a 5-fold cross-valuation study on the dataset. Table 3 reports the average F 1 performance of five CRF models trained and tested on different portions of the an-noated data. The first row reports the F 1 performance against X  All  X  X est records ( F 1 = 57 . 7) and against the records of the different product categories.

We had suspected that performance would vary signifi-cantly by the industry feature. This hypothesis was con-firmed by the other performance results on this row. No-tice how records from the books (BK) industry, , despite it w e used MALLET as our CRF toolkit mallet.cs.umass.edu the data can be downloaded from https://dropbox.com/sh/q8cyv0wfuyg0han/sXZ95rOUC8
Because of the iterative history of the dataset  X s develop-ment, the records are biased towards more challenging cases.
The other label refers to titles whose merchant category did not map to one of our existing categories; the other label refers to titles whose merchant did not appear to provide a categorization whatsoever available and described at http://www.cnts.ua.ac.be/conll2000/chunking/output.html having the second most number of records, performs partic-ularly poorly ( F 1 = 41 . 2). Given the relatively weak perfor-mance on some industries, we trained independent models with only records from each industry to see the effect of iso-lating the title data into clusters. As the table shows, two industries significantly improved F 1 performance despite the fewer available training records to the models: Books (from 41.2 to 82.0) and Arts &amp; Entertainment (from 48.8 to 61.3).
At this point we could have decided to use three models: one for Books, on for Arts &amp; Entertainment, and one based on All the training data.

Next, however we combined the AE and BK records into one training set, BK&amp;AE , to determine whether these two cat-egories could be treated as a single record cluster. Perfor-mance did improve but only for the AE records. This in-crease may be due to the relatively few AE records benefiting from the large number of BK records, while BK has sufficient records to diverge into its own model 13 .
 We tested on final permutation in this vein: Retaining the BK and AE training records to contribute to the predictions on the other industry records may be sub-optimal, so we further tested removing these records to create a not BK|AE model. This parsing model equaled the performance of the  X  X ll X  model with lower performance on the Jewelry ( JW ) titles and higher performance on ( FS ) and ( AU ) titles.

Based on this analysis, we opted to use three models: BK then the title was known to be of that industry, BK&amp;AE when the title was known to be from the AE industry, and All otherwise. 14 .
A natural baseline algorithm to the task is simply insert terms already found in an existing dictionary 15 . Given the overlap in terms (recall the many term types that a term such as  X  apple  X  can take) and the requirement that anno-tation occurs at a granular level (e.g. the token 2 may be from a product identifier, such as in iPad 2 or be a product f eature). After some experimentation, we opted for an iter-ative labeling method that proceeds from one term type, to the next. The basic pattern was to first apply brands, then product lines, then several product features, then products, black list and offer features within the record X  X  industry The baseline algorithm describe above achieves a 30.4 F 1 overall, with high scores on FashionFS and Electron-icsCE records (40.0 and 35.7 respectively) and nearly zero on Books and Arts &amp; Entertainment records because the dic-tionary only contains famous books and movies which are not found in the annotated data.
Because most of the used predictor features have been relatively well tested in other published research and none of the features are particularly time-consuming to calculate, i deally the sequential tagging model would automatically detect and adjust for the importance of these attribute val-ues and so avoid this time consuming and error prone cre-ation of a composite model a demo of the service can be found at http://www.gabormelli.com/Projects/PTPv1 we used an internal dictionary with nearly five-hundred thousand terms, but readers can use the term dictionary made publicly available for the CPROD1 contest the baseline program PTPbaseline.pl is available in the repository . titles 2% 5% 20% 33% 67% 80% 90% F we did not perform a thorough analysis of which features c ould be safely removed from the process. We did how-ever analyze the impact of the  X  X lobal X  INDUSTRY feature that we introduced that assist the model in detecting the different patterns that apply to some of the different indus-tries (such as long product identifier in books). What also motivated this analysis is the fact that the Books/BK and Arts&amp;Entertainment/AE industry titles had weak perfor-mance. Table 3 suggests that the feature is not informa-tive. This strongly suggests that the feature is not properly informing the models of the strong role that industry cate-gories have on annotation patterns. In the future we hope to investigate the work by (Krishnan &amp; Manning, 2006) [7] that uses two phases to introduce global features.
The learning curve in table 4, in which cross-validation is used to estimate performance at different proportions of held-out data, suggests that performance has begun to plateau at the current number of training records in the annotated dataset. Given the current feature set, many more anno-tated titles (possibly through self-labeling approaches) would be required to noticeably impact F 1 performance. Recently, due to this analysis, we have begun to direct our annotation effort towards titles from the industries with fewer records.
Given the ability to parse product titles we can now eval-uate the impact on a mission-critical service. As already described in sections 3 and 4, on a weekly basis we apply the parser to a large set of product titles and filter the re-sulting terms with a crowdsourced service, such that ap-proximately four-hundred terms are added weekly to the dictionary. With each batch of added terms our hyperlink insertion service has more information about the kinds of terms that relate to consumer products and may therefore be possible candidates for the insertion of a hyperlink (so long as other business rules and optimization rules are satis-fied). For example, recently the process discovered that the dictionary was missing terms such as: {  X  X able saw X , X  X unning lights X ,  X  X loor mats X ,  X  X uspension system X ,  X  X ear derailleur X  }  X  PC ; {  X  X motiva X  }  X  BN ; and {  X  X onar X ,  X  X uadro X  }  X  BN . Af-ter the addition of these terms into our dictionary the link-ing service can be more confident that the presence of these terms in a webpage indicate a valid hyperlinkable product-related mention. For example, when the service sees a pas-sage such as  X  ... the Ryobi table saw was able to ...  X  in an appropriate place on a website (based on business rule re-strictions) then, just like a website publisher might do man-ually on their own, the statistical recognizer used by the service can substitute the plain text  X  Ryobi table saw  X  with an affiliated hyperlink to a page that offers that type of ta-bles saw 17 which may earn the publisher a commission each time that a visitor to their page interested in that type of product clicked on the link and soon after decided to make a purchase from the seller at the end of the link. f or example to amazon.com/dp/B0050RBHQE
But, were the significant costs spent to introduce this semi-automated term-adding capability a worthwhile expense? In terms of costs and benefits, the costs to deliver the ser-vice had several dimensions, including: the time to proto-type the solution, the effort to annotate the data, the ef-fort to program a reliable weekly production system that in-cludes crowdsourced annotating, the effort to evaluate per-formance, and finally the expenses of the new temporary equipment to train and evaluate models. The benefit is the increased revenue generated by the link insertion server. This can be calculated from the cumulative value each added term. Our link-insertion service has been in operation for several years so were able to retrieve significant historical evidence of past click performance for each term prior to its insertion, and from this estimate future click behavior for the term. The term  X  table saw  X , for example, which, being composed of a common noun  X  table  X  and verb  X  saw  X , was likely avoided by the service prior to the addition of the term, could now be more readily inserted (though still only in appropriate textual context). One unknown was whether the increase in insertions would also result in an increase in clicks -it may be that missing terms were not valuable with respect to clicks. We modeled and extrapolated the clicks for each term based on click data before and after their inser-tion, and from this estimated that how many weeks it would require for the incremental value (if any) would pay for the costs and from then on return a positive investment. We determined that the effort recovered its expense within the first eleven weeks of its operation, and would also continue to deliver the incremental value to our organizations and our customers who benefit from the commission associated from the previously absent links. Finally, each new batch of terms continues to identify high-value terms though, as ex-pected, fewer with each batch. The service could also allow our organization to create one of the most comprehensive product-related dictionaries in the market quickly and cost-effectively. Finally, our organization can now apply a new capability to other mission-critical processes.
Given the commercial opportunity related to understand-ing product offerings on the Web, there has been significant published research on the automated identification of prod-uct properties from textual information. We review several publications that most closely related to our task and solu-tion. One of the first published investigations in our task by Ghani &amp; al [4] which applied a semi-supervised approach to the task of identifying attribute-value pairs in short product-related phrases.

We include three examples of the types of strings that they sought to parse: 1.  X  Extended Torsion bar  X  2.  X  Imported  X  3.  X  Contains 2 BIOflex concentric circle magnets  X 
These text items are related but differ significantly from the product offering titles listed in table 1. These short text items instead appear to be drawn from the lists of prod-u ct specifications that are commonly found on the product offering description pages.

Next, they propose a general annotation structure that attempts to identify attribute-value pairs in the text. This approach is very general. It can be applied to almost all other domains, not only product offering related text. Their annotation framework involves the application of three la-bels to text segments: attribute ( ATTR ), value ( VAL ), and nei-ther ( NA ), where (a TRUE value is inserted when no actual value is present in the text). As we can see in the annotated versions of the strings above, the attribute-value pairs can be readily extracted: 3. Contains NA 2 VAL. BIOflex concentric circle magnet ATTR.
Unfortunately, their representation cannot be easily trans-lated into our needs. As can be seen in their examples, sometimes chunks labeled as attributes and as values can both be mapped to product features. The representation, as (Ghani &amp; al, 2006) mention, results in a significant dis-agreement between human annotators in the X  X orrect X  X nno-tation structure for many cases. The example they offer is that of  X  Audio/JPEG navigation menu  X  which can be nat-urally annotated in three different ways, while for us the string naturally becomes a product feature ( PF ). Contrast their annotation structure to ours on their three examples: 1. Extended PF Torsion bar PC 2. Imported OF 3. Contains FT 2 OF BIOflex BN [ concentric circle ] PF mag-
Aside from the annotation structure, we notice also that their input strings are already in a succinct form relative to the long descriptive product titles in our task which often contain many values with an implicit attribute. For exam-ple, when a color is mentioned in a title the color term (say yellow ) is almost never preceded or followed by a term such as  X  X olor X .

Finally, the proposed algorithm which combines semi-su-pervised co-training along with expectation-maximization (EM) would involve significant programming effort and has not been successfully applied in other case studies, so we did not attempt to implement it as a baseline.
The work that most closely approximates ours, both in terms of task and approach appears to be the one by Put-thividhya &amp; Hu [13]. They propose the use of an iterative online supervised CRF approach to the task of labeling de-scriptive product offering titles.

Their annotation framework involves five labels: brand ( B ), garment type ( G ), size ( S ), color ( C ), and not applica-ble ( NA ). These labels are intentionally tailored for products from the fashion industry (from eBay X  X   X  X lothing and shoes X  products).

Below are two sample titles that they provide: 1. NEXT Blue Petite Bootcut jeans size 12 BNWT 18 2. Paul Smith Osmo White Plimsoll Trainers -UK 6
Followed by their reported annotations: 1. NEXT B Blue C Petite NA Bootcut NA jeans G [ size 12 ] Their labels map roughly to ours as follows: B=BN , G  X  PC , S  X  PF , C  X  PF , and NA  X  FT,PL,ME,OF .

Our annotation of their two examples is as follows: 1. NEXT BN Blue PF Petite PF Bootcut PF jeans PC [ size
Our proposal uses a broader term definition of product feature that encompasses their size and color labels (and is therefore applicable to other industries, but loses the gran-ual information). We also use product line ( PL ) which can appear in Fashion products (such as in Ralph Lauren -Polo and Tory Burch -Reva ). Additionally we use the Merchant , Offering Feature , and Function Term and avoid the use of a less informative NA .

In terms of algorithm design, they also propose the use of a supervised solution and a bootstrapped solution that grows their gazetteer (seed list). Finally, they do not report on the real-world impact of their work.
Finally, K  X  opcke &amp; al [6] address the focused task of iden-tifying the code/identifiers in largely electronics product of-fering titles using information theoretic approaches. This task is too restrictive to aid in our more general needs.
In this paper we reviewed the design and impact of a system that can identify and label the low-level semantic structure of the billions of product offering titles that per-vade consumer e-commerce websites. We leverage the cur-rent best-practices approach of supervised BIO-tagging via a trained linear CRF to train a predictive model, though we extend the approach to involve separate models based on industry information along with model ensembles that pro-duces a voting-based ranking score. We apply the trained parser to many offering titles on a regular basis in order to grow a domain-specific dictionary of offering-related terms. To ensure a low error-rate, each candidate term is reviewed via a crowdsourcing service. Terms that pass this filter are added to the dictionary, while terms that are strongly re-jected are used to select offering titles to be manually an-notated and added to the training dataset for subsequent trained parsers. Finally, we assess the impact of the added terms to our affiliate hyperlink-insertion service and we find
BNWT i n eBay is a contraction of  X  brand new with tags  X 
RRP in eBay is a contraction of  X  recommended retail price  X  that the new capability has both paid for itself, and contin-u es to add value to our organization, to our website-owning customers, and to their visitors.
 Several enhancements to the deployed system are possible. More leading-edge mechanisms for sequential tagging and active learning should be considered such as BILOU (instead of BIO ) tagging [15] as well as the two-phased approach in [7] to include global features.

Aside from enhancing the semantic parser X  X  accuracy, there are several additional applications that we would like to de-ploy. These include 1) the ability to perform record link-age on product records to find substitutable products with high-accuracy and likely also data cleaning based on con-tradictions with what the product offerings titles suggest. Finally, these applications may also benefit from the ability to create a full parse tree from a title. For now, through this case-study, we have demonstrated the cost-effective fea-sibility of data-driven shallow semantic parsing of product offering titles (for better automatic hyperlink insertion). The author would like to thank the rest of the team at VigLink for fruitful discussions and aid with the annotation. [1] S. P. Abney. Parsing by chunks . 1989. [2] C. Callison-Burch and M. Dredze. Creating speech [3] A. Culotta and A. McCallum. Confidence estimation [4] R. Ghani, K. Probst, Y. Liu, M. Krema, and A. Fano. [5] T. Gornostay. Terminology management in real use. In [6] H. K  X  opcke, A. Thor, S. Thomas, and E. Rahm. [7] V. Krishnan and C. D. Manning. An effective [8] X. Liu, S. Zhang, F. Wei, and M. Zhou. Recognizing [9] G. Melli and J. McQuinn. Requirements specification [10] G. Melli and C. Romming. An overview of the cprod1 [11] D. Nadeau and S. Sekine. A survey of named entity [12] R. Polikar. Ensemble based systems in decision [13] D. P. Putthividhya and J. Hu. Bootstrapped named [14] L. A. Ramshaw and M. P. Marcus. Text chunking [15] L. Ratinov and D. Roth. Design challenges and [16] F. Sclano and P. Velardi. Termextractor: a web [17] B. Settles and M. Craven. An analysis of active [18] F. Sha and F. Pereira. Shallow parsing with [19] W. Wong, W. Liu, and M. Bennamoun. Ontology
