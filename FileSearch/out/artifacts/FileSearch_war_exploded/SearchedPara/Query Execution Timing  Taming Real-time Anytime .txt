 Answering real -time queries, especially over probabilistic data, is becoming increasingly important for service providers. We study anytime query processing algorithms, and extend the traditional query execution plan with a timing component. Our focus is how to determine this timing component, given the queries X  deadline constraints. We consider the common multicore processors. Spe-cifically , we propose two query optimization modes: offline per i-odic optimization and online optimization. We devise efficient algorithms for both offline and online cases followed by a compe t-itive analysis to show the power of our online optimization. Final-ly, we perform a systematic experimental evaluation using real -world datasets to verify our approaches .
 H.2.4 [ Database Management ]: Systems  X  query processing . Real-time query ; Online algorithm; Multicore processor. 
We live in an interesting time, the so -called big data era. Data is often generated on a large scale with mixed quality. Examples include sensor networks, RFID, smartphones, and monitoring devices, to name just a few. In these settings, a large volume of data flows out from devices in a noisy and dynamically changing environment that is often affected by unforeseen factors. Thus, what we observe in the data is really samples of random variables. Dynamic, real -time, service -oriented queries over such data are valuable for businesses and the public. Let us look at an example. Example 1. There are projects that collect real -time traffic info r-mation from traveling vehicles equipped with GPS locating devi c-es. Then the server may provide real -time servi ces to many cu s-tomers. For instance, suppose Alice has a n important meeting to attend in her company at 9:00 AM. She would like to query the server  X  X f I leave home now, what is the 95% confidence interval of the time when I will arrive at the company? And what is the travel route? X  There is a deadline constraint on the maximum delay before a query answer must be returned. A large number of users may be using this service at the same time.
 Motivation for processing probabilistic data. Consider this question in Example 1: If Alice travels on a particular road se g-ment R, how long will she spend on R? The server has received a sequence of observations from the vehicles that traveled on R in a recent time window. Alice X  X  travel time on R is a random variable for the following reasons: (1) Even if we observe a small time window, the travel time still varies due to many factors (e.g., tra f-fic lights, fluctuation of traffic, etc.). (2) The system should really predict Alice X  X  travel time on R when she reaches R , sometime in the future depending on her path to R . (3) Missing data is co m-mon; the server may not have data on R and it has to use a model to estimate the delay. Therefore, Alice X  X  arrival time at her comp a-ny is a random variable with a considerable variance. The system should compute a distribution or a confidence interval, reflecting its best knowledge. This is a decision -making query important to the user  X  Alice will determine whether she leaves home now or later. Ignoring the uncertainty may turn a binary decision to the wrong side with disastrous consequences, which is why pro-cessing uncertain data has drawn much attention [6, 4, 8, 11, 14 ]. Anytime queries. In the literature, there are several approaches in processing queries over probabilistic tuples (e.g., [6 , 14]) and probabilistic attributes (e.g., [14 , 8, 11]). In particular, the Monte Carlo algorithm (MCA) [8, 11] is more general in that it can pr o-cess arbitrary types of query operators (SQL and beyond), as it reduces the problem to answering queries in deterministic possible worlds. For instance, it is unknown how to calculate the distrib u-tion of the shortest travel time from point A to point B using other approaches . Such operators in user defined routines or the math e-matical operators beyond the basic SQL ones (in engineering and scientific applications) are only feasible with MCA.

An MCA falls in the more general category of anytime alg o-rithms [15], which are algorithms whose quality of results i m-proves gradually as computation time increases. When few sam-pling rounds of MCA are run, we still can synthesize the results from them and get an approximate answer; the more rounds we run, the more accurate result we get. We show that we can opti-mize the quality of service using anytime algorithms even with multicore processors. Intuitively, we gain the flexibility of dyna m-ically allocating system resources with anytime algorithms as they can stop and go more easily. Note that our work is not limited to probabilistic data. When the data is deterministic, anytime query processing algorithms are termed online query processing in pr e-vious work, such as in online aggregation [9]. In general, for bre v-ity, we refer to the queries that are answered with an anytime alg o-rithm as anytime queries .

Using anytime queries in real -time service -providing applic a-tions is clearly more user -friendly . After a user issues a query, the system will provide an initial approximate answer quickly, fo l-lowed by a number of answer updates with increasingly greater accuracy. Real -time users typically prefer to get a  X  X ig picture X  and  X  X uick and dirty X  answer first, before waiting for too long [9] . Based on the rough initial answers, the users can decide whether refined answers are needed from the system or not, which may also save system resources .
 Our contributions. We formalize the notion of anytime queries with answer deadlines, and extend the traditional query execution plan with a new component, timing . Our goal is to study how to determine this timing component of the plans of many queries that arrive at any time, in order to maximize the quality of service (QoS), formalized with a number of desired properties . We pr o-pose two query optimization modes: offline periodic optimization (PO) and online optimization (OO), in the categories of offline and online algorithms [1], respectively ( Section 2 ).

Related to QoS, fairness is the optimization criterion while maximally utilizing the available computing resources. We devise an efficient PO algorithm by converting the timing of queries into a graph. The optimization algorithm works on the graph increme n-tally. We also propose a randomized algorithm under OO and show that it is average -case optimal in a competitive analysis [1, 13, 7] ( Section 3 ). Finally, we conduct a comprehensive exper i-mental study using real -world datasets ( Section 4 ) .
We consider executing queries on a multicore machine over in -memory databases or data streams. We say that a query  X  has a core running time  X  if the summation of the running times on each core is  X  .
 Definition 1 (multi -answer anytime query). A multi -answer anytime query  X  has the following components: (1) a submission time  X  0 (i.e., the time when the query is submitted), (2) a start execution time  X  0 , (3)  X  deadlines  X  1 &lt; X  2 &lt; X &lt; X  sequence of  X  minimum core running time  X  1 ,..., X  1 X  X  X   X  X  X  , the system should allocate core running time at least  X  to  X  between times  X   X  X  X 1 and  X   X  , and incrementally report an answer to  X  . In the special case of  X  =1 , we simply call  X  an anytime query , and say that the time interval [ X  0 , X  1 time of  X  . The minimum core running time  X  1 is also called the essential time of  X  , and any core running time beyond  X  the bonus time .
 Example 2. Continuing on Example 1 , Alice may specify three deadlines for her query: 1 minute, 3 minutes, and 5 minutes, r e-spectively, and require that, before each deadline, the system run at least 50 (additional) rounds of MCA and return a confidence interval (Alternatively, Alice specifies the confidence interval and the system translates it to the number of MCA rounds required). The query optimizer will estimate the minimum core running times  X  ,  X  2 , and  X  3 . The deadlines provide reasonable checkpoints for the user to examine the answer so far without waiting for too long. Note that in case Alice finds that the answer returned at some deadline (e.g., 1 minute) is already satisfying, she may cancel the subsequent deadlines.

It is easy to see that a multi -answer anytime query  X  can be d e-composed into  X  anytime queries  X   X  ( 1 X  X  X   X  X  X  ) with start exec u-tion time  X   X  X  X 1 , deadline  X   X  , and minimum core running time  X  respectively ; these anytime queries all have the same submission time as the original one (  X  0 ). Thus, from now on, we only consider (one -answer) anytime queries. We extend the conventional query execution plan with a new timing component, which specifies when the query is scheduled to execute and for how long. The query optimizer needs to determine this timing, taking into a c-count all queries that may arrive at different times.
 Quality of service (QoS) and desired properties. For QoS, each query should be allocated a minimum core running time at least, which expresses a user X  X  minimum expectation on the confidence of results . The query optimizer helps to give this information. For example, for MCA, based on the well -studied connection between the number of rounds and result confidence (e.g., [5 , 12]), one can require that a certain number of rounds be minimally required for a certain result confidence. Then from the time estimation of one round, the optimizer derives the minimum core running time. Furthermore, for QoS , we enumerate a few desired properties (DP). The query optimizer will consider these DPs when dete r-mining the timing component of query execution plans.

DP1 (M EETING D EADLINES ). If an execution plan exists that meets the deadlines of all queries, then the system should meet those deadlines.

DP2 (F ULL U TILIZATION ). At any time  X  , all cores should be fu l-ly utilized, unless there are no queries to run at  X  .

If all deadlines can be met (DP 1), to fully utilize the cores (DP2), we determine how much to extend the queries beyond the ir minimum running time s, and get results with a greater confidence . Otherwise, we perform a query shedding as described shortly. DP3 (F AIRNESS ). Consider any two of them  X   X  and  X   X  minimum core running times  X   X  and  X   X  , respectively. Let their r e-spective actual core running times allocated by the system be  X  (1+ X   X  ) and  X   X  (1+ X   X  ) . Then, whenever possible,  X  only case where  X   X   X  X  X   X  is when the system has to allocate more time for one of the queries to fully utilize the cores ( due to DP2) . Example 3. Consider queries Q1 and Q2 in Fig. 1(a) . An interval marked by a curve indicates the lifetime of a query (e.g., Q1 X  X  start execution time is 0 and deadline is 2). Assume the system has two cores. First suppose Q1 X  X  essential time is  X  1 =2 and Q2 has  X  =1 . Then it is easy to verify that  X  1 = X  2 =1 ensures fairness and both cores are fully utilized between times 0 and 3 . Now s up-pose  X  1 =2.5 and  X  2 =0.5 , then  X  1 = X  2 =0.6 already saturates the interval [0, 2] since  X  1 ( 1+ X  1 ) =4 fully occupies the interval. However, the total work in the interval [0, 3] is only  X   X  ( 1+ X  2 ) =4.8 , which is less than the maximum capacity 6 . That is, the cores are not fully utilized in [2, 3] . We may have  X  =0.6 and  X  2 =3 to fully utilize the cores.

If all query deadlines can be met, we only extend the query running time (i.e.,  X   X 0 for core running time  X (1+ X ) ). O the r-wise, some query X  X  core running time must be reduced below  X  (called query shedding ), i.e.,  X 1 X  X  X  &lt;0 .
In the offline periodic optimization (PO), the system periodica l-ly optimizes a set of queries for determining the timing componen t of their execution plans. We break the whole timeline into inte r-vals of length  X  each. We call each interval an epoch . The system determines the execution timing of queries one epoch at a time. Before each epoch is a value  X  indicating its lead time ; all queries to be scheduled to run in this epoch must have a submission time at least  X  before the start of the epoch. Intuitively, this makes the scheduling easier as all queries in the epoch are known. For i n-stance, in Example 1, we may have  X =5  X  X  X  X  X  X  X  X  X  X  and  X  = 30  X  X  X  X  X  X  X  X  X  X  . Then , every 5 minutes (an epoch), the system sche d-ules all queries in that epoch 30 seconds before its start. This is illustrated in Fig. 1(b). Suppose the system has epoch boundaries 8:00, 8:05, 8:10 AM, etc. Alice needs to submit her query at least 30 seconds before 8:00 to have it executed between 8:00 and 8:05.
In online optimization (OO), however, the system does not o p-timize the queries periodically. A query X  X  submission time can be the same as its start execution time. The system optimizes the 
Fig. 2 (a) Three queries X  timing, (b) scheduling graph, (c) filtering. query impromptu without knowing future queries. Thus, OO i m-poses less constraint to users, but is harder for optimization. PO and OO belong to offline and online algorithms [1], respectively. An online algorithm is an algorithm that receives and processes the input in partial amounts, as opposed to an offline algorithm that receives its entire input at once [1].

In PO, a query X  X  lifetime may span two or more epochs; we simply break down its essential time proportionally and schedule the pieces in each epoch. The optimization itself is a job being scheduled, which should have little overhead compared to the major query processing. We maintain a job queue  X  . Each element in  X  is called a joblet . For OO, a joblet is a small piece of work based on a fixed period of core running time (we use 200ms). When a core is available, the system will dequeue the first joblet in  X  and assign it to the core. Thus, when a joblet is small, mult i-ple joblets of a query can be run on multiple cores at the same time. One novel aspect of our optimization is that we may dyna m-ically change  X  in real time before the joblets are being executed on cores  X  this is needed in online optimization . But once a joblet is dequeued and starts running, the system can not cancel it.
In PO, we are given a set of anytime queries in an epoch { X  1 , X  2 ,..., X   X  } , where  X   X  has start execution time  X  and essential time  X   X  ( 1 X  X  X   X  X  X  ), and the system has  X  cores. We optimize the queries X  timing for the desired properties.
 Definition 2 (anteceding, covering). We say that query  X  cedes query  X   X  if  X   X  &lt; X   X  &lt; X   X  &lt; X   X  . We say that  X   X   X  X  X   X  and  X   X   X  X  X   X  .

Anteceding and covering are the only two types of relationship between two queries that have overlapping lifetimes.
 Example 4. In Fig. 2(a), we have three queries Q1, Q2, and Q3. The essential time values are given in parentheses (e.g., Q1 X  X  is 0.2 sec). We have that Q1 antecedes Q2 (as 0&lt;1&lt;2&lt;3 ) while Q1 does not antecede Q3, and Q2 covers Q3.
 Mapping queries to a graph. We first map the query timing relationships to a special directed graph  X  , called a scheduling graph , which has two types of directed edges, type I and type II. Our optimization algorithm is on this graph. Each query  X  sponds to a vertex  X   X  . We say that  X   X  has size  X   X   X  . There is a type I (type II, resp.) edge from  X   X  to  X  cedes (covers, resp.)  X   X  . For a type I edge  X   X  X  X  from  X  say that the length of  X   X  X  X  , denoted by  X ( X   X  , X   X  ) , is  X  the overlap length between the lifetimes of  X   X  and  X   X  Example 5. Fig. 2(a) has a scheduling graph in Fig. 2(b) , where we use single arrow edges for type I edges and double -arrow for type II. The length of edge  X  1,2 ,  X  (  X  1 , X  2 ) =1 , i.e., the overlap length of Q1 and Q2 X  X  lifetime. Vertex  X  1 has size 2 and load 0.2. Definition 3 (closure). A vertex  X   X  X  closure (denoted by  X  set containing  X  and all vertices reachable from  X  through type II edges. The closure  X  + of a set of vertices  X  , is the union of the closures of all  X   X  X  X  . The load of  X  + , denoted by  X  X  X  X  X  X ( X  sum of the loads of the vertices in  X  + .

In Figure 2(b),  X  2 + ={ X  2 , X  3 } and {  X  1 , X  2 } + ={ X  Definition 4 (path ). A path to vertex  X  ( through type I edges ) in a scheduling graph  X  is a 4 -tuple ( X   X  X  X  X  X  X  X  , X   X  X  X  X  X  X  , X , X ) , where  X  the first vertex in the path,  X   X  X  X  X  X  X  is the previous vertex in the path Denote by  X  X  X ] the set of paths to  X  .

Intuitively, a path corresponds to a time interval enclosing one or more queries. The length  X  of a path corresponds to the size of the time interval multiplied by the number  X  of cores, while the load factor  X  is the total essential time in the interval divided by  X  . Thus,  X  is the total capacity (core running time) of an interval, while  X  is the fraction of  X  that is used to do the essential work.
The key ideas are as follows: We identify the most heavily loaded time interval and assign each  X   X  in that interval the same  X  , the smallest of all queries. When  X   X  &lt;0 , it is query shedding . The most heavily loaded interval is a path (type I edges) with the greatest  X  . In searching for such a path, our algorithm increme n-tally builds up the path information at each vertex by extending the paths at a parent vertex by one more edge. Once the  X  this path are fixed, repeatedly search for the most loaded path from the remaining vertices, until all vertices are fixed. Algorithm PO -MULTIQ ( X ) Input :  X  : a scheduling graph, whose vertex set is  X  Output : core running time for each query 1:  X  X  X  X  X  X  X  X  X  X  = X  //set of vertices whose  X  are fixed 2:  X  1  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X   X   X  X  X  X  X  X   X  X  X  X  X  X   X  X  X  X  X  X   X   X  X  X  X  X  X  X  3: while |  X  X  X  X  X  X  X  X  X  X  | &lt;| X  X  do 4:  X  max =0 //max load factor found so far 5: for each vertex  X  in  X  do / /  X  maps to a query interval 6:  X  X  X  X  X  X  X  =  X   X   X  (1+ X   X  )  X  7:  X  X  X  = X  X  X  X  X  X  X  X  X  (  X  )  X  X  X  X  X  X  X  X  X  //remaining capacity at  X  8:  X  =  X   X  X  X  X  X  X ( X   X  )  X  9: if  X  &gt; X  max then  X  max = X  10:  X  0 [  X  ]  X  (  X , X  X  X  X  X  X ,  X  X  X , X  ) //a path within  X   X  self path 11:  X  [  X  ]  X  X  X  0 [  X  ] } //the set of paths to  X  12: for each vertex  X  in topological order in  X  1 do 13: for each  X   X   X  X  X  X  X  X  X  X  X  X ( X ) do 14: for each ( X   X  , X   X  X  X  X  X  X  , X , X ) X  X  X [ X   X  ] do //a path to parent 15: ( X , X  X  X  X  X  X , X  0 , X  0 ) X  X  X  0 [ X ] //self path at  X  16:  X   X  = X + X  0  X  X  X  X  X  X ( X   X  , X ) //merge above 2 paths 19: if  X ( X   X  ,_ ,_ , X  X ) X  X  X [ X ] s.t.  X   X  &gt; X   X  then continue 20:  X  [  X  ]  X  X  X  [  X  ]  X  X ( X   X  , X   X  , X   X  , X   X  )} 21: if  X   X  &gt; X  max then  X  max = X   X  end if 22: let  X  be all vertices on the paths that have  X  23:  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  + and  X  X   X   X  X  X  + , X   X   X  1 24: return  X  X  X  X  X  X  X  X  X  X 
The algorithm is above. The set  X  X  X  X  X  X  X  X  X  X  contains the vertices that have  X   X  fix ed. In line 2, the subgraph  X  1 only contains type I edges. In each iteration (lines 3 -23 ), we identify the paths that have the greatest load factor (  X  max ) and fix the vertices on those paths. The loop in lines 5 -11 creates a path within each vertex (corresponding to each query X  X  life interval), which we call a self -path . In lines 12 -21 , we traverse the vertices in topological order. Incrementally, the paths at a vertex are calculated from those at its parents. Line 18 is a crucial filtering step to reduce the number of paths at each vertex (proven below). In line 19, if there is already a path with the same start vertex, we update it only if this path has a greater  X  . It is possible that multiple paths have the same start and end vertices, corresponding to the same time interval; the one with the greatest load factor includes all the queries therein. Example 6. Consider Fig 2. Lines 5 -11 produce three self -paths themselves in Fig.2(a). For example, such a path at  X  ( X  2 , X  X  X  X  X  X ,4,0.8) . This path is from  X  2 to itself; hence  X  The length of the path is computed in line 7, ( 3 X 1 )  X 2=4 . The incrementally compute longer paths across multiple vertices. For example, consider the path from  X  1 to  X  2 . It is calculated at  X  based on a path at its parent  X  1 (the self -path) and the self -path at  X  . The length of the new path  X   X  (line 16) is the sum of lengths of these two paths minus their overlap,  X  X  X  X ( X   X  , X ) , which gives 6 , and the load factor  X   X  (line 17) is 3.4/6 . This iteration produces  X  max =0.8 (the self -path at  X  2 ). Thus, vertices in  X  are decided in this iteration with  X  2 = X  3 =0. 25 (line 23 ). Q2 gets a core running time 2.4 X 1. 25=3 and Q3 gets 1 . Likewise, the second iteration has  X  max =0.1 ,  X  1 =9 , and Q1 gets a core time 2 . Overall, Q2 and Q3 get the same extension factor 0. 25 , and Q1 gets a higher factor 9 so that all cores are fully utilized. Theorems 1 and 2 show the correctness of the algorithm.
 Theorem 1. The filtering step in line 18 is correct: A path only if  X   X   X  X  X  max (1 X   X  0 found so far, and  X  0 is the length of the self -path at  X  . Proof.  X  is useful in the algorithm only if either  X   X   X  X  X  the prefix of a future path  X  with a load factor  X   X  is the maximum load factor at the time of discovering  X  ; thus  X  m  X  X  X  max . The first case is consistent with the theorem and so we only consider the latter case. We think of a path as a sequence of vertices (connected by type I edges). A prefix of a path is based on a prefix of this sequence. As illustrated in Fig. 2(c) (which depicts paths as time intervals), let  X  be the concatenation of  X  and a n-other path ( X  2 , X  2 ) (omitting the first two elements of the 4 -tuple) through  X  , with an overlap length  X  1 . First,  X   X  =  X   X   X   X  + X  addition,  X  1  X  X  X  0 . Replacing  X  1 with  X  0 gives us must also be true that  X  2  X  X  X  m . Replacing  X  2 with  X   X 
The filtering step in line 18 turns out to be very important. We show in the experiments (Sec. 4) that, without the filtering, the execution cost is prohibitively high. PO -MULTIQ allocates exec u-tion time for each query. The actual running schedule during the epoch is based on this simple algorithm: run the query that is alive and has the earliest deadline until it has exhausted its allocated execution time . We show that PO -MULTIQ combined with this simple algorithm satisfies the desired properties.
 Theorem 2. PO -MULTIQ combined with the query scheduling algorithm above satis fies DP1, DP2, and DP3.
 Proof. Consider DP1. If deadlines can all be met, the total load of any path that we discover does not exceed the length of the path. Thus,  X  max  X 1 . The layered extensions of core time at different iterations ensure that all deadlines are satisfied. In addition, the earliest deadline first scheduling ensures the following property: Given the workload of each query, if some deadline cannot be satisfied under such a scheduling, then some deadline must be violated under any scheduling. This can be shown by induction on the number of queries in deadline order. Thus, DP1 is satisfied.
For DP2, if there is a time interval when CPU is not fully ut i-lized, it corresponds to some path in the algorithm, and the alg o-rithm will extend the undecided queries in such a path (interval) so that the total core running time is the same as the capacity. For DP3, if two queries are extended in the same iteration (i.e., the while loop), they get the same extension ratio. The reason that a query is not extended in this iteration is because it will get a grea t-er ratio in a later iteration in order to fully utilize the CPU r e-source of the interval corresponding to that path.  X 
We say that the vertices (i.e., queries) that are decided in the  X   X  X h while loop are at constraint level  X  . Thus, queries can be part i-tioned based on their constraint levels. Each path in a constraint level corresponds to a constraint interval , which is the minimal time interval that covers all queries in the vertices of that path. Hence, queries that are at the same constraint level in a constraint interval have the same  X   X  ratio.
In OO, the system has no knowledge on what future queries may arrive and interfere with the existing ones being optimized . Thus, OO is harder. Our algorithm is based on events, seeking to minimize the overhead, as it does not use any resources while waiting. Two interesting events are: (E1) a new query arrives, and (E2) the joblet queue  X  is empty. When E1 occurs, we make sure that all essential parts of the current queries are arranged in dea d-line order at the head of  X  . When E2 occurs, we allocate the bonus time for current queries. The algorithm is shown below.
 Algorithm OO -MULTIQ 1: while true do 2: wait to be notified for these two events: 3: E1: a new query  X   X  arrives (with lifetime length  X  4: create joblets  X   X  based on  X   X   X  X  essential part 5: insert  X   X  into  X  in query deadline order 6: while some deadline cannot be met do 7: drop a joblet from  X  uniformly at random 8: E2:  X  is empty do 9: let  X   X   X   X   X  10: choose  X   X  from the queries alive using  X  11: create a joblet  X   X  for  X   X  12: append  X   X  to the end of  X 
In line 6, we can easily determine if all current query deadlines can be met, considering all the essential work before each dea d-line. In line 9, we randomly choose a query based on a distrib u-tion, where  X   X  is proportional to  X   X  query has a probability to be selected for allocating bonus time. Recall that  X  is the number of cores and  X   X  ,  X   X  are the lifetime and essential time of  X   X  , respectively. We now analyze the algorithm. Definition 5 (EFDO ). An optimization algorithm belongs to the category of EFDO ( essential time first deadline order ), if it always schedules to run the essential parts of all current queries first in the order of their deadlines. In particular, if the system is running the essential part of  X  1 when a new query  X  2 comes in with a deadline earlier than that of  X  1 , then the system will pause the execution of  X  1 and run the essential part of  X  2 .
 Line 5 shows that OO -MULTIQ is in EFDO.
 Theorem 3. An EFDO optimization algorithm achieves DP1.
 Proof. If an optimization algorithm is in EFDO, then reorder any incoming sequence of queries into  X  1 , X  2 ,... in the order of dea d-lines. Specifically, for  X   X  , its execution of the essential part is at the earliest possible time, only subject to the work on the essential parts of  X  1 ,..., X   X  X  X 1 (to meet their own deadlines). However, those queries have earlier deadlines than  X   X  and that work must be done before  X   X   X  X  deadline; otherwise one of those deadlines would be broken. Thus, if the query deadlines can all be met, then the E F-DO algorithm will always satisfy the deadlines, achieving DP1.  X 
OO -MULTIQ and PO -MULTIQ are online and offline alg o-rithms, respectively [1]. A classical way to study the performance of an online algorithm is through competitive ratio [1], which is the worst -case analysis of the ratio between the  X  X ost X  of the online algorithm and that of the offline algorithm. However, this approach has raised a number of issues [13, 1, 7] in terms of its practicality and usability. Instead, average -case analysis has been advocated [13, 7 ]. An average -case analysis is based on a rando m-ization process of the input to an adversary who tries to create a  X  X ifficult X  input for the algorithm, while, in a worst -case analysis, the adversary has full power and can choose any deterministic input. For example, in the average -case analysis of quick sort , the adversary can only choose input elements uniformly at random (the randomization process), while he can choose any determini s-tic order for the worst -case analysis.
 Definition 6 (average -case optimal). We say that an online alg o-rithm  X  is average -case optimal w.r.t. a randomization process  X  if the adversary uses  X  to choose the input to  X  and the expected performance of  X  is the same as the optimal offline algorithm. Theorem 4. OO -MULTIQ is average -case optimal w.r.t. the fo l-lowing randomization process  X  : (1) The adversary picks a set of queries of his own choice. Each query has an essential time and a lifetime length. (2) The adversary chooses a constraint interval (defined at the end of Sec. 3.1) for each query in the above step. (3) Each query in a constraint interval has a start execution time chosen uniformly at random from that interval.
 Proof. First, when all deadlines can be met, the algorithm creates bonus joblets. In the offline algorithm a query X  X  bonus time is proportional to  X   X  in each constraint interval (as queries in a co n-straint interval get the same  X  ). Consider each constraint interval separately. We will show that a query X  X  expected bonus time is also proportional to  X   X  . For a joblet created in line 11 at time  X  , let its constraint interval length be  X   X  . Then for any  X   X  straint interval: Pr [  X   X   X  X  X   X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X   X  X  X  X  X  X  X   X  X  X   X  ] = cause queries arrive uniformly at random in the interval and  X  is its running time over  X  cores (EFDO). Thus, it is the probability that  X   X  is alive at time  X  and has finished its essential part. Then from line 9 , Pr [  X   X   X  X  X  X  X  X   X  X  X  X   X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  ]  X   X   X   X  where  X   X  is any query in the constraint interval. Therefore, the expected bonus time that a query gets is also proportional to  X  some deadlines cannot be met, line 7 drops a joblet in the queue uniformly at random. As each joblet has the same size, the number of joblets that any query  X   X  has is proportional to  X  that the expected reduction is also proportional to  X   X  .  X 
Note that we break down an adversary X  X  power into three steps in  X  of Theorem 4. The adversary has full power in steps (1) and (2), and only step (3) is randomized. We perform our experiments using the following datasets: The bank machine status dataset downloaded from VAST Challenge 2012 [16]. The dataset (7.42 GB) contains time series streams of the health status of bank machines. Its attributes i n-clude ipAddr , healthtime , numConnections , policyStatus , activit y-Flag . Another small table contains the metadata of the machines.
The weather forecast dataset obtained from the National Di g-ital Forecast Database [17 ]. It is a continuous five -hour forecast (UT 17:00 to 21:00 on 4 -21 -2012 ) of the precipitation, cloud co v-er and wind speed at the Pacific North West area at UT 00:00 on 4 -22 -2012. The dataset contains 18% invalid data.

Both datasets have a significant degree of uncertainty, which comes from invalid data entries and missing data (location and time). For the first dataset, the missing or invalid health status of a machine is assigned the distribution of that within its bank branch or data center. Similarly, for the second dataset, an invalid/missing value is assigned the distribution of its local area (nearest 10 data points). In addition, for a missing value at a time , we use linear interpolation to estimate its distribution from its neighborhood.
We implement in Java with package java.util.concurrent , and use the ThreadPoolExecutor class to create a thread pool with a number of threads equal to the number of available cores. A thread pool has a task queue. In OO, we may need to cancel a scheduled joblet or insert joblets in the middle of the queue (Sec 2.2). We have one (global) bit flag for each joblet, which is marked 1 when the joblet is canceled. The core that is assigned a joblet first checks this bit flag and does nothing if it is set. Insertions into the queue can be achieved by cancellations and then appending to the queue. In addition, for OO, the main thread listens to events E1 and E2. We use Java X  X   X  X  X  X  X  X () and  X  X  X  X  X  X  X  X  X () . We have a low prio r-ity daemon thread that monitors the joblet queue size. All exper i-ments are performed on a machine with an Intel Core i7 processor (8 cores under hyperthreading) and an 8GB memory.
We first study PO. With the bank machine status dataset, a bank customer at location (a, b) may be interested in querying  X  the ATM machines within 2 miles that are lightly loaded (a short wait time) and have normal activities with probability at least 0.8  X . Such an SQL query (without the probability part) is: where numConnections and activityFlag are uncertain attributes as discussed earlier. Two other queries are:
Q2 may be queried by a customer or a bank administrator for workload status of various branches in regions R20 and R21, while a security person may be interested in Q3 for machines that have many invalid login attempts ( activityFlag = 3 ). For the weather forecast dataset, we ask these queries: where tomorrowWeather is a forecast stream of tomorrow X  s weather (at various locations) from the evolving current time. A planner of weather -sensitive events (e.g., outdoor weddings or tennis games) will be interested in a query like Q4. For these qu e-ries, we use the MCA to estimate the probability of a result tuple or the histogram distribution of an uncertain attribute therein.
Recall that at an epoch X  X  lead time, PO estimates each query X  X  one -round cost and run PO -MULTIQ . We verify that this over-head is small enough . Clearly, the optimization time is affected by the number of queries in an epoch, which is in turn determined by two parameters: epoch length and query rate . Our experimental system handles two datasets (queries Q1 -Q5) at the same time. For PO, we generate queries (Q1 -Q5) randomly for an epoch based on the query incoming rate. Moreover, we set a query X  X  essential time to be 100 rounds in MCA based on the cost estim a-tion, and its lifetime to be a random value between one and six seconds. In addition, the number of repeated answers of each qu e-ry (Definition 1) is randomly chosen from 1 to 3.

In Fig. 3 we show the execution times of total optimization u n-der different epoch length s, and the part of query cost estimation alone . We also evaluate the significance of the filtering step in line 18 of PO -MULTIQ . We see that the total optimization time is insignificant compared to epoch length. For instance, when an epoch is 300 sec, the total optimization time is only slightly over one second. The query cost estimation time is again insignificant as we only run each query for one round. However, the running time without the filtering step becomes much longer and intoler a-ble. When epoch length is 300 sec, the running time exceeds 10 6  X  X  X  and we do not draw it in the figure. This demonstrates the crucial role of the filtering step. In Fig. 4, we fix the epoch length to be 300 sec but vary the query rate, and run the same exper i-ments. As we increase the query rate, the optimization time also increases, and once again, the filtering step is very effective.
With a schedule resulted from PO -MULTIQ, we kick off the query runs. We measure how many rounds each query actually gets to run, and compare it with the planned number from PO -MULTIQ. Any difference reflects query cost estimation errors and execution overheads (e.g., distributing jobs to cores). We show this deviation ratio (actual number of rounds vs. the estimated number) over all queries in the epoch in Fig. 5 for various query rates while fixing the epoch length at 60 sec . It shows the average difference ratio  X  standard deviation, in the form of error bars. We see that, while there are some variations among queries, the actual runs are in general consistent with the query execution plan.
We now look at the OO -MULTIQ algorithm, where we use  X  X  X  X  X  X () and  X  X  X  X  X  X  X  X  X () for events . We first examine the overhead of this optimization by calculating the ratio between the actual CPU time of optimization and that of executing queries. This is shown in Fig. 6 for different query incoming rates . We see that the ove r-head ratio is negligible, which is very important for online optim i-zation. This is attributed to the simple and efficient randomized algorithm by design, and the event based optimization.
Processing probabilistic data has attracted much attention with research projects such as Trio [4], MystiQ [6], MCDB [11], CLARO [14]. There has been work in real -time databases and systems that investigates resource scheduling and allocation, e.g., [3, 2, 10]. We discuss two lines of work that are most relevant . Bar -Noy et al. [3] provide a high level algorithm that solves a few scheduling problems by setting different parameter values. Unfo r-tunately, it does not apply to our problem. In particular, we have a desired property that all deadlines must be met if possible (DP1) . The algorithm in [3] does not satisfy DP1, but only maximizes a profit function. Consequently, we devise fundamentally different techniques than [3]. The work in [2 , 10 ] is called reward -based scheduling. They have different problem settings or assumptions than ours; they do not consider fairness or query shedding . As a result, our algorithms require different techniques. [2, 10] do not perform competitive analysis. Finally, [2] assumes that all jobs start at the same time and all deadlines can be met. By contrast, in our work, queries can come in and start at different times.
In this paper, we study how to perform query optimization on real -time anytime queries using multicores, where the query exe-cution plan is extended with a timing component. We propose realistic optimization modes, devise efficient offline and online query optimization algorithms, and analyze them. Our systematic experimental evaluations verify our approaches and analyses.
Acknowledgments. The first three authors were supported by the NSF grants IIS-1149417 and IIS -1239176. Jie Wang was su p-ported by the NSF grants CNS-1018422 and CNS-1247875.
