 Cheng Li 1( Cancer kills. It is one of leading causes of morbidity and mortality worldwide. In 2012 alone there were 14 million new cases and 8.2 million cancer-related deaths [ 1 ]. Effective treatment and care remains a dominant health concern. One expects to get better when treated. A crucial problem is that can-cer treatments themselves make you sick. They cause severe adverse events, called toxicities, such as anemia and neutropenia. These toxicities greatly impair patient care and must be reduced in clinical. But oncologists are unable to pre-patient reactions to treatments is still poorly understood. Any predictive system treatments, thus improving patient care.
 decision support to oncologists. The data is particularly interesting. A patient identified with cancer undergoes several bouts of treatments that can include both radiotherapy and chemotherapy (see Fig. 1 ). The problem is to predict toxicities continually -fortnightly, after the first treatment. The data before the prediction point can be used to predict future toxicities. This data differs from traditional time series data in the two ways: toxicity can be caused by one treatment on a given day, but treatments on other days may not cause adverse effects. Thus the toxicity outcome is not correlated with all the inputs. Further, if we aggregate all data in between prediction points, the effect of the singular data vector causing toxicity will be diminished. Thus a new approach is required for this data to solve this important problem.
 through a novel approach integrating the multi-instance learning into a multi-task learning framework. In the multi-instance learning, each data point is a bag ments and patient-related variables (see Table 1 ). A bag has several instances, but only one label -it is positive if any instance in the bag is positive and negative, otherwise. The multi-instance formulation models the impact of daily treatments on toxicity. The multi-task part of our framework induces a sharing of model parameters across different prediction points, exploiting their statisti-cal similarities. This is realized by modeling these multiple model parameters through a common subspace, which can be obtained by combining the Indian Buffet Process (IBP) prior and factor analysis. The proposed model is called the factorial multi-task learning with multiple instance learning (FMT-MIL). We derive an efficient Gibbs sampling for this model. We evaluate our model on a synthetic and a real-world cancer dataset of more than 2000 patients. The experimental results show that our FMT-MIL outperforms the baselines -the individual multi-instance algorithm [ 2 ] and the simple multi-task use of multi-instance learning [ 3 ]. Our contributions are:  X  The first formulation of toxicity prediction in cancer through a multi-instance framework. The method captures the heterogeneity of daily treatments to model the data uniqueness.  X  The proposal of a multi-task framework for toxicity prediction across pre-diction points. Our model can automatically learn the number of factors and transfer knowledge across prediction points through a common subspace shared by predictors;  X  The validation on a real-world dataset. Our model improves the prediction accuracy as much as 10 %, which validates the use of our approach for toxicity prediction. 2.1 Toxicity Prediction Toxicity prediction provides treatment-decision support for oncologists. How-ever, toxicity prediction is a difficult task since toxicities are determined by complex interactions between treatment and patient-related variables. For exam-ple, older adults more likely suffer from toxicity from cytotoxic treatments than younger patients due to the reduction of organ functionality [ 4 ]. In addition, oncologists may consider a patient X  X  ability to tolerate particular treatments or the effect of previous treatments to adjust treatment schemes, which lead to increasing difficulties in toxicity prediction.
 Research has performed toxicity prediction using statistical methods [ 5 , 6 ]. For example, Hurria et al. [ 5 ] identified risk factors of chemotherapy toxicity based on p -value and then applied a multivariate logistic regression model that incorporates identified factors to compute the probability of toxicity occurring. Kimetal.[ 6 ] studied factors in a logistic regression which cause radiation pneu-monitis in lung cancer. However, these methods are not able to capture the interactions between factors and hence they perform limited predictive power in clinical practice [ 7 ].
 Some studies [ 7  X  9 ] use machine learning and data mining approaches for tox-to predict biological outcomes by learning the relationship between treatments and effects. EI Naqa et al. [ 7 ] proposed a modified SVM kernel method to model the nonlinear relationship between factors, which can be generalized to unseen data. Pella et al. [ 9 ] implemented large scale optimization methods and tradi-consider the impact of daily treatments on toxicity. Our proposed approach not only captures the heterogeneity of daily treatments but also performs toxicity prediction at different prediction points. 2.2 The Multi-instance Learning In a standard logistic regression classifier, each data point is represented by a feature vector and has a label. The posterior probability of the label is modeled using a sigmoid function, that is, p ( y =1 | x )=  X  (  X  T 1  X  p ( y =0 | x )=1  X   X  (  X  T x ), where  X  ( z )=1 / (1 + e and  X  is a classifier weight vector. Multiple instance learning (MIL) assumes that each data point can be represented by a bag of feature vectors (or instances). A bag has only one label which means that the instances in a bag share a label. Note that instances often do not have explicit labels [ 10 ].
 R and hence the posterior probability of a negative bag is defined as p ( y x is p ( y i =1 | x i )=1  X  p ( y i =0 | x i )=1  X  M i j =1 every bag consists of only one instance, the MIL will be reduced to a standard logistic classifier.  X  can be estimated by maximizing the likelihood using the AnyBoost framework [ 11 ] or the MAP framework [ 3 ].
 used in drug activity detection [ 2 ]. Later MIL has been widely used in computer version of the MIL has been proposed in [ 3 ]. We employ MIL to model the generation of toxicity with the assumption that a treatment interval is a bag, wherein each instance is a feature vector constructed from daily treatments and patient-specific attributes. 2.3 Multi-task Learning Using Nonparametric Factor Analysis Multi-task learning jointly models multiple tasks to improve the performance of some tasks using the knowledge from other tasks. A lot of experimental work have shown the advantages of the multi-task learning compared to the single task learning [ 13 , 14 ].
 where task predictors are assumed to lie in a subspace and can be decomposed as where  X  N  X  D is a matrix consisting of N data points lying in D -dimensional formed subspace; F N  X  K is the representation matrix of  X   X  and E N  X  D is the offset error. We further decompose F so that h N  X  K is the actual subspace representation of  X  in  X  and z binary matrix indicating the presence or absence of the factors. The number of factors K may vary from applications to applications. In this model, the K can be inferred automatically due to the introduction of the Indian buffet process (IBP) [ 17 ] as a nonparametric prior.
 over a binary matrix z N  X  K . The binary matrix consists of a finite row and an unbounded number of columns. Each row can be interpreted as an object and each column can be taken as a feature. For each feature k ( k =1 , let  X  k be a prior probability over the presence of the feature k in an object i (which we denote as z ik ). A prior Beta(  X  K , 1) is further placed on  X  the strength parameter of the IBP. The generative model of z is described as:  X   X  Beta(  X  K , 1) ,z ik |  X  k  X  Bernoulli(  X  k ).
 The stick-breaking IBP construction has been proposed in [ 17 ]. Specifi-cally, let  X  = {  X  (1) , X  (2) ,  X  X  X  , X  ( K ) } be a decreasing order of Teh et al. [ 17 ] shows the following construction when K  X   X  stickIBP(  X  ). The densities of the subsequent  X  ( k )  X  X  have been derived as where I ( Q ) equals to 1 if Q is true and 0, otherwise. Note that  X  Markov structure, with  X  ( k +1) only conditionally dependent on its previous one  X  . The conditional probability of z ik is presented p ( z ik =1 | z  X  ik , X , x )  X  p ( z ik =1 | z  X  ik , X  ) p ( x where z  X  ik = z \ z ik denotes others excluding z ik ;the  X  length of the stick for the last active feature [ 17 ]. We have proposed a similar factorial multi-task learning framework that has incorporated the MIL paradigm. Our model differs from the one in [ 3 ], which simply assumes that all weight vectors share the same Gaussian prior and thus cannot capture the dynamic process between tasks. 3.1 Model Description In this section, we propose a model incorporating the multi-instance learning into a multi-task framework. Our goal is to improve the prediction performance of different tasks. In our model, the multi-instance learning is utilized to model the classification problem in which each data point consists of a bag of feature vectors. We then perform the multi-task learning with the assumption that all task predictors lie in a subspace. The subspace can be modeled by combining the IBP prior and factor analysis. We refer our model as the factorial multi-task learning with the multi-instance learning (FMT-MIL).
 data consist of N t bags D t = { ( x ti ,y ti ) } N t i =1 as  X  t . As discussed in Sect. 2.3 , we handle  X  by defining  X  h matrix indicating the presence or absence of the factors; h = subspace representation of  X  in  X  ;and K is number of factors. We introduce the the FMT-MIL model is shown in Fig. 2 and its generative process is described as where  X  is the strength parameter of the IBP; N is a Gaussian distribution; and  X  and can be specified by where  X  is the sigmoid function and M ti is the number of instances in bag i of task t .
 is a task. Each instance is a feature vector constructed from daily treatments and patient-specific attributes. The label is positive if the toxicities have been searched in 28 days after the prediction point and negative, otherwise. We aim to obtain the classifier weight vector  X  to predict toxicity for unseen patients. 3.2 Model Inference The closed form of the proposed model is intractable. We use the Gibbs sampling for the model inference. In our model, we need to update the main latent variables {  X  , z , h ,  X  } . Sampling  X  . The  X  can be sampled independently for each task. We sample  X  from the following conditional distribution p (  X  | h where R t = ||  X  t  X  ( z t h t )  X  || 2 2 and s ti = p ti is intractable as it cannot be reduced to a standard distribution. We employ the Laplace approximation to estimate the posterior of  X  t imation aims to find the mode of the posterior distribution and fits a Gaussian whose mean lies at the mode. The mode of the posterior can be computed by maximizing the posterior distribution p (  X  t |  X  X  X  ), i.e. , use the Newton-Raphson update in our implementation to obtain the optimiza-tion solution (denoted as  X  Laplace t ). The co-variance matrix of the approximate Gaussian is obtained by computing the negative of the inverse of the Hessian of the log posterior, i.e. , X  Laplace  X  Therefore, the new  X  t could be sampled from N (  X  Laplace and the second derivatives of log posterior is computed as following where  X  ti =(1  X  p ti ) /p ti and N t is the number of data points in the task t . Sampling z . The z is a T  X  K binary matrix indicating which subspace bases are used to generate the task predictors. z tk can be sampled from p ( z be obtained from the part of Eq. ( 3 ). The second term in the right hand is the likelihood of  X  t given all factors and other variables. Finally, p ( z is a Bernoulli distribution.
 Sampling  X  . The variable  X  is sampled from the following conditional distrib-ution From the Eq. ( 5 )andEq.( 6 ), we know that the above conditional distribution is a normal distribution. We further expand it and derive it as p (  X  N The detailed derivation is provided in Appendix.
 Sampling h . The posterior of h can be sampled using the conditional distrib-ution below Similar to the sampling of  X  , we derive the posterior as a normal distribution p ( h t |  X  X  X  )  X  X  (  X  h Sampling Hyperparamters. We can assume that  X  h is sampled from a Gamma distribution. Due to the conjugate property of the Gamma distribution and the Gaussian distribution, the conditional probability of  X  gamma distribution, which is tractable. The updates of other hyperparameters  X  t and  X   X  are similar to  X  h . 4.1 Synthetic Data  X  ) is generated following Eq. ( 5 ). We also generate 100 data points in each task and each data point consists of 1  X  4 instances, which are randomly sampled from a standard Gaussian distribution with the dimensionality of 10. The labels of data points then could be generated by using Eqs. ( 7 ) and ( 8 ). We perform our experiment comparison for the following algorithms:  X  FMT-MIL. The proposed factorial multi-task learning with multi-instance learning.  X  MT-MIL. The multi-task learning joint with the Bayesian multiple instance learning without feature selection [ 3 ]. All predictors share a single prior.  X  MIL. The Bayesian multiple instance learning without feature selection [ 3 ]. Tasks are independent.
 In our algorithm running, we initialized all the hyperparameters to 1. We randomly split the data into training and test sets such that each of them contains roughly half of the entire data set. We run 1000 Gibbs iteration and show the convergence in Fig. 3 (a, b). Our algorithm can converge during 200 iterations and return the true number of factors K during 1000 iterations. The AUC from the test data (seen in Fig. 3 (c)) shows that our algorithm outperforms the baselines. 4.2 Real Data Description hospital. The data are in form of EMRs (electronic medical records), which include diagnosis codes, demographic information and treatment procedures. We use all patients who have been given both chemotherapy and radiotherapy since their toxicity information is comparatively rich (seen in Fig. 5 ). Table 1 shows ing toxicity predictors. We combine the patient-specific diagnosis, demographic and treatment properties as the feature sources. We only focus on the toxicities specified by ICD-10 codes [ 18 ]. The blood diseases, nervous system and digestive disorders are usually caused by chemotherapy and the disorders of the skin and subcutaneous tissue are often caused by radiotherapy.
 We further illustrate how to construct features in Fig. 4 . The prediction points are placed in every fortnight after the first treatment. We use the data before the prediction point, including basic and treatment data, to construct a bag of instances. An instance is a feature vector which is extracted in between cancer been given in the day, we skip this day. Thus, for a patient, there are a max-imum of 14 instances in a bag. The bag is labeled positively if toxicities have been searched in toxicity horizon (28 days in our experiments) and is labeled negatively otherwise. We also extract a feature vector at each prediction point used for a standard logistic regression (LR) classifier. The LR algorithm does not consider the influence of daily treatments on toxicity.
 patients is decreasing along the prediction points as patients discharge after ratios are relatively high since oncologists may adopt intensive treatments at decrease obviously. Possible reasons are that oncologists adjust treatments to patient care or patients have adapted the treatments. The ratios remains stable after the tenth prediction point. We perform our experiments on the first 10 prediction points. 4.3 Experiment Setting and Results The goal of our experiments is to predict toxicities for unseen patients using the proposed model. We split the dataset into the training and test data based on patient IDs. About 1300 patients who are included in all prediction points are used for the training data. Other patients in prediction points are used for the test data. We initialized the hyperparameters {  X ,  X  h , X  Gibbs iterations to report the results.
 We use different proportions of patients in the training data to evaluate our FMT-MIL and the baselines. The average AUC of the test data are shown in Fig. 6 . As the number of the training patients increases, the performance of all models rises. Moreover, our model performs much better than the baselines when a small number of training data is given. For example, when 30 % patients of the training data are used for training the algorithms, the AUC of our model increases by as much as 10 % compared to the MT-MIL, 15 % compared to the MIL and 30 % compared to the LR. This is because our model joints the multi-instance learning and the muli-task learning, enhancing the generalized performance of individual tasks. It is noted that the LR performs worse than the MIL, which validates our use of the MIL for toxicity prediction. 70 % of the training data. Our model can converge at about 200 iterations (seen in Fig. 7 (a)). The number of factors computed from our model centers at 2 (seen best in all cases. The AUC gradually increases between the 1 and then begins to decrease thereafter. The trend is significant in the MIL algo-rithm since the positive samples reduce dramatically around the 9 points (seen in Fig. 5 (b)). Individual tasks in the MIL algorithm cannot share knowledge while tasks in the FMT-MIL and the MT-MIL algorithms share the common subspace. This has validated our use of the multi-task learning for the multi-point prediction. We have proposed a multi-task framework incorporating the multiple instance learning for toxicity prediction. The prediction points occur fortnightly after the first treatment. We treat daily treatments before the prediction points and patient-specific attributes to be multi-instance data. We further combine all prediction points to perform a multi-task framework. The factors in a shared subspace can be inferred automatically. We derive an efficient Gibbs sampling for our model. We evaluate our model on a synthetic dataset and a real-world dataset. The experiment results show that our model outperforms the state of the art, with the prediction accuracy increased by as much as 10 %. approach can handle toxicity prediction perfectly. This paper has explored the problem using a machine learning approach. Our approach not only captures the influence of daily treatments but also performs the multi-point prediction simul-taneously. Our current data do not include the pathology test and physician X  X  assessments, which may possibly improve the prediction accuracy if available.
