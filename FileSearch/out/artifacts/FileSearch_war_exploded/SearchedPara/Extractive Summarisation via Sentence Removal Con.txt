 Many on-line services allow users to describe their opinions about a product or a service through a review. In order to help other users to fi nd out the major opinion about a given topic, without the effort to read several reviews, multi-document summarisation is required. This research proposes an approach for extractive summarisation, supporting different scoring techniques, such as cosine similarity or divergence, as a method for fi nding representative sentences. The main contribution of this paper is the de fi nition of an algorithm for sentence removal, developed to maximise the score between the summary and the original document. Instead of ranking the sen-tences and selecting the most important ones, the algorithm itera-tively removes unimportant sentences until a desired compression rate is reached. Experimental results show that variations of the sentence removal algorithm provide good performance.
 H.3.m [ Information Search and Retrieval ]: Miscellaneous Algorithms, Design, Experimentation Opinion Summarisation, Sentence Removal, Divergence The expansion of the Web has provided an increasing number of social media sources such as blogs, discussion forums and other services, where users can express their opinions about products, companies or people. Finding out what other people think has al-ways been an important part of our decision-making process [11]. For example, customers can exploit opinion-oriented information before buying a product. In order for this information to be effec-tive and not overwhelming, intelligent sentiment-aware tools are needed.
 Automatic document summarisation is an important task which provides an effective access to information. It has been extensively tence location. A weighted combination of these features is used to select the sentences to extract.
 Instead of using subjective weights to combine features, a corpus can be used to train a classi fi er. With this approach, the summarisa-tion task has been faced as a sentence classi fi cation task. A Naive-Bayes classi fi er has been used to determine whether a sentence has to be selected to generate the summary [7].
 Other approaches based on word statistics include the use of TF-IDF weighting, which is commonly incorporated in most of the current summarisation systems [10], or the application of the log-likelihood ratio test for the identi fi cation of highly descrip-tive words, commonly referred to as topic signatures. The log-likelihood approach has been shown to be particularly effective in the task of multi-document summarisation of news [3]. Previous work in summarisation of opinionated documents has been focusing on different domains of user-generated content. The idea of a single sentence extraction, to determine the polarity of the whole document, has been suggested in [1], although results on the polarity classi fi cation task have not been reported.
 Dealing with short web comments, the task of extracting the top sentiment keywords has been faced exploiting Pointwise Mutual Information, and the result can be presented in a tag cloud [12]. Feature-based Sentiment Analysis extracts the different aspects of a topic, providing polarity information about it. Aggregating per-aspect sentiments easily leads to the generation feature-based sum-maries, but previous works in this direction have mainly focused on the aspect identi fi cation and sentiment classi fi cation tasks [6, 2]. In this section the task of extractive summarisation is formalised. A collection T has a number of topics &lt;t 1 , ..., t m &gt; . A topic t is composed by a number of documents &lt;d 1 , ..., d k &gt; , each of which is composed by a number of sentences. A topic is hence composed by all the sentences &lt;s 1 , ..., s n &gt; belonging to the k documents. The case k =1 is called single-document summarisa-tion, while k&gt; 1 represents multi-document summarisation. The task of extractive summarisation is to select the subset of sentences and to combine them into a summary which better represents the chosen score. The brute-force approach consists in enumerating all the combinations of l sentences (desired output length) out of the n forming the given topic. The concatenation of the l sentences with the highest score will form the summary. This is different from selecting one sentence at a time, as the language model for the con-catenation will be different from the language model for the indi-vidual sentences. The two baselines implementing the brute-force approach adopting the scores as in Equations 5 and 7 are referred to as, respectively, BF SIM and BF DIV in the experiments. This section describes the proposed approach to extractive sum-marisation via a Sentence Removal ( SR ) algorithm. Instead of se-lecting important sentence, the idea behind this technique is based on removing iteratively the less important ones, until the desired output size is reached. With this method, the algorithm tries to maximise the importance of the candidate summary as a whole, and does not only focus on the importance of a single sentence. In other words, the purpose is to condense the information in the orig-inal source while trying to ensuring its coverage in the summary at the same time. Algorithm 1 shows the procedure to obtain the can-didate summary. The procedure starts with the candidate summary t containing all the original set of sentences, and then iterates un-til the summary reaches the desired length l . During each iteration, the procedure removes one sentence such that the score between the candidate summary and the original set of sentences is maximised. The score in line 6 can be computed using again Equations 5 or 7 In the results section, the systems implementing the SR algorithm as in Algorithm 1 are denoted with SR SIM and SR DIV depending on the scoring function.
 Algorithm 1 Sentence Removal algorithm.
 Input: t {topic to summarise} Input: l {output size in n. of sentences} 1: t  X  t 2: while | t | &gt;l do 3: for all s i in t do 4: t i  X  t \ s i 5: end for 6: t  X  arg max 7: end while 8: return t A different version of the sentence removal algorithm can be ob-tained with a variation in the way the candidate summary, at each iteration step, is selected. Rather than computing the score between the candidate summary and the original set of sentences, one can compute the score between the candidate summary t i , and the can-didate summary at the previous iteration t . In this case, line 6 of the procedure has to be replaced with: The systems implementing this variation of the algorithm are la-belled as SR SIM and SR DIV . This version of the algorithm could also be exploited to shift the topic of the summary towards a par-ticular sub-topic, e.g. via a topic query. An experimental study has been performed using a dataset suitable for multi-document summarisation. The dataset provides a number a number of sentences (min. 50, max. 575, avg. 139), taken from different reviews from popular review web sites. For each topic, 4 or 5 golden standard (human-written) summaries are provided. The data are not labelled according to their sentiment, and different opinions can be expressed. The golden standard summaries hence present the pivot opinion for each topic, in a concise way (approx. 2 sentences each). For this reason, the maximum length of the system generated summaries is fi xed to two sentences. Another character-istic of the data is to be highly redundant, e.g. the topic itself is often repeated in different sentences.
 The ROUGE framework [9] is used to provide a quantitative assess-ment between the candidate summaries and the golden standards. Speci fi cally, the results for ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported. In order to achieve better correlation with hu-man judgement, multiple golden standard summaries are used [8]. Given the brevity of the summaries, capturing all the relevant in-formation is particularly challenging. Recall is hence particularly important, i.e. it is desirable to show all the relevant opinions in the summaries, yet maintaining their succinct nature. For this reason, the results for F 2 -scores, which emphasise the importance of recall over precision, are also reported. On top of the baselines described in Section 3.1, this study also reports the results for MEAD [13], a state-of-the-art extractive summariser based on cluster centroids. Table 1 reports the results of the experiments on the Opinosis data set. The MEAD baseline is extremely competitive with respect to recall, but shows a drop of performance on the precision side. Overall, the results show that there is not an individual approach which clearly outperforms all the others in every metric. In general, the variation of the sentence removal algorithm, SR , is outper-formed by the original de fi nition of SR , for both the cosine and the divergence-based settings. The SR algorithm consistently achieves the best recall results within the same scoring function groups when compared to greedy or brute-force approaches. Observing the cosine-based systems, the brute-force approach shows the best F -scores, but its results are not substantially better than the SR algorithm. They both outperform the greedy baseline and MEAD. On the F 2 -scores side, SR is substantially better than any other system, including the divergence-based ones and MEAD, with the second-best results being outside a 95% con fi dence interval. On the divergence-based side, the brute-force approach achieves the best F -scores due to good performance in precision. Its F 2 -scores are slightly better than the SR ones for ROUGE-2 and ROUGE-SU4, while SR achieves the best ROUGE-1 score for the divergence-based systems. This paper discussed the use of a summariser as a component of a two-stage system, which shows opinion-oriented summaries as a result of an opinion-oriented query. Such a summariser is meant to take a set of relevant sentences and condense them into a short summary. The main contribution is the introduction of a novel al-gorithm for extractive summarisation based on Sentence Removal (
SR ). The key idea is to remove unimportant sentences iteratively, until a desired output length is reached, rather then selecting di-rectly the important ones. In order to de fi ne the importance of a sentence, a scoring function can be applied. In this work, scores based on cosine similarity and on divergence have been investi-gated. An experimental study has compared the performance of the SR algorithm against MEAD, a state-of-the-art summariser, and against two baselines, namely greedy and brute-force approaches which adopt the same scoring function as the SR algorithm. Re-
