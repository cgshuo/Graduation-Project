 Data mining tasks usually work on large data warehouses where data often comes from multiple sources. The quality of mining results largely depends on the qual-ity of data. One problem that degrades the data quality is the duplicated data records among the sources. Duplicate detection/elimination then is an essential preprocessing for data mining tasks an d different methods have been proposed to deal with this problem [1,2,3,4]. The main idea of these methods is to use certain metrics to det ermine if certain pairs of data records are similar enough to be duplicates. In these methods, each data record is mostly of one same type and exists as an independent instance during the duplicate detecting process.
On the other hand, relational data is common in reality. Databases in com-plicated applications often have multiple tables to store multi-type records with relations. Semi-structured data over the Web also has the relational character-istic in terms of the referencing via hyp erlinks. The requirement of duplicate detection among relational data is then obvious. Traditional methods can still work but without acknowledging the characteristics of relational data, they tend to produce inadequate and ev en inconsistent results. Recently, several models [5] [6] have been proposed to address this i ssue. These models are built on probabil-ity theories. They capture the relational features between records to collectively de-duplicate them with more accuracy. To make the models work, labeled sam-ples should be supplied for estimating model parameters and this training process often takes a considerable amount of time due to the complexity of the model.
This paper then proposes an efficient approach to detect duplicates among relational data. The characteristics of relational data are analyzed from the per-spective of duplicate detection. We define constraint rules that capture these characteristics. Our approach then incorporates these constraint rules into a typ-ical canopy clustering process for duplicate detection. Experiments show that our approach performs well with improved a ccuracy. Furthermo re, as our approach is based on clustering, no labeled samples are essentially required and no extra training process is involved, which sometimes is good for large and raw data sets. The rest of the paper is organized as follo ws. Section 2 discusses related work. Situation of relational data and its cha racteristics are discussed in Section 3. Section 4 defines constraint rules for dup licate detection in rel ational data. Sec-tion 5 presents the constrained clustering approach. Experiments and evaluation results are shown in Section 6. Section 7 concludes the paper and discusses the future works. Duplicate detection of data was initially studied in database community as  X  X ecord linkage X  [7]. The problem was formalized with a model in [1] and was further extended in [2]. This model co mputes over features between pairs of records and generates similarity scores for them. Those pairs with scores above a given threshold are treated as duplicates and transitive closure is performed over them to yield the final result. In [8], clustering-based methods are proposed to identify duplicates in publication ref erences. Their approach performs quick canopy clustering with two thresholds at the first stage and perform more ex-pensive clustering methods within each canopy cluster at the second stage for refined results. The records for de-duplication in these methods are not rela-tional, which means each record is a separate instance with no explicit relation with another.

Supervised learning methods have also been employed to make duplicate de-tection more adaptive with given data. Cohen et al [3] propose an adaptive clustering method and introduces the notion of pairing function that can be learned to check duplicates. Tejada et al [ 9] use a mapping-rule learner consist-ing of a committee of decision tree classifiers and a transformation weight learner to help create mapping betw een records from different d ata sources. Bilenko and Mooney [4] use a stochastic model and Support Vector Machine (SVM) [10] to learn string similarity measures from samples so that accuracy can be im-proved for the given situation. These methods are more adaptive and accurate because of their various learning processes which require an adequate amount of labeled data. Again, all of these methods work on traditional data records with no relational features.

Recently, the relations between data records have been noticed in duplicate detection research community. Singla a nd Domingos [5] build a collective model that relies on Conditional Random Fields (CRFs) [11] to capture the relation of records for de-duplication. The relationship is indicated just by common field values of data records. The model prop osed by Culotta and McCallum [6], which is based on CRFs as well, deals with multi-type data records with relations other than common field values. These methods improve the accuracy of de-duplication by capturing relational features in their models. They also belong to the learning paradigm, which requires labeled samples for training the model parameters. Due to the complexity of the model, the training and inferencing require considerable time, which poses scalability problem. 3.1 Situations One situation of duplicated relational data can be found in [6], which gives an example of duplicated records of papers a nd their venues. In this example, the details of papers (author, title) form a database table and the details of venues (conference/journal name) form another table. Obviously, each paper links to a certain venue, forming a relation betw een the two records. This example can be further extended so that authors may form a separate table containing details of authors (e.g., name, email address) and papers link to certain records in the author table. This kind of normalization is common in designing databases. But it is not the favorite situation for traditional duplicate detection.
 Data on the emerging Semantic Web [12] also has this relational feature. Ontologies are introduced to align data on the Semantic Web. A data record (or instance) then has several property values according to the underlying ontology. Particularly, it may have certain proper ty values that refer to other records. Examples are like that an author record has a  X  X ublish X  property with values pointing to several publication records. More over, unlike the strict database schema, ontology allows data records to b e described in a very flexible way with different angles. For example, a public ation record can use a reverse property of  X  X ublish X , say  X  X rittenBy X , to refer to the author records. This flexibility, together with the characteristics of decen tralization on the Se mantic Web, poses challenges to record deduplication. 3.2 Characteristic of Relational Data The main characteristic of relational data is certainly the relational feature, i.e., the links between different data record s. This often implies that data records may have different types, like the discussed situation where author records link to publication records. Then, multi-type is another characteristic.

In the discussed Semantic Web situation, data instances are not formatted as well as in databases. They are often pr esented in XML format or described by certain languages (for example, OWL [13] ). Therefore, such data instances are semi-structured. In addition, as users can choose different ways to express, the resulting data instances then have different perspectives, not as unified as those in databases. Duplication in relational data can happ en on every type of related data records. However, due to the characteristics of relational data, there are some certain patterns among them, which allow us to define constraints. We first introduce some basic notations and then define constraint rules. 4.1 Notations First, for a particular domain of interest, we can obtain a set of types T ,anda set of properties P . There are two types of properties in P : data type properties that allow instances to be described with numbers and/or string values; and object properties that link instances to other instances with particular meanings (following the notions in OWL [13]). An instance then can be described with a type and a subset of properties and their corresponding values (numbers, strings, or other instances).

We identify two classes of instances. If an instance d i has certain object prop-erty values that let it link to a set D i of other instances, then d i is identified as  X  primary instance  X . For any instance d j ( d j  X  D i ), d j is identified as  X  derived instance  X . The two classes are not exclusive . That is, an instance can be both  X  X rimary X  and  X  X erived X  as long as it points to other instances and has other instance pointing to itself. Given an object property link between two instances (denoted by d i  X  d j ), it is easy to determine the classes of the instances.
If two instances d i and d j actually refer to one same real world entity, then the two instances are regarded as duplicates (denoted as d i = d j ). Duplicated instances may not be same in terms of their types, property values as they often come from different sources with di fferent qualities and perspectives. But usually they have similar values. Traditional methods thus use certain similarity measures to compute degr ees of similarity of two instances. Given a similarity function f , a clustering process can be conducted to group instances with high similarity degrees into same clusters. For an instance d i grouped into cluster c k , we denote as d i  X  c k or simply c k ( d i ). 4.2 Constraint Rules We define five constraint rules for duplicate detection using clustering approaches. Please note although we call all of them constraints, some actually act more like general rules with little constraint features. Derived distinction. Given an instance d p and D p = { d r | d p  X  d r } ,if  X  d i ,d j  X  D p ,i = j ,then d i = d j .

This rule indicates that all the derived instances from one same primary in-stance should not be duplicates of each other. The reason is quite obvious. Firstly, the application of relating one instance to two or more same other instances is very rare. A paper is always written by different authors if it has more than one author. A conference, in pr inciple, never allows two same papers to be accepted and published. Secondly, the relation between one instance and other several in-stances often occurs within one data source. Therefore, it is quite easy to maintain so that the derived instances from one same instance are not duplicates. Consider that a person manages his publications to ensure no duplicates occur on his/her web pages. As a result, the instance of this person links to different instances of publications.
 Primary similarity. Given two primary instances d a , d b and one of the resulting clusters c ,if c ( d a ,d b ), then d a and d b have high confidence to be duplicates. We denote d a  X  d b .

This rule prefers similar primary instances. This rule is based on the observa-tion of the characteristic that primary instances are often described with more detailed and accurate information while derived instances are usually given less attention and hence have less and vaguer details. Therefore, similarity between primary instances are more reliable for duplicate detection.
 Derived similarity. Given two primary instances d a , d b and d a  X  d b ,ifwehave
This rule treats derived instances that fall in same cluster as duplicates if their corresponding primary instances are treated as duplicates. Strictly speaking, if two primary instances are duplicates, all of their corresponding derived instances should be duplicates as well. However, as noise often exists, it can not be guaran-teed that the seeming primary instance duplicates are actual duplicates. To ensure high precision and to prevent false duplicate spreading, we only identify those de-rived instances that fall in same clusters to be duplicates.
 Reinforced similarity. Given instances d i , d j , d m , d n and clusters c k , c l ,ifwe
This rule addresses the issue of data exp ressed with different perspectives. Dif-ferent sources have their own views and describe data from different angles. An entity may be described as a detailed pri mary instance in one source; But in an-other source, it could be a simple derived instance. while we may not be confident in the similarity between a primary instance and a derived instance that fall in one same cluster c k , this similarity will be reinforced if their derived/primary instances also fall into one same cluster c l . As a result, we treat both pairs as duplicates. Boosted similarity. Given derived instances d i ,d j ,d m ,d n and clusters c k ,c l d , d x  X  [ d i ,d m ]and d y  X  [ d j ,d n ], then d i  X  d j and d m  X  d n .
This rule reflects the notion of co-refer encing. It is possible that two different instances mention two seemingly same ins tances that turn out to be different. But the possibility would be much less if more than one (unique) instances mention two sets of seemingly same but different inst ances. For example, two different papers may have one author X  X  name in common which actually refers to two different per-sons; But it rarely happens that two papers have two authors X  names in common which refers to four different persons. Ide ally, if more frequent primary instances are found pointing to more sets of similar derived instances (which may be imple-mented by frequent item set mining [14]), the confidence of the results would be much higher.

Fig. 1 serves to illustrate the application patterns of different constraint rules we X  X e defined.
 This section discusses how the above rules are incorporated in the clustering pro-cess. First we present the commonly used canopy clustering method in duplicate detection. Then we focus on our approach. 5.1 Canopy Clustering Canopy clustering [8] is commonly used in duplicate detection [3,4,5]. It uses two similarity thresholds ( T tight , T loose ) to judge if an instance is closely/loosely simi-lar to a randomly selected inst ance that acts as canopy center. All loosely similar instances will fall into this canopy cluster. But those closely similar instances will be removed from the list and never compared to another canopy center. Canopy clustering is very effective in duplicate det ection as most instances are clearly non-duplicates and thus fall in different canopie s. It is also very efficient since it often uses quick similarity measures such as TFIDF [15] computed using inverted index techniques.
Since the resulting canopies may be still large and overlap with each other, a second stage process such as Greedy Agglom erative Clusterin g (GAC) or Expecta-tion-Maximization (EM) cluster are usually conducted within each canopy to yield refined results [8].

When canopy clustering is applied to duplicate detection in relational data di-rectly, the performance may not be as good as it is used in normal data. This is be-cause it ignores particular characteristics of relational data. For example, for two derived instances which may represent two different papers of one person, they can be so similar that canopy clusterin g (even with GAC or EM) treats them as duplicates. 5.2 Canopy Clustering with Constraints To improve the performance of duplicate detection in relational data, we modified canopy clustering by incorporating the constraints we X  X e defined. The resulting approach can be divided into four steps.

The first step (step 1) is much like the first stage of canopy clustering except that it subjects to the constraint that no any two derived instances from one same instance fall into one same canopy. Fig. 2 shows the algorithm of this step. In the algorithm, function sim ( d i ,d r ) computes the degree of similarity between the in-stance d i and d r .

Although each resulting cluster is constrained to contain no two derived in-stances of one same instance, it still can not guarantee derived distinction due to the existence of overlapping canopies. If two clusters, each of which contains a derived instance of one same instance, both have an instance d overlap , this in-stance then actually bridges the two diff erent derived instances when we take a transitive closure. As a result, it violates derived distinction .

Step 2 then is designed to ensure derived distinction thoroughly. it is done by checking the overlapping instances and only allowing them to be with the most similar derived instance. Fig. 3 shows the algorithm of step 2.

The purpose of step 3 is to extract high confident duplicate pairs within each cluster in C 2 by following the definition of primary similarity , derived similar-ity ,and reinforced similarity .Instep4, boosted similarity is implemented to extract frequent co-referenced instance pairs as potential duplicates from the clusters. The algorithms of step 3 and 4 are illustrated in Fig. 4 and Fig. 5 respec-tively. After all the potential duplicate pairs are extracted, a transitive closure is performed to generate the final results.

Please note the constraint rules reflect ed in these steps are not incompatible with other refinement processes such as GAC. They can be added in the procedure to work together with the constraint rules. For example, GAC can be added after step 2 to further refine clusters. 5.3 Computational Complexity We informally address the complexity of our approach. The algorithm in step 1 per-forms a constraint check that normal canopy clustering doesn X  X  have. This extra check does about O ( km 2 ) judgements where k is the number of clusters and m is the average size of each cluster. In the s etting of duplicate detection, the size of each cluster usually is not very big ( k m ). The complexity of cluster adjustments in step 2 depends on the number of primary instances ( p ) and the average size of derived instances a primary instance has ( q ), which is about O ( pq 2 ). Normally, n&gt;p q where n is the number of all the instances. In step 3, the extraction of potential duplicate pairs out of each cluster performs at the complexity level of O ( km 2 + km 2 q 2 ) if we include the checking for the derived instances. The complex-ity in step 4 depends on the implementation. Our simple implementation operates at O ( p 2 q 2 ). After all, it should be noted that all the above operations (checking, adjusting, extracting) don X  X  involve very expensive computations. In fact, our ex-periments reveal that a lot of time is spent in computing the similarity between instances. There exist some commonly used data set s for duplicate detection experiments, but data instances in them don X  X  have many types and in-between relations. And mostly they are presented from one unified p erspective. This doesn X  X  represent well the real world situations of relational data. Therefore, we collected data from differ-ent sources to build the data set for our ex periments. The data set is mainly about papers, authors, conferences/jounals, publishers and their relations. Such data is collected from DBLP web site ( http://dblp.uni-trier.de ) and authors X  home pages. These data instances are converted into a working format but types, rela-tions and original content values are preserved. Manual labeling work is done to identify the true duplicates among the data for the purpose of evaluation of ap-proaches in the experiments. Totally, there are 278 data instances in the data set referring to 164 unique entities. The size may not be so big, but duplicate detection in it may not be easy since there are a certain amount of different instances with very high similarity, for example, different papers within same research fields and different authors with same/similar names. The distribution of duplicates is not uniform. About two-third of instances have one or two references to their corre-sponding entities. The most duplic ated entity has 13 occurrences.

Same as [8], we use standard metrics in information retrieval to evaluate the performance of clustering approaches for duplicate detection. They are precision, recall and F measure. Precision is defined as the fraction of correct duplicate pre-dictions among all pairs of instances that fall in the same resulting cluster. Recall is defined as the fraction of correct duplicate predictions among all pairs of instances that fall in the same original real duplicate cluster. F measure is the harmonic av-erage of precision and recall.

We evaluate our approach in comparison with the canopy-based greedy agglom-erative clustering approach (CB+GAC) [8]. CB+GAC also performs canopy clus-tering first but with no constraints. It then refine each canopy cluster using GAC: initialize each instance in the canopy to be a cluster, compute the similarity be-tween all pairs of these clusters, sort the similarity score from highest to lowest, and repeatedly merge the two most simila r clusters until clusters reach to a cer-tain number. Table 1 shows the evaluation results of different approaches. The two threshold parameters for canopy clustering in this evaluation are set as T tight =0 . 5 and T loose =0 . 35, which are obtained through a tuning on a sampled data set. The number of clusters is then automatically determined by the two parameters. In the table,  X  X B+GAC X  is the general clusteri ng approach we have just discussed.  X  X tep 12 X  is the approach that only performs step 1 and step 2 (refer to Section 5.2) and then returns the resulting clusters.  X  X tep 12+GAC X  is the approach that performs GAC after step 1 and step 2.  X  X tep 1234 X  obviously is the approach that performs all the steps to impose all the constraints we X  X e defined on the clusters. From the table, we can see that by incorporating con straint rules, the overall F measure im-proves along with the precision. In particular, when all the constraints are applied, the precision increases up to 20%, which indicates that our approach can predict duplicate with very high accuracy.

Fig. 6 shows the sensitiveness of precision of different approaches to the loose similarity threshold ( T loose ) in the canopy clustering. Since in our approach some constraint rules are used to extract duplicate pairs out of working clusters, the quality of the initial canopy clustering may affect the performance. That is, when T loose becomes more loose, each canopy cluster may have more false duplicates, which might affect the performance of those constraint rules used for duplicate extraction. The trend of dropping precision while T loose decreases is well revealed in approach  X  X tep 12 X . However, the dropping trend of approach  X 1234 X  is slightly better than that of  X  X tep 12 X , which means that constraint rules used in step 3 and 4 can tolerate noisy canopy clusters to certain degrees.

Table 2 shows the precision of detecting duplicated pairs in different steps in our approach. This can be used to roughly estimate contributions of different con-straint rules as they are implemented in different steps. The evaluation on our data set shows that the main contribution to the improved precision is made in step 3, where constraint rules of  X  X rimary similarity X ,  X  X erived similariy X  and  X  X einforced similarity X  are imposed. This paper discusses the characteristics of relational data from the perspective of duplicate detection. Based on these characteristics, we have defined constraint rules, which are implemented and incorporated in our cluster-based approach. Ex-periments show that our approach performs well with improved accuracy in term of precision and recall. Experimental evaluations also reveal that the use of con-straint rules increases the precision of dup licate detection for relational data with multiple perspectives.

One of the further studies is to conduct further experiments with larger data sets. Currently, we are keeping collectin g data from different sources and convert-ing and labeling them to build larger data sets. Besides the evaluation of accuracy on the large data sets, the efficiency of the approach will be formally evaluated.
Another further study is to design quantitative metrics to reflect characteristics of duplicated relational data. The ideal metrics will act as soft constraint rules. Thus, they are expected to be more adaptive to different duplicate problems. The authors sincerely thank the anonymous reviewers for their valuable comments. The work presented in this paper was partially supported by Australian Research Council (ARC) under dis covery grant DP0559213.

