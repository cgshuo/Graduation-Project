 Many forms of linguistic analysis, such as part of speech tagging, named entity recognition, and other sequence labeling tasks are performed on short spans of text and assume statistical dependence within a window of only a few tokens. We propose using passage retrieval to induce non-local dependencies in structured classifica-tion that generalizes earlier work in context aggregation for named-entity recognition. We introduce a new method for feature expan-sion inspired by psuedo-relevance feedback (PRF). Our results on the CoNLL 2003 task show that features from cross-document fea-ture expansion improves NER effectiveness over previous aggrega-tion models. Utilizing all the tokens in a sentence for query con-text consistently perform best on both intrinsic and extrinsic evalua-tions. Tagging models incorporating feature expansion outperform the leading NER system when evaluated on out of domain data, a collection of publicly available scanned books on the topic of his-toric Deerfield, MA. Finally, the results show that retrieval based feature expansion using an external collection of unlabeled text can result in further effectiveness improvements.
 Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Selection Process General Terms: Algorithms, Experimentation Keywords: Named Entity Recognition, Passage Retrieval, Pseudo-Relevance Feedback, Information Extraction
Despite the increased application of Natural Language Process-ing (NLP) on queries and documents to improve retrieval, there is little work exploring the use of retrieval to improve NLP tasks. In this work, we use passage retrieval to improve the effectiveness of Named Entity Recognition (NER). NER is one of many com-monly performed sequence labeling tasks including part of speech tagging, syntactic chunking, and other types of information extrac-tion. In these problems, we are given an input sequence of observed variables, x , which consists of a sequence of words. For each ob-served variable, x i  X  x the goal is to infer a corresponding output label.

In most statistical sequence models the decision about the out-put label of a given token depends only on a small local window of adjacent text. These local features sometimes do not provide enough evidence to accurately infer the output label. This prob-lem is exacerbated by tables, lists, and other structures containing non-grammatical text with little or no contextual clues. To improve NER effectiveness for these tokens, we need methods that utilize non-local dependencies within and across documents.
 Specifically, we use a technique inspired by Pseudo-Relevance Feedback (PRF) to aggregate observed features from retrieved pas-sages to more accurately estimate a feature distribution used to la-bel a token. PRF consistently improves retrieval effectiveness by providing a better estimate of the query model [12, 3].
In PRF, the top retrieved documents are assumed to be relevant, and terms are selected from these documents to add to the original query. When labeling in NER, for each x i in x meeting speci-fied criteria, we perform passage retrieval using the context of construct a query. We assume that the top retrieved passages con-taining x i have the same label as the source word sequence. Given this assumption, we extract features from the retrieved passages and aggregate them to provide a better estimate of the observation sequence. This feature expansion method addresses the problems of feature sparsity and labeling consistency.

PRF based feature expansion has several important properties that make it attractive for handling non-local dependencies in NLP tasks. First, the context of the token is used to rank passages. As we show in our retrieval evaluation, using passage context is highly ef-fective at selecting passages with matching labels in the top ranks, even for ambiguous tokens. Second, the number of dependencies created by the model can be controlled by varying the number of feedback documents. Third, the features extracted from the re-trieved passages are weighted by the retrieval model X  X  estimate of their similarity to the source passage. Finally, the number of expan-sion features can be restricted to those with the highest probability in the retrieved set, reducing the number of features added to the model.

The idea of tying labels and features across tokens has been ex-plored in previous work modeling non-local dependencies, such the skip-chain CRF model [20]. However, efforts to model non-local dependencies directly in the graph structure result in complex graphical models with loopy graphs that require approximate in-ference methods, such as Loopy BP and Gibbs sampling. The use of approximate inference for NER results in significant slower per-formance [7]. Consequently, these models are not used often in practice.

Another approach to handling non-local dependencies is based on copying and aggregating observed features [23, 19]. Copying features allows the use of simple linear models where efficient exact inference techniques for training and decoding, such as the V iterbi algorithm, can be used. However, results using this approach in the past have been mixed. The recent results of Villain et al. [23] show that feature copying improves the results on the CoNLL 2003 shared task, but not as much as they expect. One cause of error that they highlight is ambiguous tokens that refer to the same entity but take on different labels depending on the context. For exam-ple, consider the word China which in:  X  X hina beat out Finland in the match... X  is an ORG and  X  X he Beijing Olympics took place in China. X  where it is a LOC. Previous models treat all occurrences of a token within a document identically without consideration of the context. Our method addresses this problem by generating a query from the passage context and weighting passage features based on retrieval similarity.

A problem with existing models [20] is that they do not improve effectiveness on tokens that occur infrequently within a document. Our method utilizes features from external documents to aggregate features across documents. We also show that PRF feature expan-sion can leverage volumes of unlabeled text to improve effective-ness.

One of the stated design goals of NER systems is that they should be robust to unseen text. However, state-of-the-art systems such as the Stanford NER system and the LBJ NER tagger perform poorly when evaluated on out of domain data. Liu et. al. [14] recently demonstrated that the effectiveness of the Stanford NER tagger trained on CoNLL data drops to 45.8% F1 when tagging en-tities from Twitter microblog documents. In our experiments, we find similar degradation in performance to 51% when tested on the Deerfield collection of historical books. We observe that across multiple out of domain data sets the F1 score of models trained on newswire decreases by approximately 40%. Our experiments show that models incorporating feature based expansion are more robust than previous systems when evaluated on out of domain data. We note that the LBJ tagger also utilizes a greedy form of non-local dependency handling and does better than the other systems tested, but not as well as models using the principled aggregation methods proposed in this paper.

Our main contributions are:
The remainder of the paper is structured as follows. In Section 2 we provide an overview of related work utilizing non-local depen-dencies in sequence labeling. In Section 3 we outline our approach for NER. Section 4 describes passage retrieval for sequence label-ing and pseudo-relevance feedback for feature expansion. In Sec-tion 5 we evaluate our methods for passage retrieval and compare the effectiveness of feature expansion approaches to improve NER.
Named entity recognition, due to its wide array of practical ap-plications, has received substantial attention from researchers for the past twenty years. Nadeau and Sekine [17] survey much of this literature. Here we focus on attempts to augment NER systems with information beyond a candidate token X  X  local neighborhood to improve model consistency and address feature sparsity.
Recent efforts have focused on adding dependencies, mostly within a document that penalizes inconsistent labeling and enforce some degree of consistency constraints. Finkel et. al. [7] show that pre-dictions for the same entity are inconsistent within the same docu-ment and across the corpus. Sutton and McCallum [20] use a skip-chain CRF with loopy BP inference to enforce consistent decoding among string-identical tokens. Finkel [7] penalizes inconsistent la-beling and performs inference using Gibbs sampling. Bunescu and Mooney [4] use a Relational Markov Network (RMN) to explicitly model long-distance dependencies and use loopy BP for inference. Instead of modifying the graph to explicitly encode dependencies our approach aggregates feature information and allows the use of efficient exact inference methods for decoding.
A simpler, but still effective, approach to global inference is taken by two-pass or stacked architectures. A token which appears in an unindicative context in one sentence may appear in a very obvious context in another sentence. In a two pass model the pre-dictions of a first-pass system are used as features in a second-pass model that  X  X ixes up X  the labeling [10]. The simplest version of this approach enforces consistency in certain labelings by majority vote or other heuristics [16]. Other versions use nearest neighbor clas-sification to incorporate predictions in other parts of the document or corpus [14].

Two pass models fix mistakes on frequent entities with little or no ambiguity. However, the limitations of these models were recently examined by Villain et al.[23]. They fail when the first pass labels the instance incorrectly more often than correctly. Furthermore, for rare tokens the prediction information remains sparse and there may only be weak evidence in each passage considered in isolation. Aggregating feature level information across occurrences can be more effective than coordinating output decisions.

Bendersky at al.[1] tag sparse and ungrammatical web search queries by utilizing labels from top retrieved documents where in-stances are weighted using pseudo relevance feedback. Instead of aggregating labels, which can be noisy, our method utilizes the un-derlying features.
Our work on feature expansion is mostly closely related to work on context aggregation, which copies features across token instances. Ratinov and Roth [19] aggregate features for string-identical to-kens within a fixed window size of 200, even across document boundaries. The idea of our work is related to that of Villain et. al.[23], who copy  X  X isplaced features X  across related tokens within the same document. Their method uses information to copy only the most predictive features for related tokens. It requires a pre-processing step over the entire corpus to identify these features over the corpus before training or decoding. The model suffers from am-biguous token contexts, introducing noise. In contrast, our uses all features weighted based on the retrieved passage X  X  similarity to the source sequence.
A fundamental cause of inconsistent tagging is that local con-texts in isolation may be noisy and contain sparse or contradictory features. Lower-dimensional representations are useful, however, not only as a way of transferring information across domains but for mitigating sparsity within the source domain [22]. Many state-of-the-art systems exploit flat or hierarchical distributional similar-ity clustering to induce better feature representations [8, 19]. As Turian et al. [22] detail, these models are expensive to train and can take days or weeks on modest sized RCV1 news collection.
The methods we propose can be incorporated into a variety of models used to infer output values in sequence labeling. For this work we incorporate our feature expansion technique with a state sequence model based on Conditional Random Fields (CRFs) [11]. CRFs are a type of discriminatively trained undirected graphical model trained to maximize the conditional probability of output labels given an input observation sequence. Given an observed sequence of words x , the goal is to predict the values of the un-observed random variables y , which are the corresponding output labels. In this work, we utilize a linear-chain CRF with a first order Markov assumption made on hidden variables in the graph where only adjacent vertices are connected by edges. Just as with first-order HMMs, our model admits efficient inference using the forward-backward and Viterbi algorithms for training and decod-ing.

CRFs are the state-of-the-art in many sequence modeling tasks [18, 11], and their effectiveness on NER tagging is competitive with the best reported by the LBJ NER tagger [10, 23, 19]. The CRF framework allows flexibility to integrate retrieval based fea-tures. Unlike generative models like HMMs, CRFs do not attempt to model the joint distribution p ( x , y ) . Instead, they estimate p ( x ) . We train the CRF model using stochastic gradient descent (SGD). Our system is based on a popular open-source implemen-tation, LingPipe, 1 . This model corresponds roughly to the local Viterbi model described in by Finkel et al[7]. This class of models are widely used because of their efficiency and reliability.
The features used in the model include words within a window size of 4, adjacent word character prefixes and suffixes, part of speech tags, and capitalization patterns. The baseline feature set is summarized in Table 1. For each of these features there is a bi-nary feature function f k ( x i , x ) that indicates the presence of the feature in the observed variables. We also evaluate stronger mod-els that include the Wikipedia gazetteers and hierarchical Brown word clusters [2] used by Ratinov et al [19]. In Section 5 we test the combination of these models with our method for incorporat-ing non-local dependencies using various feature expansion tech-niques. h ttp://www.alias-i.com/lingpipe
This section formalizes the use of passage retrieval in the context of a sequence labeling task. We first explore various definitions of the retrieval collection and relate this to previous work. Second, we present methods for generating the query, Q , to retrieval simi-lar passages from the observation sequence. Third, we provide an overview of passage retrieval models. Finally, we present a new method for feature expansion based on pseudo-relevance feedback.
For the purposes of discussion, we define the observation se-quence of tokens x to consist of a single sentence. Likewise, the passages indexed in the collection C are also sentences.
C is a set of passages over which retrieval is performed. The set of passages used is an important factor in the effectiveness of retrieval based feature aggregation. It determines the scope of the non-local dependencies. We now examine several corpus defini-tions and relate them to previous work.

The within document restriction defines the collection of pas-sages to be the sentences that occur in the same document as previous work this is the most commonly used model [20, 7, 23]. It is simple to implement because an entire document is typically available during labeling. Since documents that mention the same entity multiple times are likely referring to the same entity, there is strong evidence that the entity shares the same label. However, this definition does not consider dependencies between occurrences across documents. This hurts recall and is problematic for short documents and rare entities.

The fixed window definition restricts the retrieved passages to ones that occur within a specified range of tokens in relation to the observed token, x i . This is the cross-document context aggrega-tion model utilized by the LBJ NER tagger [19]. The LBJ tagger uses a token window size of 200. The fixed window definition is an ad-hoc model developed based on the observation that documents close together in a newswire stream tend to be topically related. The LBJ results demonstrate that utilizing cross-document feature information improves effectiveness, but the heuristic is highly spe-cific to the CoNLL data format. We include it in our experiments for completeness.

The global corpus utilizes all passages. The size and scope of the collection can vary significantly. Incorporating global depen-dencies provides the largest amount of information. This can be useful when labeling very rare entities.
In this section, we describe methods for generating a query, Q , from an observed input sequence of tokens, x . The process of query generation for sequence labeling has two separate compo-nents: query triggering and query context generation.
Query triggering is the process of determining the variables for which feature expansion should be performed. In the simplest case for each observed variable x i in x we generate a query, Q , to re-trieve passages and perform expansion. This results in |C| queries, one for each token in the collection, which is infeasible.
W e define a binary decision function, g that determines whether to generate a query, Q for each x i in x .
The optimal decision function should minimize overall retrieval time while maximizing the improvement in NER effectiveness. The correct balance of these factors depends on the efficiency vs. effec-tiveness trade-offs of the application and we do not explore them in detail.

For our experiments we utilize several boolean combinations of the features in Table 2. For the data sets in these experiments the capitalization heuristics work well and have been successfully used in previous work [20, 7]. Beyond capitalization, very common stopword tokens represent a large number of terms and queries gen-erated from them can be slow to execute. Tokens that are short or all capitalized are likely abbreviations and are often ambiguous; not performing expansion for these tokens can avoid errors. The cap-italization features at the beginning sentences are also uncertain. Given labeled training data, it is possible to learn feature combi-nation weights using a machine learning technique. Instead, we created several hand-crafted combinations of these features which are evaluated in Section 5. A more thorough investigation of trig-gering is an area for future work. In practice, the rules are effective for our evaluation data sets. In this section we outline several methods for generating a query, Q , from the input sentence, x . The goal is to generate a query likely to retrieve passages where the target variable x i same output label. We only utilize the token text, which generalizes across sequence labeling tasks and feature sets.

In this method, the query consists of only the current observed token, x i . In previous work cite[19] [20] [7] on modeling non-local dependencies, this is the only method utilized.

This method makes a first order markov assumption and utilizes only adjacent tokens ( x i  X  1 , x i , x i +1 ). This is an important feature used in NER classification.

All of the observed tokens in x are utilized in the query. This utilizes the largest amount of context information. It is also the most expensive to execute because these queries can become quite large for long sequences.

Tokens that match the capitalization pattern Aa+ are utilized in the query. These tokens that are likely to be other related named entities.

For retrieving similar passages in NER, an important considera-tion is how the tokens are normalized. This includes case folding, stemming or lemmatization, and stopword removal. As we show in our experiments, in Section 5, features such as case sensitivity significantly impact the effectiveness of retrieval.
Given a query, Q , generated from our source sentence, we now describe how we rank passages, r , in the collection, C . This de-fines the similarity function between the query, Q , and other sen-tences in the collection. For comparison with previous work the ba-sic model is a simple set based retrieval model that performs exact string matching. The remaining models are based on the Markov Random Field retrieval model (MRF-IR) [15] using unigram and sequential dependence models. The simplest model we test is an exact match between a query, Q , consisting of the single token x i in the source sequence.
This model performs exact string matching where all passages containing the matching token are returned. All retrieved passages have the same score. This is the method used in previous feature aggregation models [20, 23]. For cross-document dependencies on larger collections the number of passages retrieved can be pro-hibitively large.
The unigram model is equivalent to the Query Likelihood model that ranks documents according to the probability of relevance us-ing a bag of words assumption of term independence. Using Dirich-let smoothing this is defined as: where f ( q i , r ) is the frequency of the query term in the passage, c q,i is the number of times a word occurs in a collection of docu-ments, | C | is the number of words in the collection, and  X  is the smoothing parameter that is set empirically.
To model dependencies between terms in the source query we utilize the sequential dependence variant of the Markov Random Field IR model. The model goes beyond individual query terms and utilizes phrases and word proximity for adjacent terms in the query.

This model can specified using the Indri 2 query language as, h ttp://www.lemurproject.org/indri The weighting parameters are set according to those suggested by Metzler and Croft, which were shown to be stable across collec-tions [15]. We now describe our approach to using non-local information. At inference time, we use weighted feature copying to provide a better estimate of p ( y | x ) . Specifically, when an input token x triggers a query, we generate a query and retrieve r passages. We append NER classifier features from these passages onto the vector of features f ( x i , x ) that fires for x i . The weights for the appended features are those features X  weights learned during CRF training, down-weighted by the passage X  X  score under the retrieval model, as we now describe.

We utilize a pseudo feedback method based on Relevance Mod-els (RM) [12]. RM provides a framework for better estimating a query language model. Given an initial short query, Q , the rele-vance model result is a distribution P ( w |  X  Q ) .
Given the set of all passages, r in the collection, the model is an aggregation of term probabilities in the collection weighted by the passage X  X  similarity to the query. We can utilize a similar formula-tion for generating an expanded feature distribution for our target input passage we are labeling. Given the query, Q x i generated from x given the formulation:
As discussed in Section 3, each f k is a binary feature function used to define the features in the CRF. The p ( f k | r ) is estimated as a binary indicator function, 1 if the feature occurs in r and 0 otherwise. The result of the model is a distribution over feature values. It is an aggregation of the feature counts in the collection weighted by the similarity of the passage to the source query.
Since for most documents the conditional probability of p ( Q is very small, we can closely approximate the above distribution by using the set of R top retrieved documents in response to the query, giving us:
We utilize this new feature distribution in place of the distribu-tion extracted from the original observed variable x i and use the local inference methods for linear chain CRFs.

Note that for exact match retrieval the P ( Q x i |  X  r ) values are all one, so it is simply an average of the feature values across the col-lection without context based weighting.

A variant of the relevance model that has been shown to be ef-fective when constructing an expanded query is RM3, where the original query is linearly interpolated with the expanded relevance model query [9] using a parameter,  X  . We perform a similar form of interpolation by adding the feature values estimated from the relevance model as distinct features in our CRF separate from the f eature space of the original passage. This allows the CRF to learn separate weights for expanded features from retrieval. In place of a fixed  X  value, each feature has its own weight which is learned by the CRF.

Adding features from the relevance model as separate features significantly expands the number of features used in the model. Over large collections the number of features can become pro-hibitively expansive. The relevance modeling framework provides parameters to control this: varying the size of R and only using a top-k subset of the highest weighted features instead of the entire distribution.
In this section, we report experimental results utilizing retrieval based feature expansion. First, we evaluate the quality of pas-sage retrieval for various retrieval models. Second, we measure the effectiveness of passage based expansion on NER labeling in the CoNLL03 shared task. Third, we test NER models that utilize external corpora as a source for expansion features. Finally, we as-sess the robustness of the models by labeling an out of domain a collection of scanned historical texts.
We perform our experiments on two data sets. Our primary data set is the standard CoNLL NER collection. As a secondary data set, we constructed an NER collection using publicly available scanned OCRed books on the history of Deerfield, Massachusetts.
We first use the standard CoNLL 2003 English data set, which was created for the shared task of the Seventh Conference on CoNLL, which focused on entity recognition. The data consists of Reuters newswire documents from 1996. The training set consists of 945 documents from August 1996 containing 14987 sentences and ap-proximately 200,000 tokens. The test (b) set consists of 231 doc-uments from December 1996 with 3584 sentences and approxi-mately 46,000 tokens. The data set is annotated with four entity types: Person (PER), Location, (LOC), Organization (ORG), and Miscellaneous (MISC).
Recent book digitization efforts by the Internet Archive 3 Google Books are making large volumes of public domain books widely available. To simulate the task of a historical researcher, we created a focused topic collection of material relevant to the history of the town of Deerfield, Massachusetts. The books were scanned and processed with OCR software, which introduces noise due to OCR errors and page structure recognition issues.

The Historic Deerfield collection has ten books containing 3,311 pages with 98,444 sentences, 2.1 million tokens, and over 60 thou-sand distinct words. It contains diverse historical resources: bi-ographies, encyclopedias, and historical catalogues of artifacts. To h ttp://www.archive.org/details/texts Table 6: Evaluation of passage retrieval ranking using Mean A verage Precision (in %). Various combinations of case sensi-tivity, retrieval model, and query generation method are eval-uated. QL indicates Query Likelihood retrieval, SD indicates Sequential Dependence. The last word indicates the query gen-eration method from Section 4.2.2. create a evaluation set for NER, we randomly sampled two pages from each book in the collection. The resulting test set contains 20 pages with 481 sentences and approximately 10 thousand tokens. The pages were manually annotated with entities consistent with the CoNLL task 4 . The dataset contains 661 entity mentions. The distribution is shown in Table 3.
We now evaluate the retrieval effectiveness of the retrieval mod-els described in Section 4.3 on the CoNLL data set. All results utilize the entire training corpus. Unlike traditional adhoc retrieval evaluation, the goal is not simply to return passage relevant to a topic. For NER expansion, the aim is to identify passages whose features are useful in the predicting the correct y i of the observed x in the source sequence x .
In the section we test the effectiveness of several combinations of query trigger features described in Section 4.2.1. Query trigger-ing determines which variables are expanded. It should occur when expansion will improve labeling effectiveness; however, this is dif-ficult to estimate directly. For a straightforward evaluation method, feature expansion should be performed only if the token is part of a named entity. This definition ensures that non-local entity infor-
W e make the judgments file available at http://ciir.cs. umass.edu/~jdalton/deerfield . It contains the publicly available book ids and annotations Table 8: F1 scores on CoNLL for feature expansion using exact s tring matching for varying corpus scopes described in Section 4.2.2. The top is the baseline model with features from Table 1. The bottom results are for a stronger model with Brown clusters and Wikipedia features. Statistically significant over local models where indicated with a * with p  X  .05. mation is considered in classification. The triggering evaluation results are shown in Table 4.

From the results, we observe that the heuristic utilizing capital-ized letters has high recall. It captures all but 2% of entity tokens, which are mostly stopwords that are part of a longer entity string (e.g. of, the), but has a significant number of false positives. The precision improves by removing stopwords, which are expensive queries to execute and are ambiguous tokens. The CapOnly heuris-tic excludes mixed and all-caps tokens which improves precision over isFirstCap . Although recall is reduced significantly, manual inspection shows that many of the missed tokens are abbreviations such as US, UN, and EU. CapOnly combined with excluding stop-words reduces the number of queries by 19%, reducing the number of false positives in half compared with the baseline isFirstCap . This is a significant savings in the number of queries executed. Most of the remaining false positives are temporal expressions such as month and days which are not labeled as named entities.
The addition of the restriction to exclude tokens at the beginning of sentences, notBos , where virtually all tokens are capitalized im-proved precision but resulted in a significant reduction in recall. Furthermore, capitalized tokens at the beginning of sentences are often ambiguous and expansion can improve effectiveness by pro-viding more features to disambiguate them. We found tagging ef-fectiveness improved by expanding these tokens.

We utilize the CapOnly &amp; notStop combination for the remain-ing experiments. It is simple and provides a satisfactory trade-off between efficiency and recall.
For retrieval based feature expansion the effectiveness of the first pass retrieval is an important factor in expansion quality because the features are weighted by the model probabilities. We therefore 2003 Named Entity Recognition test (b) set.
 Table 9: CoNLL F1 scores for feature expansion using ranked p assage retrieval with the Global retrieval scope. (QL) indi-cates Query Likelihood and (SD) indicates Sequential Depen-dence retrieval models. The query context was varied, Capital-ized includes only capitalized tokens, All has all tokens exclud-ing stopwords. Significant differences over the local model with p  X  .05 are indicated with by *. evaluate various retrieval methods to determine which is the most effective. For evaluation purposes a retrieved passage r is defined to be relevant with respect to a source query Q x i for variable x follows: where x j and y j are the corresponding variables contained in in r . The above definition states that a passage is relevant only if it con-tains a string-identical observed variable where the output labels have the same entity class.

The CoNLL newswire documents are indexed using the open-source Galago 5 retrieval system. The documents are split into sen-tences using the boundaries provided and indexed to create a pas-sage level index. We perform stopping using the Lemur 418 stop-word list and stemming using the Porter stemmer. Default Dirichlet smoothing was used with  X  =2500. For evaluation, the set of 33429 queries resulting from the query triggering method selected in Sec-tion 5.2.1 is used. The search index is loaded into memory for fast retrieval during tagging.
 We first examine the impact of case folding on effectiveness. As previously discussed, capitalization is an important feature that strongly indicates a token is an entity. To utilize this we test case-folded and case sensitive retrieval. The results are shown in Table 5. Case sensitive matching improves precision but decreases recall, the number of relevant passages decreases by approximately 20%. The number of queries with no results increases by 25%, no ex-pansion can be performed for these queries. It is notable that both models have very high MAP scores. The high MAP score indicates that most tokens in the CoNLL dataset are not ambiguous.
Next, varying combinations of retrieval models described in Sec-tion 4.3 and context query generation in Section 4.2.2 are tested. The results of the evaluation on Mean Average Precision (MAP) are shown in in Table 6. The table shows that case sensitive re-trieval results in consistent effectiveness improvements across all h ttp://www.galagosearch.org/ models. Using the entire sentence as context to generate the query performs the best. Generating the query only using the capitalized words in the sentence performs only slightly worse than using all of the words in the sentence. This is significant because these queries are significantly more efficient to run because they contain fewer terms that occur less frequently in the collection.

The best performing model is the Sequential Dependence model using all words in the source sentence to generate the query. As shown later in Section 5.3.3, this model also performs the best for NER feature expansion. This indicates that our relevance evalua-tion correlates with real NER improvements in the final combined system.
In this section we measure the impact of adding non-local feature information from retrieved passages to our baseline CRF model. We begin by evaluating the local baseline CRF models. For com-parison with previous work we also evaluate unweighted exact match boolean retrieval. Finally, we evaluate effectiveness of ranked fea-ture expansion models.
We now evaluate the baseline local tagging models systems. Ta-ble 7 shows various local NER systems and feature combinations on the CoNLL named entity recognition task. We compare the ef-fectiveness of the our baseline tagger with the the Stanford NER system 6 . The base CRF model performance is comparable to the out-of-the-box Stanford system. Although these models are widely used for their efficiency, they are not state-of-the-art.
To the baseline system we add features from external knowledge sources. In particular, they are augmented with gazetteers from Wikipedia and Brown word cluster information. These resources are bundled with the freely available Illinois LBJ Named Entity tagger 7 . Consistent with the findings of Ratinov et. al. [19], the external features provide significant improvement over the baseline model. These local NER models are the baselines we use to assess the impact of feature expansion from retrieval.
Next, we present the results of cross-document feature expansion using passages with string-identical tokens. Table 8 shows that ex-pansion provides consistent improvements over the local models. The FixedWindow expansion corresponds to the context aggrega-tion method used by the LBJ tagger [19].

For the baseline retrieval system, the FixedWindow expansion method provides a 13.4% reduction in F1 error on the CoNLL dataset. The global expansion model using all passages in the col-lection provides a smaller 9.5% reduction. FixedWindow outper-forms unweighted global feature expansion. FixedWindow restricts significance over LBJ.
 Table 10: F1 scores of the NER model trained on CoNLL and e valuated on the Deerfield collection. The results show local systems and unweighted feature expansion with varying collec-tion scopes. The top is a tagger model with baseline features. The bottom is a stronger baseline model with word clustering and Wikipedia features. The differences are statistically signif-icant with local models where indicated with a * with p  X  .05. the passages to match those near the source sentence in the news stream. It exploits locality in the CoNLL dataset. It does not per-form well on collections that do not have this property, as we show later in the Deerfield evaluation. Neither aggregation method ap-plied to the baseline model outperforms a stronger local model that uses Brown word clustering and Wikipedia gazetteers.

The results of adding exact match expansion to a stronger model incorporating Brown and Wikipedia is shown in the bottom of Ta-ble 8 there is a small, but significant improvement using the Fixed-Window model. The expansion with the global retrieval over all passages provides no significant benefit. The unweighted global aggregation has less topical cohesion and the unweighted expan-sion contains more noise. The exact match model acts as a type of global prior for a token. This can be problematic for ambiguous tokens. We now explore the use of ranked expansion models that utilize sentence context to address the problem of ambiguity.
The results for feature expansion from ranked retrieval are shown in Table 9. Because the retrieval corpus is small all passages are used for expansion. Unlike the exact match based expansion, the results show that use all expansion models result in significant im-provement over the strongest local NER model. The SD AllTok combination provides a 3.7% reduction in error over the best per-forming local model.

The models with the AllTok context outperform models using only capitalized tokens. The Sequential Dependence model pro-vides a small improvement over Query Likelihood. The models using AllTokens outperform the exact match expansion limited to the 200 token fixed window described in the previous section.
In this section we evaluate the robustness of the models trained on newswire by testing them on the collection of scanned books described in Section 5.1.2. For the Global retrieval scope all the sentences in the 20 books are indexed. Sentence splitting is per-formed using the OpenNLP MaxEnt classifier.

The results for the evaluation on the Deerfield dataset are shown in Table 10. The results show that the F1 score of the tagger drops by approximately 40% compared with the CoNLL results. We in-vestigated the errors and found that many of errors are due to spar-sity in the target domain. A significant number of the entities in the book collection are not present in the newswire training collec-tion. Our error analysis finds that often LOC chunks are confused for PER. The cause of this is that for unseen capitalized tokens the tagger relies heavily on the class prior, which is strongly biased to-wards PER tags in the newswire data. We now show the impact of feature expansion on addressing these problems.
Table 10 shows that expansion using Fixed Window of 200 to-kens does not improve effectiveness significantly. The Global scope outperforms the Fixed Window method when applied to the base-line model.

It is curious that global retrieval aggregation does not signifi-cantly improve the stronger local model that incorporates Wikipedia based gazetteers. In fact, the model performs worse than expansion applied to a weaker model. We believe this is due to the phenomena of model undertraining [21] where the strong Wikipedia features in the newswire domain result in the model underweighting token and context features. The results for ranked feature expansion are shown in Table 11. The weighted expansion models result in very substantial improve-ments in NER effectiveness. The Sequential Dependence model using a query generated from the entire sentence results in a 17% reduction in error. It outperforms the LBJ Layer 1 model which is currently the top performing NER tagger on newswire data. The results indicate that non-local dependencies created from retrieval feature expansion create a model that is more robust across do-mains.

The improvement in model effectiveness from expansion does not address OCR errors. We only copy features for identical ob-served tokens. Relaxing this constraint to copy features for similar strings could potentially improve accuracy further for these tokens, but we do not focus on this problem. Retrieval based feature expansion can also be used to improve NER effectiveness by using unlabeled data from external collec-tions. The previous experiments utilized small collections. The labeled CoNLL data contains less than 20 thousand sentences. We can create a more general model by incorporating external features from larger collections. are statistically significant with p  X  .05.
As an external source for PRF feature expansion we use a subset of the Reuters RCV1 collection [13]. RCV1 consists of Reuters newswire data collected in 1996 and 1997. It contains documents from the same source and time period as the CoNLL data set. We use the first 50,000 documents of the collection. The RCV1 subset contains 931,822 sentences and 20.5 million words.
In previous experiments all of the passages in the collection with-out a cutoff were used because of their limited size. For these ex-periments, feature expansion only uses top ranked passages. We experimented with the number of retrieved passages and report re-sults using the top 50 and 100 passages.

The results on both the CoNLL and Deerfield collections are shown in Table 12. The results compare against the top performing feature expansion models that does not utilize external data. The Sequential Dependence model with 50 feedback documents results in significant improvement in both the CoNLL and Deerfield eval-uations. It provides a 3.1% error reduction in CoNLL and a 2.7% error reduction in Deerfield.

The model using 100 feedback documents and QL retrieval does not significantly improve effectiveness and slightly hurts effective-ness on the Deerfield data. We are unsure why this model does not perform as well, especially on the CoNLL data. More error analy-sis is needed to fully understand the causes. However, we note that the QL retrieval model is less effective than the Sequential Depen-dence model. Also, the larger number of feedback documents may introduce noisy features from off-topic passages. For the Deerfield data, the additional newswire data may not contain the topics in the dataset and therefore may not be as useful for expansion.
Despite some mixed results, the external feature expansion mod-els result in the overall best performing system.
The most significant area of future work is a better method for determining which tokens in the observation sequence require fea-ture expansion. Using our current heuristics there are over 8000 queries needed on the CoNLL test set. While we used in memory indices for fast retrieval performance, the retrieval time could be significantly reduced with little loss in effectiveness. For sequences with strong evidence feature expansion is unnecessary. Further-more, a more advanced triggering model could also leverage the training data to identify where expansion hurts effectiveness due to poor retrieval effectiveness.

Another area that could be improved is a more principled ap-proach to selecting the passage collection to use for feature expan-sion. We would like to utilize strong local evidence within the doc-ument and back off to models of similar documents, and finally the entire collection. This could be done using a technique similar to the Mixture of Relevance Models (MoRM) [6]. Diaz and Metzler also investigate the utility of different external corpora for query expansion [6]. They introduce a theory of  X  X oncept density X  that measures the utility of a collection for expansion.
Many state-of-the-art named entity recognition systems pool in-formation about the context of different entity tokens. This aggre-gation may be at the level of features or by enforcing consistency in decoding. Context aggregation in documents often exploits dis-course constraints [5]; aggregation across adjacent documents in a news feed exploits the local salience of particular stories [19].
We presented a framework that embraces these and other con-text aggregation methods as forms of passage retrieval. In partic-ular, we can retrieve, and use features from, topically similar pas-sages. A summary of the results is presented in Table 13. In ad-dition to showing that passage retrieval can achieve significant im-provements on in-domain accuracy, we showed it surpasses other context aggregation methods when evaluating NER models in new domains.
This work was supported in part by the Center for Intelligent In-formation Retrieval, in part by NSF CLUE IIS-0844226 and in part by NSF grant #IIS-0910884. Any opinions, findings and conclu-sions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors. [ 1] M. Bendersky, W. B. Croft, and D. A. Smith. Structural [2] P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and [3] C. Buckley. Automatic query expansion using smart : Trec 3. [4] R. Bunescu and R. J. Mooney. Collective information [5] H. L. Chieu and H. T. Ng. Named entity recognition: a [6] F. Diaz and D. Metzler. Improving the estimation of [7] J. R. Finkel, T. Grenager, and C. Manning. Incorporating [8] F. Huang and A. Yates. Distributional representations for [9] N. A. Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, [10] V. Krishnan and C. D. Manning. An effective two-stage [11] J. Lafferty, A. McCallum, and F. Pereira. Conditional [12] V. Lavrenko and W. B. Croft. Relevance-based language [13] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new [14] X. Liu, S. Zhang, F. Wei, and M. Zhou. Recognizing named [15] D. Metzler and W. B. Croft. A markov random field model [16] A. Mikheev. A knowledge-free method for capitalized word [17] D. Nadeau and S. Sekine. A survey of named entity [18] D. Pinto, A. McCallum, X. Wei, and W. B. Croft. Table [19] L. Ratinov and D. Roth. Design challenges and [20] C. Sutton and A. McCallum. Collective segmentation and [21] C. Sutton, M. Sindelar, and A. McCallum. Reducing weight [22] J. Turian, L. Ratinov, and Y. Bengio. Word representations: a [23] M. Vilain, J. Huggins, and B. Wellner. A simple
