 1. Introduction
Data mining provides tools by which large quantities of data can be automatically analyzed and classified. The purpose of sidered as an important pre-processing step for classification, potentially improving classifiers X  performance and efficiency.
The fundamental requirement of a feature selector is to identify the most useful information from the data, and reduce the dimensionality in such a way that the most significant aspects of the data are represented by the selected features [5,15,17,36] . Feature selection strives to choose optimal subsets of the original features which still contain the information essential for the classification task, while reducing the computational burden imposed by using too many features [14,16] .
However, the increase of the size of datasets due to the ease of data collection and storage in application domains such as med-icine poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness [37,39] .
This paper presents an enhancement, which provides feature selection via Supervised Model Construction (FSSMC), by augmenting the ReliefF feature subset selection algorithm [19,20] . The resulting algorithm has proved faster to execute on large datasets as a result of a reduction of the training data. FSSMC was implemented as a component of WEKA [9]. 2. Relief algorithms
Relief is an instance based attribute ranking scheme introduced by Kira and Rendell [19], and improved by Kononenko [20] for classification (ReliefF) and Robnik and Kononenko [30] for regression (RReliefF). The Relief family are commonly viewed as feature subset selection methods applied in a pre-processing step before the model is learned, and are amongst the most successful pre-processing algorithms. The majority of the heuristic measures for estimating the quality of attributes assume conditional independence of the attributes (upon the target variable) and are therefore less appropriate in problems which possibly involve much feature interaction. Relief algorithms do not make this assumption. They are efficient, aware of contextual information, and can correctly estimate the quality of attributes in problems with strong dependencies between attributes [31].

Relief uses a statistical method to select the relevant feature attributes. It is a feature weight-based algorithm inspired by instance-based learning algorithms. Each weight W [ A ] reflects its ability to distinguish among the class labels. Features are ranked by weight and those that exceed a user-specified threshold are selected to form the final subset. Relief works by ran-domly sampling instances from the training data. For each sampled instance R, its two nearest neighbours: one from the dated according to how well its values distinguish the sampled instance from its nearest hit and miss. An attribute will re-ceive a high weight if it differentiates between instances from different classes and has the same value for instances of the same class. Relief updates weight W [ A ] of attribute A using Eq. (1): calculates the difference between two instances for a given attribute. Normalization with m in the calculation of A guaran-tees that all weights are in the range between 1 and 1. When dealing with nominal attributes, function diff ( A , I fined as: and for numerical attributes as:
The function diff is also used for calculating the distance between instances to find the nearest neighbours. The total distance is simply the sum of distances over all attributes (Manhattan distance, [4]).

ReliefF, the extension of Relief, solves the problem of dealing with a dataset with multiple class labels (compared to bin-ary class problem such as the diagnosis of an individual with or without certain disease), and can handle noisy and incom-plete data. ReliefF smoothes the influence of noise in the data by averaging the contribution of k nearest neighbours from the same and opposite class of each sampled instance, instead of the single nearest neighbour. Selection of the number of the nearest neighbours is the basic difference between Relief and ReliefF, and ensures greater robustness of the algorithm with regards to noise. A user-defined parameter k controls the locality of the estimates. The parameter is set heuristically and for most purposes, k can be safely set to 10 [20]. Multi-class datasets are handled by finding the nearest neighbours from each class that are different from the current sampled instance, and weighting their contributions by the prior probability of each class estimated from the training data [14].

Compared with Relief and ReliefF, the latter updates the quality of estimation W [ A ] for all attributes A depending on their bution of all the hits and all the misses have been averaged. The contribution to each class by the misses is weighted with the are expected to be in the range {0,1} and also symmetric, the misses X  probability weights should sum to 1. As the class of hits is missing in the sum, each probability weight with factor (1 P (class( R misses X  classes, has to be normalized. The process is repeated for m times.

To deal with incomplete data, Kononenko [20] changed the diff function. Missing values of attributes are treated proba-bilistically. The probability that two given instances have different values for given attributes conditioned over the class va-lue is calculated as follows:  X  if one instance I x has an unknown value:  X  if both instances ( I x and I y ) have unknown values: where V denotes all the values in attribute A . 3. Feature selection via Supervised Model Construction
Although ReliefF has been successfully used in some real world applications [6,21,32] , there are two important issues which remain problematic.
 1. The way to deal with categorical attributes, which exist extensively in real world datasets. ReliefF did not address the problem of multi-valued attributes [20]. At present, the similarity measurement applied in ReliefF is a numerical method.
So when coping with the categorical attributes, if the two selected instances have the same value, the result of difference function is 0, otherwise it is 1. This definition cannot measure the contribution of multi-class ( P 3) values to class labels [8,18] . It will also result in underestimating numerical attributes [31]. To illustrate this, consider that there are two instances of attribute A i , with values 2 and respectively. If A f ( A i ,2,5) = 1, since the two categorical values are different. If A ( A i ,2,5)= X 2 5 X /7 0.43. Hence numerical attributes are underestimated and categorical attributes are overestimated with reduces the separation of the two groups. 2. The setting of the number of instances m , sampled from the dataset, which decides the number of iterations (see Fig. 1 )in ReliefF is a statistical estimate. The sample has to cover enough representative boundaries between the prediction values.
There is a trade off between the use of more instances and the efficiency of computation. If a dataset has a large number of instances, we can speed up computation by controlling the number of iterations with parameter m . Two approaches have been hitherto applied. Either m is set empirically or the entire data are analyzed. Time complexity of ReiefF for a dataset with n instances and a attributes is O ( m n a ). With m being a constant, the time complexity becomes proportional to
O ( n a ). However, since m is the number instances for approximating probabilities, a larger m implies more reliable approximations. When n is very large, it often requires that m is much less than n for high efficiency. But random selection could result in information loss because it does not exploit all data characteristics, and it is difficult to determine how many instances are  X  X ufficient X  for further analysis. On the other hand, using the complete dataset for feature evaluation may lead to expensive computation. In this research, an instance selecting method has been used to find  X  X ypical X  instances that can represent the whole dataset, so that the performance can be close to those using n instances. The parameter m (sampled instances) is generated automatically and the efficiency of the algorithm should be computation-ally improved.

In order to overcome the drawbacks of ReliefF, the original ReliefF algorithm has been optimized, so that it is more appli-cable to large real world datasets. FSSMC applies frequency based encoding [18] for the transformation of data type to better deal with categorical attributes in ReliefF. This approach represents each categorical variable with a numerical value, which is derived from its relative frequency among the outcomes according to the class label. This technique has been used previ-ously, for example predicting whether a car crash will result in  X  X njury X  or  X  X on-injury X  outcome [28], by using a neural net-work to enhance classification accuracy [27]. Because ReliefF is able to measure the contribution of binary-value attributes, the transformation is only applied to multi-value ( P 3) attributes.  X  X tarter selection X  partitions data by taking advantage of class information so as to achieve the same or better performance with fewer but more relevant instances than random sampling. For this purpose, FSSMC is able to generate m automatically, which addresses the second concern of the ReliefF algorithm. The method is inspired by the use of selected instances in ac-tive learning [23,35,34] to eliminate the dependency on the selection of a  X  X ood value X  for m . Active learning has freedom to select which instances can be added to the training dataset [25]. Active learning avoids pure random sampling and exploits the fact that instances are not uniformly distributed and hence some instances are more representative than others [1,3].
This approach has been used to optimize the kNN classifier to improve its efficiency whilst alleviating the dependence on the selection of number of nearest neighbours [12].

Intuitively, since ReliefF searches for nearest hits and nearest misses of an instance, class information is important and can be used to form sub-populations for sampling. FSSMC builds an inductive learning model from the training dataset for feature selection. Because it takes advantage of class information (training data with class labels), it is a  X  X upervised X  method. The FSSMC pseudocode has been provided in Fig. 2 . 4. Methods
Twelve datasets from the University of California Irvine (UCI) repository of machine learning databases [2] and an extract from a large diabetes patient management system were used for evaluating FSSMC with three classification algorithms. Table 1 provides a summary of the characteristics. The Default Accuracy is the accuracy of the majority class on the complete dataset. The % Miss column shows the percentage of the datasets X  entries (number of features number of instances) that have missing values.

The UCI datasets were chosen because of their predominance in the literature, and the prevalence of different character-istics in the database (mixture of feature types, binary and multi-class labels, with and without missing values, and small and large sizes). This provides a variety of data to test the proposed optimization method of feature selection.
The Ulster Diabetes dataset contains 2064 type 2 diabetic patients X  relevant clinical information, recorded for routine pa-tient management. It was investigated to provide a significant data mining challenge, comprising noisy, categorical and nu-meric data, with no prior knowledge other than information appropriate to managing the condition. The selected features in the database include patient characteristics (e.g. age, duration of diagnosis), treatment (exercise, tablet or insulin), medical complication (eyes, foot, heart), care plan (primary, secondary), relevant physical measures (e.g. body mass index) and lab-control using criteria determined by the clinician, and using the HbA1c measurement, which indicates average blood glucose level over a 4 week period.

The experiments described compare classification accuracy (CA) of algorithms with and without feature selection [26].In general, we expect a classifier (Naive Bayes, IB1 and C4.5) to preserve the accuracy with the reduced set of features compared with all the available features or even to improve it due to the elimination of noisy and irrelevant features that may mislead the learning process. Processing time is another important assessment metric of feature selection. This time may be greater than the learning time of the classifier, but both should be comparable [24,22] .
 Information Gain [14] and ReliefF feature selection methods are used for comparison with FSSMC. The parameter m in
ReliefF is set to all available training instances to achieve the best performance. However, this may lead to the discussion that from the point of view of computational efficiency, i.e., FSSMC is compared with the worst circumstances of ReliefF.
Therefore, in order to confirm that FSSMC outperforms ReliefF using selective sampling instead of random sampling, the ran-dom sampling in ReliefF is also applied for the comparison (ReliefF-RS). For the consistency of evaluation, the number of se-lected instances is set equal to the number of instances selected by FSSMC.

The experiments compare runs of classification algorithms with and without feature selection on the datasets. Ten-fold cross validation was used as the sampling strategy as it produces the best estimate of error in practical datasets [38,11,13] .
Furthermore, a paired two-sided t -test was used to determine whether the difference between two algorithms was signif-icant ( p &lt; 0.05). Execution times were based on a PC with Intel Pentium 4, 1.8GHz processor, and Windows XP operating system. 5. Results and discussion
Since the relevant features are unknown in advance for these datasets, the performance of classification algorithms indi-cates the effect of InfoGain, Relief, ReiefF-RS and FSSMC in selecting useful features on various types of datasets.
Table 2 lists performance metrics of each feature selection approach. Information Gain is the fastest method as it does not calculate distance between two instances and utilizes differences in feature characteristics instead of instances. ReliefF and FSSMC consider the discriminability of a feature by calculating the similarity of instances, which is more time consuming.
ReliefF-RS and FSSMC decrease the processing time of ReliefF in large UCI datasets (instance &gt; 3000) such as Adult, Hypo-cient than FSSMC due to the additional pre-selection supervised stage required by FSSMC. With reference to Table 2 , FSSMC, ment in the larger sets, compared to ReliefF.
 Table 3 gives the dimensionality reduction after applying Information Gain, ReliefF, ReliefF-RS and FSSMC on the datasets.
The best dimensionality reducers are ReliefF and FSSMC, which both remove 23.1% of irrelevant or redundant features on average. ReliefF-RS maintains most of the features for further analysis. This may be because the random sampling strategy is not able to represent all characteristics of the data, and reduces the  X  X ilter X  ability of ReliefF.

Table 4 presents the CA results with and without feature selection for Naive Bayes, IB1 and C4.5, for a representative sub-set of the UCI data sets and the Ulster diabetes dataset. CA is averaged over 10-fold cross validation runs before and after feature selection.
 The results were considered to be significantly different in this study at the 1% level according to a paired two-sided t -test.
The symbols  X  X + X  (or  X  X   X ) were used to denote that one algorithm was (statistically) significantly better (or worse) than the other. The best CA was from FSSMC, which achieved 83.1% accuracy (averaged over all datasets), improving the performance of Naive Bayes on two data sets and degrading none of them. ReliefF was the second best, obtaining 82.7% average CA. Infor-mation Gain resulted in two improvements of CA, and ReliefF-RS had one. FSSMC also had the best performance for IB1 and
C4.5, improving the accuracy of eight and five datasets, respectively, and degradation accuracy on Nursery for Naive Bayes and C4.5. It was interesting to see that none feature selection methods worked well on Nursery for Naive Bayes and C4.5, but all of them improved the CA of IB1 significantly. IB1 may be more sensitive to irrelevant information than the other two classifiers, however the  X  X rrelevant X  features assessed by feature evaluators may be able to contribute  X  X ome X  information for C4.5 and Naive Bayes to achieve better CA. The experiment confirmed that the performance of IB1 can be improved by eliminating irrelevant features. It is interesting to note that the performance of C4.5 has been improved in most cases after removing irrelevant features. C4.5 has an inherent feature selection mechanism and has been reported to be insensitive to feature selection methods [29]. The reason accounting for the performance improvement of C4.5 could be that less features result in a smaller tree, which relieves the pruning burden and avoids the over-fitting problem associated with large decision trees. Information Gain did not enhance the performance of C4.5 significantly, which could be due to the similar feature eval-uation methods. From the experiments, FSSMC performed better than ReliefF in computing efficiency and CA ( Adult and
Nursery ). FSSMC was superior with categorical data in datasets Splice , Kr-vs-Kp and Nursery compared with ReliefF. Further-more, FSSMC preserved the classifiers X  accuracy on Letter in spite of its purely numerical data. Thus the selected data repre-sentatives maintained the information of all available data, and prevented the performance reduction of classifiers. Although tion of instances.

Overall, FSSMC had the best performance among all four feature selection methods, and ReliefF had better performance than Information Gain and ReliefF-RS, with respect to the accuracy of Naive Bayes, IB1 and C4.5. FSSMC was the only feature selection method among three which statistically improved C4.5 X  X  performance in Adult (categorical and numerical data) and
Splice (categorical data), and in the case of Ulster Diabetes (categorical and numerical data), FSSMC outperformed ReliefF in the accuracy using IB1. Additionally, it had the best performance in Nursery (categorical data) after feature reduction.
Feature selection methods benefited IB1 most. After removing the irrelevant information, IB1 performs significantly bet-ter than without feature selection. The success of RelieF and FSSMC with C4.5 could be attributable to their ability to identify feature interactions. Including strongly interacting features in a reduced subset increases the likelihood that C4.5 will dis-cover and use interactions early on in tree construction before the data becomes too fragmented. On the other hand, Naive
Bayes was unable to make use of interacting features due to its conditional independence assumption. However, Naive Bayes was able to obtain performance improvement after feature selection. Although Naive Bayes could be influenced by strongly correlated features, ReliefF and FSSMC can identify partially correlated or redundant features. According to Robnik-Sikonja and Kononenko [33,20] , the estimates of Relief algorithms are the average over local estimates in smaller parts of the in-stance space. This enables them to take into account the context of other attributes, i.e. the conditional dependencies (cor-relations) between the attributes given the predicted value, which can be detected in the context of locality. From the global point of view, these dependencies are hidden due to the effect of averaging over all training instances. Consequently, ReliefF 6. Conclusions
Choosing the best model for a given classification task depends not only on discriminatory power, but also on the cost of construction and interpretability [7]. Feature selection has been effective in dimensionality reduction, removing irrelevant data to decrease the cost of model construction, increasing accuracy, and improving comprehensibility [15]. The contribution of this work is to optimize ReliefF for better processing performance for large datasets.

In dealing with large datasets ReliefF can use the complete dataset for feature evaluation, which may lead to high com-putational expense, or apply random sampling. FSSMC exploits data characteristics and achieves better performance than random sampling: it maintains the CA of ReliefF and is computationally more efficient. In order to select representative in-stances, data is partitioned according to similarity and representative instances selected from the resulting partitions. There are many data partitioning techniques [10] reported in the literature on multidimensional indexing. Distance-based similar-ity evaluation is used for data partition in this study. By the selection of each representative, m is decided automatically for each dataset respectively, without user X  X  intervention.

Experiments have been carried out on 13 datasets. The relevant features are previously unknown. Three practical classi-fiers (Naive Bayes, IB1 and C4.5) have been used for performance evaluation. FSSMC improves the computation efficiency of
ReliefF, whilst maintaining CA. Due to the significant influence on IB1 X  X  performance caused by irrelevant features, results show that feature selection methods benefit IB1 most. On average, Naive Bayes and C4.5 enhance their accuracy using selected feature subsets, though the improvement is small. FSSMC can be of use to classification algorithms in terms of improving accuracy (compared to datasets without feature selection) and, in the case of C4.5, improving the comprehen-sibility of the induced model. Since the dimensionality of the data is reduced, all three classifiers execute noticeably faster, making data mining potentially more useful in large databases.

The overall investigation has shown encouraging results of FSSMC. There may be still a possibility for improvement such within the various domains, in particular with large practical databases such as those found in the clinical domain. Authors X  contributions YH designed and developed the algorithm, designed and ran the experiments, and is the primary author.
 PJM supervised the research, contributed to manuscripts, and verified the experiment results.
 NDB supervised the research, contributed to manuscripts, and discussed the significance of the experiment results. Statement on conflict of interest
The research was a collaboration between the University of Ulster and the Ulster Hospital (part of the South Eastern Trust in Belfast). Funding was made available to YH as a University of Ulster Vice Chancellor Research Scholarship award. The authors envisage no conflict of interest.
 Acknowledgements The authors would like to thank Professor R Harper from Ulster Hospital, Belfast, BT16 0RH, UK for assistance with Ulster
Hospital dataset and his expert advice on attributes associated with diabetes. The work received ethical approval by The Of-fice for Research Ethics Committees Northern Ireland, (Study number 02/56: The application of decision support for therapy planning in Type 2 diabetes), via University of Ulster ethical committee. The authors would like to thank both committees.
References
