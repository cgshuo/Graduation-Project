 Long-running, high-impact events such as the Boston Marathon bombing often develop through many stages and involve a large number of entities in their unfolding. Timeline summa-rization of an event by key sentences eases story digestion, but does not distinguish between what a user remembers and what she might want to re-check. In this work, we present a novel approach for timeline summarization of high-impact events, which uses entities instead of sentences for summa-rizing the event at each individual point in time. Such entity summaries can serve as both (1) important memory cues in a retrospective event consideration and (2) pointers for per-sonalized event exploration. In order to automatically create such summaries, it is crucial to identify the  X  X ight X  entities for inclusion. We propose to learn a ranking function for entities, with a dynamically adapted trade-off between the in-document salience of entities and the informativeness of entities across documents, i.e., the level of new information associated with an entity for a time point under considera-tion. Furthermore, for capturing collective attention for an entity we use an innovative soft labeling approach based on Wikipedia. Our experiments on a real large news datasets confirm the effectiveness of the proposed methods.

High-impact, real-world events often unfold in an unex-pected way, dynamically involving a variety of entities. This is especially true for long-running events when full informa-tion about the event and its development becomes available only in the course of days after the happening, as in the case of a recent Germanwings airplane crash. In this paper, we study event timeline summarization and present a novel method which shows key entities at different time points of an event thus capturing this dynamic event unfolding. In contrast to other work in event summarization [34, 23], our entity timelines use entities instead of sentences as main units of summarization as depicted in the case of the 2015 Germanwings plan crash (Figure 1). Such summaries can be easily digested and used both as starting points for per-sonalized exploration of event details, and for retrospective revisiting. The latter can be triggered by a new similar event, or by a new twist in the story. For example, the tes-timonial of the captain in the Costa Concordia trial in late 2014 triggered a revisiting of the disaster in 2012.
From a cognitive perspective, for event revisiting, we rather create  X  X emory cues X  to support remembering the unfolded events than summaries for rehearsing the details. In fact, memory cues can be regarded as  X  X  circumstance or piece of information which aids the memory in retrieving details not re-called spontaneously X  (Oxford online dictionary, 2015). In this sense, our work is related to the idea of designing or creating memory cues for real-life remembering [29]. En-tities such as persons and locations have been identified as very effective external memory cues [2]. In addition, the im-portance of entities in event summarization has also been shown in recent work [24, 25].

For creating an entity timeline,the entities to be used in the summary have to be chosen carefully. They should 1) be characteristic for a respective time point of the event, 2) not be repetitive (if nothing new happened with respect to the entities), 3) be associated to relevant event information, and 4) be interesting to the reader. For this purpose, we pro-pose an approach to entity summarization, which dynami-cally combines entity salience with the informativeness of entities at a considered point in time. Entity salience, on the one hand, considers the property of being in the focus of attention in a document has been studied in previous work [5, 12, 10]. In [5], Boguraev and Kennedy use salient text phrases for the creation of so-called capsule overviews , whereas recently methods for the identification of salient entities, e.g., in Web pages [12] and news articles [10], have been developed. Informativeness, on the other hand, as-sesses the level of new information associated with an entity in a text and can be computationally measured using fea-tures derived from statistical and linguistic information [33].
In more detail, we aim at optimizing a trade-off between the in-document salience of entities and the informativeness of entities across documents describing the unfolding of an event. Our contributions are:
We validate our contributions with experiments on a large real-world news dataset covering various types of events. The rest of the paper is organized as follows. Section 2 discusses the related work. Sections 3 and 4 detail our pro-posed method. Section 5 discusses features used in learning framework. Section 6 presents the experiments and results. Finally, Section 7 concludes our paper. News Summarization. The task of news summarization has been already studied in various contexts, which range from focusing on multi-document summarization [5, 11] to generating a timeline summary for a specific news story [34, 23, 28, 35]. News stories can be complex, having a non-linear structure and associated to multiple aspects. Shahaf et al. [27] propose a novel method for summarizing complex stories using metro maps that explicitly capture the relations among different aspects and display a temporal development of the stories. Instead of using documents or sentences as a information units, we provide a set of entities, as memory cues , for supporting event exploration and digestion at each individual point in time. To the best of our knowledge, none of the previous work provides a trade-off solution that bal-ances between content-based and collective attention-based approaches, in supporting the entity-centric summarization. Entity Ranking and Entity Salience. In recent years, entity retrieval and ranking have gained interest from the IR community moving beyond traditional document retrieval to more specific information about entities in response to a query, e.g., [24, 25]. Although a lot of interesting work on entity ranking has already been proposed, most of previous works have focused on static collections, thus ignoring the temporal dynamics of queries and documents. The relevant work closest to ours in this respect is by Demartini et al. [8], where the task tackled is to identify the entities that best describe the documents for a given query. The entities are identified by analyzing the top-k retrieved documents at the time when the query was issued as well as relevant docu-ments in the past. In IR, salient entities in news documents help improving the performance of document retrieval [25]. There is a body of work in identifying salient entities in gen-eral Web [12] and in news domain [10], but the existing work does not take into account the time dimension. Our work targets a different question, using entities as a pivot, and it considers the global context (timeline of the event) as a new dimension of entity salience determination.
 Learning to Rank Multiple Criteria. In our work, we apply a learning to rank (L2R) technique to generate time-line summaries in a similar way as done in [28, 23]. In con-trast to the previous work, we propose a novel joint learning method, which optimizes different aspects of an event. The model is able to distinguish the aspects semantically. We also incorporate the time effect (i.e., decay) into the joint model, making our learning to rank time-sensitive and suit-able for timeline summarization. Long-running Event. Following [23], we define a long-running event as  X  X  newsworthy happening in the world that is significant enough to be reported on over multiple days X . Each event is represented by a short textual description or a set of keywords q , where we will use q to denote the event in the rest of this paper. For example, the bombing incident during Boston Marathon in April 2013 can be described by the terms  X  X oston marathon bombing X . We assume that the relevant time frame is split into a sequence of consecutive non-overlapping, equal-sized time intervals T = { t 1 ,...,t in a chronological order, in our case individual days. Fur-thermore, for a given event q , there is a set of timestamped documents D q (each with a publication date) reporting on the event. We define a reporting timeline T q = { t k 1 ,...,t as an ordered list of (not necessarily consecutive) those time periods t k i in T , which contain the publication date of at least document in D q . Finally, we denote the set of all doc-uments about q published within a time period t k i as D q,i Entity. We are interested in named entities mentioned in documents, namely, persons, organizations, locations. An entity e can be identified by a canonical name, and can have multiple terms or phrases associated with e , called labels , which refer to the entity. We call an appearance of an entity label in a document, a mention of e , denoted by m . We define E q as the set of all entities mentioned in D q . Further-more, we define the text snippet surrounding m (e.g., sen-tence or phrases) as the mention context , denoted c ( m ). Entity Salience and Informativeness. Similarly to [10], we define the entity salience as the quality of  X  X eing in the focus of attention X  in the corresponding documents. An-other relevant aspect considered for selecting entities to be included in an event timeline is informativeness [12], which imposes that selected entities in an evolving event should also deliver novel information when compared to the past information. For example, although the airline  X  X erman-wings X  stays relevant for many articles reporting on the plane crash, it will only be considered as informative, if new infor-mation about the airline becomes available.
 Problem. Given a long-running event q , a time interval t in its reporting timeline T q , and the set of entities E aim to identify the top-k salient and informative entities for supporting the exploration and digestion of q at t i .
We tackle this problem by learning a function for ranking entities, which is aimed at optimizing the trade-off between in-document entity salience and the informativeness of enti-ties across documents: where y ( q ) t is the vector of ranking scores for entities in E at time interval t , E is a matrix composed from feature vec-tors of entities in E q extracted from their mention contexts.  X  , X  i are the unknown parameter vectors for ranking enti-ties based on salience and informativeness, respectively.
In our work, a ranking function is based on a learning-to-rank technique [19]. A general approach for learning-to-rank is to optimize a defined loss function L given manual annotation or judgments y ( q ) j of entities for a set of training events Q within the time intervals T q :
Two major challenges must be taken into account when learning a ranking function defined in Equation 2. First, we need a reliable source for building judgments (ground truths) for annotating entities by considering their salience with re-spect to a given event. In addition, the judgments must be dynamically adapted to the evolving of entities along the unfolding event, i.e., bearing of novel information. Second, the models of our two aspects  X  s , X  i must be unified to pro-duce a joint learned function for ranking entities. In the subsequent sections, we will explain our proposed method for these challenges in more detail.
Figure 2 gives an overview of our entity ranking frame-work covering both the training and the application/testing phase.

Given one event q , its reporting timeline, and the set of documents D q (in practice, D q can be given a priori, or can be retrieved using different retrieval models) we identify the entity set E q using our entity extraction, which consists of named entity recognition, co-reference and context extrac-tion (Section 4.1).

When the event is used for training (training phase), we link a subset of E q to Wikipedia concepts, which comprises the popular and emerging entities of the event. To facil-itate the learning process, these entities are softly labeled using view statistics from Wikipedia (Section 4.2), serving as training instances. Although we use popular entities for training, we design the features such that it can be general-ized to arbitrary entities, independent from Wikipedia.
The next component in our framework is the adaptive learning that jointly learns the salience and informativeness models, taking into account the diverse nature of events and their evolution. (Section 4.4).

In the application phase, entity and feature extraction are applied the same as in training phase. First, the input event and time interval is examined against the joint models to return the adaptive scores (details in Section 4.5). Then, entities are put into an ensemble ranking, using the adapted models, to produce the final ranks for the summary.
First, we discuss how we can extract entities, the key units in our framework. For each document in the set, , we employ a named entity recognizer to identify mentions of entities in three categories (persons, locations and organizations). For each mention, we include the containing sentences as the context. Mentions that do not contain any alphabetical characters or only stop words are removed.

We additionally use intra-and cross-document co-reference to track mentions pertaining to the same entity. First, an intra-document co-reference system is employed to identify all co-reference chains for entity mentions within a docu-ment. We include each reference in the chain together with its sentence to the set of mention contexts of the entity. Second, to identify mentions that potentially refer to the same real-world entity across documents, we adapt the state-of-the-art cross-document co-reference method proposed by Lee et al.[22]. This method first clusters documents based on their content using an Expectation Maximization (EM) al-gorithm, then iteratively merges references (verbal and nom-inal) to entities and events in each cluster.
 Speeding up co-reference resolution. To speed up the computation, we do not use EM clustering as [22], but em-ploy a set of heuristics, which have proven to be effective in practice. First, we only consider cross-document co-references from documents of the same day. Second, instead of clus-tering an entire document set, we use mentions with their contextual sentences (kept in the order of their appearance in the original documents) as  X  X seudo-documents X  for the clustering. Third, we assume that mentions to the same en-tity have similar labels. Hence, we represent entity mention labels as vectors using two-grams (for instance,  X  X bama X  be-comes  X  X b X ,  X  X a X ,  X  X m X ,  X  X a X ) and apply LSH clustering [15] to group similar mentions. LSH has been proven to perform well in entity disambiguation tasks [17], and it is much faster than the standard EM-based clustering.

Finally, for each entity mention, we merge its contextual sentences with those of all other references of the same co-reference chain to obtain the event-level context for an entity , which will be used as inputs for constructing the entity features. We note that while we are aware of other methods to increase the quality of entity extraction by linking them to a knowledge base such as YAGO or DBpedia, we choose not to limit our entities to such knowledge bases, so as to be able to identify and rank more entities in the  X  X ong tail X .
In the following, we explain how the training data is gen-erated. Specifically, given an event q , an interval t and an entity e , we aim to automatically generate the score y than the entity e 0 with respect to the event q at time t . This score can be used as a soft labelling to learn the ranking functions mentioned in Equation 2. The use of soft labeling for entities X  salience has already been proposed in [12], where user click behaviour in query logs is used as an indicator for entity salience scores. Dunnietz et al. [10] proposed treat-ing entities in news headlines as salient, and propagate those salience scores to other entities via the PageRank algorithm. The limitation of these measures is that they restrict the as-sessment of salience to the scope of individual documents, and do not consider the temporal dimension. In contrast, our soft labels are evolving, i.e., an entity can have different labels for one event at different time intervals.

The soft labeling is based on the assumption that for glob-ally trending news event, prominence of related entities can be observed by the collective attention paid to resources representing the entities. For instance, during the Boston marathon bombing, Wikipedia pages about the bomber Tsar-naev were created and viewed 15,000 times after one day, indicating their strong salience driven by the event. For soft labeling, we exploit the page view statistic of Wikipedia ar-ticles, which reflects the interest of users in an entity: Most obviously, Wikipedia articles are viewed for currently pop-ular entities indicating entity salience. However, taking the encyclopedic character of Wikipedia into account, Wikipedia articles are also viewed in expectation of new information about an entity indicating its (expected) informativeness, especially in the context of an ongoing event. Actually, Wikipedia has gained attention in recent years as a source of temporal event information [32]. Event-triggered bursts in page views, as they are for example used in [7] for event detection, are thus a good proxy for the interestingness of an event-related entity at a considered point in time, which is influenced both by the salience and the informativeness of the event.
 Therefore, we propose a new metric called View Outlier Ratio or VOR to approximate the soft labels for a combined measure of entity salience and informativeness as follows. For each entity e and for a given time interval t i , we first construct the time series of view count from the correspond-ing Wikipedia page of e in the window of size w : where each v e,j is the view count of the Wikipedia page of e at t j . From T e we calculate the median m e,i and define VOR as follows.

Definition 1. The View Outlier Ratio is the ratio of dif-ference between the entity view and the median: where m min is a minimum threshold to regularize entities with too low view activity.
We now turn our attention to defining the ranking func-tion in Equation 1. The intuition is that for each event q and time t i , we rank an entity e higher than others if: (1) e is more relevant to the central parts of documents in D where it appears (salience); and (2) the context of e is more diverse to other contexts (of e or other entities) at D q,i  X  1 Moreover, these two criteria should be unified in an adaptive way that depends on the query, that is, event and time. For example, users interested in a festival might wish to know more about salient entities of the event, while those that follow a breaking story prefer entities with more fresh in-formation. Even for one event, the importance of salience and informativeness might vary over time. For instance, informativeness is more important at the beginning when the event is updated frequently. Based on this intuition, we propose the following ranking function: where E s is the | E q | X  M matrix representing the M dimen-sional feature vectors of entities used to learn the salience score ( M is the number of salience features), and E i is the | E | X  N is the matrix of N dimensional informativeness fea-ture vectors ( N is the number of informativeness features). S ( q,t ) and I ( q,t ) represent the scores of salience and infor-mativeness tendency for an event q at t . Here we introduce another factor,  X  ( t ), which is the decay function of time t , controlling how much the informativeness should have im-pact on the overall ranking. The rationale of  X  is that when the distance between two time intervals t i and t i  X  1 1 informativeness has less impact on the overall ranking. For example, if there are only reports about the news after one year (anniversary of a past event), the changes of entities in that long time period should not contribute much to the informativeness criterion.
We now discuss how to learn the above ranking function using Equation 2. A straightforward way is to learn the two models  X  s and  X  i separately, and assign values to S,I in a pre-defined manner, then aggregate into Equation 4. However, this is not desirable, since it requires to build two sets of training data for salience and informativeness at the same time, which is expensive. Secondly, previous work has pointed out that a  X  X ard X  classification of query based on intent itself is a difficult problem, and can harm the ranking performance [14]. In this work, we exploit the divide and conquer (DAC) learning framework in [3] as follows. We define E  X  as the | E | X  ( M + N ) matrix of ( M + N ) dimen-sional extension vectors from the corresponding vectors of E s and E i matrices. Similarly, we define  X   X  s as the ( M + N ) extension vectors of zero vector 0 , and the vectors  X  s , and  X  i as the ( N + M ) extension vector of  X  i and 0 . With this transformation, Equation 4 can be written as: where g ( E  X  , X  ) =  X  T E  X  is a linear function. Incorporating Equations 2 and 5, we can co-learn the models  X   X  s , X   X  i thus  X  s , X  s ) simultaneously, using any loss functions. For instance, if we use hinge loss as in [3], we can then adapt the RankSVM [19], an algorithm that seeks to learn the linear function g in (5) by minimizing the number of misordered note that news events are not always reported on consecu-tive time intervals document pairs. For completeness, we describe here the traditional objective function of RankSVM: than entity b for the event q at time t ,  X  q,t,a,b denotes slack variables, and c sets the trade-off between the training error and model complexity. If we change the linear function g to f , we can adapt (2) into (6) to obtain the following objective function:
The adaptive scores S ( q,t ) ,I ( q,t ) and the decay function  X  ( t ) is critical to adapting the salience and informativeness models. A na  X   X ve supervised approach to pre-define the cat-egories for event ( S,I ) is impractical and detrimental to ranking performance if the training data is biased. Instead, previous work on query-dependent ranking [14, 3] often ex-ploit the  X  X ocality property X  of query spaces, i.e., features of queries of the same category are more similar than those of different categories. Bian et al.[3] constructed query fea-tures using top-retrieved documents, and clustered them via a mixture model. However, the feature setting is the same for all clusters, making it hard to infer the semantics of the query categories.

In this work, we inherit and adjust the approach in [3] as follows. For each event q and time t , we obtain all en-tities appearing in D q,t to build the  X  X seudo-feedback X  for the query ( q,t ). We then build the query features from the pseudo-feedback as follows. From each matrix E s , E take the mean and variance of the feature values of all en-tities in the pseudo feedback. As a result, each pair ( q,t ) is mapped into two feature vectors (with 2 M and 2 N di-mensions) corresponding to the salience and informativeness spaces. In each space, we use Gaussian Mixture model to calculate the centroid of the training queries, and use the distance of the query feature vector to the centroid as its corresponding adaptive score: where C  X  { I,S } indicates the event categories, x q,t C query feature in the feature space of C , and x C is the cen-troid of feature vectors in training set Q in the corresponding space. The scores are scaled between 0 and 1.
 Decayed Informativeness. The decay function  X  ( t i ) ad-justs the contribution of informativeness into the adaptive model and is defined by: where  X , X  are parameters (0 &lt;  X  &lt; 1 , X  &gt; 0), and  X  is the interval unit distance. Equation 9 represents the time impact onto the informativeness of entities: When the time lag between two intervals is high, the difference in contexts of entities between them is less likely to correlate with the informativeness quality of entities.
We now discuss the salience and informativeness features for entity ranking. Ranking features are extracted from event documents where the entity appears as follows. These features, called individual features , are extracted two differ-ent levels. First, at context level, features are extracted independently from each mention and its contexts. Features of this level include mention word offset, context length, or importance scores of the context within the document us-ing summarization algorithms (SumBasic or SumFocus fea-tures). Second, at label level, features are extracted from all mentions, for instance aggregated term (document) fre-quencies of mentions.

Based on the individual features, the entity features are constructed as follows. For each entity and feature dimen-sion, we have the list of feature values ( z 1 ,z 2 ,...,z z is the individual feature of label or mention categories. For label level, we simply take the average of z i  X  X  over all entity labels. For mention level, each z i is weighted by the the confidence score of the document containing the corre-sponding mention and context. Such confidence score can be calculated by several ways, for instance by a reference model (e.g., BM25) when retrieving the document, or by calculating the authority score of the document in the col-lection (e.g., using PageRank algorithm). For all features, we apply quantile normalization, such that all individual features (and thus entity aggregated features) are scaled be-tween [0 , 1]. Below we describe the most important features. Context Importance Features. One important evidence of entity salience is context of the entity mentions. It is well-known that text at the beginning of document contains more salient information [12, 10]. Besides position, the content of sentences, per se or in relations with other sentences, also indicates the salience of entities. We apply SumBasic [26] and LexRank [11] summarization algorithms to obtain the scores of contexts (features Sum-B and PR, respectively). Human-perceived Salience Features. Entity salience can be assessed by reader X  X  intentions or interests [12], and recent study suggests that user interest in entities can be at-tracted via serendipity in texts [6]. We follow this direction and apply features presented in[6], namely setimentality, at-titudes, and readability scores of each mention context. For readability, we also include two other standard metrics, Gun-ning Fog index [16] and Fleisch-Kincaid index [20]. Query-related Features. Another class of salience fea-tures involves ones that are dependent on the event queries. Following [31], we use the overlap between contexts and the event query at the unigram, bigram levels, and at bigram where tokens are tolerable to have 4 words in between (Bi4 feature). We also compute the query-focused SumFocus [30] score of contexts as another feature. It is worth noting that for all of these features, the event queries used are the ones that have been extended using the query expansion (see Event Document and Timelines, Section 6.1).
Informativeness for words has been studied extensively in linguistics community. We adapt the state-of-the-art mea-sure [33], which incorporates the context of mentions. Given a mention m of entity e , the context-aware informativeness score of m for event q at t i is defined by: where U q,i  X  1 is the set of mention contexts of e in D q,i  X  1 d 0 is the document consisting the context c 0 ,  X  ( c 0 ,c ( m )) is the sentence dis-similarity, and s ( d c 0 ,q,i  X  1) is the retrieval score of d c 0 . We normalize the metric to [0 , 1], and set it to 1, when the entity first appears in D q,i . To measure  X  ( c 0 ,c ( m )), we can employ different strategies, leading to different features: CTI. The most straightforward strategy is to employ a lexi-con for the sentence similarity. We use the NESim method [9] for this strategy.
 Topic Diversity. Besides lexicon-based, we also calculate the informativeness on a higher level by representing contexts by latent topics, using latent Dirichlet Allocation model [4]. Topic diversity of a context c w.r.t other context c 0 of the same entity on previous time interval is defined as where  X  is the number of topics and  X  k is the topic index. Distributional Similarity. Another strategy uses Kullback-Leibler divergence for the dissimilarity: where  X  d and  X  c are the language models for contexts c 0 We use Dirichlet smoothing method for scarce words. Label Level. Besides context level, we also calculate a number of features at label level, by aggregating the men-tion features over D q,i  X  1 for event q at time t i (Table 1). For label-level topic diversity, we use the same metric, but using concatenated contexts instead of individual ones. Ad-ditionally, we also calculate the entity difference between two context sets of the same entity in D q,i  X  1 and D q,i where CoEnt i ( e ) is set of co-occurring entities of e in D Datasets. For our experiments, we work with a real-world, large-scale news dataset. Specifically, we use KBA 2014 Filtered Stream Corpus (SC14) dataset, which is used for TREC 2014 Temporal Summarization track 2 . We extract news and mainstream articles from the dataset, consisting of 7,592,062 documents. The dataset covers 15 long-running news events from December 2012 to April 2013. All events are high-impact, discussed largely in news media, and have their own Wikipedia pages. Each event has a predefined time span (ranging from 4 to 18 days), and is represented by one textual phrase that is used as the initial event query. Based on the event type (available in the dataset), we group the events into 4 categories: Accident, riot and protest, nat-ural disaster, crime (shooting, bombing).
 Event Documents. To construct the event document set for the study, we firstly group the documents into individ-ual days by their publication timestamps, and index docu-ments for each day. In total, this results in 126 different in-dices. For each index, we remove boilerplate texts from doc-ument using Boilerpipe [21], skip stop words, and lemmatize the terms. Then we use the pre-defined textual phrases of the events, issue it as a query to the corresponding indices (indices of days within the event period), retrieving from each the top 10 documents using BM25 weighting model. We improve the results using Kullback-Leibler query expan-sion [18], and add top 30 expanded terms to construct the event query q used for query-related features computation. Timelines. To build the reporting timeline T q , for each event, we manually go through all the days of the event pe-http://s3.amazonaws.com/aws-publicdatasets/trec/ kba/index.html riod, check the content of the top-retrieved document, and remove the day from the timeline if this top-ranked docu-ments is not about the event. In total, we have 153 pairs ( event , day ) for all event reporting timelines.
 Entities. We use Stanford parser to recognize named en-tities from the boilerplated content of the documents, and match them with the entities detected by BBN X  X  Serif tool (provided in SC14 corpus) to reduce noise. For the matching entities, we use the in-document co-reference chains, which are available in SC14, and apply the cross-document coref-erence (Section 4.1) to group mentions to entities. We use the sentences as the mention contexts. In total, we detect 72,267 named entities from the corpus, with an average of 5.04 contexts per each.
 Training Data. From 153 ( event , day ) pairs, we randomly choose 4 events belonging to 4 different categories men-tioned above as a training data, resulting in 39 pairs. To build training entities (i.e. to identify subset of entities to Wikipedia concepts, see Section 4.1), we apply two named entity disambiguation softwares, WikipediaMiner and Tagme. These are the supervised machine learning tools to identify named entities from natural language texts and link them to Wikipedia. We train the models of both the tools from a Wikipedia dump downloaded in 2014 July, so as to cover all possible entities in the SC14 corpus. We only use entities co-detected by both the tools, resulting in 402 distinct enti-ties and 665 training tuples ( entity , event , day ). We use the Wikipedia page view dataset, which is publicly available, to build the soft labels for these entities.
 Parameter Settings. We modify RankSVM for our joint learning tasks. Features are normalized using the Stan-dard scaling. We tune parameters via grid search with 5-fold cross validation and set trade-off parameter c = 20. WikipediaMiner and Tagme tools are used with default pa-rameter settings. For the decay parameters, given the rather small time range of events in our dataset, we empirically set  X  = 2 , X  = 0 . 5,  X  = 1day, and leave more advance tuning for future work. For soft labeling, we set the window size w to 10 days, which is the average length of reporting timelines. The threshold m min is tuned as followed. For each training pair, a human expert knowing the 4 training events well is presented with the entities, their mention contexts and con-tent of the corresponding document. The expert is asked to put the labels on the entity from  X  X alsely detected X ,  X  X on-salient entity X ,  X  X alient but not informative X  to  X  X alient and informative X . Based on this judgement, we compute VOR scores with m min from 1 to 100, and optimize the rank of entities based on VOR scores using NDCG metric. We find that m min = 12 yields the best performance. Baselines. We compare our approach with the following competitive baselines.

TAER : Dermatini et al. [8] proposed a learning framework to retrieve the most salient entities from the news, taking into consideration information from documents previously published. This approach can be considered as  X  X alience-pro X , since the entity salience is measured within a docu-ment, although it implicitly complements the informative-ness via history documents. We train the model on the same annotated data provided by the author.

IUS [23]: This work represents the  X  X nformativeness-pro X  approach, it attempts to build update summaries for events by incrementally selecting sentences, maximizing the gain and coverage with respect to summaries on previous days. Since we are not interested in adaptively determining the cutoff values for the summary, we implement only the learning-to-rank method reported in [23] to score the sentences. We adapt IUS into entity summarization by extracting named entities from each sentence. Then, the ranking score of the entity is calculated as the average of scores of all of its sen-tences across all documents.

In addition, we evaluate three other variants of our ap-proach. The first two variants involve only salience and in-formativeness features for learning. We denote these as SAL and INF . The third variant linearly combines all salience and informativeness features, denoted as No-Adapt . All are trained and predicted using the traditional RankSVM. Evaluation Metrics. We consider the traditional informa-tion retrieval performance metrics: precisions, NDCG and MAP. Besides, we also aim to evaluate the performance of ranking in timeline summarization context, where effective systems do not just introduce relevant, but also novel and in-teresting results compared to the past. This was inspired by recent work in user experience of entity retrieval [13, 6]. One popular metric widely adopted in existing work to measure such user-perceived quality is the serendipity , which mea-sures degree to which results are  X  X ot highly relevant but interesting X  to user taste [1]. Traditional serendipity metric was proposed to contrast the retrieval results with some ob-vious baseline [6]. In our case, we propose to use serendipity to measure the informativeness and salience by contrasting the results of one day to previous day of the same event: where UNEXP is the set of entities not appearing on the previous day, and rel is the human relevance judgment of the entity. The relevance part ensures the salience of the entity, while the UNEXP part ensures the informativeness of the entity over the event reporting timeline.

We exclude the 39 training pairs from the overall 153 ( event , day ) pairs to obtain 114 pairs for testing. For each of these pairs, we pooled the top-10 entities returned by all methods. In total, this results in 3,336 tuples ( entity , event , day ) to be assessed. To accommodate the assessment, we contextualize the tuples as follows. For each tuple, we ex-tract one sentence containing the entity from the document with the highest retrieval score (BM25), using the event as the query and on the index corresponding to the day. If there are several sentences, we extract the longest one. Next, we describe our two assessments setup, expert-based and crowdsource-based.
 Expert Assessment. To evaluate the quality of the sys-tems, we employ an expert-based evaluation as follows. 5 volunteers who are IT experts and work on temporal and event analysis were asked to assess on one or several events of their interest. For each event, the assessors were encour-aged to check the corresponding Wikipedia page beforehand to gain sufficient knowledge. Then, for each tuple, we add one more contextualizing sentence, extracted from the pre-vious date of the event. If there is no such sentence, a  X  X IL X  string will be presented. We asked the assessors to check the tuple and the two sentences, and optionally, to use search engines to look for more event information on the questioned date. Then, the assessors were asked to assess the impor-tance of the entity with respect to the event and date, in four following scales. 1 : Entity is obviously not relevant to the event; 2 : Entity is relevant to the event, but it has no new information compared to the previous day; 3 : Entity is relevant to the event and linked to new information, but it does not play a salient role in the sentence; 4 : Entity is relevant to the event, has new information, and is salient in the presented sentence. The inter-assessor agreement score for this task is  X  = 0 . 4 under the Cohen X  X  Kappa score. Crowdsourced Assessment. In addition, we also set up a larger-scale assessment based on crowdsourcing. We use Crowdflower.com platform to deploy the evaluation tasks. Each tuple presented to workers consists of date, short de-scription of the event, entity, and the sentence. Instead of direct asking for salience and informativeness of entities to event and date we decide for a simpler task: Asking the workers to assess entities in two steps: (1) Assessing whether the sentence is obviously relevant to the event (workers as-sess on a 3-point Likert scale,from  X 1-Not Relavant X  to  X 3-Obviously Relevant X ); and (2) assessing whether the entity is important in the sentence (by virtue of being a subject or object of the sentence, binary feedback).

Tasks are delivered such that tuples of the same (event,day) pair go into one Crowdflower job, thus the worker has a chance to gain knowledge about event on the day and re-spond faster and more reliably. We pay USD 0 . 03 for each tuple. To maintain the quality, we follow state-of-the-art guidelines and recommendations, and receive 5 independent responses for each tuple. We create a gold standard for 311 tuples, and discard responses from workers who fail to main-tain an agreement of above 70% against the gold standard. In total, we received 20 760 responses, 8 940 from which were qualified. The inter-worker agreement was 98 . 67% under Pairwise Percent Agreement, with average variance of 42%, indicating a reasonably good quality given the fairly high complexity of the task.
The upper part of Table 2 summarizes the main results of our experiments from the expert evaluation. The results show the performance of the two baselines ( TAER and IUS ) and of the consideration of Salience and Informativeness fea-tures in isolation with respect to precision. In general, all performances are low, indicating the relatively high com-plexity of this new task. In addition, as can be seen from this part of the table, even the approach relying on our salience features or informativeness features in isolation already out-performs the two baselines. This is due to the fact that our approach does not consider documents in isolation as the baselines do. Rather, we take a more comprehensive view considering event level instead of document level features via feature aggregation. In more details, the first baseline ( TAER ) employs a quite restricted features for entity rank-ing (e.g. document frequency), and thus fails to identify important entities event-wise.

Furthermore, the results also show the performance of the non-adaptive combination of salience and informativeness ( No-Adapt ) as well as our approach ( AdaptER ), which uses an adaptive combination of informativeness and saliency. It becomes clear that an improvement by combining the salience and the informativeness features over the use of the isolated features can only be achieved by fusing the two features in a more sophisticated way: No-Adapt does not perform better than the maximum of SAL and INF ( MAX ( S,I )), it even performs worse in under some metrics such as P@10. In contrast, AdaptER clearly outperforms the maximum of SAL and INF as well as its non-adaptive version for most metrics. For instance, we achieve 16% im-provement of MAP scores as compared with the MAX ( S,I ).
Besides precision, we also consider serendipity (SRDP) as a complementary measure in our experiments, as discussed above. This metric measures how likely the approach brings unseen and interesting results to the user. Under SRDP, our approach outperforms significantly both the baseline and the maximum of SAL and INF . We achieve 14% improvement of serendipity at top-1 entities, and 29% at top-3 entities. Thus, our top-retrieved entities do not only cover relevance, but are also more interesting, often unseen on the previous day (contributing to more informative results).

The lower part of the Table 2 shows the same results for the crowdsourced assessment. The same trends of perfor-mance can be observed, where our approach outperforms in all the metrics. In comparison to the expert evaluation, the results are overall lower. A possible reason for this is the complexity of the crowdsourcing task, which requires knowl-edge about the considered event in order to give high quality feedback (see Expert Assessment setup, Section 6.1). Never-theless, the adaptive model is still able to achieve significant gain, especially under the serendipity measurement.
Next, we evaluate the effectiveness of soft labeling in cov-ering salience and informativeness. For this purpose, we manually annotate entities obtained from the reporting time-line of 4 training events, with respect to the salience and informativeness (see Parameter settings, Section 6.1). We then re-train both non-adaptive and adaptive models using this annotated data (supervised approach). Table 3 shows the precision and MAP scores of the supervised approach in comparison to our soft labelling-based approach. The simi-larity in performance between them, regardless of the mod-els they have been used for, confirms that our soft labelling properly captures both salience and informativeness. Feature Analysis. Analysing the influence of different fea-ture groups (see Section 5) can give insights into what fac-tors contribute to the entity ranking performance. To study the feature impacts, we do an ablation study and remove incrementally a group of features, and re-evaluate the per-formance using the expert assessment. Since reducing the feature dimensions directly affects the query features and its adaptive scores, to ensure the fair comparison, we perform the study for the non-adaptive setting. Table 4 shows the MAP scores of ablated models. The symbol H indicates a significant decrease with respect to No-Adapt model (with full features), and thus implies the high influence of the cor-responding feature group. From the table, we can see that N 0.421 0.320 0.240 N N 0.441 N 0.340 0.256 N the most influential feature groups include context impor-tance feature (salience features) and informativeness feature group of context level.
 Anecdotic Example. In table 5, we show one example of top-selected entities for the event  X  X oston marathon bomb-ing 2013 X . Additionally, we show some selected sentences covering the entities, to enable the understanding of the en-tities X  roles within the event on the presented days. As can be seen, the timeline corresponding to TAER approach (up-per part) gives more salience credits to entities frequently mentioned throughout the news (such as Boston marathon), keeping them in high ranks throughout the timeline. The approach is not responsive to less salient but interesting en-tities (such as Pop Francis, a rather unrelated entity to the event, but get involved via his condolecence and activities to victims of the bombing). On the other hand, using an adaptive ranking with informativeness incorporated, the re-sulting entities are not just more diverse (including related events such as Marathon Bruins), but also expose more new and emerging information.
In this paper, we have presented a novel approach for timeline summarization of high-impact news events, using entities as the main unit of summary. We propose to dy-namically adapt between entity salience and informativeness for improving the user experience of the summary. Fur-thermore, we introduce an adaptive learning to rank frame-work that jointly learns the salience and informativeness fea-tures in a unified manner. To scale the learning, we exploit Wikipedia page views as an implicit signal of user interest towards entities related to high-impact events. Our exper-iments have shown that the introduced methods consider-ably improve the entity selection performance, using both small-scale expert-based and large-scale crowdsourced as-sessments. The evaluation also confirms that integrating salience and informativeness can significantly improve the user experience of finding surprisingly interesting results.
As the problem discussed in the paper is new, there are several promising directions to explore for future work. We aim to investigate further the impact of adaptation in dif-ferent types of events, using larger and more diverse sets of events. Furthermore, we plan to study more advanced ways to mine Wikipedia temporal information as signals of collective attention towards public events. We are planning to use this for further improving our VOR measure for the soft labeling approach. Other direction includes investigat-ing deeper models to improve the performances of current entity timeline summarization systems, which are quite low. Acknowledgements. This work was partially funded by the European Commission for the FP7 project ForgetIT (under grant No. 600826) and the ERC Advanced Grant ALEXANDRIA (under grant No. 339233).
