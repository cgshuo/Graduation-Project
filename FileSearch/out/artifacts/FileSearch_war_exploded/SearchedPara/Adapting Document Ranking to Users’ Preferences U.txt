 This paper is concerned with the problem of improving document ranking in relevance to the query. During the use of the search engine we have collected users X  clicked documents are re-ranked on the top from among the relevant documents. That relevant documents. 
Usually users only look at the top ranked documents in search results (cf., [22] [23] [24]), and thus if we can rank users X  preferred documents on the top, then we will be able to improve users X  search experiences, for example, save users X  time in search. 
To the best of our knowledge, no investigation has been conducted on the problem, although there is some related work as described in Section 2. 
In this paper, we employ a machine learning approach to address the above problem and investigate a number of simple, but in our view, fundamental methods. experiments, the search engines used for collecting click-through data were also based Na X ve Bayes. Given a new query, BM25 and Na X ve Bayes can respectively construct a main method among them. relevant? Second, are the preference ranking methods indeed effective? Which method is more effective in performing the task? 
We conducted experiments in order to answer the above questions, using data from Second, experimental results indicate that the preference ranking methods using click-Na X ve Bayes performs the best. (We evaluated preference ranking results in terms of click distribution ). 
In conclusion, our methods can significantly improve preference ranking and at the same time preserve relevance ranking. 2.1 Use of Click-Through Data snippets. 
Click-through data can be recorded in operation of the search engine, as described sending the query. information. Here, by query logs we mean all the information which can be obtained during search of documents. Query log data, thus, contains click-through data. (Note that some researchers do not distinguish the two terms.) 
Methods of using click-through data have been proposed for meta-search function training, term expansion, query clustering, implicit feedback, etc. learning of a meta-search function. Specifically, he employs a method called Ranking search engine to a particular group of users. the methods by using a new metric called  X  X verage position of clicks X  calculated based on click-through data. The authors claim that the implicit evaluation method based on method based on users X  relevance judgments. quality term expansion can be conducted. 
Furthermore, Beeferman etc. [4] propose methods for harvesting clusters of queries problem, but focus on the issue of finding generalized query patterns and using them in the cache of a search engine. 
In [12] [18] and [21], click-through data is regarded as implicit feedback and used to improve relevance ranking. 
DirectHit was a commercial internet search engine (http://www.directhit.com). It is that in the current research; however, details of the technologies used are not known. 2.2 Meta Search and Relevance Feedback based on relevance , not based on users X  preference . 
Meta-search and related problems such as result fusion have been intensively to construct a meta-ranking function which combines the scores or ranks returned by regression, etc. Next, documents are retrieved with the new query. The process is repeated (cf., [20]). Since relevance judgments can be burdens to users, automatic ways of getting user X  X  query so that user X  X  information needs can be expressed more accurately. 3.1 Preference Ranking Using Click-Through Data adaptation) problem as follows: a search engine first receives a query from a user, and mechanism (e.g., BM25) are fixed). Next, a re-ranking engine re-orders the ranked list documents should be relevant and preferred by users. 
In one extreme case, the re-ranking engine completely ignores the initial relevance ranking engine only uses the initial relevance ranking. prefer those documents. In this paper, preferences are assumed to be from a group of users, not a single user. Preference is more subjective and dynamic, while relevance is fixed, while users X  preferences may change depending on user groups and time periods. users X  preferences. (Note that users X  preferences can be measured by other factors like uninteresting or even irrelevant later. The percentage of such  X  X oisy X  clicks over total We also assume here that there is no  X  X lick spam X . 
Figure 1 shows the differences between the current problem and other information retrieval problems: conventional search, meta-search, relevance feedback. The current problem focuses on preference ranking, while the other problems focus on relevance ranking. necessary and important for document retrieval. 3.2 Click Distribution An ideal way of evaluating the performance of a preference ranking method might be to select a number of users and a set of queries, and let every user to explicitly judge documents in descending order of their preference counts, and taking the order as the correct  X  X nswer X . This evaluation can give accurate results. However, it is costly, and thus is not practical. ranks (ideally the first k ranks) is better than a method that does not. can be used. The disadvantage is that click-through data may contain noise. However, as is explained above, the percentage of noise should be low. Thus, click distribution is a good enough measure for evaluation of preference ranking methods. We propose a machine learning approach to address the problem of preference ranked on the top from among the relevant documents. documents with scores. It is actually a problem of text classification. Specifically, we sources: the classifier and the search engine, in order to achieve better performances than using information from either of the sources alone. based on BM25.) For click-through data classification, we employ Na X ve Bayes (hereafter NB ). 
For combination of the two methods, we consider a number of strategies: (1) data use of click-through data. DC first combines click-through data with document data, and then uses BM. scores. NB, NB+BM, and LC-R can still work under such circumstances. 4.1 Notation D = { d } is a document collection. Q = { q } is a set of queries, and a query q consists of words. T = { t } is a set of words.
 S ( q , d m ). documents clicked by a user. Qd = { q } is the subset of Q related to document d in click-through data. 4.2 BM  X  Initial Ranking Function In BM, given query q , we first calculate the following score (using the default values of parameters in the formula of BM25) of each document d with respect to it and dl avg is average length of documents in D . ranked list of documents R BM ( q ). 4.3 NB  X  Click-Through Classification association can be found in click-through data (cf., Figure 2). Thus, we can construct document retrieval as that of text classification. follows. 
We calculate the probabilities using click-through data. 
Similarly we have we assume that they are equal for all documents. The converse category d consists of all documents except d . 
We use log of ratio of the two probabilities as score of document d with respect to query q . In NB, we rank documents by S NB ( q , d ) and then obtain R NB ( q ). 4.4 DC  X  Data Combination The above two methods rank documents based upon information from only one data methods which combine the information from both data sources. 
The simplest method of combination might be to combine the queries to their associated documents, view them as pseudo-documents, and run BM with the pseudo-documents. can rank the documents with the scores and obtain R DC ( q ). 4.5 NB+BM  X  NB First and BM Next documents whose NB scores are larger than a predetermined threshold, rank them by preference ranking. 
Let R BM ( q ) be the ranked list of documents returned by BM. Some of them can be at the top, and the remaining documents behind according to R BM ( q ). We then obtain R R
NB+BM ( q ) = &lt; d 3 , d 2 , d 1 , d 4 &gt;. 4.6 LC-S and LC-R  X  Linear Combination widely used method for such fusion is linear combination. Scores (LC-S), we rank documents by linearly combining the normalized scores returned by BM and NB: where ) , ( d q S
In Linear Combination of Ranks (LC-R) we rank documents by linearly combining the normalized ranks returned by BM and NB. Suppose that document d is ranked at i and n documents in R NB ( q ), then we have where  X  is weight. 
We determine the weight  X  in the linear combination by training on click-through data. distribution as the measure). 
Given a specific value of  X  , we can create a linear combination model and calculate data. That is to say, we choose the  X  that achieves the best performance in document model, there is no need to employ a complicated parameter optimization method. We conducted two experiments on two data sets. One was to investigate whether our preference ranking methods can preserve relevance ranking. The other was to examine whether our methods outperform the baseline BM25 for preference ranking, and among them which method works best. 5.1 Data Sets an electronic encyclopedia on the web (http://encarta.msn.com), which contains 44,723 documents. Its search engine (based on BM) has recorded click-through data. We obtained data collected over two months. WordHelp is from Microsoft Word engine (also based on BM) collected click-through data during four months. Details of the two data sets are shown in Table 1. Table 2 shows randomly selected queries in the click-through data of each data set. 
The titles and snippets in both WordHelp and Encarta are created by humans, and preference judgments based on title and snippet information. 5.2 Training and Testing Data The click-through data in Encarta and WordHelp were recorded in chronological order. We separated them into two parts according to time periods, and used the first parts for training and the second parts for testing. and 2,000 pairs for testing. 
For NB and NB+BM, we made use of the training data (click-through data) in the combination weights. For DC, we utilized the training data and the document data in the construction of models. PC-TopN: PC-Top1, PC-Top2, PC-Top5, PC-Top10, and PC-Top20. It seems that N paper, N was fixed at 5. 
In order to conduct precise analysis, we further created three data sets on the basis of &lt; q paper. The results with one_click and all_clicks have the same tendencies. 5.3 Experiment 1 We randomly selected 40 queries from the testing (click-through) data of Encarta and able to select queries from different frequency ranges. 
For each query, the six ranking methods described in Section 4 were employed to produce a ranked list, and the top-5 documents returned by the methods were merged together. Next, humans made judgments on the relevance of the documents. We then used the measure of top 1, 2, and 5 precisions to evaluate the performances of the six methods for relevance ranking . the results reported for Experiment 2 in Section 5.4. 
The results in Table 3 indicate that all the preference ranking methods using click-preserve relevance ranking. 
It should be noted that NB, the method which ranks documents by using click-through data alone, can still achieve better results than BM. The result indicates that preference is strongly related to relevance. 5.4 Experiment 2 We applied the methods of BM, NB, DC, NB+BM, LC-R, and LC-S to both Encarta and WordHelp data for preference ranking. All the results with Encarta and WordHelp show almost the same tendencies . 
We used different proportions of training data in creating NB, DC, NB+BM, LC-R, and LC-R. We only show the results here when 20% and 100% of WordHelp training data, and 10% and 100% of Encarta training data are available. This is because they are representative of the general trends in the entire results. ranking. Nearly in all cases, the methods using click-through (i.e., LC-S, LC-R, NB, NB+BM, DC) perform better than the method without using it (i.e., BM). LC-S. described in Section 3.2. Again, click distribution represents the percentage of clicked clicks on top, the better a method is. For each method, we evaluated the percentages of rank 21-. 
Figure 3 shows the click distributions when 10% of Encarta and 20% of WordHelp performs the best. Figure 4 shows the click distributions when 100% training data is available. Among the methods, LC-S, NB+BM, and BM perform best. 
We conducted sign test [10] to see the significance of differences between methods on percentage of clicks on top5 (significance level was set to 0.005). 
We use  X  X A, B} X  to represent  X  X ethods A and B do not have significant difference method B X . 
With 20% training data for WordHelp and 10% training data for Encarta, we have: (1) WordHelp: {LC-S, NB+BM} &gt;&gt; NB &gt;&gt; DC &gt;&gt; LC-R &gt;&gt; BM (2) Encarta: LC-S &gt;&gt; NB+BM &gt;&gt; NB &gt;&gt; {DC, LC-R} &gt;&gt; BM 
With 100% training data, we get the same results for WordHelp and Encarta: (3) {LC-S, NB+BM, NB} &gt;&gt; {DC, LC-R} &gt;&gt; BM. 5.5 Discussions and why LC-S can work best when different sizes of click through data are available. 
We examined how the proportion of  X  X nseen X  queries in testing data changes when future clicks, i.e., preference ranking. 
Table 4 shows the frequency distributions of queries and clicked documents in the prediction of users X  clicks based on click-through data is at least possible for common queries. can effectively enhance the performance of preference ranking. 
It appears reasonable that NB, NB+BM, LC-S perform equally well when click weight). 
Interestingly, when click-through data is not sufficient, LC-S performs best, improve performance even if training data is not enough. 
It is easy to understand LC-S always performs better than LC-R, as the former uses scores and the latter uses ranks, and there is loss in information when using LC-R. We have investigated the problem of preference ranking based on click-through data, and have proposed a machine learning approach which combines information from click-through data and document data. Our experimental results indicate that  X  click-through data is useful for preference ranking,  X  our preference ranking methods can perform as well as BM25 for relevance ranking,  X  it is better to employ a linear combination of Na X ve Bayes and BM25. experiments, we tested the cases in which the document collection was fixed. It is an document collection dynamically changes during recording of click-through data. 
