 Ranking SVM, which formalizes the problem of learning a ranking model as that of learning a binary SVM on prefer-ence pairs of documents, is a state-of-the-art ranking model in information retrieval. The dual form solution of Ranking SVM model can be written as a linear combination of the preference pairs, i.e., w = P ( i,j )  X  ij ( x i  X  x j ), where  X  denotes the Lagrange parameters associated with each pair ( i,j ). It is obvious that there exist significant interactions over the document pairs because two preference pairs could share a same document as their items. Thus it is natural to ask if there also exist interactions over the model parame-ters  X  ij , which we may leverage to propose better ranking model. This paper aims to answer the question. Firstly, we found that there exists a low-rank structure over the Ranking SVM model parameters  X  ij , which indicates that the interactions do exist. Then, based on the discovery, we made a modification on the original Ranking SVM model by explicitly applying a low-rank constraint to the param-eters. Specifically, each parameter  X  ij is decomposed as a product of two low-dimensional vectors, i.e.,  X  ij =  X  v i where vectors v i and v j correspond to document i and j , re-spectively. The learning process, thus, becomes to optimize the modified dual form objective function with respect to the low-dimensional vectors. Experimental results on three LETOR datasets show that our method, referred to as Fac-torized Ranking SVM, can outperform state-of-the-art base-lines including the conventional Ranking SVM.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Systems Applications]: Information Search and Re-trieval  X  Retrieval Models Keywords: Parameter interaction; Ranking SVM The work was conducted when Yaogong Zhang was visiting CAS Key Lab of Network Data Science and Technology. c  X 
Learning to rank has been widely used in information retrieval and recommender systems. Among the learning to rank models, Ranking SVM is a representative pairwise ranking model, evolving from the popular support vector machines (SVM) [1] for classification problems. In training, Ranking SVM first constructs the preference pairs of the documents based on their relevance labels (or click-through data [7]). Then, a binary SVM model is learned based on the preference pairs to capture the differences between docu-ments with different relevance labels. In ranking, each doc-ument is assigned a relevance score based on the learned ranking model. Usually, the ranking model can be writ-ten in its dual form, the solution is a linear combination of the training preference pairs, i.e., w = P ( i,j )  X  ij ( x where x i and x j are the first and second document in the pair ( i,j ), and  X  ij is the corresponding Lagrange multiplier.
It is obvious that the constructed preference pairs have significant interactions, since two preference pairs could share a same document as their items. In the dual form solution of Ranking SVM, each preference pair is associated with a La-grange multiplier  X  . Therefore, it is natural to ask whether there also exist interactions among these Lagrange multipli-ers . If the answer is yes, how to utilize the interactions to improve Ranking SVM?
This paper tries to answer the above questions by analyz-ing the Lagrange multipliers of the trained Ranking SVM models. Specifically, we made an arrangement of the La-grange multipliers and constructed a block diagonal matrix A , where A ( i,j ) =  X  ij if  X  ij appears in the Ranking SVM model and zero otherwise. Then, we performed singular value decomposition (SVD) on each block of A and sorted the eigenvalues in descending order. We found that for all the queries, we just need 40% dimensions to capture 90% energy, but if we want to capture 100% energy, almost all the queries need at least 80% dimensions, it means there exists a low-rank structure in the matrix, which indicates strong interactions among the Lagrange multipliers. Based on the discovery, we propose to improve the original Ranking SVM through explicitly modeling the parameter interactions in the training process. Specifically, we apply a low rank constraint over the Lagrange multipliers in the dual form solution of Ranking SVM. Each Lagrange multiplier  X  ij in the dual form objective function is factorized as the dot product of two K -dimensional latent vectors, i.e.,  X   X  v , v j  X  , where v i and v j correspond to the first and second document in the preference pair, respectively. In this way, the rank of the matrix A would not be larger than K . The learning of the ranking model, then, becomes optimizing the factorized dual form objective function with respect to the latent vectors. An effective algorithm based on gradient descent is proposed to conduct the optimization.

The proposed algorithm, referred to as Factorized Rank-ing SVM, offers several advantages: 1) The interactions among model parameters are explicitly captured when train-ing the ranking model; 2) Compared with original Ranking SVM, the space complexity is dramatically reduced. Exper-imental results indicate that Factorized Ranking SVM can outperform several state-of-the-art baseline methods includ-ing Ranking SVM, on three LETOR benchmark datasets.
Ranking SVM formalizes the problem of learning a rank-ing model as the problem of learning a binary classification over preference document pairs. Suppose we are given a set of training label-query-document tuples { ( y i ,q i , x where N is the number of training tuples, y i  X  X  r 1 ,  X  X  X  ,r the relevance label for the i -th query-document pair, q i is the query, and x i  X  R n is the feature vector encoding the i -th query-document pair. There exists a total order between the relevance labels r ` r `  X  1 ,  X  X  X  , r 1 , where  X   X  denotes a preference relationship. The set of preference pairs is defined as P  X { ( i,j ) | q i = q j ,y i y j } and the linear Ranking SVM minimizes the loss function where C &gt; 0 is a parameter and [ x ] + = max(0 ,x ). It is common to solve the dual problem of (1): where  X   X  R | P | is a vector of Lagrange multipliers in-dexed by pairs in P , e  X  R | P | is the vector of ones, and M | P | X | P | matrix.

In ranking, the learned ranking model f ( x ) =  X  w , x  X  as-signs a relevance score for any test document x . From the prime-dual relationship, optimal w and  X  satisfy
In machine learning, it is assumed that all of the train-ing instances are independently identically distributed. In Ranking SVM, however, it is obvious that the preference pairs have significant interactions because two pairs (e.g.,( i , j ) and ( i , k ))could share a document(e.g.,document i ). Since the model parameters  X  are associated with each preference pair, it is natural to ask whether these parameters also have interactions each other.
In this paper, we propose to analyze the interactions be-tween model parameters based on singular value decompo-sition (SVD). Specifically, given a learned Ranking SVM model, we rearrange the model parameters  X  as a matrix Figure 1: Distribution of queries over percentage ranges when capturing 90% and 100% of the energy.
 A  X  R N  X  N and defined as A ( i,j ) =  X  ij if ( i,j )  X  P and 0 otherwise. Note that A is a block diagonal matrix, which consists of sub-matrices A i ( i = 1 ,  X  X  X  , | Q | ), and each block A i corresponds to a training query. This is because the pref-erence pairs are constructed within the documents retrieved by one query.

We performed SVD on each submatrix A i (corresponds to a query) and sorted the eigenvalues in descending order Figure 1(a) shows the statistics on the percentages of di-mensions needed per query for capturing 90% of the energy. The statistics are based on a Ranking SVM model trained on one fold of OHSUMED dataset. The queries are grouped into different categories based on the ranges of the percent-ages. For example, all the queries in the category 0%  X  10% should need 0%  X  10% of the dimensions for capturing the 90% energy. We can see that all of the queries fall into the ranges less than 40%. Figure 1(b) shows the statistics for capturing 100% of the energy on the same data with the same Ranking SVM model,We can see that if we want to capture 100% of the energy, almost all of the queries fall in the ranges of 80%  X  90% and 90%  X  100%, which indicates that A is a nearly full rank matrix.

We also conducted the analysis on the Ranking SVM mod-els trained on the datasets of MQ2007 and MQ2008. Similar phenomenons were observed. The results indicate there ex-ist significant interactions between the Ranking SVM model parameters. The interactions can be characterized with a low-rank structure.
Based on the analysis above, we propose a new learning to rank model for capturing the parameter interactions. The model, referred to as Factorized Ranking SVM, is based on
SVD can be performed on each submatrix independently because A is block diagonal. original Ranking SVM and explicitly applies a low-rank con-straint over the model parameters. Specifically, each param-eter  X  ij is assumed to be a product of two K -dimensional latent dimension.

The ranking model in Equation (3), therefore, becomes Therefore, the loss function (1) can be written as
C X
Since the size of preference pairs is large, to conduct effi-cient optimization, we adopt a similar method used in [10, 12] in which the prime form of linear Ranking SVM is opti-mized. Specifically, It can be shown that the loss function in (5) can be written as where w is defined in Equation (4), p w ,l + i ( w ), and l are defined as follows: Please refer to [9] for the derivation details. The gradient of L ( v 1 ,  X  X  X  , v N ) in Equation (6) , then, can be written as  X  X   X  v k for k = 1 ,  X  X  X  ,N . Here u = P N i =1 l + i ( w )  X  l  X   X  ij = x i  X  x j . Thus, the updating criteria for gradient number and  X  is the learning rate. Algorithm 1 shows the pseudo code of the optimization algorithm.

The space complexity of Factorized Ranking SVM is O ( N  X  n + N  X  K + N + N | Q | ), where n is the number of features, | Q | is the number of queries in training data, N  X  n is used for storing the original training data, N  X  K is used for stor-ing v 1 ,  X  X  X  , v N , N is used for storing l + ( w ) and l | Q | is used for dynamically constructing the order-statistic tree [9]. Compared with the original Ranking SVM whose Algorithm 1 Factorized Ranking SVM Output: model parameters v 1 ,  X  X  X  , v N 1: ( v 1 ,  X  X  X  , v N )  X  random values 2: repeat 4: Calculate p w ,l + i ( w ), and l  X  i ( w ) { Equation (7) } 5: for k = 1 to N do 7: end for 8: until convergence 9: return v 1 ,  X  X  X  , v N Table 1: Statistics on OHSUMED,MQ2007,and MQ2008.
 space complexity is O ( N  X  n + N 2 ) [9], Factorized Ranking SVM needs much less memory because K N .

The time complexity of Factorized Ranking SVM is O ( T  X  ( N 2 + N  X  (log K + log N + n ) + N 2 number of iterations. N 2 is for computing w , N  X  (log K + log N + n ) is for computing p w ,l + ( w ) ,l  X  ( w ) , u with the trick of order-statistic tree (see [9] for details). N 2 for calculating the gradient  X  X   X  v
We conducted experiments to test the performances of our approach using three LETOR benchmark datasets[14]: OHSUMED, Million Query track of TREC 2007 (MQ2007), and Million Query track of TREC 2008 (MQ2008). Each dataset consists of queries, corresponding retrieved docu-ments, and human judged labels. The possible relevance la-bels are relevant, partially relevant, and not relevant. Statis-tics on the datasets are given in Table 1. The number of preference pairs for each dataset is also shown in the table.
Following the LETOR configuration, We conducted 5-fold cross-validation experiments on the three datasets. The re-sults reported were the average over the five folds. In all of the experiments, LETOR standard features were used.
As for evaluation measures, MAP (mean average preci-sion) and NDCG (normalized discounted cumulative gain)[5] at position of 1, 3, and 5 were used in our experiments. We compared the proposed Factorized Ranking SVM (denoted as  X  X ac-RSVM X  in the experiments) with several state-of-the-art baseline methods, including the conventional Rank-ing SVM model (denoted as  X  X SVM X ), and two represen-tative learning to ranking models of RankNet [2] and List-Net [4]. As for Ranking SVM, we used the implementation released in [6] 2 in all of the experiments. For RankNet and ListNet, we used the implementations in RankLib 3 . Factorized Ranking SVM has some parameters to tune. The learning rate parameter  X  , number of latent dimensions K , and parameter C were tuned based on the validation set http://svmlight.joachims.org http://pepple.cs.umass.edu/  X  vdang/ranklib.html Table 2: Ranking accuracies on dataset OHSUMED.
 Table 3: Ranking accuracies on dataset MQ2007.
 during each experiment. For all of the baselines, we also tuned their parameters based on the validation set. The experimental results on OHSUMED, MQ2007, and MQ2008 are reported in Table 2, Table 3, and Table 4, respectively. Boldface indicates the highest score among all runs. From the results, we can see that Fac-RSVM outperformed RSVM on all of the three datasets in terms of all of the evaluation measures (except on MQ2008 in terms of NDCG@1). The results indicate that Fac-RSVM effectively captures the parameter interactions and thus im-proved ranking performances. The results also showed that Fac-RSVM can outperform pair-wise and list-wise learning to rank methods of RankNet and ListNet (except for ListNet on MQ2007 in terms of NDCG@1 and NDCG@5), which fur-ther proved that Fac-RSVM is effective in relevance ranking.
In the experiments, Fac-RSVM underperformed RSVM in terms of NDCG@1 on MQ2008. MQ2008 has the most sparse labeled documents and preference pairs (see Table 1), which may make the parameter interactions weak. Note that the performances of Fac-RSVM degrade as the num-ber of preference pairs gets smaller. We will analyze the phenomenon in our future work.
Learning to rank has become one of the most active re-search topics in IR [11]. State-of-the-art learning to rank models can be categorized into pointwise methods, pair-wise methods, and listwise methods. Ranking SVM [7] is a representative pairwise learning to rank method. To make the algorithm more suitable to real-world IR applications, Joachims et al. [7] proposed to train Ranking SVM with users X  click-through data from search engines. Cao et al. [3] adapted Ranking SVM to document retrieval by modifying the loss function so that the training can focus on the top ranked documents. Qiang et al. proposed Ranking FM to rank tweets in microblog retrieval [13]. In Ranking FM, the interactions between features are modeled with factoriza-tion machine [15]. In this paper, we also propose to improve Ranking SVM by explicitly capturing the parameter inter-actions in training.

Improving the scalability of Ranking SVM is another im-portant research direction. Kuo et al. [8] proposed an ef-ficient algorithm for optimizing large scale kernel ranking SVM. Lee and Lin [9] proposed to directly optimize the prime form of linear Ranking SVM and significantly im-proved the scalability. In this paper, we adopted the method proposed in [9] for conducting the optimization.
 Table 4: Ranking accuracies on dataset MQ2008.

In this paper we investigated the parameter interactions in Ranking SVM. We empirically found that there exists a low-rank structure among the Lagrange multipliers of Rank-ing SVM model. Based on the discovery, we proposed a new ranking model in which the low-rank constraint is explic-itly applied to the Lagrange multipliers, called Factorized Ranking SVM. In training, Factorized Ranking SVM de-composes each Lagrange multiplier as a dot product of two low-dimensional vectors. An efficient algorithm was devel-oped to conduct the optimization. Advantages of factor-ized Ranking SVM include explicitly modeling parameter interactions in pairwise ranking and dramatically reduced space complexity of the ranking model. Experimental re-sults based on three LETOR benchmark datasets show that Factorized Ranking SVM outperforms the state-of-the-art methods including Ranking SVM, RankNet, and ListNet. The research work was funded by the 973 Program of China under Grants No. 2014CB340401, No. 2012CB316303, the 863 Program of China under Grants No. 2014AA015204, the National Natural Science Foundation of China under Grant No. 61203298, No. 61232010, No. 61425016, No. 61173064, No. 61472401, No. 61300166 and No. 61105049.
