 In recent years, the blogosphere has experienced a substantial in-crease in the number of posts published daily, forcing users to cope with information overload. The task of guiding users through this flood of information has thus become critical. To address this issue, we present a principled approach for picking a set of posts that best covers the important stories in the blogosphere.

We define a simple and elegant notion of coverage and formalize it as a submodular optimization problem, for which we can effi-ciently compute a near-optimal solution. In addition, since people have varied interests, the ideal coverage algorithm should incorpo-rate user preferences in order to tailor the selected posts to individ-ual tastes. We define the problem of learning a personalized cov-erage function by providing an appropriate user-interaction model and formalizing an online learning framework for this task. We then provide a no-regret algorith m which can quickly learn a user X  X  preferences from limited feedback.

We evaluate our coverage and personalization algorithms exten-sively over real blog data. Results from a user study show that our simple coverage algorithm does as well as most popular blog ag-gregation sites, including Google Blog Search, Yahoo! Buzz, and Digg. Furthermore, we demonstrate empirically that our algorithm can successfully adapt to user preferences. We believe that our technique, especially with personalization, can dramatically reduce information overload.
 I.2.6 [ Artificial Intelligence ]: Learning; G.3 [ Probability and Statis-tics ] Algorithms, Experimentation  X  X ow many blogs does the world need? X  asked TIME Magazine in 2008 [21], claiming that there are already too many. Indeed, the blogosphere has experienced a substantial increase in the number of posts published daily. One immediate consequence is that many readers now suffer from information overload.

While the vast majority of blogs are not worth reading for the average user, even the good ones are too many to keep up with. Moreover, there is often significant overlap in content among mul-tiple blogs. To further complicate matters, many stories seem to resonate in the blogosphere to an extent that is largely uncorrelated with their true importance. For example, in the spring of 2007, Politico broke a story about John Edwards X  $400 haircut in a blog post [26], which was almost instantly seized upon by the rest of the blogosphere. Over the next two weeks, the haircut story sparked several major online debates. Avoiding this story was difficult for most Web users, and nearly impossible for those interested in poli-tics but not in this particular line of debate.

The goal of this paper is to turn down the noise in the blogo-sphere. We assume that users have very limited time for reading blog posts, and thus our goal is to show them a small set of posts covering the important stories currently being discussed. Further-more, we allow users to personalize the process; after all, one man X  X  noise may be another man X  X  music.

In this paper, we formally define what it means for a set of posts to cover the blogosphere. One desired property of this notion of coverage is that it must be an efficiently computable function. For instance, due to the large size of our data sets, we cannot use most clustering algorithms, as they require quadratic computation. In addition, the coverage function must be expressive enough so that it can recognize the important stories in the blogosphere while at the same time identify the important features of a particular docu-ment. Finally, the notion should be soft, allowing partial (or prob-abilistic) coverage, as posts rarely offer complete coverage of their stories. We propose a simple and elegant notion that addresses these requirements and formalize a corresponding objective func-tion, which exhibits a natural diminishing returns property known as submodularity. We present a near-optimal efficient algorithm for optimizing this function.
 We then extend our notion of coverage to personalized coverage . Posts that cover the blogosphere for the average population may not be optimal for a particular user, given her personal preferences. For example, a user may like stories about badminton, irrespective of their prevalence. Learning a personalized coverage function allows us to show the users posts that are better suited to their tastes.
We formalize and address the problem of learning a personal-ized coverage function . First, we define an interaction model for user feedback that takes into account the order in which the posts are read. Using this model, we then define an online learning set-ting for coverage functions and provide a simple no-regret algo-rithm that guarantees we can quickly adapt to a user X  X  preferences.
We evaluate our algorithm, Turning Down the Noise (TDN), on real blog data collected over a two week period in January 2009. We compare TDN to popular blog aggregation sites (Google Blog Search [4], Yahoo! Buzz [7], Digg [3], and BlogPulse [1]), measur-ing topicality and redundancy. Re sults from a user study show that our simple, fully-automated coverage algorithm performs as well as, or better than, most of these sites, including those based on user voting or human editing.

Perhaps most importantly, we demonstrate TDN X  X  ability to suc-cessfully adapt to user preferences. Personalization not only im-proves user satisfaction, but is also able to simulate users with different interests. We believe that our algorithm, especially with personalization, can dramatically improve the information overload situation.

In summary, our main contributions are:
Figure 1(a) shows a typical day in the blogosphere (January 17, 2009). The size of a word is proportional to its frequency across the blogosphere. Examining the picture, we can spot some of the popular stories for that day: the inauguration of Barack Obama and the Israel-Gaza conflict.

Many posts cover the same story, e.g., the inauguration. More-over, stories may have a certain degree of overlap. Intuitively, our goal is to select a small set of blog posts that captures the important stories of the day. At the same time, we wish to avoid redundancy. In the following section we formally state the problem of coverage and present an efficient optimization algorithm.
We characterize the posts in the blogosphere by features. Fea-tures can be any arbitrary collection of objects, high-or low-level, for example: significant wor ds (such as nam ed entitie s and noun phrases), topics extracted from the corpus, or even higher-level se-mantic relations. As an example, refer again to Figure 1(a). Here, our features are common named entities. Each document will be about one or more of these features. More formally: D EFINITION 2.1 (B LOGOSPHERE ). A blogosphere is a triplet U and Posts is a finite set of posts. The relation between posts and features is captured by the covering function. cover j ( i ): quantifies the amount post j  X  Posts covers feature u i .
In the simplest case, cover  X  (  X  ) is a binary indicator function, turn-ing posts into subsets of features. Later, we explore other softer notions of coverage functions, e.g., ones with probabilistic inter-pretations.
Given our model U , Pos t s , cover  X  (  X  ) , we wish to determine how effectively a given small set of posts can cover the important sto-ries in the blogosphere. More formally, our goal is to pick a set of k posts A X  Pos t s , in order to maximize some coverage objective. In this section we define desired properties of this objective function, and propose a solution that addresses these requirements.
Perhaps the most natural idea is to first cluster the posts, where posts in the same cluster cover the same features. Then, given clus-ters, we can pick a representative post from each of the k largest clusters. Such clustering approaches are common in the litera-ture [28]. However, most clustering methods require us to compute the distance between every pair of posts, which amounts to O ( n comparisons for n posts. Due to the sizable amount of posts pub-lished daily, methods that require O ( n 2 ) computation are practi-cally infeasible. Our first desirable property for a coverage function is scalability , i.e., we should be able to evaluate coverage in time linear in the number of posts.

Another solution, which does not require quadratic complexity, would be to formulate coverage as maximizing the function, where the cover A ( i ) function measures the degree to which posts A cover feature u i . If posts correspond to a collection of features, and cover  X  (  X  ) are binary indicator functions, then Eq. 1 reduces to the Budgeted Maximum Coverage problem: Given a set of ground elements U , a collection S = { S 1 subsets of U , and a budget k  X  0 , select A X  S of size at most k which maximizes the number of covered elements, | In our setting, this coverage can be formalized as maximizing:
Although max-coverage is an NP-hard problem, there are several efficient and effective approximation algorithms for this task. How-ever, this na X ve approach suffers from some serious drawbacks:
We now address each of these three issues. To address Feature significance in corpus , we can simply assign weights w i feature u i : If features are words, the weights can correspond to their frequency in the data set. incremental coverage of Obama is much smaller than the regular coverage.
We now turn our attention to Feature significance in post . Each post should exhibit different degrees of coverage for the features it contains, which can be achieved by softening the notion of cov-erage, cover j ( i ) . One approach is to use a generative model to estimate the probability of a feature given a post, P ( u for example, our features are topics discovered by a topic model, then this term is simply the probability that document j is about topic i . More generally, any generative model for the particular set of features can be used to define this probability.

Given such a probabilistic model, we can define the notion of soft coverage more formally. If our features are sufficiently high-level, e.g., topics in a topic model, then a post can be thought of as being about a single feature, in which case cover j ( i )= P ( u Alternatively, for lower-level features, such as named entities, we could assume that each post is about features. If these features are picked, for example, at random with replacement from P ( u post j ) , then our coverage will become cover j ( i )=1  X  post j )) . By requiring that all posts cover the same number of fea-tures, we alleviate the problem of  X  X ame-dropping, X  since a post cannot cover a large number of features well.

The probabilistic approach allows us to define feature impor-tance in individual posts as well as in the whole corpus. However, if we define coverage as F ( A )= then the Incremental coverage problem would persist, as this func-tion does not possess the diminishing returns property. Instead, extending the probabilistic interpretation further, we can view set-coverage as a sampling procedure: each post tries to cover feature i with probability cover j ( i ) , and the feature is covered if at least one of the posts in A succeeded. Thus, as A grows, adding a post provides less and less additional coverage. Formally, we can define the probabilistic coverage of a feature by a set of posts Finally, we propose the following objective function for the prob-lem of probabilistic coverage of the blogosphere: Ourtaskistofind k posts maximizing the above objective function:
Using the notion of coverage in Eq. 2, our goal now is to find the set of posts A that maximizes our objective function in Eq. 3. Unfortunately, we can show by reduction from max-coverage that this objective is NP-complete, suggesting that the exact maximiza-tion of this function is intractable. However, our objective function satisfies an intuitive diminishing returns property, submodularity , which allows us to find good approximations very efficiently:
D EFINITION 2.3 (S UBMODULARITY ). A set function F is sub-modular if,  X A X  X  X  X  ,  X  s  X  X \B ,F ( A X  X  s } )  X  F ( A )  X  F ( B X  X  s } )  X  F ( B ) .

C LAIM 2.4. The probabilistic coverage function for the blogo-sphere in Eq. 3 is submodular [15].
 Intuitively, submodularity characterizes the notion that reading a post s after reading a small set of posts A provides more coverage than reading s after having already read the larger set B X  X 
Although maximizing submodular functions is NP-hard [20], by discovering this property in our problem, we can take advantage of several efficient approximation algorithms with theoretical guaran-tees. For example, the classic result of Nemhauser et al. [24] shows that by simply applying a greedy algorithm to maximize our objec-tive function in Eq. 3, we can obtain a (1  X  1 e ) approximation of the optimal value. Thus, a simple greedy optimization can provide us with a near-optimal solution. However, since our set of posts is very large, a na X ve greedy approach can be too costly. Therefore, we use CELF [22], which provides the same approximation guaran-tees, but uses lazy evaluations, often leading to dramatic speedups.
Thus far, we have defined a global notion of coverage for the blogosphere. However, each user has different interests, and the selected posts that cover the prevalent stories may contain many topics that do not interest him. Instead, our goal in this section is to utilize user feedback in order to learn a personalized notion of coverage for each user.

Recall that, in the previous section, F ( A ) assigns a fixed weight w i to every feature, representing its importance. In practice, fea-ture importance varies among different users. One user might care about a feature  X  X ASCAR, X  while others may be indifferent to it. To address this issue, we augment the fixed weights w i with per-sonalized preferences  X  i for each feature i . In the following, we assume that a user X  X  coverage function is of the form: for some unknown set of weights {  X   X  i } . Our goal now is to learn a user X  X  coverage function F  X   X  ( A ) by learning this optimal set of preferences {  X   X  i } .
In order to receive personalized results, users need to communi-cate their preferences. Since F  X  is a set function, the most natu-ral notion of feedback from a machine learning perspective would be for users to provide a single label for the set of posts that they are presented, indicating whether they like or dislike the entire set. However, this approach suffers from two limitations. First, from the point of view of the user, it is not very natural to provide feed-back on an entire set of posts. Second, since there are exponentially many such sets, we are likely to need an extensive amount of user feedback (in terms of sets of posts) before we could learn this func-tion. Instead, we assume that users go through a list of posts order, submitting feedback f j ( X  X iked X = +1,  X  X ndifferent X  = 0,  X  X is-liked X  = -1) for each post a j  X  X  . We take no feedback on a post to mean  X  X ndifferent. X 
Our objective function is defined in terms of sets, but our feed-back is in terms of individual posts. How should we provide an appropriate credit assignment?
One possible solution would be to assume that the feedback that a user provides for a particular post is independent of the other posts presented in the same set. In this case, one can view the user feedback as being labeled data on which we can train a classifier to determine which posts the user likes. However, this assumption does not fit with our interaction model, as a user might not like a post either because of its content or because previous posts have already covered the story.

To address this issue, we consider the incremental coverage of a post, i.e., the advantage it provides over the previous posts. The incremental coverage we receive by adding post a j to the set Note that if cover A ( i ) is defined as in Eq. 2, then the incremental coverage is the probability that a j is the first post to cover feature u . Furthermore, if we view the set of documents A as an ordered set
A = { a 1 ,...,a k } 1 , the sum of incremental coverages is a tele-scoping sum that yields the coverage of a set of documents
X where a 1: j  X  1 is shorthand for the set of documents { a
Using incremental coverages, we can now define the reward we receive after presenting A to a user with preferences  X  and obtain-ing feedback f : If the user liked all of the documents in A (i.e.,  X  j, f reward becomes exactly the coverage function we are seeking to maximize, F  X  ( A )=
Our algorithm maintains an estim ate of the user X  X  preferences at each time step t ,  X  ( t ) . Given this estimate, we optimize F and pick a set of documents A ( t ) to show the user. After receiving feedback f ( t ) , we gain a reward of Rew (  X  ( t ) , A ( t ) time steps, our average reward is therefore:
This ordering could be defined by the order the posts are presented to the user, e.g., the one picked by the greedy algorithm. Since our decisions at time t can only take into account the feed-back we have received up to time t  X  1 , the decisions we made may have been suboptimal. For comparison, consider the reward we would have received if we had made an informed choice for the user X  X  preferences  X  considering all of the feedback from the T time steps: That is, after seeing all the user feedback, what would have been the right choice for user preference weights  X  ? The difference between our reward and this best choice in retrospect is called the regret :
D EFINITION 3.1 (R EGRET ). Our average regret after T time steps is the difference BestAvgRew ( T )  X  AvgRew ( T ) . Positive regret means that we would have preferred to use the weights  X  that maximize Eq. 6 instead of our actual choice of weights  X  A no-regret learning algorithm , such as the one we describe in the next section, will allow us to learn  X  ( t ) such that, as T goes to infinity, the regret will go to zero at a rapid rate. Intuitively, this no-regret guarantee means that we learn a sequence  X  ( t ) as well as any fixed  X   X  X ncluding the true user preferences,  X  the sets of posts that the user is presented. By learning the person-alized coverage function for a particular user in this manner, the posts we provide will be tailored to his tastes.

A stronger guarantee would be to show that the weights  X  ( t ) only do well on the sets of posts from which they were learned, but also on the posts that would have been selected had we used the true  X   X  as the user preference weights for each day. For example, consider a user who is interested in politics and sports, but is also passionate about bagpiping. We may never show him any bagpip-ing posts, since they are not likely to be common. Thus, we may never receive feedback that would allow us to accurately model this portion of the user X  X  true preferences. We intend to address this is-sue in future work.
We now describe our algorithm for learning  X   X  from repeated user feedback sessions. Like many online algorithms [12], our ap-proach updates our estimated  X  ( t ) using a multiplicative update rule. In particular, our approach can be viewed as a special case of Freund and Schapire X  X  multiplicative weights algorithm [18].
The algorithm starts by choosing an initial set of weights  X  (WLOG, we assume weights are normalized to sum to 1, since the coverage function is insensitive to scaling.) In the absence of prior knowledge about the user, we can choose the uniform distribution: If we have prior knowledge about the user, we can start from the corresponding set of weights.

At every round t , we use our current distribution  X  ( t ) posts, A ( t ) , to show the user. After receiving feedback f would like to increase the weight of features covered by posts the user liked, and decrease the weight of features covered by posts the user disliked. These updates can be achieved by a simple multi-plicative update rule: where Z is the normalization constant,  X   X  (0 , 1) is the learning rate, and, intuitively, M ( i, f ( t ) ) measures the contribution (posi-tive or negative) that feature i had on our reward: where the normalization by 2max i w i is simply used to keep this term in the range [  X  0 . 5 , 0 . 5] .

If the learning rate  X  is small, we make large moves based on the user feedback. As the learning rate tends to 1, these updates become less significant. Thus, intuitively, we will start with a small value of  X  and slowly increase it.

C LAIM 3.2. If, for number of personalization epochs T ,weuse alearningrate  X  T given by: then our preference learning procedure will have regret bounded by: Since our regret goes to zero as T goes to infinity, our approach is called a no-regret algorithm. The proof follows from Freund and Schapire [18], by formalizing our learning process as a two-player repeated matrix game involving our algorithm and the user ( cf. extended version of this paper for details [15]).
We evaluate our algorithm on real blog data collected over a two week period in January 2009. These posts come from a diverse set of blogs, including personal blogs, blogs from mainstream news sites, commercial blogs, and many others.

We obtain the data from Spinn3r, which indexes and crawls 12 million blogs at the r ate of approximately 500,000 posts per day [5]. After performing some simple data cleaning steps, such as duplicate post removal, we reduce this number to about 200,000 posts per day in our data set. However, as this is real Web data, it is still invariably noisy even after cleaning. Thus, our algorithm must be robust to content extraction problems.

For each post, we extract named e ntities and noun phrases using the Stanford Named Entity Rec ognizer [17] and the LBJ Part of Speech Tagger [25], respectively. We remove infrequent named entities and uninformative noun phrases (e.g., common nouns such as  X  X ear X ), leaving us with a total collection size of nearly 3,000. (More details can be found in the extended version [15].)
We evaluate an instantiation of our algorithm with high level topic model-based features, which we refer to as TDN+LDA. We define our set of features as topics from a latent Dirichlet allocation (LDA) [9] topic model learned on the noun phrases and named en-tities described above. We take the weight of each feature to be the fraction of words in the corpus assigned to that topic. As described in Section 2.2, we can directly define cover j ( i )= P ( u which in the setting of topic models is the probability that post about topic i . We use a Gibbs sampling implementation of LDA [19] with 100 topics and the default parameter settings. Once we have extracted the n amed entitie s and noun phrases, LDA is the slowest part of running TDN+LDA. After a 300 itera-tion burn-in period, we run 2,500 iterations of Gibbs sampling and select 500 samples from them. On a single 3GHz processor, this process takes less than 2GB of RAM and between 1-2 hours to run for an eight hour corpus of blog posts. The submodular function optimization needed to generate posts takes under a minute.
We also evaluate a variant of our algorithm with features con-sisting of the named e ntities and noun phrases directly, which we refer to as TDN+NE. As this variant uses a lower-level feature set, it assumes a post can cover multiple features, and thus uses the cov-erage function for covering features described in Section 2.2. The value of is set to be the average number of occurrences of named entities and nouns per document in our co rpus, which is approxi-mately 16. In this setting, post selection takes about five minutes.
As detailed in Section 2, the main objective of our algorithm is to select a set of posts that best covers the important and prevalent sto-ries currently being discussed in the blogosphere. The major world events that took place during the time corresponding to our data set included the Israel-Gaza conflict, the inauguration of Barack Obama, the gas dispute between Russia and Ukraine, as well as the global financial crisis. As an example, here is the set of posts that our algorithm selects for an eight hour period on January 18, if our budget k is set to five: 1. Israel unilaterally halts fire as rockets persist 2. Downed jet lifted from ice-laden Hudson River 3. Israeli-trained Gaza doctor loses three daughters and niece to 4. EU wary as Russia and Ukraine reach gas deal 5. Obama X  X  first day as president: prayers, war council, economists,
The selected five posts all cover important stories from this par-ticular day. The Israel-Gaza conflict appears twice in this set, due to its extensive presence in the blogosphere at the time. It is impor-tant to note, however, that these two posts present different aspects of the conflict, each being a prevalent story in its own right. By ex-panding the budget to fifteen posts, t he algorithm makes additional selections related to other major stories of the day (e.g., George W. Bush X  X  legacy), but also selects  X  X ifestyle X  posts on religion and cooking, since these represent the large portion of the blogosphere that is not directly related to news and current events.
As another example, here are the top five selected posts from the morning of January 23, the day after the Academy Award nomina-tions were announced: 1. Button is top Oscar nominee 2. Israel rules out opening Gaza border if Hamas gains 3. Paterson choos es Gillibrand for U.S. Senate 4. Fearless Kitchen: Recipe: Medieval Lamb Wrap 5. How Obama avoided a misguided policy blunder
A post describing the Oscar-nominated movie The Curious Case of Benjamin Button supplants the Israel-Gaza conflict at the top of the list, while a cooking post makes it up to the fourth position.
We wish to quantitatively evaluate how well a particular post selection technique achieves the notion of coverage we describe above on real blog data . However, the standard information re-trieval metrics of precision and recall are not directly applicable in our case, since we do not have labels identifying all the prevalent stories in the blogosphere on a given day and assigning them to spe-cific posts. Rather, we measure the topicality of individual posts as well as the redundancy of a set of posts. We say a post is topical with respect to a given time period if its content is related to a major news event from that period. A post r is redundant with respect to a previous post p if it contains little or no additional information to post p . An ideal set of posts that covers the major stories discussed in the blogosphere would have high topicality and low redundancy.
We conducted a study on 27 users to obtain labels for topical-ity and redundancy on our data. We compared TDN+LDA and Figure 2: Topic representing the peanut butter recall from Jan-uary 18, 2009, with the size of a word proportional to its impor-tance in the topic.
 TDN+NE to four popular blog aggregation sites: the front page of Digg, Google Blog Search, Nielsen BuzzMetrics X  BlogPulse, and Yahoo! Buzz. We intended on evaluating Technorati as well, but their RSS feed was unavailable for most days in our evaluation pe-riod. Additionally, we also examine the performance of simpler objective functions on the post selection task.
In order for users to measure the topicality of a blog post, they need an idea of what the major news stories are from the same time period. We express this information to our study participants by providing them with headlines gathered from major news sources in five different categories: world news, politics, business, sports, and entertainment. The headlines for each category are aggregated from three different news sources to provide a wider selection for the users and to avoid naming a single source as the definitive news outlet for a category. For instance, for politics we present headlines from Reuters, USA Today ,and The Washington Post . This collec-tion of headlines is akin to a condensed newspaper, and we refer to these stories as reference stories .

We present the participants with reference stories gathered at a particular time, e.g., January 18, 2009, 2:00pm EST, which we call the reference time . We then show each participant a set of ten posts that was chosen by one of the six post selection techniques, and ask them to mark whether each post is  X  X elated X  to the reference stories. Each post is presented as a title along with a short descrip-tion. The users are not made aware of which technique the posts come from, so as not to bias their ratings. The posts selected by TDN+LDA and TDN+NE were chosen from an eight hour window of data ending at the reference time, while the posts selected by the popular blog aggregation sites were retrieved from these sites within fifteen minutes of the reference time.

Figure 3(left) shows the results of the topicality user ratings on the six techniques. On average, the sets of ten posts selected by Google Blog Search, TDN+LDA and Yahoo! Buzz each contain five topical posts out of ten presented. The topicality of these tech-niques is significantly better than that of TDN+NE, Digg and Blog-Pulse. BlogPulse selects the most linked-to posts of the day, which does not seem to be a good heuristic for covering the important stories. Many of these posts are technology how-to pages, such as  X  X elp With Social Bookmarking Sites, X  the highest ranked post from January 18. Digg selects its top posts by user voting, and thus the top selected posts consist of a few prevalent stories and many entertaining or shocking posts, such as  X  X een Stabbed But Makes It To Job Interview, X  the top post from February 6.

TDN+LDA outperforms TDN+NE because high-level features, such as LDA topics, capture stories in a better way than low-level features do. For example, for one eight hour period in our data set, there is a coherent LDA topic about the EU-Russia gas crisis. Therefore, when we cover this topi c, we will present a story that is about the crisis. However, the named entity  X  X ussia X  may be cov-ered by multiple stories. TDN+ NE selects a post about Russia X  X  plan to go ahead with the opening of a pediatric medical center in Moscow despite the current financial crisis, since it contains impor-tant named entitie s and nouns like  X  X ussia, X   X  X utin, X   X  X risis, X  etc. Hence, if we only cover low-level features, we might select a post that is not topical, yet contains multiple important features.
While topicality captures a major aspect of our notion of cov-erage, in that important current events are covered by the selected posts, one drawback of this evaluation method is that lifestyle blog posts are not adequately represented. It is difficult to define a set of reference sites that summarize the day X  X  most important recipes or most prevalent do-it-yourself tips, for instance. Furthermore, in our case, we did not want to show our study participants more than five categories of reference stories, so as not to overwhelm them. As a result, a post related to an important technology story would likely not be considered topical, as we left this category out.
The user study described in the previous section allowed us to measure whether posts were topical or not. However, topicality is not enough to judge the goodness of a set of posts, since they may all be about the same story, and hence not interesting. Instead, we want the posts to be diverse, so that they capture all of the im-portant stories in the blogosphere, as well as appeal to everyone X  X  interests. As part of our user study, we asked users to look at a set of fifteen posts selected by one of the six previously described post selection techniques, and mark any occurrences they thought were redundant. Each of 27 participants was presented with either two or three sets of posts generated by different algorithms over the same time period. The users were not aware of the sources of the posts.
Figure 3(right) shows that both variants of our algorithm outper-form Digg, BlogPulse and Google Blog Search on the redundancy metric. In other words, our algorithm selects diverse sets of posts. This diversity is primarily due to the diminishing returns property of our objective function. If we have covered the important fea-tures of a story once, covering it again yields only a small reward. Google Blog Search has the highest number of redundant results, and has high variance, suggesting that on some days many of the posts on its front page are similar. In fact, on average, the posts selected by Google Blog Search are nearly six times as redundant as those selected by TDN+LDA.

However, it should be noted that performing well on the redun-dancy metric alone is not sufficient. For example, it may turn out that all the posts picked by an algorithm are non-redundant, but meaningless, and hence of no interest to a user. Thus, an algorithm needs to perform well on both the topicality and the redundancy metric in order for it to be useful.

TDN+LDA and Yahoo! Buzz were the two techniques that per-formed well in both metrics. However, while Yahoo! Buzz uses Web search trends, user voting and other features to select its posts, TDN+LDA achieves the same topicality and redundancy perfor-mance by selecting posts only using simple text features. Fur-thermore, TDN+LDA adapts its results to user preferences, as de-scribed in Section 4.2.
As an alternative to the submodular objective function defined in Eq. 3, we consider two simpler objective functions.
 LDA-based Modular Function. A modular function is an additive set function where each element is associated with a fixed score, and the value for a set A is the sum of the scores of the elements of
A . Since the score of a post does not depend on the other ele-ments in the set, there is no incentive to select a diverse set of posts. The na X ve way of selecting posts using LDA fits under this modular framework. We first pick the top k topics based on their weight in the corpus. For each one, we pick the post that covers it the most. In addition to the potential for re dundancy mentioned above, this technique suffers from the fact that it commits to a topic irrespec-tive of the quality of the posts covering it. Furthermore, even if a post covers multiple topics well, it might not be selected as there may be some posts that better cover each individual topic. Using a strictly submodular objective function alleviates these problems.
For example, if we define our features based on a 50-topic LDA model trained on an eight hour data set from January 18, the topic with the lowest weight is about the peanut butter recall, a major news story at this time ( cf. Figure 2). Thus, if we select fifteen posts following the na X ve LDA approach, we do not pick a post from this topic. However, the weight of this topic (0.019) is not much lower than the mean topic weight (0.020). Moreover, since this topic closely corresponds to a prevalent news story, many posts cover it with high probability. TDN selects such a post because, unlike the na X ve LDA approach, it simultaneously considers both the topic weights and the post coverage probabilities.
 Budgeted Maximum Coverage. Another simple objective func-tion we consider is budgeted maximum coverage, introduced in Definition 2.2, but with each feature (in this case, noun phrases and named entities) weighted by its corpus frequency. Optimizing this objective leads to the aforementioned  X  X ame-dropping X  posts. For example, on an eight hour data set from January 20, the second post selected announces the schedule of a rock band X  X  upcoming world tour, and thus completely covers the features,  X  X ashington, X   X  X oston, X   X  X ew York, X   X  X ondon, X   X  X ome, X  and a few dozen more cities and countries. Once this pos t has been selected, there is no further incentive to cover these features.
There are two methods by which we evaluate how well our algo-rithm personalizes the posts it selects in response to user feedback. In one setting, we conduct a user study to directly measure how many of the presented posts a study participant would like to read. In the second setting, we simulate user preferences on a targeted set of blog posts and observe how our objective function F ( changes with respect to the unpersonalized case.
We divide our blog data into 33 eight hour segments (epochs), and pick a starting segment at random for a particular user. We present our user with a set of ten posts from his starting segment, selected using TDN+LDA. The posts are displayed as a title and short summary. The user is instructed to read down the list of posts and, one by one, mark each post as  X  X ould like to read, X   X  X ould not like to read, X  or  X  X ndifferent. X  The user is told to make each decision with respect to the previous posts displayed in that set, so as to capture the notion of incremental coverage. For example, a user might be excited to read a post about Obama X  X  inauguration appearing at the top slot in a particular result set, and thus would mark it as  X  X ike to read. X  However, if four other very similar posts appear below it, by the time he gets to rating the fifth inauguration post in a row, he will likely label it as  X  X ot like to read. X 
After each set of ten posts, our personalization algorithm uses the user ratings to update the weights  X  ( t ) , and selects a personalized set of posts for the next epoch 2 . Wealsoasktheusertomarkhis preferences on unpersonalized posts presented for the same epochs. The order in which these two conditions are presented is random-ized. We repeat this process for a total of five epochs. As this is not a longitudinal study, and we do not wish it to be overly tedious for our participants, we accelerate the personalization process by us-ing a learning rate  X  of 0.5, corresponding to a short-term learning horizon (i.e., T  X  9 from Eq. 9).

Figure 4(a) shows the result of this study on twenty users. The vertical axis of the plot shows the average number of posts liked by a user in a single epoch. As one would expect, at epoch 0, when the posts are always unpersonalized, the number of liked posts is approximately the same between the personalized and unpersonal-ized runs. However, in just two epochs, the users already show a preference towards the personalized results.

If a user only prefers sports posts, personalization is easy, as the user X  X  interests are narrow. In our study, however, the participants were simply instructed to rate posts with their own personal pref-erences. As people are often eclectic and have varied interests, this task is harder, but more realistic. Thus, it is notable that we are still able to successfully adjust to user tastes in very few epochs, showing a significant improvement over the unpersonalized case.
If instead of asking users to rate posts according to their personal tastes, we ask them to pretend that they only want to read posts on a specific subject (e.g., India), we observe interesting qualitative behavior. Initially, the top posts selected are about the main stories of the day, including the Israel-Gaza conflict and the Obama inau-
As topics tend to change from one epoch to the next, we employ a simple bipartite matching algorithm to map personalization weights across epochs. Alternatively, one could use more recent topic mod-els that are designed to work on streaming data ([10]). guration. After a few epochs of marking any India-related posts as  X  X ike X  and all others as  X  X islike, X  the makeup of the selected posts changes to include more posts about the Indian subcontinent (e.g.,  X  X akistan flaunts its all-weather ties with China X ). This is particularly notable given that these posts appear relatively infre-quently in our data set, and thus without personalization, are rarely selected. Also, while after enough epochs, stories about India even-tually supplant the other major news stories at the top of the result set, the Israel-Gaza stories do not disappear from the list, due to their high prevalence. We believe this is precisely the behavior one would want from such a personalization setting.
We consider the case of a hypothetical sports fan, who always loves to read any sports-related post. In particular, every day, he is presented with a set of posts from the popular sports blog Fan-House.com, and he marks that he likes all of them. We simulate such a user in order to empirically examine the effect of personal-ization on the objective function.
 Specifically, we simulate this sports fan by marking all Fan-House.com posts as  X  X iked X  over a specified number of personal-ization epochs, updating the personalization weights  X  ( t ) epoch. On the next epoch, which we call the evaluation epoch, we compute our objective function F ( A ) on three different sets of posts. First, we compute F ( A ) on the FanHouse.com posts from this epoch, hypothesizing that the more epochs we spend person-alizing prior to the evaluation epoch, the higher this value will be. Second, we compute F ( A ) on all the posts from DeadSpin.com, another popular sports blog. We also expect to see a higher value of our objective in this case. Finally, we compute F ( A ) on all the posts from the HuffingtonPost.co m Blog, a popular politics b log. The expectation is that by personalizing on sports posts for several days, F ( A ) for a set A of politics posts will decrease with respect to the unpersonalized case.

Figure 4(b) shows the results of this experiment with a  X  value of 0.5, and we observe precisely the hypothesized behavior. The vertical axis of this plot shows the ratio of F ( A ) computed with the learned personalization weights to that of F ( A ) with the un-personalized uniform weights, allowing us to compare across the three blogs. Thus, points on the plot that appear higher along the vertical axis than 1 indicate an improvement over the unpersonal-ized case, while any value below 1 indicates a decline with respect to the unpersonalized case.

Figure 4(c) shows the same simulation but with  X  =0 . 1 .Thisis an aggressive setting of the learning rate, and thus, as expected, the plot shows the objective function changing in the same direction but more rapidly when compared to Figure 4(b). These figures capture an important trade off for a deployed system, in that by varying the learning rate  X  , we trade off the speed of personalization with the variety of selected posts.
Recently, there has been an increase in the number of websites that index blogs and display a list of the most popular stories. Some examples of such websites are Google Blog Search [4], Yahoo! Buzz [7], Digg [3], Technorati [6], and Blogpulse [1]. Some of these websites display posts without any manual intervention, e.g., Google Blog Search and Blogpulse. However, most of these web-sites display posts which have either been handpicked by editors or have been voted for by users of the website. Most websites that pick posts automatically use a combination of features such as link structure [2], trends in search engine queries [7], and the number of times a post is emailed or shared. Currently, we are only using features derived from the text of the posts, although in the future we hope to incorporate the link structure between posts into our al-gorithm. Another key difference is that most of these websites lack the personalization functionality we provide.

In a recent paper [8], Agarwal et. al address a problem similar to ours. Their task is to select four out of a set of sixteen stories to be displayed on the Yahoo! homepage. The sixteen stories are manually picked by human editors; hence, all are of high quality. The authors use click-through rate to learn online models for each article. Their setting differs significantly from ours, since we tackle the problem of selecting ten out of roughly 60,000 posts for each eight hour segment. Moreover, as described in section 4, our data is very noisy, and we do not have access to click-through rates.
Another line of related research is the area of subtopic retrieval [27, 13, 11]. In subtopic retrieval, the task is to retrieve documents that cover many subtopics of the given query. In the traditional information retrieval setting, it is assumed that the relevance of each document is independent of the other documents. However, in subtopic retrieval the utility of a document is contingent on the other retrieved documents. In particular, a newly retrieved doc-ument is relevant only if it covers subtopics other than the ones covered by previous documents. Thus, the concept of relevance in subtopic retrieval is similar to our notion of  X  X overage, X  which has a diminishing returns characteristic. However, while subtopic retrieval is query-based, we intend to cover all the popular stories being discussed in the blogosphere.

Two common approaches to personalization are collaborative fil-tering [23, 14] and content-based filtering . In collaborative filter-ing, user preferences are learned in a content-agnostic manner by correlating the user X  X  past activity with data from the entire user community. In a content-based approach, documents are recom-mended to a user if they are similar to documents that the user previously liked, where similarity is based on document content. Using a content-based approach, we provide theoretical guarantees for personalization. Moreover, we currently do not have the kind of user base that is needed for collaborative filtering to be effective.
Leskovec et al. pr opose a solution to the problem of selecting which blogs to read in order to come across all the important sto-ries quickly [22]. Although related to our problem, a fundamental difference is that instead of trying to select which blogs to read, we present the user with a selection of posts from various blogs. Moreover our approach is completely content based, whereas the approach of Leskovec et al. is based only on the links between blogs. In addition, we also incorporate personalization into our algorithm, which they do not.

There has also been extensive work on building models and an-alyzing the structure of the blogosphere. For example, Finin et al. [16] present a model of information flow in the blogosphere. We could potentially leverage such analysis in the future in order to extract better features for our algorithms. Blogscope [2] is in-tended to be an analysis and visualization tool for the blogosphere. Unlike us, they are not trying to cover the blogosphere. Instead, Blogscope presents the user with a search interface, and suggests some related words based on the search query. They give a pref-erence to words whose frequency increases by a large amount in the past 24 hours (e.g., words with a high  X  X urstiness X ). Moreover, they do not employ any personalization.
In this paper we describe the problem of turning down the noise in the blogosphere. While the vast majority of blog posts are not interesting for the average user, their quantity is truly remarkable. For this reason, many readers suffer from information overload. Our goal is to show them a small set of posts covering only the important stories currently being discussed.

We start by exploring different desired properties of coverage functions. We then formalize the notion of coverage as a submod-ular optimization problem, and present an efficient algorithm to se-lect the top stories in the blogosphere.

Next, we generalize the coverage notion to the personalized case, where we assume that each user has his own coverage function based on his personal preferences. We introduce the problem of learning these coverage functions from limited user feedback. We formalize the notion of feedback, and illustrate a simple online per-sonalization method based on mu ltiplicative updates of weights. This method achieves no-regret personalization.

We derive two different algorithms based on our general frame-work, each using different feature instantiations. Both algorithms are efficient enough that they can be run on large, real-world blog feeds. We compare both algorithms against popular blog aggrega-tion websites like Google Blog Search, Yahoo! Buzz, Digg, and BlogPulse. In addition to post content, most of these websites use richer features such as click-through rate, trends in search queries and link structure between posts, or use human intervention to pick posts. We present results based on simulations and a user study. Our TDN algorithm outperforms all others except for Yahoo! Buzz (with which it is comparable), despite having access to text-based features only. Furthermore, our experiments demonstrate that our algorithm can adapt to individual users X  preferences.

Our results emphasize that the simple notion of coverage we in-troduced successfully captures the salient stories of the day. We believe that this combination of coverage and personalization will prove to be a useful tool in the battle against information overload. Acknowledgments. We thank Spinn3r for providing us access to their data. We are grateful to Geoff Gordon for helpful discus-sions and to the reviewers for their useful comments. Tag clouds in this paper were generated using wordle.net. This work was partially supported by the ARO under MURI W911NF0710287 and W911NF0810242, by NSF Career IIS-0644225, and by NSF NeTS-NOSS CNS-0625518.
