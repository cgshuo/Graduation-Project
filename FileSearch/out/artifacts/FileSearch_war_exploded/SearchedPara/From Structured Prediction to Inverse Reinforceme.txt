 Machine learning is all about making predictions; language is full of complex rich structure. Struc-tured prediction marries these two. However, structured prediction isn X  X  always enough: some-times the world throws even more complex data at us, and we need reinforcement learning tech-niques. This tutorial is all about the how and the why of structured prediction and inverse reinforce-ment learning (aka inverse optimal control): par-ticipants should walk away comfortable that they could implement many structured prediction and IRL algorithms, and have a sense of which ones might work for which problems. The first half of the tutorial will cover the  X  X a-sics X  of structured prediction: the structured per-ceptron and Magerman X  X  incremental parsing al-gorithm. It will then build up to more advanced al-gorithms that are shockingly reminiscent of these simple approaches: maximum margin techniques and search-based structured prediction.

The second half of the tutorial will ask the ques-tion: what happens when our standard assump-tions about our data are violated? This is what leads us into the world of reinforcement learning (the basics of which we X  X l cover) and then to in-verse reinforcement learning and inverse optimal control.

Throughout the tutorial, we will see exam-ples ranging from simple (part of speech tagging, named entity recognition, etc.) through complex (parsing, machine translation).

The tutorial does not assume attendees know anything about structured prediction or reinforce-ment learning (though it will hopefully be inter-esting even to those who know some!), but does assume some knowledge of simple machine learn-ing (eg., binary classification).
