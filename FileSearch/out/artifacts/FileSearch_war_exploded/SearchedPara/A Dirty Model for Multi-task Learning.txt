 leveraging this common structure could be more statistical ly efficient. function estimation have natural extensions to the simulta neous-sparse setting [12, 2, 11].  X  sparse structure. Particular examples include results usi ng the  X  norm [7, 10].
 common. The second concern with such block-sparse regulari zers is that the  X  that would be a worrisome caveat.
 of corresponding dirty models , which might require new approaches to biased high-dimensi onal simultaneously sparse multiple regression.
 sparse matrix (corresponding to the non-shared features). As we show both theoretically and em-or block-sparse regularizers (at times remarkably so).
 are demonstrated in Sec 4.
 Notation: For any matrix M , we denote its j th row as M and its support by Supp ( M ) . Also, for any matrix M , let k M k absolute values of the elements, and k M k Multiple regression. We consider the following standard multiple linear regress ion model: certain rows would be elementwise sparse, corresponding to those features which are relevant for automatically adapt to different levels of sharedness, and yet enjoy the following guarantees: which is union of its supports for the different tasks. In par ticular, denoting U k -th column of  X   X  , and U = S Error bounds: We are also interested in providing bounds on the elementwis e  X  estimator b  X  , 2.1 Our Method in
B and elementwise sparsity in S . The corresponding  X  X lean X  models would either just use blo ck-able to outperform both classes of these  X  X lean models X , for all regimes  X   X  . Algorithm 1 Dirty Block Sparse the Lasso [14, 18] and  X  elementwise  X  recovery and bounded elementwise  X  method vis-a-vis Lasso and the  X  and where we have standard Gaussian design matrices as in The orem 2. Further, while each of two the behaviors of the different regularization methods vary with the extent of overlap  X  . Denote a particular rescaling of the sample-size  X  [18] show, when the rescaled number of samples scales as  X  when the sample size scales as  X  to one. For the  X   X  a smaller sample size, while for  X  &gt; 2 / 3 ( X  X ore sharing X ) the  X  perform better.
 size of  X  methods. Figure 3 shows the empirical performance of each of the three methods; as can be seen, 3.1 Sufficient Conditions for Deterministic Designs block-regularization methods [8, 10].
 A0 Column Normalization X ( k ) Let U supports for each task. Then we require that A1 Incoherence Condition  X  We will also find it useful to define  X  Note that by the incoherence condition A1 , we have  X  A2 Eigenvalue Condition C A3 Boundedness Condition D Further, we require the regularization penalties be set as Theorem 1. Suppose A0-A3 hold, and that we obtain estimate b  X  from our algorithm with regular-ization parameters chosen according to (2) . Then, with probability at least 1  X  c we are guaranteed that the convex program (1) has a unique opt imum and Here the positive constants c of n, p, r , the problem dimensions of interest.
 enough to be detectable above the noise. 3.2 General Gaussian Designs U . We require these covariance matrices to satisfy the follow ing conditions: C1 Incoherence Condition  X  C2 Eigenvalue Condition C is bounded away from zero.
 C3 Boundedness Condition D on the covariance matrix of the (randomly generated) rows of the design matrix. Further, defining s := max with probability at least 1  X  c positive numbers c the following conditions: 3.3 Sharp Transition for 2 -Task Gaussian Designs where r = 2 and the design matrix has rows generated from the standard Ga ussian distribution experimentally, our method strictly outperforms both Lass o and  X  and full support sharing (where it matches that of  X  rescaling of the sample size n as We will also require the assumptions that generated from the standard Gaussian distribution N (0 , I Then the estimate b  X  of the problem (1) satisfies the following: ( Failure ) If  X  ( n, p, s,  X  ) &lt; 1 there is no solution (  X  B,  X  S ) for any choices of  X  We note that we require the gap  X   X  (1) large, the dependence of the sample scaling on the gap is quit e weak. real-world data. The synthetic experiments explore the acc uracy of Theorem 3, and compare our estimator with LASSO and the  X  Again we compare against LASSO and the  X  dirty model outperforms both LASSO and  X  chosen via cross-validation; see supplemental material fo r more details. 4.1 Synthetic Data Simulation zero entry is then chosen to be i.i.d. Gaussian with mean 0 and variance 1. n samples are then generated from this. We then attempt to estimate using three methods: our dirty model,  X  support recovery. We describe these process in more details in this section.  X  / X   X  regularizer. Fig 1(a) shows the probability of success for t he case  X  = 0 . 3 (when LASSO is better than  X  (see Fig 1(b)), LASSO and  X  Fig 1(c),  X  in this case as well. transition threshold for dirty model, LASSO and  X  threshold for dirty model is always lower than the phase tran sition for LASSO and  X  izer. 4.2 Handwritten Digits Dataset handwritten sample consists of p = 649 features.
 method with  X  via cross-validation.
 many features it identifies as  X  X hared X  and how many as  X  X on-s hared X . For the other methods we just report the straight row and support numbers, since they do not make such a separation. We acknowledge support from NSF grant IIS-101842, and NSF CA REER program, Grant 0954059. [1] A. Asuncion and D.J. Newman. UCI Machine Learning Reposi tory, [2] F. Bach. Consistency of the group lasso and multiple kern el learning. Journal of Machine [3] R. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine , 24(4):118 X 121, 2007. [4] R. Caruana. Multitask learning. Machine Learning , 28:41 X 75, 1997. [5] C.Zhang and J.Huang. Model selection consistency of the lasso selection in high-dimensional [6] X. He and P. Niyogi. Locality preserving projections. In NIPS , 2003. [8] S. Negahban and M. J. Wainwright. Joint support recovery under high-dimensional scaling: [9] S. Negahban and M. J. Wainwright. Estimation of (near) lo w-rank matrices with noise and
