
Faculty of Computing, Federal University of Uberlandia, Uberlandia, Brazil Department of Computer Science, Institute of Mathematics and Computer Science, University of Sao Paulo, Sao Carlos, SP, Brazil 1. Introduction
The study and modeling of several natural phenomena depend on the automatic analysis of large volumes of data [37]. Some of those phenomena produce endless sequences, called data streams [35], which are often found in computationally intensive domains, such as climate and weather analysis [3], text mining [7,29,32], genomic analysis, and advanced scientific experiments [48,50].

The literature of several research fields has described two main types of phenomena that produce data streams [37]. The first is characterized by the need for data storage space and fast computation. In this situation, the data are stored in secondary memory, present low transfer rates, and are accessed linearly. The second is even more computationally demanding: the data are collected at high rates and, afterwards, disposed. For this second type of phenomena, models must be continuously obtained throughout the endless data gathering process, whose set of dynamical properties, i.e., behavior, is expected to evolve over time [27,37,46].

The severe constraints imposed by the nature of data streams have pushed the understanding and development of automated data analysis strategies [25]. One of the most commonly used strategies to approach such phenomena is data clustering, which aims to represent relevant patterns and trends by evaluating data proximity, similarity and density [24]. The data clustering process, illustrated in Fig. 1, is traditionally organized into the following steps [53]: data gathering, selection or extraction of relevant data features, design or selection of the clustering algorithm, algorithm execution, validation and interpretation of results, and, finally, knowledge extraction.

This typical process (Fig. 1) was designed to approach finite and unordered data sets and, therefore, does not meet data streams requirements. A comparison between clustering method requirements for data sets and data streams is summarized in Table 1 [35]. Firstly, a data stream must be accessed and processed as a sequence, as it cannot be completely stored in the memory. Secondly, the unbounded nature of the data streams demands continuous and automatic analyse, while data set clustering allows more involvement of specialists. Thirdly, while stable phenomena can be accurately represented by a bounded data sample, being appropriate to traditional data clustering techniques, data streams usually represent unstable phenomena [4] and demand the continuous re-evaluation of the clustering models. Finally, the involvement of domain experts is different from that of traditional processes when data streams are clustered. In data set clustering, specialists are often required to empirically extract, select and analyze data features to define the clustering algorithm and the validation criterion. Conversely, in data streams, the fast and continuous production of large amounts of data restricts human intervention, due to the limited capability of specialists to ma ke well-founded decisions under such constraints.
In order to deal with data stream characteristics, researchers have focused on the development of clustering techniques with low time-complexity requirements. From another point of view, fewer studies have been conducted to understand and represent behavior changes [12]. Currently, most of the data stream clustering techniques include a phase of micro-clustering or sub-sampling to first reduce the amount of data and, later, apply a traditional clustering algorithm, such as k -means [24]. Although this latter approach reduces time complexity, it ignores the evolution of data stream properties, delegating further decisions to human intervention. Behavior changes can, for instance, impose modifications on the number and format of clusters over time [46].
 The problem of adapting the clustering process has been partially tackled by techniques, such as Winner Takes All approaches [6,8,15,54] and Grow When Required (G WR ) [30]. Clustering algorithms for data streams that implement the Winner Takes All paradigm adapt clusters according to learning heuristics. G WR uses bio-inspired rules of learning and employs a model of a neurological phenomenon known as habituation [47] to dynamically establish thresholds of pertinence of samples to clusters, and, therefore partly tackles behavior changes. In G WR , as in other Winner Takes All approaches [54], those rules, known as parameters, 1 are empirically defined by specialists according to prior knowledge and maintained constant during the clustering process. However, as the behavior of data stream changes, those parameters, which guide the clustering algorithm, should be adapted accordingly. For that reason, the use of pre-defined and static parameters by current techniques limits data stream clustering.
The need for dynamic clustering algorithms capable of monitoring and reacting to behavior changes motivated this paper, which proposes an on-line and adaptive clustering approach for data streams. This approach, named Data Stream Dynamic Clustering (D SDC ), is based on the traditional k -means algo-rithm to obtain and update cluster prototypes, and on the statistical model of Markov chains to represent data stream behavior. A Markov chain is a model for temporal relations between clusters used to sup-port the measurement of behavior changes in data s treams [17]. The amount of uncertainty of temporal relations of a Markov chain, quantified by the Shannon entropy [44], can be seen as a tool to verify the isomorphism, i.e., structural equivalence among models [36]. In this context, the verification of iso-morphism of consecutive models indicates the need for adapting parameters that guide the clustering process, resulting in a better behavior representation.

Experiments with D SDC applied to a set of data streams collected from several domains with distinct purposes and properties evidenced good results of the new approach. The results were compared against the traditional k -means [21] and Grow When Required [30], which also adapts clustering according to behavior changes. For most of the data streams, D SDC outperformed G WR and k -means in terms of Within Cluster Sum of Squares [21] and Silhouette [40].

The remainder of this paper is organized as follows: Section 2 introduces data stream clustering meth-ods; Section 3 presents the proposed approach; Section 4 describes the data streams employed, experi-ments, and results; and Section 5 draws conclusions and delineates future works. 2. Related works
Data stream clustering techniques can be organized into two major types. The first reduces a stream into a finite and non-sequential data set to apply traditional clustering techniques [24,25]. The second works on data streams clustering directly by associating and updating a cluster to a sample at a time, without reduction or any other data transformation.

In one of the most influential works on the first type, Zhang et al. [55] introduced a highly efficient data structure called Concept Feature tree to represent fine-grained clusters, also known as micro-clusters. The Concept Feature tree has near-linear computational complexity to access and modify micro-clusters. This structure allows the maintenance of micro-clusters, by means of approximating statistical proper-ties, as elements to compose high-level (traditional) clusters.

The micro-clustering strategy has influenced a generation of data stream clustering algorithms [1,18, 22,45,51]. The work by Aggarwal et al. [1] is specially concerned with behavior changes and proposes a technique named Clustream, which employs a pyramidal time-window hierarchy of Concept Feature trees.
Clustream structures information ranging from long to short-term periods, which depend on a special-ist X  X  parametrization. This pyramidal structure is used to organize micro-clusters and form traditional clusters (or macro-clusters). This hierarchy helps to improve clustering quality and provides users with a better understanding of the data evolution. Additionally, this algorithm requires the manual configuration of the macro-clustering phase, which is based on k -means. However, the parameters of the time-window hierarchy and the number of clusters do not vary according to data evolution, leading to model degener-ation/degradation.

In general, the criticisms made to Clustream are valid to most of the techniques of the first type, which deal with data streams, as if they were long and static data sets.

The second type of clustering algorithms directly processes data by means of cluster adaptation. These algorithms are capable of better dealing with behavior changes than those of the first type; however they are less often used due to the absence of major theoretical guarantees to the clustering process. The main theoretical issue of this type of clustering technique is the lack of stabilization [53,54]. Stabilization of a clustering algorithm refers to the problem of finding parameter values to reach a reasonable trade-off between the number of clusters and their size. The stabilization is a difficult task in data stream analysis due to behavior changes, which demand the continuous adaptation of parameters during the algorithm execution.
 Several clustering algorithms of the second type have been proposed [6,8,15,54]. A typical one is Grow When Required (G WR ) [30], a technique based on Kohonen X  X  Self-Organizing Map [28] that associates a varying number of artificial neurons with clusters. The G WR belongs to a class of adaptive self-organizing maps and introduces the use of a model of biological habituation [47] in order to reach stabilization. It operates on-line to adapt the neural map, when appropriate. G WR neural map consists of prototypes that represent the centroid of the data samples collected. A new prototype is created in the network when no matching neuron has been found, according to a static matching threshold. Finally, the prototypes compose a topological map to represent the current behavior of a data stream. [54] introduced an incremental clustering algorithm based on the paradigm of competitive learning Winner Takes All. In order to obtain stabilization, they proposed a concept called starvation trace , which attributes starvation coefficients to prototypes to represent samples, providing all protoypes with the eventual update and better data representation.

According to experiments in simulated and real scenarios, G WR and Young X  X  algorithm offer incre-mental adaptation to data changes. However, similarly to other incremental clustering techniques, they are not able to offer theoretical conditions for stabilization.

In summary, data stream clustering algorithms present limitations to recognize and adjust parameters according to behavior changes. Moreover, most of the algorithms of the first type, i.e., those which reduce data streams to finite sets, are not designed to recognize temporal evolution, while algorithms of the second type, i.e., those which directly work on data streams, are usually designed to recognize it, although they do not use such information to provide behavior change scenarios with stabilization. 3. Data stream dynamic clustering: The proposed approach
As discussed in the previous section, the use of pre-defined and static parameters is a limitation to data stream clustering. This limitation motivated the development of the Data Stream Dynamic Clustering (D SDC ) approach, which adapts the clustering process according to behavior changes.

D SDC adaptation is accomplished by a two-part strategy. The first part consists in defining a partition of clusters using k -means [21]. In that scenario, prototypes represent the behavior states of the target system. 2 A behavior state is a natural conjunction of variables of a system. For example, the state of a car with manual transmission can be described by the gear stick position, which is directly related to the vehicle X  X  acceleration and speed.

The second part, supported by the statistical model of Markov chains [9], consists in describing rela-tionships among behavior states over time. Markov chains represent a wide range of natural phenomena with temporal dependencies described by probabilistic transitions between states. They are intended to represent systems in which the probability of transition to a next state X ( t +1) depends solely on the current state X ( t ) , and not on previous visits to { X ( t  X  k ) ,k 1 } .

D SDC connects the two parts of its strategy by mapping the cluster prototypes partition obtained with k -means algorithm to Markov chains states, as illustrated in Fig. 2. In this figure, three data clusters that compose a partition  X  (Gamma) are represented by letters A, B, and C. Each of data clusters is mapped to the Markov chain in the right part of the figure. The transition probabilities, represented by edge weights, are estimated according to the order in which data are assigned to clusters over time using the method of estimation from one sample with long length [9].

The estimation of a Markov chain starts by D SDC accumulating the amount of samples needed to estimate the transition probabilities.

The amount of samples needed is determined according to the current number of states k . The min-imum amount of samples necessary to estimate Markov transition probabilities should be determined in terms of a maximum error d and a confidence level  X  . Unfortunately, a procedure to determine the sample size to estimate a Markov chain is unknown. Nevertheless, a estimation method for multinomial distribution probabilities can be used as an approximation [49]. The multinomial distribution can be seen as a simplified case of the Markov chain. This distribution is defined by a set of categories related to the Markov chain states, in which transition probabilities are equal for every state. Given this similarity, the estimation method for multinomial probabilities provides D SDC with a conservative upper-bound for the minimum sample size necessary to guarantee a maximum error. This upper-bound can be consulted in Table 1 in [49], which relates the amount of samples required to estimate the probabilities for the multinomial distribution, a maximum error and a level of significance to assure that this maximum error is respected. Under these conditions, it is possible to choose, for a maximum error d and a level of sig-nificance  X  , the sample size to be collected. For the D SDC algorithm, the maximum error is d =0 . 05 and the level of significance is  X  =0 . 1 ,whichresultsin k  X  403 samples. This estimation method is represented in Algorithm 1 by the function sample _ size ( k,d =0 . 05 , X  =0 . 1) = k  X  403 .
Once the samples have been collected, k -means provides the Markov chain estimation method with a partition of the data [9]. The estim ation method is based on [9], which is a maximum likelihood estimator for the Markov chain probabilities. Each cluster in the partition is associated with a state of the Markov chain. The estimation method counts the number of times t ij a sample has been clustered in i ,followed by a sample clustered in j and divides it by the number of samples clustered in i to approximate the transition probability from X i to X j .

Amatrix M with dimensions k  X  k represents the transition probabilities computed as previously mentioned. Additionally, in the process of estimation of Markov chains, each transition between states is associated with at least a very small probability, typically 10 e  X  10 , even if it has not been observed, allowing the modeling of sequences of improbable events [43]. As the transition matrix M has no zero probability, a result from linear algebra known as the Perron-Frobenius theorem [43] guarantees the ex-istence of an eigenvector of M that describes the probabilities of the Markov chain steady-state behavior. The steady-state behavior probabilities represents the asymptotic tendencies of the phenomena [43]. These probabilities are obtaine d according to Eq. (1), in which p is the eigenvector representing the steady-state probabilities of M [43]. Eigenvector p can be estimated, for example, by multiplying M a sufficiently high number n of times lim n  X  M n , until every line has equalled p [43].
Each Markov chain is compared to the previous one by means of its steady-state behavior. This com-parison supports the recognition of behavior changes by employing the Shannon entropy [44] H ,defined in Eq. (2), where p i represents the steady-state probability of state i and k is the number of states in the Markov chain. The Shannon entropy has the property of isomorphism, i.e., invariance to the represen-tation of Markov chains [36]. If the steady-state behavior probabilities of two Markov chains have the same entropy, they are said to be equivalent systems, differing only in representation aspects, e.g. the cluster prototypes.

In order to exemplify the evaluation of behavior changes, consider two Markov chains M 0 and M 1 obtained, respectively, at time instants 0 and 1 . To test if occurred behavior changes between them, their higher than a threshold  X  H , then a behavior change has occurred.

The detection of behavior changes triggers the adaptation process of the clustering algorithm, which, in terms of k -means, leads to a search for a number of prototypes that will better represent the current behavior. In the long term, D SDC is expected to produce an updated sequence of partitions that represent theevolutionofadatastream. 3.1. The algorithm
Algorithm 1 describes the Data Stream Dynamic Clustering (D SDC ) approach. The algorithm requires the definition of an entropy variation threshold  X  H parameter used to decide on the similarity of two consecutive Markov chains and an initial number of clusters  X  k max used in the first step of the algorithm to search for the best initial partition. D SDC ensures the output of a sequence of partitions  X  t , which are constituted by clusters of the current chunk of samples c t , indexed by a time variable t obtained from the data stream.

The data streams is divided into chunks to select samples for clustering and the Markov chain estima-
Before each iteration of the main loop (line 3 ), the number of samples, in this case a and b ,tobe accumulated in the next data chunk c t is determined according to the current number of clusters k .For a limited collection of observations, as described in Section 3, the amount of samples to estimate the transition probabilities of a Markov chain is determined in terms of a maximum error d and a confidence level  X  , represented in the algorithm by the function sample _ size ( k,d =0 . 05 , X  =0 . 1) = k  X  403 (line 3 ).
 Algorithm 1 Data Streams Dynamic Clustering Require: Initial maximum number of clusters  X  k max and entropy similarity threshold  X  H Ensure: A sequence of partitions  X  t 1:  X  t := search-k -means( c 0 , k max ) 2: H t := H ( M ( X  t )) 3: for all data chunk c t of sample _ size ( k,d =0 . 05 , X  =0 . 1) length do 5: H t := H ( M ( X  t )) 7: if  X  H t &gt;  X  H then 8: ( X  t ,k ):= search-k -means( c t , k max = 2  X  k ) 9: else 11: end if 12: end for
The D SDC algorithm begins by computing  X  t using the first chunk of samples c 0 (line 1 ). This compu-tation is initially performed by the search-k -means( c 0 , k max ) method, which implements an exhaustive search for a partition of k clusters, with k varying from 2 up to the maximum k max . This method returns the partitio n found by k -means which had the highest Silhouette value [40], a measure of quality defined in the [  X  1 , 1] interval, in which the higher the value, the greater coherence and separation of clusters (see Eq. (4)).
 The following step (line 2 ) is to compute the steady-state behavior entropy, represented by the H ( M ( X  t )) function, of the Markov chain estimated for the current clustering partition  X  t .
In the main loop (line 3 ), D SDC first checks if consecutive Markov chains are isomorphic by com-compared against threshold  X  H (line 7 ) to decide on restructuring the cluster prototypes using search-k -means( c t , k max ) (line 8 ), or updating them to the current data chunk (line 10 ). The update in line 10 D
SDC modifies the length of the current data chunk c t to sample _ size (2  X  k,d =0 . 05 , X  =0 . 1) and performs a new search step using the search-k -means( c t , k max ) method.
 At the beginning of a new iteration of the main l oop, a new Markov chain is obtained by calling the H ( M ( X  t )) function (line 5 ), which represents the updated model for the data stream. 3.2. Follow-up example
This section describes a typical execution of Algorithm 1. First, consider Fig. 8 to represent obser-vations obtained from an artificial data stream composed of two parts. The first part consists of 5 , 000 samples, which form a four-cluster partition and the second presents 3 clusters over 5 , 000 samples.
D SDC algorithm is executed with parameters selected by a specialist, i.e., k max =5 and  X  H =0 . 02 .It begins searching for the partition produced with k -means, for k varying from 2 up to 5 with the highest value of Silhouette (line 1 ), i.e, k =4 . For this search D SDC used 2 , 015 samples. Line 2 obtains entropy H fetches a chunk of data c t of length 1 , 612 (line 3 ). This chunk is used in line 5 to estimate a partition, its Markov chain and entropy H t . Entropies H t and H t  X  1 are computed in lines 6 and 7 to decide if a lower part of Fig. 8 around Index 3 , 000 , the partition is only updated. The converse is observed for data a chunk from around 4 , 000 to 5 , 100 ,forwhich  X  H is noticeably higher than the threshold. For this data chunk, the search at line 8 is performed and a new partition is defined and maintained up to the end of the samples. 4. Evaluation This section compares the D SDC approach against G WR [30] and the traditional k -means [21]. The G
WR was chosen due to its capabilities of adapting clustering according to behavior changes and also because it is a good portrait of current clustering algorithms that directly process data streams using incremental learning. The k -means is considered a comparison guideline due to its relative simplicity, widespread use and consistent good theoretical and experimental results [24]. Moreover, the use of k -means represents the use of other clustering techniques that first reduce data streams to finite and non-sequential data sets, such as Clustream [1] and B IRCH [55].

The evaluation is based on data streams commonly used in the literature, whose properties allow the observation of D SDC performance under a variety of conditions. T hey have characteristics often found in real contexts, such as high-noise level, chaoticity, s low/quick non-stationarity , high-frequenc y sampling, and mixtures of different sources.

A list of the selected data streams is found in Table 2, which shows their name, number of dimensions, nature (deterministic or stochastic), and origin (real or artificial). These data streams are available for public use [14]. For the following experiments, each dimension was normalized in the [0 , 1] range.
Data streams A , B , C , D ,and E were used in a contest of time series modeling [52]. A is a sequence obtained from a laser generating a deterministic and chaotic signal. B is a multivariate data stream recorded from a patient in a sleep laboratory which contains heart rate, chest volume, and blood oxygen concentration measurements. C is a data stream composed of records of exchange rates from Swiss francs to US dollars from August 7, 1990 to April 18, 1991. D is a computer-generated non-stationary sequence based on the motion of a damped particle. E consists of noisy astrophysical data measurements of the light curve, i.e., recordings of time variation of the intensity of the variable white dwarf star PG1159-035 in March 1989.

The Control Charts set is a collection of 600 artificial non-stationary sub-sequences of 6 different types, originally to be used in monitoring systems [2]. In order to perform the experiments, the sub-sequences were randomly sorted (with no alteration in the internal order of the sub-sequences), resulting in a sequence of 36 , 000 data samples .

The Posture data stream was originally collected to r ecord the position of four human body parts (belt, left ankle, right ankle, and chest) during several physical activities in order to recognize falls [26]. The data were sequentially collected from 5 different people. Each sample consists of a three-dimensional position of a pers on X  X  body part.
 The performance of the clustering algorithms can be compared with intrinsic or extrinsic measures [5]. The intrinsic measures, also known as internal measures, use the distances among the elements within the same cluster and the distances among different clusters to evaluate clustering adequacy. The extrinsic, or external measures, use information from outside the clustering process, e.g. information supplied by domain specialists to define a reference to which the algorithm is considered to produce correct results. Although widespread in the clustering literature, the use of the second approach has been discussed [13] and is less appropriate for data streams scenarios, where such extra information is rare and/or costly.
Within Cluster Sum of Square Distance ( WCSS ) [24] and Silhouette [40] are intrinsic measures often used in data stream clustering evaluations and, therefore, considered for the comparison of clus-tering algorithms. In fact, several other measures, such as the popular Dunn index [11], Davies-Bouldin index [10], and C-index [23] can be used for the evaluation of clusters [33]. Although each of these methods has advantages and limitations, Silhouette has been verified as an accurate tool in several do-mains [19,20,38,41]. It takes into account two important objectives for clustering, cohesion and sepa-ration [19]. For example, in the context of clustering microarray data, Silhouette is considered a good predictor of how well the genes are clustered and less noise-sensitive than Dunn Index [20]. In another example, Silhouette has shown to be more accurate than Davies-Bouldin index for application in an Intrusion Detection System, which is a common scenario to observe data streams. However, the main drawback of Silhouette is its bias towards better evaluating spheric-shaped clusters [20], which is also a characteristic of k -means. Within Cluster Sum of Square Distance is also included as it is the measure of error that the k -means aims to minimize.

Within Cluster Sum of Square Distance WCSS , Eq. (3), in which  X  i is the i -th prototype that rep-resents cluster  X  i in partition  X = {  X  1 ,..., X  k } consisted of samples x j  X   X  i ,isusedtoevaluatethe representativeness of k prototypes. WCSS decreases when prototypes are well-representing data, or when the number of clusters increases.

Silhouette is commonly used to evaluate clustering quality based on the average dissimilarity of sam-ples within the same cluster and on the average dissimilarity among different clusters. Equation (4) defines Silhouette S for a set of n samples as the proportion between a i , the average distance of each data element i to elements of its cluster, and b i , the minimum average distance of i to other clusters. This equation is defined for partitions with at least 2 clusters, and  X  1 is assumed for one-cluster parti-tions. Silhouette does not have a unit of measurement and ranges from  X  1 , when samples are wrongly clustered, up to 1 , when data are perfectly grouped [40].
 4.1. Sensitivity analysis
This section analyses the sensitivity of evaluation measures with regard to the variation of D SDC parameters and its application on different data streams. An evaluation measure Y for a given partition obtained with a clustering algorithm f ( X 1 ,X 2 ,...,X i ,... ) is a function of its parameters and data, called factors, { X 1 ,X 2 ,...,X i ,... } .

Sensitivity indexes have been used in model analysis to verify how they respond to changes in fac-tors [16,42]. For example, university rankings use a set of factors to compare a list of institutions. A sensitivity analysis can be used to find the most influential factor to the rank and to determine if the imprecision of data describing this factor changes ranking positions significantly.

The first-order sensitivity index is defined as the ratio of variance of Y due to a factor X i , according to Eq. (5). In this equation, Sensitivity index S i is obtained using Var ( Y | X i = x i )=  X  2 i ,whichisthe fractional variance of evaluation measure when factor X i is subject to a fixed value x , and the overall variance Var ( Y )=  X  2 . The higher the values of S i , the higher the sensitivity of Y to X i .
The sensitivity analysis of D SDC considered the influence of variation of parameters and data streams on the evaluation measures Silhouette and WCSS . The two parameters for the execution of D SDC are an entropy similarity threshold ,i.e.,  X  H , used to detect behavior variation, and an initial maximum number of clusters , i.e., k max , which is the starting point to determine the first partition. The variation
Figures 3 and 6 display the sensitivity index curves of Silhouette for the parameters, while Figs 4 and 5 show the corresponding curves of WCSS . These figures presented the index curves by means of box plots using median, upper quartile, lower quartile, 2% percentile, and 98% percentile. The variation of the parameter values is shown in the horizontal axis. The evolution of the index curves in Figs 3 and 4 shows a low variation of the median of sensibilities of Silhouette and WCSS in the  X  H  X  (0 , 0 . 55) interval, with a tendency of a gradual increase in the quartile area along with an increase of the parameter  X  H . As this threshold is used to verify if two Markov chains are structurally equal and should be set closer to zero, the specialist does not need to set a precise value to this threshold when considering  X  H  X  (0 , 0 . 55) .

The evolution of the sensibility curves of WCSS and Silhouette for parameter k max in Figs 5 and 6 shows a WCSS stability curve stable until k max =9 , with quartile areas decreasing as k max increases. Conversely, the sensibility of Silhouette to k max reaches more stability to k max  X  (8 , 15) .Theinverse trends of stability curves to k max between WCSS and Silhouette suggest the existence of a trade-off point to the stability index of both evaluation measures around 8 .
 4.2. Execution and results In order to perform the experiments, it was necessary to set parameters for all techniques, i.e., D SDC , G
WR ,and k -means. For D SDC , based on the sensitivity analysis previously described, the entropy sim-ilarity threshold  X  H was set to 0 . 02 to establish a margin to compare the similarity between Markov chains and detect behavior changes, and the initial maximum number of clusters k max was set to 10 as it allowed a reasonable space to search the best starting partition.
 0 . 1 , employed to adapt clusters according to behavior changes; insertion threshold a T =0 . 99 ,used to define the sample pertinence to clusters; adaptation parameters b =0 . 2 , n =0 . 06 ,  X  b =3 . 33 ,  X  n =14 . 3 , which guided the learning of prototypes; and the maximum number of nodes, set to 40 .
The implementation of k -means considered in this paper is the Hartigan-Wong version [21], provided by the R-project (package stats ) [39]. The application of k -means consists in using the number of clusters found in the initial processing step of D SDC , which selects a k that provides the partition with the highest Silhouette. In order to avoid local minima of k -means, every partition is obtained with the execution of 30 random starts. Observe that once the number of clusters has been set, the usage of k -means allows simulating cases in which specialists select the parameters, maintained constant regardless of behavior changes.

The experimental results are summarized in Table 3. Column k shows the average and standard de-viations of the number of clusters and columns S and WCSS display the averages of Silhouette and Within Cluster Sum of Square Distance (along with their standard deviations inside parentheses) for the clusters obtained by a given technique. These measures were computed using sub-sequences of 500 samples obtained by sliding time windows . For example, the first sub-sequence is composed of samples from time instants 1 up to 500 , the second comprises samples from 2 up to 501 , and so forth. While this approach allows the comparison of techniques, which may output clustering partitions at different moments, some sub-sequences may be constituted by onl y one cluster causing Silhouette to be evaluated as  X  1 . For the same reason, the use of sliding time windows may also produce variations in the number of clusters k during the evaluation of static k -means, as observed in Table 3.

In order to verify the validity of the results, Wilcoxon-Whitney hypothesis tests, in which the results of k -means and G WR are compared against D SDC , were performed. The null hypotheses h 0 considered are the average over time of Silhouette, or WCSS ,of k -means or G WR algorithms is higher than those obtained with D SDC . The rejection of h 0 is denoted by  X  when the p -value of the Wilcoxon-Whitney rank sum test is lower than the significance level of 0 . 05 . Similarly,  X  denotes the converse scenario, the rejection of null hypotheses h 0 : average Silhouette, or WCSS , lower than the one obtained with D SDC .
In general, the hypothesis tests suggested that D SDC presented better results than the static k -means and the adaptive G WR . Observe that the average Silhouette obtained with D SDC for all data streams is at least similar to the one obtained by the other techniques. Also, in comparison to k -means, D SDC presented lower values of WCSS , although both techniques usually had similar k values over time windows. Remarkably, G WR produced an average of clusters higher than 10 for 5 data streams, which led to lower WCSS results.

Figures 9 and 10 illustrate two typical performances of the algorithms with regard to 3 Control Charts and B data streams. Figure 10 displays results for B ,inwhichD SDC approach exhibits unstable, how-ever good, Silhouette values. Despite its instability, the Silhouette obtained with D SDC outperformed k -means and G WR . On the other hand, G WR is better evaluated when considering WCSS , at the cost of a high number of clusters.
 The results for data stream Control Charts, presented in Fig. 9, showed a converse scenario. The Silhouette values obtained for D SDC clusters were more stable and better than those provided by k -means and Gwr . On the other hand, although WCSS results of the static k -means are more stable, D SDC varied from k -means results to higher values, which is attributed to k adaptation. These results have shown the advantages of adapting the parameters of clustering process according to behavior variation. 5. Conclusions and future work
The Data Streams Dynamic Clustering (D SDC ) approach addresses the problem of clustering data streams according to behavior change detection. It uses theoretical results from Information Theory [44] and Dynamic Systems [36] to identify behavior changes and update clusters accordingly. Experiments with D SDC have exhibited promising results when applied to data streams from several fields with dis-tinct purposes and properties. The results were compared with those from a traditional k -means and Grow When Required neural network. For most data streams, D SDC outperformed G WR and k -means in terms of Within Cluster Sum of Distances and Silhouette, evidencing the advantages of parameter adaptation for clustering data streams. Future works include the study of other methods of adapting clustering to data streams behavior changes by means of, for example, the use of long-term temporal information and the application of other combinations of algorithms and models besides of k -means and Markov chains.
 References
