 The rapid growth of modern Web 2.0 folksonomies originates in the ease of collabo-rative content generation and content sharing for nonexpert mass users. In recent years, substantial research progress was achieved regarding folksonomy mining and designing suitable recommendation, search, and retrieval solutions. However, some fundamental questions remain still open. One of the key challenges for Web 2.0 content management and sharing is a deeper understanding of the complex, multimodal con-tent nature. In general, a Web 2.0 folksonomy provides several facets of media knowl-edge, including content annotations (e.g., image tags in Flickr), spatial knowledge (e.g., GPS coordinates, names of locations), user-specific aspects, and media proper-ties. However, the construction of appropriate rich folksonomy models and their formal analysis remain a difficult enterprise. For this reason, existing solutions and models are often restricted to particular, most  X  X romising X  folksonomy aspects (e.g., analysis of social networks or content annotations), which are then considered in an isolated manner.

Despite the huge amount of shared content in popular folksonomies (such as Flickr 1 or YouTube 2 ), the available data for particular modeling aspects is often sparse and imprecise. The image-sharing scenario with Flickr is characteristic for this kind of problem. On one hand, the concentration of media related to popular locations, events, and objects is awesome. For instance, the query  X  london  X  returns on Flickr over 13.6 Mio matches (observed in 2011). However, the prevalent majority of these resources is annotated by few, quite heterogeneous, tags. This input is clearly insufficient for fine-grained similarity estimation and relevance ranking (e.g., for content categorization, personalized filtering, or tag recommendation scenarios), even less for discovery of nontrivial, complex media aspects (e.g., event detection as input for faceted, aspect-oriented browsing). The support for content-sharing tasks can be improved with respect to novel properties of mass social media. In fact, photos and videos produced on state-of-the-art multime-dia devices are frequently accompanied with spatial information (e.g., coordinates from integrated GPS receivers). For instance, in the CoPhIR dataset, 3 around 4 million out of 54 million resources (Flickr images) are associated with geographical coordinates. The number of geographically annotated images is supposed to increase in future, as more devices will be able to capture the spatial information. In this sense, our work is motivated by multiple emergent technological trends related to social media (avail-ability of integrated GPS modules in modern endpoint devices like digital cameras and smartphones, growing popularity of collaborative platforms for geocaching and track sharing, integration of spatial knowledge with multimedia standards like EXIF, geo-tagging in social media like Flickr).

Basically, relatedness/similarity of resources or tags can be directly estimated by calculating distances between resource locations (using spatial coordinates) or by mea-suring overlaps between annotations of resources (using tags, or advanced tag-based models like ESA [Gabrilovich and Markovitch 2009]). However, tags or geodata taken alone are often insufficient for reliable content disambiguation. For instance, in our sample scenario, both photos of the London Tower and of the Tower Bridge can origi-nate from the same location (e.g., trip boat on the Thames river) but show two differ-ent attractions. On the other hand, views of large buildings and monuments (e.g., the London Eye) can be captured from different locations but show the same object and also have similar annotations. Consequently, we may expect that a combination of both tags and spatial information will allow for better content characterization. From the conceptual perspective, the contribution of this article is twofold. On one hand, we demonstrate in systematic studies the benefits of combinational approaches for textual and geospatial features for a broad spectrum of scenarios related to social media: recommender systems, automatic content organization, and filtering, and min-ing of complex patterns (such as event detection). On the other hand, we also establish a simple and technically sound model for this combination that can be seen as a refer-ence baseline framework for future research in the field of geotagged social media. The model GeoFolk (an acronym for Geodata in Folksonomies) is designed for Web2.0 content characterization with spatial awareness. We employ Bayesian sta-tistical models that explicitly describe geodata (spatial coordinates) jointly with tag co-occurrence patterns. The resulting model can be seen as an extension of latent Dirichlet allocation (LDA), a popular approach that has been frequently used in the recent literature for representing various multimodal dependencies in social media. Beyond LDA-like tag generation process, our model also integrates topic-specific dis-tributions for spatial location (e.g., latitude and longitude). Sampling-based parame-ter estimation, based on the Monte Carlo Markov chain method (MCMC), is driven to discover latent topics that simultaneously capture word co-occurrences and spatial localization of characteristic patterns.

The rest of the contribution is organized as follows. Section 2 gives a detailed expla-nation of the GeoFolk model. Subsequently, we instantiate our model for characteris-tic application scenarios (content categorization and clustering, tag recommendation, event detection) and show results of systematic evaluations with real-life datasets in Section 3. In Section 4 we discuss related work. Section 5 summarizes lessons learned and shows directions for future work. For constructing the GeoFolk model, we exploit the idea of Bayesian latent topic mod-els, that is, mechanisms for discovering low-dimensional, multifaceted summaries of documents in a probabilistic manner. This section includes the problem formal-ization for Web 2.0 setting, explains our design choices, and shows possible model instantiations. In our model we are given an arbitrary collection D = { d 1 .. d D } of D folksonomy re-sources (e.g., shared photos, videos). Each resource d in this collection is annotated by some tags 1 .. N d (we assume  X  d , N d  X  1). These tags are taken from the vocabu-lary V = { w 1 ..w V } that consists of V different words w i . Additionally, each resource d is annotated by numeric attributes lat d  X  R and lon d  X  R , which represent spatial coordi-nates of the position of its creation (geotags). The approach aims to explain observed properties of D (i.e., tags and geotags of all d  X  D ) by the means of a Bayesian model with T latent topics. For the sake of clarity, Table I summarizes the notation used in the following sections. The idea of explaining resources by the means of an individual mixture of latent topics (i.e., topics that contributed to their generation) originates in a well-known Bayesian model coined latent Dirichlet allocation (LDA) [Blei et al. 2003]. In our application scenario, LDA can be directly exploited for explaining assignments of tags taken from V to resources contained in D . This generative process is summarized as follows. For each resource d , a multinomial distribution  X  d over topics is randomly sampled from a prior Dirichlet distribution with parameter vector  X  . To generate each tag w d ( i =1 .. N d ), a topic z d drawn by a topic-specific multinomial distribution, characterized by  X  z N d of tags attached to the resource d is assumed to be drawn by a Poisson distribu-tion. We also assume that for each topic T , the parameters  X  T of the corresponding multinomial distribution over tags are drawn from a prior Dirichlet distribution with parameter vector  X  .

The corresponding graphical representation for this tag assignment model is shown in Figure 2, and the generative algorithm (using the common Bayesian pseudocode notion) in Figure 1.

Given a collection of annotated resources D , the learning algorithm estimates pa-rameters of distributions  X  d and  X  z that provide a good fit with observed data (i.e., explain the generation of D with high probability). In contrast, the number of latent topics T and parametrization of the Dirichlet distributions  X  and  X  are meta param-eters, that is, they must be specified by model designer and may require empirical tuning. In our GeoFolk approach, topic discovery is additionally affected by spatial informa-tion, which is considered jointly with tag co-occurrences. The key objective is to pro-vide a joint explanation for resource annotation and its coordinates by a mixture of latent topics. However, spatial coordinates are usually assigned to resources only once. Therefore, they cannot be naturally explained by a mixture of topics like tag assign-ments. To achieve the desired functionality, the model demands certain conceptual adaptation.

The suggested generative process for annotated resources with assigned coordi-nates can be summarized as follows. For each resource d , a multinomial distribution  X  over topics is randomly sampled from a prior Dirichlet distribution with parameter vector  X  . To generate each tag w d distribution, and then a tag w d with parameters  X  z lat d i and lon d i from two topic-specific Gaussian distributions of tags attached to the resource d is assumed to be drawn by a Poisson distribution. Gaussian parameters  X  lat z and  X  lon z (i.e., means for topic-specific latitude and longitude) are assumed to be drawn from a certain interval (which represents coordinate ranges in observed data) using independent uniform distributions  X  lat and  X  lon . Variances of all Gaussian distributions are assumed to be drawn from a certain interval using an independent uniform distribution.

The generative process of GeoFolk consists of steps shown in Figure 4, the corre-sponding graphical representation is shown in Figure 3. The presented solution can be seen as a modeling compromise, in order to capture correlations between coordi-nates and tags by a mixture of latent topics. In doing so, the GeoFolk model actually describes data in which coordinates are associated with each tag. When fitting our model from real data, coordinates of each training resource are attached to all tags of this resource. However, after fitting, GeoFolk runs as a common generative model. In other words, this process would generate different coordinates for the tags attached to the same resource. This kind of model deficiency is common for other similar LDA extensions, such as topics over time [Wang and McCallum 2006]. The actual modeling problem arises with ambiguous prediction of coordinates for new, previously unseen, resources. However, this aspect is of less practical importance for GeoFolk. In our Web 2.0 application scenario, we may usually assume that spatial coordinates of the resources are known (and there is no need to predict them). Practically relevant fields of model application, such as predictions of tags and similarity estimation between resources, are not affected by this issue at all.

We note that the behavior of latent topics in GeoFolk is different from LDA. Since topic-specific location is described by normal distributions, the particular topic is  X  X ctive X  in a certain spatial area. This (intended) behavior establishes relationships between tag distributions and locations of interest. If the same pattern of frequent tags appears at different places, the resulting model will contain multiple latent topics with similar multinomial distributions over tags, but different normal distributions for locations.

The GeoFolk model presented so far combines discrete and continuous distributions in a nontrivial manner. The resulting model is complex and does not allow for exact inference and parameter estimation. For this reason, GeoFolk performs approximate inference by Gibbs sampling, using the Monte Carlo Markov chain method (MCMC) [Andrieu et al. 2003]. The derivation of estimation formulas for Bayesian learning with GeoFolk model is fully analogous to Wang and McCallum [2006].

Spatial information is continuous by its nature. Coordinate assignments produced by topic z can be naturally modeled by a bivariate normal distribution  X  z .Asanal-ternative, we may also consider two Gaussian distributions  X  lat z and  X  lon z (i.e., sep-arately for latitude and longitude). Our earlier experiments were based on a joint distribution  X  z , which turned out to be a practical bottleneck, due to poor conver-gency of parameter estimates. For all the results in this article we employ the (sim-pler) second choice and consider latitude and longitude as two separate continuous variables.
 As a possible design alternative, we may consider discretization of coordinates. However, the choice of appropriate grid sizes is highly application-and data-specific and may cause additional modeling problems. For instance, the constant grid size can appear too small for some regions (for instance, London vs. Madrid, in our sample scenario) and too large for others (say Westminster vs. Houses of Parliament in Lon-don). As a result of granularity problems, the model may miss spatially compact but meaningful topics. From the computational perspective, sparsity of discrete spatial data may also cause fitting problems for Gibbs sampling with MCMC. For these prag-matic reasons, GeoFolk avoids discretization by associating with each topic continuous distributions over coordinates.
 From the application point of view, the set of latent topics can be used to answer var-ious questions about the similarity of resources and annotations. In our model inter-pretation, two resources are similar to the extent that their generation is explained by the same topics. Analogously, two annotations are similar to the extent that they are generated by the same topics.

The distance between probability distributions (specially resource-specific multi-nomial distributions  X  d 1991]. A standard measure for estimating divergence between two distributions is the Kullback X  X eibler divergence (KL). Given two resources d x and d y with corresponding distributions over topics  X  d To obtain a symmetric distance measure, it is convenient to consider the symmetric variant of KL divergence: As an alternative, we may also consider the symmetrized and smoothed Jensen X  Shannon (JS) divergence [Landauer et al. 2007; Lin 1991], also known as information radius. This measure estimates dissimilarity between  X  d of both distributions: Since the square root of (3) has been shown to be a proper metric [Fuglede and Topsoe 2004] (i.e., it satisfies the usual axioms of non-negativity, identity of indiscernables, and triangle inequality X  X nlike (3) itself), JS  X  d measure in the common IR sense.

Of course, it is also possible to consider topic distributions as  X  X ust X  numerical, L 1 -normalized vectors and to apply simple geometric similarity functions like dot product or cosine measure [Manning et al. 2008]. The distance measures introduced so far can be exploited for a variety of realistic scenarios for social media, including content organization, search, and tag/content recommendation.

Content organization. In particular, the topic-based feature space can be exploited in a straightforward manner for content structuring (e.g., clustering of photos). Analo-gously, supervised learning methods (classification) can be applied for content catego-rization and filtering. In our sample scenario, we may think of automatic clustering of photos from the last trip to London, combined with automatic annotation of clusters by most characteristic tags from Flickr for cluster coordinates.

Keyword-based search and ranking. A keyword-based query q = { w q 1 ..w qp } can be treated as a new annotated resource d q with tags w q 1 ..w qp . Consequently, d q can eas-ily be transformed into the topic-based feature space of the tag assignment model. The required parameter estimation for  X  d learned (and now fixed) tag-topic distributions  X  z for z =1 .. T . As a result, we obtain for the query q its distribution  X  d and ranked retrieval of resources in the topical feature space. As an example, we consider the query  X  london eye  X  for our sample scenario. Query transformation would allow for finding resources annotated by semantically coherent tags (say  X  londoneye  X ,  X  millennium-wheel  X , etc.) and also filtering out thematically irrelevant matches that show significantly different distribution over latent topics (say  X  golden eye  X ).
Keyword-based search with spatial awareness. Analogously, a keyword-based query q = { w q 1 ..w qp } with spatial preferences lat q and lon q can be transformed into the topical feature space of the GeoFolk model. The required parameter estimation for  X  d by Gibbs sampling, with previously learned (and then fixed) tag-topic distributions  X  z and spatial distributions  X  lat z and  X  lon z for all topics z =1 .. T . In our sample scenario, the search for  X  piccadilly  X  may be combined with coordinates of the London city cen-ter. This would help to filter out identically annotated but irrelevant photos of the Manchester Piccadilly train station.

An interesting special case of retrieval with GeoFolk is the prediction of coordinates for keyword-based queries. For a keyword-based query q = { w q 1 ..w qp } , its distribu-tion over topics  X  d Gibbs sampling, when the GeoFolk model is not conditioned on fixed values for lat q and lon q . This feature can be exploited for assisting the user in navigation through locations, for example, by displaying the map of the predicted location together with relevant matches identified in the nearest vicinity of this position. In our sample sce-nario, the search for  X  tower  X  may display the map with close view on the London Tower and the Tower Bridge, with previews of most relevant photos for both attractions.
As discussed in Section 2.3, the GeoFolk prediction of locations is ambiguous for queries that consist of more than one keyword. An alternative generative process de-scription of GeoFolk (better suited to explain such queries) is one in which a single pair of values for lat q and lon q is generated for q . The desired behavior can be achieved with GeoFolk by importance sampling, from a mixture of per-topic Gaussian distributions, with mixtures weight as the per-resource  X  d over topics. In this case, the distribution of coordinates remains parameterized by the set of coordinate-generating Gaussian distributions, but the visible model outcome allows for easier practical interpretation.
Tag recommendation and exploration. From a different perspective, per-topic tag distributions  X  z (i.e., topic-specific multinomial distributions that represent tag gener-ation probabilities) can be used for constructing per-tag feature vectors. In fact, gen-eration probabilities for a particular tag w across topics  X  z ( w )( z  X  1 .. T ) can be seen as features of w in a new vector space, which is constructed over latent topics 1 .. T as new dimensions. We note that resulting feature vectors do not represent probability distri-butions and additional L 1 -normalization may be necessary. Tag-specific vectors in this space can be exploited in the usual IR-like manner for tag recommendation (based on similarity estimation between tags) and for computation of semantically coherent tag clouds (by classification or clustering of tag vectors).

Event detection. As a prominent example of more complex analysis for social media, we consider the detection of social events in online content. In the majority of cases, the  X  X ingerprints X  of social events are clearly localized in space and time. The introduced GeoFolk model can be adapted to capture the temporal aspect (i.e., generative process for resource timestamps time d  X  R ) as shown in Figure 5. In this generative procedure, latent topics z i d generate tags and geotags in the same manner as before. Additionally, z d also generates the timestamp time a specific distribution  X  z (beta distribution); the corresponding hyper parameters a z und b z of the distribution are chosen from an application-specific interval by means of a uniform prior distribution. Figure 6 shows the graphical representation for the modified model.

In this setting, the problem of event detection can be seen as a topic selection task, whereby topics with specific properties (especially low variance of distributions for geotags and timestamps, e.g., using empirically defined thresholds) are interpreted as models for  X  X atent events X . Accordingly, topic-characteristic tags and resources can be used for presenting discovered topic-based events to the user. The key objective of our evaluation studies is to demonstrate the usefulness of the combination between text and spatial knowledge for better understanding of social media. The evaluation methodology includes two main directions. On one hand, we analyze statistical properties of the introduced GeoFolk model (such as model fit and model complexity) in different settings. On the other hand, our aim is to demonstrate the practical viability of the introduced modeling approach. Consequently, we instan-tiate our application-oriented evaluation with four realistic scenarios for social media: content classification, content clustering, tag recommendation, and event detection.
In our comparative evaluations we consider several modeling approaches for social media. The choice of discussed methods is also motivated by our top-level evaluation objective: analysis of the combined model in comparison with baseline methods that exploit particular aspects of social media (tags, coordinates) in an isolated manner. In particular, the GeoFolk model is primarily compared with LDA-like tag assignment model from Section 2.2. Wherever applicable, we also consider two simple baseline models that represent spatial coordinates and tags without any complex transforma-tions. To ensure objectiveness, all user-centric evaluation tasks discussed below were performed by five independent experts (computer scientists working in the field of so-cial media research) and then aggregated by majority voting. The evaluation infrastructure for GeoFolk was implemented as a Java 1.6 framework. For performing approximate inference by Gibbs sampling (using the Monte Carlo Markov chain simulations), the JAGS framework 4 was used. Results of JAGS simu-lations (i.e., estimated model parameters) were processed in the statistical computing framework R 5 and finally stored in an Oracle 11g database instance for subsequent application-oriented experiments. The majority of experiments was performed on a server with 72 Gb main memory and 16 Xeon cores. In our experiments we used parts of the publicly accessible CoPhIR dataset 6 that con-tains metadata for over 54 millions of Flickr resources (as observed in 2009). The dataset includes resource descriptors, tag assignments, extracted low-level features, spatial attributes, and other meta-knowledge for social media. For evaluating Geo-Folk, we were primarily interested in tag assignments and spatial attributes of re-sources and did not consider further aspects (e.g., low-level features) for modeling. In line with our preceding argumentation, we restricted the scope of the evaluation dataset to several popular locations (London, Barcelona, Paris, New York) that were evaluated using separate models. This design choice had several pragmatic reasons. From the data availability viewpoint, we were able to collect sufficient content with geospatial metadata (more than 20,000 resources) only for a limited number of mass tourism destinations. From the application point of view, the scenario for one partic-ular location is especially hard to handle regarding categorization of content and tag recommendation, due to substantial overlap of tag annotations and close proximity of considered attractions. From the technical point of view, restriction of the model scope allows for satisfactory scalability, in order to perform systematic series of experiments in reasonable time. From the pragmatic evaluation point of view, the choice of a famous location allows for better content understanding by human experts and, subsequently, high inter-rater agreement.

For application-oriented evaluation, at least 1000 resources from each dataset were manually inspected by human experts and associated with one of the famous locations in the respective city (such as Westminster, Marble Arch, etc., for London). This la-beling with spatial awareness was then exploited as a gold standard for our resource categorization experiments. In our experiments, we compared a number of practically relevant model properties and settings. All models discussed in this section were defined by scripts in the JAGS framework, and instantiated with data of the reference Flickr dataset described in pre-vious section. The approximate inference was performed by Gibbs sampling (using the Monte Carlo Markov chain simulations). By convention, the MCMC output is divided into two parts: an initial burn-in period, which is discarded (at least 1000 iterations in each experiment), and the remainder of the run (10,000 iterations in each experi-ment), in which the output is considered to have converged to the target distribution. Samples from the second part are used to create approximate summary statistics.
In many practical cases, topic models are quite sensitive to the choice of hyper-parameters, that is, in our case T (number of latent topics) as well as  X  and  X  (settings for Dirichlet distributions). In the context of our GeoFolk target application scenario, we observed that the sensitivity to hyper-parameters was not very strong. We used symmetric Dirichlet distributions with  X  = 50/ T and  X  =0 . 1 in all presented experiments.

The complexity of data (i.e., tag assignments alone vs. tags with coordinates) has direct influence on the model complexity, main memory consumption, and computa-tional overhead for model convergency. The complexity of each model also increases with growing number of latent topics. Table II shows practical statistics for models learned in our experiments (averaged values). Models with a larger number of la-tent topics require rapidly growing processing times, but add no principally new value to the observed behavior. However, the numbers for observed memory consumption should be seen as quite raw estimates, since the simulation software incrementally allocates memory in blocks and also applies a number of optimizations.

A natural way to compare Bayesian models is to consider trade-offs between the fit of the data to the model, and the corresponding model complexity. For our models presented so far, we consider the deviance information criterion (DIC) as proposed in Bishop et al. [2002]. The criterion DIC = D + p D combines the model deviance D (  X  ) and the model complexity p D . Models with smaller DIC are better supported by the data. The model deviance is defined as where  X  are stochastic parameters of the model. Deviance reflects the model fit for ob-served data; we consider the total deviance by summing up deviance for all stochastic nodes in the model. The model complexity is measured by estimate of the  X  X ffective number of parameters, X  given by posterior mean deviance minus deviance evaluated at the posterior mean of the parameters: Analogously, the total complexity is estimated by summing up complexity for all stochastic nodes in the model. In our experiments, complexity was estimated across five chains for each learned model. Figure 7 shows the collected DIC values for models in our evaluation. Since LDA and GeoFolk explain different data (without/with con-sideration of spatial information) their values cannot be directly compared. However, it can be observed that the GeoFolk model with larger number of topics is significantly better supported by the data than the model with few topics. For the LDA model, such significant improvement cannot be observed. We assume that this behavior is caused by the high sparsity of the tag data in our Cophir subsets that have been used for evaluation. The objective of our classification experiments was to evaluate the ability of the meth-ods presented for content categorization in two settings: supervised learning (with explicitly given training data) and unsupervised learning (without a priori available training data). The comparisons include four representational models for Flickr data: GeoFolk, an LDA-like tag assignment model, simple tag-based resource representa-tion, and resource coordinates. Conceptually, all resource representations mentioned use the vector space model. However, the way of constructing resource-specific fea-tures is quite different. GeoFolk characterizes resources by probabilistic distributions over latent topics, jointly learned for tags and spatial data. Similarly, the LDA-like tag assignment method constructs the latent topic model for tags alone. The distance between feature vectors of two resources in these models was estimated by square root of the Jensen X  X hannon (JS) divergence (3). As an alternative, simple geometric sim-ilarity measures (e.g., cosine measure) can be used as well; notably, at the confidence level 0.95, the variation of results by applying different similarity/distance measures was not statistically significant in our experiments. The tag-based resource represen-tation uses the term-based vector space model. Consequently, similarity between two resources is estimated by common cosine similarity between tf  X  idf weighted feature vectors [Manning et al. 2008] (in our setting, the tf value is binary, either 1 or 0, and just indicates the presence or absence of the tag in resource annotation). For coordi-nates, spatial distances between geographic points were considered as a distance mea-sure. Spatial distances were approximated by Euclidean distances in two-dimensional coordinate space; due to the relatively small size of the considered area, the three-dimensional nature of the geosphere was neglected. For evaluations, we employed simple baseline categorization methods (kNN for classification, k-means for cluster-ing) which can be used directly with all the data models discussed.
 our evaluation datasets (Section 3.2) were treated as classes. We kept 50% of resources as training data for modeling the classifier. The remainder was considered as unla-beled test data and passed to the classifier. Our quality measure is the fraction of correctly classified resources (accuracy) among all classes. In all classification experi-ments, we considered as our baseline method the kNN ( k nearest neighbor) classifier with k = 10. kNN assigns the majority class of the k nearest neighbors (in the cus-tom sense of distances for each particular method) to a test resource. The method requires no explicit training and can be easily implemented for all the data models considered. Subsequently, we computed micro-averaged results for 50 independently performed experiments (with random partitioning of resources into training/test sets). Table III summarizes results for baseline methods in comparison with GeoFolk and LDA with T = 10 latent topics (macro-averaged across all evaluated models). All methods provide substantially better accuracy than at random (we note that, for our multiclass datasets with 25 to 35 topics, random assignments would result in a calcu-lational accuracy around 0.03 to 0.04). However, it can be observed that neither simple tag representation nor spatial coordinates, taken alone, are sufficient for reliable con-tent categorization. This can be explained by extreme sparsity and diversity of tag annotations and tight proximity of target locations. LDA provides necessary smooth-ing/stabilization over the tag space and improves the classification accuracy. The joint consideration of text and spatial factors in GeoFolk helps to further increase the ro-bustness and accuracy of classification decisions. At the standard confidence level 0.95, the difference between results for GeoFolk and for baseline methods is statistically significant.

Figure 8 shows evaluation results for GeoFolk and LDA with different numbers of latent topics. As previously observed in Section 3.3, increasing the number of latent topics does not necessarily improves the model quality for LDA, due to the lower in-formation content of tag annotations. In contrast, the richer multimodal GeoFolk data allows for better results with more complex models. However, the price for this is a higher computational overhead, as previously discussed in Section 3.3.
 dataset were also used to evaluate unsupervised clustering. For categorization of re-sources, we employed the common k-means algorithm [Manning et al. 2008]. Unlike classification results, the clusters do not have explicit topic labels. Therefore, vari-ous mappings between clusters and classes (and therefore different result interpreta-tions) are possible. This makes the accuracy estimation ambiguous and interpretation dependent. For this reason, the notion of clustering accuracy was slightly adjusted. We calculate the clustering accuracy (i.e., the fraction of correctly assigned resources) for the  X  X ptimal X  cluster-class interpretation, which maximizes the overall overlap be-tween clusters and classes. Let k be the number of classes and clusters; N i the total number of clustered resources in class i ; N ij the number of resources contained in class i and having cluster label j . We define the clustering accuracy as follows: Table IV summarizes results for baseline methods in comparison with GeoFolk and LDA with T = 10 latent topics (macro-averaged across all evaluated models). Figure 9 shows evaluation results for GeoFolk and LDA with different numbers of latent topics. The observations are similar to the ones discussed for the supervised case. At the stan-dard confidence level 0.95, the difference between results for GeoFolk and for baseline methods is also statistically significant. In the tag recommendation scenario, we evaluated the ability of our methods to rec-ognize semantic relationships between tags and to capture semantically coherent tag clouds. As discussed in Section 2.4, tag probabilities in latent topics can be exploited for constructing tag-characteristic feature vectors. Subsequently, we can estimate sim-ilarity between tags. Since tag probabilities across topics do not form probability dis-tributions, we employed a cosine similarity measure for this experiment. In our exper-iments we randomly removed by one some tags from resource annotations and tested the ability of our topical models to reconstruct the missing by producing a similarity-based ranked list of suggestions. Table V shows the mean reciprocal rank for the po-sition of the desired tag in the list of recommended tags. Mean reciprocal rank (MRR) is defined as the average of the reciprocal ranks of tag positions for a series of Q tag reconstruction experiments, as follows: where rank i indicates the position of the desired tag in the ranked list of recommen-dations produced in the i th experiment (basically, lower MRR values indicate lower reconstruction quality). Since the relevance judgement for other tags in the particular list is not known, the result of this experiment indicates the lower bound of method accuracy, and thus should not be interpreted as a full-fledged evaluation of the tag rec-ommendation scenario. Nevertheless, it can be used as an indication for the ability of methods to identify latent relationships between tags. From this perspective, results from Table V (macro-averaged across all evaluated models) indicate the suitable abil-ity of GeoFolk to cope with semantically coherent tags for real tag recommendation scenarios. At the standard confidence level 0.95, the difference between results for GeoFolk and for baseline methods was statistically significant. As discussed before, we consider latent topics from the event detection model as  X  X in-gerprints X  of social events if they are clearly  X  X ocalized X  in time and space. For beta dis-tributions of timestamps, we required high cumulation of the density function around the mean (parameter settings a &gt; 15 , b &gt; 15). For coordinate distributions, we empir-ically required the compactness of the density function, as follows: at least 3/4 of the probability mass for both latitude and longitude should be concentrated within 1 km 2 of space, that is, within an interval that corresponds to spatial dimensions 1 x 1 km .In line with understanding events as social occurrences, we also required the support of each topic by relevant resources from at least 10 different users.

The topics selected so far were presented to evaluators as a summary of the top-20 most characteristic terms and top-10 most characteristic resources, and binary classified as  X  X vent X  or  X  X onevent X . In doing so, evaluators were asked to highlight topics that appropriately reflect localized social and cultural occurrences with clear temporal and spatial localization, such as festivals, concerts, exhibitions, fares, and so on. The necessary rankings for tags and resources result directly from the model produced distributions  X  z (term relevance in topics) or  X  d (topic relevance for resource characterization).

Figure 10 summarizes the results of event detection scenario (macro average for all experiments). We can observe the ability of the Bayesian model to produce character-istic patterns that are indicative for (at least certain kinds of) latent events in social media. It can also be observed that the increasing number of latent topics allows for more  X  X ine-grained X  analysis of data and results in higher accuracy of event detection.
From a practical perspective, the lessons from the event detection evaluation are twofold. On one hand, experiments have demonstrated reasonably high accuracy of event detection in the lab setting. However, empirical filtering requirements (in par-ticular, support for successful topic candidates by resources from multiple, different users) led to a rather small recall. In the experimental setting presented, only 89 event candidates have been identified. Relaxation of prefiltering conditions leads to a higher recall, at the same time the accuracy of event recognition decreases. For instance, reducing the min number of contributing users per event candidate to 5 improves recall (265 event candidates), at the same time the accuracy of event de-tection decreases to 0.62. This behavior originates in insufficient support of proba-bilistic event detection by input data: while some  X  X eal X  events are reflected in only few geo-annotated resources, active users (e.g., tourists) with modern endpoint devices may induce  X  X ursts X  of resources with spatial metadata that cannot be distinguished from event-like occurrences by means of the techniques presented. In line with recent trends on the markets of mobile communication and consumer electronics, we expect that geospatial metadata will become an integral part of mass social content in near future. Accordingly, the influence of particular users in mass intelligence scenarios will decrease, and the robustness/viability of the presented approach will be substantially improved.
 Shared content in Web 2.0 folksonomies is quite different from common collections of text documents or Web pages. In general, the low number of tag assignments to resources makes the available data in folksonomies very sparse. The enrichment of models by simultaneously considering multiple content aspects (tags, spatial knowl-edge, timestamps, low-level features of shared content, etc.) helps to identify semanti-cally coherent clouds of features. Subsequently, better content understanding helps to improve the quality of various applications (e.g., tag-based search, automatic catego-rization of resources, or tag recommendation). The approach presented is built on top of the work in Sizov [2010], with several updates and extensions: a novel scenario and model for event detection, systematic evaluation methodology, more comprehensive experiments on evaluation, and a broader discussion of related work.

From the conceptual perspective, the model presented in this article can be seen as a generalization of the work on place and event semantics of tags presented in Rattenbury and Naaman [2009] and Rattenbury et al. [2007], whereby  X  X artial com-putatio X  and  X  X ignificance tests X  (in the original terms of the approach) is performed, not for particular tags, but for distribution-based models over tags; this allows us to recognize event and place models rather than particular terms with event and place semantics.

Recent contributions [Cattuto et al. 2008; Jaschke et al. 2007; Sigurbj  X  ornsson and van Zwol 2008] present different methods for finding semantic relationships between tags. The target application is usually tag recommendation [Jaschke et al. 2007; Sigurbj  X  ornsson and van Zwol 2008] or semantic analysis of tags and tag clouds [Cattuto et al. 2008]. Extraction of structured vocabularies from folksonomies is discussed in Mika [2007] and Schmitz [2006]. Tag recommendation using external data sources (e.g., resource content, anchor text, Wikipedia articles) is discussed in Heymann et al. [2008] and Wee and Hassan [2008]. In contrast to these approaches, GeoFolk aims to explain the semantic relatedness of tags by means of latent topics in a probabilistic Bayesian framework. In our model, tag similarity/relatedness is esti-mated in a natural manner, by comparing tag distributions over latent topics.
The work on explicit semantic models (ESA) [Gabrilovich and Markovitch 2009] can be seen as an alternative to latent semantic models discussed in this article. ESA ex-ploits the semantics of structured large-scale document collections (such as Wikipedia) and constructs the feature space over collection documents, treated as  X  X xplicit X  topics. Our recent theoretical investigations discovered interesting relations between ESA [Gabrilovich and Markovitch 2009], the generalized vector space model [Wong et al. 1985], and LDA [Blei et al. 2003]. Our analysis shows conceptual equivalence between ESA and GVSM, independent of the choice of term weights for the reference docu-ments [Gottron and Sizov 2011]. In this sense, it is possible to show that ESA es-sentially captures term correlation information from the reference corpus (in contrast to the concept hypothesis in prior work). This correlation information is exploited to better match different terms in the documents being compare. In corpus-related re-trieval models (like ESA, GVSM, or pairwise tag co-occurrence approaches like that of Sigurbj  X  ornsson and van Zwol [2008]), correlations are captured directly, through doc-uments that contain correlated terms (or resources that are coannotated by correlated tags). In probabilistic model-based approaches (like LDA), analogous correlations are captured through inference mechanisms during estimation of model parameters (cor-related terms will belong to same latent topic with comparably high probabilities). In-sofar, corpus-related models and probabilistic approaches have different foundations of similarity (empirical vs. statistical), but exploit corpus properties in a similar way. In this light, it is not surprising that corpus-related methods show similar performance at the baseline (e.g., as reported in Sigurbj  X  ornsson and van Zwol [2008]). However, in contrast to GeoFolk, these methods lack flexibility in incorporating additional content aspects/properties (such as geospatial metadata) in a natural, flexible manner.
Another group of methods for tag-relatedness estimation employs graph analysis methods for representing relations between users, tags, and resources (e.g., FolkRank [Jaschke et al. 2007] as an example of two-dimensional approach, or tensor factoriza-tion [Symeonidis et al. 2008] as a multidimensional counterpart). Our preliminary experiments have shown that both approaches mentioned in the original form have similar weak points in the tag recommendation scenario: if the same active user (or a reasonably large group of users) produces in parallel content that belongs to differ-ent topics of interest (say cars and birds), the resulting model tends to mix up these themes together (i.e., car-related tags may be recommended for photos with birds, and vice versa). This is not the case for GeoFolk, since user-tag and user-content relations are not directly considered as part of the learning task.

In the field of latent semantic analysis [Abbasi and Staab 2008], can be seen as a close competitor for the tag assignment model of GeoFolk. This article reported on the use of SVD-based latent semantic indexing (LSI) [Deerwester et al. 1990] for dimen-sionality reduction in tag-based search (with observed scalability problems). In con-trast, our approach exploits the notion of probabilistic Bayesian models, which show better scalability and can be easily customized for multimodal data analysis (as done in GeoFolk for text annotations together with spatial knowledge).

In the area of Bayesian learning, several recent models (at least partly related to and relevant for our Web 2.0 setting) have associated the generation of additional modalities with topics. For example, the Group-Topic (GT) model [Wang et al. 2005] sets conditions on topics for both word generation and relational links. Analogously, the topics over time (TOT) model [Wang and McCallum 2006] aims to exploit relation-ships between text and document timestamps. Similarly to GeoFolk, the results of GT and TOT evaluations show that jointly modeling an additional modality improves the relevance of the topics. In this article, we raised the conceptual question of possible benefits and opportuni-ties resulting from added geospatial knowledge in social Web applications. Can ex-isting search, browsing, recommendation, and mining scenarios really benefit from emergent trends in the fields of mobile endpoint devices (GPS-enabled smartphones, cameras) and social platforms (geocaching, track sharing, geotagging) that result in rapidly growing amounts of geotagged online media? Our answer is yes. In this con-nection, we presented systematic studies that demonstrate the benefits of combina-tional approaches for textual and geospatial features for a broad range of character-istic applications related to social media: recommender systems, automatic content organization and filtering, and mining of complex patterns (in particular, the popular scenario of event detection).

To answer the question how a combination of textual and geospatial features can be implemented, we established GeoFolk, a Bayesian model that can be seen as a reference (baseline) framework for future research in the field of geotagged social media. The relative simplicity of GeoFolk allows for several extensions in order to better address the multimodal nature of social media in Web 2.0 applications in the future. Possible extensions are the integration of temporal knowledge (as discussed in the event detection scenario), low-level features of shared content, or authorship information. On the other hand, the GeoFolk framework can be easily integrated with models for Web 2.0 social relations (e.g., for Flickr groups). As a result, we may obtain novel, personalized, and thematically focused solutions for content management (recommendation, categorization, filtering) with spatial and social awareness. From the practical point of view, the GeoFolk model can be used optimally in quality-oriented applications, as it helps to achieve higher quality results at the cost of additional computational overhead for model fitting (parameter estimation through comprehensive sampling).

In summary, the contribution of this article can be seen as a first step towards next-generation models and methods for understanding social media with seamlessly integrated support of geospatial knowledge.

