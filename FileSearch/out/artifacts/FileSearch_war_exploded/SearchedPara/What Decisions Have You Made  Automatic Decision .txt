 Making decisions is an important aspect of conver-sations in collaborative work. In the context of meet-ings, the proposed argumentative models, e.g., in Pallotta et al. (2005) and Rienks et al. (2005), have specified decisions as an essential outcome of meet-ings. Whittaker et al. (2005) have also described how reviewing decisions is critical to the re-use of meeting recordings. For example, a new engineer who just get assigned to a project will need to know what major decisions have been made in previous meetings. Unless all decisions are recorded in meet-ing minutes or annotated in the speech recordings, it is difficult to locate the decision points by the brows-ing and playback utilities alone.

Banerjee and Rudnicky (2005) have shown that it is easier for users to retrieve the information they seek if the meeting record includes information about topic segmentation, speaker role, and meet-ing state (e.g., discussion, presentation, briefing). To assist users in identifying or revisiting decisions in meeting archives, our goal is to automatically iden-tify the dialogue acts and segments where decisions are made. Because reviewing decisions is indis-pensable in collaborative work, automatic decision detection is expected to lend support to computer-assisted meeting tracking and understanding (e.g., assisting in the fulfilment of the decisions made in the meetings) and the development of group infor-mation management applications (e.g., constructing group memory). Spontaneous face-to-face dialogues in meetings vi-olate many assumptions made by techniques pre-viously developed for broadcast news (e.g., TDT and TRECVID), telephone conversations (e.g., Switchboard), and human-computer dialogues (e.g., DARPA Communicator). In order to develop techniques for understanding multiparty dialogues, smart meeting rooms have been built at several insti-tutes to record large corpora of meetings in natural contexts, including CMU (Waibel et al., 2001), LDC (Cieri et al., 2002), NIST (Garofolo et al., 2004), ICSI (Janin et al., 2003), and in the context of the IM2/M4 project (Marchand-Mailet, 2003). More recently, scenario-based meetings, in which partic-ipants are assigned to different roles and given spe-cific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005).
The availability of meeting corpora has enabled researchers to begin to develop descriptive models of meeting discussions. Some researchers are mod-elling the dynamics of the meeting, exploiting dia-logue models previously proposed for dialogue man-agement. For example, Niekrasz et al. (2005) use the Issue-Based Information System (IBIS) model (Kunz and Ritte, 1970) to incorporate the history of dialogue moves into the Multi-Modal Discourse (MMD) ontology. Other researchers are modelling the content of the meeting using the type of struc-tures proposed in work on argumentation. For ex-ample, Rienks et al. (2005) have developed an ar-gument diagramming scheme to visualize the rela-tions (e.g., positive, negative, uncertain) between ut-terances (e.g., statement, open issue), and Marchand et al. (2003) propose a schema to model different ar-gumentation acts (e.g., accept, request, reject) and their organization and synchronization. Decisions are often seen as a by-product of these models.
Automatically extracting these argument mod-els is a challenging task. However, researchers have begun to make progress towards this goal. For example, Gatica et al. (2005) and Wrede and Shriberg (2003) automatically identify the level of emotion in meeting spurts (e.g., group level of in-terest, hot spots). Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical fea-tures with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al., 2003) and struc-tural information (e.g., the previous and following speaker) (Galley et al., 2004). More recently, Purver et al. (2006) have tackled the problem of detecting one type of decision, namely action items, which embody the transfer of group responsibility. How-ever, no prior work has addressed the problem of au-tomatically identifying decision-making units more generally in multiparty meetings. Moreover, no pre-vious research has provided a quantitative account of the effects of different feature types on the task of automatic decision detection. Our aim is to develop models for automatically de-tecting segments of conversation that contain deci-sions directly from the audio recordings and tran-scripts of the meetings, and to identify the feature combinations that are most effective for this task.
Meetings can be viewed at different levels of granularity. In this study, we first consider how to detect the dialogue acts that contain decision-related information (DM DAs). Since it is often difficult to interpret a decision without knowing the current topic of discussion, we are also interested in detect-ing decision-making segments at a coarser level of granularity: topic segments. The task of automatic decision detection can therefore be divided into two subtasks: detecting DM DAs and detecting decision-making topic segments (DM Segments).

In this study we propose to first empirically identify the features that are most characteristic of decision-making dialogue acts and then computa-tionally integrate the characteristic features to locate the DM DAs in meeting archives. For the latter task, previous research on automatic meeting understand-ing and tracking has commonly utilized a classifica-tion framework, in which variants of generative and conditional models are computed directly from data. In this study, we use a Maximum Entropy (MaxEnt) classifier to combine the decision characteristic fea-tures to predict DM DAs and DM Segments. 4.1 Decision Annotation In this study, we use a set of 50 scenario-driven meetings (approximately 37,400 dialogue acts) that have been segmented into dialogue acts and anno-tated with decision information in the AMI meet-ing corpus. These meetings are driven by a sce-nario, wherein four participants play the role of Project Manager, Marketing Expert, Industrial De-signer, and User Interface Designer in a design team in a series of four meetings. Each series of meet-ing recordings uses four distinctive speakers differ-ent from other series. The corpus includes manual transcripts for all meetings. It also comes with in-dividual sound files recorded by close-talking head-mounted microphones and cross-talking sound files recorded by desktop microphones. 4.1.1 Decision-Making Dialogue Acts
In fact, it is difficult to determine whether a di-alogue act contains information relevant to any de-cision point without knowing what decisions have been made in the meeting. Therefore, in this study DM DAs are annotated in a two-phase process: First, annotators are asked to browse through the meeting record and write an abstractive summary directed to the project manager about the decisions that have been made in the meeting. Next, another group of three annotators are asked to produce ex-tractive summaries by selecting a subset (around 10%) of dialogue acts which form a summary of this meeting for the absent manager to understand what has transpired in the meeting.

Finally, this group of annotators are asked to go through the extractive dialogue acts one by one and judge whether they support any of the sentences in the decision section of the abstractive summary; if a dialogue act is related to any sentence in the decision section, a  X  X ecision link X  from the dialogue act to the decision sentence is added. For those extracted dialogue acts that do not have any closely related sentence, the annotators are not obligated to specify a link. We then label the dialogue acts that have one or more decision links as DM DAs.

In the 50 meetings we used for the experiments, the annotators have on average found four decisions per meeting and specified around two decision links to each sentence in the decision summary section. Overall, 554 out of 37,400 dialogue acts have been annotated as DM DAs, accounting for 1.4% of all di-alogue acts in the data set and 12.7% of the orginal extractive summary (which is consisted of the ex-tracted dialogue acts). An earlier analysis has es-tablished the intercoder reliability of the two-phase process at the level of kappa ranging from 0.5 to 0.8. In this round of experiment, for each meeting in the 50-meeting dataset we randomly choose the DM DA annotation of one annotator as the sourec of its ground truth data. 4.1.2 Decision-Making Topic Segments
Topic segmentation has also been annotated for the AMI meeting corpus. Annotators had the free-dom to mark a topic as subordinated (down to two levels) wherever appropriate. As the AMI meetings are scenario-driven, annotators are expected to find that most topics recur. Therefore, they are given a standard set of topic descriptions that can be used as labels for each identified topic segment. Annota-tors will only add a new label if they cannot find a match in the standard set. The AMI scenario meet-ings contain around 14 topic segments per meeting. Each segment lasts on average 44 dialogue acts long and contains two DM DAs.

DM Segments are operationalized as topic seg-ments that contain one or more DM DAs. Over-all, 198 out of 623 (31.78%) topic segments in the 50-meeting dataset are DM Segments. As the meet-ings we use are driven by a predetermined agenda, we expect to find that interlocutors are more likely to reach decisions when certain topics are brought up. Analysis shows that some topics are indeed more likely to contain decisions than others. For example, 80% of the segments labelled as Costing and 58% of those labelled Budget are DM Segments, whereas only 7% of the Existing Product segments and none of the Trend-Watching segments are DM Segments. Functional segments, such as Chitchat, Opening and Closing, almost never include decisions. 4.2 Features Used To provide a qualitative account of the effect of dif-ferent feature types on the task of automatic decision detection, we have conducted empirical analysis on four major types of features: lexical, prosodic, con-textual and topical features. 4.2.1 Lexical Features
Previous research has studied lexical differences (i.e., occurrence counts of N-grams) between var-ious aspects of speech, such as topics (Hsueh and Moore, 2006), speaker gender (Boulis and Osten-dorf, 2005), and story-telling conversation (Gordon and Ganesan, 2005). As we expect that lexical dif-ferences also exist in DM conversations, we gener-ated language models from the DM Dialogue Acts in the corpus. The comparison of the language models generated from the DM dialogue Acts and the rest of the conversations shows that some differences exist between the two models: (1) decision making con-versations are more likely to contain we than I and You ; (2) in decision-making conversations there are more explicit mentions of topical words, such as ad-vanced chips and functional design ; (3) in decision-making conversations, there are fewer negative ex-pressions, such as I don X  X  think and I don X  X  know . In an exploratory study using unigrams, as well as bigrams and trigrams, we found that using bigrams and trigrams does not improve the accuracy of clas-sifying DM DAs, and therefore we include only uni-grams in the set of lexical features in the experiments reported in Section 6. 4.2.2 Prosodic Features
Functionally, prosodic features, i.e., energy, and fundamental frequency (F0), are indicative of seg-mentation and saliency. In this study, we follow Shriberg and Stolcke X  X  (2001) direct modelling ap-proach to manifest prosodic features as duration, pause, speech rate, pitch contour, and energy level. We utilize the individual sound files provided in the AMI corpus. To extract prosodic features from the sound files, we use the Snack Sound Toolkit to com-pute a list of pitch and energy values delimited by frames of 10 ms, using the normalized cross correla-tion function. Then we apply a piecewise linearisa-tion procedure to remove the outliers and average the linearised values of the units within the time frame of a word. Pitch contour of a dialogue act is ap-proximated by measuring the pitch slope at multi-ple points within the dialogue act, e.g., the first and last 100 and 200 ms. The rate of speech is calcu-lated as both the number of words spoken per sec-ond and the number of syllables per second. We use Festival X  X  speech synthesis front-end to return phonemes and syllabification information. An ex-ploratory study has shown the benefits of including immediate prosodic contexts, and thus we also in-clude prosodic features of the immediately preced-ing and following dialogue acts. Table 1 contains a list of automatically generated prosodic features used in this study. 4.2.3 Contextual Features
From our qualitative analysis, we expect that con-textual features specific to the AMI corpus, such as the speaker role (i.e., PM, ME, ID, UID) and meet-ing type (i.e., kick-off, conceptual design, functional design, detailed design) to be characteristic of the DM DAs. Analysis shows that (1) participants as-signed to the role of PM produce 42.5% of the DM DAs, and (2) participants make relatively fewer de-cisions in the kick-off meetings. Analysis has also demonstrated a difference in the type, the reflexiv-DAs and the non-DM DAs. For example, dialogue acts of type inform , suggest , elicit assessment and elicit inform are more likely to be DM DAs.

We have also found that immediately preceding and following dialogue acts are important for iden-tifying DM DAs. For example, stalls and frag-ments preceding and fragments following a DM contrast, there is a lower chance of seeing sug-gest and elicit-type DAs (i.e., elicit-inform , elicit-suggestion , elicit-assessment ) in the preceding and following DM DAs. 4.2.4 Topical Features
As reported in Section 4.1.2, we find that inter-locutors are more likely to reach decisions when cer-tain topics are brought up. Also, we expect decision-making conversations to take place towards the end of a topic segment. Therefore, in this study we in-clude the following features: the label of the current topic segment, the position of the DA in a topic seg-ment (measured in words, in seconds, and in %), the distance to the previous topic shift (both at the top-level and sub-topic level)(measured in seconds), the duration of the current topic segment (both at the top-level and sub-topic level)(measured in seconds). 5.1 Classifying DM DAs Detecting DM DAs is the first step of automatic de-cision detection. For this purpose, we trained Max-Ent models to classify each unseen sample as ei-ther DM DA (POS) or non-DM DA (NEG). We per-formed a 5-fold cross validation on the set of 50 meetings. In each fold, we trained MaxEnt mod-els from the feature combinations in the training set, wherein each of the extracted dialogue acts has been labelled as either POS or NEG. Then, the models were used to classify unseen instances in the test set as either POS or NEG. In Section 4.2, we described the four major types of features used in this study: unigrams (LX1), prosodic (PROS), contextual (CONT), and topical (TOPIC) features. For comparison, we report the naive baseline ob-tained by training the models on the prosodic fea-tures alone, since the prosodic features can be gen-erated fully automatically. The different combina-tions of features we used for training models can be divided into the following four groups: (A) us-ing prosodic features alone (BASELINE), (B) us-ing lexical, contextual and topical features alone (LX1, CONT, TOPIC); (C) using all available fea-tures except one of the four types of features (ALL-LX1, ALL-PROS, ALL-CONT, ALL-TOPIC); and (D) using all available features (ALL). 6.1 Classifying DM Segments Detecting DM segments is necessary for interpret-ing decisions, as it provides information about the current topic of discussion. Here we combine the predictions of the DM DAs to classify each unseen topic segment in the test set as either DM Segment (POS) or non-DM Segment (NEG). Recall that we defined a DM Segment as a segment that contains one or more hypothesized DM DAs. The task of de-tecting DM Segments can thus be viewed as that of detecting DM Dialogue Acts in a wider window. 6.2 EXP1: Classifying DM DAs Table 2 reports the performance on the test set. The results show that models trained with all features (ALL), including lexical, prosodic, contextual and topical features, yield substantially better perfor-mance than the baseline on the task of detecting DM DAs. We carried out a one-way ANOVA to exam-ine the effect of different feature combinations on overall accuracy (F1). The ANOVA suggests a reli-able effect of feature type ( F (9 , 286) = 3 . 44; p &lt; 0 . 001) . Rows 2-4 in Table 2 report the performance of models in Group B that are trained with a sin-gle type of feature. Lexical features are the most predictive features when used alone. We performed sign tests to determine whether there are statistical differences among these models and the baseline. We find that when used alone, only lexical features (LX1) can train a better model than the baseline ( p &lt; 0 . 001) . However, none of these models yields a comparable performance to the ALL model.

To study the relative effect of the different fea-ture types, Rows 5-8 in the table report the perfor-mance of models in Group C, which are trained with all available features except LX1, PROS, CONT and TOPIC features respectively. The amount of degra-dation in the overall accuracy (F1) of each of the models in relation to that of the ALL model indi-cates the contribution of the feature type that has been left out of the model. We performed sign tests to examine the differences among these models and the ALL model. We find that the ALL model out-performs all of these models ( p &lt; 0 . 001) except the model trained by leaving out contextual features (ALL-CONT). A closer investigation of the preci-sion and recall of the ALL-CONT model shows that the contextual features are detrimental to recall but beneficial for precision. The mixed result is due to the fact that models trained with contextual features are tailored to recognize particular types of DM di-alogue acts. Therefore, using these contextual fea-tures improves the precision for these types of DM DAs but reduces the overall recognition accuracy.
The last three columns of Table 2 are the results obtained using a lenient match measure, allowing a window of 10 seconds preceding and following a hy-pothesized DM DA for recognition. The better re-sults show that there is room for ambiguity in the assessment of the exact timing of DM DAs. 6.3 EXP2: Classifying DM Segments As expected, the results in Table 3 are better than those reported in Table 2, achieving at best 83% overall accuracy.The model that combines all fea-tures (ALL) yields significantly better results than the baseline. The ANOVA shows a reliable effect of different feature types on the task of detecting DM Segments ( F (11 , 284) = 2 . 33; p &lt; = 0 . 01) . Rows 2-4 suggest that lexical features are the most pre-dictive in terms of overall accuracy. Sign tests con-firm the advantage of using lexical features (LX1) over the baseline (PROS) ( p &lt; 0 . 05) . Interest-ingly, the model that is trained with topical features alone (TOPIC) yields substantially better precision ( p &lt; 0 . 001) . The increase from 49% precision for the task of detecting DM DAs (in Table 2) to 91% for that of detecting DM Segments stems from the fact that decisions are more likely to occur in certain types of topic segments. In turn, training models with topical features helps eliminate incorrect pre-dictions of DM DAs in these types of topic seg-ments. However, the accuracy gain of the TOPIC model on detecting certain types of DM Segments does not extend to all types of DM Segments. This is shown by the significantly lower recall of the TOPIC model over the baseline ( p &lt; 0 . 001) .

Finally, Rows 5-8 report the performance of the models in Group (C) on the task of detecting DM Segments. Sign tests again show that the model that is trained with all available features (ALL) outper-forms the models that leave out lexical, prosodic, or topical features ( p &lt; 0 . 05) . However, the ALL model does not outperform the model that leaves out contextual features. In addition, the contextual fea-tures degrade the recall but improve the precision on the task of detecting DM Segments. Calculat-ing how much the overall accuracy of the models in Group C degrades from the ALL model shows that the most predictive features are the lexical features, followed by the topical and prosodic features. As suggested by the mixed results obtained by the model that is trained without the contextual features, the two-phase decision annotation procedure (as de-scribed in Section 4.1) may have caused annota-tors to select dialogue acts that serve different func-tional roles in a decision-making process in the set of DM DAs. For example, in the dialogue shown Table 3: Effects of different combinations of features on detecting DM Segments. in Figure 1, the annotators have marked dialogue act (1), (5), (8), and (11) as the DM DAs related to this decision:  X  X here will be no feature to help find the remote when it is misplaced X  . Among the four DM DAs, (1) describes the topic of what this decision is about; (5) and (8) describe the arguments that support the decision-making process; (11) in-dicates the level of agreement or disagreement for this decision. Yet these DM DAs which play dif-ferent functional roles in the DM process may each have their own characteristic features. Training one model to recognize DM DAs of all functional roles may have degraded the performance on the classifi-cation tasks. Developing models for detecting DM DAs that play different functional roles requires a larger scale study to discover the anatomy of gen-eral decision-making discussions. This is the first study that aimed to detect segments of the conversation that contain decisions. We have (1) empirically analyzed the characteristic features of DM dialogue acts, and (2) computational devel-oped models to detect DM dialogue acts and DM topic segments, given the set of characteristic fea-tures. Empirical analysis has provided a qualitative account of the DM-characteristic features, whereas training the computational models on different fea-ture combinations has provided a quantitative ac-count of the effect of different feature types on the task of automatic decision detection. Empiri-cal analysis has exhibited demonstrable differences
Figure 1: Example decision-making discussion in the words (e.g., we ), the contextual features (e.g., meeting type , speaker role , dialogue act type ), and the topical features. The experimental results have suggested that (1) the model combining all the avail-able features performs substantially better, achiev-ing 62% and 82% overall accuracy on the task of detecting DM DAs and that of detecting DM Seg-ments, respectively, (2) lexical features are the best indicators for both the task of detecting DM DAs and that of detecting DM Segments, and (3) combining topical features is important for improving the pre-cision for the task of detecting DM Segments.
Many of the features used in this study require hu-man intervention, such as manual transcriptions, an-notated dialogue act segmentations and labels, anno-tated topic segmentations and labels, and other types of meeting-specific features. Our ultimate goal is to identify decisions using automatically induced fea-tures. Therefore, studying the performance degra-dation when using the automatically generated ver-sions of these features (e.g., ASR words) is essen-tial for developing a fully automated component on detecting decisions immediately after a meeting or even for when a meeting is still in progress. An-other problem that has been pointed out in Section 6 and in Section 7 is the different functional roles of DM dialogue acts in current annotations. Purver et al. (2006) have suggested a hierarchical annotation scheme to accommodate the different aspects of ac-tion items. The same technique may be applicable in a more general decision detection task. This work was supported by the European Union In-tegrated Project AMI (Augmented Multi-party Inter-action, FP6-506811, publication AMI-204).

