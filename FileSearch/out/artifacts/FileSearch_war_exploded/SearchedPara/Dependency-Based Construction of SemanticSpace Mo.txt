 Saarland University University of Edinburgh this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art. 1. Introduction
Vector space models of word co-occurrence have proved a useful framework for repre-senting lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Sch X tze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correc-tion (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998).
 sent word meaning simply by using distributional statistics. The central assumption here is that the context surrounding a given word provides important information about its meaning (Harris 1968). The semantic properties of words are captured in a multi-dimensional space by vectors that are constructed from large bodies of text by observing the distributional patterns of co-occurrence with their neighboring words.
Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique word, commonly referred to as  X  X arget word, X  and each column represents a given linguistic context. The semantic similarity between any two words can then be quantified directly using a distance measure such as cosine or Euclidean distance.
 (Lund and Burgess 1996; Lowe and McDonald 2000) or as entire paragraphs X  X ven documents (Salton, Wang, and Yang 1975; Landauer and Dumais 1997). Latent Semantic
Analysis (LSA; Landauer and Dumais 1997) is an example of a document-based vector space model that is commonly used in information retrieval and cognitive science. Each target word t is represented by a k element vector of paragraphs p of each vector element is a function of the number of times t occurs in p the Hyperspace Analogue to Language model (HAL; Lund and Burgess 1996) creates a word-based semantic space: each target word t is represented by a k element vector, whose dimensions correspond to context words c 1 ... k . The value of each vector element is a function of the number of times each c i occurs within a window of size n before or after t in a large corpus.
 ordered words, without even taking parts of speech into account (e.g., to drink and adrink are represented by a single vector). In fact, with the exception of function words (e.g., the , down ), which are often removed, it is often assumed that all context words within a certain distance from the target word are semantically relevant. Because no linguistic knowledge is taken into account, the construction of semantic space models is straightforward and language-independent X  X ll that is needed is a segmented corpus of written or spoken text.
 to a word X  X  meaning is clearly a simplification. There is ample evidence demonstrating course processing (West and Stanovich 1986; Neville et al. 1991; Fodor 1995; Miltsakaki 2003) and modulate cognitive behavior in sentence priming tasks (Morris 1994). Fur-thermore, much research in lexical semantics hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995).
 models with morpho-syntactic information. Extensions range from part of speech tag-ging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analy-sis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a). In these semantic space models, contexts are defined over words bearing a syntactic relationship to the target words of interest. This makes semantic spaces more flexible; different types of contexts can be selected; words do not have to co-occur within a small, fixed word window; and word order or argument structure differences can be naturally mirrored in the semantic space.
 tualizes context in terms of syntactic relations. We introduce an algorithm for construct-ing semantic space models from texts annotated with syntactic information (specifi-cally dependency relations) and illustrate how different model classes can be derived structure in general and argument structure in particular is a close reflection of lexical meaning (Levin 1993). We thus model meaning by quantifying the degree to which words are attested in similar syntactic environments. The expressive power of our framework stems from three novel parameters which guide model construction. The first parameter determines which types of syntactic structures contribute towards the 162 representation of lexical meaning. The second parameter allows us to weigh the relative importance of different syntactic relations. Finally, the third parameter determines how the semantic space is actually represented, for instance as co-occurrences of words with other words, words with parts of speech, or words with argument relations (e.g., subject, object).
 start by simulating semantic priming, a phenomenon that has received much atten-tion in computational psycholinguistics and is typically modeled using word-based semantic spaces (Landauer and Dumais 1997; McDonald and Brew 2004). We next consider the problem of recognizing synonyms by selecting an appropriate synonym for a target word from a set of semantically related candidate words. Specifically, we evaluate the performance of our model on synonym questions from the Test of English as a Foreign Language (TOEFL). These are routinely used as a testbed for assessing how well vector-based models capture lexical knowledge (Landauer and Dumais 1997;
Turney 2001; Sahlgren 2006). Our final experiment concentrates on unsupervised word sense disambiguation (WSD), thereby exploring the potential of the proposed frame-work for NLP applications requiring large scale semantic processing. We automatically infer predominant senses in untagged text by incorporating our syntax-based semantic spaces into the modeling paradigm proposed by McCarthy et al. (2004). In all cases, we show that our framework consistently outperforms word-based models yielding results that are comparable or superior to state of the art.
 corporates syntactic information in the form of dependency relations and generalizes previous syntax-based vector-based models; an application of this framework to a wide range of tasks relevant to cognitive modeling and NLP; and an empirical comparison of our dependency-based models against state-of-the-art word-based models.
 models. In Section 3, we present our modeling framework and relate it to previous work. priming experiment, Section 6 presents our study on the TOEFL synonymy task, and
Section 7 describes our sense-ranking experiment. Discussion of our results and future work concludes the article (Section 8). 2. Overview of Semantic Space Models 2.1 Word-Based and Syntax-Based Models
To facilitate comparisons with our framework, we begin with a brief overview of exist-ing semantic space models. We describe traditional word-based co-occurrence models as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a).
 b 1 ... D of basis elements , the dimensions of the space. B can be a set of words (Lund and
Burgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows 2003) or words with a syntactic relation such as subject or object (Lin 1998a). Usually, the dimensionality of the matrix is restricted to a relatively small number. A popular choice is the k most frequent words (minus the stop words) in a corpus, typically 100 X  2,000 (McDonald 2000; Levy and Bullinaria 2001). A is a lexical association function applied to the co-occurrence frequency of target word t with basis element b so that each word is represented by a vector v = A ( f ( t , b 1 )), A ( f ( t , b identity function, the raw frequencies are used. Functions such as mutual information or the log-likelihood ratio are often applied to factor out co-occurrences due to chance.
S is a similarity measure that maps pairs of vectors onto a continuous-valued scale of contextual similarity. V is an optional transformation that reduces the dimensionality of the semantic space. Singular value decomposition (SVD; Berry, Dumais, and O X  X rien 1994; Golub and Loan 1989) is commonly used for this purpose. SVD can be thought of as a means of inferring latent structure in distributional data, while making sparse matrices more informative. For the rest of this article, we will ignore V and other statistical transformations and concentrate primarily on ways of inducing structure from grammatical and syntactic information.
 might carry sweet apples . For a word-based space, we might use the basis elements the association function A . Each target word t i  X  T will then be represented by a five-dimensional row vector, and the value of each vector element will record the number of times each basis element b i  X  B occurs within a window of two words to the left and two words to the right of the target word t i . The co-occurrence matrix that we obtain according to these specifications is shown in Figure 1. A variety of distance measures can be used to compute the similarity S between two target words (see Lee [1999] for an overview), the cosine being the most popular: co-occurrence by capturing syntactic relationships between words such as subject X  X erb or modifier X  X oun, irrespectively of whether they are physically adjacent or not. The basis elements are generally assumed to be tuples ( r , w ) where w is a word occurring in relation type r with a target word t . The relations typically reflect argument structure (e.g., subject, object, indirect object) or modification (e.g., adjective X  X oun, noun X  X oun) and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999;
Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran (Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a).
 164 using the basis elements ( subj,lorry ), ( aux,might ), ( mod,sweet ), and ( obj,apples ). The binary association function A records whether the target word possesses the feature (denoted by x in Figure 2) or not. Because the cells of the matrix do not contain numerical values, a similarity measure that is appropriate for categorical values must be chosen. Grefenstette (1994) uses a weighted version of Jaccard X  X  coefficient, a measure of association commonly employed in information retrieval (Salton and McGill 1983).
Assuming Attr ( t ) is the set of basis elements co-occurring with t , Jaccard X  X  coefficient is defined as:
Lin (1998a) constructs a semantic space similar to Grefenstette (1994) except that the matrix cells represent the number of times a target word t co-occurs with basis element ( r , w ), as shown in Figure 3. He proposes an information theoretic similarity measure based on the distribution of target words and basis elements: where I ( t , r , w ) is the mutual information between t and r , w and T ( t )isthesetofbasis elements ( r , w ) such that I ( t , r , w ) is positive and 2.2 Discussion
Because syntax-based models capture more linguistic structure than word-based mod-els, they should at least in theory provide more informative representations of word meaning. Unfortunately, comparisons between the two types of models have been few and far between in the literature. Furthermore, the potential of syntax-based models has not been fully realized since most previous approaches limit themselves to a specific model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002).
This section discusses these issues in more detail and sketches how we plan to address them.

Modeling of syntactic context. All existing syntax-based semantic space models we are aware of incorporate syntactic information in a rather limited fashion. For example, different relations. Even in cases where many relations are used (Lin 1998a; Lin and
Pantel 2001), only direct relations are taken into account, ignoring potentially important co-occurrence patterns between, for instance, the subject and the object of a verb, or between a verb and its non-local argument (e.g., in control structures).
 Comparison between model classes. Syntax-based vector space models have been used in
NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefen-stette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and
Carroll 2003). Comparisons between word-based and syntax-based models on the same task are rare, and the effect of syntactic knowledge has not been rigorously investigated or quantified. The few studies on this topic reveal an inconclusive picture. On the one hand, Grefenstette compared the performance of the two classes of models on the task of automatic thesaurus extraction and found that a syntactically enhanced model gave significantly better results over a simple word co-occurrence model. A replication of
Grefenstette X  X  study with a more sophisticated parser (Curran and Moens 2002) re-vealed that additional syntactic information yields further improvements. On the other hand, attempts to generate more meaningful indexing terms for information retrieval (IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson et al. 2002) have been largely unsuccessful. Experimental results show minimal differences in retrieval effectiveness at a substantially greater processing cost (see Voorhees [1999] for details).

Impact on cognitive modeling. Despite their widespread use in NLP, syntax-based seman-cholinguistics. Wiemer-Hastings and Zipitria (2001) construct a semantic space similar to LSA, but enhanced with part-of-speech tags with the aim of modeling human raters in an intelligent tutoring context. Their results, however, show that the tagged LSA space yields worse performance than a word-based model. Kanejiya, Kumar, and Prasad (2003) attempt to capture syntactic context in a shallow manner by enhancing target words with the parts-of-speech of their immediately preceding words. They argue that this representation can provide useful information for the upcoming target words, as is often the case in language modeling and left-to-right parsing. They employ a document-based semantic space, which they submit to SVD and subsequently compare against an
LSA model that contains no syntactic information, again in the context of an intelligent 166 tutoring system. Their results indicate that the syntactically enhanced model has better coverage than the LSA model (i.e., it is able to evaluate more student answers), although it displays a lower correlation with human raters than raw LSA.
 models in more depth. We provide a general definition for these models, which incor-porates a wider range of syntactic relations than previously considered and subsumes existing syntax-based and word-based models. In order to demonstrate the scope of our framework, we evaluate our models on tasks popular in both cognitive science and based models and show that the additional processing cost incurred by syntax-based models is worthwhile. 3. A General Framework for Semantic Space Models
Once we move away from words as the basic context unit, the issue of representa-tion of syntactic information becomes pertinent. An ideal syntactic formalism should abstract over surface word order, mirror semantic relationships as closely as possible, and incorporate word-based information in addition to syntactic analysis. It should be also applicable to different languages. These requirements point towards dependency grammar , which can be considered as an intermediate layer between surface syntax and semantics. More formally, dependency relations are asymmetric binary relationships between a head and a modifier (Tesni X re 1959). The structure of a sentence is analyzed as a directed graph whose nodes correspond to words. The graph X  X  edges correspond to dependency relationships and each edge is labeled with a specific relationship type (e.g., subject, object).
 in Figure 4. On the left, the sentence is represented as a graph. The sentence head is the main verb carry which is modified by its subject lorry , its object apples and the auxiliary might . The subject and object are modified respectively by a determiner ( a ) and an ad-jective ( sweet ). On the right side of Figure 4, an adjacency matrix notation is used. Edges in the graph are represented as triples of a dependent word (e.g., lorry ), a dependency the part of speech of the modifier (capitalized, e.g., N ) , the dependency relation itself (in lower case, e.g., subj ), and the part of speech of the head (also capitalized, e.g., V ). context over which the semantic space will be constructed. We base our discussion and experiments on the broad-coverage dependency parser MINIPAR 1998a, 2001). However, there is nothing inherent in our formalization that restricts us to this particular parser. Any other parser with broadly similar dependency output (e.g., Briscoe and Carroll 2002) could serve our purposes.
 algorithm for the construction of semantic spaces. Then, we proceed to discuss each con-struction step (context selection, basis mapping, and quantification of co-occurrences) in more detail. Finally, we show how our framework subsumes existing models. Table 1 lists the notation we use in the rest of the article. 3.1 The Construction Algorithm
Our algorithm for creating semantic space models is summarized in Figure 5. Central to the construction process is the notion of paths , namely sequences of dependency edges extracted from the dependency parse of a sentence (we define paths formally in
Section 3.2). Consider again the graph in Figure 4. Besides individual edges (i.e., paths of length 1), it contains several longer paths, such as the path between lorry and between lorry and carry ( lorry , carry ), and so forth. The usage of paths allows us to represent direct and indirect relationships between words and gives rise to three novel parameters: 1. The context selection function cont ( t ) determines which paths in the 2. The path value function v assigns weights to paths, thus allowing  X   X   X  Dependency path (in a given dependency tree)  X  Set of all undirected paths
 X 
 X  168 3. The basis mapping function  X  creates the dimensions of the semantic
As discussed in Section 2, the main difference among variants of semantic space models lies in the specification of basis elements B . By treating the dependency paths as distinct from the basis elements, we obtain a general framework for vector-based models that can be parametrized for different tasks and allows for the construction of spaces with flexibility, in conjunction with the context selection and path value functions, allows our model to subsume both traditional word-based and syntax-based models (see
Section 3.6 for more details). 3.2 Step 1: Building the Context
The first step in constructing a semantic space from a large collection of dependency
We define contexts as anchored paths, that is, paths in a dependency graph that start t is a superset of the paths that can contribute relevant distributional information about t .
Definition 1. The dependency parse p of a sentence s is a directed graph p where E s  X  V s  X  V s . The nodes v  X  V s are labeled with individual words w simplicity, we use nodes and their labels interchangeably, and the set of nodes corresponds to the words of the sentence: V s = { w alabel l : E s  X  Cat  X  R  X  Cat where Cat belongs to a set of POS tags and R toasetof dependency relations. We assume that this set is finite and parser-specific. edge labels in square brackets. [Det,det,N] and [N,subj,V] are examples for labels provided by MINIPAR (see Figure 4, right-hand side).

We are now ready to define paths in our dependency graph, save one important issue: Should we confine ourselves to directed paths or perhaps disregard the direction of the edges? In a dependency graph, directed paths can only capture the relationship between a head and its (potentially transitive) dependents (e.g., carry and sweet in
Figure 4). This excludes informative contexts representing, for instance, the relationship between the subject and the object of a predicate (e.g., lorry and apples in Figure 4). Our following, we assume undirected paths.

Definition 2. An (undirected) path  X  is an ordered tuple of nodes v some sentence s that meets the following two constraints: In the rest of the article, we use the term path as a shorthand for undirected path.
Definition 3. Apath  X  is anchored at a word t iff start (  X  ) = t . We write  X  set of all paths anchored at t in sentence s .

As an example, the set of paths anchored at lorry in Figure 4 is: Definition 4. The context selection function cont : W  X  2 of the paths anchored at t . We call this subset the syntactic context of t .
The context selection function allows direct control over the type of linguistic informa-tion represented in the semantic space. In traditional vector-based models, the context 170 selection function does not take any syntactic information into account: All paths  X  are selected for which the absolute difference ( abs ) between the positions ( pos ) of the anchor start (  X  ) and the end word end (  X  ) does not exceed the window size k :
The dependency-based models proposed by Grefenstette (1994) and Lin (1998a) con-sider minimal syntactic contexts in the form of individual dependency relations, namely, dependency paths of length 1:
The context selection function as defined herein permits the elimination of paths from the semantic space on the basis of linguistic or other information. For example, it can be argued that subjects and objects convey more semantic information than determiners or auxiliaries. We can thus limit our context to the set of all anchored paths consisting exclusively of subject or object dependencies:
When this context specification function is applied to the dependency graph in Figure 4, quences (such as [N,det,Det] for lorry , a ) are disallowed by (7). 3.3 Step 2: Basis Mapping
The second step in the construction of our semantic space model is to specify its dimensions, the basis elements, following Lowe X  X  (2001) terminology.
 Definition 5. The basis mapping function  X  :  X   X  B maps paths onto basis elements.
By dissociating dependency paths and basis elements in this way, we decouple the observed syntactic context from its representation in the final semantic space. The basis mapping allows us to exploit underlying relationships among different paths:
Two paths which are (in some sense) equivalent can be mapped onto the same basis element. The function effectively introduces a partitioning of paths into equivalence classes  X  X abeled X  by basis elements, thus offering more flexibility in defining the basis elements themselves.
 paths ending at word w are mapped onto the basis element w , resulting in a semantic space with context words as basis elements (recall that all paths in the local context start at the target word):
A word-based mapping is also possible when paths are defined over dependency graphs. As an example consider the paths anchored at lorry in Figure 4. Using (8), these paths are mapped to the following basis elements:
A different mapping is used in Grefenstette (1994) and Lin (1998a), who consider only paths of length 1. In their case, paths are mapped onto pairs representing a dependency relation r and the end word w (see the discussion in Section 2):
However, in this article we restrict ourselves to models which use a word-based ba-sis mapping. The resulting spaces are similar to traditional word-based spaces X  X oth use sets of context words X  X hich allows for direct comparisons between our models and word-based alternatives. Crucially, our models differ from traditional models in the more general treatment of (syntactic) context: Only paths in the syntactic context, and not surface co-occurrences, contribute towards counts in the matrix. The context selection function supports inference over classes of basis elements (which in previous models would have been considered distinct) as well as fine-grained control over the types of relationships that enter into the space construction. 3.4 Step 3: Quantifying Syntactic Co-occurrence
The last step in the construction of the dependency-based semantic models is to specify the relative importance (i.e., value) of different paths:
Definition 6. The path value function v assigns a real number to a path: v :  X 
Traditional models do not exploit this possibility, thus giving equal weight to all paths:
The path value function provides additional flexibility for incorporating linguistic in-formation into our framework. Even if two paths are mapped onto the same basis element (by the basis mapping), the path value function can weigh their respective contributions differently. For instance, it could discount longer paths that express indirect relationships between words. An example of such a length-based path value 172 function is given in Equation (11). It assigns a value of 1 to paths of length 1 and fractions to longer paths:
A more linguistically informed path value function can be defined by taking into account the obliqueness hierarchy of grammatical relations (Keenan and Comrie 1977). more salient than obliques (e.g., prepositional phrases). And obliques are more salient than genitives. We thus define a linear relation-based weighting scheme that ranks paths according to their most salient grammatical function, without considering their length: tic context of a token t . We can next define the local co-occurrence frequency between t and a basis element b as the sum of the path values v (  X  ) for all paths  X  mapped onto b . Because our semantic space construction algorithm operates over word types , we sum the local co-occurrence frequencies for all instances of a target word type t (written as W ( t )) to obtain its global co-occurrence frequency . The latter is a measure of the co-occurrence of t and b over the entire corpus.

Definition 7. The global co-occurrence frequency of a basis element b and a target t is computed by the function f : B  X  T  X  R defined by The global co-occurrence frequency f ( b , t ) could be used directly as the matrix value M [ b ][ t ]. However, as Lowe (2001) notes, raw counts are likely to give misleading results.
This is due to the non-uniform distribution of words in corpora which will introduce a frequency bias so that words with similar frequency will be judged more similar than they actually are. It is therefore advisable to use a lexical association function A to factor out chance co-occurrences explicitly.

Manning and Sch X tze [1999] for an overview). In our experiments, we follow Lowe and McDonald (2000) in using the well-known log-likelihood ratio G
We can visualize the computation using a two-by-two contingency table whose four cells correspond to four events (Kilgarriff 2001):
The top left cell records the frequency k with which t and b co-occur (i.e., k corresponds to raw frequency counts). The top right cell l records how many times b is attested with any word other than t , the bottom left cell m represents the frequency of any word other than b with t , and the bottom right cell n records the frequency of pairs involving neither b nor t . The function G 2 : R 4  X  R is defined as
A naive implementation of the log-likelihood ratio would keep track of all four events path and would render the construction of the space prohibitively expensive. This can be avoided by computing only k = f ( t , b ), the global co-occurrence frequency, and using the marginal frequencies of paths and targets to estimate l , m and n as follows: l =
For example, l can be computed as the total value of all paths in the corpus that are mapped onto b minus the value of those paths that are anchored at t . 3.5 Definition of Semantic Space Our framework of semantic space models can now be formally specified by extending Lowe X  X  (2001) definition from Section 2: basis elements, T the set of target words, and M is the matrix M = B
M [ t j ][ b i ] A :
R 4  X  R is the lexical association function. Our additional parameters are the content selection function cont : T  X  2  X  s , the basis mapping function  X  :  X  the path value function v :  X   X  R .

Note that the set of target words T can contain either word types or word tokens .In the preceding definitions, we have assumed that co-occurrence counts are constructed over word types, however the framework can be also used to represent word tokens. In this case, each set of target tokens contains exactly one word ( W ( t ) = summation step in Definition 7 trivially does not apply. We work with type-based spaces in the rest of this article. The use of tokens may be appropriate for other applications such as word sense discrimination (Sch X tze 1998).
 again the sentence A lorry might carry sweet apples . According to Definition 8, in order to construct vectors for the target words T = { lorry , might , carry , sweet , fruit 174 must provide a context selection function, a basis mapping function, and a path value function. The space resulting from a context selection function which considers exclu-sively subject and object dependencies (see Equation (7)), a word-based basis mapping function (see Equation (8)), and a length-based path value function (see Equation (11)), is shown in Figure 6. 3.6 Discussion
We have proposed a general framework for semantic space models which operates on dependency relations and allows linguistic knowledge to inform the construction of the semantic space. The framework is highly flexible: Depending on the context selection and basis mapping functions, semantic spaces can be constructed over words, words and parts of speech, syntactic relations, or combinations of words and syntactic relations. This flexibility unavoidably increases the parameter space of our models, because there is a potentially large number of context selection or path value functions for which semantic spaces can be constructed.
 our framework, and facilitates comparisons across different kinds of spaces (compare due to the choice of a more selective context specification function (see Equations (5) and (7)). However, this is expected because our main motivation is to distinguish between informative and uninformative syntactico-semantic relations. Using a minimal context selection function results in a space that contains indisputably valid semantic relations, excluding potentially noisy relations like the one between might and sweet .By adding richer linguistic information to the context selection function, the space can be expanded in a principled manner. In comparison with previous syntax-based models, which only use direct dependency relations (see Equation (6)), our dependency-based space additionally represents indirect semantic relations (e.g., between lorry and apples ). and path value functions into one parameter, for example, by defining context selection directly as a function from (anchored) paths to their path values, and thus assigning a value of zero to all paths  X   X  cont ( t ). However, we refrained from doing this for two reasons, a methodological one and a technical one. On the methodological side, we believe that it makes sense to keep the two concepts of context selection and context weighting distinct. The separation allows us to experiment with different path value functions while keeping the set of paths resulting from context selection constant.
On the technical side, the two functions are easier to specify declaratively when kept separately. Also, a separate context selection function can be used to efficiently isolate relevant context paths without having to compute the values for all anchored paths. anchored, cycle-free, and connected. These three preconditions on paths are meant to reflect linguistic properties of reasonable syntactic contexts while at the same time they guarantee the efficient construction of the semantic space. Anchoredness ensures that all paths are semantically connected to the target; this also means that the search space can be limited to paths starting at the target word. Cycle-freeness and connectedness exclude linguistically meaningless paths such as paths of infinite length (cycles) or paths consisting of several unconnected fragments. These properties guarantee that context paths can be created incrementally, and that construction terminates. 3.7 Runtime and Implementation Our implementation uses path templates to encode the context selection function (see
Appendix A for more details). The runtime of the semantic space construction algorithm presented in Section 3 is O ( max g  X | cont | X  t ), where max in the grammar, | cont | the number of path templates used for context selection, and t the number of target tokens in the corpus. This assumes that  X  (  X  )and v (  X  ) can be computed in constant time, which is warranted in practice because most linguistically interesting paths will be of limited length (in our study, all paths have a length of at most 4). The linear runtime in the size of the corpus provides a theoretical guarantee that the method is applicable to large corpora such as the British National Corpus (BNC).
 der the GNU General Public License from http://www.coli.uni-saarland.de/ dv/dv.html . The system can create dependency spaces from the output of MINIPAR (Lin 1998b, 2001). We also provide an interface for integrating other parsers. The distribution includes a set of prespecified parameter settings, namely the word-based basis mapping function, and the path value and context selection functions used in our experiments. 4. Experimental Setup
In this section, we describe the corpus and parser chosen for our experiments. We also discuss our parameter and model choice procedure, and introduce the baseline word-based model which we use for comparison with our approach. Our experiments are then presented in Sections 5 X 7. 4.1 Corpus and Parser
All our experiments were conducted on the British National Corpus (BNC), a 100-million word collection of samples of written and spoken English (Burnard 1995). The corpus represents a wide range of British English, including samples from newspapers, magazines, books (both academic and fiction), letters, essays, as well as spontaneous conversations, business or government meetings, radio shows, and phone-ins. The BNC has been used extensively in building vector space models for many tasks relevant for cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald and Brew 2004) and NLP (McCarthy et al. 2004; Weeds 2003; Widdows 2003).
 version 0.5 (Lin 1998b, 2001), a wide-coverage dependency parser.
 manually constructed grammar and a lexicon derived from WordNet with the addition of proper names (130,000 entries in total). Lexicon entries contain part-of-speech and 176 subcategorization information. The grammar is represented as a network of 35 nodes
MINIPAR uses a distributed chart parsing algorithm. Grammar rules are implemented as constraints associated with the nodes and edges. When evaluated on the corpus (Sampson 1995), the parser achieved a precision of 89% and a recall of 79% in identifying labeled dependencies (Lin 1998b). 4.2 Model Selection
The construction of semantic space models involves a large number of parameters: the dimensions of the space, the size and type of the employed context, and the choice of similarity function. A number of studies (Patel, Bullinaria, and Levy 1998; Levy and
Bullinaria 2001; McDonald 2000) have explored the parameter space for word-based models in detail, using evaluation benchmarks such as human similarity judgments or synonymy choice tests. The motivation behind such studies is to identify parameters or parameter classes that yield consistently good performance across tasks. To avoid overfitting, exploration of the parameter space is typically performed on a development data set different from the test data (McDonald 2000).
 tinely used in NLP and cognitive science for development purposes X  X or example, for evaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky and
Hirst 2001; Banerjee and Pedersen 2003) or for exploring the parameter space of vec-tor space models (McDonald 2000). It consists of 65 noun-pairs ranging from highly synonymous ( gem-jewel ) to semantically unrelated ( noon-string ). For each pair, a sim-ilarity judgment (on a scale of 0 to 4) was elicited from human subjects. The average rating for each pair represents an estimate of the perceived similarity of the two words.
Correlation analysis is often used to examine the degree of linear relationship between the human ratings and the corresponding automatically derived similarity values. models on the Rubenstein and Goodenough (1965) data set. The best performing model was then used in all our subsequent experiments. We expect a dependency model optimized on the semantic similarity task to perform well across other related lexical for all tasks reported in this article, namely priming (Experiment 1), inferring whether two words are synonyms (Experiment 2), and acquiring predominant word senses (Experiment 3). Some performance gains could be expected, if parameter optimization took place separately for each task. However, such a strategy would unavoidably lead to overfitting, especially because our data sets are generally small (see Experiments 1 and 2).
 an emphasis on the influence of the context selection and path value functions.
Parameters. Dependency contexts were defined over a set of 14 dependency relations, each of which occurred more than 500,000 times in the BNC and which in total ac-counted for about 76 million of the 88 million dependency relations found in the corpus.
These relations are: amod (adjective modifier), comp1 (first complement), conj (coordina-tion), fc (finite complement), gen (genitive noun modifier), i (the relationship between a main clause and a complement clause), lex-mod (lexical modifier), mod (modifier), nn (noun-noun modifier), obj (object of a verb), pcomp-n (nominal complement of preposi-tions), rel (relative clause), s (surface subject), and subj (subject of a verb). From these, we constructed three context selection functions (fully described in Appendix A), which we implemented as parser-specific templates (one template per non-lexical dependency path):
The context specification functions were combined with the three path value functions introduced in Section 3: nine model instantiations. 2 To facilitate comparisons with traditional semantic space models, we used a word-based basis mapping function (see Equation (8)) and the log-likelihood score (see Equation (13)) as our lexical association function. We also created semantic spaces with different dimensions, using the 500, 1,000, and 2,000 most frequent basis elements obtained from the BNC. Finally, we experimented with a variety of similarity measures: cosine, Euclidean distance, L 1 norm, Jaccard X  X  coefficient, Kullback-
Leibler divergence, skew divergence, and Lin X  X  (1998a) measure. 178 Results. The effects of different parameters on modeling semantic similarity (using
Rubenstein and Goodenough X  X  [1965] data set) are illustrated in Tables 2 and 3. We report the Pearson product moment correlation ( X  X earson X  X  r X ) between human ratings of similarity and vector-based similarity. Rubenstein and Goodenough report an inter-subject correlation of r = 0 . 85 on the rating task. The latter can be considered an upper bound for what can be expected from computational models. For the sake of brevity, we only report results with 2,000 basis elements, because we found that models with fewer dimensions (e.g., 500 and 1,000) generally obtained worse performance. Lin X  X  (1998a) similarity measure uniformly outperformed all other measures by a large margin. For comparison, we also give the results we obtained with the cosine similarity measure (see Table 2).
 giving a low weight to a large number of possibly informative paths without subjects or objects. A similar result is reported in Henderson et al. (2002), who find that using the obliqueness hierarchy to isolate important index terms in an information retrieval task degrades performance. The use of the less fine-grained length path value function delivers better results for the medium and maximum context configurations (see Table 3).
Finally, we observe that the medium context yields the best overall performance. Within the currently explored parameter space, medium appears to strike the best balance: It includes some dependency paths beyond length one (corresponding to informative indirect relations), but also avoids very long and infrequent contexts which could potentially lead to overly sparse representations. In sum, the best dependency-based model uses the medium content selection and length path value functions, 2,000 basis elements, and Lin X  X  (1998a) similarity measure. This model will be used for our subse-quent experiments without additional parameter tuning. We will refer to this model as the optimal dependency-based model.
  X   X   X   X   X   X   X   X  4.3 Baseline Model
Our experiments will compare the optimal dependency model just described against a state-of-the art word-based vector space model commonly used in the literature. The as basis elements, and assumes that all words are given equal weight. In order to allow a fair comparison, we trained the word-based model on the same corpus as the dependency-based model (the complete BNC) and selected parameters that have been considered  X  X ptimal X  in the literature (Patel, Bullinaria, and Levy 1998; Lowe and
McDonald 2000; McDonald 2000). Specifically, we built a word-based model with a symmetric 10 word window as context and the most frequent 500 content words from the BNC as dimensions. 4 We used log-likelihood as our lexical association function and the cosine similarity measure 5 as our distance measure. 5. Experiment 1: Single-Word Priming
A large number of modeling studies in psycholinguistics have focused on simulating semantic priming phenomena (Lund and Burgess 1996; Lowe and McDonald 2000;
McDonald 2000; McDonald and Brew 2004). The semantic priming paradigm provides a natural test bed for semantic space models, as it concentrates on the semantic similarity vector-based models should capture. If dependency-based models indeed represent more linguistic knowledge, they should be able to model semantic priming better than traditional word-based models.

In single-word semantic priming, the transient presentation of a prime word like tiger directly facilitates pronunciation or lexical decision on a target word like lion : responses are usually faster and more accurate when the prime is semantically related to the target than when it is unrelated. Hodgson set out to investigate which types of lexical relations induce priming. He collected a set of 144 word pairs exemplifying six different lexical relations: (a) synonymy (words with the same meaning, e.g., value and worth ); (b) superordination and subordination (one word is an instance of the kind expressed by the other word, e.g., pain and sensation ); (c) category coordination (words which express two instances of a common superordinate concept, e.g., truck and train ); (d) antonymy (words with opposite meaning, e.g., friend and enemy ); (e) conceptual association (the first word subjects produce in free association given the other word, e.g., leash and dog ); and (f) phrasal association (words which co-occur in phrases, e.g., private and property ).
The pairs covered the most prevalent parts of speech (adjectives, verbs, and nouns); they were selected to be unambiguous examples of the relation type they instantiate and were matched for frequency. Hogdson found equivalent priming effects (i.e., reduced restricted to particular types of prime X  X arget relation.

McDonald and Brew (2004), using an incremental vector-based model of contextual 180 facilitation. Their ICE model (short for Incremental Construction of Semantic Expec-tations) simulates the difference in effort between processing a target word preceded by a related prime and processing the same target preceded by an unrelated prime. represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. When the target word is observed, the representation is updated using a Bayesian inference mechanism to reflect the newly arrived infor-mation. McDonald and Brew use a traditional semantic space that takes only word co-occurrences into account and is defined over the 500 most frequent words of the spoken portion of the BNC. They measure distance in semantic space using relative entropy (also known as Kullback X  X eibler divergence) and successfully model the data by predicting that the distance should be lower for related prime-target pairs than for unrelated prime X  X arget pairs. 5.1 Method
In this experiment we follow McDonald and Brew X  X  (2004) methodology in simulating semantic priming. However, because our primary focus is on the representation of the semantic space, we do not adopt their incremental model of semantic processing. We simply model reading time for prime X  X arget pairs by distance in the semantic space, without making explicit predictions about upcoming words.
 missing in the original data set), seven pairs containing at least one low-frequency word (less than 100 occurrences in the BNC) were removed to avoid creating vectors with unreliable counts. 6 We constructed a dependency-based model with the parameters that yielded best performance on our development set (see Section 4.2) and a baseline word-based model (see Section 4.3). Each prime X  X arget pair was represented by two vectors (one corresponding to the prime and one corresponding to the target).
 ables (i.e., the variables directly manipulated by Hodgson [1991] in his original experi-ment) are (1) the type of Lexical Relation (antonyms, synonyms, conceptual associates, phrasal associates, category coordinates, superordinate-subordinates), and (2) the Prime (related, unrelated). The dependent variable (i.e., the quantity being measured) is the distance between the vector space representations of the prime and the target. The priming effect is simulated by comparing the distances between Related and Unrelated prime X  X arget pairs. Because the original materials do not provide Unrelated primes, we emulated the unrelated pairs as described in McDonald and Brew (2004), by using the average distance of a target to all other primes of the same relation.
 mantic priming. Failure to do so would indicate that our model is deficient because it cannot capture basic semantic relatedness, a notion underlying many tasks in cognitive science and NLP. Second, we predict that the dependency-based model will be better at simulating priming than a traditional word-based one. 5.2 Results
We carried out a two-way analysis of variance (A NOVA data generated by the optimal dependency-based and the baseline word-based model. The factors were the two independent variables introduced herein, namely Lexical
Relation (six levels) and Prime (two levels). A reliable Prime effect was observed for the dependency-based model (F(1, 129) = 182 . 46, MSE = 0 . 93, p &lt; 0 . 01): the distance between a target and its Related prime was significantly smaller than between a tar-tional word-based model that did not use any syntactic information (F(1, 129) = 106 . 69,
MSE = 2 . 92, p &lt; 0 . 01). There was no main effect of Lexical Relation for either model (F(5, 129) &lt; 1).
 only indicates that there are differences between the Related and Unrelated prime-target models, for example, by quantifying the magnitude of the Prime effect. Eta-squared (  X  )isastatistic 7 often used to measure the strength of an experimental effect (Howell overall variability in the dependent variable (in our case distance in semantic space) can be explained or accounted for by the independent variable (i.e., Prime). The use of  X  allowed us to perform comparisons between models (the higher the  X  model). The Prime effect size was greater for the dependency model, which obtained an  X  of 0.332 compared to the word-based model whose  X  2 was 0.284. In other words, the dependency model accounted for 33.2% of the variance, whereas the word-based model accounted for 28.4%.
 across all relations, we next conducted separate A NOVAS for each type of Lexical Rela-tion. The A NOVAS revealed reliable priming effects for all six relations. Table 4 shows the mean distance values for each relation in the Related and Unrelated condition and the
Prime Effect size for the dependency model. The latter was estimated as the difference in distance values between related and unrelated prime-target pairs (asterisks indicate whether the difference is statistically significant, according to a two-tailed paired t -test).
For comparison, we also report the Prime Effect size that McDonald and Brew (2004) obtained in their simulation.
 dency relations simulates direct priming across a wide range of lexical relations. Fur-thermore, our model obtained a priming effect that is not only reliable but also greater in magnitude than the one obtained by a traditional word-based model. Although we used a less sophisticated model than McDonald and Brew (2004), without an update procedure and an explicit computation of expectations, we obtained priming effects across all relations. In fact, we consider the two models complementary. McDonald and
Brew X  X  model could straightforwardly incorporate syntax-based semantic spaces like the ones defined in this article.
 whether the proposed dependency model can reliably distinguish synonyms from non-synonyms. This capability may be exploited to automatically generate corpus-based 182 thesauri (Grefenstette 1994; Lin 1998a; Curran and Moens 2002) or used in applications that utilize semantic similarity. Examples include contextual spelling correction (Jones and Martin 1997), summarization (Barzilay 2003; Erkan and Radev 2004) and question answering (Lin and Pantel 2001). 6. Experiment 2: Detecting Synonymy
The Test of English as a Foreign Language (TOEFL) is commonly used as a benchmark for comparing the merits of different similarity models. The test is designed to assess non-native speakers X  knowledge of English. It consists of multiple-choice questions, each involving a target word embedded in a sentence and four potential synonyms.
The task is to identify the real synonym. An example is shown below where crossroads is the real synonym for intersection .

Landauer and Dumais (1997) were the first to propose the TOEFL items as a test for lexical semantic similarity. Their LSA model achieved an accuracy of 64.4% on 80 items, a performance comparable to the average score attained by non-native speakers taking the test. Sahlgren (2006) uses Random Indexing, a method comparable to LSA, to represent the meaning of words and reports a 75.0% accuracy on the same TOEFL items.
It should be noted that both Landauer and Dumais and Sahlgren report results on seen peaked.
 (2001) and Higgins (2004) propose models that capitalize on the collocational nature of semantically related words. Two words are considered similar if they tend to occur near each other . Turney uses pointwise mutual information (PMI) to measure the similarity between a target word and each of its candidate synonyms. Co-occurrence frequencies are retrieved from the Web using an information retrieval (IR) engine: where P ( w 1 , w 2) is estimated by the number of hits (i.e., number of documents) returned by the IR engine (Turney used AltaVista) when submitting a query with the NEAR operator. 8 The PMI-IR model obtained an accuracy of 72.5% on the TOEFL data set. NEAR operator by concentrating on word pairs that are strictly adjacent:
Note that Equation (16) takes the minimum number of hits for the two possible orders w 1, w 2 and w 2 , w 1 in an attempt to rule out the effects of collocations and part-of-speech ambiguities. The LC-IR (local-context information retrieval) model outperformed PMI-
IR, achieving an accuracy of 81.3% on the TOEFL items. 6.1 Method
For this experiment, we used the TOEFL benchmark data set pared our optimal dependency-based model against the baseline word-based model. We would also like to compare the vector-based models against Turney X  X  (2001) and
Higgins X  (2004) collocational models. Ideally, such a comparison should take place on the same corpus. Unfortunately, downloading and parsing a snapshot of the whole Web is outside the scope of the present article. Instead, we assessed the performance of these models on the BNC, using a search engine which simulated AltaVista. Specifically, we indexed the BNC using Glimpse (Manber and Wu 1994), a fast and flexible indexing and query system. 10 Glimpse supports approximate and exact matching, Boolean queries, wild cards, regular expressions, and many other options.
 the number of documents containing w 1 and w 2 or w 2 and w
The target w 1 and its candidate synonym w 2 did not have to be adjacent, but the number of the intervening words was bounded by the length of the sentence. The frequencies hits( w 1 ) and hits( w 2 ) were estimated similarly by counting the number of documents in which w 1 and w 2 occurred. Ties were resolved by randomly selecting one of the candidate synonyms. The BNC proved too small a corpus for the LC-
IR model, which relies on w 1 and w 2 occurring in directly adjacent positions. This is not a problem when frequencies are obtained from Web-scale corpora, but in our case most queries retrieved no documents at all (96.6% of hits( w hits( w 2 , w 1 ) were zero). We thus report only the performance of the PMI-IR model on the BNC.
 decide which one of the four alternatives was synonymous with the target word. For the vector-based models, we computed the distance between the vector representing the candidate word and each of the candidate synonyms, and selected the candidate with the smallest distance. Analogously, the candidate with the largest PMI-IR value was chosen for Turney X  X  (2001) model. Accuracy was measured as the percentage of 184 right decisions the model made. We also report the accuracy of a naive baseline model which guesses synonyms at random.
 dency model carries over to a different task and data set. We are further interested to see whether linguistic information (represented in our case by dependency paths) makes up for the vast amounts of data required by the collocational models. We therefore compare directly previously proposed Web-based similarity models with BNC-based vector space models. 6.2 Results
Our results 11 are summarized in Table 5. We used a  X  2 test to determine whether the differences in accuracy are statistically significant. Not surprisingly, all models are sig-nificantly better than random guessing (p &lt; 0 . 01). The dependency model significantly outperforms the word-based model and PMI-IR when the latter uses BNC frequencies (p &lt; 0 . 05). PMI-IR performs comparably to our model when using Web frequencies.
The Web-based LC-IR numerically outperforms the dependency model, however the difference is not statistically significant on the TOEFL data set (p &lt; 1). Expectedly, Web-based PMI-IR and LC-IR are significantly better than the word-based vector model and the BNC-based PMI-IR (p &lt; 0 . 05).
 word-based model on the synonymy detection task. On the BNC, it also outperforms the collocation-based PMI-IR. Our interpretation is that the conceptually simpler collo-cation models suffer from data sparseness, whereas the dependency model can profit from the additional distributional information it incorporates. It is a matter of future work to examine whether dependency models can carry over their advantage to larger corpora.
 to word sense disambiguation (WSD), a task which has received much attention in NLP and is ultimately important for document understanding. 7. Experiment 3: Sense Ranking
The ability to identify the intended reading of a polysemous word (the word sense ) in context is crucial for accomplishing many NLP tasks. Examples include lexicon acquisition, discourse parsing, or metonymy resolution. Applications such as question answering or machine translation could also benefit from large scale word sense disam-biguation (WSD).
 a variety of approaches have been proposed for disambiguating word senses. To date, most accurate WSD systems are supervised and rely on the availability of training data (see Yarowsky and Florian [2002], Mihalcea and Edmonds [2004], and the references therein). Although supervised methods typically achieve better performance than their unsupervised alternatives, their applicability is limited to those words for which sense-labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available. Furthermore, if the distribution of senses is skewed, as is often the case, the simple heuristic of choosing the most common or predominant sense in the training data (henceforth  X  X he first sense heuristic X ) delivers results competitive with supervised approaches based on local context (Hoste et al. 2002).
 consuming. More importantly, one would expect that a word X  X  first sense varies across domains and text genres (the word court in legal documents will most likely mean  X  X ribunal X  rather than  X  X ard X ). Therefore, manual annotation must be redone for most new languages, domains, and sense inventories. McCarthy et al. (2004) show that the annotation bottleneck can be avoided by inferring the first sense heuristic automatically method in itself, it can be usefully combined with context-based disambiguation meth-ods in order to alleviate the data requirements for WSD. Their method builds on the observation that a word X  X  distributionally similar neighbors often provide cues about its senses. In their model, sense ranking is equivalent to quantifying the degree of similarity between each neighbor and each sense description of a polysemous word. The sense most similar to the neighbors is the first sense.
 bors to acquire more or less accurate first senses. In this experiment, we examine whether the dependency-based models discussed in this article can be used for the sense ranking task, thereby assessing their potential for practical NLP tasks. The aims of our experiment are twofold: (1) to investigate whether our dependency-based framework can be used to acquire distributionally similar words that differ in quality from those obtained with word-based models and (2) to observe their impact on WSD. We first de-scribe McCarthy et al. X  X  sense-ranking model, which forms the basis of our experiments, and then detail our methodology and results. 7.1 The Sense-Ranking Model each sense ws i a  X  X redominant sense score X  PS ( ws i ) as follows: 186 where
The predominant sense of w is simply the one with the largest PS ( ws that is maximally similar to its neighbors n j  X  N ( w ) according to Equations (17) and (18). which distributionally similar words are acquired, (2) the measure of distributional measure of sense similarity ( sim WN ). The PS score combines distributional similarity and sense similarity, taking into account both lexical knowledge gathered from corpora and the organization and structure of the lexical resource that provides the sense inven-tory. A large number of sense similarity measures have been developed for WordNet and WordNet-like taxonomies. These vary from simple edge-counting (Rada, Mili, and
Bicknell 1989) to attempts to factor in peculiarities of the network structure by consid-ering link direction (Hirst and St-Onge 1998), relative depth (Leacock and Chodorow 1998), and density (Agirre and Rigau 1996). A number of hybrid approaches have also been proposed that combine WordNet with corpus statistics (Resnik 1995; Jiang and Conrath 1997).
 of all nouns attested in SemCor, a subset of the Brown corpus containing 23,346 lemmas annotated with senses according to WordNet 1.6. They acquire distributionally similar words from a large collection of dependency relations obtained from the written part of the BNC (90 million words) using Briscoe and Carroll X  X  (2002) parser. Their model and is restricted to a small set of dependency relations (verb X  X ubject, verb X  X bject, noun X  X oun, and adjective X  X oun). They employ a basis mapping function that maps measure (see Equation (3)). They obtained a type-level accuracy of 54% (a random baseline achieved 32%) at recovering the most prevalent sense (using 50 neighbors and either Lesk X  X  [1986] or Jiang and Conrath X  X  [1997] measures). They also used a token disambiguator that always defaults to the automatically acquired first sense and obtained a token-level disambiguation accuracy of 48% for Lesk (50 neighbors) and 46% for Jiang and Conrath (50 neighbors). Their baseline for this task was 24%. 7.2 Method
We replicated McCarthy et al. X  X  (2004) study using our optimal dependency-based model ( medium context selection, length path value functions, 2,000 basis elements, Lin X  X  [1998a] similarity measure, and the log-likelihood association function) and the baseline word-based model. We used Equation (17) to find the first sense for all polysemous nouns in SemCor (according to WordNet 1.6). Following McCarthy et al., we only considered polysemous nouns attested in SemCor with a frequency &gt; 2, and in our parsed version of the BNC with a frequency  X  10. The total number of nouns after applying the frequency cutoffs was 2,750 12 and the average sense ambiguity was 4.55 (the most ambiguous word had 30 senses, and least ambiguous 2). For each one of the 2,750 nouns, we generated the set of its distributionally similar neighbors from the set of the nouns in the intersection between the BNC and WordNet (15,656 in total). the number of distributionally similar neighbors required for the computation of the prevalence score. McCarthy et al. (2004) undertook a thorough comparison and ob-tained best results with 50 neighbors using Lesk X  X  (1986) and Jiang and Conrath X  X  (1997) measures. They argue that the latter measure is more efficient for large scale WSD and use it exclusively in all subsequent work (McCarthy et al. 2004; Koeling, McCarthy, and
Carroll 2005). We thus adopted the parameters that McCarthy et al. found to be optimal, namely 50 neighbors and Jiang and Conrath X  X  similarity measure, which we will briefly describe.
 senses by combining taxonomic information with corpus data. It is based on the notion of information content (IC) of a WordNet synset s . IC is defined as the negative log-likelihood of s :
Jiang and Conrath define a distance measure that combines IC with edge counting parameters,  X  and  X  , that control the influence of node depth and density, respectively. Setting  X  to zero and  X  to one, their measure simplifies to: synsets (that is, senses) s 1 and s 2 . We used the WordNet Similarity Package (Pedersen, Patwardhan, and Michelizzi 2004), which provides an implementation of Jiang and
Conrath X  X  (1997) measure (version 0.06). 13 We re-estimated the IC counts from the BNC because those provided with the package are derived from the manually annotated SemCor and would positively bias our results.
 dominant sense against the manually annotated SemCor. We use the following notation to describe our evaluation measures: W is the set of all word types (
W ps is the set of word types with a predominant sense, that is, with a sense that is more frequent than the second sense in SemCor ( | W ps | = 2, 338). S ( w )isthesetofWordNet senses for word type w ,and T ( w ) the set of all tokens of w . Finally, we use ps ps ( w ) to refer to the predominant sense of word w according to SemCor and the sense ranking model, respectively, and sense sc ( t ) to denote the sense annotated in SemCor for a particular token t .
 namely, on identifying the predominant sense for a word type, if one exists: 188
A baseline for the sense ranking task can be easily defined by selecting a sense at ran-dom for each word type from its sense inventory and assuming that this is the first sense:
Like McCarthy et al. (2004), we also assessed the word sense disambiguation potential the predominant sense (according to the ranking model) to every noun token, without sense given by the ranking model is identical to the SemCor gold standard sense: A baseline disambiguator can be defined by assigning a random sense to each token: 7.3 Results
Table 6 shows the results for the optimal dependency-based model, the random base-line, the baseline word-based model, and McCarthy et al. X  X  (2004) state of the art model.
As an upper bound, we report WSD accuracy when defaulting to the first (i.e., most frequent) sense provided by SemCor. All models use 50 nearest neighbors and Jiang and
Conrath X  X  (1997) WordNet-based semantic similarity measure. As far as distributional similarity is concerned, our dependency model employs Lin X  X  (1998a) measure and so do McCarthy et al., whereas the traditional word co-occurrence model uses cosine.
Our model differs from McCarthy et al. in the context selection, path value, and basis mapping functions (see the subsequent discussion). We used a  X  the differences in performance are statistically significant. Note that we have a slightly different set of nouns from McCarthy et al. (2004); this is due to the use of a different  X  parser and a larger corpus. We work on the assumption that this difference is negligible.
We use a set of diacritics to denote statistical significance, explanations for which are provided in Table 6.
 that all models significantly outperform the random baseline (p &lt; 0 . 01). Furthermore, both the dependency-based model and McCarthy et al. (2004) significantly outperform the word-based model. The two dependency models yield comparable performances the random baseline (p &lt; 0 . 01). Our dependency model significantly outperforms the word-based model and McCarthy et al. (p &lt; 0 . 01). The word-based model performs worse than the upper bound (p &lt; 0 . 01).
 et al. (2004) by a large margin (8.3%) on the WSD task, whereas the two models yield comparable performances on sense ranking. Also, the word-based model performs significantly better than McCarthy et al. on WSD, while it is significantly worse than
McCarthy et al. in sense ranking. This indicates that the words for which each model assignments reveals that McCarthy et al. and our dependency model have only 35.7% nouns in common for which they predict the first sense correctly. McCarthy et al. has 34.8% nouns in common with the word-based model, which in turn has 40.3% nouns in common with our dependency model.
 quency influence the performance of our ranking model. In theory, an automatically acquired sense ranker should have a good accuracy on all ambiguous words in order to do well on WSD. However, in practice the sense ranker X  X  performance depends crucially on its ability to correctly predict the first sense for highly frequent and highly ambiguous words. An additional complicating factor is the sense distribution of the words in question. For words whose sense distributions are not particularly skewed, getting the first sense wrong will not be entirely detrimental as long as the WSD method misclassifies relatively frequent senses as predominant.
 11 senses according to WordNet 1.6. Among these, sense 1 is found seventeen times, sense 2 fifteen, sense 3 ten, and sense 4 nine (all other senses have considerably smaller frequencies). Now suppose that the sense ranking method wrongly identifies sense 2 as the predominant sense for corner . Using this sense, our WSD system will correctly disambiguate 24.6% of the instances of corner in Semcor, despite the fact that it will not receive any credit for identifying the first sense. Note that the right first sense would yield only slightly better accuracy (i.e., 27.4%).
 quencies were estimated from the BNC as it constitutes a larger sample of English than
Semcor). Table 7 illustrates our models X  sense ranking and WSD accuracy according to these bands; we also list the average sense ambiguity and number of word types for each band. As can be seen, our dependency model obtains consistently good performance on both tasks, even in the high ambiguity bands (Bands 1,000 X 5,000 and 5,000+, highlighted in Table 7). The obtained accuracies are well above the baseline of choosing a sense at random (for example, an average ambiguity of 8.3 in the 5000+ band corresponds to frequent words are represented by more reliable vectors. As a result, the acquired neighbors are of higher quality, which counteracts the increased ambiguity. 190 ranking accuracy in high-frequency bands (most notably in Band 1,000 X 5,000), which seems counterintuitive. This effect can be explained by taking into account the observed sense frequencies and the types of errors introduced by our model in these bands. The distribution of senses in the high-frequency bands tends to be less skewed, at least ac-cording to Semcor (82% of nouns in Band 1,000 X 5,000 and 65% in Band 5,000+ have a first sense with frequency &lt; 50). Our model X  X  mistakes are often  X  X ear misses X , that is, the first and second sense ranks are flipped. Specifically, near misses are observed for 25% of the noun types in Band 1,000 X 5,000, and 15% in Band 5,000+. Now, for nouns with non-skewed sense distributions, disambiguating with the second sense will boost
WSD accuracy even though this is not the case for sense ranking (see the previous discussion).
 presented in this article can be successfully used for the automatic acquisition of first senses from raw text. We obtained results similar to McCarthy et al. (2004) on the sense ranking task and demonstrated that our model performs significantly better on WSD.
Furthermore, it outperformed a word-based semantic space on both tasks. Our model differs from McCarthy et al. in three important ways: (a) following our terminology, they use a semantic space with the minimum context selection (paths of length one) and plain path value (no path weighting) functions, whereas our model employs the medium content selection and length path value functions; (b) their space is constructed over a limited set of dependency paths, namely subject, object, and adjective-noun modification relations, whereas our model uses a wider range of relations including information about tense (for example, whether a complement is finite or not), relativi-sation, etc. (see Section 4.2 for details); and (c) their basis mapping function maps paths to tuples whereas we employ a word-based function and restrict the dimensions of the space to the 2,000 most frequent elements (McCarthy et al. do not employ any cutoffs). amounting to 90% of the total corpus) and a different parser (Briscoe and Carroll 2002). the scope of this article, we should note that the two parsers yield comparable perform-ances and employ a similar inventory of dependency relations (see Curran (2004) for more discussion). We thus suspect that differences in WSD accuracy cannot be uniquely attributed to parser performance. We can, however, assess whether the difference is due to corpus size by examining its effect on the performance of our model. If it is indeed sensitive to corpus size, we would expect a relatively large drop in performance when our semantic space is built on smaller corpora. We randomized the order of sentences in the BNC and constructed semantic spaces on data sets progressively increasing in size: The first space was constructed from 5% of the BNC, the next from 10%, and so resulting learning curves. When the dependency model is constructed on 5% of the
BNC, it delivers a WSD accuracy of 51%, which eventually increases to 54.3% when the entire corpus is used. This result indicates that the model performs well when trained on a small corpus and that its good performance cannot be attributed solely to corpus size. However, it also suggests that a large increase in corpus size is necessary to obtain substantial improvements with the present sense ranking strategy, which uses distributional similarity as a corrective for taxonomy-based similarity: Accuracy increases by approximately 4% when our corpus size increases by a factor of 20. due to differences in the basis mapping function. Because McCarthy et al. (2004) use all available basis elements, their semantic space grows linearly with vocabulary (i.e., cor-pus) size. Each target word is represented by a set of  X  X eatures X  X  X elation X  X ord pairs with a non-zero occurrence frequency X  X hich may vary widely between target words.
In contrast, our model defines a modest number of basis elements (2,000) which are shared between all target words. The resulting representation is a vector space which is less sparse and the resulting neighbors capture more succinctly the semantic properties of words. Additional evidence comes from the performance of the word-based model, which also uses a word basis mapping function and a fixed number of dimensions (500 words). Although this model does not incorporate syntactic information in any way, it manages to outperform McCarthy et al. on the WSD task. In sum, we attribute the superior performance of the vector-based model to two key factors: low dimensionality 192 (as seen by the comparison to McCarthy et al.) and the incorporation of linguistic knowledge (as seen by the comparison to the word-based model). 8. General Discussion
In this article, we presented a general framework for the construction of semantic space models. The framework operates on paths of dependency relations, allowing linguistic knowledge to guide the construction of semantic spaces. It extends previous work on traditional word-based semantic space models as well as syntax-based models by providing a principled way for defining the context and the dimensions of the semantic space. More specifically, we isolated three important parameters of space construction: the context selection function, the basis mapping function, and the path value function. dimensions (e.g., words, parts of speech or word X  X elation tuples), and dependency relations (e.g., subjects, objects) contribute towards the construction of a semantic space. compared it against state-of-the-art models. Experiment 1 revealed that semantic space models defined over dependency relations adequately simulate semantic priming. Ex-periments 2 and 3 examined the usefulness of our framework for NLP: we used our model to detect synonymy relations and to automatically acquire prevalent senses for polysemous words. In all cases, syntactically enriched models outperformed traditional word-based models that did not take account of syntax.
 terizations, evaluate the resulting models on a development set, and select a broadly optimal model for testing on unseen data. Therefore, our models were not specifically tuned for the tasks at hand and we have only explored a relatively small subset of the parameter space. Our examination of different parameter combinations in Section 4.2 revealed that medium syntactic contents yield consistently better performance when combined with a path value function that penalizes longer paths ( length ). An important avenue for future work concerns defining more fine-grained path value functions. Our results show that a path value function inspired by the obliqueness hierarchy delivers worse results than the linguistically naive length function. Alternatively, we could define a function that combines gram-rel with length , or more generally learn a weighting scheme for paths by optimizing some objective function.
 that maps dependency paths to words. It should also be interesting to experiment with different types of basis mapping functions. For example, we could experiment with more coarse-grained functions based on parts-of-speech or more fine-grained ones such as the relation X  X ord pairs used by McCarthy et al. (2004). We would also like to observe the impact of singular value decomposition (SVD) on our semantic spaces along the lines of Kanejiya, Kumar, and Prasad X  X  (2003) cognitive modeling work. They use SVD to reduce the dimensionality of a semantic space that uses (word, part-of-speech) pairs as basis elements, obtaining better coverage compared with an LSA space constructed over word co-occurrences. Further studies must examine the effect of parser quality on the obtained co-occurrences, and the influence of the chosen similarity measure. in this article. The potential applications are many and varied both for cognitive science and NLP. Our syntactically enriched models retain the simplicity of word co-occurrence models while allowing for the role of syntactic structure to influence the representa-plausibility X  X t is not mere lexical association that accounts for the meaning of words but rather their lexical and syntactic dependencies. Arguably, this property holds great promise for languages less configurational than English. A prediction that we intend to test in the future is that syntax-based semantic space models should be able to represent meaning more adequately than traditional word-based models for languages that allow constituent scrambling (e.g., German) or have free word order (e.g., Czech). traditional and LSA-based models have accounted for. Possible future experiments in-clude mediated priming (Lowe and McDonald 2000) and multiple priming (McDonald and Brew 2004), intelligent tutoring (Kanejiya, Kumar, and Prasad 2003), and coherence rating (Foltz, Kintsch, and Landauer 1998). A number of NLP tasks could also benefit from the framework presented in this article. Examples include word sense discrimi-nation (Lin 1998a; Sch X tze 1998), automatic thesaurus construction (Grefenstette 1994;
Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general similarity-based approaches to NLP.
 Appendix A. Context Selection Functions In what follows we present the context selection functions we used in our experiments.
These are encoded as non-lexicalized path templates and are distributed as part of the software package that implements our dependency-based semantic space framework label sequence. Path templates are denoted by a comma-separated sequence of one or more edge labels; each edge label is a colon-separated triple POS1:relation:POS2 (see word t and a context selection function c , the context of t consists of all paths  X  paths anchored at t ) so that there is a path template temp label sequence l (  X  t ).
 Minimum: 194
Maximum contains all medium medium medium templates and: Acknowledgments 196 198
