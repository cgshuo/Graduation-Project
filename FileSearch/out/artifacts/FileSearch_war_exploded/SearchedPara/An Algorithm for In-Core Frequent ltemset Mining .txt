 summary structure. 
Memory Efficient Accurate Mining: A key limitation of the existing work on frequent itemset mining has been the high memory requirements when the number of distinct items is large and/or the support level desired is quite low. Our single pass algorithm has a property that it does not produce false negatives, i.e., all frequent itemsets with desired sup-port level are reported. The false positives reported by our algorithm can be easily removed through a second pass on the dataset. Our two pass algorithm provides high memory efficiency, while not compromising accuracy in any way. determining frequent items (or 1-itemsets) [14]. They present a two pass algorithm for this purpose, which requires only (I/@) memory, where 19 is the desired support or frequency level. Their first pass computes a superset of frequent items, and the second pass eliminates any false positives. Our work addresses three major challenges in applying their ideas for frequent itemset mining in a streaming environment. First, we have developed a method for finding frequent k-itemsets, while still keeping the memory requirements limited. Second, we have developed a way to have a bound on the superset computed after the first pass. Third, we have developed a new data structure and a number of other implementation optimizations to support efficient execution. thetic and real datasets. Our results can be summarized as follows. First, our one pass algorithm is very accurate in prac-tice. Second, our algorithm is very memory efficient. For ex-ample, using the TlO.14.NlOK dataset and a support level of 1%, we can consistently handle 4 million to 20 million trans-actions with less than 2.5 MB main memory. In comparison, 
Manku and Motwani's algorithm [15] requires an out-of-core data-structures on top of a 44 MB buffer to process 1 million transactions. Finally, the algorithm can handle large number of distinct items and small support levels using a reasonable amount of memory. For example, a dataset with 100,000 dis-tinct items and a support level of 0.05% could be handled with less than 200 MB main memory, a factor of 5 improvement over Apriori. algorithm. Initially, we discuss a new approach for finding frequent items from Karp et al. [14]. We then discuss the challenges in extending this idea to frequent itemset mining, and finally outline our ideas for addressing these issues. 
In this paper, we build a frequent itemset mining algorithm 1. Dealing with Transaction Sequences: The algorithm from 
Karp et al. assumes that a sequence is comprised of el-ements, i.e., each transaction in the sequence only con-tains one-items. In frequent itemset mining, each trans-action has a number of items, and the length of every transaction can also be different. 2. Dealing with k-itemsets: Karp et al.'s algorithm only finds the frequent items, or 1-itemsets. In a frequent itemset mining algorithm, we need to find all k-itemsets, k 2 1, in a single pass. 
Note that their algorithm can be directly extended to find i-itemsets in the case where each transaction has a fixed length, I. This can be done by eliminating a group of (110) x (f) different i-itemsets together. This, however, requires R((1/0) x (f)) space, which becomes extremely high when 1 and i are large. Furthermore, in our problem, we have to find all i-itemsets, i 2 single pass. 3. Providing an Accuracy Bound: Karp et al.'s algorithm can provably find a superset of the frequent items. How-ever, no accuracy bound is provided for the item(set)s in the superset, which we call the potential frequent item(set)s. For example, even if an item appears just a single time, it can still possibly appear in the superset reported by the algorithm. In frequent itemset mining, we will like to improve above result, and provide a bound on the frequency of the itemsets that are reported by the algorithm. 
We now outline how we can address the three challenges we 
Recall that most of the existing work on frequent itemset 
Our idea is to use a hybrid approach to mine frequent item-
In this section, we introduce our new algorithm in three 
Before detailing each algorithm, we first introduce some 
To store and manipulate the candidate frequent itemsets where, k is largest frequent itemset, and Li, 1 5 the number of distinct items in the dataset, is typically not we can find the exact frequent 1-itemsets in the stream D. the potential frequent 2-itemsets. 
As we stated in the previous section, we deal with all k-ceived transactions. The buffer will be accessed several times 
StreamMining-Fixed(Stream D, 8 ) global Lattice C; local Buffer 7; local Transaction t; ltl * (ltl -111% foreach (t E 2)) 
Update(&amp; C, 2); Output (L); Figure 3: StreamMining-Fixed: Algorithm Assuming 
Fixed Length Transactions 
The algorithm we present here mines frequent itemsets from a stream, under the assumption that each transaction has the same length Itl. The algorithm has two interleaved phases. frequent k-itemset is guaranteed to be in the output of our al-gorithm. Lemma l provides an estimate of the memory costs for CZ. 
StreamMining-Bounded(Stream 'D, 8, E ) ltl * (ltl -0; // Number of ReducFreq Invocations Update(t, L,l); 
Update(t, C, 2); if lL21 2 [1/8~1 .f if s.count 5 8lDl -c -Figure 5: StreamMining-Bounded: Algorithm with a Bound on Accuracy 
We now extend the algorithm from the previous subsection to provide a bound on the accuracy of the reported results. 
As described in Subsection 2.3, the bound is described by an user-defined parameter, E, where 0 &lt; r 5 1. Based on this parameter, the algorithm ensures that the frequent itemsets reported do occur more than (1 -r)81D1 times in the dataset. 
The basic idea for achieving such a bound on frequent items computation was illustrated in Figure 2. We can extend this idea to finding frequent itemsets. Our new algorithm is de-scribed in Figure 5. Note that we still assume that each trans-action has the same length. 
This algorithm provides the new bound on accuracy in two steps. In the first step, we invoke the algorithm in Figure 3 with the frequency level 86. This will report a superset of itemsets occurring with frequency more than Nee. We record the number of invocations of ReducFreq, c, in the first step. 
Clearly, c is bounded by NO X . In the second step, we remove all items whose reported frequency is less than NO -c NB(1 -e). This is achieved by the last foreach loop. 
The new algorithm has the following property: 1) if an itemset has frequency more than 8, it will be reported. 2) if an itemset is reported as a potential frequent itemset, it must have a frequency more than 8(1 -E). Theorem 2 formally states this property, and its proof is available in a technical report 1131. 
THEOREM 2. In using the algorithm StreamMining-Bounded on a set of transactions with a fixed length, for any k 
Ck 
Note that the number of invocations of ReducFreq, c, is usually much smaller than Nee after processing a data stream. 
To motivate the need for taking such a weighted average, consider the natural alternative, which will be maintaining f as the average number of 2-itemsets that each transaction seen so far has. This will not work correctly. For example, suppose there are 3 transactions, which have the length 2, 2, and 3, respectively, and I9 is 0.5. The first two transactions will have a total of two 2-itemsets, and the third one has 6 2-itemsets. We will preform an elimination when the number of different 2-itemsets is larger than or equal to (118) x f. When the first two transactions arrive, an elimination will happen (assuming that the two 2-itemsets are different). When the third one arrives, the average number of 2-itemsets is less than 3, so another elimination will be performed. Unfortunately, a frequent 2-itemset that appears in both transactions 1 and 3 will be deleted in this way. 
In our approach, the number of invocations of ReducFreq, c, is less than IDl(ee), where ID/ is the number of transactions processed so far in the algorithm. Lemma 3 formalizes this, and its proof is available in a technical report [13]. LEMMA 3. c &lt; I'DI(I9c) is an invariant in the algorithm StreamMining. 
Note that by using the Lemma 3, we can deduce that the property of the Theorem 2 still holds for mining a stream of transaction with variable transaction lengths. Formally, 
THEOREM 3. In using the algorithm StreamMining on a stream of transactions with variable lengths, for any k L; g Lk g Lk . 
An interesting property of our method is that in the situ-ation where each transaction has the same length, our final algorithm, StreamMining will work in the same fashion as the algorithm previously shown in Figure 5. 
Note, however, that unlike the case with fixed length trans-actions, the size of L2 cannot be bound by a closed formula. Also, in all the algorithms discussed in this section, the size rithms use the Apriori property to reduce their sizes. In the next section, we evaluate the memory cost of our algorithm experimentally. 
In our experiments, we are interested in evaluating a num-ber of different aspects of our algorithm. 
Comparing the execution time and memory requirements of our one pass and two pass algorithm with those of Apriori and FP-tree based algorithms. 
Evaluating the execution time and memory requirements of our new algorithms with increasing dataset size and decreasing support levels. 
Evaluating the accuracy of our algorithm with different levels of E. 
Demonstrating the ability of our algorithm to handle very large number of distinct items and very low support levels. 
For comparing our algorithm against the Apriori algorithm, we used a well-known public distribution from Borgelt [4]. Earlier versions of this code have been incorporated in a com-mercial data mining tool called Clementine. For comparisons with FP-tree based approach, the implementation we used is from Goethals [8]. All our experiments were conducted on a 933 MHz Pentium I11 machine with 512 MB main memory. Figure 7: Execution Time with Changing Support Level (T10.14.NlOK Dataset) 
Figure 8: Memory Requirements with Changing Sup-port Level (T10.14.NlOK Dataset) Figure 9: Execution Time with Increasing Dataset Size (threshold=O.l%, T10.14.NlOK Dataset) trend, which is that our algorithm is significantly better both in terms of execution time and memory requirements. De-tailed results are presented in a technical report [13]. 
The real dataset we use is the BMS-WebView-1 dataset which contains several months of click-stream data from one e-commerce website. A portion of it has been used in the KDD-Cup 2000 competition and also used by Zhang et al. [22] to evaluate traditional offline association mining algorithms. 
The characteristics of the BMS-WebView-1 dataset are quite different from the IBM Quest synthetic datasets. The origi-nal dataset has 59,602 transactions and contains 497 distinct items. The maximum transaction size is 267, while the av-erage transaction size is just 2.5. For our experiments, we duplicated and randomized the original dataset to obtain 1 million transactions. 
Because of the small size of the dataset and the small num-ber of distinct items, we did not expect to outperform Apriori on this dataset. However, we have still compared the per-formance with Apriori to show that the algorithm can give accurate results in one pass, and can still be competitive. 
In our experiments, we use e = 0.6. Further, we provide an-other parameter m to represent the maximal frequent itemsets we are interested in. This is because if we have some addi-tional knowledge about the length of the maximal frequent itemsets, the performance of our implementation can be im-proved. In this dataset, as the support level is 0.2%, 0.4%, 0.6%, 0.8% or 1%, the maximal frequent itemsets is 2, 3, 3, 4, and 6, respectively. For the online checking optimization we had described earlier, the threshold we define is 10, i.e, two transactions in the buffer will not have a common subset which contains more than 10 items. Since we have only less than 500 distinct items, we maintain all of the 2-itemsets as an array in the main memory. 
Figure 12 compares the execution time. Stream-m* refers to ~Gearn~inin~ with some knowledge of maximal frequent itemsets. For support level of 0.2%, we had m = 4, and for others, we had m = 3. Stream-m6 refers to the version using m = 6 in all cases. Stream refers to StreamMzning having no knowledge about the maximal frequent itemsets. Stream+m*, Stream+m6 and Stream+ refer to the corresponding two pass versions. The three versions have very similar results for accuracy. For threshold levels between 1% and 0.4%, they achieve 100% accuracy. For the threshold of 0.2%, the accuracy is nearly 99%. 
We can see that the performance of Stream-m* is quite sim-ilar to Apriori. For the Stream-m6 and Stream, we can see as the additional information on maximal frequent itemsets is re-duced, the algorithm performance becomes less competitive. For the two-pass algorithm, we can see that the second pass just adds a fairly small and constant time. 
In this section, we compare our work with related research efforts in the areas of approximate (one-pass) and accurate frequent itemset mining. 
The work closest to our work on handling streaming data is by Manku and Motwani [15]. They have also presented a one pass algorithm that does not allow false negatives, and has a provable bound on false positives. They achieve this through a very different approach, called lossy counting. The differences in the two approaches are in space requirements. For finding frequent items, the approach we use takes 0(1/B) space. Their approach requires O((l/B)log(BN)) space, where 
In this paper, we have developed a new approach for fre-
Our detailed experimental evaluation has shown the follow-----
