 The area of distrib uted computing systems pro vides a promising domain for applications of machine learning methods. One of the most interesting aspects of such applications is that learning algorithms that are embedded in a distrib uted computing infrastructure are themselv es part of that infrastructure and must respect its inherent local computing constraints (e.g., constraints on bandwidth, latenc y, reliability , etc.), while attempting to aggre gate information across the infrastructure so as to impro ve system performance (or availability) in a global sense.
 Consider , for example, the problem of detecting anomalies in a wide-area netw ork. While it is straightforw ard to embed learning algorithms at local nodes to attempt to detect node-le vel anoma-lies, these anomalies may not be indicati ve of netw ork-le vel problems. Indeed, in recent work, [8] demonstrated a useful role for Principal Component Analysis (PCA) to detect netw ork anomalies. The y sho wed that the minor components of PCA (the subspace obtained after remo ving the compo-nents with lar gest eigen values) revealed anomalies that were not detectable in any single node-le vel trace. This work assumed an environment in which all the data is continuously pushed to a central site for off-line analysis. Such a solution cannot scale either for netw orks with a lar ge number of monitors nor for netw orks seeking to track and detect anomalies at very small time scales. Designing scalable solutions presents several challenges. Viable solutions need to process data  X in-netw ork X  to intelligently control the frequenc y and size of data communications. The key underlying problem is that of developing a mathematical understanding of how to trade off quantization arising from local data ltering against delity of the detection analysis. We also need to understand how this tradeof f impacts overall detection accurac y. Finally , the implementation needs to be simple if it is to have impact on developers.
 In this paper , we present a simple algorithmic frame work for netw ork-wide anomaly detection that relies on distrib uted tracking combined with approximate PCA analysis, together with supporting theoretical analysis. In brief, the architecture involv es a set of local monitors that maintain parame-terized sliding lters. These sliding lters yield quantized data streams that are sent to a coordinator . The coordinator mak es global decisions based on these quantized data streams. We use stochastic matrix perturbation theory to both assess the impact of quantization on the accurac y of anomaly detection, and to design a method that selects lter parameters in a way that bounds the detection error . The combination of our theoretical tools and local ltering strate gies results in an in-netw ork tracking algorithm that can achie ve high detection accurac y with low communication overhead; for instance, our experiments sho w that, by choosing a relati ve eigen-error of 1 : 5% (yielding, approxi-mately , a 4 % missed detection rate and a 6 % false alarm rate), we can lter out more than 90 % of the traf c from the original signal.
 Prior Work. The original work on a PCA-based method by Lakhina et al. [8] has been extended by [17 ], who sho w how to infer netw ork anomalies in both spatial and temporal domains. As with [8], this work is completely centralized. [14 ] and [1] propose distrib uted PCA algorithms distrib uted across blocks of rows or columns of the data matrix; howe ver, these methods are not applicable to our case. Furthermore, neither [14 ] nor [1] address the issue of continuously tracking principal components within a given error tolerance or the issue of implementing a communication/accurac y tradeof f; issues which are the main focus of our work. Other initiati ves in distrib uted monitoring, proling and anomaly detection aim to share information and foster collaboration between widely distrib uted monitoring box es to offer impro vements over isolated systems [12 , 16]. Work in [2, 10] posits the need for scalable detection of netw ork attacks and intrusions. In the setting of simpler statistics such as sums and counts, in-netw ork detection methods related to ours have been explored by [6]. Finally , recent work in the machine learning literature considers distrib uted constraints in learning algorithms such as kernel-based classication [11 ] and graphical model inference [7]. (See [13 ] for a surv ey). We consider a monitoring system comprising a set of local monitor nodes M 1 ; : : : ; M n , each of which collects a locally-observ ed time-series data stream (Fig. 1(a)). For instance, the monitors may collect information on the number of TCP connection requests per second, the number of DNS transactions per minute, or the volume of traf c at port 80 per second. A central coor dinator node aims to continuously monitor the global collection of time series, and mak e global decisions such as those concerning matters of netw ork-wide health. Although our methodology is generally applicable, in this paper we focus on the particular application of detecting volume anomalies . A volume anomaly refers to unusual traf c load levels in a netw ork that are caused by anomalies such as worms, distrib uted denial of service attacks, device failures, miscongurations, and so on. Each monitor collects a new data point at every time step and, assuming a nai ve,  X continuous push X  protocol, sends the new point to the coordinator . Based on these updates, the coordinator keeps track of a sliding time windo w of size m (i.e., the m most recent data points) for each monitor time series, organized into a matrix Y of size m n (where the i th column Y i captures the data from monitor i , see Fig. 1(a)). The coordinator then mak es its decisions based solely on this (global) Y matrix. In the netw ork-wide volume anomaly detection algorithm of [8] the local monitors measure the total volume of traf c (in bytes) on each netw ork link, and periodically (e.g., every 5 minutes) centralize the data by pushing all recent measurements to the coordinator . The coordinator then performs PCA on the assembled Y matrix to detect volume anomalies. This method has been sho wn to work remarkably well, presumably due to the inherently low-dimensional nature of the underlying data [9]. Ho we ver, such a  X periodic push X  approach suf fers from inherent limitations: To ensure fast detection, the update periods should be relati vely small; unfortunately , small periods also imply increased monitoring communication overheads, which may very well be unnecessary (e.g., if there are no signicant local changes across periods). Instead, in our work, we study how the monitors can effecti vely lter their time-series updates, sending as little data as possible, yet enough so as to allo w the coordinator to mak e global decisions accurately . We pro vide analytical bounds on the errors that occur because decisions are made with incomplete data, and explore the tradeof f between reducing data transmissions (communication overhead) and decision accurac y.
 Using PCA for centralized volume anomaly detection. As observ ed by Lakhina et al. [8], due to the high level of traf c aggre gation on ISP backbone links, volume anomalies can often go unno-ticed by being  X buried X  within normal traf c patterns (e.g., the circle dots sho wn in the top plot in Fig 1(b)). On the other hand, the y observ e that, although, the measured data is of seemingly high dimensionality ( n = number of links), normal traf c patterns actually lie in a very low-dimensional subspace; furthermore, separating out this normal traf c subspace using PCA (to nd the principal traf c components) mak es it much easier to identify volume anomalies in the remaining subspace (bottom plot of Fig. 1(b)).
 As before, let Y be the global m n time-series data matrix, centered to have zero mean, and let y = y ( t ) denote a n -dimensional vector of measurements (for all links) from a single time step t . Formally , PCA is a projection method that maps a given set of data points onto principal compo-nents ordered by the amount of data variance that the y capture. The set of n principal components, f v and are the n eigen vectors of the estimated covariance matrix A := 1 PCA reveals that the Origin-Destination (OD) o w matrices of ISP backbones have low intrinsic dimensionality: For the Abilene netw ork with 41 links, most data variance can be captured by the rst k = 4 principal components. Thus, the underlying normal OD o ws effecti vely reside in a (lo w) k -dimensional subspace of R n . This subspace is referred to as the normal traf c subspace S no . The remaining ( n k ) principal components constitute the abnormal traf c subspace S ab . Detecting volume anomalies relies on the decomposition of link traf c y = y ( t ) at any time step into normal and abnormal components, y = y no + y ab , such that (a) y no corresponds to modeled normal traf c (the projection of y onto S no ), and (b) y ab corresponds to residual traf c (the projection of y onto S ab ). Mathematically , y no ( t ) and y ab ( t ) can be computed as where P = [ v 1 ; v 2 ; : : : ; v k ] is formed by the rst k principal components which capture the dom-inant variance in the data. The matrix C no = PP T represents the linear operator that performs projection onto the normal subspace S no , and C ab projects onto the abnormal subspace S ab . As observ ed in [8], a volume anomaly typically results in a lar ge change to y ab ; thus, a useful metric for detecting abnormal traf c patterns is the squared prediction error (SPE): (essentially , a quadratic residual function). More formally , their proposed algorithm signals a vol-ume anomaly if SPE &gt; Q , where Q denotes the threshold statistic for the SPE residual function at the 1 condence level. Such a statistical test for the SPE residual function, kno wn as the Q -statistic [4], can be computed as a function Q = Q ( k +1 ; : : : ; n ) of the ( n k ) non-principal eigen values of the covariance matrix A . We now describe our version of an anomaly detector that uses distrib uted tracking and approximate PCA analysis. A key idea is to curtail the amount of data each monitor sends to the coordinator . Because our job is to catch anomalies, rather than to track ongoing state, we point out that the coordinator only needs to have a good approximation of the state when an anomaly is near . It need not track global state very precisely when conditions are normal. This observ ation mak es it intuiti ve that a reduction in data sharing between monitors and the coordinator should be possible. We curtail the amount of data o w from monitors to the coordinator by installing local lter s at each monitor . These lters maintain a local constr aint , and a monitor only sends the coordinator an update of its data when the constraint is violated. The coordinator thus recei ves an approximate, or  X perturbed,  X  vie w of the data stream at each monitor and hence of the global state. We use stochastic matrix perturbation theory to analyze the effect on our PCA-based anomaly detector of using a perturbed global matrix. Based on this, we can choose the ltering parameters (i.e., the local constraints) so as to limit the effect of the perturbation on the PCA analysis and on any deterioration in the anomaly detector' s performance. All of these ideas are combined into a simple, adapti ve distrib uted protocol. 3.1 Ov erview of our appr oach Fig. 2 illustrates the overall architecture of our system. We now describe the functionality at the monitors and the coordinator . The goal of a monitor is to track its local raw time-series data, and to decide when the coordinator needs an update. Intuiti vely , if the time series does not change much, or doesn' t change in a way that affects the global condition being track ed, then the monitor does not send anything to the coordinator . The coordinator assumes that the most recently recei ved update is still approximately valid. The update message can be either the current value of the time series, or a summary of the most recent values, or any function of the time series. The update serv es as a prediction of the future data, because should the monitor send nothing in subsequent time interv als, then the coordinator uses the most recently recei ved update to predict the missing values. For our anomaly detection application, we lter as follo ws. Each monitor i maintains a ltering sends nothing. The windo w parameter i is called the slac k ; it captures the amount the time series can drift before an update to the coordinator needs to be sent. The center parameter R i ( t ) denotes the approximate representation, or summary , of Y i ( t ) . In our implementation, we set R i ( t ) equal to the average of last ve signal values observ ed locally at monitor i . Let t denote the time of the most recent update happens. The monitor needs to send both Y i ( t ) and R i ( t ) to the coordinator when it does an update, because the coordinator will use Y i ( t ) at time t and R i ( t ) for all t &gt; t until the next update arri ves. For any subsequent t &gt; t when the coordinator recei ves no update from that monitor , it will use R i ( t ) as the prediction for Y i ( t ) .
 The role of the coordinator is twofold. First, it mak es global anomaly-detection decisions based upon the recei ved updates from the monitors. Secondly , it computes the ltering parameters (i.e., the slacks i ) for all the monitors based on its vie w of the global state and the condition for triggering an anomaly . It gives the monitors their slacks initially and updates the value of their slack parameters when needed. Our protocol is thus adapti ve. Due to lack of space we do not discuss here the method for deciding when slack updates are needed. The global detection task is the same as in the centralized scheme. In contrast to the centralized setting, howe ver, the coordinator does not have an exact version of the raw data matrix Y ; it has the approximation ^ Y instead. The PCA analysis, including the computation of S ab is done on the perturbed covariance matrix ^ A := A . The magnitude of the perturbation matrix is determined by the slack variables i ( i = 1 ; : : : ; M ). 3.2 Selection of ltering parameters A key ingredient of our frame work is a practical method for choosing the slack parameters i . This choice is critical because these parameters balance the tradeof f between the savings in data commu-nication and the loss of detection accurac y. Clearly , the lar ger the slack, the less the monitor needs to send, thus leading to both more reduction in communication overhead and potentially more in-formation loss at the coordinator . We emplo y stoc hastic matrix perturbation theory to quantify the effects of the perturbation of a matrix on key quantities such as eigen values and the eigen-subspaces, which in turn affect the detection accurac y.
 Our approach is as follo ws. We measure the size of a perturbation using a norm on . We deri ve an upper bound on the changes to the eigen values i and the residual subspace C ab as a function of k k . We choose i to ensure that an approximation to this upper bound on is not exceeded. This in turn ensures that i and C ab do not exceed their upper bounds. Controlling these latter terms, we are able to bound the false alarm probability .
 Recall that the coordinator' s vie w of the global data matrix is the perturbed matrix ^ Y = Y + W , where all elements of the column vector W i are bounded within the interv al [ i ; i ] . Let i and ^ i ( i = 1 ; : : : ; n ) denote the eigen values of the covariance matrix A = 1 m Y T Y and its perturbed version ^ A := 1 on the eigen value perturbation in terms of the Frobenius norm k : k F and the spectral norm k : k 2 of Applying the sin theorem and results on bounding the angle of projections to subspaces [15 ] (see [3] for more details), we can bound the perturbation of the residual subspace C ab in terms of the Frobenius norm of : where denotes the eigengap between the k th and ( k +1) th eigen values of the estimated covariance matrix ^ A .
 To obtain practical (i.e., computable) bound on the norms of , we deri ve expectation bounds instead of worst case bounds. We mak e the follo wing assumptions on the error matrix W : Note that the independence assumption is imposed only on the error  X this by no means implies that the signals recei ved by dif ferent monitors are statistically independent. Under the abo ve assumption, we can sho w that k k F = p n is upper bounded in expectation by the follo wing quantity: Similar results can be obtained for the spectral norm as well. In practice, these upper bounds are very tight because 1 ; : : : ; n tend to be small compared to the top eigen values. Given the tolerable perturbation T ol F , we can use Eqn. (3) to select the slack variables. For example, we can divide the overall tolerance across monitors either uniformly or in proportion to their observ ed local variance. 3.3 Guarantee on false alarm probability Because our approximation perturbs the eigen values, it also impacts the accurac y with which the trigger is red. Since the trigger condition is k C ab y k 2 &gt; Q , we must assess the impact on both of these terms. We can compute an upper bound on the perturbation of the SPE statistic, SPE = k C ab y k 2 , as follo ws. First, note that jk ^
C ab ^ y k k C ab y kj k ( ^ C ab C ab ) ^ y k + k C ab ( y ^ y ) k The dependenc y of the threshold Q on the eigen values, k +1 ; : : : ; n , can be expressed as [4]: where c is the (1 ) -percentile of the standard normal distrib ution, h 0 = 1 2 1 3 P To assess the perturbation in false alarm probability , we start by considering the follo wing random variable c deri ved from Eqn. (5) : The random variable c essentially normalizes the random quantity k C ab y k 2 and is kno wn to ap-proximately follo w a standard normal distrib ution [5]. The false alarm probability in the centralized system is expressed as where the lefthand term of this equation is conditioned upon the SPE statistics being inside the normal range. In our distrib uted setting, the anomaly detector res a trigger if k ^ C ab ^ y k 2 &gt; ^ Q . We thus only observ e a perturbed version ^ c for the random variable c . Let c denote the bound on j ^ c c j . The deviation of the false alarm probability in our approximate detection scheme can then be approximated as P ( c c &lt; U &lt; c + c ) , where U is a standard normal random variable. We implemented our algorithm and developed a trace-dri ven simulator to validate our methods. We used a one-week trace collected from the Abilene netw ork 1 . The traces contains per -link traf c loads measured every 10 minutes, for all 41 links of the Abilene netw ork. With a time unit of 10 minutes, data was collected for 1008 time units. This data was used to feed the simulator . There are 7 anomalies in the data that were detected by the centralized algorithm (and veried by hand to be true anomalies). We also injected 70 synthetic anomalies into this dataset using the method described in [8], so that we would have suf cient data to compute error rates. We used a threshold Q corresponding to an 1 = 99 : 5% condence level. Due to space limitations, we present results only for the case of uniform monitor slack, i = .
 The input parameter for our algorithm is the tolerable relati ve error of the eigen values ( X relati ve eigen-error X  for short), which acts as a tuning knob . (Precisely , it is T ol F = q 1 is dened in Eqn. (3) .) Given this parameter and the input data we can compute the ltering slack for the monitors using Eqn. (3) . We then feed in the data to run our protocol in the simulator with the computed . The simulator outputs a set of results including: 1) the actual relati ve eigen errors and the relati ve errors on the detection threshold Q ; 2) the missed detection rate, false alarm rate and communication cost achie ved by our method. The missed-detection rate is dened as the fraction of missed detections over the total number of real anomalies, and the false-alarm rate as the fraction of false alarms over the total number of detected anomalies by our protocol, which is (dened in Sec. 3.3) rescaled as a rate rather than a probability . The communication cost is computed as the fraction of number of messages that actually get through the ltering windo w to the coordinator . the relationship between the relati ve eigen-error and the ltering slack when assuming ltering errors are uniformly distrib uted on interv al [ ; ] . With this model, the relationship between the relati ve eigen-error and the slack is determined by a simplied version of Eqn. (3) (with all 2 The results mak e intuiti ve sense. As we increase our error tolerance, we can lter more at the monitor and send less to the coordinator . The slack increases almost linearly with the relati ve eigen-error because the rst term in the right hand side of Eqn. (3) dominates all other terms.
 In Fig. 3(b) we compare the relati ve eigen-error to the actual accrued relati ve eigen-error (dened as as computed by our coordinator . We can see that the real accrued eigen-errors are always less than the tolerable eigen errors. The plot sho ws a tight upper bound, indicating that it is safe to use our model' s deri ved ltering slack . In other words, the achie ved eigen-error always remains belo w the requested tolerable error specied as input, and the slack chosen given the tolerable error is close to being optimal. Fig. 3(c) sho ws the relationship between the relati ve eigen-error and the relati ve error of detection threshold Q 2 . We see that the threshold for detecting anomalies decreases as we tolerate more and more eigen-errors. In these experiments, an error of 2% in the eigen values leads to an error of approximately 6% in our estimate of the appropriate cutof f threshold.
 We now examine the false alarm rates achie ved. In Fig. 3(d) the curv e with triangles represents the upper bound on the false alarm rate as estimated by the coordinator . The curv e with circles is the actual accrued false alarm rate achie ved by our scheme. Note that the upper bound on the false alarm rate is fairly close to the true values, especially when the slack is small. The false alarm rate increases with increasing eigen-error because as the eigen-error increases, the corresponding detection threshold Q will decrease, which in turn causes the protocol to raise an alarm more often. (If we had plotted ^ Q rather than the relati ve threshold dif ference, we would obviously see a decreasing ^ Q with increasing eigen-error .) We see in Fig. 3(e) that the missed detection rates remain belo w 4% for various levels of communication overhead.
 The communication overhead is depicted in Fig. 3(f). Clearly , the lar ger the errors we can tolerate, the more overhead can be reduced. Considering these last three plots (d,e,f) together , we observ e several tradeof fs. For example, when the relati ve eigen-error is 1 : 5% , our algorithm reduces the data sent through the netw ork by more than 90%. This gain is achie ved at the cost of approximately a 4% missed detection rate and a 6% false alarm rate. This is a lar ge reduction in communication for a small increase in detection error . These initial results illustrate that our in-netw ork solution can dramatically lower the communication overhead while still achie ving high detection accurac y. We have presented a new algorithmic frame work for netw ork anomaly detection that combines dis-trib uted tracking with PCA analysis to detect anomalies with far less data than pre vious methods. The distrib uted tracking consists of local lters, installed at each monitoring site, whose parameters are selected based upon global criteria. The idea is to track the local monitoring data only enough so as to enable accurate detection. The local ltering reduces the amount of data transmitted through the netw ork but also means that anomaly detection must be done with limited or partial vie ws of the global state. Using methods from stochastic matrix perturbation theory , we pro vided an analysis for the tradeof f between the detection accurac y and the data communication overhead. We were able to control the amount of data overhead using the the relati ve eigen-error as a tuning knob . To the best of our kno wledge, this is the rst result in the literature that pro vides upper bounds on the false alarm rate of netw ork anomaly detection.

