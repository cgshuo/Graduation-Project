 The performance of many machine learning algorithms depends on a good metric that reflects the relationship between the data in the input space. In the classical Vector Space Model (VSM), each text document is represented as vector of terms. These vectors define an input space where each distinct term represents an axis of the space. The cosine similarity defined in this space can give an effective approximation of similarities between text documents. However, cosine similarity fails to discover the semantic relationship of terms such as synonymy and polysemy.
 assumed between terms whose occurrence patterns in the documents of corpus are correlated. In this paper, we extend the similar idea as [1], and try to get a more semantic kernel K , so that x T Ky can better than x T y through the analysis of the term co-occurrences. Considering that when the training set is small, we introduce a supervised method to adjust the weight between the terms to improve the construction of term affinity matrix, called Supervised Affinity Construction. However, the affinity matrix will still have noises. We apply clustering to the term affinity matrix to decrease the noise and get semantic  X  X mall worlds X  [1], which may discover semantic relationships. The final similarity matrix of terms is gained through diffusion kernels [3]. Experiments results show that the proposed method is better than the cosine similarity with either Nearest Neighbor classifier or SVM classifier, and both the clustering and supervised affinity construction contribute to the improvement. Humans write articles using terms, so terms can be regarded as the basic element of text documents.
 tionship with each other, and based on the cosine similarity, the inner product of  X  X ouse X  and  X  X uilding X  will lead to zero, even though they really have the similar meaning. In figure 1, this relationship can be represented by a linked graph and how to measure this relationship is introduced in section 3. 2.1 Unsupervised Term Affinity Matrix Construction via Given a collection of text documents D, d j  X  D, 1  X  j  X  n, | D | = n and a word vocabulary T, t i  X  T, 1  X  i  X  m, | T | = m , a text document can be represented as d j =[ w ij ] T 1  X  i  X  m based on a Boolean model [5], where w ij =1 or 0 means whether the term t i occurs in the text document d j or not. The term-document matrix is defined as B =[ d j ] 1  X  j  X  n . As we can see, a text document d j will contribute to a part of the term affinity matrix G as G j = d j d T j .So G is gained by formulation 1 2.2 Supervised Term Affinity Matrix Construction Until now, the term affinity matrix construction process is still an unsuper-vised technique. When the training set is small, the statistical characteristic of co-occurrence information may be lost. In order to incorporate the supervised information, we have treated the document in the same class as a  X  X uge X  doc-ument. So, there will be only | C |  X  X uge X  text document in our problem ( | C | is the number of classes). Another view is that the class label can be treated as  X  X ement X  in Figure 1, and it connects the two terms together. In order to add more discriminative information, we modify the weight of each pair in the  X  X uge X  documents the like follows. The assumption here is that the fewer classes matrix. Based on G obtained by formulation 1, its element g ij can be modified as follows where k ij is the number of classes in which the term pair ( t i ,t j ) occurs together, and I ij is the indicating function to represent whether k ij is above zero or not. This leads to our supervised affinity matrix construction. In this formulation,  X  controls the amounts to which the supervised information should be incorpo-rated. Typically,  X  = 0 will lead to the original unsupervised method. 3.1 Clustering as a Denoising Process Since some term pairs happen to be in the same document without any re-lationship, a clustering step is needed to eliminate noises. Those terms highly connected will probably fall into the same cluster, and the noisy links will be cut off. It is easy to transform the term affinity matrix into a graph ( V, E )by taking the each term as a node and connecting each pair of terms by an edge. The weight on that edge should reflect the likelihood that two terms belong to one cluster. Based on the term affinity matrix G in section 2, we can define the graph edge weight connecting two nodes i and j as ( W =[ w ij ] i  X  j ) where g m = max( g ij ) i = j ,and  X &gt; 0.
 algorithm [6] proposed for image segmentation by solving an eigenvalue problem is adopted. A recursive two-way partition will partition the graph into more pieces or clusters. After the clustering, some elements of W will become zero to indicate that there is no link among the corresponding clusters. 3.2 Term Similarity Measure Through Diffusion Kernels Diffusion kernel is a method to generate valid kernel on graphs [3]. Suppose the K is the kernel matrix, the inner product of two documents becomes H is called the Laplacian matrix. First diagonalize H = P T DP , which is always possible because H is symmetric. Then compute where  X  is a positive decay factor. L = e  X D/ 2 P gives us the diffusion kernel representation. Another advantage of clustering is that the computation of for-mulation 7 is reduced. Thus, let y = L x , the similarity of two documents in the feature space becomes 4.1 Experiment Settings Two well-known data sets Reuters-21578 1 (Reuters) and 20 Newsgroups 2 are used in our experiments. For Reuters-21578, we only chose the most frequent 25 topics, about 9,000 documents and for 20 Newsgroups, we chose the five classes about computer, about 5,000 documents as our experiment data set. Stop words and punctuation were removed from the documents and the Porter stemmer was applied to the terms. The terms in the documents were weighted according to the widely used tfidf scheme. Two classifiers, nearest neighbor and support vector machine, are used in our experiments. We use F1 measure [4] to evaluate the results. 4.2 Experiment Results We present results pertaining to two experiments. In the first experiments, we compare the term-clustering based (unsupervised) methods against traditional inner product under classifier Nearest Neighbor and SVM. In the second exper-iment, we compare supervised term affinity method against unsupervised term affinity and traditional methods when the size of training data varies. training and the rest for testing. The number of the term clusters varies from 1 to more than 2000. Figure 2 and 3 show the results under Nearest Neighbor and SVM, respectively and the axis of the number of clusters is represented as a logarithmic scale. We use TIP and TCB to refer to T raditional I nner P roduct method and our T erm C lustering B ased method, respectively. (Note: TIP has no relationship with the number of clusters, just for the ease of comparison). For SVM, the baseline is the result using the linear kernel. This experiment was conducted on Retuers-21578 data set. From the figure 2, we see that our TCB outperforms the TIP most of the time under classifiers: Nearest Neighbor and SVM, and the best improvement is above 4%. We find that clustering is an importance step for our approach; when the number of cluster is too small, the performance is worse than the TIP because of the noisy edges in the graph. Typically, when the number of clusters is only 1 (no clustering), the performance has been reduced about 2%  X  3%. The experiments result of Nearest Neighbor is not sensitive to the number of clusters in a large range, since the most of the noisy edges in the graph are cut off at the beginning. TCB Macro-F1 of SVM is a little sensitive. One of the reasons may be that the choice of parameters of SVM in our experiment is not very appropriate.
 affinity construction. We use STCB to represent S upervised T erm affinity con-struction combined C lustering B ased method. The number of term clusters and the parameter  X  in formulation 2 were both well tuned in our experiment. We selected the Nearest Neighbor as the classifier. This experiment was conducted on both Retuers-21578 and 20 Newsgroups data sets. Figure 3 shows the results on Retuers-21578 and Figure 4 on 20 Newsgroup (we present the results of 20 Newsgroup separately because the curves of Micro-F1 and Macro-F1 overlap too much).
 STCB performs better than TCB and TCB performs better than TIP, and the best improvement is about 5%. The supervised information does help giving pos-itive results in this situation. When the training data set is large ( &gt; 60%  X  70%), STCB performs equally to TCB, both better than TIP. Actually, the parameter  X  in formulation 2 is zero now, so STCB has become TCB. The supervised infor-mation does little help when the training data is enough. In this case, the train-ing data can already give reasonable statistical characteristic of co-occurrence information. In this paper, we introduce a term clustering based kernel to enhance the per-formance of text classification. The term affinity matrix is constructed via the co-occurrence of the terms in both unsupervised and supervised ways. Normal-ized cut is used to cluster the terms by cutting off the noisy edges. The final kernel is generated through the exponential matrix operation of diffusion ker-nel. Experiment result shows our proposed method can explore the semantic relationship between terms effectively.
 We would like to express our special appreciation to Ni Lao, and Lu Wang for their insightful suggestions.

