 Sparse and overcomplete image models were first introduced i n [1] for modeling the spatial recep-tive fields of simple cells in the human visual system. The lin ear decomposition of a signal using a few atoms of a learned dictionary, instead of predefined ones X  X uch as wavelets X  X a s recently led to state-of-the-art results for numerous low-level image pro cessing tasks such as denoising [2], show-ing that sparse models are well adapted to natural images. Un like principal component analysis decompositions, these models are in general overcomplete , with a number of basis elements greater than the dimension of the data. Recent research has shown tha t sparsity helps to capture higher-order correlation in data. In [3, 4], sparse decompositions are us ed with predefined dictionaries for face and signal recognition. In [5], dictionaries are learned fo r a reconstruction task, and the correspond-ing sparse models are used as features in an SVM. In [6], a discriminative method is introduced for various classification tasks, learning one dictionary p er class; the classification process itself is based on the corresponding reconstruction error, and does n ot exploit the actual decomposition co-efficients. In [7], a generative model for documents is learn ed at the same time as the parameters of a deep network structure. In [8], multi-task learning is per formed by learning features and tasks are selected using a sparsity criterion. The framework we prese nt in this paper extends these approaches by learning simultaneously a single shared dictionary as we ll as models for different signal classes in a mixed generative and discriminative formulation (see a lso [9], where a different discriminative term is added to the classical reconstructive one). Similar joint generative/discriminative frame-works have started to appear in probabilistic approaches to learning, e.g., [10, 11, 12, 13, 14], and in neural networks [15], but not, to the best of our knowledge , in the sparse dictionary learning framework. Section 2 presents a formulation for learning a d ictionary tuned for a classification task, which we call supervised dictionary learning, and Section 3 its interpretation in term of probabil-ity and kernel frameworks. The optimization procedure is de tailed in Section 4, and experimental results are presented in Section 5. We present in this section the core of the proposed model. In c lassical sparse coding tasks, one con-the dictionary overcomplete). In this setting, sparse codi ng with an  X  1 regularization 1 amounts to computing It is well known in the statistics, optimization, and compre ssed sensing communities that the  X  1 penalty yields a sparse solution, very few non-zero coeffici ents in  X  , although there is no explicit analytic link between the value of  X  1 and the effective sparsity that this model yields. Other spa rsity penalties using the  X  0 regularization 2 can be used as well. Since it uses a proper norm, the  X  1 formulation of sparse coding is a convex problem, which make s the optimization tractable with algorithms such as those introduced in [16, 17], and has prov en in practice to be more stable than its  X  counterpart, in the sense that the resulting decomposition s are less sensitive to small perturbations of the input signal x . Note that sparse coding with an  X  0 penalty is an NP-hard problem and is often approximated using greedy algorithms.
 In this paper, we consider a setting, where the signal may bel ong to any of p different classes. We first consider the case of p = 2 classes and later discuss the multiclass extension. We cons ider a Our goal is to learn jointly a single dictionary D adapted to the classification task and a function f which should be positive for any signal in class +1 and negative otherwise. We consider in this paper two different models to use the sparse code  X  for the classification task: (i) linear in  X  : f ( x ,  X  ,  X  ) = w T  X  + b , where  X  = { w  X  R k , b  X  R } parametrizes the model. (ii) bilinear in x and  X  : f ( x ,  X  ,  X  ) = x T W  X  + b , where  X  = { W  X  R n  X  k , b  X  R } . In this case, the model is bilinear and f acts on both x and its sparse code  X  .
 The number of parameters in (ii) is greater than in (i), which allows for richer models. Note that one can interpret W as a linear filter encoding the input signal x into a model for the coefficients  X  , which has a role similar to the encoder in [18] but for a discri minative task.
 A classical approach to obtain  X  for (i) or (ii) is to first adapt D to the data, solving Note also that since the reconstruction errors || x i  X  D  X  i || 2 2 are invariant to scaling simultaneously D by a scalar and  X  i by its inverse, we need to constrain the  X  2 norm of the columns of D . Such a constraint is classical in sparse coding [2]. This reconstr uctive approach (dubbed REC in this paper) provides sparse codes  X  i for each signal x i , which can be used a posteriori in a regular classifier such as logistic regression, which would require to solve where C is the logistic loss function ( C ( x ) = log(1 + e  X  x ) ), which enjoys properties similar to that of the hinge loss from the SVM literature, while being di fferentiable, and  X  2 is a regularization parameter, which prevents overfitting. This is the approach chosen in [5] (with SVMs). However, our goal is to learn jointly D and the model parameters  X  . To that effect, we propose the formulation where  X  0 controls the importance of the reconstruction term, and the loss for a pair ( x i , y i ) is In this setting, the classification procedure of a new signal x with an unknown label y , given a learned dictionary D and parameters  X  , involves supervised sparse coding : responds to a generative model. We will refer later to this mo del as SDL-G (supervised dictionary learning, generative). Note the explicit incorporation of the reconstructive and discriminative com-ponent into sparse coding, in addition to the classical reco nstructive term (see [9] for a different classification component).
 However, since the classification procedure from Eq. (6) com pares the different costs S  X  ( x , D ,  X  , y ) of a given signal for each class y =  X  1 , +1 , a more discriminative approach is to not only make S  X  ( x i , D ,  X  , y i ) , which is the purpose of the logistic loss function C . This leads to: As detailed below, this problem is more difficult to solve tha n (4), and therefore we adopt instead a mixed formulation between the minimization of the generati ve Eq. (4) and its discriminative version (7), (see also [13]) X  X hat is, where controls the trade-off between the reconstruction from Eq. (4) and the discrimination from Eq. (7). This is the proposed generative/discriminative mo del for sparse signal representation and classification from learned dictionary D and model  X  . We will refer to this mixed model as SDL-D , (supervised dictionary learning, discriminative). Note a lso that, again, we constrain the norm of the columns of D to be less than or equal to one.
 All of these formulations admit a straightforward multicla ss extension, using softmax discriminative function, and learning one model  X  i per class. Other possible approaches such as one-vs-all or one-vs-one are of course possible, and the question of choos ing the best approach among these possibilities is still open. Compared with earlier work usi ng one dictionary per class [6], our model has the advantage of letting multiple classes share some fea tures, and uses the coefficients  X  of the sparse representations as part of the classification pro cedure, thereby following the works from [3, 4, 5], but with learned representations optimized for th e classification task similar to [9, 10]. Before presenting the optimization procedure, we provide b elow two interpretations of the linear and bilinear versions of our formulation in terms of a probab ilistic graphical model and a kernel. 3.1 A probabilistic interpretation of the linear model Let us first construct a graphical model which gives a probabi listic interpretation to the training and classification criteria given above when using a linear mode l with zero bias (no constant term) on the coefficients X  X hat is, f ( x ,  X  ,  X  ) = w T  X  . It consists of the following components (Figure 1):  X  The matrices D and the vector w are parameters of the problem, with a Gaussian prior on w , are considered independent of each other.  X  The coefficients  X  i are latent variables with a Laplace prior, p (  X  i )  X  e  X   X  1 ||  X  i || 1 .  X  The signals x i are generated according to a Gaussian probability distribu tion conditioned on D  X  The labels y i are generated according to a probability distribution cond itioned on w and  X  i , and (  X  i , x i , y i ) are independent.
 What is commonly called  X  X enerative training X  in the literat ure (e.g., [12, 13]), amounts to finding the maximum likelihood estimates for D and w according to the joint distribution tively. It can easily be shown (details omitted due to space l imitations) that there is an equiva-lence between this generative training and our formulation in Eq. (4) under MAP approximations. 3 Although joint generative modeling of x and y through a shared representation has shown great promise [10], we show in this paper that a more discriminativ e approach is desirable.  X  X iscrim-respect to D and w : Given some input data, one finds the best parameters that wil l predict the labels of the data. The same kind of MAP approximation relates this d iscriminative training formulation to the discriminative model of Eq. (7) (again, details omitt ed due to space limitations). The mixed approach from Eq. (8) is a classical trade-off between gener ative and discriminative (e.g., [12, 13]), where generative components are often added to discriminat ive frameworks to add robustness, e.g., to noise and occlusions (see examples of this for the model in [9]). 3.2 A kernel interpretation of the bilinear model Our bilinear model with f ( x ,  X  ,  X  ) = x T W  X  + b does not admit a straightforward probabilistic interpretation. On the other hand, it can easily be interpre ted in terms of kernels: Given two signals x regression classifier amounts to finding a decision function of the same form as f . It is a product of two linear kernels, one on the  X   X  X  and one on the input signals x . Interestingly, Raina et al. [5] learn a dictionary adapted to reconstruction on a training s et, then train an SVM a posteriori on the decomposition coefficients  X  . They derive and use a Fisher kernel, which can be written as K simple experiments, which are not reported in this paper, we have observed that the kernel K , where the signals x replace the residuals r , generally yields a level of performance similar to K  X  and often actually does better when the number of training samples is s mall or the data are noisy. Classical dictionary learning techniques (e.g., [1, 5, 19] ), address the problem of learning a recon-structive dictionary D in R n  X  k well adapted to a training set, which is presented in Eq. (3). It can be seen as an optimization problem with respect to the dictio nary D and the coefficients  X  . Altough not jointly convex in ( D ,  X  ) , it is convex with respect to each unknown when the other one i s fixed. This is why block coordinate descent on D and  X  performs reasonably well [1, 5, 19], although not necessarily providing the global optimum. Training when = 0 (generative case), i.e., from Eq. (4), enjoys similar properties and can be addressed with the same optimization procedure. Equation (4) can be rewritten as: Block coordinate descent consists therefore of iterating b etween supervised sparse coding , where D and  X  are fixed and one optimizes with respect to the  X   X  X  and supervised dictionary update , where the coefficients  X  i  X  X  are fixed, but D and  X  are updated. Details on how to solve these two problems are given in sections 4.1 and 4.2. The discriminati ve version SDL-D from Eq. (7) is more problematic. To reach a local minimum for this difficult non-convex optimization problem, we have chosen a continuation method, starting from the generative case and ending with the discriminative one as in [6]. The algorithm is presented in Figure 2, and deta ils on the hyperparameters X  settings are given in Section 5. 4.1 Supervised sparse coding The supervised sparse coding problem from Eq. (6) ( D and  X  are fixed in this step) amounts to minimizing a convex function under an  X  1 penalty. The fixed-point continuation method (FPC) from (parameters); 0  X  1  X  2  X  . . .  X  m  X  1 (increasing sequence).
 Output: D  X  R n  X  k (dictionary);  X  (parameters).
 Initialization: Set D to a random Gaussian matrix with normalized columns. Set  X  to zero. Loop: For = 1 , . . . , m , [17] achieves good results in terms of convergence speed for this class of problems. For our specific problem, denoting by g the convex function to minimize, this method only requires  X  g and a bound on the spectral norm of its Hessian H g . Since the we have chosen models g which are both linear in  X  , there exists, for each supervised sparse coding problem, a vector a in R k and a scalar c in R such that and it can be shown that, if || U || 2 denotes the spectral norm of a matrix U (which is the magnitude of 2  X  0 || D T D || 2 . 4.2 Dictionary update The problem of updating D and  X  in Eq. (11) is not convex in general (except when is close to 0), but a local minimum can be obtained using projected gradient descent (as in the general literature on dictionary learning, this local minimum has experimentall y been found to be good enough in terms of classification performance). ). Denoting E ( D ,  X  ) the function we want to minimize in Eq. (11), we just need the partial derivatives of E with respect to D and the parameters  X  . When considering the linear model for the  X   X  X , f ( x ,  X  ,  X  ) = w T  X  + b , and  X  = { w  X  R k , b  X  R } , we obtain Partial derivatives when using our model with multiple clas ses or with the bilinear models f ( x ,  X  ,  X  ) = x T W  X  + b are not presented in this paper due to space limitations. We compare in this section the reconstructive approach, dub bed REC , which consists of learning a reconstructive dictionary D as in [5] and then learning the parameters  X  a posteriori; SDL with generative training (dubbed SDL-G ); and SDL with discriminative learning (dubbed SDL-D ). We also compare the performance of the linear ( L ) and bilinear ( BL ) models.
 Table 1: Error rates on the MNIST and USPS datasets in percent s for the REC , SDL-G L and SDL-D L approaches, compared with k-nearest neighbor and SVM with a Gaussian kernel [20]. Before presenting experimental results, let us briefly disc uss the choice of the five model parameters  X  ,  X  1 ,  X  2 , and k (size of the dictionary). Tuning all of them using cross-val idation is cumbersome and unnecessary since some simple choices can be made, some o f which can be made sequentially. We define first the sparsity parameter  X  =  X  1  X  When the input data points have unit  X  2 norm, choosing  X  = 0 . 15 was empirically found to be a good choice. For reconstructive tasks, a typical value often use d in the literature (e.g., [19]) is k = 256 for m = 100 000 signals. Nevertheless, for discriminative tasks, increas ing the number of parameters is likely to lead to overfitting, and smaller values like k = 64 or k = 32 are preferred. The scalar  X  2 is a regularization parameter for preventing the model to over fit the input data. As in logistic regression or support vector machines, this parameter is crucial when t he number of training samples is small. Performing cross validation with the fast method REC quickly provides a reasonable value for this parameter, which can be used afterward for SDL-G or SDL-D .
 Once  X  , k and  X  2 are chosen, let us see how to find  X  0 , which plays the important role of controlling the trade-off between reconstruction and discrimination. First, we perform cross-validation for a few iterations with = 0 to find a good value for SDL-G . Then, a scale factor making the costs S  X  dis-criminative for &gt; 0 can be chosen during the optimization process: Given a set of computed costs our experiments: Starting from small values for  X  0 and a fixed  X  , we apply the algorithm in Figure 2, and after a supervised sparse coding step, we compute the b est scale factor  X   X  , and replace  X  0 and  X  1 by  X   X   X  0 and  X  X  1 . Typically, applying this procedure during the first 10 iterations has proven to lead to reasonable values for these parameters. Since we a re following a continuation path from = 0 to = 1 , the optimal value of is found along the path by measuring the classification performance of the model on a validation set during the optim ization. 5.1 Digits recognition In this section, we present experiments on the popular MNIST [20] and USPS handwritten digit datasets. MNIST is composed of 70 000 28  X  28 images, 60 000 for training, 10 000 for testing, each of them containing one handwritten digit. USPS is composed o f 7291 training images and 2007 test images of size 16  X  16 . As is often done in classification, we have chosen to learn pa irwise binary classifiers, one for each pair of digits. Although our framew ork extends to a multiclass formulation, pairwise binary classifiers have resulted in slightly bette r performance in practice. Five-fold cross train three sets of pairwise classifiers. For a given image x , the test procedure consists of selecting the class which receives the most votes from the pairwise cla ssifiers. All the other parameters are obtained using the procedure explained above. Classificati on results are presented on Table 1 using the linear model. We see that for the linear model L , SDL-D L performs the best. REC BL offers a larger feature space and performs better than REC L , but we have observed no gain by using SDL-G BL or SDL-D BL instead of REC BL (this results are not reported in this table). Since the linear model is already performing very well, one side effec t of using BL instead of L is to increase the number of free parameters and thus to cause overfitting. N ote that our method is competitive since the best error rates published on these datasets (with out any modification of the training set) are 0 . 60% [18] for MNIST and 2 . 4% [21] for USPS, using methods tailored to these tasks, wherea s ours is generic and has not been tuned for the handwritten dig it classification domain. The purpose of our second experiment is not to measure the raw performance of our algorithm, but to answer the question  X  X re the obtained dictionaries D discriminative per se? X  . To do so, we have trained on the USPS dataset 10 binary classifiers, one per digit in a one vs all fashion on the training set. For a given value of , we obtain 10 dictionaries D and 10 sets of parameters  X  , learned by the SDL-D L model.
 To evaluate the discriminative power of the dictionaries D , we discard the learned parameters  X  and use the dictionaries as if they had been learned in a reconstr uctive REC model: For each dictionary, Figure 3: On the left, a reconstructive and a discriminative dictionary. On the right, average error rate in percents obtained by our dictionaries learned in a di scriminative framework ( SDL-D L ) for various values of , when used at test time in a reconstructive framework ( REC-L ). Table 2: Error rates for the texture classification task usin g various methods and sizes m of the training set. The last column indicates the gain between the error rate of REC BL and SDL-D BL . we decompose each image from the training set by solving the s imple sparse reconstruction problem from Eq. (1) instead of using supervised sparse coding. This provides us with some coefficients  X  , which we use as features in a linear SVM. Repeating the sparse decomposition procedure on the test set permits us to evaluate the performance of these lear ned linear SVMs. We plot the average error rate of these classifiers on Figure 3 for each value of . We see that using the dictionaries obtained with discrimative learning ( &gt; 0 , SDL-D L ) dramatically improves the performance of the basic linear classifier learned a posteriori on the  X   X  X , showing that our learned dictionaries are discriminative per se. Figure 3 also shows a dictionary adap ted to the reconstruction of the MNIST dataset and a discriminative one, adapted to  X  9 vs all X . 5.2 Texture classification In the digit recognition task, our bilinear framework did no t perform better than the linear one L . We believe that one of the main reasons is due to the simplicity o f the task, where a linear model is rich enough. The purpose of our next experiment is to answer the qu estion  X  X hen is BL worth using? X  . We have chosen to consider two texture images from the Brodat z dataset, presented in Figure 4, and to build two classes, composed of 12  X  12 patches taken from these two textures. We have compared the classification performance of all our methods, includin g BL , for a dictionary of size k = 64 and  X  = 0 . 15 . The training set was composed of patches from the left half o f each texture and the test sets of patches from the right half, so that there is no overla p between them in the training and test set. Error rates are reported in Table 2 for varying sizes of t he training set. This experiment shows that in some cases, the linear model performs very poorly whe re BL does better. Discrimination helps especially when the size of the training set is large. N ote that we did not perform any cross-validation to optimize the parameters k and  X  for this experiment. Dictionaries obtained with REC and SDL-D BL are presented in Figure 4. Note that though they are visually quite similar, they lead to very different performances. we have introduced in this paper a discriminative approach t o supervised dictionary learning that effectively exploits the corresponding sparse signal deco mpositions in image classification tasks, and have proposed an effective method for learning a shared dict ionary and multiple (linear or bilinear) models. Future work will be devoted to adapting the proposed framework to shift-invariant models that are standard in image processing tasks, but not readily generalized to the sparse dictionary learning setting. We are also investigating extensions to u nsupervised and semi-supervised learning and applications to natural image classification. Acknowledgments This paper was supported in part by ANR under grant MGA. Guill ermo Sapiro would like to thank Fernando Rodriguez for insights into the learning of discri minatory sparsity patterns. His work is partially supported by NSF, NGA, ONR, ARO, and DARPA.

