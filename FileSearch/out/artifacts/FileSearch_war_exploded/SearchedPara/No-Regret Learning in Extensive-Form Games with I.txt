 Martin Zinkevich 2 MAZ @ YAHOO -INC . COM
University of Alberta, Edmonton, Alberta, Canada T6G 2E8 Yahoo! Inc., Sunnyvale, CA, USA, 94089 Many real-world problems can be modeled as a repeated decision-making task. For problems involving multiple agents, one can model the repeated task as a normal-form game. When the task incorporates sequential decisions in-volving imperfect information or stochastic events, an ex-tensive game is a useful alternative. In such decision prob-lems, a typical goal is to minimize regret: the amount of utility lost by playing a past sequence of strategies, versus playing the best, stationary strategy in hindsight. In this paper, we consider the problem of minimizing regret in an extensive game. A common approach to achieving low regret in extensive games is the Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2008) algorithm. CFR uses a regret minimizer at every decision point with an alternative notion of regret, which provably minimizes regret in the entire extensive game. However, convergence is limited to games exhibiting perfect recall: players never forget information that was revealed to them, nor the or-der in the which the information was revealed. For games with imperfect recall, CFR X  X  original analysis provides no general guarantees.
 Imperfect recall brings about a number of complications. In games with perfect recall, every mixed strategy (probabil-ity distribution over pure strategies) has a utility-equivalent behavioral strategy (probability distribution over actions at each decision point) (Kuhn, 1953). While certain loss-less imperfect recall games share this property (Kaneko &amp; Kline, 1995), it is not true for imperfect recall games in general (Piccione &amp; Rubinstein, 1996). In addition, the de-cision problem of determining if a player can assure them-self a certain payoff in an imperfect recall game is NP-complete (Koller &amp; Megiddo, 1992). Two-player zero-sum games can be solved by constructing an appropriate linear program (Koller et al., 1994) or minimizing regret (Zinke-vich et al., 2008), provided the game has perfect recall. Without perfect recall, however, the problem becomes ex-ponential in the worst case (Koller et al., 1994). On the other hand, imperfect recall extensive games are more versatile than perfect recall games for modelling large real-world problems. While perfect recall requires all past information to be remembered, imperfect recall allows ir-relevant information to be forgotten so that the size of the game is smaller. As CFR X  X  memory requirements are lin-ear in the size of the game, more games become feasible through imperfect recall. Despite the complications above, CFR has empirically been shown to work well when ap-plied to imperfect recall abstractions of Texas Hold X  X m poker (Waugh et al., 2009b), but there is currently no theory to suggest why this is so.
 This paper presents theoretical groundings for applying CFR to games exhibiting imperfect recall. We define a gen-eral class of imperfect recall games and provide a bound on CFR X  X  regret in such games. For a subset of this class, CFR minimizes average regret in the extensive game. Moreover, our results also provide regret guarantees when applying CFR to an abstract game, provided the abstract game be-longs to our general class. We test our theory in three differ-ent domains: die-roll poker, phantom tic-tac-toe, and Bluff. To the best of our knowledge, this work demonstrates the first theoretically-grounded, practical use of imperfect re-call in extensive games. An extensive-form game  X  with imperfect information (Os-borne &amp; Rubinstein, 1994) is a tuple  X  N,A,H,Z,P, X  u, I X  , where N is a finite set of players . A is a finite set of actions . H is a finite set of histories : a subset of the set of sequences of elements in A . A prefix of a history h 0  X  H is a history h  X  H where h 0 begins with the se-quence h ; we denote prefix histories by h v h 0 . For ev-ery h  X  H , define A ( h ) = { a : a  X  A,ha  X  H } , the set of valid actions at history h ; P ( h )  X  N  X  X  c } is the player to act at the history h , or chance if P ( h ) = c ; and H i = { h | h  X  H,P ( h ) = i } . Z  X  H is the set of termi-nal histories . A terminal history z  X  Z is a history where there does not exist any history h  X  H , h 6 = z such that z v h . The utility function u i : Z  X  R gives the utility to player i  X  N at each terminal history. If | N | = 2 and for all z  X  Z , P i  X  N u i ( z ) = 0 , we say the game is zero-sum . For each player i  X  N , I i is a partition of H i with the property that A ( h ) = A ( h 0 ) whenever h and h 0 are in the same member of the partition. We call I i the information partition of player i , and a set I  X  I i is an information set for player i . A player, when taking actions, cannot dis-tinguish between two histories in the same information set. For I  X  X  i , we denote A ( I ) as the set A ( h ) for any h  X  I . Define I ( h ) to be the information set containing h . In this paper, we restrict ourselves to games where players can-not reach the same information set twice in a single game. Thus, we assume that for all i  X  N and h,h 0  X  H i , Furthermore,  X  c is the fixed  X  X trategy X  of the special player chance .  X  c ( h,a ) gives the probability that chance event a occurs at h . For all h  X  H c , P a  X  A ( h )  X  c ( h,a ) = 1 and the decisions at any h are independent of the decision at any other h 0 6 = h .
 Given a history h , define X i ( h ) to be the sequence of infor-mation set, action pairs such that ( I,a )  X  X i ( h ) if I  X  I and there exists h 0 v h such that h 0  X  I and h 0 a v h . The order of the pairs in X i ( h ) is the order in which they occur in h . Define X ( h ) to be the sequence of information set, action pairs belonging to all players in the order in which they occur in h , and X  X  i ( h ) similarly, by removing player i  X  X  information set, action pairs from X ( h ) . Also, define X ( h,h 0 ) to be the sequence of information set, action pairs belonging to all players that start at h and end at h 0 when h v h 0 ; if h 6v h 0 , X ( h,h 0 ) is defined to be the empty sequence. X i ( h,h 0 ) and X  X  i ( h,h 0 ) are similarly defined. Definition 1 An extensive game has perfect recall if for every player i  X  N , for every information set I  X  I i , for any h,h 0  X  I : X i ( h ) = X i ( h 0 ) . Otherwise, the game has imperfect recall .
 Intuitively, with perfect recall every player has an infallible memory: they cannot  X  X orget X  anything during a play of the game that they once knew. Hence, what a player knows at I is a composition of what the player has discovered in the past up to this point and the precise order in which infor-mation was discovered. Note that every perfect recall game satisfies equation (1), but not every imperfect recall game does.
 A (behavioral) strategy  X  i for player i is a function such that for each history h  X  H i ,  X  i ( h ) is a probability distri-bution over A ( h ) . Furthermore, it is required that  X  i  X  ( h 0 ) for all h,h 0  X  I , and we denote that as  X  i ( I ) . The set of all such strategies for player i is denoted by  X  i strategy profile  X   X   X  is a collection of strategies, one for each player, i.e. in a two-player game  X  = (  X  1 , X  2 By notational convention,  X   X  i refers to the set of strategies including every strategy in  X  except player i  X  X  strategy. For any  X   X   X  , i  X  N  X  { c } , and h  X  H , define  X  that player i plays to reach history h under  X  . We can then history h is reached under  X  . Let  X   X   X  i ( h ) be the product of all players X  contribution (including chance) except that of player i . Furthermore, let  X   X  i ( h,h 0 ) be the probability of player i playing to reach history h 0 after h , given h has oc-curred. Let  X   X  ( h,h 0 ) and  X   X   X  i ( h,h 0 ) be defined similarly. Finally, the expected utility of a strategy profile  X  for player i is We will say that a game  X  0 =  X  N,A 0 ,H,Z,P, X  c ,u, I 0  X  is an abstraction , or an abstract game , of  X  =  X  N,A,H,Z,P, X  c ,u, I X  if for all i  X  N and h,k  X  H i , A ( h )  X  A ( h ) , and I ( h ) = I ( k ) implies I 0 ( h ) = I this paper, we only consider abstractions where A = A 0 . A typical use of abstraction is to reduce the size of the game by ensuring that |I 0 | &lt; |I| .
 We now introduce a game that we will use as a running example throughout the paper.
 Die-roll poker (DRP) is a simplified two-player poker game that uses dice rather than cards. To begin, each player antes one chip to the pot . There are two betting rounds, where at the beginning of each round, players roll a pri-vate six-sided die. The game has imperfect information due to the players not seeing the result of the opponent X  X  die rolls. During a betting round, a player may fold (forfeit the game), call (match the current bet), or raise (increase the current bet) by a fixed number of chips, with a maxi-mum of two raises per round. In the first round, raises are worth two chips, whereas in the second round, raises are worth four chips. If both players have not folded by the end of the second round, a showdown occurs where the player with the largest sum of their two dice wins all of the chips in the pot.
 DRP is naturally a game with perfect recall; players re-member the exact sequence of bets made and the exact out-come of each die roll from both rounds. However, consider an imperfect recall version of DRP, DRP-IR , where at the beginning of the second round, both players forget their first die roll and only know the sum of their two dice. DRP-IR is an abstraction of DRP where any two histories are in the same abstract information set if and only if the sum of the player X  X  private dice is the same and the sequence of betting is the same. DRP-IR has imperfect recall since his-tories that were distinguishable in the first round (for exam-ple, a roll of 1 and a roll of 4) are no longer distinguishable in the second round (for example, a roll of 1 followed by a roll of 5, and a roll of 4 followed by a roll of 2). Given a sequence of strategy profiles  X  1 , X  2 ,..., X  (external) regret for player i , is the amount of utility player i could have gained had she played the best single strategy in hindsight for all time steps t  X  { 1 , 2 ,...,T } . An algorithm minimizes regret , or is a no-regret algorithm , for player i if the average positive regret approaches zero; i.e. , lim T  X  X  X  R T, + i /T = 0 , where x + = max { x, 0 } . Having no regret is a desirable property. For example, it is well known that in a zero-sum game, if both players X  average regret is bounded above by , then the average of the strategy profiles generated is a 2 -Nash equilibrium.
 Counterfactual Regret Minimization (CFR) is an itera-tive no-regret learning algorithm for extensive-form games having perfect recall. On each iteration t , CFR recursively traverses the entire game tree, computing the expected util-ity for player i at each information set I  X  I i under the current profile  X  t , assuming player i plays to reach I . This expectation is the counterfactual value for player i , where Z I is the set of terminal histories passing through I and z [ I ] is the prefix of z contained in I ( z [ I ] is unique by equation (1)). For each action a  X  A ( I ) , these val-ues determine the counterfactual regret at iteration t , r ( I,a ) = v i (  X  t I  X  a ,I )  X  v i (  X  t ,I ) , where  X  profile  X  except at I , action a is always taken. The regret r ( I,a ) measures how much player i would rather play ac-tion a at I than play  X  t . Finally,  X  t is updated by applying regret matching (Hart &amp; Mas-Colell, 2000) to the imme-diate counterfactual regrets , R T i ( I,a ) = P T t =1 r according to with actions chosen uniformly at random when the denom-inator is zero. Regret matching is a no-regret learner that minimizes the per-information set immediate counterfac-tual regret (Zinkevich et al., 2008), where  X  i = max z,z 0  X  Z u i ( z )  X  u i ( z 0 ) . In games having perfect recall, minimizing the immediate counterfactual re-grets at every information set in turn minimizes average regret, R T i /T . This is because perfect recall implies that the regret is bounded by the sum of the positive parts of the immediate counterfactual regrets (Zinkevich et al., 2008), and thus where | A i | = max I  X  X  i | A ( I ) | . CFR must store the imme-diate counterfactual regret for each information set, action pair, and thus CFR X  X  memory requirements are O ( |I i || A While equation (2) still holds in imperfect recall games, equation (3) and consequently equation (4) are not guar-anteed to hold. An example game where CFR would ex-hibit high regret is provided in Section 7. Consequently, the regret for playing according to the CFR algorithm is unknown in general for imperfect recall games. However, the advantage of applying CFR to DRP-IR, for example, is that this imperfect recall game contains fewer information sets than the full game, and thus less memory is required. Although DRP is a toy example and is small enough to run CFR on the full game, DRP is useful for understanding the concepts in the rest of this paper. In this section, we investigate the application of CFR to games with imperfect recall. We begin by showing that CFR minimizes regret for a class of games that we call  X  X ell-formed games. X  We then present a bound on the av-erage regret for a more general class of imperfect recall games that we call  X  X kew well-formed games. X  5.1. Well-formed Games For games  X  =  X  N,A,H,Z,P, X  c ,u, I X  and  X   X  =  X  N,A,H,Z,P, X  c ,u,  X  I X  , we say that  X   X  is a perfect recall refinement of  X  if  X   X  has perfect recall and  X  is an abstrac-tion of  X   X  . The information available to players in  X  forgotten, and is at least as informative as the information available to them in  X  . For example, DRP is a perfect recall refinement of DRP-IR. Every game has at least one perfect recall refinement by simply making  X   X  a perfect information call game is a perfect recall refinement of itself. For I  X  X  we define to be the set of all information sets in  X  I i that are subsets of I . Note that our notion of refinement is similar to the one described by Kaneko &amp; Kline (1995). Our definition dif-fers in that we consider any possible refinement, whereas Kaneko &amp; Kline consider only the coarsest such refine-ment.
 Definition 2 For a game  X  and a perfect recall refinement  X   X  , we say that  X  is a well-formed game with respect to  X  for all z  X  Z  X  I : (ii)  X  c ( z ) = `  X  I,  X  I 0  X  c (  X  ( z )) , (iii) In  X  , X  X  i ( z ) = X  X  i (  X  ( z )) , and (iv) In  X  , X i ( z [  X  I ] ,z ) = X i (  X  ( z )[  X  I 0 We say that  X  is a well-formed game if it is well-formed with respect to some perfect recall refinement.
 Recall that Z I is the set of terminal histories containing a prefix in the information set I , and that z [ I ] is that prefix. Intuitively, a game is well-formed if for each information set I  X  X  i , the structures around each  X  I,  X  I 0  X   X  perfect recall refinement are isomorphic across four condi-tions. Conditions (i) and (ii) state that the corresponding utilities and chance frequencies at each terminal history are proportional. Condition (iii) asserts that the opponents can never distinguish the corresponding histories at any point in  X  . Finally, condition (iv) states that player i cannot distin-guish between corresponding histories from  X  I and  X  I 0 the end of the game.
 Consider again DRP as a perfect recall refinement of DRP-IR. In DRP, the available actions are independent of dice outcomes, and the final utilities are only dependent on the final sum of the players X  dice. Therefore, in DRP the utili-ties are equivalent between, for example, the terminal his-tories where player i rolled a 1 followed by a 5, and the terminal histories where player i rolled a 4 followed by a 2 (condition (i)). In addition, the chance probabilities of reaching each terminal history are equal (condition (ii)). Furthermore, the opponents can never distinguish between two isomorphic histories since player i  X  X  rolls are private (condition (iii)). Finally, in DRP-IR, player i never remem-bers the outcome of the first roll from the second round on (condition (iv)). Thus, DRP-IR is well-formed with respect Any perfect recall game is well-formed with respect to it-self since  X  P ( I ) = { I } ,  X  equal to the identity bijection, and k
I 0 = `  X  I,  X  I 0 = 1 satisfies Definition 2. However, many imperfect recall games are also well-formed, with DRP-IR being one example. An additional example is presented in Section 6.
 We now show that CFR can be applied to any well-formed game to minimize average regret. A sketch of the proof is described below, while a full proof is provided in an ex-tended version of this paper (Lanctot et al., 2012). Theorem 1 If  X  is well-formed with respect to  X   X  , then the average regret in  X   X  for player i of choosing strategies ac-cording to CFR in  X  is bounded by where K = P I  X  X  Proof sketch. One can show that conditions (i) to (iv) of Definition 2 imply that the positive regrets are proportional between any two information sets in  X   X  that are merged in the well-formed game,  X  . In other words, for all I  X  I i  X  I,  X  I 0  X   X  P ( I ) , and a  X  A ( I ) , Since regrets between  X  and  X   X  are additive, i.e. , the proportionality implies that minimizing regret at each I  X  I i minimizes regret at each  X  I  X   X  I i . Because perfect recall, applying equation (3) gives the result. Since the strategy space is more expressive in  X   X  than in  X  (  X   X   X   X  ), R T i  X   X  R T i and thus it immediately follows that the average regret in  X  is minimized. In the case when  X  has perfect recall, because  X  is well-formed with respect to itself, Theorem 1 with K = |I i | is a direct generaliza-tion of the original CFR bound in equation (4). Theorem 1 not only guarantees regret minimization for perfect recall games, but also for well-formed imperfect recall games. 5.2. Skew Well-formed Games We now present a generalization of well-formed games to which a regret bound can still be derived.
 Definition 3 For a game  X  and a perfect recall refinement  X   X  , we say that  X  is a skew well-formed game with respect to  X  [0 ,  X  ) such that for all z  X  Z  X  I : (ii)  X  c ( z ) = `  X  I,  X  I 0  X  c (  X  ( z )) , (iii) In  X  , X  X  i ( z ) = X  X  i (  X  ( z )) , and (iv) In  X  , X i ( z [  X  I ] ,z ) = X i (  X  ( z )[  X  I 0 We say that  X  is a skew well-formed game if it is skew well-formed with respect to some perfect recall refinement. The only difference between Definitions 2 and 3 is in con-dition (i). While utilities must be exactly proportional in a well-formed game, in a skew well-formed game they must only be proportional up to a constant  X   X  I, that any well-formed game is skew well-formed by setting  X  For example, consider a new version of DRP called Skew-DRP(  X  ) with slightly modified payouts at the end of the game. Whenever the game reaches a showdown, player 1 receives a bonus  X  times the number of chips in the pot from player 2 if player 1 X  X  second die roll was even; otherwise, no bonus is awarded. The pot is then awarded to the player with the highest dice sum as usual. Analogously, define Skew-DRP-IR(  X  ) to be the imperfect recall abstraction of Skew-DRP(  X  ) where in the second round, players only re-member the sum of their two dice. Now, Skew-DRP-IR(  X  ) is not well-formed with respect to Skew-DRP(  X  ). To see this, note that the utilities resulting from the rolls 1,5 and the rolls 4,2 and the same sequence of betting are not ex-actly proportional because the second roll 5 is odd but 2 is even (utilities are off by  X  times the pot size). How-ever, Skew-DRP-IR(  X  ) is skew well-formed with respect to Skew-DRP(  X  ) with  X   X  I,  X  I 0 =  X  times the maximum pot size attainable from I .
 Unfortunately, there is no guarantee that regret will be min-imized by CFR in a skew well-formed game. However, we can still bound regret in a predictable manner according to the degree in which the utilities are skewed: Theorem 2 If  X  is skew well-formed with respect to  X   X  , then the average regret in  X   X  for player i of choosing strategies according to CFR in  X  is bounded by where K = P I  X  X  The proof is similar to that of Theorem 1. Theorem 2 shows that as T approaches infinity, the bound on our regret ap-proaches P I  X  X  demonstrate that as the skew  X  grows, so does our regret in Skew-DRP(  X  ) after a fixed number of iterations.
 Remarks. Theorems 1 and 2 are, to our knowledge, the first to provide such theoretical guarantees in imperfect re-call settings. However, these results are also relevant with regards to regret in the full game when CFR is applied to an abstraction. Recall that if  X  has perfect recall, then  X  is a perfect recall refinement of any (skew) well-formed ab-stract game. Thus, if we choose an abstraction that yields a (skew) well-formed game, then applying CFR to the ab-stract game achieves a bound on the average regret in the full game ,  X  . This is true regardless of whether the abstrac-tion exhibits perfect recall or imperfect recall. Previous counterexamples show that abstraction in general provides no guarantees in the full game (Waugh et al., 2009a). In contrast, our results show that applying CFR to an abstract game leads to bounded regret in the full game, provided we restrict ourselves to (skew) well-formed abstractions. If such an abstract game is much smaller than the full game, a significant amount of memory is saved when running CFR. To complement our theoretical results, we apply CFR to both players simultaneously in several zero-sum imperfect recall (abstract) games, and measure the sum of the average regrets for both players in a perfect recall refinement (the full game). Along with the small DRP domain and its vari-ants, we also consider the challenging domains of phantom tic-tac-toe and Bluff, which we now describe.
 Phantom tic-tac-toe. As in regular tic-tac-toe, phantom tic-tac-toe (PTTT) is played on a 3-by-3 board, initially empty, where the goal is to claim three squares along the same row, column, or diagonal. However, in PTTT, play-ers X  actions are private. Each turn, a player attempts to take a square of their choice. If they fail due to the opponent having taken that square on a previous turn, the same player keeps trying to take an alternative square until they suc-ceed. Players are not informed about how many attempts the opponent made before succeeding. The game ends im-mediately if there is ever a connecting line of squares be-longing to the same player. The winner receives a payoff of +1 , while the losing player receives  X  1 . In PTTT, the total number of histories | H | X  10 10 .
 Bluff. Bluff, also known as Liar X  X  Dice, Dudo, and Perudo, is a dice-bidding game. In our version, Bluff( D 1 each die has six sides with faces 1 to 6. Each player i rolls D i of these dice and looks at them without showing them to the opponent. Each round, players alternate by bidding on the outcome of all dice in play until one player claims that the other is bluffing ( i.e. , claims that the bid does not hold). A bid consists of a quantity of dice and a face value. A face of 6 is considered  X  X ild X  and counts as matching any other face. For example, the bid 2x5 represents the claim that there are at least two dice with a face of 5 (or 6) among both players X  dice. To place a new bid, the player must in-crease either the quantity or face value of the current bid; in addition, lowering the face is allowed if the quantity is increased. The player calling bluff wins the round if the opponent X  X  last bid is incorrect, and loses otherwise. The losing player removes one of their dice from the game and a new round begins, starting with the player who won the previous round. When a player has no more dice left, they have lost the game. A utility of +1 is given for a win and  X  1 for a loss. In this paper, we restrict ourselves to the case where D 1 = D 2 = 2 . Note that since Bluff(2,2) is a multi-round game, the expected values of Bluff(1,1) are precomputed for payoffs at the leaves of Bluff(2,1), which is then solved for leaf payoffs in the full Bluff(2,2) game. In Bluff(2,2), the total number of histories | H | X  10 10 We consider several different imperfect recall abstractions for DRP, Skew-DRP(  X  ), PTTT, and Bluff. For the DRP games, we apply DRP-IR and Skew-DRP-IR(  X  ) respec-tively as described in Section 5. Our PTTT and Bluff exper-iments, however, also investigate the effects of imperfect recall beyond skew well-formed games. In the full, perfect recall version of PTTT, each player remembers the order of every failed and every successful move she makes through-out the entire game. In our first abstract game, FOSF , play-ers forget the order of successive failures within the same Game Abstr. Well-for. |  X  | Savings PTTT None Yes 11695314  X  PTTT FOSF Yes 9347010 20.08% PTTT FOI No 1147530 90.19% PTTT FOS No 1484168 87.31% PTTT FOE No 47818 99.59% Bluff None Yes 704643030  X  Bluff r = 10 No 295534218 58.06% Bluff r = 8 No 108323418 84.63% Bluff r = 6 No 22518468 96.80% Bluff r = 4 No 2329068 99.67% Bluff r = 3 No 543900 99.92% Bluff r = 2 No 97608 99.97%
Bluff r = 1 No 12600 99.99% turn. Clearly, there is an isomorphism between any two merged information sets  X  I,  X  I 0  X   X  P ( I ) since the order of the actions does not affect the available future moves or utilities. Players still remember which turn each success and each failure occurred, and so the opponent X  X  sequences of actions must be equal across the isomorphism. Thus, FOSF is well-formed. Our remaining PTTT abstractions, however, are not even skew well-formed. In FOI , play-ers independently remember the sequence of failures and the sequence of successful actions, but not how the actions interleave. In FOS , players remember the order of failed actions, but not the order of successes. Finally, in FOE , players only know what actions they have taken and re-member nothing about the order in which they were taken. FOI, FOS, and FOE are not skew well-formed because no isomorphism can preserve the order of the opponent X  X  pre-vious information set, action pairs (breaking condition (iii) of Definitions 2 and 3). In Bluff, we use abstractions de-scribed by Neller and Hnath (2011) that force players to forget everything except the last r bids. Similarly, these abstract games are not skew well-formed because the play-ers forget information that the opponent could previously distinguish. The size of each DRP, PTTT, and Bluff game is given in Table 1, where we define |  X  | = |{ ( I,a ) : i  X  N,I  X  I i ,a  X  A ( I ) }| to be the total number of infor-mation set, action pairs for all players. Note that Skew-DRP(  X  ) is the same size as DRP regardless of the skew, and recall that CFR requires space linear in |  X  | . For each game, we ran CFR on both players, meaning that each player X  X  opponent was an identical copy of the same no-regret learner. Similar to Zinkevich et al. (2008), we used the chance sampling variant of CFR. The sum of the average positive regrets for each player over number of it-erations is shown in Figure 1. The Skew-DRP-IR(  X  ) ex-periments show that as  X  increases, so does the regret as predicted by Theorem 2, though P I  X  X  to be a very loose bound on the final regret. In PTTT, regret diverges from zero for FOI, FOS, and FOE, where FOS appears to provide slightly better strategies than FOI and FOE. While our theory cannot explain why FOS per-forms better, this does match our intuition that remember-ing information about the opponent X  X  moves is important. For a small increase in average regret, FOS reduces the space required by 87% compared to FOSF X  X  20% reduc-tion. Note that for both DRP and PTTT, running CFR on the full, perfect recall game achieves the same regret as in the well-formed abstractions (Skew-DRP-IR(0) and FSOF) and is thus not shown. In Bluff, we see that regret con-sistently worsens as fewer previous bids are remembered. This suggests that a result similar to Theorem 2 for skew-well-formed games may hold if condition (iii) of Defini-tion 2 is less constrained, though the proper formulation for such a relaxation remains unclear. Nonetheless, choos-ing r = 8 saves 85% of the memory with only a very small increase in average regret after millions of iterations. Well-formed games are described by four conditions pro-vided in Definition 2. Recall that Koller &amp; Megiddo (1992) prove that determining a player X  X  guaranteed payoff in an imperfect recall game is NP-complete. However, Koller &amp; Megiddo X  X  NP-hardness reduction creates an imperfect recall game that breaks conditions (i), (iii), and (iv) of Def-inition 2. In this section, we discuss the following question: For minimizing regret, how important is it to satisfy each individual condition of Definition 2? Skew well-formed games and Theorem 2 show that one can relax condition (i) of Definition 2 and still derive a bound on the average regret. In addition, most of our PTTT and Bluff abstractions from the previous section do not satisfy condition (iii), but CFR still produces reliable results. This suggests that it may be possible to relax condition (iii) in a similar manner to the relaxation of condition (i) introduced by skew well-formed games. While we leave this question open, we now demonstrate that breaking condition (iii) can lead CFR to a dead-lock situation where one player has constant average regret.
 Let us walk through the process of applying CFR to the game in Figure 2. Note that this game satisfies all of the conditions of Definition 2, except for condition (iii). To begin, the current strategy profile  X  1 is set to be uniform random at every information set. Under this profile, when player 1 is at I 3 , each of the four histories are equally likely. Thus, v i (  X  1 ( I and c at I 1 and I 2 . Player 2, however, has positive immedi-ate counterfactual regret for passing ( p ) at histories ac and ec (to always receive  X  utility) and for continuing ( c ) at bc and de (to always avoid receiving  X   X  utility), and has neg-ative immediate counterfactual regret for continuing at ac and ec and for passing at bc and de . Therefore, the next profile  X  2 still has player 1 playing uniformly random ev-erywhere, but player 2 now always passes at ac and ec , and always continues at bc and dc . On the second iteration of CFR, the positive regrets for player 1 at I 3 remain the same because the histories bcc and dcc are equally likely. Also, player 2 X  X  positive regrets remain the same at all four histories in H 2 . However, player 1 X  X  expected utility for continuing at I 1 or I 2 is now negative since player 2 now passes at ac and ec , and player 1 gains positive regret for passing at both I 1 and I 2 . This leads us to the next pro-0 , ( dc,p ) = 0 , ( ec,p ) = 1 , ( I 3 ,l ) = 0 . 5 } . One can check that running CFR for more iterations yields  X  t =  X  3 for all t  X  3 . The average regret for playing this way will be constant and hence does not approach zero because player 1 would rather play  X  0 1 = { ( I 1 ,p ) = 1 , ( I 0 , ( I 3 ,l ) = 0 } and get u 1 (  X  0 1 , X  3 2 ) = (1  X   X  ) / 4 &gt; u for  X   X  (0 , 1) . A similar example can be constructed where condition (iii) holds, but chance X  X  probabilities are not pro-portional (breaking condition (ii)).
 Despite the problem of breaking condition (iii), condition (iv) of Definition 2 can be relaxed. Rather than enforcing player i  X  X  future information to be the same across the bi-jection  X  , we only require that the corresponding subtrees be isomorphic, allowing player i to re-remember informa-tion that was previously forgotten. The details for this re-laxation are in the extended version of this paper (Lanctot et al., 2012). It is not clear that this relaxation is possible in skew well-formed games, nor does it seem to provide any practical advantage. We have provided the first set of theoretical guarantees for CFR in imperfect recall games. We defined well-formed and skew well-formed games and provided bounds on the average regret that results from applying CFR to such games. In addition, our theory shows that we can achieve low average regret in a full, perfect recall game when em-ploying CFR on an abstract version of the game, provided the abstract game is skew well-formed (with or without im-perfect recall). Our DRP experiments confirm these theo-retical results, while our PTTT and Bluff experiments hint that it may be possible to still bound regret in other types of imperfect recall games. Future work will look to expand on the set of imperfect recall games to which CFR can be reliably applied. In particular, it may be possible to derive regret bounds for a new class of games where conditions (ii) and (iii) of Definition 2 are relaxed.
 We would like to thank the Computer Poker Research Group at the University of Alberta for their helpful dis-cussions. This work was supported by NSERC, Alberta Innovates  X  Technology Futures, and the use of computing resources provided by WestGrid and Compute Canada.
