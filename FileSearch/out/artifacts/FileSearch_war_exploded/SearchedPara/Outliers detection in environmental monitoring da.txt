 1. Introduction
Online environmental monitoring is an important task asso-ciated to the operation of industrial plants, since these measure-mentsareusedtominimizetheimpactofpollutantsinsurrounding areas. This impact can be assessed by measuring a range of substances considered contaminants such as sulphur dioxide (SO 2 ), nitrogen dioxide (NO 2 ), ozone (O 3 ), particulate material (PM 10 or PM 2.5 ), etc. All of them are measured in m g/m chromatographs. These substances in dangerous concentrations can cause multiples health problems in the population, for instance high levels of SO 2 can cause respiratory diseases such as bronchitis, pulmonary edema, and even heart attack. High levels of NO cause from skin irritation to severe lung damage. To carry out environmentalmonitoringanetworkofairqualitystations,located inside and outside the industrial complex, is normally deployed.
These stations measure the concentrations of contaminant sub-stances and meteorological stations provide measurements of wind speed, wind direction, temperature, and humidity. Usually the air quality stations consider data loggers that acquire the measurements obtained by chromatographs and send them to an environmental database hosted in a master server, which can be used by clients to get and analyze the data. Unfortunately, these measurements can have outliers generated by specific abnormal operation conditions in the emission source, bad calibrated or faulty instruments, electrical faults, problems in the data logger or communication channels. Outliers do not have a formal definition, but many authors define them as measurements not consistent with most of the dataset. Their presence has a masking effect if the outliers are considered as normal measurements and they are not removed from the dataset. On the other hand, if a measurement is wronglylabelledasan outlier,thenit representsaswampingeffect. Masking effect implies contaminated dataset with measurements that do not represent normal system relationships and swamping effect involves a dataset that is not representative of the full system relationships. Pearson (2002) analyzes the detection of outliers in datasets used for system identification, and he shows that outliers are not always associated to failures in measurements or data collection systems, they can also be a product of physical effects representing patterns mainly related to abnormal system opera-tion. Outliers can produce many negative effects in the estimated values if they are not taken into account in the analysis of the dataset, as it has been reported for instance in Pearson (2002) , Kadlec et al. (2009) , and Warne et al. (2004 ).

Outliers detections have been applied in many fields such as chemical engineering and data mining. In spectroscopy, for instance, Wiegand et al. (2009) present simultaneous variable selection and outliers detection based on robust genetic algorithm. Bao and Dai (2009) propose an iterative procedure for outliers detection based on robust scaling of Partial Least Square (PLS) to predict gasoline properties. Lin et al. (2007) present outlier detec-tion in virtual sensor design for chemical processes based on unidimensional Hampel identifier and multidimensional Principal Component Analysis (PCA) approach. In a more general setting,
Warne et al. (2004) describe two fundamental tasks in inferential measurement design: data collection and influential variable selection. In this context, the method proposed by Jolliffe (1986) has been used to detect outliers. Chiang et al. (2003) compare a set of outlier detection algorithms with simulated data from an industrial process and also show the effect of multiple outliers in auto scaling and robust scaling of the dataset. Their results outper-form conventional outlier detection methods based on Principal
Components Analysis (PCA). Fortuna et al. (2007) also consider outlier detection based on Jolliffe X  X  method within their methodol-ogy for designing industrial virtual sensors. All these references show the importance outlier detection in any data-driven task, such as empirical modelling or virtual sensor design based on experimental data.

Partial Least Square (PLS) has been extensively used for outlier detection in dataset with linear features ( Jolliffe, 1986; Fortuna et al.,2007 ). In orderto deal with datasethaving nonlinearfeatures,
Baffi et al. (1999a) have developed a nonlinear approach for the innermodelinPLS.Theinputweightsareupdatedbyanerrorbased procedure considering a quadratic relation between input and output scores called Quadratic Partial Least Squares (QPLS). This approach has been tested using nonlinear datasets demonstrating better performance than conventional linear PLS in estimation tasks. The same authors have also developed a second nonlinear approach using Multilayer Perceptron (MLP) neural networks for the inner model in PLS Baffi et al. (1999b) (MLPPLS). They have demonstrated that an error based update procedure for input weights improves performance estimation compared to linear PLS,
QPLS, and MLPPLS without input weights update. Bang et al. (2003) propose a Takagi Sugeno X  X ang (TSK) fuzzy model for the inner model, this approach is closely related to the work carried out by
Baffi et al. (1999b) using radial basis functions (RBF). The main advantage of fuzzy inner model with respect to  X  X  X lack box X  X  models is its interpretability. A fuzzy model provides the possibility of using expert X  X  knowledge in order to design the inner structure design process. This Fuzzy PLS regression method, including both input weights update and constant input weights, has been tested using simulated dataset and spectral measurements from diesel fuels. The results demonstrate that this method can outperform other PLS approaches such as LPLS and QPLS. More recently, Li et al. (2007) have presented an approach based on the work of Baffi et al. (1999b) to estimate the residual life of a large generator stator insulation. The nonlinear modelling task was decomposed into a set of linear outer relation and a simple nonlinear inner relation, which were performed by a number of single input X  X ingle output models. The results outperformed linear regression tools such as multivariate linear regression and linear PLS.

This work considers a MLP neural network inner model, since environmentaldatausuallypresentnonlinearfeaturesandtheyare not clustered. The training algorithm for estimating the parameters of MLP plays an important role in the final performance attained by the network. The presence of outliers precludes the use of standard error-backpropagation algorithm. Outliers in training datasets usually produce large differences between the real measurement and the estimated one, which leads to standard methods to provide biased estimated. Different approaches have been proposed to mitigate the effect of outliers on the estimated parameters. Zhao et al. (2004) have proposed the use of a weighted error-back-propagation algorithm in order to deal with training dataset contaminated with outliers. Their method weight the error accord-ing to a scalar detection index in order to attenuate the effects of outliers. Since this method does not remove the outliers from the training dataset, it can be used with small dataset. However, the detection index is not robust when a large number of outliers are present. Liano (1996) analyzes the mean square error behaviour, normally used as minimization index in neural networks training methods, and shows that outliers increase the estimation error deviating theparameters fromthe real values.In orderto lessen the effect of outliers, he has proposed a method based on maximum likelihood estimators and a Cauchy error distribution, which has heavier tails respect to Gaussian distribution. Thus, if there are outliers in the dataset, then the error is kept bounded and hence parameters update is more stable. The main disadvantage is its slow convergence, because during the initial training iterations gross errors are mainly due to model initialization. Chen and Jain (1994) present a modified neural networks training algorithm, which is robust against outliers, by defining a time variant minimization index based on robust estimators. This time variant index uses some knowledge about the underlying nonlinear function obtained from a MLP model to reject samples with gross error and reduce their effect over the parameters update. This approach also attenuates parameter update in the initial stages of training due to the initial lack of knowledge of the underlying function. Chuang et al. (2000) , however, propose a solution to the problem of robust neural networks training by using a time variant outlier detection test. The detection threshold of the detection tests is adjusted while the model is trained, since at the beginning the function to be identified is unknown and the error may not provide a good indication to identify outliers. On the other hand, after several training epochs the model has learned the unknown function and the error can be used to discriminate outliers. This method outperforms other robust neural networks training meth-ods with noise-free, small noise and gross error model datasets, but performance presents strong dependence on threshold initializa-tion and the definition of an epoch dependent function.
In this work, instead of using the standard Levenberg X  X arquant method proposed by Baffi et al. (1999b) and the robust training method previouslydescribed, a simpleand improved methodbased on Quasi-Newton method with a regularization term is proposed.
The regularization term improves generalization of input/output relation, gives a smooth parameters update and provides a robust without the drawbacks of the previously described methods.
This paper is organized as follows: Section 2 describes methods for outlier detection and Section 3 describes the proposed method for training the inner nonlinear structure of a PLS model. In Section 4 some synthetic examples are used to demonstrate the perfor-mance of the proposed algorithm. In addition, some experiment using real data are also presented. Finally, in Section 5 some conclusions and future work are outlined. 2. Methods for outliers detection
Outlier detection methods can be classified into unsupervised and supervised ones. Unsupervised distance based techniques calculate a scalar value representing how far a particular measure-ment is from the centre (or reference) of the data considering parameters such as location and dispersion. Supervised projection techniques, usually based on Partial Least Squares or Principal
Components Regression, project the original dataset through a model into a set of latent variables where outliers became apparent by applying a function over projected dataset. Distance based approaches are sensitive to location and dispersion parameters and the value of the threshold distance to detect outliers based on a particular distribution. Projection methods do not assume a data distribution, but they are sensitive to model structure representing the nonlinear patterns contained in the original dataset and projected into the latent variables. These methods can also be appliedtoanalyzehighdimensionaldatasetsuchasgeochemicalor chemometrics, where the dimensions range in the thousands ( Filzmoser et al., 2008 ). In this work, a distance based method (Hamper identifier) and two projection methods, i.e. PLS and linear regression method will be considered in order to compare their performance with the proposed one. 2.1. Hampel identifier
This method is unsupervised since it does not require the estimation of any parameter. It is based on robust scaling of unidimensional measurements, instead of using the mean and variance, that is calculated with the median ~ x defined to odd and even sample lengths, as in (1a) and (1b), respectively, absolute valueofthedeviationfromthemedian(MAD)definedin(2)andthe robust scaled variable v in (3): ~ x ~ x  X 
MAD  X  1 : 4826 median  X  9 X j ~ x 9  X  X  2  X  v  X 
This procedure can simultaneously scale the dataset and detect outliers, becauseoutliers distort the meanand amplify the variance of the data. The real scaled values of the system outputs cannot be determineda priori, and thenmedian and MADare approximations of real mean and variance values. A measurement is labelled as an outlier if the robust scaled value exceeds a threshold adjusted to symmetric distributions. The disadvantage is that a measured variable does not have necessarily symmetrical probability dis-tribution and then the extreme values may erroneously be labelled as outlier. 2.2. Jolliffe statistics with PLS
PLS is a model based method to separate real system informa-tion from noise in highly correlated and redundant dataset. This method transforms a multivariable regression problem in the original dataset into a set of unidimensional linear regressions over input and output scores. The relations between real data and the inputs and output scores are defined in (4) and (5): X  X  Y  X  where X is the matrix of independent variables sorted in columns, Y is the matrix of the dependent variable, t j is the j th input score calculated in (6), u  X  j is the j th estimated output score (or latent (8), p j is the j th input loading calculated in (9), q j loading calculated in (10), E , F are residuals matrix of X and Y , respectively, and a is the number of latent variables: t  X  Xw j , j  X  1 , 2 , ... , a  X  6  X  ^ u  X  c j t j , j  X  1 , 2 , ... , a  X  7  X  w p  X  q  X 
Jolliffe method consists on calculate from PLS data transforma-tion Jolliffe coefficients defined below, where a measurement is labelled as outlier if any of the coefficients exceeds a limit value ( Jolliffe, 1986 ): c  X  c  X  c  X  u latentvariable, ~ a isthenumberoflatentvariableswhosevarianceis less than one, and d x is the number of inputs; Eqs. (11) and (12) are used to detect data that is not linear correlated with the data structure and Eq. (13) is used to detect data that inflate variance value and can be potentially outliers. The disadvantage of this method is the rigid linear inner model structure given by PLS inner model. If the dataset contains nonlinear relationships between the different variables, then the latent variables can be insufficient to capture these interactions. 2.3. Residual analysis on linear regression
This method consist on modelling the relationship between variables as linear functions and calculating the residual error vector defined in (14) between real output and estimated output withmultivariablelinearregression,overthenormaliseddatasetin (16), where the values of vector b are calculated with least square method as in (15): e  X  Y ^ Y  X  14  X  b  X  X  X T X 1 X T Y  X  15  X  ^ Y  X  X b  X  16  X 
A measurement is labelled as outlier, if the absolute error value is over the residual threshold. The limitations of this method are as follows: the use of rigid linear input/output model, non-Gaussian error distribution which leads to biased values of vector b and numerical problems in computation the inverse matrix of X in (15) when data is highly correlated. 3. Outliers detection based on nonlinear regression with MLPPLS
PLShasbeenshowntobeapowerfullinearregressiontechnique when the data is noisy and correlated, but in many practical situations data exhibit nonlinear behaviour and a linear inner model may be not suitable for this task. To solve this problem the linear inner model is replaced by a neural network structure. 3.1. Nonlinear regression MLPPLS
In linear PLS the relationship between input and output score is defined in Eq. (7), in MLPPLS is changed by (17), where W respectively, in neural network inner model to the j th latent variable and s is nonlinear activation function in the hidden layer: ^ u  X  W 2 j s  X  W 1 j t j  X  b 1 j  X  X  b 2 j , j  X  1 , 2 , ... , a  X  17  X 
Baffi et al. (1999b) propose an error based input weights update procedure in MLPPLS to improve performance in estimated output when data is highly nonlinear compared to MLPPLS without input weights update and QPLS, this aspect is relevant when relation between input and output latent variables is nonlinear. Error based input weights update are based in a Taylor series expansion which is possible under the assumptions that inner relation between inputand output scores are continuous and differentiable in the input space.
This series expansion of inner model is given by (18), where H is the i thinputweightand h 0 isreplacedin(18)byestimatedoutputscore ^ u : u  X  h
To compute the first derivative of function H respect to i th input weight D w i , thepartialderivatives (19a)and(19b) arereplacedin (18) and the series expansion of MLP inner model is obtained as (20) @ H  X  Xw  X  @ w @  X  Xw  X  @ w u  X  ^ u  X  1 2 W 2  X  1 s 2  X  W 1  X  Xw  X  X  b 1  X  X  W 1 x i D w
If matrix Z is defined, where each column z i is calculated with (21), then (20) can be written as (22) being D w the vector of input weights updates: z  X  @ H  X  Xw  X  u  X  ^ u  X  Z D w  X  22  X 
Thus,theweightsupdatesarecalculatedbyusing(23),beingthe error e defined as the difference between real output and estimated one as Eq. (24). The final algorithm is shown in Table 1 :
D w  X  X  Z T Z 1 Z T e  X  23  X  e  X  u ^ u  X  Z D w  X  24  X  3.2. Inner model MLP and training procedure
The inner model in MLPPLS has a number of inputs and output equal to the number of samples in dat aset, the nonlinear structure is defined in (17) by a linear output layer and a nonlinear activation function in the hidden layer. The pr oposed method for calculating the neural networks parameters is based on a reduced memory Quasi-
Newton algorithm with regularization term. This algorithm does not require the inverse of Jacobian matrix, such as the Levenberg X 
Marquardt method, it needs less memory than normal Quasi-Newton and is less sensitive to line search mi nimization. Regularization is included to prevent over-fitting due to outliers into dataset, to improve generalization of input/output relation and to prevent large fluctuations in parameters updat e. The expression that define para-meters variations in layer j with reduced memory Quasi-Newton and of parameters (weights and bias sorted in a vector) at k th iteration inside MLP train procedure, d  X  k  X  1  X  j thecolumnvectorwithupdate quantities of each parameter, D y  X  k  X  j is the parameters variations output at k th iteration in training procedure, A j and B define reduced memory Quasi-Newton training procedure, Z learning rate and r j is the regularization rate:
A  X  1  X 
B  X  d j  X  D g
D y y j  X  y 3.3. Outliers detection based on residual analysis of nonlinear regression with MLPPLS
Outliers detection procedure is based on the generalization propertyof the outputscore by theMLP inner model, whichimplies that an abnormal measurement can be an outlier if the absolute value of residual between real and estimated output exceeds a threshold value proportional at maximum absolute value of this residual. To detect measurements that increase the residual variance value, outlier detection procedures also use information about the variation of this residual, calculating an approximation of the derivative over the residual, which is useful when outliers are isolated in the dataset. In this case, a measurement is labelled as outlier, if exceeds a threshold value proportional at maximum absolute value of this approximate derivative. The outlier detection procedure is shown in Fig. 1 . Combining information of residual absolute value and its variations, isolated outliers can successfully be detected, because the absolute value from residual detects outliers over high values and approximate derivative detects isolated values that inflate residual variance which are masked before the use of this detection test. 4. Cases of study
In this section, three cases of outliers detection based on MLPPLS procedure and the proposed weight update algorithm are presented. The first one corresponds to simulated pH neutralization process with synthetic outliers. Second and third cases correspond to datasets from environmental monitoring, where the independent variables are as follows: wind direction, wind speed at different heights, temperature and temperature difference at diffe rent heights. The dependent vari-able is the concentration of sulphur dioxide (SO 2 ). All environmental variables are sampled at 15 min. The second case considers a dataset contaminated with synthetic outliers and the final one considers a dataset having natural outliers.
 4.1. Performance indices
To quantify the global performance of outliers detection pro-cedure, several indexes defined in (30) X (32) are evaluated in datasets with synthetic outliers. The synthetic_outliers are mea-surements contaminated with a fictitious outlier, labelled_outliers are all the measurements meeting the test of outlier detection, finally detected_outliers are measurements with outliers and they have been correctly labelled by respective outliers detection procedure:
Performance  X  %  X  X  100 detected_outliers synthetic_outliers 4.2. Outliers detection in simulated data from pH neutralization process with synthetic outliers
This section considers a simulated pH neutralization process in a continuous stirred tank reactor (CSTR), where the independent variables are the inlet flows of acid and basic solutions, and the dependent one is the pH measured in the solution. The dataset is contaminated with a set of synthetic outliers marked as red dots shown in Fig. 2 .

One of the main features of the proposed outlier detection procedure is the improved generalization of inner input/output term in the training algorithm. This first example presents results of and 4 , the regularization term in MLP training improves input/output generalization, and therefore also improves regression performance measured in term of Mean Square Error shown in Table 2 . These results demonstrate that the use of regularization term improves performance of MLP inner model and later MLPPLS regression.

To find an optimal inner model structure, a set of tests were carried out, and the structure that minimizes mean square error calculated over the validation set have been chosen. The results applied on both dataset are summarized in Table 3 . Finally, all variables are robust scaled using Eqs. (1a) X (2) before applying the outlier detection procedures.

The values of residual absolute value and absolute value from derivative of absolute residual are shown in Fig. 5 , where red lines represent threshold values. On the basis of these results the final detection is shown in Fig. 6 , where the green dots represent the detectedoutliers. Table4 showstheperformanceindexesforthiscase. 4.3. Outliers detection in environmental monitoring database with synthetic outliers
The dataset, in this case, considers clean a dataset; i.e. without outliers, from an environmental monitoring station located near an industrial site. This dataset was contaminated with a set of synthetic outliers marked as red dots in Fig. 7 . The values of residual absolute error value and absolute of its derivative are shown in Fig. 8 . Based on these results the final detection is shown in Fig. 9 , where the green dots represent the detected outliers and the red one the undetected.

Table 5 shows the performance indices (in percentage (%)) obtained by applying MLPPLS method, and conventional outliers detection procedures, such as residual analysis on linear regression and Jolliffe statistics with PLS. By comparing the performance indices it can be seen that the proposed method based on MLPPLS outperforms conventional methods. 4.4. Outliers detectionin environmental monitoringdata with natural outliers
This dataset corresponds to the same monitoring station described in Section 4.3, but this time the dataset has some outliers as product of SO 2 emissions produced by abnormal operations in the emissions source. Fig. 10 shows the thresholds for detecting the presence of outliers. The dataset and the detection results are shown in Fig. 11 , where it can be seen that the outlier detection algorithm detects any deviation of the normal conditions labelled in the figure as green points. 5. Conclusions
In environmental monitoring is very important to have tools to validate the measurements and screen abnormal situations, since deviations of contaminants concentration from their permitted ranges, can cause serious problems on population health and environment. This paper has presented an outlier detection algo-rithmbasedonMLPPLS,sincethisstructurehasdemonstratedtobe more efficient than the use of a simple Neural Network structure because it can handle correlated data. The proposed training algorithm for the MLP inner model is computational efficient and considers regularization in order to prevent over-fitting and improve performance, so that the generalization of nonlinear inner relation is preserved. This is very important, since generalization implies that an outlier in the dataset amplifies the regression residual. The application of this outlier detection procedure based on MLPPLS to environmental monitoring has been encouraging.
This method can also be used in other applications such as data preprocessing in virtual sensor design, data-driven modelling or validation of operational variables in industrial databases. Further work is underway to include additional information given by measurements from neighboring monitoring stations, study other inner structures, which may be adequate when data is clustered and generate a data driven tool to detect outliers and predict air quality through virtual sensor techniques.
 References
