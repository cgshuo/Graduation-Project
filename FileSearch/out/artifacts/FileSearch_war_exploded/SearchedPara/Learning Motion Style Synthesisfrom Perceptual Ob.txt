 lorenzo@riya.com Human motion perception can be generally thought of as the re sult of interaction of two factors, formed. In computer animation, the separation of the underl ying content of a movement from its similar to the user-specified target.
 of data-driven animation: sample-based concatenation met hods, and techniques based on learned generating novel movements by concatenation of motion capt ure snippets. Since motion is produced by cutting and pasting pre-recorded examples, the resultin g animations achieve realism similar to that of pure motion-capture play back. Snippet concatenati on can produce novel content by gener-ating arbitrarily complex new movements. However, this app roach is restricted to synthesize only been proposed that attempt to overcome some of these limitat ions. Unfortunately, most of these methods learn simple parametric motion models that are unab le to fully capture the subtleties and complexities of human movement. As a consequence, animatio ns resulting from these systems are often plagued by low quality and scarce realism.
 The technique introduced in this paper is a compromise betwe en the pure concatenative approaches and the methods based on learned parametric models. The aim i s to maintain the animated preci-animations can be generated via space-time interpolation o f pairs of motion sequences. We propose This allows us to create motions with arbitrary styles witho ut compromising animation quality. learn the mapping between motion blending parameters and re sulting animation styles. This enables movement observation, Laban Movement Analysis, describin g styles by means of a set of rigorously defined perceptual attributes. motion styles according to a movement notation system, call ed Laban Movement Analysis or LMA [7]. We focus on a subset of Laban Movement Analysis: the  X  X MA -Effort X  dimensions. This system does not attempt to describe the coarse aspects of a motion, e .g. whether someone is walking, or  X  X ound X  or  X  X ree X ? Each LMA-Effort factor varies in intensi ty between opposing poles, and takes values in a continuous range. The factors are briefly describ ed as follows: a three-dimensional perceptual space derived by translati ng the LMA-Effort notations for each of these factors into numerical values ranging in the interval [  X  3 , 3] . animations. The training animations are observed by a human expert who assigns LMA labels to each sequence. This set of supervised data is used to learn a m apping between the space of motion while the following sections give specific details of each co mponent. 3.1 Training: Learning the Style of Motion Interpolation sequences recorded with a motion capture system. We represe nt the motion as a time-varying vector of joint angles. In the training stage each motion sequence i s manually segmented by an LMA human expert into fragments corresponding to fundamental a ctions or units of motions. Let X denote the joint angle data of the i -th fragment in the database.
 Step 1: Matching motion content. We apply a motion matching algorithm to identify fragment pairs ( X warping. This allows us to compare kinematic contents while factoring out differences in timing or acceleration, more often associated to variations in style .
 Step 2: Space-time interpolation. We use these motion matches to augment the database with new synthetically-generated styles: given matching motio n fragments X fragments to produce a new motion X  X  Step 3: Style interpolation learning. Both the synthesized animations X  X  motion capture data X note the three-dimensional vectors encoding the LMA-Effor t qualities of X interpolation algorithm. This regression defines a functio n f predicting LMA-Effort factors e  X  from the style attributes and joint angle data of fragments i and j : perceptual space of movement styles. 3.2 Testing: Style Transfer use dynamic-time warping to segment the input sequence into snippets Y matches the content of a set of analogous motions { X possible pairwise blends X  X  provides the best approximation to the target style  X  e . This objective can be formulated as The animation resulting from space-time interpolation of f ragments X nating these artificially-generated snippets will produce the desired output. The objective of the matching algorithm is to identify pairs of sequences having similar motion which movements are performed. Previous work [2, 12] has sho wn that the differences in movement styles can be found by examining the parameters of timing and movement acceleration. By contrast, an action is primarily characterized by changes of body confi gurations in space rather than over time. Thus we compare the content of two motions by identifyi ng similar spatial body poses while between motion snippets X measure of the distance between spatial body configurations X aligning the two snippets. We can then formally define SSD ( X subject to constraints: We say that two motions i and j have similar content if SSD ( X A time warping strategy is also employed to synthesize novel animations from the pairs of content-X and X timings. Suppose parameter values n 0 will be replayed with its original timing. However, if we use these same parameter values on se-quence X correspond to playing sequence j with the timing of sequence i . Similarly, n 1 sen, such that q  X  ( n 1 time path of length T  X  by estimating the joint angles X sequences will move in synchrony according to the new interm ediate timing. From these two syn-chronized sequences, a novel motion X  X  to mixing coefficients (1  X   X  ) and  X  : X  X  sized motion X  X  the timings of sequences. Given a pair of content-matching snippets X needs to be applied to space-time interpolation in order to p roduce a motion X  X  of our seed motion sequences { X corresponding LMA-Effort qualities { e it onto a low-dimensional linear subspace computed using Pr incipal Component Analysis (PCA). components in order to obtain a discriminative representat ion of the motion contents. Let c the vector containing the PCA coefficients computed from X learn the optimal parameters  X  of a parameterized function f ( z  X  between z  X  the objective function: and improving generalization. We experimented with severa l function parameterizations and loss linear in input space: computation of the vectors  X ( z and choose mappings  X  such that the inner product  X ( z function k ( z a subset of the training data, the set of support vectors. the motion database a pair of sequences having content simil ar to Y and whose interpolation can is destined to fail as Y can be any arbitrarily long and complex sequence, possibly c onsisting of several movements performed one after the other. As a conseq uence, we might not have in the database examples that match sequence Y in its entirety. 7.1 Input segmentation and matching The solution that we propose is inspired by concatenative me thods. The idea is to determine the concatenation of database motion examples [ X sequence Y . Our approach relies again on dynamic programming and can be interpreted as a gen-alignment is sought between a given sequence and a concatena tion of a variable number of exam-sequence Y at frame p and those of example X path w ( n ) = ( p ( n ) , q ( n ) , i ( n )) that minimizes the global error The above mentioned conditions can be formalized as follows : if w ( n ) = ( p, 1 , i ) , then w ( n  X  1)  X  X  ( p  X  1 , 1 , i ) , ( p  X  1 , T p (1) = 1 , q (1) = 1 , p ( L ) = T, q ( L ) = T i ( L ) (11) where J denotes the number of fragments in the database, L the length of the time warping path, T the number of frames of the input sequence, and T The global minimum of the objective in Equation (8), subject to constraints (9),(10),(11), can be found using a dynamic programming method originally develo ped by Ney [14] for the problem of connected word recognition in speech data. Note that this ap proach induces a segmentation of the input sequence Y into snippets [ Y [ X 1 , ..., X N ] 7.2 Piecewise Style synthesis The final step of our algorithm uses the concatenation of exam ples [ X X defined in Equation 3. Let { X their LMA-Effort values. { X of snippet Y the pair of examples ( i approximation to target style  X  e , according to the learned style-prediction function f : of candidate motion fragments. We then select the pair ( i from the target style  X  e . In order to estimate the optimal values of  X  for pair ( i f ( z  X  i and thus a good estimate of the global minimum in the specified interval can be obtained even with a modest number M of samples. The approximation is further refined using a gold en section search rather than [0,1], we give the algorithm the ability to extra polate from existing motion styles. Given optimal parameters (  X   X  , k  X  , l  X  ) , space-time interpolation of fragments X parameter value  X   X  produces an animation with content similar to that of Y by concatenating all of the fragments generated via interpo lation with optimal parameters. The system was tested using a motion database consisting of 1 2 sequences performed by differ-ent professional dancers. The subjects were asked to perfor m a specific movement phrase in their own natural style. Each of the 12 sequences was segmented by a n LMA expert into 5 fragments corresponding to the main actions in the phrase. All fragmen ts were then automatically clustered into 5 content groups using the SSD criterion outlined in section 4. The motions were recorded for motion interpolation. From these 60 motion fragments, 1 05 novel motions were synthesized both those recorded and those artificially generated, were a nnotated with LMA-Effort qualities by an LMA expert. From this set of motions, 85 training examples were randomly selected to train the style regression models. The remaining 20 examples were use d for testing.
 Table 1 summarizes the LMA-Effort prediction performance i n terms of mean squared error for the analysis the linear style interpolation model, commonly us ed in previous work. This model assumes styles of the two seed motions: e  X  approximation methods, we used a Gaussian RBF kernel. The hy perparameters (i.e. the kernel despite the absence of sparsity of this solution. The simple linear interpolation model performed reasonably well only on the Flow dimension. Overall, non-li near regression models proved to be via space-time interpolation is a complex function of the or iginal styles and motions. Figure 1 shows the LMA-Effort qualities predicted by kernel ridge regression while varying  X  for three different sample values of the inputs ( X non-linear function models and the linear approximations, as outlined in Table 1. Several additional motion examples performed by dancers no t included in the training data were were always correctly segmented by the dynamic programming algorithm into the five fragments of recovered parameter values were used to synthesize animati ons with the specified desired styles. http://movement.nyu.edu/learning-motion-styles/ . In order to test the generalization from those in the training set. All of the synthesized sequen ces were visually inspected by LMA We have presented a novel technique that learns motion style synthesis from artificially-generated examples. Animations produced by our system have quality si milar to pure motion capture play-interpolation or extrapolation to generate new styles. In p revious LMA-based animation systems [3], heuristic and hand-designed rules have been adopted to implement the style changes associated to LMA-Effort variations. To the best of our knowledge, our w ork represents the first attempt at automatically learning the mapping between LMA attributes and animation parameters. Although our algorithm has shown to produce good results with small tr aining data, we expect that larger varied motion styles. Our approach could be easily generali zed to other languages and notations, motion sequences.
 Acknowledgments This work was carried out while LT was at Stanford University and visiting New York University. Maya animations. Special thanks to Jan Burkhardt, Begonia C aparros, Ed Groff, Ellen Goldman and Pamela Schick for LMA observations and notations. This work has been supported by the National Science Foundation.

