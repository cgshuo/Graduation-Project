 Community question answering (CQA) services such as Yahoo! Answers 1 ,BaiduZhi-dao 2 and Sina Iask 3 have become more and more popular during recent years in pro-viding platforms for people to share knowledge online. Actually, CQA has existed for decades as part of bulletin board systems and Usernet, but has recently been popular-ized within web portals in which users answer questions posted by other users. CQA has proven to be more effective since users can post full natural language questions rather than issuing several word queries to search engines. The question answering content in online communities provides an alternative for users to obtain information in the form of answers authored by other users, rather than as lists of results or documents from search engines. In CQA, people are supposed to find the answer of any question once all people fully participate into community and share their knowledge.

Typically, a CQA service is organized in categories. When an asker has a question in his mind, he first chooses a proper category in the service system, and post the question in that category, then waits others (answerers) to answer his question. With existing CQA systems, askers must passively wait for other users to visit the system, read their questions, and provide answers. It may take hours or days from asking a question in a CQA system before a user can expect to receive answers. Besides, the asker may expect some  X  X xperts X  in the system to answer his question so that he has a relatively more chance to get correct answer. On the other hand, a user who knows well the answer to a particular question may not answer the question because the user may not visit the system frequently or the user may be faced with many open questions. While a user who answers a question may just happen to see the question, but is not an expert on the questions subject. To improve on this a rrangement, it X  X  a natural idea to push new questions to the right persons in a CQA system to obtain quick, high-quality answers. The reduced waiting time and improvements in the quality of answers are expected to improve user satisfaction.

In this paper, we are interested in predicting best answerers for new questions pro-posed in the question answering community. Generally speaking, when a new question is posted to the CQA system by an asker, we try to find the best answerers who can give correct answers to that question which may satisfy the asker. We model interests of the answerers by tracking users X  answering history in the community. For a spe-cific answerer, those questions the answerer has given best answers in the history make up the profile of that answerer. Based on the user profile, the relationship between the answerer and a new question is measured by traditional language model and the la-tent dirichlet allocation (LDA) model. Besides, user authority and user activity are also taken into consideration. The basic assumption is that more authoritative answerers may give more accurate answers, and more active answerers may be more willing to answer the new-coming questions.

The rest of this paper is organized as fo llows. Section 2 introduces some prior re-search related to our work. Section 3 discusses the proposed framework for predicting best answerers for new questions. Experimental results are presented in Section 4. We conclude this paper and point out future work in Section 5. Community-based Question Answering(CQA) services such as Yahoo! Answers, Baidu Zhidao and Sina Iask have b een building very large archives of questions and their answers during the past few years. There is a large body of work conducted on the archives([1],[5],[7],[8],[11],[15]).

Lata et al. [1] studied Yahoo Answer! question answering system, and investigated user behavior and the quality of user generated content in the system.

Kevin K. Nam et al. [15] analyzed the characteristics of knowledge generation and user participation behavior in the largest question-answering online community in South Korea, Naver KnowledgeCiN, and found that altruism, learning, and competency are frequent motivations for top answerers to participate.

In order to avoid the lag time involved with waiting for a personal response, a Q&amp;A service will typically automatically search the whole QA archive to see if the same ques-tion has previously been asked. In question search, given a question as query, the task is to find similar solved questions in CQA. Jeon et al. in [13] proposed an approach to es-timate the question semantic similarity based on their answers. Additionally, Jeon et al. proposed a translate model to find similar questions in large question-answer archives in [12]. Besides searching for similar questions, recommending related questions in CQA, proposed by Y. Cao in [14] recently, is an alternative to similar question search.
Recently, E. Agichtein et al. proposed me thod in [9] to distinguish high-quality con-tent, both questions and answers, from the rest. P. Jurczyk and E. Agichtein [8] ex-ploited link analysis to identify authoritative users in community question answering, while M. Bouguessa et al. [6] use an alternative method to identify authoritative actors in question-answering forums.

There are relatively fewer work which aims to tackle the problem of predicting best answerers for new questions in community question answering. Y.H. Zhou et.al [3] present three approaches based on language models to represent the expertise of users based on their previous question answering activities, and then  X  X ush X  the right ques-tions to the right persons in online forums. Guo et.al [5] focus on recommending answer providers in order to increase the par ticipation rate of users in CQA service.
Our work mostly relates to the work done in [3], [5] and [7], however, we propose to use different methods to model user interests. The prior information such as user activity and user authority is incorporated into the probabilistic framework for the first time when predicting best answerers for new questions. In this section, we predict best answerers for news questions in community question answering. A framework is proposed to find the top-k users for a given new question based on probabilistic model. Formally, given a new question q , the probability of user u being the best answerer for the question is measured as where P ( u ) is the prior probability of user u , P ( q | u ) models the probability of gener-ating q from the profile of user u and P ( q ) is the probability of generating the question q , which is the same for all candidate users. In this study, our main task is therefore to compute P ( q | u ) and P ( u ) . Generally, P ( q | u ) models the degree of interest of user u on question q , and the prior information P ( u ) includes activity and authority of user u . Best answerers for the question q are ranked according t o the combination of P ( q | u ) and P ( u ) . 3.1 Modeling Interests of Answerers The fact that an answerer will give the best answer to a given question typically indicates that the answerer has some degree of interest on that question. Thus, to predict the best answerer for a given new question, it X  X  natural to model users X  X  interests. Generally speaking, users X  interests can be learned from his answering history, such as all the questions the answerer has given best answers to. In this section, two models are utilized to describe users X  interests. Language Model. Suppose a question q = { w 1 ,w 2 , ... ,w N } ,where w i is a non-stop word in the question, i =1 , 2 , ... N . In our approach, each word in question q is sup-posed to be generated independently, and the interest of user u on question q is repre-sented by a multinomial probability distribution over the vocabulary of words. There-fore, the degree of interest of user u on question q can be obtained by taking the product of all the words in the question: where P ( w |  X  u ) is the probability of a word given  X  u  X  the profile of user u ,and n ( w, q ) is number of times word w occurs in question q .Userprofile  X  u is composed of questions that are answered by user u and u is chosen as the best answerer for each of those questions. To calculate P ( w |  X  u ) , one straight way is to compute the fraction of word w in  X  u . However, typically many users only answer a small number of questions, thus, due to sparsity, P ( w |  X  u ) is zero when w does not occur in  X  u , which leads to the result that user u will never be predicted as best answerer for any question containing word w . Thus, we need to do smoothing on P ( w |  X  u ) , so that we can avoid zero prob-ability of P ( w |  X  u ) to unseen words and also make the estimated model more accurate. For example, the Dirichlet smoothing method can be used to smooth P ( w |  X  u ) , called the language model (LM)[10]: where P ( w ) denotes the background language model built on the entire question collec-tion Q and  X   X  [0, 1] is a coefficient to control the influence of the background model, which is calculated as where tf ( w,  X  u ) is the frequency of w in the profile of user u , and the empirical pa-rameter  X  is manually set to 1000 , which can lead to relatively better results in the experiment. The background model P ( w ) is computed through a maximum likelihood estimation: where n ( w, Q ) denotes the frequency of word w occurring in the question collection Q and | Q | is the number of all words in collection Q .
 Topic Model Based on Latent Dirichlet Allocation. The lexical gap problem is very important for problems related to natural language processing[16]. However, the lan-guage model is based on words matching only, and does not represent any semantic information, thus does not address the problem of lexical gaps between new questions and user profiles. In a typical question answering cycle, users always answer questions by first choosing a specific topic in an implicit way which the user has interest, and then picking out a question on that topic to answer. Note that one may have several or many interests, representing that the user should be modeled by different topics. To analyze a user X  X  interest by investigating his previously asked questions, the set of those questions is viewed as the profile of that user. The user profile has some latent topics within it. To describe those topics, we utilize the latent dirichlet allocation (LDA, [2]) model, a topic model already used in the literature of documental information retrieval[4], which has the ability to model topics in large corpus(user profiles). The LDA model for generating user profile is represented as a probabilistic graphical model in Figure 1.

In LDA, the topic mixture is drawn from a conjugate Dirichlet prior that remains the same for all users. The process of generating user profile  X  u for a specific user u is as follows: 1) choose a mu ltinomial distribution  X  z for each topic z from a Dirichlet distribution with parameter  X  ,  X  z describes words distribution within topic z ;2)forthe user profile  X  u , pick a multinomial distribution  X  U 4 from a Dirichlet distribution with parameter  X  ,  X  u models topic distribution of u ; 3) for each word token w in user profile  X  , select a topic z  X  X  1 , ... ,K } 5 from the multinomial distribution  X  U ;4)pickword w from the mulitnomial distribution  X  z . Repeat this procedure for N U (number of words in  X  u ) times, then the user profile for u is generated.

For all of N users in the system, their profiles can be generated by repeating above procedure for N times, thus the likelihood L of the whole user profile collection is P ( u 1 , ... ,u N |  X ,  X  )=
Compared to the simple language model, the LDA model has a new representation for a user profile based on topics. Note that the LDA model allows multiple topics per user, reflecting the fact that each user has multiple interest, which may be more reasonable in real world. After the posterior estimation of  X  and  X  , the probability of generating a word in a user profile is calculated as following, where  X  and  X  are the posterior estimates of  X  and  X  respectively.

It can be seen that when measuring corre lations between a word and a specific user profile, the LDA model does not require the word to appear in the user profile, since the LDA model measures their relations in the topic space rather than based on word matching. Thus, with LDA model the lexical gap problem[16] is alleviated.
 Combining LM and LDA. It X  X  natural to combine Language Model and LDA together to predict best answerers based on user profile. For example, LM and LDA can be linearly combined as Here  X  is set to 0.5 empirically, which gives equal weights to both LM and LDA model when combining them. 3.2 The Prior Information of Answerers To this end, in order to predict the best answerer u for a given new question q ,wehave calculated the likelihood, P ( q | u ) , based on user profile. In this section, we model the prior information of u as P ( u ) , which is made up of two parts: user authority and user activity. Later on we will find that the prior information will also play an important role when predicting best answerers.
 User Authority. The underlying idea is that more authoritative users will probably give more authoritative answers to new questions in the future. And more authoritative answers will have a higher probability to be selected as the best answers by askers or other voters 6 . Thus, user authority is helpful to best answerer prediction. The problem of identifying authoritative users in question answering community has been well studied in the literature of community question answering ([6][8]). A common approach is to use link analysis techniques similar to the PageRank and Hub algorithm[8] for web page ranking, which are based on a large graph representing the discovered relationships between all of the askers and answerers in the community. However, more recently, M. Bouguessa et al. [6] pointed out that there were many drawbacks within the link-based methods. Furthermore, they found the Indegree technique can be used to rate the authority of users in the Q&amp;A community effectively, where the InDegree of a user is simply the number of best answers given by that user. Here, with the same spirit of InDegree, we model the authority of users as, where numans u is the number of best answers user u has authored during his life in the question answering community. We use its log value to smooth the influence of user authority on best answerer prediction. User Activity. From the point of view of the asker, when posting a new question, be-sides the correctness of the expected answer, typically the asker would like the question to be answered as soon as possible. However, users may be quite different from each other in the degree of their activity. Some users could be active for a long time, while others may keep silent (don X  X  give best answer to any question) for a very long time. Thus, more  X  X ctive X  users may be preferred, and it X  X  reasonable to take user activity into consideration when predicting best answerers for questions in real Q&amp;A services. For example, a user answered lots of questions in the first half year of 2008, but in the second half of that year, he gave no answer to any question at all. The activity of a user may be influenced by lots of facts, such as he may lose interest on a specific topic, or he may be quite involved in his work and have no spare time to serve as an expert in the Q&amp;A community[15]. In this paper, we measure user X  X  activity for a given question q as where t q is the posting time of the question, t u is the most recent time when the user authored an answer to a question.

With user authority and user activity, the prior information in the probabilistic frame-work is calculated as the combination of them, and the probability that user u will provide the best answer to the question q is Given a test question q , all candidate users(answer ers) are ranked according to P ( u | q ) , and the top ones are  X  X ushed X  (recommended) to the asker of q , who are considered as the best answerers for that question. 4.1 Dataset The experimental dataset is collected from a community-based question answering site, Iask 7 , one of the leading web 2.0 sites in China. Overtime, the service has built up a very large archive of questions and answers written in Chinese. Topics of questions are very diverse, ranging from computer problems to restaurant recommendations. In this pa-per, our experiments are based on two top-level categories, Computer/Networks/IT and Health/Medical. Questions in both categor ies are resolved questions, thus each question has a best answer 8 corresponding to one best answerer. For the first category, the time span is from January 1, 2007 to February 18, 2009, and for the second category, the time span is from August 13,2004 to February 18, 2009. Table 1 shows the statistical information of the dataset. Numbers of resol ved questions (best answers) in each of the 100 weeks before January 1, 2009 are depicted in Fig.3, from which we can see that user participation rate is relatively higher in the category of Computer/Network/IT than in the Health/Medical category.

For best answerer prediction, those answerers who have authored at least N (say 10) best answers are considered. Specifically , in the category of Computer/Network/IT, up to 51,379 users have given at least one best answer, while only 4,798 of them give at least ten best answers. Those 4,798 best answerers are very important to the question answering community, in fact, they have given best answers to 369,490 questions out of the total 461,712 questions. In the Health/Medical category, 5,613 active answerers contributed best answers to 349,989 questions out of 501,735 questions.

All questions are segmented using a Chinese word segmentation tool. After removing stop words, questions with less than two Chinese characters are ignored.

Test questions are drawn from the questions answered by these active users after a specific time stamp. Fig.2 shows how user profi le and test questions are separated by the time stamp for an active user. To keep balance, not all questions answered by the user after the time stamp are picked out as test questions, in this study, we limit the num-ber of test questions picked out from one answerer to 10 at most. Also note that some best answerers give no best answer to questions after the time stamp in the corpus, thus there isn X  X  any test question drawn from them, however, they are still candidate answer-ers when predicting best answerers for a given question. As a result, there are 3,126 test questions for the first category, and 1,693 te st questions for the other category. For each of these test questions, they are recommended to 4,798 and 5,631 answerers, respec-tively. The answerer who actually gave the best answer to a test question is viewed as the ground truth for that question.
 4.2 Experimental Setup The parameter estimation and inference of the LDA model is based on Gibbs sampling, and the code implementation by X. Phan and C. Nguyen 9 is utilized. The number of topics in the LDA model is set to 100 empirically, and the Dirichlet prior parameters are set to the default values, which are typical in many other previous work. The iteration number is set to 2000. Note that the result will change with different iterations, however, in our experiment we found that the difference is slight when the iteration times is relatively large (say above 100). 4.3 Evaluation Method Evaluation of predicting best answerers for news questions is not a trivial task. Here we propose a new metric for the evaluation which is different from the method used in [7]. In [7], each new question is recommended to the users who actually answered it(including the best answerer and other answerers whose answers were not chosen as the best finally), and those users are ran ked according to their ranking method, the ranking position of the best answerer of the question is taken to measure the correctness of the recommendation of that question. Unlike [7], for a given news question, instead of only recommending it to the users who actually answered it, we  X  X ush X  the question to all possible users who may answer it. Wh en candidate users are ranked according to our model, if the  X  X eal X  best answerer of the question is among the top N (say 1,10 or 100) predicted users, the prediction for that question is considered to be successful. We call this metric a success-at-N (S@N) metri c, which has actually been widely used in the literature of information retrieval. Take S@1 for example, for each question in the test set, if its best answerer is the top one among the users predicted, the S@1 value is 1, otherwise its S@1 value is 0. The S@1 value of the whole test set is averaged by the S@1 value of all the test questions. Note that if there are many candidate users, the S@N value could be very small when N is relatively small. 4.4 Topics Found in User Profile by LDA Table 2 and table 3 show several typical topics found in user profile by the LDA model in categories of Computer/Network/IT and Me dical/Health, respectively. For example, in table 2, topic 3 focuses on mobile phone, topic 12 relates to computer virus, while topic 26 talks about printer. In table 3, topic 3,8 and 12 mainly focuses on diseases of cancer, diabetes and hepatitis, respectively, while topic 78 centers with exercising. 4.5 Results of Best Answerer Prediction Experimental results are organized into three groups. The language model method is used as the baseline, and the number in the parenthesis are improvements over the baseline by different methods. In the first group, the results are based on the user profile only, while in the second group, user authoritie s are taken into consideration, and in the third group, both user authorities and user act ivities are utilized besides user profiles.
Table 4 shows the results of predicting best answerers for questions in the category of Computer/Network/IT. From table 4 we can see that without consideration of user authority and user activity, the combination of Language Model and LDA performed best. When taking user authority into consid eration, the prediction accuracy become better, which is based on the combination of LM and LDA model or LDA model alone. The best performance is got when both user a uthority and user activity are involved, which is typically based on the LDA model alone. Since there are many candidate users which may give best answer to a specific question, and there are probably some users who indeed are  X  X xperts X  to the question, but they did not give best answers to that question due to some reasons, the prediction accuracy could be reasonably small. Note that for the measure of S@1, a random-guess method will lead to a low accuracy of less than 0.021%.
 Table 5 shows the results of predicting best answerers for questions in the category of Medical/Health. Similar to the case in the Computer/Network/IT category, we can also see that without consideration of user authority and user activity, the combination of Language Model and LDA performed better tha n either alone. Much better performance is reached when taking both user authority and user activity into consideration.
Generally speaking, to predict best answerers for new questions, we need semantic model such as LDA besides the language mode l. Additionally, the prior information such as user authority and user activity also ma kes great contributions to the prediction. In this paper, we have proposed a framework for predicting best answerers for new ques-tions in the question answering community. By tracking answerers X  X  answering history, interests of answerers are modeled with the mixture of the Language Model and the LDA model. User activity and authority levels are also incorporated into the proba-bilistic framework. Experimental results sh ow that our proposed method can effectively predict the best answerers for new questions.

As future work, we intend to investigate the performance of the proposed framework in a larger scale dataset besides the two categories used in this study. We also plan to investigate more accurate user authority and user activity models, and more precise prediction resu lts are expected.

