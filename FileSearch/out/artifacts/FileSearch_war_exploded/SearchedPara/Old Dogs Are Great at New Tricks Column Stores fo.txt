 We make the suggestion that instead of implementing cus-tom index structures and query evaluation algorithms, IR researchers should simply store document representations in a column-oriented relational database and implement ranking models using SQL. For rapid prototyping, this is particu-larly advantageous since researchers can explore new scoring functions and features by simply issuing SQL queries, with-out needing to write imperative code. We demonstrate the feasibility of this approach by an implementation of conjunc-tive BM25 using two modern column stores. Experiments on a web collection show that a retrieval engine built in this manner achieves effectiveness and efficiency on par with custom-built retrieval engines, but provides many additional advantages, including cleaner query semantics, a simpler ar-chitecture, built-in support for error analysis, and the ability to exploit advances in database technology  X  X or free X . Categories and Subject Descriptors : H.3.4 [Information Storage and Retrieval]: Systems and Software X  X erformance Evaluation Keywords: Relational Databases; BM25
Information retrieval researchers and practitioners have long implemented specialized, custom-built data structures and query evaluation algorithms for document ranking [15]. Today, these techniques can be quite complex, especially with  X  X tructured queries X  that span multiple nested clauses with a panoply of query operators [11]. We revisit the idea that the information retrieval community can (and should!) make use of general-purpose data management solutions, instead of building specialized backend technology. We demonstrate that storing posting lists as relations and expressing ranking models as SQL queries is now a viable alternative to cus-tom inverted indexes if we leverage modern column-oriented relational databases (or  X  X olumn stores X ).

The contribution of this work is to demonstrate empirically that we can now safely delegate the data management needs of IR engines to modern column stores without concerns regarding efficiency. We show experimentally that an IR engine built in this manner achieves not only effectiveness and efficiency on par with custom-built retrieval engines, but provides many additional advantages. This suggests a somewhat radical message: IR researchers should stop writing their own retrieval engines and just use column stores. We advocate this approach especially for rapid prototyping X  when developing new scoring models, features functions, etc. X  X ut such an architecture has additional advantages. Although we are not the first to make this claim, there have been a number of developments since previous work that make our design more attractive than before.
What advantages does using a database for IR have? We see many, beginning with a precise formal framework. As more complex query operators are introduced for document ranking, it sometimes becomes unclear how to properly score documents, particularly in corner cases. Developers often resort to heuristics and other shortcuts. Relational databases provide a formal and theoretically-sound framework in which to express any query evaluation algorithm X  X amely, rela-tional calculus (or, practically, SQL). This forces IR re-searchers to be precise about query semantics, which may be especially useful when complex query operators are intro-duced in document ranking.

Second, taking advantage of relational databases yields a cleaner architecture. Almost all IR systems today are monolithic agglomerations of components for text processing, document inversion, integer compression, memory/disk man-agement, query evaluation, etc. By offloading the storage management to a relational database, we introduce a clean abstraction (via SQL) between the  X  X ow-level X  components of the engine and the IR-specific components (e.g., learning to rank). This (hopefully) reduces overall system complexity and may allow different IR engines to inter-operate.
Third, retrieval systems can benefit from advances in data management. Performance is a dominant preoccupation of database researchers, who make regular breakthroughs that propagate to the IR community (for example, PForDelta [17] compression originated from database researchers). By using relational databases, IR systems can benefit from future advances more rapidly, and  X  X or free X .

Fourth, a database provides integrated analytical tools useful for error analysis. Consider a simple example, where we wish to examine whether our scoring model has a length bias. This might be accomplished by examining scatterplots of length vs. retrieval scores. With a database, generating these data can be accomplished with a straightforward join between qrels (easy to store in the database), document representations, and the results set. In contrast, with a custom-built retrieval engine, one would need to write ad-ditional code to dump out document lengths from internal data structures and perform the join in an ad hoc fashion. Furthermore, there is a rich ecosystem of external analytical toolkits that are able to interface directly with relational databases, which could also be helpful for IR researchers. Finally, databases form a flexible rapid prototyping tool. Many IR researchers do not really care about index structures and query evaluation per se  X  X hey are merely means to an end, such as assessing the effectiveness of a particular ranking model or feature. In this case, forcing researchers to design data structures and query evaluation algorithms is a burden. Using a relational database, researchers can rapidly experi-ment by issuing declarative SQL queries without needing to write (error-prone) imperative or object-oriented code.
Since retrieval operates over collections of documents, analytics-optimized (OLAP) relational databases are more appropriate than transaction-optimized (OLTP) databases. There are many similarities between query evaluation in document retrieval and online analytical processing (OLAP) tasks in modern data warehouses. Both frequently involve scans, aggregations, and sorting. Thus, we believe that column-oriented databases, which excel at OLAP queries, are amenable to retrieval tasks. An overview of such databases is beyond the scope of this work, but the basic insight is to decompose relations into columns for storage and pro-cessing [4]. This storage model allows us to mask random access memory latencies and take full advantage of modern hardware. We use two different column stores, MonetDB [2, 9] and VectorWise [16], to illustrate document ranking on a portion of the ClueWeb12 collection.

We are not the first to claim that databases may have something to offer for IR. Examples of early work include [13, 10, 6]; key references on runtime efficiency are discussed in a 2005 survey [3]. Perhaps the first  X  X eal X  SQL results for IR queries were presented in [7], but at excessively high costs in terms of the hardware required. More viable results were presented in the TREC 2006 terabyte track, but the approach required hand-written query plans [8]. Follow-up research by the same group allowed the retrieval model to be expressed at the conceptual level [5], however, using a rather  X  X xotic X  query language based on array comprehensions. After all these years, we have finally reached the state where database engines can take on IR workloads expressed in standard SQL, without forcing the database admin to resort to low-level tuning. As far as we know, this paper is the first report of experimental work where competitive IR results (in terms of both efficiency and effectiveness) have been obtained using standard relational database technology.
In a custom-built IR engine, a document collection must first be processed (e.g., tokenized) and indexed before re-trieval can be performed. In an architecture based on column stores, there are equivalent steps: the collection must be pro-cessed and loaded into the database prior to query evaluation. This section provides details on our system architecture.
We take advantage of the massive scale-out capabilities of Hadoop MapReduce to convert a document collection into a collection of relational tables. Document processing includes tokenization, stemming using the Krovetz stemmer, and stopword removal. The stemmed and filtered terms are mapped to integer ids and stored in a dictionary table. In the main terms table, we store all terms in a document (by term id), along with the position in which they occur. To give a concrete example, consider the document doc1 with the content  X  X  put on my robe and wizard hat X . After stopword removal, the relational tables generated from this document are as shown in Figure 1. In more detail, these tables are first generated as flat text files using a two-pass approach on Hadoop; the first pass builds the term to term id mapping, and the second pass builds the terms and the docs tables. These flat text files are then bulk loaded into the database.
We implemented Okapi BM25 as an SQL query, but our approach can be easily extended to other ranking functions. Our experiments focused on conjunctive query evaluation, where the document must contain all query terms; previous work [1] has shown that this approach yields comparable end-to-end effectiveness to disjunctive query evaluation, but is faster. When scoring documents based on BM25, the only score component that depends on the query is the term frequency f ( q i , D ). An obvious opportunity for optimization here would be to precalculate the term frequencies for each term/document combination, since the positions of the terms do not influence the ranking score for conjunctive queries. By also sorting the terms table by term id (in effect performing document inversion), the database is able to avoid scanning the entire table and instead use binary search, with greatly improved efficiency. The complete ranking function can be expressed in SQL, shown in Figure 2.
 We map conjunctive BM25 ranking to SQL in three parts: First, we find the entries in the terms table for the query terms (Lines 1 and 2). In this case, the query terms have ids 10575, 1285, and 191. 1 The second step calculates the individual scores for all term/document combinations (Lines 4-13). To express the conjunctivity in the query, we filter this intermediate result to include only combinations with exactly three different term ids. (Lines 9-11). We collect information about document ids and lengths (Line 12) as well as the document frequencies of the terms (Line 13). We calculate the individual BM25 scores for each term/document combi-nation (Lines 5 and 6), sum the results and sort (Lines 14 to 16). With the term frequency precalculation optimization, Figure 2: Conjunctive BM25 in SQL. The numbers printed in bold are the only parts of the SQL query that depend on the document collection and query terms. the grouping of the qterms CTE (Line 8) would be replaced by a straight selection from the term frequencies table. Note that this approach can be extended to any scoring function that is a sum of matching query terms. Other modifications are straightforward: disjunctive query evaluation can be im-plemented by replacing the number of matching terms in Line 10. Phrase queries can be performed by arithmetic over term positions and enforcing distance constraints.
The main point of this paper is that relational database technology  X  in particular, columnar storage  X  is suitable for rapid prototyping for IR. So far, we have shown how a retrieval model can be implemented without writing any im-perative code. In further support of this claim, we present ex-perimental results demonstrating that our approach achieves effectiveness and efficiency on par with custom-built retrieval engines. We compare two different relational backends to three open-source IR engines: the open-source columnar database MonetDB [9] (v11.17.13), the commercial database VectorWise [16] (v3.0.1); and Lucene (v4.3), Indri [14] (v5.5), and Terrier [12] (v3.5). Comparing MonetDB and Vector-Wise, the latter combines columnar storage with lightweight compression and a pipelining execution model, which makes it the current top performer on the well-known TPC-H bench-mark for OLAP databases, and especially suited for very large workloads.
 Our experiments used the first segment on the first disk of ClueWeb12 (  X  45 million documents) with queries 201 X 250 from the TREC 2013 web track. This setup is realistic, since production search engines usually adopt a partitioned archi-tecture (and we focus on a single partition). The qrels from the TREC topics were filtered to only include documents that are contained in the segment we used. To ensure that all the IR engines work on the same text, we used Hadoop to pre-process the documents in the same manner as in the rela-tional setup: this was accomplished by dumping the processed collection as plain text, turning off stemming/stopword re-moval in the IR engine, and tokenizing by whitespace. In all cases we retrieved the top 1000 results. All experiments were run on a Linux desktop computer (Fedora 20, Kernel 3.12.10, 64 Bit) with 8 cores (Intel Core i7-2600K, 3.4 GHz) and 16 GB of main memory.
 Effectiveness results are shown in Table 1. MonetDB and VectorWise produce exactly the same rankings and scores, which is of course to be expected, given that both execute the same SQL queries. However, the effectiveness differences between Indri and Terrier do come as a surprise, since both systems implement BM25 X  X hese differences cannot be at-tributed to tokenization and stopword differences, given the unified document processing step described above.

From these results we draw two conclusions: First, our architecture yields effectiveness that is at least on par with existing custom-built IR engines. Second, these results high-light the advantage of SQL in providing concise yet precise semantics. While we do not claim to have the  X  X orrect X  BM25 implementation, at the very least our model is concisely spec-ified in a few lines (the SQL query) and thus easy to inspect, not buried in code. Furthermore, different backends produce exactly the same results.

In terms of efficiency, we measured query latency. For each system, queries were run sequentially in isolation. As mentioned previously, pre-calculating the term frequency for conjunctive queries is an obvious optimization in the rela-tional representation, and halves the size of the terms table. In our experiments we examined this optimization indepen-dently ( X  X recalc X  vs.  X  X ull X ). As an additional optimization, we have placed the MonetDB database on a compressed file system (BTRFS with zlib compression, marked as  X  X  X ), since MonetDB does not natively compress data. This reduced the disk footprint of the MonetDB database to 1 / 3 of the original size. In addition, we consulted with the developers of the Terrier system after observing pathologically slow response times. They made two suggestions: 1) changing the index storage model from on-disk to in-memory, which effectively leads to parts of the index being copied into memory on startup, and 2) changing the retrieval model from term-at-a-time (TAAT) to document-at-a-time (DAAT). We did not include the first optimization here, as it makes the cold cache runs pointless (see below). However, the second optimization led to greatly improved performance. For fairness, we report on both the original configuration for Terrier (Orig) as well as on the optimized configuration (DAAT).

Figure 3 shows the query latency distribution across topics as box plots; the boxes contain the observations between the 25 th and 75 th percentiles. In addition, the plots are annotated with the median query latency. Since the indexes are stored on a hard disk, we expect disk I/O to have a large impact on performance. To test this, we ran the query set two times in succession. Before the first run, we asked the operating system to empty all caches and restarted the system under test. The two runs are presented in separate plots, the first ( X  X old X  caches) in (a) and the second in (b).
Considering cold runs, we see that Indri was the fastest system, taking a median of 0.56 seconds to complete a query. Second and third were VectorWise, with the precaclulated term frequencies clearly faster, followed by Lucene with a comparable time. MonetDB on a compressed file system and Terrier are comparable. The uncompressed MonetDB experiments clearly show the necessity of compressing an IR index. The vastly greater index size increased the cold response times substantially.

For the hot runs, Lucene leads the field by a large margin, which we suspect to be due to result caching. For Terrier, we see vastly improved timings for the DAAT setting when parts of the index are available in memory and do not have to be loaded from disk. We see that MonetDB (uncompressed) is able to take advantage of caching by the operating system, improving performance by about an order of magnitude compared to the cold runs.

Summarizing these results, it is clear that our database architecture achieves query latencies that are at least on par with existing custom-built IR engines.
This paper argues for the use of column stores for proto-typing IR systems. The well-defined relational model allows for the precise expression of retrieval models independent of system implementations. Furthermore, clear abstraction via the declarative SQL interface allows the backend database engine to be swapped, allowing IR systems to quickly bene-fit from advances in the database field. We have described how to express a standard ranking function using an SQL query and experiments show that using a column store yields effectiveness and efficiency that are at least on par with custom-built IR engines. Hopefully, this will inspire addi-tional IR researchers to think about using columnar databases for building future systems, as doing so would let us focus on the questions on how to best satisfy information needs.
This research was supported by the Netherlands Organi-zation for Scientific Research (NWO project 640.005.001), the Dutch national program COMMIT/ , and the U.S. National Science Foundation (IIS-1144034 and IIS-1218043). Any opinions, findings, or recommendations expressed are the authors X  and do not necessarily reflect those of the sponsors.
