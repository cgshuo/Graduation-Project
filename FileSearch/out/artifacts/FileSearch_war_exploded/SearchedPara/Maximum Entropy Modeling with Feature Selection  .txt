 Text categorization is the process of assi gning predefined categories to textual docu-when we know the categories of the web pages can we decide whether they are offen-been conducted with different machine learning techniques, including Na X ve Bayes [4], k-Nearest Neighbors [3], Linear Least Squares Fit [8], Support Vector Machines [1], and Maximum Entropy Modeling [5]. 
This paper explores the use of different feature selection methods for text categori-zation in the context of maximum entropy modeling. In addition to some commonly-and we want to see how these feature selection methods can affect the performance of a text classifier based on maximum entropy modeling and how is maximum entropy modeling is compared with other popular text categorization methods. Most text categorization systems use word types and their frequency counts for document representation. We refer to these word types as features. Feature selection not only reduces computational cost in space and time, but also improves performance by carefully selecting good features for classification [9]. 2.1 Existing Feature Selection Methods Document frequency stands for the number of documents in which a feature occurs in a document collection. This method favors features whose document frequencies fall into a mid-range, since low-frequency features do not contribute much to the distinc-tion of most documents and high-frequency features are so common that they reduce the distinction between documents.  X  2 ranking [3] favors features that are strongly dependent on relevant or irrelevant classes. One problem with this method is that it may give a high score to a rare fea-100,000 documents, but if all these 5 documents belong to the relevant class, the fea-ture may still get a high score, which is counter-intuitive.

Likelihood ratio attempts to address the issue of assigning high scores to rare fea-ranking, but it also works well for a small sample size. 
Mutual Information only measures the dependency between a feature and its rele-vant class, and as a result, it tends to favor rare terms if they are mostly used for rele-vant documents [9]. 
Information Gain is a measure based on entropy, which has been successfully ap-entropy the most are favored for this method. 
Orthogonal Centroid chooses features by an objective function based on transfor-orthogonal centroid algorithm, an optimal orthogonal centroid algorithm was pro-posed to provide a simple solution for feature selection [7]. 
Term Discrimination tries to measure the ability of a feature for distinguishing one document from the others in a collection [6]. A very popular feature often has a nega-tive discrimination value, since it tends to reduce the differences between documents, while a rare feature usually has a close-to-zero value, since it is not significant enough to affect the space density. 2.2 Count Difference method called Count Difference (CD), which tries to reflect the above two factors in ranking features. 
Given a feature, we can partition the set of training documents into four regions in the following contingency table. of the document frequency of a feature for one class over the average document fre-quency for the same class: 
Here, a and b denote the average document frequencies for the relevant and ir-relevant classes, which are computed as follows: where M is the number of original features before the selection process. 
With the relative document frequencies, we can then define the count difference score of a feature as the difference between its two relative document frequencies: frequency will be low, whereas if a feature is popular, its relative document frequency frequencies for one class are higher than those for the other class. If a feature is popu-lar for both classes, its count difference score will be reduced. Maximum entropy provides a reasonable way of estimating probability distributions from training data. The key principle is that we should agree with everything that is form as possible (thus the maximum entropy). 3.1 Feature Functions Following Nigam et al. [5], we represent f eatures as feature functions. For text cate-gorization, we can define a feature function for each word-class combination: Here, N(d, c) is the number of times word w occurring in document d, and N(d) is the number of words in document d. 3.2 Log-Linear Models A maximum entropy model generally takes the following parametric form: is normalization constant. The model is also called the log-linear model, since by taking the logarithm on both sides, we get a linear combination for the feature functions. 
The log-linear model above allows overlapping/dependent features. Although we use individual words as features for text categorization, we could easily extend the set pages). Even for individual words, we do not require them to be independent as the case for Na X ve Bayes method. by carefully assigning weights  X  i to different feature functions. In particular, by set-ting  X  i = 1, we essentially eliminate that feature in the combination process. categories being CCAT (Corporate/Industri al), ECAT (Economics), GCAT (Govern-ment/Social), and MCAT (Markets). We use F 1.0 (harmonic mean of recall and preci-sion) to measure the classification performance. For multiple categories, we use both macro-average (per-class average) and micro-average (per-document average). 4.1 Classification Performance and hierarchical classification (only the doc uments belonging to a parent category are tested further for its sub-categories). We choose the hierarchical classification, since it is naturally suited for a hierarchical scheme. 
We use the Improved Iterative Scaling algorithm [5], which iteratively updates the reached. To avoid over fitting to the training data, we can terminate the process after terminate the training after 50 iterations. 
Table 2 shows the classification results based on the above setting, where the val-ues are the micro-averages of F 1.0 measures. First, we see that the classification per-formance goes up for all selection methods as we increase the number of features but computational cost but also improves the performance. methods. We can roughly divide the selection methods into several groups. The best group contains Document Frequency, Count Difference, and Optimal Orthogonal 0.795 with 1000 features. The group with the highest overlaps of featuers, including values and improve slowly to reach their peaks. Note that Mutual Information (used in Nigam et al. [5]) has the worst performance since it tends to favor rare features. 
Finally, we see that the performance between different selection methods become selection as long as most important features are included in the pool of features, since features used for text categorization. 4.2 Comparison with Other Methods Reuters RCV1 data set provides benchmark results for several text categorization methods, including Support Vector Machines, K-Nearest Neighbors, and Rocchio-style classifiers. macro-and micro-averages. We see that MaxEnt (maximum entropy modeling) is a Nearest Neighbors and Rocchio-style classifiers in terms of micro-averages, although the performance is still not as good as that for Support Vector Machines. In terms of conclude that MaxEnt tends to perform be tter when a category is adequately covered terms of maximum likelihood estimate (N(d,w)/N(d)). Future work remains to smooth this probability with data from parent categories. maximum entropy modeling. We showed that feature selection is an effective way of performance. We demonstrated that our own feature selection method, Count Differ-ence, is promising for text categorization, not only achieving the best performance but trated that maximum entropy modeling is a competitive method for text categorization and has potential for further improvements. 
