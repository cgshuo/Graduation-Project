
Pingfu Chao 1 , 2 ,ZhuGao 2 , Yuming Li 1 , 2 , Junhua Fang Entity matching aims to identify entities referring to the same real-world object. However, the rapid growth of web data and User Generated Content (UGC) brings new challenges for entity matching. For instance, in the scenario of C2C (Customer to Customer) online markets, as the rarity of descriptions, missing of uniform schema or intended errors generated by users, tradition entity matching methods are not able to get good match performance.
 Though MapReduce provides a new platform for solving massive entity matching problem, new challenges occur: load balancing problem and net-work transmission cost. Blocking-based entity matching algorithms have been presented to deal with the imbalance problem. Some of the most influential works include sorted neighborhood-based and load-balanced entity matching in Dedoop[ 3 ], and document-similarity computation[ 1 ]. But for processing non-structured data, these kinds of work meet high network cost and computation cost.
 This paper sketches out a random-based framework for entity matching based on MapReduce for semi-structured and unstructured data. Inspired by previous studies, our method expects to reduce both the computation cost and network transmission cost whilst promises the processing performance. We convert high dimensional entity features into low dimensional bit vector by Locality Sensitive Hash (LSH) function in map phase[ 4 ], which reduces the network transmission cost dramatically. We do t rounds of random permutations to those bit vectors. It helps to make similar items paired with high probabilities. Our random-based design can also ensure load-balanced during matching process. Finally, we design a new solution for removing redundant computation in reduce phase. Our entity matching framework is shown in Fig. 1 . We represent each entity by its high dimensional feature vector generated from the structured, unstructured or semi-structured description data, if any. These vectors are the input of our MapRe-duce job, as shown in Fig. 2 . The first round of MapReduce job implements the Entity Matching job, while the second one realizes the Redundancy Control. Entity Matching. The input is a set of ( key , value ) pairs with the entity ID as its key and its k -dimension vector V u as its value. In map phase, we generate a signature for each item u using the LSH function h r defined in Eqn. 1 . We gener-ate a random k -dimension vector set V r with | V r | = d between u and every vector in V r using h r , we get a d -bits vector for item u , d k . Then we apply t rounds of random permutations to every signa-ture S u and get t different d -dimension bit vectors { P this result as our map output. So for each entity u ,wehave t different map outputs as ( i, P ui ,E u ), in which i refers the permutation series number ( the i th permutation result, and E u is the entity ID. In Reduce phase, each reducer receives permuted signatures of the same series number. It sorts all signatures and generates pairs between each signature and its m nearest neighbors. Then we calculate the hamming distance of every pair. We output the entity pairs with their similarities as ( E u&lt;v . We use the LSH function preserving cosine similarity [ 2 ] to generate a signa-ture
S u for each entity u . Since the signature carries most of the characteristics of a vector, we can measure the similarity of two vectors by comparing their signatures. We use the hamming distance between two signatures to represent the similarity, which is reasonable and well proved [ 4 ]. In reduce phase, we pro-pose a random permutation algorithm inspired by PLEB algorithm[ 4 ] to ensure entities with high similarity to be paired with high probabilities. Redundancy Control. There can be many duplicated pairs in different groups during reduce phrase as marked in Fig. 2 . It may cause significant redundant computation cost. We introduce an extra MapReduce job to reduce duplication. In reduce phase of the first MapReduce job, we remove the similarity computa-tion step, and directly send all the pair-wise data to the second MapReduce job. The second map job does nothing. After the shuffle phase, all pairs with the same entity IDs are grouped together. So we pick one pair of permuted signatures in the group and calculate its hamming distance on behalf of the others. At last, we output the similarity ( E u E v , similarity ) as our result. We run experiments on a 22-node HP blade cluster. Each node has two Intel Xeon processors (E5335 2.00GHz) with four cores and one thread per core, 16GB of RAM, and two 1TB hard disks. All nodes run CentOS 6.5, Hadoop 1.2.1, and Java 1.7.0. We use CiteSeerX data set, which contains nearly 1.32 Million citations of total size 2.89 GB in XML format. Each citation includes record ID, author, title, date, page, volume, publisher, etc and also abstract . We compare the performance of our algorithms with Document Similarity Self-Join (DSSJ)[ 1 ] and Dedoop[ 3 ]. We use accuracy and run-time metrics to evaluate performance. In order to measure the accuracy, we manually generate a validation set which contains 200 records. We output the top 10, 20 and 50 similar pairs for each algorithm. Since Dedoop compares all possible pairs and calculates cosine similarity directly, Dedoop is the best as in Fig. 1 . Ours achieves better accuracy than DSSJ with much less computation cost as in Fig. 3 . For processing speed, since Dedoop and DSSJ generate enormous size of pairs, they cost much network transmission and bring big burden for in memory processing as in Fig. 3 .Inour experiment, the transmission data generated by Dedoop or DSSJ is up to several Name Top 10 Top 20 Top 50 DSSJ 90% 95% 94% Ours 90% 100% 94% Dedoop 100% 100% 100% terabyte for 200MB source data. However, our algorithm is significantly faster than Dedoop, and far more stable even dealing with gigabytes of input data. In this paper, we study the problem of matching the entities with high-dimensional feature vectors based on MapReduce. We take the MapReduce framework as our programming model and point out the two major challenges met on this model, which were load balancing problem and network transmis-sion cost. We propose a random-based matching method to solve the matching problem. We use LSH function to generate signatures for entities and based on random permutations, we can promise similar candidate to be paired with high probabilities. Given the proposed algorithm, we implement it in Hadoop and compare with the other algorithms. We achieve much lower computation cost while still keep high accuracy.
 Jia Zhu 1 , Pui Cheong Gabriel Fung 2( One of the most important properties of social networking sites is its reachabil-ity  X  no physical location constraint. In addition, all social networking sites allow us to search people with common interests, so we can find friends anywhere in the world easier than ever. With the help of social media, it seems that expaning our social networks is physical location independent. Motivated by the above observa-tions, we study the role of physical location in social media. If physical location is no longer a barrier and physical interaction can be ignored, then our online social networks should have the following characteristics: (1) A number of our friends are from different places in the world other than the places that we have been; (2) A number of our friends are not from our physical social circles  X  they are not our colleagues, not our high school friends, etc. Let
G =( U,E ) be a social network. U = { u and
E = { e an edge represents two individuals are friends in G . The one-hop network of is called friendship network of u i , and is denoted by F collection of the names of organizations that are listed in and/or employment history. Given u i and u j , u i can reach u  X  X  profile is not hidden to u i . We randomly picked 20,000 user profiles from Facebook and crwal their friends and finally obtained round 2 million profiles. 2.1 Education History and Employment History Analysis Question: if u x and u y are friends, then what is the probability that from: (1) the same school? (2) the same employer? (3) both? If the probabilities are high, then physical social circle plays an important role in the structure of a social network. Figure 1 shows the result. The probability of  X  X riend bar X  is far more than the  X  X andom bar X .
 and education histories (28%). We may consider the result in this section is a baseline case. (2) two friends who are studying/working in the same school/company does not necessarily mean they must know each other via in-person. Yet, this does not affect our conclusion: physical location plays an impor-tant role in online social network. 2.2 Home Town and Current City Analysis Question: if u x and u y are friends, then what is the probability that have: (1) the same home town and/or the same current city? (2) the same home town, current city, school and/or employer? Figure 2 shows the results. From the findings we obtained, we conclude that geographical location does play a significant role in our online social network, even thought social media in theory does not have any physical boundary.
 cities but does not provide any information about what other cities a user has been. E.g., u a and u b are friends. u a  X  X  home town and current city are both San Jose and u  X  X  home town and current city are both Phoenix. Suppose u other because they both lived in Rochester for a while. In their Facebook profiles, we only know u a and u b are friends but never realize they both stay in the same place. We may underestimate the importance of geographical location. 2.3 Language Analysis Question: given a user, what is the probability that a friend of this user uses the same language setting? Figure 3 shows the result. Overall, given a random user, 83% of his/her friends will use the same language code. This percentage is surprisingly high. Almost for all languages, the probability that two random users who are friends will set the same language as their default one will always be &gt; 70%. There are a few exceptions. The most obvious one is Malaysian. There are 2,386 users set Malaysian as their default Facebook language. For a user who set Malaysian as his/her default language, only 20.1% of his/her friends will also set Malaysian as default, where as 70.8% of his/her friends would use English. These finding strongly suggested that language also plays a significant role in determining how our social networks in a social media are shaped. In addition, language is somehow also location dependent. 2.4 Further Discussion Question: what is the probability that two random people are friends given that they have common interest? If the probability is high, it may implies social media is an important place for people to find and make friends with common interest. It may then suggest that physical interaction may not be too important in some cases. We randomly join 300 groups that are related to some particular interests in Facebook and then compute the probability that two random people from the same group are friends. Figure 4 shows the result. The larger the group, the less likelihood that two random people from the group are friends. Perhaps most people joining some common-interest-groups is for sharing and obtaining information, but not for acquiring new friends. There is indeed a very strong relationship between physical locations and our online social networks. In theory there is no physical boundary in social media, few of us will try to utilize this property when we use social media. The reacha-bility of social media is strong but the expandability is limited. This may further suggest that most people use social media sites are not aimming at expanding their networks, but reconnecting some lost networks or maintaining their existing ones. The problem similar to, but in fact quite distinct, to ours is the problem related to homophily [ 1 ]: given two people are connected, are they similar to each other? Their primary goal not to answer questions such as: what is the role an attribute (e.g., physical location) plays in the structure of a network?
