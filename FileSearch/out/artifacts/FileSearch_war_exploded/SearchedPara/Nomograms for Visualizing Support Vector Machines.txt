 We propose a simple yet potentially very effective way of visualizing trained support vector machines. Nomograms are an established model visualization technique that can graphically encode the complete model on a single page. The dimensionality of the visualization does not depend on the number of attributes, but merely on the properties of the kernel. To represent the effect of each predictive feature on the log odds ratio scale as required for the nomograms, we employ logistic regression to convert the distance from the separating hyperplane into a probability. Case studies on selected data sets show that for a technique thought to be a black-box, nomograms can clearly expose its internal structure. By providing an easy-to-interpret visualization the analysts can gain insight and study the effects of predic-tive factors.
 G.6 [ Probability and Statistics ]: [Multivariate Statis-tics]; H.5.2 [ Information interfaces and presentation (e.g., HCI) ]: User Interfaces X  Theory and methods Theory, Human Factors nomogram, visualization, support vector machines, machine learning
Within predictive data mining, methods that build classi-fication models have received much attention. These meth-ods consider a set of class-labelled data instances and in-duce classification models that should both predict well and, preferably and through the model inspection, can uncover interesting relations and patterns. The latter is particu-larly important when predictive data mining is used for knowledge discovery, where presentation of the classifica-tion model should help the user to answer questions such as  X  X hich are the most important factors that determine the class of the instance? X , and  X  X hat is the magnitude of the effect of these? X , and  X  X ow do various factors interact? X , and alike.

A support vector machine (SVM) [23, 22] is a popular and much applied supervised machine learning method. It is known for good predictive performance, but may be at a disadvantage in terms of intuitive presentation of the clas-sifier, particularly when compared to some other supervised learning techniques like classification trees and rules. While an SVM model can be presented as a weighted list of support vectors, as a subset of learning instances that defines the de-cision boundary, this only reduces the number of instances to consider in the interpretation but does not answer any of the questions posed above directly. It is possible to show the SVM classifier directly in the attribute space, but this is only appropriate when the attribute space does not have more than two or three dimensions. When the SVM model is a hyperplane, we can also present it with the hyperplane X  X  normal vector, but this technique is of limited utility with multi-valued or continuous attributes.

In the paper, we propose a new approach for visualization of SVM models. The main advantage of our approach is that it captures a complete classification model in a single, easy-to-interpret graph and for all common types of attributes and even for non-linear SVM kernels. The particular model visualization we use is called a nomogram . Nomograms were invented by French mathematician Maurice d X  X cagne in 1891 to graphically represent a class of mathematical func-tions. In the beginning of 2005 a search for  X  X omogram* X  on PubMed/MEDLINE, a database of biomedical article cita-tions, yielded over 2400 papers (a search for  X  X upport vector machine* X  yielded fewer than 400). A search for nomograms on Google resulted in 77000 web pages. Nomograms are not an uncertain novelty, but a milestone in the history of visualization [6].

To visualize a logistic regression model, the use of nomo-grams was first proposed by Lubsen and coauthors [17]. With an excellent implementation of logistic regression nomograms in S-Plus and R statistical packages by Har-rell [7], the idea has recently been picked up and the nomo-grams have been used much to present probabilistic classi-fication models in, for instance, clinical medicine and on-cology (e.g., [15]). A na  X   X ve Bayesian classifier can too be visualized in the form of a nomogram [19].

The nomograms for support vector machines that we in-Figure 1: A nomogram of the SVM model that pre-dicts the probability of costly housing in a given Boston area. The dots illustrate the classification of a specific instance. troduce in the paper use a similar presentation as those of Harrell for logistic regression. To illustrate the general idea, consider the nomogram in Figure 1 which represents a lin-ear SVM model induced from the Boston Housing data set (StatLib, http://lib.stat.cmu.edu/datasets/ ,alsosee [8]). The Housing data set consists of 506 different instances (areas of Boston); about 50% of the areas have the median value of housing price lower than $21000.

For convenience of this presentation we use only four rep-resentative attributes: the average number of rooms per dwelling ( Rooms ), weighted distances to five Boston employ-ment centers ( Employment dis ), pupil-teacher ratio by town ( Pupil-teacher , discretized to two nominal values), and pro-portion of lower status population ( Low status , discretized to four nominal values). There are two classes of areas, the expensive with the median values above $21000, and the cheap. To make a prediction using a nomogram, the con-tributions of attributes on the scale of the log odds ratios [11] (topmost axis of the nomogram) are summed up, and used to determine the probability whether the price is less than $21000 (bottommost axis of the nomogram). For in-stance summing the effects of 6 rooms per average dwelling, unknown distance from employment centers, a high pupil-teacher ratio and a high rate of lower status population re-sults in the log odds ratio 0 . 21 + 0 . 0+0 . 49 + 0 . 5=1 . 20 on the  X  X og OR Sum X  axis. This sum is then projected to the bottommost  X  X ( &lt; = 21) X  probability axis, yielding the final probability of the target class of approximately 0.76. On the other hand, if the area was known to be far away from employment centers (12.5), Employment dis contribution to final sum would be around 1.5 instead of 0, and the final probability would be higher than 0.93.

Besides prediction, nomograms provide a clear and com-prehensive presentation of the underlying model. Our SVM nomogram from Fig. 1, for instance, clearly exposes that the housing values in Boston from a particular data set are most associated with the average number of rooms. The corresponding line in the nomogram is the longest, and try-ing to predict housing values for a certain area simply with the information that the average number of rooms is small (3.2), the probability for price under $21000 jumps from a priori 0.5 to over 0.9 a posteriori. The other three at-tributes carry less importance, especially the pupil-teacher ratio. The nomogram also exposes how different attribute values affect the outcome; for instance, the value of hous-ing goes up when the employment centers are nearby. Note that we can include continuous as well as discrete attributes in the nomogram. The nomogram also clearly exposes the  X  X eutral X  values of the attributes near 0.0 on the Log OR axis. They do not affect the probability of the outcome. If a particular attribute value is not given for the test instance, it is these neutral values that will be effectively imputed.
Nomograms  X  like the one from our example  X  are used to assess the probability of the observed outcome, where the effects of the attributes are independent given the class and are added up to form the final prediction. Assume an in-stance [ x ,y ], where the range of the label is assumed to be y = { X  1 , 1 } ,and x  X  X is described by a set of attributes A = { A 1 ,A 2 ,...,A k } . The nomogram can visualize a prob-ability function of the type where  X  0 is the intercept , a constant delineating the prior probability in the absence of any attributes, f j is an ef-fect function that maps the value of an attribute A for the instance x into a point score, and F is the inverse link func-tion that maps the response of an instance into the outcome probability. The nomogram in Fig. 1 is based upon one ef-fect function for each attribute. Each line in the nomogram corresponds to a single attribute, and a single effect func-tion. Because the effect function f Low status (high) = 0 . 05, the tick corresponding to the value  X  X igh X  for the attribute Low status is aligned with 0.05 on the  X  X og OR X  axis.
The class of models of the above type are the generalized additive models (GAM, [9]). When each effect function is linear, we speak of a generalized linear model (GLM). For a GLM, the response or the systematic component is written as  X  0 + j [  X  ] j [ x ] j ,where[ x ] j is the j -thcoordinateofthe vector x . Using the dot product  X  ,  X  we may express it more simply as  X  0 +  X  , x . We may refer to the vector  X  as the effect vector .

We start by showing how support vector machines based on appropriate kernels can be decomposed into the above additive model. To enable the use of the nomograms for support vector machines, we need them to predict outcome probabilities. The basic SVM alone does not attempt to model the probability, but attempts to achieve the sepa-ration of instances in the feature space with a separating hyperplane, each side of which represents a different class. Therefore, the effect functions need to be calibrated and thus placed on the log odds ratio scale. In the experimen-tal section we examine the performance of linear SVM in comparison to other methods that can be visualized with nomograms. We also compare linear SVM to the SVM with the RBF kernel, which cannot be visualized with a low-dimensional nomogram, observing that the losses are not very large. We also show that nomograms are suitable for graphically comparing support vector machines to other generalized additive models, such as the na  X   X ve Bayesian clas-sifier and logistic regression.
Not every support vector machine is appropriate for vi-sualization using a nomogram. The first requirement is the ability to additively separate the contribution towards the response of an individual attribute or of a small group of attributes as in (1). We achieve this goal by using a kernel of a particular family. The second requirement is related to how we represent the lack of information: ideally, zero value of the transformed attribute should indicate missing attribute values.

Our approach to visualization will take the following steps, which will be addressed in detail in the subsequent sections. 1. Transform each instance x  X  X into the feature space 2. Train the support vector machine using the dot prod-3. Employ univariate logistic regression to obtain two pa-4. Collect the terms of the effect vector that belong to 5. Visualize the effect functions and the intercept in a A Simple Example. As an example, we have taken the Fisher X  X  iris data, selected the I. versicolor and I. vir-ginica species, and the petal length and the petal width attributes. We have trained the SVM model with a linear kernel with both attributes standardized, and have used the cross-calibration to obtain probabilistic outputs from SVM. The top-down visualization of the data and the model to the left of Fig. 2 should be familiar. The true SVM X  X  sep-arating hyperplane would lie at the contour corresponding to the probability of approximately 0.49. The hyperplane is specified with the equation 1 . 82 length +3 . 64 width =15 . 21. The vector [1 . 82 , 3 . 64] T can be understood as a weight vec-tor, but we should note that it depends on the scaling of the attributes: it should not seem that the width is more than twice as important as the length. On the other hand, the nomogram shown to the right of Fig. 2 clearly shows the effect of individual attribute values on the outcome. Unlike the top-down view which is restricted to two attributes and two dimensions, we can include a larger number of attributes in the nomogram without increasing the dimensionality.
Support vector machines can be applied purely with a kernel function K ( x , x ) and the resulting Gram matrix. However, nomogram visualization requires us to concern ourselves with the kernel map from the instance space X into the feature space H using the reproducing kernel map  X : X  X  X  . First we will discuss various kernel maps. Later we will describe the notion of a decomposable kernel, which is limited by the number of dimensions required for visualizing the resulting classifier in the form of a nomogram.
All attributes need to be transformed into real-valued variables before a model can be trained. We standardize continuous attributes so that zero implies the mean, and  X  1 implies one standard deviation distance from the mean. Some m -th coordinate of the linearly transformed instance [ X ( x )] m equals the standardized value of a continuous at-tribute ( A ( x )  X   X  A ) / X  A . SVM based on a purely linear ker-nel and logistic regression both have linear effect functions, and can therefore be seen as generalized linear models.
However, attributes may have non-linear effects on the outcome. For example, both very high and very low body temperatures indicate risks when predicting the health sta-tus, and this pattern cannot be captured by a single real-valued variable. We can allow for the nonlinear effect func-tions by employing non-linear kernel maps. This way, a sin-gle attribute is internally represented with more than one dimension, and the actual support vector machine can be trained using the dot product on the feature space H , but not on the attribute space X .

The simplest example of a non-linear map relates to handling multi-valued nominal attributes. A discrete at-tribute B with V values is transformed into a set of V fea-B = b v +1 ,[ x ] m + v = 1 and  X  j =0 ,...,v  X  1 ,v +1 ,...,V : [ x ] m + j = 0. Thus, the kernel map assigns its own dimension to each attribute value, and also provides ground to inter-pret setting all corresponding [ x ] m + j to zero as a missing value.
 The same concept can be applied to continuous attributes. Using discretization , we convert a continuous attribute x into a V -valued discrete one, each value of which corre-sponds to an interval of the range of x . Thisisanextremely simple method for handling nonlinear effects. For example, we could discretize the body temperature into a 3-valued nominal attribute with the range { &lt; 36.6 , 36.6-37.4 ,&gt; 37.4 The corresponding effect vector [ b 1 ,b 2 ,b 3 ] T is obtained from SVM, and the effect function f ( x ) then takes the following form: Discretization essentially involves modelling the effect of an attribute with a piecewise-constant function.

Using polynomialization we transform a continuous at-tribute x into a vector of features [ x, x 2 ,...,x d ] T responding effect vector [ a 1 ,a 2 ,...,a d ] T results in a polyno-mial effect function for x : f ( x )= a 1 x + a 2 x 2 + ... + a Other forms of transforming continuous attributes may be employed while maintaining the dot product kernel. Such functions can be easily rendered inside the nomogram, so nomogram (Right). that f ( x ) is shown on the horizontal axis, and x on the ver-tical axis or using a label.

In addition to discretization and polynomialization, other univariate expansions can be employed, such as piecewise-linear functions, splines, sigmoids, or even univariate radial-basis functions.
 ous attributes can be shown on a single line as in Figs. 1 and 2. However, non-linear effect functions, especially non-monotonic ones, could be confusing if presented in such a way. An alternative approach is illustrated in Fig. 3, where the effect of an attribute is presented in the form of a two-dimensional graph. The vertical dimension is used to list different values and the horizontal dimension shows the ef-fect of the value on the outcome. The graph reveals how the attribute X  X  impact on the outcome probability gradually changes as its value changes from the lowest to the high-est interval. This kind of presentation is also suitable for ordered discrete attributes.

We have used the  X  X orse Colic X  data set from the UCI repository [10]. Among the attributes, we have chosen the respiratory rate and the body temperature, as they are con-tinuous attributes with potentially non-linear effects. It is clear that there is a particular range of normal body tem-peratures centered near 38  X  with low risk. The deviations in any direction (fever, hypothermia) carry increased risk. The pulse appears more monotonic, but the effect of the pulse is distinctly non-linear with respect to the pulse scale.
The intervals for the piecewise constant effect function were set manually. The RBF effects are defined through 4 radial bases, covering separate intervals of an attribute X  X  range.
Discretization and polynomialization correspond to non-linear kernels, but the non-linearity is always restricted to within a single attribute. We will now employ an example to show why general non-linear kernels introduce problems for nomogram visualization. Let us focus on the quadratic kernel x , x 2 . Specifically, for an instance x =[ x 1 ,x in a two-dimensional continuous attribute space, we can in-troduce the following kernel map  X ( x )=[ x 2 1 , Then the quadratic kernel can be linearized [22]: We can see that in addition to polynomializing each at-tribute to the degree of 2, the quadratic kernel introduces interactions involving each pair of attributes and the label, corresponding to the coordinates x 1 and x 2 . The effect func-tion would take the form of f ( x 1 ,x 2 ) so that one attribute X  X  effects only appear in a single place. Otherwise, the effect of x 1 would appear under [ x ] 2 corresponding to x 1 x 2 der [ x ] 1 corresponding to x 2 1 , and under the intercept term. Such effect functions are more difficult to be effectively vi-sualized in two dimensions: the effect of x 1 depends on the value of x 2 . Such visualization would involve simulating the third dimension either with color or with shape on a two-dimensional computer monitor.

Of course, non-linear kernels can be used for nomogram visualization as well, under some restrictions. We will now describe a general form of a kernel suitable for visualization. Assume a partitioning of the set of attributes X into m disjoint subsets S 1 , S 2 ,..., S m ,sothat m i =1 S i = interactions between attributes happen within each subset S , but not across the subsets. We can then visualize any SVM based on such a kernel K that is expressible in terms of such a sum: temperature all indicate an increased risk of death. A kernel that can be expressed in such a way will be referred to as an (additively) decomposable kernel .Here,  X  K S i arbitrary nonlinear positive semidefinite sub-kernel that acts upon [ x ] S i , the subset of the coordinates of x that corre-spond to the attributes in S i . The full reproducing kernel Hilbert space is then a concatenation of the reproducing kernel Hilbert spaces for each  X  K S i . It is then possible to re-trieve the the effect functions f ( S i ), localized for each subset of interacting attributes. The dimensionality of the result-ing nomogram visualization is max i |S i | +1. The kernel (2) is a special case of the kernels proposed by [5, 1, 16]. In particular, [5] motivated the choice of these kernels through the ability to effectively visualize them.
 ison between models that use interactions in the learning phase and models that do not. If no interaction is assumed, we employ the linear kernel. If an interaction between at-tributes A and B is assumed, we employ the following sub-kernel: We have used German credit risk data set that contains 20 attributes and 1000 past applicants. Each applicant was classified according to the risk (high vs. low). The risk is low if the applicant is very likely to return the money, and high otherwise. Due to space restrictions, we have used only 6 attributes: the status of applicant X  X  account in the bank, duration of the credit in months, purpose of the credit, the amount of credit asked for, the duration of the applicant X  X  present employment, and applicant X  X  duration of residence. The joint effect can be illustrated in the nomogram by pick-ing one attribute as the  X  X ontrol X , a condition. The other at-tribute X  X  influence is then interpreted in the context of the control. We examined the effect of employment duration , controlling for residence duration . This pair of attributes in  X  X erman-credit X  has been identified as significantly interact-ing, using the methodology of interaction analysis [14].
Both nomograms are very similar when comparing the first four attributes. Among those four, the most influential attribute is the purpose of the credit: buying used cars in-curs low risk, and education incurs high risks. The difference between two model occurs in the effect of unemployment on the risk: without the interaction, the model regards unem-ployment as almost unimportant, while the second regards it as highly important for determining low risk (if residence duration is higher than 2.5 years) and for determining high risk (if residence duration is less than 2.5 years). It is prob-able that people that live at this place for more than 2.5 years are unemployed because they do not need or can not work, i.e. are retired, while people that are residents for less than 2.5 years are unable to find a job. This comparison stresses the importance of interactions and shows that they can be effectively visualized with nomograms.

Through this more complex example we show that apart from revealing the structure of the SVM classifier, nomo-grams may be used as a data mining tool to depict differ-ent properties of problem domains. Gunn and Kandola [5] present examples of how interactions of real-valued variables can be visualized using 3D plots, but not in the context of the nomograms. duration on the assessment of credit risk.
Given N training instances [ x ( j ) ,y ( j ) ], j =1 , 2 ,...,N ,the resulting support vector model can be described with the vector  X  and the bias b . The (signed) separating hyperplane distance of an instance [ x ,y ] is denoted as  X  ( x ). Given a dot product kernel x , x ( j ) , the distance be described as: Here, b is the bias, while the scaling constant  X  assures that the distance is Euclidean. The sign of the hyperplane dis-tance indicates the predicted value of the label.
Because of the kernel map  X , we work in the feature space with the dot product kernel. We can now remove the ref-erence to support vectors x (  X  ) , and represent the distance with the bias b and the vector w . We now define the weight vector w . It has the same dimensionality as the feature space, and is defined as: The length of the weight vector w thus obtained is not 1. For that reason we needed the normalizing constant  X  in (3), where  X  =1 / w . The signed hyperplane distance of an instance x canthusbeexpressedas To simplify further notation, b = b / w , w = w / w . The w is the separating hyperplane normal.

If we are working with a non-linear kernel of the form (2), it is easy to see that a particular coordinate [ w ] k corre-sponds to one or more attributes, but only within a single group S i . Of course, multiple coordinates can correspond to a single attribute if nonlinear univariate sub-kernels are used, and one coordinate may correspond to multiple at-tributes if nonlinear multivariate sub-kernels are used. This way, (5) can be written as: Therefore, each sub-kernel  X  K S i is independently linearized by  X   X  S i . The approach works even if  X  K is an RBF kernel in-volving a potentially infinite-dimensional reproducing kernel Hilbert space: the dimensionality of  X   X  S i ([ x ] S i ) will depend on the size of the data set, but will be finite for a finite data set.
The horizontal scale in nomogram-based visualizations is based on the probability of the label. However, the signed hyperplane distance  X  ( x ) of an instance x has no probabilis-tic meaning. This is the role of the link function. The link function connects probability (the random component) with the response (the systematic component). The link function in classification maps a probability p intoaresponse d .The inverse link function F instead maps a response d into a probability. The most frequently used link functions are the identity( p )= p , probit (the inverse of the cumulative Gaus-sian distribution) and logit( p )=log( p/ (1  X  p )). The inverse logit link function is F ( d )=1 / (1 + exp d ), and it has been used in the past [21].

While the logistic regression too employs a generalized linear model with the logit link, the effect vector  X  is op-timized directly in order to minimize the probabilistic loss (deviance) of the resulting model. The hyperplane distance  X  does not attempt to optimize the calibration performance using the logit link, merely achieve the separation. For that reason, Platt linearly transforms the SVM output with two additional parameters,  X  and  X , using a procedure that re-sembles univariate logistic regression with the hyperplane distance acting as the independent variable, and the label as the dependent variable. The two parameters ensure that F ( d )basedon d = X   X  ( x ) +  X  is a well-calibrated proba-bilistic classifier using the logit link.

It often happens that the separation of the support vec-tor machine on the training set is perfect. In such a case, the inverse of the logistic link will tend towards a step func-tion. However, on a separate test set, the same performance is rarely as good. For that reason Platt [21] proposed per-forming internal cross-validation where the training set is partitioned into two sets of instances, one is used for SVM training, and the other for learning the parameters  X  and  X . The error arising from generalization is thus accounted for: the two parameters capture the uncertainty associated with generalization to unseen data.
 There are two parameters to such a calibration procedure. The first parameter is the data hiding protocol used for separating training from test data. For example, for 10-fold cross-calibration, 90% of the data is used for training and 10% remains hidden for calibration. The more data we hide, the more conservative are our predictions. The second parameter is the number of replications. A single cross-calibration depends on a particular shuffling of instances. To remove this dependence, the cross-calibration procedure should be replicated as many times as it is practical.
With the logit link the end result can be represented as If we apply logistic regression to the problem of associating the hyperplane distance with the label, we find such values of  X  and  X  that maximize the thus defined conditional log-likelihood of y given x in the above model across the N training instances [ x ( j ) ,y ( j ) ]:
The calibrated response function on the log odds ratio scale is d ( x )= X + X   X  ( x ). We can now map these symbols  X  ,  X  , w and b so that they will correspond to the notation of a (linearized) generalized additive model (1) based on the intercept  X  0 and the effect vector  X  .Theintercept  X  0 marks both the outcome probability of 0 . 5 and the log odds ratio of 0 . 0, so it can be seen as probabilistically calibrated bias b .The k -th coordinate [  X  ] k of the effect vector corresponds to the probabilistically calibrated coordinate of the normal [ w ] k . The mapping is as follows: The linear effect function for the set of attributes S i is simply f odds ratio scale, and they can thus be directly presented in a nomogram. It is important to distinguish the weight vector w , the hyperplane normal w = w / w ,theGLMeffect vector  X   X  , and the Lagrange multipliers  X  : all are different.
In this section, we examine the performance of support vector machines in comparison to other methods that can be visualized with the nomograms. To address this, we compare the nomogram-based probability estimations with those ob-tained from SVM with RBF kernel (Did we lose anything assuming the decomposability into effect functions?) and two popular methods for probabilistic classification, namely logistic regression and the na  X   X ve Bayesian classifier (What is the overall performance in class probability prediction?). Nomograms may be used to study the differences between various modelling methods from the family of generalized Figure 5: A general scheme of a cross-calibration procedure, based on N folds, R replications, the re-sponse learning algorithm L , the calibration learning algorithm C , and the training data T .

R X  X  X { Calibration training set. } for all r :1  X  r  X  R do { for each replication } end for (  X  : x  X  R )  X  L ( T ) { Hyperplane distance. } (  X  P ( y =1 | x ): x  X  [0 , 1])  X  C ( R , X  ) { Calibrated prob. additive models. We present a nomogram-based compari-son of SVM and the na  X   X ve Bayesian classifier model.
As for earlier nomograms, all experiments were performed within the Orange toolkit [4]. We employed LIBSVM [3] with default settings for training the SVM classifiers, and iteratively re-weighted least squares fitting [18] of the logistic regression model, as implemented in the Orange extensions package [12]. We experimented on 16 well-known UCI [10] data sets with a binary outcome. For data sets with more than 1000 examples ( X  X ushroom X  and  X  X pam base X ) we have selected a stratified random subset of 1000 examples which were used throughout the experiments.

We evaluated each method on three criteria: classifica-tion accuracy, outcome probability estimation (as measured by Brier score, the mean square error of predicted class probabilities given the true class probabilities for each in-stance [2]), and the area under the receiver operating charac-teristic. Table 1 compares the na  X   X ve Bayesian classifier (NB), logistic regression (LR), support vector machines with RBF kernels (SVM), and support vector machines with a linear kernel (dot and dot X ) on each of these three criteria. The first six data sets (the upper part of the table) include no contin-uous attributes. Elsewhere, the continuous attributes were discretized for NB and dot X  into 10 intervals with approxi-mately equal number of examples for each discretized value, as to provide the capacity for handling nonlinear effects. In computation of the Brier score, the predicted probabilities were calibrated for all methods, except for logistic regression (which is considered not to require calibration). Note that Brier score measures the loss, so lower values are better than higher.

The observed methods perform similarly, with some ex-ceptions. For instance, linear SVM performs poorly on  X  X ono-sphere X  unless the attributes are discretized. This indicates non-linear attribute effects in this data set, and we illustrate an example of them in Fig. 6. The SVM using the RBF ker-nel captures this nonlinearity better than any method based on discretization. An unexpectedly good performer is the na  X   X ve Bayesian classifier, which achieved good probability estimation results and reasonable ranking results. data sets.

Since our paper shows how to visualize SVM with linear kernels, it is of interest how much performance needs to be given up by not using the more powerful RBF kernels. As expected, SVM with RBF kernels generally performs best of all methods. Nonetheless, the difference between SVM with RBF and dot kernels is only a few percent (except in the already mentioned  X  X onosphere X ). We expected that the discretization would alleviate the linear restrictions of the model, but experimental results (dot vs dot X ) do not con-firm that. Still, dot X  provided a considerable improvement in the  X  X onosphere X  and  X  X iver/BUPA X  data sets. This indi-cates that the non-linearities appear only in certain data sets. We need to apply the more sophisticated models and visualizations only if the non-linearity is justified through a higher classification performance.
Judging from the experimental comparison of SVM to other machine learning techniques, SVM sometimes achieves worse results on Brier score while having comparable classifi-cation accuracy at the same time.  X  X huttle X  and  X  X itanic X  are examples of such data sets. The reason for the problem can be easily explained with a nomogram. We will compare the na  X   X ve Bayesian classifier (NB) and SVM to predict the prob-ability for passenger X  X  survival of the HMS Titanic disaster. The NB nomogram [19] in Fig. 7 (the data set was obtained at http://hesweb1.med.virginia.edu/biostat/s/data/ ), includes three attributes: the passenger status (first, second, and third class, or a crew member), the age (adult or child), and the sex of the passenger.

For NB, the attribute with the biggest potential influence on the probability of survival is gender of the passenger: be-ing female increases the chances of survival most (log odds of 1.7), while being male decreases the odds (log odds of about  X  0 . 6). Of the three attributes, the age is apparently the least influential, although children had a higher probability of survival. Most lucky were the passengers of the first class for which  X  considering the status only  X  the probability of survival was much higher than the prior. Comparing this nomogram to the SVM nomogram in Fig. 7 of  X  X itanic X , we observed a very interesting difference between them. SVM, Figure 6: The nonlinearities in the  X  X onosphere X  data set. Two features were used for each attribute, rep-resenting an RBF basis. as it is known, aims to optimize the classification accuracy and considering this it induced a model that predicts sur-vival of a passenger by considering only the sex attribute. Both methods, NB and SVM, consider this attribute as very important, but unlike NB, SVM disposes of age and status as completely irrelevant attributes. Using only the sex at-tribute, SVM achieves comparable classification accuracy, but the fidelity of the outcome probability estimates are slightly worse, as measured by Brier score.
The role of the nomogram is to visualize the probabilis-tic predictions of a support vector machine without losing any information. The effect function is a full representation of the contribution of an individual attribute towards the probability of the outcome. The visualization is not a par-tial approximation to the SVM model: instead it captures the SVM model completely and exactly. Clearly, our intu-itive conception of attribute  X  X mportance X  might not match that of the effect function. There are other ways of inter-preting attribute importance that do not correspond to ef-fect functions, perhaps the most popular of which is mutual information.

There are some pitfalls to interpreting the importance of attributes derived from the nomogram-based visualization of a support vector machine. We distinguish two distinct situations:
We have shown that support vector machines can be effec-tively visualized even in attribute spaces with many dimen-sions, using nomograms. Namely, individual attributes are stacked vertically in a nomogram, packing multiple dimen-sions into a single one. We have described the methodology for converting a support vector machine into the form of a generalized additive model. Furthermore, we have extended the form of a nomogram with two-dimensional graph repre-sentations of a nonlinear and non-monotonic effect function, as we have seen in Sect. 2.2. In addition to nonlinear univari-ate effects, we also show how interactions between attributes can be modelled and visualized.

We did not discuss the problem of determining what de-composable kernel to use in detail. There are three ways of addressing this. First, interaction analysis [14] is a heuris-tic that can aid the construction of kernels that capture the interactions. Secondly, we can see it as an issue of model se-lection. Finally, it is possible to express a preference for sparse and smooth kernels as a part of the optimization problem, combining the quest for decomposability and the actual learning [5, 20].

With the example of Sect. 3.2, we pointed out that nomo-grams may be the right tool for experimental comparison of different models and modelling techniques, as it allows to easily spot the similarities and differences in the structure of the model. Furthermore, we can use nomograms to outline possible weaknesses of models, such as those of linear mod-els by comparing them to the models obtained on discretized data.

KDD practitioners are often concerned with data sets that contain hundreds or thousands of attributes. Nomograms have no inherent problems with such situations: the dimen-sionality of the visualization depends on the structure of interactions, not on the number of attributes. To simplify the interpretation, the attributes should be arranged by im-portance, and the more important attributes would be ex-amined first. Nomograms provide a measure of importance that is based on the length of the effect line: it indicates the range of the effects provided by the attribute. Although this measure should be weighted by the frequency of individual attribute values, it it nonetheless intuitive and useful. An interesting question is also the stability of the model. The effect of a particular attribute can be thought as an uncertain quantity. To present the uncertainty, we can em-ploy the notion of an error bar or a confidence interval. We obtain the error bars by training a separate SVM model for each bootstrap resample of the original data. Each separate model results in an effect function, and for each value of the attribute, we can obtain the lower and upper bound of the effect across the resamples. This yields the effect error bar.
Finally, all that was said about classification applies also to regression. The only difference is that the range of the dependent variable replaces the log-odds, and that the cali-bration is not required. The authors are grateful to J. Brank for helpful advice. This work was supported by a grant from the Slovene Min-istry of Education, Science and Sports and the IST Pro-gramme of the European Community under SEKT Seman-tically Enabled Knowledge Technologies (IST-1-506826-IP) and PASCAL Network of Excellence (IST-2002-506778). This publication only reflects the authors X  views. [1] Y. Altun, A. Smola, and T. Hofmann. Exponential [2] G. W. Brier. Verification of forecasts expressed in [3] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [4] J. Dem X  sar and B. Zupan. Orange: From experimental [5] S. R. Gunn and J. S. Kandola. Structural modelling [6] T. L. Hankins. Blood, dirt, and nomograms: A [7] F. E. Harrell. Regression modeling strategies: with [8] D. Harrison and D. L. Rubinfeld. Hedonic prices and [9] T. Hastie and R. Tibshirani. Generalized Additive [10] S. Hettich and S. D. Bay. The UCI KDD archive. [11] D. W. Hosmer and S. Lemeshow. Applied Logistic [12] A. Jakulin. Extensions to the Orange data mining [13] A. Jakulin and I. Bratko. Analyzing attribute [14] A. Jakulin and I. Bratko. Testing the significance of [15] M. W. Kattan, J. A. Eastham, A. M. Stapleton, T. M. [16] J. Lafferty, X. Zhu, and Y. Liu. Kernel conditional [17] J. Lubsen, J. Pool, and E. van der Does. A practical [18] A. J. Miller. Algorithm AS 274: Least squares [19] M. Mo X  zina, J. Dem X  sar, M. W. Kattan, and B. Zupan. [20] K. Pelckmans, J.A.K. Suykens, and B. De Moor. [21] J. C. Platt. Probabilistic outputs for support vector [22] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels . [23] V. N. Vapnik. The Nature of Statistical Learning
