 Robert J. Durrant r.j.durrant@cs.bham.ac.uk Ata Kab  X an a.kaban@cs.bham.ac.uk Random projection is fast becoming a workhorse in high dimensional learning (e.g. Boyali &amp; Kavakli , 2012 ; Fard et al. , 2012 ; Mahoney , 2011 ; Maillard &amp; Munos , 2012 ; Paul et al. , 2012 ; Pillai et al. , 2011 ). However, except in a few specific settings, little is known about its effect on the generalization performance of a classifier.
 Previous work quantifying the generalization error of a linear classifier trained on randomly projected data has, to the best of our knowledge, only considered specific families of classifiers and each approach previously employed has also assumed constraints of some form on the data. The earliest work is in a seminal paper by Arriaga &amp; Vempala ( 1999 ), where the effect of randomly projecting well-separated data on the performance of the Perceptron is quantified. However the bounds in Arriaga &amp; Vempala ( 1999 ) make use of high-probability geometry preservation guarantees via the Johnson-Lindenstrauss lemma (JLL) and therefore, contrary to expectation and ex-perience, they become looser as the sample complexity increases. More recently, Calderbank et al. ( 2009 ) gave guarantees for SVM working with randomly projected sparse data using ideas from the field of compressed sensing (CS) -these however become looser as the number of non-zero features in the sparse representation of the data increases. Generative clas-sifiers are considered in Davenport et al. ( 2010 ) where Neyman-Pearson detector was analyzed assuming spherical Gaussian classes, while Durrant &amp; Kab  X an ( 2010 ); Durrant &amp; Kab  X an ( 2011 ) considered Fisher X  X  Linear Discriminant, assuming general sub-Gaussian classes. These bounds tighten with the sample com-plexity, but the assumptions on the class-conditional distributions may not hold in practice.
 Along very different lines, Garg et al. ( 2002 ) use ran-dom projections to estimate the generalization error of a classifier learnt in the original data space, i.e. learn-ing is not in the randomly projected domain, but ran-dom projections are used instead as a tool for deriving their generalization bounds. They have the nice idea, which we will also use, of quantifying the effect of ran-dom projection by how it changes class labels of pro-jected points w.r.t. to the data space classifier. Their approach yields a data-dependent term that captures the margin distribution, and allows the use of existing VC-dimension bounds in the low dimensional space. However, although their result improves on previous margin bounds, it is still generally trivial (the proba-bility of misclassification obtained is greater than 1). This is mainly because their estimate of how likely a class label is to be  X  X lipped X  with respect to its label in the data space is extremely loose; in fact its con-tribution to the generalization error bound is typically greater than 1 and it never attains its true value. Here we turn around the approach in Garg et al. ( 2002 ) in order to derive bounds for the generalization er-ror of generic linear classifiers learnt by empirical risk minimization (ERM) from randomly-projected data. Moreover, instead of using bounds on the label-flipping probability (i.e. the margin distribution) as obtained in Garg et al. ( 2002 ) or Garg &amp; Roth ( 2003 ), we de-rive the exact form of this quantity. Finally, we show that one can sometimes improve on their use of Markov inequality by Chernoff-bounding the dependent sum, and gain some additional improvement 1 . As a conse-quence we obtain non-trivial bounds on the generaliza-tion error of the randomly-projected classifier, which we note can also be extended to improve the results in Garg et al. ( 2002 ) in a straightforward way. 2.1. The Classification Problem We consider a 2-class classification problem where we observe N examples of labelled training data T N = known data distribution D over R d  X { 0 , 1 } . For a given class of functions H , our goal is to learn from T N the classification function  X  h  X  X  with the low-est possible generalization error in terms of some loss function L . That is, find  X  h such that L (  X  h ( x q ) , y arg min query point with unknown label y q .
 Here we use the (0 , 1)-loss L (0 , 1) : { 0 , 1 } X { 0 , 1 } X  { 0 , 1 } which is the measure of performance of interest in classification, defined by: Working with the original data, the learned classifier  X  h is a vector in R d which, without loss of generality, we take to pass through the origin. For an unlabelled query point x q the label returned by  X  h is then: where 1 { X } is the indicator function which returns 1 if its argument is true and 0 otherwise. Since we are only interested in the sign of the dot product above, we may clearly assume without loss of generality that in the data space all data lie on the unit sphere S d  X  1  X  R d and that k  X  h k = 1, where k X k denotes the Euclidean norm. Now consider the case when d is very large and, for practical reasons, we would like to work with a lower dimensional representation of the data. There are many methods for carrying out such dimensionality reduction (see e.g. Fodor , 2002 , for a survey) but here we focus on random projection which is a recent and very promising data-independent approach. Ran-domly projecting the data consists of simply left mul-tiplying the data with a random matrix R  X  X  k  X  d , k  X  d , where R has entries r ij drawn i.i.d from a zero-mean subgaussian distribution. Again many ma-trices fit this bill  X  examples can be found in Achlioptas ( 2003 ); Dasgupta &amp; Gupta ( 2002 ); Ailon &amp; Chazelle ( 2006 ) and Matou X sek ( 2008 )  X  but for concreteness and analytical tractability we will focus here on matrices R where the entries r ij i.i.d  X  X  (0 ,  X  2 ). We are interested in quantifying the effect on the gen-eralization error of randomly projecting the training set to a k -dimensional subspace, k  X  d , and learn-ing the classifier there instead of in the original data space. In this setting, the training set now con-sists of instances of randomly-projected data T N R = tor in R k (possibly not through the origin -translation does not affect our proof technique) which we will de-note by  X  h R . The label returned by  X  h R is therefore: where b  X  R . Denoting by  X  h R ( Rx q ) the label returned by this classifier, we want to estimate: E where ( x q , y q )  X  X  is a query point with unknown label y q . To keep our results general we only assume that the data points are drawn i.i.d from D , but we make no particular assumptions on the data distri-bution D , in particular we make no assumption of a sparse data structure, nor do we assume that the classes are linearly separable. Our main result is the following bound on the gener-alization error of a classifier trained by ERM on the randomly projected data set: Theorem 3.1 (Generalization Error) . Let T N = dimensional labelled training examples of size N , and let  X  h be the linear ERM classifier estimated from T N . Let R  X  X  k  X  d , k &lt; d be a random projection ma-trix with entries r ij i.i.d  X  X  (0 ,  X  2 ) . Denote by T N { ( Rx i , y i ) } N i =1 the random projection of the training data T N , and let  X  h R be the linear classifier estimated from T N R . Then for all  X   X  (0 , 1] , with probability at least 1  X  2  X  w.r.t. the random choice of T N and R , the generalization error of  X  h R w.r.t the (0,1)-loss is bounded above by: + where f k (  X  i ) := Pr R { sign (  X  hR T Rx i ) 6 = sign ( is the flipping probability for the i -th training exam-ple with  X  i the principal angle between  X  h and x i , and  X  E ( T N ,  X  h ) = 1 N P ( x pirical risk of the data space classifier.
 This theorem says that with high probability, the generalization error of any linear classifier trained on randomly projected data is upper bounded by the training error of the data space classifier plus the average flipping probabilities of the train-ing points plus a  X  X rojection-penalty X  term, either the VC-complexity in the projection space. Notice that the terms involving flipping probabilities vanish when no flipping occurs and in particular, as k  X  d , our bound recovers exactly the classical VC-bound for linear classifiers in R d . On the other hand, when k &lt; d , these terms represent the bias of the classi-fier in the randomly projected domain and quantify the price paid for working there instead of in the data space.
 Notice also that the average flipping probability term depends on the angles between the training points and the classifier; we therefore see from the geometry that when there is a large margin separating the classes this term will generally be small, and our bound cap-tures well the effects of separated classes. On the other hand, a small average flipping probability is still pos-sible even when the margin is small  X  for example provided that not too many points are close to the decision hyperplane (in other words, if the data are soft-separable with a large (soft) margin).
 Finally we note that our theorem implies that we can get close to the best linear classifier in R d , but working in R k and even with a relatively small sample complex-ity, provided that the data have some special struc-ture which keeps this average flipping probability small (and we have already identified two such special struc-tures). A key tool in obtaining theorem 3.1 , which may also be of independent interest, is the following theorem 3.2 : Theorem 3.2 (Flipping Probability) . Let h, x  X  R d and let the angle between them be  X   X  [0 ,  X / 2] . Without loss of generality take k h k = k x k = 1 .
 Let R  X  X  k  X  d , k &lt; d , be a random projection matrix with entries r ij i.i.d  X  X  (0 ,  X  2 ) and let Rh, Rx  X  R k be the images of h, x under R with angular separation  X  R . 1. Denote by f k (  X  ) the  X  X lipping probability X  f k (  X  ) := 2. The expression above can be rewritten as the quo-3. The flipping probability is monotonic decreasing 4.1. Proof of Flipping Probability -Theorem Let h, x  X  R d be two unit vectors 2 with the an-gle between them  X   X  [0 ,  X / 2] which we randomly project by premultiplying them with a random matrix R  X  X  k  X  d with entries drawn i.i.d from the Gaus-sian N (0 ,  X  2 ) to obtain Rh, Rx  X  R k with the angle between them  X  R . As a consequence of the Johnson-Lindenstrauss lemma, the angle between the projected vectors Rh, Rx is approximately  X  with high proba-bility (see e.g. Arriaga &amp; Vempala ( 1999 )) and the images of the vectors h, x under the same random pro-jection are not independent.
 We want to find the probability that following random projection the angle between these vectors becomes  X 
R &gt;  X / 2, i.e. switches from being acute to being ob-tuse. We call this probability the  X  X lipping probability X  because its effect is to  X  X lip X  the predicted class label in the projected space w.r.t the data space from the 1 class to the 0 class. It is easy to see that this probabil-ity is symmetric in the class labels, e.g. by considering the angle of x with  X  h , and so mutatis mutandis the probability of flipping from the 0 class to the 1 class has the same form.
 We will prove parts 1 &amp; 2 of theorem 3.2 here. Part 3 of our theorem is easy to believe using part 2 and the fact that the proportion of the surface of the k -dimensional unit sphere covered by a spherical cap with angle of Lemma 2.2, Pg 11); to save space we omit a rigorous proof of part 3  X  this can be found in Durrant ( 2013 ). Before proving theorem 3.2 we make some preliminary observations. First note, from the definition of the dot product, for  X   X  [0 ,  X / 2] in the original d -dimensional space and  X  R in the k -dimensional randomly-projected and this is the probability of our interest. In fact the arguments for the proof of parts 1 &amp; 2 of our theorem will not rely on the condition  X   X  [0 ,  X / 2] -this is only needed for part 3. Regarding random Gaussian matrices we note that, for any non-zero vector x  X  R d , the event Rx = 0 has probability zero with respect to the random choices of R . This is because the null space of R , ker( R ) = R ( R d )  X  , is a linear subspace of R d with dimension d  X  k &lt; d , and therefore ker( R ) has zero Gaussian measure in R d . Hence Pr R { x  X  ker( R ) } = Pr R { Rx = 0 } = 0. Likewise, R almost surely has rank k . In this setting we may therefore safely assume that h, x /  X  ker( R ) and that R has rank k . With these details out of the way, we begin: 4.1.1. Proof of part 1.
 First we expand out the terms of ( Rh ) T Rx to obtain Pr
R { ( Rh ) T Rx &lt; 0 } :
Recall that the entries of R are independent and iden-tically distributed with r ij i.i.d  X  X  (0 ,  X  2 ) and make the change of variables u i = P d j =1 r ij h j and v i = P j =1 r ij x j . A linear combination of Gaussian vari-ables is again Gaussian, however u i and v i are now no longer independent since they both depend on the same row of R . On the other hand, for i 6 = j the vec-since the i -th row of R is independent of its j -th row. Moreover ( u i , v i )  X  ( u j , v j ),  X  i, j so it is enough to consider a single term of the outer sum in ( 4.1 ). We have:
Since u i and v i are zero mean, the expectation of this distribution is just (0 , 0) T , and its covariance is: Then: Now, when j 6 = j  X  , r ij and r ij  X  are independent, and so since r ij  X  X  (0 ,  X  2 ). Hence: since k h k = 1. Likewise Var( v i ) =  X  2 .
 Next the covariance Cov( u i , v i ) is:
Now, when j 6 = j  X  the expectation is zero, as before, and when j = j  X  we have for ( 4.4 ):
Hence for each i  X  X  1 , . . . , k } the covariance matrix is: since k h k = k x k = 1, and we have ( u i , v i ) T i.i.d (0 ,  X  u,v ). Now the probability in ( 4.1 ) can be writ-ten as: which it will be helpful to further rewrite as: where the probability is now over the distribution of ( u i , v i ) T . Making the final change of variables: where the new variables y i , z i are independent unit variance spherical Gaussian variables, ( y i , z i ) T iid N (0 , I ), we substitute into ( 4.6 ) to obtain the flip probability in the form: where the probability now is w.r.t the standard Gaus-sian distribution. Now diagonalizing the symmetric U
T U = I and  X  a diagonal matrix of its eigenvalues, we can rewrite ( 4.8 ) as:
The standard Gaussian distribution is invariant under orthogonal transformations, and so the form of U does not affect this probability. We can therefore take U = I without loss of generality and rewrite ( 4.9 ) as:
Now we need the entries of  X , which are the eigenval-ues of:
Using the fact that the eigenvalues of AB are the same as the eigenvalues of BA these are the eigenvalues of which are  X  =  X  2 (cos(  X  )  X  1). Substituting into the inequality ( 4.9 ) and dropping the positive scaling con-stant 1 2  X  2 since it does not affect the sign of the left hand side, the probability we are after is:
Now, y i and z i are standard univariate Gaussian vari-( 4.10 ) is F -distributed ( Mardia et al. , 1979 , Appendix B.4, pg 487). Therefore: where  X  = (1  X  cos(  X  )) / (1 + cos(  X  )) and  X (  X  ) is the gamma function. This proves the first part of Theorem 3.2 .  X  4.1.2. Proof of part 2.
 Note that  X  = tan 2 (  X / 2) and make the substitution w = tan 2 (  X / 2). Then, from the trigonometric identity tan 2 (  X / 2)), we obtain:
To put the expression ( 4.11 ) in the form of the sec-ond part of the theorem, we need to show that the gamma term outside the integral is the reciprocal of R 0 sin ward way using the beta function. Recall that the beta function is defined by (e.g. Abramowitz &amp; Stegun , 1972 , 6.2.2, pg 258): and therefore from equation ( 4.12 ) we have:
Next, from the symmetry of the sine function about  X / 2, equation ( 4.12 ), and using  X (1 / 2) =
Now we just need to show that the leftmost factor on the right hand side of ( 4.11 ):
To do this we use the duplication formula (( Abramowitz &amp; Stegun , 1972 ), 6.1.18, pg 256): with z = k/ 2. Then the left hand side of ( 4.13 ) is equal to: as required. Putting everything together, we arrive at the alternative form for ( 4.11 ) given in equation ( 3.3 ), namely:
This proves the second part of Theorem 3.2 .  X  4.1.3. Proof of part 3.
 For reasons of space we omit the proof that the flipping probability is monotonic decreasing in the projection dimension k -this can be found in Ch. 6 of Durrant ( 2013 ). Note that although the value of the expres-sions in ( 3.3 ) and ( 3.2 ) can be calculated exactly for any given k and  X  , e.g. using integration by parts, as k grows this becomes increasingly inconvenient. The final part of the theorem, bounding the flipping prob-ability in the ( k + 1)-dimensional case above by the flipping probability in the k -dimensional case, is there-fore useful in practice. 4.2. Proof of Generalization Error Bound -We begin by considering the case when R  X  X  k  X  d , k &lt; d , is a fixed instance of a Gaussian random pro-jection matrix. From classical VC theory (e.g. Vapnik , 1999 ; Herbrich , 2002 ) if  X  h R is the classifier with min-imal empirical risk in the randomly projected space then we have, for any fixed R and any  X   X  (0 , 1), with probability 1  X   X  over the random draws of the training set T N the following: where  X  E ( T N R ,  X  h R ) denotes the empirical risk
P N i =1 1 {  X  h R ( Rx i ) 6 = y i } . Further, since  X  linear classifier in k -dimensional space we also have V Cdim = k + 1 and we see immediately that random projection reduces the complexity term w.r.t the data space where V Cdim = d +1. However, unless the data have some special structure, the empirical risk in the projected space will typically be greater than in the data space so we would especially like to quantify the effect of random projection on this term. With this goal in mind we first bound the empirical risk further by: where  X  E ( T N R , R  X  h ) denotes the empirical error of a pro-jected d -dimensional classifier evaluated on the pro-jected training set, i.e. 1 N P N i =1 1 { ( R  X  h ) T Rx some  X  h  X  R d .
 The inequality ( 4.15 ) holds because  X  h R and R  X  h lie in the same k -dimensional subspace of R d , and  X  h R is the ERM classifier in that subspace. Now  X  h  X  R d is an arbitrary vector that we can choose to minimize this bound, but we will take it to be the ERM classifier in R d in order to keep the link between the randomly projected classifier  X  h R and its high-dimensional coun-terpart  X  h . Now observe that: and so, for any fixed R , w.p. 1  X   X  w.r.t. random draws of T N we have: Denote S := 1 N P N i =1 1 n sign (  X  h T x i ) 6 = sign ( in the above bound. This is an empirical estimate of the average flipping probability on this data from a single random projection. Our next step is to show that this estimate is not far from its expectation (with respect to random matrices R ), that is S is close to E R [ S ] = 1 N P probability of theorem 3.2 . The main technical issue is the dependency between the  X  h T R T Rx i due to the common random matrix instance R and hence we cannot obtain decay with N since the random variable of interest is the projection matrix R and it is independent of N . To make the best of the situation, we derive two large deviation bounds for S : The first is a straightforward application of Markov inequality to give w.p. at least 1  X   X  : where we recall that E R [ S ] = 1 N P N i =1 f k (  X  i ). Re-placing the empirical estimate of the average flipping probability in ( 4.16 ) by RHS of ( 4.17 ) yields one high probability upper bound on the generalization error. The upper bound on S given in ( 4.17 ) can be improved somewhat for small values of  X  by using the follow-ing lemma, which is Corollary 3 on page 24 of Siegel ( 1995 ).
 Lemma 4.1 (Chernoff bound for dependent vari-ables) . Let X = P N i =1 X i , where the X i may be depen-dent. Let Y = P N i =1 Y i where the Y i are independent and Y i  X  X i (i.e. Pr { Y i 6 a } = Pr { X i 6 a } ,  X  i ). Let B be a Chernoff bound on Pr { Y  X  E [ Y ] &gt;  X  } then: Now let R i , i  X  X  1 , 2 , . . . , N } be a collection of N i.i.d draws of random matrices with i.i.d zero-mean Gaussian entries and define S N := sum S N differs from S in that S has the same random matrix in each summand while S N has independent random matrices in each summand. However, for any i  X  X  1 , ..., N } , the i -th term of S has the same distri-bution as the i -th term of S N and a Chernoff bound for S N can therefore be used to bound the deviation of S from its expectation, via lemma 4.1 .
 Now, by construction S N is a sum of indepen-dent Bernoulli variables. Using a standard Cher-noff bound for sums of Bernoulli random variables (e.g. Anthony &amp; Bartlett , 1999 , Pg 360) we obtain  X   X   X  (0 , 1): and applying lemma 4.1 then yields: This bound is  X  X hernoff tight X , i.e. tight w.r.t the Chernoff bound ( 4.18 ), when no assumptions are made on the set of points T N and, in particular, it gives the appropriate Chernoff bound when all points of T N are identical.
 Now, specifying  X   X  (0 , 1), setting  X  to the LHS of eq. ( 4.19 ), and using the fact that E R [ S N ] = E R [ S ], we obtain  X  p E R [ S ] = p 3 log(1 / X  ). Rearranging we obtain, w.p. at least 1  X   X  :
Replacing the empirical estimate of the average flipping probability in ( 4.16 ) with RHS of ( 4.20 ) yields a further high probability upper bound on general-ization error. Taking the minimum over these two bounds, and finally applying union bound delivers the theorem.  X  It is easy to verify that ( 4.14 ) recovers the known result for k = 1, namely  X / X  , as given in Goemans &amp; Williamson ( 1995 , Lemma 3.2). Geomet-rically, when k = 1 the flipping probability is the quo-tient of the length of the arc with angle 2  X  by the circumference of the unit circle which is 2  X  . In the form of ( 4.14 ) our result gives a natural generalization of this result, as follows: Recall that the surface area of the unit hypersphere in R k +1 is given by ( Kendall , 2004 ): while the surface area of the hyperspherical cap with angle 2  X  is given by: Now taking the quotient of these two areas all but the last factors cancel and so we obtain our flipping prob-ability as given in ( 4.14 ). Therefore, the probability that the sign of a dot product flips from being posi-tive to being negative (equivalently the angle flips from acute to obtuse) after Gaussian random projection is given by the ratio of the surface area in R k +1 of a hy-perspherical cap with angle 2  X  to the surface area of the unit hypersphere.
 Note the rather useful fact that the flipping proba-bility depends only on the angular separation of the two vectors and on the projection dimensionality k : It is independent of the embedding dimensionality d which can therefore be arbitrarily large without af-fecting this quantity. Moreover this geometric inter-pretation shows that equation ( 4.14 ) decays exponen-tially with increasing k , since the proportion of the surface of the k -dimensional unit sphere covered by a spherical cap with angle of 2  X  is bounded above by exp  X   X  1 2 k cos 2 (  X  )  X  ( Ball , 1997 , Lemma 2.2, Pg 11). Therefore we see that the additional loss arising from random projection (that is, the cost of working with randomly-projected data rather than working with the original high-dimensional data) is both independent of the original data dimensionality and will decay ap-proximately exponentially as a function of k  X  approx-imately because there is some trade-off with the com-plexity term and the upper bound exp  X   X  1 2 k cos 2 (  X  )  X  is not tight. Our results therefore generalize the findings of Davenport et al. ( 2010 ); Durrant &amp; Kab  X an ( 2010 ) where the same exponential decay was observed, but for specific choices of classifier. It is perhaps also worth noting that this loose upper bound on the flipping probability already improves considerably on the es-timate of Garg et al. ( 2002 ), which is seen by substi-tuting cos(  X  ) for  X  in their error bounds. 6.1. Upper Bound on Generalization Error for In proving theorem 3.1 we made no assumption that the data classes were linearly separable. However, if the classes are separable with a margin, m , in the data space ( Cristianini &amp; Shawe-Taylor , 2000 ) then straightforward geometry combined with Ball ( 1997 , Lemma 2.2, Pg 11) yields an upper bound on our flipping probability of exp  X   X  1 2 km 2  X  . This bound holds deterministically, and so we then have the fol-lowing high probability guarantee for separable data: Corollary 6.1 (Generalization Error -Separable Classes) . If the conditions of Theorem 3.1 hold and the data classes are also separable with a margin, m , in the data space then for all  X   X  (0 , 1) with probability at least 1  X  2  X  we have: + min + 2 Here we see that the bias introduced to the classi-fier by random projection decays exponentially with the square of the margin. Note that if we con-sider the margin at each training point individu-ally then we have a setting analogous to the mar-gin distribution considered in Shawe-Taylor ( 1998 ); Shawe-Taylor &amp; Cristianini ( 1999 ). Using the mar-gin distribution a tighter upper bound on the flipping probability is straightforward to derive -for reasons of space we do not do so here. 6.2. Upper Bound on Generalization Error in An upper bound on the average label flipping proba-bility whose exact form we derived here is key in the bounds of Garg et al. ( 2002 ), where it serves as a data-dependent complexity measure (termed the  X  X rojection profile X ) to characterize the generalization error of data space linear classifiers. It is now straightforward to use our exact form in place of their projection profile term to give the following bound on the generalization er-ror of data space classifiers as a corollary, which is an improvement on the main result in ( Garg et al. , 2002 ): Corollary 6.2 (Data Space Generalization Error) . belled training examples drawn i.i.d. from some data distribution D , and let  X  h be a linear classifier estimated from T 2 N by ERM. Let k  X  X  1 , 2 , . . . , d } be an inte-ger and let R  X  X  k  X  d be a random projection matrix, with probability at least 1  X  4  X  w.r.t. the random draws of T 2 N and R the generalization error of  X  h w.r.t the (0,1)-loss is bounded above by: Proof Sketch : Follow the two-part proof in Garg et al. ( 2002 ): One part bounds the generalization error us-ing classical tools of the double sample trick and Sauer lemma after making a move into the random projec-tion space; while the other is an estimate of our flipping probability obtained using the JLL. To obtain the re-sult in ( 6.1 ) plug in our exact form for the flipping probability for their estimate and use lemma 4.1 as well as Markov inequality in their lemma 3.4. We derived the exact probability of  X  X abel flipping X  as a result of Gaussian random projection, and used it to derive sharp upper bounds on the generaliza-tion error of a randomly-projected classifier. Un-like earlier results of Arriaga &amp; Vempala ( 1999 ) and Calderbank et al. ( 2009 ), we require neither a large margin nor data sparsity for our bounds to hold, while unlike Davenport et al. ( 2010 ) and Durrant &amp; Kab  X an ( 2010 ); Durrant &amp; Kab  X an ( 2011 ) our guarantees hold for an arbitrary data distribution.
 Our proof makes use of the orthogonal invariance of the standard Gaussian distribution, which cannot be applied for other random matrices with entries whose distribution is not orthogonally invariant: It would be interesting to extend these results to more general ran-dom projection matrices, and we are working on ways to do this. Furthermore we note that the form of VC complexity term in our bounds is not optimal, for ex-ample better guarantees (albeit without explicit con-stants) are given in Bartlett &amp; Mendelson ( 2002 ) and these could be used in place of the bounds we adopted to sharpen our results further.
 Our findings show that good generalization perfor-mance can be obtained from a classifier trained on randomly projected data, provided that the data have some structure which keeps the probability of label flipping low  X  we saw that two such structures are when data classes are separable or soft-separable with a margin. Identifying other structural properties of data which also imply a low flipping probability re-mains for future work.

