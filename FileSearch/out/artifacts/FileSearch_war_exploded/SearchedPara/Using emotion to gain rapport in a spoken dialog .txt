 As information sources become richer and technol-ogy advances, the use of computers to deliver in-formation is increasing. In particular, interactive voice technology for information delivery is becom-ing more common due to improvements in tech-nologies such as automatic speech recognition, and speech synthesis.

Several problems exist in these voice technologies including speech recognition accuracy and lack of common sense and basic knowledge. Among these problems is the inability to achieve rapport.
Gratch et al. (2007) defines rapport as a feel-ing of connectedness that seems to arise from rapid and contingent positive feedback between partners and is often associated with socio-emotional pro-cesses . In the field of neuro-linguistics, O X  X onnel Communication Accommodation Theory states that people use nonverbal feedback to establish social distance during conversation. In order to gain rap-port, people would most likely want to decrease social distance in order to achieve the connected-ness and smoothness in conversation that is seen in human social interaction. Research in human-computer interaction has pursued these nonverbal behaviors through appropriate backchanneling, head nods, and gaze techniques, but still missing is atten-tion to user emotional state, which can be detected through some of these nonverbal behaviors in voice.
Two methods for describing emotions are discrete and dimensional. Discrete emotions include anger, disgust, fear, joy, sadness, and surprise. Dimen-sional emotions use two or more components to de-scribe affective state. More commonly used dimen-sions are Osgood et al.  X  X  (1957) evaluation (a.k.a. valence), activity, and potency (a.k.a. power). Emo-tion research has had limited success at detecting discrete emotions, e.g. (D X  X ello et al., 2008). In choose topics and content depending on user emo-tional state. The resulting system will advance the state of the art in three technologies: recognizing appropriate emotion, planning accordingly, and syn-thesizing appropriate emotion. The system will also demonstrate how to integrate these components.
In addition to choosing the correct content based on user emotional state, this research will investi-gate the effect of adding emotion to voice for rap-port. The second hypothesis of the research is that expressing emotion in voice and choosing words, compared to expressing emotion only by choosing words, will be more effective for building rapport with users. This section outlines the steps that have been com-pleted and those that are still pending to accomplish the goals of the research. 4.1 Corpus Analysis and Baseline System This work is based on a persuasive dialog corpus consisting of audio recordings of 10 interactions av-eraging 16 minutes in length. The corpus consists of rougly 1000 turns between a graduate coordina-tor and individual students. The graduate coordina-tor was a personable female staff member who was hired by the University to raise the graduate student count. The students were enrolled in an introduc-tory Computer Science course and participated in the study as part of a research credit required for course completion. The students had little knowl-edge of the nature or value of graduate school and of the application process. Preliminary analysis of the corpus showed evidence of a graduate coordinator building rapport with students by using emotion.
A baseline system built using commercial state-of-the-art software was implemented based on the corpus (mainly the topics covered). Informal user comments about the baseline system helped deter-mine missing features for automated rapport build-ing technology. One salient feature that is missing is attention to emotion in voice. This confirmed the direction of this research.

This corpus was transcribed and annotated with dimensional emotions (activation, valence, and power) by two judges. Activation is defined as
In the first example, the coordinator noticably raises her pitch at the end of her utterance. This is probably so that she can sound polite or inter-ested. On line S2, the subject displays a falling pitch (which sounds negative) and the coordinator responds with a lower fundamental frequency and a slower speed. The subject sounds unsure by display-ing a rising pitch in his answer (S3). The coordinator mirrors his response (GC4) and finally both inter-locutors end with normal pitch and normal speed.
In the second example, the subject speaks faster than usual (S5). The coordinator compensates by adjusting her speed as well. From S6 through GC8, when the subject X  X  voice gets louder, the coordina-tor X  X  voice gets softer, almost as though she is back-ing off and letting the subject have some space. In GC9 the coordinator responds to the student X  X  posi-tive response (liking the class) and becomes imme-diately faster and louder.

A next step for the analysis is to determine the most expressive acoustic correlates for emotions. In-formal auditory comparisons show some possible correlations (see Table 1). These correlations seem promising because many correspond with previous work (Schroder, 2004).

The emotion annotations of the two judges show that strategies for adaptive emotion responses can be extracted from the corpus. Communication Ac-comodation Theory states that interlocutors mir-ror nonverbal behaviors during interaction when at-tempting to decrease social distance. The coordina-tor X  X  emotional responses were correlated with the student X  X  emotional utterances to determine if emo-tional mirroring (matching student emotion and co-ordinator response) was present in the persuasive di-alog corpus. This was the case in the valence dimen-sion, which showed a correlation coefficient of 0.34. cludes the integration of all components.

The following is a scenario that depicts how the full system will operate. 1. The system begins by saying  X  X ow are you do-2. The user says  X  X  X  X  doing good X  with a negative 3. The voice signal is then processed through the 4. This data is sent to the user modeling com-5. This user state update information is then will be a control configuration, perhaps one that will display a random emotion ( random ). A user study (hopefully within subjects) will be conducted that will ask users to interact with four versions of the system (baseline, voiced , not voiced , and random ). A post-test questionnaire consisting of Likert scales will ask users how much rapport they felt with each version of the system. In addition, some objective metrics such as disfluency count and interaction time will be collected. This will help test the two hy-potheses of this research. First, it is expected that subjects will have more rapport with the not voiced configuration than with the baseline system. The second hypothesis will be verified by determining if subjects have more rapport with the voiced than with the not voiced system. The random configura-tion will be used to determine whether the system X  X  adaptive responses are better than random responses. This research addresses methods for gaining rap-port as an important dimension of successful human-computer interaction, and one likely to be useful even for business-like dialogs. For example, build-ing rapport with customers can decrease the number of disfluencies, which are currently a problem for speech recognizers. In addition, customer support systems will have the ability to tailor responses to decrease negative emotion.

Similarly, the learned rules for detecting emotion and responding appropriately could be used to train people how to more effectively gain rapport. Lastly, this work can supplement other rapport research that uses other forms of nonverbal behavior such as gaze and gestures seen especially in embodied conversa-tional agents. I would like to thank my advisor, Nigel Ward for his help. Also, I would like to thank Anais Rivera and Sue Walker for the collection of the persuasive dia-log corpus and Jun Zheng for his help in fine tuning the baseline system. This work is supported in part by NSF grant IIS-0415150.

