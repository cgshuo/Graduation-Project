 Keywords: Knowledge representation  X  Entity topics  X  Topic model  X  Knowl-edge graph completion Knowledge Graphs (KGs) aim at storing the facts of the real world with ( h,r,t ) triples, where h (or t ) denotes the head (or tail) entity and r represents the relation between entities. As crucial resources, large-scale KGs such as Freebase [1] have been widely used in NLP tasks, including Question Answering (QA) [9] and Document Classification [17]. However, with the increasing scales of triples, KGs often suffer from the data sparseness and incompleteness problems [16]. learns low-dimensional semantic embeddings of entities and relations has at-tracted extensive attention in recent years. Most of existing KRL methods, such as TransE [2], TransH [13], and TransR [8], learn the semantic representations based on triples and show good performance in knowledge graph completion and triple classification tasks. However, these methods are sensitive to the triple sparsity problem, since they rely on a large amount of triples for training. For example, more than 20% entities in FB15k [2] are associated with less than 20 triples, which leads to a poor entity representation and diminishes the KRL per-formance. Therefore, recent researches turn to utilize the text information to enhance the performance of KRL [16,14]. In particular, Xie et al. [16] proposed a DKRL model, which explored the continuous bag-of-words (CBOW) and con-volutional neural networks (CNN) for encoding entity descriptions. In [14], a TEKE model was proposed to learn the co-occurrence word networks for enti-ties based on the Wikipedia pages. However, these methods utilize the word-level information, and cannot well capture the latent topics of entities which are as-sumed to be useful to improve the KRL performance in this paper. For instance,  X  X gatha X  is a playwright as demonstrated in the triple and has the descriptions as shown in Fig.1. If a new entity  X  X oseph X  has a description, namely  X  X oseph was an American satirical novelist , short story writer and dramatist  X , it is reasonable to infer that  X  X oseph X  is also a playwright since he has the common topics like  X  X riter X  with  X  X gatha X  and  X  X ramatist X  with  X  X laywright X . topic based representation learning (ETRL) method in this paper. Specifically, we first use the entity descriptions from KGs for topic modeling, which shows strong semantic correlations with entities. The Nonnegative Matrix Factorization (NMF) model is applied to learn the topic representations of entities. Then, two variants of ETRL models are proposed. The basic model (ETRL(basic)) treats the obtained topic representations as constants, and integrates them with the triple based representations, and the advanced model (ETRL(adv)) jointly learn the topic and triple based representations simultaneously. Noting the two essentially different representations (topic based and triple based), we present projection matrices to map the two representations into a united space for ETRL. The contributions of our works are as follows: 1. We propose two variants of ETRL models, which integrates the topical infor-2. The projection matrices are utilized in ETRL, which can map different rep-3. The proposed models are evaluated on knowledge graph completion and related to our works. Section 3 reveals the details of our method. Section 4 evaluates our models on two tasks and analyzes the results. Section 5 concludes our works. In this section, we will introduce several works aiming at embedding the entities and relations into low-dimensional continuous vector space to address the short-ages of large-scale KGs mentioned above. Depending on utilizing the external textual information or not, we classify these methods into two categories. 2.1 Triple Based Representation Methods TransE [2], the most famous method in KRL, has shown its advantages among the methods proposed before. For each correct ( h,r,t ) triple, TransE regards that h + r  X  t which means that the entity h is translated to the entity t through the relation r . TransE defines the score function as shown in (1) and use SGD to optimize the loss function. Although TransE is efficient and performs well in modeling the triples, it has issue in modeling the 1-N, N-1 and N-N relations [14]. After TransE, there are various methods proposed to address this issue. TransH [13] projected the entities on relation hyperplane. TransR [8] learned the entity and relation embeddings into different semantic space and used relation-related matrices to map them. TranSparse [6] replaced projection matrices with adaptive sparse matrices, and the sparse degrees were computed by the number of entities. Additionally, other methods such as KB2E [5], TransG [15], and TransD [18] committed to this problem and achieved good performance of KRL. 2.2 Text Enhanced Represenation Methods Methods only based on triples are suffering from data sparsity problem. There are increasing methods fusing external text information and triple information to alleviate this problem and enrich the semantic information of KRL. The model proposed by Wang et al. [19] jointly learns the triple information and text infor-mation from entity descriptions in word-level which neglect the order of words. DKRL [16] learns text representations with deep learning method CNN. TEKE [14] memorizes the words co-occured in contexts of entities from wikipedia pages and then constructs co-occurrence networks to bridge the KG and text-corpus. However, most of these methods disregard the entity topics which contain strong semantic relevancy between entities. Noting that state-of-the-art method TEKE [14] has achieved some success in discovering entity topics for regarding co-occurrence words as topics of entities, but is limited by the performance of entity linking tools and the external text-corpus. In this section, we will present our ETRL method. The structure of our method is shown in Fig. 2. Firstly, we use entity descriptions in KGs as our text-corpus and learn entity topic representations from text information with NMF [7]. Then, we construct ETRL(basic) model to incorporate topic embeddings into triple embeddings. In addition, we build ETRL(adv) model to jointly learn two em-beddings simultaneously. Finally, we train our models with Adagrad method [3]. 3.1 Notations and Definitions For a given KG, We denote E as the set of entities and R as the set of relations. Meanwhile, E is the set of entity embeddings of E and R is relation embeddings of R . Then, we denote T as the set of ( h , r , t ) triples in KG where h , t  X  E and denote W as the set of words arising in D and denote v e as the topic embedding k is the embedding vector size. 3.2 Topic Representation of Entities There are a large number of texts such as Wikipedia pages and descriptions of entities in KGs. In this paper, we use entity descriptions as our text-corpus for the reasons listed as follows: 1. It is much easier for us to obtain entity descriptions from existing KGs rather 2. Most entities in KGs have their own descriptions which can be regarded as topic distribution of documents and discover the topic semantic information from documents. Considering about the performance, efficiency, and complexity of topic models [12], we adopt NMF [7] to learn topic representations of entities. follows: where M is the n  X  m word-frequency matrix of entities preprocessed from entity topic representation of word w , and m is the size of W . To factorize matrix M into entity and word topic embeddings, Euclidean distance and KullbackLeibler divergence are adopted by NMF [7]. In this paper, we select Euclidean distance defined in (3) as the convergence criterion of NMF.
 where M i,j is the frequency of word w j appearing in the description of entity e , v e Noting that in NMF, for all e i  X  E and w j  X  W we have v e We pre-train the NMF and obtain the entity topic representations in store for triple representation learning. 3.3 Joint Representation Learning of Triples and Topics The most important step is mapping both entity topic and triple representations into same semantic vector space. In other words, entity topic representations must be aligned to triple representations for training.
 simply integrates two representations in training while treats the topic represen-tations as constants. The latter jointly learns the representations from both topic and triple information inspired by DKRL [16].
 tioned in (1). We present projection matrices to the alignment of topic and triple representations. Besides, these matrices can help models learn useful topic information and discard useless automatically in training. of triple. Entity representations h  X  and t  X  are defined as follows: where v h (or v t ) is the entity topic embedding of head (or tail), h and t are the original entity representations in (1), and M e is the k  X  k projection matrix(we also call it alignment matrix).
 we union the topic embeddings both of heads and tails as the relation topic representations v r in (6) and new relation embeddings r  X  are defined in (7). where v r is the relation topic embedding and M r is the k  X  k projection ma-trix. It is worth noting that, for a certain relation (especially complex relations which have various head-tail pairs), different head-tail pairs may have different v r to affect the relation embeddings, thus improving the variousness of relation representations to against complex relation modeling problem.
 for both two models as follows: 3.4 Loss Optimization and Training ETRL(basic). To force the scores of correct triples as close as possible to zero and make the wrong X  X  as far as possible, we define the loss function in (9) which is similar to TransE [2].
 where max (  X  , 0) is hinge loss, T is train set including correct triples while T 0 is the set of negative samples denoted in (10).
 ( h , r , t ) /  X  T . We follow the  X  X ern X  method mentioned in TransH[13] to select the negative samples. ETRL(adv). Inspired by the works of DKRL [16], we propose an advanced model to jointly learn both the topic and triple representations. The loss function is defined in (11).
 where we optimize triple loss (9) and topic loss (3) simultaneously.
 Training. We adopt Adagrad [3] in the optimization. In order to accelerate the produced by TransE and initialize the projection matrices as identify matrices. In this section, we will evaluate our models on two tasks, including knowledge graph completion and triple classification. Then we analyze results comparing with other state-of-the-art methods. 4.1 Datasets and Experiment Settings Datasets. Freebase [1] is one of the most widely-used large-scale KGs which contains more than 1.9 billion triples up to now. In this paper, we adopt FB15k dataset extracted from Freebase as our experimental dataset. FB15k is first proposed by the authors of TransE [2] and is widely used in many KRL methods such as DKRL [16],and TEKE [14]. The statistics of FB15k are listed in Tb. 1. Text-Corpus. We extract the English entity descriptions from the latest Free-base Dump [4] for each entity in FB15k as our text-corpus. The average number of words in each entity description is 124. Then we preprocess the text-corpus with three steps: 1. We filter out the stop words in text-corpus. 2. We connect phrases which contain the entity names in text-corpus. 3. We simply remove the suffixes of the words in text-corpus with Porter stem-Experiment Settings. We train our two models and set the best hyperpa-rameters as the number of iterations iter = 1000, embeddings size and topic numbers k = 100, margin  X  = 2 and the regularizer of the learning rate scaling factor = 1  X  10  X  7 by experience. We select TransE and two state-of-the-art methods, TEKE and DKRL as our baseline. We also compare our models to other methods such as TransH and TransR. Because of the dataset is same, we directly compare the results reported in their works. 4.2 Knowledge Graph Completion The task of knowledge graph completion (also called link prediction) aims at predicting and completing the triples which have missing components. For ex-ample, given a triple ( h,r,t ) where head entity h is missing, the model firstly fill all entities e  X  E as candidate entities in the triple and then compute the score of triple ( e,r,t ) for each entity. Finally, it ranks the scores of candidate entities and completes the triple with the entity e which has the lowest score. hits@10 (hits@1 for missing relation). The former means the average rank of the correct one in the list of candidates while the latter means the rate of correct one ranked in the top 10. We also follow the two settings as  X  X aw X  and  X  X ilter X  mentioned in TransE [2].
 pletion is divided into two sub-tasks named as entity prediction and relation prediction. We evaluate our models in both the entity and relation prediction. Entity Prediction Results. In entity prediction task, we remove the head and tail entities respectively in triples from test set and compute MR and hits@10 results. The reuslts listed in Tb. 2 show that our models especially ETRL(adv) outperform all the baseline except the mean rank(filter). By comparing with other experimental results listed in Tb. 2 we find that: 1. All the evaluation metrics of our models better than baseline show that the 2. For the reason that TransR has more projection matrices [8], mean rank(filter) 3. The performance of ETRL(adv) is better than ETRL(basic). We can in-Relation Prediction Results. In relation prediction task, we remove the rela-tions in triples and then evaluate the methods as same as entity prediction task. It should be noted that we adopt hits@1 for relation prediction which different from entity prediction task. As shown in Tb. 3, we observe that ETRL(adv) outperforms other baselines. It is thought-provoking that our models perform worse in hits@1. Comparing with results of DKRL and TransE, we can infer that: 1. Although we use project matrices to dynamically learn from topic embed-2. However, our models outperform TransE which demonstrate the effectiveness Complex Relation Modeling Problem. Complex relation modeling prob-lem, which leads to low performance in modeling 1-N, N-1 and N-N relations, is one of the most important problems in KRL. According to the unbalance of the ratio of head to tail, researchers divide relations into 1-1, 1-N, N-1 and N-N relations where the latter three are called complex relations. We test the ability of modeling complex relation problem of our models. The results listed in Tb. 4 show our models outperform others in modeling 1-N, N-1 and N-N relations. Based on the results, we believe that with the addition of projection matrices both on entities and relations, our models can have advantages in handling the complex relation modeling problem.
 4.3 Triple Classification We evaluate our models in another classical task, triple classification [11], which is widely used in NLP tasks such as QA. In this task, for given triple ( h,r,t ), we judge whether this triple is true or not. Following the steps of NTN [11], we firstly build the set of negative triples by ourselves. Then, we learn the relation-specific threshold  X  with maximizing the accuracy on the valid dataset. We compute the score with (8) and judge if the triple is ture or not by the comparison between the score and  X  . Finally, because of the dataset are different from other methods, we just implement TransE and our models, and then report results in Tb. 5. From the results we find that our models are all outperform TransE. It proves that entity topic embeddings is propitious to learn the representations of entities and relations more exactly and elevate the accuracy of triple classification. In this paper, to make full use of entity topic information in entity descriptions, we propose ETRL model using NMF to learn entity topic embeddings and map-ping both topic and triple embeddings into same semantic space with projection matrices. Experimental results show that our models outperform most state-of-the-art methods which enhancing KRL with external text information. Our models can work well in knowledge graph completion and triple classification tasks and prove the effectiveness of entity topics.
 1. To lower the error brought by entity topic information, we will extend our 2. We will extend our models to other state-of-the-art triple based representa-
