
College of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China Key Laboratory of Medical Image Computing of Ministry of Education, Northeastern University, Shenyang, Liaoning, China Computing Science, University of Alberta, Edmonton, Alberta, Canada 1. Introduction Class imbalance has been recognized as a crucial problem in machine learning and data mining [7,17]. This problem occurs when the training data is not evenly distributed among classes; that is when some classes are significantly larger than others. This problem is growing in importance and has been identified as one of the 10 main challenges of Data Mining [39]. This problem is also especially critical in many real world applications, such as anomaly detection in credit card transaction, fraud detection, medical diagnosis etc. The imbalanced data issue occurs not only in stationary environments, but also in data overwhelmed by the majority class and ignore the minority class examples, since most classifiers assume an even distribution of examples among classes. Therefore, we need to improve traditional algorithms so as to handle imbalanced data.

Much work has been done in addressing the class imbalance problem. The proposed methods can be grouped in two categories: the data perspective and the algorithm perspective [6,37]. The methods with the data perspective re-balance the class distribution by re-sampling the data space either randomly or misclassification of samples of the minority class [12,14]. In addition, ensemble methods [8,9,20] are also a good solution for solving the class imbalance problem, since ensemble learning is incorporated with a re-sampling technique or a weighting strategy to acquire better classification performance and generalization capability.

The re-sampling technique is the most straightforward and effective method for dealing with imbal-ance, since it is not dependent on the classifier and is simple to implement. Weiss et al. observed that the naturally occurring distribution is not always optimal [36]. Therefore, one needs to modify the orig-inal data distribution. The existing re-sampling indeed improves the classification performance on the imbalanced data as a whole. Nevertheless, they target only the characteristic of the imbalanced distribu-tion between classes. Besides the between-class imbalance, the presence of inherent complex structures in the data distribution, such as within-class imbalance [22,23] and high-dimensionality [29], are other critical factors of decreasing classification performance. Within-class imbalance refers to the case where a class is formed of a number of sub-clusters with different sizes, concerns itself with the distribution of representative data for subconcepts within a class [22,23]. The existence of within-class imbalance is closely intertwined with the problem of small disjuncts, which has been shown to greatly decrease classification performance [24,32,35]. In addition, high-dimensionality poses further challenges when dealing with class-imbalanced prediction. The complex data distribution aggravates the imbalanced data complexity and the degree of imbalance.

Although traditional re-sampling methods across the entire data distribution can reduce the imbalance between two classes from a global perspective, they cannot solve the within-class imbalance issue as they cannot create appropriate instances without considering the spatial distribution from local distribu-tion, leading to decreased performance. Moreover, existing re-sampling techniques only manipulate the instance in the whole feature space; the irrelevant and redundant features in the high dimensional space may cause the synthetic instances to be inaccurate and biased. To counter the complications introduced above, the solution needs to follow three criteria. (1) The within-class imbalance may occur in the class distribution. The re-sampling procedure needs (2) Unlike the scheme duplicating the instances or interpolating new instances along the line between (3) The problem becomes even more severe when imbalanced data sets are involved with high di-
These criteria above suggest that when conducting re-sampling, two key issues need to be considered: where the new instances are generated, and how the new instances are produced. The criteria aim to generate more accurate samples which can obey the real sampling, in order to improve the recognition ability of the minority class without a big loss of the prediction ability on the majority class.
The contributions of this work can be listed as follows: (1) On the basis of the criteria introduced above, we propose a hybrid probabilistic re-sampling al-(2) To avoid the impact on the clustering of GMM and procedure of re-sampling due to some irrel-
This paper is organized as follows: After presenting a brief review of within-class imbalance as well as small disjuncts issues, and re-sampling algorithms under class imbalance in Section 2, we introduce in Section 3 our proposed method: HPS-DRS. Section 4 details the experimental results comparing our approach to other methods in the literature. 2. Related work 2.1. Within-class imbalanced and small disjuncts
Within-class imbalance refers to the case where a class is formed of a number of sub-clusters with different sizes, concerns itself with the distribution of representative data for subconcepts within a class [22,23]. This can cause problems as we shall discuss later. Real data is commonly distributed according to a mixture density whose components have relative densities that may vary greatly, since the data from the same class may not be homogeneous or arise from noisy misclassified instances. Hence the data with within-class imbalanced distribution occurs. When faced with such a situation the existing methods that address the class imbalance problem may be counterproductive. While they decrease the difference between the prior probabilities of the classes (the between-class imbalance), there is a chance they will increase the difference between the relative densities of the sub components within each class (the within-class imbalance).

Within-class imbalanced data distribution may yield small disjuncts, which is the essential challenge in the within-class imbalanced data issue. A phenomenon sometimes referred to as the  X  X roblem with small disjuncts X  and that these small disjuncts collectively contribute a significant portion of the total test errors [18]. Weiss suggests that there is a relation between the problem of small disjuncts and class imbalance, stating that one of the reasons why small disjuncts have a higher error rate than large disjuncts is due to between-class imbalance [35]. Japkowicz enhances this hypothesis stating that the problem of learning with class imbalance is increased when it yields small disjuncts [23].
 Holte et al. [18] evaluate several strategies for improving learning in the presence of small disjuncts. They show that the strategy of eliminating all small disjuncts is ineffective, because the emancipated examples are then even more likely to be misclassified. The common effective solutions to overcome the small disjuncts as well as within-class imbalance are to design better classifiers that address the problem with small disjuncts, and re-sampling approaches. Ting [33] designed a hybrid method using C4.5 to determine if an example is covered by a small or large disjunct. If it is covered by a large disjunct, then C4.5 is used to classify the example. However, if the example is covered by a small disjunct, then IB1, an instance-based learner, is used to classify the example. Japkowicz proposed a cluster-based oversampling algorithm (CBOS), which solves the between-imbalance and within-imbalance issues at the same time [23]. It makes use of K-means clustering to separate into multiple clusters the instances in each class, then a random oversampling method is used for targeting within-class imbalance in tandem with the between-class imbalance. However, some issues remain: (1) The number of clusters needs to be fixed by the user. It is difficult to determine this number, and (2) The clustering is done in the whole feature space. Many features are redundant and noisy, poten-(3) The instances chosen to be over-sampled are random, resulting in overfitting [6]. Moreover, there 2.2. Re-sampling methods for imbalanced data learning
Re-sampling methods are attractive under most imbalanced circumstances. This is because re-sampling adjusts only the original training data set instead of modifying the learning algorithm; therefore it provides a convenient and effective way to deal with imbalanced learning problems using standard classifiers. In this paper, we are only interested in sampling based approaches and hence we provide a brief overview of the methods proposed in this category. 2.2.1. Over-sampling techniques 2.2.2. Under-sampling techniques
Besides over-sampling methods, under-sampling is a popular method in dealing with class-imbalance problems. Under-sampling uses only a subset of the majority class based on the data characteristics of the majority class. Liu et al. [28] proposed two ensemble methods to strengthen the use of majority class examples, called EasyEnsemble and BalanceCascade. They construct an ensemble of ensembles with Bagging, where each individual classifier is also an ensemble of AdaBoost. Yen et al. [40] pro-posed a cluster-based under-sampling approach for selecting the representative data from the majority class, so as to improve the classification accuracy for the minority class. Zhang et al. [30] proposed an informed under-sampling using K-nearest neighbor (KNN) classifier to achieve under-sampling. Based on the characteristics of the given majority data distribution, four KNN under-sampling methods were sampling method based on the idea of ant colony optimization (ACO) to filter less informative majority samples and search the corresponding optimal training sample subset. 3. HRS-DRS method
In this section we begin by describing the hybrid probabilistic sampling. We then describe how to incorporate this method into the diverse random subspace ensemble to create HPS-DRS. 3.1. Hybrid probabilistic sampling
Gaussian Mixture Models (GMM) are generative probabilistic models of several Gaussian distribu-tions for density estimation in machine learning applications. A Gaussian mixture can be constructed to acceptably approximate any given density. Therefore, we assume the distribution of two classes follows the Gaussian mixture model with unknown parameters. The parametric probability density function of GMM is defined as a weighted sum of Gaussians. The finite Gaussian mixture model with k components may be written as: and Gaussian with specified mean and variance.

We need to estimate the parameters of GMM with the existing instances of both the classes. The standard method used to fit finite mixture models to observe data is the expectation-maximization (EM) algorithm, which converges to a maximum likelihood estimate of the mixture parameters. However, the drawbacks are that it is sensitive to initialization and it requires the number of components to be set by users. Since the FJ algorithm [15] tries to overcome the major weaknesses of the basic EM algorithm particularly vis- X -vis the initialization, and can automatically select the number of component, we use it here to estimate the parameters of GMM.

Each instance x i will then be assigned to the cluster k where it has the largest posterior probability the numeric attributes is obtained by a Gaussian density function, and for the nominal attributes, the probab ilities of occurre nce of each distinct value a re determined usin g Laplace estimates. At the same time, we obtain the parameters of each Gaussian component. For different clusters, the re-sampling rates are different; within the cluster, the probabilities of each instance to be chose for re-sampled are different.

We use the over-sampling combined with under-sampling to balance the class size. The sizes of the two classes are M maj and M min . The gap G between two uneven classes is: G = M maj  X  M min . Thus, the amount of instances in the minority class for over-sampling is: OS min = G  X   X  , and the amount of instances in the majority class for under-sampling is: US maj = G  X  (1  X   X  ) . To adjust the within class imbalance, we need to balance cluster sizes in each class. For the majority class, the numbers of instances to be under-sampled are proportional to the size of the cluster; for the minority class, the number of instances to be over-sampled is inversely proportional to the size of the cluster. For example, minority class. If  X  is set to 50%, the gap G is 30, N US = N OS =15 . The sizes of the three clusters in the majority class become 13, 10 and 7 after under-sampling, while both the sizes of the two clusters in the minority class become 15 after over-sampling. This reduces the within class imbalance, and in this case equalizes the class sizes.

Furthermore, we use the probabilities o f each instance to conduct the re -sampling with maintaining the data structure, in order to address the two type imbalance issues. In the clusters of the majority class, the instances with higher probability are dense, they are frequent in the subclass, and hence they have higher chance to be under-sampled. We choose the instances to be under-sampled according to the Gaussian distribution. In the clusters of the minority class, the new instances are produced according to the probability function of Gaussian distribution, resulting in finding more potentially interesting regions. The main steps in under-sampling for the clusters of the majority class and over-sampling for the clusters of the minority cla ss according to the distributi on probability are the following: Over-sampling phase:
Step 1: In the over-sampling for the minority class, the smaller the size of cluster within the class, the
Step 2: In the i -th cluster, N i OS instances are generated with the parameters from the current Gaussian Under-sampling phase:
Step 1: In the under-sampling for the majority class, we calculate the amount of instances to be under-Step 2: In each component Gaussian distribution, the center region is denser than the border region.
The procedure described above is the main scheme of HPS. This general idea of HPS and the differ-ence of this idea to SMOTE are visualized in Fig. 2. The (a) is the original skewed data distribution. We can see the minority class has two subclasses with within-class imbalance and an outlier instance. These factors may decrease effectiveness of the learning and over-sampling. The (b) is the result of the SMOTE. The procedure of SMOTE conducts the linear interpolation between nearest neighbor in-stances, resulting in generating many wrong minority instances under the complex distribution. We see that, some wrong minority samples are interpolated into the region of the majority class since noise and distribution. The (c) and (d) show the strategy of our HPS. The clustering result of GMM is shown in (c). (d) is the final result of the HPS. We can see that HPS is able to broaden the decision regions and the concept of minority class from a global perspective to a perspective that encompasses local information in order to deal with within-class imbalance and small disjuncts issues. 3.2. Integration of HPS and diverse random subspace, HPS-DRS The redundancies and noise in the feature set hinder the re-sampling techniques to achieve their goals. Moreover, the quality of probability estimation and classification will largely depend on the feature set. The irrelevant or redundant features can lead to a decrease in performance on the re-sampling and prediction.

An important trend in machine learning is the appearance of ensemble learning which combines the decisions of multiple weak classifiers to form an integrated output, so as to provide a diversity for avoiding the overfitting for some algorithms. More over, ensemble learning is also a good solution for solving the class imbalance problem, as it is incorporated with re-sampling technique to acquire better classification performance and generalization capability. Ho showed that the random subspace method is able to improve the generalization error [19]. In the random subspace ensemble, the individual classifier is built by randomly projecting the original data into subspaces and training a proper base learner on these subspaces to capture possible patterns that are informative on classification. The majority voting scheme is utilized wh en combining each specific cl assifier X  X  p rediction.

Under the current standard random subspace scheme, there are three disadvantages requiring improve-ment: 1) it only picks the feature subset for the original feature set randomly without considering the diversity of instances. Projecting the feature space on a given subspace could produce or enhance noisy instances and even contradicting instances that would lead to poor performance. This is the case when independently from the feature subspaces of other classifiers in the ensemble, the standard RS scheme has random characteristics through the selection of feature subsets. However, there are still strong over-laps of the instances with feature selected when constructing individual classifiers on different subspaces, as there is no formulation to guarantee small or reduced overlap; 3) because some subspaces may con-tain noisy features and individual classifier developed from these subspaces are not informative, it is not correct treating each classifier as if it contributed equally to the group X  X  performance; there is a lack of attention to appropriate weight assignments to individual classifiers according to their respective performance based on the different subspace.

Therefore, we propose an improvement of RS, called DRS (Diverse Random Subspace) for addressing these disadvantages. Firstly, we extend the common random subspace by integrating bootstrapping sam-ples in order to obtain the diversity with respect to instances and features. In the bootstrapping method, different training subsets are generated with uniform random selection with replacement. Secondly, it cannot ensure the diversity of each subset since the instances and the features are chosen randomly without considering previously selected subspaces for other classifiers. Therefore, to improve diversity between each subset, we use a formulation to make sure each subset is diverse. We introduce a concept of overlapping rate of subsets: and instance size of each subset; e.g., in Fig. 3, the overlapping rate is 16%.

We then introduce a threshold T over to control the intersection between each subset. The overlapping rate of all the subsets should be smaller than the threshold T over .
The GenerateDiverseSets described in Algorithm 1 generates a diverse set DiverseSet , by iter-atively projecting bootstrap sample D k into the specific random subspace RS ( D k ) . The function the previously collected projections in DiverseSet based on the overlapping region threshold T over .The generation of projections stops when there is stagnation sr , after enough trials, no new projection is diverse enough from the collected subsets. It enforces the diversity or independence by minimizing the overlapping region among the subset with subspace used previously.
 Algorithm 1 GenerateDiverseSets Require: Ensure: 1: change =0; DiverseSet = {} 2: while change &lt;sr do 3: A bootstrap sample D k selected with replacement from D train with R s 4: Select an random subspace with R f from D k 5: if isDiverse ( subspace ( D k ) , DiverseSet ,T over )== true then 6: DiverseSet.add ( subspace ( D k )); change =0; 7: else 8: change = change +1; 9: end if 10: end while
Thirdly, we employ a weighted average while combining classifiers according to the performance of each component. In the diverse subsets, some of the selected subspaces may have better performance on the imbalanced dataset; others lack the ability to properly discriminate between the different classes. We utilized the out-of-bag (OOB) sampl es in determinin g different classifier X  X  voting power, and then each base classifier is weighted when combined to create the final decision function. The goal is to assign weights that reflect the relative contribution of each classifier in improving the overall performance of the ensemble. It is known that the use of overall accuracy is not an appropriate evaluation measure for imbalanced data. For example, a dataset for which the majority class represents 99% of the data, and the minority class represents 1% of the data (this dataset is said to have an imbalance ratio of 99:1). In such cases, the classifier which always predicts the majority class will have an accuracy of 99%. When the performance of both classes is concerned, two accuracies of both classes are expected to be high simultaneously. Kubat et al. [25] suggested the G-mean defined as the geometric mean of accuracies measured separately on each class: ( G -mean = ACC maj  X  ACC min ). G-mean measures the balanced performance of a learning algorithm between these two classes, and is commonly utilized when performance of both classes is concerned and expected to be high simultaneously. Therefore G-mean is chosen to be the metric for representing the performance of each classifier.
 The HPS-DRS algorithm is described in Algorithm 2.
 Algorithm 2 HPS-DRS Require: 1: Ensemble = NULL 2: DiverseSets = GenerateDiverseSets ( D train ,R s ,R f ,T over ) 3: for each subset D k in DiverseSets do 4: Apply HPS on the subset D k , and generate a new balanced set BD k s with  X  5: Construct a classifier C k on the BD k 6: Evaluate C k on the OOB ( D k ) and obtain the value of G-mean, GM k 7: C k . Subspace = Subspace ( D k ) ; C k .GM = GM k 8: end for 9: Ensemble = Ensemble  X  C k 10: Calculate and normalize the weights of each classifier in Ensemble according to its GM 11: Calculate output from each classifier of Ensemble with D test 12: Generate the final output by aggregating all the outputs with weighted voting
To reduce the learning time of HPS-DRS, the procedure of sampling and learning in each subset D k can be carried out in parallel before aggregating. Moreover, each classifier is trained in the reduced subset with fewer instances and features. Therefore the computational time is acceptable.
Since data often exhibits characteristics at a local rather than global level, DRS can find more valu-able local data properties so as to improve the quality of sampling. Moreover, the different imbalanced data distribution in each random subset makes the ensemble classifier robust to the evolving testing distribution. Furthermore, DRS can alleviate the effect of class overlapping on the imbalanced data dis-tribution [10], since the two classes may be separable in some reduced subspace. 4. Experimental study 4.1. Dataset description
To evaluate the effectiveness of our method on the classification of different datasets, and to compare with other methods specifically devised for imbalanced data, we tried several datasets from the UCI database containing imbalance (Table 1). Some of these are originally multiple class datasets and were converted into binary class problems by keeping the smallest class as minority and the rest as majority. The datasets used contain different degrees of imbalance from 4% up to an almost balance at 47%. 4.2. Experiment 1: Evaluating the effectiveness of HPS-DRS
In this experiment, we evaluate the effectiveness of our proposed algorithm HPS-DRS. We conduct the comparison between the basic classifier without re-sampling (basic), HPS, HPS-DRS, as well as CHS working on the original RSM framework (HPS-RS). We chose unpruned decision tree (C4.5) with Laplace-smoothing as our base classifier, because it is the most commonly used classifier with sampling for imbalanced datasets in the literature. The ensemble size of all the ensemble methods is set to 50. In the parameters setting of HPS, the  X  is set to 70% for avoiding loss of reducing too many instances, and the parameters K issetto5.IntheparameterssettingofDRS, R s is 0.7, R f is 0.5. The best value of T over can be obtained from the training data, then the ensemble size can be determined adaptively. In this ensemble size according to experiments. Although it is not necessarily the best, it can guarantee the diversity among each subset. All the experiments are carried out by 10-fold cross-validation. The results are shown in Table 2.

From these experiments, we can show that HPS is a good re-sampling method, since it improves C4.5 categorically on all the datasets except Glass. In addition, we can also point out that DRS is a good ensemble framework which can inject more diversity into the bias of sampling and learning algorithm, so as to help HPS acquire better over-sampling performance and achieve a better generalization abil-ity. Especially for the datasets with high dimensional features such as Spambase and Sonar, HPS-DRS achieves good generalization and avoids the negative impact of high dimensionality as well as the strong bias of noisy features; the diverse random subspace method emphasizes ensemble diversity explicitly during training and fuses all the components with weighted voting achieving better performance than the traditional random subspace ensemble on the imbalanced data. This indicates that the diversity in the ensemble can facilitate class imbalance learning. Ho wever, our method cannot achieve the best result on some low dimensional datasets, such as Transfusion with only 4 attributes. That is because the random subspace method is more effective when datasets have a large number of attributes.

From the results, we can also notice that HPS-RS and HPS-DRS are performing well and improve upon the basic classifier in the case of a balanced dataset such as Sonar, while the improvement by single HPS is not evident. This is because in the almost balanced data, the value of G is so small ( still exist within-class im balance. Since HPS is bound by G the effect of HPS is indeed minimal when we have balanced classes. In the case of Sonar it was the DRS that improved upon basic thanks to the ensemble with diverse subspaces. Sonar still has within-class imbalance based on GMM. By lifting the G restriction on HPS, one could further improve the results.

The scarcity in the minority class includes relative scarcity and absolute scarcity. Relative scarcity is when despite the imbalance the minority class is still well represented. Absolute scarcity is when the minority class does not have enough instances to represent it. As HPS is based on the estimation of data distribution, when the amount of instances in the minority is not sufficient, it cannot obtain the exact parameters of data distribution, resulting in low quality of hybrid sampling, such as Glass dataset with only 9 instances. Although the Letter dataset contains the same imbalance level as Glass, it has enough instances in the minority class to be represented in the learned model. Therefore, HPS based methods question that is not well addressed by current approaches. The minority class is so weakly represented that very little can be learned from its rare instances. 4.3. Experiment 2: Comparison between HPS-DRS and state of the art methods
We compare between our method HPS-DRS, and the state-of-the-art methods, such as MetaCost (MC) [12], SMOTE over-sampling (SM) [6], SMOTEBoost over-sampling (SMB) [8], ENN [37], Cluster-based Over-Sampling (CBOS) [23] and COG with random over-sampling(COG-OS) [38]. These methods were considered because they are commonly used in research on class imbalance, some from the algorithm perspective and some from the re-sampling perspective. Moreover, SMB and MC are the methods integrating the ensemble framework, while CBOS and COG-OS are both based on the cluster-ing techniques for spl itting the data into segments. We don X  X  use the non-heuristic random re-sampling in our comparison since that they have potential drawbacks such as information loss or causing overfit-ting [6,17].

All the SMOTE based over-sampling methods are set to the commonly used 200% threshold. More-over, the number of nearest neighbors is set to five when generating new synthetic instances as done in the literature. The ensemble sizes in all the ensemble classification method are set to 50 in our ex-periments. ENN does not require a user specified under-sampling ratio, and K is set to the default value In CBOS, the value of K in the K-means clustering for each class is set to 2. In COG-OS, the cluster number is set to be 4, and the minority class is over-sampled to the average size of the partitioned large class approximately.

To make our comparisons more convincing, we further use AUC as the performance evaluation. AUC measures the performance of ranking the minority examples above the majority example. It can capture the trade-off between true positives and false positives, producing a robust metric for even the most imbalanced datasets.

From Tables 3 and 4, we find that, HPS-DRS provides the best results in terms of G-mean and AUC in most of the datasets. SMOTE and SMB consider the class skewness and properties of the dataset as a whole, and manipulate the instances blindly without taking the majority class into consideration, result-ing in overgeneralization [31]. It leads to the creation of wrong instances when the class dispersion or the class noise exists, decreasing the value of G-mean and AUC. Although CBOS solves both imbalance issues, the disadvantages of CBOS introduced in 2.3 lead to poor results. COG-OS is only effective on the linear classifier, thus it cannot offer consistent performance based on the decision tree.
HPS-DRS shows appealing performances on imbalanced data through hybrid sampling with probabil-ity function and the diverse ensemble framework. HPS-DRS solves the issues of two types of imbalance more flexibly and efficiently. From Tables 3 and 4, we can also notice that for the datasets with high di-mensional features such as Spambase and Sonar, HPS-DRS offers a great advantage over other sampling techniques. 4.4. Experiment 3: The effectiveness of sampling ratio  X  on HPS-DRS
The optimal re-sampling ratio is usually unknown. In order to observe the influence of the re-sampling ratio in HPS-DRS on the classification performance, we chose German dataset with a moderate imbal-anced level 30% as an example of the variation of performance with the ranging of  X  . The range of  X  is averaged G-mean and AUC results.
 From Fig. 4, we can see the changes of G-mean and AUC when varying the value of  X  in HPS-DRS. When  X  is 0, only under-sampling for the majority class is carried out and no new instances are gener-ated. Important information of the majority class may be lost, hence the performance is lowest. When increasing the value of  X  , the two performances increase. When  X  is 1, over-sampling for minority class is performed without removing any redundant instance from the majority class. The issue of overfitting may occur due to the large amount of the minority class as well as the redundant information of ma-jority class. Moreover, we found G-mean and AUC to be highest when  X  is 60% and 65% respectively. It demonstrates the hybrid re-sampling scheme with an appropriate sampling ratio can achieve opti-mal classification performance. Moreover, it illustrates the effectiveness of the hybrid sampling method compared with each individual re-sampling technique.

Clearly, the choice of  X  affects directly the final performance, so it is desirable to obtain the optimal parameter of sampling ratio. However, many studies have shown that for certain imbalanced data sets, the prior degree of imbalance between class distribution is not the only factor influencing performance of classification of imbalanced data. Data set complexity (overlapping, lack of representative data as well as final classification performance. For instance, in a dataset with a high between-class imbalanced ratio, if a minority class with a simple concept has sufficient instances, it may not require extra synthetic instances, so the corresponding value of  X  may be a lower value. If a minority class lacks instances to represent its own concept, the corresponding value of  X  may be a higher value. Furthermore, the value of  X  depends on the redundancy level of the majority class. Therefore, there are no explicit relationships between the class prior ratio and the optimal sampling ratio. As a result, we are unable to obtain the optimal sampling ratio beforehand and have to empirically discover the optimal value of the sampling ratio parameter for obtaining the best performance on each dataset.

In order to estimate the optimal parameter  X  , the best  X  is chosen by cross validation in the data set. In imbalanced data cases, available data instances, mainly instances of the minority classes, are insufficient for traditional cross validation in the training set. For this reason, we randomly divided the original data set into two sets: the training set (80%) and the validation set (20%) for measuring the performance of each value of  X  . This process is repeated 10 times. The output is a value of  X  which yields the best measure metric value among all tests. We chose G-mean and AUC as the guidance metric to tune the parameter, respectively. Moreover, we compared the optimized HPS-DRS with SMB of which over-sampling percentage is optimized by G-mean in the [100%, 500%] range with 50% step. The parameter of sampling level of both ensemble based sampling need to be searched separately. This ensured that both approaches were independently optimized in the same fashion to achieve their best performances.
After identifying the optimal parameter of sampling level for both sampling methods on each dataset, we compared them having this selected sampling parameter. All the methods are conducted by 10 fold cross validation. The section of the 10 fold cross validation is totally independent from the one of cross validation for obtaining the optimal sampling parameter. All the results of G-mean are shown in Table 5. Moreover, we list the optimal sampling parameter value. In the majority of cases, for HPS-DRS, the G-mean value from the G-mean wrapper is higher than the one of the AUC wrapper, but in some cases, the G-mean value from the AUC wrapper is higher, such as Vowel and Spambase datasets. From this, we believe that by using AUC as the wrapper evaluation function we get better performances. Moreover, the optimized HPS-DRS outperform the optimized SMB with the same metric of G-mean on 8 datasets. 4.5. Experiment 4: The robustness of noise effective
Data are said to be noisy if they contain erroneous data values. These erroneous values can occur in the dependent (class noise) or independent (attribute noise) variables in a data set [43]. Noise in imbalanced datasets may exhibit unpredictably negative effects on the performance of learning algorithms. In order to systematically investigate the robustness of HRS-DRS, we manually introduced noise with different levels into the German dataset, then assessed the performance of HRS-DRS and other comparative methods on the new datasets. We generated two types of noisy instances manually: class label noisy instances and attribute noisy instances, so as to simulate and evaluate the robustness of our method in both cases.
 We employ the same experimental procedure as in [1,9] to inject the label noisy instances into the German dataset. Given a noise label l , each instance sees its label permutated with a probability of l %. Table 6 shows the AUC value of HRS-DRS and other comparative learning algorithms under different class label noise levels. Moreover, we employ the same experimental procedure as in [9,42] to generate the attribute noisy instances. Given a noise level l , each attribute in each instance is changed with a different value with a probability of l %. This new value is selected uniformly among the other possible values for this attribute. Table 7 shows the AUC value of HRS-DRS and other comparative learning algorithms under different attribute noise levels. We find that the class noise is more detrimental to classification performance. The results clearly indicate the superiority of our method in the presence of noisy data, particularly the higher the noise level gets.

In our method, the random subspace ensemble decreases the influence of noise in the attribute values while bootstrap sampling reduces the effect of noise in labels. It is possible that noise in the training set do not show up in some certain subset, as the instances with noisy label may not be selected by the bootstrap sampling or the noise attributes may not be chosen by the pseudo-random selection. Moreover, in the procedure of DRS, some instances with noisy label matching the nearest neighbors rule can be removed so as to avoid the impact on the estimation of the probability distribution and the subsequent sampling to some extent.

SMB achieves the best AUC on the original German dataset. However, Boosting is less robust in noisy settings. This is expected because noisy examples tend to be misclassified, and the weight will increase for these instances, so SMB has a lower performance as the noise level increases. In addition, SMOTE is sensitive to noise since the interpolation of new instances is generated along the line between nearest neighbors with the same class label in the feature space. All simulation results presented in this section illustrate the robustness of HRS-DRS to the negative influence of noise. 4.6. Experiment 5: Lung nodule candidate data classification
In the process of lung cancer diagnosis pulmonary nodules are first uncovered. Computer Aided Di-agnosis (CAD) systems to detect lung nodule in chest radiographic images ca be broadly divided into the suspicious nodule candidates, the initial detection of the CAD requires high sensitivity, and so, it produces a number of false positives. The purpose of false-positive reduction is to remove these false positives (FPs) as much as possible while retaining a relatively high sensitivity. This is known as the False Positive Reduction (FPR) problem, which is a binary classification between the nodules (TPs) and non-nodules (FPs). The most significant problem in the FPR is that the two classes are skewed and have unequal misclassification costs.

Class imbalanced data has detrimental effects on the performance of conventional classifiers, resulting in lowering the performance of discrimination in the candidate nodule. However, in nodule classification, the problem has attracted less attention. Only few papers have been published addressing this problem. The authors in [4] use Tomek links with SVM (TL-SVM)to remove borderline false nodule cases in order to achieve 100% sensitivity. Campadelli et al. prove that cost-sensitive SVM (CS-SVM) trained with imbalanced data sets achieve promising results in terms of sensitivity and specificity, by means of adjusting the misclassifications cost of false positives versus false negatives [5]. Dolejsi et al. use asymmetric Adaboost (Asy-Adaboost)learning to improve the sensitivity by setting different weights for two classes [11]. These are the three contenders against which we will compare our approach.
Constructing an accurate classification method requires a training data set that represents different aspects of nodule features. Our feature extraction process generated 43 image features, features that are commonly used in medical image processing for characterizing lesions. Using these features, we constructed the input space for the compared classifiers.
 Our database consists of 98 thin section CT scans containing 106 solid nodules, obtained from the Guangzhou hospital in China. These databases included nodules of different sizes (3 X 30 mm). We ob-tained the appropriate candidate nodule samples using a candidate nodule detection algorithm, which identifies 95 true nodules as positive class and 592 non-nodules as negative class from the total CT scans. Figure 5 shows an example result images of candidate nodule detection.

We chose SVM as the classifier, as it is the most commonly used classification model in the nodule recognition. The intrinsic parameters (C and  X  ) are obtained by grid search under the guidance of G-mean prior to be combined with sampling or cost strategy. Experimental results in Table 8 show that HPS-DRS improved the performance of nodule recognition as compared to the other three methods. It means that our method can be applied on the nodule or other lesion detection in medical images and improve upon state of the art. 5. Conclusion
In this paper, we presented a probabilistic local over-sampling method, combined with diverse random subspace ensemble for classification on two-class imbalanced data. The class imbalance problem may not be a problem in itself. Rather, the complex distribution such as within-class imbalance and high dimension are responsible for the performance decrease. The main idea of the method is adding more accurate synthetic instances for the minority class on local regions according to the probability function for targeting within-class imbalance in tandem with the between-class imbalance. In addition, the diverse random subspace provides a diverse framework to reduce the affect of high dimension and enhance the generalization ability. We showed that HPS-DRS can achieve better results than state-of-the-art methods for imbalanced data through extensive experiments on multiple benchmark datasets from UCI and a real world dataset regarding lung nodule detection.

As a new method for imbalanced learning problems, there are several interesting future research di-rections for HPS-DRS. In this paper, HPS-DRS is only evaluated on imbalanced binary class learning; we will extend it to multi-class for improving its applicability in our future research. Moreover, the two ratio parameters ( R s and R f ) in DRS dependent on the data distribution are determined empirically. We will explore a method to determine them automatically according to the data distribution. Further-more, we will evaluate our method on text data with much higher dimensionality. Two main issues in text data classification are the high dimensional feature space and high feature-to-instance ratio, a classifier usually suffers from the  X  X urse of dimensionality X . Random subspace ensemble can overcome these issues very well [13] and we can use a multinomial mixture distribution in place of the Gaussian mixture distribution to model and estimate the probability of text data [27] since the bag of words is the typical representation used in text mining. In addition, since HPS is contingent on G , the differential between the majority size and the minority size, we intend to change the formulation within HPS not to be restricted on G , as completely balanced distribution is not always the best distribution. Acknowledgments This work is supported by the Alberta Innovates Centre for Machine Learning as well as the Chinese National Natural Science Foundation under grant (No. 61172002 and 61001047). Moreover, one author was supported by the China Scholarship Council for two years at the University of Alberta. References
