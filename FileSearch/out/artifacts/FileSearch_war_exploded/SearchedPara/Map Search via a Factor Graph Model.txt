 Map search has received considerable attention in recent years. With map search, users can specify target locations with textual queries. However, these queries do not always include well-formed addresses or place names. They may contain transpositions, misspellings, fragments and so on. Queries may significantly differ from items stored in the spatial database. In this paper, we propose to connect this task to the semi-structured retrieval problem. A novel factor graph-based semi-structured retrieval framework is intro-duced to incorporate concept weighting, attribute selection, and word-based similarity metrics together. We randomly sampled a number of queries from logs of a commercial map search engine and manually labeled their categories and relevant results for analysis and evaluation. The results of several experimental comparisons demonstrate that our method outperforms both state-of-the-art semi-structured retrieval methods and some commercial systems in retrieving freeform location queries.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Information Search and Retrieval Map Search, Query Representation, Factor Graph Model
In recent years, along with the quick expansion of mo-bile Internet and the use of smartphones, map search has become one of the most popular applications. With map search, users can retrieve geographic information from spatial databases using textual queries. Queries can be formal postal addresses or informal textual descriptions of category, name, location, or geographic coordinates. To satisfy increasing requirements, both research and industry communities have paid attention to this task [13, 17, 22, 32, 36, 39].

With the purpose of understanding the needs of users, we analyze query logs of a commercial map search engine and find that many queries contain transpositions, misspellings, conflicting information, and similar variations. Consider the following query examples which may be used to retrieve the restaurant named  X  X lexander a  X rs Steakhouse X , located at  X 410330 North Wolfe Road, Cupertino, CA 95014 X :
From the above examples, we can observe that freeform textual queries may significantly differ from the items stored in the spatial database. Moreover, different countries and territories may have different address formats. Textual attributes in some spatial databases are even more complex. Attributes may contain unstructured descriptions or multi-ple elements that cannot be easily structured. For example,  X 165 Lujiazui ring road, Shangri-La Hotel across the way X  is a valid postal address in China. Hence, the primary target of this work is to handle the matching problem between freeform textual queries and item s in semi-structured spatial databases.

Previous work devoted to this task can be roughly divided into two directions: spatial keyword search and geo intention discovering. Studies belonging to the spatial keyword search category consider both distance and textual relevance for location-based services [39, 14, 38, 26, 17]. They have proposed numerous specialized index structures, which combine inverted indexes with space-filling curves [12] and tree-based methods [14, 25, 39], to deal with the efficiency problem. Christoforaki et al. [13] introduced coarse-grained spatial structures to the inverted indexes. However, queries with misspellings, conflicting information, transpositions, and other issues are much less discussed in these studies.
On the other hand, a number of researchers use unsuper-vised, semi-supervised, and supervised methods to parse a query into a semantic interpretation [31, 32, 36, 15]. Some studies focus on the location disambiguation task and used rule-based [3], ontological-based [32], and learning-based [1] methods to resolve it. The CLEF 2007 Cross-Language Geographic Information Retrieval Track [28] defined seman-tic components of queries. Since automatically analyzing semantic interpretation of a query is still challenging, if directly creating intermediate interpretation of queries for map searches, the performance of retrieval methods may suffer from the low performance.

In this work, we propose to connect the map search task to the semi-structured retrieval problem and directly model the relationships between query terms and locations. The pro-posed method is based on factor graphs. The query concept, which models dependency between arbitrary query terms, is treated as the basic unit in this model. To handle variations of queries and verbal terms, concept weighting is processed by factor functions. We also incorporate the mapping relationship of attributes and concepts into factor functions to handle attribute selection. Experimental evaluations demonstrate that by integrating these characteristics, the proposed method can significantly improve the effectiveness of map searches.

Overall, the contributions of this paper can be summa-rized as fourfold. First, we propose to model the map search task as a semi-structured retrieval task. Second, a novel fac-tor graph-based semi-structured model is introduced to deal with textual issues in map searches by combining concept weighting and attribute selection in a unified framework. Third, a word-based similarity metric is proposed to match names between the query and items in a spatial database. Fourth, a detailed analysis of query logs is provided to show the specific characteristics of map search queries.
The remainder of the paper is organized as follows: In section 2, we review a number of related works and the state-of-the-art approaches in related areas. Section 3 presents the proposed method. Experimental results in test collections and analyses are shown in section 4. Section 5 concludes this paper.
The proposed approach builds on contributions from several research communities: (1) spatio-textual index struc-ture, (2) geo intention discovering, and (3) semi-structured information retrieval. In the following of this section, we give brief description of the previous works on these areas.
Spatial keyword search approaches are based on input location and keywords to find all relevant POIs. There are many previous studies focusing on this task [39, 16, 14, 38, 26]. Zhou et al. [39] described a hybrid index structure and three different combining schemes to handle both textual and location aware queries. The index structure integrated inverted files and R*-trees. Felipe et al. [16] defined the top-k spatial keyword search problem and proposed IR 2 -Tree, which also combined R-Trees and signature files to answer spatial keyword queries. G  X  obel et al. [19] also proposed an R-tree based approach, of which index structure was optimised for a single criterion adding special treatment for the other criterion at the leaf nodes. These methods are mainly focusing on index structures for efficiency.
Cao et al. [9] introduced another problem of retrieving a group of spatial web objects and proposed approximate solu-tions based on IR-Tree. Fan et al. [17] studied spatio-textual similarity search problem on regions-of-interest (ROIs).They proposed grid-based signatures and efficient hybrid filtering algorithms. Roy and Chakrabarti [4] introduced the problem of location-aware type-ahead search on spatial databases. They proposed an integrated Trie tree with a spatial data structure for this task.

We can observe that most of previous spatial keyword search methods mainly focused on index structures to process spatial overlap. However, as can be seen from the example given in the introduction section, textual issues are also challenging in practice. Christoforaki et al. [13] also reported that text-first could beat both R  X  -tree methods and some index structures. Hence, in this work, we restrict our attention to complex textual issues.
In order to satisfy the information needs of users, vari-ous approaches have been proposed to discover geographic intention from user queries. The cross-language evalu-ation forum even proposed a cross-language geographic information retrieval track in 2007 (GeoCLEF) [28]. A number of unsupervised, semi-supervised, and supervised methods have been proposed to parse queries to semantic interpretations [31, 32, 22, 36, 15].

Datha et al. [15] proposed a hybrid algorithm to extract entity names and attributes from queries. Spatial processing was used to determine viable partial interpretations. Joshi et al. [22] also introduced spatial constraints to resolve the ambiguities problem.

Sengar et al. [32] proposed to find a direct mapping from subsequences in the text query to specific entities in a spatial database. Given a textual query, a list of multiple approximate matches between query subsequences and entity attributes are calculated based on pre-computed indexes. Then, their algorithm tried to generate a set of candidate interpretations. Finally, a ranked list of interpretations is returned as final results.

Yi et al. [36] proposed city language model and methods using the model on three tasks: 1) identify users X  implicit geo intent; 2) determine whether the geo-intent is localized around the current geographic location of user; 3) predict cities from queries.
 The goal of these investigations are related with us. However, in this paper, the proposed factor graph based method directly models the relation between query and entries in a spatial database without any intermediate steps. Although, geographic intention is not explicitly dealt with under this framwork, one can easily determine it through final result.
The proposed method is also related to the research on structured and semi-struct ured information retrieval. The goal of this task is to allow users to enter freeform queries to retrieve complex semi-structured XML data, structured database, and RDF resources. The task has been addressed from both IR and database perspectives by various approaches[30, 27, 23, 11, 2, 18, 8]. Category Description Example Percentage
POI The intentions of the queries are to find a particular POI.
List The intentions of the queries are to find POIs belonging to
Address Queries contain only the address of the target location.
Complex Queries contain more than one attributes (place name,
Verbose Queries contain verbal terms, which are used as grammatical
Typo Queries contain misspelled street, place name, category, or
Conflict Queries contain conflict information provided by different Ogilvie and Callan [30] adapted a language model for XML component retrieval. The method assigned different weights for each element based on the length of text content or the importance for retrieval. Lu et al. [27] extended document level field-weighted retrieval function BM25F to element level retrieval function BM25E. Each element was treated as a document. Kim et al. [23] proposed a probabilistic method which infers the mapping between each query term and XML element based on collection statistics. Probabilistic semi-structured retrieval model use the mapping probability as weights to combine each element intoadocumentscore.

Chen et al. [11] gave an overview of this task in the tutorial of SIGMOD 2009. Agrawal et al. [2] studied the task of retrieving relevant information from structured databases for web search queries. They proposed search engine-integrated approach, which used web documents to enhance the performance. Ganti et al. [18] studied the problem of keyword search over entity databases, and proposed an approach to a keyword query to a structured query. Bicer et al. [8] focused on keyword search result ranking and proposed a relevance-based language models for ranking aggregated structured results.

In this work, we connect the map search task to semi-structured information retrieving task and proposed a novel factor-graph based approach to model the similarity between query and entries of semi-st ructured spatial database.
Given a query q , which contains n terms { t 1 ,t 2 , ..., t aim of map search is to find a list of related POIs (point of interests) from a spatial database. A spatial database consists of a huge number of POIs which have geometry and textual attributes. Geometry attributes include longitude, latitude, altitude, target shape description, and so on. Textual attributes contain street address, city, state, name, category, landmark, ZIP code, et al. We use p i to represent the i th POI in the spatial database. { att i 1 ,att i 2 , ..., att represents attributes set of POI p i .

In this work, we make no assumptions about the form and structure of inputs. It means that users can specify a target location using either well-formed or ill-formed address, place name, category and any other textual contents as user wishes. Hence, efficiently matching subsequences of queries with attributes of POIs is a challenging problem. For example,  X  X an Francisco 748 Van Ness Ave X  X ould be a legal query, and the POI named X  X hilz Coffee X , which is located in this address, should be retrieved as the corresponding result. All the queries described in the introduction section are also can be used to retrieve  X  X lexander X  X  Steakhouse X .
To better understand the intention of users, analyzing search query logs is one of the commonest way. Most of the previous analyses on map search focus on discovering the characteristics of distance, search session, type of target place, and so on [21, 35]. In this work, we examine query logs from the perspective of retrieving. We collected query logs from Nov. 1st, 2012 to Nov. 10th, 2012 with the help of a commercial map search engine in China. For analysis, we sampled 10,000 queries from query logs and manually labeled their categories.
 In this paper, we work on the following questions: 1) Whether the intention of user is to find a particular location or a set of locations? 2) What kinds of information are provided through queries? 3) Whether queries are exactly the same as items in spatial database?. To answer these questions, we propose to classify queries into 10 categories, which can be classified into three groups. The description of the query categories, examples and percentage of queries for each category are shown in Table 1.

From the statistic, we can observe that over 86% queries belonging to POI category. It means that users often already have a clear target place when they enter the query. For the distribution of attributes in map search queries, we observe that 24.7% queries contain more than one attribute. For example, the query  X  X lexander steakhouse north wolfe cupertino X  contains sub-queries for three attributes: POI name, road, and city name. Among all List queries, about 81.8% of them are name query and the other 18.2% queries contains not only a name of POI but also other information. However, among the POI queries, the distributions of Name,Address, and Complex categories are very close to the distributions of all queries, which are 52.4%, 19.8%, and 27.8% respectively.

We also study the categories of queries variants. From the percentages of queries belonging to  X  X erbose X ,  X  X ypo X ,  X  X ragment X , and  X  X onflict X  categories, we can observe that most of the queries are not strictly equal to the items in spatial database. About 62.9% queries do not contain complete place names or addresses. Verbose queries also occur frequently, which are around 29.5% among all queries. Since only 3.5% queries contain conflict information in total, most of information provided by users can be directly used as constraints for retrieval results. Around 16.8% queries contain typos. We think that the longer length of map search queriesisoneofthemainreason.
From the definition and analysis of the task, we can observe that the problem of matching subsequences of queries with attributes of POIs can be converted into semi-structured search task. To handle fragment, misspelling, verbose, and other issues, we propose a novel factor graph based semi-structured search model, which combines con-cept weighting and attribute selection together. According to the definition given by Kschischang et al. [24], a factor graph is a bipartite graph, which has a variable node for each variable x i ,a factor node for each local function  X  and an edge-connecting variable node x i to factor node  X  if and only if x i is an argument of  X  j .
 Figure 1 shows the graph structure used in this work. Inspired by hypergraph model, which was proposed by Bendersky and Croft [5] for processing higher-order term dependencies, in this work, we also treat concept as the basic unit for retrieving. Vertex c i represents the variable of concept i . It models dependency between a subset of query terms. In order to show the relationships between concepts and query terms, we use dashed lines to connect them. Query terms are not directly incorporated into calculation. Vertex q and POI represent the query and POI respectively. Vertex att j represents the j -th attribute of the POI. Edges in the graph are based on two kinds of factors  X  e ( c, att  X  ( q,POI ). In following sections, we introduce details of concepts, edges, and factors used in this work.
Under this framework, concept is treated as the basic unit for retrieving. Structures group concepts belonging to the same type together. In previous works, several structures have been proved to be useful for document or passage level Figure 1: The factor graph used in this work for textual based location search task. retrieval task. In this work, we propose to use following structures for the location search task: UNI-Structure The unigram (UNI) structure treats the BI-Structure The bigram (BI) structure contains adjacent NE-Structure The named entity (NE) structure contains
As shown in Figure 1, two kinds of factors are used in this work: attribute factor (  X  e ( c, att )) and POI factor (  X  e ( c, POI )). Attribute factor assigns a score to the occurrences of concept c in all attributes of a POI, regardless of the other query concepts. Since the order of terms is important for location search, we incorporate POI factor to assign scores according to the number of common term sequence between query and attributes of POI.
 To model the occurrences of a concept c in a text fragment X , we follow previous works on log-linear retrieval models [5, 6]. We estimate the occurrence matching function f ( c, X using a log of the language modeling. In order to overcome the zero probability problem, Dirichlet smoothing is also used in this work[37]. where tf ( c, X ) function represents the number of occur-rences of the concept c in a text fragment or a collection;  X  is the smoothing parameter; | X | and | C | represent the total number of terms in X and the collection respectively.
Based on occurrence function f ( c, X ), attribute factors are defined as follows: where  X  ( c ) represents the importance weight of concept c ; P ( att | c ) represents the mapping probability between concept c and attribute att based on collection statistics; f ( c, att ) represents occurrence matching function.  X  ( c )is assigned by several weighting features, of which detailed explanations are given below.

From analyzing the query log, we observe that queries may contain one or more attributes such as POI name, street name, city name, and so on. Since these entities may contain multiple terms, term sequences should be taken into consideration. In this work, we use POI factor (  X  e ( q,POI to incorporate the number and order of common terms between query and attributes. Formally, where q is input query, which can be represent by concepts belonging to UNI-Structure ; Jaro word represents a similar metric, which is extended from Jaro metric [20] and will be discussed in the following section;  X  is the weighting parameter between two kinds of factor functions.
With the definitions of attribute, POI factor and factor graph ranking function in Equation 1, we can explicitly rewrite the relevant score Score ( q,POI ) between query and aPOIas: Score ( q,POI )= where  X  C represents the entire concept set.
According to the analysis of query log, we can observe that about 29.5% queries contain verbal terms, which have grammatical meaning for communication between humans. Previous works have proposed several frameworks to address this problem [6]. In this work, we also use weighting features and linear weighted combination to model concept weight as fellows where  X  represents weighting feature set;  X  is weighting feature. Table 2 presents the features used for concept weighting. These features can be grouped into two sets: frequencies and occurrences. Moreover, both of them can be efficiently calculated for even large scale collections.
Since user queries may contain one or more information belonging to different attributes and do not have explicit structure information, we propose to follow PRM-S[23], which estimates the probability based on statistic from attributes. Using Bayes X  theorem, we can estimate the pos-terior attribute mapping probability P ( att i | c ) by combing the prior probability P ( att i ) and the probability of a concept occurring in a given attribute type P ( c | att i ). P ( att i | c )= P ( c | att i ) P ( att i ) where P ( c | att i ) can be estimated based on bigram language model P att i ( t k | t k  X  1 ) for each attribute att i , sents the prior probability of attribute att i being mapped into any query concept.
Jaro metric [20] is used to measure similarities between two strings. It is based on the number and order of common characters. In this work, to emphasize the impact of the sequence of common terms between query and attributes, we extend it to word level Jaro word , which is based on the number and order of the common words between two strings. Given string s = a 1 ...a K and string t = b 1 ...b L , define a word a i in s to be common with t there is b j ,sim ( a i ,b j ) Let s = a 1 ...a K ,bewordsin s which are common with t (in the same order they appear in s )andlet t = b 1 ...b L be analogous; Then define a transposition for s , t to be a position i such that sim ( a i ,b i ) &lt; X  .Let T s ,t be one-half the number of transpositions for s and t .The Jaro word similarity metric for s and t is where the similarity between words sim ( a i ,b j )canbe different metrics, such as edit-distance metrics, token-based distance metrics, hybrid methods, or phonemic similarity metrics.
To applying this framework, we need to estimate pa-rameters  X   X  in Equation 6, and assign value for  X  in Equation 5. In this work, we also propose to use coordinate ascent (CA) algorithm [29] to do it. The CA algorithm has demonstrated its simplicity and effectiveness though several previous works [5, 7]. Given a target optimization metric, it iteratively optimizes a series of one-dimensional line searches. In each iteration, one parameter is processed, while all other parameters are fixed.

We perform the optimization in two stages. Firstly, we optimize  X   X  used in local factor function for concept weighting. Then we fix the parameters for weights and optimize parameters in global factor function. Features GF ( c ), QF ( c ), and NF ( c ),whicharedescribedinTable 2 are normalize by their respective maximum frequencies. The whole iteration is performed until the performance gain is below a given threshold.
In this section, we present experimental results. We com-pare the proposed factor graph based map search (FGMS) method with several semi-structured retrieval models, a query parsing based method and three online map search services. Evaluation queries are randomly sampled from a query log. In the following sections, we firstly describe the corpus, evaluation queries, and golden standard construction strategies. Then we present comparative results to demon-strate the effectiveness of the proposed method.
The spatial database used in this work contains a total of 10.8 million POIs. Each POI in the database has nine attributes: name, address, city, state, category, ZIP code, telephone number, longitude, latitude. The average length of  X  X ame X  and  X  X ddress X  are 9.17 and 16.26 characters respectively.
 We use queries belonging to four cities Shanghai, Hefei, Nanjing, and Shenyang for evaluation. For each city, we randomly selected a number of queries from the query log. To compare with commercial services, we also convert these queries and check that whether at least one relevant result existed in their spatial databases. Among the queries which all the commercial services and our spatial database contain relevant results, we randomly selected 600 queries for each city. There are 2,400 queries in total. Queries are split into two sets: 400 queries are used to train the parameters, the others are used to evaluate. Five human annotators were instructed to rate the relevance between query and retrieved results. A three-point scale is given for each query and candidate pair by annotators. The following descriptions of the scale are shown to annotators.
Percentage 
Figure 2: Number of relevant results per query.
The final score for each result is set to the value which the maximum number of annotators labeled. To compare with commercial online map search services, we also collect top 10 results of the 2,000 evaluation queries from Baidu Sogou 2 , and Google 3 through APIs provided by themselves. Since different services may use different spatial databases, annotators were also asked to label each of them with the same mechanism.

Even with the description of detail evaluation mechanism, labeling task is also a highly subjective process. To evaluate the quality of corpus, we validate the agreements of human annotations using Cohen X  X  kappa coefficient. The average  X  among all annotators is 0.526. If we omit the difference between 1 and 2 scales, the average  X  improves to 0.708. It indicates that the annotations of the corpus are reliable.
Figure 2 illustrates the distribution of the numbers of relevant results per query using different methods. LS1, LS2, and LS3 represent three commercial search services. We treat results whose final scores are 1 and 2 as relevant for the query. The zero number of relevant results means that no relevant results were returned by the method. Since LS2 cannot handle verbal terms well, about 16.9% queries do not have relevant results. We manually remove verbal terms from these queries, relevant results for most of the queries can be returned by LS2. Due to different strategies used by commercial services, the distributions of number of relevant queries are much different from each other. We implement the calculation Eq.(4) with two steps. Firstly, Indri [33] is used to calculate the first part of Eq.(4) http://developer.baidu.com/map/ http://map.sogou.com/api/ http://developers.google.com/maps/ services. Best result per column is marked in boldface. retrieved results are reranked based on the whole equation. Indri 4 is an open source search engine and provides a rich structured query language. Hence concepts and weighting schemes can be easily implemented.

To evaluate the retrieval performance, we employ the following standard performance measures: Normalized Dis-counted Cumulative Gain at 10 (nDCG@10), Mean Average Precision (MAP) and Precision at 10 (P@10). The graded relevance score in nDCG is set based the relevance score in ground truth. For the evaluation metric Precision ,we use two methods: P 2 @10 and P 12 @10. P 2 @10 is used to measure the precision of top results whose relevance scores are equal to 2. P 12 @10 is used to measure relaxed relevant results whose relevance scores are equal to 1 or 2. The two-tailed t-test is conducted for significance. MAP is the same as Precision .BothMAP 12 and MAP 2 are used as evaluation metrics. The ireval package provided in the Lemur toolkit is used for evaluation and significance test.
In this section, we compare the performance of the proposed method FGMS, with two semi-structured retrieval methods, a query parsing bas ed method and three commer-cial online map search services. Two semi-structured re-trieval methods are used to compare with. HLM represents the method proposed by Ogilvie and Callan [30]. It assigns different weights for each element based on the length of text content or the importance for retrieval. In this work, we tune the weights based on training data and distributions of attributes in sampled query log. PRMS , which is proposed by Kim et al. [23], uses the mapping probability as weights to combine each attribute into a final score. We re-implement it and tune the parameters with the same training data as described above. CRFs represents the query parsing based http://www.lemurproject.org/indri.php method. Firstly, we use conditional random fields to detect street, city, POI name, and other attributes. Then, these identified subsequences are used as queries for corresponding attributes for retrieving and ranking. To train the model, we manually labeled 2,000 queries. The token accuracy achieves 87.10%, which is comparable with the state-of-the-art systems [34, 10].

Table 3 shows the performance results of baseline method-s,threecommercialservicesaswellasourproposedmethod. We can observe that the proposed FGMS method is superior to the baseline methods, which only consider weights of attributes. For all the spatial databases of different cities, the result holds with all the evaluation metrics. Query parsing based method, CRFs, is better than PRMS, HLM, and LS2 in most evaluation metrics. However, due to the fact that its performance is relied heavily on the results of attribute detection, the variance of the performance is larger than others. It is also clear from the Table 3 that the PRMS method outperforms the HLM method in most cases. This result demonstrates the effectiveness of attribution weighting based on corpus statistic. In most cases, the retrieval effectiveness is also improved by the concept weighting and word sequence similarity.

Although different commercial services use different spa-tial databases, some evaluation metrics can also be used to compare with in some degree. From the results, we can ob-serve that the proposed method achieve better performance than commercial search services in most cases, especially under the evaluation metric P 2 @10. The performance of LS3 is the best one among all the commercial services.
To detailedly analyze the performances of methods for different kinds of queries, we compare performances of categories belonging to the same group. Fig. 3, Fig. 4, and Fig. 5 show the results of comparisons. As discussed in Section 3.1, we establish ten query categories, which can be grouped into 3 groups. Figure 3: Performances of different methods on POI and List queries.

Fig. 3 depicts the performance comparison of different methods in processing queries belonging to the categories of POI and list. We can observe that except for LS3, the other methods achieve better performance in processing POI queries. For list queries, LS3 achieve the best result. The performance of the proposed method FGMS is significantly better than other methods in processing POI queries. For list queries, FGMS only worse than LS3. HLM and PRMS can not process list queries well either, comparing with other methods. Figure 4: Performances of different methods on Name, Address and Complex queries.

Fig. 4 shows the performance comparisons of different methods on name, address, and complex queries. For address queries, most of the methods, except CRFs and PRMS, achieve better performance than name and complex queries. We think that it is because almost all the address queries belonging to POI category and address queries are usually related to only one POI. As the same reason, per-formances of complex queries are better than name queries can be explained. Since queries containing addresses usually contain more characters than name queries, the performance of CRFs may suffer from the low entity identification precision in processing address and complex queries.
The performance comparisons of different variants of queries are shown in Fig. 5. We observe that LS1, LS2, LS3, and the proposed FGMS can handle verbose queries well. However, HLM and PRMS are suffer from verbal Figure 5: Performances of different methods on Verbose and Normal queries. terms. We think that concept weighting contribute a lot for this issue. For conflict queries, all three commercial search engines can handle them well comparing with other query variants. LS1 achieve the best result in processing conflict queries. CRFs highly suffers from the conflict information. We think that the hard weighting strategies of CRFs may be the primary reason. Comparing to HLM and PRMS, the proposed FGMS achieve better performance for all variants of queries than them. Hence, the contribution of concept weighting and word based similarity can be demonstrated. According to the static shown in Table 1, a large of queries are not exactly same as the items in spatial database. Hence, methods which can correctly process variants can achieve better performance.
In this section, we analyze the inference of parameters used in the proposed method. In Section 4.6, we proposed to use coordinate ascent algorithm to optimize the parameters, which are used in local and global factor functions. To verify the stableness of parameter estimation, we list the value of parameters and present their inferences to one of the evaluation metric nDCG@10.

Table 4 shows the optimized weights of concept impor-tance features and global factor. The corresponding results of these parameters are shown in Table 3. We observe that among all the features used for concept weighting QF is the highest. It demonstrates that highly distinct concepts can be found from query logs. The optimized weight  X  for global factor is 0.08. We also note that all the parameters are assigned positive weight. This indicates all the features can contribute to the retrieval performance.

Fig. 6 provides analyses of robustness of weights for concept importance features. It shows the nDCG@10 scores ofdifferentweightsforGF,QF,NF,CB,SB,andTB in training data. When varying weights for one feature, weights of the other features are set to the optimized values listed in Table 4. We observe that although the weight of GF is relatively small, the performance drops quickly without it. Consistently with previous conclusion, the effectiveness gains achieved by QF also demonstrate its importance. Fig. 6 also demonstrates the effectiveness of each concept features. From the perspective of difference with and without a feature, the performance order is QF &gt; GF &gt; NF &gt; TB &gt; CB &gt; SB. Table 4: The optimized parameterizations of local and global factors.
In this paper, we focused on the location search task with freeform queries. We converted the problem as a semi-structured information retri eval task and proposed a novel factor graph based map search method, which incorporated concept weighting and attribute selection together. To analyze the intention of users, we collected a number of query logs with the help of a commercial online map search company in China. We defined several query categories and analyzed the distributions of different categories. Several experimental comparisons demonstrated that the proposed method outperformed baseline methods and some commer-cial systems for retrieving unstructured location queries. The results also indicated that concept weighting, attributes importance, and word based similarity metric benefited the performance gains.
The authors wish to thank the anonymous reviewers for their helpful comments and Suntec for providing us the valuable query logs. This work was partially funded by 973 Program (2010CB327900), Shanghai Leading Academic Dis-cipline Project (B114), Nationa l Natural Scie nce Foundati on of China (61003092, 61073069), National Major Science and Technology Special Project of China (2014ZX03006005), Key Projects in the National Science &amp; Technology Pillar Program(2012BAH18B01) and Shanghai Municipal Science and Technology Commission (12511504500).
