 The way a searcher interacts with query results can reveal a lot about what is being sought. Considerable research has gone into using implicit relevance feedback to identify relevant content in real-time, but li ttle is known about how to best present this newly identified relevant content to users. In this paper we compare a traditional search interface with one that dynamically re-ranks and recommends search results as the user interacts with it in order to build a picture of how and when users should be offered dynamically identified relevant content. We present several stud-ies that compare logged behavior for hundreds of thousands of users and millions of queries as we ll as self-reported measures of success across the two interaction models. Compared to tradition-al web search, users presented with dynamically ranked results exhibit higher engagement and find information faster, particular-ly during exploratory tasks. Thes e findings have implications for how search engines might best exploit implicit feedback in real-time in order to help users identify the most relevant results as quickly as possible. H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval---search process Interactive information retrieval, query log analysis, web search, dynamic ranked retrieval, im plicit relevance feedback. Searchers do not always find what they seek after a single query. Instead, they often issue multiple queries, incorporating what they learn from the results to iterate and refine how they express their information needs. While many search engines try to make this easier by providing reformulation suggestions and some searchers are able to quickly incorporate this feedback, others explore each set of search results exhaustively and fail to benefit from any new information identified during the search process [3]. To address this, search engines have begun to incorporate session context into the search results they return for subsequent queries in a session [20]. For example, if a person issues the query apples and then clicks on a website about fruit, future queries in that session can be biased to return more fru it-related results versus company-related results. Different types of session context have been ex-plored, including past queries [18], the topic or reading level [12] of clicked results and the snippet text [26]. The onus of reformulat-ing the query, however, continues to rest upon the user. In cases where users X  search skills are limited [30], their knowledge of the [31], reformulation can be daun ting and sometimes overwhelming. There is, however, an opportunity for search engines to exploit the implicit feedback users provide following a query to present the most relevant results immediately, without requiring a subsequent query. This can be done by dynamica lly altering the result set, in response to real-time im plicit relevance feedb ack, as the user inter-acts with it. Little is currently known, however, about how people might best encounter and use newly identified content in the course of a single query. To provide insi ght into dynamic result set inter-action, we explore how and when ne w content is useful to search-ers in the course of a single qu ery. We present several large-scale studies of people X  X  interactions with Surf Canyon, a popular com-mercial search system that re-ranks search results in real-time fol-lowing each user action. Contextually relevant results that initially might have been ranked deep within the result set have the opportunity to be promoted based on the user X  X  implicit si gnals. An example for the query apples is shown in Figure 1. Upon returning to the result page after clicking on the second result (a he alth food website) the user is presented with a  X  X eal-time recommendation X  based on this action, including a result about the dental benefits of apples. Because this search system is used daily by hundreds of thousands how real-time, dynamically ranked results impact people X  X  behav-ior. We present the results of two controlled studies designed to compare dynamic ranking with traditional, static web search. Our 
Figure 1. Real-time recommendations are presented inline analysis reveals that users have a higher engagement in the search process when provided with a dyna mic feedback experience, click-ing on more results and spending a longer time searching. Partici-pants aregiven exploratory tasks to complete on the two systems found more results while expendi ng less time when given dynamic results as compared to a traditional search interface. There is growing interest in the IR community around understand-ing and supporting personalization [18] [26]. Researchers have explored short-term personalization based on session context, in-cluding previous searches and clicke d results, to identify relevant results [21,9]. Teevan et al. [27] present a framework to identify queries that would benefit the mo st from personalization. In this paper we explore which queries be nefit most from the real-time personalized results. Relevance feedback is the prim ary post-query method for automat-ically improving system representation of a searcher X  X  information need and has been studied extensively. Explicit relevance feedback [13] allows users to select documents or terms to be used for query expansion. Explicit re levance feedback is rarely used, however, as it requires direct interaction, placing cognitive load on the user, and often results in the identificat ion of irrelevant content [10]. To overcome some of the challenges with explicit relevance feed-back, researchers have explored the use of implicit relevance feed-back. With implicit relevance f eedback, user behavior, such as clicking documents or scrolling, is unobtrusively monitored and used to expand the understanding of users X  information needs be-yond the query [32]. In addition to both explicit and implicit relevance feedback, other adaptive search systems have been explored. Kaplan et al. [11], for example, describe a navi gation scheme that adapts to user behavior by using associative matrices that encode user preferences. The term dynamic ranking and a theoretical justification for the ap-proach was formally introduced by Brandt et al. [7] where they proposed a retrieval model that optimizes relevance of dynamic recommendations based on the user X  X  choice of documents. Cramer et al. [17] present a pre liminary study of user behavior with dynamic ranking based on l og data. They find that dynamic ranking increases the click thro ugh rate of recommended URLs compared to a baseline system. We extend this line of research by conducting user centric evaluation and presenting additional analy-sis of searcher behavior during the interaction with a dynamic ranking enabled system. Search systems that rely on context and personalization are diffi-cult to evaluate because they depart from the notion of global rele-vance that can be effectively assessed by judges to a notion of personalized relevance that is ha rd to measure by anyone other than the particular user. As such, Bennett et al. [4] suggest evaluat-ing personalized search algorithms online by automatically identi-fying search result click behaviors that suggests satisfaction. Ageev et al. [1] propose a scalable, game-like framework for con-ducting remote user studies of searchers X  successes. We compare a system that dynamically ranks results to a baseline system by con-ducting a recall-based user study with similar tasks to those used by Ageev et al. [1]. Online controlled experiments (e.g., [14]) and crowdsourcing evaluation (e.g., [6 ]) employ similar approaches. To summarize, previous work into personalization, relevance feed-back and adaptive search has re sulted in promising ways to use context and implicit feedback to identify relevant results. Evaluat-ing such systems, however, is difficult and little is known about how people actually interact with relevant content that is identified and presented to users in real-time . In this paper we conduct user-centric evaluations of a dynamic ranking system that provides helpful insights for researchers and practitioners. We begin by discussing the details of the dynamic ranking system we studied. To understand how people interact with dynamically ranked re-sults, we study Surf Canyon, a popular commercial search system. Surf Canyon provides a browser extension (available for IE, Fire-fox, and Chrome) that applies an interactive dynamic layer on top the results returned by a number of existing popular search engines (e.g., Google, Bing, and Yahoo!) wh ich re-ranks search results as the users interact with them. In this section we briefly describe how dynamically ranked results are identified and look more close-ly at how these results are then displayed. When a query is issued to a major search engine using a browser that has the Surf Canyon extension running, the system begins by fetching, in the background, the top 50-100 results from the under-lying search engine. The user is initially presented with the top 10 results, as identified by the underl ying search engine, in a typical fashion. However, by then monitoring user actions, including link clicks, web page visits, scrolling, and back button clicks, Surf Canyon starts generating a real-time model of inferred intent. In-tent is derived from the titles, snippets and URLs presented to the user, and, in some cases, the page content and metadata infor-mation of clicked and skipped resu lts. All activity in the current information session is used to build the model. This model is used to expand upon the initial understa nding of the user X  X  information need which was derived only from the query and other data availa-ble prior to the user submitting the query. The real-time inferred intent model is then exploited immediately after each user action in order to re-rank the result set. Surf Canyon presents users with newly identified results by re-ranking the result set and then displaying content that has not yet been viewed. This is done in tw o ways: 1) as indented recommen-dations following a search result click, and 2) as an additional page of results following a request for more results. Only unseen content is displayed as previous research has shown that results that change as users interact with the search page can interfere with the ability to find information because results no longer appear where ex-pected [25]. After viewing a result, should the user X  X  information need remains unsatisfied, for whatever reason, and the user clicks  X  X ack X  to re-turn to the search page, this is an opportunity to display re-ranked results. When users return to a result page while using Surf Can-yon, newly identified results are displayed beneath the selected document. These results are drawn from a large set of  X  X nseen X  results that have been fetched from subsequent pa ges of results returned by the underlying search engine. Because changes to a search page that is actively being used can be disorienting [24], new results are indicated by indenting. Each time a user clicks a search result the real-time inferred intent model is updated, the entire result set is re-ranked and the most relevant previously unseen results are brought forward to the cur-rent search results page. This ha ppens even when an indented re-ranked result is selected, in which case the recommendations are nested, which goes to a maximum of three levels deep. The Surf Canyon click-based reco mmendations are intended to add a new element to existing search interfaces while minimizing the disturbance to a user X  X  normal work flow. In practice users are typi-cally able use the re-ranking feature without instruction or guid-ance. In this paper we focus on how people interact with dynami-cally re-ranking search results, bu t the notion of adding dynamical-ly identified new content based on implicit behavior is quite gen-eral and may easily be extended to other types of information, such as relevant entities and contextual advertisement. With Surf Canyon, users are also able to interact with new, contex-tually relevant content when na vigating to subsequent pages of results. Upon clicking  X  X ore Results X , the real-time inferred intent model is once again updated. Rather than show results 11 to 20 as initially computed at the beginning of the session, the system dis-plays the ten most relevant result s as computed by the model. Be-cause this content has not been previously viewed by the user, there is no need to visually indicate that this content is new by indenting. We explore how users interact w ith dynamically ranked results by studying the Surf Canyon logs. To compare the traditional static search experience with users X  experiences with dynamically-ranked content, we directed a portion of Surf Canyon X  X  traffic to a traditional static sear ch experience. Although this method allowed us to compare the behavior of users with and without the dynamic not easy to control for tasks. For this reason we conducted an addi-tional user study with controlled tasks with self-reported success. We studied user behaviors in a set of controlled traffic experiments in which different user groups were exposed to different configura-major search engines collected from the Surf Canyon browser extension. The extension captures users X  interactions with the search result pages, including querie s, clicks on results and various other pieces of information. The data sets also contained session IDs, defined using a period of in activity for 30 minutes as a session boundary [28]. Anonymous user IDs were used to group queries into information sessions. Since many of the clicks were on dy-namically ranked results or re-ra nked results on pages beyond the first, we utilized metadata associated with e ach click to distinguish them from regular result clicks. Since the results on subsequent pages were all re-ranked, all clicks on the second page and beyond were considered clicks on dynamically ranked results. To quantify the differences in search behaviors caused by the two different ways dynamic results are displayed to the searcher, we ran two experiments for a large fraction of user traffic where we turned each off separately for 24 hours. To study the impact of dynamically ranked results, we turned off dynamic ranking for 20% of users. The other 80% of user s received the standard dynamic ranking experience. To study the impact of dynamically ranked subsequent-page content, we turned off re-ranking for 50% of the users. The net result was 387,347 queries available to study the impact of click-based recommendations and 1,560,996 queries to study the impact of dynamically re-ranked next-page content. Although traffic in the controlled online experiment was randomly split, there could be some task-based variation. Previous research has shown that search behavior can vary significantly by task [2]. For this reason, we further filte red to only look at overlapping queries by first aggregating the data from each group by query and then taking the intersection of the traffic based on the query string. This yielded 11,655 unique queries in the case of the dynamically ranked results. As there were a limited number of overlapping queries in the next-page case, we did not do this additional analysis for this experiment. Navigational queries (tar geted at navigating to a specific website) are common but search behavior surrounding these queries is known to be particularly different from other types of search be-havior [2]. Conversely, people are more inclined to click many results following a query when their information need is explorato-ry in nature [31]. We hypothesi zed that dynamically ranked con-tent is particularly likely to be useful for exploratory, open-ended searches and thus separate the data by task type in our analysis. To study the log data by task, we built a classifier to identify naviga-tional queries based on methodologies in previous work [15]. To further control for task, we supplemented the log data with smaller-scale data where we provided users with tasks and asked for explicit feedback of success. To do this, we built upon previous work using an information search game for modeling success to evaluate different variants of interaction models. We used the UFindIt framework [1] to collect search behavior data from paid Amazon Mechanical Turk users. As in the original UFindIt game, we used Apache web server proxies to log all pages visited by users during the game. The searchers were given task descriptions as well as initial que ries. While we pre-populated the search box with the initial query, searchers were allowed to change to whatev-er query terms they thought reasonable. The study was a 2x2 design where half of the participants interact-ed with dynamically ranked result s while the other half did not. In each case, half of the participants were given fact finding questions and asked to find a single answer, while the other half were given search topics that were intended to be more exploratory and were asked to find five different, rele vant URLs. The fact finding ques-tions were drawn from the 18 orig inal UFindIt questions, and in-cluded, for example, "What were the deadliest tornadoes in histo-ry?" The exploratory search topics were drawn from Web track of TREC 2010 [8]. We randomly selected 10 topics and used their subtopics as intent specific task descriptions while providing topic names as initial queries. In our analysis we used two definitions of search success: 1) self-reported success (i.e., whether any answer URL was submitted by the player), and 2) the correctness of the submitted URLs. Answer correctness was determined by examining the submitted URLs and checking if they contained correct answers for the task question. For this definition of success we obtained labels for 260 URLs submitted in factoid tasks as well 939 URLs submitted in explora-tory tasks. The labeling process was crowdsourced through CrowdFlower, where each submitted URL was checked by inde-pendent assessors. To quantify agreement among the assessors we calculated Fleiss X  kappa. For exploratory question the kappa value was 0.6, which indicated reasona ble agreement among the judges. These two definitions of success a llowed us to evaluate the quanti-ty and quality of the interaction. Users who completed less than two tasks were dropped to ensure tr ustworthiness. The final dataset included the results of 826 tasks (417 fact finding, 409 exploratory) by 91 users. In this section we compare people X  X  interactive experiences with dynamic ranking versus the traditi onal search experience based on controlled log analysis and user study. We analyzed the controlled traffi c data to understand how search behavior differed when people we re presented with dynamically ranked results or re-ranked content on subsequent result pages. Specifically, we looked at the av erage amount of time people spent on a query and the average number of clicks people made on the search result content, broken down by whether they clicked on the original results returned prior to dynamic ranking, the dynamically recommended results or results on subsequent result pages. A summary of these behaviors are shown in Tables 1 through 3. Sta-tistically significant differences (based on t-tests) are marked with asterisks (* for p &lt; .05, ** for p &lt; .01, and *** for p &lt; .001). Table 1 shows query-level results. We observe that users presented with dynamically ranked results spent more time searching and clicked on more results in total than the static results group. Users spent over 5 seconds more interacting with the result page when the results contained dynamic co ntent (244.36 seconds) compared to when they did not (239.05 s econds). They also clicked more results (1.21 clicks compared with 1.19 clicks). Although people were less likely to click on results that were part of the original result set when presented with dynamic content, they more than made up for this by clicking on the dynamically ranked results. We hypothesized that dynamic content would be particularly use-ful for tasks that are more exploratory in nature. The bottom two rows of Table 1 show the behavior observed for non-navigational queries, where people tend to sear ch longer and click more. In particular, we observed that they are more likely to interact with dynamically ranked results. (We will explore these differences in greater depth in Section 6 when we look at how the use of dynamic results is correlated with characteristics of the queries, sessions and users.) The differences between th e non-navigational behavior with and without dynamically ranked results echo what was seen for general query traffic, except that they are uniformly larger in mag-nitude and percentage. Table 2 shows the results for th e controlled study where subse-quent page re-ranking was turned off for the control group. Behav-ior on subsequent pages is different when that content was generat-ed dynamically during the course of the search. The percentage of clicks on subsequent page cont ent is significantly higher when those results were re-ranked based on within-query user behavior than when they were not. Since users were exposed to the same interface, the difference in the percent of clicks may be attributed to a difference in the quality or content of the results. Note that the average number of results clicks is much higher in Table 1 than Table 2. We also looked at session-level be havior for both experimental conditions, with summary statistics presented in Table 3. Session-level behavior is generally simila r to query-level behavior. As was the case for the query-level statics, we observed that users who received click-based recommendations spent longer (by 10 seconds) searching than other users. They also clicked on si gnificantly more results in total, with some of those clicks going to dynamic results at the expense of clicks on the originally presented results. Notably, people who received cl ick-based recommendations issued fewer queries (2.86 vs. 2.91) during a session than people who did not. This may be because the dynamic nature of the page surfaced results that mitigated the need to re-query. While at a query level we only saw an interaction between the presence or absence of next-page re-ranking with people X  X  next-page behavior in Table 2, havior as a function of the experimental condition. People with dynamic subsequent page content not only clicked on more next-page results, but also clicked on more results overall. The controlled log data suggests th at the inclusion of dynamically ranked content increases people X  X  engagement with the results and the amount of time they spend searching, while decreasing the number of queries they need to issue. While previous research suggests that in creased engagement tends to indicate a positive change for users [3], we wanted to better understand the impact of these changes on people X  X  ability to find what they are looking for. We also wanted to further explore the particularly large changes observed for non-navigational queries. For this reason, we look at the results of the controlled user study. As with the controlled log study, in the user study people were directed to a version of the search engine that either included dy-namically ranked results or did not. In this study, however, users were performing directed tasks and asked to provide information about what they found so that their success could be measured as described in Section 4.2. Results are summarized in Table 4, where we report task-level averages for completion time, success rate, number of clicks on original results, number of clicks on dynamically ranked results, number of clicks on subsequent page results and number of queries. Note that we have two measures of success which focuses on the quantity (self-reported success) and quality (correctness of the URLs) of the interaction. We begin our analysis of the resu lts by looking at how successful people were at completing the two di fferent types of search tasks: factoid and exploratory. Participan ts generally reported that they were very successful: 87% to 99% of all tasks were believed to be successfully completed. For fact oid tasks there was no difference in reported success, but for exploratory tasks the users with dy-namic recommendations felt signific antly more successful (94% vs. 99%; p &lt; 0.01 using a Proportions test). In actuality, however, people we re much less successful than they believed, with questions being answered correctly only 38% to 41% of the time. For factoid tasks, participants in the dynamic ranking group completed 41.4% of the tasks correctly, while participants in the static ranking group had a very similar 41.0%. Likewise, for exploratory tasks the mean succe ss rates were 38.7% and 38.9% respectively. In both cases, any observed differences were not statistically significant ( p &gt; .5 using a proportions test). The results indicate that dynamic ranking helps increase the quan-tity of the results although not necessarily the quality. This is un-derstandable in that click-based recommendations allow users to explore more results than static search interface without query reformulation. It is also inte resting to note th at although people thought they were more successful for exploratory tasks than fac-toid tasks, they were less likely to answer correctly. Task completion time is another important metric that has been used in prior work [33] to provide an intuitive criterion for as-sessing utility of a search interfac e. Figure 2(a) shows task comple-tion times broken down by the avai lability of dynamically ranked results for factoid tasks. On average, players in the dynamic rank-ing group spent about the same ti me searching for answers (107.7 seconds versus 109.3 seconds) as th e static ranking group. Differ-ences between the means of the dynamic and static rankings are not statistically significant with a Student t-test ( p = .94). Similarly, Figure 2(b) shows task completion times for exploratory tasks. In contrast to what we ob served with factoid tasks, comple-tion time varied significantly between the dynamic and static rank-ing groups for exploratory tasks. Participants in the dynamic rank-ing group were able to complete tasks in 123.7 seconds on average whereas participants with stat ic ranking had to spend 173.3 sec-onds, or 40% more time. The difference was significant ( p &lt; .01). Another striking difference between factoid and exploratory tasks can be observed in the number of dynamically ranked results clicked for factoid (0.05) and exploratory (0.44) questions. This is consistent with the differences we observed in the logs for naviga-tional and non-navigational queries (Table 1 and 5), again confirm-ing our intuition that an interactive experience is best suited for more complex tasks. Looking at the impact of the dynamic ranking on the click and query count, we found that for factoid questions the average num-ber of clicks for the dynamic ranking and static ranking groups was 1.79 and 2.48 respectively. For e xploratory tasks it was 4.26 and 6.12 respectively. Differences in the number of clicks were not significant ( p &gt;.2, Wilcoxon-Mann-Whitney test). The number of queries remained relati vely similar for groups and task types, alt-hough the dynamic ranking group issued slightly fewer queries. When comparing traditional search behavior with people X  X  interac-tions with dynamically ranked c ontent in the logs, we observed that people engaged more with search results when they were giv-en dynamically ranked results as they searched, clicking more and searching longer. In contrast, wh en we controlled the tasks that 
Queries per Task
Figure 2: Task completion ti me box plots for factoid tasks users completed, we found that their searches were faster and they clicked less frequently. Given the levels of success people achieved with dynamic content, it may be that for fixed tasks the dynamically ranked results reduce the effort required to complete the task, but for real world tasks that can evolve and grow beyond the initial target, the new content encourages additional interaction and exploration. The fact that we observed the opposite trend in ta sk duration between controlled traffic analysis and user study stresses the importance of control-ling the search task in IR research. We have compared dynamically ranking search results with tradi-tional static web search. By studyi ng user behaviors in a set of controlled traffic experiments, we found that dynamic content leads to higher user engagement in the search process. To ensure complete control of search task, we also conducted a user study with 91 participants and two types of search tasks, where the re-sults showed that the click-ba sed recommendation feature of dy-namic ranking improves the user performance of exploratory search task significantly measured in task completion time and the number of the self-reported URL answers. While our study was based on the interaction model empl oyed by Surf Canyon, we be-lieve that many of our findings have implications in enabling rich user interactions for web search in general. Future work includes the extension of dynamic ranking by removing some of its re-strictions. [1] Mikhail Ageev, Qi Guo, Dmitry Lagun, and Eugene Agichtein, "Find it if you can: a game for modeling different types of web search success using interaction data," in SIGIR , New York, 
NY, USA, 2011, pp. 345-354. [2] Azin Ashkan, Charles L. Clarke, Eugene Agichtein, and Qi Guo, "Classifying and Characterizing Query Intent," in ECIR , 
Berlin, Heidelberg, 2009, pp. 578-586. [3] Anne Aula, Paivi Majaranta, and Kari-Jouko Raiha, "Eye-tracking reveals the personal styles for search result evaluation," in Proceedings of the 2005 IFIP TC13 , Berlin, 
Heidelberg, 2005, pp. 1058-1061. [4] Paul N. Bennett, Filip Radlinski, Ryen W. White, and Emine 
Yilmaz, "Inferring and using location metadata to personalize web search," in SIGIR , 2011, pp. 135-144. [6] Roi Blanco et al., "Repeatable and reliable search system evaluation using crowdsourcing," in SIGIR , New York, NY, 
USA, 2011, pp. 923-932. [7] Christina Brandt, Thorsten Joachims, Yisong Yue, and Jacob Bank, "Dynamic ranked retrieval," in WSDM , New York, NY, 
USA, 2011, pp. 247-256. [8] Nick Craswell, Dennis Fetterly, and Marc Najork, "Microsoft 
Research at TREC 2010 Web Track," 2010. [9] Mariam Daoud, Lynda Tamine-Lechani, and Mohand 
Boughanem, "Towards a graph-based user profile modeling for a session-based personalized search," Knowl. Inf. Syst. , vol. 21, no. 3, pp. 365-398, 2009. [10] Bernard J. Jansen, Amanda Spink, and Tefko Saracevic, "The Use of Relevance Feedback on the Web: Implications for Web 
IR System Design," in WebNet , 1999, pp. 550-555. [11] Craig A. Kaplan, Justine Fenwick, and James Chen, "Adaptive Hypertext Navigation Based On User Goals and Context," User 
Modeling and User-adapted Interaction , vol. 3, pp. 193-220, 1993. [12] Jin Young Kim, Kevyn Collin s-Thompson, Paul N. Bennett, and Susan T. Dumais, "Characterizing web content, user interests, and search behavior by reading level and topic," pp. 213-222, 2012. [13] Jurgen Koenemann and Nicholas J. Belkin, "A Case For Interaction: A Study Of Inte ractive Information Retrieval 
Behavior And Effectiveness," , 1996, pp. 205-212. [14] Ron Kohavi, Roger Longbotham, Dan Sommerfield, and 
Randal M. Henne, "Controlled ex periments on the web: survey and practical guide," Data Min. Knowl. Discov. , vol. 18, no. 1, pp. 140-181, Feb 2009. [15] Uichin Lee, Zhenyu Liu, and Junghoo Cho, "Automatic identification of user goals in Web search," in WWW , New 
York, NY, USA, 2005, pp. 391-400. [17] M. Wertheim, and D. Hardtke M. Cramer, "Demonstration of improved search result relevancy using real-time implicit relevance feedback," in SIGIR Workshop , 2009. [18] Nicolaas Matthijs and Filip Radlinski, "Personalizing web search using long term browsing history," in Web Search and 
Data Mining , 2011, pp. 25-34. [20] Barry Schwartz. (2012) Surviving Personalization With Bing &amp; 
Google. [Online]. http://www.seroundtable.com/smx12-personal-bing-google-15250.html [21] Xuehua Shen, Bin Tan, and ChengXiang Zhai, "Context-sensitive information retrieval using implicit feedback," in 
SIGIR , New York, NY, USA, 2005, pp. 43-50. [24] Jaime Teevan, "How people recall, recognize, and reuse search results," ACM Transactions on Information Systems , vol. 26, pp. 1-27, 2008. [25] Jaime Teevan, Eytan Adar, Rosie Jones, and Michael A. S. 
Potts, "Information re-retrieval: repeat queries in Yahoo's logs," in SIGIR , New York, NY, USA, 2007, pp. 151-158. [26] Jaime Teevan, Susan T. Dumais, and Eric Horvitz, "Personalizing search via automated analysis of interests and activities," in SIGIR 2005, pp. 449-456. [27] Jaime Teevan, Susan T. Dumais, and Daniel J. Liebling, "To personalize or not to personalize: modeling queries with variation in user intent," in SIGIR 2008, pp. 163-170. [28] Ryen W. White, Paul N. Bennett, and Susan T. Dumais, "Predicting short-term interest s using activity-based search context," in CIKM , 2010, pp. 1009-1018. [29] Ryen W. White, Susan T. Dumais, and Jaime Teevan, "Characterizing the influence of domain expertise on web search behavior," in WSDM , 2009, pp. 132-141. [30] Ryen W. White and Dan Morris, "Investigating the querying and browsing behavior of advanced search engine users," in 
Research and Development in Information Retrieval , 2007, pp. 255-262. [31] Ryen W. White and Resa A. Roth, Exploratory Search: Beyond the Query-Response Paradigm .: Morgan &amp; Claypool, 2009. [32] Ryen W. White, Ian Ruthven, and Joemon M. Jose, "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes," in SIGIR , New York, NY, USA, 2002, pp. 57-64. [33] Ya Xu and David Mease, "Evaluating web search using task completion time," in SIGIR , New York, NY, USA, 2009, pp. 676-677. 
