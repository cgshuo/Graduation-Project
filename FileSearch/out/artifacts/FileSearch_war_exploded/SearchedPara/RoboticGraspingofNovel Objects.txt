 In this paper , we address the problem of grasping novel objects that a robot is percei ving for the rst time through vision.
 Modern-day robots can be carefully hand-programmed or  X scripted X  to carry out man y comple x ma-nipulation tasks, ranging from using tools to assemble comple x machinery , to balancing a spinning top on the edge of a sw ord. [15] Ho we ver, autonomously grasping a pre viously unkno wn object still remains a challenging problem. If the object was pre viously kno wn, or if we are able to obtain a full 3-d model of it, then various approaches, for example ones based on friction cones, [5] form-and force-closure, [1] pre-stored primiti ves, [7] or other methods can be applied. Ho we ver, in practical scenarios it is often very dif cult to obtain a full and accurate 3-d reconstruction of an object seen for the rst time through vision. This is particularly true if we have only a single camera; for stereo systems, 3-d reconstruction is dif cult for objects without texture, and even when stereopsis works well, it would typically reconstruct only the visible portions of the object. Finally , even if more specialized sensors such as laser scanners (or acti ve stereo) are used to estimate the object' s shape, we would still have only a 3-d reconstruction of the front face of the object.
 In contrast to these approaches, we propose a learning algorithm that neither requires, nor tries to build, a 3-d model of the object. Instead it predicts, directly as a function of the images, a point at which to grasp the object. Informally , the algorithm tak es two or more pictures of the object, and then tries to identify a point within each 2-d image that corresponds to a good point at which to grasp the object. (For example, if trying to grasp a cof fee mug, it might try to identify the mid-point of the handle.) Given these 2-d points in each image, we use triangulation to obtain a 3-d position at which to actually attempt the grasp. Thus, rather than trying to triangulate every single point within each image in order to estimate depths X as in dense stereo X we only attempt to triangulate one (or at most a small number of) points corresponding to the 3-d point where we will grasp the object. This allo ws us to grasp an object without ever needing to obtain its full 3-d shape, and applies even to textureless, translucent or reecti ve objects on which standard stereo 3-d reconstruction fares poorly . To the best of our kno wledge, our work represents the rst algorithm capable of grasping novel objects (ones where a 3-d model is not available), including ones from novel object classes, that we are percei ving for the rst time using vision.
 In prior work, a few others have also applied learning to robotic grasping. [1] For example, Jebara et al. [8] used a supervised learning algorithm to learn grasps, for settings where a full 3-d model of the object is kno wn. Hsiao and Lozano-Perez [4] also apply learning to grasping, but again assuming a fully kno wn 3-d model of the object. Piater' s algorithm [9] learned to position single ngers given a top-do wn vie w of an object, but considered only very simple objects (specically , square, triangle and round  X blocks X ). Platt et al. [10 ] learned to sequence together manipulation gaits, but again assumed a specic, kno wn, object. There is also extensi ve literature on recognition of kno wn object classes (such as cups, mugs, etc.) [14 ], but this seems unlik ely to apply directly to grasping objects from novel object classes.
 To pick up an object, we need to identify the grasping point X more formally , a position for the robot' s end-ef fector . This paper focuses on the task of grasp identication, and thus we will consider only objects that can be pick ed up without performing comple x manipulation. 1 We will attempt to grasp a number of common ofce and household objects such as toothbrushes, pens, books, mugs, martini glasses, jugs, keys, duct tape, and mark ers. (See Fig. 1.) The remainder of this paper is structured as follo ws. In Section 2, we describe our learning approach, as well as our probabilistic model for inferring the grasping point. In Section 3, we describe the motion planning/trajectory planning (on our 5 degree of freedom arm) for mo ving the manipulator to the grasping point. In Section 4, we report the results of extensi ve experiments performed to evaluate our algorithm, and Section 5 concludes. Because even very dif ferent objects can have similar sub-parts, there are certain visual features that indicate good grasps, and that remain consistent across man y dif ferent objects. For example, jugs, cups, and cof fee mugs all have handles; and pens, white-board mark ers, toothbrushes, scre w-dri vers, etc. are all long objects that can be grasped roughly at their mid-point. We propose a learning approach that uses visual features to predict good grasping points across a lar ge range of objects. Given two (or more) images of an object tak en from dif ferent camera positions, we will predict the 3-d position of a grasping point. An image is a projection of the three-dimensional world into an image plane, and does not have depth information. In our approach, we will predict the 2-d location of the grasp in each image; more formally , we will try to identify the projection of a good grasping point onto the image plane. If each of these points can be perfectly identied in each image, we can then easily  X triangulate X  from these images to obtain the 3-d grasping point. (See Fig. 4a.) In are multiple grasping points, then the correspondence problem X i.e., deciding which grasping point in one image corresponds to which point in another image X must also be solv ed). On our robotic platform, this problem is further exacerbated by uncertainty in the position of the camera when the Figure 3: Synthetic images of the objects used for training. The classes of objects used for training were martini glasses, mugs, teacups, pencils, whiteboard erasers, and books. images were tak en. To address all of these issues, we develop a probabilistic model over possible grasping points, and apply it to infer a good position at which to grasp an object. 2 2.1 Featur es In our approach, we begin by dividing the image into small rectangular patches, and for each patch predict if it is a projection of a grasping point onto the image plane. For this prediction problem, we chose features that represent three types of local cues: edges, textures, and color . [11 , 13] We compute features representing edges by con volving the intensity channel 3 with 6 oriented edge lters (Fig. 2). Texture information is mostly contained within the image intensity channel, so we apply 9 La ws masks to this channel to compute the texture ener gy. For the color channels, low frequenc y information is most useful to identify grasps; our color features are computed by applying a local averaging lter (the rst La ws mask) to the 2 color channels. We then compute the sum-squared ener gy of each of these lter outputs. This gives us an initial feature vector of dimension 17. To predict if a patch contains a grasping point, local image features centered on the patch are insuf -cient, and one has to use more global properties of the object. We attempt to capture this information by using image features extracted at multiple spatial scales (3 in our experiments) for the patch. Ob-capture these variations. In detail, we compute the 17 features described abo ve from that patch as well as the 24 neighboring patches (in a 5x5 windo w centered around the patch of interest). This gives us a feature vector x of dimension 1 17 3 + 24 17 = 459 . 2.2 Synthetic Data for Training We apply supervised learning to learn to identify patches that contain grasping points. To do so, we require a labeled training set, i.e., a set of images of objects labeled with the 2-d location of the grasping point in each image. Collecting real-w orld data of this sort is cumbersome, and manual labeling is prone to errors. Thus, we instead chose to generate, and learn from, synthetic data that is automatically labeled with the correct grasps.
 In detail, we generate synthetic images along with correct grasp (Fig. 3) using a computer graph-ics ray tracer , 4 as this produces more realistic images than other simpler rendering methods. 5 The adv antages of using synthetic images are multi-fold. First, once a synthetic model of an object has been created, a lar ge number of training examples can be automatically generated by rendering the object under dif ferent (randomly chosen) lighting conditions, camera positions and orientations, etc. In addition, to increase the diversity of the training data generated, we randomized dif ferent proper -ties of the objects such as color , scale, and text (e.g., on the face of a book). The time-consuming part of synthetic data generation is the creation of the mesh models of the objects. Ho we ver, there are man y objects for which models are available on the internet, and can be used with only minor modications. We generated 2500 examples from synthetic data, comprising objects from six object classes (see Figure 3). Using synthetic data also allo ws us to generate perfect labels for the training set with the exact location of a good grasp for each object. In contrast, collecting and manually labeling a comparably sized set of real images would have been extremely time-consuming. 2.3 Pr obabilistic Model On our manipulation platform, we have a camera mounted on the robotic arm. (See Fig. 6) When ask ed to grasp an object, we command the arm to mo ve the camera to two or more positions, so as to acquire images of the object from these positions. Ho we ver, there are inaccuracies in the physical positioning of the arm, and hence some slight uncertainty in the position of the camera when the images are acquired. We now describe how we model these position errors. Formally , let C be the image that would have been tak en if the robot had mo ved exactly to the commanded position and orientation. Ho we ver, due to robot positioning error , instead an image ^ C is tak en from a slightly errors in camera position/pose should usually be small, 6 and we model the dif ference between ( u; v ) and (^ u; ^ v ) using an additi ve Gaussian model: ^ u = u + For each location ( u; v ) in an image C , we dene the class label to be z ( u; v ) = Here, P ( the probability of a 2-d position ( u + where x 2 R 459 are the features for the rectangular patch centered at ( u + ^ C (described in Section 2.1). The parameter of this model w 2 R 459 is learned using standard maximum lik elihood for logistic regression: w = arg max the synthetic training examples (image patches and labels), as described in Section 2.2. Fig. 5 sho ws the result of applying the learned logistic regression model to some real (non-synthetic) images. Given two or more images of a new object from dif ferent camera positions, we want to infer the 3-d position of the grasping point. (See Fig. 4.) Because logistic regression may have predicted multiple grasping points per image, there is usually ambiguity in the correspondence problem (i.e., which grasping point in one image corresponds to which graping point in another). To address this while also taking into account the uncertainty in camera position, we propose a probabilistic model over possible grasping points in 3-d space. In detail, we discretize the 3-d work-space of the robotic arm into a regular 3-d grid G R 3 , and associate with each grid element j a random variable y = 1 f grid cell j is a grasping point g .
 From each camera location i = 1 ; :::; N , one image is tak en. In image C passes. Let r Figure 4: (a) Diagram illustrating rays from two images C (sho wn in dark blue). (b) Actual plot in 3-d sho wing multiple rays from 4 images intersecting at the grasping point. All grid-cells with at least one ray passing nearby are colored using a light blue-green-dark blue colormap, where dark blue represents those grid-cells which have man y rays passing near them. (Best vie wed in color .) We kno w that if any of the grid-cells r is a grasp point. More formally , z For simplicity , we use a (ar guably unrealistic) nai ve Bayes-lik e assumption of independence, and model the relation between P ( z Assuming that any grid-cell along a ray is equally lik ely to be a grasping point, this therefore gives Ne xt, using another nai ve Bayes-lik e independence assumption, we estimate the probability of a particular grid-cell y where P ( y value in our experiments). Using Equations 2, 3, 5, and 7, we can now compute (up to a constant of proportionality that does not depend on the grid-cell) the probability of any grid-cell y valid grasping point, given the images. 2.4 MAP Infer ence We infer the best grasping point by choosing the 3-d position (grid-cell) that is most lik ely to be a valid grasping point. More formally , using Eq. 5 and 7, we will choose: where P ( z itly computes the sum abo ve for every single grid-cell would give good grasping performance, but be extremely inef cient (over 110 seconds). For real-time manipulation, we therefore used a more efcient search algorithm in which we explicitly consider only grid-cells y R ( u; v ) intersects. Further , the counting operation in Eq. 9 is implemented using an efcient count-ing algorithm that accumulates the sums over all grid-cells by iterating over all the images N and rays R Figure 5: Grasping point classication. The red points in each image sho w the most lik ely locations, predicted to be candidate grasping points by our logistic regression model. (Best vie wed in color .) Figure 6: The robotic arm picking up various objects: box, scre wdri ver, duct-tape, wine glass, a solder tool holder , powerhorn, cellphone, and martini glass and cereal bowl from dishw asher . Ha ving identied a grasping point, we have to mo ve the end-ef fector of the robotic arm to it, and effector to an approach position, 8 and then mo ves the end-ef fecter in a straight line forw ard towards the grasping point. Our robotic arm uses two classes of grasps: downwar d grasps and outwar d grasps. These arise as a direct consequence of the shape of the workspace of our 5 dof robotic arm (Fig. 6). A  X do wnw ard X  grasp is used for objects that are close to the base of the arm, which the arm will grasp by reaching in a downw ard direction. An  X outw ard X  grasp is for objects further away from the base, for which the arm is unable to reach in a downw ard direction. The class is determined based on the position of the object and grasping point. 4.1 Hard war e Setup Our experiments used a mobile robotic platform called ST AIR (ST anford AI Robot) on which are mounted a robotic arm, as well as other equipment such as our web-camera, microphones, etc. ST AIR was built as part of a project whose long-term goal is to create a robot that can navig ate home and ofce environments, pick up and interact with objects and tools, and intelligently con verse with and help people in these environments. Our algorithms for grasping novel objects represent a rst step towards achie ving some of these goals. The robotic arm we used is the Harmonic Arm made by Neuronics. This is a 4 kg, 5-dof arm equipped with a parallel plate gripper , and has a positioning accurac y of 1 mm. Our vision system used a low-quality webcam mounted near the end-ef fector . Table 1: Average absolute error in locating the grasp point for dif ferent objects, as well as grasp success rate for picking up the dif ferent objects using our robotic arm. (Although training was done on synthetic images, testing was done on the real robotic arm and real objects.) 4.2 Results and Discussion We rst evaluated the predicti ve accurac y of the algorithm on synthetic images (not contained in the training set). (See Fig. 5.) The average accurac y for classifying whether a 2-d image patch is a projection of a grasping point was 94.2% (evaluated on a balanced test set), although the accurac y in predicting 3-d grasping points was higher because the probabilistic model for inferring a 3-d grasping point automatically aggre gates data from multiple images, and therefore  X x es X  some of the errors from indi vidual classiers.
 We then tested the algorithm on the physical robotic arm. Here, the task was to use input from a web-camera, mounted on the robot, to pick up an object placed in front of the robot. Recall that the parameters of the vision algorithm were trained from synthetic images of a small set of six object classes, namely books, martini glasses, white-board erasers, cof fee mugs, tea cups and pencils. We performed experiments on cof fee mugs, wine glasses (partially lled with water), pencils, books, and erasers X b ut all of dif ferent dimensions and appearance than the ones in the training set X  as well as a lar ge set of objects from novel object classes, such as rolls of duct tape, mark ers, a translucent box, jugs, knife-cutters, cellphones, pens, keys, scre wdri vers, staplers, toothbrushes, a thick coil of wire, a strangely shaped power horn, etc. (See Fig. 1.) We note that man y of these objects are translucent, textureless, and/or reecti ve, making 3-d reconstruction dif cult for standard stereo systems. (Indeed, a carefully-calibrated Point Gray stereo system, the Bumblebee BB-COL-20,  X with higher quality cameras than our web-camera X f ails to accurately reconstruct the visible portions of 9 out of 12 objects.) In extensi ve experiments, the algorithm for predicting grasps in images appeared to generalize very well. Despite being tested on images of real (rather than synthetic) objects, including man y very dif ferent from ones in the training set, it was usually able to identify correct grasp points. We note that test set error (in terms of average absolute error in the predicted position of the grasp point) on the real images was only some what higher than the error on synthetic images; this sho ws that the algorithm trained on synthetic images transfers well to real images. (Ov er all 5 object types used in the synthetic data, average absolute error was 0.81cm in the synthetic images; and over all the 13 real test objects, average error was 1.83cm.) For comparison, neonate humans can grasp simple objects with an average accurac y of 1.5cm. [2] Table 1 sho ws the errors in the predicted grasping points on the test set. The table presents results separately for objects which were similar to those we trained on (e.g., cof fee mugs) and those which were very dissimilar to the training objects (e.g., duct tape). In addition to reporting errors in grasp positions, we also report the grasp success rate, i.e., the fraction of times the robotic arm was able to physically pick up the object (out of 4 trials). On average, the robot pick ed up the novel objects 87.5% of the time.
 For simple objects such as cellphones, wine glasses, keys, toothbrushes, etc., the algorithm per -formed perfectly (100% grasp success rate). Ho we ver, grasping objects such as mugs or jugs (by the handle) allo ws only a narro w trajectory of approach X where one  X nger X  is inserted into the handle X so that even a small error in the grasping point identication causes the arm to hit and mo ve the object, resulting in a failed grasp attempt. Although it may be possible to impro ve the algorithm' s accurac y, we belie ve that these problems can best be solv ed by using a more adv anced robotic arm that is capable of haptic (touch) feedback.
 In man y instances, the algorithm was able to pick up completely novel objects (a strangely shaped is a dif cult problem for standard vision (e.g., stereopsis) algorithms because of reections, etc. Ho we ver, as sho wn in Table 1, our algorithm successfully pick ed it up 100% of the time. The same rate of success holds even if the glass is 2/3 lled with water . Videos sho wing the robot grasping the objects, are available at We also applied our learning algorithm to the task of unloading items from a dishw asher . 9 Fig. 5 demonstrates that the algorithm correctly identies the grasp on multiple objects even in the presence of clutter and occlusion. Fig. 6 sho ws our robot unloading some items from a dishw asher . We proposed an algorithm to enable a robot to grasp an object that it has never seen before. Our learning algorithm neither tries to build, nor requires, a 3-d model of the object. Instead it predicts, directly as a function of the images, a point at which to grasp the object. In our experiments, the algorithm generalizes very well to novel objects and environments, and our robot successfully grasped a wide variety of objects, such as wine glasses, duct tape, mark ers, a translucent box, jugs, shaped power horn, and others, none of which were seen in the training set.
 Ackno wledgment
