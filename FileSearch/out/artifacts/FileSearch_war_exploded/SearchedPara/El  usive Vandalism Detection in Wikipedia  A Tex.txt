 The open collaborative nature of wikis encourages participa-tion of all users, but at the same time exposes their content to vandalism. The current vandalism-detection techniques, while effective against relatively obvious vandalism edits, prove to be inadequate in detecting increasingly prevalent sophisticated (or elusive) vandal edits. We identify a num-ber of vandal edits that can take hours, even days, to correct and propose a text stability-based approach for detecting them. Our approach is focused on the likelihood of a certain part of an article being modified by a regular edit. In addi-tion to text-stability, our machine learning-based technique also takes into account edit patterns. We evaluate the per-formance of our approach on a corpus comprising of 15000 manually labeled edits from the Wikipedia Vandalism PAN corpus. The experimental results show that text-stability is able to improve the performance of the selected machine-learning algorithms significantly.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval -Clustering; I.2.6 [ Artificial Intelligence ]: Learn-ing -Parameter learning Algorithm, Experimentation Classification, Vandalism detection, Wiki
Wikipedia has revolutionized content creation and doc-umentation by encouraging users to actively participate in collaborative editing without requiring them to authenticate or identify themselves. This openness, while being an essen-tial feature of wikis, also exposes them to vandalism that compromises the integrity of articles by introducing false information or modifying good information. Several stud-ies have shown that vandalism is quite prevalent on public wikis, consisting of around 5% of the edits at Wikipedia [8, 9]. While around half of all the vandal edits were rectified within minutes, some vandal edits persisted for hours, even days. They expose false information to millions of Wikipedia visitors. For example, the article Martin Luther King, Jr. Day (timestamped on 20:29, 22 May 2006) was damaged by racist edits that were not reverted for nearly four hours.
Many vandalism detection methods rely upon simple text features (such as introduction of abusive or obscene words) or simple edit patterns (such as deleting massive amounts of content from an article). These methods work reasonably well for regular vandal edits (adding common swear words, performing massive changes, etc.). However, they are be-coming ineffective against increasingly sophisticated vandal edits, which we term as elusive vandal edits . As pointed by Chin et.al. [5], these types of vandal edits are hard to identify. Our study reveals that their numbers have been increasing since August, 2004 and has remained at about 10% of all vandal edits overall.

We introduce text-stability as a measure to quantify the perceived stability of a text-block that essentially evaluates the likelihood of a certain text-block of an article being mod-ified. This strategy is motivated by the following key obser-vation: the content of a wiki article is subjected to modifi-cation (addition, deletion, and update) at various points in time. A piece of text that has remained intact for a signifi-cantly long period of time and through many versions repre-sents stable and presumably accurate information pertinent to the article, with its likelihood of being modified through a regular edit being fairly low. For example, the birth date of a person is unlikely to get changed if it has not been changed for a long time. In addition, we incorporate novel edit-type characterizations to a set of standard Wikipedia revision features. These enhanced feature set is used as the basis for various classifiers. Through extensive experimenta-tion, we demonstrate the effectiveness of our classifiers over the Wikipedia Vandalism PAN Corpus [1].
Our approach combines both the stability of text and the characteristics of an edit in relationship to the current ver-sion of the article. Below, we discuss the factors representing text stability and factors that characterize the edit patterns. These features are used to drive machine learning-based clas-sifiers for detecting vandal edits. regular version.
 Feature Article StatInv TopicRep NumRep SmallDiff Rewritten
Text stability considers how well the content of an arti-cle has withstood the acceptance of users, which effectively measures the likelihood of being modified for the content of an article. We borrow the measure of number of versions in the literature of quality assessment for wiki articles [3 , 6, 11]. This measure counts the number of versions a certain piece of text has survived, referred as stab-version . Two other measures exist: amount of time and number of views . However, the former gives a high survival length to content of articles which are seldom browsed or revised, simply due to their long existence. The latter can become heavily bi-ased towards popular articles that attract a large amount of browsing traffic.
In addition to text-stability, the characteristics of an ed it can provide important clues to vandalism detection. Our selection criterion of a characteristic is based on whether it presents distinctive text-stability distribution betwee n reg-ular and vandal edits. Below we describe five of them. Statement Inverse (StatInv): This type of edit inverse the meaning of a sentence. To identify these instances, we check whether the content of a new edit contains the words  X  X ot X ,  X  X one X , or it adds the prefixes of  X  X n- X ,  X  X is- X  to exis t-ing words. A concrete example is shown in the first row of Table 1 in which the prefix of  X  X ntrue X  is removed. Topic Replacement (TopicRep): This type of edit re-places the link of one Wikipedia topic with another Wikipedi a topic. A Wikipedia topic is the title of an article. Our ex-periments show that the majority of hyperlinks in an article between Wikipedia topics are mainly created at the earlier stage of an article. After its content gets stabilized, new edits are less likely changing these hyperlinks. The second row in Table 1 gives such an example in which the hyperlink to the article  X  X arth X  is changed to  X  X ars X .
 Number Replacement (NumRep): Replacing or chang-ing a number is one form of editing that is prone to vandal-ism and is very hard to detect. We use regular expression to identify numbers in a text and check whether both the deleted text and the inserted text involve different numbers . An example is shown in the third row of Table 1. The square feet of the Florida Mall in the city of Orlando, Florida was changed to a wrong number.
 Small Difference (SmallDiff): This type of edit makes minor changes to existing content usually due to fixes of spelling or grammar errors. To locate corresponding in-stances, we first compute the longest common sequence be-tween the deleted text and the inserted text. Then we com-pare whether their distance is no more than two characters. To see an example, the word  X  X otion X  was changed to  X  X o-tion X  in the fourth row in Table 1. Rewritten (Rewritten): Paraphrasing a sentence without changing its actually meaning. Since a fully automatic ap-proach is unlikely, we took a heuristic approach. There are four steps involved: 1) stem every word in the sentence by using the Porter stemmer; 2) check whether it contains words that inverse the meaning of the current sentence; 3) remove all the common words such as  X  X s X ,  X  X he X  from each word list; 4) compare the number of common words between these two word lists. If they share more than 80% of com-mon words, we tag this instance as rewritten. In the last row of Table 1, we see that the vandal modified the punctuation of a sentence from  X . X  to  X ; X .
 Each of the above features shows distinctive text-stabilit y distribution between regular and vandal edits. Table 2 show s the statistics of text stability in each feature category me a-sured by the stab-version approach. Let us use the fea-ture NumReplace as an example. With the stab-version ap-proach, the stability of text changed by regular edits has a median of 18 versions. By comparison, the stability of text changed by vandal edits has a median of 138 versions. This is due to the fact that inaccurate information is normally corrected at the earlier stage of the article. The difference s in other features are also large in terms of mean, standard deviation, and median.
Wikipedia Vandalism PAN corpus is a large-scale corpus released for the evaluation task of vandalism detection at Wikipedia. The training collection comprises about 15 , 000 edits (on 14 , 000 articles) being annotated by 3 human an-notators crowd-sourced using Amazon Mechanical Turk. As our focus is on the elusive edits using text-stability, we fir st remove articles whose content does not go through the reg-ular revision process or articles that are very small or not popular. They include all talk articles, user articles and the articles containing only a few sentences of text or have not been visited for more than a year. Furthermore, since the text stability-based approach is proposed to detect elusiv e vandal edits, we exclude those revisions that can be easily characterized by simple text features. Specially, we remov e those revisions with large change ratios to exclude massive inserts or deletes. Finally, we exclude those revisions tha t contain obscene words or illegal words. We use the obscene word list from ClueBot. For illegal words identification, we use the GNU Aspell as the spelling checker. Table 2: Statistics of text stability for each feature between regular and vandalism edits Feature StatInv 160 226 84 666 825 361 TopicRep 84 149 26 345 479 171 NumRep 95 217 18 283 398 138 SmallDiff 104 220 26 276 383 145 Rewritten 126 252 39 252 334 138
We evaluate the effectiveness of classification on the PAN corpus with two objectives in mind. First, to analyze the effectiveness of the text-stability feature and the edit cat -egorization feature. Second, to evaluate the impact of the text-stability feature and compare the performance betwee n the time-based approach and the version-based approach. We choose not to limit our claims to a single classifier and run two standard supervised-learning classifiers over each dataset. The standard implementations of these classifiers are used from Weka, a machine learning tool, and we run them using 10-fold cross-validation. We use F1-measure to evaluate and compare the results of the classifiers. The F1-measure evenly weights the precision and the recall. Each classifier is run over the PAN dataset with the features de-tailed in Section 2.2.
 Figure 1 shows the F1-measure of the two classifiers. The F1-measures in both classifiers get improved. It shows an improvement of 12.1% in C4.5 Decision Tree stab-version and 63.2% in AdaBoost stab-version. The current results are significant considering that all the vandal edits in the PAN dataset were eluded by the currently deployed vandalism-detection bots on Wikipedia.

The reason that the F1-measure cannot be improved above 90% is due to low values in the precision measure. We sys-tematically studied the misclassified vandal edits and iden -tify two contributing factors. The first factor is that the cl ass of an edit is not always obvious, even under careful human review. For example, in the PAN data set, an anonymous user incorrectly modified the format of a heading in the arti-cle Florence (timestamped on 20:23, 5 December 2009). We consider it as a vandal edit since this heading has existed for more than three-hundred versions. However, all users labeled it as regular. Maybe their decisions considered the fact that the same anonymous user removed his incorrect edit an hour later. This example shows that the class of an edit can depend on the future edits. Since an anti-vandalism algorithm can not foresee the future, this type of edits are likely to be reported as vandal edits, resulting in an increa se in the rate of false positive. The second factor is that most stable Wikipedia articles continue to receive regular edit s such as refining and updating data with latest resources. In these cases, it is hard to differentiate between regular and elusive vandal edits. Therefore, better techniques may be developed based on other meta-information such as the cat-egory of users and their editing records.
 Figure 1: F1-measure of classification using the PAN data set
Typical anti-vandalism methods are either machine learnin g-based or rule-based. In these methods, the common fea-tures or patterns that characterize vandalism normally in-clude size of changes, statistics in new edits, obscenity of words, and anonymity of users [2, 7]. Some work also con-siders features from statistical language models [5]. Even though the early work considers a broad range of vandalism behavior, they are inadequate to detect elusive vandal edit s.
There are also approaches that are not machine learning-based or rule-based. West et.al. [10] uses of the meta-infor mation of versions and evaluate the credibility of new edits based on reputation of users, measured by their editing frequen-cies, location of users, and registration profiles. We expec t this approach to help us reduce the false positive ratios. Based on the PAN data set, half of the regular small edits are contributed by registered users. Most of these register ed users have well-established profiles with a median of 131 ver -sions edited per user. Belani [4] considers the occurrence frequency of words before and after modifications. This approach assumes that the versions created due to small changes are unlikely to contain vandal edits. Therefore, it is ineffective at detecting elusive vandal edits.

Some work related to vandalism detection come from the research that assesses the quality and trustworthiness of Wikipedia articles [3, 8, 11]. The idea is to build a rep-utation system for users based on the persistence of their edits. A user acquires high credibility if his/her contribu -tions are persistent over time. The problem of vandalism detection is essentially different from the problem of repu-tation system because a low-quality edit may or may not be a vandal edit. Furthermore, half of regular edits in small edits are contributed by anonymous users. In these cases, it is hard to build a profile for these users.
We propose a new approach to detect elusive vandal ed-its that cannot be characterized by simple text features and simple editing patterns of users. In the approach, we in-troduce text stability, which evaluates the likelihood of a certain part of an article being modified through a regu-lar edit. In addition, we consider the relationship between text-stability and the characteristics of an edit. We utili ze these factors as features, which are used to drive machine learning-based supervised learning classifiers. We evalua te the performance of our approach using the Wikipedia Van-dalism PAN corpus. The experimental results show that the stability-based approach improves the performance of selected machine learning algorithms significantly. We dis -cuss the contributing factors to high false positive rate. W e also point out the inherent limitation of automatic detec-tion schemes and show concrete examples for the necessity of human involvement in the detection process.

As a next step, we plan to have a more comprehensive cat-egorization for various editing patterns as currently ther e are 17% of uncategorized versions based on the proposed edit-ing patterns. We will study their statistics and relationsh ips to text stability, as well as include them in our machine-learning algorithms. We are also interested in using the meta-information of users (e.g., number of contributed ed-its) to improve the accuracy of our approach.
 ACKNOWLEDGMENTS This research has been par-tially funded by National Science Foundation by IUCRC, CyberTrust, CISE/CRI, and NetSE programs, National In-stitutes of Health grant U54 RR 024380-01, PHS Grant (UL1 RR025008, KL2 RR025009 or TL1 RR025010) from the Clinical and Translational Science Award program, Nationa l Center for Research Resources, and gifts, grants, or con-tracts from Wipro Technologies, Fujitsu Labs, Amazon Web Services in Education program, and Georgia Tech Founda-tion through the John P. Imlay, Jr. Chair endowment. Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foun-dation or other funding agencies and companies mentioned above. [1] Pan 2010 lab evaluation corpus. [2] Cluebot. [3] B. T. Adler and L. de Alfaro. A content-driven [4] A. Belani. Vandalism detection in wikipedia: a [5] S.-C. Chin, W. N. Street, P. Srinivasan, and [6] A. Halfaker, A. Kittur, R. Kraut, and J. Riedl. A jury [7] M. Potthast, B. Stein, and R. Gerling. Automatic [8] R. Priedhorsky, J. Chen, S. T. K. Lam, K. Panciera, [9] F. B. Vi  X egas, M. Wattenberg, and K. Dave. Studying [10] A. G. West, S. Kannan, and I. Lee. Detecting [11] T. W  X  ohner and R. Peters. Assessing the quality of
