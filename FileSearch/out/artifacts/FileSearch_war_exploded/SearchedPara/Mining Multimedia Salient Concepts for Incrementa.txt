 We propose a novel algorithm for extracting information by mining the feature space clusters and then assigning salient concepts to them. Bayesian techniques for extracting concepts from multimedia usually suffer either from lack of data or from too complex concepts to be represented by a single statistical model. An incremental information extraction approach, working at different levels of abstraction, would be able to handle concepts of varying complexities. We present the results of our research on the initial part of an incremental approach, the extraction of the most salient concepts from multimedia information. I.2.10 [ Vision and Scene Understanding ]: Video analysis; H.3.1 [ Content Analysis and Indexing ]: Abstracting methods; H.3.3 [ Information Search and Retrieval ]: Clustering. Algorithms, Measurement, Experimentation. Multimedia information extraction, multimedia clustering. The first multimedia information extraction algorithms for retrieval applications have used Bayesian techniques for extracting concepts of different complexities (e.g. sky vs. pencil ) [1]. Most of the problems in multimedia information extraction reside in the degree of ambiguity of most concepts versus the available training data. Some concepts have an audio-visual representation too complex to be captured by a single model: the confusion between concepts is reflected on the high degree of uncertainty of the extracted information, [2]. Some approaches formulate the information extraction problem as a cross-lingual retrieval problem and proposed a multimedia equivalent solution, the cross-media relevance model [3]. One might suppose that for some concepts the learning task is just too difficult to be accomplished. For example, concepts such as sun , outdoor or indoor , may be easy to detect, but concepts such as bird , plane or superman , may be more reliably detected if other, more basic/salient, concepts were detected previously. In an (filter methods) use a feature dimension similarity measure to decide which dimensions are merged or removed (e.g. PCA [4], ICA [4], cross-entropy based algorithm proposed by Koller et al. [5]). The second type (wrapper methods) involves embedding the feature selection with the clustering algorithm. In the algorithm X  X  second step, the feature sp ace clustering is done under the assumption that features are independent. That is, each feature is processed individually. We use the finite-mixture of Gaussians clustering proposed in [6]. The finite-mixture learning algorithm follows the EM algorithm with embedded model selection using a Minimum Message Length criterion. This algorithm is based on EM and avoids some of its drawbacks: sensitivity to initialization, possible convergence to the boundary of the parameter space, and its the estimation of different feature importance. The algorithm is also capable of selecting the mixture X  X  number of components. After learning the finite-mixture model we consider each Gaussian component to be a salient pattern (a cluster), {P 1 , ... , P L }. So far, we have ignored the labels of the training examples: the algorithm worked completely unsupervised and with the entire training set. Now, we use the labels to learn a Bayesian network, [7]: the relations between patterns {P 1 , ..., P L } and concepts {C 1 , ..., C K }, and the parameters of the concepts X  nodes. We used the K2 algorithm to search the Directed Acyclic Graph space [7]. The DAG space is composed by DAGs which only have edges between pattern nodes and concepts nodes are considered (we ignore the DAGs with concepts relations). The concept nodes {C 1 ... C K } are of mixture-of-Gaussians type, and their parameters are learned after the network structure learning. The algorithm was tested with the subset of COREL Stock imahes that was used by Duygulu et al. in [8] and evaluated with average precision and mean average precision. The low-level features we used are: Tamura, Gabor, and marginal HSV color, see [9]. To reduce the feature space dimensions we used PCA, ICA and the cross-entropy based algorithm [5]. Only concepts with more than 100 training examples were considered (36 total). Figure 2 presents the average precision of the tested concepts and the 5 best/worst concepts X  average precisions. Table 1 presents the mean average precisions for the test and train set. The reported measures refer to the results using PCA feature selection. 
