 1. Introduction
Most retrieval engines suffer from the term-mismatch problem because of insufficient information of the ( Kurland &amp; Lee, 2004, 2005; Liu, 2004 ).
 calculating the document similarities, which are necessary for clustering, is a critical issue.
The adaptive document clustering scheme has been investigated to effectively perform clustering of docu-of the adaptive document clustering to information retrieval.

This paper provides a conceptual interpretation of the adaptive document clustering by regarding that it verification of the updating algorithm by comparing several similarity measures. based similarity measure should be examined. Developing approximation scheme is the next problem. In fact, optimization method. As shown in Appendix B , Yu X  X  approach is one of the methods to approximate the spe-cial similarity measure (or distance measure).

A query is tightly coupled thematic unit rather than a simple term, since more complex terms such as noun is successful.

At this point, this paper derives three query-based similarities measures based on the language modeling based on the traditional clustering algorithm.

From now on, we use the term of online learning of document similarities to clarify that similarities are learned. Clustering is a by-product of these learned similarities.

The rest of the paper is organized as follows: Section 2 describes our online learning schemes and query-based similarities in the language modeling framework. Sections 3 and 4 describe conduced experiments on
TREC data. Section 5 describes related works. Finally, Section 6 concludes and addresses some directions for future work. 2. Online learning framework of document similarities val is performed to create the top retrieved documents R .In Update , similarities between two documents D and D 0 in R are increased by f ( D , D 0 , Q ). f ( D , D redundancy, data structure such as hash, B+ tree are useful. For a dynamic environment in which new doc-will also change once the collection changes. However, this paper does not consider such dynamic environment.
 Algorithm 1. Online learning framework of document similarities Input: N documents in collection C end if The main advantages of online document clustering will be summarized in the following items: process to reflect the change. It only needs to wait until the similarities are relearned.
User adaptability: This learning scheme provides useful bias. Some documents may not be an interest to some users. Normally, documents have high utilities when the users are much interesting in them. For a hot topic, many users will provide similar queries. Then, the learnings are concentrated on related docu-has more knowledge of such documents. Less interesting documents will be slowly learned. However, it is It is important to note that the framework is able to control the user adaptability.
In fact, Algorithm 1 contains a method of calculating traditional term-based document similarity. Consider two document vectors.
D by the set of documents indexed with the term, and f ( D i terms, similarities of Eq. (1) are calculated on all pairs of documents.

Now, the query-based similarity is defined by taking query as a concept: w query as the point of continuous vector space and refined similarity. For convenience, document vector and query vector are restricted to 1 of p -norm: example, assume that the vector space, which consists of two terms, h and h and i th document vector and w i q is the inner-product between query vector q and document vector d situations, only the finite queries are processed. The discrete version such as tice where D q indicates a very small value.

The critical problem is to define the query weight on each document, which is different from the retrieval section, we define KL divergence-oriented similarity and derive inner-product-oriented similarity and a heuristic measure with their comparison. 2.1. Query-based KL divergence
Language modeling approach uses the negative KL divergence between the document models for measur-ing their similarity based on individual terms as follows ( Lafferty &amp; Zhai, 2001 ): naive extension is derived by regarding a sample unit as a concept instead of the term: where P ( Q j h D ) is the generation probability of query Q = q the multiplication of probabilities of each term: bility defined on query space since the summation of P ( Q j h one.
 query models. We denote P ( h Q j h D ) by the probability of query model q for given document model h sponding to P ( Q j h D ): ability, a new similarity measure is described as follows: larger than the number of terms in a document (the length of a document). In this case, Z the same among the documents, exists. In other words: Theorem 1. When V is much larger than the number of document terms, Z documents.
 Proof. Detailed derivation is described in Appendix A . h
The similarity of Eq. (5) has several advantages as follows: ization factors among the documents, Eq. (4) is proportional to geometric (weighted) mean of query gen-eration probabilities, equivalent to the negative KL divergence. document frequencies (DF). Thus, a well-defined query-based similarity should reflect the query impor-tance. Fortunately, Eq. (4) implicitly reflects such importance using query clarity, by multiplying
Q The only difference is the entropy of the query model.
 query Q : where ^ h D is the MLE document model. Using score ( D ; Q ), Eq. (5) is rewritten by ignoring Z Despite of its theoretical advantages, there is a critical problem. Consider the probability P ( h h , when h Q is h D . Since the query model is exactly the same as the given document model, the probability could be the maximum value. However, P ( h D j h D ) is not a maximum. The maximum is obtained using h by setting P ( t j h Q ) to 1, where t indicates a term that maximizes P ( t j h divergence does not reflect an the important and basic weighting characteristic well. 2.2. Query-based inner-product
Another query-based similarity can be derived from the vector space model, by using the negative KL divergence between h Q and h D as a weight: KL( h Q i h D
KL divergence for query document weight w iQ since it is not a positive value. Thus, a mapping function e w iQ is defined as follows: based inner-product on language models.
 reflect the importance of query well. To demonstrate this, KL( h 2004 )): respectively, Eq. (9) becomes sidered well in the inner-product similarity scheme.
 2.3. Query-based weighted inner-product inner-product, the query importance is reflected by introducing the new inner-product as follows: query-based weighted inner-product .

Unfortunately, the inner-product based measures are not consistent with the language modeling frame-work since they are not defined by the KL divergence or the likelihood-based scheme. Despite of it, in easily by simply redefining the inner-product. In comparison, the KL-divergence oriented measure is not trivial to refine, where more sophisticated works should be investigated to reflect such useful features (characteristics).

Table 1 summarizes three similarity measures based on language models in three aspects. Here, f ( D , D indicates the quantity of similarity to be updated between documents. Query importance signifies how query importance is reflected, and Query weighting signifies how document weight of query ( w measures, the query weights of all methods are equivalent to the ranking of documents in the language mod-eling approach. 2.4. Retrieval based on learned similarity
The cluster-based retrieval is one of the methods for improving retrieval performance by using a learned ginal document score with cluster score as follows: where c D indicates a cluster to which document D belongs and c is used as an interpolation. proportional to the P ( Q j c D ). where b is the smoothing parameter of cluster-language model. Since a cluster contains larger terms than a normal document, it is reasonable to differentiate them by using b .

Since the document similarity is partially learned, some documents do not have any nearest neighbors. In such documents, score ( c D ; Q ) is exactly the same as score ( D ; Q ) since c and cluster model. In our terminology, it is rewritten by c degree of interpolation. 3. Experimental setting 3.1. Experimental setting 3.1.1. Test collections posed query-based similarities are extended into more larger test collections or not. ciated Press, San Jose Mercury News and Wall Street Journal. Since there are some researches performed on number of documents and topics, respectively. All queries and documents are stemmed using the Porter stem-mer after stopping the use at about 400 stopwords.

For the baseline language modeling approach, Jelinek-Mercer smoothing is used where k is 0.25. For clus-mance, mean average precision (MAP) is used. 3.1.2. Pseudo-query generation ing steps: 1. Randomly select a document: Document is assumed to be uniformly distributed. 2. Select a term according to the document language model, and add into the query: To contain many topical terms in a query, we use parsimonious language model instead of MLE model ( Hiemstra, Robertson, &amp;
Zaragoza, 2004 ), where only the document terms are selected when their summation is less than P . P is set to 0.4. examples of randomly generated queries in AP test collections: bigram or trigram in step 2. 3.1.3. Evaluation measure for learned similarity
To evaluate the learned similarity, we constructed top 100 nearest neighbor documents using term-based similarity for each document. Negative KL divergence between documents in language modeling approach &amp; Ribeiro-Neto, 1999 ). Regarding each document as a query, top 100 similar documents (neighbors) from Then, NS and AvgNS are defined as 4. Experimental results 4.1. Performance of online learned document similarities
To evaluate three presented query-based similarity measures, we additionally evaluate three proposed query-based similarities. Three proposed measures are annotated with: KL, Inner (Inner-product) and
Weighted-Inner (Weighted inner-product). Fig. 1 shows AvgNS of online learned similarities using three set by 20.
 based methods show the best, while KL is the worst in terms of AvgNS. From these two collections, inner-term-based similarity. The hypothesis will be discussed in details in the next section. 4.2. Effects of cluster-based retrieval using online learned document similarities measures improve the baseline. In particular, performance improvement in MED collection is high, showing about more than 4% MAP increase. As we have expected, the curve of retrieval performance does not follow the tendency to AvgNS. Although AvgNS is gradually increased according to the number of processed queries ences at learning points between 10,000 queries and 100,000. Even the rank of AvgNS over query-based similarities do not remain as that of retrieval performance. As a straightforward example, among four query-based similarities, KL performs the worst in MED, although AvgNS in KL performs the best in that collection.

As in AvgNS, the inner-product measures have a high reliability showing the best performance in both test measure is somewhat related to whether the measure is similar to the term-based similarity or not. 4.3. Experiments in large test collections size of test collection, where TREC test collections is much larger than MED and CISI collections in size. pared: KL, Inner (Inner-product) and Weighted-Inner (Weighted inner-product). It is evident that all three methods improve the performance of the baseline, especially in AP and WSJ. When T is more than 100,000, their performances have already improved over baselines. However, the performances are not con-decreased after T becomes more than 200,000. In particular, the performances are worse than the baseline when T is 1,000,000 on two inner-product based methods (Inner and Weighted Inner). Surely, it does not mean that KL-based method is relatively reliable. KL-based method shows a slightly unreliable performance in WSJ compared to other two methods. It seems that after the number of queries becomes 200,000, there are many noisy queries or queries that are unrelated to the evaluation topic in SJM.
The KL-based method tends to draw a different performance curve, compared to the inner-product meth-ment similarities in a current setting. 4.3.1. Comparison with clustering methods
To compare our adaptive cluster-based retrieval with other methods, we compare our method to two meth-ods based on K -means clustering and document expansion. We perform the K -means clustering according to ument expansion.

Table 3 summarizes our results. K -means( X ) indicate the performance when X is the number of clusters, and DocExp indicates the performance of document expansion. formance based on our three adaptive methods. As shown in Table 3 , adaptive clustering framework shows formance than other traditional methods. In K -means method, the best performances are achieved when the number of clusters is 500 for test collections. 4.3.2. Combining different clustering methods
We combine two different methods to see if the performance can be further improved. For K -means, the number of clusters are set at 500. Table 4 shows the best performance when two different methods are com-bined by changing the interpolation parameter from 0.1 to 0.9. To test the Inner method, we only use the inner-product method, instead of the weighted inner-product method. A bold face indicates the best perfor-
AP test collection shows some improvements when K -means and document expansion are combined. How-ever, the improvement is relatively minor.

From our experiments, we conclude that online learning framework based on query-based similarities pro-domly generated where many noisy terms are included. However, current query-based similarity measure shows unstable dynamics on the number of processed queries. It remains as an open problem for us. 5. Related works
Considering its computational complexity, the traditional offline calculation of similarities spends many computation cost for fetching posing file of indexing terms. In this regard, our online learning framework may have advantages where overhead is only updating time, since the retrieval time is necessary for online ument similarities.

Yu X  X  method is understood as an approximation scheme ( Yu et al., 1985 ). The method performs two dif-ferent movements of positions: Moving relevant documents and Moving non-relevant documents . The docu-ments are judged by the user and partitioned into relevant and non-relevant subsets. The documents that ones judged as non-relevant documents are moved away from the relevant documents. In Appendix B , Yu X  X  method on similarity-based criterion is derived and generalized.

In a traditional viewpoint, our learning framework of document similarities belongs to long-term learning 1971 ), which alters vectors of relevant documents in order to obtain better representation as follows: and 1.
 inner-document similarities to enhance the performance.
 The learned document similarities are one of corpus structures which help the retrieval performance. hidden parameters, all reflect such structure implicitly or explicitly.

However, it is different from ours since it is not an online scheme. 6. Conclusion
In this paper, we provide a conceptual viewpoint of the adaptive document clustering using the query-based tive document clustering by the approximation scheme for obtaining document similarities. In addition, we derive three new similarities which are originated from the language modeling framework, and evaluate them on randomly generated queries, by comparing to other clustering methods. From experiments, we found that discover an approximation scheme, which effectively creates clusters. Such approximation method may be is to consider the framework on only inserting environments such as the retrieval of Electronic Journals. unstable.
 Acknowledgement
This work was supported by the KOSEF through the Advanced Information Technology Research Center (AITrc) and by the BK21 project.
 Appendix A. Approximation of normalization factor According to our definition of query-based language model, the normalization factor Z model h D is applied with smoothing by Jelinek-Mercer smoothing with collection model h Then,
For simplification, we denote a i as ( a P 1):
Of total V terms, we select such terms where P ( t i j h D the number V j D j , is assigned to index after j D j where a
Again, we write a query model h Q by V -dimensional vector x =  X  x is ( V 1)-simplex) To derive further, we use the following formula: which is the volume of K -simplex with edges of length of A . To substitute it into Z and V j D j , respectively. Then, the Eq. (A.6) becomes The following lemma is the core of our approximation.
 Lemma 1. If K is sufficiently large so that B ln a K, then the following approximation is reasonable: Proof. Let f ( n )be
Since f (0) is ( a B 1)/ ln a , f ( n )is
Since polynomial expansion of e x is a B is written by e B ln a which is expanded with polynomial. By substituting polynomial expansion of e
Eq. (A.9) , f ( n ) becomes ratio with a very small error. By dropping all terms after second term, f ( n )is K corresponds to V j D j which is very large. Thus, we can agree that this approximation is acceptable. Lemma 1 is consequently applied to further derive Eq. (A.5) . After applying j D j 1 times, Z approximated with Finally, Lemma 1 is applied once more, Z D is
It is  X  X pproximately X  document-independent, which is the content of Theorem 1 ! Appendix B. Derivation of Yu X  X  method the dissimilarity function g ( D i , D j , Q ). For given Q , g ( D between i th and j th documents. Let z i be position of i th document assigned on a real line ( 1 , 1 ). First, the following criterion explains moving relevant documents: approximated dissimilarities. For the given query Q, g ij and z j such that minimizes J , partial derivative of J on z where sign ( x )is1if x is positive, 0 otherwise. By going g where g is a learning rate (0 &lt; g &lt; 1). This is scheme of moving relevant documents.
Although Yu used an one-dimensional point to represent a position, a high-dimensional vector will provide neighbor documents in real time.
 References
