 With the rapidly increasing amounts of data stored in electronic formats, the concerns about privacy of personal information have emerged globally. For ex-ample, bank databases with transactional information about every aspect of business which are now measured in gigabytes and terabytes, contain much sen-sitive information such as address, a ccount balance, credit card number etc. Although data mining is a useful tool for such large databases and can discover valuable, non-obvious information, in the absence of adequate safeguards it can also jeopardize information privacy [16].

These privacy concerns have triggered r egulations and law s protecting pri-vacy in data collection and publishing. In particular, health-related data are very sensitive and most countries have established corresponding rules. For example, according to the United States Health Insurance Portability and Accountabil-ity Act (HIPAA), health records have to be de-identified before they can be released to some third party. For exam ple, ZIP codes have to be generalized to their first three digits or replaced by 000 for units with 20,000 or fewer persons [8]. The HIPPA requirements correspond t o different range constraints for the individual attributes such as  X  X he first 3 digits X  minimum range constraint(range threshold) on ZIP codes. Alternatively, a single global range constraint can be set by choosing the maximum of the minimum range thresholds over all dimen-sions. The HIPPA requirements also imply a significance constraint, for example,  X 20,000 X  is the minimum number of de-identified patients in a published group.
The topic of privacy-preserving data mining, has been initialized by [3], where a randomized approach [1, 2, 14] is proposed by adding random noise to all data records to protect the privacy and reconstr ucting the original data distribution to generate new dataset for data mining tasks such as classification. Another approach is by data generalization which hides individual record via generalized values. A typical model in this approach is k -anonymity [4, 20, 21] which gener-alizes the attribute values of the records such that for any record, there are at least k other records in the dataset from which it cannot be distinguished. The randomized approach meets the minimum range constraint required in privacy policies, however attributes are assumed to be independently distributed and thus the reconstructed data distribution may not accurately reflect the correlations among multiple attributes. The k -anonymity model, on the other hand, specifies a significance constraint, but no range constraint. Thus, the original data can be estimated very accurately from the anonymized data in the case of k similar or even identical records.
Figure 1 depicts a small sample database where the k -anonymity method will output the three dashed cir-cles with radius r .The k -anonymity method condenses the k most similar (or even the same) objects into an anonymized group, so that the attacker can estimate the private attribute values with confidence interval of r which can become arbitrarily small. In addition, if the attacker has prior-knowledge of some original record, other information can be estimated step by step. For example if he knows one attribute x of a point is 20, from the published information of k -anonymity, he can infer another attribute y of this point with 100% con-fidence within a range of r . On the other hand, a model with significance and range constraints outputs one big solid circle, where the radius R is equal to r . This model on average has a better protection for the privacy. Even if the attacker knows that the x value of a point is 20, he can only guess the other attribute y of this point with 100% confidence within a range of R ( &gt;&gt; r ).
In this paper, we propose a novel privacy-preserving transformation scheme which meets the constraints of privacy policies and is robust to different kinds of privacy attacks. Our framework, which well extends the k -anonymity model, is based on so-called privacy-preserving MicroClusters ( PPMicroClusters )that satisfy a privacy constraint (minimum radius) as well as a significance constraint (minimum number of corresponding data records). Within the constraints given by these requirements, we try to minimize the overall cost of the PPMicroClus-ters resulting in maximum accuracy of subsequent analysis. These sphere-like MicroClusters are made public for data mining purposes, but the original data records are kept private.

The contributions of this paper are as follows: (1) We propose the novel concept of PPMicroClusters that satisfy a privacy con-straint (minimum radius) as well as a significance constraint (minimum number of corresponding data records). (2) We develop an efficient local-search-based method for generating and main-taining PPMicroClusters in a dynamic database with updates. (3) Our experimental evaluation on synth etic and real data s ets demonstrates that the proposed methods achieve accurate clustering results while preserving the privacy.
 The rest of the paper is organized as fo llows. Section 2 surveys related work. Section 3 introduces the new model. S ection 4 and Section 5 present the algo-rithms. We give experimental results in Section 6 and conclude the paper in Section 7. The research on privacy preserving data mining was initialized by [3] and the current approaches to enforce privacy-protection in data mining applications can be categorized as follows:  X 
Randomized Approaches: These methods deliberately introduce noise in the data that hides data of the individual records. However, patterns summarizing the trends in the dataset are the information most relevant to data analysts that does not really need to acces s individual records. Novel r econstruction techniques to accurately reconstruct the distribution of original data values are presented in [3] and [1]. A perturbation mechanism was proposed in [2], where the model parameters are themselves characterized as random variables, and demonstrate that this feature provides improvements in privacy at a very marginal cost in accuracy. However, this approach only assumes data distributes independently in each attributes.  X 
Cryptographic Techniques: [18] introduces the problem of distributed data mining between more parties such that confidential information of any party is not disclosed. The method uses the results in [24] which forms the basis for applying cryptographic techniques to privacy-preserving in data mining. The challenge of this approach is the cost of computation/communication in cryp-tion/decryption for large databases.  X 
Data Generalization: The major idea in this category is to transform the values of certain attributes that are marked for having the potential to breach the privacy of a dataset and are called Quasi-Identifiers in [20]. The k -anonymity problem is discussed in [20] and [21], where a data transformation technique is described which makes use of concept hierarchies to bring about generalizations in the Quasi-Identifiers such that at least k or more tuples satisfy any given value combination for the Quasi-Identifiers . [17] presents a method based on a set of geometric data transformation primitives, to maintain the correlations. But the original attribute values can still be easily estimated with small vari-ance.  X 
Miscellaneous: [9] discusses a novel genetic algorithm based method to achieve loss due to hierarchy-based generalization. Schloer [22] develops a matrix based method for multidimensional data transformation. The authors in [3] use a mea-sure that defines privacy in terms of variance, while in [1] a privacy measure based on the differential entropy of the generated distribution is proposed. The work of [14] applies a slightly different measure that considers the privacy of the model with respect to the actual objects in the dataset. Some recent work [4, 5, 6, 12] extend k -anonymity in performance or the quality of preserved privacy. While none of them considers the minimum radius requirement of the anonymized data, which means the precise estimate of the original data can still be easily reached if the anonymized data are bound in very small dense space . The recent work on privacy-preserving clustering incudes distribution-based method where the data for a single entity are vertically split across multiple sites [23]; or the data are distributed horizontally [14], [10], among the sites. Another approach in privacy-preserving clustering is centralization-based method including [17] which proposes a target-oriented transformation method based on geometric transformations of digital images. In this section, we introduce a framewor k based on privacy-preserving Micro-Clusters ( PPMicroClusters ) that satisfy a privacy constraint (minimum radius) as well as a significance constraint (minimum number of corresponding data records). Given a set of d -dimension al records, X = { X 1 ... X k ... , X n } ,and each X i is a record containing d dimensions which are denoted by X i =( x 1 i ... x d i ). In order to perform a robust data transformation for distance-based data mining, we propose to use MicroClus ters to approximate the subsets of any dataset with an user-specified privacy threshold.
 Definition 1 (MicroCluster). The MicroCluster C for a d -dimensional dataset CF 1 and CF 2 each corresponds to the linear sum and the sum of the squares of the data values for each dimension respectively. The number of data points | C | is the MicroCluster is r =max n j =1 ( X j  X  CF 3( C )) 2 .
 Note any point p which is within the radius from the center of a MicroCluster, will be assigned to this MicroCluster so that p may belong to multiple MicroClusters. Definition 2 (Privacy Preserving MicroCluster). Given a radius r t (pri-vacy threshold) and a number of objects n t (statistical significance threshold). We call a MicroCluster C =( n, CF 1( C ) , CF 2( C ) , CF 3( C ) , r ) a Privacy-Preserving MicroCluster, (PPMicroCluster) if the constraints: (1) r  X  r t (2) n  X  n PPMicroClusters are  X  X mall clusters X  of po ints with sufficient statistics and pri-vacy protection that can well represent the overall original data distribution including the inter-attribute correlations. If users wish to improve the privacy, they can increase the threshold r t . Similarly, users can improve the significance by increasing the threshold n t . However, the radius threshold should not become too large, because otherwise the accuracy of a subsequent data mining method will deteriorate. This tradeoff motivates the following definition of a privacy-preserving MicroClu stering that best reflects the or iginal cluster structure. Definition 3 (Privacy Preserving MicroClustering). Given a d -dimen-sional database X , the radius threshold r t and the number of objects thresh-old n t , the task of Privacy-Preserving MicroClustering (PPMicroClustering) is to find a set of PPMicroClusters C 1 , ..., C m satisfying r t and n t such that the cost of The smaller the sum of the squared distances of points to the center, the more PPMicroClusters, which means the smaller the size of each PPMicroCluster, and the less overlap among them. This motivates the objective of minimizing the sum of squared distances.
 Theorem 1. PPMicroClustering is an NP-hard problem.
 Proof. Since the special case of PPMicro Cluster, where parameters n t , r t , m are set to 0, 0 and K respectively, is equivalent to the classic K -clustering problem [7], PPMicroClustering is an NP-hard problem. n t = k =6.Incase1(Figure2( a )), when both models generate exactly the same two far-apart PPMicroClusters of 6 points, both achieve the same degree of pri-vacy protection. In case 2 (Figure 2( b )) where points are distributed densely. The k -anonymity model will return 3 MicroClusters, while our model generates only one PPMicroCluster, leading to a much wider confidence interval when an adver-sary tries to estimate the private attributes. In case 3 (Figure 2( c )), k -anonymity assigns the points in the overlap area uniquely to the closest PPMicroCluster, i.e.,  X  X  X  is assigned to PPM 1 and  X  X  X  belongs to PPM 2 .Noticethatnowthe real radius of PPM 1 or PPM 2 is less than the minimum radius constraint (i.e., the distance between the center and the farthest point in each PPMicroCluster after assigning  X  X  X  and  X  X  X  becomes less than r t ). However, we assign  X  X  X  and  X  X  X  X oboth PPM 1 and PPM 2 , both PPMicroClusters will be  X  X afer X  in privacy since it would be harder for attacker to guess the individual record under the user-specified privacy standard. So in gen eral, our model provides better privacy protection than k -anonymity.

Note that the constraint n t in both our model is conceptually different from the constraint k in k -anonymity model. The value of k determines the privacy degree of the microcluster, while the value of n t determines the statistical sig-nificance (accuracy) of the microcluster . To reach the expected privacy degree obtained by our model, k -anonymity needs to increase the value of k for the data in dense area, but meantime it has more difficulty to identify desired mi-croclusters with good accu racy in sparse area. The generated PPMicroCluster can be used published for query processing and data mining applications with several forms: (1) Represent a PPMicroCluster by its center, radius and number of points contained. (2) Represent a PPMicroCluster by the CF-value obtained from aggregating all points located in a PPMicroCluster. (3) Randomly generate points in each PPMicroCluster. In this section, we introduce an efficient local search based algorithm to generate PPMicroclusters. The algorithm proceeds in the following three steps:
Initially, the algorithm picks an unmarked data point randomly in step 1, and generates a PPMicroCluster for the point which is explained in section 4.1. Then it picks next seed considering the new g enerated PPMicroCl uster. The details on how to pick next seed are elaborated in section 4.2.
 4.1 Forming PPMicroCluster The major step in this algorithm is the second step. We have the following observations to guide our algorithm design. Before going into details we define several notations to facilitate our explanation. Let denote point p  X  X  k  X  1nearest neighborhood as kNN p (here k = n t , p is included), denote the radius of kNN p as r kNN p . The mean vector of points in kNN p is denoted as  X  p . If p is one of k nearest neighbors of  X  p , we call this neighboring region kNN  X  p accordingly. Lemma 1. The sum of squared distance of kNN p is greater than or equal to the sum of squared distance of kNN  X  p Proof. For k points p 1 ,p 2 ,...,p k which are members of kNN p ,and p is the means (  X  p )ofsuch p points, i.e., p = 2 p ) ( p 1 + ... + p k )+ If there is a point p which is in kNN p but not in kNN  X  p , it shows that the distance between p and  X  p is greater than the radius of kNN  X  p .Thus,we conclude that the sum of squared distance of k points in kNN  X  p to  X  p is smaller than or at least equal to the sum of squared distance of k points in kNN p to p . In another word, kNN  X  p is more compact. Since our objective function is the sum of square distance between any poi nt and the center, the compacter the PPMicroClusters are, the smaller the overall sum of squared distance is. Hence we can base on this observation by replacing p by a  X  X etter X   X  p as a center to form a compact PPMicroCluster.
 can, so we still need consider every point q in kNN p as candidates and find corresponding potential PPMicroclusters for p in the following manner: find kNN q , denote the mean vector of points in kNN q as  X  q .Ifany kNN  X  q exists (i.e., k -nearest neighbors of q include p ), then determine potential PPMicro-Cluster PPM q from center  X  q by extending radius r kNN  X  q of kNN  X  q if r t is not satisfied. Furthermore, choosing a PPMicroCluster PPM of these potential PPMicroClusters with a minimal sum of squared distance, denote its center as mcenter , which is different from the mean vector of all the points in PPM since PPM often has more than k = n t objects to meet r t constraint. All the points including p in PPM are then marked. Due to the different cardinality of potential PPMicroclusters, we make the decision based on the average squared distance of each PPMicroCluster.
 In general, the strategy described above is trying to assign p to a better PPMicroCluster from possible PPMicroClusters which are not too far away from p and compact enough.
 Remark. The k nearest neighbor search function considered in th is section is slightly different from the general kNN search in that it needs not only consider the distance of the k -th neighbor but also retrieve the distance of the k -th neigh-bor when only unmarked points are issued. The details of obtaining guarantee of r t is presented in section 4.2.
 One minor issue left in step 2 is when the number of unmarked points is small, the r kNN p could be far less than the k nearest neighbor distance for only unmarked points (refereed to kNN p ). Thus such a criterion holds: when 2  X  r kNN p  X  r kNN p , p is assigned to the closest PPMic roCluster generated before. 4.2 Picking the Next Seed After forming a PPMicroCluster PPM p for point p , it is very important to choose a  X  X ood X  point as a  X  X eed X  to start another round of forming PPMicroClusters. As illustrated in Figure 4, we employ a range query on  X  p with radius r PPM p (bold line) plus the radius constraint r t (dashed line), then picking the point which is located within radius [ r PPM p ,r PPM p + r t ] and unmarked as the seed. Figure 4 also shows point Q is valid candidate for the next seed. When proceeding the seed searching, we consider points returned from the range query in the order of farthest point w.r.t the query center to the closest one. If no candidate is available, a randomly picked point will serve as the next seed.

The strategy we use in the seed selecting step follows the observation that the forming two touch PPMicroClusters is the most efficient way to cover all the corresponding points. 4.3 Handling Outliers In the proposed framework, outliers will lead to MicroClusters that either have a too large radius or do not meet the significance threshold, and may conse-quently have more overlaps with existing MicroClusters. Note that we may ap-ply different measures from the literatur e to define outliers, for example, the distance-based outlier definition [11, 19]. In this paper, we measure the outlier at the abstraction level of MicroClusters which is more appropriate in our con-text than a definition at the abstraction level of individual records. Definition 4 Outlying MicroCluster Given any MicroCluster C of r and n , a percentage threshold of p , 0  X  p  X  1 ,if n  X  p  X  n t , C is called an O utlying MicroCluster.
 Concerning the treatment of outliers when refining the MicroClusters, we pro-pose the following three methods:  X  Method A: No special treatment of out liers, i.e., according to the algorithm  X  Method B: Detect which MicroCluste rs are outliers and remove them. The  X  Method C: Detect which MicroClusters are outliers, increase their radius In each PPMicroCluster, we keep information as follows: 1. PPMicroCluster mcenter (which is denoted in 4.1) c ,mean  X  and standard 2. The number of points located in the radius of r t around c denoted as core 3. The number of points located outside of the core region.
 When a new data point p new comes, it is assigned to mcenter c of the closest PPMicroCluster. If p new is located in c  X  X  core region. The number of data points in the core region, mean value vector and standard deviation, are updated ac-cordingly. If a PPMicroCluster contains less than 2 k points, we do not split it. However, when the number of points in a PPMicroCluster exceeds 2 k , a split operation is not necessarily triggered, but depending on the tradeoff between keeping all the points in this PPMicroCluster and splitting it into two or more PPMicroClusters. Obviously, the benefit of splitting a MicroCluster aims to de-crease the sum of squared error. On the other hand, if the new split PPMicro-Clusters are highly overlapped, the points located inside the overlapping region contribute twice to the objective function , and the resulting qualities in accuracy are not desired. Thus, we introduce two criteria to evaluate the necessity of a split operation.  X  The loss-and-gain criterion: evaluate the cost difference in the unsplit PP-MicroCluster and the split PPMicroClusters in terms of the sum of squared error. If the gain wins over the loss, the split occurs; otherwise no split operation on this PPMicroCluster.  X  Effective radius criterion: evaluate the  X  X ffective radius X  r e ( r e  X  r ) of PP-MicroCluster, which refers to the radius covering a region which a high percent-age of points are included. If r e is small, which means most of points in this PP-MicroCluster locates dense ly around the center, so there is no necessary to split, just keep it as before. For the sake of quantifying the tradeoff in gain-and-loss cri-terion, we first estimate the possible overlap. To simplify the analysis, assuming points are uniformed distributed in the PPMicroCluster (note that this assump-tion is reasonable for estimating the contribution of points to the new split clus-ters, and other non-uniformed distributions have less trade-off to consider split). figure as well as the sum of squared error. Thus, we have the following equation of points in the overlapping region O and dist ( c 1 ,c 2) is the distance between where m is the total number of data points in a PPMicroCluster. Finally, the
Next we quantify the gain of moving the center of a MicroCluster to the mean of the data points located in the MicroCluster based on the following observation. Lemma 2. Given a set of points P = { p 1 ,...,p m } , and mean of them  X  .The differencebetweenthesumofsquareddistanceofpointsin P to a center c and to the mean  X  is m  X  dist ( c,  X  ) 2 .
 Proof. This observation can be proved easily. Consider the case showing in Fig-ure 6, the mcenter of the split PPMicroCluster is labeled c and the mean of the data points in region R is  X  . Based on the triangle inequality, the corresponding Now we compare the difference between of loss and gain, if gain wins over loss, then the split operation can be employed ; otherwise no split occurs in the current PPMicroCluster. Based on lemma 2, we conclude that the gain of splitting a In order to estimate the  X  X ffective X  radius r e ( r e  X  r ) of current PPMicro-Cluster, we first define r e to be the radius of a region which includes a high percentage (say 95%) of points in the PPMicroCluster. Afterwards, if the mean and the standard deviation of a random variable are labeled X with respect to  X  x and  X  x respectively, based on the Chebyshev X  X  inequality theorem [15], for any positive constant t , Pr ( | X  X   X  x | &lt;t )  X  1  X   X  x 2 t 2 .
Therefore, estimating the radius of a PPM icroCluster is equivalent to find the to r t . Otherwise, the estimated radius is t .

In general, these two criteria help to provide a better maintenance of PPMi-croClusters in a dynamic environment. We have conducted an experimental evaluation of our method using a synthetic dataset and a real dataset. The synthetic dataset consists of 100,000, five di-mensional records with 10 clusters generated from Gaussian distributions. The real dataset is a set of 50,000, seven dimensional health records. The number of  X  X atural X  clusters (10) in this dataset was determined by clustering with a series of different K -values and choosing the clustering, i.e., K -value, with the highest silhouette coefficient [13]. To ev aluate the quality of the PPMicroClus-ters, we apply K -means clustering algorithm on the microclusters generated by our method and Charu X  X  condensation method [4] respectively, and compare the results measured by (1) th e accuracy of the clusterin g based on the PPMicro-Clusters (2) the degree of the privacy ac hieved. Both measures are analyzed with respect to the two parameters of PPMicroClustering, minimum radius threshold r and minimum number of objects threshold n t .

We have implemented our methods in C++. All the experiments were con-ducted on an Intel 1GHZ processor with 512M RAM, 40G hard disk, running Windows XP.
 Clustering Accuracy: The quality of clusters is evaluated by comparing the results of our modified K -means method on PPMicroClusters with the results of the traditional K -means method on the original database. We evaluate both the external quality and the internal quality of the clustering results. (1) The external quality will be measured by the entropy wi th respect to the given  X  X rue X  cluster labels in the original dataset. For both the synthetic data and the real data, we take the results of K -means method on the original dataset as a substitute for the true class labels (their entropies are zero), and investigate the influence of changing the values of minimum radius r t and n t to the entropies. For the modified K -means clustering results of PPMicroClusters, the entropy of each cluster is measured based on the ratio of the number of objects with  X  X rue X  label in the current cluster to the number of all objects in this cluster. (2) The internal quality is measured by the sum-of-squares of the K clusters with respect to the closest centroids of clusters. That is, after K centroids are obtained by applying our modified K -means method over PPMicroClusters, the total sum of distances of all objects to the closest c luster centroids is derived and can be compared with the cost of baseline K -means method over the original dataset. We investigate the impact of changing values of r t to the clustering cost. The entropy evaluations with respect to the minimum radius r t are shown in Figures 7 and 8 where n t = 10. For both datasets, the entropies increase approxi-mately linearly with increasing r t values. PPMicroClusters can be obtained with three different ways of handling outliers (introduced in subsection 4.3 as method A, method B and method C). All three methods achieve relative good accuracy (entropies &lt; 0.4). The best one is PPMicroClustering method A (i.e., outliers are assigned to the nearest PPMicroCluster) which achieves overall the lowest entropy values. We argue that this is due to the fact that the objects in the same PPMicroCluster always belong to the same cluster, and also outliers are always identified in the same (nearest) cluster as the traditional K -means method does. The PPMicroClustering method B (with removing outliers) obtains the second lowest entropies, and the PPMicroClustering method C (with  X  X ake objects X ) yields clusters with larger entr opies than the above two methods.

We also performed experiments to eval uate the clustering accuracy with re-spect to increasing n t in Figures 9 and 10 where r t = 20. For both datasets, all three methods achieve relatively low entropies, and the entropies increase steadily with increasing n t values. The PPMicroClustering method A is slightly lower than the PPMicroClustering method B in entropy based on the similar reason as for the changing values of r t (the increase of r t leads to an increase of n t and vice versa). The PPMicroClustering method C again ranks third w.r.t. entropy. Since method A outperformed th e other outlier handling methods, we use method A in all the following experiments. We compare the clustering accu-racy on PPMicroClusters generated by PPMicroClustering with that by Charu X  X  Condensation method in both static and incremental ways (as shown in Figures 11 and 12). PPMicroClusters generated by PPMicroClustering static method are slightly less accurate than Condensation static method, meanwhile privacy con-straint r t are guaranteed. Since our increment al method evaluates split criteria of loss-and-gain and  X  X ffective X  radius, it always prevents a microcluster from unnecessarily splitting into several  X  X ad X  microclusters. So th e generated micro-clusters are always more accurate than those generated by Charu X  X  incremental method.

The cost evaluations with respect to the minimum radius n t are shown in Fig-ures 13 and 14. Here r t = 20 and we report K -means results on microclusters by PPMicroClustering and Charu X  X  Condensation method in both static and incre-mental methods. The cost increases moderately with increasing minimum points n . The clustering cost on microclusters g enerated by static PPMicroClustering is almost the same as by Charu X  X  static Condensation, while PPMicroClusters have privacy guarantee r t . As indicated by the costs, the clustering cost on microclusters generated by both increm ental methods are higher than by their static methods. Similar to the previous reason, the clustering cost on micro-clusters generated by our incremental method is slightly lower than by Charu X  X  incremental method.
 Privacy Measure. Since the generation of the PPMicroClusters is generaliz-ing all attributes simultaneously, the level of privacy can be measured by as follows: (1) the variance of the radius of the PPMicroClusters; (2) the aver-age radius of the PPMicroClusters. We analyze the influence of the changing value of r t on these two measures. The variance of radius r is measured by Var ( r )= E [( r  X  r t ) 2 ]= croClusters. Here, to examine the privacy degree of PPMicroClusters, our radius variance is computed as the average squared deviation of each PPMicroCluster X  X  radius from its radius (privacy) threshold r t . Figures 15 and 16 show that the impact of changing numbers of r t to the radius variance of PPMicroClusters. For both datasets, the smaller the minimum privacy threshold r t , the smaller variance of the radius of each PPMicroCluster, which means the lower degree of privacy. On the other hand, the variance of radius of PPMicroClusters is larger which means the higher degree of privacy. The average radius of PPMicroClus-ters is defined by 1 m We examine the average radius of PPMicroClusters with respect to the changing values of r t . Figures 17 and 18 depict the phenomena that for both datasets, the smaller the minimum radius threshold, the smaller average radius of PPMicro-Clusters (i.e., r t =1 . 5 in read data, the average radius of PPMicroClusters is 1.5), which means less privacy guarantee; while the average radius increases with increasing values of r t , which generally means more privacy will be obtained. The figures on the accuracy and the privacy measures show the fundamental trade-off of privacy-preserving clustering: with increasing minimum radius (or minimum numbers), the accuracy decrea ses while privacy increases. In this paper, we propose a novel robust transformation for privacy-preserving data mining based on the concept of privacy-preserving MicroClusters that preserving MicroClusters are made public for data mining purposes, but the orig-inal data records are kept private. We pr esent methods for generating privacy-preserving MicroClusters and show that standard clustering algorithms can eas-ily be adapted to cluster the public MicroClusters instead of the private data records. Our experimenta l evaluation on synthetic and real data sets demon-strates that the proposed methods achieve accurate clusterings while preserving the privacy.
 Acknowledgement. We would like to thank Dr. Jiawei Han, University of Illinois at Urbana-Champaign and Dr. Martin Ester, Simon Fraser University for their valuable suggestions on the previous drafts.

