 The performance of search engines crucially depends on their ability to capture the meaning of a query most likely in-tended by the user. We study the problem of mapping a search engine query to those nodes of a given subject tax-onomy that characterize its most likely meanings. We de-scribe the architecture of a classification system that uses a web directory to identify the subject context that the query terms are frequently used in. Based on its performance on the classification of 800,000 example queries recorded from MSN search, the system received the Runner-Up Award for Query Categorization Performance of the KDD Cup 2005. Most web search queries contain only two or three terms and therefore provide very limited information about the user X  X  information need to the search engine. Utilizing this infor-mation is a key factor to constructing effective web search engines. One way of approaching this problem computa-tionally is to approximate the intended meaning of a query by a node, or a set of nodes, in a given subject taxonomy. For instance, a query  X  X he raven X  can indicate that a user searches for information on entertainment/movies or on zo-ology . Thus, the intuitive problem of capturing the intended meaning of a query is reduced to the computational problem of mapping the query string to a set of nodes in a given  X  fixed, but arbitrary  X  subject taxonomy.
 The KDD Cup 2005 1 casts this problem setting into a com-petitive benchmarking framework that allows to evaluate and compare methods and systems which solve this prob-lem under a specified experimental setting. The KDD Cup data set comprises of a two-level taxonomy with 67 second level nodes and 800,000 MSN web search queries, 111 of which are labeled with up to five nodes of the taxonomy. The rest of this paper is organized as follows. We discuss related work in Section 2, detail the problem setting in Sec-tion 3, and describe the classification architecture in Section 4. Section 5 discusses the evaluation, Section 6 concludes. http://kdd05.lac.uic.edu/kddcup.html Capturing the intended meaning of a search query with the help of word sense disambiguation has been heavily investi-gated. Early studies (see [12] for a complete overview) have found no or only little improvement in precision. Word sense disambiguation is often resolved implicitly when queries are sufficiently long; in these rare cases, the additional words provide sufficient context to resolve the intended meaning. Web search queries most often contain only two or three words and the work of Sch  X utze and Pederson [13], Gonzalo et al. [9] and Stokoe et al. [14] demonstrates that word sense disambiguation can improve information retrieval. A different line of research focuses on interactive query ex-pansion or query reformulation; Bruza and Dennis [4; 6] develop a hyper-index to provide the user with query re-formulation recommendations. Anick and Tipirneni [3] de-velop an interactive query refinement system that is based on the analysis of semantically related lexical compounds. Allan and Raghavan [2] refine query semantics with the use of part-of-speech patterns and formulate clarification ques-tions for the user. Glance [8] studies a collaborative rec-ommendation system that records the queries of all users and supports new searches by recommending related query strings of other users.
 Clustering the set of search results is another way to deal with query ambiguity: ideally, ambiguous queries result in multiple clusters that reflect the possible interpretations of the query. After a topic insensitive web search the results are clustered based on content [5; 16; 11] and link structure [15].
 Our solution is based on a taxonomic mapping between a web directory and the subject taxonomy. Integrating ob-jects from one taxonomy into another has been studied by different authors. Agrawal and Srikant [1] develop an en-hanced naive Bayes algorithm that is based on the intuition that if two documents share their category in one taxonomy, they are likely also share their category in another taxonomy. Zhang and Lee [17] use a similar approach in combination with Support Vector Machines; in addition, they develop a co-bootstrapping method.
 Instead of exchanging objects between taxonomies Doan et al. [7] learn a direct mapping between taxonomy nodes us-ing the joint distribution of concepts. All those approaches require a substantial number of training examples in the source as well as in the target taxonomy. In our case, the web directory provides an abundance of filed web pages whereas there are only 111 example queries and no example docu-ments for the subject taxonomy. The problem that we address is driven by the intuition that a significant aspect of the semantics behind a search query can be captured by identifying likely nodes in a subject tax-onomy. An instance of the query categorization problem is described by an arbitrary but fixed subject taxonomy. The nodes are named (e.g., living , entertainment , sports for first level, living/pets and animals , sports/olympic games for sec-ond level nodes and so on), and their semantics are primarily characterized by these names. In addition, example queries that are labeled with nodes may be available that exemplify search queries for documents within these categories. For the KDD Cup 2005, a two-level taxonomy of 67 nodes is provided together with a total of 111 example queries; each of these 111 queries is manually tagged with up to five nodes in the taxonomy. An example excerpt of these data is displayed in Table 1. The example queries are not guaranteed to be drawn from the same distribution that governs the evaluation data.
 From this input, a classifier has to be obtained that auto-matically labels new queries with one or several nodes of the taxonomy. The subject taxonomy can be assumed to be fixed (over a reasonable period of time) for any given problem instance. Therefore, it is reasonable to allow for some manual involvement over the process of generating a classifier. The classifier, on the other hand, has to work automatically and be efficient in order to process a large volume of queries posted to a search engine.
 For the KDD Cup, a set of 800,000 hold-out queries are available. For a subset of these queries (it has been un-known which subset), manually annotated labels are avail-able. A natural performance criterion to be maximized is the F-measure, the harmonic mean of classification preci-sion and recall. In addition, a second sub-task of the Cup is to maximize precision, subject to the constraint that the F-measure be within the ten highest values for F-measure obtained by all submissions. This criterion is difficult to maximize because it requires a conjecture about the sub-missions of competitors. The classification system that we devise consists of a compo-nent that searches a given query in a web directory; we use the Google directory search interface that searches the open web directory Dmoz.org. This component generates an or-dered list of categories of the web directory. In general, the categories of the web directory will be distinct from the sub-ject taxonomy that defines the query categorization problem instance at hand. Therefore, a second component of the ar-chitecture maps directory categories to nodes of the subject taxonomy. Web directories tend to be large and manually mapping each of their nodes to a node of the given subject taxonomy is impractical; we therefore construct a tool that supports a semi-automatic mapping.
 Given a query, the third component processes the sorted list of categories returned by the web directory search compo-nent, consults the mapping component to translate these into nodes of the subject taxonomy, and combines all infor-mation into probability scores for the nodes of the subject taxonomy. This component conjectures the final result of up to five nodes in the subject taxonomy that maximize ei-ther the F-measure, or the precision at a desired minimum F-measure level. This component of our classification architecture searches a web directory  X  we use the Domz.org web directory  X  for the query. The component preprocesses the query by first removing search options (e.g.,  X  X iletype: X ) from the query string, and then posts the query to the Google directory search which scans the Dmoz directory for occurrences of the query within the Dmoz categories.
 The web directory searching component processes the first 100 search results and transforms them into an ordered list of those directory categories that the 100 retrieved docu-ments are classified into. Note, however, that the Dmoz taxonomy (or any other web directory) is generally distinct from the given subject taxonomy, in our case the KDD Cup taxonomy.
 The web directory searching component exploits two ad-ditional features of the Google directory search service. Google suggests alternative queries that in many cases cor-rect spelling errors in the posted queries ( X  X id you mean: . . .  X ). The web directory searching component always ac-cepts these suggestions which in many cases improves the re-sults on misspelled search queries. Google directory search also provides a list of  X  X elated categories X  along with the search results. These related categories are stored and in-fluence the final weighting process. The taxonomy of the web directory generally differs from the subject taxonomy that defines the instance of the query classification problem. The category mapping component of our classification architecture therefore translates the direc-tory taxonomy into the subject taxonomy.
 The subject taxonomy is characterized by the descriptive names of its nodes and very few example queries X  X or many nodes, only a single exemplifying query is available in the KDD Cup data. The web directory taxonomy, on the other hand, is defined by the collection of web pages filed under each node in addition to the descriptive name of each node. Given the salient role played by the descriptive names in the subject taxonomy and given the substantial size of the Dmoz web directory that contains thousands of nodes, a semi-automatic mechanism is advised. Fully automatic learning of a taxonomy mapping is an elegant alternative path that can be taken when sufficiently many example queries for both taxonomies are available X  X n our case, a total of 111 training examples renders this approach little promising. We manually assign directory categories to categories of the subject taxonomy by inspecting the web directory up to the second level, in some branches to the third or forth level where a finer granularity is needed. Bounding the depth for the manual inspection is necessary to keep the task manage-able. Each directory node is assigned up to three categories of the subject taxonomy, resulting in an n : m mapping between the taxonomies. Some examples of the resulting mapping table are shown in Table 2.
 The web directory organizes country specific pages in the same structure as the main taxonomy. They can be reached from the main taxonomy under the regional branch. Be-cause the regional information is not relevant for the cate-gorization the path nodes that correspond to regional infor-mation get removed, so that the regional directory nodes are projected onto the non-regional parts of the main taxonomy. For example, the category regional/Europe/Germany/health is treated like health .
 When a node of the web taxonomy is mapped to a node of the subject taxonomy then, by default, its entire subtree is mapped to the same node. This default behavior is overruled when nodes within that subtree are explicitly mapped to a distinct node of the subject taxonomy.
 The manual mapping of the first two (partially up to four) levels of the taxonomies imposes the risk of missing relevant nodes hidden deeper in the hierarchy. On the other hand, constructing a mapping for thousands of nodes is cumber-some. We develop an automatic recommendation system that provides suggestions for correspondences of deeper cat-egories. We define a characteristic query for each category of the subject taxonomy; e.g., information/law &amp; politics is assigned the query  X  X egal OR politics X . The mapping recommendation system posts these queries to the web di-rectory search engine; the categories of retrieved web pages are added to a candidate list of recommended mappings. After manually inspecting these recommendations for the KDD Cup task and accepting some 200 of them, we ob-tain a mapping that entails 763 rules following the schema visualized in Table 2. The web directory searching component returns for each query q an ordered list of up to 100 web directory categories C ( q ), we denote the elements of C  X  ( q ) as c  X  i . The mapping component maps a web directory category c  X  i to a list of sub-ject taxonomy categories M ( c  X  i ) whose elements we write as c . Conversely, let M  X  1 ( q, c j ) be the ordered subset of C whose elements c  X  i have c j in their assigned subject taxon-omy nodes; i.e., M  X  1 ( q, c j )  X  C  X  ( q ) with c  X  i  X  M if and only if c j  X  M ( c  X  i ). The ordering of the elements of M  X  1 ( q, c j ) is equal to their ordering in C  X  ( q ). Equation 1 defines the weight that associates the query q to subject taxonomy category c .
The terms of the weighting function are defined as fol-lows. Many technical decisions were made according to pre-liminary experimental studies. The weighting parameters  X  , . . . ,  X  8 are determined through an iterative procedure. Starting with intuitive values, we repeatedly pick one of those parameters and manually adjust it to achieve opti-mal accuracy on the training set while holding the other ones fixed. The final classification conjecture for a query q is based on three factors: firstly, the weights w ( q, c j ) for each cat-egory c j . Secondly, the rank r w ( q, c j ) of category c the position at which c j occurs when all categories are or-dered according to w ( q, c j ). Thirdly, an additional margin criterion that measures the distance between the weights of c j and the next likely category,  X  ( q, c j ) = w ( q, c We determine the category weights w ( q, c j ) for all training queries q and fit a logarithmic curve to the data to model the probability P ( c | w ( q, c j )). Figure 1 shows the empirical data and the fitted curve; queries are grouped into equally sized batches. We estimate the discrete distribution P ( c j | r from the training data, Figure 2 shows the result. Figure 1: Relation between category weight w ( q, c j ) of a query-category pair and its probability of correctness P ( c | w ( q, c j )) on the training data. Figure 2: Relation between category rank r w ( c j ) and its probability of correctness P ( c j | r w ( c j )) on the training data. A logistic regression model combines the weight, rank, and margin into a single probability P ( c j | q ). We fit the logistic regression model [10] as given in Equation 2 using the Mitch data analysis software package.

The final step is to determine the probability thresholds for the two tasks (F-measure and precision). The probabil-ity threshold  X  for the best F-measure score is determined through a hill climbing procedure. The value  X  is initialized to 0.5 and we determine the direction of search by comput-ing the F-measure gradient  X  F (  X  ) = lim  X   X  0 F (  X  +  X  )  X  F (  X  ). This gradient is dependent on a binary random variable X that determines whether a query with probability P ( c j | q ) =  X  +  X  is correct. We calculate the expectation over X in Equation 3. The expected gradient of F can be split up in Equation 4 because for the case that X = 1 only TP in-creases and only the gradient with respect to TP is relevant because  X  F (  X  ) =  X F  X T P (  X  ) and for X = 0 only the one with respect to FP is relevant. TP and FP are the number of true positives and false positives respectively. We get Equation 5 by substituting P ( X = 1) =  X  +  X  and P ( X = 0) = 1  X  (  X  +  X  ).  X  F (  X  ) = lim With Equation 6 we determine the direction of search from each new threshold value  X  and conduct a binary search. In each search step the search space is halved and the direction of search is determined through the F-gradient.
 For the precision task we need to specify a minimum F-measure that we want to achieve. For finding the best pre-cision we start with a threshold  X  = 1 and step down in suffi-ciently small steps until the minimum F-measure is reached. The trade-off between F-measure and precision, dependent on the probability threshold  X  , is visualized in Figure 3. The hill climbing procedure described in Section 4.3 successfull y finds the maximum F-measure at 0.51. Table 3 gives an overview of the different solutions of our team and the win-ning team. We can see that the F-measure performance of our submission on the held out data set that is used for the competition result is lower (0.41) than the estimation on the training data. In this case measuring the performance on the 111 training examples can only be used as a rough estimate of the actual performance. The winning solution for the F-measure task of the winning team reaches 0.44; our submission of 0.41 is rated second best and receives the Runner-Up Award.
 For the precision task we have to make a guess at the F-measures of the competing teams as the goal is to achieve the maximum precision while remaining within the top ten F-measure submissions. We prepare two guesses, one with an F-measure of 0.29 and the other with an F-measure of 0.51 (same as the submission for the F-measure task) on the training data (see Table 3). Assuming that most competing teams would risk a high precision at a relatively low recall level, we decided to submit the first solution that achieves 75% precision on the hold-out data. Refuting our assump-tion, the ten teams whose solutions achieve the highest F-measure decide for more balanced solutions that outperform our F-measure without getting close to our precision. Our second guess of an F-measure of 0.41 and a precision of 45% would even have beaten the winning team X  X  submission of an F-measure of 0.43 and 42% precision. So the winning team not only had an excellent solution to the query categoriza-tion problem, but also solved the  X  X oker playing X  problem of predicting their competitors X  hands well. Figure 3: Relation between F-measure/precision and the probability threshold  X  estimated on the training data. Extracting semantic information from search queries is im-portant for a search engine to understand a user X  X  informa-tion need. Mapping a search query to a set of nodes of a subject taxonomy captures an aspect of that query X  X  seman-tics.
 We devised an architecture that allows to map queries to an arbitrary subject taxonomy; the instantiation of the ar-chitecture to a given taxonomy requires manual effort that is supported by a tool that suggests translation rules from the web directory to the taxonomy. The architecture com-prises of a web directory search component that determines a ranked list of categories of directory categories that are relevant to the query. A taxonomy mapping component translates these categories into (possibly multiple) nodes of the target taxonomy, and a conjecturing component com-bines this information into the final taxonomy categories. A logistic regression model trained with the Mitch software package combines the association weights between query and taxonomy nodes, the ranking of the nodes, and a margin term into a probability used to determine the most likely nodes.
 The KDD Cup 2005 provides a benchmarking framework for the query categorization task. Our system achieved the second-highest F-measure among all participants; this ar-gues that our system and architecture provide a good and appropriate solution to this problem. The work reported here was partially supported by the Ger-man Science Foundation DFG under Grants SCHE540/10-1 and SCHE540/10-2. We acknowledge A.I. Insight, Inc. and MEDai, Inc. for the use of their predictive modeling technology Mitch (Multiple Intelligent Tasking Computer Heuristics). [1] R. Agrawal and R. Srikant. On integrating catalogs. In [2] J. Allan and H. Raghavan. Using part-of-speech pat-[3] P. Anick and S. Tipirneni. The paraphrase search assis-[4] P. Bruza and S. Dennis. Query reformulation on the [5] D. R. Cutting, J. O. Pedersen, D. Karger, and J. W. [6] S. Dennis, P. Bruza, and R. McArthur. Web searching: [7] A. Doan, J. Madhavan, P. Domingos, and A. Halevy. [8] N. Glance. Community search assistant. In Proceedings [9] J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarran. In-[10] D. Hosmer and S. Lemeshow. Applied logistic regres-[11] D. W. J. Stefanowski. Carrot2 and language proper-[12] M. Sanderson. Retrieving with good sense. Information [13] H. Schutze and J. Pederson. Information retrieval based [14] C. Stokoe, M. Oakes, and J. Tait. Word sense disam-[15] M. K. Y. Wang. C4-2: Combining link and contents [16] O. Zamir and O. Etzioni. Grouper: a dynamic cluster-[17] D. Zhang and W. Lee. Learning to integrate web tax-David Vogel is the Lead Scientist at A.I. Insight, where he has spent over 7 years leading the development of their modeling tool MITCH (Multiple Intelligent Tasking Computer Heuristics). David X  X  research interests include development of innovative modeling techniques, scalable algorithms, and new applications for predictive models. Steffen Bickel is Ph.D. student at Humboldt Uni-versity, Berlin. His research interests are machine learning methods for information retrieval.
 Peter Haider, Rolf Schimpfky and Peter Siemen are Master X  X  students in computer science at Humboldt Uni-versity, Berlin, with a focus on machine learning methods. Steve Bridges is the Software Architect at MEDai. He has more than sixteen years of experience in developing software, much of it software for decision support and data mining applications.
 Tobias Scheffer is Assistant Professor at Humboldt University, Berlin. He received his PhD in 1999 from Berlin University of Technology. He was awarded an Emmy Noether Fellowship of the German Research Foundation in 2002 and an Ernst von Siemens Fellowship in 1999. He has previously been working at Magdeburg University, the University of New South Wales, Sydney, and Siemens Corporate Research, Princeton. His research focuses on machine learning and information retrieval.

