 Query auto-completion (QAC) is a prominent feature of modern search engines. It is aimed at saving user X  X  time and enhancing the search experience. Current QAC models mostly rank matching QAC candidates according to their past popularity, i.e., frequency. However, query popularity changes over time and may vary drasti-cally across users. Hence, rankings of QAC candidates should be adjusted accordingly. In previous work time-sensitive QAC mod-els and user-specific QAC models have been developed separately. Both types of QAC model lead to important improvements over models that are neither time-sensitive nor personalized. We pro-pose a hybrid QAC model that considers both of these aspects: time-sensitivity and personalization.

Using search logs, we return the top N QAC candidates by pre-dicted popularity based on their recent trend and cyclic behavior. We use auto-correlation to detect query periodicity by long-term time-series analysis, and anticipate the query popularity trend based on observations within an optimal time window returned by a re-gression model. We rerank the returned top N candidates by in-tegrating their similarities with a user X  X  preceding queries (both in the current session and in previous sessions by the same user) on a character level to produce a final QAC list. Our experimental re-sults on two real-world datasets show that our hybrid QAC model outperforms state-of-the-art time-sensitive QAC baseline, achiev-ing total improvements of between 3% and 7% in terms of MRR. H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Query auto-completion; personalization; time-sensitive
To improve the search quality, common search engines and pop-ular online properties such as online shopping and email, all pro-vide a query auto-completion (QAC) service, where the goal is to help users formulate queries by providing possible completions matching the first few keystrokes typed. As a user enters a query Figure 1: (Top) Query auto-completion by Google for the pre-fix cikm . (Bottom) The refined completions with after continu-ing to type conference after cikm . The snapshot was taken on Thursday, April 10, 2014. in search box, matching completions appear below the search box as a drop-down menu with the typed characters highlighted in each completion. Once the matching candidates are filtered, they can be ranked according to different criteria. For instance, in an online store such completions may be ordered according to the price of products [12]. In pre-computed auto-completion systems, the list of matching candidates for each prefix are generated in advance and stored in efficient data structure for fast lookups. When needed, as shown in Fig. 1, continuing typing more characters can dynami-cally refine the completions by exact prefix matching until you find a more appropriate completion. Where offered, the facility is heav-ily used and highly influential on search results [3, 32].
Clearly, query auto-completion, or  X  X ype ahead X  functionality, just takes a few initial keystrokes as input and returns matching queries to auto-complete the search clue. QAC candidates can be weighted by previous popularity and can either be influenced or hard-sorted by this factor or any other criterion, so that the most possible completions are listed first. Generally, queries can be completed from one or more of the following sources: (i) previ-ous query popularity; (ii) search behavior; (iii) for email search, sender/recipient names using the field aliases (to:, from: and cc:).
A common approach in previous work on QAC is to extract past queries with each prefix from a period of query logs, and rank them by their past popularity [3, 32, 33], which assumes that current or future query popularity is the same as past query popularity. Al-though this approach results in satisfactory QAC performance on average, it is far from optimal since it fails to take strong clues from time, trend and user-specific context into consideration while such information often influences the queries most likely to be typed. As illustrated in Fig. 2, personalized QAC may inject the most popu-lar completions from a user as query completions to that user; see Fig. 2a (not personalized) and 2b (personalized). From Fig. 2c and Fig. 2d according to Google Trends, we can also find the query popularity strongly depends on the time (a clear burst for MH370 Figure 2: (Top) Google QAC of the typed prefix c under dif-ferent logging settings. (Bottom) Relative query popularity for different queries over time. Queries: MH370 in blue, movie in red and christmas in yellow. Among the three queries, movie is more popular weekly on weekends, while christmas is issued more commonly by users yearly, and MH370 presents a sharp increase from a relatively low level near March 8, 2014. The snapshot was taken on Thursday, April 10, 2014. around 8 March, 2014), and it presents cyclic phenomena (yearly for christmas and weekly for movie ), which can be explored to fore-cast the future query popularity. 1 This motivates a QAC approach that takes both the temporal aspect and the personal context into account. This work is an attempt towards this objective.
To begin with, we differentiate query auto-completion (QAC) with query suggestion (QS) as follows: Definition Let a string s with length l be keystrokes typed by user u in search box. A query suggestion QS of s for u with the form QS ( s,u ) is a set of candidate queries q , where q  X  QS ( s,u ) is considered to be relevant to s . Similarly, a query auto completion QAC of s for u with the form QAC ( s,u ) is a set of candidate queries q , where QAC ( s,u ) = { q : q StartsWith ( s ) } , namely q [ i ] = s [ i ] for i from 1 to l .
 From this definition, QS ( s,u ) may cover more general queries than QAC ( s,u ) . Also, we classify queries that need to be con-sidered for completion into two categories. The first category cor-responds to periodic queries that are: (i) consistently popular with short-term periodicity (e.g., movie ); (ii) temporally recurring with long-term periodicity (e.g., christmas ). The second category corre-sponds to aperiodic queries related to entirely unforeseeable break-ing events and phenomena (e.g., MH370 ). Therefore, achieving optimal QAC effectiveness for a user, on average, is attributed to two factors: time-sensitive query popularity and personal context.
In our QAC model we first return the top N query completions by predicted popularity, not only based on the recent trend but also based on cyclic phenomena; we then rerank the returned top N http://www . google . com/trends completions by user-specific context to output a final query com-pletion list. Predicted query popularity is based on two aspects, i.e., periodicity of a query detected by long-term time-series analysis plus its recent trend as indicated by observations during an optimal time window returned by a regression model. Rather than summa-rizing recent variations of query popularity in a fixed time span, we anticipate the trend from different periods of observations for each query. Secondly, we exploit each user X  X  previous queries, both during the current session and from historical logs as user-specific context for reranking the top N QAC candidates. We combine the contributions from the predicted popularity and user-specific query similarity to produce a personalized list of QAC candidates.
We show that the predicted popularity values produced by our time-sensitive approach are closer approximations to what will be observed later in the logs, and are more effective for QAC after integrating user query similarity, with improvements in Mean Re-ciprocal Rank (MRR) scores by 3% on the AOL log and 7% on a search log from an audiovisual archive when compared to a state-of-the-art time-sensitive baseline [36].

Our contributions in this paper can be summarized as follows: 1. We tackle the challenge of query auto-completion in a novel 2. We propose a new query popularity prediction method that 3. We analyze the effectiveness of our hybrid QAC model which
Query auto-completion (QAC) [4, 5, 19, 32] is a prominent fea-ture of common search engines. It relies on query logs to generate QAC candidates, and is among the first services that users inter-act when using an information retrieval system as they search and formulate their queries [31]. In major web search scenarios, the common and straightforward approach to rank QAC candidates is to use Maximum Likelihood Estimation (MLE) based on the past popularity of queries [3]. Bar-Yossef and Kraus [3] refer to this type of ranking as the Most Popular Completion (MPC) model: where f ( q ) denotes the number of occurrences of query q in search log Q , and C ( p ) is a set of query completion candidates that start with prefix p . In essence, the MPC model assumes that the cur-rent query popularity distribution will remain the same as that pre-viously observed, and hence completions are ranked by their past popularity in order to maximize QAC effectiveness for all users on average. As mentioned earlier, query popularity may change over time and the ranking of completions is also user-dependent (see Fig. 2). Accordingly, the QAC candidates must be adjusted con-sequently to account for time-sensitive and user-specific changes.
Time-sensitive query auto-completion (TS-QAC) takes time in-formation, such as recency [16, 26, 36] and seasonality [30, 32], into consideration for ranking QAC candidates. It leverages time-series analysis techniques for classifying seasonal queries and fore-casting their future popularity [30, 32]. Alfonseca et al. [1] cluster queries based on time-series features; they suggest that their ap-proach can be used for query completion and query categorization.
Rather than ranking QAC candidates by their previously observed popularity, Shokouhi and Radinsky [32] propose a long-term time-series modeling approach to forecast the query frequencies via ap-plying a fixed moving time window. Queries recurring during spe-cific temporal intervals, such as day/night, workday/weekend, sum-mer/winter, etc. are modeled differently to predict future popularity for QAC ranking at different times. The forecasts obtained by such time-series modeling are substantially more reliable. However, the detailed analysis of the performance impact of the time window period selection and how to choose the optimal length of time win-dow is unsolved. Similarly, Strizhevskaya et al. [33] study actu-alization techniques for measuring prediction accuracy of various daily query popularity prediction models using query logs.
Another aspect of time-sensitive QAC is the problem of search trend prediction. Short-range query popularity prediction has seen little attention. Golbandi et al. [16] develop a regression model to detect bursting queries for enhancing trend detection. By analyzing query logs, they seek to accurately predict what are the most trend-ing query items on the Web. Various attempts have been made to make search trend prediction more accurate with low latency rela-tive to the actual event that sparked the trend. Kulkarni et al. [21] classify queries into different categories based on the changes in popularity over time, and show that monitoring the query popular-ity can reveal strong signals for detecting the trend in query intent.
White and Marchionini [35] propose a query completion model that produces an updated list of additional terms as a searcher en-ters his query. Real-time query completion is found to help users form better queries. Chien and Immorlica [13] demonstrate that queries with similar temporal patterns can be semantically related for query completion despite no lexical overlap. Liu et al. [24] in-troduce a unified model for forecasting query frequency, in which the forecast for each query is influenced by the frequencies pre-dicted for semantically similar queries. Michail et al. [25] develop a compressed representation for time-series and propose a model for detecting bursts in query frequencies; these approaches can be utilized to boost QAC effectiveness.

Recently, considering both recent trend and past query popular-ity, Whiting and Jose [36] propose several practical QAC ranking approaches to deal with both robust and time-sensitive QAC task (as baselines), such as outputting query popularity evidence from a sliding window of past 2 to 28 days or the query popularity distri-bution in a recent query chunk observed with a given prefix, as well as predicting query popularity based on recently observed trends.
Our TS-QAC approach differs from previous work as we con-sider both periodicity and recent trends in query frequency. Addi-tionally, none of the work listed so far caters for individual users, returning the same QAC list of typed prefixes. We return a per-sonalized QAC ranking list to boost QAC effectiveness based on a time-sensitive QAC ranking list output by forecasted query popu-larity, which will specifically benefit revisiting search tasks.
In most work mentioned so far, QAC candidates are computed same list of candidates. But exploiting the user X  X  personal context has led to increases in QAC effectiveness [3, 22, 28, 31].
Bar-Yossef and Kraus [3] treat the user X  X  recent queries as con-text and exploit users with shared search activity, considering the similarity of QAC candidates with this context for ranking. Their hybrid model computes the final score of each candidate by linearly combining the MPC score and a context-similarity score. Our ap-proach to personalized QAC differs in the definition of and in the way we measure context-similarity. Shokouhi [31] exploits pro-files to extract user-based features to model the likelihood that a user will issue certain queries, and explores the effectiveness of considering a user X  X  age, gender, location and longer search history in QAC ranking, to therefore personalize the QAC ranking.
Guo et al. [18] propose a two-step approach, in which the user X  X  session context is matched against pre-generated topic models for ranking QAC candidates. Similarly, Cao et al. [10] and Liao et al. [22] first cluster queries in the click graph into a smaller set of vir-tual concepts. They match the users X  context captured based on their recent queries against these clusters for ranking QAC can-didates. Weber and Castillo [34] focus on showing differences in query likelihood across demographics. They predict the sec-ond term in a query based on an unsupervised probabilistic model. Building on temporal intuitions, Sengstock and Gertz [29] consider query completions that depend on the time of day, i.e., taking the search time as a user-specific context. Arias et al. [2] propose a QAC algorithm for mobile search; their completions are thesaurus-based concepts whose relatedness to the user X  X  context is fixed and pre-determined by a rule-based mechanism.

Bhatia et al. [7] mine frequently occurring phrases and n -grams from indexed documents for generating and ranking QAC candi-dates for partial queries in the absence of search logs. Fan et al. [14] propose a generative model that incorporates the topical co-herence of terms based on Latent Dirichlet Allocation (LDA) for ranking QAC candidates. Closely similar to QAC, Bickel et al. [8] learn a linearly interpolated n -gram model for sentence completion based on lexicon statistics of text collections. Grabski and Scheffer [17] deploy an index-based retrieval algorithm and a cluster-based approach for their sentence-completion task.

Most user specific QAC approaches (re-)rank QAC candidates by measuring their similarity with the content in search logs, over-looking the updated query popularity. To the best of our knowl-edge, there is no published work on QAC considering both time-sensitivity and user-specificity. Previous work either focused on one or the other of these two aspects of QAC. Combining them is our goal here: time-sensitive personalized query auto-completion. By doing so our approach to personalized QAC stands to gain from repetitions in user search behavior.
In this section we describe our time-sensitive personalized query auto-completion approach, a hybrid model, that not only inherits the merits of time-sensitive query auto-completion but considers a user X  X  personal context. Table 1 provides an overview of the QAC approaches we discuss; the baselines (rows 1 X 3) are described in the literature; we detail our models (rows 4 X 10) in three steps: time-sensitive QAC, personalized QAC, and hybrid QAC. We propose a time-sensitive QAC (TS-QAC) method that ranks QAC candidates by predicted query popularity (i.e., its frequency) based on its periodicity and recent trend to detect both cyclicly and instantly frequent queries. TS-QAC not only inherits the merits of time-series analysis on long-term observations of query popularity, but also considers the recent variation of query counts. Specifi-cally, we predict a query q  X  X  next-day popularity  X  y t t + 1 before day t 0 by both its recent trend and periodicity with a free parameter  X  (0  X   X   X  1) controlling each contribution:  X  y 0 +1 ( q, X  ) =  X   X   X  y t 0 +1 ( q ) trend + (1  X   X  )  X   X  y where  X  = 1 for aperiodic queries and 0  X   X  &lt; 1 for periodic of predictions from recent N days observations: where norm (  X  i ) normalizes the contributions from each day to en-sure P i  X  i = 1 . We introduce a temporal decay function to output the weight before normalizing as  X  i = f TD ( i )  X  1 , where f is a de-cay factor and TD ( i ) refers to the interval from day i to the future day t 0 + 1 . We identify the highest prediction accuracy parameter N days for each query based on its past observations in the whole log using a multiple linear regression model, following [36]. The prediction  X  y t 0 +1 ( q,i ) trend from each day i ( i = 1 ,...,N derived from the first order derivative of q  X  X  daily count C ( q,t ) as:  X  y where y t 0  X  TD ( i ) ( q,i ) is the observed query count of q at day i .
The periodicity term  X  y t 0 +1 ( q ) peri in (2) is smoothed by simply averaging the recent M observations y t p at preceding time points t = t 0 + 1  X  1  X  T q ,...,t 0 + 1  X  M  X  T q in the log: where T q denotes q  X  X  periodicity. For detecting cyclic aspects of query q  X  X  frequency, we use the autocorrelation coefficients [11], which measure the correlation between N s successive count obser-vations C ( q,t ) at different times t = 1 , 2 ,...,N s in the query log. The correlation is computed between a time series and the same series lagged by i time units: r where  X  x 1 is the mean of the first N s  X  i observations and  X  x mean of the last N s  X  i observations. For N s reasonably large, the denominator in (6) can be simplified by approximation. First, the difference between the sub-period means  X  x 1 and  X  x 2 Second, the difference between summations over observations 1 to N s  X  i and i + 1 to N s can be ignored. Accordingly, r i can be approximated by: where  X  x = P N s t =1 C ( q,t ) is the overall mean.
 Algorithm 1 Time-sensitive query auto-completion (TS-QAC). Input: All queries: Q ; Length of training and validation days: L Output: Predictions:  X  Q = {  X  y t 0 +1 ( q ) : q  X  Q }; 1: for each q  X  Q do 2: T q  X  autocor ( Count ( q )) ; 3: for i = 1 ,  X  X  X  ,L t do 4: for j = 1 ,  X  X  X  ,L v do 5:  X  y t 0 +1 ( q ) trend [ j ]  X  Regression ( Count ( q )[1 : i ]) ; 6: AbsoluteError [ j ]  X   X  y t 0 +1 ( q ) trend [ j ]  X  y 7: end for 8: MAE ( i )  X  mean ( AbsoluteError ) ; 9: end for 11: Update  X  y t 0 +1 ( q ) trend with optimal N days and Com-12: end for 13: Find an optimal  X   X  by (8); 14:  X   X   X   X  ; 15: for each q  X  Q do 16:  X  y t 0 +1 ( q, X  )  X   X   X   X  y t 0 +1 ( q ) trend +(1  X   X  )  X   X  y 17: end for 18: for each q  X  Q do 19: for each prefix p of q do 20: Return top N completions of p ranked by  X  y t 0 +1 ( q, X  ) ; 21: end for 22: end for
Additionally, we choose an optimal free parameter  X   X  by min-imizing a forecast accuracy metric Mean Absolute Error (MAE) (described in  X 4.3) as: at day s during the validation period ( L v days), respectively.
Algorithm 1 details the major steps of time-sensitive QAC. For our time-sensitive QAC method with a fixed  X  we write  X  -TS-QAC; we write  X   X  -TS-QAC when we use an optimal  X   X  .
We extend our time-sensitive QAC described in  X 3.1 with per-sonalized QAC in this section. After sorting the queries with typed prefix p by predicted popularity following (2), we are given a rank-ing list of top N QAC candidates. Let S ( p ) represent the set of returned top N QAC candidates of prefix p .
Our personalized QAC works here by scoring the candidates q  X  X  ( p ) using a combination of similarity scores Score ( Q and Score ( Q u ,q c ) , where Q s relates to the recent queries in the current search session and Q u refers to those of the same user is-sued before, if available, as:
Pscore ( q c ) =  X   X  Score ( Q s ,q c ) + (1  X   X  )  X  Score ( Q where  X  controls the weight of the individual component. Person-alized QAC works at the session-based and user-dependent level.
To compute the similarity scores, we first consider how to repre-sent queries in Q s and Q u . A naive approach would be to represent a query by n -grams or its terms as  X  X  bag of words X . The resulting similarity measure can capture syntactic reformulations. However, the problem is that queries are short, and thus their vocabulary is too sparse to capture semantic relationships. In order to overcome this sparsity problem, we use another solution to measure similar-we find that users often request the same query or reformulate the query by extending or simplifying previous ones within the same session. We treat a user X  X  preceding queries Q s in the current ses-sion and Q u in the historical log as context to personalize QAC where we measure similarity at the character level.

We represent each query q s  X  Q s and q c  X  S ( p ) by their query terms as { w s 1 ,w s 2 ,...,w sm } and { w c 1 ,w c 1 ,...,w N ( w  X  ,q  X  ) denote the frequency of term w  X  appearing in q score the similarity between q c and Q s as a conditional probability:
Score ( Q s ,q c ) = p ( q c | Q s ) = X where norm (  X  s ) introduces a decay function  X  s = f TD ( s )  X  1 in (3) except that here TD ( s ) refers to the interval between q q , and p ( q c | q s ) is calculated following [9] as: where W ( w ci ) = { w : w  X  q s | w [0] = w ci [0] } is a set of terms in q s sharing the same start with w ci , and where len ( common ( w ci ,w j )) is the maximal length of common string appearing in w ci and w j from the beginning.

We compute Score ( Q u ,q c ) in a different manner from Score ( Q q query count and time interval. We output Score ( Q u ,q c Score ( Q u ,q c ) = p ( q c | Q u ) = X where norm (  X  s ) only depends on the query count X  X e assume that frequent queries reflect a user X  X  personal search clues. We introduce a hybrid QAC model that combines time-sensitive QAC (TS-QAC) with personalized QAC. First, TS-QAC produces Algorithm 2 Hybrid QAC model Input: Predictions:  X  Q ; user: u ; prefix p ; N ;  X  ; Output: Ranking list of top N QAC candidates of p ; 1: Produce S ( p ) consisting of top N QAC candidates by (2); 2: List u  X  X  queries Q u and Q s ; 3: for each q c  X  S ( p ) do 4: Compute TSscore ( q c ) based on (14); 5: for each q s  X  Q s do 6: p ( q c | q s ) = Similarity ( q c ,q s ) ; 7: end for 8: Compute Score ( Q s ,q c ) based on (10); 9: for each q u  X  Q u do 10: p ( q c | q u ) = Similarity ( q c ,q u ) ; 11: end for 12: Compute Score ( Q u ,q c ) based on (12); 13: Compute Pscore ( q c ) based on (9) and (15); 14: end for 15: Re-rank S ( p ) by HQscore ( q c ) based on (13); 16: Return a reranked list of S ( p ) ; a list of QAC candidates S ( p ) of prefix p . We assign TSscore ( q to each candidate q c  X  S ( p ) using its predicted popularity, i.e.,  X  y +1 ( q c , X  ) in (2). Like [3], we then define our hybrid models as convex combinations of two scoring functions:
Hscore ( q c ) =  X   X  TSscore ( q c ) + (1  X   X  )  X  Pscore ( q As TSscore ( q c ) and Pscore ( q c ) use different units and scales, they need to be standardized before being combined. We standard-ize TSscore ( q c ) (used in [3]) as: where  X  T and  X  T are the mean and standard deviation of predicted popularity of queries in S ( p ) . Similarly, we use (9) to obtain where  X  P and  X  P are the mean and standard deviation of similarity scores of queries in S ( p ) . Algorithm 2 describes our hybrid QAC model; (13) provides the overall ranking score (see line 15).
We write  X  -H-QAC to refer to the hybrid of  X  -TS-QAC (used at line 4 in Algorithm 2) and the personalization approach described in the previous section, and  X   X  -H-QAC for the variant where  X  has been optimized according to (8).
For comparison, we also introduce further combined QAC mod-els that combine popularity and personalization. G-QAC and  X  H G -QAC rank QAC candidates according to a combined score:
Cscore ( q c ) =  X   X  MPCscore ( q c ) + (1  X   X  )  X  Gscore ( q where G-QAC MPCscore ( q c ) is obtained using MPC-ALL and Gscore ( q c ) by measuring similarity between q c and previous queries using an n -gram representation;  X   X  -H G -QAC obtains the score Cscore ( q c ) by combining  X   X  -TS-QAC score (see  X 3.1) as MPC -score ( q c ) with the n -gram similarity score as Gscore ( q
Below,  X 4.1 lists the research questions to guide our experiments;  X 4.2 describes the datasets;  X 4.3 gives details about our evaluation metrics and baselines; we detail our settings and parameters in  X 4.4. The research questions guiding the remainder of the paper are: RQ1 As a sanity check, what is the relative performance of our RQ2 How does the trade-off between recent trend and periodicity RQ3 How do our time-sensitive QAC models (  X  -TS-QAC and  X  RQ4 Does our  X   X  -H-QAC significantly outperform time-sensitive RQ5 How does  X   X  -H-QAC compare against personalized QAC RQ6 Which part contributes more to a better QAC ranking, the RQ7 How do  X   X  -H G -QAC and  X   X  -H-QAC compare? (See  X 5.7.)
We use two query log datasets 2 in our experiments: AOL [27] and one made available by The Netherlands Institute for Sound and Vision, 3 a large audiovisual archive [20], which we will refer to as  X  X nV. X  AOL is publicly available and sufficiently large to guaran-tee statistical significance and SnV is one of the largest audiovi-sual archives in Europe. The AOL queries were sampled between March 1, 2006 and May 31, 2006. In total there are 16,946,938 queries submitted by 657,426 unique users while the SnV logs were recorded for one year between January 1, 2013 and December 31, 2013 using an in-house system tailored to the archive X  X  online in-terface. For consistency, we partitioned each log into two parts: a training set consisting of 75% of the query log, and a test set con-sisting of the remaining 25%. Traditional k -fold cross-validation is not applicable to streaming sequence since it would disorder the temporal data [15]. Queries in the training set were submitted be-fore May 8, 2006 in the AOL dataset and before October 1, 2013 in the SnV dataset. We also use the last week of training data to generate the optimal parameters N days in (3) and  X   X  in (8).
Moreover, we filtered out a large volume of navigational queries containing URL substrings (.com, .net, .org, http, .edu, www.) from the AOL dataset and removed queries starting with special charac-ters such as &amp; , $ and # from both datasets. Additionally, only queries appearing in both two partitions were kept. In total, 95,043 unique queries (21%) in the processed AOL and 6,023 (7%) in SnV show cyclic phenomena in terms of query frequency. Session boundaries are identified in the AOL dataset by 30 seconds of inac-tivity; in the SnV dataset a session boundary occurs when a query has no overlapping terms with the previous query as users routinely view audiovisual material during the search process; this can lead to periods of inactivity even though the user is still fully engaged in the search process [20]. Table 2 details the statistics of the datasets.
We display the overlaps of queries with various ways of bin-ning in Fig. 3. Fig. 3a shows the rates of unique &lt; user , query &gt; pairs posted at different number of repeats. A considerable number of queries are posted more than once by the same user within the training period (15.9% for AOL and 56.9% for SnV). The discrep-ancy between the rates can be explained by considering the type of
We did not use the MSN and Sogou query logs as the former lacks users IDs and the latter is too small. http://www . beeldengeluid . nl Table 2: Statistics of processed AOL and SnV Dataset. Queries: Qs, Sessions: Ss, Users: Us, URLs: Clicked Ds.

Variables Training Testing Training Testing #Qs 6,904,655 3,609,617 291,392 154,770 #Unique Qs 456,010 456,010 86,049 86,049 #Ss 5,091,706 2,201,990 176,893 102,496 #Unique Us 466,241 314,153 1051 804 #Qs/Ss 1.36 1.63 1.65 1.51 #Qs/Us 14.81 11.49 277.25 192.50 Figure 3: Query repeat rates (left) and variation rates (right) for AOL and SnV. users the search engine serves. Fig. 3b gives us the distribution of sessions containing queries that  X  X volved X  from preceding queries within the session, where we say that query q 2 evolved from query q if q 2 is issued after q 1 and shares at least one common query term with q 1 . Sessions with more than one query are considered. In total, there are 983,983 sessions in AOL and 35,942 in SnV left. Clearly, users reformulate a query very often from its previous queries. The difference between the sum of all rates (0.531 for AOL and 1 for SnV) is a consequence of different session segmentation methods. We first measure our forecast accuracy for the time-sensitive QAC model and then evaluate the effectiveness of the resulting QAC rankings. For each task, we use metrics from statistics and information retrieval for measurement, which are widely used in the literature on QAC task [3, 31, 32, 36].

Mean Absolute Error (MAE) is widely used to measure the ac-curacy of forecasts and is defined as follows: unbounded measure and is not strongly resilient to outliers. There-fore, its is often used along with another metric as Symmetric Mean Absolute Percentage Error (SMAPE) to diagnose the forecast vari-ation. SMAPE is defined as: In contrast to MAE, SMAPE is bounded between 0 and 1.
 To evaluate the effectiveness of QAC rankings, Mean Reciprocal Rank (MRR) is a standard measure. For a query q with prefix p in the query set Q associated with a list of QAC candidates S ( p ) and the user X  X  finally completed query q 0 , Reciprocal Rank (RR) is computed as: Table 3: The forecast metrics produced by different methods on AOL and SnV dataset. The best performer in each column is highlighted. Statistical significance of pairwise differences (  X  -TS-QAC vs. the best baseline P  X  and  X   X  -TS-QAC vs. the best baseline P  X  ) are detected by the t-test ( N / H for  X  = .01, or / O for  X  = .05).
 Then MRR is computed as the mean of RR for all queries in Q .
Statistical significance of observed differences between the per-formance of two approaches is tested using a two-tailed paired t-test and is denoted using N / H for significant differences for  X  = . 01 , or / O for  X  = . 05 .

We consider several QAC baselines: (1) the most popular com-pletion (MPC) QAC method based on the whole log, referred as MPC-ALL [3]; (2) an MPC-based QAC method within recent time windows (TW = 2, 4, 7, 14 and 28 days, respectively) denoted as MPC-R-TW [36]; (3) a recent QAC method with an optimized time window referred as O-MPC-R, which learns the optimal time win-dow for each prefix and performs best on the AOL dataset in [36].
Following [6], we set the factor f = 0 . 95 in decay function in  X 3.1. For time-sensitive prediction, we use a fixed  X  = 0 . 5 in (2) to compare with the results produced with an optimal  X   X  returned by (8). To detect periodicity, we count queries per hour for AOL and per day for SnV because of the difference in time spans of the collected data. This means that for SnV, we compute  X  y t in (5) directly by averaging the day-level predictions y t while for AOL, we firstly generate predictions per hour and then accumulate them to produce y t 0 +1  X  m  X  T q . For identifying trends, we use per day counts to overcome sparsity. For smoothing in (5), we set M = 3 , as it performs best when M changes from 1 to 10 in our trials. In our time-sensitive QAC experiments, we are given a list of top N QAC candidates; we set N = 10 as this is commonly used by many web search engines.

We balance the contributions of Q s and Q u in (9), if available, by setting  X  = 0 . 5 , and construct Q u using at most ten queries issued before while collecting all preceding queries in the current session to form Q s (see Table 2). For personalized QAC comparisons, we set the size of n -grams to be n = 4 , which has been recommended in string search [23] to represent queries. For hybrid models, we set  X  = 0 . 5 in (13).
In  X 5.1, we examine the performance of our time-sensitive QAC model in terms of its query popularity prediction performance, which we follow with a section about the trade-off of the parameter  X  in  X 5.2. We examine the performance of various TS-QAC approaches in  X 5.3.  X 5.4 details the effectiveness of our hybrid QAC model;  X 5.5 provides an analysis of the hybrid QAC model with various personalized QAC scenarios;  X 5.6 zooms in on the effect on QAC ranking via varying the contribution weight in hybrid QAC model.  X 5.7 compares the performance of combined QAC models. Figure 4: Impact of the trade-off  X  in TS-QAC on the accuracy of query popularity prediction for AOL (left) and SnV (right).
Since the true popularity of QAC candidates is unavailable at runtime, QAC ranking models order candidates according to their previously observed popularity [3] or predicted popularity inferred from previous logs [32]. In this section, we evaluate the predic-tion accuracy on query popularity, and measure the impact of these predictions on the quality of QAC rankings in section  X 5.3.
Our time-sensitive prediction method considers both the recent and long-term query frequency as predicted popularity for future. To compare, the predicted query frequencies are aggregated over a past query log (used in [32]) or only contributed over recent trend as described in (4). We denote the former by P k where k is the number of previous days used for averaging ( k  X  X  1 , 3 , 6 } ) and refer to the latter as P trend . We do not take the prediction produced only by pe-riodicity as baseline because of deficiency of periodic queries (21% in AOL and 7% in SnV, see  X 4.2). Table 3 includes the forecast er-ror rates of different methods on datasets. The numbers show that our  X   X  -TS-QAC performs better in terms of MAE and SMAPE than all aggregation-and trend-based baselines, as well as  X  -TS-QAC.
We take a closer look at the error rates produced by various meth-ods. The MAE achieved on the AOL dataset is much smaller than 1 owing to the sparseness of query frequencies. Among the aggre-gated baselines, MAE favors P 6 and SMAPE prefers P 1 on AOL. However, for SnV, P 1 wins the competition on both metrics. The numbers show that with the exception of P 1 on SnV, our predictions are better than all aggregated baselines on both metrics. The dif-ferences is statistically significant on SMAPE but not so according to MAE. Overall, the competitive performance on the AOL dataset can be explained by the fact that compared to the daily query fre-quency used in the SnV dataset, the data here is less sparse and have lower variance.
To answer RQ2 , we manually vary the parameter  X  in (2) to achieve the best prediction accuracy. We show the results in Fig. 4
For AOL in Fig. 4a,  X   X  -TS-QAC performs best in terms of pre-diction accuracy under the setting  X   X  = 0 . 62 achieved by opti-mization as (2), suggesting the predictions emphasize a bit more on recent variations. We repeat our analysis on SnV and summarized the results in Fig. 4b. The results are consistent with the overall AOL numbers. SnV receives an optimal  X   X  = 0 . 83 in our exper-iments. This is due to the fact SnV contains less periodic queries than AOL and hence it favors the predictions from the trend more. Next, we turn to RQ3 and use MPC-based models to generate QAC rankings for each query to compare with our results produced by time-sensitive QAC models, namely,  X  -TS-QAC and  X  QAC. Table 4 contains the evaluation results of different QAC mod-els in terms of MRR. On both two datasets, each prefix is used to Table 5: MRR changes observed by comparing O-MPC-R against  X   X  -H-QAC and  X   X  -TS-QAC against  X   X  -H-QAC, re-spectively, with a query prefix p length of 1-5 characters on AOL and SnV query logs. The symbol  X - X  before MRR changes means  X   X  -H-QAC outperforms the corresponding method. Sta-tistical significance of pairwise differences (O-MPC-R vs.  X  H-QAC and  X   X  -TS-QAC vs.  X   X  -H-QAC) is detected by the t-test ( N / H for  X  = .01, or M / O for  X  = .05).
 # p O-MPC-R  X   X  -TS-QAC O-MPC-R  X   X  -TS-QAC generate 10 QAC rankings, one for each day in the test period. For now, ignore the  X   X  -H-QAC column as we will get to it later. The numbers in the table are averaged across all queries with different length of typed prefix p . All pairwise differences are detected and marked if statistically significant.

We find that  X   X  -TS-QAC outperforms all MPC-based baselines as well as  X  -TS-QAC in terms of MRR, whereas  X  -TS-QAC loses two competitions against O-MPC-R under # p =1 and 2 on AOL. Detailedly,  X   X  -TS-QAC offers a maximal MRR increase against O-MPC-R by up to 3.2% significantly at # p = 4 , and  X  -TS-QAC brings an increase by up to 1.7% over O-MPC-R at # p = 4 on AOL. Specifically on SnV, we see the best performance im-provement over O-MPC-R almost 7.1% brought by  X   X  -TS-QAC and 3.3% by  X  -TS-QAC both when typing a 2-character prefix. The limited improvement of  X  -TS-QAC is probably due to predic-tions on occasional queries as news search, whereas  X   X  -TS-QAC smoothes it with cyclic phenomena for QAC ranking tasks.
Our research question RQ4 aims at examining whether a user X  X  personal query similarity helps generate better QAC rankings. We first give the absolute MRR scores of our  X   X  -H-QAC in Table 4. For convenience, we report the MRR changes produced by compar-ing O-MPC-R against  X   X  -H-QAC and  X   X  -TS-QAC against  X   X  QAC in Table 5. With the appropriate regression model and query similarity measure,  X   X  -H-QAC is able to marginally outperform the baselines on both query logs at each prefix length. However, de-spite the additional overhead of scoring similarity between queries,  X  -H-QAC presents relatively small (  X  2%) improvements over  X  -TS-QAC on AOL. This is due to the fact that no strongly differ-ential features are explored yet for users.

In contrast with AOL,  X   X  -H-QAC on SnV is more sensitive to user X  X  search log with longer prefix, although AOL on most cases does have much lower QAC effectiveness than SnV, see Table 4. In part, this may be caused by following factors. Firstly, AOL contains more queries than SnV queries, although these are spread sparsely over a three-month period. This could suggest that a search en-gine serving more queries is able to generate better completion candidates since it has a larger sample of similar behavior. Sec-ondly, AOL is a more general search log across topics while SnV focuses on multimedia search. Thirdly, there may be underlying demographic differences between users of the two search logs that lead to changes in query distributions, for example, AOL covers more public users while SnV mostly serves for media profession-als. Additionally, the higher performance of SnV as compared to Figure 5: QAC performance in terms of MRR observed for each approach, with a query prefix p length of 1-10 characters for AOL (left) and SnV (right) query logs.
 Table 6: MRR scores of G-QAC and Personalized QAC, as well as MRR changes in bracket produced by comparing G-QAC against  X   X  -H-QAC (in Table 4), and Personalized QAC against  X  -H-QAC, respectively, with a query prefix p length of 1-5 characters tested on AOL and SnV query logs. Statistical sig-nificance of pairwise differences are detected by the t-test ( for H  X  = .01, or M / O for  X  = .05).
 AOL could be a consequence of the difference in user activity as # Qs/Us in Table 2 indicates SnV users submit v 20 times more queries than AOL ones.

Clearly, for both two query logs,  X   X  -H-QAC is considerably more effective with a longer prefix, see Table 4 and 5. To verify this, we examine the MRR metric with a longer prefix of up to 10 characters in Fig. 5. We find that effectiveness converges more quickly on SnV than AOL when the length of prefix increases, probably because QAC is constrained by how much evidence is available, and a slightly longer prefix hugely narrows more possi-ble completion candidates in any case on SnV. To help us answer RQ5 , we compare the performance of  X   X  QAC with two personalized QAC scenarios (G-QAC and Person-alized QAC listed in Table 1) and record the MRR scores of these two methods in Table 6. We also report the MRR changes produced by comparing G-QAC against  X   X  -H-QAC, as well as Personalized QAC against  X   X  -H-QAC in brackets in Table 6.
 We find that  X   X  -H-QAC significantly outperforms G-QAC and Personalized QAC on both AOL and SnV in terms of MRR scores at all cases, which again confirms the above observations in Ta-ble 4. For AOL, Personalized QAC does not work well and its MRR scores are always substantially lower than those of G-QAC, sug-gesting that ranking QAC candidates only according to query sim-ilarity on bigger dataset is not reliable because the number of QAC candidates is beyond control and users often issue new queries. On the contrary, Personalized QAC wins all competition against G-QAC on SnV. This is due to: (i) users of SnV frequently issue similar queries; (ii) users of SnV submit considerable queries; (iii) queries within the same session in SnV must be similar.

Additionally, the MRR improvement produced by  X   X  -H-QAC against G-QAC are still very high, indicating that MPC-ALL in G-QAC may often eliminate useful QAC candidates. This drawback can be further exaggerated owing to the low volume of queries as the relative changes on SnV (around 15%) are larger than those on AOL (around 7%). We conclude that a small dataset suffers more from uncertainty on query popularity for ranking QAC candidates.
To answer RQ6 , we examine the effect on the overall QAC per-formance by varying the contribution weight  X  in (13) in our hybrid QAC model,  X   X  -H-QAC, from 0 to 1 gradually tested on AOL and SnV. See Fig. 6.

For AOL, see Fig. 6a, when the value of  X  used in  X   X  -H-QAC goes up from 0 to 0.4, the performance increases more dramati-cally compared with the results under other settings ( 0 . 4 &lt;  X   X  1 ). When we rank QAC candidates only by query similarity, i.e.,  X  = 0 , the performance is worse than any other result. The MRR value of  X   X  -H-QAC reaches its peak around  X  = 0 . 7 for all cases, which demonstrates the fact that our  X   X  -H-QAC ranking model fa-vors time-sensitive popularity than user X  X  query similarity on AOL. This finding can be further confirmed by averaging MRR values produced under different settings: 0  X   X   X  0 . 5 and 0 . 5  X   X   X  1 for each length of prefix. Obviously, the average MRR of the latter ( 0 . 5  X   X   X  1 ) is higher for all cases.

In contrast to AOL, the optimal  X  on SnV, see Fig. 6b, makes a substantial move to around 0.3, which indicates that QAC ranking on SnV favors user X  X  query similarity a bit more. The discrepancy between the optimal  X  on SnV and the optimal  X  on AOL can per-haps be explained by considering the number of issued queries of each user. Sufficient personal queries result in effective personal-ized QAC on SnV. The MRR of SnV tends to be more sensitive to  X  than that of AOL as it varies dramatically with the increase of  X  , especially under setting 0 . 5  X   X   X  1 . The overall MRR result of  X  -H-QAC is better than that produced by just setting  X  = 0 or  X  = 1 , which is consistent with our findings for AOL. Figure 6: Performance of  X   X  -H-QAC when varying the combi-nation weight  X  with a query prefix p length of 1-5 characters for AOL (left) and SnV (right) query logs.
To answer RQ7 , we compare  X   X  -H G -QAC (in Table 1) with our  X  -H-QAC (MRR scores reported in Table 4). The MRR scores of  X   X  -H G -QAC and the corresponding changes against  X   X  tested on AOL and SnV are recorded in Table 7. We find that  X  H
G -QAC performs better on SnV than on AOL, with higher MRR scores in all cases. However,  X   X  -H-QAC still wins all competitions against  X   X  -H G -QAC as the MRR changes produced by comparing  X  -H G -QAC against  X   X  -H-QAC are always negative.

Another interesting finding is that  X   X  -H G -QAC performs very competitive with  X   X  -H-QAC, especially on SnV, and the differ-ences are limited (MRR changes  X  1%). This appears to be due to the fact that (i)  X   X  -H G -QAC completes personalized QAC on the similar character level but confronts the sparseness problem, and (ii) the number of grams n is artificially fixed, resulting in failure to rank QAC candidates properly.
Most previous work on query auto-completion (QAC) focuses on either time-sensitive maximum likelihood estimation or context-aware similarity. In this paper we have adopted a combination of the two aspects of the QAC problem. We proposed to use time-series analysis to identify the periodicity and trend of query pop-ularity for predicting its future frequency. We assigned an opti-Table 7: MRR scores of  X   X  -H G -QAC, as well as MRR changes produced by comparing  X   X  -H G -QAC against  X  QAC (MRR scores presented in Table 4), with a query prefix p length of 1 X 5 characters tested on AOL and SnV query logs. Statistical significance of pairwise differences (  X   X  -H  X  -H-QAC) determined using the t-test ( N / H for  X  = .01, or for  X  = .05).
 # p MRR MRR changes MRR MRR changes mal time window after learning to each query to find its popular-ity trend, which led to better prediction. To understand a user X  X  personal search task, we extended our time-sensitive QAC method with personalized QAC, which infers the similarity between current requests and preceding queries in his current search session and previous search tasks at a char-level. We verified the effectiveness of our best performer  X   X  -H-QAC on two datasets, showing signifi-cant improvements over various time-sensitive QAC baselines.
As to future work, parallel processing may enhance the effi-ciency of our method and other metrics can be used to evaluate the QAC rankings. Meanwhile, we aim to transfer our approach to other datasets with long-term query logs, which helps us to ben-efit from queries with longer periodicity than we had access to in our current work. Finally, a further possible step is to model personalized temporal patterns for active users, especially profes-sional searchers, requiring a generalization from actual query terms to topics or intents. This might help generate a better QAC ranking.
