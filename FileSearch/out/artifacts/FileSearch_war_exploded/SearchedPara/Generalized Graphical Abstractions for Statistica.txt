 We present a unified graphical model framework based on (Filali and Bilmes, 2006) for statistical ma-chine translation. Graphical models utilize graphical descriptions of probabilistic processes, and are capa-ble of quickly describing a wide variety of different sets of model assumptions. In our approach, either phrases or words can be used as the unit of transla-tion, but as a first step, we have only implemented word-based models since our main goal is to show the viability of our graphical model representation and new software system.

There are several important advantages to a uni-fied probabilistic framework for MT including: (1) the same codebase can be used for training and de-coding without having to implement a separate de-coder for each model; (2) new models can be pro-totyped quickly; (3) combining models (such as in a speech-MT system) is easier when they are en-coded in the same framework; (4) sharing algo-rithms across different disciplines (e.g., the MT and the constraint-satisfaction community) is facilitated. A Graphical Model (GM) represents a factorization of a family of joint probability distributions over a set of random variables using a graph. The graph specifies conditional independence relationships be-tween the variables, and parameters of the model are associated with each variable or group thereof. There are many types of graphical models. For ex-ample, Bayesian networks use an acyclic directed graph and their parameters are conditional probabili-ties of each variable given its parents. Various forms of GM and their conditional independence proper-ties are defined in (Lauritzen, 1996).
 Our graphical representation, which we call Multi-dynamic Bayesian Networks (MDBNs) (Filali and Bilmes, 2006), is a generalization of dynamic Bayesian networks (DBNs) (Dean and Kanazawa, 1988). DBNs are an appropriate representation for sequential (for example, temporal) stochastic pro-cesses, but can be very difficult to apply when de-pendencies have arbitrary time-span and the exis-tence of random variables is contingent on the val-ues of certain variables in the network. In (Filali and Bilmes, 2006), we discuss inference and learn-ing in MDBNs. Here we focus on representation and the evaluation of our new implementation and framework. Below, we summarize key features un-derlying our framework. In section 3, we explain how these features apply to a specific MT model.  X  Multiple DBNs can be represented along with rules for how they are interconnected  X  the rule de-scription lengths are fixed (they do not grow with the length of the DBNs).  X  Switching dependencies (Geiger and Hecker-man, 1996): a variable X is a switching parent of Y if X influences what type of dependencies Y has with respect to its other parents, e.g., an alignment variable in IBM Models 1 and 2 is switching.  X  Switching existence : A variable X is  X  X witch-ing existence X  with respect to variable Y if the value of
X determines whether Y exists. An example is a fertility variable in IBM Models 3 and above.  X  Constraints and aggregation: BN semantics can encode various types of constraints between groups of variables (Pearl, 1988). For example, in the con-struct A  X  B  X  C where B is observed, B can constrain A and C to be unequal. We extend those semantics to support a more efficient evaluation of constraints under some variable order conditions. In this section we present a GM representation for IBM model 3 (Brown et al., 1993) in fig. 1. Model 3 is intricate enough to showcase some of the features of our graphical representation but not as complex as, and thus is easier to describe, than model 4. Our choice of representing IBM models is not because we believe they are state of the art MT models X  although they are still widely used in producing alignments and as features in log-linear models X  but because they provide a good initial testbed for our architecture.

The topmost random variable (RV),  X  , is a hid-den switching existence variable corresponding to the length of the English string. The box abutting  X  includes all the nodes whose existence depends on the value of  X  . In the figure,  X  = 3 , thus resulting in three English words e second-order Markov chain. To each English word e corresponds a conditionally dependent fertility  X  which indicates how many times e in the French string. Each  X  to a set of RVs under it. Given the fertilities (the fig-ure depicts the case  X  each word e tence and are denoted by the tablet  X  of e actual observed French sequence f represented as a shared constraint between all the f ,  X  , and  X  variables which have incoming edges into the observed variable v . v  X  X  conditional probability table is such that it is one only when the associated constraint is satisfied. The variable  X  ing dependency parent with respect to the constraint variable v and determines which f an equality constraint with  X 
In the null word sub-model, the constraint that successive permutation variables be ordered is im-plemented using the observed child w of  X   X  when the constraint is satisfied and zero otherwise.
The bottom variable m is a switching existence node (observed to be 6 in the figure) with cor-responding French word sequence and alignment variables. The French sequence participates in the v constraint described above, while the alignment variables a the fertilities to take their unique allowable values (for the given alignment). Alignments also restrict the domain of permutation variables, constraint variable x . Finally, the domain size of each a forced by the variable u . The dashed edges connect-ing the alignment a variables represent an extension We have developed (in C++) a new entirely self-contained general-purpose MT training/decoding system based on our framework, of which we pro-vide a preliminary evaluation in this section. Al-though the framework is perfectly capable of rep-resenting phrase-based models, we restrict ourselves to word-based models to show the viability of graph-ical models for MT and will consider different trans-lation units in future work. We perform MT ex-periments on a English-French subset of the Eu-roparl corpus used for the ACL 2005 SMT evalu-ations (Koehn and Monz, 2005). We train an En-glish language model on the whole training set us-ing the SRILM toolkit (Stolcke, 2002) and train MT models mainly on a 10k sentence pair sub-set of the ACL training set. We test on the 2000 sentence test set used for the same evaluations. For comparison, we use the MT training program, GIZA++ (Och and Ney, 2003), the phrase-base de-coder, Pharaoh (Koehn et al., 2003), and the word-based decoder, Rewrite (Germann, 2003).

For inference we use a backtracking depth-first search inference method with memoization that ex-tends Value Elimination (Bacchus et al., 2003). The same inference engine is used for both training and decoding. As an admissible heuristic for decod-ing, we compute, for each node V with Conditional Probability Table c , the largest value of c over all possible configurations of V and its parents (Filali and Bilmes, 2006).
 Table 1: BLEU scores on first 500, 1000, 1500, and 2000 sentences (ordered from shortest to longest) of the ACL05 English-French 2000 sentence test set us-ing a 700k sent train set. The last row is our MDBN system X  X  simulation of a M-HMM model.
 Table 1 compares MT performance between (1) Pharaoh (which uses beam search), (2) our system, and (3) Rewrite (hill-climbing). (1) and (2) make HMM model specified using our tool, and neither uses minimum error rate training. (3) uses Model 4 parameters learned using GIZA++. This compari-son is informative because Rewrite is a special pur-pose model 4 decoder and we would expect it to perform at least as well as decoders not written for a specific IBM model. Pharaoh is more general in that it only requires, as input, a lexical table from lored for the translation task. Pharaoh was able to decode the 2000 sentences of the test set in 5000s on a 3.2GHz machine; Rewrite took 84000s, and we allotted 400000s for our engine (200s per sentence). We attribute the difference in speed and BLEU score between our system and Pharaoh to the fact Value Elimination searches in a depth-first fashion over the space of partial configurations of RVs , while Pharaoh expands partial translation hypotheses in a best-first search manner. Thus, Pharaoh can take ad-vantage of knowledge about the MT problem X  X  hy-pothesis space while the GM is agnostic with respect to the structure of the problem X  X omething that is desirable from our perspective since generality is a main concern of ours. Moreover, the MDBN X  X  heuristic and caching of previously explored sub-trees have not yet proven able to defray the cost, associated with depth-first search, of exploring sub-trees that do not contain any  X  X ood X  configurations.
Table 2 shows BLEU scores of different MT mod-els trained using our system. We decode using Pharaoh because the above speed difference in its favor allowed us to run more experiments and fo-cus on the training aspect of different models. M1, M2, M-HMM, M3, and M4 are the standard IBM models. M2d and M-Hd are variants in which the distortion between the French and English po-sitions is used instead of the absolute alignment po-sition. M-Hdd is a second-order M-HMM model (with distortion). M3H (see fig 1) is a variant of model 3 that uses first-order dependencies between alignment variables. M-Hr is another HMM model that uses the relative distortion between the current alignment and the previous one. This is similar to the model implemented by GIZA except we did Table 2: BLEU scores for various models trained using GM and GIZA (when applicable). All models are decoded using Pharaoh. not include the English word class dependency. Fi-nally, model M4H is a simplified model 4, in which only distortions within each tablet are modeled but a Markov dependency is also used between the align-ment variables.

Table 2 also shows BLEU scores obtained by training equivalent IBM models using GIZA and the standard training regimen of initializing higher models with lower ones (we use the same sched-ules for our GM training, but only transfer lexical ta-bles). The main observation is that GIZA-trained M-HMM, M3 and 4 have about 1% better BLEU scores than their corresponding MDBN versions. We at-tribute the difference in M3/4 scores to the fact we use a Viterbi-like training procedure (i.e., we con-sider a single configuration of the hidden variables in EM training) while GIZA uses pegging (Brown et al., 1993) to sum over a set of likely hidden variable configurations in EM.

While these preliminary results do not show im-proved MT performance, nor would we expect them to since they are on simulated IBM models, we find very promising the fact that this general-purpose graphical model-based system produces competitive MT results on a computationally challenging task. We have described a new probabilistic framework for doing statistical machine translation. We have focused so far on word-based translation. In fu-ture work, we intend to implement phrase-based MT models. We also plan to design better approximate inference strategies for training highly connected graphs such as IBM models 3 and 4, and some novel extensions. We are also working on new best-first search generalizations of our depth-first search in-ference to improve decoding time. As there has been increased interest in end-to-end task such as speech translation, dialog systems, and multilingual search, a new challenge is how best to combine the complex components of these systems into one framework. We believe that, in addition to the finite-state trans-ducer approach, a graphical model framework such as ours would be well suited for this scientific and engineering endeavor.

