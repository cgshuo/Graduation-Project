 Roi Blanco  X   X  Alvaro Barreiro
Abstract Recent studies demonstrated that it is possible to reduce Inverted Files (IF) sizes by reassigning the document identifiers of the original collection, as this lowers the distance between the positions of documents related to a single term. Variable-bit encoding schemes can exploit the average gap reduction and decrease the total amount of bits per document pointer. This paper presents an efficient solution to the reassignment problem, which consists in reducing the input data dimensionality using a SVD transformation, as well as considering it a Travelling Salesman Problem (TSP). We also present some efficient solutions based on clustering. Finally, we combine both the TSP and the clustering strategies for reordering the document identifiers. We present experimental tests and performance results in two text
TREC collections, obtaining good compression ratios with low running times, and advance the possibility of obtaining scalable solutions for web collections based on the techniques presented here.

Keywords Document identifier reassignment . SVD . Indexing
Clustering 1. Introduction
Large-scale Information Retrieval (IR) systems need an indexing mechanism for efficient representation of the original document collection, organized in posting lists . Each entry which documents and positions the term appears. In this work we assume a document level granularity, therefore the posting list for term t i is: where f t i stands for the frequency of the term t i (number of documents in which t and d i is the document identifier. As the notation implies, the document identifiers are ordered.
Since this structure requires a large amount of storage space, posting lists usually are compressed. Several studies have addressed the efficient encoding of document identifiers contained in each entry (Witten et al., 1999). Posting lists are stored as a sequence of differ-ences between consecutive document identifiers ( d-gaps ). This method improves compres-sion, as variable-length encoding schemes represent small integers with less bits than large ones. Small d -gaps are more frequent than large ones, so inverted files can be compressed efficiently. Recent work has tried to increase the number of small d -gaps by reordering the document identifiers, hoping that the average length of the coded d -gaps would be lowered.
The reassignment of document identifiers has been addressed by two different strategies: splitting the collection with graph-partitioning or clustering algorithms and obtaining a new order which exploits locality (Blandford and Blelloch, 2002; Silvestri et al., 2004), and consid-ering the task as a graph-layout problem (Shieh et al., 2003). In the work presented in Blanco and Barreiro (2005), the heuristic that considers the problem as a Travelling Salesman Prob-lem (TSP) introduced in Shieh et al. (2003) allowed an efficient solution of the reassignment after reducing the input data dimensionality using a Singular Value Decomposition (SVD) transformation. The TSP-based strategy considers the reassignment of document identifiers as a graph-traversal problem and finds a path according to a global minimum. Although the strategy is not explicitly based on locality, it has obtained very appealing results in practice.
As it is shown later in Section 2, this path is the one that maximizes the bitwise intersection of the document vectors. In Section 6 we give an explanation for the good behaviour of the TSP heuristic for this problem.

The main reason why a SVD reduction is useful in this context, is that the TSP technique needs to compute similarity values between documents for choosing the edges that will connect the nodes in the final path. Thus, SVD computation deals with a necessary efficiency matter given the dimensions of the space of terms by documents, while also being the best dimensionality space.

This paper extends the work presented in Blanco and Barreiro (2005) by reviewing and reimplementing two clustering algorithms previously adapted to the reassignment problem (Silvestri et al., 2004), in order to evaluate the two different approaches within the same ment identifiers is done on the fly , i.e. without the existence of a prior built inverted file. provements after the reassignment of document identifiers with respect to the random order and the identity (original) collection. Then, we present a new technique that combines the two strategies in such a way that the best tradeoff between compression and efficiency is achieved.

The rest of the paper is organized as follows. Section 2 describes the TSP-based solutions in depth. Section 3 shows the way of reducing the dimension of the document similarity matrix by computing its Singular Value Decomposition and how we applied this result to the reassignment problem. Section 4 applies the SVD technique again, but after dividing the original problem into subproblems. We postpone the introduction of the clustering so-lutions and the technique that combines the TSP and clustering until Section 5, because the implementations, experiments and results of each technique are included in their correspond-ing section. The paper continues with the discussion and future work in Section 6, and ends with a conclusions section. 2. The TSP approach to the document identifiers reassignment problem
Given that we will illustrate the use of the dimensionality reduction technique for document reassignment with the TSP-based solutions, we briefly review the work in Shieh et al. (2003). 2.1. The document identifiers reassignment problem as a TSP a single term appearing in the document collection, expressed as a sequence of encoded d -gaps G t ={ g 1 ,..., g f t } . The document reassignment problem tries to find the bijective function f that  X  maps each document identifier into a new identifier in the range [1  X  minimizes the cost in bits of coding the posting lists
Similarity between documents is defined as the number of common terms, and maintained in a similarity matrix Sim , where Sim ij represents the similarity between the document i and the document j .

Shieh et al. (2003) proposed a gap-reduction strategy based in the transformation of the problem into a Travelling Salesman Problem (TSP). The TSP is stated as follows: given a weighted graph G = ( V , E ) where e ( v i ,v j ) is the weight of the edge from v , find a minimal path P ={ v 1 ,v 2 ,...,v n } containing all the vertexes in V , such as if
P ={ v 1 ,v 2 ,...,v n } is another path in G , n i = 2 e (
Considering Sim a weighted adjacency matrix, it is possible to build a Document Simi-larity Graph (DSG) expressing the similarities between documents. This graph can be tra-versed by a gap-reduction strategy based on the similarity factor between documents. The duce the d -gaps in common terms postings. This traversing problem can be transformed into a TSP just by considering the complement of the similarity as the weight in the TSP.
The solution of the TSP is the path that minimizes the sum of the distances between doc-problem. 2.2. Heuristic approximations
The TSP is an NP -complete problem, so some polynomial-time heuristic approximations were modified for the reassignment problem. These algorithms were classified as greedy algorithms and spanning tree algorithms. We tested our low-dimension approximation with the Greedy-NN algorithm.

When describing the heuristic approximation, we consider the TSP as obtaining the path that maximizes the similarity sum between consecutive documents. The Greedy-NN (Nearest
Neighbor) expands the path by adding the closest vertex to the tail of the current path. In each iteration the algorithm adds a new vertex (document), by choosing the most similar vertex to the last one in the path. This approximation is high time consuming. Each vertex is inserted only once in the path P and at iteration i the algorithm does d (the remaining documents) involving the term size t of both documents. Therefore the overall complexity is O ( d 2 t ). 2.3. Implementation considerations
The TSP approximation for the identifier reassignment problem was evaluated in Shieh et al. (2003). The solution demonstrated significant improvements in the compression ratio, although it presented some design challenges and poor performance time and space results.
First, this approach requires a big amount of space. The similarity matrix is symmetric ( Sim ij = Sim ji ) and the elements in the diagonal are not relevant, so it is easy to prove that amount can become unmanageable, so a matrix partitioning technique has to be developed.
Second, building this matrix can be very expensive if it does not fit into memory, as each update has to access the disk twice, involving big delays.

Experimental results were presented for two medium sized collections (FBIS and LATimes in TREC disk 5), to prove the effectiveness of this mechanism. These tests are summarized in Table 1.

It is important to remark that the work in Shieh et al. (2003) provides bar graphs that show an approximated gain of one bit per gap when reassigning with the Greedy-NN for delta and gamma coding. The temporal costs include the process of building the similarity matrix, running the greedy algorithm and recompressing the inverted file. However, the results show that this full TSP approach may be unacceptable for very large collections, as it takes 23 hours and 2.17 GB to process a 475 MB collection. 3. Document identifiers reassignment by dimensionality reduction and the TSP strategy
The TSP strategy achieves very good compression ratios. For this reason, we propose a new approach based on dimensionality reduction in which the TSP algorithm can operate effi-ciently. This technique is based on the application of a SVD transformation to the input data, in order to obtain a new representation of the data structures that allows the reordering of the document identifiers in a dimensionality-reduced space. Once the new ordering is obtained, the posting lists in the inverted file are recompressed, using the document identifiers given by the assignment function. It is worth noting the different nature of this SVD application in Information Retrieval with respect to other typical SVD IR usages, like the LSI retrieval model. 3.1. Singular Value Decomposition
Singular Value Decomposition (SVD) is a well known mathematical technique used in a wide variety of fields. It is used to decompose an arbitrary rectangular matrix into three matrices containing singular vectors and singular values. These matrices show a breakdown of the original relationships into linearly independent factors. The SVD technique is used as the mathematical base of the Latent Semantic Indexing (LSI) IR model (Deerwester et al., 1990). Analytically, we start with X ,a t  X  d matrix of terms and documents. Then, applying the SVD X is decomposed into three matrices:
T and D 0 have orthonormal columns, and S 0 is diagonal and, by convention, s s  X  s jj  X  i  X  j . T of X . However it is possible to obtain a k -ranked approximation of the X original matrix by keeping the k largest values in S 0 and setting the remaining ones to zero, thus obtaining the matrix S with k  X  k dimensions. As S is a diagonal matrix with k non-zero values, the corresponding columns of T 0 and D 0 can be deleted to obtain T , sized t k  X  d , respectively.
 This way we can obtain  X  X , which is a reduced rank k approximation of X : i.e. the matrix which minimizes || X  X   X  X || 2 N where || X ||
The i -th row of DS gives the representation of the document i in the reduced k -space and the similarity matrix ( X )is k -approximated by (  X  X ): where  X  X is the transposed matrix of  X  X and D is the transposed of D .

If D d  X  k ={ z ij } and { s i } is the set of diagonal elements of S , it is easy to prove that { s } set and the d  X  k matrix D , instead of computing and writing the full rank matrix ( X )
The output of the SVD of X ,  X  X has been used in the computation of (
The same result could be obtained by calculating the SVD of ( X ) uniqueness property of SVD (Bartell et al., 1992). Since SVD computes the best rank k approximation, it is proved that the best rank k approximation of ( X ) is obtained starting from X and without the need of computing ( X ). 3.2. SVD in the document reassignment problem
Figure 1describes the system built for testing this approach. The inverted file builder mecha-nism outputs the X data matrix to a SVD module. This module produces the matrices D and S k  X  k that allow the computation of (  X  X ), therefore it is no longer needed to store the similarity matrix ( X ) d  X  d . The reassignment module uses the SVD output matrix to compute the TSP approach described in Section 2.2. As k is a constant factor, we can conclude that the space usage of the algorithm is O ( d ) now, i.e. linear in collection size and not dependent on document size. The output of the TSP reassignment module is used by an inverted file recoding program which exploits the new locality of the documents to enhance the d -gaps compression. Finally, some statical information is taken into account to make suitable com-parisons between compression ratios achieved by the original encoding and those obtained after reassignment.

The summarized steps are detailed next, where SVDGreedyNN is a modified version of the Greedy-NN algorithm that uses the (  X  X ) matrix for computing the similarity measure between vertexes v i and v j .

The main difference in this model is that computing the similarity between two documents d and d j requires k operations ( k  X  1  X  = 0 ( DS ) i  X  ( DS ) ment, making a total of k  X  d for the full matrix. This representation can fit smoothly into memory by adjusting the parameter k and uses considerably less space than the original d matrix. Furthermore, the space usage can be precalculated so suitable scalable algorithms can be easily developed. Considering 32 bits per float (real number), our implementation uses 4  X  k  X  d bytes of main memory.

NN algorithm which was also employed to solve the TSP. The algorithm (Section 2.2) tion of the similarity for every document pair ( d i , d j propose a less time-expensive heuristic, consisting in calculating, after dimensionality re-value. 3.3. Experiments and results
We performed several experiments for testing the low-dimension approach on the two TREC document collections described in Table 1. These collections were not preprocessed, so indexing and reordering did include stop words and terms were not stemmed. The machine used was a 2.5 GHz AMD with 40 GB ATA disk and 1 GB of main memory, running Linux
OS. The original index file was built with MG4J from the University of Milan, a free Java implementation of the indexing and compression techniques described in Witten et al. (1999) and originally implemented in the MG software (http://www.cs.mu.oz.au/mg/). For the SVD module we used SVDLIBC (http://tedlab.mit.edu/  X  dr/SVDLIBC/) , a C library based on the
SVDPACKC library. We wrote the reassignment, recoding and statistical software in Java. It should be pointed out that we needed to modify the MG4J software to output data directly to the SVDLIBC module. Also some modifications were made that allowed us to encode document pointers with interpolative coding.

The first experiment assessed the performance of the system with the Greedy-NN algo-rithm, in terms of average bits per compressed document pointer ( d -gap). The document collections were inverted, the IF was inputted to the SVD module and the program computed the Greedy-NN in the reduced dimension for the reassignment task. After reordering the collection, the inverted file was recompressed. The software measured the average bits per gap in the inverted file, before and after reordering and recompressing, which reflects the amount of compression gained by reordering the document collection. We ran several tests varying the following parameters:  X  the parameter k which reflects the desired dimensionality reduction  X  coding schemes for document pointers: delta coding, gamma coding, golomb coding or interpolative coding (Moffat and Turpin, 2002).
 Best results are obtained considering X as a binary matrix in the reassignment process.
The elements of X represent the presence or absence of a term in a given document. The recompressing module acts over the original index file which contains within-document term frequency and frequency of the term in the collection. Results are given in bits per document gap because it is a measure independent of these indexing options. As stated in Section 3, the memory usage leads to 4  X  d  X  k bytes, concretely 0 . 0 . 503143  X  k MB for the LATimes (for k = 200 less than 101 MB in both collections).
We have experimented with different coding algorithms that had been successfully used for inverted file compression. These algorithms cover some of the different categories for static coding methods (Moffat and Turpin, 2002): two global non-parameterized methods (gamma  X  and delta  X  (Elias, 1975)), one global parameterized (Golomb code (Golomb, 1966)), and one local context-sensitive (interpolative coding (Moffat and Stuiver, 2000)).
Elias delta and gamma codes (Elias, 1975) do not take into account any data from the document collection, as they drive the same representation for an integer x regardless its frequency. For the gamma code the implied probability for a gap of length x is for the case of delta code the probability is
The associated code lengths for x are 1 + 2 log x and 1 + tively. On the other hand, global Golomb coding (Golomb, 1966) makes use of the density of pointers in the inverted file and is dependent on a parameter b . Golomb coding is optimal if the implied probability follows a geometric distribution:
The code length for an integer x is ( x  X  1) / b + logb bits. The b parameter can be chosen so the average number of bits for coding the inverted file is minimized (Gallager and van
Voorhis 1975) and it depends on the geometric distribution. Finally, the interpolative coding (Moffat and Stuiver, 2000) is a context-sensitive model that takes into account the clustering properties of the collection. The behaviour of the interpolative code is nearly as efficient as the Golomb code in the worst case, although it can lead to greater gains of compression when coping with contiguous sets of integers.

Tables 2 and 3 show the results for the different coding schemes. In the rows labeled with each of the coding methods, columns refer to bits per document gap results for: ran-dom reassignment, original document identifiers and reassignment after reducing the dimen-sionality with different k values. Actually, the default starting point for any rearrangement is the pre-existing order, which already includes an element of clustering because of the chronology of the documents. However, in previous studies (Blandford and Blelloch, 2002;
Silvestri et al., 2004), the authors used the random reassignment as a baseline, and for com-those used in retrieval (Dumais, 1994), the low-dimension algorithm operates with gains that produce improvements in bits per gap. As expected, the method behaves better as the k value increases. Also, the tables seem to show a asymptotic behaviour. With k the LATimes collection (FBIS collection) we achieved a 13.65% (8.02%) gain in compres-sion ratio with respect to the original document identifier order with the gamma encoding, 13.24%(8.66%) for the delta encoding, and 11.32% (5.15%) for the interpolative coding.
These values are 17.67% (21.92%), 17.80% (21.10%) and 13.66% (14.58%) respectively for both collections and the three encoding schemes, with respect to a random reassign-ment. Computing the Greedy-NN TSP with the reduced space approximation ( worthy compression ratios in every case. The gains in the FBIS collection are worse than the ones in the LATimes, although starting from a randomized order the result is inverted.
This is the expected behaviour if the FBIS collection exhibits a better original document order. One point to remark upon is that even in the case of interpolative coding, where the starting point is much better, the method is able to produce gains in bit per document gap.
Regarding the compression ratios with Golomb coding, it performed the best for the ran-dom assignment, and it is clear that it does not benefit from the locality of the documents.
The reason for this behaviour is that the Golomb code is not affected by the skewness of the document distribution, as it renders a geometric distribution (Moffat and Turpin, 2002).
However, interpolative coding with reordering is the option that achieves best compression results.

Our tests did not include the computation of the full dimension solution as presented in Shieh et al. (2003), because it requires the development of matrix partition techniques and partial reading/writing, which are the tasks we want to avoid. Shieh et al. (2003) pro-vided bar graph results for gamma and delta encoding in the LATimes and FBIS collec-tions. However, exact compression values depend on the indexing software and particu-lar indexing options. This information is not explicitly provided, thus it is not possible to make exact comparisons between their published full-dimension results and the k -dimension solutions.

The last two rows of Tables 2 and 3 are devoted to running times. Time measurement is divided in three parts: inverted file construction, SVD running time and reordering and recompressing time. Times are given for delta coding only, as the tables illustrate the time variation within the k parameter. The time variance due to k , is only dependent on SVD compressing time is about 16 seconds for both collections). As the system was built upon different modules, the different software pieces employ a lot of temporal I/O transfer time, the FBIS collection and 6 03 for the LATimes collection and it is not shown in the ta-bles. Although the SVD software performs well for the collections and k values used, the
TSP greedy algorithm running time still rises to high values. Hence, we conclude that it is possible to achieve good compression ratios with reasonable time performance with our as the experiments are aimed at clarifying if the different reordering variants give better bits per gap values, along with their respective temporal cost, without focusing on coding methods. 4. The c-blocks algorithm
In order to further exploit the advantages of the TSP strategy and the dimensionality reduction, we developed a simple new algorithm based on the division of the original problem in c subproblems, hereinafter c -GreedyNN. Later on, we will be able to compare the performance between this algorithm and others based on collection partitioning.

The c -GreedyNN algorithm operates as follows: first, it divides the DS matrix (which represents the document similarities in the k space) in c blocks of [ d
Then, each block is reordered by running the greedy algorithm. Finally, a block order is decided by running another greedy with c documents, each one selected from different blocks. For a simpler explanation we consider d an exact multiple of c . Analytically, the
Greedy-NN after dimensionality reduction does d comparisons to select the first document, and
The new approach chooses c block-representatives and then performs c greedy runs with d documents, resulting in d + c ( d c ( d c  X  1) 2 ) = d 2 ( operations is reduced in a 1 / c factor. Experimental results with different values of the number of blocks c are presented in Tables 4 and 5.

Results are provided for the LATimes and FBIS collections, k
Tables 4 and 5 also show that the compression factor increases as the number of blocks decreases, with a goal value of 6 . 29 (5 . 80) for the LATimes (FBIS) collection, which is the value of considering the matrix as one single block.

The algorithm is efficient in running time, as expected from the analytical form, and it gives acceptable compression values. We ran all the experiments with k to drive the best performance results as well as the slowest reordering times, so lowering those times while keeping good compression ratios is a challenge for the c-blocks algorithm.
The method enhances the original compression ratio 7 . 25 (6 collections and delta coding. These improvements are higher when comparing to the randomly ordered collection compression ratio 7 . 65 (7 . 35). The tradeoff between the final compression achieved and the time usage, can be parameterized by selecting the c and k values. 5. Clustering strategies 5.1. Clustering solutions to the document identifiers reassignment problem
Recent work addressed the document identifiers reassignment problem with clustering tech-niques. The main idea is to enhance the locality in the collection by assigning similar docu-ments to the same cluster, and hoping that the d -gaps are lowered if documents on the same cluster are assigned close identifiers. For this particular case, two families of algorithms were developed to compute an efficient document assignment: the top-down assignment and the bottom-up assignment . The top-down assignment schemes start with the whole collection and recursively split it into sub-collections, inserting similar documents into the same sub-collections. After this phase, the algorithm merges the sub-collections obtaining a single and ordered group of documents, which is used to compute the assigning order. Bottom-up schemes start from a set of documents, extracting disjoint sequences containing similar doc-uments. Each sequence is ordered, and the final assignment is computed by considering an arbitrary order between sequences.

The first top-down algorithm was introduced by Blandford and Blelloch (2002), who developed a technique that improved the compression ratio in about 14% in TREC text collections. The technique employs a similarity graph as described in Section 2.1, and operates in three different phases. The first phase constructs the document-document similarity graph from the original inverted file. The second part of the algorithm calls to a graph partitioning package which implements the Metis (Karypis and Kumar, 1995) algorithm for splitting rotations to the clustered graph outputted by the second part in order to optimize the obtained order. The final assignment of the document identifiers is obtained by simply depth-first constructing a full similarity graph is O ( n 2 ), so the raw technique may not be suitable for very large collections. Nevertheless, the efficiency of the algorithm can be controlled by two parameters:  X  and  X  . The first parameter,  X  , acts as a threshold for discarding high-frequency terms, i.e., if a term t i has a number of document occurrences construction of the similarity graph. Actually the algorithm works with a sample of the full similarity graph. The parameter  X  stands for how aggressively the algorithm sub-samples the data: if the index size is n it extracts one element out of n technique may lead to a tradeoff between efficiency and time and memory usage.
The work in Silvestri et al. (2004) proposes a different approach by assigning the document identifiers on the fly during the inversion of the text collection. This is done by calculating a transactional representation form of the documents, which stores for each document d a set of 4-byte integers representing the MD5 Message-Digest (Rivest, 1992) of each term presented and evaluated, as well as two bottom-up algorithms: the k-means and k-scan .
These techniques were tested in the Google Programming Contest collection. As the whole collection is ordered while indexing, the baseline chosen for measuring the performance of these algorithms was a randomized collection. Our previous results with the FBIS and
LATimes presented in Section 3.3, showed that randomly ordering the collection always yields worse compression ratios than those obtained with the natural order , i.e. leaving the collection with its original numbering of documents. This motivated us to reimplement these algorithms, but in a scenario of reassignment of document identifiers, with the existence of an inverted file, rather than performing the assignment on the fly.

In our implementation of the clustering algorithms the construction of the inverted file makes unnecessary the transactional hashing of the terms. In order to improve the computa-tional times of the document-to-document similarities, we worked not only with the inverted file, but also with the direct file. This structure can be seen as an inverted file where the index keys are the documents, and the posting lists contain term identifiers. It is worth noting that this direct file is also compressed with the techniques used to compress the inverted file. 5.2. Bisecting and k -scan
In this section, we adapt the bisecting and k -scan algorithms from Silvestri et al. (2004) to the reassignment scenario. The bisecting algorithm belongs to the top-down family, and operates as follows: first, it randomly selects two documents as the centres of mass of a two-partition of the collection. Next, it assigns every document in the input data to one of the two partitions, according to the similarity between the document and the centre of mass.
The technique keeps these two partitions equally sized, so many documents at the end of the input may fall into an undesired cluster. This partitioning scheme is called recursively until the input partition only contains one element. The merging phase is done by comparing the borders of the two partitions. If the similarity between the document at the right border of cluster D and document at the left border of cluster D is less or equal than the similarity between the document at the right border of cluster D and document at the left border of cluster D , in the final order cluster D must precede D . Otherwise D must precede
D . In the pseudo-code,  X  represents the concatenation of two sets containing document identifiers.

The k -scan is a simplified version of the popular k -means clustering algorithm. The algo-rithm sorts the elements of the collection by descending document length, and chooses the largest unselected document as the cluster representative. Then it assigns the times.

The work in Silvestri et al. (2004) shows that the space storage and the computational cost of the bisecting algorithm are both superlineal O ( | D |
For k -scan, the space occupied is lineal O ( | S || D | ) and the complexity is O ( is the average document length. 5.3. Experiments and results
The experiments were designed to assess the performance of the clustering techniques and compare them to the TSP-based approaches, measuring the improvements in compression ratios with respect to both the original and randomized collections. The similarities between documents were measured with the Jaccard distance, repeating the conditions described in we repeated the experiments measuring the similarities between documents with the inner product in the reduced dimensionality space. In these cases, the k parameter in the SVD transformation was fixed to 200, as this value proved to be worthy (Sections 3.3 and 4).
Hereinafter, the k parameter appearing in the tables refers to the number of scans of D in the k -scan algorithm. In this set of experiments the compression of the posting lists was done with delta coding.
The bisecting algorithm achieves different gains in bits per gap in each run, due to the random centre selection for the different executions, although they are stable around a certain value. In 10 runs of the algorithm, for the LATimes collection the values obtained fall in the range [6 . 82 , 6 . 87] for the Jaccard distance and [7 . the worse values in these ranges to the original (random) collection order 7.25 (7.65), the bisecting method obtains a gain in compression ratio of 5.24%(10.19%) for the Jaccard Distance and  X  0.97% (4.30%) for the inner product.

The main point to consider is that the gains with respect to the natural order are very small, or they may not exist. This observation was ratified by the results of the bisecting algorithm in the FBIS collection, which we skip here. Comments on the difference between the results obtained with the two different measures of similarity for every experiment can be found at the end of this section.

Table 6(7) shows the final compression results measured in bits per gap for the LATimes (FBIS) collection for the k -scan algorithm. The first row presents the data obtained measuring the similarity with inner product and 200 dimensions in the reduced space, which are exactly the same conditions as those of the experiments with the c-blocks (Section 4) and the bisecting algorithms. The second row shows the results with the Jaccard distance. The k parameter stands for the number of scans of the algorithm. Overall, the results give good compression values for a large enough k value (1000). However, considering the variation of k , there is a point where the bits per document gap stop decreasing, and slightly increase until reaching the limit value where k =| D | , which is just ordering the collection by document length.
It is remarkable that the best values obtained with the k -scan algorithm in the LATimes collection (6.60) and the values given by the c-blocks algorithm are similar (6.68). This is not true for the FBIS collection, where the c-blocks (5.98) algorithm obtains a better value than the k -scan (6.23). In terms of gain with respect to the original compression ratio (6.35) this would be 1.9% vs 5.9%. These comparisons are fair, as the metric used by the two algorithms in this measure is exactly the same (inner product in the k 5.4. TSP and clustering combination
Finally, we present a technique that combines the best of the previous approaches: the real performance in bits per gap of the TSP and the efficiency in time consumption of clustering.
The basic idea is the same as in the c-blocks algorithm (see Section 4), namely split the from several hours to just minutes. The difference lies in the way the blocks are chosen; instead of driving a raw partition of equally-sized blocks, we select each sub-collection with the k -scan algorithm. This way, it is expected that each partition holds similar documents, so the TSP would improve an already good result.

Tables 8 and 9 shows the results obtained for the LATimes and FBIS collection for this technique, respectively. The performance of the algorithm is approximately the same as running a Greedy-NN (TSP for the whole collection) with a 200 dimension reduced space (the best results obtained so far). One point to consider is that this technique obtained better perform well when the collection brings a natural order with a good compression ratio, which is harder to improve with any algorithm, but even more with the clustering approaches tested before. Also, the reordering times for the values of k shown are in the order of the c-blocks, because the partitions are size balanced. Of course, this also applies to the algorithms described in the previous section, although this particular technique performs a reorder of each sub-collection in addition to compute the clusters.

It is possible to notice that the Jaccard measure performs better than the inner product in the bisecting and k -scan + TSP algorithms, although the inner product behaves slightly better for the k -scan, especially with low values of k . The problem with the Jaccard distance is that it is an undefined measure in the SVD reduced space, because it is related to the intersection of the original terms. However, the results with the inner product are useful for comparing fairly the clustering and c-blocks techniques. 6. Discussion and future work strategy. This is supported by the implementations, tests and comparisons with clustering algorithms and in particular by the combination of the two main techniques. As a summary of the results presented throughout the paper, Figs. 2 and 3 show some of the results obtained by the algorithms described here. The plots give some details on the effectiveness and efficiency tradeoff that can be achieved with the TSP and cluster-based combinations. The x -axis shows reordering and recompressing time in a logarithmic scale, and the y -axis represents the bits per gap achieved for delta coding. Each line in the plot stands for a different reordering technique, namely TSP (Section 2), c-blocks (Section 4), k -scan (Section 5.2), bisecting (Section 5.2) and the TSP and clustering combination using k -scan (Section 5.4).
For example, it is possible to achieve reduction in the compression ratios of 13.24% (6.29 bits per gap) (LATimes, delta coding) with a time cost of more than 8 hours driving a TSP algorithm only, while the TSP + k -scan combination gives a ratio of 12.55% (6.34 bits per gap) with much less computational effort (in the order of 10 min).

Despite of the fact that the TSP-based technique performs better than other approaches and that it can be adapted to other situations to fit time consumption and memory usage (SVD reduction, collection partitioning, and combination with clustering) it does not necessarily give the optimal solution to the problem of reassignment of document identifiers.
In order to find better solutions than those provided by the TSP strategy, it will be necessary to discover heuristics that take into account the exact cost function to be minimized. This minimization must allow for the coding scheme, as the aim is to minimize the function where  X  is the reassignment bijective function that maps each document identifier in the range { 1 ... d } to a new one in the same interval, and length( x ) gives the total number of bits needed for coding an integer x . Most static codes represent an integer x with O (log( x )) bits. In order to obtain good compression ratios, we should minimize the d -gap products.
This way, the log of the products and the sum of the logs would be minimal, with practical TSP, although producing good results, is only a strategy to address the document identifier reassignment problem. However, it is possible an explanation of why it performs well in reducing the d -gap log.

Consider the distance matrix L d  X  d where each l ij measures the distance between docu-ments i and j . Considering the documents as sets of terms, the distance can be measured as | ( d  X  d j ) \ ( d i  X  d j ) | . The traversal found by the exact solution of the TSP minimizes the
B , where a one the position b ij represents an occurrence of the term i in document j .For each row of this binary matrix, each time a 0 X 1 or 1 X 0 switch appears, sumdist ( by one, so the order that minimizes sumdist (  X  ) is the one which minimizes this number of switches. This would maximize the bitwise intersection of document vectors, but it does not imply minimizing the average d -gap logarithm sum. However, this ensures that the final configuration of the binary matrix has a big number of consecutive 1s in each row, so it is reasonable to expect a low average logarithm d -gap sum (as the logarithm does not penalize bigger d -gaps as much as it benefits from smaller ones).

It is possible to trace a new line of active research, coming up with new heuristics based solution achieves with respect to the optimum. This also holds for TSP solutions: as long as this algorithm in the reduced dimension space performs well, we may pursue a formal characterization to the distance of the optimal solution reached, with this sort of heuristic solutions.

The combination of clustering and TSP would also hold the same scalability problem as the TSP-only approaches described in Shieh et al. (2003) in very large collections. The combination of TSP, SVD and clustering may be suitable for these cases, and it would be interesting to measure the gains that these techniques are able to achieve, as the clustering gives good results but they become higher by applying the TSP in each cluster. Moreover, this is especially indicated for collection partitioning techniques that may produce unbalanced would probably be softened with the SVD, so the TSP technique may run with acceptable times.
 mensionality reduction allows efficient implementations of the TSP-based approaches, but when dealing with web scalability, when several Gigabytes or even Terabytes are involved, it becomes necessary to come up with more efficient SVD implementations, see for exam-ple Kokiopoulou and Saad (2004), or other different matrix transformations, with similar properties as the SVD, but more efficient in computing time. 7. Conclusions
We presented a smart approximation for the document identifier reassignment problem by using a previous dimensionality reduction with SVD. The results presented allow to report time-efficient methods that yield good inverted file reduction gains. Concretely, we imple-mented the TSP Greedy-NN approach in the reduced dimension space and one variant, that applies this solution to sub-collections of the original data, reordering them next. We also presented an implementation of two clustering techniques to the problem of reassignment and evaluated and compared them with TSP-based reordering. Finally, in the discussion theoretical aspects and scalability issues of the proposed work, as well as future research lines.
 References http://tedlab.mit.edu/  X  dr/SVDLIBC/ SVDLIBC.
