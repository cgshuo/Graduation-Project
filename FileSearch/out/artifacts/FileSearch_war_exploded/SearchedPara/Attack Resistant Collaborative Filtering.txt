 The widespread deployment of recommender systems has lead to user feedback of varying quality. While some users faithfully express their true opinion, many provide noisy r at-ings which can be detrimental to the quality of the gener-ated recommendations. The presence of noise can violate modeling assumptions and may thus lead to instabilities in estimation and prediction. Even worse, malicious users can deliberately insert attack profiles in an attempt to bias the recommender system to their benefit.

While previous research has attempted to study the ro-bustness of various existing Collaborative Filtering (CF) ap-proaches, this remains an unsolved problem. Approaches such as Neighbor Selection algorithms, Association Rules and Robust Matrix Factorization have produced unsatisfac-tory results. This work describes a new collaborative al-gorithm based on SVD which is accurate as well as highly stable to shilling. This algorithm exploits previously est ab-lished SVD based shilling detection algorithms, and com-bines it with SVD based-CF. Experimental results show a much diminished effect of all kinds of shilling attacks. This work also offers significant improvement over previous Ro-bust Collaborative Filtering frameworks.
 H.3 [ Information Storage And Retrieval ]: Information Search and Retrieval; K.4.4 [ Computers and Society ]: Electronic Commerce  X  Security Collaborative Filtering, Shilling, SVD, Recommendation A l-gorithm Algorithms, Security
Personalization for large scale systems is a well researche d topic in Computer Science with several successful algorith ms and improvements over past years. While early algorithms exploited similarity in small groups amongst a large popula -tion of users, later algorithms made use of advanced statist i-cal models. Improvement in accuracy was the main objective of previous research, though research has been conducted to incorporate aspects like Trust and Privacy. One important issue with Collaborative Filtering (CF) to emerge recently is the vulnerability towards well designed attacks. Such at -tacks require a group of users to collude and insert maliciou s ratings on chosen items; this is aimed at manipulating a rec-ommender system to recommend a chosen item more/less frequently than normally. Such attacks have been called shilling attacks [9] or profile injections attacks [2]. While recent algorithms [2, 10, 11, 21] are successful in identify ing shilling attacks in collaborative filtering, it is desirabl e to de-velop algorithms which are robust to shillers from the groun d up. Previous work [21] has used a probabilistic model based on Singular value Decomposition (SVD) to detect shillers, however the detection rate is low, and the algorithm is inef-fective against average attacks.

Recent work [11] introduced a detection algorithm based on Principal Component Analysis (and hence SVD) which was very accurate against a broader range of attacks. Our aim in this paper is to built detection into the CF in a computationally effective manner. A robust collaborative filtering algorithm would provide protection from insertio n of random noise as well as attack profiles injected into the system without any explicit input or tuning. Recent work concluded that noise resistant statistical methods using r o-bust regression are not completely effective for robustifyi ng collaborative filtering [12].

The contribution of this paper is a robust CF algorithm which is stable against moderate shilling attacks on large datasets. Our proposed algorithm leverages the accuracy of PCA-based attack detection [11] while preserving the pre-dictive accuracy of SVD, which has also been successfully exploited previously [19, 21].
Collaborative Filtering (CF) [8] is one of the most pop-ular and successful filtering techniques that has been used to date. It is used in a setting where users have a choice of a number of items (say, a book store) and can rate items that they know about. Collaborative Filtering helps users to make choices based on the opinions of other similar users in a system and find relevant items that they may not have explored so far. The basic idea employed is that users who agree with each other on some items based on their ratings are likely to agree or disagree on future items.

Collaborative filtering algorithms have been classified into two general categories, commonly referred to as memory-based and model-based algorithms. Memory-based algo-rithms are the more prevalent of the two categories and use all available data in order to make a prediction for the se-lected user. Memory based CF algorithms retain all relevant data in memory and compute the required prediction on de-mand in real. Model-based algorithms operate differently by abstracting from the observed data and creating a statisti-cal model of observed data. This model is learnt based on known ratings and is subsequently used in the recommenda-tion process. The CF algorithm presented in this paper is a model-based algorithm as well.
Collaborative Filtering systems are essentially social sys-tems which base their recommendation on the judgment of a large number of people. Like other social systems, they are also vulnerable to manipulation by malicious social ele-ments. In a well knonw incident, a loosely organized group managed to trick the Amazon recommender into recommend to some readers of the book Six Steps to a Spiritual Life (written by the evangelist Pat Robertson), a book for gay
A lot of web-enabled systems provide free access to users via a simple registration process. This can be exploited by attackers to create multiple identities for the same system and insert ratings in a manner that affect the robustness of a system or algorithm, as has been studied in recent work [9, 13]. Shilling attacks add a few user profiles which need to be identifiedandprotectedaga inst.Sh illi ngattackscanbeclas-sified into two basic categories: inserting malicious profiles which rate a particular item highly are called push attacks, whil einsertingmali ciousprofilesaimedatdowngradingthe popularity of an item are called nuke attacks [13]. Various attack strategies were then invented; these include [2]: 1. Random attacks , where a subset of items is rated ran-2. Average attacks , where a subset of items is rated ran-3. Bandwagon attacks , where a subset of items is rated Random and Bandwagon attacks are low-knowledge attacks requiring information only about some popular items and overall vote statistics. Average attacks require more infor-mation , and have been shown to be near optimal [10] in im-pact. They have also been observedly difficult to detect [21].
The strength of shilling attacks is specified using two met-rics: filler size and attack size . Filler size is the set of items which are voted for in the attacker profile, usually measured in %. Attack size refers to the number of shilling profiles inserted into user data. The impact of the attacks is mea-sured by the increase in the number of users to whom an attacked item is recommended. Generally, average attacks are strogner than random or bandwagon attacks. 1 Story at http://news.com.com/2100-1023-976435.html . 2 Note that Gaussian distributions N  X , X  have been used for generating the random votes rather than the uniform ran-dom distribution
Recent research in this area aimed at detecting algorithms for profile injection attacks. The earliest shiling detection al-gorithm was invented by Chirita et al. [3] and exploited fea-tures of spam profiles. While this algorithm was successful in detecting shilling attacks with dense attacker profiles, it was unsuccessful against attacks, which are small in size or have high sparsity. Mobasher et al. [2] compare their feature-based classification algorithm which performs significantly better than the Chirita algorithm by taking more features into account. The Mobasher et al. [2] algorithm trains a clas-sifier given enough example spam and authentic profiles and is fairly accurate in detecting spam attacks of varying sizes and density. However, as a supervised approach, it needs a large number of examples, and can detect only profiles simi-lar to the examples profiles. Secondly, both algorithms per-form badly when the spam profiles are obfuscated. Adding noise, shifting targets, or shifting all user ratings differently makes the attack profiles more difficult to detect for exist-ing feature based detection algorithms. Williams et al. [20] discusses these obfuscation strategies and their effect on de-tection precision. O X  X ahony et al. [14] have taken up a more principled approach using signal processing theory to detect natural and malicious noise; however, the accuracy remains low (15 X 25%).

Recent work [10, 11] provide highly accurate algorithm called VarSelect for detecting shilling attacks. The algo-rithm exploites the group effect : a property of shillers being effective when working in groups. Since shillers want to maximize their effect, they need to work together. A result of this is that attack profiles are similar to one another; this property can be exploited by methods based on dimension-ality reduction to detect spam with high accuracy. Clearly, detection procedures can be applied only sparingly due to their highly computational and batch nature. Our approach in this paper uses the insight gained in the design of detec-tion procedures to create a robust Collaborative Filtering Algorithm; VarSelect is an important step in our proposed algorithm.
The earliest work on Robust CF was a modified k X  X N al-gorithm with heuristics for neighbor selection [15] where the concept of profile utility was introduced. Association rules have also been used to add robustness to CF at the cost of decreased accuracy and coverage [16], though at the cost of decreasd accuracy, and much lower coverage. Recent work [12] has investigated the effectiveness of robust statistics in protecting against shilling attacks with an algorithm called Robust Matrix Factorization (RMF). This approach simi-lar in spirit to SVD but is more stable to noisy data. The uniqueness in this algorithm is the usage of M-estimators, which bounds the effect of outliers and noisy data. Experi-mental results have shown that application of Robust statis-tics adds significant stability (10-30% lower prediction shift); however the algorithm is not completely immune in the face of attacks.
 Still, RMF adds significant stability as compared to other CF methods like PLSA and k-NN. The major positive out-come of this work is that RMF outperforms all other algo-rithms based on latent semantics (PLSA, SVD) in prediction accuracy.

Clearly, RMF is only a partial success; however it allows us to understand that attack profiles are quite homogeneous. Detecting such profiles is clearly possible (see [11]), since they exhibit characteristic properties; notably, the similari-ties between attack profiles are much higher than with nor-mal users, due to the way these profiles are constructed. Ideally, the detection phase should be built into the recom-mendation algorithm; detected attack profiles can then be removed and the recommendation phase can begin. How-ever, there are two major problems: firstly, the detection phase is expensive and may need to be repeated periodi-cally, resulting in high computational costs. Secondly, th e accuracy of detection methods is not 100%, thus removing false positives will adversly affect the user experiance.
In the next section we explore SVD in detail since the algorithm describe in this work depends heavily on SVD based methods.
Our strategy for robust collaboratve filtering is to exploit previously developed detection strategies in a computatio n-aly SVD stands for Singular Value Decomposition; it is a method of factorizing a matrix into two orthonormal ma-trices and a diagonal matrix. SVD has become an impor-tant linear algebra procedure over the last 2 decades due to its extensive application in Information Retrieval and Dat a mining. It has been used for Latent Semantic Analysis [4] and Collaborative Filtering [18] with much success. Since SVD is fundamental to the algorithm in this paper, we ex-plore SVD in detail. Notably, we explain a recent iterative algorithm for SVD using Generalized Hebbian Learning [6]. Further, we briefly explain the Robust Matrix Factorization algorithm described in [12] which is also based on SVD and is robust variant of SVD. Finally, we explain our proposed VarSelect SVD variant asa robust CF solution.
SVD is a more general form of Eigen value decomposition SVD factorizes a rectangular n  X  m matrix D as follows where U , V are unitary normal matrices and  X  is a diagonal matrix of size rank( D )  X  min( m, n ), where rank( D ) is the rank of the matrix D . Moreover, the entries on the diagonal of  X  are in non-increasing order such that  X  i  X   X  j for all i &lt; j . Note that we may chose to set all singular values  X  = 0, i &gt; k for some k  X  rank( D ) (say k = 10), leading to a low-rank approximation D k of the matrix D .
 where U ,  X  , V are now n  X  k , k  X  k and m  X  k dimensional matrices, respectively. It can be shown that D k is the min-imizer of k D  X   X  D k 2 for all matrices  X  D of rank less or equal to k .
 SVD for Collaborative Filtering: Applications of SVD to Collaborative Filtering assume the representation of user-item ratings by such a n  X  m matrix D . Here each of the n users corresponds a row in the matrix, whereas the m items are represented as columns, with D ij representing the vote of user i on item j . The application of SVD to D leads to a low rank estimate  X  D , which generalizes the ob-served data, since it may result in non-zero values  X  D il for user-item pairs ( i, l ) that are unrated (often set to zero in D , i.e. D il = 0). U is an Unitary normal Matrix and  X  is a diagonal matrix containing eigenvalues of A
Typically, user X  X tem matrices are very sparse (  X  5% non-zero entries). Initial applications of SVD to CF (c.f. [18]) compensated for sparsity by replacing the missing values by overall mean. This approach, though more successful than previous CF approaches, is highly biased towards the used means. In addition, the lack of sparsity implies a larger com -putational problem to solve. In the last decade, there has been significant research on SVD for large and sparse ma-trices e.g. PROPACK 4 and SVDPACK 5 . However, these approaches do not treat missing values in a principled fash-ion, either treating them as zeros, or doign mean imputa-tion. [21] discusses the use of the Expectation Maximiza-tion [5] procedure to approximate SVD optimally in the log-likelihood sense. However, their approach requires a SVD to be performed at each EM iteration, which is computation-ally very expensive and not practical for large matrices wit h millions of rows and columns.

A recent algorithm by Gorrell [6] proposed a new approach to computing SVD for virtually unbounded matrices. This method is based on the Generalized Hebbian Algorithm [17] and calculates SVD by iterating through only observed val-ues. The method has been found to be highly accurate for CF and scales easily to the NetFlix dataset with 100 million votes. Below we describe this approach in detail.
Gorrell [6] extends an existing method for eigen decompo-sition to non-symmetric matrices of arbitrary sizes. In her approach (referred to now onwards as SVD-GHA), multiple eigen-values/vectors can be computed with this simple ob-servation: the second eigen-value/vector of a matrix can be calculated by removing the projection of the previous eigen -pair. This means that if u 1 and v 1 are the first singular vectors corresponding to the largest eigenvalue  X  1 , then a matrix D rem can be defined as follows The first eigen-value of D rem is exactly the second eigen-value of D . This observation can be generalized to compute the first k eigenvectors/eigenvalues of a large sparse matrix. This method had been referred to as Hotelling X  X  Deflation Method [7].

Mathematically the Hebbian learning rule can be ex-pressed as follows: suppose u and v are the first eigenvectors being trained for Matrix D , and D ij = x . Further, suppose the eigenvalue  X  is absorbed into the singular vectors u and v to yield  X  u and  X  v . The estimate for x would then be The total error is a sum of residuals r ( x ) The above error can be minimized by Gradient Descent, following the derivative of the estimate at every observed matrix entry: where  X  is the learning rate. It can be shown that with the suitable choice of decaying learning rates, the repeate d iteration of the above equations converges to the required singular vectors has been learnt, their projection can be re -moved ( x  X  x  X  u 1 v 1 ) and the next pair can be learnt. Webb [19] modified this basic algorithm by introducing weight de-cay regularization and range clipping. Several contestant s of the NetFlix Prize use modified versions of the above al-gorithm.
Robust regression problems have been studied in a linear setting where observables Y and inputs X are known and Y is assumed to be noisy. Robust Matrix Factorization (RMF) is algorithm which performs a robust SVD for CF using an alternating fitting scheme [12]. The core idea is the use of bounded cost-functions, which limit the effect of outliers. There is an entire body of work on such bounded functions which are effective against noise; these functions are calle d Maximum Likelihood estimators or M-estimators . Mehta et al. chose the Huber M-estimator which is defined as follows:
Armed with a robust estimator, we would like the perform the following Matrix factorization: assume we want to find the rank X 1 factors G , H as for data D . such that argmin
The above optimization is solved using Iteratively Re-weighted Least Squares (see [12] for details). First H is minimized with a fixed G ; then H is fixed and G is mini-mized. This repeated till both factors converge. Given the rank X 1 estimates G , H , higher rank estimates can be easily computed in a similar manner to SVD-GHA.

Experiments show that Robust Matrix factorization algo-rithm also performs well in the face of moderate attacks. Clearly, the effect of shilling is low at small attack sizes, a s the majority opinion is given more importance. However, once the number of votes by shillers are more than actual users, RMF starts treating the shillers X  view as the majorit y opinion. Mehta et al. also show that RMF is more tolerant to shilling and model deviations than SVD and pLSA: the prediction accuracy of RMF is higher than any other method ; this trend continues even in the face of attacks.However fo r larger attacks, RMF is clearly inadequate at a robust CF al-gorithm. In the next section, we show how the RMF and SVD frameworks can be further robustified to yield our de-sired robust CF algorithm. VarSelect [11] is a variable selection algorithm based on PCA for detecting attack profiles. Shilling profiles tend to be highly correlated, which is a result of the colluded natur e of shilling attacks. It is known that for multivariate data, highly correlated variables add very little information, a nd thus are eliminated by dimensionality reduction methods. VarSelect uses Principal Component Analysis to find which users add least information, and produces a ranking of users in order of utility. Experiments have shown that shillers ar e found with high precision at the top of these rankings. converges to a local minimum.

VarSelect SVD : We first describe the broad framework for our proposed algorithm. SVD and PCA are closely re-lated since PCA can be achieved via SVD. In essence, PCA seeks to reduce the dimensionality of the data by finding a few orthogonal linear combinations (called the Principal Components ) of the original variables with the largest vari-ance. A principal component is a linear combination of the variables and there are as many PCs as the number of the original variables. The first principal component s 1 = w where the p-dimensional coefficient vector w 1 = ( w 1 , ..., w solves Principally, PCA is equivalent to performing an eigen de-composition of the covariance matrix of the original data. Since we want to combine VarSelect with Collaborative Fil-tering, SVD provides the required framework. The frame-work we devise would support two phases: detection (fol-lowed by removal of profiles/votes), and recommendation model building. For efficiency, it is required that these two phases can share computational steps. Since the de-tection may not be perfect, no user profiles should be com-pletely deleted and even suspected attackers should be able to receive recommendations. Further, the entire procedure should be unsupervised, i.e. no further input should be re-quired after the detection phase has been performed (e.g. thresholding how many shillers are there in the system).
An important observation we make here is that Principal components can be computed directly from the data ma-trix. We also observe that for X n  X  m , for n &lt; m , the first n As previously established, the Principal components of X are given by S = U T X . Thus, calculating the covariance is unnecessary; we can compute the SVD of X to get the loading matrix U . This saves a significant computational effort as Eigen-decomposition of large covariance matrices is very expensive. Note that PCA requires X to be zero-mean.
Notice also, that the VarSelect procedure does not re-quire all eigenvector-eigenvalue pairs to be computed. Our experiments have also shown that the first 3 X 5 Principal components suffice to detect attack profiles reliably. Thus a complete SVD is not required: instead, partial eigen-decomposition can be performed. Such routines are avail-able as svds and eigs in MATLAB and Octave, using the VarSelect procedure) by applying svds on z-scores of the user data matrix.
 As noted earlier, Varselect gives a ranked list of user score s, with the lowest scores corresponding to attack profiles with high probability. In previous work, the authors have artifi-cially inserted r attack profiles, and have tested for exactly top-r users in the ranked list of PCA scores. In general, r may not be known in advance, and thus our Robust CF algorithm needs to detect such parameters. We propose be-low several heuristics to detect r , the number of suspected attack profiles. can be used to chose which matrix to use for computing eigenvalues, especially is one is much smaller. ware at www.caam.rice.edu/software/ARPACK/ Algorithm 1 VarSelectSVD ( D ) 1: D  X  z-scores( D ) { D has N users and M items } 2: U  X  V T = SVD( D ,3) { Get 3 principal components U T } 3: P CA 1  X  U (: , 1), P CA 2  X  U (: , 2), P CA 3  X  U (: , 3) 4: for all columnid user in D do 6: end for 7: Normalize and Sort Score { Score now sum to 1. } 11: Flag top r users with smallest Score values 14: repeat 19: end if 20: until Convergence of  X  G i ,  X  H j for all i, j 21: end for
Finding suspected Attack profiles: PCA can find a set of variables which are highly correlated, a fact exploited in the design of Varselect. Varselect essentially performs Va ri-able Selection using a selection criteria called NLC. There are several other selection procedures discussed in litera ture ([1] provides a good overview of these criteria). Table 1 briefly describe various criteria. While several other crit eria have been proposed as well, they require a complete eigen-decomposition, and/or conditional correlation matrices. As an example, consider B 2 (see Table 1): this strategy in-volves doing a thin SVD, however for selecting k variables, a k -dimension SVD has to be performed. If we want to elim-inate 500 variables, the computation effort involved is very high. In contrast, strategies LC , SLC and QLC require only the first 3 X 5 eigenvectors. Given that our data is not suitable for complete decomposition, we omit other variabl e selection methods.

Each of the strategies described in Table 1 gives a ranked list as output; however a suitable cutoff-point still has to be selected. Table 2 shows the recall of various variable selection strategies. we note that the simplest strategy LC performs the best. The numbers that are reported here, are based on 3 dimensions; we find no further improvement by using more dimensions. We choose the following heuristic: normalize the scores so that the sum to 1, and then choose all user with scores below 1 /n for n users. We observe also that 50% recall is the lowest observed; thus we hypothesize that for attacks of upto 10%, flagging top-20% should suffice. These selected users are known as flagged users.
 The recommendation model is finally based on SVD as well. In essense, we perform SVD on the data matrix treating flagged users in a special manner. To simplify the predic-tion model, we absorb the eigenvalues into the left and right factors, as in the GHA-based SVD method. As in Sec. 3.2, the data matrix is factorized into a factors G and H , such Table 1: Criteria for Variable Selection based on PCA Table 2: Detection Accuracy for various Variable Se-that the Frobenius norm of the remainder is minimized: In the context of Collaborative Filtering, note that the lef t matrix G is user specific, i.e. each user has a corresponding row encoding their hidden preferences. Similarly, the righ t matrix H contains a column for each item. The solution to the above optimization requires iterating through all user votes and performing Hebbian updates via Eq. (6). Every vote potential influences both G and H during the training phase.

Our modification in presence of flagged users is to only update the left vectors and not the right vectors. In other words, the contributions of suspicious users towards the pr e-diction model is zero, while the model can still predict the votes for flagged users. For normal users, we update both left and right vectors as with SVD-GHA. This elegant solu-tion comes with a very small computational cost of checking if a given user is flagged as suspicious. Note also that the model can be initialized with values learnt from the partial SVD performed for PCA. We note that this results in faster convergence for the already computed dimensions. Addi-tionally, we use a regularization parameter  X  (set to 0.01); this step has been found to provide better model fitting and faster convergence. Algorithm 1 describes all steps of our algorithm.

One issue with the above algorithm is that coverage is low for high number of suspected users r . It is possible that some items are voted on mostly by flagged users, hence enough information may not be known even interested users may not be recommended that To improve coverage, we ignore only the extreme votes of the flagged users (i.e. maximum vote 5/5 and minimum 1/5); middle votes can be still used to train the right vectors. This removal significantly weake ns potential bandwagon attacks as well as average attacks, sin ce the largest deviations in prediction occur due to .
As outlined in Algorithm 1, we have implemented an SVD procedure based on Hebbian Learning. We chose the larger MovieLens dataset with 6040 users and 3942 movies. As in previous sections, we add artificial shilling profiles gener ated using the Average attack model. We then apply SVD using Hebbian Learning as the baseline Collaborative Filtering a l-gorithm. 20% of the dataset has been randomly taken out and the used as a test set.

Our experimentation strategy involves adding attack pro-files generated by well established and standard models, and then applying various CF algorithms on this data. Three metrics are crucial to testing robustness of a CF algorithm: these are prediction shift , hit ratio , and mean average error . Prediction Shift measures the change in prediction of the attacked item (before and after attack) of a CF algorithm. This metric is also sensitive to the strength of an attack, with stronger attacks causing a larger prediction shift. P = 1
Hit Ratio measures the effect of attack profiles on top-k recommendations. Since the end effect of a recommender system is a list of items recommended to a particular user, this metric captures the fraction of users affected by shilli ng attacks. Let H u,i = 1 if an item i is a top-k recommendation to user u , and H u,i = 0 otherwise. Hit ratio is a fraction be-tween 0 X 1; when expressed as a percentage (%), it is defined as follows:
Finally, Mean Average Error is the overall prediction er-ror on missing values. We measure MAE over the test set, which contains 20% of all votes in the dataset. MAE is commonly used to compare the predictive accuracy of CF algorithms; here, we are interested in finding out the effect of flagging users. Clearly, if too many users are flagged, the prediction model is trained with lesser data. A comparison of prediction accuracy of our proposed algorithm with the CF on original data (without attack profiles) provides us a measure of accuracy sacrificed for gain in robustness. We vary the attack and filler size of the inserted profiles and measure the Mean average error over the existing votes in the test set. We do not use Prediction shift in order to Table 3: Effect of random Shilling attacks on various 1% 3% 7% 10% make different algorithms comparable; prediction shifts fo r more effective algorithms can give artificially good results as compared to less accurate CF approaches. We also compare our results with CF run on the original dataset without any The impact of shilling attacks: Tables 3 and 4 shows the de-sired results; the proposed Robust CF algorithm performs significantly better than all approaches in all conditions. Our experiments show that VarSelect SVD adds significant robustness over SVD and RMF. For smaller random attacks, VarSelect SVD is virtually unaffected by the presence of at-tack profiles. Note that the hit ratio is close to perfect (0%) for such attacks, and the prediction shift is less than one-fourth that of SVD and a third that of RMF. Till addition of 450 attackers, VarSelect SVD is almost perfect, suffering only when higher filler rates are used. For huge attack vol-umes numbering 10% of the entire user propulation, there is noticible impact. However, this impact is still far less o f SVD or RMF. Note that our baseline methods are already significantly better than k-NN and PLSA; thus the improve-ment due to VarSelect is very significant.

The picture is a little worse for average attacks. Average attacks are stronger than other forms of attack and are also dangerous in small amounts. We note that for large attacks, the impact is higher than for random attacks; prediction shifts increase as the strength of the attack increases. The reason for this is that VarSelect is less accurate for detect ing average attacks; thus shillers who are not flagged continue t o have an impact. This is akin to a setup where is no detection and a small number of shillers are present.

We also notice that hit ratios are lower for all attack sizes than SVD &amp; RMF; occasionally however, we notice a neg-ative hit ratio. This indicates some users (say set U s ) who itself has no shilling profiles. This assumption is unverifie d. Table 4: Effect of Average Shilling attacks on various 1% 3% 7% 10% might have been normally recommended the attacked item, may not be recommended that item any more. On detailed examination we observe that this is a result of users in U being flagged . Thus recommendations for some users may change with VarSelect SVD due to flagging. We finally note that bandwagon attakcs have similar results as random at-tacks, with slightly higher prediction shift; we omit the re -sults due to lack of space.

Predictive performance: We also investigated the predic-tive performance of various CF algorithms on a held-out test set, when shilling attacks are added. Table 6 shows the ef-fect on VarSelect SVD in comparison to other algorithms. We note that all SVD based algorithms in the test perform well and significantly better than k-NN. Also, there is very small departure from the baseline (SVD without attackers); RMF infact outperforms SVD, a result which has also been noted in [12]. VarSelect performs slightly better than SVD, but is less accurate in prediction than RMF. Note however, that all results are in a band of  X  1.5% which is not very significant statistically. The important conlcusion is tha t VarSelect SVD provides additional robust at no additional cost of predictive accuracy.

The impact of parameter  X  r  X : We are also interested in finding how stable the VarSelect algorithm is to the selectio n of the number of flagged users r . We fix r to values between 5 X 90% and run the VarSelect algorithm over a dataset where 350 attackers have been added following an Average Attack model. One caveat though: to improve coverage of predic-tion, we ignore only extreme vote for the flagged users, thus losing only limited data and not complete data; this is also necessary for numerical stability. We know from previous experiments that a 5% attack is a rather large attack, how-ever we expect the model to be more stable at higher fraction of untrusted users. Fig. 1 &amp; 2 provide experimental proof of this hypothesis. As the number of flagged users is increased, we observe a decreasing hit ratio, with zero additional user s being recommended the attacked item. Similarly, the predic -tion shift also decreases quickly at first, and then stabiliz es to a minimum. At higher values of r , we observe a slight in-crease in prediction shift, this is possibly due to a converg e of predicted values towards the user mean, which is what the SVD model predicts in the absense of much data about an item.

Dependence on Dataset: One additional characteristic of the algorithm we explore is data-dependence. It is well known that different datasets show different characteris-tics. Specifically, the small Movielens and large Movielens datasets have different number of users (944 vs 6040), spar-sity (6% vs 4%) and average votes per user. Since [11] re-port much higher detection rates for average attacks with the smaller ML dataset, we verify if there is indeed such a large difference. Our results (Tab. 5) show that indeed VarSelect works much better on the smaller dataset, and has worse results with more data. This shows that the suc-cess of VarSelect is dependant on the characteristics of the dataset; the ranking mechanisms used for Variable Selectio n may have to be modified based on data. This is a direction for future work.
 Table 5: Effect of random Shilling attacks on the smaller 5% 10% 15%
The impact of pure noise: Often, a lot of spam is purely junk, with no specific pattern, but random insertion of data. This phenomenon has been observed both with email spam and web spam. We investigated the effect of random noise on the predictive accuracy of SVD, RMF and Varselect. Such attack profiles have a subset of filler items filled with a uni-form random generator between 1 and 5. We observe that the predictive accuracy of both RMF and VarSelect SVD is higher than SVD-GHA; SVD-GHA suffers from a 2% in-crease in MAE on a hidden test set as compared to SVD without any noise. The impact on RMF and Varselect is less than 0.5%, or too little for statistical significance. N o-tably, attack based on uniform random generators have very low prediction shift and hit ratio.
In this paper, we describe a robust and accurate algorithm for Collaborative Filtering, which is very stable in the fac e of shilling attacks. This algorithm combines the detective accuracy of previously established detection models based on SVD, and is also extremely accurate on rating prediction. A variety of experiments show that attacks of different streng th are rendered much weaker by VarSelect SVD. In addition, the algorithm is highly scalable due to its computational efficiency and use of sparsity.

Directions of future work include improved ranking mech-anisms for eliminating users. We also believe that the di-mension of time brings important information and has not Figure 1: MAE, Pred Shift and Hit ratio for 5% Figure 2: MAE, Pred Shift and Hit ratio for 5% Table 6: Overall MAE on a test set for various CF ap-3% 5% 0.6755 0.6763 0.6694 0.6768 0.8115 5% 5% 0.6755 0.6767 0.6699 0.6761 0.8087 10% 5% 0.6755 0.6774 0.6723 0.6783 0.8062 been exploited in this algorithm; shillings attacks are con -centrated in a short period of time as opposed to real users. We also need to devise more accurate ways of detecting the cutoff parameter to save flagged users from any potential impact. Finally, we would like to explore more about the properties of this algorithm e.g. prediction shifts on item s related to the attacked item.
