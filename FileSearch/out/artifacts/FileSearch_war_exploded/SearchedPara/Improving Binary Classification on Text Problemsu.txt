 We describe an efficient technique to weigh word-based fea-tures in binary classification tasks and show that it signifi-cantly improves classification accuracy on a range of prob-lems. The most common text classification approach uses a document X  X  ngrams (words and short phrases) as its features and assigns feature values equal to their frequency or TFIDF score relative to the training corpus. Our approach uses val-ues computed as the product of an ngram X  X  document fre-quency and the difference of its inverse document frequen-cies in the positive and negative training sets. While this technique is remarkably easy to implement, it gives a sta-tistically significant improvement over the standard bag-of-words approaches using support vector machines on a range of classification tasks. Our results show that our technique is robust and broadly applicable. We provide an analysis of why the approach works and how it can generalize to other domains and problems.
 Categories and Subject Descriptors: I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology-classifier design and evaluation; I.5.4[Pattern Recognition]: Applications-text processing; General Terms: Experimentation Keywords: Text classification, support vector machine, SVM, sentiment. With the vast amounts of content being authored on the Web, text analysis problems such as sentiment search have become increasingly popular. Sentiment search involves find-ing documents that express sentiment about a given topic. This type of search is in great demand in both the public and private sectors. Governmental use of textual sentiment anal-ysis in blogs can help identify potential suicide victims and terrorists. Textual sentiment analysis can also provide busi-ness intelligence for market research, financial investments, and politics.

Many researchers have adopted the vector space model and the bag-of-words machine learning approach. Joachims [2] demonstrated that support vector machines (SVMs) with a bag-of-words vector space are resistant to noise such as spelling and grammatical errors when determining docu-ment topic. Pang [7] demonstrated that the same general technique provides a strong baseline accuracy of 82.7% for sentiment classification in movie reviews. In the bag-of-words model each dimension of the vector space is typically weighted by the count of a specific word or ngram word pair. Later researchers have produced many variations on this basic scheme in an attempt to improve classification accuracy further. Common variants include counting the Boolean presence of words, or weighting the numerical word counts by their inverse document frequency (IDF) scores.
With the exception of IDF, these methods value every feature occurrence equally even though not all features are equal. IDF weights, however, are a poor choice for domain specific datasets. Examining the movie review dataset re-veals that obviously sentimental words like those shown in Table 1 rank in the bottom 0.3% of 300,000 features. Words that express clear value judgments are some of the lowest scoring IDF features for subjectivity detection. Similarly, personal pronouns signal an opinion, but score very low un-der the standard IDF approach. We show how Delta TFIDF fixes these problems and further explain why the approach described in [5] works on a broad range of classification tasks.
In analyzing political speeches, [8] exploited the argument structure found in speaker reference links to help determine how a members of congress would vote given their congres-sional floor speeches. The speakers X  votes were used to de-termine ground truth class labels for the dataset. Manual annotations provide links between the various speakers.
In classifying movie reviews, [6] recognized that reviews often start with predominantly objective plot summary be-fore expressing opinions. They trimmed out such objective content from movie reviews and used an SVM bag of words classifier to determine the sentiment polarity of the remain-der of the review. In determining objective sentences, they cast the task as a graph problem and used the minimum cut between the subjective node and the objective node to form a classifier. To do this they constructed a graph of review sentences cast as nodes and inserted nodes representing a positive negative pole. The distance between sentence nodes and the poles was calculated using their distance from the margin of an additional SVM subjectivity classifier trained on a different set of movie reviews. Then they assigned scores to edges between sentences by their proximity within the review. Next, they found the minimum cut between the positive and negative poles, and threw away the sentences on the objective side. Finally, they trained and tested another SVM bag-of-words classifier on their trimmed reviews.
However, some research take a different approach, instead of trying to classify the documents as a whole they deter-mined sentiment about the features of a product, like a cam-era X  X  picture quality or size. The data they presented in [1] allowed us to test how well our technique picks out features
Given the following definitions: 1. C t,d is the frequency of term t in document d 2. P t is the number of documents in the positively labeled 3. | P | is the number of documents in the positively la-4. N t is the number of documents in the negatively la-5. | N | is the number of documents in the negatively la-6. V t,d is the feature value for term t in document d We can simplify the formula for feature values if we can assume that the training set is balanced, i.e. has approxi-mately the same number of positive and negative examples.
The first equation assigns evenly divided features zero weight, but prefers words that are increasingly unevenly dis-tributed between the positive and negative classes using in-verse document frequency (IDF) values. Prominent or high IDF scoring features in a given class are rarer in that class, their presence in a document indicates that the document does not belong to that class.

Features that are more common in the negative training set than the positive one receive negative scores, perfectly balanced features receive a zero, and predominantly positive features receive positive scores. Regular TFIDF lacks this capability, which allow us to display the top positive and negative features to verify how effective our technique will be for a domain. As Tables 2 and 3 show, the best IDF scoring words for each domain are much less useful than the class specific features determined by our technique.
Delta TFIDF is very accurate for determining positive and negative sentiment words in movie reviews. Not only are the top scoring positive and negative features clearly more sentimental than the features valued by IDF, they are also correctly oriented. Most of our top features are either ob-vious complements, insults, sentimentally expressive words, or sentimental phrases. Mentions of very popular films, such as seven-time Academy Award winner Fargo , correlate with positive sentiment, while mentions of unpopular films are, no surprisingly, just rare. The rest of the top 1000 posi-tive and negative features using Delta TFIDF are just as intuitive and powerful.
 Delta IDF is also very effective for subjectivity detection. Many objective features identified are story related because the reviewer must summarize the plot, this involves talking about how the main character discovers something about his past, or falls in love with some other character, or where the main character receives the help of other characters and de-feats a villain. Subjective features such as  X  X ntertaining X  and  X  X aughs X  express a clear value judgment. Other top subjec-tive features indicate a change of expectation such as  X  X ut X  , or prime the reader for a value judgment with references to the author, the reader, and generic mentions like  X  X he movie X  X  X  .

Expressing political support is complex. While obvious features like  X  X ooking forward X  and  X  X ot oppose X  exist, many features are more complicated. For example,  X   X  X s amended X  indicates support because it shows that both sides have had a chance to compromise, or at least buy votes with pork, and come to an agreement. In politics mentioning inflam-matory issues pertaining to race, religion, discrimination, sex, and party affiliation is a quick way to close down real debate and compromise. Discussing party affiliation is a sure sign that partisan politics are in play. Even mentions of partisanship and bipartisanship are toxic, features such as  X  X arty-line vote X  , and  X  X ipartisan spirit X  were predominantly used by opponents of the bill. If you have to talk about a bipartisan spirit you certainly don X  X  have it. These types of features show up in the top 1000 out of over 300000 fea-tures ranked by Delta TFIDF. Furthermore, these features are ranked much higher by Delta TFIDF than by raw IDF.
Spam classification features as shown in Table 3 are easy to understand. Spammers advertise medications and prod-ucts. Our top 1000 spam features include a long list of in-vestment related terms, pain relief related terms, and terms relating to deals or free stuff. Top not-spam features in-clude terms related to Enron and HPL, which was acquired by Enron. Real business communications frequently involve forwarding messages, communicating with shared coworkers, and attaching documents, many of which are spreadsheets. The 27 th highest not-spam feature is  X .xls X  . Popular first names also feature prominently in emails that are not spam.
Many of the top scoring Delta IDF features are dominated by product sentiment imbalance in the training set. People love their digital cameras, but hate their anti-virus software: camera reviews are positive by at least a nine to one ratio while reviews for anti-virus software are three to one neg-ative. Features like  X  X ery easy X  ,  X  X istake X  ,  X  X o avoid X  , and  X  X  refund X  are present in greater numbers when using Delta TFIDF than when using regular IDF weights.

We show that our technique finds features that correspond to human judgments using the data in [1]. Many of our top features agree with Liu X  X  hand annotated product-specific attributes. Our top discovered positive features for cam-era size include  X  X o small X  ,  X  X s small X  ,  X  X ery small X  ,  X  X mall X  ,  X  X mall , X  ,  X  X mall and X  , and  X  X  small X  . As you would expect for cameras, none of the top 1000 negative features con-tained the word small. Top positive features for the camera picture attribute include  X  X uality pictures X  and  X  X reat pic-tures X  while the top negative features include  X  X icture after X  and  X  X o picture X  .

The difference of the IDF scores must be multiplied by the number of occurrences for that feature to produce its Delta TFIDF score. The movie review used in Table 4 shows that Delta TFIDF X  X  top scoring features are clearly more sentimental than either TFIDF or plain term frequencies when used to represent a document. TFIDF X  X  top scoring features appear to be the topics of the review. The top raw terms are dominated by stop words. In this example Delta TFIDF places a much greater weight on sentimental words than either of the alternatives.
Tables 2 and 3 show evidence for the discriminative power of our top features. Using these weighted features to repre-sent data points provides a statistically significant improve-ment to state-of-the-art machine classification accuracy. Table 5: This table summarizes the accuracy of Delta TFIDF and term frequency for binary clas-sification on our five datasets, showing the 10 fold cross validation accuracy and variance. Boldface re-sults are significant to the 98% confidence level. annotated for overall product opinion, we labeled them for sentiment using the sum of product feature scores. While this method does not account for the importance of differ-ent product feature to the reviewer, we believe it is a good approximation for human labels. This could be one reason why the results in Table 5 fall just short of a 95% confidence interval. With a P value of .0509 we can still be reasonably confident that our results represent a modest gain.
We suspect that a cause for this weaker result is an imbal-ance of class labels for each product. The dataset consists of 399 positive documents and 198 negative documents, but the actual imbalance for any given product type is much worse. Canon cameras had 75 positive but only eight neg-ative reviews  X  not even enough negative Canon reviews to put one in each fold! Reviewer X  X  are also overwhelmingly positive about routers. Norton AntiVirus has 32 negative reviews to nine positive ones. This distorts the distribution of product specific features.

Our technique produces significant improvements over the baseline for other kinds of binary classification problems as well, including spam detection and predicting a congress member X  X  vote on a bill given their comments about it. Our baseline congressional classifier matches the method de-scribed in [8] for SVM speech classification and produces equivalent results, but Delta TFIDF improves on the base-line with a P value of .000582. Our Enron email spam clas-sifier out-performs the baseline with a P value of 1 . 7  X  10  X  5 .
Most bag-of-words approaches weight features using only a function of their occurrence count in the document. TFIDF is a notable exception where the term frequency of a feature is multiplied by its pre-computed IDF score in the corpus. However, IDF weights are a bad choice for domain specific datasets because they prefer rare features. Since the doc-uments are all from the same domain the most descriptive features will be much more frequent than normal resulting in some of the best features having the worst IDF scores.
