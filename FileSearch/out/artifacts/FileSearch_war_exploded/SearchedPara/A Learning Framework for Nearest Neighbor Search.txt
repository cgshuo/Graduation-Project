 Nearest neighbor (NN) searching is a fundamental operation in machine learning, databases, signal processing, and a variety of other disciplines. We have a database of points X = { x 1 ,...,x n } , and on an input query q , we hope to return the nearest (or approximately nearest, or k -nearest) point(s) to q in X using some similarity measure.
 A tremendous amount of research has been devoted to designing data structures for fast NN retrieval. Most of these structures are based on some clever partitioning of the space and a few have bounds (typically worst-case) on the number of distance calculations necessary to query it.
 learning . In contrast to the various data structures built using geometric intuitions, this learning framework allows one to construct a data structure by directly minimizing the cost of querying it. In our framework, a sample query set guides the construction of the data structure containing the database. In the absence of a sample query set, the database itself may be used as a reasonable prior. The problem of building a NN data structure can then be cast as a learning problem: A major benefit of this framework is that one can seamlessly handle situations where the query distribution is substantially different from the distribution of the database.
 We consider two different function classes that have performed well in NN searching: KD-trees examine the generalization properties of a data structure learned from one of these classes. We derive generalization bounds for both of these classes in this paper.
 Can the framework be practically applied? We present very simple learning algorithms for both of these data structure classes that exhibit improved performance over their standard counterparts. There is a voluminous literature on data structures for nearest neighbor search, spanning several academic communities. Work on efficient NN data structures can be classified according to two criteria: whether they return exact or approximate answers to queries; and whether they merely assume the distance function is a metric or make a stronger assumption (usually that the data are Euclidean). The framework we describe in this paper applies to all these methods, though we focus in particular on data structures for R D .
 Perhaps the most popular data structure for nearest neighbor search in R D is the simple and con-venient KD-tree [1], which has enjoyed success in a vast range of applications. Its main downside is that its performance is widely believed to degrade rapidly with increasing dimension. Variants of the data structure have been developed to ameliorate this and other problems [2], though high-dimensional databases continue to be challenging. One recent line of work suggests randomly pro-jecting points in the database down to a low-dimensional space, and then using KD-trees [3, 4]. Locality sensitive hashing (LSH) has emerged as a promising option for high-dimensional NN search in
R D [5]. It has strong theoretical guarantees for databases of arbitrary dimensionality, though they are for approximate NN search. We review both KD-trees and LSH in detail later.
 For data in metric spaces, there are several schemes based on repeatedly applying the triangle in-equality to eliminate portions of the space from consideration; these include Orchard X  X  algorithm [6] and AESA [7]. Metric trees [8] and the recently suggested spill trees [3] are based on similar ideas and are related to KD-trees. A recent trend is to look for data structures that are attuned to the intrinsic dimension , e.g. [9]. See the excellent survey [10] for more information.
 this line of work is perhaps most similar to ours. Indeed, we discovered at the time of press that the algorithm for KD-trees we describe appeared previously in [12]. Nevertheless, the learning theoretic approach in this paper is novel; the study of NN data structures through the lens of generalization ability provides a fundamentally different theoretical basis for NN search with important practical implications. In this section we formalize a learning framework for NN search. This framework is quite general and will hopefully be of use to algorithmic developments in NN searching beyond those presented in this paper.
 Let X = { x 1 ,...,x n } denote the database and Q the space from which queries are drawn. A typical example is X  X  R D and Q = R D . We take a nearest neighbor data structure to be a mapping f : Q  X  2 X ; the interpretation is we compute distances only to f ( q ) , not all of X . For example, the structure underlying LSH partitions R D into cells and a query is assigned to the subset of X that falls into the same cell.
 What quantities are we interested in optimizing? We want to only compute distances to a small fraction of the database on a query; and, in the case of probabilistic algorithms, we want a high probability of success. More precisely, we hope to minimize the following two quantities for a data structure f : In -approximate NN search, we only require a point x such that d ( q,x )  X  (1 + ) d ( q,X ) , so we instead use an approximate miss rate: None of the previously discussed data structures are built by explicitly minimizing these quantities, focused on worst-case size f and miss f rates, which require minimizing these functions over all q  X  X  . Q is typically infinite of course.
 In this work, we instead focus on average-case size f and miss f rates X  i.e. we assume q is a draw from some unknown distribution D on Q and hope to minimize To do so, we assume that we are given a sample query set Q = { q 1 ,...,q m } drawn iid from D . We attempt to build f minimizing the empirical size and miss rates, then resort to generalization bounds to relate these rates to the true ones. We propose two learning algorithms in this section. The first is based on a splitting rule for KD-trees designed to minimize a greedy surrogate for the empirical size f function. The second is a algorithm that determines the boundary locations of the cell structure used in LSH that minimize a tradeoff of the empirical size f and miss f functions. 4.1 KD-trees KD-trees are a popular cell partitioning scheme for R D based on the binary search paradigm. The data structure is built by picking a dimension, splitting the database along the median value in that dimension, and then recursing on both halves. procedure B UILD T REE ( S ) if | S | &lt; MinSize , return leaf. else : Pick an axis i .
 To find a NN for a query q , one first computes distances to all points in the same cell, then traverses up the tree. At each parent node, the minimum distance between q and points already explored is compared to the distance to the split. If the latter is smaller, then the other child must be explored. Typically the cells contain only a few points; a query is expensive because it lies close to many of the cell boundaries and much of the tree must be explored.
 Learning method Rather than picking the median split at each level, we use the training queries q i to pick a split that greedily minimizes the expected cost. A split s divides the sample queries (that are in the cell being require exploring both sides of the split. The split also divides the database points (that are in the cell being split) into X l and X r . The cost of split s is then defined to be at most 2 m + n possible values and each can be evaluated quickly. Using a sample set led us to a very simple, natural cost function that can be used to pick splits in a principled manner. 4.2 Locality sensitive hashing LSH was a tremendous breakthrough in NN search as it led to data structures with provably sublinear (in the database size) retrieval time for approximate NN searching. More impressive still, the bounds on retrieval are independent of the dimensionality of the database. We focus on the LSH scheme for the k X k p norm ( p  X  (0 , 2] ), which we refer to as LSH p . It is built on an extremely simple space partitioning scheme which we refer to as a rectilinear cell structure (RCS). procedure B UILD RCS( X  X  R D ) Let R  X  R O (log n )  X  d with R ij iid draws from a p -stable distribution. 1 Project database down to O (log n ) dimensions: x i 7 X  Rx i .
 Uniformly grid the space with B bins per direction.
 See figure 3, left panel, for an example. On query q , one simply finds the cell that q belongs to, and returns the nearest x in that cell.
 In general, LSH p requires many RCSs, used in parallel, to achieve a constant probability of success; in many situations one may suffice [13]. Note that LSH p only works for distances at a single scale R : the specific guarantee is that LSH p will return a point x  X  X within distance (1 + ) R of q as long as d ( q,X ) &lt; R . To solve the standard approximate NN problem, one must build O (log( n/ )) LSH p structures.
 Learning method We apply our learning framework directly to the class of RCSs since they are the core structural component of LSH p . We consider a slightly wider class of RCSs where the bin widths are allowed to vary. Doing so potentially allows a single RCS to work at multiple scales if the bin positions are chosen appropriately. We give a simple procedure that selects the bin boundary locations. tradeoff parameter (alternatively, one could fix a miss rate that is reasonable, say 5%, and minimize the size). The optimization is performed along one dimension at a time. Fortunately, the optimal binning along a dimension can be found by dynamic programming. There are at most m + n possible can be decomposed as c [ p 1 ,p 2 ] +  X  X  X  + c [ p B ,p B +1 ] , where Let D be our dynamic programming table where D [ p,i ] is defined as the cost of putting the i th D [ p 0 ,i  X  1] . In our framework, a nearest neighbor data structure is learned by specifically designing it to per-form well on a set of sample queries. Under what conditions will this search structure have good performance on future queries? Recall the setting: there is a database X = { x 1 ,...,x n } , sample queries Q = { q 1 ,...,q m } drawn iid from some distribution D on Q , and we wish to learn a data structure f : Q X  2 X drawn from a this section).
 Suppose a data structure f is chosen from some class F , so as to have low empirical cost Can we then conclude that data structure f will continue to perform well for subsequent queries drawn from the underlying distribution on Q ? In other words, are the empirical estimates above necessarily close to the true expected values E q  X  X  size f ( q ) and E q  X  X  miss f ( q ) ? There is a wide range of uniform convergence results which relate the difference between empirical and true expectations to the number of samples seen (in our case, m ) and some measure of the complexity of the two classes { size f : f  X  F} and { miss f : f  X  F} . The following is particularly convenient to use, and is well-known [14, theorem 3.2].
 Theorem 1. Let G be a set of functions from a set Z to [0 , 1] . Suppose a sample z 1 ,...,z m is drawn from some underlying distribution on Z . Let G m denote the restriction of G to these samples, that is, Then for any  X  &gt; 0 , the following holds with probability at least 1  X   X  : This can be applied immediately to the kind of data structure used by LSH.
 cells given by where each h i : R  X  X  1 ,...,B } is a partition of the real line into B intervals.
 Theorem 3. Fix any vectors u 1 ,...,u d  X  R D , and, for some positive integer B , let the set of data points X  X  R D . Suppose there is an underlying distribution over queries in R D , from which m sample queries q 1 ,...,q m are drawn. Then and likewise for size f .
 Proof. Fix any X = { x 1 ,...,x n } and any q 1 ,...,q m . In how many ways can these points be there are B  X  1 boundaries to be chosen and only m + n distinct locations for each of these (as far carve up the points. Thus the functions { miss f : f  X  X } (or likewise, { size f : f  X  X } ) collapse to a set of size just ( m + n ) d ( B  X  1) when restricted to m queries; the rest follows from theorem 1. This is good generalization performance because it depends only on the projected dimension, not the original dimension. It holds when the projection directions u 1 ,...,u d are chosen randomly, but, more remarkably, even if they are chosen based on X (for instance, by running PCA on X ). If we learn the projections as well (instead of using random ones) the bound degrades substantially. Theorem 4. Consider the same setting as Theorem 3, except that now F ranges over ( u 1 ,...,u d ,B ) -rectilinear cell structures for all choices of u 1 ,...,u d  X  R D . Then with proba-bility at least 1  X   X  , and likewise for size f . Figure 1: Left : Outer ring is the database; inner cluster of points are the queries. Center : KD-tree with standard median splits. Right : KD-tree with learned splits.
 KD-trees are slightly different than RCSs: the directions u i are simply the coordinate axes, and the number of partitions per direction varies ( e.g. one direction may have 10 partitions, another only 1). Theorem 5. Let F be the set of all depth  X  KD-trees in R D and X  X  R D be a database of points. Suppose there is an underlying distribution over queries in R D from which q 1 ,...q m are drawn. Then with probability at least 1  X   X  , A KD-tree utilizing median splits has depth  X   X  log n . The depth of a KD-tree with learned splits can be higher, though we found empirically that the depth was always much less than 2 log n (and can of course be restricted manually). KD-trees require significantly more samples than RCSs to generalize; the class of KD-trees is much more complex than that of RCSs. 6.1 KD-trees First let us look at a simple example comparing the learned splits to median splits. Figure 1 shows a 2-dimensional dataset and the cell partitions produced by the learned splits and the median splits. The KD-tree constructed with the median splitting rule places nearly all of the boundaries running right through the queries. As a result, nearly the entire database will have to be searched for queries drawn from the center cluster distribution. The KD-tree with the learned splits places most of the boundaries right around the actual database points, ensuring that fewer leaves will need to be exam-ined for each query.
 We now show results on several datasets from the UCI repository and 2004 KDD cup competition. in which KD-trees are typically applied. These experiments were all conducted using a modified version of Mount and Arya X  X  excellent KD-tree software [15]. For this set of experiments, we used a randomly selected subset of the dataset as the database and a separate small subset as the test queries. For the sample queries, we used the database itself X  i.e. no additional data was used to build the learned KD-tree .
 The following table shows the results. We compare performance in terms of the average number of database points we have to compute distances to on a test set. The learned method outperforms the standard method on all of the datasets, showing a very large im-provement on several of them. Note also that even the standard method exhibits good performance, Figure 2: Percentage of DB examined as a function of (the approximation factor) for various query distributions.
 often requiring distance calculations to less than one percent of the database. We are showing strong improvements on what are already quite good results.
 We additionally experimented with the  X  X orel50 X  image dataset. It is divided into 50 classes ( e.g. air shows, bears, tigers, Fiji) containing 100 images each. We used the 371-dimensional  X  X emantic space X  representation of the images recently developed in a series of image retrieval papers (see e.g. [16]). This dataset allows us to explore the effect of differing query and database distributions in a natural setting. It also demonstrates that KD-trees with learned parameters can perform well on high-dimensional data.
 Figure 2 shows the results of running KD-trees using median and learned splits. In each case, 4000 images were chosen for the database (from across all the classes) and images from select classes were chosen for the queries. The  X  X ll X  queries were chosen from all classes; the  X  X nimals X  were chosen from the 11 animal classes; the  X  X . American animals X  were chosen from 5 of the animal classes; and the  X  X ears X  were chosen from the two bear classes. Standard KD-trees are performing somewhat better than brute force in these experiments; the learned KD-trees yield much faster re-trieval times across a range of approximation errors. Note also that the performance of the learned KD-tree seems to improve as the query distribution becomes simpler whereas the performance for the standard KD-tree actually degrades. 6.2 RCS/LSH Figure 3 shows a sample run of the learning algorithm. The queries and DB are drawn from the same distribution. The learning algorithm adjusts the bin boundaries to the regions of density. Experimenting with RCS structures is somewhat challenging since there are two parameters to set (number of projections and boundaries), an approximation factor , and two quantities to compare (size and miss). We swept over the two parameters to get results for the standard RCSs. Results for learned RCSs were obtained using only a single (essentially unoptimized) parameter setting. Rather than minimizing a tradeoff between size f and miss f , we constrained the miss rate and optimized the size f . The constraint was varied between runs (2%, 4%, etc. ) to get comparable results. Figure 4 shows the comparison on databases of 10k points drawn from the MNIST and Physics datasets (2.5k points were used as sample queries). We see a marked improvement for the Physics dataset and a small improvement for the MNIST dataset. We suspect that the learning algorithm helps substantially for the physics data because the one-dimensional projections are highly non-uniform whereas the MNIST one-dimensional projections are much more uniform. The primary contribution of this paper is demonstrating that building a NN search structure can be fruitfully viewed as a learning problem. We used this framework to develop algorithms that learn RCSs and KD-trees optimized for a query distribution. Possible future work includes applying the learning framework to other data structures, though we expect that even stronger results may be obtained by using this framework to develop a novel data structure from the ground up. On the theoretical side, margin-based generalization bounds may allow the use of richer classes of data structures.
 Acknowledgments We are grateful to the NSF for support under grants IIS-0347646 and IIS-0713540. Thanks to Nikhil Rasiwasia, Sunhyoung Han, and Nuno Vasconcelos for providing the Corel50 data.

